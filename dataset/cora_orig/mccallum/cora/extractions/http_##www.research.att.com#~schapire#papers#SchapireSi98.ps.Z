URL: http://www.research.att.com/~schapire/papers/SchapireSi98.ps.Z
Refering-URL: http://www.research.att.com/~schapire/publist.html
Root-URL: 
Email: fschapire,singerg@research.att.com  
Title: Improved Boosting Algorithms Using Confidence-rated Predictions  
Author: Robert E. Schapire Yoram Singer 
Address: 180 Park Avenue, Florham Park, NJ 07932  
Affiliation: AT&T Labs,  
Note: Proceedings of the Eleventh Annual Conference on ComputationalLearning Theory, 1998.  
Abstract: We describe several improvements to Freund and Schapire's AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Peter L. Bartlett. </author> <title> The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. </title> <journal> IEEE Transactions on Information Theory, </journal> <note> 1998 (to appear). </note>
Reference-contexts: Our version differs from Freund and Schapire's in that (1) weak hypotheses can have range over all of R rather than the restricted range <ref> [1; +1] </ref> assumed by Freund and Given: (x 1 ; y 1 ); : : : ; (x m ; y m ) ; x i 2 X , y i 2 f1; +1g Initialize D 1 (i) = 1=m. <p> Despite these differences, we continue to refer to the algorithm of Figure 1 as AdaBoost. As discussed below, when the range of each h t is restricted to <ref> [1; +1] </ref>, we can choose ff t appropriately to obtain Freund and Schapire's original AdaBoost algorithm (ignoring superficial differences in notation). Here, we give a simplified analysis of the algorithm in which ff t is left unspecified. <p> We describe a number of methods for this purpose. 3.1 Deriving Freund and Schapire's choice of ff t We begin by showing how Freund and Schapire's [10] version of AdaBoost can be derived as a special case of our new version. For weak hypotheses h with range <ref> [1; +1] </ref>, their choice of ff can be obtained by approximating Z as follows: Z = i i 2 1 u i e ff : (3) This upper bound is valid since u i 2 [1; +1], and is in fact exact if h has range f1; +1g (so that u <p> For weak hypotheses h with range <ref> [1; +1] </ref>, their choice of ff can be obtained by approximating Z as follows: Z = i i 2 1 u i e ff : (3) This upper bound is valid since u i 2 [1; +1], and is in fact exact if h has range f1; +1g (so that u i 2 f1; +1g). Next, we can analytically choose ff to minimize the right hand side of Eq. (3) giving ff = 1 1 r where r = P i D (i)u i . <p> Plugging into Eq. (3), this choice gives the upper bound Z 1 r 2 : We have thus proved the following corollary of Theorem 1 which is equivalent to Freund and Schapire's Theorem 6 [10]: Corollary 2 ([10]) Using the notation of Figure 1, assume each h t has range <ref> [1; +1] </ref> and that we choose ff t = 1 1 r t where r t = i fi fl Then the training error of H is at most T Y q t : Thus, with this setting of ff t , it is reasonable to try to find h t <p> It is closely related to ordinary error since, if h t has range f1; +1g then Pr i~D t h t (x i ) 6= y i = 2 so maximizing r t is equivalent to minimizing error. More generally, if h t has range <ref> [1; +1] </ref> then (1 r t )=2 is equivalent to the definition of error used by Freund and Schapire (* t in their notation). The approximation used in Eq. (3) is essentially a linear upper bound of the function e ffx on the range x 2 [1; +1]. <p> h t has range <ref> [1; +1] </ref> then (1 r t )=2 is equivalent to the definition of error used by Freund and Schapire (* t in their notation). The approximation used in Eq. (3) is essentially a linear upper bound of the function e ffx on the range x 2 [1; +1]. Clearly, other upper bounds which give a tighter approximation could be used instead, such as a quadratic or piecewise linear approximation. 3.2 A numerical method for the general case We next give a general numerical method for exactly mini mizing Z with respect to ff. <p> This analysis predicts that AdaBoost will overfit if run for too many rounds. Schapire et al. [20] proposed an alternative analysis to explain AdaBoost's empirically observed resistance to over-fitting. Following the work of Bartlett <ref> [1] </ref>, this method is based on the margins achieved by the final hypothesis on the training examples. The margin is a measure of the confidence of the prediction. Schapire et al. show that larger margins imply lower generalization error regardless of the number of rounds. <p> f (x) = t Since the h t 's are bounded and since we only care about the sign of f , we can rescale the h t 's and normalize the ff t 's allowing us to assume without loss of generality that each h t : X ! <ref> [1; +1] </ref>, each ff t 2 [0; 1] and P also assume that each h t belongs to a hypothesis space H. Schapire et al. define the margin of a labeled example (x; y) to be yf (x). The margin then is in [1; +1], and is positive if and only <p> h t 's are bounded and since we only care about the sign of f , we can rescale the h t 's and normalize the ff t 's allowing us to assume without loss of generality that each h t : X ! [1; +1], each ff t 2 <ref> [0; 1] </ref> and P also assume that each h t belongs to a hypothesis space H. Schapire et al. define the margin of a labeled example (x; y) to be yf (x). <p> that each h t : X ! <ref> [1; +1] </ref>, each ff t 2 [0; 1] and P also assume that each h t belongs to a hypothesis space H. Schapire et al. define the margin of a labeled example (x; y) to be yf (x). The margin then is in [1; +1], and is positive if and only if H makes a correct prediction on this example. We further regard the magnitude of the margin as a measure of the confidence of H's prediction. <p> Thus, H : X ! 2 Y and, with respect to a distribution D, the loss is 1 E (x;Y )~D jh (x) D Y j where D denotes symmetric difference. (The leading 1=k is meant merely to ensure a value in <ref> [0; 1] </ref>.) We call this measure the Hamming loss of H, and we denote it by hloss D (H). To minimize Hamming loss, we can naturally decompose the problem into k orthogonal binary classification problems. <p> Since exact analytic solutions seem hard to come by for ranking loss, we next consider approximations such as those in Section 3.1. Assuming weak hypotheses h with range in <ref> [1; +1] </ref>, we can use the same approximation of Eq. (3) which yields Z 1 r 2 e ff where r = 1 X D (i; ` 0 ; ` 1 )(h (x i ; ` 1 ) h (x i ; ` 0 )): (20) As before, the right hand
Reference: [2] <author> Eric Bauer and Ron Kohavi. </author> <title> An empirical comparison of voting classification algorithms: Bagging, boosting, and variants. </title> <type> Unpublished manuscript, </type> <year> 1997. </year>
Reference: [3] <author> Eric B. Baum and David Haussler. </author> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1(1) </booktitle> <pages> 151-160, </pages> <year> 1989. </year>
Reference-contexts: The VC-dimension of the final hypothesis can be computed using the methods of Baum and Haussler <ref> [3] </ref>. This analysis predicts that AdaBoost will overfit if run for too many rounds. Schapire et al. [20] proposed an alternative analysis to explain AdaBoost's empirically observed resistance to over-fitting.
Reference: [4] <author> Avrim Blum. </author> <title> Empirical support for winnow and weighted-majority based algorithms: results on a calendar scheduling domain. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 64-72, </pages> <year> 1995. </year>
Reference-contexts: No other levels of confidence are allowed. By allowing the weak hypothesis to effectively say I don't know, we introduce a model analogous to the specialist model of Blum <ref> [4] </ref>, studied further by Freund et al. [11].
Reference: [5] <author> Leo Breiman. </author> <title> Arcing classifiers. </title> <journal> Annals of Statistics, </journal> <note> to appear. </note>
Reference: [6] <author> Thomas G. Dietterich. </author> <title> An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization. </title> <type> Unpublished manuscript, </type> <year> 1998. </year>
Reference: [7] <author> Thomas G. Dietterich and Ghulum Bakiri. </author> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 263-286, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: This algorithm is simpler but apparently as effective as the methods given by Freund and Schapire. In addition, we propose a modification of this method which combines these techniques with Dietterich and Bakiri's <ref> [7] </ref> output-coding method. In the second extension to multi-label problems, the learned hypothesis instead predicts, for a given instance, a ranking of the labels, and it is evaluated based on its ability to place the correct labels high in this ranking. <p> Let k 0 = jY 0 j. It is desirable to choose to be a function which maps different labels to sets which are far from one another, say, in terms of their symmetric difference. This is essentially the approach advocated by Dietterich and Bakiri <ref> [7] </ref> in a somewhat different setting. 1 Specifically, they suggested using error correcting codes which are designed to have exactly this property.
Reference: [8] <author> Harris Drucker and Corinna Cortes. </author> <title> Boosting decision trees. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 479-485, </pages> <year> 1996. </year>
Reference: [9] <author> Yoav Freund and Robert E. Schapire. </author> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 148-156, </pages> <year> 1996. </year>
Reference-contexts: Note, however, that discrete AdaBoost.MR and discrete AdaBoost.MH are equivalent algorithms for two-class problems. We compared the three algorithms on a collection of benchmark problems available from the repository at University of California at Irvine [17]. We used the same experimental set-up as Freund and Schapire <ref> [9] </ref>. Namely, if a test binary and multiclass benchmark problems from the UCI repository. (See caption for Figure 6.) set was already provided, experiments were run 20 times and the results averaged (since some of the learning algorithms may be randomized). <p> We tested on the same set of benchmarks, except that we dropped the vowel dataset. Each version of AdaBoost was run for 1000 rounds. We used the simplest of the weak learners tested by Freund and Schapire <ref> [9] </ref>. This weak learner finds a weak hypothesis which makes its prediction based on the result of a single test comparing one of the attributes to one of its possible values. For discrete attributes, equality is tested; for continuous attributes, a threshold value is compared.
Reference: [10] <author> Yoav Freund and Robert E. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 55(1) </volume> <pages> 119-139, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: The result is a powerful set of boosting methods for handling more expressive weak hypotheses, as well as an advanced methodology for designing weak learners appropriate for use with boosting algorithms. We base our work on Freund and Schapire's <ref> [10] </ref> Ada-Boost algorithm which has received extensive empirical and theoretical study [2, 5, 6, 7, 8, 9, 15, 16, 18, 19, 20, 22]. <p> Such problems arise naturally, for instance, in text categorization problems where the same document (say, a news article) may easily be relevant to more than one topic (such as politics, sports, etc.). Freund and Schapire <ref> [10] </ref> gave two algorithms for boosting multiclass problems, but neither was designed to handle the multi-label case. In this paper, we present two new extensions of AdaBoost for multi-label problems. <p> The idea of boosting is to use the weak learner to form a highly accurate prediction rule by calling the weak learner repeatedly on different distributions over the training examples. A slightly generalized version of Freund and Schapire's Ada-Boost algorithm <ref> [10] </ref> is shown in Figure 1. <p> Our goal is to find ff which minimizes or approximately minimizes Z as a function of ff. We describe a number of methods for this purpose. 3.1 Deriving Freund and Schapire's choice of ff t We begin by showing how Freund and Schapire's <ref> [10] </ref> version of AdaBoost can be derived as a special case of our new version. <p> Plugging into Eq. (3), this choice gives the upper bound Z 1 r 2 : We have thus proved the following corollary of Theorem 1 which is equivalent to Freund and Schapire's Theorem 6 <ref> [10] </ref>: Corollary 2 ([10]) Using the notation of Figure 1, assume each h t has range [1; +1] and that we choose ff t = 1 1 r t where r t = i fi fl Then the training error of H is at most T Y q t : Thus, <p> + 2 W W + : (4) For this case, Freund and Schapire's original AdaBoost algorithm would instead have made the more conservative choice ff = 1 2 W 0 2 W 0 giving a value of Z which is necessarily inferior to Eq. (4), but which Freund and Schapire <ref> [10] </ref> are able to upper bound by q 2 W 0 )(W + + 1 If W 0 = 0 (so that h has range f1; +1g), then the choices of ff and resulting values of Z are identical. 4 A Criterion for Finding Weak Hypotheses So far, we have only <p> Two methods of analyzing the generalization error of AdaBoost have been proposed. The first, given by Freund and Schapire <ref> [10] </ref>, uses standard VC-theory to bound the gen eralization error of the final hypothesis in terms of its training error and an additional term which is a function of the VC-dimension of the final hypothesis and the number of training examples. <p> Indeed, Freund and Schapire's pseudoloss-based algorithm AdaBoost.M2 <ref> [10] </ref> is a special case of the use of ranking loss in which all data are single-labeled, the weak learner attempts to maximize jr t j as in Eq. (20), and ff t is set as in Eq. (21). <p> Note that, in the single-label case, this algorithm is identical to Freund and Schapire's <ref> [10] </ref> AdaBoost.M2 algorithm. We used these algorithms for two-class and multiclass problems alike. Note, however, that discrete AdaBoost.MR and discrete AdaBoost.MH are equivalent algorithms for two-class problems. We compared the three algorithms on a collection of benchmark problems available from the repository at University of California at Irvine [17].
Reference: [11] <author> Yoav Freund, Robert E. Schapire, Yoram Singer, and Man-fred K. Warmuth. </author> <title> Using and combining predictors that specialize. </title> <booktitle> In Proceedings of the Twenty-Ninth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 334-343, </pages> <year> 1997. </year>
Reference-contexts: No other levels of confidence are allowed. By allowing the weak hypothesis to effectively say I don't know, we introduce a model analogous to the specialist model of Blum [4], studied further by Freund et al. <ref> [11] </ref>.
Reference: [12] <author> David Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100(1) </volume> <pages> 78-150, </pages> <year> 1992. </year>
Reference-contexts: This case is not of much interest, however, since our focus is on weak hypotheses with real-valued predictions. To extend the margins theory, then, let us define d to be the pseudodimension of H (for definitions, see, for instance, Haussler <ref> [12] </ref>). Then using the method sketched 5 Given: (x 1 ; Y 1 ); : : : ; (x m ; Y m ) where x i 2 X , Y i Y Initialize D 1 (i; `) = 1=(mk).
Reference: [13] <author> David Haussler and Philip M. </author> <title> Long. A generalization of Sauer's lemma. </title> <journal> Journal of Combinatorial Theory, Series A, </journal> <volume> 71(2) </volume> <pages> 219-240, </pages> <year> 1995. </year>
Reference-contexts: Output the final hypothesis: H (x; `) = sign t=1 ! in Section 2.4 of Schapire et al. together with Haussler and Long's <ref> [13] </ref> Lemma 13, we can prove the following upper bound on generalization error which holds with probability 1 ffi for all &gt; 0 and for all f of the form above: Pr S yf (x) + p 2 + log (1=ffi) : Here, Pr S denotes probability with respect to choosing
Reference: [14] <author> Michael Kearns and Yishay Mansour. </author> <title> On the boosting ability of top-down decision tree learning algorithms. </title> <booktitle> In Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, </booktitle> <year> 1996. </year>
Reference-contexts: For growing decision trees, this measure turns out to be identical to one earlier proposed by Kearns and Mansour <ref> [14] </ref>. Although we primarily focus on minimizing training error, we briefly outline methods that can be used to analyze generalization error as well. <p> In other words, the decision tree could be built by greedily choosing the split which causes the greatest drop in the value of the function given in Eq. (9). In fact, exactly this splitting criterion was proposed by Kearns and Mansour <ref> [14] </ref>.
Reference: [15] <author> Richard Maclin and David Opitz. </author> <title> An empirical evaluation of bagging and boosting. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 546-551, </pages> <year> 1997. </year>
Reference: [16] <author> Dragos D. Margineantu and Thomas G. Dietterich. </author> <title> Pruning adaptive boosting. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference, </booktitle> <pages> pages 211-218, </pages> <year> 1997. </year>
Reference: [17] <author> C. J. Merz and P. M. Murphy. </author> <title> UCI repository of machine learning databases, </title> <note> 1998. http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: We used these algorithms for two-class and multiclass problems alike. Note, however, that discrete AdaBoost.MR and discrete AdaBoost.MH are equivalent algorithms for two-class problems. We compared the three algorithms on a collection of benchmark problems available from the repository at University of California at Irvine <ref> [17] </ref>. We used the same experimental set-up as Freund and Schapire [9].
Reference: [18] <author> J. R. Quinlan. Bagging, </author> <title> boosting, </title> <booktitle> and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 725-730, </pages> <year> 1996. </year>
Reference-contexts: This method turns out to be closely related to a heuristic method proposed by Quinlan <ref> [18] </ref> for boosting decision trees. Our analysis can be viewed as a partial theoretical justification for his experimentally successful method. Our technique also leads to a modified criterion for selecting such domain-partitioning weak hypotheses. <p> Moreover, c j will be close to zero (a low confidence prediction) if there is a roughly equal split of positive and negative examples in block j. Likewise, c j will be far from zero if one label strongly predominates. A similar scheme was previously proposed by Quin-lan <ref> [18] </ref> for assigning confidences to the predictions made at the leaves of a decision tree. Although his scheme differed in the details, we feel that our new theory provides some partial justification for his method.
Reference: [19] <author> Robert E. Schapire. </author> <title> Using output codes to boost multiclass learning problems. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference, </booktitle> <pages> pages 313-321, </pages> <year> 1997. </year>
Reference-contexts: Once a function has been chosen we can apply AdaBoost.MH directly on the transformed training data (x i ; (y i )). How then do we classify a new instance x? The most direct use of Dietterich and Bakiri's approach is to evaluate 1 Schapire <ref> [19] </ref> proposes another method of combining boosting and output coding. Although superficially similar, his method is in fact quite different from what is presented here.
Reference: [20] <author> Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. </author> <title> Boosting the margin: A new explanation for the effectiveness of voting methods. </title> <journal> Annals of Statistics, </journal> <note> to appear. </note>
Reference-contexts: The VC-dimension of the final hypothesis can be computed using the methods of Baum and Haussler [3]. This analysis predicts that AdaBoost will overfit if run for too many rounds. Schapire et al. <ref> [20] </ref> proposed an alternative analysis to explain AdaBoost's empirically observed resistance to over-fitting. Following the work of Bartlett [1], this method is based on the margins achieved by the final hypothesis on the training examples. The margin is a measure of the confidence of the prediction.
Reference: [21] <author> Robert E. Schapire and Yoram Singer. BoosTexter: </author> <title> A system for multiclass multi-label text categorization. </title> <type> Unpublished manuscript, </type> <year> 1998. </year>
Reference-contexts: By 1,000 rounds, this gap in test error has narrowed somewhat to 19.7%, 17.6% and 16.4%. Finally, we give results for a large text-categorization problem. More details of our text-categorization experiments are described in a companion paper <ref> [21] </ref>. In this problem, there are six classes: DOMESTIC, ENTERTAINMENT, FINANCIAL, INTERNATIONAL, POLITICAL, WASHINGTON. The goal is to assign a document to one, and only one, of the above classes.
Reference: [22] <author> Holger Schwenk and Yoshua Bengio. </author> <title> Training methods for adaptive boosting of neural networks for character recognition. </title> <booktitle> In Advances in Neural Information Processing Systems 10, </booktitle> <year> 1998. </year>
References-found: 22

