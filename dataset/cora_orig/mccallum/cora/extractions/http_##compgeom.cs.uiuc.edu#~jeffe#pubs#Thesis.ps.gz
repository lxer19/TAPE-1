URL: http://compgeom.cs.uiuc.edu/~jeffe/pubs/Thesis.ps.gz
Refering-URL: http://compgeom.cs.uiuc.edu/~jeffe/pubs/thesis.html
Root-URL: http://www.cs.uiuc.edu
Title: Lower Bounds for Fundamental Geometric Problems  
Author: by Jeffrey Gordon Erickson 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA at BERKELEY Committee in charge: Professor Raimund Seidel, Chair Professor Umesh Vazirani Professor Bernd Sturmfels  
Date: 1992  Fall 1996  
Address: 1987  Irvine)  
Affiliation: B.A. (Rice University)  M.S. (University of California at  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Pankaj K. Agarwal. </author> <title> Partitioning arrangements of lines: II. </title> <journal> Applications. Discrete Comput. Geom., </journal> <volume> 5 </volume> <pages> 533-573, </pages> <year> 1990. </year>
Reference-contexts: Further research replaced the n " term in this upper bound with a succession of smaller and smaller polylogarithmic factors. The running time was improved by Edelsbrunner, Guibas, Hershberger, Seidel, Sharir, Snoeyink, and Welzl [63] to O (n 4=3 log 4 n) (expected); then by Agarwal <ref> [1] </ref> to O (n 4=3 log 1:78 n); then by Chazelle [35] to O (n 4=3 log 1=3 n); and most recently by Matousek [111] to n 4=3 2 O (log fl n) . This is currently the fastest algorithm known. <p> Matousek's algorithm and Theorem 6.21 immediately give us the following theorem. Theorem 6.25. d (n; m) = O However, other algorithms do not use the point location data structure to locate hyperplanes, at least not at all levels of recursion. In these algorithms <ref> [63, 1] </ref>, the query regions form a decomposition of space into cells of constant complexity, typically simplices or trapezoids. The algorithms determine which cells a given hyperplane hits by iteratively "walking" through the cells. <p> (B).) We can encode the statement "P is contained in C and is r-separable from H" as the following elementary formula: r _ _ conv (B) has at most r faces 8 &gt; &gt; &gt; &gt; &gt; &gt; : 9 1 ; 2 ; : : : ; n 2 <ref> [0; 1] </ref> v : n ^ 0 v X a j ij = p i ^ j=1 1 9 &gt; &gt; &gt; &gt; &gt; &gt; ; Equivalently, in English: For some integer v, and for some set B of v points whose convex hull has at most r faces, there exists <p> set of configurations P fi A fi fl 2 (IR d ) n fi C v fi (<ref> [0; 1] </ref> v ) n that satisfy the subexpression (A v B) ^ i=1 @ j=1 v X ij = 1 A is the intersection of the closed convex polytope C n+v fi [0; 1] vn , at most (v - d)r closed algebraic halfspaces, vn quadratic surfaces, and vn hyperplanes, and is therefore closed and bounded.
Reference: [2] <author> Pankaj K. Agarwal, N. Alon, B. Aronov, and S. Suri. </author> <title> Can visibility graphs be represented compactly? In Proc. </title> <booktitle> 9th Annu. ACM Sympos. Comput. Geom., </booktitle> <pages> pages 338-347, </pages> <year> 1993. </year>
Reference-contexts: These results apply immediately to monochromatic covers of arbitrary sign matrices. See also <ref> [2] </ref> for a geometric application of bipartite clique covers.
Reference: [3] <author> Pankaj K. Agarwal, D. Eppstein, and J. Matousek. </author> <title> Dynamic half-space reporting, geometric optimization, and minimum spanning trees. </title> <booktitle> In Proc. 33rd Annu. IEEE Sympos. Found. Comput. Sci., </booktitle> <pages> pages 80-89, </pages> <year> 1992. </year>
Reference-contexts: bound of (n log m + n 2=3 m 2=3 + m log n) for the following halfspace emptiness problem: Given a set of n points and m hyperplanes in IR 5 , is every point above every hyperplane? This matches the best known upper bound up to polylogarithmic factors <ref> [107, 3, 29] </ref>, and improves the previously best lower bound (n log m + m log n) [138, 16]. We also obtain marginally better bounds in higher dimensions. Our lower bound applies to partitioning algorithms in which every query region is a polyhedron with a constant number of facets. <p> Combining Clark-son's and Matousek's data structures, for a fixed parameter n s n bd=2c , one can answer queries in time O ((n log n)=s 1=bd=2c ) after O (s polylog n) preprocessing time <ref> [107, 3, 29] </ref>. <p> Combining Clark-son's and Matousek's data structures, for a fixed parameter n s n bd=2c , one can answer queries in time O ((n log n)=s 1=bd=2c ) after O (s polylog n) preprocessing time [107, 3, 29]. For extensions and applications of halfspace range reporting, see <ref> [3, 4, 27, 29, 110, 108] </ref>. 93 Given n points and m halfspaces, we can solve the o*ine halfspace emptiness problem in time O n log m + (nm) bd=2c=(bd=2c+1) polylog (n + m) + m log n using either Clarkson's data structure or one of Matousek's data structures, depending on
Reference: [4] <author> Pankaj K. Agarwal and J. Matousek. </author> <title> Ray shooting and parametric search. </title> <journal> SIAM J. Comput., </journal> <volume> 22(4) </volume> <pages> 794-806, </pages> <year> 1993. </year>
Reference-contexts: Chan [29] describes an improvement to the gift-wrapping algorithm, using ray shooting data structures of Agarwal and Matousek <ref> [4] </ref> and Matousek and Schwarzkopf [108] to speed up the pivoting step. In each pivoting step, the gift-wrapping algorithm finds a new facet containing a given ridge of the convex hull. <p> Combining Clark-son's and Matousek's data structures, for a fixed parameter n s n bd=2c , one can answer queries in time O ((n log n)=s 1=bd=2c ) after O (s polylog n) preprocessing time [107, 3, 29]. For extensions and applications of halfspace range reporting, see <ref> [3, 4, 27, 29, 110, 108] </ref>. 93 Given n points and m halfspaces, we can solve the o*ine halfspace emptiness problem in time O n log m + (nm) bd=2c=(bd=2c+1) polylog (n + m) + m log n using either Clarkson's data structure or one of Matousek's data structures, depending on
Reference: [5] <author> A. Aggarwal, M. Hansen, and T. Leighton. </author> <title> Solving query-retrieval problems by compacting Voronoi diagrams. </title> <booktitle> In Proc. 22nd Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 331-340, </pages> <year> 1990. </year>
Reference-contexts: In two and three dimensions, we can easily build a linear-size data structure, in O (n log n) time, that allows halfspace emptiness queries to be answered in logarithmic time <ref> [5, 39, 57] </ref>. In higher dimensions, a randomized algorithm due to Clarkson [47] answers half-space emptiness queries in time O (log n) after O (n bd=2c+" ) preprocessing time.
Reference: [6] <author> Nancy M. Amato and Edgar A. Ramos. </author> <title> On computing Voronoi diagrams by divide-prune-and-conquer. </title> <booktitle> In Proc. 12th Annu. ACM Sympos. Comput. Geom., </booktitle> <pages> pages 166-175, </pages> <year> 1996. </year>
Reference-contexts: An algorithm of Chan, Snoeyink, and Yap [28] constructs four-dimensional hulls in time O ((n + f) log 2 f), and 32 a recent improvement by Amato and Ramos <ref> [6] </ref> constructs five-dimensional hulls in time O ((n + f) log 3 f). The fastest algorithm in higher dimensions, due to Chan [29], runs in time O (n log f + (nf) 1-1=(bd=2c+1) polylog n); this algorithm is optimal when f is sufficiently small. <p> There are a few convex hull algorithms which seem to fall outside our framework, most notably the divide-prune-and-conquer algorithm of Chan, Snoeyink, and Yap [28] and its improvement by Amato and Ramos <ref> [6] </ref>. The two-dimensional version of their algorithm uses sidedness queries, along with first-, second-, and even third-order comparisons; higher-dimensional versions use even more complex primitives. 3.4 Open Problems Several open problems remain to be answered.
Reference: [7] <author> Nina Amenta and Gunter Ziegler. </author> <title> Deformed products and maximal shadows of polytopes. </title> <type> Report 502-1996, </type> <institution> Technische Univesitat Berlin, </institution> <month> May </month> <year> 1996. </year> <note> Available electronically at ftp://ftp.math.tu-berlin.de/pub/Preprints/combi/ Report-502-1996.ps.Z. </note>
Reference-contexts: Arguments based on merging facets of cyclic or product polytopes also fail, as do variations of Amenta and Ziegler's deformed products <ref> [7, 8] </ref>. I conjecture that the answer is no, even for polyhedral 3-spheres. A common application of convex hull algorithms is the construction of Delaunay triangulations and Voronoi diagrams.
Reference: [8] <author> Nina Amenta and Gunter Ziegler. </author> <title> Shadows and slices of polytopes. </title> <booktitle> In Proc. 12th Annu. ACM Sympos. Comput. Geom., </booktitle> <pages> pages 10-19, </pages> <year> 1996. </year> <month> 109 </month>
Reference-contexts: Arguments based on merging facets of cyclic or product polytopes also fail, as do variations of Amenta and Ziegler's deformed products <ref> [7, 8] </ref>. I conjecture that the answer is no, even for polyhedral 3-spheres. A common application of convex hull algorithms is the construction of Delaunay triangulations and Voronoi diagrams.
Reference: [9] <author> Arne Anderson, Torben Hagerup, Stefan Nilsson, and Rajeev Raman. </author> <title> Sorting in linear time? In Proc. </title> <booktitle> 27th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 427-436, </pages> <year> 1995. </year>
Reference-contexts: The situation may be comparable to sorting or element uniqueness|(n log n) time is required to sort using algebraic decision trees [16], but there are significantly faster sorting algorithms in integer RAM models <ref> [83, 9] </ref>. Are there faster algorithms for useful special cases? For example, a set of n points in the plane in (loosely) convex position has only n collapsible triangles, and we can easily detect colinear triples in such a set in O (n log n) time.
Reference: [10] <author> D. Avis and K. Fukuda. </author> <title> A pivoting algorithm for convex hulls and vertex enumeration of arrangements and polyhedra. </title> <journal> Discrete Comput. Geom., </journal> <volume> 8 </volume> <pages> 295-313, </pages> <year> 1992. </year>
Reference-contexts: The fastest algorithm in higher dimensions, due to Chan [29], runs in time O (n log f + (nf) 1-1=(bd=2c+1) polylog n); this algorithm is optimal when f is sufficiently small. For related results, see <ref> [10, 30, 49, 50, 101, 134] </ref>. There are still large gaps between these upper bounds and the lower bound (n log f + f). <p> and Kapur [30] and Swart [142], the "beneath-beyond" method of Seidel [132], Clarkson and Shor's [50] and Seidel's [136] randomized incremental algorithms, Chazelle's worst-case optimal algorithm [36], and the recursive partial-order algorithm of Clarkson [49]. 38 Seidel's "shelling" algorithm [134] and the space-efficient gift-wrapping algorithms of Avis and Fukuda 1 <ref> [10] </ref> and Rote [130] require only sidedness queries and "second-order" coordinate comparisons between vertices of the dual hyperplane arrangment. Matousek [110] and Chan [29] improve the running times of these algorithms (in an output-sensitive sense), by finding the extreme points more quickly.
Reference: [11] <author> David Avis and David Bremner. </author> <title> How good are convex hull algorithms? In Proc. </title> <booktitle> 11th Annu. ACM Sympos. Comput. Geom., </booktitle> <pages> pages 20-28, </pages> <year> 1995. </year>
Reference-contexts: For related results, see [10, 30, 49, 50, 101, 134]. There are still large gaps between these upper bounds and the lower bound (n log f + f). Avis, Bremner, and Seidel <ref> [11, 12] </ref> describe families of polytopes on which current convex hull algorithms perform quite badly, sometimes requiring exponential time (in d) even when the output size is only polynomial.
Reference: [12] <author> David Avis, David Bremner, and Raimund Seidel. </author> <title> How good are convex hull algorithms? Comput. </title> <journal> Geom. Theory Appl., </journal> <note> 1996. To appear. Full version of [11]. Available electronically at ftp://mutt.cs.mcgill.ca/pub/doc/hgch.ps.gz. </note>
Reference-contexts: For related results, see [10, 30, 49, 50, 101, 134]. There are still large gaps between these upper bounds and the lower bound (n log f + f). Avis, Bremner, and Seidel <ref> [11, 12] </ref> describe families of polytopes on which current convex hull algorithms perform quite badly, sometimes requiring exponential time (in d) even when the output size is only polynomial.
Reference: [13] <author> Saugata Basu. </author> <title> On bounding the Betti numbers and computing the Euler characteristic of semi-algebraic sets. </title> <booktitle> In Proc. 28th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 408-417, </pages> <year> 1996. </year>
Reference-contexts: The generalization hinges on the following theorem of algebraic geometry independently proven by Petrovski and Olenik [120, 121], Thom [147], and Milnor [115]. (See also <ref> [13, 14, 152] </ref>.) A semialgebraic set is the set of points satisfying a finite number of polynomial equations and inequalities. Theorem 1.1 (Petrovski/Olenik/Thom/Milnor). Let V be a semialgebraic set in IR n , defined by t polynomial inequalities of maximum degree d. <p> Fortunately, this is precisely the framework in which these lower bound arguments apply. As a consequence, it is quite easy to derive (n log n) lower bounds for many of these problems. Unfortunately, this is the best we can do. The Petrovski/Olenik/ Thom/Milnor theorem and its generalizations <ref> [13, 14, 152] </ref> imply that the complexity of W, in any reasonable sense of the word "complexity", is at most n O (n) . Thus, for these problems, no known lower bound technique can imply lower bounds bigger than (n log n) in the algebraic decision tree model.
Reference: [14] <author> Saugata Basu, Richard Pollack, and Marie-Fran coise Roy. </author> <title> On the number of cells defined by a family of polynomials on a variety. </title> <journal> Mathematika. </journal> <note> To appear. Available electronically at http://www.math.nyu.edu/faculty/pollack/finalvariety.ps. </note>
Reference-contexts: The generalization hinges on the following theorem of algebraic geometry independently proven by Petrovski and Olenik [120, 121], Thom [147], and Milnor [115]. (See also <ref> [13, 14, 152] </ref>.) A semialgebraic set is the set of points satisfying a finite number of polynomial equations and inequalities. Theorem 1.1 (Petrovski/Olenik/Thom/Milnor). Let V be a semialgebraic set in IR n , defined by t polynomial inequalities of maximum degree d. <p> Fortunately, this is precisely the framework in which these lower bound arguments apply. As a consequence, it is quite easy to derive (n log n) lower bounds for many of these problems. Unfortunately, this is the best we can do. The Petrovski/Olenik/ Thom/Milnor theorem and its generalizations <ref> [13, 14, 152] </ref> imply that the complexity of W, in any reasonable sense of the word "complexity", is at most n O (n) . Thus, for these problems, no known lower bound technique can imply lower bounds bigger than (n log n) in the algebraic decision tree model.
Reference: [15] <author> Walter Baur and Volker Strassen. </author> <title> The complexity of partial derivatives. </title> <journal> Theor. Comput. Sci., </journal> <volume> 22 </volume> <pages> 317-330, </pages> <year> 1983. </year>
Reference-contexts: of Yao [154], this argument was later generalized to higher-order 3 Like many "obvious" statements, this is actually false! The discriminant Q i&lt;j (x i - x j ) can be directly computed using O (n log n) multiplications (and O (n log 2 n) additions) without sorting the inputs <ref> [15, 141] </ref>. The input elements x i are distinct if and only if this expression is not equal to zero, but the expression gives us (almost) no information about the sorted order of the inputs. 6 algebraic decision trees by Steele and Yao [138] and Ben-Or [16].
Reference: [16] <author> M. Ben-Or. </author> <title> Lower bounds for algebraic computation trees. </title> <booktitle> In Proc. 15th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 80-86, </pages> <year> 1983. </year>
Reference-contexts: The input elements x i are distinct if and only if this expression is not equal to zero, but the expression gives us (almost) no information about the sorted order of the inputs. 6 algebraic decision trees by Steele and Yao [138] and Ben-Or <ref> [16] </ref>. The generalization hinges on the following theorem of algebraic geometry independently proven by Petrovski and Olenik [120, 121], Thom [147], and Milnor [115]. (See also [13, 14, 152].) A semialgebraic set is the set of points satisfying a finite number of polynomial equations and inequalities. Theorem 1.1 (Petrovski/Olenik/Thom/Milnor). <p> It follows immediately that the depth must be (log #W - n log d); in particular, the complexity of the element uniqueness problem is (n log n). Ben-Or <ref> [16] </ref> strengthened the argument further, deriving a similar lower bound in the stronger algebraic computation tree model. <p> An !(n log n) lower bound for any natural problem solvable in polynomial time would be a major breakthrough. Quadratic lower bounds are known for a few NP-complete problems <ref> [59, 16, 93] </ref>, but again, this is the best lower 7 bound we can prove, since for these problems the number of defining inequalities is only singly-exponential. 1.1.3 Semigroup Arithmetic The semigroup arithmetic model was introduced by Fredman [82, 81] and refined by Yao [155] and Chazelle [34] to study the <p> In Part I (Chapters 2 through 5), we derive lower bounds for several degeneracy-detection problems. For each of the problems we consider, the previously best lower bound, in any model of computation, was only (n log n) <ref> [138, 16] </ref>. In Chapter 2, we show that, in the worst case, (n d ) sidedness queries are required to decide, given n points in IR d , whether any d + 1 lie on a common hyperplane. <p> Our planar lower bound is within a factor of 2 O (log fl (n+m)) of the best 11 known upper bound [111]. 5 The previously best lower bound was only (n log m+m log n) <ref> [138, 16] </ref>. We develop our lower bounds in two stages. First we define a combinatorial representation of the relative order type of a set of points and hyperplanes, called a monochromatic cover, and derive lower bounds on its size in the worst case. <p> halfspace emptiness problem: Given a set of n points and m hyperplanes in IR 5 , is every point above every hyperplane? This matches the best known upper bound up to polylogarithmic factors [107, 3, 29], and improves the previously best lower bound (n log m + m log n) <ref> [138, 16] </ref>. We also obtain marginally better bounds in higher dimensions. Our lower bound applies to partitioning algorithms in which every query region is a polyhedron with a constant number of facets. At the end of each chapter, we outline some relevant open problems and suggest directions for further research. <p> In the algebraic decision tree and algebraic computation tree models, there is a somewhat trivial lower bound of (n log n) on finding affine degeneracies in any dimension, since it takes (n log n) time just to determine whether all the points are distinct <ref> [138, 16] </ref>. Prior to the results described in this chapter, no better lower bound was known in any model of computation. Two sets of labeled points are said to have the same order type if corresponding simplices have the same orientation. <p> The other possibility, of course, is that there is a subquadratic algorithm in some completely different model of computation. The situation may be comparable to sorting or element uniqueness|(n log n) time is required to sort using algebraic decision trees <ref> [16] </ref>, but there are significantly faster sorting algorithms in integer RAM models [83, 9]. <p> Yao [154] proved a lower bound of (n log n) on the complexity of identifying the convex hull vertices, in the quadratic decision tree model. This lower bound was later generalized to the algebraic decision tree and algebraic computation tree models by Ben-Or <ref> [16] </ref>. It follows that both Graham's scan and Preparata and Hong's algorithm are optimal in the worst case. <p> This matches known upper bounds when d is odd [36]. The only lower bound previously known for either of these problems is (n log n), following from the techniques of Yao [154] and Ben-Or <ref> [16] </ref>. When the dimension is allowed to vary with the input size, deciding if a convex hull is simplicial is coNP-complete [31], and counting the number of facets is #P-hard in general, and NP-hard for simplicial polytopes [61]. Our results apply when the dimension d is fixed. <p> There are (n dd=2e-1 ) collapsible simplices, one for each degenerate facet in conv (! d (X)). Finally, the n log n term follows from the algebraic decision tree lower bound of Ben-Or <ref> [16] </ref>. 2 A three-dimensional version of our construction is illustrated in Figure 3.1. (See also the proof of Theorem 4.3!) Our lower bound matches known upper bounds when d is odd [36]. <p> An (n log n) lower bound for any linear satisfiability problem follows easily from techniques of Dobkin and Lipton in the linear decision tree model [58], Steele and Yao in the algebraic decision tree model [138], and Ben-Or in the algebraic computation tree model <ref> [16] </ref>. The first better lower bound is due to Fredman [80], who proved an (n 2 ) lower bound on the number of comparisons required to detect duplicate elements in the Minkowski sum X + Y of two sets of real numbers; his proof relies on a simple adversary argument. <p> The only previously known lower bound is (n log m + m log n), in the algebraic decision tree and algebraic computation tree models, by reduction from the problem of detecting an intersection between two sets of real numbers <ref> [138, 16] </ref>. In this chapter, we establish new lower bounds on the complexity of Hopcroft's problem. <p> The only lower bound previously known for this problem is (n log m + m log n), in the algebraic decision tree or algebraic computation tree models, by reduction from the set intersection problem <ref> [138, 16] </ref>. Thus, the two- and three-dimensional algorithms are optimal, but there is still a large gap in dimensions four and higher.
Reference: [17] <author> Samuel W. Bent and John W. John. </author> <title> Finding the median requires 2n comparisons. </title> <booktitle> In Proc. 17th ACM Sympos. Theory Comput., </booktitle> <pages> pages 213-216, </pages> <year> 1985. </year>
Reference-contexts: The minimum number of comparisons required 2 unless we allow a small probability of error [148] 5 to solve the last two problems is still not known; the simplest open case is k = 3. Several more references can be found in <ref> [17] </ref>. We can generalize comparison trees by allowing more complicated query expressions.
Reference: [18] <author> A. Bjorner, M. Las Vergnas, N. White, B. Sturmfels, and G. Ziegler. </author> <title> Oriented Ma-troids. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference-contexts: This suggestion is far too vague to call a "conjecture"; in particular, I haven't mentioned a specific model of com 6 Mnev showed any primary semialgebraic set defined over the integers is stably homotopy-equivalent to the realization space of some rank-3 oriented matroid (i.e., some pseudoline arrangement). See <ref> [18] </ref> for further implications of this remarkable result. 30 putation. Nevertheless, any results in this direction (even just formalizing the "conjecture") would be interesting. [items 1-19 omitted] 20. I have no arguments to offer, my figures are my proofs. 21.
Reference: [19] <author> Anders Bjorner, Laszlo Lovasz, and Andrew C. C. Yao. </author> <title> Linear decision trees: Volume estimates and topological bounds. </title> <booktitle> In Proc. 24th Annu. ACM Sympos. Theory. Comput., </booktitle> <pages> pages 170-177, </pages> <year> 1992. </year> <month> 110 </month>
Reference-contexts: Ben-Or [16] strengthened the argument further, deriving a similar lower bound in the stronger algebraic computation tree model. More recent techniques imply lower bounds based on different complexity measures of the set W, such as its higher order Betti numbers [158], its Euler characteristic <ref> [19, 159] </ref>, or the number of its lower-dimensional faces [118, 157, 94]. Lower bounds can also be derived by considering the complexity of the complement of W, the interior of W, the closure of W, or the intersection of W with another semi-algebraic subset of IR n .
Reference: [20] <author> S. Bloch, J. Buss, and J. Goldsmith. </author> <title> How hard are n 2 -hard problems? SIGACT News, </title> <booktitle> 25(2) </booktitle> <pages> 83-85, </pages> <year> 1994. </year>
Reference-contexts: Given these reductions, one might think that we have just proven that every 4 Some earlier papers, including [73], used the more suggestive but potentially misleading term "n 2 - hard" [85] (but see <ref> [20] </ref>). 5 This observation was the initial inspiration for my "weird moment curve" argument. 28 3sum-hard problem requires (n 2 ) time. Unfortunately, this is not the case. Many of the reductions discussed in [86] require primitives that our models of computation do not allow.
Reference: [21] <author> Manuel Blum, Robert W. Floyd, Vaughan Pratt, Ronald R. Rivest, and Robert E. Tarjan. </author> <title> Time bounds for selection. </title> <journal> J. Comput. System Sci., </journal> <volume> 7(4) </volume> <pages> 448-461, </pages> <year> 1973. </year>
Reference-contexts: Adversary arguments have also been used to prove lower bounds on the number of comparisons required to choose the largest and smallest elements in a list, the second-largest element [102, 104] (see also [60]), the median element <ref> [21, 122] </ref>, or the kth largest element for arbitrary values of k [122, 99]. The minimum number of comparisons required 2 unless we allow a small probability of error [148] 5 to solve the last two problems is still not known; the simplest open case is k = 3.
Reference: [22] <author> J. Bochnak, M. Coste, and M.-F. Roy. Geometrie algebraique reelle, </author> <title> volume 12 of Ergebnisse der Mathematik und ihrer Grenzgebeite 3. </title> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: A real closed field is an ordered field, no proper algebraic extension of which is also an ordered field. The real closure e K of an ordered field K is the smallest real closed field that contains it. We refer the interested reader to <ref> [22] </ref> or [124] for further details and more formal definitions, and to [25, 26] for previous algorithmic applications of real closed fields. An elementary formula 1 is a finite quantified boolean formula, each of whose clauses is a multivariate polynomial inequality with real coefficients. <p> The following principle was originally proven by Tarski [146], in a slightly different form. See <ref> [22] </ref> for a more recent proof. The Transfer Principle: Let e K and f K 0 be two real closed fields. An elementary formula holds in e K if and only if it holds in f K 0 . <p> " 3 ), where each of the coefficients a i is real, is given by the sign of the first nonzero coefficient; in particular, the element is zero if and only if every 1 or more formally, a formula in the first-order language of ordered fields with parameters in IR <ref> [22] </ref> 50 a i is zero. Infinitesimals have been used extensively in perturbation techniques [67, 71, 160], in algorithms dealing with real semialgebraic sets [25, 26], and in at least one other lower bound argument [93]. Let us now formally define our model of computation.
Reference: [23] <author> H. Bronnimann, B. Chazelle, and J. Pach. </author> <title> How hard is halfspace range searching? Discrete Comput. </title> <journal> Geom., </journal> <volume> 10 </volume> <pages> 143-155, </pages> <year> 1993. </year>
Reference-contexts: Vaidya [150] proved lower bounds for orthogonal range searching in higher dimensions, which were later improved by Chazelle [34]. Chazelle also derived lower bounds for simplex range searching [33], and with Bronnimann and Pach, halfspace range searching <ref> [23] </ref>. All of these online lower bounds give tradeoffs between the space required by the data structure and the resulting query time. <p> Chazelle and Rosenberg [42, 41] derived lower bounds for computing partial sums in multi-dimensional arrays (a special case of orthogonal range searching where the points lie on a lattice), and Chazelle [38] proved lower bounds for both orthogonal and simplex range searching. With the exception of halfspace range searching <ref> [23] </ref>, all of these lower bounds are optimal, up to polylogarithmic or n " factors, in the semigroup model. For further details, we refer the reader to an excellent survey by Matousek [112]. <p> All these lower bounds hold in the Fredman/Yao semigroup arithmetic model; see Section 1.3. For related results, see also <ref> [23, 44] </ref>. Lower bounds in the semigroup model are based on the existence of configurations of points and ranges, such as the planar Erd-os configuration, whose incidence graphs have (a subgraph with) no large complete bipartite subgraphs. Our lower bounds have a similar basis. <p> Bronnimann, Chazelle, and Pach <ref> [23] </ref> have proven time-space tradeoffs for halfspace counting data structures in the Fredman/Yao semigroup model.
Reference: [24] <author> Stefan A. Burr, Branko Grunbaum, and N. J. A. Sloane. </author> <title> The orchard problem. </title> <journal> Geom. Dedicata, </journal> <volume> 2 </volume> <pages> 397-424, </pages> <year> 1974. </year>
Reference-contexts: It follows that the set of lines fL (2i=n) j 1 i ng has 1 + bn (n - 3)=6c concurrent triples. See Figure 2.4 (a). See [84] for further details. Related results are described in <ref> [24] </ref> and [72].
Reference: [25] <author> J. Canny. </author> <title> Some algebraic and geometric configurations in PSPACE. </title> <booktitle> In Proc. 20th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 460-467, </pages> <year> 1988. </year>
Reference-contexts: The real closure e K of an ordered field K is the smallest real closed field that contains it. We refer the interested reader to [22] or [124] for further details and more formal definitions, and to <ref> [25, 26] </ref> for previous algorithmic applications of real closed fields. An elementary formula 1 is a finite quantified boolean formula, each of whose clauses is a multivariate polynomial inequality with real coefficients. <p> Infinitesimals have been used extensively in perturbation techniques [67, 71, 160], in algorithms dealing with real semialgebraic sets <ref> [25, 26] </ref>, and in at least one other lower bound argument [93]. Let us now formally define our model of computation.
Reference: [26] <author> J. Canny. </author> <title> Computing roadmaps in general semialgebraic sets. </title> <journal> Comput. J., </journal> <volume> 36 </volume> <pages> 409-418, </pages> <year> 1994. </year>
Reference-contexts: The real closure e K of an ordered field K is the smallest real closed field that contains it. We refer the interested reader to [22] or [124] for further details and more formal definitions, and to <ref> [25, 26] </ref> for previous algorithmic applications of real closed fields. An elementary formula 1 is a finite quantified boolean formula, each of whose clauses is a multivariate polynomial inequality with real coefficients. <p> Infinitesimals have been used extensively in perturbation techniques [67, 71, 160], in algorithms dealing with real semialgebraic sets <ref> [25, 26] </ref>, and in at least one other lower bound argument [93]. Let us now formally define our model of computation.
Reference: [27] <author> Timothy M. Chan. </author> <title> Fixed-dimensional linear programming queries made easy. </title> <booktitle> In Proc. 12th Annu. ACM Sympos. Comput. Geom., </booktitle> <pages> pages 284-290, </pages> <year> 1996. </year>
Reference-contexts: Combining Clark-son's and Matousek's data structures, for a fixed parameter n s n bd=2c , one can answer queries in time O ((n log n)=s 1=bd=2c ) after O (s polylog n) preprocessing time [107, 3, 29]. For extensions and applications of halfspace range reporting, see <ref> [3, 4, 27, 29, 110, 108] </ref>. 93 Given n points and m halfspaces, we can solve the o*ine halfspace emptiness problem in time O n log m + (nm) bd=2c=(bd=2c+1) polylog (n + m) + m log n using either Clarkson's data structure or one of Matousek's data structures, depending on <p> A problem closely related to halfspace range searching is linear programming. The best known data structures of linear programming queries are based on data structures for halfspace emptiness [110] and halfspace reporting queries <ref> [27] </ref>. However, no nontrivial lower bounds are known for linear programming queries in any model of computation. One application of particular interest is deciding, given a set of points, whether every point is a vertex of the set's convex hull.
Reference: [28] <author> Timothy M. Chan, Jack Snoeyink, and Chee-Keng Yap. </author> <title> Output-sensitive construction of polytopes in four dimensions and clipped Voronoi diagrams in three. </title> <booktitle> In Proc. 6th ACM-SIAM Sympos. Discrete Algorithms (SODA '95), </booktitle> <pages> pages 282-291, </pages> <year> 1995. </year>
Reference-contexts: It follows that both Graham's scan and Preparata and Hong's algorithm are optimal in the worst case. If the output size f is also taken into account, the lower bound drops to (n log f) [101], and a number of algorithms match this bound both in the plane <ref> [101, 28, 29] </ref> and in three dimensions [50, 40]. In higher dimensions, the problem is not quite so completely solved. Seidel's "beneath-beyond" algorithm [132] constructs d-dimensional convex hulls in time O (n dd=2e ). <p> Several faster algorithms are known when the output size is also considered. In 1970, Chand and Kapur [30] described an algorithm that constructs convex hulls in time O (nf), where f is the number of facets in the output. An algorithm of Chan, Snoeyink, and Yap <ref> [28] </ref> constructs four-dimensional hulls in time O ((n + f) log 2 f), and 32 a recent improvement by Amato and Ramos [6] constructs five-dimensional hulls in time O ((n + f) log 3 f). <p> Thus, even if we allow a pivoting step to be performed in constant time, our lower bound still holds. There are a few convex hull algorithms which seem to fall outside our framework, most notably the divide-prune-and-conquer algorithm of Chan, Snoeyink, and Yap <ref> [28] </ref> and its improvement by Amato and Ramos [6]. The two-dimensional version of their algorithm uses sidedness queries, along with first-, second-, and even third-order comparisons; higher-dimensional versions use even more complex primitives. 3.4 Open Problems Several open problems remain to be answered.
Reference: [29] <author> Timothy M. Y. Chan. </author> <title> Output-sensitive results on convex hulls, extreme points, and related problems. </title> <booktitle> In Proc. 11th Annu. ACM Sympos. Comput. Geom., </booktitle> <pages> pages 10-19, </pages> <year> 1995. </year>
Reference-contexts: bound of (n log m + n 2=3 m 2=3 + m log n) for the following halfspace emptiness problem: Given a set of n points and m hyperplanes in IR 5 , is every point above every hyperplane? This matches the best known upper bound up to polylogarithmic factors <ref> [107, 3, 29] </ref>, and improves the previously best lower bound (n log m + m log n) [138, 16]. We also obtain marginally better bounds in higher dimensions. Our lower bound applies to partitioning algorithms in which every query region is a polyhedron with a constant number of facets. <p> It follows that both Graham's scan and Preparata and Hong's algorithm are optimal in the worst case. If the output size f is also taken into account, the lower bound drops to (n log f) [101], and a number of algorithms match this bound both in the plane <ref> [101, 28, 29] </ref> and in three dimensions [50, 40]. In higher dimensions, the problem is not quite so completely solved. Seidel's "beneath-beyond" algorithm [132] constructs d-dimensional convex hulls in time O (n dd=2e ). <p> The fastest algorithm in higher dimensions, due to Chan <ref> [29] </ref>, runs in time O (n log f + (nf) 1-1=(bd=2c+1) polylog n); this algorithm is optimal when f is sufficiently small. For related results, see [10, 30, 49, 50, 101, 134]. There are still large gaps between these upper bounds and the lower bound (n log f + f). <p> Matousek [110] and Chan <ref> [29] </ref> improve the running times of these algorithms (in an output-sensitive sense), by finding the extreme points more quickly. Clarkson [49] describes a similar improvement to a randomized incremental algorithm. <p> We are not suggesting that the computational primitives used by these algorithms cannot be used to break our lower bounds; only that the ways in which these primitives are currently applied are inherently limited. Chan <ref> [29] </ref> describes an improvement to the gift-wrapping algorithm, using ray shooting data structures of Agarwal and Matousek [4] and Matousek and Schwarzkopf [108] to speed up the pivoting step. In each pivoting step, the gift-wrapping algorithm finds a new facet containing a given ridge of the convex hull. <p> This problem can be decided in O (n 2 ) time (using only sidedness queries!) by invoking a linear programming algorithm once for each point [48, 109, 113, 136]. This upper bound can be improved to O (n 2bd=2c=(bd=2c-1) polylog n) using an algorithm due to Chan <ref> [29] </ref>. Except for the polylogarithmic term, this algorithm is almost certainly optimal. It seems unlikely that a collapsible simplex argument could be used to imply a reasonable lower bound for this problem. Perhaps the techniques we describe in Part II are more applicable. <p> Combining Clark-son's and Matousek's data structures, for a fixed parameter n s n bd=2c , one can answer queries in time O ((n log n)=s 1=bd=2c ) after O (s polylog n) preprocessing time <ref> [107, 3, 29] </ref>. <p> Combining Clark-son's and Matousek's data structures, for a fixed parameter n s n bd=2c , one can answer queries in time O ((n log n)=s 1=bd=2c ) after O (s polylog n) preprocessing time [107, 3, 29]. For extensions and applications of halfspace range reporting, see <ref> [3, 4, 27, 29, 110, 108] </ref>. 93 Given n points and m halfspaces, we can solve the o*ine halfspace emptiness problem in time O n log m + (nm) bd=2c=(bd=2c+1) polylog (n + m) + m log n using either Clarkson's data structure or one of Matousek's data structures, depending on <p> One application of particular interest is deciding, given a set of points, whether every point is a vertex of the set's convex hull. Bounds for this problem closely match the best known bounds for halfspace emptiness <ref> [29] </ref>, but the best known lower bound is (n log n). It seems unlikely that a lower bounds can be derived for this problem in the partitioning algorithm model, since the extremity of a point depends on several other points arbitrarily far away.
Reference: [30] <author> D. R. Chand and S. S. Kapur. </author> <title> An algorithm for convex polytopes. </title> <journal> J. ACM, </journal> <volume> 17 </volume> <pages> 78-86, </pages> <year> 1970. </year>
Reference-contexts: Several faster algorithms are known when the output size is also considered. In 1970, Chand and Kapur <ref> [30] </ref> described an algorithm that constructs convex hulls in time O (nf), where f is the number of facets in the output. <p> The fastest algorithm in higher dimensions, due to Chan [29], runs in time O (n log f + (nf) 1-1=(bd=2c+1) polylog n); this algorithm is optimal when f is sufficiently small. For related results, see <ref> [10, 30, 49, 50, 101, 134] </ref>. There are still large gaps between these upper bounds and the lower bound (n log f + f). <p> These include the "gift-wrapping" algorithms of Chand and Kapur <ref> [30] </ref> and Swart [142], the "beneath-beyond" method of Seidel [132], Clarkson and Shor's [50] and Seidel's [136] randomized incremental algorithms, Chazelle's worst-case optimal algorithm [36], and the recursive partial-order algorithm of Clarkson [49]. 38 Seidel's "shelling" algorithm [134] and the space-efficient gift-wrapping algorithms of Avis and Fukuda 1 [10] and Rote
Reference: [31] <author> R. Chandrasekaran, Santosh N. Kabadi, and Katta G. Murty. </author> <title> Some NP-complete problems in linear programming. </title> <journal> Oper. Res. Lett., </journal> <volume> 1 </volume> <pages> 101-104, </pages> <year> 1982. </year>
Reference-contexts: The only lower bound previously known for either of these problems is (n log n), following from the techniques of Yao [154] and Ben-Or [16]. When the dimension is allowed to vary with the input size, deciding if a convex hull is simplicial is coNP-complete <ref> [31] </ref>, and counting the number of facets is #P-hard in general, and NP-hard for simplicial polytopes [61]. Our results apply when the dimension d is fixed. Our lower bounds follow from a generalization of the previous chapter's adversary argument.
Reference: [32] <author> B. Chazelle. </author> <title> Reporting and counting segment intersections. </title> <journal> J. Comput. Syst. Sci., </journal> <volume> 32 </volume> <pages> 156-182, </pages> <year> 1986. </year> <month> 111 </month>
Reference-contexts: Given a set of n points and n lines in the plane, does any point lie on a line? Hopcroft's problem arises as a special case of many other geometric problems, including collision detection, ray shooting, and range searching. The earliest sub-quadratic algorithm for Hopcroft's problem, due to Chazelle <ref> [32] </ref>, runs in time O (n 1:695 ). (Actually, this algorithm counts intersections among a set of n line segments in the plane, but it can easily be modified to count point-line incidences instead.) A very simple algorithm, attributed to Hopcroft and Seidel [51], described in [62, p. 350], runs in <p> Each algorithm divides space into a number of regions, determines which points and hyperplanes intersect each region, and recursively solves the resulting subprob-lems. In some cases <ref> [32, 66, 43] </ref>, the number of regions used at each level of recursion is a constant, and these algorithms fit naturally into the partitioning algorithm framework.
Reference: [33] <author> B. Chazelle. </author> <title> Lower bounds on the complexity of polytope range searching. </title> <journal> J. Amer. Math. Soc., </journal> <volume> 2 </volume> <pages> 637-666, </pages> <year> 1989. </year>
Reference-contexts: Vaidya [150] proved lower bounds for orthogonal range searching in higher dimensions, which were later improved by Chazelle [34]. Chazelle also derived lower bounds for simplex range searching <ref> [33] </ref>, and with Bronnimann and Pach, halfspace range searching [23]. All of these online lower bounds give tradeoffs between the space required by the data structure and the resulting query time. <p> Of course, we cannot apply this argument to either the decision version or the counting version of Hopcroft's problem, since the output size for these problems is constant. Our planar lower bounds are ultimately based on the Erd-os configuration. Chazelle <ref> [33, 38] </ref> has established lower bounds for the closely related simplex range searching problem: Given a set of points and a set of simplices, how many points are in each simplex? For example, any data structure of size s that supports triangular range queries among n points in the plane requires <p> the closely related simplex range searching problem: Given a set of points and a set of simplices, how many points are in each simplex? For example, any data structure of size s that supports triangular range queries among n points in the plane requires (n= p s) time per query <ref> [33] </ref>. It follows that answering n queries over n points requires (n 4=3 ) time in the worst case. <p> The theorem now follows from Lemma 6.4. 2 Theorem 6.6. ffi 2 (n; m) = (n + n 2=3 m 2=3 + m) 3 Perhaps it is more interesting that Chazelle's static lower bounds <ref> [33, 38] </ref> do not use this construction. 72 Proof: Consider any configuration of n points and m=2 lines with (n + n 2=3 m 2=3 + m) point-line incidences, as given by Lemma 6.4.
Reference: [34] <author> B. Chazelle. </author> <title> Lower bounds for orthogonal range searching, I: the reporting case. </title> <journal> J. ACM, </journal> <volume> 37 </volume> <pages> 200-212, </pages> <year> 1990. </year>
Reference-contexts: few NP-complete problems [59, 16, 93], but again, this is the best lower 7 bound we can prove, since for these problems the number of defining inequalities is only singly-exponential. 1.1.3 Semigroup Arithmetic The semigroup arithmetic model was introduced by Fredman [82, 81] and refined by Yao [155] and Chazelle <ref> [34] </ref> to study the complexity of range searching data structures. In this model, each point in the input is given a value from an abelian semigroup satisfying a mild technical condition 4 . <p> The model was first applied to static range searching problems by Yao [155], who proved lower bounds for orthogonal range searching in two dimensions. Vaidya [150] proved lower bounds for orthogonal range searching in higher dimensions, which were later improved by Chazelle <ref> [34] </ref>. Chazelle also derived lower bounds for simplex range searching [33], and with Bronnimann and Pach, halfspace range searching [23]. All of these online lower bounds give tradeoffs between the space required by the data structure and the resulting query time. <p> Query algorithms are also allowed to modify the data structure by adding new nodes and adding or deleting edges between previously visited nodes. The query time is the number of nodes visited; all other aspects of computation are ignored. Chazelle <ref> [34] </ref> derives lower bounds for orthogonal range reporting in this model; his techniques were applied to simplex range reporting by Chazelle and Rosenberg [44].
Reference: [35] <author> B. Chazelle. </author> <title> Cutting hyperplanes for divide-and-conquer. </title> <journal> Discrete Comput. Geom., </journal> <volume> 9(2) </volume> <pages> 145-158, </pages> <year> 1993. </year>
Reference-contexts: The running time was improved by Edelsbrunner, Guibas, Hershberger, Seidel, Sharir, Snoeyink, and Welzl [63] to O (n 4=3 log 4 n) (expected); then by Agarwal [1] to O (n 4=3 log 1:78 n); then by Chazelle <ref> [35] </ref> to O (n 4=3 log 1=3 n); and most recently by Matousek [111] to n 4=3 2 O (log fl n) . This is currently the fastest algorithm known. <p> Composing all the point location data structures used by the algorithm in all recursive subproblems gives us the algorithm's partition graph. Many of these algorithms alternate between primal and dual spaces at various levels of recursion <ref> [35, 111] </ref>. The data structures used in primal space give us the primal nodes in the partition graph, and the data structures used in dual space give us the dual nodes. <p> What about the hyperplanes? Many algorithms also use the point location data structures to determine the regions hit by each hyperplane. Algorithms of this type fit into our model perfectly. In particular, Matousek's algorithm [111], which is based on Chazelle's hierarchical cuttings <ref> [35] </ref> and is the fastest algorithm known, can be modeled this way. Matousek's algorithm and Theorem 6.21 immediately give us the following theorem. <p> We could use, for example, the randomized incremental construction of Seidel [135] in the plane, or the hierarchical cuttings data structure of Chazelle <ref> [35] </ref> in higher dimensions. The modified algorithm can then be described as a partitioning algorithm. Other algorithms construct a point location data structure for the arrangement of the entire set of hyperplanes [51, 66, 35]. <p> The modified algorithm can then be described as a partitioning algorithm. Other algorithms construct a point location data structure for the arrangement of the entire set of hyperplanes <ref> [51, 66, 35] </ref>. Usually, this is done only when the number of hyperplanes is much smaller than the number of points.
Reference: [36] <author> B. Chazelle. </author> <title> An optimal convex hull algorithm in any fixed dimension. </title> <journal> Discrete Comput. Geom., </journal> <volume> 10 </volume> <pages> 377-409, </pages> <year> 1993. </year>
Reference-contexts: In Chapter 3, using similar techniques, we show that (n dd=2e-1 ) sidedness queries are required to determine is the convex hull of n points in IR d is simplicial, or to count the number of convex hull facets. This matches known upper bounds when d is odd <ref> [36] </ref>. In Chapter 4, we show that (n d+1 ) insphere queries are required to decide if any d + 2 points lie on a common finite-radius sphere in IR d . <p> In higher dimensions, the problem is not quite so completely solved. Seidel's "beneath-beyond" algorithm [132] constructs d-dimensional convex hulls in time O (n dd=2e ). After a ten-year wait, Chazelle <ref> [36] </ref> improved the running time to O (n bd=2c ) by derandom-izing a randomized incremental algorithm of Clarkson and Shor [50]; see also [136]. <p> We show that in the worst case, (n dd=2e-1 +n log n) sidedness queries are required to decide whether the convex hull of n points in IR d is simplicial, or to determine the number of convex hull facets. This matches known upper bounds when d is odd <ref> [36] </ref>. The only lower bound previously known for either of these problems is (n log n), following from the techniques of Yao [154] and Ben-Or [16]. <p> Finally, the n log n term follows from the algebraic decision tree lower bound of Ben-Or [16]. 2 A three-dimensional version of our construction is illustrated in Figure 3.1. (See also the proof of Theorem 4.3!) Our lower bound matches known upper bounds when d is odd <ref> [36] </ref>. <p> These include the "gift-wrapping" algorithms of Chand and Kapur [30] and Swart [142], the "beneath-beyond" method of Seidel [132], Clarkson and Shor's [50] and Seidel's [136] randomized incremental algorithms, Chazelle's worst-case optimal algorithm <ref> [36] </ref>, and the recursive partial-order algorithm of Clarkson [49]. 38 Seidel's "shelling" algorithm [134] and the space-efficient gift-wrapping algorithms of Avis and Fukuda 1 [10] and Rote [130] require only sidedness queries and "second-order" coordinate comparisons between vertices of the dual hyperplane arrangment.
Reference: [37] <author> B. Chazelle. </author> <title> A spectral approach to lower bounds. </title> <booktitle> In Proc. 35th Annu. IEEE Sympos. Found. Comput. Sci., </booktitle> <pages> pages 674-682, </pages> <year> 1994. </year>
Reference-contexts: Willard [153] considers dynamic orthogonal range searching data structures and shows that under some fairly restrictive assumptions, allowing subtractions cannot make these structures more efficient. Chazelle derives nontrivial (but very weak) lower bounds for o*ine halfplane <ref> [37] </ref> and orthogonal range counting [38] by examining the spectra of point-range incidence matrices. Absolutely 9 nothing is known about range searching over more complicated domains such as rings or fields.
Reference: [38] <author> B. Chazelle. </author> <title> Lower bounds for off-line range searching. </title> <booktitle> In Proc. 27th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 733-740, </pages> <year> 1995. </year>
Reference-contexts: Chazelle and Rosenberg [42, 41] derived lower bounds for computing partial sums in multi-dimensional arrays (a special case of orthogonal range searching where the points lie on a lattice), and Chazelle <ref> [38] </ref> proved lower bounds for both orthogonal and simplex range searching. With the exception of halfspace range searching [23], all of these lower bounds are optimal, up to polylogarithmic or n " factors, in the semigroup model. <p> Willard [153] considers dynamic orthogonal range searching data structures and shows that under some fairly restrictive assumptions, allowing subtractions cannot make these structures more efficient. Chazelle derives nontrivial (but very weak) lower bounds for o*ine halfplane [37] and orthogonal range counting <ref> [38] </ref> by examining the spectra of point-range incidence matrices. Absolutely 9 nothing is known about range searching over more complicated domains such as rings or fields. <p> Of course, we cannot apply this argument to either the decision version or the counting version of Hopcroft's problem, since the output size for these problems is constant. Our planar lower bounds are ultimately based on the Erd-os configuration. Chazelle <ref> [33, 38] </ref> has established lower bounds for the closely related simplex range searching problem: Given a set of points and a set of simplices, how many points are in each simplex? For example, any data structure of size s that supports triangular range queries among n points in the plane requires <p> It follows that answering n queries over n points requires (n 4=3 ) time in the worst case. For the o*ine version of the same problem, where all the triangles are known in advance, Chazelle establishes a slightly weaker bound of (n 4=3 = log 4=3 n) <ref> [38] </ref>, although an (n 4=3 ) lower bound follows immediately from the Erd-os construction using Chazelle's methods. <p> The theorem now follows from Lemma 6.4. 2 Theorem 6.6. ffi 2 (n; m) = (n + n 2=3 m 2=3 + m) 3 Perhaps it is more interesting that Chazelle's static lower bounds <ref> [33, 38] </ref> do not use this construction. 72 Proof: Consider any configuration of n points and m=2 lines with (n + n 2=3 m 2=3 + m) point-line incidences, as given by Lemma 6.4. <p> The lower bound follows from the following result of Chazelle <ref> [38, Lemma 3.3] </ref>. (Chazelle's lemma only deals with the case n = m, but his proof generalizes immediately to the more general case.) Lemma 6.16 (Chazelle). <p> Better such configurations would immediately lead to better lower bounds. Lower bounds in the Fredman/Yao semigroup arithmetic model have a similar basis. For example, Chazelle's lower bounds for o*ine simplex range searching <ref> [38] </ref> is based on a similar configuration of points and slabs. (See also [44].) Can we derive better polyhedral cover size bounds for points and hyperplanes from these configurations? Another open problem is to prove tight lower bounds for online halfspace range query problems.
Reference: [39] <author> B. Chazelle, L. J. Guibas, and D. T. Lee. </author> <title> The power of geometric duality. </title> <journal> BIT, </journal> <volume> 25 </volume> <pages> 76-90, </pages> <year> 1985. </year>
Reference-contexts: Since there is an algorithm that solves this problem in time O (n d ) <ref> [39, 68, 69] </ref>, our lower bound is tight. <p> In 1983, van Leeuwen [151] asked for an algorithm to solve this problem in time o (n 2 log n). Chazelle, Guibas, and Lee <ref> [39] </ref> and Edelsbrunner, O'Rourke, and Seidel [68] independently discovered an algorithm that runs in time and space O (n 2 ) by constructing the arrangement of lines dual to the input points. 1 Edelsbrunner et al. [68] also solved the higher-dimensional version of this problem, which we call the affine degeneracy <p> This is not quite as unreasonable a restriction as it may appear at first glance; all known algorithms for determining degeneracy or order type rely (or can be made to rely) exclusively on sidedness queries <ref> [39, 65, 68] </ref>. Our lower bound implies that there is no hope of improving these algorithms unless other primitives are used. These lower bounds follow from an extremely simple adversary argument. <p> An pseu-doline arrangement is stretchable if it can be continuously deformed into an arrangement of straight lines. A theorem of Mnev [116] (see also [137, 97, 128]) implies that determining if a pseudoline arrangement is stretchable is NP-hard. 6 Every known algorithm that detects degeneracies in arrangements of lines <ref> [39, 65, 68, 69] </ref> can also be used to detect degeneracies in arrangements of pseudolines. In fact, there is no known algorithmic separation of lines and pseudolines. <p> In two and three dimensions, we can easily build a linear-size data structure, in O (n log n) time, that allows halfspace emptiness queries to be answered in logarithmic time <ref> [5, 39, 57] </ref>. In higher dimensions, a randomized algorithm due to Clarkson [47] answers half-space emptiness queries in time O (log n) after O (n bd=2c+" ) preprocessing time.
Reference: [40] <author> B. Chazelle and J. Matousek. </author> <title> Derandomizing an output-sensitive convex hull algorithm in three dimensions. </title> <type> Technical report, </type> <institution> Dept. Comput. Sci., Princeton Univ., </institution> <year> 1992. </year>
Reference-contexts: If the output size f is also taken into account, the lower bound drops to (n log f) [101], and a number of algorithms match this bound both in the plane [101, 28, 29] and in three dimensions <ref> [50, 40] </ref>. In higher dimensions, the problem is not quite so completely solved. Seidel's "beneath-beyond" algorithm [132] constructs d-dimensional convex hulls in time O (n dd=2e ).
Reference: [41] <author> B. Chazelle and B. Rosenberg. </author> <title> Computing partial sums in multidimensional arrays. </title> <booktitle> In Proc. 5th Annu. ACM Sympos. Comput. Geom., </booktitle> <pages> pages 131-139, </pages> <year> 1989. </year>
Reference-contexts: Chazelle and Rosenberg <ref> [42, 41] </ref> derived lower bounds for computing partial sums in multi-dimensional arrays (a special case of orthogonal range searching where the points lie on a lattice), and Chazelle [38] proved lower bounds for both orthogonal and simplex range searching.
Reference: [42] <author> B. Chazelle and B. Rosenberg. </author> <title> The complexity of computing partial sums off-line. </title> <journal> Internat. J. Comput. Geom. Appl., </journal> <volume> 1(1) </volume> <pages> 33-45, </pages> <year> 1991. </year>
Reference-contexts: Chazelle and Rosenberg <ref> [42, 41] </ref> derived lower bounds for computing partial sums in multi-dimensional arrays (a special case of orthogonal range searching where the points lie on a lattice), and Chazelle [38] proved lower bounds for both orthogonal and simplex range searching.
Reference: [43] <author> B. Chazelle, M. Sharir, and E. Welzl. </author> <title> Quasi-optimal upper bounds for simplex range searching and new zone theorems. </title> <journal> Algorithmica, </journal> <volume> 8 </volume> <pages> 407-429, </pages> <year> 1992. </year>
Reference-contexts: Edelsbrunner, Guibas, and Sharir [66] developed a randomized algorithm with expected running time O (n 4=3+" ) 1 ; see also [64]. A somewhat simpler algorithm with the same running time was developed by Chazelle, Sharir, and Welzl <ref> [43] </ref>. Further research replaced the n " term in this upper bound with a succession of smaller and smaller polylogarithmic factors. <p> Each algorithm divides space into a number of regions, determines which points and hyperplanes intersect each region, and recursively solves the resulting subprob-lems. In some cases <ref> [32, 66, 43] </ref>, the number of regions used at each level of recursion is a constant, and these algorithms fit naturally into the partitioning algorithm framework.
Reference: [44] <author> Bernard Chazelle and Burton Rosenberg. </author> <title> Simplex range reporting on a pointer machine. </title> <journal> Comput. Geom. Theory Appl., </journal> <volume> 5 </volume> <pages> 237-247, </pages> <year> 1996. </year>
Reference-contexts: The query time is the number of nodes visited; all other aspects of computation are ignored. Chazelle [34] derives lower bounds for orthogonal range reporting in this model; his techniques were applied to simplex range reporting by Chazelle and Rosenberg <ref> [44] </ref>. Again, this model is inappropriate for studying range emptiness problems, since if a query range is empty, we don't need to do any work at all. 1.2 New Results The remainder of this thesis divides naturally into two parts. <p> All these lower bounds hold in the Fredman/Yao semigroup arithmetic model; see Section 1.3. For related results, see also <ref> [23, 44] </ref>. Lower bounds in the semigroup model are based on the existence of configurations of points and ranges, such as the planar Erd-os configuration, whose incidence graphs have (a subgraph with) no large complete bipartite subgraphs. Our lower bounds have a similar basis. <p> Better such configurations would immediately lead to better lower bounds. Lower bounds in the Fredman/Yao semigroup arithmetic model have a similar basis. For example, Chazelle's lower bounds for o*ine simplex range searching [38] is based on a similar configuration of points and slabs. (See also <ref> [44] </ref>.) Can we derive better polyhedral cover size bounds for points and hyperplanes from these configurations? Another open problem is to prove tight lower bounds for online halfspace range query problems. Bronnimann, Chazelle, and Pach [23] have proven time-space tradeoffs for halfspace counting data structures in the Fredman/Yao semigroup model. <p> Chazelle and Rosenberg <ref> [44] </ref> have developed quasi-optimal tradeoffs for simplex reporting data structures in Tarjan's pointer machine model, but no lower bounds are known for halfspace reporting. No lower bounds are known for online halfspace emptiness queries in any model of computation.
Reference: [45] <author> F. R. K. Chung, P. Erd-os, and J. Spencer. </author> <title> On the decomposition of graphs into complete bipartite subgraphs. </title> <editor> In Paul Erd-os, editor, </editor> <booktitle> Studies in pure mathematics, </booktitle> <pages> pages 95-101. </pages> <publisher> Birkhauser, </publisher> <year> 1983. </year> <month> 112 </month>
Reference-contexts: Tuza [149], and independently Chung, Erd-os, and Spencer <ref> [45] </ref>, showed that every n fi m bipartite graph has such a cover of size O (nm= log (max (m; n))) and that this bound is tight in the worst case, up to constant factors. These results apply immediately to monochromatic covers of arbitrary sign matrices.
Reference: [46] <author> K. Clarkson, H. Edelsbrunner, L. Guibas, M. Sharir, and E. Welzl. </author> <title> Combinatorial complexity bounds for arrangements of curves and spheres. </title> <journal> Discrete Comput. Geom., </journal> <volume> 5 </volume> <pages> 99-160, </pages> <year> 1990. </year>
Reference-contexts: The corresponding upper bound was first proven by Szemeredi and Trotter [143]. A simpler proof, with better constants, was later given by Clarkson, Edelsbrunner, Guibas, Sharir, and Welzl <ref> [46] </ref>. A completely elementary proof has recently discovered by Szekely [119]. <p> The probabilistic counting techniques of Clarkson et al. <ref> [46] </ref> imply the better upper bound O (n + n 4=5 m 3=5 + m). We omit further details. Theorem 6.9. 3 (n; m) = (n + n 5=6 m 1=2 + n 1=2 m 5=6 + m) Proof: Consider the case n 1=3 &lt; m n. <p> Finally, Lemma 6.12 implies that any d hyperplanes in H intersect in a unique point. We omit further details. 2 Note that the lower bound for dimension d only improves the bound for dimension d - 1 when n = (m (d-1)=2 ). Again, using probabilistic counting techniques <ref> [46] </ref>, we can prove an upper bound of I (P; H) = O (n + n (2d-2)=(2d-1) m d=(2d-1) + m) if any d hyperplanes in H intersect in at most one point. The previous lemma immediately gives us the following lower bound for d (n; m).
Reference: [47] <author> K. L. Clarkson. </author> <title> New applications of random sampling in computational geometry. </title> <journal> Discrete Comput. Geom., </journal> <volume> 2 </volume> <pages> 195-222, </pages> <year> 1987. </year>
Reference-contexts: In two and three dimensions, we can easily build a linear-size data structure, in O (n log n) time, that allows halfspace emptiness queries to be answered in logarithmic time [5, 39, 57]. In higher dimensions, a randomized algorithm due to Clarkson <ref> [47] </ref> answers half-space emptiness queries in time O (log n) after O (n bd=2c+" ) preprocessing time. <p> The lower bounds then follow immediately from Theorem 7.6 and 7.7. 2 Partitioning algorithms for the halfspace emptiness problem can (and do <ref> [47, 107] </ref>) apply a version of the "containment shortcut" described in Section 6.3.4. If some query region lies entirely in a hyperplane's lower halfspace, then the hyperplane need not traverse the corresponding edge.
Reference: [48] <author> K. L. Clarkson. </author> <title> A Las Vegas algorithm for linear programming when the dimension is small. </title> <booktitle> In Proc. 29th Annu. IEEE Sympos. Found. Comput. Sci., </booktitle> <pages> pages 452-456, </pages> <year> 1988. </year>
Reference-contexts: Another similar problem is deciding, given a set of points, which ones are vertices of the set's convex hull. This problem can be decided in O (n 2 ) time (using only sidedness queries!) by invoking a linear programming algorithm once for each point <ref> [48, 109, 113, 136] </ref>. This upper bound can be improved to O (n 2bd=2c=(bd=2c-1) polylog n) using an algorithm due to Chan [29]. Except for the polylogarithmic term, this algorithm is almost certainly optimal. <p> In this chapter, we will consider the following formulation of the problem: Given a set of points and hyperplanes, is every point above every hyperplane? Using linear programming <ref> [48, 109, 113, 136] </ref>, we can decide in linear time whether the union of a set of halfspaces is IR d .
Reference: [49] <author> K. L. Clarkson. </author> <title> More output-sensitive geometric algorithms. </title> <booktitle> In Proc. 35th Annu. IEEE Sympos. Found. Comput. Sci., </booktitle> <pages> pages 695-702, </pages> <year> 1994. </year>
Reference-contexts: The fastest algorithm in higher dimensions, due to Chan [29], runs in time O (n log f + (nf) 1-1=(bd=2c+1) polylog n); this algorithm is optimal when f is sufficiently small. For related results, see <ref> [10, 30, 49, 50, 101, 134] </ref>. There are still large gaps between these upper bounds and the lower bound (n log f + f). <p> These include the "gift-wrapping" algorithms of Chand and Kapur [30] and Swart [142], the "beneath-beyond" method of Seidel [132], Clarkson and Shor's [50] and Seidel's [136] randomized incremental algorithms, Chazelle's worst-case optimal algorithm [36], and the recursive partial-order algorithm of Clarkson <ref> [49] </ref>. 38 Seidel's "shelling" algorithm [134] and the space-efficient gift-wrapping algorithms of Avis and Fukuda 1 [10] and Rote [130] require only sidedness queries and "second-order" coordinate comparisons between vertices of the dual hyperplane arrangment. <p> Matousek [110] and Chan [29] improve the running times of these algorithms (in an output-sensitive sense), by finding the extreme points more quickly. Clarkson <ref> [49] </ref> describes a similar improvement to a randomized incremental algorithm. Since every point in our adversary configuration is extreme, our lower bound still holds even if the extremity of a point can be decided for free.
Reference: [50] <author> K. L. Clarkson and P. W. Shor. </author> <title> Applications of random sampling in computational geometry, II. </title> <journal> Discrete Comput. Geom., </journal> <volume> 4 </volume> <pages> 387-421, </pages> <year> 1989. </year>
Reference-contexts: If the output size f is also taken into account, the lower bound drops to (n log f) [101], and a number of algorithms match this bound both in the plane [101, 28, 29] and in three dimensions <ref> [50, 40] </ref>. In higher dimensions, the problem is not quite so completely solved. Seidel's "beneath-beyond" algorithm [132] constructs d-dimensional convex hulls in time O (n dd=2e ). <p> Seidel's "beneath-beyond" algorithm [132] constructs d-dimensional convex hulls in time O (n dd=2e ). After a ten-year wait, Chazelle [36] improved the running time to O (n bd=2c ) by derandom-izing a randomized incremental algorithm of Clarkson and Shor <ref> [50] </ref>; see also [136]. Since an n-vertex polytope in IR d can have (n bd=2c ) facets [87], Seidel's algorithm is optimal in even dimensions, and Chazelle's algorithm is optimal in all dimensions, in the worst case. Several faster algorithms are known when the output size is also considered. <p> The fastest algorithm in higher dimensions, due to Chan [29], runs in time O (n log f + (nf) 1-1=(bd=2c+1) polylog n); this algorithm is optimal when f is sufficiently small. For related results, see <ref> [10, 30, 49, 50, 101, 134] </ref>. There are still large gaps between these upper bounds and the lower bound (n log f + f). <p> These include the "gift-wrapping" algorithms of Chand and Kapur [30] and Swart [142], the "beneath-beyond" method of Seidel [132], Clarkson and Shor's <ref> [50] </ref> and Seidel's [136] randomized incremental algorithms, Chazelle's worst-case optimal algorithm [36], and the recursive partial-order algorithm of Clarkson [49]. 38 Seidel's "shelling" algorithm [134] and the space-efficient gift-wrapping algorithms of Avis and Fukuda 1 [10] and Rote [130] require only sidedness queries and "second-order" coordinate comparisons between vertices of the
Reference: [51] <author> R. Cole, M. Sharir, and C. K. Yap. </author> <title> On k-hulls and related problems. </title> <journal> SIAM J. Comput., </journal> <volume> 16 </volume> <pages> 61-77, </pages> <year> 1987. </year>
Reference-contexts: algorithm for Hopcroft's problem, due to Chazelle [32], runs in time O (n 1:695 ). (Actually, this algorithm counts intersections among a set of n line segments in the plane, but it can easily be modified to count point-line incidences instead.) A very simple algorithm, attributed to Hopcroft and Seidel <ref> [51] </ref>, described in [62, p. 350], runs in time O (n 3=2 log 1=2 n). Cole, Sharir, and Yap [51] combined these two algorithms, achieving a running time of O (n 1:412 ). <p> intersections among a set of n line segments in the plane, but it can easily be modified to count point-line incidences instead.) A very simple algorithm, attributed to Hopcroft and Seidel <ref> [51] </ref>, described in [62, p. 350], runs in time O (n 3=2 log 1=2 n). Cole, Sharir, and Yap [51] combined these two algorithms, achieving a running time of O (n 1:412 ). Edelsbrunner, Guibas, and Sharir [66] developed a randomized algorithm with expected running time O (n 4=3+" ) 1 ; see also [64]. <p> The modified algorithm can then be described as a partitioning algorithm. Other algorithms construct a point location data structure for the arrangement of the entire set of hyperplanes <ref> [51, 66, 35] </ref>. Usually, this is done only when the number of hyperplanes is much smaller than the number of points.
Reference: [52] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: For example, consider the simple problem of choosing the largest of n 1 We assume the reader is familiar with the standard asymptotic notations o (); O (); fi (); (), and !(). Otherwise, see <ref> [52, 103] </ref>. 4 input items. Any reasonable algorithm obviously requires linear time 2 , but since there are only n possible outputs, the information-theoretic lower bound is only (log n).
Reference: [53] <author> M. de Berg, M. Overmars, and O. Schwarzkopf. </author> <title> Computing and verifying depth orders. </title> <booktitle> In Proc. 8th Annu. ACM Sympos. Comput. Geom., </booktitle> <pages> pages 138-145, </pages> <year> 1992. </year>
Reference-contexts: We mention one specific example, the cyclic overlap problem. Given a set of nonintersecting line segments 91 in IR 3 , does any subset form a cycle with respect to the "above" relation? The fastest known algorithm for this problem, due to de Berg, Overmars, and Schwarzkopf <ref> [53] </ref>, runs in time O (n 4=3+" ), using a divide-and-conquer strategy very similar to algorithms for Hopcroft's problem. In the algebraic decision tree model, the cyclic overlap problem is at least as hard as Hopcroft's problem [75].
Reference: [54] <author> M. de Berg and O. Schwarzkopf. </author> <title> Cuttings and applications. </title> <type> Report RUU-CS-92-26, </type> <institution> Dept. Comput. Sci., Utrecht Univ., </institution> <address> Utrecht, Netherlands, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: However, the multiplicative constants hidden in the big-Oh notation depend on ", and tend to infinity as " approaches zero. 63 n points and m lines in the plane in time O (n log m + n 2=3 m 2=3 2 O (log fl (n+m)) + m log n) <ref> [54] </ref>, or more generally among n points and m hyperplanes in IR d in time O n log m + n d=(d+1) m d=(d+1) 2 O (log fl (n+m)) + m log n : The lower bound history is much shorter.
Reference: [55] <author> M. Dietzfelbinger. </author> <title> Lower bounds for sorting of sums. </title> <journal> Theoret. Comput. Sci., </journal> <volume> 66 </volume> <pages> 137-155, </pages> <year> 1989. </year>
Reference-contexts: In this model of computation, our lower bound is as large as possible. Previously, this lower bound was known only for even r, and only for one special case <ref> [55, 56, 80] </ref>. A key step in the lower bound proof is the introduction of formal infinitesimals into the adversary configuration. <p> The first trick is to allow our adversary configurations to contain formal in-finitesimals, instead of just real numbers. Tarski's Transfer Principle implies that for any algorithm, if there is a hard configuration with infinitesimals, then a corresponding real configuration exists with the same properties. Previously, Dietzfelbinger and Maass <ref> [56, 55] </ref> used a similar technique to prove lower bounds, using "inaccessible" numbers, or numbers having "different orders of magnitude". Unlike their technique, using infinitesimals makes it possible, and indeed sufficient, to derive a single adversary configuration for any problem, rather than explicitly constructing a different configuration for every algorithm. <p> Fredman's result was generalized by Dietzfelbinger <ref> [55] </ref>, who derived an (n r=2 ) lower bound on the depth of any comparison tree algorithm that determines, given a set of n reals, whether any two subsets of size r=2 have the same sum. <p> Rather than constructing such configurations explicitly in terms 56 of the coefficients of the query polynomials, as was done in <ref> [56, 55] </ref>, we nonconstructively derive their existence from our infinitesimal construction.
Reference: [56] <author> Martin Dietzfelbinger and Wolfgang Maass. </author> <title> Lower bound arguments with "inaccessible" numbers. </title> <journal> J. Comput. Syst. Sci., </journal> <volume> 36 </volume> <pages> 313-335, </pages> <year> 1988. </year>
Reference-contexts: In this model of computation, our lower bound is as large as possible. Previously, this lower bound was known only for even r, and only for one special case <ref> [55, 56, 80] </ref>. A key step in the lower bound proof is the introduction of formal infinitesimals into the adversary configuration. <p> The first trick is to allow our adversary configurations to contain formal in-finitesimals, instead of just real numbers. Tarski's Transfer Principle implies that for any algorithm, if there is a hard configuration with infinitesimals, then a corresponding real configuration exists with the same properties. Previously, Dietzfelbinger and Maass <ref> [56, 55] </ref> used a similar technique to prove lower bounds, using "inaccessible" numbers, or numbers having "different orders of magnitude". Unlike their technique, using infinitesimals makes it possible, and indeed sufficient, to derive a single adversary configuration for any problem, rather than explicitly constructing a different configuration for every algorithm. <p> This nonuniform algorithm can be adapted to solve any of the linear satisfiability problems we consider, in the same amount of time <ref> [56] </ref>. Thus, there is no hope of proving lower bounds bigger than (n 4 log n) for any of these problems in the linear decision tree model. <p> Rather than constructing such configurations explicitly in terms 56 of the coefficients of the query polynomials, as was done in <ref> [56, 55] </ref>, we nonconstructively derive their existence from our infinitesimal construction.
Reference: [57] <author> D. P. Dobkin and D. G. Kirkpatrick. </author> <title> Determining the separation of preprocessed polyhedra a unified approach. </title> <booktitle> In Proc. 17th Internat. Colloq. Automata Lang. Program., volume 443 of Lecture Notes in Computer Science, </booktitle> <pages> pages 400-413. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year> <month> 113 </month>
Reference-contexts: In two and three dimensions, we can easily build a linear-size data structure, in O (n log n) time, that allows halfspace emptiness queries to be answered in logarithmic time <ref> [5, 39, 57] </ref>. In higher dimensions, a randomized algorithm due to Clarkson [47] answers half-space emptiness queries in time O (log n) after O (n bd=2c+" ) preprocessing time.
Reference: [58] <author> D. P. Dobkin and R. J. Lipton. </author> <title> On the complexity of computations under varying sets of primitives. </title> <journal> J. Comput. Syst. Sci., </journal> <volume> 18 </volume> <pages> 86-91, </pages> <year> 1979. </year>
Reference-contexts: Typically, the parameter d is omitted, and assumed to be a fixed constant. An important special case of algebraic decision trees are linear decision trees, in which every query polynomial is linear (i.e., d = 1) <ref> [58, 117] </ref>. The element uniqueness problem asks, given a list of n numbers, whether any two are equal. It seems "obvious" that any algorithm that solves this problem must sort the input values 3 , and so the decision tree complexity "ought" to be (n log n). <p> Unfortunately, since there are only two possible outputs, YES and NO, the information-theoretic bound is trivial. A simple adversary argument establishes a lower bound of n - 1 in the comparison tree model, but that is hardly satisfactory. Dobkin and Lipton <ref> [58] </ref> proved a lower bound of (n log n) for the element uniqueness problem in the linear decision tree model, and therefore also in the comparison tree model, as follows. <p> An (n log n) lower bound for any linear satisfiability problem follows easily from techniques of Dobkin and Lipton in the linear decision tree model <ref> [58] </ref>, Steele and Yao in the algebraic decision tree model [138], and Ben-Or in the algebraic computation tree model [16]. <p> Given an input X 2 IR n , the sign of q v (X) is computed, where v is the root of the tree, and the computation proceeds recursively in the appropriate subtree. When a leaf is reached, its label is returned as the output of the algorithm. (Compare <ref> [58, 138] </ref>.) An r-linear decision tree is a linear decision tree, each of whose query polynomials has at most r terms. Let K be any ordered field extension of the reals.
Reference: [59] <author> David Dobkin and Richard J. Lipton. </author> <title> A lower bound of 1 2 n 2 on linear search programs for the knapsack problem. </title> <journal> J. Comput. Sys. Sci., </journal> <volume> 16(3) </volume> <pages> 413-417, </pages> <year> 1978. </year>
Reference-contexts: An !(n log n) lower bound for any natural problem solvable in polynomial time would be a major breakthrough. Quadratic lower bounds are known for a few NP-complete problems <ref> [59, 16, 93] </ref>, but again, this is the best lower 7 bound we can prove, since for these problems the number of defining inequalities is only singly-exponential. 1.1.3 Semigroup Arithmetic The semigroup arithmetic model was introduced by Fredman [82, 81] and refined by Yao [155] and Chazelle [34] to study the
Reference: [60] <author> Charles Lutwidge Dodgson. St. James Gazette, </author> <month> August 1, </month> <pages> 1883, pages 5-6. </pages>
Reference-contexts: Adversary arguments have also been used to prove lower bounds on the number of comparisons required to choose the largest and smallest elements in a list, the second-largest element [102, 104] (see also <ref> [60] </ref>), the median element [21, 122], or the kth largest element for arbitrary values of k [122, 99].
Reference: [61] <author> M. E. Dyer. </author> <title> The complexity of vertex enumeration methods. </title> <journal> Math. Oper. Res., </journal> <volume> 8 </volume> <pages> 381-402, </pages> <year> 1983. </year>
Reference-contexts: When the dimension is allowed to vary with the input size, deciding if a convex hull is simplicial is coNP-complete [31], and counting the number of facets is #P-hard in general, and NP-hard for simplicial polytopes <ref> [61] </ref>. Our results apply when the dimension d is fixed. Our lower bounds follow from a generalization of the previous chapter's adversary argument. We start by constructing a set whose convex hull contains a large number of independent degenerate facets.
Reference: [62] <author> H. Edelsbrunner. </author> <title> Algorithms in Combinatorial Geometry, </title> <booktitle> volume 10 of EATCS Monographs on Theoretical Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, West Germany, </address> <year> 1987. </year>
Reference-contexts: For example, any algorithm 3 that constructs an arrangement of n hyperplanes in IR d must take time (n d ) 1 in the worst case, because the arrangement can have that many cells <ref> [62] </ref>. Similarly, any algorithm that constructs the convex hull of a set of n points in IR d must take time (n bd=2c ) in the worst case, because there are polytopes with that many facets [87]. <p> outside) the sphere defined by d + 1 points p 0 ; : : : ; p d in IR d , then the image of q lies below (resp. above) the hyperplane in IR d+1 defined by the images of p 0 ; : : : ; p d <ref> [62] </ref>. Sidedness queries on the lifted point set are thus equivalent to insphere queries in the original d-dimensional point set. Two-dimensional insphere queries are also called incircle queries. <p> Proof: As before, let Q A denote the set of query polynomials used by A. Each query in Q A induces a hyperplane in the configuration space IR n , and these hyperplanes define a cell complex, called the arrangement <ref> [62] </ref>. Color each hyperplane "red" if it corresponds to a direct query, and "green" otherwise. Each input configuration corresponds to a point in some cell C in this arrangement. Nondegenerate configurations correspond to points in n-dimensional cells; degenerate configurations correspond to points in cells of lower dimension. <p> Since an arrangement of N hyperplanes 2 For any integer a 0, the falling factorial power n a is defined as n (n - 1) (n a+ 1) = n!=(n a)! [92]. 59 in IR D has at most P D N = O (N D ) cells <ref> [62] </ref>, there are at most O ((2n r=2 ) 2n ) = O ((2n) rn ) possible orderings. <p> problem, due to Chazelle [32], runs in time O (n 1:695 ). (Actually, this algorithm counts intersections among a set of n line segments in the plane, but it can easily be modified to count point-line incidences instead.) A very simple algorithm, attributed to Hopcroft and Seidel [51], described in <ref> [62, p. 350] </ref>, runs in time O (n 3=2 log 1=2 n). Cole, Sharir, and Yap [51] combined these two algorithms, achieving a running time of O (n 1:412 ). <p> Erd-os constructed a set of n points and n lines in the plane with (n 4=3 ) incident point-line pairs <ref> [62, p. 112] </ref>. It follows immediately that any algorithm that reports all incident pairs requires time (n 4=3 ) in the worst case. <p> In Section 6.3, we generalize this connection to higher dimensions. 6.2.2 Two Dimensions To derive lower bounds for ffi 2 (n; m) and 2 (n; m), we use the following combina torial result of Erd-os. (See [82] or <ref> [62, p.112] </ref> for proofs.) Lemma 6.4 (Erd-os). For all n and m, there is a set of n points and m lines in the plane with (n + n 2=3 m 2=3 + m) incident pairs. Thus, I 2 (n; m) = (n + n 2=3 m 2=3 + m). <p> Finally, a few algorithms partition the points or the hyperplanes arbitrarily into subsets, without using geometric information of any kind <ref> [62, p. 350] </ref>,[51]. In this case, every hyperplane becomes part of every subproblem.
Reference: [63] <author> H. Edelsbrunner, L. Guibas, J. Hershberger, R. Seidel, M. Sharir, J. Snoeyink, and E. Welzl. </author> <title> Implicitly representing arrangements of lines or segments. Discrete Com put. </title> <journal> Geom., </journal> <volume> 4 </volume> <pages> 433-466, </pages> <year> 1989. </year>
Reference-contexts: Further research replaced the n " term in this upper bound with a succession of smaller and smaller polylogarithmic factors. The running time was improved by Edelsbrunner, Guibas, Hershberger, Seidel, Sharir, Snoeyink, and Welzl <ref> [63] </ref> to O (n 4=3 log 4 n) (expected); then by Agarwal [1] to O (n 4=3 log 1:78 n); then by Chazelle [35] to O (n 4=3 log 1=3 n); and most recently by Matousek [111] to n 4=3 2 O (log fl n) . <p> Matousek's algorithm and Theorem 6.21 immediately give us the following theorem. Theorem 6.25. d (n; m) = O However, other algorithms do not use the point location data structure to locate hyperplanes, at least not at all levels of recursion. In these algorithms <ref> [63, 1] </ref>, the query regions form a decomposition of space into cells of constant complexity, typically simplices or trapezoids. The algorithms determine which cells a given hyperplane hits by iteratively "walking" through the cells.
Reference: [64] <author> H. Edelsbrunner, L. Guibas, and M. Sharir. </author> <title> The complexity of many cells in arrangements of planes and related problems. </title> <journal> Discrete Comput. Geom., </journal> <volume> 5 </volume> <pages> 197-216, </pages> <year> 1990. </year>
Reference-contexts: Cole, Sharir, and Yap [51] combined these two algorithms, achieving a running time of O (n 1:412 ). Edelsbrunner, Guibas, and Sharir [66] developed a randomized algorithm with expected running time O (n 4=3+" ) 1 ; see also <ref> [64] </ref>. A somewhat simpler algorithm with the same running time was developed by Chazelle, Sharir, and Welzl [43]. Further research replaced the n " term in this upper bound with a succession of smaller and smaller polylogarithmic factors. <p> Since i ? j, each such plane determines a unique line. Furthermore, since all these lines are tangent to a parabola, no three of them are concurrent. It follows that the intersection of any three planes in H consists of at most one point. 2 Edelsbrunner, Guibas, and Sharir <ref> [64] </ref> prove an upper bound of O (n log m + n 4=5+2" m 3=5-" +m) on the maximum number of incidences between n points and m planes, where no three planes contain a common line.
Reference: [65] <author> H. Edelsbrunner and L. J. Guibas. </author> <title> Topologically sweeping an arrangement. </title> <journal> J. Com put. Syst. Sci., </journal> <volume> 38 </volume> <pages> 165-194, </pages> <year> 1989. </year> <note> Corrigendum in 42 (1991), 249-251. </note>
Reference-contexts: al. [68] also solved the higher-dimensional version of this problem, which we call the affine degeneracy problem Their algorithm, given n points in IR d , determines whether d + 1 of them lie on the same hyperplane, in time and space O (n d ). 2 Edelsbrunner and Guibas <ref> [65] </ref> later improved the space bound to O (n) in all dimensions. <p> This is not quite as unreasonable a restriction as it may appear at first glance; all known algorithms for determining degeneracy or order type rely (or can be made to rely) exclusively on sidedness queries <ref> [39, 65, 68] </ref>. Our lower bound implies that there is no hope of improving these algorithms unless other primitives are used. These lower bounds follow from an extremely simple adversary argument. <p> An pseu-doline arrangement is stretchable if it can be continuously deformed into an arrangement of straight lines. A theorem of Mnev [116] (see also [137, 97, 128]) implies that determining if a pseudoline arrangement is stretchable is NP-hard. 6 Every known algorithm that detects degeneracies in arrangements of lines <ref> [39, 65, 68, 69] </ref> can also be used to detect degeneracies in arrangements of pseudolines. In fact, there is no known algorithmic separation of lines and pseudolines. <p> In this chapter, we show that (n 3 ) incircle queries are required to detect circular degeneracies in the plane, and (n d+1 ) insphere queries are required to detect proper spherical degeneracies in IR d . Both lower bounds are tight <ref> [65, 68, 69] </ref>. Like our previous 41 results, the lower bounds in this chapter are based on adversary arguments using "collapsible tuples". 4.1 Circular Degeneracies Theorem 4.1. Any decision tree algorithm that detects proper circular degeneracies, using only incircle queries, must have depth (n 3 ).
Reference: [66] <author> H. Edelsbrunner, L. J. Guibas, and M. Sharir. </author> <title> The complexity and construction of many faces in arrangements of lines and of segments. </title> <journal> Discrete Comput. Geom., </journal> <volume> 5 </volume> <pages> 161-196, </pages> <year> 1990. </year>
Reference-contexts: Cole, Sharir, and Yap [51] combined these two algorithms, achieving a running time of O (n 1:412 ). Edelsbrunner, Guibas, and Sharir <ref> [66] </ref> developed a randomized algorithm with expected running time O (n 4=3+" ) 1 ; see also [64]. A somewhat simpler algorithm with the same running time was developed by Chazelle, Sharir, and Welzl [43]. <p> Each algorithm divides space into a number of regions, determines which points and hyperplanes intersect each region, and recursively solves the resulting subprob-lems. In some cases <ref> [32, 66, 43] </ref>, the number of regions used at each level of recursion is a constant, and these algorithms fit naturally into the partitioning algorithm framework. <p> The modified algorithm can then be described as a partitioning algorithm. Other algorithms construct a point location data structure for the arrangement of the entire set of hyperplanes <ref> [51, 66, 35] </ref>. Usually, this is done only when the number of hyperplanes is much smaller than the number of points.
Reference: [67] <author> H. Edelsbrunner and E. P. Mucke. </author> <title> Simulation of simplicity: a technique to cope with degenerate cases in geometric algorithms. </title> <journal> ACM Trans. Graph., </journal> <volume> 9 </volume> <pages> 66-104, </pages> <year> 1990. </year>
Reference-contexts: Infinitesimals have been used extensively in perturbation techniques <ref> [67, 71, 160] </ref>, in algorithms dealing with real semialgebraic sets [25, 26], and in at least one other lower bound argument [93]. Let us now formally define our model of computation.
Reference: [68] <author> H. Edelsbrunner, J. O'Rourke, and R. Seidel. </author> <title> Constructing arrangements of lines and hyperplanes with applications. </title> <journal> SIAM J. Comput., </journal> <volume> 15 </volume> <pages> 341-363, </pages> <year> 1986. </year>
Reference-contexts: Since there is an algorithm that solves this problem in time O (n d ) <ref> [39, 68, 69] </ref>, our lower bound is tight. <p> In the plane, (n 3 ) incircle queries are required to decide if any four points lie on a common circle or line. These lower bounds are optimal <ref> [68, 69] </ref>. <p> In 1983, van Leeuwen [151] asked for an algorithm to solve this problem in time o (n 2 log n). Chazelle, Guibas, and Lee [39] and Edelsbrunner, O'Rourke, and Seidel <ref> [68] </ref> independently discovered an algorithm that runs in time and space O (n 2 ) by constructing the arrangement of lines dual to the input points. 1 Edelsbrunner et al. [68] also solved the higher-dimensional version of this problem, which we call the affine degeneracy problem Their algorithm, given n points <p> Chazelle, Guibas, and Lee [39] and Edelsbrunner, O'Rourke, and Seidel <ref> [68] </ref> independently discovered an algorithm that runs in time and space O (n 2 ) by constructing the arrangement of lines dual to the input points. 1 Edelsbrunner et al. [68] also solved the higher-dimensional version of this problem, which we call the affine degeneracy problem Their algorithm, given n points in IR d , determines whether d + 1 of them lie on the same hyperplane, in time and space O (n d ). 2 Edelsbrunner and Guibas [65] later <p> The fastest known algorithm for determining the order type of a set of points constructs its dual hyperplane arrangement in time and space O (n d ) <ref> [68, 69] </ref>. Even though all known representations of order type require space (n d ), there is some hope of a smaller representation, and thus, a faster algorithm, since it is known that there are only (n=d) fi (d 2 n) = 2 fi (n log n) order types [89]. <p> This is not quite as unreasonable a restriction as it may appear at first glance; all known algorithms for determining degeneracy or order type rely (or can be made to rely) exclusively on sidedness queries <ref> [39, 65, 68] </ref>. Our lower bound implies that there is no hope of improving these algorithms unless other primitives are used. These lower bounds follow from an extremely simple adversary argument. <p> An pseu-doline arrangement is stretchable if it can be continuously deformed into an arrangement of straight lines. A theorem of Mnev [116] (see also [137, 97, 128]) implies that determining if a pseudoline arrangement is stretchable is NP-hard. 6 Every known algorithm that detects degeneracies in arrangements of lines <ref> [39, 65, 68, 69] </ref> can also be used to detect degeneracies in arrangements of pseudolines. In fact, there is no known algorithmic separation of lines and pseudolines. <p> In this chapter, we show that (n 3 ) incircle queries are required to detect circular degeneracies in the plane, and (n d+1 ) insphere queries are required to detect proper spherical degeneracies in IR d . Both lower bounds are tight <ref> [65, 68, 69] </ref>. Like our previous 41 results, the lower bounds in this chapter are based on adversary arguments using "collapsible tuples". 4.1 Circular Degeneracies Theorem 4.1. Any decision tree algorithm that detects proper circular degeneracies, using only incircle queries, must have depth (n 3 ).
Reference: [69] <author> H. Edelsbrunner, R. Seidel, and M. Sharir. </author> <title> On the zone theorem for hyperplane arrangements. </title> <journal> SIAM J. Comput., </journal> <volume> 22(2) </volume> <pages> 418-429, </pages> <year> 1993. </year> <month> 114 </month>
Reference-contexts: Since there is an algorithm that solves this problem in time O (n d ) <ref> [39, 68, 69] </ref>, our lower bound is tight. <p> In the plane, (n 3 ) incircle queries are required to decide if any four points lie on a common circle or line. These lower bounds are optimal <ref> [68, 69] </ref>. <p> The result of a sidedness query is given by the sign of 1 We refer readers unfamiliar with projective duality to [140]. 2 The original analysis of their algorithm was flawed. A correct proof of the crucial Zone Theorem was later given by Edelsbrunner, Seidel, and Sharir <ref> [69] </ref>. 14 the following determinant. fi fi fi fi fi fi 1 p 11 p 12 p 1d . . . . . . 1 p d1 p d2 p dd fi fi fi fi fi The value of this determinant is d! times the signed volume of the simplex spanned <p> The fastest known algorithm for determining the order type of a set of points constructs its dual hyperplane arrangement in time and space O (n d ) <ref> [68, 69] </ref>. Even though all known representations of order type require space (n d ), there is some hope of a smaller representation, and thus, a faster algorithm, since it is known that there are only (n=d) fi (d 2 n) = 2 fi (n log n) order types [89]. <p> An pseu-doline arrangement is stretchable if it can be continuously deformed into an arrangement of straight lines. A theorem of Mnev [116] (see also [137, 97, 128]) implies that determining if a pseudoline arrangement is stretchable is NP-hard. 6 Every known algorithm that detects degeneracies in arrangements of lines <ref> [39, 65, 68, 69] </ref> can also be used to detect degeneracies in arrangements of pseudolines. In fact, there is no known algorithmic separation of lines and pseudolines. <p> In this chapter, we show that (n 3 ) incircle queries are required to detect circular degeneracies in the plane, and (n d+1 ) insphere queries are required to detect proper spherical degeneracies in IR d . Both lower bounds are tight <ref> [65, 68, 69] </ref>. Like our previous 41 results, the lower bounds in this chapter are based on adversary arguments using "collapsible tuples". 4.1 Circular Degeneracies Theorem 4.1. Any decision tree algorithm that detects proper circular degeneracies, using only incircle queries, must have depth (n 3 ).
Reference: [70] <author> H. Edelsbrunner and M. Sharir. </author> <title> A hyperplane incidence problem with applications to counting distances. </title> <editor> In P. Gritzman and B. Sturmfels, editors, </editor> <booktitle> Applied Geometry and Discrete Mathematics: The Victor Klee Festschrift, volume 4 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science, </booktitle> <pages> pages 253-263. </pages> <publisher> AMS Press, </publisher> <year> 1991. </year>
Reference-contexts: The best upper bound we can prove for the number of incidences between n points and m hyperplanes in IR 4 , where every point is above or on every hy-perplane and no four hyperplanes contain a line, is O (n + n 2=3 m 2=3 + m). (See <ref> [70] </ref> for the derivation of a similar upper bound.) No superlinear lower bounds are known in any dimension, so there is some hope for a linear upper bound. However, we can achieve a superlinear number of incidences in five dimensions, under a weaker combinatorial general position requirement.
Reference: [71] <author> I. Emiris and J. Canny. </author> <title> A general approach to removing degeneracies. </title> <booktitle> In Proc. 32nd Annu. IEEE Sympos. Found. Comput. Sci., </booktitle> <pages> pages 405-413, </pages> <year> 1991. </year>
Reference-contexts: Infinitesimals have been used extensively in perturbation techniques <ref> [67, 71, 160] </ref>, in algorithms dealing with real semialgebraic sets [25, 26], and in at least one other lower bound argument [93]. Let us now formally define our model of computation.
Reference: [72] <author> P. Erd-os and G. Purdy. </author> <title> Two combinatorial problems in the plane. </title> <journal> Discrete Comput. Geom., </journal> <volume> 13(3-4):441-443, </volume> <year> 1995. </year>
Reference-contexts: It follows that the set of lines fL (2i=n) j 1 i ng has 1 + bn (n - 3)=6c concurrent triples. See Figure 2.4 (a). See [84] for further details. Related results are described in [24] and <ref> [72] </ref>.
Reference: [73] <author> J. Erickson and R. Seidel. </author> <title> Better lower bounds on detecting affine and spherical degeneracies. </title> <journal> Discrete Comput. Geom., </journal> <volume> 13 </volume> <pages> 41-57, </pages> <year> 1995. </year>
Reference-contexts: Our results imply a quadratic lower bound for 3sum; we will present further details in Chapter 5. Given these reductions, one might think that we have just proven that every 4 Some earlier papers, including <ref> [73] </ref>, used the more suggestive but potentially misleading term "n 2 - hard" [85] (but see [20]). 5 This observation was the initial inspiration for my "weird moment curve" argument. 28 3sum-hard problem requires (n 2 ) time. Unfortunately, this is not the case. <p> this problem can be solved in O (n dd=4e ) time if dd=2e is odd, and in O (n dd=4e log n) time if dd=2e is even, 36 simplex highlighted, and (c) the corresponding collapsed polytope. by an algorithm that uses more complicated queries, similar to the algorithm described in <ref> [73] </ref>. The convex hull of the adversary configuration ! d (X 0 ) has dd=2e - 1 more facets than the convex hull of any collapsed configuration. Thus, we immediately have the following lower bound. Theorem 3.4. <p> The following construction is primarily due to Raimund Seidel <ref> [73] </ref>. 43 The point set S in question is the union of d + 1 smaller sets, S 1 [ [ S d [ D, where each S i consists of n=2 even integer points on the positive x i -axis, and D consists of about (d + 1)n=2 "odd" points <p> Thus, no simplex in S changes orientation. 2 4.3 Open Problems We conjecture that (n d+1 ) insphere queries are required to detect arbitrary spherical degeneracies. (I claimed this lower bound in <ref> [73] </ref>, but my "proof" was incorrect.) A proof of this conjecture would follow immediately from the construction of a set of numbers having (n d+1 ) (d + 2)-tuples in the zeroset of a certain symmetric polynomial, by applying the same "sliding adversary" argument used to prove many of our previous
Reference: [74] <author> Jeff Erickson. </author> <title> Lower bounds for linear satisfiability problems. </title> <booktitle> In Proc. 6th ACM-SIAM Sympos. Discrete Algorithms (SODA '95), </booktitle> <pages> pages 388-395, </pages> <year> 1995. </year>
Reference-contexts: Consequently, our linear satisfiability lower bounds do not imply lower bounds for these geometric problems. 60 Can the techniques of this chapter be applied directly to higher-dimensional problems? (My original presentation of these results <ref> [74] </ref> claimed to do just that, but the proofs were flawed; Lemma 4.1 in [74] is actually false.) Ultimately, we would like to prove a lower bound larger than (n log n) for any non-NP-hard polynomial satisfiability problem, in some general model of computation such as linear decision trees, algebraic decision <p> Consequently, our linear satisfiability lower bounds do not imply lower bounds for these geometric problems. 60 Can the techniques of this chapter be applied directly to higher-dimensional problems? (My original presentation of these results <ref> [74] </ref> claimed to do just that, but the proofs were flawed; Lemma 4.1 in [74] is actually false.) Ultimately, we would like to prove a lower bound larger than (n log n) for any non-NP-hard polynomial satisfiability problem, in some general model of computation such as linear decision trees, algebraic decision trees, or even algebraic computation trees.
Reference: [75] <author> Jeff Erickson. </author> <title> On the relative complexities of some geometric problems. </title> <booktitle> In Proc. 7th Canad. Conf. Comput. Geom., </booktitle> <pages> pages 85-90, </pages> <year> 1995. </year>
Reference-contexts: set of points and lines in the plane, does any point lie on any line? These and similar questions arise as subproblems or special cases of a large number of more complicated geometric problems, including point location, range searching, motion planning, collision detection, ray shooting, visibility, and hidden surface removal <ref> [86, 75] </ref>. In the last several years, computational geometers have developed powerful techniques for solving these problems efficiently, at least in theory, and there is a common belief that the best known algorithms for these problems are optimal or very close to optimal. <p> In the algebraic decision tree model, the cyclic overlap problem is at least as hard as Hopcroft's problem <ref> [75] </ref>. Apparently, however, this problem cannot even be solved by a partitioning algorithm, since the answer might depend on arbitrarily large tuples of segments, arbitrarily far apart. <p> In particular, the four dimensional case is wide open. It is not even known whether the four-dimensional halfspace emptiness problem is harder, or easier, than Hopcroft's problem in the plane <ref> [75] </ref>. The inner product doubling maps d can be used to reduce Hopcroft's problem in IR d to halfspace emptiness in IR d (d-3)=2 in linear time.
Reference: [76] <author> Jeff Erickson. </author> <title> New lower bounds for convex hull problems in odd dimensions. </title> <booktitle> In Proc. 12th Annu. ACM Sympos. Comput. Geom., </booktitle> <pages> pages 1-9, </pages> <year> 1996. </year>
Reference: [77] <author> Jeff Erickson. </author> <title> New lower bounds for halfspace emptiness. </title> <booktitle> In Proc. 37th Annu. IEEE Sympos. </booktitle> <institution> Found. Comput. Sci., </institution> <note> page to appear, </note> <year> 1996. </year>
Reference: [78] <author> Jeff Erickson. </author> <title> New lower bounds for Hopcroft's problem. </title> <journal> Disc. Comput. Geom., </journal> <note> 1996. Special issue of papers from the 11th ACM Sympos. Comput. Geom., to appear. </note>
Reference-contexts: We consider this problem in greater detail in the next chapter. Our arguments can easily be modified to apply to almost any range searching problem, and a wide range of other related problems; see <ref> [78] </ref>. We leave the details and further generalizations as exercises for the reader. Of course, none of the sub-quadratic algorithms listed previously follow the models of computation considered in this section. <p> Any extension of our lower bounds to a more general model, which would explicitly allow different strategies for locating points and hyperplanes, would be interesting. Our techniques imply lower bounds for several other problems similar to Hopcroft's problem <ref> [78] </ref>. Unfortunately, the partitioning algorithm model is specifically tailored to detect intersections or containments between pairs of objects, and there are a number of similar geometric problems for which the model simply does not apply. We mention one specific example, the cyclic overlap problem.
Reference: [79] <author> Stefan Felsner. </author> <title> On the number of arrangements of pseudolines. </title> <booktitle> In Proc. 12th Annu. ACM Sympos. Comput. Geom., </booktitle> <pages> pages 30-37, </pages> <year> 1996. </year>
Reference-contexts: See Chapter 5 for further discussion of nonuniform algorithms.) There are 2 fi (n 2 ) combinatorially distinct arrangements of n pseudolines in the plane <ref> [105, 79] </ref>. (Recall from the beginning of this chapter that only n fi (n) of these are stretchable [89].) It follows immediately that determining the order type of a pseudoline arrangement requires (n 2 ) time.
Reference: [80] <author> M. L. Fredman. </author> <title> How good is the information theory bound in sorting? Theoret. </title> <journal> Comput. Sci., </journal> <volume> 1 </volume> <pages> 355-361, </pages> <year> 1976. </year>
Reference-contexts: In this model of computation, our lower bound is as large as possible. Previously, this lower bound was known only for even r, and only for one special case <ref> [55, 56, 80] </ref>. A key step in the lower bound proof is the introduction of formal infinitesimals into the adversary configuration. <p> We show that in these models, any algorithm that solves any r-variable linear satisfiability problem must perform (n dr=2e ) direct queries in the worst case. This matches known upper bounds when r is odd, and is within a logarithmic factor when r is even. Moreover, results of Fredman <ref> [80] </ref> establish the existence of nonuniform algorithms whose running times match our lower bounds exactly. The adversary arguments we use to establish lower bounds for these require two 48 new tricks. The first trick is to allow our adversary configurations to contain formal in-finitesimals, instead of just real numbers. <p> The first better lower bound is due to Fredman <ref> [80] </ref>, who proved an (n 2 ) lower bound on the number of comparisons required to detect duplicate elements in the Minkowski sum X + Y of two sets of real numbers; his proof relies on a simple adversary argument. <p> We use the following result of Fredman <ref> [80] </ref> to show that our lower bounds cannot be improved in this case. Lemma 5.6 (Fredman). Let be a subset of the n! orderings of f1; : : : ; ng for some fixed n.
Reference: [81] <author> M. L. Fredman. </author> <title> A lower bound on the complexity of orthogonal range queries. </title> <journal> J. ACM, </journal> <volume> 28 </volume> <pages> 696-705, </pages> <year> 1981. </year> <month> 115 </month>
Reference-contexts: Quadratic lower bounds are known for a few NP-complete problems [59, 16, 93], but again, this is the best lower 7 bound we can prove, since for these problems the number of defining inequalities is only singly-exponential. 1.1.3 Semigroup Arithmetic The semigroup arithmetic model was introduced by Fredman <ref> [82, 81] </ref> and refined by Yao [155] and Chazelle [34] to study the complexity of range searching data structures. In this model, each point in the input is given a value from an abelian semigroup satisfying a mild technical condition 4 . <p> In effect, answers must be computed symbolically. Subtraction of semigroup values is also disallowed, even if the semigroup is actually a group, as is frequently the case. The semigroup model was originally introduced by Fredman <ref> [81, 82] </ref>, who derived lower bounds for dynamic orthogonal and halfplane range searching data structures, which must support the insertion and deletion of points as well as range queries.
Reference: [82] <author> M. L. Fredman. </author> <title> Lower bounds on the complexity of some optimal data structures. </title> <journal> SIAM J. Comput., </journal> <volume> 10 </volume> <pages> 1-10, </pages> <year> 1981. </year>
Reference-contexts: Quadratic lower bounds are known for a few NP-complete problems [59, 16, 93], but again, this is the best lower 7 bound we can prove, since for these problems the number of defining inequalities is only singly-exponential. 1.1.3 Semigroup Arithmetic The semigroup arithmetic model was introduced by Fredman <ref> [82, 81] </ref> and refined by Yao [155] and Chazelle [34] to study the complexity of range searching data structures. In this model, each point in the input is given a value from an abelian semigroup satisfying a mild technical condition 4 . <p> In effect, answers must be computed symbolically. Subtraction of semigroup values is also disallowed, even if the semigroup is actually a group, as is frequently the case. The semigroup model was originally introduced by Fredman <ref> [81, 82] </ref>, who derived lower bounds for dynamic orthogonal and halfplane range searching data structures, which must support the insertion and deletion of points as well as range queries. <p> A key step in the proofs of many semigroup lower bounds is the construction of a set of points and ranges, such that (some subgraph of) the incidence graph has several edges but no large complete bipartite subgraphs. For example, Fredman's lower bounds for dynamic halfplane range searching <ref> [82] </ref> rely on a construction of Erd-os of n points and n lines in the plane with (n 4=3 ) point-line incidences; since any pair of lines intersects in at most one point, the incidence graph for this point-line configuration has no K 2;2 . <p> In Section 6.3, we generalize this connection to higher dimensions. 6.2.2 Two Dimensions To derive lower bounds for ffi 2 (n; m) and 2 (n; m), we use the following combina torial result of Erd-os. (See <ref> [82] </ref> or [62, p.112] for proofs.) Lemma 6.4 (Erd-os). For all n and m, there is a set of n points and m lines in the plane with (n + n 2=3 m 2=3 + m) incident pairs. <p> For all n and m, there is a set of n points and m lines in the plane with (n + n 2=3 m 2=3 + m) incident pairs. Thus, I 2 (n; m) = (n + n 2=3 m 2=3 + m). Fredman <ref> [82] </ref> uses Erd-os' construction to prove lower bounds for dynamic range query data structures in the plane. 3 This lower bound is asymptotically tight. The corresponding upper bound was first proven by Szemeredi and Trotter [143].
Reference: [83] <author> Michael Fredman and Dan E. Willard. </author> <title> Surpassing the information-theoretic bound with fusion trees. </title> <journal> J. Comput. Syst. Sci., </journal> <volume> 47(3) </volume> <pages> 424-436, </pages> <year> 1993. </year> <note> The extended abstract (STOC 1990) had a much more colorful title. </note>
Reference-contexts: The situation may be comparable to sorting or element uniqueness|(n log n) time is required to sort using algebraic decision trees [16], but there are significantly faster sorting algorithms in integer RAM models <ref> [83, 9] </ref>. Are there faster algorithms for useful special cases? For example, a set of n points in the plane in (loosely) convex position has only n collapsible triangles, and we can easily detect colinear triples in such a set in O (n log n) time.
Reference: [84] <author> Z. Furedi and I. Palasti. </author> <title> Arrangements of lines with a large number of triangles. </title> <journal> Proc. Amer. Math. Soc., </journal> <volume> 92(4) </volume> <pages> 561-566, </pages> <year> 1984. </year>
Reference-contexts: Furedi and Palasti <ref> [84] </ref> improve this lower bound to roughly n 2 =6 using a slightly different construction, which we describe below. We can use their construction to slightly improve our lower bound for the two-dimensional affine degeneracy problem. <p> Three lines L (ff); L (fi); L (fl) are concurrent if and only if ff + fi + fl 0 (mod 2). It follows that the set of lines fL (2i=n) j 1 i ng has 1 + bn (n - 3)=6c concurrent triples. See Figure 2.4 (a). See <ref> [84] </ref> for further details. Related results are described in [24] and [72].
Reference: [85] <author> A. Gajentaan and M. H. Overmars. </author> <title> n 2 -hard problems in computational geometry. </title> <type> Report RUU-CS-93-15, </type> <institution> Dept. Comput. Sci., Utrecht Univ., </institution> <address> Utrecht, Netherlands, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Our results imply a quadratic lower bound for 3sum; we will present further details in Chapter 5. Given these reductions, one might think that we have just proven that every 4 Some earlier papers, including [73], used the more suggestive but potentially misleading term "n 2 - hard" <ref> [85] </ref> (but see [20]). 5 This observation was the initial inspiration for my "weird moment curve" argument. 28 3sum-hard problem requires (n 2 ) time. Unfortunately, this is not the case. Many of the reductions discussed in [86] require primitives that our models of computation do not allow.
Reference: [86] <author> A. Gajentaan and M. H. Overmars. </author> <title> On a class of O(n 2 ) problems in computational geometry. </title> <journal> Comput. Geom. Theory Appl., </journal> <volume> 5 </volume> <pages> 165-185, </pages> <year> 1995. </year>
Reference-contexts: set of points and lines in the plane, does any point lie on any line? These and similar questions arise as subproblems or special cases of a large number of more complicated geometric problems, including point location, range searching, motion planning, collision detection, ray shooting, visibility, and hidden surface removal <ref> [86, 75] </ref>. In the last several years, computational geometers have developed powerful techniques for solving these problems efficiently, at least in theory, and there is a common belief that the best known algorithms for these problems are optimal or very close to optimal. <p> We quickly note that the minimum measure simplex is not necessarily collapsible; see The planar affine degeneracy problem is an example of what Gajentaan and Over-mars <ref> [86] </ref> call 3 sum-hard problems. 4 Formally, a problem is 3sum-hard if the following problem can be reduced to it in subquadratic time: 3sum: Given a set of real numbers, do any three sum to zero? Thus, a subquadratic algorithm for any 3sum-hard problem would imply a subquadratic algorithm for 3sum, <p> Examples of 3sum-hard problems include several degeneracy detection, separation, hidden surface removal, and motion planning problems in two and three dimensions. Gajentaan and Overmars <ref> [86] </ref> show that the planar affine degeneracy problem is 3sum-hard, by considering a lifting from the reals to the unit cubic. 5 In fact, the restricted problem considered in Section 2.1.1 is equivalent to 3sum since there are simple linear-time reductions in both directions. <p> Unfortunately, this is not the case. Many of the reductions discussed in <ref> [86] </ref> require primitives that our models of computation do not allow. In these cases, one may still be able to achieve quadratic lower bounds by directly applying the techniques in this chapter. <p> Is there a "structure-sensitive" algorithm for detecting affine degeneracies, whose running time depends favorably on the number of collapsible triangles? Such an algorithm might be useful for solving real-world instances of other 3sum-hard problems such as planar motion planning and hidden surface removal <ref> [86] </ref>. 2.5 Out on a Limb At the risk of annoying the reader, let me close this chapter by outlining some more evidence that the three-colinear-points problem "really" requires (n 2 ) time. <p> For example, finding a d-tuple in the zeroset of the polynomial P d i=1 t i can be reduced to the d-dimensional affine degeneracy problem. (See Lemma 2.3.) Several more good examples can be found in Gajentaan and Overmars' collection of 3sum-hard problems <ref> [86] </ref>. Unfortunately, these reductions use primitives disallowed by the models of computation in which our lower bounds hold.
Reference: [87] <author> David Gale. </author> <title> Neighborly and cyclic polytopes. </title> <editor> In V. Klee, editor, Convexity, </editor> <booktitle> volume VII of Proc. Symposia in Pure Mathematics, </booktitle> <pages> pages 225-232. </pages> <publisher> Amer. Math. Soc., </publisher> <year> 1963. </year>
Reference-contexts: Similarly, any algorithm that constructs the convex hull of a set of n points in IR d must take time (n bd=2c ) in the worst case, because there are polytopes with that many facets <ref> [87] </ref>. This approach is clearly worthless if the output size is always small, or if we want to prove output-sensitive lower bounds of the form fi (f (n) + r), where r is the output size. <p> After a ten-year wait, Chazelle [36] improved the running time to O (n bd=2c ) by derandom-izing a randomized incremental algorithm of Clarkson and Shor [50]; see also [136]. Since an n-vertex polytope in IR d can have (n bd=2c ) facets <ref> [87] </ref>, Seidel's algorithm is optimal in even dimensions, and Chazelle's algorithm is optimal in all dimensions, in the worst case. Several faster algorithms are known when the output size is also considered. <p> However, degenerate facets are possible. The following lemma characterizes degenerate convex hull facets on the weird moment curve. The result is quite similar to Gale's evenness condition <ref> [87] </ref>, which describes which vertices of a cyclic polytope form its facets. Lemma 3.1. Let X be a set of real numbers, and let x 0 ; x 1 ; : : : ; x d be elements of X whose sum is zero.
Reference: [88] <author> J. E. Goodman and R. Pollack. </author> <title> Multidimensional sorting. </title> <journal> SIAM J. Comput., </journal> <volume> 12 </volume> <pages> 484-507, </pages> <year> 1983. </year>
Reference-contexts: Two sets of labeled points are said to have the same order type if corresponding simplices have the same orientation. The order type of a set of points can be represented by the face lattice of its dual hyperplane arrangement or by its lambda-matrix <ref> [88] </ref>, both representations requiring space (n d ). One might consider representing order types by canonical sets of points.
Reference: [89] <author> J. E. Goodman and R. Pollack. </author> <title> Allowable sequences and order types in discrete and computational geometry. </title> <editor> In J. Pach, editor, </editor> <booktitle> New Trends in Discrete and Computational Geometry, volume 10 of Algorithms and Combinatorics, </booktitle> <pages> pages 103-134. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Even though all known representations of order type require space (n d ), there is some hope of a smaller representation, and thus, a faster algorithm, since it is known that there are only (n=d) fi (d 2 n) = 2 fi (n log n) order types <ref> [89] </ref>. Prior to the results in this chapter, the information-theoretic lower bound of (n log n) was the only lower bound known for this problem. <p> See Chapter 5 for further discussion of nonuniform algorithms.) There are 2 fi (n 2 ) combinatorially distinct arrangements of n pseudolines in the plane [105, 79]. (Recall from the beginning of this chapter that only n fi (n) of these are stretchable <ref> [89] </ref>.) It follows immediately that determining the order type of a pseudoline arrangement requires (n 2 ) time. <p> It would be more correct, but also unwieldier, to say "A is at least as simple as B". 98 Since there are only a finite number of combinatorial equivalence classes of convex polytopes with v vertices <ref> [89] </ref>, the formula is finite, and therefore defines a semi-algebraic set. It remains only to show that this set is closed.
Reference: [90] <author> J. E. Goodman, R. Pollack, and B. Sturmfels. </author> <title> The intrinsic spread of a configuration in R d . J. </title> <journal> Amer. Math. Soc., </journal> <volume> 3 </volume> <pages> 639-651, </pages> <year> 1990. </year>
Reference-contexts: One might consider representing order types by canonical sets of points. Unfortunately, the full field of algebraic numbers is required to represent every planar order type [96], and even among integer order types, point coordinates must be doubly-exponential in the worst case <ref> [90] </ref>. The fastest known algorithm for determining the order type of a set of points constructs its dual hyperplane arrangement in time and space O (n d ) [68, 69].
Reference: [91] <author> R. L. Graham. </author> <title> An efficient algorithm for determining the convex hull of a finite planar set. </title> <journal> Inform. Process. Lett., </journal> <volume> 1 </volume> <pages> 132-133, </pages> <year> 1972. </year>
Reference-contexts: Over twenty years ago, Graham described an algorithm that constructs the convex hull of n points in the plane in O (n log n) time <ref> [91] </ref>. The same running time was first achieved in three dimensions by Preparata and Hong [123]. Yao [154] proved a lower bound of (n log n) on the complexity of identifying the convex hull vertices, in the quadratic decision tree model.
Reference: [92] <author> Ronald R. Graham, Donald E. Knuth, and Oren Patashnik. </author> <note> Concrete Mathematics: </note>
Reference-contexts: Since an arrangement of N hyperplanes 2 For any integer a 0, the falling factorial power n a is defined as n (n - 1) (n a+ 1) = n!=(n a)! <ref> [92] </ref>. 59 in IR D has at most P D N = O (N D ) cells [62], there are at most O ((2n r=2 ) 2n ) = O ((2n) rn ) possible orderings. <p> Our construction relies on the following lemma, whose (simple) proof we omit. We refer the reader to [98] or <ref> [92] </ref> for relevant background. Lemma 6.7. P n i=1 i k '(i) = fi (n k+2 ) for any nonnegtive integer k. 73 Lemma 6.8.
References-found: 92

