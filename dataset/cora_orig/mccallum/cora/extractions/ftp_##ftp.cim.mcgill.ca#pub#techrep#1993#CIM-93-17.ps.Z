URL: ftp://ftp.cim.mcgill.ca/pub/techrep/1993/CIM-93-17.ps.Z
Refering-URL: ftp://ftp.cim.mcgill.ca/pub/techrep/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Autonomous Exploration: Driven by Uncertainty  
Author: Peter Whaite and Frank P. Ferrie 
Note: Submitted to IEEE Trans. Pattern Analysis and Machine Intelligence,  
Address: Montreal, Quebec, Canada  Address: 3480 University Street, Montreal, Quebec, Canada H3A 2A7  
Affiliation: Vision Group Centre for Intelligent Machines McGill University  Postal  
Pubnum: TR-CIM-93-17  3D  
Email: Email: cim@cim.mcgill.ca  
Phone: Telephone: (514) 398-6319 Telex: 05 268510 FAX: (514) 283-7897  
Date: 1993 (Modified March 1994)  March 1994  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> N. Ahuja and J. Veenstra. </author> <title> Generating octrees from object silhouettes in orthographic views. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 11 </volume> <pages> 137-149, </pages> <month> Feb. </month> <year> 1989. </year>
Reference-contexts: The question of sensor placement appears to have received very little attention in the computer vision literature, especially when operating in an unknown environment. Of this work both Connolly [6], then Ahuja and Veenstra <ref> [1] </ref> considered the problem of the views needed to build an octree representation of a 3D scene. More recently Maver and Bajcsy [17] developed a technique for filling in the range image shadows when sampling a scene with a light stripe range finder.
Reference: 2. <editor> R. Bajcsy and F. Solina. </editor> <title> Three dimensional object recognition revisited. </title> <booktitle> In Proceedings, 1ST International Conference on Computer Vision, </booktitle> <address> London,U.K., </address> <month> June </month> <year> 1987. </year> <booktitle> Computer Society of the IEEE, </booktitle> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The procedure we use to solve this problem is a special case of the more general model described by Kimia and adapted for surfaces [15]. Finally, at the highest level of abstraction, surface regions defined by part boundaries are described by parametric forms (i.e. models) such as superquadrics <ref> [2] </ref>. In addition to serving as a basis for the characterization of uncertainty, these descriptors provide additional cues for maintaining correspondence at the level of parts, for describing general shape properties, and for recognition [10]. 6.1. Closing the Loop.
Reference: 3. <author> V. Caglioti. </author> <title> The optimal next exploration. </title> <booktitle> In PROC. European Conference on Robotics and Intelligent Systems, </booktitle> <year> 1991. </year>
Reference-contexts: Looking: The Gaze Planning Strategy 11 and quadratically, as one moves away from the data. The highest values of 2 D (x; y) therefore occur on the edges of the objects, and as others have shown <ref> [3] </ref>, the best place to probe for new data is on one of the vertices. The strategy that results in the greatest decrease in C n at each step is to successively sample from the vertex where 2 D is highest.
Reference: 4. <author> W. Cheung, F. P. Ferrie, G. Carayannis, and J. B. Edwards. </author> <title> Rockpile surface decomposition: Machine vision in mining. </title> <booktitle> In PROC. Canadian Conference on Industrial Automation, </booktitle> <address> Montreal, Quebec, </address> <month> June 1-3, </month> <year> 1992 1992. </year>
Reference-contexts: From an analysis of the geometric structure of the acquired surfaces, the data are partitioned into patches corresponding to the component rocks. An approximation of the position, orientation, and shape of each component is then determined by fits to superquadric models (Figure 1b) <ref> [4] </ref>. While this strategy appears to work well in the example shown, it is in fact flawed. This can be seen in the example in surface of a noisy hemisphere.
Reference: 5. <author> T. Choi, H. Delingette, M. DeLusie, Y. Hsin, M. Hebert, and K. </author> <title> Ikeuchi. A perception and manipulation system for collecting rock samples. </title> <booktitle> In Proceedings, 4TH Annual Space Operations, Applications, and Research Symposium: SOAR 90, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1990. </year> <note> References 31 </note>
Reference-contexts: This ability is essential for autonomous systems which must operate in unstructured environments where it is difficult (if not impossible) to characterize the environment beforehand. Consider, for example, a mobile robot designed to collect rock samples for planetary exploration <ref> [5] </ref>. In order to grasp and manipulate such samples, information is required about their three-dimensional shape. But given the wide range of shapes that are possible, it is not feasible to represent each and every instance.
Reference: 6. <author> C. Connolly. </author> <title> The determination of next best views. </title> <booktitle> Proc. IEEE Int. Conference of Robotics Automation, </booktitle> <pages> pages 432-435, </pages> <year> 1985. </year>
Reference-contexts: The question of sensor placement appears to have received very little attention in the computer vision literature, especially when operating in an unknown environment. Of this work both Connolly <ref> [6] </ref>, then Ahuja and Veenstra [1] considered the problem of the views needed to build an octree representation of a 3D scene. More recently Maver and Bajcsy [17] developed a technique for filling in the range image shadows when sampling a scene with a light stripe range finder.
Reference: 7. <author> V. Federov. </author> <title> Theory of optimal experiments. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: The ratio of C before and after an additional measurement is seen from (18) to be C n 1 D (x n+1 )= 2 : (21) A similar result can also be found in another field, the theory of optimal experiments as developed by Federov <ref> [7] </ref>, and as expounded upon in the more accessible work of MacKay [16].
Reference: 8. <author> F. P. Ferrie, J. Lagarde, and P. Whaite. Darboux frames, snakes, and super-quadrics: </author> <title> Geometry from the bottom-up. </title> <booktitle> In Proceedings IEEE Workshop on Interpretation of 3D Scenes, </booktitle> <pages> pages 170-176, </pages> <address> Austin, Texas, </address> <month> Nov. 27-29 </month> <year> 1989. </year> <journal> Computer Society of the IEEE, IEEE Computer Society Press. IEEE Trans. </journal> <note> PAMI to appear. </note>
Reference-contexts: Shape descriptions must be computed from more general purpose models that can be adapted according to measurements obtained by sensors. In the context of artificial perception, the latter often takes the form of determining the parameters of some model used to reflect the salient properties of the environment <ref> [8, 10, 19] </ref>. the rock pile using superquadrics. system. From an analysis of the geometric structure of the acquired surfaces, the data are partitioned into patches corresponding to the component rocks. <p> Figure 12 shows a block diagram of the resulting implementation. The left side corresponds to a classical model of bottom-up vision in which sensor data are transformed into various levels of representation through successive stages of processing <ref> [8, 10] </ref>. In our implementation data are acquired through a laser range-finding system mounted on the end-effector of an inverted PUMA 560 robot as shown in Figure 13. The system has a field of view of approximately 1m 3 which can be positioned anywhere in the robot workspace.
Reference: 9. <author> F. P. Ferrie, J. Lagarde, and P. Whaite. Darboux frames, snakes, and super-quadrics: </author> <title> Geometry from the bottom up. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(8) </volume> <pages> 771-784, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: In our laboratory we sample 3D coordinates from surfaces in the scene with a laser range scanner and, after several layers of "bottom-up" processing, infer those superellipsoid models which best explain the measurements <ref> [9] </ref>. Points fs i ; i = 1; : : : ; ng on the surface of a superellipsoid model with parameters m satisfy the implicit equation D (s i ; m) = D i (m) = 0; (24) which is highly non-linear in both s i and m.
Reference: 10. <author> F. P. Ferrie, J. L. Lagarde, and P. Whaite. </author> <title> Recovery of volumetric descriptions from laser rangefinder images. </title> <booktitle> In Computer Vision - ECCV 90, </booktitle> <pages> pages 387-396, </pages> <address> Antibes, France, </address> <month> 23-27 Apr. </month> <year> 1990. </year> <title> INRIA,France, </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Shape descriptions must be computed from more general purpose models that can be adapted according to measurements obtained by sensors. In the context of artificial perception, the latter often takes the form of determining the parameters of some model used to reflect the salient properties of the environment <ref> [8, 10, 19] </ref>. the rock pile using superquadrics. system. From an analysis of the geometric structure of the acquired surfaces, the data are partitioned into patches corresponding to the component rocks. <p> Figure 12 shows a block diagram of the resulting implementation. The left side corresponds to a classical model of bottom-up vision in which sensor data are transformed into various levels of representation through successive stages of processing <ref> [8, 10] </ref>. In our implementation data are acquired through a laser range-finding system mounted on the end-effector of an inverted PUMA 560 robot as shown in Figure 13. The system has a field of view of approximately 1m 3 which can be positioned anywhere in the robot workspace. <p> The perceptual basis of the algorithm is the Hoffman and Richards [12] principle of transversality regularity. Objects in the scene are represented as conjunctions of convex solids. Boundaries between parts of objects thus correspond to concave discontinuities and/or negative local minima in the principle curvatures of the surface <ref> [10] </ref>. These features are made explicit as a by-product of the reconstruction procedure. However, the task of interpolating such features in to part boundaries is non-trivial and the subject of much work in the literature, e.g. the work of Kimia et al. [14]. <p> In addition to serving as a basis for the characterization of uncertainty, these descriptors provide additional cues for maintaining correspondence at the level of parts, for describing general shape properties, and for recognition <ref> [10] </ref>. 6.1. Closing the Loop. Much as in the same way that feedback serves to reduce plant uncertainty in a conventional control system, the autonomous explorer uses feedback to minimize the uncertainty of the parametric models used to describe the scene.
Reference: 11. <author> F. P. Ferrie, S. Mathur, and G. Soucy. </author> <title> Feature extraction for 3-d model building and object recognition. </title> <editor> In A. Jain and P. Flynn, editors, </editor> <title> 3D Object Recognition Systems. </title> <publisher> Elsevier, </publisher> <address> Amster-dam, </address> <year> 1993. </year>
Reference-contexts: To facilitate estimation of these parameters and to provide the necessary structure from which to perform shape analysis, a visual reconstruction procedure is used to turn the discrete sampled data from the rangefinder into a piecewise-smooth (C 2 ) representation of the surfaces in the scene <ref> [11] </ref>. From there data acquired from different vantage points can be fused by determining the correspondence between features in adjacent views. A temporal extension to our reconstruction procedure [20] is used to determine the transformation parameters on a local basis.
Reference: 12. <author> D. Hoffman and W. Richards. </author> <title> Parts of recognition. </title> <journal> Cognition, </journal> <volume> 18 </volume> <pages> 65-96, </pages> <year> 1984. </year>
Reference-contexts: Implementation of an Autonomous Explorer 27 with a 1m 3 field of view mounted on the end-effector of an inverted PUMA 560 robot. parts of an object. The perceptual basis of the algorithm is the Hoffman and Richards <ref> [12] </ref> principle of transversality regularity. Objects in the scene are represented as conjunctions of convex solids. Boundaries between parts of objects thus correspond to concave discontinuities and/or negative local minima in the principle curvatures of the surface [10]. These features are made explicit as a by-product of the reconstruction procedure.
Reference: 13. <author> R. A. Horn and C. R. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference-contexts: We do so by finding its eigenvalues with the aid of two results which follow from the basic definition of an eigenvalue (e.g. <ref> [13, definition 1.1.2] </ref>). i) If is an eigenvalue of A then 1 + is an eigenvalue of I + A. ii) If x is an m-dimensional vector and A an m fi m matrix then there is only one eigenvalue of xx T A and its value is x T Ax.
Reference: 14. <author> B. B. Kimia, A. R. Tannenbaum, and S. W. Zucker. </author> <title> Shapes, shocks, and deformations, I: The components of shape and the reaction-diffusion space. </title> <type> Technical Report LEMS 105, </type> <institution> LEMS, Brown University, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: These features are made explicit as a by-product of the reconstruction procedure. However, the task of interpolating such features in to part boundaries is non-trivial and the subject of much work in the literature, e.g. the work of Kimia et al. <ref> [14] </ref>. The procedure we use to solve this problem is a special case of the more general model described by Kimia and adapted for surfaces [15]. Finally, at the highest level of abstraction, surface regions defined by part boundaries are described by parametric forms (i.e. models) such as superquadrics [2].
Reference: 15. <author> A. Lejeune and F. P. Ferrie. </author> <title> Partitioning range images using curvature and scale. </title> <booktitle> In PROC. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <address> New York City, New York, </address> <month> June 15-17 </month> <year> 1993. </year> <note> to appear. </note>
Reference-contexts: The procedure we use to solve this problem is a special case of the more general model described by Kimia and adapted for surfaces <ref> [15] </ref>. Finally, at the highest level of abstraction, surface regions defined by part boundaries are described by parametric forms (i.e. models) such as superquadrics [2].
Reference: 16. <author> D. J. MacKay. </author> <title> Information-based objective functions for active data selection. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 590-604, </pages> <year> 1992. </year>
Reference-contexts: an additional measurement is seen from (18) to be C n 1 D (x n+1 )= 2 : (21) A similar result can also be found in another field, the theory of optimal experiments as developed by Federov [7], and as expounded upon in the more accessible work of MacKay <ref> [16] </ref>. It shows us i) that adding any data will always result in a reduction of C n , and ii) that C n+1 can be minimized by taking a measurement at the location when 2 D is largest.
Reference: 17. <author> J. Maver and R. </author> <title> Bajcsy. Occlusions as a guide for planning the next view. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(5) </volume> <pages> 417-433, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Of this work both Connolly [6], then Ahuja and Veenstra [1] considered the problem of the views needed to build an octree representation of a 3D scene. More recently Maver and Bajcsy <ref> [17] </ref> developed a technique for filling in the range image shadows when sampling a scene with a light stripe range finder. Unlike these geometrically based methodologies, ours is unique in that it generalizes the characterization and 2.
Reference: 18. <author> A. M. Mood and F. A. Graybill. </author> <title> Introduction to the Theory of Statistics. </title> <publisher> McGraw-Hill Book Company Inc., </publisher> <address> New York, N.Y., </address> <year> 1963. </year>
Reference-contexts: Because C 1 is symmetric and positive definite this surface is ellipsoidal. Now we do not know the value of Q (m T ) but it is a well known result of statistical theory that the quantity is randomly sampled from a chi-square distribution with p degrees of freedom <ref> [18] </ref>. For some confidence level fl we can find from that distribution a number 2 fl for which there is a probability of fl that Q (m T ) &lt; 2 fl . <p> A well know result from statistics shows that there is a simple mapping between the model and prediction covariances <ref> [18] </ref>. If a random vector ^x is sampled from a p-variate normal distribution with zero mean and covariances C, and if A is a linear mapping ^y = A^x to the q-dimensional vector ^y, then ^y will be sampled from a q-variate normal distribution with covariances ACA T .
Reference: 19. <author> A. Pentland. </author> <title> Recognition by parts. </title> <booktitle> In Proceedings, 1ST International Conference on Computer Vision, </booktitle> <pages> pages 612-620, </pages> <address> London,UK, </address> <month> June </month> <year> 1987. </year> <booktitle> Computer Society of the IEEE, </booktitle> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Shape descriptions must be computed from more general purpose models that can be adapted according to measurements obtained by sensors. In the context of artificial perception, the latter often takes the form of determining the parameters of some model used to reflect the salient properties of the environment <ref> [8, 10, 19] </ref>. the rock pile using superquadrics. system. From an analysis of the geometric structure of the acquired surfaces, the data are partitioned into patches corresponding to the component rocks.
Reference: 20. <author> G. Soucy. </author> <title> View correspondence using curvature and motion consistency. </title> <type> Master's thesis, </type> <institution> Dept. of E.E., McGill Univ., </institution> <year> 1992. </year>
Reference-contexts: From our point of view another important requirement is the need to convert scanner centered 3D coordinates into the scene coordinates used to fit models <ref> [20] </ref> (x6). Because the position of the sensor is uncertain we can only do this by the registration of overlapping data sets, and smaller movements let us to keep the area scanned small whilst still maintaining a useful overlap. <p> From there data acquired from different vantage points can be fused by determining the correspondence between features in adjacent views. A temporal extension to our reconstruction procedure <ref> [20] </ref> is used to determine the transformation parameters on a local basis. An important feature of this algorithm is its ability to deal with non-rigid motions. Such might be the case, for example, when dealing with objects that can change configuration in between changes of sensor position.
Reference: 21. <author> P. Whaite and F. P. Ferrie. </author> <title> From uncertainty to visual exploration. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(10) </volume> <pages> 1038-1049, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: While this strategy appears to work well in the example shown, it is in fact flawed. This can be seen in the example in surface of a noisy hemisphere. Each of the resulting models (Figure 2b) describes the data to within the same error of fit <ref> [21, 22] </ref>, yet the models look quite different as they move away from the data. The problem is that the data acquired do not 2 Autonomous Exploration: Driven by Uncertainty sufficiently constrain the model. <p> In particular we are able to show that the exploration strategy we proposed previously for intuitive reasons <ref> [21, 22] </ref> has a sound theoretical basis. Non-linear analytic solutions are usually hard to obtain and invariably one must resort to numerical iterative techniques to get results. Once this is done however the system can be linearized around the solution, and the linear analysis applied. <p> Specific details, e.g. the form of (24), are given in <ref> [21] </ref>. In fact D has the metric property that it is the radial distance of s i from the surface. <p> By following the linear analysis in x2 the prediction variance at a point on the surface of a superellipsoid 12 Autonomous Exploration: Driven by Uncertainty model is 2 @m ! T @m ! We have previously derived this quantity by other means <ref> [21] </ref> where we called it the prediction error ffi fl = q fl 2 We know from the linear theory that the best place to take new data is where 2 D is greatest, but it is not clear to what extent this is true in the non-linear case.
Reference: 22. <author> P. Whaite and F. P. Ferrie. </author> <title> Uncertain views. </title> <booktitle> In PROC. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 3-9, </pages> <address> Champaign, Illinois, </address> <month> June 15-18 </month> <year> 1992. </year>
Reference-contexts: While this strategy appears to work well in the example shown, it is in fact flawed. This can be seen in the example in surface of a noisy hemisphere. Each of the resulting models (Figure 2b) describes the data to within the same error of fit <ref> [21, 22] </ref>, yet the models look quite different as they move away from the data. The problem is that the data acquired do not 2 Autonomous Exploration: Driven by Uncertainty sufficiently constrain the model. <p> We find a theoretical solution to this problem which is identical to a proposition we have made in previous work <ref> [22] </ref> | that the best sensor locations are those where our ability to predict is worst. This leads to a gaze-planning strategy, described in x4, that uses model uncertainty as a basis for selecting viewpoints. <p> In particular we are able to show that the exploration strategy we proposed previously for intuitive reasons <ref> [21, 22] </ref> has a sound theoretical basis. Non-linear analytic solutions are usually hard to obtain and invariably one must resort to numerical iterative techniques to get results. Once this is done however the system can be linearized around the solution, and the linear analysis applied. <p> For superellipsoid models the non-linearities have so far proven intractable, so we have used Monte Carlo simulations to test the validity of the linear theory <ref> [22] </ref>. In these experiments we simulated data acquisition by a range scanner in orbit on a view sphere about a superellipsoid model. <p> For a more detailed description of the experiment see <ref> [22] </ref>. a function of prediction error for non-linear superellipsoid models. 14 Autonomous Exploration: Driven by Uncertainty A problem with the approach is that it is difficult to determine the region of sensor locations over which the non-linear relationships of the model can be approximated sufficiently well by the linearized form (25).
Reference: 23. <author> P. Whaite and F. P. Ferrie. </author> <title> Active exploration: Knowing when we're wrong. </title> <booktitle> In PROC. Fourth International Conference on Computer Vision, </booktitle> <pages> pages 41-48, </pages> <address> Berlin, Germany, </address> <month> May 11-14 </month> <year> 1993. </year> <booktitle> Computer Society of the IEEE, </booktitle> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: To this end we have devised a sequential estimator of sensor noise which operates within the servo loop. We have shown that this estimator reliably detects misfit even when the sensor noise is not that of the theory, and when the noise level varies during the course of exploration <ref> [23] </ref>. 6. Implementation of an Autonomous Explorer The concepts of the previous sections are now used to implement an autonomous explorer capable of building an articulated volumetric description of its environment through a sequence of exploratory probes. Figure 12 shows a block diagram of the resulting implementation.
References-found: 23

