URL: ftp://ftp.eecs.umich.edu/people/aprakash/softeng/cas-pup.ibmsj.ps.Z
Refering-URL: http://www.eecs.umich.edu/~aprakash/csrg_pub.html
Root-URL: http://www.cs.umich.edu
Title: Investigating Reverse Engineering Technologies: The CAS Program Understanding Project  
Author: E. Buss R. De Mori M. Gentleman J. Henshaw H. Johnson K. Kontogiannis E. Merlo H. Muller J. Mylopoulos S. Paul A. Prakash M. Stanley S. Tilley J. Troster K. Wong 
Keyword: Legacy software systems, program understanding, software reuse, reverse engineering, software metrics, software quality.  
Note: Copyright c 1994 IBM Corporation. To appear in IBM Systems Journal, 33(3), 1994.  
Abstract: Corporations face mounting maintenance and re-engineering costs for large legacy systems. Evolving over several years, these systems embody substantial corporate knowledge, including requirements, design decisions, and business rules. Such knowledge is difficult to recover after many years of operation, evolution, and personnel change. To address this problem, software engineers are spending an ever-growing amount of effort on program understanding and reverse engineering technologies. This article describes the scope and results of an on-going research project on program understanding undertaken by the IBM Software Solutions Toronto Laboratory Centre for Advanced Studies (CAS). The project involves, in addition to a team from CAS, five research groups working cooperatively on complementary reverse engineering approaches. All groups are using the source code of SQL/DS (a multi-million line relational database system) as the reference legacy system. The article also discusses the approach adopted to integrate the various toolsets under a single reverse engineering environment. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. A. Standish. </author> <title> An essay on software reuse. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(5):494-497, </volume> <month> September </month> <year> 1984. </year>
Reference-contexts: Since these systems cannot easily be replaced without reliving their entire history, managing long-term software evolution is critical. It has been estimated that fifty to ninety percent of evolution work is devoted to program understanding <ref> [1] </ref>. Hence, easing the understanding process can have significant economic savings. One of the most promising approaches to the problem of program understanding for software evolution is reverse engineering. Using reverse engineering technologies has been proposed to help refurbish and maintain software systems.
Reference: [2] <author> P. Selfridge, R. Waters, and E. Chikofsky. </author> <title> Challenges to the field of reverse engineering | a position paper. </title> <booktitle> In WCRE '93: Proceedings of the 1993 Working Conference on Reverse Engineering, </booktitle> <address> (Bal-timore, Maryland; May 21-23, </address> <year> 1993), </year> <pages> pages 144-150. </pages> <publisher> IEEE Computer Society Press (Order Number 3780-02), </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: There is little work in program understanding that involves large, real-world systems with multiple teams of researchers experimenting on a common target <ref> [2] </ref>. Networking opportunities ease the exchange of research ideas. Moreover, colleagues can explore related solutions in different disciplines. This strategy introduces new techniques to help tackle the problems in industry and, as well, strengthens academic systems to deal with complex, industrial software systems.
Reference: [3] <author> R. Brooks. </author> <title> Towards a theory of the comprehension of computer programs. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 18 </volume> <pages> 543-554, </pages> <year> 1983. </year>
Reference-contexts: For example, one might extract syntactic knowledge from the source code and rely on programming knowledge to form semantic abstractions. Brooks's work on the theory of domain bridging <ref> [3] </ref> describes the programming process as one of constructing mappings from a problem domain to an implementation domain, possibly through multiple levels. Program understanding then involves reconstructing part or all of these mappings.
Reference: [4] <author> R. Arnold. </author> <title> Software Reengineering. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: Although there are many forms of reverse engineering, the common goal is to extract information from existing software systems. This knowledge can then be used to improve subsequent development, ease maintenance and re-engineering, and aid project management <ref> [4] </ref>. The reverse engineering process identifies the system's current components, discovers their dependencies, and generates abstractions to manage complexity [5]. It involves two distinct phases [6]: (1) the identification of the system's current components and their dependencies; and (2) the discovery of system abstractions and design information.
Reference: [5] <author> E. J. Chikofsky and J. H. </author> <title> Cross II. Reverse engineering and design recovery: A taxonomy. </title> <journal> IEEE Software, </journal> <volume> 7(1) </volume> <pages> 13-17, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: This knowledge can then be used to improve subsequent development, ease maintenance and re-engineering, and aid project management [4]. The reverse engineering process identifies the system's current components, discovers their dependencies, and generates abstractions to manage complexity <ref> [5] </ref>. It involves two distinct phases [6]: (1) the identification of the system's current components and their dependencies; and (2) the discovery of system abstractions and design information. During this process, the source code is not altered, although additional information about the system is generated.
Reference: [6] <author> R. Arnold. </author> <title> Tutorial on software reengineering. </title> <booktitle> In CSM'90: Proceedings of the 1990 Conference on Software Maintenance, </booktitle> <address> (San Diego, California; November 26-29, 1990). </address> <publisher> IEEE Computer Society Press (Order Number 2091), </publisher> <month> November </month> <year> 1990. </year>
Reference-contexts: This knowledge can then be used to improve subsequent development, ease maintenance and re-engineering, and aid project management [4]. The reverse engineering process identifies the system's current components, discovers their dependencies, and generates abstractions to manage complexity [5]. It involves two distinct phases <ref> [6] </ref>: (1) the identification of the system's current components and their dependencies; and (2) the discovery of system abstractions and design information. During this process, the source code is not altered, although additional information about the system is generated.
Reference: [7] <author> A. O'Hare and E. Troan. RE-Analyzer: </author> <title> From source code to structured analysis. </title> <journal> IBM Systems Journal, </journal> <volume> 33(1), </volume> <year> 1994. </year>
Reference-contexts: In contrast, the process of re-engineering typically consists of a reverse engineering phase, followed by a forward engineering or re-implementation phase that alters the subject system's source code. Definitions of related concepts may be found in <ref> [7] </ref>. The discovery phase is a highly interactive and cognitive activity. The analyst may build up hierarchical subsystem components that embody software engineering principles such as low coupling and high cohesion [8].
Reference: [8] <author> G. Myers. </author> <title> Reliable Software Through Composite Design. </title> <address> Petrocelli/Charter, </address> <year> 1975. </year>
Reference-contexts: Definitions of related concepts may be found in [7]. The discovery phase is a highly interactive and cognitive activity. The analyst may build up hierarchical subsystem components that embody software engineering principles such as low coupling and high cohesion <ref> [8] </ref>. Discovery may also include the reconstruction of design and requirements specifications (often referred to as the "domain model") and the correlation of this model to the code. 2.3 Program understanding research Many research groups have focused their efforts on the development of tools and techniques for program understanding.
Reference: [9] <author> M. R. Olsem and C. Sittenauer. </author> <title> Reengineering technology report (Volume I). </title> <type> Technical report, </type> <institution> Software Technology Support Center, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: At a higher semantic level, it may focus on behavioral features such as memory usage, uninitialized variables, value ranges, and algorithmic plans. Each of these points of investigation must be addressed differently. 4 There are many commercial reverse engineering and re-engineering tools available; catalogs such as <ref> [9, 10] </ref> describe several hundred such packages. Most commercial systems focus on source-code analysis and simple code restructuring, and use the most common form of reverse engineering: information abstraction via program analysis.
Reference: [10] <author> N. Zvegintzov, </author> <title> editor. Software Management Technology Reference Guide. Software Management News Inc., </title> <address> 4.2 edition, </address> <year> 1994. </year>
Reference-contexts: At a higher semantic level, it may focus on behavioral features such as memory usage, uninitialized variables, value ranges, and algorithmic plans. Each of these points of investigation must be addressed differently. 4 There are many commercial reverse engineering and re-engineering tools available; catalogs such as <ref> [9, 10] </ref> describe several hundred such packages. Most commercial systems focus on source-code analysis and simple code restructuring, and use the most common form of reverse engineering: information abstraction via program analysis.
Reference: [11] <author> G. Arango, I. Baxter, P. Freeman, and C. Pidgeon. </author> <title> TMM: Software maintenance by transformation. </title> <journal> IEEE Software, </journal> <volume> 3(3) </volume> <pages> 27-39, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Most commercial systems focus on source-code analysis and simple code restructuring, and use the most common form of reverse engineering: information abstraction via program analysis. Research in reverse engineering consists of many diverse approaches, including: formal transformations <ref> [11] </ref>, meaning-preserving restructuring [12], plan recognition [13], function abstraction [14], information abstraction [15], maverick identification [16], graph queries [17], and reuse-oriented methods [18]. The CAS program understanding project is guided, in part, by the need to produce results directly applicable to the SQL/DS product team.
Reference: [12] <author> W. G. Griswold. </author> <title> Program Restructuring as an Aid to Software Maintenance. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1991. </year>
Reference-contexts: Most commercial systems focus on source-code analysis and simple code restructuring, and use the most common form of reverse engineering: information abstraction via program analysis. Research in reverse engineering consists of many diverse approaches, including: formal transformations [11], meaning-preserving restructuring <ref> [12] </ref>, plan recognition [13], function abstraction [14], information abstraction [15], maverick identification [16], graph queries [17], and reuse-oriented methods [18]. The CAS program understanding project is guided, in part, by the need to produce results directly applicable to the SQL/DS product team.
Reference: [13] <author> C. Rich and L. M. Wills. </author> <title> Recognizing a program's design: A graph-parsing approach. </title> <journal> IEEE Software, </journal> <volume> 7(1) </volume> <pages> 82-89, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Most commercial systems focus on source-code analysis and simple code restructuring, and use the most common form of reverse engineering: information abstraction via program analysis. Research in reverse engineering consists of many diverse approaches, including: formal transformations [11], meaning-preserving restructuring [12], plan recognition <ref> [13] </ref>, function abstraction [14], information abstraction [15], maverick identification [16], graph queries [17], and reuse-oriented methods [18]. The CAS program understanding project is guided, in part, by the need to produce results directly applicable to the SQL/DS product team. Hence, the work of most research groups is oriented towards analysis.
Reference: [14] <author> P. A. Hausler, M. G. Pleszkoch, R. C. Linger, and A. R. Hevner. </author> <title> Using function abstraction to understand program behavior. </title> <journal> IEEE Software, </journal> <volume> 7(1) </volume> <pages> 55-63, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Most commercial systems focus on source-code analysis and simple code restructuring, and use the most common form of reverse engineering: information abstraction via program analysis. Research in reverse engineering consists of many diverse approaches, including: formal transformations [11], meaning-preserving restructuring [12], plan recognition [13], function abstraction <ref> [14] </ref>, information abstraction [15], maverick identification [16], graph queries [17], and reuse-oriented methods [18]. The CAS program understanding project is guided, in part, by the need to produce results directly applicable to the SQL/DS product team. Hence, the work of most research groups is oriented towards analysis.
Reference: [15] <author> J. E. Grass. </author> <title> Object-oriented design archaeology with CIA++. </title> <journal> Computing Systems, </journal> <volume> 5(1) </volume> <pages> 5-67, </pages> <month> Winter </month> <year> 1992. </year>
Reference-contexts: Most commercial systems focus on source-code analysis and simple code restructuring, and use the most common form of reverse engineering: information abstraction via program analysis. Research in reverse engineering consists of many diverse approaches, including: formal transformations [11], meaning-preserving restructuring [12], plan recognition [13], function abstraction [14], information abstraction <ref> [15] </ref>, maverick identification [16], graph queries [17], and reuse-oriented methods [18]. The CAS program understanding project is guided, in part, by the need to produce results directly applicable to the SQL/DS product team. Hence, the work of most research groups is oriented towards analysis.
Reference: [16] <author> R. Schwanke, R. Altucher, and M. Platoff. </author> <title> Discovering, visualizing, and controlling software structure. </title> <booktitle> ACM SIGSOFT Software Engineering Notes, </booktitle> <volume> 14(3) </volume> <pages> 147-150, </pages> <month> May </month> <year> 1989. </year> <booktitle> Proceedings of the Fifth International Workshop on Software Specification and Design. </booktitle>
Reference-contexts: Research in reverse engineering consists of many diverse approaches, including: formal transformations [11], meaning-preserving restructuring [12], plan recognition [13], function abstraction [14], information abstraction [15], maverick identification <ref> [16] </ref>, graph queries [17], and reuse-oriented methods [18]. The CAS program understanding project is guided, in part, by the need to produce results directly applicable to the SQL/DS product team. Hence, the work of most research groups is oriented towards analysis. However, no single analysis approach is sufficient by itself.
Reference: [17] <author> M. Consens, A. Mendelzon, and A. Ryman. </author> <title> Visualizing and querying software structures. </title> <booktitle> In ICSE'14: Proceedings of the 14th International Conference on Software Engineering, </booktitle> <address> (Melbourne, Australia; May 11-15, </address> <year> 1992), </year> <pages> pages 138-156, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Research in reverse engineering consists of many diverse approaches, including: formal transformations [11], meaning-preserving restructuring [12], plan recognition [13], function abstraction [14], information abstraction [15], maverick identification [16], graph queries <ref> [17] </ref>, and reuse-oriented methods [18]. The CAS program understanding project is guided, in part, by the need to produce results directly applicable to the SQL/DS product team. Hence, the work of most research groups is oriented towards analysis. However, no single analysis approach is sufficient by itself.
Reference: [18] <author> T. J. Biggerstaff, B. G. Mitbander, and D. Webster. </author> <title> The concept assignment problem in program understanding. </title> <booktitle> In WCRE '93: Proceedings of the 1993 Working Conference on Reverse Engineering, </booktitle> <address> (Baltimore, Maryland; May 21-23, </address> <year> 1993), </year> <pages> pages 27-43. </pages> <publisher> IEEE Computer Society Press (Order Number 3780-02), </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: Research in reverse engineering consists of many diverse approaches, including: formal transformations [11], meaning-preserving restructuring [12], plan recognition [13], function abstraction [14], information abstraction [15], maverick identification [16], graph queries [17], and reuse-oriented methods <ref> [18] </ref>. The CAS program understanding project is guided, in part, by the need to produce results directly applicable to the SQL/DS product team. Hence, the work of most research groups is oriented towards analysis. However, no single analysis approach is sufficient by itself.
Reference: [19] <author> E. Buss and J. Henshaw. </author> <title> A software reverse engineering experience. </title> <booktitle> In Proceedings of CASCON '91, </booktitle> <address> (Toronto, Ontario; October 28-30, </address> <year> 1991), </year> <pages> pages 55-73. </pages> <institution> IBM Canada Ltd., </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: The following sections describe the program understanding project's main research results on defect filtering, structural redocumentation, and pattern matching. 3 Defect filtering The IBM team, led by Buss and Henshaw, perform defect filtering <ref> [19] </ref> using the commercial Software Refinery product (REFINE) [20] to parse the source code of SQL/DS into a form suitable for analysis. This work applies the experience of domain experts to create REFINE "rules" to find certain families of defects in the subject software.
Reference: [20] <author> S. Burson, G. B. Kotik, and L. Z. Markosian. </author> <title> A program transformation approach to automating software re-engineering. </title> <booktitle> In COMPSAC '90: Proceedings of the 14th Annual International Computer Software and Applications Conference, </booktitle> <address> (Chicago, Illinois; October, </address> <year> 1990), </year> <pages> pages 314-322, </pages> <year> 1990. </year>
Reference-contexts: The following sections describe the program understanding project's main research results on defect filtering, structural redocumentation, and pattern matching. 3 Defect filtering The IBM team, led by Buss and Henshaw, perform defect filtering [19] using the commercial Software Refinery product (REFINE) <ref> [20] </ref> to parse the source code of SQL/DS into a form suitable for analysis. This work applies the experience of domain experts to create REFINE "rules" to find certain families of defects in the subject software.
Reference: [21] <author> J. Troster. </author> <title> Assessing design-quality metrics on legacy software. </title> <booktitle> In Proceedings of CASCON '92, </booktitle> <address> (Toronto, Ontario; November 9-11, </address> <year> 1992), </year> <pages> pages 113-131, </pages> <month> November, </month> <year> 1992. </year>
Reference-contexts: Their initial work resulted in several prototype toolkits, each of which focuses on detecting specific errors in the reference system. Troster performed a design-quality metrics analysis (D-QMA) study of SQL/DS <ref> [21] </ref>. These measurements guided the creation of a more flexible defect filtering approach, in which the reverse engineering toolkit automatically applies defect filters against the SQL/DS source code. Filtering for quality (FQ) proved be to a fruitful approach to improving the quality of the reference system [22].
Reference: [22] <author> J. Troster, J. Henshaw, and E. Buss. </author> <title> Filtering for quality. </title> <booktitle> In the Proceedings of CASCON '93, </booktitle> <address> (Toronto, Ontario; October 25-28, </address> <year> 1993), </year> <pages> pages 429-449, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: These measurements guided the creation of a more flexible defect filtering approach, in which the reverse engineering toolkit automatically applies defect filters against the SQL/DS source code. Filtering for quality (FQ) proved be to a fruitful approach to improving the quality of the reference system <ref> [22] </ref>.
Reference: [23] <author> E. Buss and J. Henshaw. </author> <title> Experiences in program understanding. </title> <booktitle> In CASCON'92: Proceedings of the 1992 CAS Conference, </booktitle> <address> (Toronto, Ontario; November 9-12, </address> <year> 1992), </year> <pages> pages 157-189. </pages> <institution> IBM Canada Ltd., </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: They cannot be easily shared. The prototypes for SQL/DS where specifically built to demonstrate the capability for analysis in all of these domains. Some of the prototypes are documented in <ref> [23] </ref>. The results from these prototype toolkits were encouraging. The experiments demonstrated the feasibility of defect detection in legacy software systems.
Reference: [24] <author> D. N. Card and R. L. Glass. </author> <title> Measuring Software Design Quality. </title> <publisher> Prentice-Hall, </publisher> <year> 1990. </year>
Reference-contexts: Judicious use of software quality metrics is one way of obtaining insight into the development process to improve it. To confirm the applicability of such metrics to IBM products, Troster initiated the design-quality metrics analysis (D-QMA) project. The purpose of assessing design-quality metrics <ref> [24] </ref> is to examine the design process by examining the end product (source code), to predict a product's quality, and to improve the design process by either continuous increments or quantum leaps.
Reference: [25] <author> D. N. Card. </author> <title> Designing software for producibility. </title> <journal> Journal of Systems and Software, </journal> <volume> 17(3) </volume> <pages> 219-225, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Intra-module design metrics include measures of control flow, data flow, and logic within a module. These "clear-box" measures require knowledge of the inner working of the module. Both the inter-module and intra-module versions of structural complexity, data complexity, and system complexity <ref> [25] </ref> were measured. Other module-level measurements are shown in Figure 1. ) Insert sidebar "The Conformance Hierarchy" here. ( The experiment applied the reverse engineering toolkit developed by Buss and Henshaw (described in Section 3.1) to extract the metrics from the reference system.
Reference: [26] <author> H. L. Ossher. </author> <title> A mechanism for specifying the structure of large, layered systems. </title> <editor> In B. D. Shriver and P. Wegner, editors, </editor> <booktitle> Research Directions in Object-Oriented Programming, </booktitle> <pages> pages 219-252. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: These artifacts include software components such as procedures, modules, and interfaces; dependencies among components such as client-supplier, inheritance, and control-flow; and attributes such as component type, interface size, and interconnection strength. The structure of a system is the organization and interaction of these artifacts <ref> [26] </ref>. One class of techniques of reconstructing structural models is reverse engineering. Using reverse engineering approaches to reconstruct the architecture aspects of software can be termed structural redocumentation. The University of Victoria's work is centered around Rigi [27]: an environment for understanding evolving software systems.
Reference: [27] <author> H. A. Muller. </author> <title> Rigi A Model for Software System Construction, Integration, and Evolution based on Module Interface Specifications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: One class of techniques of reconstructing structural models is reverse engineering. Using reverse engineering approaches to reconstruct the architecture aspects of software can be termed structural redocumentation. The University of Victoria's work is centered around Rigi <ref> [27] </ref>: an environment for understanding evolving software systems. Output from this environment can also serve as input to conceptual modelling, design recovery, and project management processes.
Reference: [28] <author> M. Shaw. </author> <title> Larger scale systems require higher-level abstractions. </title> <booktitle> ACM SIGSOFT Software Engineering Notes, </booktitle> <volume> 14(3) </volume> <pages> 143-146, </pages> <month> May </month> <year> 1989. </year> <booktitle> Proceedings of the Fifth International Workshop on Software Specification and Design. </booktitle>
Reference-contexts: To gain useful knowledge, one must effectively summarize and abstract the information. In a sense, a key to program understanding is deciding what information is material and what is immaterial: knowing what to look for|and what to ignore <ref> [28] </ref>. 4.2 Redocumentation strategy There are tradeoffs in program understanding environments between what can be automated and what should (or must) be left to humans. Structural redocumentation in Rigi is initially automatic and involves parsing the source code of the subject system and storing the extracted artifacts in the repository.
Reference: [29] <author> H. A. Muller, M. A. Orgun, S. R. Tilley, and J. S. Uhl. </author> <title> A reverse engineering approach to subsystem structure identification. </title> <journal> Journal of Software Maintenance: Research and Practice, </journal> <volume> 5(4) </volume> <pages> 181-204, </pages> <month> December </month> <year> 1993. </year> <month> 28 </month>
Reference-contexts: This partnership is synergistic as the analyst also learns and discovers interesting relationships by interactively exploring software systems using Rigi. Subsystem composition is a recursive process whereby building blocks such as data types, procedures, and subsystems are grouped into composite subsystems. This builds multiple, layered hierarchies for higher-level abstractions <ref> [29] </ref>. The criteria for composition depend on the purpose, audience, and domain. For program understanding purposes, the process is guided by dividing the resource-flow graph using established modularity principles such as low coupling and strong cohesion. <p> Exact interfaces and modularity/encapsulation quality measures can be used to evaluate the generated software hierarchies. Subsystem composition is supported by a program representation known as the (k; 2)-partite graph <ref> [29] </ref>. These graphs are layered or stratified into strict levels so that arcs do not skip levels. The levels represent the composition of subsystems.
Reference: [30] <author> J. K. Ousterhout. </author> <title> An Introduction to Tcl and Tk. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year> <note> To be published. </note>
Reference-contexts: To make the Rigi system programmable and extensible, the user interface and editor engine were decoupled to make room for an intermediate scripting layer based on the embeddable Tcl and Tk libraries <ref> [30] </ref>. This layer allows each event of importance to the user (for example, key stroke, mouse motion, button click, menu selection) to be tied to a scripted, user-defined command. Many previously tedious and repetitive activities can now be automated.
Reference: [31] <author> S. R. Tilley and H. A. Muller. </author> <title> Using virtual subsystems in project management. In CASE '93: </title> <booktitle> The Sixth International Conference on Computer-Aided Software Engineering, </booktitle> <institution> (Institute of Systems Science, National University of Singapore, </institution> <address> Singapore; July 19-23, </address> <year> 1993), </year> <pages> pages 144-153, </pages> <address> July 1993. </address> <publisher> IEEE Computer Society Press (Order Number 3480-02). </publisher>
Reference-contexts: Moreover, this layer allows an analyst to complement the built-in operations with external, possibly application-specific, algorithms for graph layout, complexity measures, pattern matching, slicing, and clustering. For example, the Rigi system has been applied to various selected domains: project management <ref> [31] </ref>, personalized hypertext [32], and redocumenting legacy software systems. 13 4.5 Redocumenting SQL/DS The analysis of SQL/DS using Rigi has shown that the subsystem composition method and graph visualizing editor scale up to the multi-million lines of code range.
Reference: [32] <author> S. R. Tilley, M. J. Whitney, H. A. Muller, and M.-A. D. Storey. </author> <title> Personalized information structures. </title> <booktitle> In SIGDOC '93: The 11th Annual International Conference on Systems Documentation, </booktitle> <address> (Waterloo, Ontario; October 5-8, </address> <year> 1993), </year> <pages> pages 325-337, </pages> <month> October </month> <year> 1993. </year> <title> ACM Order Number 6139330. </title>
Reference-contexts: Moreover, this layer allows an analyst to complement the built-in operations with external, possibly application-specific, algorithms for graph layout, complexity measures, pattern matching, slicing, and clustering. For example, the Rigi system has been applied to various selected domains: project management [31], personalized hypertext <ref> [32] </ref>, and redocumenting legacy software systems. 13 4.5 Redocumenting SQL/DS The analysis of SQL/DS using Rigi has shown that the subsystem composition method and graph visualizing editor scale up to the multi-million lines of code range.
Reference: [33] <author> J. H. Johnson. </author> <title> Identifying redundancy in source code using fingerprints. </title> <booktitle> In Proceedings of CAS-CON '92, </booktitle> <address> (Toronto, Ontario; November 9-11, </address> <year> 1992), </year> <pages> pages 171-183, </pages> <month> November, </month> <year> 1992. </year>
Reference-contexts: In particular, large source codes have lots of internal structure as a result of their evolution. The NRC research focuses on techniques that consider the source code in raw or preprocessed textual forms, dealing with more of the incidental implementation artifacts than other methods. The work by Johnson <ref> [33] </ref> at NRC concerns the identification of exact repetitions of text in huge source codes. One goal is to relax the constraint of exact matches to approximate matches, while preserving the ability to handle huge source texts.
Reference: [34] <author> R. M. Karp and M. O. Rabin. </author> <title> Efficient randomized pattern-matching algorithms. </title> <journal> IBM J. Res. Develop., </journal> <volume> 31(2) </volume> <pages> 249-260, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: A file of substring fingerprints and locations provides the information needed to extract source-code redundancies. There are several issues to be addressed: discovering efficient algorithms for computing fingerprints, determining the appropriate set of substrings, and devising postprocessing techniques to make the generated fingerprint file more useful. Karp and Rabin <ref> [34] </ref> have proposed an algorithm based on the properties of residue arithmetic by which fingerprints can be incrementally computed during a single scan. A modified version of this algorithm is used.
Reference: [35] <author> Y. Chen, M. Nishimoto, and C. Ramamoorthy. </author> <title> The C Information Abstraction System. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(3) </volume> <pages> 325-334, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Based on regular expressions, these tools do not exploit the rich syntactic structure of the programming language. Source code contains numerous syntactic, structural, and spatial relationships that are not fully captured by the entity-relation-attribute model of a relational database either. For example, systems such as CIA <ref> [35] </ref> and PUNS [36] only handle simple statistical and cross-reference queries. Graph-based models represent source code in a graph where nodes are software components (such as procedures, data types, and modules), and arcs capture dependencies (such as resource flows).
Reference: [36] <author> L. Cleveland. PUNS: </author> <title> A program understanding support environment. </title> <type> Technical Report RC 14043, </type> <institution> IBM T.J. Watson Research Center, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Based on regular expressions, these tools do not exploit the rich syntactic structure of the programming language. Source code contains numerous syntactic, structural, and spatial relationships that are not fully captured by the entity-relation-attribute model of a relational database either. For example, systems such as CIA [35] and PUNS <ref> [36] </ref> only handle simple statistical and cross-reference queries. Graph-based models represent source code in a graph where nodes are software components (such as procedures, data types, and modules), and arcs capture dependencies (such as resource flows).
Reference: [37] <author> R. Al-Zoubi and A. Prakash. </author> <title> Software change analysis via attributed dependency graphs. </title> <type> Technical Report CSE-TR-95-91, </type> <institution> Department of EECS, University of Michigan, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: For example, systems such as CIA [35] and PUNS [36] only handle simple statistical and cross-reference queries. Graph-based models represent source code in a graph where nodes are software components (such as procedures, data types, and modules), and arcs capture dependencies (such as resource flows). The SCAN system <ref> [37] </ref> uses a graph-based model that is an attributed abstract syntax representation. This model does capture the structural information necessary; however, it does not capture the strong typing associated with programming-language objects.
Reference: [38] <author> S. Paul and A. Prakash. </author> <title> Source code retrieval using program patterns. </title> <booktitle> In CASE'92: Proceedings of the Fifth International Workshop on Computer-Aided Software Engineering, </booktitle> <address> (Montreal, Quebec; July 6-10, </address> <year> 1992), </year> <pages> pages 95-105, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: However, the focus in REFINE has not been on the design of efficient source code search primitives. 5.2.2 SCRUPLE The University of Michigan group has developed the SCRUPLE source code search system (Source Code Retrieval Using Pattern LanguagEs) <ref> [38] </ref>. SCRUPLE is based on a pattern-based query language that can be used to specify complex structural patterns of code not expressible using other existing systems. The pattern language allows users flexibility regarding the degree of precision to which a code structure is specified.
Reference: [39] <author> S. Paul and A. Prakash. </author> <title> A framework for source code search using program patterns. </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> June </month> <year> 1994. </year>
Reference-contexts: However, current source code query systems, including SCRUPLE, succeed in handling only subsets of the wide range of queries possible on source code, trading generality and expressive power for ease of implementation and practicality. To address the problem, Paul and Prakash have designed a source code algebra (SCA) <ref> [39] </ref> as the formal framework on top of which a variety of high-level query languages can be implemented. In principle, these query languages can be graphical, pattern-based, relational, or flow-oriented. The modeling of program source code as an algebra has four important consequences for reverse engineering.
Reference: [40] <author> K. Bruce and P. Wegner. </author> <title> An algebraic model of subtype and inheritance. </title> <booktitle> In Advances in Database Programming Languages. </booktitle> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: Second, query languages built using the algebra will have formal semantics. Third, the algebra itself serves as low-level applicative query language. Finally, source code queries expressed as algebra expressions can be optimized using algebraic transformation rules and heuristics. Source code is modeled as a generalized order-sorted algebra <ref> [40] </ref>, where the sorts are the program objects with operators defined on them. The choice of sorts and operators directly affects the modeling and querying power of the SCA. Essentially, SCA is an algebra of objects, sets, and sequences.
Reference: [41] <author> K. Kontogiannis. </author> <title> Toward program representation and program understanding using process algebras. </title> <booktitle> In CASCON'92: Proceedings of the 1992 CAS Conference, </booktitle> <address> (Toronto, Ontario; November 9-12, </address> <year> 1992), </year> <pages> pages 299-317. </pages> <institution> IBM Canada Ltd., </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: The core of the query system will be language independent. This tool generation technique is similar to yacc, a parser generator. 5.3 Semantic analysis The McGill research <ref> [41] </ref> involves four subgoals. First, program representations are needed to capture both the structural and semantic aspects of software. Second, comparison algorithms are needed to find similar code fragments. Third, pattern matching algorithms are needed to find instances of programming plans (or intents) in the source code.
Reference: [42] <author> L. M. Wills. </author> <title> Automated program recognition: A feasibility demonstration. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 45(1-2), </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Annotations stored in the AST may be used by other analysis routines. 5.3.2 Programming plans More generally, comparison methods are needed to help recognize instances of programming plans (abstracted code fragments). There are several other pattern matching techniques besides similarity measures. GRASP <ref> [42] </ref> compares the attributed data flow subgraphs of code fragments and algorithmic plans and uses control dependencies as additional constraints. PROUST [43, 44] compares the syntax tree of a program with suites of tree templates representing the plans.
Reference: [43] <author> W. Johnson and E. Soloway. </author> <title> PROUST. </title> <journal> Byte, </journal> <volume> 10(4) </volume> <pages> 179-190, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: There are several other pattern matching techniques besides similarity measures. GRASP [42] compares the attributed data flow subgraphs of code fragments and algorithmic plans and uses control dependencies as additional constraints. PROUST <ref> [43, 44] </ref> compares the syntax tree of a program with suites of tree templates representing the plans. A plan-instance match is recognized if a code fragment conforms to a template, and certain constraints and subgoals are satisfied.
Reference: [44] <author> W. Kozaczynski, J. Ning, and A. Engberts. </author> <title> Program concept recognition and transformation. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(12) </volume> <pages> 1065-1075, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: There are several other pattern matching techniques besides similarity measures. GRASP [42] compares the attributed data flow subgraphs of code fragments and algorithmic plans and uses control dependencies as additional constraints. PROUST <ref> [43, 44] </ref> compares the syntax tree of a program with suites of tree templates representing the plans. A plan-instance match is recognized if a code fragment conforms to a template, and certain constraints and subgoals are satisfied.
Reference: [45] <author> S. Letovsky. </author> <title> Plan Analysis of Programs. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Yale University, </institution> <month> December </month> <year> 1988. </year> <month> 29 </month>
Reference-contexts: PROUST [43, 44] compares the syntax tree of a program with suites of tree templates representing the plans. A plan-instance match is recognized if a code fragment conforms to a template, and certain constraints and subgoals are satisfied. In CPU <ref> [45] </ref>, comparisons are performed by applying a unification algorithm on code fragments and programming plans represented by lambda calculus expressions. Textual- and lexical-matching techniques encounter problems when code fragments contain irrelevant statements or when plans are delocalized. Moreover, program behavior is not considered.
Reference: [46] <author> T. McCabe. </author> <title> A complexity measure. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-7(4):308-320, </volume> <month> September </month> <year> 1976. </year>
Reference-contexts: They are: 1. the number of functions called from a software component (i.e., fan-out); 2. the ratio of I/O variables to the fan-out; 3. McCabe's cyclomatic complexity <ref> [46] </ref>; 4. Albrecht's Function Point quality metric [47]; and 5. Henry-Kafura's information flow quality metric [48]. Similarity is gauged by a distance measure on the tuples.
Reference: [47] <author> A. Albrecht. </author> <title> Measuring application development productivity. </title> <booktitle> In Proceedings of the IBM Applications Development Symposium, </booktitle> <pages> pages 83-92, </pages> <month> October </month> <year> 1979. </year>
Reference-contexts: They are: 1. the number of functions called from a software component (i.e., fan-out); 2. the ratio of I/O variables to the fan-out; 3. McCabe's cyclomatic complexity [46]; 4. Albrecht's Function Point quality metric <ref> [47] </ref>; and 5. Henry-Kafura's information flow quality metric [48]. Similarity is gauged by a distance measure on the tuples.
Reference: [48] <author> S. Henry, D. Kafura, and K. Harris. </author> <title> On the relationships among the three software metrics. </title> <booktitle> In Proceedings of the 1981 ACM Workshop/Symposium on Measurement and Evaluation of Software Quality, </booktitle> <month> March </month> <year> 1981. </year>
Reference-contexts: They are: 1. the number of functions called from a software component (i.e., fan-out); 2. the ratio of I/O variables to the fan-out; 3. McCabe's cyclomatic complexity [46]; 4. Albrecht's Function Point quality metric [47]; and 5. Henry-Kafura's information flow quality metric <ref> [48] </ref>. Similarity is gauged by a distance measure on the tuples.
Reference: [49] <author> V. Basili and H. Rombach. </author> <title> Tailoring the software process to project goals and environments. </title> <booktitle> In ICSE '9: The Ninth International Conference on Software Engineering, </booktitle> <pages> pages 345-359, </pages> <year> 1987. </year>
Reference-contexts: The implementation of these analyses uses the REFINE product. 5.3.4 Goal-driven program understanding Another design recovery strategy that has been explored by the McGill group is a variation of the GQM <ref> [49] </ref> model: the goal, question, analysis, action model [50]. A number of available options are compared, and the one that best matches a given objective is selected. The choice is based on experience and formal knowledge. This process can be used to find instances of programming plans.
Reference: [50] <author> K. Kontogiannis, M. Bernstein, E. Merlo, and R. D. Mori. </author> <title> The development of a partial design recovery environment for legacy systems. </title> <booktitle> In the Proceedings of CASCON '93, </booktitle> <address> (Toronto, Ontario; October 25-28, </address> <year> 1993), </year> <pages> pages 206-216, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: The implementation of these analyses uses the REFINE product. 5.3.4 Goal-driven program understanding Another design recovery strategy that has been explored by the McGill group is a variation of the GQM [49] model: the goal, question, analysis, action model <ref> [50] </ref>. A number of available options are compared, and the one that best matches a given objective is selected. The choice is based on experience and formal knowledge. This process can be used to find instances of programming plans.
Reference: [51] <author> A. Corazza, R. De Mori, R. Gretter, and G. Satta. </author> <title> Computation of probabilities for an island-driven parser. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <year> 1989. </year>
Reference-contexts: The choice is based on experience and formal knowledge. This process can be used to find instances of programming plans. The comparison process is iterative, goal-driven, and affected by the purpose of the analysis and the results of previous work. A moving frontier <ref> [51] </ref> divides recognized plans and original program material. Subgoals are set around fragments that have been recognized with high confidence. The analysis continues outward seeking the existence of other parts of the plan in the code.
Reference: [52] <author> N. Kiesel, A. Schurr, and B. Westfechtel. </author> <title> GRAS: A graph-oriented database system for (software) engineering applications. In CASE '93: </title> <booktitle> The Sixth International Conference on Computer-Aided Software Engineering, </booktitle> <institution> (Institute of Systems Science, National University of Singapore, </institution> <address> Singapore; July 19-23, </address> <year> 1993), </year> <pages> pages 272-286, </pages> <address> July 1993. </address> <publisher> IEEE Computer Society Press (Order Number 3480-02). </publisher>
Reference-contexts: For example, some of the many dependencies generated by the defect filtering system might be explored and summarized using the Rigi graph editor. However, the defect detection system uses the REFINE object-oriented repository, and the Rigi system uses the GRAS graph-based repository <ref> [52] </ref>. Integrating the representations employed by REFINE and Rigi is a non-trivial problem. With such integration in mind, a new phase of the project was launched early in 1993.
Reference: [53] <author> J. Mylopoulos, A. Borgida, M. Jarke, and M. Koubarakis. </author> <title> Telos: Representing knowledge about information systems. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 8(4) </volume> <pages> 325-362, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The remainder of this section describes the schema developed for the first phase. The information model adopted for the repository schema is Telos, originally developed at the University of Toronto <ref> [53] </ref>. Features of Telos include: an object-oriented framework that supports generalization, classification and attribution, a meta-modelling facility, and a novel treatment of attributes including multiple inheritance of attributes and attribute classes.

References-found: 53

