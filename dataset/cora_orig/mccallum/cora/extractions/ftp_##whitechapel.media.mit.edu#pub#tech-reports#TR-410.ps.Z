URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-410.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: brand@media.mit.edu;  
Title: "Inverse Hollywood Problem": From video to scripts and storyboards via causal analysis  
Author: Matthew Brand 
Web: www.media.mit.edu/~brand  
Address: 20 Ames Street, Cambridge, MA 02139 USA  
Affiliation: The Media Lab, MIT  
Note: The  
Abstract: c flMIT Media Lab Perceptual Computing / Learning and Common Sense Technical Report 410 28dec96 Abstract We address the problem of visually detecting causal events and fitting them together into a coherent story of the action witnessed by the camera. We show that this can be done by reasoning about the motions and collisions of surfaces, using high-level causal constraints derived from psychological studies of infant visual behavior. These constraints are naive forms of basic physical laws governing substantiality, contiguity, momentum, and acceleration. We describe two implementations. One system parses instructional videos, extracting plans of action and key frames suitable for storyboarding. Since learning will play a role in making such systems robust, we introduce a new framework for coupling hidden Markov models and demonstrate its use in a second system that segments stereo video into actions in near real-time. Rather than attempt accurate low-level vision, both systems use high-level causal analysis to integrate fast but sloppy pixel-based representations over time. The output is suitable for summary, indexing, and automated editing. c fl1997 AAAI. All rights reserved.
Abstract-found: 1
Intro-found: 1
Reference: <author> Azarbayejani, A., and Pentland, A. </author> <year> 1996. </year> <title> Real-time self-calibrating stereo person tracking using 3-D shape estimation from blob features. </title> <booktitle> In Proc. </booktitle> <address> ICPR96. </address>
Reference-contexts: These two systems are arranged in an uncalibrated wide-disparity stereo vision rig, and feed their results to a third system which recursively estimates maximum-likelihood 3D positions of the objects <ref> (Azarbayejani & Pentland 1996) </ref>. To remove high-frequency noise, the stream of 3D positions is resampled at 60Hz, low-pass filtered, and downsampled. The result is then differenced both spatially and temporally, yielding object velocities _x, relative distances d, and relative velocities _ d.
Reference: <author> Brand, M., and Oliver, N. </author> <year> 1996. </year> <title> Coupled hidden markov models for complex action recognition. </title> <booktitle> In Proc. </booktitle> <address> CVPR97. </address>
Reference-contexts: Coupled hidden Markov models To address the problem of compositional state, we introduce a method for coupling and training hidden Markov models. We sketch the method here; a full mathematical exposition can be found in <ref> (Brand 1996) </ref>. An HMM is a quantization of a system's configuration space into discrete regions S = fa 1 ; a 2 ; a 3 ; : : : ; a N g, which are selected by a time-indexed state variable s t 2 S. <p> Coupled HMMs have substantially outperformed conventional HMMs in a number of simulated and real vision tasks, with better data fitting, classification, and robustness to initial conditions <ref> (Brand & Oliver 1996) </ref>. Implementation Using coupled HMMs as our event detectors, we assembled a vision system as follows: The motion blob tracker was replaced with two vision systems that each track three blobs of connected pixels of similar color, regardless of motion.
Reference: <author> Brand, M. </author> <year> 1996. </year> <title> Coupled hidden markov models for modeling interacting processes. Forthcoming (under review). Available as MIT Media Lab Vision & Modeling TR #405. </title>
Reference-contexts: Coupled hidden Markov models To address the problem of compositional state, we introduce a method for coupling and training hidden Markov models. We sketch the method here; a full mathematical exposition can be found in <ref> (Brand 1996) </ref>. An HMM is a quantization of a system's configuration space into discrete regions S = fa 1 ; a 2 ; a 3 ; : : : ; a N g, which are selected by a time-indexed state variable s t 2 S. <p> Coupled HMMs have substantially outperformed conventional HMMs in a number of simulated and real vision tasks, with better data fitting, classification, and robustness to initial conditions <ref> (Brand & Oliver 1996) </ref>. Implementation Using coupled HMMs as our event detectors, we assembled a vision system as follows: The motion blob tracker was replaced with two vision systems that each track three blobs of connected pixels of similar color, regardless of motion.
Reference: <editor> Essa, I., ed. </editor> <booktitle> 1996. Proc., 2nd International Conf. on Automatic Face and Gesture Recognition. </booktitle> <address> Killington, VT: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: These systems were intended as front ends for robotic pick-and-place mimicry and emphasized scene geometry, taking somewhat ad hoc approaches to causality and action. Presently there is a growing literature in gesture recognition from motions <ref> (Essa 1996) </ref>, with an emphasis on classification rather than interpretation of structured activity. (Siskind & Morris 1996) blurs this distinction somewhat by using Markov models to classify short sequences of individual motions as throwing, dropping, lifting, and pushing gestures, given relative velocity profiles between an arm and an object. (Mann, Jepson,
Reference: <author> Ikeuchi, K., and Suehiro, T. </author> <year> 1994. </year> <title> Towards an assembly plan from observation, part 1: Task recognition with polyhedral objects. </title> <journal> IEEE Transactions on Robotics and Automation 10(3). </journal>
Reference-contexts: Related vision work Early approaches to action understanding emphasized reconstruction followed by analysis; lately attention is turning to applying causal constraints directly to motion traces. (Kuniyoshi & Inoue 1993) and <ref> (Ikeuchi & Suehiro 1994) </ref> described systems that recognize actions in assembly tasks with simple geometric objects, e.g., blocks. These systems were intended as front ends for robotic pick-and-place mimicry and emphasized scene geometry, taking somewhat ad hoc approaches to causality and action.
Reference: <author> Kuniyoshi, Y., and Inoue, H. </author> <year> 1993. </year> <title> Qualitative recognition of ongoing human action sequences. </title> <booktitle> In Proc. IJCAI93. </booktitle>
Reference-contexts: Gisting may be thought of as the "inverse Hollywood problem" | begin with a movie, end with a script and storyboard. Related vision work Early approaches to action understanding emphasized reconstruction followed by analysis; lately attention is turning to applying causal constraints directly to motion traces. <ref> (Kuniyoshi & Inoue 1993) </ref> and (Ikeuchi & Suehiro 1994) described systems that recognize actions in assembly tasks with simple geometric objects, e.g., blocks. These systems were intended as front ends for robotic pick-and-place mimicry and emphasized scene geometry, taking somewhat ad hoc approaches to causality and action.
Reference: <author> Mann, R.; Jepson, A.; and Siskind, J. M. </author> <year> 1996. </year> <title> Computational perception of scene dynamics. </title> <booktitle> In Proc. </booktitle> <address> ECCV96, II:528-539. </address>
Reference-contexts: motions (Essa 1996), with an emphasis on classification rather than interpretation of structured activity. (Siskind & Morris 1996) blurs this distinction somewhat by using Markov models to classify short sequences of individual motions as throwing, dropping, lifting, and pushing gestures, given relative velocity profiles between an arm and an object. <ref> (Mann, Jepson, & Siskind 1996) </ref> present a system that analyzes kinematic and dynamic relations between objects on a frame-by-frame basis. The program finds minimal systems of Newtonian equations that are consistent with each frame, but these are not necessarily consistent over time nor do they mark causal events.
Reference: <author> Siskind, J., and Morris, Q. </author> <year> 1996. </year> <title> A maximum-likelihood approach to visual event classification. </title> <booktitle> In Proc. </booktitle> <address> ECCV96, II:347-360. </address>
Reference-contexts: These systems were intended as front ends for robotic pick-and-place mimicry and emphasized scene geometry, taking somewhat ad hoc approaches to causality and action. Presently there is a growing literature in gesture recognition from motions (Essa 1996), with an emphasis on classification rather than interpretation of structured activity. <ref> (Siskind & Morris 1996) </ref> blurs this distinction somewhat by using Markov models to classify short sequences of individual motions as throwing, dropping, lifting, and pushing gestures, given relative velocity profiles between an arm and an object. (Mann, Jepson, & Siskind 1996) present a system that analyzes kinematic and dynamic relations between <p> motions (Essa 1996), with an emphasis on classification rather than interpretation of structured activity. (Siskind & Morris 1996) blurs this distinction somewhat by using Markov models to classify short sequences of individual motions as throwing, dropping, lifting, and pushing gestures, given relative velocity profiles between an arm and an object. <ref> (Mann, Jepson, & Siskind 1996) </ref> present a system that analyzes kinematic and dynamic relations between objects on a frame-by-frame basis. The program finds minimal systems of Newtonian equations that are consistent with each frame, but these are not necessarily consistent over time nor do they mark causal events. <p> Many examples can be found in three-place pragmatic verbs in language ("X gave Y the Z; U placed the V on the W;" etc.). The strategy used in <ref> (Siskind & Morris 1996) </ref> | reducing two-party actions to a single spatial relation which is then tracked by a hidden Markov model (HMM) | will not work because there are a variety of relations between three objects whose relative importance varies over time.
Reference: <author> Spelke, E. S., and Van de Valle, G. </author> <year> 1993. </year> <title> Perceiving and reasoning about objects: Insights from infants. </title> <editor> In Eilan, N.; McCarthy, R.; and Brewer, W., eds., </editor> <title> Spatial Representation. </title> <publisher> Oxford: Basil Blackwell. </publisher>
Reference-contexts: Indeed, there is a growing body of psychological evidence showing that infants are fluent perceivers of lawful causality and violations thereof. Spelke and Van de Valle found that infants aged 7.5 to 9.5 months will detect a wide range of apparent violations of the causality of motion <ref> (Spelke & Van de Valle 1993) </ref>.
Reference: <author> Wren, C.; Azarbayejani, A.; Darrell, T.; and Pentland, A. </author> <year> 1995. </year> <title> Pfinder: Real-time tracking of the human body. </title> <booktitle> In SPIE Proceedings, </booktitle> <volume> volume 2615. </volume> <pages> 6 </pages>
Reference-contexts: From visual events to causal events The gister reasons about changes in the integrity and motions of a single foreground blob | a connected map of image pixels that change due primarily to motion. The blob is obtained from a real-time vision system developed by <ref> (Wren et al. 1995) </ref>. Discontinuities in the blob's visual behavior signal changes of causality.
References-found: 10

