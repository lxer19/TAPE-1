URL: http://www.cs.rochester.edu/u/www/u/kyros/courses/CS577/papers/3D-photo-TR.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/kyros/courses/CS577/schedule.html
Root-URL: 
Email: fbouguetj,peronag@vision.caltech.edu  
Title: 3D photography on your desk  
Author: Jean-Yves Bouguet and Pietro Perona yz 
Keyword: 3D surface reconstruction, Active lighting, Shape recovery  
Address: 136-93, Pasadena, CA 91125, USA  Italy  
Affiliation: California Institute of Technology,  Universita di Padova,  
Abstract: A simple and inexpensive approach for extracting the three-dimensional shape of objects is presented. It is based on `weak structured lighting'; it differs from other conventional structured lighting approaches in that it requires very little hardware besides the camera: a desk-lamp, a pencil and a checkerboard. The camera faces the object, which is illuminated by the desk-lamp. The user moves a pencil in front of the light source casting a moving shadow on the object. The 3D shape of the object is extracted from the spatial and temporal location of the observed shadow. Experimental results are presented on three different scenes demonstrating that the error in reconstructing the surface is less than 1%.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Paul Besl, </author> <title> Advances in Machine Vision, chapter 1 Active optical range imaging sensors, </title> <address> pages 1-63, </address> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: So far, the commercial 3D scanners (e.g. the Cyberware scanner) have emphasized accuracy over the other parameters. These systems use motorized transport of the object, and active (laser, LCD projector) lighting of the scene, which makes them very accurate, but expensive and bulky <ref> [1, 17, 18, 14, 2] </ref>. fl Working paper Please, do not circulate.
Reference: [2] <author> P.J. Besl and N.D. McKay, </author> <title> "A method for registration of 3-d shapes", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(2) </volume> <pages> 239-256, </pages> <year> 1992. </year>
Reference-contexts: So far, the commercial 3D scanners (e.g. the Cyberware scanner) have emphasized accuracy over the other parameters. These systems use motorized transport of the object, and active (laser, LCD projector) lighting of the scene, which makes them very accurate, but expensive and bulky <ref> [1, 17, 18, 14, 2] </ref>. fl Working paper Please, do not circulate.
Reference: [3] <author> J.F. Canny, </author> <title> "A computational approach to edge detection", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8(6) </volume> <pages> 679-698, </pages> <year> 1986. </year>
Reference-contexts: One could notice that this property does not hold for all techniques. One example could be the image gradient approach for edge detection (like canny edge detector <ref> [3] </ref>). The maximum spatial gradient point does not necessarily match with the maximum temporal gradient point (in our problem this depends on the speed of the scanning).
Reference: [4] <author> Y. Chen and G. Medioni, </author> <title> "Object modelling by registration of multiple range images", </title> <journal> Image and Vision Computing, </journal> <volume> 10(3) </volume> <pages> 145-155, </pages> <year> 1992. </year>
Reference-contexts: Other extensions of this work relate to multiple view integration. We wish to extend the alignment technique to a method allowing the user to move freely the object in front of the camera and the lamp between scans in order to achieve a full coverage <ref> [13, 8, 6, 4] </ref>. That is necessary to construct complete 3D models.
Reference: [5] <author> Brian Curless and Marc Levoy, </author> <title> "Better optical triangulation through spacetime analysis", </title> <booktitle> Proc. 5 th Int. Conf. Computer Vision, </booktitle> <pages> pages 987-993, </pages> <year> 1995. </year>
Reference-contexts: Both stages correspond to finding the shadow edge, but the search domains are different: first one operates on the spatial coordinates (image coordinates) and the other one on the temporal coordinate. Curless in Levoy demonstrated in <ref> [5] </ref> that such a spatio-temporal approach is appropriate to preserve sharp discontinuities in the scene.
Reference: [6] <author> Brian Curless and Marc Levoy, </author> <title> "A volumetric method for building complex models from range images", </title> <booktitle> SIGGRAPH96, Computer Graphics Proceedings, </booktitle> <year> 1996. </year>
Reference-contexts: Other extensions of this work relate to multiple view integration. We wish to extend the alignment technique to a method allowing the user to move freely the object in front of the camera and the lamp between scans in order to achieve a full coverage <ref> [13, 8, 6, 4] </ref>. That is necessary to construct complete 3D models. <p> Our merging technique presents two advantages: (a) obtaining more coverage of the scene and (b) reducing the estimation noise. Moreover, since we do not move the camera between scans, we do not have to solve for the difficult problem of view alignment <ref> [13, 8, 6] </ref>. 46 Such an approach can easily be extended to more than two scans. In addition, notice that re-calibrating the lamp between scans is very easy provided some non-occluded regions are available for lamp calibration (refer to section 2.1).
Reference: [7] <author> O.D. Faugeras, </author> <title> Three dimensional vision, a geometric viewpoint, </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: The reader can also refer to Faugeras in <ref> [7] </ref> for further insights on camera calibration. 28 B Lamp Calibration The lamp calibration stage consists of estimating the location of the point light source S in space. Denote by S c the coordinates of S in the camera reference frame.
Reference: [8] <author> Berthold K.P. Horn, </author> <title> "Closed-form solution of absolute orientation using unit quater-nions", </title> <journal> J. Opt. Soc. Am. A, </journal> <volume> 4(4) </volume> <pages> 629-642, </pages> <year> 1987. </year>
Reference-contexts: Other extensions of this work relate to multiple view integration. We wish to extend the alignment technique to a method allowing the user to move freely the object in front of the camera and the lamp between scans in order to achieve a full coverage <ref> [13, 8, 6, 4] </ref>. That is necessary to construct complete 3D models. <p> Our merging technique presents two advantages: (a) obtaining more coverage of the scene and (b) reducing the estimation noise. Moreover, since we do not move the camera between scans, we do not have to solve for the difficult problem of view alignment <ref> [13, 8, 6] </ref>. 46 Such an approach can easily be extended to more than two scans. In addition, notice that re-calibrating the lamp between scans is very easy provided some non-occluded regions are available for lamp calibration (refer to section 2.1).
Reference: [9] <author> Jan J. Koenderink and A. J. van Doorn, </author> <title> "Geometrical modes as a general method to treat diffuse interreflections in radiometry", </title> <journal> J. Opt. Soc. Am., </journal> <volume> 73(6) </volume> <pages> 843-850, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: This profile is characteristic even when there is a fair amount of internal reflections in the scene <ref> [11, 9, 10, 16] </ref>.
Reference: [10] <author> Jurgen R. Meyer-Arendt, "Radiometry and photometry: </author> <title> Units and conversion factors", </title> <journal> Applied Optics, </journal> <volume> 7(10) </volume> <pages> 2081-2084, </pages> <month> October </month> <year> 1968. </year>
Reference-contexts: This profile is characteristic even when there is a fair amount of internal reflections in the scene <ref> [11, 9, 10, 16] </ref>.
Reference: [11] <author> Shree K. Nayar, Katsushi Ikeuchi, and Takeo Kanade, </author> <title> "Shape from interreflections", </title> <journal> Int. J. of Computer Vision, </journal> <volume> 6(3) </volume> <pages> 173-195, </pages> <year> 1991. </year>
Reference-contexts: This profile is characteristic even when there is a fair amount of internal reflections in the scene <ref> [11, 9, 10, 16] </ref>.
Reference: [12] <author> Athanasios Papoulis, </author> <title> Probability, Random Variables and Stochastic Processes, </title> <address> Mac Graw Hill, </address> <year> 1991, </year> <note> Third Edition. </note>
Reference-contexts: The final depth is computed by weighted average of Z L c and Z R Z c = ! L Z L c (91) c and Z L c were Gaussian distributed, and independent, they would be optimally averaged using the inverse of the variances as weights <ref> [12] </ref>: ! L = 2 Z R =( 2 Z R ) = ff 2 =(1 + ff 2 ) and Z L =( 2 Z R ) = 1=(1 + ff 2 ), where ff = V L =V R .
Reference: [13] <author> A.J. Stoddart and A. Hilton, </author> <title> "Registration of multiple point sets", </title> <booktitle> Proceedings of the 13th Int. Conf. of Pattern Recognition, </booktitle> <year> 1996. </year>
Reference-contexts: Other extensions of this work relate to multiple view integration. We wish to extend the alignment technique to a method allowing the user to move freely the object in front of the camera and the lamp between scans in order to achieve a full coverage <ref> [13, 8, 6, 4] </ref>. That is necessary to construct complete 3D models. <p> Our merging technique presents two advantages: (a) obtaining more coverage of the scene and (b) reducing the estimation noise. Moreover, since we do not move the camera between scans, we do not have to solve for the difficult problem of view alignment <ref> [13, 8, 6] </ref>. 46 Such an approach can easily be extended to more than two scans. In addition, notice that re-calibrating the lamp between scans is very easy provided some non-occluded regions are available for lamp calibration (refer to section 2.1).
Reference: [14] <author> Marjan Trobina, </author> <title> "Error model of a coded-light range sensor", </title> <type> Technical Report BIWI-TR-164, </type> <institution> ETH-Zentrum, </institution> <year> 1995. </year> <month> 47 </month>
Reference-contexts: So far, the commercial 3D scanners (e.g. the Cyberware scanner) have emphasized accuracy over the other parameters. These systems use motorized transport of the object, and active (laser, LCD projector) lighting of the scene, which makes them very accurate, but expensive and bulky <ref> [1, 17, 18, 14, 2] </ref>. fl Working paper Please, do not circulate.
Reference: [15] <author> R. Y. Tsai, </author> <title> "A versatile camera calibration technique for high accuracy 3d machine vision metrology using off-the-shelf tv cameras and lenses", </title> <journal> IEEE J. Robotics Automat., </journal> <volume> RA-3(4):323-344, </volume> <year> 1987. </year>
Reference-contexts: This method is very much inspired by the algorithm proposed by Tsai <ref> [15] </ref>. Note that since our calibration rig is planar, the optical center cannot be recovered through that process, and therefore is assumed to be fixed at the center of the image. A description of the whole procedure can be found in appendix A.
Reference: [16] <author> John W. T. Walsh, </author> <title> Photometry, </title> <publisher> Dover, </publisher> <address> NY, </address> <year> 1965. </year>
Reference-contexts: This profile is characteristic even when there is a fair amount of internal reflections in the scene <ref> [11, 9, 10, 16] </ref>.
Reference: [17] <author> Y.F. Wang, </author> <title> "Characterizing three-dimensional surface structures from visual images", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(1) </volume> <pages> 52-60, </pages> <year> 1991. </year>
Reference-contexts: So far, the commercial 3D scanners (e.g. the Cyberware scanner) have emphasized accuracy over the other parameters. These systems use motorized transport of the object, and active (laser, LCD projector) lighting of the scene, which makes them very accurate, but expensive and bulky <ref> [1, 17, 18, 14, 2] </ref>. fl Working paper Please, do not circulate.
Reference: [18] <author> Z. Yang and Y.F. Wang, </author> <title> "Error analysis of 3D shape construction from structured lighting", </title> <journal> Pattern Recognition, </journal> <volume> 29(2) </volume> <pages> 189-206, </pages> <year> 1996. </year> <month> 48 </month>
Reference-contexts: So far, the commercial 3D scanners (e.g. the Cyberware scanner) have emphasized accuracy over the other parameters. These systems use motorized transport of the object, and active (laser, LCD projector) lighting of the scene, which makes them very accurate, but expensive and bulky <ref> [1, 17, 18, 14, 2] </ref>. fl Working paper Please, do not circulate.
References-found: 18

