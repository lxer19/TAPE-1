URL: http://www.lehigh.edu/~ob00/integrated/proposal.ps.gz
Refering-URL: http://www.lehigh.edu/~ob00/integrated.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Knowledge Integration and Rule Extraction in Neural Networks Ph.D. Proposal  
Author: by Olcay Boz 
Date: 10 15- 1995  
Affiliation: Lehigh University EECS Department  
Abstract-found: 0
Intro-found: 1
Reference: <author> Cohen, W., </author> <year> (1995), </year> <title> Fast Effective Rule Induction., </title> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <address> ML95. </address>
Reference-contexts: For example if we want to train a decision tree and if the problem is something like: 9 If exactly 2 out of 6 attributes are 1 Then conclusion Solving these kind of problems with neural networks is easier. 3.1.2 Ripper Ripper <ref> (Cohen, 1995) </ref> is a machine learning technique for creating classification rules from examples. It creates if-then rules. Before learning Riper, first heuristically orders the classes (in increasing frequency, decreasing frequency, in the order given by user or optimal ordering found by ripper).
Reference: <author> Craven , M.W., and Shavlik, J.W., </author> <title> (1993),Learning Symbolic Rules Using Artificial Neural Networks., </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 73--80, Amherst , MA. </address> <note> (ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/craven.mlc93.ps) Craven, </note> <author> M.W., and Shavlik, J.W., </author> <year> (1994), </year> <title> "Using Sampling and Queries to Extract Rules from Trained Neural Networks", </title> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pp. 37-45. </pages> <address> (ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/craven.mlc94.ps) Fu, L.M., </address> <year> (1991), </year> <title> Rule Learning by Searching on Adapted Nets., </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence (pp. </booktitle> <volume> 590 - 595). </volume> <publisher> Anaheim, </publisher> <address> CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Fu, L.M., </author> <year> (1992), </year> <title> A Neural Network Model for learning Rule-Based Systems., </title> <publisher> IEEE. </publisher>
Reference-contexts: KBCNN (Fu, 1993) developed by LiMin Fu is a learning model which uses domain knowledge for creating a neural net and creating rules from the same neural net after training. KBCNN is an extension of an earlier system, Knowledgetron, <ref> (Fu, 1992) </ref> developed by Fu. Fus system shares the same advantages and disadvantages of KBANN. 2.4 Rule Extraction From Neural networks Neural networks are known as black boxes. Getting an explanation about the reasoning of the neural net is not very easy.
Reference: <author> Fu, L.M., </author> <year> (1993), </year> <title> Knowledge-Based Connectionism for Revising Domain Theories., </title> <journal> in IEEE transactions on Systems, Man and Cybernetics, </journal> <volume> Vol. 23, No. 1, </volume> <month> January/February </month> <year> 1993. </year>
Reference-contexts: KBANN may, depending on the initial knowledge domain theory, create very deep networks. Another disadvantage of KBANN is that, it uses only binary valued inputs. In real world applications inputs do not get only binary values, they also get integer, real and nominal (ordered or unordered) values. KBCNN <ref> (Fu, 1993) </ref> developed by LiMin Fu is a learning model which uses domain knowledge for creating a neural net and creating rules from the same neural net after training. KBCNN is an extension of an earlier system, Knowledgetron, (Fu, 1992) developed by Fu.
Reference: <author> Fu, L.M., </author> <year> (1994), </year> <title> Rule Creation From Neural Networks., </title> <journal> in IEEE transactions on systems, man and cybernetics, </journal> <volume> vol. 24, no. 8, </volume> <month> Aug. </month> <year> 1994. </year>
Reference: <author> Gallant, S.I., </author> <year> (1988), </year> <title> Connectionist Expert Systems., </title> <journal> Communications of the ACM, February 1988, </journal> <volume> vol. 31, no 2. </volume>
Reference-contexts: A connectionist expert system <ref> (Gallant 1988, 1993) </ref> is another variation of the coupled models. In connectionist expert systems, neural network is used to represent knowledge locally as weights between symbolic nodes. Each node has a symbolic meaning in these systems.
Reference: <author> Gallant, S.I., </author> <year> (1993), </year> <title> Neural Network learning and Expert Systems., </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: <author> Hilario, M., </author> <year> (1995), </year> <title> An Overview of Strategies for Neurosymbolic Integration. </title> <type> Technical Report, </type> <institution> CUI - University of Geneva, UNIGE-AI-95-04, </institution> <year> 1995. </year> <title> (ftp://cui.unige.ch/AI/tech-reports/unige-ai-95-04.ps) Medsker, L.R., (1994), Hybrid Neural Network and Expert Systems., </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Medsker, L.R., and Belley, D.L., </author> <year> (1992), </year> <title> "Models and Guidelines For Integrating Expert Systems and Neural Networks.", in "Hybrid architectures for intelligent 14 system", Abraham Kandel &Gideon Langholz. </title>

Reference: <author> Quinlan, J.R., </author> <year> (1986), </year> <title> Induction of Decision Trees., </title> <note> Machine Learning 1 81-106. </note>
Reference-contexts: But the algorithms are exponential. Therefore Fu put a limit in the number of antecedents in a rule. These algorithms also require binary inputs. 3. Symbolic Inductive Learning Algorithms 3.1.1 C4.5 C4.5 is a descendent of ID3 system developed by Ross Quinlan <ref> (Quinlan, 1986) </ref> (Quinlan, 1993). This is the system which has the greatest impact on machine learning research in the last years. It is a supervised learning system and constructs decision trees from examples.
Reference: <author> Quinlan, J.R., </author> <year> (1993), </year> <title> C4.5 : Programs For Machine Learning., </title> <publisher> Morgan Kauffman Publishers, </publisher> <address> San Mateo, California. </address>
Reference-contexts: But the algorithms are exponential. Therefore Fu put a limit in the number of antecedents in a rule. These algorithms also require binary inputs. 3. Symbolic Inductive Learning Algorithms 3.1.1 C4.5 C4.5 is a descendent of ID3 system developed by Ross Quinlan (Quinlan, 1986) <ref> (Quinlan, 1993) </ref>. This is the system which has the greatest impact on machine learning research in the last years. It is a supervised learning system and constructs decision trees from examples. The search space for a particular problem is all the possible trees that can be constructed with the samples.


References-found: 11

