URL: file://ftp.cs.toronto.edu/pub/carl/hansen.adaptive.nnsp94.ps.gz
Refering-URL: http://www.cs.toronto.edu/~carl/pub.html
Root-URL: 
Email: email: lkhansen,ed,csvarer,jlarsen@ei.dtu.dk  
Title: ADAPTIVE REGULARIZATION  
Author: L. K. Hansen, C. E. Rasmussen C. Svarer, and J. Larsen 
Address: B349  DK-2800 Lyngby, Denmark  
Affiliation: CONNECT, Electronics Institute  University of Denmark,  
Pubnum: Technical  
Abstract: Regularization, e.g., in the form of weight decay, is important for training and optimization of neural network architectures. In this work we provide a tool based on asymptotic sampling theory, for iterative estimation of weight decay parameters. The basic idea is to do a gradient descent in the estimated generalization error with respect to the regularization parameters. The scheme is implemented in our Designer Net framework for network training and pruning, i.e., is based on the diagonal Hessian approximation. The scheme does not require essential computational overhead in addition to what is needed for training and pruning. The viability of the approach is demonstrated in an experiment concerning prediction of the chaotic Mackey-Glass series. We find that the optimized weight decays are relatively large for densely connected networks in the initial pruning phase, while they decrease as pruning proceeds. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Akaike, </author> <title> "Fitting Autoregressive Models for Prediction," </title> <journal> Ann. Inst. Stat. Mat., </journal> <volume> 21, </volume> <pages> 243-247, </pages> <year> (1969). </year>
Reference-contexts: Further, we have shown how the generalization error of the network may be estimated without extensive cross-validation using a modification of Akaike's Final Prediction Error (FPE) estimate <ref> [1] </ref>. The minimal FPE constitutes a useful stopping fl Present address: Dept. of Computer Science, University of Toronto, Canada. 1 criterion for pruning. However, our previous work has been conditioned on the correct setting of several parameters, most prominently the weight decay parameters. <p> The generalization error estimates were also used for answering the question of how many weights it may be possible to delete in a pruning session in [11, 12]. We applied Akaike's FPE estimate <ref> [1] </ref> of the test error in terms of the training error which reads: b E test = p + N E train ; (10) where p is the number of training samples, and N is the number of parameters in the model.
Reference: [2] <author> L.K. Hansen and C.E. Rasmussen, </author> <title> "Pruning from Adaptive Regularization," Accepted for Neural Computation, </title> <institution> CONNECT Electronics Institute, Technical University of Denmark, </institution> <note> preprint (1993). </note>
Reference-contexts: The results obtained can be viewed as a sampling theory alternative to the Bayesian or Evidence based techniques for adaptive regularization developed by MacKay [8, 9]. An analytical comparison of these two techniques has recently been given in <ref> [2] </ref>. LEARNING The use of system identification tools for neural net learning has been pioneered by Moody (see e.g., [10]) who derived estimators for the average generalization error. The main source of uncertainty in the learning process is the shortage of training data. <p> We find it more natural to optimize the quantity that is our basic objective, namely the test error. It is at present not clear what the relation between the Evidence and the generalization error is. Empirically, they have been found to be related <ref> [2, 8] </ref>. We use a simple gradient descent procedure for minimization of the generalization error: ff (n + 1) = ff (n) @E test @ff fi fi ; (12) where is a gradient descent parameter, and n is the iteration index (one epoch). <p> Expression (13) contains two unknown quantities: the teacher weights w fl ij and the noise variance 2 . The teacher weights are replaced by the current estimated weights of the network (see <ref> [2] </ref> for a 2 In fact the notion of an effective number of parameters is quite delicate see [6]. 3 In the derivative of the test error we have kept the dependence 2 =( + 2ff=p) 3 (rather than 1=) providing a stabilizing effect similar to the Moore-Penrose pseudo inverse dis
Reference: [3] <author> L.K. Hansen, </author> <title> "Stochastic Linear Learning: Exact Test and Training Error Averages," </title> <booktitle> Neural Networks 6, </booktitle> <pages> 393-396, </pages> <year> (1993). </year>
Reference-contexts: Other important contributions to uncertainty are: Lack of fit, noise in the training process, and non-stationarity of the data-generating environment. Lack of fit 1 was discussed in, e.g., [5], while noise in the training process has been discussed in <ref> [3] </ref>. In this presentation we will neglect these three effects. Lack of fit can be minimized by starting the pruning process from large enough networks, while noise in the training process can be relieved by careful search in weight space.
Reference: [4] <author> J. Hertz, A. Krogh and R.G. Palmer, </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison Wesley, </publisher> <address> New York, </address> <year> (1991). </year>
Reference: [5] <author> J. Larsen, </author> <title> Design of Neural Network Filters, </title> <type> Ph. D. Thesis, </type> <institution> Electronics Institute, Technical University of Denmark, </institution> <month> March </month> <year> (1993). </year>
Reference-contexts: INTRODUCTION Learning based on the conventional feed-forward net may be analyzed with statistical methods and the result of such analysis can be applied to model optimization <ref> [5, 6, 10, 11, 12] </ref>. We have shown how pruning and regularization can be combined to design compact networks for time series prediction [11, 12]. <p> The main source of uncertainty in the learning process is the shortage of training data. Other important contributions to uncertainty are: Lack of fit, noise in the training process, and non-stationarity of the data-generating environment. Lack of fit 1 was discussed in, e.g., <ref> [5] </ref>, while noise in the training process has been discussed in [3]. In this presentation we will neglect these three effects. <p> The rest term R contains higher order quantities and terms that do not affect the estimate of the regularization parameters, see <ref> [5, 6] </ref> for further discussion. The estimate is based on linearization of the networks as regards the fluctuations in the weights resulting from different training sets.
Reference: [6] <author> J. Larsen and L.K. Hansen, </author> <title> "Generalization Performance of Regularized Neural Network Models," </title> <booktitle> In proceedings of the IEEE Workshop on Neural Networks for Signal Processing NNSP'94. </booktitle>
Reference-contexts: INTRODUCTION Learning based on the conventional feed-forward net may be analyzed with statistical methods and the result of such analysis can be applied to model optimization <ref> [5, 6, 10, 11, 12] </ref>. We have shown how pruning and regularization can be combined to design compact networks for time series prediction [11, 12]. <p> Using the diagonal approximation for the Hessian this error (also referred to as the test error) can be estimated as <ref> [6] </ref>: b E test = N eff +2 ij p w fl ij + 2ff w =p +2 j p W fl fl j + 2ff W =p + R; with N eff = ij ij 2 N W X fl j + 2ff W =p ; (9) where the 's <p> The rest term R contains higher order quantities and terms that do not affect the estimate of the regularization parameters, see <ref> [5, 6] </ref> for further discussion. The estimate is based on linearization of the networks as regards the fluctuations in the weights resulting from different training sets. <p> The teacher weights are replaced by the current estimated weights of the network (see [2] for a 2 In fact the notion of an effective number of parameters is quite delicate see <ref> [6] </ref>. 3 In the derivative of the test error we have kept the dependence 2 =( + 2ff=p) 3 (rather than 1=) providing a stabilizing effect similar to the Moore-Penrose pseudo inverse dis cussed in [6]. discussion), while the noise variance is estimated from the training error in the same approximation <p> 2 In fact the notion of an effective number of parameters is quite delicate see <ref> [6] </ref>. 3 In the derivative of the test error we have kept the dependence 2 =( + 2ff=p) 3 (rather than 1=) providing a stabilizing effect similar to the Moore-Penrose pseudo inverse dis cussed in [6]. discussion), while the noise variance is estimated from the training error in the same approximation as in eq. (11): b 2 = 1 p E train : (14) EXPERIMENTS We illustrate the virtues of the adaptive regularization scheme on two time series forecasting problems.
Reference: [7] <author> Y. Le Cun, J.S. Denker, and S.A. Solla, </author> <title> "Optimal Brain Damage," </title> <booktitle> In Advances in Neural Information Processing Systems 2, </booktitle> <pages> 598-605, </pages> <publisher> Mor-gan Kaufman, </publisher> <year> (1990). </year>
Reference-contexts: We have shown how pruning and regularization can be combined to design compact networks for time series prediction [11, 12]. Our "Designer Net" framework is based on the Optimal Brain Damage (OBD) method of Le Cun et al. <ref> [7] </ref> and we use simple weight decay for regularization. The benefits from compact architectures are three-fold: Their generalization ability is better, they carry less computational burden, and they are faster to adapt if the environment changes. <p> Hence, the network weights, u, are trained to recognize the short time structure of the training set time series. PRUNING The OBD method proposed by Le Cun et al. <ref> [7] </ref> was successfully applied to reduce large networks for recognition of handwritten digits. The basic idea is to estimate the increase in the training error when deleting weights. <p> The saliency definition used here takes into account that the weight decay terms force the weights to depart from the minimum of the training set error. As in <ref> [7] </ref> we approximate the second derivative by the positive semi-definite expression: @ 2 E train @u 2 p k=1 @F u (x (k)) 2 The major assumptions entering the derivation of OBD are: 1) Terms of third and higher orders in the deleted weights can be neglected. 2) The off-diagonal terms
Reference: [8] <author> D. MacKay, </author> <title> "Bayesian interpolation". </title> <booktitle> Neural Computation 4 448-472, </booktitle> <year> (1992). </year>
Reference-contexts: In this contribution we provide the possibility of adapting regularization parameters within the Designer Net framework. The results obtained can be viewed as a sampling theory alternative to the Bayesian or Evidence based techniques for adaptive regularization developed by MacKay <ref> [8, 9] </ref>. An analytical comparison of these two techniques has recently been given in [2]. LEARNING The use of system identification tools for neural net learning has been pioneered by Moody (see e.g., [10]) who derived estimators for the average generalization error. <p> We find it more natural to optimize the quantity that is our basic objective, namely the test error. It is at present not clear what the relation between the Evidence and the generalization error is. Empirically, they have been found to be related <ref> [2, 8] </ref>. We use a simple gradient descent procedure for minimization of the generalization error: ff (n + 1) = ff (n) @E test @ff fi fi ; (12) where is a gradient descent parameter, and n is the iteration index (one epoch).
Reference: [9] <author> D. MacKay, </author> <title> "A practical framework for backpropagation networks". </title> <booktitle> Neural Computation 4 415-447 (1992). </booktitle>
Reference-contexts: In this contribution we provide the possibility of adapting regularization parameters within the Designer Net framework. The results obtained can be viewed as a sampling theory alternative to the Bayesian or Evidence based techniques for adaptive regularization developed by MacKay <ref> [8, 9] </ref>. An analytical comparison of these two techniques has recently been given in [2]. LEARNING The use of system identification tools for neural net learning has been pioneered by Moody (see e.g., [10]) who derived estimators for the average generalization error.
Reference: [10] <author> J.E. Moody, </author> <title> "Note on Generalization, Regularization and Architecture Selection in Nonlinear Systems," </title> <booktitle> In Neural Networks For Signal Processing Proceedings of the 1991 IEEE-SP Workshop (Eds. </booktitle> <editor> B.H. Juang, S.Y. Kung, and C. Kamm), </editor> <publisher> IEEE Service Center, </publisher> <pages> 1-10, </pages> <year> (1991). </year>
Reference-contexts: INTRODUCTION Learning based on the conventional feed-forward net may be analyzed with statistical methods and the result of such analysis can be applied to model optimization <ref> [5, 6, 10, 11, 12] </ref>. We have shown how pruning and regularization can be combined to design compact networks for time series prediction [11, 12]. <p> An analytical comparison of these two techniques has recently been given in [2]. LEARNING The use of system identification tools for neural net learning has been pioneered by Moody (see e.g., <ref> [10] </ref>) who derived estimators for the average generalization error. The main source of uncertainty in the learning process is the shortage of training data. Other important contributions to uncertainty are: Lack of fit, noise in the training process, and non-stationarity of the data-generating environment. <p> Since we have regularized the training procedure by weight-decay terms ff w ; ff W , hence, suppressed the ability of the (otherwise) ill-determined parameters to model noise, we need to modify the standard FPE estimate by replacing the total number of parameters with the effective number of parameters, see <ref> [10, 11, 12] </ref> 2 : b E test = p + N eff E train ; (11) With the above tool we can obtain a generalization error estimate for each pruned network. By selecting the network with the lowest estimated generalization error we obtain a stopping criterion for pruning.
Reference: [11] <author> C. Svarer, L.K. Hansen, and J. Larsen, </author> <title> "On Design and Evaluation of Tapped Delay Line Networks," </title> <booktitle> In Proceedings of the 1993 IEEE International Conference on Neural Networks San Francisco, </booktitle> <pages> 46-51, </pages> <year> (1993). </year>
Reference-contexts: INTRODUCTION Learning based on the conventional feed-forward net may be analyzed with statistical methods and the result of such analysis can be applied to model optimization <ref> [5, 6, 10, 11, 12] </ref>. We have shown how pruning and regularization can be combined to design compact networks for time series prediction [11, 12]. <p> INTRODUCTION Learning based on the conventional feed-forward net may be analyzed with statistical methods and the result of such analysis can be applied to model optimization [5, 6, 10, 11, 12]. We have shown how pruning and regularization can be combined to design compact networks for time series prediction <ref> [11, 12] </ref>. Our "Designer Net" framework is based on the Optimal Brain Damage (OBD) method of Le Cun et al. [7] and we use simple weight decay for regularization. <p> A simulator based on batch mode, second order local optimization has been developed, as described in <ref> [11, 12] </ref>. The scheme is based on the diagonal approximation of the cost-function Hessian (the second derivative matrix). <p> The estimate is based on linearization of the networks as regards the fluctuations in the weights resulting from different training sets. The generalization error estimates were also used for answering the question of how many weights it may be possible to delete in a pruning session in <ref> [11, 12] </ref>. We applied Akaike's FPE estimate [1] of the test error in terms of the training error which reads: b E test = p + N E train ; (10) where p is the number of training samples, and N is the number of parameters in the model. <p> Since we have regularized the training procedure by weight-decay terms ff w ; ff W , hence, suppressed the ability of the (otherwise) ill-determined parameters to model noise, we need to modify the standard FPE estimate by replacing the total number of parameters with the effective number of parameters, see <ref> [10, 11, 12] </ref> 2 : b E test = p + N eff E train ; (11) With the above tool we can obtain a generalization error estimate for each pruned network. By selecting the network with the lowest estimated generalization error we obtain a stopping criterion for pruning. <p> The first experiment explores the functional dependence of the derivative of the estimated test error cf. equation (13). The forecasting problem is the sunspot benchmark involving estimation of the yearly sunspot activity from the past twelve years activity (see, e.g., <ref> [11] </ref> for a detailed description of the benchmark). To simplify we consider a linear model for which the parameters are uniquely determined when using the least squares cost function. The sunspot benchmark involves three data sets: A training set and two test sets. <p> The two upper figures show the errors (see <ref> [11] </ref> for a definition) on test sets both having shallow minima just below a weight decay of 0:1, while the bottom figure shows the derivative of the estimated test error as function of weight decay. Note that a gradient descent procedure will converge to the zero point. <p> The very low value of the input-to-hidden weight decay for the small networks is also in line with our earlier observations, namely that one can retrain the optimal architecture without weight decay and get slightly improved generalization <ref> [11, 12] </ref>. CONCLUSION A scheme has been derived for adaptation of weight decay parameters. The scheme is based on asymptotic sampling theory. Two examples were given to illustrate the virtues of such adaptation.
Reference: [12] <author> C. Svarer, L.K. Hansen, and J. Larsen, and C.E. Rasmussen, </author> <title> "Designer Networks for Time Series Processing," </title> <booktitle> Proceedings of the 1993 IEEE Workshop on Neural Networks for Signal Processing (NNSP'93) Balti-more (Eds. </booktitle> <address> C.A. </address> <institution> Kamm et al.), </institution> <month> 78-87, </month> <year> (1993). </year>
Reference-contexts: INTRODUCTION Learning based on the conventional feed-forward net may be analyzed with statistical methods and the result of such analysis can be applied to model optimization <ref> [5, 6, 10, 11, 12] </ref>. We have shown how pruning and regularization can be combined to design compact networks for time series prediction [11, 12]. <p> INTRODUCTION Learning based on the conventional feed-forward net may be analyzed with statistical methods and the result of such analysis can be applied to model optimization [5, 6, 10, 11, 12]. We have shown how pruning and regularization can be combined to design compact networks for time series prediction <ref> [11, 12] </ref>. Our "Designer Net" framework is based on the Optimal Brain Damage (OBD) method of Le Cun et al. [7] and we use simple weight decay for regularization. <p> A simulator based on batch mode, second order local optimization has been developed, as described in <ref> [11, 12] </ref>. The scheme is based on the diagonal approximation of the cost-function Hessian (the second derivative matrix). <p> The estimate is based on linearization of the networks as regards the fluctuations in the weights resulting from different training sets. The generalization error estimates were also used for answering the question of how many weights it may be possible to delete in a pruning session in <ref> [11, 12] </ref>. We applied Akaike's FPE estimate [1] of the test error in terms of the training error which reads: b E test = p + N E train ; (10) where p is the number of training samples, and N is the number of parameters in the model. <p> Since we have regularized the training procedure by weight-decay terms ff w ; ff W , hence, suppressed the ability of the (otherwise) ill-determined parameters to model noise, we need to modify the standard FPE estimate by replacing the total number of parameters with the effective number of parameters, see <ref> [10, 11, 12] </ref> 2 : b E test = p + N eff E train ; (11) With the above tool we can obtain a generalization error estimate for each pruned network. By selecting the network with the lowest estimated generalization error we obtain a stopping criterion for pruning. <p> To further illustrate the role of adaptive regularization in the Designer Net framework we present tentative results on a standard problem of nonlinear dynamics, viz. the Mackey-Glass chaotic time series. This forecasting problem was previously studied in <ref> [12] </ref>. The Mackey-Glass attractor is a non-linear chaotic system described by the following equation: dz (t) = bz (t) + a 1 + z (t t ) 10 (15) where the constants are a = 0:2, b = 0:1 and t = 17. <p> After the initial training phase, further pruning and adaptation took place with pruning of 2% of the remaining weights per retraining round (400 epochs). In line with <ref> [12] </ref> it is seen that the stop criterion is able to select the optimal network. <p> In figure 2 the normalized training errors, test errors cf. eq. (16), and the corresponding FPE error (after the initial training phase) are sketched for a training set size of 500 examples and the test set comprises 8500 examples. In <ref> [12] </ref> we is based on eq. (11). The vertical line indicates the network for which the estimated test error is minimal. compared the performances of pruned networks with those of fully connected nets, a linear model, and with a K-nearest-neighbor linear model. <p> The very low value of the input-to-hidden weight decay for the small networks is also in line with our earlier observations, namely that one can retrain the optimal architecture without weight decay and get slightly improved generalization <ref> [11, 12] </ref>. CONCLUSION A scheme has been derived for adaptation of weight decay parameters. The scheme is based on asymptotic sampling theory. Two examples were given to illustrate the virtues of such adaptation.
References-found: 12

