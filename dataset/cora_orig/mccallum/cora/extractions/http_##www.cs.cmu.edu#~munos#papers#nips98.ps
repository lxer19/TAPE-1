URL: http://www.cs.cmu.edu/~munos/papers/nips98.ps
Refering-URL: http://www.cs.cmu.edu/~munos/papers.html
Root-URL: 
Email: E-mail:fmunos, awmg@cs.cmu.edu  
Title: Barycentric Interpolators for Continuous Space Time Reinforcement Learning  
Author: Remi Munos Andrew Moore 
Keyword: Category Reinforcement Learning and Control Preference oral presentation  
Address: Pittsburgh, PA 15213, USA.  
Affiliation: Robotics Institute, Carnegie Mellon University  
Abstract: In order to find the optimal control of continuous state-space and time reinforcement learning (RL) problems, we approximate the value function (VF) with a particular class of functions called the barycentric interpolators. We establish sufficient conditions under which a RL algorithm converges to the optimal VF, even when we use approximate models of the state dynamics and the reinforce ment functions.
Abstract-found: 1
Intro-found: 1
Reference: [AMS97] <author> C. G. Atkeson, A. W. Moore, and S. A. Schaal. </author> <title> Locally Weighted Learning. </title> <journal> AI Review, </journal> <volume> 11 </volume> <pages> 11-73, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: For example, if we use nearest neighbor for our dynamics learning, we need to ensure enough data so that every observation is O (ffi 2 ) from its nearest neighbor. If we use local regression, then a mere O (ffi) density is all that is required <ref> [Omo87, AMS97] </ref>. 6 PROOF OF THE CONVERGENCE RESULT 6.1 Description of the approximation scheme We use a convergent scheme derived from Kushner (see [Kus90]) in order to approximate the continuous control problem by a finite MDP.
Reference: [Bar94] <author> Guy Barles. </author> <title> Solutions de viscosite des equations de Hamilton-Jacobi, volume 17 of Mathematiques et Applications. </title> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: The following hypothesis is a sufficient condition for V to be continuous within O (see <ref> [Bar94] </ref>) and is required for proving the convergence result of the next section.
Reference: [BS91] <author> Guy Barles and P.E. Souganidis. </author> <title> Convergence of approximation schemes for fully nonlinear second order equations. Asymptotic Analysis, </title> <booktitle> 4 </booktitle> <pages> 271-283, </pages> <year> 1991. </year>
Reference-contexts: from the definition (7) of , F ffi fi fl from which we deduce that the scheme F ffi is consistent : in a formal sense, lim sup ffi!0 ffi jF ffi [W ](x) W (x)j ~ H (W; DW; x) (15) and obtain, from the general convergence theorem of <ref> [BS91] </ref> (and a result of strong unicity obtained from hyp.1), the convergence of the scheme : V ffi ! V as ffi ! 0. 6.2 Use of the "weak contraction" result of convergence Since in the RL approach used here, we only have an approximation n , t n , ...
Reference: [Dav96] <author> Scott Davies. </author> <title> Multidimensional triangulation and interpolation for reinforcement learning. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 8, </volume> <year> 1996. </year>
Reference-contexts: Remark 2 Depending on the interpolation method we use, the time needed for computing the values will vary. Following <ref> [Dav96] </ref>, the continuous multi-linear interpolation must process 2 d values, whereas the linear continuous interpolation inside a simplex processes (d + 1) values in O (d log d) time. In comparison to [Gor95], the functions used here are averagers that satisfy the barycentric interpolation property (1).
Reference: [FS93] <author> Wendell H. Fleming and H. Mete Soner. </author> <title> Controlled Markov Processes and Viscosity Solutions. Applications of Mathematics. </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Dynamic Programming (DP) that introduces the value function (VF) : the maximal value of J as a function of initial state x : V (x) = sup J (x; u (:)): From the DP principle, we deduce that V satisfies a first-order differential equation, called the Hamilton-Jacobi-Bellman (HJB) equation (see <ref> [FS93] </ref> for a survey) : Theorem 1 If V is differentiable at x 2 O, let DV (x) be the gradient of V at x, then the following HJB equation holds at x.
Reference: [Gor95] <author> G. Gordon. </author> <title> Stable function approximation in dynamic programming. </title> <booktitle> International Conference on Machine Learning, </booktitle> <year> 1995. </year>
Reference-contexts: Following [Dav96], the continuous multi-linear interpolation must process 2 d values, whereas the linear continuous interpolation inside a simplex processes (d + 1) values in O (d log d) time. In comparison to <ref> [Gor95] </ref>, the functions used here are averagers that satisfy the barycentric interpolation property (1).
Reference: [Kus90] <author> Harold J. Kushner. </author> <title> Numerical methods for stochastic control problems in continuous time. </title> <journal> SIAM J. Control and Optimization, </journal> <volume> 28 </volume> <pages> 999-1048, </pages> <year> 1990. </year>
Reference-contexts: If we use local regression, then a mere O (ffi) density is all that is required [Omo87, AMS97]. 6 PROOF OF THE CONVERGENCE RESULT 6.1 Description of the approximation scheme We use a convergent scheme derived from Kushner (see <ref> [Kus90] </ref>) in order to approximate the continuous control problem by a finite MDP.
Reference: [Mun97] <author> Remi Munos. </author> <title> A convergent reinforcement learning algorithm in the continuous case based on a finite difference method. </title> <booktitle> International Joint Conference on Artificial Intelligence, </booktitle> <year> 1997. </year>
Reference-contexts: This point is important in the RL framework since this allows on-line improvement of the model of the state dynamics and the reinforcement functions. Remark 4 This result extends the previous results of convergence obtained by Finite-Element or Finite-Difference methods (see <ref> [Mun97] </ref>).
Reference: [Mun98] <author> Remi Munos. </author> <title> A general convergence theorem for reinforcement learning in the continuous case. </title> <booktitle> European Conference on Machine Learning, </booktitle> <year> 1998. </year>
Reference: [Omo87] <author> S. M. Omohundro. </author> <title> Efficient Algorithms with Neural Network Behaviour. </title> <journal> Journal of Complex Systems, </journal> <volume> 1(2) </volume> <pages> 273-347, </pages> <year> 1987. </year>
Reference-contexts: For example, if we use nearest neighbor for our dynamics learning, we need to ensure enough data so that every observation is O (ffi 2 ) from its nearest neighbor. If we use local regression, then a mere O (ffi) density is all that is required <ref> [Omo87, AMS97] </ref>. 6 PROOF OF THE CONVERGENCE RESULT 6.1 Description of the approximation scheme We use a convergent scheme derived from Kushner (see [Kus90]) in order to approximate the continuous control problem by a finite MDP.
References-found: 10

