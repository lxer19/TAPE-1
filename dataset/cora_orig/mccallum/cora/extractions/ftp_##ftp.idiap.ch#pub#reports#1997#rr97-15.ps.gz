URL: ftp://ftp.idiap.ch/pub/reports/1997/rr97-15.ps.gz
Refering-URL: http://www.idiap.ch/~perry/allpubs.html
Root-URL: http://www.idiap.ch/~perry/allpubs.html
Title: E  IDIAP Martigny Valais Suisse Handwritten Digit Recognition with Binary Optical Perceptron  
Author: E R H E R R P I. Saxena a P. Moerland b E. Fiesler a A. Pourzand c 
Note: internet  published in Proceedings of the International Conference on Artificial Neural Networks (ICANN'97), Lausanne, Switzerland,  a Physical Optics Corporation, Gramercy Place, Torrance,  
Address: I  P.O.Box 592 Martigny Valais Switzerland  CA 90501, U.S.A b IDIAP, CP 592, CH-1920 Martigny, Switzerland.  Neuch^atel, CH-2000 Neuch^atel, Switzerland  
Affiliation: I  for Perceptual Artificial Intelligence  c IMT, University of  
Pubnum: IDIAP-RR 97-15  
Email: e-mail secretariat@idiap.ch  E-mail: Perry.Moerland@idiap.ch  
Phone: phone +41 27 721 77 11 fax +41 27 721 77 12  
Date: May 97  October 1997, 1253-1258  
Web: http://www.idiap.ch  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Abramson, D. Saad, and E. Marom, </author> <title> "Training a Neural Network with Ternary Weights Using the CHIR Algorithm," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(6), </volume> <pages> 997-1000, </pages> <year> (1993). </year>
Reference-contexts: 1 Introduction In hardware implementations of neural networks, it is attractive to use binary weights and inputs. In electronic implementations, reduction of chip area, reduced computation and improved system speed drive the motivation to enable the use of binary or a minimum number of discrete weights <ref> [1] </ref>. Therefore, training algorithms are sought that enable to successfully implement binary level devices in neural networks. Optical neural network implementations can benefit from the faster speed inherent in the free space propagation of information as light.
Reference: [2] <author> S. E. Broomfield, M. A. A. Neil, and E. G. S. Paige, </author> <title> "Programmable multiple-level phase modulation that uses ferroelectric liquid crystal spatial light modulation," </title> <journal> Applied Optics, </journal> <volume> 34(29), </volume> <month> 6652-6665 </month> <year> (1995). </year>
Reference-contexts: Typically nematic liquid crystal based devices are used which have frame rates of 50Hz and non-linear responses which allow several gray level weights and/or inputs to be implemented. On the other hand, higher frame rates several orders of magnitude faster, up to a few 100 kHz <ref> [2] </ref> are offered by emerging ferro-electric liquid-crystal devices which have a limited number of (binary) weighting levels.
Reference: [3] <author> N. Collings, A. R. Pourzand, and R. Volkel, </author> <title> "Construction of a programmable multilayer analogue neural network using space invariant interconnects," </title> <booktitle> in SPIE Vol. </booktitle> <volume> 2565, </volume> <pages> pp. </pages> <month> 40-47 </month> <year> (1995). </year>
Reference-contexts: first matrix-vector multiplier or MVM takes as input a 16*16 array of 4-bit precision numbers, generates 16*16 copies of the input array, and multiplies each copy by a 16*16 array of 4-bit weight values on a point-by-point basis employing one pixel of the weight screen, the LCTV2, per interconnect channel <ref> [3] </ref>. The chosen LCTV screens used for these results have a complexity of 480*440 pixels (Seiko Epson video projector, model #VPJ-2000), which are, sufficient for the full (16*16) complexity multiplier except for the problem of exact pixel address.
Reference: [4] <author> E. Fiesler, A. Choudry, and H. J. Caulfield, </author> <title> "A Weight Discretization Paradigm for Optical Neural Networks," </title> <booktitle> in Proceedings of the International Congress on Optical Science and Engineering, </booktitle> <volume> vol. SPIE 1281, </volume> <pages> pp. 164-173, </pages> <address> SPIE, Bellingham, Washington (1990). </address>
Reference-contexts: The possibility of subtraction has therefore been included in our training algorithm whereby all positive weights are implemented by including a subtractive-feedback to enable useful optical neural networks [7]. The training of the optical neural network is performed by the weight discretization and update algorithm described in <ref> [4] </ref>. 3 Experiment and Results Input digit 0 1 2 3 4 5 6 7 8 9 Pattern Computed 1.0 0.65 0.79 0.89 0.72 0.53 0.82 0.77 0.75 0.56 Optical: Gray-scale 1.0 0.83 0.94 0.86 0.81 0.56 0.74 0.72 0.72 0.48 Optical: Binary 1.0 0.91 0.32 0.55 0.76 0.64 0.79 0.82
Reference: [5] <author> M. D. Garris and R. A. Wilkinson, </author> <title> data from NIST Special Database 3, </title> <institution> National Institute of Standards and Technology (1992). </institution>
Reference-contexts: Training of the perceptron using the training algorithm (including weight discretization, compensation for the LCLV non-linearities, and subtractive feedback for all-positive weights) described in section 2.2 was simulated on a computer. A subset of 50 digits from the handwritten digit data set of the NIST database <ref> [5] </ref> was used to train the perceptron and the resultant weights were implemented on the LCTV of the optical perceptron for testing recall with only 10 of the 50 digits.
Reference: [6] <author> P. Moerland, E. Fiesler, and I. Saxena, </author> <title> "Incorporation of Liquid-Crystal Light Valve Non-Linearities in Optical Multilayer Neural Networks," </title> <journal> Applied Optics, </journal> <volume> 35(26), </volume> <month> 5301-5307 </month> <year> (1996). </year> <editor> P. Moerland, E. Fiesler, and I. </editor> <publisher> Saxena. </publisher>
Reference: [7] <author> P. Moerland, E. Fiesler, and I. Saxena, </author> <title> "Discrete All-Positive Multilayer Perceptrons for Optical Implementations," </title> <address> IDIAP-RR 97-02, IDIAP, Martigny, Switzerland, </address> <note> February 1997 (accepted for publication in Optical Engineering). </note>
Reference-contexts: optimize the light budget is prevented here as it results in a reduction of the contrast ratio as compared to that with a collimated read beam. 2.2 Training Algorithm Optimization There are three important aspects of algorithm development that we have addressed which now allow optical multilayer neural network implementations <ref> [7] </ref>. <p> The possibility of subtraction has therefore been included in our training algorithm whereby all positive weights are implemented by including a subtractive-feedback to enable useful optical neural networks <ref> [7] </ref>.
Reference: [8] <author> M. G. Robinson and K. M. Johnson, </author> <title> "Noise Analysis of Polarization-Based Optoelectronic Connectionist Machines," </title> <journal> Applied Optics, </journal> <volume> 31(2), </volume> <month> 263-272 </month> <year> (1992). </year>
Reference-contexts: These concern the questions on the usefulness of the non-linearities offered by LCLVs, of implementing the well-known only-positive-weights limitation of incoherent optical processing systems, and determining the (minimum) number of possible weight discretization levels for a particular optical neural network to overcome analog hardware non-idealities as implemented by LCTVs <ref> [8] </ref>. Empirical studies on all-positive neural networks without the possibility of subtraction (of bias thresholds, for instance) strongly indicate that such networks are not useful (due to monotonicity).
Reference: [9] <author> I. Saxena, P. Moerland, E. Fiesler, A. R. Pourzand, N. Collings, </author> <title> "An Optical Thresholding Perceptron," </title> <booktitle> in Proceedings of the Workshop on Optics and Computer Science, </booktitle> <address> Geneva, Switzerland (1997). </address>
Reference-contexts: The limitations that exist in 2-D electronic implementations are further overcome in 3-dimensional free space optical interconnections, allowing much higher intercon-nectivity. In optical implementations, where inputs and weights are implemented by liquid crystal television (LCTV) screens, and non-linear thresholding by LCLVs (for example, <ref> [9] </ref>), the processing speed is limited by the liquid crystal response time. Typically nematic liquid crystal based devices are used which have frame rates of 50Hz and non-linear responses which allow several gray level weights and/or inputs to be implemented. <p> These two areas affecting implementation and whether they can be consolidated in a useful neural network application is explored here from the perspective of binary level devices. This is illustrated by some experimental results of recall on a optical perceptron, based on an optical system described earlier <ref> [9] </ref>. <p> and Training Algorithm The optical multilayer perceptron is based mostly on available hardware with the input layer and programmable weights for the interconnects between layers being implemented by liquid crystal television (LCTV) screens and the hidden layer and the output layer by liquid crystal light valves (LCLVs), as described in <ref> [9] </ref>.
Reference: [10] <author> G. Thimm, P. Moerland, and E. Fiesler, </author> <title> "The Interchangeability of Learning Rate and Gain in Backpropagation Neural Networks," </title> <journal> Neural Computation, </journal> <volume> 8(2), </volume> <month> 451-460 </month> <year> (1996). </year>
References-found: 10

