URL: http://www.csl.sri.com/reports/postscript/zum95.ps.gz
Refering-URL: http://www.csl.sri.com/reports/postscript/
Root-URL: 
Title: Mechanizing Formal Methods: Opportunities and Challenges  
Author: John Rushby 
Address: Menlo Park, CA 94025, USA  
Affiliation: Computer Science Laboratory, SRI International,  
Date: September 1995  
Note: Reprint of an invited paper presented at the 9th International Conference of Z Users (ZUM '95, Springer Verlag LNCS 967, pp. 105-113), Limerick, Ireland,  
Abstract: Mechanization makes it feasible to calculate properties of formally specified systems. This ability creates new opportunities for using formal methods as an exploratory tool in system design. Achieving enough efficiency to make this practical raises challenging problems in automated deduction. These challenges can be met only by approaches that integrate consideration of its mechanization into the design of a specification language.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. S. Boyer and J S. Moore. </author> <title> Integrating decision procedures into heuristic theorem provers: A case study with linear arithmetic. </title> <booktitle> In Machine Intelligence, </booktitle> <volume> volume 11. </volume> <publisher> Oxford University Press, </publisher> <year> 1986. </year>
Reference-contexts: As this description suggests, rewriting and decision procedures cannot stand apart: truly effective theorem provers must integrate them very tightly. A classic account of the issues in such integration is given by Boyer and Moore <ref> [1] </ref>. Integration is a pervasive theme in the effective mechanization of formal methods: many individual techniques work well on selected examples, but fail in more realistic contexts because problems seldom fall exactly within the scope of one method.
Reference: [2] <author> Alonzo Church. </author> <title> Introduction to Mathematical Logic, </title> <booktitle> volume 1 of Princeton Mathematical Series. </booktitle> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1956. </year> <note> Volume 2 never appeared. </note>
Reference-contexts: (notably, first-order logic with axiomatic set theory) were created in order to be studied, not in order to be used|the": : : interest in formalized languages being less often in their actual and practical use as languages than in the general theory of such use and its possibilities in principle" <ref> [2, page 47] </ref>. Unsurprisingly, therefore, set theory has characteristics that pose difficulty for mechanization|for example, as already noted, functions are inherently partial in set theory (they are sets of pairs).
Reference: [3] <author> David L. Dill, Andreas J. Drexler, Alan J. Hu, and C. Han Yang. </author> <title> Protocol verification as a hardware design aid. </title> <booktitle> In 1992 IEEE International Conference on Computer Design: VLSI in Computers and Processors, </booktitle> <pages> pages 522-525. </pages> <publisher> IEEE Computer Society, </publisher> <address> 1992. Cambridge, MA, </address> <month> October 11-14. 9 </month>
Reference-contexts: Experience indicates that examining all the cases of such an abstracted system description is generally more effective at finding faults than testing or simulating some of the cases of the full description <ref> [3] </ref>. Once a preferred system architecture has been selected we may, recursively, explore architectures for its components or|once a sufficiently detailed level has been reached|investigate algorithms for those components.
Reference: [4] <author> L. Lamport and P. M. Melliar-Smith. </author> <title> Synchronizing clocks in the presence of faults. </title> <journal> Journal of the ACM, </journal> <volume> 32(1) </volume> <pages> 52-78, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: As with the earlier stages, the great benefits of using mechanized formal methods to examine algorithms are the abilities to explore alternatives, to prune assumptions, and to adapt to design changes. For example, the journal presentation of the interactive convergence clock synchronization algorithm <ref> [4] </ref> has an assumption that all initial clock adjustments are zero. Friedrich von Henke and I retained this assumption when we formally verified the algorithm [13]. Subsequently, when contemplating design of a circuit to implement part of the algorithm, it became clear that this assumption is exceedingly 4 inconvenient.
Reference: [5] <author> Leslie Lamport, Robert Shostak, and Marshall Pease. </author> <title> The Byzantine Generals problem. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3) </volume> <pages> 382-401, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: For example, formal examination of an architecture for fault masking and transient recovery in flight control systems reveals the need for interactive consistency on sensor inputs [11]. This can be achieved by a Byzantine Agreement algorithm <ref> [5] </ref>. Inputs to the majority vote function must also satisfy interactive consistency and it may therefore appear as if these, too, need to be run through a Byzantine Agreement algorithm.
Reference: [6] <author> Patrick Lincoln and John Rushby. </author> <title> A formally verified algorithm for interactive consistency under a hybrid fault model. </title> <booktitle> In Fault Tolerant Computing Symposium 23, </booktitle> <pages> pages 402-411, </pages> <address> Toulouse, France, June 1993. </address> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: The ability to make these enhancements to complex algorithms, rapidly and reliably, is an opportunity created by mechanized formal methods. Informal methods of proof are unreliable in these domains (see <ref> [6, 9, 13] </ref> for examples) and it requires superhuman discipline to bring the same level of care and skepticism to the scrutiny of a modified algorithm as to the original.
Reference: [7] <author> Patrick Lincoln and John Rushby. </author> <title> Formal verification of an interactive consistency algorithm for the Draper FTP architecture under a hybrid fault model. </title> <booktitle> In COMPASS '94 (Proceedings of the Ninth Annual Conference on Computer Assurance), </booktitle> <pages> pages 107-120, </pages> <address> Gaithersburg, MD, </address> <month> June </month> <year> 1994. </year> <institution> IEEE Washington Section. </institution>
Reference-contexts: In another example, Lincoln and I developed the formal specification and verification of a Byzantine Agreement algorithm for an asymmetric architecture in less than a day by modifying an existing treatment for a symmetric architecture <ref> [7] </ref>. The ability to make these enhancements to complex algorithms, rapidly and reliably, is an opportunity created by mechanized formal methods.
Reference: [8] <author> Robyn R. Lutz. </author> <title> Analyzing software requirements errors in safety-critical embedded systems. </title> <booktitle> In IEEE International Symposium on Requirements Engineering, </booktitle> <pages> pages 126-133, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: development and quality assurance of program code and detailed hardware designs are sufficiently effective that very few significant faults are introduced and remain undetected at these late stages of the lifecycle (for example, of 197 critical faults found during integration testing of two JPL spacecraft, only three were programming mistakes <ref> [8] </ref>), and only a small fraction of overall development costs (typically, less than 10%) are incurred here. In contrast, mechanized formal methods are quite expensive to apply at these stages. <p> Lutz reports that 50% of these faults were due to flawed requirements (mainly omissions) for individual components, 25% were due to flawed designs for these components, and the remaining 25% were due to flawed interfaces between components and incorrect interactions among them <ref> [8] </ref>. Rapid prototyping and simulation provide more repeatable and systematic examination of these issues, but often force premature consideration of implementation questions and thereby divert attention from the most important topics.
Reference: [9] <author> Sam Owre, John Rushby, Natarajan Shankar, and Friedrich von Henke. </author> <title> Formal verification for fault-tolerant architectures: Prolegomena to the design of PVS. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(2) </volume> <pages> 107-125, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: In the following sections, I will briefly describe the main opportunities created by mechanized formal methods, and the technical challenges in achieving effective mechanization. My perspective on these topics is influenced by experiences in the development and use of PVS <ref> [9] </ref>. 2 Opportunities for Making Effective Use of Mechanized Formal Methods It is often assumed that the main goal of mechanized formal methods is "proving correctness" of programs or detailed hardware designs, and this assumption may be reinforced by the term "verification system" that is commonly used to describe mechanized tools <p> The ability to make these enhancements to complex algorithms, rapidly and reliably, is an opportunity created by mechanized formal methods. Informal methods of proof are unreliable in these domains (see <ref> [6, 9, 13] </ref> for examples) and it requires superhuman discipline to bring the same level of care and skepticism to the scrutiny of a modified algorithm as to the original.
Reference: [10] <author> S. Rajan, N. Shankar, </author> <title> and M.K. Srivas. An integration of model-checking with automated proof checking. </title> <editor> In Pierre Wolper, editor, </editor> <booktitle> Computer-Aided Verification, CAV '95, volume 939 of Lecture Notes in Computer Science, </booktitle> <pages> pages 84-97, </pages> <address> Liege, Belgium, June 1995. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: additional benefits of a system that provides both theorem proving and model checking are that model checking can be used to discharge some cases of a larger proof or, dually, that theorem proving can be used to justify the reduction to finite state that is required for automated model checking <ref> [10] </ref>. Model checking can provide a further benefit: before undertaking a potentially difficult and costly proof, we may be able to use model checking to examine some restricted or special cases. Any errors that can be discovered and eliminated in this way will save time and effort in theorem proving.
Reference: [11] <author> John Rushby. </author> <title> A fault-masking and transient-recovery model for digital flight-control systems. </title> <editor> In Jan Vytopil, editor, </editor> <booktitle> Formal Techniques in Real-Time and Fault-Tolerant Systems, Kluwer International Series in Engineering and Computer Science, chapter 5, </booktitle> <pages> pages 109-136. </pages> <publisher> Kluwer, </publisher> <address> Boston, Dordecht, London, </address> <year> 1993. </year> <note> An earlier version appeared in [16, pp. 237-257]. </note>
Reference-contexts: For example, formal examination of an architecture for fault masking and transient recovery in flight control systems reveals the need for interactive consistency on sensor inputs <ref> [11] </ref>. This can be achieved by a Byzantine Agreement algorithm [5]. Inputs to the majority vote function must also satisfy interactive consistency and it may therefore appear as if these, too, need to be run through a Byzantine Agreement algorithm. <p> Mechanized formal analysis allows this attribute of the architecture to be determined with certainty, and it also allows determination of the exact circumstances under which a modified architecture provides recovery from transient faults <ref> [11] </ref>. As with requirements validation, exploration of architectural design choices is an iterative process that is greatly facilitated by the rigor and repeatability of mechanized formal methods.
Reference: [12] <author> John Rushby. </author> <title> A formally verified algorithm for clock synchronization under a hybrid fault model. </title> <booktitle> In Thirteenth ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 304-313, </pages> <address> Los Angeles, CA, </address> <month> August </month> <year> 1994. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: In other revisions to this algorithm and its verification, I have tightened the bound on the achieved clock skew, and extended the fault model so that the algorithm tolerates larger numbers of simple faults, without compromising its ability to resist arbitrary (i.e., Byzantine) faults <ref> [12] </ref>. In each case, the effort required to investigate the proposed revision and to rework the formal specification and verification was on the order of a day or two.
Reference: [13] <author> John Rushby and Friedrich von Henke. </author> <title> Formal verification of algorithms for critical systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 19(1) </volume> <pages> 13-23, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: For example, the journal presentation of the interactive convergence clock synchronization algorithm [4] has an assumption that all initial clock adjustments are zero. Friedrich von Henke and I retained this assumption when we formally verified the algorithm <ref> [13] </ref>. Subsequently, when contemplating design of a circuit to implement part of the algorithm, it became clear that this assumption is exceedingly 4 inconvenient. <p> The ability to make these enhancements to complex algorithms, rapidly and reliably, is an opportunity created by mechanized formal methods. Informal methods of proof are unreliable in these domains (see <ref> [6, 9, 13] </ref> for examples) and it requires superhuman discipline to bring the same level of care and skepticism to the scrutiny of a modified algorithm as to the original.
Reference: [14] <author> Robert E. Shostak. </author> <title> An algorithm for reasoning about equality. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 583-585, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: For example, reasoning about equality in the presence of uninterpreted fl This work was partially supported by the Air Force Office of Scientific research under contract F49620-95-C0044. function symbols is crucial to most applications of mechanized formal methods, and efficient techniques for achieving this (such as congruence closure <ref> [14] </ref>) require that functions are total. However, it is draconian and rather unnatural to force all functions (including, for example, division) to be total, so an effectively mechanized formal method requires careful and integrated design choices to be made for both the specification language and its supporting mechanization.
Reference: [15] <author> Robert E. Shostak. </author> <title> Deciding combinations of theories. </title> <journal> Journal of the ACM, </journal> <volume> 31(1) </volume> <pages> 1-12, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: Since function symbols are pervasive in formal specifications, a better choice than true Presburger arithmetic is the theory of ground (i.e., unquantified) linear arithmetic, which can be combined with the theory of equality over uninterpreted function symbols <ref> [15] </ref>. Small extensions to decidable theories can have considerable value.
Reference: [16] <editor> J. Vytopil, editor. </editor> <booktitle> Formal Techniques in Real-Time and Fault-Tolerant Systems, volume 571 of Lecture Notes in Computer Science, </booktitle> <address> Nijmegen, The Netherlands, </address> <month> January </month> <year> 1992. </year> <note> Springer-Verlag. 10 </note>
References-found: 16

