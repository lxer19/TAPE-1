URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR97695-S.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~roth/papers.html
Root-URL: 
Title: Optimizing Fortran90D/HPF for Distributed-Memory Computers  
Author: by Gerald H. Roth 
Degree: A Thesis Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Approved, Thesis Committee: Ken Kennedy, Noah Harding Professor Computer Science John Mellor-Crummey, Faculty Fellow Computer Science William W. Symes, Professor Computational and Applied Mathematics R. Gregg Brickner, Technical Staff Member Los Alamos  
Date: April, 1997  
Address: Houston, Texas  
Affiliation: RICE UNIVERSITY  National Laboratory  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. Abu-Sufah. </author> <title> Improving the Performance of Virtual Memory Computers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1979. </year>
Reference-contexts: The fusion of loops, however, is not always safe. A data dependence between two adjacent loops is called fusion-preventing if after fusion the direction of the dependence is reversed <ref> [1, 154] </ref>. The existence of such a dependence means that fusion is not safe. In our current model however, no such fusion-preventing dependences can exist between adjacent scalarized loops. This is due to the fact that the generation of communication causes all subgrid loops to operate on "perfectly aligned" data.
Reference: [2] <author> J. Adams, W. Brainerd, J. Martin, B. Smith, and J. Wagener. </author> <title> Fortran 90 Handbook. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Other array-related features of Fortran90 include allocatable arrays, pointers, and the where statement. For more information on these or any of the above mentioned constructs, the reader is referred to an appropriate text <ref> [2, 127] </ref>. 10 2.3 Fortran D and High Performance Fortran Fortran D and High Performance Fortran (HPF) are versions of Fortran that have been designed to assist both the programmer and compiler in producing efficient data-parallel programs for distributed-memory machines.
Reference: [3] <author> V. Adve, J. Mellor-Crummey, and A. Sethi. </author> <title> HPF analysis and code generation using integer sets. </title> <type> Technical Report CS-TR97-275, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: In Figure 7.4 (a) we see two simple Fortran90 array assignment statements which operate on arrays with identical iteration spaces, but have different distributions. In Figure 7.4 (b) we see the loops that would result from a naive scalarization. The Rice dHPF compiler <ref> [3] </ref> generates the code shown in machine. Since the scalarized loops have the same number of iterations and do not have any fusion-preventing dependences, many parallel compilers that support loop fusion would fuse the two loops into a single loop, such as seen in Figure 7.4 (d).
Reference: [4] <author> T. Agerwala, J. Martin, J. Mirza, D. Sadler, D. Dias, and M. Snir. </author> <title> SP2 system architecture. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 152-184, </pages> <year> 1995. </year>
Reference-contexts: This network allows messages to be sent from one PE to another. These messages can be used to allow PEs to share data with one another. The details of such a network are not important to this dissertation. Examples of MIMD machines include the SP2 from IBM <ref> [4] </ref>, and the Paragon from Intel [97, 67]. See Figure 2.1 for a simple schematic of such a machine. 2.1.2 Distributed-Memory SIMD Architectures Distributed-memory SIMD machines are quite similar to MIMD machines in that they consist of an array of PEs connected by a communication network.
Reference: [5] <author> A. V. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: Analyzing a program's data flow can provide information on reaching definitions, available expressions, and live variables, which in turn can enable optimizations such as global common subexpression elimination, loop invariant code motion, strength reduction, and global register allocation <ref> [5] </ref>. Data-flow analysis and associated optimizations have been standard components of optimizing compilers for many years. In this section we discuss analyzing data-flow through Fortran90 array statements.
Reference: [6] <author> A. Aiken and A. Nicolau. </author> <title> Loop quantization: An analysis and algorithm. </title> <type> Technical Report 87-821, </type> <institution> Dept. of Computer Science, Cornell University, </institution> <month> March </month> <year> 1987. </year>
Reference-contexts: This transformation is called scalar replacement and is described in detail elsewhere [43]. 9.6.3 Unroll-And-Jam Unroll-and-jam is a transformation that can be used in conjunction with scalar replacement to improve the performance of many memory-bound loops <ref> [6, 10, 44] </ref>. The transformation unrolls an outer loop and then fuses the resulting inner loops back together. Using unroll-and-jam, more computation can be introduced into an innermost loop body without a proportional increase in memory references.
Reference: [7] <author> E. Albert, K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Compiling Fortran 8x array features for the Connection Machine computer system. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: This strategy is used in the majority of Fortran90D/HPF compilers for SIMD architectures [123, 140, 149], many of which are descendents of compiler technology developed by Compass <ref> [7, 9, 108, 109, 110] </ref>, and is also employed by some MIMD compilers [28, 32]. We refer to such compilers as array operation compilers or native Fortran90 compilers. The main phases of a compiler exploiting this model are depicted in Figure 2.4. <p> The compilers they produced would be classified as array operation compilers as described in the preceding chapter. The group at Compass, along with their associates at Thinking Machines Corp., were the first to investigate the challenges of compiling Fortran90-style data-parallel constructs for execution on distributed-memory machines <ref> [7, 8, 9, 110] </ref>. Together they created what many would consider to be the first, commercially viable, distributed-memory compiler. In addition to the general SIMD compiler development effort, Compass did much of the ground-breaking research in the area of data optimization [108, 109, 107, 111, 122]. <p> The compiler uses the distribution functions discussed in Section 4.2 to determine ownership. The owner computes rule could be replaced by a data optimization phase, which may determine an alternate distribution for the computation and associated intermediate results, in an attempt to reduce the amount of communication <ref> [7, 56, 109, 119] </ref>. 4.5 Communication Generation Once data and computation distributions are finalized, the compiler must insert any necessary communication operations. These are required to move data so that all operands of an expression reside on the PE which performs the computation. <p> The new split version reduced the execution time of the hand-optimized MPL version by 12%, compared to the 13% reduction of the original split version. 101 7.6 Related Work Work at Compass by Albert, et al., describes the generation and optimization of context setting code <ref> [7] </ref>. They avoid redundant context computations when adjacent statements operate under the same context. They also perform classical optimizations on the context expressions, such as common subexpression elimination. They mention the possibility of reordering computations to minimize context changes, but they do not discuss such transformations.
Reference: [8] <author> E. Albert, J. Lukas, and G. Steele, Jr. </author> <title> Data parallel computers and the forall statement. </title> <booktitle> In Frontiers '90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: The compilers they produced would be classified as array operation compilers as described in the preceding chapter. The group at Compass, along with their associates at Thinking Machines Corp., were the first to investigate the challenges of compiling Fortran90-style data-parallel constructs for execution on distributed-memory machines <ref> [7, 8, 9, 110] </ref>. Together they created what many would consider to be the first, commercially viable, distributed-memory compiler. In addition to the general SIMD compiler development effort, Compass did much of the ground-breaking research in the area of data optimization [108, 109, 107, 111, 122].
Reference: [9] <author> E. Albert, J. Lukas, and G. Steele, Jr. </author> <title> Data parallel computers and the forall statement. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(2) </volume> <pages> 185-192, </pages> <month> Oc-tober </month> <year> 1991. </year>
Reference-contexts: This strategy is used in the majority of Fortran90D/HPF compilers for SIMD architectures [123, 140, 149], many of which are descendents of compiler technology developed by Compass <ref> [7, 9, 108, 109, 110] </ref>, and is also employed by some MIMD compilers [28, 32]. We refer to such compilers as array operation compilers or native Fortran90 compilers. The main phases of a compiler exploiting this model are depicted in Figure 2.4. <p> The compilers they produced would be classified as array operation compilers as described in the preceding chapter. The group at Compass, along with their associates at Thinking Machines Corp., were the first to investigate the challenges of compiling Fortran90-style data-parallel constructs for execution on distributed-memory machines <ref> [7, 8, 9, 110] </ref>. Together they created what many would consider to be the first, commercially viable, distributed-memory compiler. In addition to the general SIMD compiler development effort, Compass did much of the ground-breaking research in the area of data optimization [108, 109, 107, 111, 122].
Reference: [10] <author> F. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <editor> In J. Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: Such programs exhibit poor temporal data locality and a high loop-overhead/computation ratio. To address these problems our compiler uses loop fusion <ref> [10] </ref> to merge multiple loop nests covering the same iteration space into a single loop nest. The fusion of loops, however, is not always safe. A data dependence between two adjacent loops is called fusion-preventing if after fusion the direction of the dependence is reversed [1, 154]. <p> It is useful for Fortran90 compilers that target MIMD architectures as well. When compiling for MIMD machines, in fact any parallel architecture, loop fusion <ref> [10] </ref> is an important optimization as it can enhance the granularity of parallelism and increase the possibility of data reuse while reducing the overhead of parallel loop execution. <p> This transformation is called scalar replacement and is described in detail elsewhere [43]. 9.6.3 Unroll-And-Jam Unroll-and-jam is a transformation that can be used in conjunction with scalar replacement to improve the performance of many memory-bound loops <ref> [6, 10, 44] </ref>. The transformation unrolls an outer loop and then fuses the resulting inner loops back together. Using unroll-and-jam, more computation can be introduced into an innermost loop body without a proportional increase in memory references.
Reference: [11] <author> F. Allen and J. Cocke. </author> <title> A proram data flow analysis procedure. </title> <journal> Communications of the ACM, </journal> <volume> 19 </volume> <pages> 137-147, </pages> <year> 1976. </year> <month> 141 </month>
Reference-contexts: The results of these analyses are used by the optimizing transformations presented in the remainder of this dissertation. 5.2 Data-Flow Analysis Data-flow analysis is a classical analysis technique which tracks the flow of data through the program's variables <ref> [11, 100] </ref>. Analyzing a program's data flow can provide information on reaching definitions, available expressions, and live variables, which in turn can enable optimizations such as global common subexpression elimination, loop invariant code motion, strength reduction, and global register allocation [5].
Reference: [12] <author> J. R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and Its Application to Program Transformations. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: The search for parallelism is simplified by considering only those loop nests which iterate over the distributed dimensions of the arrays. The methods used to detect and exploit data parallelism in sequential code have been well documented <ref> [12, 13, 25, 153, 161, 163] </ref> and will not be discussed any further here. 4.4 Computation Partitioning The next step is to map the parallel operations to the processors. <p> Since each PE is in fact a serial processor, the array expressions must be scalarized; i.e., translated into serial code <ref> [12, 14, 161] </ref>. This process replaces the array expression with a loop nest containing array references with only scalar subscripts. <p> Data dependence is fundamental to compilers that attempt reordering transformations since it specifies statement orderings that must be preserved to maintain program semantics <ref> [12, 161, 166] </ref>. <p> Complexity classes include ziv (zero index variables), siv (single index variable), and miv (multiple index variables). Separability refers to whether or not different subscript positions contain common induction variables. A subscript position is separable if the indices it contains do not appear in other subscript positions <ref> [12, 41] </ref>. If different subscript positions contain the same index, they are said to be coupled [121]. The concept of separability is important when testing multidimensional arrays in that it allows dependence testing to proceed subscript-by-subscript without a loss of precision. <p> To enable the analysis of Fortran90 programs, we have enhanced these operators to handle array sections by incorporating regular section descriptors (RSDs) <ref> [12] </ref>. For instance, the update operator takes three arguments: the array being modified, a section descriptor indicating the affected elements, and the new values to be stored in those locations. Figure 5.5 contains some examples of array section references and their corresponding SSA form. <p> Before proceeding to discuss our strategy, we need to extend the definition of our overlap shift routine (see Section 6.3.2 for the original definition). We add an optional fourth argument that takes a regular section descriptor (RSD) <ref> [12] </ref>. The RSD is used to specify those data elements in the overlap areas of other dimensions are to be transferred along with the specified subgrid elements. This extension allows us to include "corner" elements that are a part of multi-offset arrays.
Reference: [13] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Thus the program 15 must be written using array syntax, array intrinsics, or forall constructs wherever possible. This requires the programmer to translate all sequential code to parallel code, either by hand or using an automated tool <ref> [116, 13] </ref>. But some code sequences containing parallelism are not expressible with these constructs; e.g., pipelined computations. <p> The search for parallelism is simplified by considering only those loop nests which iterate over the distributed dimensions of the arrays. The methods used to detect and exploit data parallelism in sequential code have been well documented <ref> [12, 13, 25, 153, 161, 163] </ref> and will not be discussed any further here. 4.4 Computation Partitioning The next step is to map the parallel operations to the processors. <p> Examples of such optimizations are communication placement [49, 84], message pipelining [136], vector message pipelining [95], and iteration reordering [113]. Improving Parallelism: Recognizing reductions and parallel-prefix scan operations [53, 114] can help to improve the available parallelism. Loop interchange and strip-mining <ref> [13, 160, 162] </ref> can be used to adjust the granularity of pipelined computations to balance parallelism and communication [93]. Storage Management: The use of overlap areas [73] and hash tables can ease the details of buffer management for certain types of computations. <p> An outline of the algorithm is given in Figure 5.1. This algorithm has been used with great success in the PFC compiler <ref> [13] </ref>, the ParaScope programming environment [104], and the Fortran D compiler [93, 152]. 5.3.3 Dependence Representation Data dependences are often represented using direction vectors and/or distance vectors [161]. The direction vector is an ordering vector, containing &lt;, =, &gt;, or fl, that 41 1. <p> In this work we discuss only direction vectors, although the algorithms presented could easily be adapted to work with distance vectors. Direction vectors are useful in determining if a dependence is loop-carried or loop-independent <ref> [13] </ref>. For loop-carried dependences, the direction vector also tells us which loop carries the dependence and in which direction. The vectors contain an element for each loop which encloses both statements involved in the dependence. <p> The algorithm works on the data dependence graph (ddg) [117] which must be acyclic. Since we apply it to a set of statements within a basic block, our dependence graph contains only loop-independent dependences <ref> [13] </ref> and thus meets that criterion.
Reference: [14] <author> J. R. Allen and K. Kennedy. </author> <title> Vector register allocation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(10) </volume> <pages> 1290-1317, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Since each PE is in fact a serial processor, the array expressions must be scalarized; i.e., translated into serial code <ref> [12, 14, 161] </ref>. This process replaces the array expression with a loop nest containing array references with only scalar subscripts. <p> Thus for scalarization dependences, it is no longer the case that a true dependence with a "&gt;" as the first non-"=" direction is equivalent to an antidependence with the direction reversed, as has been previously noted by others <ref> [14, 40] </ref>. By definition, scalarization dependences are loop-independent with regard to surrounding loops. This has several implications. <p> This violates the "load-before-store" semantics of the Fortran90 array assignment statement. Fortunately, data dependence information can tell us when the scalarized loop is correct. Allen and Kennedy <ref> [14] </ref> have shown that a scalarized loop is correct if and DO I=2, N END DO (a) array statement (b) naively scalarized code 104 only if it does not carry a true dependence. Using this fact, most compilers perform scalarization in the following manner: 1. <p> The code transformations that can be applied to handle the loop carried true dependences include loop reversal, loop interchange, prefetching, and as a last resort the generation of array temporaries. The interested reader is referred to Allen and Kennedy <ref> [14] </ref> for a complete discussion. The Allen & Kennedy algorithm requires two passes over the code, one to perform the naive scalarization and another to perform code transformations to restore the semantics of the program if the initial scalarization is invalid.
Reference: [15] <author> B. Alpern, M. Wegman, and K. Zadeck. </author> <title> Detecting equality of variables in programs. </title> <booktitle> In Proceedings of the Fifteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: the triplets have been translated, we exploit whichever multi-subscript test is available in our system [121, 134, 164]. 5.4 Static Single Assignment Form In recent years, Static Single Assignment (SSA) form [64, 65] and related intermediate representations have gained in popularity because of their efficiency in program analysis and transformations <ref> [15, 137, 155] </ref>. SSA is loosely characterized by the trait that each variable has only a single definition. This is achieved by creating a new instance of a variable, typically indicated by a subscript, each time the variable is assigned a new value.
Reference: [16] <author> S. Amarasinghe, J. Anderson, M. Lam, and C.-W. Tseng. </author> <title> An overview of the SUIF compiler for scalable parallel machines. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Fran-cisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: system at Rice University [83, 91, 92, 93, 94, 115, 152], the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign [78, 79, 80, 147, 22, 133], Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University <ref> [16, 145, 157] </ref>. For the rest of this chapter we concentrate solely on projects whose main purpose is the compilation of Fortran90 constructs. Compass Compilers Compass (1961-1991) was an independent software house which was involved in the design and implementation of several SIMD compilers.
Reference: [17] <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: These optimizations fall into several categories: Reducing Communication: Here we perform optimizations that attempt to reduce the amount of communication. These include message vectorization [21, 73], message coalescing [95], message aggregation [120, 131], redundant communication elimination <ref> [17] </ref>, and the exploitation of collective communication [120]. Hiding Communication: These transformations attempt to hide the cost of communication by overlapping communication and computation. Examples of such optimizations are communication placement [49, 84], message pipelining [136], vector message pipelining [95], and iteration reordering [113].
Reference: [18] <author> ANSI X3J3/S8.115. </author> <title> Fortran 90, </title> <month> June </month> <year> 1990. </year>
Reference-contexts: Examples of SIMD machines include the CM-2/CM-200 from Thinking Machines Corporation [90, 148, 150], and the MP-1/MP-2 from MasPar [26, 130, 124]. A simple diagram of a SIMD machine can be seen in Figure 2.2. 9 2.2 Fortran90 Fortran90 <ref> [18] </ref> adds a number of interesting and useful features to the Fortran language, the most popular being the array features. With Fortran90, entire arrays can be referenced by simply referring to the array name no subscripts are necessary. Arrays or subsections of arrays can be specified by using triplet subscripts.
Reference: [19] <author> Applied Parallel Research, </author> <title> Sacramento, CA. Forge High Performance Fortran xhpf User's Guide, </title> <note> version 2.1 edition, </note> <year> 1995. </year>
Reference-contexts: See Figure 2.3 for an outline of a compiler which utilizes this model. This strategy is used by many of the HPF compilers for MIMD architectures <ref> [81, 85, 19] </ref>, and it is not limited to compilers for distributed-memory machines [20]. The advantages of this model are fairly clear. By exploiting an existing Fortran77D compiler, a Fortran90D compiler can be created in a much shorter time span. <p> And finally, the compiler performs enhanced analysis and optimization of the resulting node program, including vectorizing loops for certain architectures. xHPF Applied Parallel Research Inc.'s xHPF translator is the most recent addition to the FORGE90 parallel programming environment <ref> [19, 146, 75] </ref>. The system is identical to the company's xHPF77 system, with the addition of a preprocessor that converts Fortran90 syntax into Fortran77. This structure classifies the compiler as a scalarizing HPF compiler, similar to IBM's xlhpf.
Reference: [20] <author> D. F. Bacon, S. L. Graham, and O. J. Sharp. </author> <title> Compiler transformations for high-performance computing. </title> <journal> ACM Computing Surveys, </journal> <volume> 26(4) </volume> <pages> 345-420, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: See Figure 2.3 for an outline of a compiler which utilizes this model. This strategy is used by many of the HPF compilers for MIMD architectures [81, 85, 19], and it is not limited to compilers for distributed-memory machines <ref> [20] </ref>. The advantages of this model are fairly clear. By exploiting an existing Fortran77D compiler, a Fortran90D compiler can be created in a much shorter time span. In addition, the Fortran90D compiler gains from the years of effort that went into creating and optimizing the Fortran77/Fortran77D compiler.
Reference: [21] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: After the input is parsed, the distributions and alignments of arrays are analyzed. This is used to generate required communication and to partition the computation among the PEs. And as communication operations are so expensive, an attempt is usually made to optimize them using methods such as message vectoriza-tion <ref> [21, 73, 91] </ref>, message aggregation [120, 131, 152], and the exploitation of collective communication operations [120]. The major advantage of this model is its simplicity. The compiler takes the parallelism that is explicitly stated by the programmer and maps it to the parallel 14 operation Fortran90D/HPF compiler. hardware. <p> As with the optimizations we performed at the array level, these optimizations address the many issues involved with generating efficient code for distributed-memory machines. These optimizations fall into several categories: Reducing Communication: Here we perform optimizations that attempt to reduce the amount of communication. These include message vectorization <ref> [21, 73] </ref>, message coalescing [95], message aggregation [120, 131], redundant communication elimination [17], and the exploitation of collective communication [120]. Hiding Communication: These transformations attempt to hide the cost of communication by overlapping communication and computation.
Reference: [22] <author> P. Banerjee, J. Chandy, M. Gupta, E. Hodges, J. Holm, A. Lain, D. Palermo, S. Ramaswamy, and E. Su. </author> <title> The Paradigm compiler for distributed-memory multicomputers. </title> <journal> IEEE Computer, </journal> <volume> 28(10) </volume> <pages> 37-47, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: The interested reader is referred to any of the following projects which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University [83, 91, 92, 93, 94, 115, 152], the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign <ref> [78, 79, 80, 147, 22, 133] </ref>, Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University [16, 145, 157].
Reference: [23] <author> U. Banerjee. </author> <title> Speedup of ordinary programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year> <note> Report No. 79-989. 142 </note>
Reference-contexts: (i+1,j) = F 1 (Z (i+1,j+1),Z (i+1,j)) ENDIF IF (i.gt.0) THEN Y (i,j+1) = F 1 (Z (i+1,j+1),Z (i,j+1)) ENDIF ENDDO ENDDO (d) After loop fusion (e) SPMD code with loop fusion the Shallow weather prediction code. 89 the subgrid loop by performing loop splitting, also called index set splitting <ref> [23, 161] </ref>. By splitting the iteration space into disjoint sets, each requiring a single context, we can safely hoist the context setting code out of the resulting loops. We call this optimization context splitting.
Reference: [24] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: Input dependences are different than the others, in that they do not restrict the order of execution. 40 Dependence analysis is the process of determining whether a data dependence exists between two statements <ref> [24] </ref>.
Reference: [25] <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D. Padua. </author> <title> Automatic program parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 211-243, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The search for parallelism is simplified by considering only those loop nests which iterate over the distributed dimensions of the arrays. The methods used to detect and exploit data parallelism in sequential code have been well documented <ref> [12, 13, 25, 153, 161, 163] </ref> and will not be discussed any further here. 4.4 Computation Partitioning The next step is to map the parallel operations to the processors.
Reference: [26] <author> T. Blank. </author> <title> The MasPar MP-1 architecture. </title> <booktitle> In Proceedings of the 1990 Spring COMPCON, </booktitle> <address> San Francisco, CA, </address> <month> February </month> <year> 1990. </year>
Reference-contexts: It is the responsibility of the compiler to generate the code to set the correct PE context for all computations executed on the PE array. Examples of SIMD machines include the CM-2/CM-200 from Thinking Machines Corporation [90, 148, 150], and the MP-1/MP-2 from MasPar <ref> [26, 130, 124] </ref>. A simple diagram of a SIMD machine can be seen in Figure 2.2. 9 2.2 Fortran90 Fortran90 [18] adds a number of interesting and useful features to the Fortran language, the most popular being the array features.
Reference: [27] <author> Z. Bozkus. </author> <title> Compiling the FORALL statement on MIMD parallel computers. </title> <type> Technical Report SCCS-389, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: The basic structure of this compiler is composed of four major modules parsing, partitioning, communication generation, and code generation. The parsing module translates all parallel constructs, such as array assignments and where statements, into equivalent forall constructs <ref> [27] </ref>. In this way all subsequent modules need only deal with forall statements.
Reference: [28] <author> Z. Bozkus. </author> <title> Compiling Fortran 90D/HPF for Distributed Memory MIMD Computers. </title> <type> PhD thesis, </type> <institution> Syracuse University, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: This strategy is used in the majority of Fortran90D/HPF compilers for SIMD architectures [123, 140, 149], many of which are descendents of compiler technology developed by Compass [7, 9, 108, 109, 110], and is also employed by some MIMD compilers <ref> [28, 32] </ref>. We refer to such compilers as array operation compilers or native Fortran90 compilers. The main phases of a compiler exploiting this model are depicted in Figure 2.4. After the input is parsed, the distributions and alignments of arrays are analyzed. <p> NPAC's Fortran90D The Fortran90D compiler developed by the Northeast Parallel Architectures Center (NPAC) at Syracuse University, like the CM Fortran compiler, is classified as an array operation compiler <ref> [28, 29, 30, 61] </ref>. The compiler only exploits the parallelism expressed in the data parallel constructs. It does not attempt to parallelize scalar constructs as would a Fortran77D compiler. The basic structure of this compiler is composed of four major modules parsing, partitioning, communication generation, and code generation.
Reference: [29] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> Fortran 90D/HPF compiler for distributed memory MIMD computers: Design, implementation, and performance results. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 351-360, </pages> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: NPAC's Fortran90D The Fortran90D compiler developed by the Northeast Parallel Architectures Center (NPAC) at Syracuse University, like the CM Fortran compiler, is classified as an array operation compiler <ref> [28, 29, 30, 61] </ref>. The compiler only exploits the parallelism expressed in the data parallel constructs. It does not attempt to parallelize scalar constructs as would a Fortran77D compiler. The basic structure of this compiler is composed of four major modules parsing, partitioning, communication generation, and code generation.
Reference: [30] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, S. Ranka, and M. Wu. </author> <title> Compiling Fortran 90D/HPF for distributed memory MIMD computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 15-26, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: NPAC's Fortran90D The Fortran90D compiler developed by the Northeast Parallel Architectures Center (NPAC) at Syracuse University, like the CM Fortran compiler, is classified as an array operation compiler <ref> [28, 29, 30, 61] </ref>. The compiler only exploits the parallelism expressed in the data parallel constructs. It does not attempt to parallelize scalar constructs as would a Fortran77D compiler. The basic structure of this compiler is composed of four major modules parsing, partitioning, communication generation, and code generation. <p> These patterns are then replaced by calls to the optimized run-time support system routines. The runtime support system includes parallel intrinsic functions, data distribution functions, communication primitives, and several other miscellaneous routines." <ref> [30] </ref> This compiler's reliance on run-time support is evident in the fact that its run-time library contains over 500 routines. This shifting of responsibility from compile-time to run-time has its advantages and its disadvantages. The major advantage is that it simplifies the design and development of the compiler.
Reference: [31] <author> Z. Bozkus, L. Meadows, D. Miles, S. Nakamoto, V. Schuster, and M. Young. </author> <title> Techniques for compiling and executing HPF programs on shared-memory and distributed-memory parallel systems. </title> <booktitle> In Proceedings of the First International Workshop on Parallel Processing, </booktitle> <address> Bangalore, India, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: enhances the opportunities for analysis and makes it possible to eliminate extra array temporaries and copying an important consideration for any Fortran90 compiler which desires to achieve performance comparable to Fortran77 programs. pghpf The pghpf compiler, from The Portland Group Inc., is very similar in design to NPAC's Fortran90D compiler <ref> [32, 31, 126] </ref>. This is no surprise given the close association the two groups have had during recent years. The compiler still relies heavily on run-time routines, which enables it to target both shared-memory and distributed-memory machines. <p> However, they do not describe their algorithm for accomplishing this, and it is unknown whether they would be able to eliminate the redundant communication that arises from shifts over the same dimension and direction but of different distances. The Portland Group's pghpf compiler, as described by Bozkus, et al. <ref> [31, 32] </ref>, performs stencil recognition and optimizes the computation by using overlap shift 134 communication. They also perform a subset of our communication unioning optimization. However, they are limited to single-statement expressions in both cases. In general, there have been several different methods for handling specific subclasses of stencil computations.
Reference: [32] <author> Z. Bozkus, L. Meadows, S. Nakamoto, V. Schuster, and M. Young. </author> <title> PGHPF an optimizing High Performance Fortran compiler for distributed memory machines. </title> <journal> Scientific Programming, </journal> <volume> 6(1) </volume> <pages> 29-40, </pages> <year> 1997. </year>
Reference-contexts: This strategy is used in the majority of Fortran90D/HPF compilers for SIMD architectures [123, 140, 149], many of which are descendents of compiler technology developed by Compass [7, 9, 108, 109, 110], and is also employed by some MIMD compilers <ref> [28, 32] </ref>. We refer to such compilers as array operation compilers or native Fortran90 compilers. The main phases of a compiler exploiting this model are depicted in Figure 2.4. After the input is parsed, the distributions and alignments of arrays are analyzed. <p> enhances the opportunities for analysis and makes it possible to eliminate extra array temporaries and copying an important consideration for any Fortran90 compiler which desires to achieve performance comparable to Fortran77 programs. pghpf The pghpf compiler, from The Portland Group Inc., is very similar in design to NPAC's Fortran90D compiler <ref> [32, 31, 126] </ref>. This is no surprise given the close association the two groups have had during recent years. The compiler still relies heavily on run-time routines, which enables it to target both shared-memory and distributed-memory machines. <p> However, they do not describe their algorithm for accomplishing this, and it is unknown whether they would be able to eliminate the redundant communication that arises from shifts over the same dimension and direction but of different distances. The Portland Group's pghpf compiler, as described by Bozkus, et al. <ref> [31, 32] </ref>, performs stencil recognition and optimizes the computation by using overlap shift 134 communication. They also perform a subset of our communication unioning optimization. However, they are limited to single-statement expressions in both cases. In general, there have been several different methods for handling specific subclasses of stencil computations.
Reference: [33] <author> T. Brandes. </author> <title> Automatic translation of data parallel programs to message passing programs. In Proceedings of AP'93 International Workshop on Automatic Distributed Memory Parallelization, Automatic Data Distribution and Automatic Parallel Performance Prediction, </title> <booktitle> Saarbrucken, </booktitle> <address> Germany, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: The main limitation of the system is that the base distributed memory parallelization tool (DMP) does not attempt code restructuring transformations to enhance the detection or generation of parallel code. Adaptor Adaptor is an HPF-like compilation system that is structured very much like the CM Fortran compiler <ref> [33, 34, 35] </ref>. It only takes advantage of parallelism present in the explicitly parallel HPF constructs. It has no feature for automatic parallelization. In addition to batch compilation support, Adaptor also has a graphical user environment which allows the user to assist in directing the translation process.
Reference: [34] <author> T. Brandes. </author> <title> Compiling data parallel programs to message passing programs for massively parallel MIMD systems. </title> <booktitle> In Working Conference on Massively Parallel Programming Models, </booktitle> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: The main limitation of the system is that the base distributed memory parallelization tool (DMP) does not attempt code restructuring transformations to enhance the detection or generation of parallel code. Adaptor Adaptor is an HPF-like compilation system that is structured very much like the CM Fortran compiler <ref> [33, 34, 35] </ref>. It only takes advantage of parallelism present in the explicitly parallel HPF constructs. It has no feature for automatic parallelization. In addition to batch compilation support, Adaptor also has a graphical user environment which allows the user to assist in directing the translation process. <p> This code is equivalent to the code produced by several other commercial and research compilers <ref> [34, 110, 139] </ref>. 4.6 Fortran90-level Analysis and Optimization In this phase of the compilation process, we perform array-level analysis and optimizations. The array-level analyses performed are those that are necessary to support 28 our optimizations, and include data flow analysis, dependence analysis, and the generation of static single assignment form. <p> Besides cshift operations written by users, compilers for distributed-memory machines commonly insert them to perform data movement needed for operations on array sections that have different processor mappings <ref> [110, 139, 34] </ref>. Section 4.5 describes how the communication generation phase of the compiler transforms certain array section references into calls to shift intrinsics that perform the necessary inter-processor data movement. <p> All stencil and stencil-like computations can be translated into this general form by factoring expressions and introducing temporary arrays. In fact, this is the intermediate form used by several distributed-memory compilers <ref> [110, 139, 34] </ref>.
Reference: [35] <author> T. Brandes. </author> <title> Adaptor: A compilation system for data parallel fortran programs. </title> <editor> In Christoph W. Kessler, editor, </editor> <title> Automatic Parallelization | New Approaches 143 to Code Generation, Data Distribution, and Performance Prediction. </title> <publisher> Vieweg, Wiesbaden, </publisher> <year> 1994. </year>
Reference-contexts: The main limitation of the system is that the base distributed memory parallelization tool (DMP) does not attempt code restructuring transformations to enhance the detection or generation of parallel code. Adaptor Adaptor is an HPF-like compilation system that is structured very much like the CM Fortran compiler <ref> [33, 34, 35] </ref>. It only takes advantage of parallelism present in the explicitly parallel HPF constructs. It has no feature for automatic parallelization. In addition to batch compilation support, Adaptor also has a graphical user environment which allows the user to assist in directing the translation process.
Reference: [36] <author> R. G. Brickner, W. George, S. L. Johnsson, and A. Ruttenberg. </author> <title> A stencil compiler for the Connection Machine models CM-2/200. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: In fact, on the SP-2 the cshift operations account for 65% of the total execution time for the largest subgrid. The corresponding number for the overlap shift operations is 7.3%. Additional experimental results are presented in Chapter 9. 6.8 Related Work 6.8.1 The CM-2 Stencil Compiler The stencil compiler <ref> [36, 39] </ref> for the CM-2 avoids the memory-to-memory copying for shift operations that occur within specific, stylized, array-assignment statements. These statements, or stencils, must be in the form of a weighted sum of circularly-shifted arrays. <p> the lowest common denominator a form into which our compiler can transform all stencil computations. 132 9.9 Related Work One of the first major efforts to specifically address the compilation of stencil computations for a distributed-memory machine was the stencil compiler for the CM-2, also known as the convolution compiler <ref> [36, 37, 39] </ref>. They eliminated the intraprocessor data movement and optimized the interprocessor data movement by exploiting the CM-2's polyshift communication [72]. The final computation was performed by hand-optimized library microcode that took advantage of several loop transformations and a specialized register allocation scheme.
Reference: [37] <author> R. G. Brickner, K. Holian, B. Thiagarajan, and S. L. Johnsson. </author> <title> A stencil compiler for the Connection Machine model CM-5. </title> <type> Technical Report CRPC-TR94457, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: the lowest common denominator a form into which our compiler can transform all stencil computations. 132 9.9 Related Work One of the first major efforts to specifically address the compilation of stencil computations for a distributed-memory machine was the stencil compiler for the CM-2, also known as the convolution compiler <ref> [36, 37, 39] </ref>. They eliminated the intraprocessor data movement and optimized the interprocessor data movement by exploiting the CM-2's polyshift communication [72]. The final computation was performed by hand-optimized library microcode that took advantage of several loop transformations and a specialized register allocation scheme.
Reference: [38] <author> P. Briggs. </author> <title> Register Allocation via Graph Coloring. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: We have chosen to base our optimistic analysis framework on the SSA intermediate representation [65] extended as discussed in Section 5.4. In addition to the SSA graph, we generate an interference graph <ref> [38] </ref>. The interference graph indicates those SSA variables with overlapping live ranges, and is used to check for violations of criteria 1 or 2. The live-ness of arrays is computed as described in Section 5.2.
Reference: [39] <author> M. Bromley, S. Heller, T. McNerney, and G. Steele, Jr. </author> <title> Fortran at ten gi-gaflops: The Connection Machine convolution compiler. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Thinking Machines documented many of the shortcomings, and suggested methods that programmers may use to work around them [142]. Thinking Machines had also done some extensive work on compiling stencils <ref> [39] </ref>. A stencil is a computational pattern that calculates a new value for a matrix element by combining elements from neighboring matrix locations. The proper handling of stencils is very important for distributed-memory compilers, and can result in substantial performance gains. <p> In fact, on the SP-2 the cshift operations account for 65% of the total execution time for the largest subgrid. The corresponding number for the overlap shift operations is 7.3%. Additional experimental results are presented in Chapter 9. 6.8 Related Work 6.8.1 The CM-2 Stencil Compiler The stencil compiler <ref> [36, 39] </ref> for the CM-2 avoids the memory-to-memory copying for shift operations that occur within specific, stylized, array-assignment statements. These statements, or stencils, must be in the form of a weighted sum of circularly-shifted arrays. <p> the lowest common denominator a form into which our compiler can transform all stencil computations. 132 9.9 Related Work One of the first major efforts to specifically address the compilation of stencil computations for a distributed-memory machine was the stencil compiler for the CM-2, also known as the convolution compiler <ref> [36, 37, 39] </ref>. They eliminated the intraprocessor data movement and optimized the interprocessor data movement by exploiting the CM-2's polyshift communication [72]. The final computation was performed by hand-optimized library microcode that took advantage of several loop transformations and a specialized register allocation scheme. <p> Our compilation scheme handles a strict superset of patterns handled by the CM-2 stencil compiler. In their own words, they "avoid the general problem by restricting the domain of applicability." <ref> [39] </ref> We have placed no such restrictions upon our work. Our strategy optimizes single-statement stencils, multi-statement stencils, cshift intrinsic stencils, and array-syntax stencils all equally well. And since our optimizations were designed to be incorporated into an F90D/HPF compiler, they benefit those computations that only slightly resemble stencils.
Reference: [40] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and paralleliza-tion. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: Thus for scalarization dependences, it is no longer the case that a true dependence with a "&gt;" as the first non-"=" direction is equivalent to an antidependence with the direction reversed, as has been previously noted by others <ref> [14, 40] </ref>. By definition, scalarization dependences are loop-independent with regard to surrounding loops. This has several implications.
Reference: [41] <author> D. Callahan. </author> <title> Dependence testing in PFC: Weak separability. Supercomputer Software Newsletter 2, </title> <institution> Dept. of Computer Science, Rice University, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: Complexity classes include ziv (zero index variables), siv (single index variable), and miv (multiple index variables). Separability refers to whether or not different subscript positions contain common induction variables. A subscript position is separable if the indices it contains do not appear in other subscript positions <ref> [12, 41] </ref>. If different subscript positions contain the same index, they are said to be coupled [121]. The concept of separability is important when testing multidimensional arrays in that it allows dependence testing to proceed subscript-by-subscript without a loss of precision.
Reference: [42] <author> D. Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> March </month> <year> 1987. </year>
Reference-contexts: This is particularly critical in Fortran90 compilation, where the scalarization of array statements generates many loop nests each containing a single assignment statement. In previous work on loop fusion for parallel machines <ref> [42, 154, 159] </ref> two loops are candidates for fusion if their headers are conformable and there do not exist any fusion-preventing dependences. Two loop headers are conformable if they specify the same number of iterations and are both either parallel or sequential loops.
Reference: [43] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Our strategy involves the following compiler optimizations to improve data locality: 1. Improve the order of memory accesses through loop permutation [47]. 2. Improve loop balance through unroll-and-jam and scalar replacement <ref> [43, 46] </ref>. Note that strip-mine-and-interchange can be included here [158]. We have omitted it because of its relative instability and the large amount of cache reuse that already exists in stencil computations [63, 118]. <p> This transformation is called scalar replacement and is described in detail elsewhere <ref> [43] </ref>. 9.6.3 Unroll-And-Jam Unroll-and-jam is a transformation that can be used in conjunction with scalar replacement to improve the performance of many memory-bound loops [6, 10, 44]. The transformation unrolls an outer loop and then fuses the resulting inner loops back together.
Reference: [44] <author> D. Callahan, J. Cocke, and K. Kennedy. </author> <title> Estimating interlock and improving balance for pipelined machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(4) </volume> <pages> 334-358, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Unfortunately, due to the large number of array references found in such a loop, this metric is insufficient. To better measure the performance of subgrid loops in relation to their memory accesses we use the notion of balance as defined by Callahan, et al. <ref> [44] </ref>. <p> This transformation is called scalar replacement and is described in detail elsewhere [43]. 9.6.3 Unroll-And-Jam Unroll-and-jam is a transformation that can be used in conjunction with scalar replacement to improve the performance of many memory-bound loops <ref> [6, 10, 44] </ref>. The transformation unrolls an outer loop and then fuses the resulting inner loops back together. Using unroll-and-jam, more computation can be introduced into an innermost loop body without a proportional increase in memory references.
Reference: [45] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The Fortran D compiler uses the "owner computes" rule, where every processor only performs computations that update data it owns <ref> [45, 165] </ref>. In essence, the data distribution specified by the programmer is also a specification for distributing the computation. The compiler uses the distribution functions discussed in Section 4.2 to determine ownership. <p> For such complex cases it may be necessary to compute the local iteration sets [54, 105] or it may require the insertion of explicit guards <ref> [45, 99, 136] </ref>. We do not discuss the details of these at this point. SIMD Context Switching Since SIMD machines have a single instruction stream, the subgrid iteration space must include the union of all the iteration spaces required by the individual PEs.
Reference: [46] <author> S. Carr and K. Kennedy. </author> <title> Improving the ratio of memory operations to floating-point operations in loops. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(6) </volume> <pages> 1768-1810, </pages> <year> 1994. </year> <month> 144 </month>
Reference-contexts: Our strategy involves the following compiler optimizations to improve data locality: 1. Improve the order of memory accesses through loop permutation [47]. 2. Improve loop balance through unroll-and-jam and scalar replacement <ref> [43, 46] </ref>. Note that strip-mine-and-interchange can be included here [158]. We have omitted it because of its relative instability and the large amount of cache reuse that already exists in stencil computations [63, 118]. <p> Their method computes the unroll amount for a loop that best balances the nest with respect to a target architecture while limiting register pressure. For a detailed discussion of this method, see their paper <ref> [46] </ref>. 9.7 An Extended Example In this section, we trace our compilation strategy through an extended example. This detailed examination shows how our strategy is able to produce code that matches or beats hand-optimized code.
Reference: [47] <author> S. Carr, K. S. M c Kinley, and C.-W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Our strategy involves the following compiler optimizations to improve data locality: 1. Improve the order of memory accesses through loop permutation <ref> [47] </ref>. 2. Improve loop balance through unroll-and-jam and scalar replacement [43, 46]. Note that strip-mine-and-interchange can be included here [158]. We have omitted it because of its relative instability and the large amount of cache reuse that already exists in stencil computations [63, 118]. <p> The result is fewer idle cycles waiting on main memory. For a more complete discussion of loop permutation see Wolf and Lam [158], Kennedy and McKinley [102] and Carr, et al. <ref> [47] </ref>. 9.6.2 Scalar Replacement Even with better cache performance through loop permutation, a loop may still not perform as well as possible. If a loop is memory bound, then its balance must be lowered.
Reference: [48] <author> G.J. Chaitin, M.A. Auslander, A.K. Chandra, J. Cocke, M.E. Hopkins, and P.W. Markstein. </author> <title> Register allocation via coloring. </title> <journal> Computer Languages, </journal> <volume> 6 </volume> <pages> 45-57, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: = T0 + T1 ENDDO ENDDO Since the values held in scalar quantities will probably be in registers, the load of A (I,J) has been removed, resulting in a reduction in the memory cycle requirements of the loop (the register copy, T1 = T0, can be removed by unrolling I) <ref> [48] </ref>. This transformation is called scalar replacement and is described in detail elsewhere [43]. 9.6.3 Unroll-And-Jam Unroll-and-jam is a transformation that can be used in conjunction with scalar replacement to improve the performance of many memory-bound loops [6, 10, 44].
Reference: [49] <author> S. Chakrabarti, M. Gupta, and J-D. Choi. </author> <title> Global communication analysis and optimization. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: These include message vectorization [21, 73], message coalescing [95], message aggregation [120, 131], redundant communication elimination [17], and the exploitation of collective communication [120]. Hiding Communication: These transformations attempt to hide the cost of communication by overlapping communication and computation. Examples of such optimizations are communication placement <ref> [49, 84] </ref>, message pipelining [136], vector message pipelining [95], and iteration reordering [113]. Improving Parallelism: Recognizing reductions and parallel-prefix scan operations [53, 114] can help to improve the available parallelism.
Reference: [50] <author> B. Chapman, H. Herbeck, and H. Zima. </author> <title> Automatic support for data distribution. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University [83, 91, 92, 93, 94, 115, 152], the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign [78, 79, 80, 147, 22, 133], Vienna Fortran and the SUPERB-2 system at the University of Vienna <ref> [52, 50, 51, 68] </ref>, and the SUIF project at Stanford University [16, 145, 157]. For the rest of this chapter we concentrate solely on projects whose main purpose is the compilation of Fortran90 constructs.
Reference: [51] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Handling distributed data in Vienna Fortran procedures. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University [83, 91, 92, 93, 94, 115, 152], the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign [78, 79, 80, 147, 22, 133], Vienna Fortran and the SUPERB-2 system at the University of Vienna <ref> [52, 50, 51, 68] </ref>, and the SUIF project at Stanford University [16, 145, 157]. For the rest of this chapter we concentrate solely on projects whose main purpose is the compilation of Fortran90 constructs.
Reference: [52] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University [83, 91, 92, 93, 94, 115, 152], the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign [78, 79, 80, 147, 22, 133], Vienna Fortran and the SUPERB-2 system at the University of Vienna <ref> [52, 50, 51, 68] </ref>, and the SUIF project at Stanford University [16, 145, 157]. For the rest of this chapter we concentrate solely on projects whose main purpose is the compilation of Fortran90 constructs.
Reference: [53] <author> S. Chatterjee, G. Blelloch, and M. Zagha. </author> <title> Scan primitives for vector computers. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Hiding Communication: These transformations attempt to hide the cost of communication by overlapping communication and computation. Examples of such optimizations are communication placement [49, 84], message pipelining [136], vector message pipelining [95], and iteration reordering [113]. Improving Parallelism: Recognizing reductions and parallel-prefix scan operations <ref> [53, 114] </ref> can help to improve the available parallelism. Loop interchange and strip-mining [13, 160, 162] can be used to adjust the granularity of pipelined computations to balance parallelism and communication [93].
Reference: [54] <author> S. Chatterjee, J. Gilbert, F. Long, R. Schreiber, and S. Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: For such complex cases it may be necessary to compute the local iteration sets <ref> [54, 105] </ref> or it may require the insertion of explicit guards [45, 99, 136]. We do not discuss the details of these at this point.
Reference: [55] <author> S. Chatterjee, J. Gilbert, R. Schreiber, and S. Teng. </author> <title> Optimal evaluation of array expressions on massively parallel machines. </title> <type> Technical Report CSL-92-11, </type> <institution> Xerox Corporation, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: and Steele's work on automatically aligning and partitioning arrays to minimize data movement associated with binary operations on arrays [109], and Chatterjee, Gilbert, Schreiber, and Teng's work on optimally mapping the computation of array expressions to processors to minimize data movement when the mappings of data to processors are fixed <ref> [55] </ref>. While interprocessor data movement is more costly per element, the number of elements moved within the memory of a single processor may be much larger, causing the cost of local data movement to be dominant.
Reference: [56] <author> S. Chatterjee, J. Gilbert, R. Schreiber, and S. Teng. </author> <title> Automatic array alignment in data-parallel programs. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Charleston, SC, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: The compiler uses the distribution functions discussed in Section 4.2 to determine ownership. The owner computes rule could be replaced by a data optimization phase, which may determine an alternate distribution for the computation and associated intermediate results, in an attempt to reduce the amount of communication <ref> [7, 56, 109, 119] </ref>. 4.5 Communication Generation Once data and computation distributions are finalized, the compiler must insert any necessary communication operations. These are required to move data so that all operands of an expression reside on the PE which performs the computation.
Reference: [57] <author> M. Chen and J. Cowie. </author> <title> Prototyping Fortran-90 compilers for massively parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year> <month> 145 </month>
Reference-contexts: The system concentrates on producing correct code. It does not contain an optimization phase. Fortran90-Y The Fortran-90-Y compiler, developed at Yale University, is designed to support rapid prototyping of compilation and optimization techniques <ref> [57, 59, 58] </ref>. The compiler uses an abstract semantic algebra, Yale Intermediate Representation (YR), as its intermediate language. YR defines a series of semantic domains and sets of operators within each domain, and combines them with shapes that represent iteration spaces. <p> Furthermore, due to the limited dependence analysis performed by the compiler, only compiler-generated scalar code is moved. It was this work that motivated us to investigate the context partitioning problem. 102 Chen and Cowie also recognize the need to fuse parallel loops in their Fortran90 compiler <ref> [57] </ref>. However, they only fuse adjacent loops and perform no code motion to increase the chances of fusion. 7.7 Summary We have developed a double-edged sword to combat the cost of context switching in codes for SIMD machines.
Reference: [58] <author> M. Chen and Y. Hu. </author> <title> Optimizations for compiling iterative spatial loops to massively parallel machines. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: This allows for the parallelization and distribution of computation not only for code written with parallel constructs but also for parallelizable code that is not expressible using array syntax or forall; e.g., pipelined computations <ref> [93, 152, 58] </ref>. The major drawback of this scheme is that the scalarization process can obfuscate the code, making it much more difficult to analyze and optimize than the original 13 Fortran90D code. <p> The system concentrates on producing correct code. It does not contain an optimization phase. Fortran90-Y The Fortran-90-Y compiler, developed at Yale University, is designed to support rapid prototyping of compilation and optimization techniques <ref> [57, 59, 58] </ref>. The compiler uses an abstract semantic algebra, Yale Intermediate Representation (YR), as its intermediate language. YR defines a series of semantic domains and sets of operators within each domain, and combines them with shapes that represent iteration spaces.
Reference: [59] <author> M. Chen and J. Wu. </author> <title> Optimizing FORTRAN-90 programs for data motion on massively parallel systems. </title> <type> Technical Report YALE/DCS/TR-882, </type> <institution> Dept. of Computer Science, Yale University, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: The system concentrates on producing correct code. It does not contain an optimization phase. Fortran90-Y The Fortran-90-Y compiler, developed at Yale University, is designed to support rapid prototyping of compilation and optimization techniques <ref> [57, 59, 58] </ref>. The compiler uses an abstract semantic algebra, Yale Intermediate Representation (YR), as its intermediate language. YR defines a series of semantic domains and sets of operators within each domain, and combines them with shapes that represent iteration spaces.
Reference: [60] <author> J. Choi, R. Cytron, and J. Ferrante. </author> <title> Automatic construction of sparse data flow evaluation graphs. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 55-66, </pages> <address> Orlando, FL, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: There are several variants of SSA available. The variations can enhance the precision of different analysis algorithms. For our purposes, we have found that the pruned-SSA form <ref> [60] </ref> best fits our needs. In the pruned-SSA form, dead -functions have been deleted. We also employ both def-use and use-def edges in our graph.
Reference: [61] <author> A. Choudhary, G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, S. Ranka, and C.-W. Tseng. </author> <title> Compiling Fortran 77D and 90D for MIMD distributed-memory machines. </title> <booktitle> In Frontiers '92: The 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: NPAC's Fortran90D The Fortran90D compiler developed by the Northeast Parallel Architectures Center (NPAC) at Syracuse University, like the CM Fortran compiler, is classified as an array operation compiler <ref> [28, 29, 30, 61] </ref>. The compiler only exploits the parallelism expressed in the data parallel constructs. It does not attempt to parallelize scalar constructs as would a Fortran77D compiler. The basic structure of this compiler is composed of four major modules parsing, partitioning, communication generation, and code generation.
Reference: [62] <author> A. Choudhary, G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, S. Ranka, and C.-W. Tseng. </author> <title> Unified compilation of Fortran 77D and 90D. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 2(1-4):95-114, </volume> <month> March-December </month> <year> 1993. </year>
Reference-contexts: We present a more complete comparison between our work and the stencil compiler in Chapter 9 where we discuss our own stencil compilation strategy. 6.8.2 Scalarizing Compilers Previous work on Fortran90D <ref> [62] </ref>, like the stencil compiler, is capable of avoiding some intraprocessor data movement for stylized expressions. In this case, the expressions have to use array syntax.
Reference: [63] <author> S. Coleman and K. S. M c Kinley. </author> <title> Tile size selection using cache organization. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Improve loop balance through unroll-and-jam and scalar replacement [43, 46]. Note that strip-mine-and-interchange can be included here [158]. We have omitted it because of its relative instability and the large amount of cache reuse that already exists in stencil computations <ref> [63, 118] </ref>. In the rest of this section we give an overview of loop permutation, unroll-and-jam and scalar replacement. More information can be found elsewhere [125]. 9.6.1 Loop Permutation Not all loops exhibit good cache locality, resulting in idle computational cycles while waiting for main memory to return data.
Reference: [64] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Austin, TX, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: Once the triplets have been translated, we exploit whichever multi-subscript test is available in our system [121, 134, 164]. 5.4 Static Single Assignment Form In recent years, Static Single Assignment (SSA) form <ref> [64, 65] </ref> and related intermediate representations have gained in popularity because of their efficiency in program analysis and transformations [15, 137, 155]. SSA is loosely characterized by the trait that each variable has only a single definition.
Reference: [65] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Once the triplets have been translated, we exploit whichever multi-subscript test is available in our system [121, 134, 164]. 5.4 Static Single Assignment Form In recent years, Static Single Assignment (SSA) form <ref> [64, 65] </ref> and related intermediate representations have gained in popularity because of their efficiency in program analysis and transformations [15, 137, 155]. SSA is loosely characterized by the trait that each variable has only a single definition. <p> To be able to verify such a criterion requires an optimistic analysis framework; i.e., one must assume all shifted arrays are offsetable, and then attempt to disprove the assumption. We have chosen to base our optimistic analysis framework on the SSA intermediate representation <ref> [65] </ref> extended as discussed in Section 5.4. In addition to the SSA graph, we generate an interference graph [38]. The interference graph indicates those SSA variables with overlapping live ranges, and is used to check for violations of criteria 1 or 2. <p> This also means that our algorithm is quite efficient. The cost of the algorithm is actually dominated by the cost of generating SSA form and building the interference graph, both of which are O (n 2 ) in the worst case (although building SSA is O (n) in practice <ref> [65] </ref>). Once these structures are built, the rest of the algorithm is linear. Finding offsetable arrays is O (n) and their propagation through the program is O (e). In addition, the checking of interferences is O (i).
Reference: [66] <author> K. Droegemeier, M. Xue, P. Reid, J. Bradley, and R. Lindsay. </author> <title> Development of the CAPS advanced regional prediction system (ARPS): An adaptive, massively parallel, multi-scale prediction model. </title> <booktitle> In Proceedings of the 9th Conference on Numerical Weather Prediction, </booktitle> <publisher> American Meteorological Society, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: Many of the codes are available in Fortran90 and HPF source, as well as Fortran77 and message-passing source. In addition to the HPFA codes, we also tested our compiler on the Advanced Regional Prediction System (ARPS) <ref> [66] </ref> weather prediction code from the Center for Analysis and Prediction of Storms (CAPS), University of Oklahoma. 7 Our algorithm worked as expected and was able to handle some difficult codes. <p> In Section 9.8 we report the results of performing context partitioning on a stencil code executing on an IBM SP-2, a MIMD machine. For our SIMD experiments, we performed the optimizations by hand on a section of code taken from a Fortran90 version of the ARPS weather prediction code <ref> [66] </ref>. 99 The code initializes 16 two-dimensional arrays. We chose this section of code since context partitioning would not benefit additionally from data reuse nor would it be penalized for generating excessive register pressure.
Reference: [67] <author> R. Esser and R. Knecht. </author> <title> Intel Paragon XP/S architecture and software environment. </title> <type> Technical Report KFA-ZAM-IB-9305, </type> <institution> KFA Research Centre, Juelich, </institution> <month> April </month> <year> 1993. </year> <month> 146 </month>
Reference-contexts: These messages can be used to allow PEs to share data with one another. The details of such a network are not important to this dissertation. Examples of MIMD machines include the SP2 from IBM [4], and the Paragon from Intel <ref> [97, 67] </ref>. See Figure 2.1 for a simple schematic of such a machine. 2.1.2 Distributed-Memory SIMD Architectures Distributed-memory SIMD machines are quite similar to MIMD machines in that they consist of an array of PEs connected by a communication network.
Reference: [68] <author> T. Fahringer, R. Blasko, and H. Zima. </author> <title> Automatic performance prediction to support parallelization of Fortran programs for massively parallel systems. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University [83, 91, 92, 93, 94, 115, 152], the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign [78, 79, 80, 147, 22, 133], Vienna Fortran and the SUPERB-2 system at the University of Vienna <ref> [52, 50, 51, 68] </ref>, and the SUIF project at Stanford University [16, 145, 157]. For the rest of this chapter we concentrate solely on projects whose main purpose is the compilation of Fortran90 constructs.
Reference: [69] <author> R. Fatoohi. </author> <title> Performance analysis of four SIMD machines. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: The cost of such a shift operation is described by the following model <ref> [69] </ref>: T shift = g (g d) t onpe + C onpe + g d t offpe + C offpe (6:1) 54 where t onpe and t offpe represent the time to perform an intraprocessor and inter-processor copy respectively, and C onpe and C offpe represent the startup time (or latency) <p> These costs relate to the temporary arrays that are often created to receive the results of the shifts. Not only must time be spent allocating and deallocating these arrays, but their additional memory 4 From Fatoohi <ref> [69] </ref>, c fl1993 ACM. 55 Parameter CM2 MPP MP-1 610C t offpe 9.0 3.2 2.7 3.2 C offpe 20.0 13.4 41.9 7.2 t onpe 0.7 - 5.6 9.0 C onpe 35.0 - 59.1 18.0 Table 6.1 Measured cost of communication parameters for a 32-bit word (in sec). requirements often limit the
Reference: [70] <author> M. Flynn. </author> <title> Very high-speed computing systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 54(12) </volume> <pages> 1901-1909, </pages> <month> December </month> <year> 1966. </year>
Reference-contexts: These classes comprise two of the four classes proposed in Flynn's taxonomy of computer architectures <ref> [70] </ref>. The first is Multiple Instruction streams, Multiple Data streams, or MIMD architectures. The second is Single Instruction stream, Multiple Data streams, or SIMD architectures. We give a brief overview of these two architectures in the following subsections.
Reference: [71] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C.-W. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: The symbol "fl" marks dimensions that are not distributed. Choosing the distribution for a decomposition maps all arrays aligned with the decomposition to the machine. Entire descriptions complete with detailed examples for both Fortran D and HPF can be found elsewhere <ref> [71, 89, 112] </ref>.
Reference: [72] <author> W. George, R. Brickner, and S. L. Johnsson. </author> <title> Polyshift communications software for the Connection Machine systems CM-2 and CM-200. </title> <booktitle> Scientific Programming, </booktitle> <address> 3(1):83, </address> <month> Spring </month> <year> 1994. </year>
Reference-contexts: They eliminated the intraprocessor data movement and optimized the interprocessor data movement by exploiting the CM-2's polyshift communication <ref> [72] </ref>. The final computation was performed by hand-optimized library microcode that took advantage of several loop transformations and a specialized register allocation scheme. Our general compilation methodology produces code equivalent to that produced by this specialized compiler. We both eliminate intraprocessor data movement and minimize interprocessor data movement.
Reference: [73] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: After the input is parsed, the distributions and alignments of arrays are analyzed. This is used to generate required communication and to partition the computation among the PEs. And as communication operations are so expensive, an attempt is usually made to optimize them using methods such as message vectoriza-tion <ref> [21, 73, 91] </ref>, message aggregation [120, 131, 152], and the exploitation of collective communication operations [120]. The major advantage of this model is its simplicity. The compiler takes the parallelism that is explicitly stated by the programmer and maps it to the parallel 14 operation Fortran90D/HPF compiler. hardware. <p> As with the optimizations we performed at the array level, these optimizations address the many issues involved with generating efficient code for distributed-memory machines. These optimizations fall into several categories: Reducing Communication: Here we perform optimizations that attempt to reduce the amount of communication. These include message vectorization <ref> [21, 73] </ref>, message coalescing [95], message aggregation [120, 131], redundant communication elimination [17], and the exploitation of collective communication [120]. Hiding Communication: These transformations attempt to hide the cost of communication by overlapping communication and computation. <p> Improving Parallelism: Recognizing reductions and parallel-prefix scan operations [53, 114] can help to improve the available parallelism. Loop interchange and strip-mining [13, 160, 162] can be used to adjust the granularity of pipelined computations to balance parallelism and communication [93]. Storage Management: The use of overlap areas <ref> [73] </ref> and hash tables can ease the details of buffer management for certain types of computations. Message block ing [95] can be used in situations where buffer storage is limited. <p> In the following subsections, we present criteria for determining when offset arrays are safe and profitable and present the code transformations that avoid intraprocessor copying by exploiting offset arrays. To hold the data that must move between PEs, we use overlap areas <ref> [73] </ref>. Overlap areas are subgrid extensions to hold data received from neighboring PEs.
Reference: [74] <author> M. Gerndt. </author> <title> Work distribution in parallel programs for distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: By reducing the loop bounds, the compiler can often avoid iterations for which the PE has no work and thus does not need to introduce any guard statements into the subgrid loop body in those cases <ref> [74, 152] </ref>. We now discuss the details of context splitting. To simplify the discussion, we first discuss one-dimensional cyclic, block, and block cyclic distributions, and then show how to combine one-dimensional splitting to handle multidimensional cases.
Reference: [75] <author> V. Getov, T. Brandes, B. Chapman, T. Hey, and D. Pritchard. </author> <title> A comparison of HPF-like systems: Early prototypes. </title> <booktitle> In Proceedings of the Workshop on Performance Evaluation and Benchmarking of Parallel Systems, </booktitle> <address> Coventry, U.K., </address> <year> 1994. </year>
Reference-contexts: And finally, the compiler performs enhanced analysis and optimization of the resulting node program, including vectorizing loops for certain architectures. xHPF Applied Parallel Research Inc.'s xHPF translator is the most recent addition to the FORGE90 parallel programming environment <ref> [19, 146, 75] </ref>. The system is identical to the company's xHPF77 system, with the addition of a preprocessor that converts Fortran90 syntax into Fortran77. This structure classifies the compiler as a scalarizing HPF compiler, similar to IBM's xlhpf.
Reference: [76] <author> G. Goff, K. Kennedy, and C.-W. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: The principal focus of dependence analysis is to determine dependences that arise from subscripted array references that appear within loop nests, since it is not always easy to determine if such references access the same memory location. 5.3.2 Partition-based Dependence Testing In the partition-based dependence testing algorithm <ref> [76] </ref> used in the analysis and transformation systems at Rice University, pairs of array references are classified before being tested. This enables us to choose the most efficient test for a given pair of references and lets us test the subscripts in the order of less expensive to more expensive.
Reference: [77] <author> K. Gopinath and J. L. Hennessy. </author> <title> Copy elimination in functional languages. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Austin, TX, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: He then develops a set of value transmission functions that can be used to determine the safety of a destructive use within the language SETL. 77 5-point stencil computation on 16K MasPar MP-1. execution time spent performing shift operations. Gopinath and Hennessy <ref> [77] </ref> address the problem of copy elimination by targeting, or the proper selection of a storage area for evaluating an expression, the goal of which is to reuse the storage of the input parameters.
Reference: [78] <author> M. Gupta and P. Banerjee. </author> <title> Automatic data partitioning on distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year> <month> 147 </month>
Reference-contexts: The interested reader is referred to any of the following projects which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University [83, 91, 92, 93, 94, 115, 152], the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign <ref> [78, 79, 80, 147, 22, 133] </ref>, Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University [16, 145, 157].
Reference: [79] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The interested reader is referred to any of the following projects which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University [83, 91, 92, 93, 94, 115, 152], the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign <ref> [78, 79, 80, 147, 22, 133] </ref>, Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University [16, 145, 157].
Reference: [80] <author> M. Gupta and P. Banerjee. </author> <title> PARADIGM: A compiler for automatic data distribution on multicomputers. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: The interested reader is referred to any of the following projects which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University [83, 91, 92, 93, 94, 115, 152], the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign <ref> [78, 79, 80, 147, 22, 133] </ref>, Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University [16, 145, 157].
Reference: [81] <author> M. Gupta, S. Midkiff, E. Schonberg, V. Seshadri, D. Shields, K. Wang, W. Ching, and T. Ngo. </author> <title> An HPF compiler for the IBM SP2. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: See Figure 2.3 for an outline of a compiler which utilizes this model. This strategy is used by many of the HPF compilers for MIMD architectures <ref> [81, 85, 19] </ref>, and it is not limited to compilers for distributed-memory machines [20]. The advantages of this model are fairly clear. By exploiting an existing Fortran77D compiler, a Fortran90D compiler can be created in a much shorter time span. <p> It simplifies the compiler, and when it works it works surprisingly well. But when it cannot match a pattern, the code produced is mediocre at best. An example of the vast differences in code quality produced by such compilers is presented later in this dissertation. xlhpf IBM's xlhpf compiler <ref> [81] </ref> is naturally classified as a scalarizing compiler as defined in Section 2.4.1. This is due to the fact that the first action taken after the intermediate representation is created is the scalarization of the array language into Fortran77 scalar form. <p> This strategy is shared by many Fortran90/HPF compilers that really only want to handle scalarized code, as discussed in Section 2.4.1. As with the CM-2 stencil compiler, our methodology is a strict superset of this strategy. Gupta, et al. <ref> [81] </ref>, in describing IBM's xlhpf compiler, state that they are able to reduce the number of messages for multi-dimensional shifts by exploiting methods similar to ours.
Reference: [82] <author> M. Gupta, E. Schonberg, and H. Srinivasan. </author> <title> A unified data-flow framework for optimizing communication. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Dependence information is also used for communication placement optimizations and selective loop distribution. The compiler uses symbolic analysis to generate efficient code even when the size of arrays or the number of processors is not known at compile-time. The analysis of data availability enables the elimination of redundant communication <ref> [82] </ref>. And static single assignment (SSA) form is exploited to produce an efficient mapping of scalar variables. 20 Taking just the opposite tack as the NPAC Fortran90D compiler, xlhpf actually in-lines library routines such as Fortran90 intrinsic functions.
Reference: [83] <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: In fact, most of these common issues were solved first by research projects concentrating on the compilation of Fortran77. The interested reader is referred to any of the following projects which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University <ref> [83, 91, 92, 93, 94, 115, 152] </ref>, the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign [78, 79, 80, 147, 22, 133], Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University [16, 145, 157].
Reference: [84] <author> R. v. Hanxleden and K. Kennedy. </author> <title> Give-N-Take | A balanced code placement framework. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: These include message vectorization [21, 73], message coalescing [95], message aggregation [120, 131], redundant communication elimination [17], and the exploitation of collective communication [120]. Hiding Communication: These transformations attempt to hide the cost of communication by overlapping communication and computation. Examples of such optimizations are communication placement <ref> [49, 84] </ref>, message pipelining [136], vector message pipelining [95], and iteration reordering [113]. Improving Parallelism: Recognizing reductions and parallel-prefix scan operations [53, 114] can help to improve the available parallelism.
Reference: [85] <author> J. Harris, J. Bircsak, M. R. Bolduc, J. A. Diewald, I. Gale, N. Johnson, S. Lee, C. A. Nelson, and C. Offner. </author> <title> Compiling High Performance Fortran for distributed-memory systems. </title> <journal> Digital Technical Journal of Digital Equipment Corp., </journal> <volume> 7(3) </volume> <pages> 5-23, </pages> <month> Fall </month> <year> 1995. </year>
Reference-contexts: See Figure 2.3 for an outline of a compiler which utilizes this model. This strategy is used by many of the HPF compilers for MIMD architectures <ref> [81, 85, 19] </ref>, and it is not limited to compilers for distributed-memory machines [20]. The advantages of this model are fairly clear. By exploiting an existing Fortran77D compiler, a Fortran90D compiler can be created in a much shorter time span.
Reference: [86] <author> T. Haupt, S. Reddy, and G. Vengurlekar. </author> <title> Low level HPF compiler benchmark suite. </title> <type> Technical Report SCCS-735, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, Syracuse, </institution> <address> NY, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: The majority of the codes we tested were taken from the High Performance Fortran Application (HPFA) project <ref> [86, 128] </ref> at the Northeast Parallel Architecture Center (NPAC), Syracuse University. 6 The HPFA project contains codes ranging from code fragments and benchmarking kernels to complete applications. Many of the codes are available in Fortran90 and HPF source, as well as Fortran77 and message-passing source. <p> It will also demonstrate how we are able to handle stencil computations that cause other methods to fail. For this exercise, we have chosen to use Problem 9 of the Purdue Set [135], as adapted for Fortran D benchmarking by Thomas Haupt of NPAC <ref> [128, 86] </ref>. The program kernel is shown in Figure 9.11. The arrays T, U, RIP, and RIN are all two-dimensional and have been distributed in a (block,block) fashion. This kernel computes a standard 9-point stencil, identical to that computed by the single-statement stencil shown in Figure 9.2.
Reference: [87] <author> Paul Havlak. </author> <title> Interprocedural Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1994. </year> <note> Also available as CRPC-TR94451 from the Center for Research on Parallel Computation and CS-TR94-228 from the Rice Department of Computer Science. </note>
Reference-contexts: Often the SSA representation of a program is in the form of a sparse graph, in which nodes represent definition points of variables and edges connect a variable to all its uses. For the implementation of SSA that exists within the Fortran D compiler <ref> [87] </ref>, the SSA graph of a program does not exist as a separate object but rather is built on top of the CFG. It is important to realize that we keep a close correspondence between the SSA graph and the CFG.
Reference: [88] <author> J. Hennessy and D. Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: The second is Single Instruction stream, Multiple Data streams, or SIMD architectures. We give a brief overview of these two architectures in the following subsections. For a more complete discussion of general MIMD and SIMD architectures see an appropriate computer architecture book <ref> [88, 96] </ref>. 2.1.1 Distributed-Memory MIMD Architectures A MIMD computer contains many independent central processing units (CPUs) operating asynchronously, each executing its own instruction stream. The MIMD architectures in which we are interested associate some local memory with each CPU, from which the CPU fetches instructions and reads/writes data.
Reference: [89] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: The symbol "fl" marks dimensions that are not distributed. Choosing the distribution for a decomposition maps all arrays aligned with the decomposition to the machine. Entire descriptions complete with detailed examples for both Fortran D and HPF can be found elsewhere <ref> [71, 89, 112] </ref>.
Reference: [90] <author> W. Hillis. </author> <title> The Connection Machine. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year> <month> 148 </month>
Reference-contexts: It is the responsibility of the compiler to generate the code to set the correct PE context for all computations executed on the PE array. Examples of SIMD machines include the CM-2/CM-200 from Thinking Machines Corporation <ref> [90, 148, 150] </ref>, and the MP-1/MP-2 from MasPar [26, 130, 124]. A simple diagram of a SIMD machine can be seen in Figure 2.2. 9 2.2 Fortran90 Fortran90 [18] adds a number of interesting and useful features to the Fortran language, the most popular being the array features.
Reference: [91] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: After the input is parsed, the distributions and alignments of arrays are analyzed. This is used to generate required communication and to partition the computation among the PEs. And as communication operations are so expensive, an attempt is usually made to optimize them using methods such as message vectoriza-tion <ref> [21, 73, 91] </ref>, message aggregation [120, 131, 152], and the exploitation of collective communication operations [120]. The major advantage of this model is its simplicity. The compiler takes the parallelism that is explicitly stated by the programmer and maps it to the parallel 14 operation Fortran90D/HPF compiler. hardware. <p> In fact, most of these common issues were solved first by research projects concentrating on the compilation of Fortran77. The interested reader is referred to any of the following projects which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University <ref> [83, 91, 92, 93, 94, 115, 152] </ref>, the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign [78, 79, 80, 147, 22, 133], Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University [16, 145, 157].
Reference: [92] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: In fact, most of these common issues were solved first by research projects concentrating on the compilation of Fortran77. The interested reader is referred to any of the following projects which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University <ref> [83, 91, 92, 93, 94, 115, 152] </ref>, the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign [78, 79, 80, 147, 22, 133], Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University [16, 145, 157]. <p> The PE array itself is considered to have a rank equal to the number of distributed dimensions of the distributed array. To simplify our discussion, we limit the number of distributed dimensions to two. The compiler uses a distribution function <ref> [92] </ref> to calculate the mapping of an array element to a subgrid location within a PE. Given an array A, the distribution function A (~-) maps an array index~- into a pair consisting of a PE index ~ pid and a subgrid index ~|.
Reference: [93] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: This allows for the parallelization and distribution of computation not only for code written with parallel constructs but also for parallelizable code that is not expressible using array syntax or forall; e.g., pipelined computations <ref> [93, 152, 58] </ref>. The major drawback of this scheme is that the scalarization process can obfuscate the code, making it much more difficult to analyze and optimize than the original 13 Fortran90D code. <p> But some code sequences containing parallelism are not expressible with these constructs; e.g., pipelined computations. In such cases a compiler must have dependence information to detect the available parallelism and to perform the necessary loop tiling transformations to effectively exploit it <ref> [93, 152] </ref>. 2.5 Summary In this chapter of background information we have introduced the Fortran90D and HPF languages, and have given an overview of the target distributed-memory architectures. We have also outlined two possible models that a compiler for these languages might utilize. <p> In fact, most of these common issues were solved first by research projects concentrating on the compilation of Fortran77. The interested reader is referred to any of the following projects which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University <ref> [83, 91, 92, 93, 94, 115, 152] </ref>, the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign [78, 79, 80, 147, 22, 133], Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University [16, 145, 157]. <p> Improving Parallelism: Recognizing reductions and parallel-prefix scan operations [53, 114] can help to improve the available parallelism. Loop interchange and strip-mining [13, 160, 162] can be used to adjust the granularity of pipelined computations to balance parallelism and communication <ref> [93] </ref>. Storage Management: The use of overlap areas [73] and hash tables can ease the details of buffer management for certain types of computations. Message block ing [95] can be used in situations where buffer storage is limited. <p> An outline of the algorithm is given in Figure 5.1. This algorithm has been used with great success in the PFC compiler [13], the ParaScope programming environment [104], and the Fortran D compiler <ref> [93, 152] </ref>. 5.3.3 Dependence Representation Data dependences are often represented using direction vectors and/or distance vectors [161]. The direction vector is an ordering vector, containing &lt;, =, &gt;, or fl, that 41 1. Partition the subscripts into separable and minimal coupled groups. 2.
Reference: [94] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: In fact, most of these common issues were solved first by research projects concentrating on the compilation of Fortran77. The interested reader is referred to any of the following projects which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University <ref> [83, 91, 92, 93, 94, 115, 152] </ref>, the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign [78, 79, 80, 147, 22, 133], Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University [16, 145, 157].
Reference: [95] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Evaluating compiler optimizations for Fortran D. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 27-45, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: These optimizations fall into several categories: Reducing Communication: Here we perform optimizations that attempt to reduce the amount of communication. These include message vectorization [21, 73], message coalescing <ref> [95] </ref>, message aggregation [120, 131], redundant communication elimination [17], and the exploitation of collective communication [120]. Hiding Communication: These transformations attempt to hide the cost of communication by overlapping communication and computation. Examples of such optimizations are communication placement [49, 84], message pipelining [136], vector message pipelining [95], and iteration reordering <p> 73], message coalescing <ref> [95] </ref>, message aggregation [120, 131], redundant communication elimination [17], and the exploitation of collective communication [120]. Hiding Communication: These transformations attempt to hide the cost of communication by overlapping communication and computation. Examples of such optimizations are communication placement [49, 84], message pipelining [136], vector message pipelining [95], and iteration reordering [113]. Improving Parallelism: Recognizing reductions and parallel-prefix scan operations [53, 114] can help to improve the available parallelism. Loop interchange and strip-mining [13, 160, 162] can be used to adjust the granularity of pipelined computations to balance parallelism and communication [93]. <p> Loop interchange and strip-mining [13, 160, 162] can be used to adjust the granularity of pipelined computations to balance parallelism and communication [93]. Storage Management: The use of overlap areas [73] and hash tables can ease the details of buffer management for certain types of computations. Message block ing <ref> [95] </ref> can be used in situations where buffer storage is limited.
Reference: [96] <author> K. Hwang and F. Briggs. </author> <title> Computer Architecture and Parallel Processing. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1984. </year>
Reference-contexts: The second is Single Instruction stream, Multiple Data streams, or SIMD architectures. We give a brief overview of these two architectures in the following subsections. For a more complete discussion of general MIMD and SIMD architectures see an appropriate computer architecture book <ref> [88, 96] </ref>. 2.1.1 Distributed-Memory MIMD Architectures A MIMD computer contains many independent central processing units (CPUs) operating asynchronously, each executing its own instruction stream. The MIMD architectures in which we are interested associate some local memory with each CPU, from which the CPU fetches instructions and reads/writes data.
Reference: [97] <author> Intel Corporation, </author> <title> Supercomputing Systems Division, Beaverton, OR. Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: These messages can be used to allow PEs to share data with one another. The details of such a network are not important to this dissertation. Examples of MIMD machines include the SP2 from IBM [4], and the Paragon from Intel <ref> [97, 67] </ref>. See Figure 2.1 for a simple schematic of such a machine. 2.1.2 Distributed-Memory SIMD Architectures Distributed-memory SIMD machines are quite similar to MIMD machines in that they consist of an array of PEs connected by a communication network.
Reference: [98] <author> S. L. Johnsson. </author> <title> Language and compiler issues in scalable high performance scientific libraries. </title> <booktitle> In Proceedings of the Third Workshop on Compilers for Parallel Computers, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: S. L. Johnsson <ref> [98] </ref> 6.1 Introduction For Fortran90D or HPF to gain acceptance as a vehicle for parallel scientific programming, they must achieve high performance on problems for which they are well suited.
Reference: [99] <author> W. Kelly, W. Pugh, and E. Rosser. </author> <title> Code generation for multiple mappings. </title> <booktitle> In Frontiers '95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: For such complex cases it may be necessary to compute the local iteration sets [54, 105] or it may require the insertion of explicit guards <ref> [45, 99, 136] </ref>. We do not discuss the details of these at this point. SIMD Context Switching Since SIMD machines have a single instruction stream, the subgrid iteration space must include the union of all the iteration spaces required by the individual PEs.
Reference: [100] <author> K. Kennedy. </author> <title> A survey of data flow analysis techniques. </title> <editor> In S. Muchnick and N. Jones, editors, </editor> <booktitle> Program Flow Analysis, </booktitle> <pages> pages 5-54. </pages> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: The results of these analyses are used by the optimizing transformations presented in the remainder of this dissertation. 5.2 Data-Flow Analysis Data-flow analysis is a classical analysis technique which tracks the flow of data through the program's variables <ref> [11, 100] </ref>. Analyzing a program's data flow can provide information on reaching definitions, available expressions, and live variables, which in turn can enable optimizations such as global common subexpression elimination, loop invariant code motion, strength reduction, and global register allocation [5].
Reference: [101] <author> K. Kennedy, J. Mellor-Crummey, and G. Roth. </author> <title> Optimizing Fortran 90 shift operations on distributed-memory multicomputers. </title> <booktitle> In Languages and Compilers for Parallel Computing, Eighth International Workshop, </booktitle> <address> Columbus, OH, </address> <month> August </month> <year> 1995. </year> <note> Springer-Verlag. 149 </note>
Reference-contexts: Each of these optimize the program by exploiting reuse of data values. 9.4 Eliminating Intraprocessor Movement As briefly mentioned in the preceding section, intraprocessor data movement associated with stencil computations is eliminated by employing our offset array optimization <ref> [101] </ref>. Since Chapter 6 was entirely devoted to describing this optimization, we do not discuss it further here. However, it is important to note that due to the algorithm's optimistic nature, it is able to eliminate the intraprocessor data movement associated with shift operations in many difficult situations.
Reference: [102] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: In this case, we have attained locality of reference for B (I,J) and A (I+1,J) by moving reuse points closer together. The result is fewer idle cycles waiting on main memory. For a more complete discussion of loop permutation see Wolf and Lam [158], Kennedy and McKinley <ref> [102] </ref> and Carr, et al. [47]. 9.6.2 Scalar Replacement Even with better cache performance through loop permutation, a loop may still not perform as well as possible. If a loop is memory bound, then its balance must be lowered.
Reference: [103] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Typed fusion with applications to parallel and sequential code generation. </title> <type> Technical Report TR93-208, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: The partitioning could group scalar statements and communication statements together; however, we prefer to separate them so that the communication operations can be further optimized by subsequent phases (see Section 9.5). 7.2.1 Context Partitioning Algorithm To accomplish context partitioning, we use an algorithm proposed by Kennedy and M c Kinley <ref> [103] </ref>. While they were concerned with partitioning parallel and serial loops into fusible groups, we are partitioning Fortran90 statements into congruence classes. The algorithm works on the data dependence graph (ddg) [117] which must be acyclic. <p> Thus the algorithm can compute the fusion node pred by taking the maximum of k and the next higher node than m. The interested reader is referred to the paper by Kennedy and M c Kinley for complete details of the algorithm <ref> [103] </ref>. classes: A and B. Statement 6 is a scalar statement. Figure 7.2 (a) shows the original source code, and Figure 7.2 (b) shows the data dependence graph.
Reference: [104] <author> K. Kennedy, K. S. M c Kinley, and C.-W. Tseng. </author> <title> Interactive parallel programming using the ParaScope Editor. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 329-341, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: An outline of the algorithm is given in Figure 5.1. This algorithm has been used with great success in the PFC compiler [13], the ParaScope programming environment <ref> [104] </ref>, and the Fortran D compiler [93, 152]. 5.3.3 Dependence Representation Data dependences are often represented using direction vectors and/or distance vectors [161]. The direction vector is an ordering vector, containing &lt;, =, &gt;, or fl, that 41 1. Partition the subscripts into separable and minimal coupled groups. 2.
Reference: [105] <author> K. Kennedy, N. Nedeljkovic, and A. Sethi. </author> <title> A linear-time algorithm for computing the memory access sequence in data-parallel programs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: For such complex cases it may be necessary to compute the local iteration sets <ref> [54, 105] </ref> or it may require the insertion of explicit guards [45, 99, 136]. We do not discuss the details of these at this point.
Reference: [106] <author> K. Kennedy and G. Roth. </author> <title> Context optimization for SIMD execution. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: This allows our stencil compilation strategy to eliminate the intraprocessor data movement in situations that other strategies would not even consider. After offset arrays have been identified and optimized, we apply our context partitioning algorithm <ref> [106] </ref>. As explained in Section 7.2, this optimization separates a set of statements into groups of congruent array statements, scalar expressions, and communication operations. This assists the compilation of stencils in two ways.
Reference: [107] <author> K. Knobe, J. Lukas, and W. Dally. </author> <title> Dynamic alignment on distributed memory systems. </title> <booktitle> In Proceedings of the Third Workshop on Compilers for Parallel Computers, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Together they created what many would consider to be the first, commercially viable, distributed-memory compiler. In addition to the general SIMD compiler development effort, Compass did much of the ground-breaking research in the area of data optimization <ref> [108, 109, 107, 111, 122] </ref>. The purpose of data optimization is to automatically align data to improve locality and thus minimize interprocessor communication. Their method assumes an unlimited number of virtual processors.
Reference: [108] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Massively parallel data optimization. </title> <booktitle> In Frontiers '88: The 2nd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> Fairfax, VA, </address> <month> October </month> <year> 1988. </year>
Reference-contexts: This strategy is used in the majority of Fortran90D/HPF compilers for SIMD architectures [123, 140, 149], many of which are descendents of compiler technology developed by Compass <ref> [7, 9, 108, 109, 110] </ref>, and is also employed by some MIMD compilers [28, 32]. We refer to such compilers as array operation compilers or native Fortran90 compilers. The main phases of a compiler exploiting this model are depicted in Figure 2.4. <p> Together they created what many would consider to be the first, commercially viable, distributed-memory compiler. In addition to the general SIMD compiler development effort, Compass did much of the ground-breaking research in the area of data optimization <ref> [108, 109, 107, 111, 122] </ref>. The purpose of data optimization is to automatically align data to improve locality and thus minimize interprocessor communication. Their method assumes an unlimited number of virtual processors.
Reference: [109] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: This strategy is used in the majority of Fortran90D/HPF compilers for SIMD architectures [123, 140, 149], many of which are descendents of compiler technology developed by Compass <ref> [7, 9, 108, 109, 110] </ref>, and is also employed by some MIMD compilers [28, 32]. We refer to such compilers as array operation compilers or native Fortran90 compilers. The main phases of a compiler exploiting this model are depicted in Figure 2.4. <p> Together they created what many would consider to be the first, commercially viable, distributed-memory compiler. In addition to the general SIMD compiler development effort, Compass did much of the ground-breaking research in the area of data optimization <ref> [108, 109, 107, 111, 122] </ref>. The purpose of data optimization is to automatically align data to improve locality and thus minimize interprocessor communication. Their method assumes an unlimited number of virtual processors. <p> The compiler uses the distribution functions discussed in Section 4.2 to determine ownership. The owner computes rule could be replaced by a data optimization phase, which may determine an alternate distribution for the computation and associated intermediate results, in an attempt to reduce the amount of communication <ref> [7, 56, 109, 119] </ref>. 4.5 Communication Generation Once data and computation distributions are finalized, the compiler must insert any necessary communication operations. These are required to move data so that all operands of an expression reside on the PE which performs the computation. <p> For this reason, much of the prior research on minimizing data movement has focused on the interprocessor case. Notable research in this area includes Knobe, Lucas and Steele's work on automatically aligning and partitioning arrays to minimize data movement associated with binary operations on arrays <ref> [109] </ref>, and Chatterjee, Gilbert, Schreiber, and Teng's work on optimally mapping the computation of array expressions to processors to minimize data movement when the mappings of data to processors are fixed [55].
Reference: [110] <author> K. Knobe, J. Lukas, and M. Weiss. </author> <title> Optimization techniques for SIMD Fortran compilers. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(7) </volume> <pages> 527-552, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: This strategy is used in the majority of Fortran90D/HPF compilers for SIMD architectures [123, 140, 149], many of which are descendents of compiler technology developed by Compass <ref> [7, 9, 108, 109, 110] </ref>, and is also employed by some MIMD compilers [28, 32]. We refer to such compilers as array operation compilers or native Fortran90 compilers. The main phases of a compiler exploiting this model are depicted in Figure 2.4. <p> The compilers they produced would be classified as array operation compilers as described in the preceding chapter. The group at Compass, along with their associates at Thinking Machines Corp., were the first to investigate the challenges of compiling Fortran90-style data-parallel constructs for execution on distributed-memory machines <ref> [7, 8, 9, 110] </ref>. Together they created what many would consider to be the first, commercially viable, distributed-memory compiler. In addition to the general SIMD compiler development effort, Compass did much of the ground-breaking research in the area of data optimization [108, 109, 107, 111, 122]. <p> This code is equivalent to the code produced by several other commercial and research compilers <ref> [34, 110, 139] </ref>. 4.6 Fortran90-level Analysis and Optimization In this phase of the compilation process, we perform array-level analysis and optimizations. The array-level analyses performed are those that are necessary to support 28 our optimizations, and include data flow analysis, dependence analysis, and the generation of static single assignment form. <p> Besides cshift operations written by users, compilers for distributed-memory machines commonly insert them to perform data movement needed for operations on array sections that have different processor mappings <ref> [110, 139, 34] </ref>. Section 4.5 describes how the communication generation phase of the compiler transforms certain array section references into calls to shift intrinsics that perform the necessary inter-processor data movement. <p> All stencil and stencil-like computations can be translated into this general form by factoring expressions and introducing temporary arrays. In fact, this is the intermediate form used by several distributed-memory compilers <ref> [110, 139, 34] </ref>.
Reference: [111] <author> K. Knobe and V. Natarajan. </author> <title> Data optimization: Minimizing residual inter-processor data motion on SIMD machines. </title> <booktitle> In Frontiers '90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: Together they created what many would consider to be the first, commercially viable, distributed-memory compiler. In addition to the general SIMD compiler development effort, Compass did much of the ground-breaking research in the area of data optimization <ref> [108, 109, 107, 111, 122] </ref>. The purpose of data optimization is to automatically align data to improve locality and thus minimize interprocessor communication. Their method assumes an unlimited number of virtual processors.
Reference: [112] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year> <month> 150 </month>
Reference-contexts: The symbol "fl" marks dimensions that are not distributed. Choosing the distribution for a decomposition maps all arrays aligned with the decomposition to the machine. Entire descriptions complete with detailed examples for both Fortran D and HPF can be found elsewhere <ref> [71, 89, 112] </ref>.
Reference: [113] <author> C. Koelbel and P. Mehrotra. </author> <title> Programming data parallel algorithms on distributed memory machines using Kali. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Hiding Communication: These transformations attempt to hide the cost of communication by overlapping communication and computation. Examples of such optimizations are communication placement [49, 84], message pipelining [136], vector message pipelining [95], and iteration reordering <ref> [113] </ref>. Improving Parallelism: Recognizing reductions and parallel-prefix scan operations [53, 114] can help to improve the available parallelism. Loop interchange and strip-mining [13, 160, 162] can be used to adjust the granularity of pipelined computations to balance parallelism and communication [93].
Reference: [114] <author> P. Kogge and H. Stone. </author> <title> A parallel algorithm for the efficient solution of a general class of recurrence equations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-22(8):786-793, </volume> <month> August </month> <year> 1973. </year>
Reference-contexts: Hiding Communication: These transformations attempt to hide the cost of communication by overlapping communication and computation. Examples of such optimizations are communication placement [49, 84], message pipelining [136], vector message pipelining [95], and iteration reordering [113]. Improving Parallelism: Recognizing reductions and parallel-prefix scan operations <ref> [53, 114] </ref> can help to improve the available parallelism. Loop interchange and strip-mining [13, 160, 162] can be used to adjust the granularity of pipelined computations to balance parallelism and communication [93].
Reference: [115] <author> U. Kremer. </author> <title> Automatic Data Layout for Distributed Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: In fact, most of these common issues were solved first by research projects concentrating on the compilation of Fortran77. The interested reader is referred to any of the following projects which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University <ref> [83, 91, 92, 93, 94, 115, 152] </ref>, the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign [78, 79, 80, 147, 22, 133], Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University [16, 145, 157].
Reference: [116] <author> Kuck & Associates, Inc. </author> <title> KAP User's Guide. </title> <address> Champaign, IL 61820, </address> <year> 1988. </year>
Reference-contexts: Thus the program 15 must be written using array syntax, array intrinsics, or forall constructs wherever possible. This requires the programmer to translate all sequential code to parallel code, either by hand or using an automated tool <ref> [116, 13] </ref>. But some code sequences containing parallelism are not expressible with these constructs; e.g., pipelined computations.
Reference: [117] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: While they were concerned with partitioning parallel and serial loops into fusible groups, we are partitioning Fortran90 statements into congruence classes. The algorithm works on the data dependence graph (ddg) <ref> [117] </ref> which must be acyclic. Since we apply it to a set of statements within a basic block, our dependence graph contains only loop-independent dependences [13] and thus meets that criterion. <p> This produces a set of imperfectly nested DO-loops. We then apply loop distribution <ref> [117, 129] </ref> to produce a set of perfectly nested DO-loops, each of which operates under a single context.
Reference: [118] <author> M. Lam, E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Improve loop balance through unroll-and-jam and scalar replacement [43, 46]. Note that strip-mine-and-interchange can be included here [158]. We have omitted it because of its relative instability and the large amount of cache reuse that already exists in stencil computations <ref> [63, 118] </ref>. In the rest of this section we give an overview of loop permutation, unroll-and-jam and scalar replacement. More information can be found elsewhere [125]. 9.6.1 Loop Permutation Not all loops exhibit good cache locality, resulting in idle computational cycles while waiting for main memory to return data.
Reference: [119] <author> J. Li and M. Chen. </author> <title> Index domain alignment: Minimizing cost of cross-referencing between distributed arrays. </title> <booktitle> In Frontiers '90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: The compiler uses the distribution functions discussed in Section 4.2 to determine ownership. The owner computes rule could be replaced by a data optimization phase, which may determine an alternate distribution for the computation and associated intermediate results, in an attempt to reduce the amount of communication <ref> [7, 56, 109, 119] </ref>. 4.5 Communication Generation Once data and computation distributions are finalized, the compiler must insert any necessary communication operations. These are required to move data so that all operands of an expression reside on the PE which performs the computation.
Reference: [120] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: This is used to generate required communication and to partition the computation among the PEs. And as communication operations are so expensive, an attempt is usually made to optimize them using methods such as message vectoriza-tion [21, 73, 91], message aggregation <ref> [120, 131, 152] </ref>, and the exploitation of collective communication operations [120]. The major advantage of this model is its simplicity. The compiler takes the parallelism that is explicitly stated by the programmer and maps it to the parallel 14 operation Fortran90D/HPF compiler. hardware. <p> And as communication operations are so expensive, an attempt is usually made to optimize them using methods such as message vectoriza-tion [21, 73, 91], message aggregation [120, 131, 152], and the exploitation of collective communication operations <ref> [120] </ref>. The major advantage of this model is its simplicity. The compiler takes the parallelism that is explicitly stated by the programmer and maps it to the parallel 14 operation Fortran90D/HPF compiler. hardware. <p> For these reasons a Fortran90D compiler should exploit collective communication routines whenever possible. This requires the compiler to recognize applicable patterns in the array syntax used in assignment statements. Our compiler uses a variant of the pattern matching techniques proposed by Li and Chen <ref> [120] </ref>. This requires an analysis of the array subscripts that are used, in conjunction with information about the array's alignment and distribution. After the communication operations have been inserted, all computations reference data that are strictly local to the associated PEs. <p> These optimizations fall into several categories: Reducing Communication: Here we perform optimizations that attempt to reduce the amount of communication. These include message vectorization [21, 73], message coalescing [95], message aggregation <ref> [120, 131] </ref>, redundant communication elimination [17], and the exploitation of collective communication [120]. Hiding Communication: These transformations attempt to hide the cost of communication by overlapping communication and computation. Examples of such optimizations are communication placement [49, 84], message pipelining [136], vector message pipelining [95], and iteration reordering [113]. <p> These optimizations fall into several categories: Reducing Communication: Here we perform optimizations that attempt to reduce the amount of communication. These include message vectorization [21, 73], message coalescing [95], message aggregation [120, 131], redundant communication elimination [17], and the exploitation of collective communication <ref> [120] </ref>. Hiding Communication: These transformations attempt to hide the cost of communication by overlapping communication and computation. Examples of such optimizations are communication placement [49, 84], message pipelining [136], vector message pipelining [95], and iteration reordering [113].
Reference: [121] <author> Z. Li, P. Yew, and C. Zhu. </author> <title> Data dependence analysis on multi-dimensional array references. </title> <booktitle> In Proceedings of the 1989 ACM International Conference on Supercomputing, </booktitle> <address> Crete, Greece, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Separability refers to whether or not different subscript positions contain common induction variables. A subscript position is separable if the indices it contains do not appear in other subscript positions [12, 41]. If different subscript positions contain the same index, they are said to be coupled <ref> [121] </ref>. The concept of separability is important when testing multidimensional arrays in that it allows dependence testing to proceed subscript-by-subscript without a loss of precision. In contrast, coupled subscripts must be tested as a group to obtain exact results. <p> If the subscripts became coupled because corresponding triplets did not appear in matching subscript positions, then the linear functions generated for the corresponding triplets share the same pseudo-induction variable. Once the triplets have been translated, we exploit whichever multi-subscript test is available in our system <ref> [121, 134, 164] </ref>. 5.4 Static Single Assignment Form In recent years, Static Single Assignment (SSA) form [64, 65] and related intermediate representations have gained in popularity because of their efficiency in program analysis and transformations [15, 137, 155].
Reference: [122] <author> J. Lukas and K. Knobe. </author> <title> Data optimization and its effect on communication costs in MIMD Fortran code. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Houston, TX, </address> <month> March </month> <year> 1991. </year>
Reference-contexts: Together they created what many would consider to be the first, commercially viable, distributed-memory compiler. In addition to the general SIMD compiler development effort, Compass did much of the ground-breaking research in the area of data optimization <ref> [108, 109, 107, 111, 122] </ref>. The purpose of data optimization is to automatically align data to improve locality and thus minimize interprocessor communication. Their method assumes an unlimited number of virtual processors.
Reference: [123] <institution> MasPar Computer Corporation, Sunnyvale, CA. </institution> <note> MasPar Fortran Reference Manual, software version 1.1 edition, </note> <month> August </month> <year> 1991. </year>
Reference-contexts: Such compilers are characterized by their ability to directly translate the data parallelism found in array expressions or forall statements for execution on the distributed-memory machine. This strategy is used in the majority of Fortran90D/HPF compilers for SIMD architectures <ref> [123, 140, 149] </ref>, many of which are descendents of compiler technology developed by Compass [7, 9, 108, 109, 110], and is also employed by some MIMD compilers [28, 32]. We refer to such compilers as array operation compilers or native Fortran90 compilers.
Reference: [124] <institution> MasPar Computer Corporation, Sunnyvale, CA. MasPar System Overview, </institution> <month> March </month> <year> 1991. </year> <month> 151 </month>
Reference-contexts: It is the responsibility of the compiler to generate the code to set the correct PE context for all computations executed on the PE array. Examples of SIMD machines include the CM-2/CM-200 from Thinking Machines Corporation [90, 148, 150], and the MP-1/MP-2 from MasPar <ref> [26, 130, 124] </ref>. A simple diagram of a SIMD machine can be seen in Figure 2.2. 9 2.2 Fortran90 Fortran90 [18] adds a number of interesting and useful features to the Fortran language, the most popular being the array features.
Reference: [125] <author> K. S. McKinley, S. Carr, and C.-W. Tseng. </author> <title> Improving data locality with loop transformations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 18(4) </volume> <pages> 424-453, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: We have omitted it because of its relative instability and the large amount of cache reuse that already exists in stencil computations [63, 118]. In the rest of this section we give an overview of loop permutation, unroll-and-jam and scalar replacement. More information can be found elsewhere <ref> [125] </ref>. 9.6.1 Loop Permutation Not all loops exhibit good cache locality, resulting in idle computational cycles while waiting for main memory to return data.
Reference: [126] <author> L. Meadows, D. Miles, C. Walinsky, M. Young, and R. Touzeau. </author> <title> The Intel Paragon HPF compiler. </title> <booktitle> In Proceedings of the 1995 Intel Supercomputer Users Group, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: enhances the opportunities for analysis and makes it possible to eliminate extra array temporaries and copying an important consideration for any Fortran90 compiler which desires to achieve performance comparable to Fortran77 programs. pghpf The pghpf compiler, from The Portland Group Inc., is very similar in design to NPAC's Fortran90D compiler <ref> [32, 31, 126] </ref>. This is no surprise given the close association the two groups have had during recent years. The compiler still relies heavily on run-time routines, which enables it to target both shared-memory and distributed-memory machines.
Reference: [127] <author> M. Metcalf and J. Reid. </author> <title> Fortran 90 Explained. </title> <publisher> Oxford Science Publications, </publisher> <year> 1990. </year>
Reference-contexts: Other array-related features of Fortran90 include allocatable arrays, pointers, and the where statement. For more information on these or any of the above mentioned constructs, the reader is referred to an appropriate text <ref> [2, 127] </ref>. 10 2.3 Fortran D and High Performance Fortran Fortran D and High Performance Fortran (HPF) are versions of Fortran that have been designed to assist both the programmer and compiler in producing efficient data-parallel programs for distributed-memory machines.
Reference: [128] <author> A. Mohamed, G. Fox, G. v. Laszewski, M. Parashar, T. Haupt, K. Mills, Y. Lu, N. Lin, and N. Yeh. </author> <title> Applications benchmark set for Fortran-D and High Performance Fortran. </title> <type> Technical Report SCCS-327, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, Syracuse, </institution> <address> NY, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The majority of the codes we tested were taken from the High Performance Fortran Application (HPFA) project <ref> [86, 128] </ref> at the Northeast Parallel Architecture Center (NPAC), Syracuse University. 6 The HPFA project contains codes ranging from code fragments and benchmarking kernels to complete applications. Many of the codes are available in Fortran90 and HPF source, as well as Fortran77 and message-passing source. <p> It will also demonstrate how we are able to handle stencil computations that cause other methods to fail. For this exercise, we have chosen to use Problem 9 of the Purdue Set [135], as adapted for Fortran D benchmarking by Thomas Haupt of NPAC <ref> [128, 86] </ref>. The program kernel is shown in Figure 9.11. The arrays T, U, RIP, and RIN are all two-dimensional and have been distributed in a (block,block) fashion. This kernel computes a standard 9-point stencil, identical to that computed by the single-statement stencil shown in Figure 9.2.
Reference: [129] <author> Y. Muraoka. </author> <title> Parallelism Exposure and Exploitation in Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> February </month> <year> 1971. </year> <note> Report No. 71-424. </note>
Reference-contexts: This produces a set of imperfectly nested DO-loops. We then apply loop distribution <ref> [117, 129] </ref> to produce a set of perfectly nested DO-loops, each of which operates under a single context.
Reference: [130] <author> J. Nickolls. </author> <title> The design of the MasPar MP-1: A cost effective massively parallel computer. </title> <booktitle> In Proceedings of the 1990 Spring COMPCON, </booktitle> <address> San Francisco, CA, </address> <month> February </month> <year> 1990. </year>
Reference-contexts: It is the responsibility of the compiler to generate the code to set the correct PE context for all computations executed on the PE array. Examples of SIMD machines include the CM-2/CM-200 from Thinking Machines Corporation [90, 148, 150], and the MP-1/MP-2 from MasPar <ref> [26, 130, 124] </ref>. A simple diagram of a SIMD machine can be seen in Figure 2.2. 9 2.2 Fortran90 Fortran90 [18] adds a number of interesting and useful features to the Fortran language, the most popular being the array features.
Reference: [131] <author> D. Palermo, E. Su, J. Chandy, and P. Banerjee. </author> <title> Communication optimizations used in the Paradigm compiler for distributed-memory multicomputers. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: This is used to generate required communication and to partition the computation among the PEs. And as communication operations are so expensive, an attempt is usually made to optimize them using methods such as message vectoriza-tion [21, 73, 91], message aggregation <ref> [120, 131, 152] </ref>, and the exploitation of collective communication operations [120]. The major advantage of this model is its simplicity. The compiler takes the parallelism that is explicitly stated by the programmer and maps it to the parallel 14 operation Fortran90D/HPF compiler. hardware. <p> These optimizations fall into several categories: Reducing Communication: Here we perform optimizations that attempt to reduce the amount of communication. These include message vectorization [21, 73], message coalescing [95], message aggregation <ref> [120, 131] </ref>, redundant communication elimination [17], and the exploitation of collective communication [120]. Hiding Communication: These transformations attempt to hide the cost of communication by overlapping communication and computation. Examples of such optimizations are communication placement [49, 84], message pipelining [136], vector message pipelining [95], and iteration reordering [113].
Reference: [132] <author> J. Palmer and G. Steele, Jr. </author> <title> Connection Machine model CM-5 system overview. </title> <booktitle> In Frontiers '92: The 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: This two stage approach makes each stage conceptually clean, but prevents them from interacting. CM Fortran Thinking Machines Corporation developed three generations of distributed-memory architectures, the first two being SIMD machines (the CM-1 and CM-2) [148] and the third being a MIMD machine (the CM-5) <ref> [132, 151] </ref>. CM Fortran, their Fortran derivative, was an implementation of Fortran77 augmented with array constructs from Fortran90. Their compiler for CM Fortran was also developed through three generations. The first generation was the Paris, or fieldwise, compiler which uses the bit-serial processors on the CM-1 and CM-2.
Reference: [133] <author> C. Polychronopoulos, M. Girkar, M. Haghighat, C. Lee, B. Leung, and D. Schouten. </author> <title> The structure of Parafrase-2: An advanced parallelizing compiler for C and Fortran. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The interested reader is referred to any of the following projects which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University [83, 91, 92, 93, 94, 115, 152], the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign <ref> [78, 79, 80, 147, 22, 133] </ref>, Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University [16, 145, 157].
Reference: [134] <author> W. Pugh. </author> <title> A practical algorithm for exact array dependence analysis. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: If the subscripts became coupled because corresponding triplets did not appear in matching subscript positions, then the linear functions generated for the corresponding triplets share the same pseudo-induction variable. Once the triplets have been translated, we exploit whichever multi-subscript test is available in our system <ref> [121, 134, 164] </ref>. 5.4 Static Single Assignment Form In recent years, Static Single Assignment (SSA) form [64, 65] and related intermediate representations have gained in popularity because of their efficiency in program analysis and transformations [15, 137, 155].
Reference: [135] <author> J. R. Rice and J. Jing. </author> <title> Problems to test parallel and vector languages. </title> <type> Technical Report CSD-TR-1016, </type> <institution> Dept. of Computer Science, Purdue University, </institution> <year> 1990. </year> <month> 152 </month>
Reference-contexts: + RIN T = T + CSHIFT (U,SHIFT=-1,DIM=2) T = T + CSHIFT (U,SHIFT=+1,DIM=2) T = T + CSHIFT (RIP,SHIFT=-1,DIM=2) T = T + CSHIFT (RIP,SHIFT=+1,DIM=2) T = T + CSHIFT (RIN,SHIFT=-1,DIM=2) T = T + CSHIFT (RIN,SHIFT=+1,DIM=2) 9 This example was taken from Problem 9 of the Purdue Set <ref> [135] </ref> as adapted for Fortran D bench marking by Thomas Haupt of NPAC. 112 CM Fortran compiler would translate it into the sequence of statements shown in For the rest of this chapter we assume that all stencil computations have been put into this form, and that all arrays are distributed <p> It will also demonstrate how we are able to handle stencil computations that cause other methods to fail. For this exercise, we have chosen to use Problem 9 of the Purdue Set <ref> [135] </ref>, as adapted for Fortran D benchmarking by Thomas Haupt of NPAC [128, 86]. The program kernel is shown in Figure 9.11. The arrays T, U, RIP, and RIN are all two-dimensional and have been distributed in a (block,block) fashion.
Reference: [136] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Hiding Communication: These transformations attempt to hide the cost of communication by overlapping communication and computation. Examples of such optimizations are communication placement [49, 84], message pipelining <ref> [136] </ref>, vector message pipelining [95], and iteration reordering [113]. Improving Parallelism: Recognizing reductions and parallel-prefix scan operations [53, 114] can help to improve the available parallelism. Loop interchange and strip-mining [13, 160, 162] can be used to adjust the granularity of pipelined computations to balance parallelism and communication [93]. <p> For such complex cases it may be necessary to compute the local iteration sets [54, 105] or it may require the insertion of explicit guards <ref> [45, 99, 136] </ref>. We do not discuss the details of these at this point. SIMD Context Switching Since SIMD machines have a single instruction stream, the subgrid iteration space must include the union of all the iteration spaces required by the individual PEs.
Reference: [137] <author> B. Rosen, M. Wegman, and K. Zadeck. </author> <title> Global value numbers and redundant compuations. </title> <booktitle> In Proceedings of the Fifteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: the triplets have been translated, we exploit whichever multi-subscript test is available in our system [121, 134, 164]. 5.4 Static Single Assignment Form In recent years, Static Single Assignment (SSA) form [64, 65] and related intermediate representations have gained in popularity because of their efficiency in program analysis and transformations <ref> [15, 137, 155] </ref>. SSA is loosely characterized by the trait that each variable has only a single definition. This is achieved by creating a new instance of a variable, typically indicated by a subscript, each time the variable is assigned a new value.
Reference: [138] <author> G. Roth and K. Kennedy. </author> <title> Dependence analysis of Fortran90 array syntax. </title> <booktitle> In Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications (PDPTA'96), </booktitle> <address> Sunnyvale, CA, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: We call these dependences scalarization dependences <ref> [138] </ref>. Since scalarization dependences arise from parallel constructs in the Fortran90 program, they do not have the same behavior as non-parallel dependences. Note that it is valid for any of the three direction specifiers to appear in the triplet-related vector positions. <p> produced by the methods described in Chapter 5, we propose a new algorithm that eliminates the need for the first pass and is able to determine a valid scalarization before attempting any transformations. 8.3 One-Pass Scalarization Our new scalarization algorithm begins by performing dependence analysis directly on Fortran90 array statements <ref> [138] </ref>. When attempting to scalarize an array statement, we only need to be concerned with the scalarization dependences of that statement on itself. As discussed in Section 5.3.4, such dependences are always antidependences and may contain any of the three direction specifiers in triplet positions.
Reference: [139] <author> G. Sabot. </author> <title> A compiler for a massively parallel distributed memory MIMD computer. </title> <booktitle> In Frontiers '92: The 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The CM Fortran compiler can take advantage of the slicewise model of the machine in different ways to improve program performance for many engineering and scientific applications. The third generation compiler targeted the CM-5, and was basically an updated version of the slicewise compiler <ref> [139, 141] </ref>. As such, it treated the CM-5 as 18 a SIMD machine and was thus not able to produce code that took full advantage of the architecture. Even though the slicewise compiler gave improved performance, it also had several weaknesses. <p> This code is equivalent to the code produced by several other commercial and research compilers <ref> [34, 110, 139] </ref>. 4.6 Fortran90-level Analysis and Optimization In this phase of the compilation process, we perform array-level analysis and optimizations. The array-level analyses performed are those that are necessary to support 28 our optimizations, and include data flow analysis, dependence analysis, and the generation of static single assignment form. <p> Besides cshift operations written by users, compilers for distributed-memory machines commonly insert them to perform data movement needed for operations on array sections that have different processor mappings <ref> [110, 139, 34] </ref>. Section 4.5 describes how the communication generation phase of the compiler transforms certain array section references into calls to shift intrinsics that perform the necessary inter-processor data movement. <p> All stencil and stencil-like computations can be translated into this general form by factoring expressions and introducing temporary arrays. In fact, this is the intermediate form used by several distributed-memory compilers <ref> [110, 139, 34] </ref>.
Reference: [140] <author> G. Sabot. </author> <title> Optimized CM Fortran compiler for the Connection Machine computer. </title> <booktitle> In Proceedings of the 25th Annual Hawaii International Conference on System Sciences, </booktitle> <address> Kauai, HI, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: Such compilers are characterized by their ability to directly translate the data parallelism found in array expressions or forall statements for execution on the distributed-memory machine. This strategy is used in the majority of Fortran90D/HPF compilers for SIMD architectures <ref> [123, 140, 149] </ref>, many of which are descendents of compiler technology developed by Compass [7, 9, 108, 109, 110], and is also employed by some MIMD compilers [28, 32]. We refer to such compilers as array operation compilers or native Fortran90 compilers. <p> Their compiler for CM Fortran was also developed through three generations. The first generation was the Paris, or fieldwise, compiler which uses the bit-serial processors on the CM-1 and CM-2. The second generation CM Fortran compiler was the slicewise compiler <ref> [140] </ref>. The slicewise compiler ignored the bit-serial processors and used only the floating-point accelerator chips of the CM-2. The CM Fortran compiler can take advantage of the slicewise model of the machine in different ways to improve program performance for many engineering and scientific applications. <p> In a later paper describing the internals of the compiler, he describes how it attempts to perform code motion so that subgrid loops may become adjacent and thus fused <ref> [140] </ref>. However, the code motion performed is limited to only moving scalar code from between subgrid loops, not in moving the loops themselves. Furthermore, due to the limited dependence analysis performed by the compiler, only compiler-generated scalar code is moved. <p> Array-syntax stencils produced the same CSHIFT intrinsic calls but then had the additional overhead of the vector masking operations required for handling the array subsections <ref> [140] </ref>. 113 Since the costs of these two actions were discussed in Chapter 6, we do not address them further here. The sum of products is calculated within the subgrid loop nest, which is the result of scalarization and SPMD code generation as discussed in Section 4.8.
Reference: [141] <author> G. Sabot. </author> <title> Optimizing CM Fortran compiler for Connection Machine computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 23(1) </volume> <pages> 224-238, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The CM Fortran compiler can take advantage of the slicewise model of the machine in different ways to improve program performance for many engineering and scientific applications. The third generation compiler targeted the CM-5, and was basically an updated version of the slicewise compiler <ref> [139, 141] </ref>. As such, it treated the CM-5 as 18 a SIMD machine and was thus not able to produce code that took full advantage of the architecture. Even though the slicewise compiler gave improved performance, it also had several weaknesses.
Reference: [142] <author> G. Sabot, (with D. Gingold, and J. Marantz). </author> <title> CM Fortran optimization notes: Slicewise model. </title> <type> Technical Report TMC-184, </type> <institution> Thinking Machines Corporation, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Thinking Machines documented many of the shortcomings, and suggested methods that programmers may use to work around them <ref> [142] </ref>. Thinking Machines had also done some extensive work on compiling stencils [39]. A stencil is a computational pattern that calculates a new value for a matrix element by combining elements from neighboring matrix locations. <p> Continuing our previous example X (1:256) = X (1:256) + 1.0, where the array X is distributed as in Figure 4.3, scalarization generates the following loop. DO I = 1, 256 ENDDO 1 Congruence is a stronger restriction than conformance <ref> [142] </ref>, which just considers shape and size. 32 Loop bounds reduction alters this loop to create the following loop, DO I = 1, Extent 1 ! Extent 1 = 16 X 0 (I) = X 0 (I) + 1.0 ENDDO where Extent 1 is the size of the subgrid and is <p> However, unless an effort is made to make congruent array statements adjacent, many small subgrid loops may still be generated. Sabot recognized this problem, and recommended that users of the CM Fortran compiler rearrange program statements, when possible, to avoid the inefficiencies of such sub-grid loops <ref> [142] </ref>. In order to alleviate this problem automatically, our compiler has an optimization phase that reorders statements within a basic block. The reordering attempts to create separate partitions of congruent array statements, scalar statements, and communication statements. We call this optimization context partitioning. <p> While giving some optimization hints for the Slicewise CM Fortran compiler, Sabot describes the need for code motion to increase the size of elemental code blocks (blocks of code for which a single subgrid loop can be generated) <ref> [142] </ref>. He goes on to state that the compiler does not perform this code motion on user code, and thus it is up to the programmer to make them as large as possible.
Reference: [143] <author> P. Schnorf, M. Ganapathi, and J. Hennessy. </author> <title> Compile-time copy elimination. </title> <journal> Software|Practice and Experience, </journal> <volume> 23(11) </volume> <pages> 1175-1200, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Unfortunately, solving their equations to a fixpoint is at least exponential in time. To avoid this cost within their implementation of the SAL language, they use type information to "guess" the fixpoint, which they can then verify in linear time. Schnorf et al. <ref> [143] </ref> describe their efforts to eliminate aggregate copies in the single-assignment language SISAL. Their work analyzes edges in a data flow graph and attempts to determine when edges, representing values, may share storage.
Reference: [144] <author> J. T. Schwartz. </author> <title> Optimization of very high level languages I. Value transmission and its corollaries. </title> <journal> Computer Languages, </journal> <volume> 1(2) </volume> <pages> 161-194, </pages> <year> 1975. </year>
Reference-contexts: The source array src is not modified while this definition of dst is live. 2. The destination array dst is not partially modified 5 while src is live. From the work on copy elimination in functional and higher-order programming languages <ref> [144] </ref>, we know that the above two criteria are necessary and sufficient conditions for when the two objects can share the same storage. However, the sharing of storage may not always be profitable. To insure profitability, we add the following efficiency criteria: 3. <p> It is imperative that compilers for such languages eliminate a majority of the unnecessary copies if they hope to generate efficient code. This task is known as copy optimization. Although our problem differs somewhat from those faced by functional languages, it is worthwhile to review their work. Schwartz <ref> [144] </ref> characterizes the task of copy optimization as the destructive use (reuse) of an object v at a point P in the program where it can be shown that all other objects that may contain v are dead at P .
Reference: [145] <author> Stanford SUIF Compiler Group. </author> <title> SUIF: A parallelizing & optimizing research compiler. </title> <type> Technical Report CSL-TR-94-620, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: system at Rice University [83, 91, 92, 93, 94, 115, 152], the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign [78, 79, 80, 147, 22, 133], Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University <ref> [16, 145, 157] </ref>. For the rest of this chapter we concentrate solely on projects whose main purpose is the compilation of Fortran90 constructs. Compass Compilers Compass (1961-1991) was an independent software house which was involved in the design and implementation of several SIMD compilers.
Reference: [146] <author> E. De Sturler and V. Strumpen. </author> <title> Scientific programming with High Performance Fortran: A case study using the xHPF compiler. </title> <journal> Scientific Programming, </journal> <volume> 6(1) </volume> <pages> 127-152, </pages> <year> 1997. </year>
Reference-contexts: And finally, the compiler performs enhanced analysis and optimization of the resulting node program, including vectorizing loops for certain architectures. xHPF Applied Parallel Research Inc.'s xHPF translator is the most recent addition to the FORGE90 parallel programming environment <ref> [19, 146, 75] </ref>. The system is identical to the company's xHPF77 system, with the addition of a preprocessor that converts Fortran90 syntax into Fortran77. This structure classifies the compiler as a scalarizing HPF compiler, similar to IBM's xlhpf.
Reference: [147] <author> E. Su, A. Lain, S. Ramaswamy, D. J. Palermo, E. W. Hodges IV, and P. Banerjee. </author> <title> Advanced compilation techniques in the PARADIGM compiler for distributed-memory multicomputers. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year> <month> 153 </month>
Reference-contexts: The interested reader is referred to any of the following projects which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University [83, 91, 92, 93, 94, 115, 152], the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign <ref> [78, 79, 80, 147, 22, 133] </ref>, Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University [16, 145, 157].
Reference: [148] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> Connection Machine CM-2 Technical Summary, </note> <month> April </month> <year> 1987. </year>
Reference-contexts: It is the responsibility of the compiler to generate the code to set the correct PE context for all computations executed on the PE array. Examples of SIMD machines include the CM-2/CM-200 from Thinking Machines Corporation <ref> [90, 148, 150] </ref>, and the MP-1/MP-2 from MasPar [26, 130, 124]. A simple diagram of a SIMD machine can be seen in Figure 2.2. 9 2.2 Fortran90 Fortran90 [18] adds a number of interesting and useful features to the Fortran language, the most popular being the array features. <p> This two stage approach makes each stage conceptually clean, but prevents them from interacting. CM Fortran Thinking Machines Corporation developed three generations of distributed-memory architectures, the first two being SIMD machines (the CM-1 and CM-2) <ref> [148] </ref> and the third being a MIMD machine (the CM-5) [132, 151]. CM Fortran, their Fortran derivative, was an implementation of Fortran77 augmented with array constructs from Fortran90. Their compiler for CM Fortran was also developed through three generations.
Reference: [149] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM Fortran Reference Manual, version 1.0 edition, </note> <month> February </month> <year> 1991. </year>
Reference-contexts: Such compilers are characterized by their ability to directly translate the data parallelism found in array expressions or forall statements for execution on the distributed-memory machine. This strategy is used in the majority of Fortran90D/HPF compilers for SIMD architectures <ref> [123, 140, 149] </ref>, many of which are descendents of compiler technology developed by Compass [7, 9, 108, 109, 110], and is also employed by some MIMD compilers [28, 32]. We refer to such compilers as array operation compilers or native Fortran90 compilers.
Reference: [150] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> Connection Machine CM-200 Technical Summary, </note> <month> June </month> <year> 1991. </year>
Reference-contexts: It is the responsibility of the compiler to generate the code to set the correct PE context for all computations executed on the PE array. Examples of SIMD machines include the CM-2/CM-200 from Thinking Machines Corporation <ref> [90, 148, 150] </ref>, and the MP-1/MP-2 from MasPar [26, 130, 124]. A simple diagram of a SIMD machine can be seen in Figure 2.2. 9 2.2 Fortran90 Fortran90 [18] adds a number of interesting and useful features to the Fortran language, the most popular being the array features.
Reference: [151] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <booktitle> The Connection Machine CM5 Technical Summary, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: This two stage approach makes each stage conceptually clean, but prevents them from interacting. CM Fortran Thinking Machines Corporation developed three generations of distributed-memory architectures, the first two being SIMD machines (the CM-1 and CM-2) [148] and the third being a MIMD machine (the CM-5) <ref> [132, 151] </ref>. CM Fortran, their Fortran derivative, was an implementation of Fortran77 augmented with array constructs from Fortran90. Their compiler for CM Fortran was also developed through three generations. The first generation was the Paris, or fieldwise, compiler which uses the bit-serial processors on the CM-1 and CM-2.
Reference: [152] <author> C.-W. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: This allows for the parallelization and distribution of computation not only for code written with parallel constructs but also for parallelizable code that is not expressible using array syntax or forall; e.g., pipelined computations <ref> [93, 152, 58] </ref>. The major drawback of this scheme is that the scalarization process can obfuscate the code, making it much more difficult to analyze and optimize than the original 13 Fortran90D code. <p> This is used to generate required communication and to partition the computation among the PEs. And as communication operations are so expensive, an attempt is usually made to optimize them using methods such as message vectoriza-tion [21, 73, 91], message aggregation <ref> [120, 131, 152] </ref>, and the exploitation of collective communication operations [120]. The major advantage of this model is its simplicity. The compiler takes the parallelism that is explicitly stated by the programmer and maps it to the parallel 14 operation Fortran90D/HPF compiler. hardware. <p> But some code sequences containing parallelism are not expressible with these constructs; e.g., pipelined computations. In such cases a compiler must have dependence information to detect the available parallelism and to perform the necessary loop tiling transformations to effectively exploit it <ref> [93, 152] </ref>. 2.5 Summary In this chapter of background information we have introduced the Fortran90D and HPF languages, and have given an overview of the target distributed-memory architectures. We have also outlined two possible models that a compiler for these languages might utilize. <p> In fact, most of these common issues were solved first by research projects concentrating on the compilation of Fortran77. The interested reader is referred to any of the following projects which address the basics of distributed-memory compilers: the Fortran D compilation system at Rice University <ref> [83, 91, 92, 93, 94, 115, 152] </ref>, the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign [78, 79, 80, 147, 22, 133], Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University [16, 145, 157]. <p> These are required to move data so that all operands of an expression reside on the PE which performs the computation. A Fortran77D compiler <ref> [152] </ref> generates individual Send and Receive pairs for non 26 mapped in a cyclic manner onto a 16 PE machine. fashion onto a 16 PE machine configured as a 4 fi 4 matrix. local data accesses, and then depend upon later compilation phases to optimize them. <p> Message block ing [95] can be used in situations where buffer storage is limited. For details on these optimizations see the individual citations or refer to Tseng's dissertation <ref> [152] </ref> where they are all discussed in terms of an optimizing Fortran77D compiler. 29 4.8 Scalarization and Subgrid Looping As a final step in the compilation process the compiler must generate code that iterates over the subgrids allocated to the individual PEs. This step is composed of several sub-steps. <p> An outline of the algorithm is given in Figure 5.1. This algorithm has been used with great success in the PFC compiler [13], the ParaScope programming environment [104], and the Fortran D compiler <ref> [93, 152] </ref>. 5.3.3 Dependence Representation Data dependences are often represented using direction vectors and/or distance vectors [161]. The direction vector is an ordering vector, containing &lt;, =, &gt;, or fl, that 41 1. Partition the subscripts into separable and minimal coupled groups. 2. <p> This is due to the existence of the conditional and IMOD function within the second loop. When faced with such loops, Tseng <ref> [152] </ref> proposes using loop distribution to avoid the complications of code generation. But this is actually doing double work: one phase of the Fortran90D compiler fuses loops only to have a later phase distribute them. The proper resolution is to avoid such loop fusion in the first place. <p> By reducing the loop bounds, the compiler can often avoid iterations for which the PE has no work and thus does not need to introduce any guard statements into the subgrid loop body in those cases <ref> [74, 152] </ref>. We now discuss the details of context splitting. To simplify the discussion, we first discuss one-dimensional cyclic, block, and block cyclic distributions, and then show how to combine one-dimensional splitting to handle multidimensional cases.
Reference: [153] <author> P.-S. Tseng. </author> <title> A parallelizing compiler for distributed memory parallel computers. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The search for parallelism is simplified by considering only those loop nests which iterate over the distributed dimensions of the arrays. The methods used to detect and exploit data parallelism in sequential code have been well documented <ref> [12, 13, 25, 153, 161, 163] </ref> and will not be discussed any further here. 4.4 Computation Partitioning The next step is to map the parallel operations to the processors.
Reference: [154] <author> J. Warren. </author> <title> A hierachical basis for reordering transformations. </title> <booktitle> In Conference Record of the Eleventh Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Salt Lake City, UT, </address> <month> January </month> <year> 1984. </year>
Reference-contexts: The fusion of loops, however, is not always safe. A data dependence between two adjacent loops is called fusion-preventing if after fusion the direction of the dependence is reversed <ref> [1, 154] </ref>. The existence of such a dependence means that fusion is not safe. In our current model however, no such fusion-preventing dependences can exist between adjacent scalarized loops. This is due to the fact that the generation of communication causes all subgrid loops to operate on "perfectly aligned" data. <p> This is particularly critical in Fortran90 compilation, where the scalarization of array statements generates many loop nests each containing a single assignment statement. In previous work on loop fusion for parallel machines <ref> [42, 154, 159] </ref> two loops are candidates for fusion if their headers are conformable and there do not exist any fusion-preventing dependences. Two loop headers are conformable if they specify the same number of iterations and are both either parallel or sequential loops.
Reference: [155] <author> M. Wegman and K. Zadeck. </author> <title> Constant propagation with conditional branches. </title> <booktitle> In Conference Record of the Twelfth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> New Orleans, LA, </address> <month> January </month> <year> 1985. </year>
Reference-contexts: the triplets have been translated, we exploit whichever multi-subscript test is available in our system [121, 134, 164]. 5.4 Static Single Assignment Form In recent years, Static Single Assignment (SSA) form [64, 65] and related intermediate representations have gained in popularity because of their efficiency in program analysis and transformations <ref> [15, 137, 155] </ref>. SSA is loosely characterized by the trait that each variable has only a single definition. This is achieved by creating a new instance of a variable, typically indicated by a subscript, each time the variable is assigned a new value.
Reference: [156] <author> M. Weiss. </author> <title> Strip mining on SIMD architectures. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Then based on usage patterns, it maps arrays to the virtual processors, striving to align them so that communication costs are minimized. A later stage of the compiler then uses strip mining to map the virtual processors to the physical processors <ref> [156] </ref>, also known as array distribution. This two stage approach makes each stage conceptually clean, but prevents them from interacting. <p> The resulting loop is known as the subgrid loop. Some Fortran90 compilers combine scalarization and loop bounds reduction into a single step <ref> [156] </ref>. We have separated them since our compiler needs to also lower the bounds of Fortran77 loops written by the programmer. Continuing our previous example X (1:256) = X (1:256) + 1.0, where the array X is distributed as in Figure 4.3, scalarization generates the following loop.
Reference: [157] <author> R. Wilson et al. </author> <title> SUIF: An infrastructure for research on parallelizing and optimizing compilers. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29(12) </volume> <pages> 31-37, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: system at Rice University [83, 91, 92, 93, 94, 115, 152], the Parafrase-2 and Paradigm compilers at University of Illinois at Urbana-Champaign [78, 79, 80, 147, 22, 133], Vienna Fortran and the SUPERB-2 system at the University of Vienna [52, 50, 51, 68], and the SUIF project at Stanford University <ref> [16, 145, 157] </ref>. For the rest of this chapter we concentrate solely on projects whose main purpose is the compilation of Fortran90 constructs. Compass Compilers Compass (1961-1991) was an independent software house which was involved in the design and implementation of several SIMD compilers.
Reference: [158] <author> M. E. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Our strategy involves the following compiler optimizations to improve data locality: 1. Improve the order of memory accesses through loop permutation [47]. 2. Improve loop balance through unroll-and-jam and scalar replacement [43, 46]. Note that strip-mine-and-interchange can be included here <ref> [158] </ref>. We have omitted it because of its relative instability and the large amount of cache reuse that already exists in stencil computations [63, 118]. In the rest of this section we give an overview of loop permutation, unroll-and-jam and scalar replacement. <p> In this case, we have attained locality of reference for B (I,J) and A (I+1,J) by moving reuse points closer together. The result is fewer idle cycles waiting on main memory. For a more complete discussion of loop permutation see Wolf and Lam <ref> [158] </ref>, Kennedy and McKinley [102] and Carr, et al. [47]. 9.6.2 Scalar Replacement Even with better cache performance through loop permutation, a loop may still not perform as well as possible. If a loop is memory bound, then its balance must be lowered.
Reference: [159] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1982. </year>
Reference-contexts: This is particularly critical in Fortran90 compilation, where the scalarization of array statements generates many loop nests each containing a single assignment statement. In previous work on loop fusion for parallel machines <ref> [42, 154, 159] </ref> two loops are candidates for fusion if their headers are conformable and there do not exist any fusion-preventing dependences. Two loop headers are conformable if they specify the same number of iterations and are both either parallel or sequential loops.
Reference: [160] <author> M. J. Wolfe. </author> <title> Advanced loop interchanging. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Examples of such optimizations are communication placement [49, 84], message pipelining [136], vector message pipelining [95], and iteration reordering [113]. Improving Parallelism: Recognizing reductions and parallel-prefix scan operations [53, 114] can help to improve the available parallelism. Loop interchange and strip-mining <ref> [13, 160, 162] </ref> can be used to adjust the granularity of pipelined computations to balance parallelism and communication [93]. Storage Management: The use of overlap areas [73] and hash tables can ease the details of buffer management for certain types of computations.
Reference: [161] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year> <month> 154 </month>
Reference-contexts: The search for parallelism is simplified by considering only those loop nests which iterate over the distributed dimensions of the arrays. The methods used to detect and exploit data parallelism in sequential code have been well documented <ref> [12, 13, 25, 153, 161, 163] </ref> and will not be discussed any further here. 4.4 Computation Partitioning The next step is to map the parallel operations to the processors. <p> Since each PE is in fact a serial processor, the array expressions must be scalarized; i.e., translated into serial code <ref> [12, 14, 161] </ref>. This process replaces the array expression with a loop nest containing array references with only scalar subscripts. <p> Data dependence is fundamental to compilers that attempt reordering transformations since it specifies statement orderings that must be preserved to maintain program semantics <ref> [12, 161, 166] </ref>. <p> An outline of the algorithm is given in Figure 5.1. This algorithm has been used with great success in the PFC compiler [13], the ParaScope programming environment [104], and the Fortran D compiler [93, 152]. 5.3.3 Dependence Representation Data dependences are often represented using direction vectors and/or distance vectors <ref> [161] </ref>. The direction vector is an ordering vector, containing &lt;, =, &gt;, or fl, that 41 1. Partition the subscripts into separable and minimal coupled groups. 2. Label each subscript pair as ziv, siv, or miv. 3. <p> (i+1,j) = F 1 (Z (i+1,j+1),Z (i+1,j)) ENDIF IF (i.gt.0) THEN Y (i,j+1) = F 1 (Z (i+1,j+1),Z (i,j+1)) ENDIF ENDDO ENDDO (d) After loop fusion (e) SPMD code with loop fusion the Shallow weather prediction code. 89 the subgrid loop by performing loop splitting, also called index set splitting <ref> [23, 161] </ref>. By splitting the iteration space into disjoint sets, each requiring a single context, we can safely hoist the context setting code out of the resulting loops. We call this optimization context splitting.
Reference: [162] <author> M. J. Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1996. </year>
Reference-contexts: Examples of such optimizations are communication placement [49, 84], message pipelining [136], vector message pipelining [95], and iteration reordering [113]. Improving Parallelism: Recognizing reductions and parallel-prefix scan operations [53, 114] can help to improve the available parallelism. Loop interchange and strip-mining <ref> [13, 160, 162] </ref> can be used to adjust the granularity of pipelined computations to balance parallelism and communication [93]. Storage Management: The use of overlap areas [73] and hash tables can ease the details of buffer management for certain types of computations.
Reference: [163] <author> M. J. Wolfe and U. Banerjee. </author> <title> Data dependence and its application to parallel processing. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(2) </volume> <pages> 137-178, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: The search for parallelism is simplified by considering only those loop nests which iterate over the distributed dimensions of the arrays. The methods used to detect and exploit data parallelism in sequential code have been well documented <ref> [12, 13, 25, 153, 161, 163] </ref> and will not be discussed any further here. 4.4 Computation Partitioning The next step is to map the parallel operations to the processors.
Reference: [164] <author> M. J. Wolfe and C.-W. Tseng. </author> <title> The Power test for data dependence. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(5) </volume> <pages> 591-601, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: If the subscripts became coupled because corresponding triplets did not appear in matching subscript positions, then the linear functions generated for the corresponding triplets share the same pseudo-induction variable. Once the triplets have been translated, we exploit whichever multi-subscript test is available in our system <ref> [121, 134, 164] </ref>. 5.4 Static Single Assignment Form In recent years, Static Single Assignment (SSA) form [64, 65] and related intermediate representations have gained in popularity because of their efficiency in program analysis and transformations [15, 137, 155].
Reference: [165] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: The Fortran D compiler uses the "owner computes" rule, where every processor only performs computations that update data it owns <ref> [45, 165] </ref>. In essence, the data distribution specified by the programmer is also a specification for distributing the computation. The compiler uses the distribution functions discussed in Section 4.2 to determine ownership.
Reference: [166] <author> H. Zima and B. Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> Addison-Wesley, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: Data dependence is fundamental to compilers that attempt reordering transformations since it specifies statement orderings that must be preserved to maintain program semantics <ref> [12, 161, 166] </ref>.
References-found: 166

