URL: ftp://ftp.isi.edu/isi-pubs/rs-96-438.ps.Z
Refering-URL: http://www.isi.edu/isi-technical-reports.html
Root-URL: http://www.isi.edu
Email: smith@aig.jpl.nasa.gov  rosenbloom@isi.edu  
Title: Induction as Knowledge Integration  
Author: Benjamin D. Smith Paul S. Rosenbloom 
Address: 4800 Oak Grove Drive M/S 525-3660 Pasadena, CA 91109-8099  4676 Admiralty Way Marina del Rey, CA 90292  
Affiliation: Jet Propulsion Laboratory California Institute of Technology  Information Sciences Institute Computer Science Dept. University of Southern California  
Abstract: Two key issues for induction algorithms are the accuracy of the learned hypothesis and the computational resources consumed in inducing that hypothesis. One of the most promising ways to improve performance along both dimensions is to make use of additional knowledge. Multi-strategy learning algorithms tackle this problem by employing several strategies for handling different kinds of knowledge in different ways. However, integrating knowledge into an induction algorithm can be difficult when the new knowledge differs significantly from the knowledge the algorithm already uses. In many cases the algorithm must be rewritten. This paper presents KII, a Knowledge Integration framework for Induction, that provides a uniform mechanism for integrating knowledge into induction. In theory, arbitrary knowledge can be integrated with this mechanism, but in practice the knowledge representation language determines both the knowledge that can be integrated, and the costs of integration and induction. By instantiating KII with various set representations, algorithms can be generated at different trade-off points along these dimensions. One instantiation of KII, called RS-KII, is presented that can implement hybrid induction algorithms, depending on which knowledge it utilizes. RS-KII is demonstrated to implement AQ-11 (Michalski 1978), as well as a hybrid algorithm that utilizes a domain theory and noisy examples. Other algorithms are also possible. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Blummer, A.; Ehrenfect, A.; Haussler, D.; and War-muth, M. </author> <year> 1989. </year> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery 36(4) </journal> <pages> 929-965. </pages>
Reference-contexts: Additionally, the set representation for the instantiation effectively defines a class of knowledge from which hypotheses can be induced in polynomial time. This would complement the results in the PAC literature, which deal with polynomial-time learning from examples only (e.g.,(Vapnik & Chervonenkis 1971), (Valiant 1984), <ref> (Blummer et al. 1989) </ref>). Conclusions Integrating additional knowledge is one of the most powerful ways to increase the accuracy and reduce the cost of induction. KII provides a uniform mechanism for doing so.
Reference: <author> Chomsky, N. </author> <year> 1959. </year> <title> On certain formal properties of grammars. </title> <booktitle> Information and Control 2. </booktitle>
Reference-contexts: The space of possible set representations maps onto the space of grammars. Every computable set is the language of some grammar. Similarly, every computable set representation is equivalent to some class of grammars. These classes include, but are not limited to, the classes of the Chomsky hierarchy <ref> (Chomsky 1959) </ref>|regular, context free, context sensitive, and recursively enumerable (r.e.). The complexity of set operations generally increases with the expressiveness of the language class. Allowing H, C, and P to be recursively enumerable (i.e., arbitrary Turing machines), would certainly provide the most expressiveness.
Reference: <author> Clark, P., and Niblett, T. </author> <year> 1989. </year> <title> The CN2 induction algorithm. </title> <booktitle> Machine Learning </booktitle> 3(?):261-283. 
Reference-contexts: The complexity of RS-KII when using only AQ-11 biases is O (e 5 k 2 ). These derivations can be found in (Smith 1995), and generally follow the complexity derivations for AQ-11 in <ref> (Clark & Niblett 1989) </ref>. RS-KII is a little more costly because it assumes that the LEF bias, encoded by P , is a partial order, where it is in fact a total order. This causes RS-KII to make unnecessary comparisons that AQ-11 avoids.
Reference: <author> Cohen, W. W. </author> <year> 1992. </year> <title> Compiling prior knowledge into an explicit bias. </title> <editor> In Sleeman, D., and Edwards, P., eds., </editor> <booktitle> Machine Learning: Proceedings of the Ninth International Workshop, </booktitle> <pages> 102-110. </pages>
Reference-contexts: This approach is formalized in a framework called KII. This framework represents constraints and preferences as sets, and provides set-based operations for integrating knowledge expressed in this way, and for inducing hypotheses from the integrated knowledge. Converting knowledge into constraints and preferences is handled by translators <ref> (Cohen 1992) </ref>, which are written by the user for each knowledge fragment, or class of related knowledge fragments. Since KII is defined in terms of sets and set oper-ations, some set representation must be specified in order for KII to be operational. <p> Most notably, Incremental Version Space Merging (Hirsh 1990) can be generated by using a boundary set representation for constraints (i.e., version spaces), and an empty representation for preferences; and an algorithm similar to Grendel <ref> (Cohen 1992) </ref> can be instantiated from KII by representing sets as antecedent description grammars (essentially context free grammars). These will be discussed briefly. A new algorithm, RS-KII, is instantiated from KII by representing sets as regular grammars. This algorithm seems to strike a good balance between expressiveness and complexity. <p> These operators are described in detail below. Translation Knowledge is converted from the form in which it occurs (its naturalistic representation (Rosenbloom et al. 1993)) into hH; C; P i triples by translators <ref> (Cohen 1992) </ref>. Since knowledge is translated into constraints and preferences over the hypothesis space, the implementation of each translator depends on both the hypothesis space and the knowl edge. In the worst case, a different implementation is required for each pair of knowledge fragment and hypothesis space. <p> RS-KII can also make use of knowledge other than those shown here by writing appropriate translators. Precursors to KII KII has its roots in two knowledge integration systems, Incremental Version Space Merging (Hirsh 1990), and Grendel <ref> (Cohen 1992) </ref>. These systems can also be instantiated from KII, given appropriate set representations. These systems and their relation to KII are described below. IVSM. Incremental Version Space Merging (IVSM) (Hirsh 1990) was one of the first knowledge integration systems for induction, and provided much of the mo-tivation for KII. <p> KII strictly subsumes IVSM, in that IVSM can be cast as an instantiation of KII in which C is a version space one of the possible representations, and P is expressed in the null representation, which can only represent the empty set. Grendel. Grendel <ref> (Cohen 1992) </ref> is another cognitive ancestor of KII. The motivation for Grendel is to express biases explicitly in order to understand their effect on induction.
Reference: <author> Hirsh, H. </author> <year> 1990. </year> <title> Incremental Version Space Merging: </title>
Reference-contexts: This sets a practical limit on the kinds of knowledge that can be utilized by induction. Among the set representations below this limit, there are a number that generate useful instantiations of KII. Most notably, Incremental Version Space Merging <ref> (Hirsh 1990) </ref> can be generated by using a boundary set representation for constraints (i.e., version spaces), and an empty representation for preferences; and an algorithm similar to Grendel (Cohen 1992) can be instantiated from KII by representing sets as antecedent description grammars (essentially context free grammars). <p> Most existing induction algorithms involve only the enumeration operator and perhaps an Empty or Unique query. The Candidate Elimination algorithm (Mitchell 1982) and Incremental Version Space Merging (IVSM) <ref> (Hirsh 1990) </ref> use all four queries, but do not select a hypothesis from the solution set (they return the entire set). The queries and selection of a hypothesis from the solution set can be implemented in terms of a single enumeration operator. <p> Translators for Novel Biases The following translators are for biases that AQ-11 does not utilize, namely consistency with one class of noisy examples, and an assumption that the target hypothesis is a specialization of an overgeneral domain theory. Noisy Examples with Bounded Inconsistency Bounded inconsistency <ref> (Hirsh 1990) </ref> is a kind of noise in which each feature of the example can be wrong by at most a fixed amount. <p> Hypotheses that are consistent with none of the examples in E are not consistent with e 0 , and therefore not the target concept. This is the approach used by Hirsh <ref> (Hirsh 1990) </ref> in IVSM to translate noisy examples with bounded inconsistency. <p> RS-KII can also make use of knowledge other than those shown here by writing appropriate translators. Precursors to KII KII has its roots in two knowledge integration systems, Incremental Version Space Merging <ref> (Hirsh 1990) </ref>, and Grendel (Cohen 1992). These systems can also be instantiated from KII, given appropriate set representations. These systems and their relation to KII are described below. IVSM. Incremental Version Space Merging (IVSM) (Hirsh 1990) was one of the first knowledge integration systems for induction, and provided much of the <p> Precursors to KII KII has its roots in two knowledge integration systems, Incremental Version Space Merging <ref> (Hirsh 1990) </ref>, and Grendel (Cohen 1992). These systems can also be instantiated from KII, given appropriate set representations. These systems and their relation to KII are described below. IVSM. Incremental Version Space Merging (IVSM) (Hirsh 1990) was one of the first knowledge integration systems for induction, and provided much of the mo-tivation for KII.
References-found: 5

