URL: http://www.cs.wustl.edu/~jxh/research/papers/web-servers.ps.gz
Refering-URL: http://www.cs.wustl.edu/~jxh/research/research.html
Root-URL: 
Email: fjxh,sumedh,schmidtg@cs.wustl.edu  
Title: Principles for Developing and Measuring High-performance Web Servers over ATM  
Author: James C. Hu Sumedh Mungee, Douglas C. Schmidt 
Address: St. Louis, MO 63130, USA  
Affiliation: Department of Computer Science, Washington University  
Abstract: This paper has been submitted for publication. It my be referenced as Technical Report WUCS-97-09. Abstract High-performance Web servers are essential to meet the growing demands of the Internet. Satisfying these demands requires a thorough understanding of the key factors that affect Web server performance. This paper provides three contributions to the design, implementation, and evaluation of high-performance Web servers. First, we report the results of a comprehensive empirical study of popular high-performance Web servers (such as Apache, Netscape Enterprise, PHTTPD, and Zeus) over high-speed ATM networks. This study illustrates their relative performance and identifies their performance bottlenecks. To measure performance accurately, we developed a new benchmarking technique that subjects Web servers to varying connection frequencies. We found that once network and disk I/O overheads are reduced to negligible constant factors, the main determinant of Web server performance is the concurrency strategy employed by the server. Moreover, no single strategy performs optimally for all load conditions and traffic types. Second, we describe the performance optimizations used to develop our a high-performance Web server: JAWS. JAWS is an object-oriented Web server that was explicitly designed to alleviate the performance bottlenecks we identified in existing Web servers. It consistently outperforms every server in our test suite. The performance optimizations used in JAWS include adaptive pre-spawned threading, intelligent caching, and prioritized request processing. Third, we outline the design principles and concurrency patterns of JAWS, focusing on its novel object-oriented software architecture. This architecture substantially improves the portability and flexibility of JAWS, relative to other Web servers. For instance, JAWS can configure itself automatically at run-time or installation-time to use the most efficient concurrency strategy for a given OS platform. In addition, JAWS can be configured to support multiple transport protocols (such as HTTP/1.0 and HTTP/1.1). Our work illustrates 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jussara Almeida, Virg ilio Almeida, and David J. Yates. </author> <title> Measuring the Behavior of a World-Wide Web Server. </title> <type> Technical Report TR-CS-96-025, </type> <institution> Department of Computer Science, Boston University, </institution> <month> October 29 </month> <year> 1996. </year>
Reference-contexts: Existing research on improving Web performance has focused largely on reducing network latency, primarily through caching techniques [17] or protocol optimizations [11, 13, 3]. Moreover, measurements of server performance have focused largely on low-speed networks <ref> [1, 9] </ref>. However, this is the first paper that we are aware of that describes the design principles and techniques necessary to develop high-performance Web servers. <p> SGI's WebStone is widely considered as the standard benchmarking system for measuring the performance of Web servers [6]. A detailed analysis of a Pentium-based Apache Web server and its bottlenecks has been done at Boston University <ref> [1] </ref>. Another way to improve performance is by removing overhead in the protocol itself. The W 3 C is currently standardizing HTTP/1.1 that enables multiple requests over a single connection. This connection-caching strategy can significantly enhance the performance over HTTP/1.0 [21, 13].
Reference: [2] <author> Anselm Baird-Smith. </author> <title> Jigsaw performance evaluation. </title> <note> Available from http://www.w3.org/, October 1996. </note>
Reference-contexts: The first was market penetration, in order to gauge the performance of servers that are widely used. The second was reported performance, as reported by benchmark results published by Jigsaw and NCSA, as well as information available from WebCompare <ref> [2, 9, 20] </ref>. The third was source/object code availability, which is essential for white-box analysis. Based on these criteria, we selected a mixture of commercial and free server implementations, with source available for all of the free servers.
Reference: [3] <author> Azer Bestavros. </author> <title> Using speculation to reduce server load and service time on the WWW. In Proceedings of CIKM'95: The 9 The disadvantage of the pool approaches have to do with accept. In Solaris 2.5, accept is not a system call, and is not atomic. In BSD 4.4, accept is a system call. More information is available in [19]. </title> <booktitle> Fourth ACM International Conference on Information and Knowledge Management, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: In addition, we outline the adaptive concurrency design of JAWS, which can adjust its run-time concurrency behavior to provide optimal performance for particular traffic characteristics and workloads. Existing research on improving Web performance has focused largely on reducing network latency, primarily through caching techniques [17] or protocol optimizations <ref> [11, 13, 3] </ref>. Moreover, measurements of server performance have focused largely on low-speed networks [1, 9]. However, this is the first paper that we are aware of that describes the design principles and techniques necessary to develop high-performance Web servers.
Reference: [4] <author> Alexander Carlton. </author> <title> An Explanation of the SPECweb96 Benchmark. Standard Performance Evaluation Corporation whitepaper, </title> <note> 1996. Available from http://www.specbench.org/. </note>
Reference-contexts: We had two motivations for this design: * Predictability: Our client traffic generator permits us to control the request rate per second to within 3 significant digits. For example, 2:15 requests per second would correspond to 129 requests per minute. Conventional benchmark-ing tools <ref> [6, 4] </ref> also issue requests at various rates. <p> The content requested by clients was a 5 megabyte GIF file, which is the equivalent of a large desktop-sized 256-color picture. 2. Small file transfer: This black-box experiment consisted of requests for a single small file. This design is motivated by previous workload studies <ref> [8, 4] </ref>. These studies show that more than 80% of all requested files from a typical Web server are 10 kB or smaller, and that 85% of the requested files are from 15% of the files available on the Web server.
Reference: [5] <author> J.R. Eykholt, S.R. Kleiman, S. Barton, R. Faulkner, A Shiv-alingiah, M. Smith, D. Stein, J. Voll, M. Weeks, and D. Williams. </author> <title> Beyond Multiprocessing... Multithreading the SunOS Kernel. </title> <booktitle> In Proceedings of the Summer USENIX Conference, </booktitle> <address> San Antonio, Texas, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: However, bound threads are more expensive to create <ref> [5] </ref>. This cost becomes negligible, however, when pre-spawned thread pools are used. * Single-threaded concurrent: Threads are a good choice overall, since they scale well to multiple CPU platforms. However, for single CPU platforms, the use of threads increase context switching and synchronization overhead.
Reference: [6] <author> Gene Trent and Mark Sake. WebSTONE: </author> <title> The First Generation in HTTP Server Benchmarking. Silicon Graphics, </title> <publisher> Inc. </publisher> <address> whitepaper, </address> <month> February </month> <year> 1995. </year> <note> Available from http://www.sgi.com/. </note>
Reference-contexts: We had two motivations for this design: * Predictability: Our client traffic generator permits us to control the request rate per second to within 3 significant digits. For example, 2:15 requests per second would correspond to 129 requests per minute. Conventional benchmark-ing tools <ref> [6, 4] </ref> also issue requests at various rates. <p> In addition, the workload of Web servers have been modeled analytically at the University of Saskatchewan [8]. SGI's WebStone is widely considered as the standard benchmarking system for measuring the performance of Web servers <ref> [6] </ref>. A detailed analysis of a Pentium-based Apache Web server and its bottlenecks has been done at Boston University [1]. Another way to improve performance is by removing overhead in the protocol itself. The W 3 C is currently standardizing HTTP/1.1 that enables multiple requests over a single connection.
Reference: [7] <author> Aniruddha Gokhale and Douglas C. Schmidt. </author> <title> Measuring the Performance of Communication Middleware on High-Speed Networks. </title> <booktitle> In Proceedings of SIGCOMM '96, </booktitle> <pages> pages 306317, </pages> <address> Stanford, CA, </address> <month> August </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: A maximum of 32 Kbytes is allotted per ATM virtual circuit connection for receiving and transmitting frames (for a total of 64 K). This allows up to eight switched virtual connections per card. This testbed is similar to the one used in <ref> [7] </ref>. 2.1.2 Client Traffic Generators To provide a thorough and accurate understanding of server performance bottlenecks, we designed and implemented a new benchmarking technique. In this technique, a client traffic generator sends HTTP GET requests to a Web server at a particular rate.
Reference: [8] <author> Martin F. Arlitt and Carey L. Williamson. </author> <title> Intenet Web Servers: Workload Characterization and Performance Implications. </title> <type> Technical report, </type> <institution> University of Saskatchewan, </institution> <note> De-cember 1996. A shorter version of this paper appeared in ACM SIGMETRICS '96. </note>
Reference-contexts: The content requested by clients was a 5 megabyte GIF file, which is the equivalent of a large desktop-sized 256-color picture. 2. Small file transfer: This black-box experiment consisted of requests for a single small file. This design is motivated by previous workload studies <ref> [8, 4] </ref>. These studies show that more than 80% of all requested files from a typical Web server are 10 kB or smaller, and that 85% of the requested files are from 15% of the files available on the Web server. <p> Existing research on improving Web performance has focused largely on reducing network latency, primarily through caching techniques or protocol optimizations. In addition, the workload of Web servers have been modeled analytically at the University of Saskatchewan <ref> [8] </ref>. SGI's WebStone is widely considered as the standard benchmarking system for measuring the performance of Web servers [6]. A detailed analysis of a Pentium-based Apache Web server and its bottlenecks has been done at Boston University [1].
Reference: [9] <author> Robert E. McGrath. </author> <title> Performance of Several HTTP Demons on an HP 735 Workstation. </title> <note> Avaiable from http://www.ncsa.uiuc.edu/, April 25 1995. </note>
Reference-contexts: Existing research on improving Web performance has focused largely on reducing network latency, primarily through caching techniques [17] or protocol optimizations [11, 13, 3]. Moreover, measurements of server performance have focused largely on low-speed networks <ref> [1, 9] </ref>. However, this is the first paper that we are aware of that describes the design principles and techniques necessary to develop high-performance Web servers. <p> The first was market penetration, in order to gauge the performance of servers that are widely used. The second was reported performance, as reported by benchmark results published by Jigsaw and NCSA, as well as information available from WebCompare <ref> [2, 9, 20] </ref>. The third was source/object code availability, which is essential for white-box analysis. Based on these criteria, we selected a mixture of commercial and free server implementations, with source available for all of the free servers.
Reference: [10] <author> Marshall Kirk McKusick, Keith Bostic, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.4BSD Operating System. </title> <publisher> Addison Wesley, </publisher> <year> 1996. </year>
Reference-contexts: To speed up common operations like directory listings, newer versions of UNIX implement stat more efficiently (i.e., requiring less than 50 s on average) by leaving the pointer at the name of the last successful stat call <ref> [10] </ref>. The information returned in a struct stat buffer must still be parsed, however. For instance, once stat returns, a Web server must immediately test to see if the requested file is a directory.
Reference: [11] <author> Jeffrey C. Mogul. </author> <note> The Case for Persistent-Connection HTTP. Technical Report 95/4, </note> <institution> Digital Equipment Corporation Western Research Laboratory, </institution> <month> May </month> <year> 1995. </year> <note> Available online at http://www.research.digital.com/. </note>
Reference-contexts: In addition, we outline the adaptive concurrency design of JAWS, which can adjust its run-time concurrency behavior to provide optimal performance for particular traffic characteristics and workloads. Existing research on improving Web performance has focused largely on reducing network latency, primarily through caching techniques [17] or protocol optimizations <ref> [11, 13, 3] </ref>. Moreover, measurements of server performance have focused largely on low-speed networks [1, 9]. However, this is the first paper that we are aware of that describes the design principles and techniques necessary to develop high-performance Web servers. <p> The W 3 C is currently standardizing HTTP/1.1 that enables multiple requests over a single connection. This connection-caching strategy can significantly enhance the performance over HTTP/1.0 [21, 13]. The need for persistent connections to improve latency was noted by Mogul in <ref> [11] </ref>. Latency also can be improved by using caching proxies and caching clients, although the removal policy needs to be carefully considered [17].
Reference: [12] <author> Nancy J. Yeager and Robert E. McGrath. </author> <title> Web Server Technology: The Advanced Guide for World Wide Web Information Providers. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: The need for persistent connections to improve latency was noted by Mogul in [11]. Latency also can be improved by using caching proxies and caching clients, although the removal policy needs to be carefully considered [17]. Yeager and McGrath of NCSA discuss many of these issues in <ref> [12] </ref>. 5 Concluding Remarks The research presented in this paper was motivated by a desire to build high-performance Web servers. Naturally, it is always possible to improve performance with more expensive hardware (e.g., additional memory and faster CPUs) and a more efficient operating system.
Reference: [13] <author> R. Fielding, J. Gettys, J. Mogul, H. Frystyk, and T. Berners-Lee. </author> <title> Hypertext Transfer Protocol HTTP/1.1. Standards Track RFC 2068, </title> <institution> Network Working Group, </institution> <month> January </month> <year> 1997. </year> <note> Available from http://www.w3.org/. </note>
Reference-contexts: In addition, we outline the adaptive concurrency design of JAWS, which can adjust its run-time concurrency behavior to provide optimal performance for particular traffic characteristics and workloads. Existing research on improving Web performance has focused largely on reducing network latency, primarily through caching techniques [17] or protocol optimizations <ref> [11, 13, 3] </ref>. Moreover, measurements of server performance have focused largely on low-speed networks [1, 9]. However, this is the first paper that we are aware of that describes the design principles and techniques necessary to develop high-performance Web servers. <p> Another way to improve performance is by removing overhead in the protocol itself. The W 3 C is currently standardizing HTTP/1.1 that enables multiple requests over a single connection. This connection-caching strategy can significantly enhance the performance over HTTP/1.0 <ref> [21, 13] </ref>. The need for persistent connections to improve latency was noted by Mogul in [11]. Latency also can be improved by using caching proxies and caching clients, although the removal policy needs to be carefully considered [17].
Reference: [14] <author> Douglas C. Schmidt. GPERF: </author> <title> A Perfect Hash Function Generator. </title> <booktitle> In Proceedings of the 2 nd C++ Conference, </booktitle> <pages> pages 87102, </pages> <address> San Francisco, California, </address> <month> April </month> <year> 1990. </year> <booktitle> USENIX. </booktitle> <pages> 9 </pages>
Reference-contexts: Perfect hashing is feasible since the password file is relatively static and changes infrequently, thereby amortizing the cost of reconstructing the perfect hash. Moreover, since the perfect hash function can be computed off-line, sophisticated tools <ref> [14] </ref> can be used to generate the perfect hash table. A JAWS server can be notified on-demand as the password file changes, in order to reload the cache.
Reference: [15] <author> Douglas C. Schmidt. </author> <title> ACE: an Object-Oriented Framework for Developing Distributed Applications. </title> <booktitle> In Proceedings of the 6 th USENIX C++ Technical Conference, </booktitle> <address> Cambridge, Massachusetts, </address> <month> April </month> <year> 1994. </year> <institution> USENIX Association. </institution>
Reference-contexts: By hold network and file I/O constant, allowed us to precisely pinpoint the request rates where a server starts to overload. These results help to create an accurate behavioral model for Web servers. Our client traffic generator is based on the ACE network programming toolkit <ref> [15] </ref>, which is an object-oriented framework composed of strategic and tactical design patterns that simplify the development of high-performance, concurrent communication software.
Reference: [16] <institution> Pure Software. </institution> <note> Quantify User's Guide, </note> <year> 1996. </year>
Reference-contexts: The timer for latency measurement is started just before the client benchmarking software sends the HTTP request and stops just after the client receives the first response from the server. Several white-box methods (such as thread creation and synchronization overhead) were collected using Quantify v2.2 <ref> [16] </ref>. Quantify analyzes server performance by identifying the functions and system calls that contribute significantly to server performance degradation at high-frequency of requests. <p> Notably, the Netscape Enterprise server attained only about half the throughput of the JAWS and Zeus servers. 3 2.2.4 White-box Analysis of Web Server Performance * Description: In this experiment, the Apache server was instrumented with Quantify <ref> [16] </ref>. Apache server was then subjected to workloads identical to those described in Section 2.2.2 and Section 2.2.3. For the purposes of this paper, only the Apache server was analyzed using white-box analysis.
Reference: [17] <author> Stephen Williams, Marc Abrams, Charles R. Standridge, Ghaleb Abdhulla, and Edward A. Fox. </author> <title> Removal Policies in Network Caches for World-Wide Web Documents. </title> <booktitle> In Proceedings of ACM SIGCOMM'96 Conference: Applications, Technologies, Architectures, and Protocols for Computer Communications, </booktitle> <volume> volume 26, </volume> <month> October </month> <year> 1996. </year>
Reference-contexts: In addition, we outline the adaptive concurrency design of JAWS, which can adjust its run-time concurrency behavior to provide optimal performance for particular traffic characteristics and workloads. Existing research on improving Web performance has focused largely on reducing network latency, primarily through caching techniques <ref> [17] </ref> or protocol optimizations [11, 13, 3]. Moreover, measurements of server performance have focused largely on low-speed networks [1, 9]. However, this is the first paper that we are aware of that describes the design principles and techniques necessary to develop high-performance Web servers. <p> This connection-caching strategy can significantly enhance the performance over HTTP/1.0 [21, 13]. The need for persistent connections to improve latency was noted by Mogul in [11]. Latency also can be improved by using caching proxies and caching clients, although the removal policy needs to be carefully considered <ref> [17] </ref>. Yeager and McGrath of NCSA discuss many of these issues in [12]. 5 Concluding Remarks The research presented in this paper was motivated by a desire to build high-performance Web servers.
Reference: [18] <author> W. Richard Stevens. </author> <title> Advanced Programming in the UNIX Environment. </title> <publisher> Addison Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: This process is repeated until the file is transferred. While this technique is straightforward to implement, it is not very efficient <ref> [18] </ref>. The problem is that the data path of the transfer copies the bytes twice: once from the filesystem into main memory and once again from main memory to the network adapter.
Reference: [19] <author> W. Richard Stevens. </author> <title> UNIX Network Programming, Second Edition. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1997. </year>
Reference: [20] <author> David Strom. </author> <note> Web Compare. Available from http://webcompare.iworld.com/, 1997. </note>
Reference-contexts: The first was market penetration, in order to gauge the performance of servers that are widely used. The second was reported performance, as reported by benchmark results published by Jigsaw and NCSA, as well as information available from WebCompare <ref> [2, 9, 20] </ref>. The third was source/object code availability, which is essential for white-box analysis. Based on these criteria, we selected a mixture of commercial and free server implementations, with source available for all of the free servers.
Reference: [21] <author> T. Berners-Lee, R. T. Fielding, and H. Frystyk. </author> <title> Hypertext Transfer Protocol HTTP/1.0. Informational RFC 1945, </title> <institution> Network Working Group, </institution> <month> May </month> <year> 1996. </year> <note> Available from http://www.w3.org/. </note>
Reference-contexts: Another way to improve performance is by removing overhead in the protocol itself. The W 3 C is currently standardizing HTTP/1.1 that enables multiple requests over a single connection. This connection-caching strategy can significantly enhance the performance over HTTP/1.0 <ref> [21, 13] </ref>. The need for persistent connections to improve latency was noted by Mogul in [11]. Latency also can be improved by using caching proxies and caching clients, although the removal policy needs to be carefully considered [17].
Reference: [22] <author> J.S. Turner. </author> <title> An optimal nonblocking multicast virtual circuit switch. </title> <booktitle> In Proceedings of the Conference on Computer Communications (INFOCOM), </booktitle> <pages> pages 298305, </pages> <month> June </month> <year> 1994. </year> <month> 10 </month>
Reference-contexts: To accomplish this, we constructed a hardware and software testbed consisting of a high-performance server and client connected by a high-speed ATM switch <ref> [22] </ref> (shown in Figure 1). 2 The experiments in this paper were conducted using a Bay Networks LattisCell 10114 ATM switch connected to two dual-processor UltraSparc-2s running SunOS 5.5.1. The Lat-tisCell 10114 is a 16 Port, OC3 155 Mbs/port switch.
References-found: 22

