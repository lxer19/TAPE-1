URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/danthesis.ps.gz
Refering-URL: http://www.cs.cmu.edu/~WebKB/
Root-URL: 
Title: Using HTML Formatting to Aid in Natural Language Processing on the World Wide Web  
Author: Dan DiPasquo 
Degree: Senior Honors Thesis  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: June 23, 1998  
Abstract: Because of its magnitude and the fact that it is not computer understandable, the World Wide Web has become a prime candidate for automatic natural language tasks. This thesis argues that there is information in the layout of a web page, and that by looking at the HTML formatting in addition to the text on a page, one can improve performance in tasks such as learning to classify segments of documents. A rich representation for web pages, the HTML Struct Tree, is described. A parsing algorithm for creating Struct Trees is presented, as well as a set of experiments that use Struct Trees as a feature set for learning to extract a company's name and location from its Web pages. Through these experiments we found that it is useful to consider the layout of a page for these tasks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> About the world wide web. </institution> <note> http://www.w3.org/WWW/. </note>
Reference-contexts: Motivations The World Wide Web is defined by the World Wide Web Consortium (W3C) as "the universe of network-accessible information, the embodiment of human knowledge" <ref> [1] </ref>. While the Web may not currently live up to this definition, it is true that with over 200,000,000 pages and its incredible rate of growth, it is quickly moving in this direction.
Reference: [2] <institution> Proceedings of the Fourth Message Understanding Conference (MUC-4), </institution> <address> McLean, Vir-ginia, June 1992. </address> <publisher> Morgan Kaufmann Publisher, Inc., </publisher> <address> San Francisco. </address>
Reference-contexts: In this section, we will look at the current state of the art in text processing, and specifically text processing on the web. The traditional information extraction task (IE) is defined by the Message Understanding community (MUC) <ref> [2, 3] </ref> as given a document, automatically find the essential details in the text (i.e. given a newswire article about a terrorist attack, find the names of the attacking 3 group and the victims of the attack).
Reference: [3] <institution> Proceedings of the Fifth Message Understanding Conference (MUC-5), </institution> <address> Baltimore, Mary-land, August 1993. </address> <publisher> Morgan Kaufmann Publisher, Inc., </publisher> <address> San Francisco. </address>
Reference-contexts: In this section, we will look at the current state of the art in text processing, and specifically text processing on the web. The traditional information extraction task (IE) is defined by the Message Understanding community (MUC) <ref> [2, 3] </ref> as given a document, automatically find the essential details in the text (i.e. given a newswire article about a terrorist attack, find the names of the attacking 3 group and the victims of the attack).
Reference: [4] <author> E. Amitay. </author> <title> Hypertext: The importance of being different. </title> <type> Master's thesis, </type> <institution> University of Edinburgh, Centre for Cognitive Science, </institution> <year> 1997. </year>
Reference-contexts: The field of linguistics explores these conventions in written text 4 and speech genres; however, it has been proposed that hypertext is a different genre than flat text and that we must explore its linguistic conventions <ref> [4] </ref>. Our proposal is that the hypertext on the World Wide Web (HTML) has some linguistic conventions not only in its hyperlinks, but also in its page layout. This section proposes a representation that could be useful in exploiting these conventions.
Reference: [5] <author> N. Ashish and C. Knoblock. </author> <title> Wrapper generation for semi-structured internet sources. </title> <booktitle> In ACM SIGMOD Workshop on Management of Semi-structured Data, </booktitle> <year> 1997. </year>
Reference-contexts: For these reasons, the Web has become a prime candidate for systems that attempt to automatically catalog information into an ontology [14], automatically extract knowledge from highly regular [9, 16] and semi-regular <ref> [5] </ref> sources, and use agents to actively search the Web [6, 21]. Traditional natural language processing techniques seem immediately applicable to the Web, but problems arise when applying an algorithm designed for flat text for processing Web pages. <p> The first of these is often reduced to the more general problem of "wrapper induction," for which much work has been developed <ref> [5, 13] </ref>. Webfoot is an attempt at more general information extraction on the Web. It uses CRYSTAL, an information extraction system that takes parsed annotated sentences and finds patterns for extraction in novel sentences.
Reference: [6] <author> M. Balabanovic and Y. Shoham. </author> <title> Learning information retrieval agents: Experiments with automated web browsing. </title> <booktitle> In AAAI Spring Symposium on Information Gathering from Heterogeneous, Distributed Environments, </booktitle> <year> 1995. </year>
Reference-contexts: For these reasons, the Web has become a prime candidate for systems that attempt to automatically catalog information into an ontology [14], automatically extract knowledge from highly regular [9, 16] and semi-regular [5] sources, and use agents to actively search the Web <ref> [6, 21] </ref>. Traditional natural language processing techniques seem immediately applicable to the Web, but problems arise when applying an algorithm designed for flat text for processing Web pages. These techniques often expect complete, well formed sentences, but on the Web the norm is short informative sentence fragments.
Reference: [7] <author> M. E. Califf and R. J. Mooney. </author> <title> Relational learning of pattern-match rules for information extraction. </title> <booktitle> In Working Papers of ACL-97 Workshop on Natural Language Learning, </booktitle> <year> 1997. </year>
Reference-contexts: This differs, however, from the traditional work done on flat text. Most of these early techniques relied on hand crafted algorithms for extracting information. Recently, much research has been focussed on using machine learning for information extraction tasks <ref> [7, 10, 12, 19, 20, 23] </ref>, and this has shown the most promise for information extraction from the Web.
Reference: [8] <author> M. Craven, S. Slattery, and K. Nigam. </author> <title> First-order learning for web mining. </title> <booktitle> In Proceedings of the 10th European Conference on Machine Learning, </booktitle> <address> Chemnitz, GERMANY, April 1998. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: These simple techniques generalize well to the Web because they are not affected by lack of traditional syntactic structure. However, on the Web, these techniques can be improved by taking advantage of other sources of information. Craven, Slattery, and Nigam report in <ref> [8] </ref> that greater accuracy can be achieved by representing the Web as a graph in which each page is a node and each hyperlink an edge. By giving a first order learner the words on each page and this graph as data, one can exploit this information.
Reference: [9] <author> R. Doorenbos, O. Etzioni, and D. S. Weld. </author> <title> A scalable comparison-shopping agent for the world-wide web. </title> <booktitle> In Proc. Autonomous Agents, </booktitle> <year> 1997. </year>
Reference-contexts: For these reasons, the Web has become a prime candidate for systems that attempt to automatically catalog information into an ontology [14], automatically extract knowledge from highly regular <ref> [9, 16] </ref> and semi-regular [5] sources, and use agents to actively search the Web [6, 21]. Traditional natural language processing techniques seem immediately applicable to the Web, but problems arise when applying an algorithm designed for flat text for processing Web pages.
Reference: [10] <author> D. Freitag. </author> <title> Information extraction from HTML: Application of a general learning approach. </title> <booktitle> In Proceedings of the Fifteenth National Conference on Artificial Intelligence, </booktitle> <address> Madison, WI, 1998. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This differs, however, from the traditional work done on flat text. Most of these early techniques relied on hand crafted algorithms for extracting information. Recently, much research has been focussed on using machine learning for information extraction tasks <ref> [7, 10, 12, 19, 20, 23] </ref>, and this has shown the most promise for information extraction from the Web. <p> It takes a user defined feature set, and hand tagged training documents, and attempts to learn rules for extraction [11]. When SRV was given Web Pages, and feature sets that included some HTML information, it was able to learn rules for extracting from the Web <ref> [10] </ref>. At recent Text REtrieval Conferences (TREC), much work has been reported on document classification. Craven, et al. [14] shows that document classification is useful for finding interesting ontology entries on the Web.
Reference: [11] <author> D. Freitag. </author> <title> Toward general-purpose learning for information extraction. </title> <booktitle> In Proceedings of COLING/ACL-98, </booktitle> <year> 1998. </year>
Reference-contexts: Rather than taking parsed sentences, Webfoot takes fragments which are broken up by looking at HTML [24]. Finally, SRV is a learning architecture for information extraction. It takes a user defined feature set, and hand tagged training documents, and attempts to learn rules for extraction <ref> [11] </ref>. When SRV was given Web Pages, and feature sets that included some HTML information, it was able to learn rules for extracting from the Web [10]. At recent Text REtrieval Conferences (TREC), much work has been reported on document classification.
Reference: [12] <author> J.-T. Kim and D. I. Moldovan. </author> <title> Acquisition of linguistic patterns for knowledge-based information extraction. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <year> 1995. </year>
Reference-contexts: This differs, however, from the traditional work done on flat text. Most of these early techniques relied on hand crafted algorithms for extracting information. Recently, much research has been focussed on using machine learning for information extraction tasks <ref> [7, 10, 12, 19, 20, 23] </ref>, and this has shown the most promise for information extraction from the Web.
Reference: [13] <author> N. Kushmerick. </author> <title> Wrapper Induction for Information Extraction. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1997. </year> <note> Tech Report UW-CSE-97-11-04. </note>
Reference-contexts: The first of these is often reduced to the more general problem of "wrapper induction," for which much work has been developed <ref> [5, 13] </ref>. Webfoot is an attempt at more general information extraction on the Web. It uses CRYSTAL, an information extraction system that takes parsed annotated sentences and finds patterns for extraction in novel sentences.
Reference: [14] <author> M.Craven, D. DiPasquo, D. Freitag, A. McCallum, T. Mitchell, K. Knigam, and S. Slat-tery. </author> <title> Learning to extract symbolic knowledge from the world wide web. </title> <type> Technical report, </type> <institution> Department of Computer Science, Carnegie Mellon University, </institution> <year> 1998. </year>
Reference-contexts: Both of these options are an ineffective means of accessing the information available now, and as the Web continues to grow so will this problem. For these reasons, the Web has become a prime candidate for systems that attempt to automatically catalog information into an ontology <ref> [14] </ref>, automatically extract knowledge from highly regular [9, 16] and semi-regular [5] sources, and use agents to actively search the Web [6, 21]. Traditional natural language processing techniques seem immediately applicable to the Web, but problems arise when applying an algorithm designed for flat text for processing Web pages. <p> When SRV was given Web Pages, and feature sets that included some HTML information, it was able to learn rules for extracting from the Web [10]. At recent Text REtrieval Conferences (TREC), much work has been reported on document classification. Craven, et al. <ref> [14] </ref> shows that document classification is useful for finding interesting ontology entries on the Web. Many traditional document classification techniques view a document as a "bag of words" in which order does not matter [15].
Reference: [15] <author> T. Mitchell. </author> <title> Machine Learning. </title> <publisher> The McGraw-Hill Companies, Inc., </publisher> <year> 1997. </year> <month> 15 </month>
Reference-contexts: Craven, et al. [14] shows that document classification is useful for finding interesting ontology entries on the Web. Many traditional document classification techniques view a document as a "bag of words" in which order does not matter <ref> [15] </ref>. These simple techniques generalize well to the Web because they are not affected by lack of traditional syntactic structure. However, on the Web, these techniques can be improved by taking advantage of other sources of information.
Reference: [16] <author> M. Perkowitz and O. Etzioni. </author> <title> Category translation: learning to understand information on the Internet. </title> <booktitle> In Proceedings of the 15th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 930-936. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: For these reasons, the Web has become a prime candidate for systems that attempt to automatically catalog information into an ontology [14], automatically extract knowledge from highly regular <ref> [9, 16] </ref> and semi-regular [5] sources, and use agents to actively search the Web [6, 21]. Traditional natural language processing techniques seem immediately applicable to the Web, but problems arise when applying an algorithm designed for flat text for processing Web pages.
Reference: [17] <author> J. R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-2666, </pages> <year> 1990. </year>
Reference-contexts: Therefore, a simple key word search would have an accuracy at this task of 49%. 5.2. The FOIL-PILFS Algorithm We used the FOIL-PILFS [22] algorithm which combines Quinlan's FOIL relational learning algorithm <ref> [17, 18] </ref> with a naive bayes text classifier.[15] This algorithm was designed for learning in hypertext domains.
Reference: [18] <author> J. R. Quinlan and R. M. Cameron-Jones. </author> <title> FOIL: A midterm report. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 3-20, </pages> <address> Vienna, Austria, </address> <year> 1993. </year>
Reference-contexts: Therefore, a simple key word search would have an accuracy at this task of 49%. 5.2. The FOIL-PILFS Algorithm We used the FOIL-PILFS [22] algorithm which combines Quinlan's FOIL relational learning algorithm <ref> [17, 18] </ref> with a naive bayes text classifier.[15] This algorithm was designed for learning in hypertext domains.
Reference: [19] <author> E. Riloff. </author> <title> Automatically generating extraction patterns from untagged text. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1044-1049. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: This differs, however, from the traditional work done on flat text. Most of these early techniques relied on hand crafted algorithms for extracting information. Recently, much research has been focussed on using machine learning for information extraction tasks <ref> [7, 10, 12, 19, 20, 23] </ref>, and this has shown the most promise for information extraction from the Web.
Reference: [20] <author> E. Riloff. </author> <title> An empirical study of automated dictionary construction for information extraction in three domains. </title> <journal> Arificial Intelligence, </journal> <volume> 85 </volume> <pages> 101-134, </pages> <year> 1996. </year>
Reference-contexts: This differs, however, from the traditional work done on flat text. Most of these early techniques relied on hand crafted algorithms for extracting information. Recently, much research has been focussed on using machine learning for information extraction tasks <ref> [7, 10, 12, 19, 20, 23] </ref>, and this has shown the most promise for information extraction from the Web.
Reference: [21] <author> Shakes, J. Langheinrich, M., and O. Etzioni. </author> <title> Dynamic reference sifting: a case study in the homepage domain. </title> <booktitle> In Proceedings of Sixth International World Wide Web Conference, </booktitle> <address> Santa Clara, CA, </address> <year> 1996. </year>
Reference-contexts: For these reasons, the Web has become a prime candidate for systems that attempt to automatically catalog information into an ontology [14], automatically extract knowledge from highly regular [9, 16] and semi-regular [5] sources, and use agents to actively search the Web <ref> [6, 21] </ref>. Traditional natural language processing techniques seem immediately applicable to the Web, but problems arise when applying an algorithm designed for flat text for processing Web pages. These techniques often expect complete, well formed sentences, but on the Web the norm is short informative sentence fragments.
Reference: [22] <author> S. Slattery and M. Craven. </author> <title> Combining statistical and relational methods for learning in hypertext domains. </title> <booktitle> In Proceedings of the 8th International Conference on Inductive Logic Programming. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1998. </year>
Reference-contexts: Therefore, a simple key word search would have an accuracy at this task of 49%. 5.2. The FOIL-PILFS Algorithm We used the FOIL-PILFS <ref> [22] </ref> algorithm which combines Quinlan's FOIL relational learning algorithm [17, 18] with a naive bayes text classifier.[15] This algorithm was designed for learning in hypertext domains.
Reference: [23] <author> S. Soderland. </author> <title> Learning Text Analysis Rules for Domain-specific Natural Language Processing. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1996. </year> <note> Available as Department of Computer Science Technical Report 96-087. </note>
Reference-contexts: This differs, however, from the traditional work done on flat text. Most of these early techniques relied on hand crafted algorithms for extracting information. Recently, much research has been focussed on using machine learning for information extraction tasks <ref> [7, 10, 12, 19, 20, 23] </ref>, and this has shown the most promise for information extraction from the Web.
Reference: [24] <author> S. Soderland. </author> <title> Learning to extract text-based information from the World Wide Web. </title> <booktitle> In Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining, </booktitle> <year> 1997. </year> <month> 16 </month>
Reference-contexts: Webfoot is an attempt at more general information extraction on the Web. It uses CRYSTAL, an information extraction system that takes parsed annotated sentences and finds patterns for extraction in novel sentences. Rather than taking parsed sentences, Webfoot takes fragments which are broken up by looking at HTML <ref> [24] </ref>. Finally, SRV is a learning architecture for information extraction. It takes a user defined feature set, and hand tagged training documents, and attempts to learn rules for extraction [11].
References-found: 24

