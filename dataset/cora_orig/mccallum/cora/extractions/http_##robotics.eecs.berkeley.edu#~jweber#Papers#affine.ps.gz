URL: http://robotics.eecs.berkeley.edu/~jweber/Papers/affine.ps.gz
Refering-URL: http://robotics.eecs.berkeley.edu/~jweber/rigid.html
Root-URL: 
Title: Rigid Body Segmentation and Shape Description from Dense Optical Flow under Weak Perspective algorithm assumes
Note: The  
Abstract: We present an algorithm for identifying and tracking independently moving rigid objects from optical flow. Some previous attempts at segmentation via optical flow have focused on finding discontinuities in the flow field. While discontinuities do indicate a change in scene depth, they do not in general signal a boundary between two separate objects. The proposed method uses the fact that each distinct object has a unique epipolar constraint associated with its motion. Thus motion discontinuities based on self-occlusion are distinguished from those due to separate objects. The use of epipolar geometry allows for the determination of individual motion parameters for each object as well as the recovery of relative depth for each point on the object. The segmentation problem is formulated as a scene partitioning problem and a statistic-based algorithm which uses only nearest neighbor interactions and a finite number of iterations is developed. After the initial segmentation, each rigid object is tracked in subsequent frames. We present a Kalman filter based approach for tracking motion parameters with time. 
Abstract-found: 1
Intro-found: 1
Reference: [Adi85] <author> Gilad Adiv. </author> <title> Determining three-dimensional motion and structure from optical flow generated by several moving objects. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(4) </volume> <pages> 384-401, </pages> <year> 1985. </year>
Reference-contexts: Black and Jepson [BJ94] first find regions of uniform brightness and calculate flow within these regions. In a number of segmentation algorithms, the assumption that the optical flow field was locally affine in image coordinates was used <ref> [Adi85, MW86, NSKO94, RCV92, WA94] </ref>. This assumes that the scene is piecewise-continuous in depth. * Adiv [Adi85] groups optical flow vectors that have similar affine coordinates using a Hough transform. Assuming each cluster represents the motion of a planar object, the object's motion is recovered. <p> In a number of segmentation algorithms, the assumption that the optical flow field was locally affine in image coordinates was used [Adi85, MW86, NSKO94, RCV92, WA94]. This assumes that the scene is piecewise-continuous in depth. * Adiv <ref> [Adi85] </ref> groups optical flow vectors that have similar affine coordinates using a Hough transform. Assuming each cluster represents the motion of a planar object, the object's motion is recovered. Clusters with similar motions are then merged.
Reference: [BA90] <author> M. Black and P. Anandan. </author> <title> Constraints for the early detection of discontinuity from motion. </title> <booktitle> In Proceedings of the National Conference on A.I., </booktitle> <pages> pages 1060-1066, </pages> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: As a result, the first class of algorithms are operating in regions of the image where the optical flow is least accurate. 2.1 Motion Field Segmentation Early work on segmentation via motion looked for discontinuities in one or both components of the displacement field <ref> [SU87, TMB85, BA90, Bla92] </ref>. Since under general perspective projections the motion field is continuous as long as the depth of the viewed surface is continuous, discontinuities in the flow field signal depth discontinuities. Unfortunately, the flow field at discontinuities is difficult to recover.
Reference: [Bel93] <author> Peter N. Belhumeur. </author> <title> A Baysian Approach to the Stereo Correspondence Problem. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1993. </year>
Reference-contexts: Thus initial sets can be constrained to nearest neighbors and avoid the combinatorial explosion. In stereopsis, the epipolar constraint is used in conjunction with priors on the scene structure to help constrain the problem. The violation of the prior can then also be used to detect boundaries <ref> [Bel93] </ref>. These priors usually take the form of a penalty for high depth gradients, biasing the solution toward a piecewise continuous depth map [MS85]. One work making use of a prior on the structure in the context of motion is [CFCHB89].
Reference: [BF70] <author> C. Brice and C. Fennema. </author> <title> Scene analysis using regions. </title> <journal> Artificial Intelligence, </journal> <volume> 1(3) </volume> <pages> 205-226, </pages> <year> 1970. </year>
Reference-contexts: Leclerc [Lec89] showed that the minimal solution could also be interpretated as the minimal length encoding describing the scene in terms of a given descriptive language. There are stochastic [GG84], region-growing <ref> [BF70, HP74] </ref>, and continuation [Lec89, BZ87, GY91] methods for finding solutions to the scene partitioning problem when it is described in terms of a cost functional.
Reference: [BJ94] <author> M. Black and A. Jepson. </author> <title> Estimating optical flow in segmented images using variable-order parametric models with local deformations. </title> <type> Technical Report SPL-94-053, </type> <note> Xerox PARC, </note> <year> 1994. </year>
Reference-contexts: At lo-cations of depth edges, motion will introduce regions of occlusion and disocclusion which are often not explicitly modeled in optical flow routines. Some algorithms attempt to locate regions near a flow boundary before computing the flow <ref> [BR87, Sch89, BJ94] </ref>. The first two papers look at the gradient constraint within a local region. Schunck [Sch89] performs a cluster analysis on the constraint lines while Bouthemy and Rivero [BR87] use a statistical test to find separate motions. <p> The first two papers look at the gradient constraint within a local region. Schunck [Sch89] performs a cluster analysis on the constraint lines while Bouthemy and Rivero [BR87] use a statistical test to find separate motions. Black and Jepson <ref> [BJ94] </ref> first find regions of uniform brightness and calculate flow within these regions. In a number of segmentation algorithms, the assumption that the optical flow field was locally affine in image coordinates was used [Adi85, MW86, NSKO94, RCV92, WA94]. <p> After 10 frames, the rotation directions were switched. 26 depth map as seen from a side view are shown below. 27 depth map are shown below. straight over jagged boundaries could be introduced as well as combining information from intensity and texture boundaries <ref> [BJ94] </ref>. 10 Discussion and Future Work We have shown that even with just the optical flow of an image sequence, it is possible to segment the image into regions with a consistent rigid motion and determine the motion parameters for that rigid motion.
Reference: [Bla92] <author> Michael Black. </author> <title> Combining intensity and motion for incremental segmentation and tracking over long image sequences. </title> <booktitle> In Proceedings of the second EECV, </booktitle> <address> Santa Margherita, </address> <year> 1992. </year> <month> 30 </month>
Reference-contexts: As a result, the first class of algorithms are operating in regions of the image where the optical flow is least accurate. 2.1 Motion Field Segmentation Early work on segmentation via motion looked for discontinuities in one or both components of the displacement field <ref> [SU87, TMB85, BA90, Bla92] </ref>. Since under general perspective projections the motion field is continuous as long as the depth of the viewed surface is continuous, discontinuities in the flow field signal depth discontinuities. Unfortunately, the flow field at discontinuities is difficult to recover.
Reference: [BR87] <author> P. Bouthemy and J. Santillana Rivero. </author> <title> A hierarchical likelihood approach for region segmentation according to motion-based criteria. </title> <booktitle> In ICCV, </booktitle> <pages> pages 463-467, </pages> <address> London, </address> <year> 1987. </year>
Reference-contexts: At lo-cations of depth edges, motion will introduce regions of occlusion and disocclusion which are often not explicitly modeled in optical flow routines. Some algorithms attempt to locate regions near a flow boundary before computing the flow <ref> [BR87, Sch89, BJ94] </ref>. The first two papers look at the gradient constraint within a local region. Schunck [Sch89] performs a cluster analysis on the constraint lines while Bouthemy and Rivero [BR87] use a statistical test to find separate motions. <p> Some algorithms attempt to locate regions near a flow boundary before computing the flow [BR87, Sch89, BJ94]. The first two papers look at the gradient constraint within a local region. Schunck [Sch89] performs a cluster analysis on the constraint lines while Bouthemy and Rivero <ref> [BR87] </ref> use a statistical test to find separate motions. Black and Jepson [BJ94] first find regions of uniform brightness and calculate flow within these regions. <p> This continues until we reach the final value of . See [Web94] for details and application of the algorithm on a range of scene segmentation problems. The use of a statistical test for flow-based segmentation can also be found in <ref> [NSKO94, BR87] </ref>. In these cases however, the deviation of the optical flow from affine parameters is tested instead. The statistical test can not be used to compare an affine region with a non-affine region because there exists a subspace of solutions for the Fundamental Matrix in the affine case.
Reference: [BZ87] <author> Andrew Blake and Andrew Zisserman. </author> <title> Visual reconstruction. </title> <publisher> MIT press, </publisher> <address> Cam-bridge, MA., </address> <year> 1987. </year>
Reference-contexts: Leclerc [Lec89] showed that the minimal solution could also be interpretated as the minimal length encoding describing the scene in terms of a given descriptive language. There are stochastic [GG84], region-growing [BF70, HP74], and continuation <ref> [Lec89, BZ87, GY91] </ref> methods for finding solutions to the scene partitioning problem when it is described in terms of a cost functional. Stochastic methods use simulated annealing programs in which a gradient descent method is perturbed by a stochastic process which decreases in magnitude as the global minimum is approached. <p> If the deformation is slow enough, the solution will track to the global minimum of the original cost functional. This is the basis of the Graduated Non-Convexity Algorithm of Blake and Zisserman <ref> [BZ87] </ref> and the mean field theory approach of Geiger and Girosi [GG91]. Our solution will use the region-growing method described in [Web94] to solve for the partition. <p> The delta function ffi () is equal to one when its argument is zero, and zero otherwise. The vector ~ n i is the estimate of the Fundamental Matrix at pixel i. In terms of the standard form of a cost functional <ref> [BZ87] </ref>, D ( ~ n) represents a goodness of fit term which attempts to keep the estimate close to the data, and P ( ~ n; ff) is a discontinuity penalty term which tries to limit the frequency of discontinuities.
Reference: [CFCHB89] <author> B. Cernuschi-Frias, D.B. Cooper, Y.P. Hung, </author> <title> and P.N. Belhumeur. Toward a model-based bayesian theory for estimating and recognizing parameterized 3-d objects using two or more images taken from different positions. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 11 </volume> <pages> 1028-1052, </pages> <year> 1989. </year>
Reference-contexts: In addition, the constraint holds for each point on an object, not just at the boundaries. The optical flow can therefore be sparse at the object boundaries. As pointed out by Koenderink and van Doorn [KvD91], and implemented by Shapiro et al. and Cernuschi-Frias et al. <ref> [SZB94, SZB93, CFCHB89] </ref>, under weak perspective projection motion parameters and shape descriptions can be obtained (modulo a relief transformation such as depth scaling) from just two views. Thus even under the restrictions of scaled orthography, important motion information can be obtained. <p> These priors usually take the form of a penalty for high depth gradients, biasing the solution toward a piecewise continuous depth map [MS85]. One work making use of a prior on the structure in the context of motion is <ref> [CFCHB89] </ref>. We have not used a prior on the scene structure for two reasons. First, most common priors try to limit some derivative of the depth as a function of image coordinates. This biases the result toward fronto-parallel solutions.
Reference: [GG84] <author> S. Geman and D. Geman. </author> <title> Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-741, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: The cost functional is often modeled as the probability of a given solution assuming gaussian departures from the model. Leclerc [Lec89] showed that the minimal solution could also be interpretated as the minimal length encoding describing the scene in terms of a given descriptive language. There are stochastic <ref> [GG84] </ref>, region-growing [BF70, HP74], and continuation [Lec89, BZ87, GY91] methods for finding solutions to the scene partitioning problem when it is described in terms of a cost functional.
Reference: [GG91] <author> Davi Geiger and Frederico Girosi. </author> <title> Parallel and deterministic algorithms from mrf's: Surface reconstruction. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(5) </volume> <pages> 401-412, </pages> <year> 1991. </year>
Reference-contexts: If the deformation is slow enough, the solution will track to the global minimum of the original cost functional. This is the basis of the Graduated Non-Convexity Algorithm of Blake and Zisserman [BZ87] and the mean field theory approach of Geiger and Girosi <ref> [GG91] </ref>. Our solution will use the region-growing method described in [Web94] to solve for the partition. This method uses a statistic-based region growing algorithm which assumes the solution is piecewise continuous in image coordinates. 3 Pro jections and Rigid Motions This section will describe the weak perspective camera.
Reference: [GY91] <author> Davi Geiger and Alan Yuille. </author> <title> A common framework for image segmentation. </title> <journal> International Journal of Computer Vision, </journal> <volume> 6(3) </volume> <pages> 227-243, </pages> <year> 1991. </year>
Reference-contexts: Leclerc [Lec89] showed that the minimal solution could also be interpretated as the minimal length encoding describing the scene in terms of a given descriptive language. There are stochastic [GG84], region-growing [BF70, HP74], and continuation <ref> [Lec89, BZ87, GY91] </ref> methods for finding solutions to the scene partitioning problem when it is described in terms of a cost functional. Stochastic methods use simulated annealing programs in which a gradient descent method is perturbed by a stochastic process which decreases in magnitude as the global minimum is approached.
Reference: [HL89] <author> Thomas Huang and C.H. Lee. </author> <title> Motion and structure from orthographic projections. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 11(5) </volume> <pages> 536-540, </pages> <year> 1989. </year>
Reference-contexts: The angle in (17) can not be obtained under weak perspective from just two views because of an unknown scaling factor, as proved by Huang and Lee <ref> [HL89] </ref>. Koenderink and van Doorn [KvD91] solve for from three views using a non-linear algorithm. 4 Solving for the Fundamental Matrix In the previous section we saw how under the assumptions of an affine camera model, the displacement field for a rigidly moving object satisfies an affine epipolar constraint (11).
Reference: [HP74] <author> S.L. Horowitz and T. Pavlidis. </author> <title> Picture segmentation by a directed split-and-merge procedure. </title> <booktitle> In Proceedings of the 2nd International Conference on Pattern Recognition, </booktitle> <pages> pages 424-433, </pages> <year> 1974. </year>
Reference-contexts: Leclerc [Lec89] showed that the minimal solution could also be interpretated as the minimal length encoding describing the scene in terms of a given descriptive language. There are stochastic [GG84], region-growing <ref> [BF70, HP74] </ref>, and continuation [Lec89, BZ87, GY91] methods for finding solutions to the scene partitioning problem when it is described in terms of a cost functional.
Reference: [KvD91] <author> J.J. Koenderink and A.J. van Doorn. </author> <title> Affine structure from motion. </title> <journal> J. Optical Society of America A, </journal> <volume> 8(2) </volume> <pages> 377-385, </pages> <year> 1991. </year>
Reference-contexts: This allows for segmentation without any priors on shape or scene structure. In addition, the constraint holds for each point on an object, not just at the boundaries. The optical flow can therefore be sparse at the object boundaries. As pointed out by Koenderink and van Doorn <ref> [KvD91] </ref>, and implemented by Shapiro et al. and Cernuschi-Frias et al. [SZB94, SZB93, CFCHB89], under weak perspective projection motion parameters and shape descriptions can be obtained (modulo a relief transformation such as depth scaling) from just two views. <p> The linear constraint introduced by a rigid motion leads to a special form of the Fundamental Matrix. We will also introduce the representation for rotations used by Koenderink and van Doorn <ref> [KvD91] </ref> and show how the components of the Fundamental Matrix can be used to obtain the rotation and scale 8 parameters. 3.1 The Weak Perspective Camera The weak perspective camera is a projection from 3D world coordinates to 2D scene coordinates. The projection is scaled orthographic and preserves parallelism. <p> A particularly useful representation for vision was introduced by Koen-derink and van Doorn <ref> [KvD91] </ref>. <p> The angle in (17) can not be obtained under weak perspective from just two views because of an unknown scaling factor, as proved by Huang and Lee [HL89]. Koenderink and van Doorn <ref> [KvD91] </ref> solve for from three views using a non-linear algorithm. 4 Solving for the Fundamental Matrix In the previous section we saw how under the assumptions of an affine camera model, the displacement field for a rigidly moving object satisfies an affine epipolar constraint (11).
Reference: [LDFP93] <author> Q.-T. Luong, R. Deriche, O.D. Faugeras, and T. Papadopoulo. </author> <title> On determining the Fundamental matrix: analysis of different methods and experimental results. </title> <type> Technical Report RR-1894, </type> <institution> INRIA, </institution> <year> 1993. </year> <note> A shorter version appeared in the Israelian Conf. on Artificial Intelligence and Computer Vision. </note>
Reference-contexts: This constraint is unique to each rigid transformation and can be used to identify independently moving objects. The epipolar constraint has been used in a number of structure from motion algorithms [LH81, TH84, TK92]. The epipolar constraint can be used even in the case of uncalibrated cameras <ref> [LF94, LDFP93] </ref>. This allows for segmentation without any priors on shape or scene structure. In addition, the constraint holds for each point on an object, not just at the boundaries. The optical flow can therefore be sparse at the object boundaries. <p> If a recovery of the full perspective case was required, the same algorithm could be used. However, the calculation of the Fundamental Matrix from small displacements such as found in optical flow is not stable <ref> [WAH93, LDFP93] </ref>. 28 patterns while a toy train traverses the foreground. An example optical flow recovered is also shown. The labeled image is shown below. The background parts (colored grey) were identified as undergoing pure translational motion by the singular value ratio test.
Reference: [Lec89] <author> Yvan G. Leclerc. </author> <title> Constructing simple stable descriptions for image partitioning. </title> <journal> International Journal of Computer Vision, </journal> <volume> 3 </volume> <pages> 72-102, </pages> <year> 1989. </year> <month> 31 </month>
Reference-contexts: the two problems can lead to more robust estimation 7 of both structure and motion, but that the proper prior has not been investigated. 2.3 Scene Partitioning Problem In Section 6 we will see that we can formulate the segmentation of the optical flow field into a scene partitioning problem <ref> [Lec89] </ref>. In such problems, a partition of the image into distinct regions according to an assumed model or descriptive language is desired. The problem is formulated in terms of a cost functional which attempts to balance a number of model constraints. <p> These constraints include terms for fitting a smooth model to the data while simultaneously minimizing the number of distinct regions. The cost functional is often modeled as the probability of a given solution assuming gaussian departures from the model. Leclerc <ref> [Lec89] </ref> showed that the minimal solution could also be interpretated as the minimal length encoding describing the scene in terms of a given descriptive language. <p> Leclerc [Lec89] showed that the minimal solution could also be interpretated as the minimal length encoding describing the scene in terms of a given descriptive language. There are stochastic [GG84], region-growing [BF70, HP74], and continuation <ref> [Lec89, BZ87, GY91] </ref> methods for finding solutions to the scene partitioning problem when it is described in terms of a cost functional. Stochastic methods use simulated annealing programs in which a gradient descent method is perturbed by a stochastic process which decreases in magnitude as the global minimum is approached. <p> Thus the field ~ n is piecewise constant, being constant within each region and changing abruptly between regions. This problem is an example of the scene partitioning problem <ref> [Lec89] </ref> in which the scene is to be partitioned into distinct regions according to an assumed model or descriptive language.
Reference: [LF94] <author> Q.-T. Luong and O.D. Faugeras. </author> <title> The Fundamental matrix: theory, algorithms, and stability analysis. </title> <journal> International Journal of Computer Vision, </journal> <note> 1994. To appear. </note>
Reference-contexts: In the next section we look at previous work using dense flow for scene segmentation. A 4 review of the affine camera, and how under a rigid transformation it leads to a special form of the Fundamental Matrix <ref> [LF94] </ref> (which expresses the epipolar constraint in matrix form), can be found in Section 3. Under the affine camera model the epipolar constraint is linear in image coordinates, allowing for a standard least squares solution which is outlined in Section 4. <p> This constraint is unique to each rigid transformation and can be used to identify independently moving objects. The epipolar constraint has been used in a number of structure from motion algorithms [LH81, TH84, TK92]. The epipolar constraint can be used even in the case of uncalibrated cameras <ref> [LF94, LDFP93] </ref>. This allows for segmentation without any priors on shape or scene structure. In addition, the constraint holds for each point on an object, not just at the boundaries. The optical flow can therefore be sparse at the object boundaries.
Reference: [LH81] <author> H.C. Longuet-Higgins. </author> <title> A Computer Algorithm for Reconstructing a Scene from Two Projections. </title> <journal> Nature, </journal> <volume> 293 </volume> <pages> 133-135, </pages> <year> 1981. </year>
Reference-contexts: This constraint is unique to each rigid transformation and can be used to identify independently moving objects. The epipolar constraint has been used in a number of structure from motion algorithms <ref> [LH81, TH84, TK92] </ref>. The epipolar constraint can be used even in the case of uncalibrated cameras [LF94, LDFP93]. This allows for segmentation without any priors on shape or scene structure. In addition, the constraint holds for each point on an object, not just at the boundaries.
Reference: [MS85] <author> David Mumford and J. Shah. </author> <title> Boundary detection by minimizing functionals. </title> <booktitle> In IEEE Computer Society Computer Vision and Pattern Recognition conference proceedings, </booktitle> <volume> volume 22, </volume> <year> 1985. </year>
Reference-contexts: The violation of the prior can then also be used to detect boundaries [Bel93]. These priors usually take the form of a penalty for high depth gradients, biasing the solution toward a piecewise continuous depth map <ref> [MS85] </ref>. One work making use of a prior on the structure in the context of motion is [CFCHB89]. We have not used a prior on the scene structure for two reasons. First, most common priors try to limit some derivative of the depth as a function of image coordinates.
Reference: [MW86] <author> D.W. Murray and N.S. Williams. </author> <title> Detecting the image boundaries between optical flow fields from several moving planar facets. </title> <journal> Pattern Recognition Letters, </journal> <volume> 4 </volume> <pages> 87-92, </pages> <year> 1986. </year>
Reference-contexts: Black and Jepson [BJ94] first find regions of uniform brightness and calculate flow within these regions. In a number of segmentation algorithms, the assumption that the optical flow field was locally affine in image coordinates was used <ref> [Adi85, MW86, NSKO94, RCV92, WA94] </ref>. This assumes that the scene is piecewise-continuous in depth. * Adiv [Adi85] groups optical flow vectors that have similar affine coordinates using a Hough transform. Assuming each cluster represents the motion of a planar object, the object's motion is recovered. <p> Clusters with similar motions are then merged. Wang and Adelson perform a split-and-merge algorithm on the same parameterization. * Under the assumption of planar objects, the motion and shape can be computed in a least-squares fashion from a collection of measurements. Murray and Williams <ref> [MW86] </ref> begin by computing these parameters for small patches of the image. Patches are merged if they have similar parameters.
Reference: [NSKO94] <author> H.-H. Nagel, G. Socher, H. Kollnig, and M. Otte. </author> <title> Motion boundary detection in image sequences by local stochastic tests. </title> <booktitle> In Proceedings of the third EECV, </booktitle> <volume> volume II, </volume> <pages> pages 305-316, </pages> <address> Stockholm, </address> <year> 1994. </year>
Reference-contexts: Black and Jepson [BJ94] first find regions of uniform brightness and calculate flow within these regions. In a number of segmentation algorithms, the assumption that the optical flow field was locally affine in image coordinates was used <ref> [Adi85, MW86, NSKO94, RCV92, WA94] </ref>. This assumes that the scene is piecewise-continuous in depth. * Adiv [Adi85] groups optical flow vectors that have similar affine coordinates using a Hough transform. Assuming each cluster represents the motion of a planar object, the object's motion is recovered. <p> Murray and Williams [MW86] begin by computing these parameters for small patches of the image. Patches are merged if they have similar parameters. A boundary is detected when residuals from the fit to the parameters are high. * Nagel et al. <ref> [NSKO94] </ref> fit the derivatives of the intensity function directly to affine flow parameters. Assuming that the deviations from affine flow are normally distributed random variables, they detect boundaries via a statistical test. * Rognone et al. [RCV92] fit local patches of flow to five different flow templates. <p> This continues until we reach the final value of . See [Web94] for details and application of the algorithm on a range of scene segmentation problems. The use of a statistical test for flow-based segmentation can also be found in <ref> [NSKO94, BR87] </ref>. In these cases however, the deviation of the optical flow from affine parameters is tested instead. The statistical test can not be used to compare an affine region with a non-affine region because there exists a subspace of solutions for the Fundamental Matrix in the affine case.
Reference: [NXT93] <author> E. Nishimura, G. Xu, and S. Tsuji. </author> <title> Motion segmentation and correspondence using epipolar constraint. </title> <booktitle> In Proc. 1st Asian Conf. Computer Vision, </booktitle> <pages> pages 199-204, </pages> <address> Osaka, Japan, </address> <year> 1993. </year>
Reference-contexts: Thus even under the restrictions of scaled orthography, important motion information can be obtained. The use of the epipolar constraint to associate correspondences to distinct rigid objects has been used by Torr [Tor93, TM94], Nishimura et al. <ref> [NXT93] </ref> and Soatto and Perona [SP93]. Each of these papers form many subsets of the measurements and use a statistical test to determine which subsets are possible correct partitions. This would lead to a combinatorial explosion in hypothesis tests for dense correspondences.
Reference: [RCV92] <author> A. Rognone, M. Campani, and A. Verri. </author> <title> Identifying multiple motions from optical flow. </title> <booktitle> In Proceedings of the Second ECCV, </booktitle> <pages> pages 258-266, </pages> <address> Santa-Margerita, Italy, </address> <year> 1992. </year>
Reference-contexts: Black and Jepson [BJ94] first find regions of uniform brightness and calculate flow within these regions. In a number of segmentation algorithms, the assumption that the optical flow field was locally affine in image coordinates was used <ref> [Adi85, MW86, NSKO94, RCV92, WA94] </ref>. This assumes that the scene is piecewise-continuous in depth. * Adiv [Adi85] groups optical flow vectors that have similar affine coordinates using a Hough transform. Assuming each cluster represents the motion of a planar object, the object's motion is recovered. <p> Assuming that the deviations from affine flow are normally distributed random variables, they detect boundaries via a statistical test. * Rognone et al. <ref> [RCV92] </ref> fit local patches of flow to five different flow templates. Each of the templates consisted of first order flow (subsets of full affine). The results of these fits are clustered into possible labels. A relaxation labeling is then performed to assign these labels to the image patches.
Reference: [Sch89] <author> B. Schunck. </author> <title> Image flow segmentation and estimation by constraint line clustering. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 11 </volume> <pages> 1010-1027, </pages> <year> 1989. </year>
Reference-contexts: At lo-cations of depth edges, motion will introduce regions of occlusion and disocclusion which are often not explicitly modeled in optical flow routines. Some algorithms attempt to locate regions near a flow boundary before computing the flow <ref> [BR87, Sch89, BJ94] </ref>. The first two papers look at the gradient constraint within a local region. Schunck [Sch89] performs a cluster analysis on the constraint lines while Bouthemy and Rivero [BR87] use a statistical test to find separate motions. <p> Some algorithms attempt to locate regions near a flow boundary before computing the flow [BR87, Sch89, BJ94]. The first two papers look at the gradient constraint within a local region. Schunck <ref> [Sch89] </ref> performs a cluster analysis on the constraint lines while Bouthemy and Rivero [BR87] use a statistical test to find separate motions. Black and Jepson [BJ94] first find regions of uniform brightness and calculate flow within these regions.
Reference: [SFP94] <author> Stefano Soatto, Ruggero Frezza, and Pietro Perona. </author> <title> Recursive motion estimation on the essential manifold. </title> <booktitle> In Proceedings of the third ECCV, </booktitle> <volume> volume II, </volume> <pages> pages 61-72, </pages> <address> Stockholm, </address> <year> 1994. </year>
Reference-contexts: In this way the motion parameters can change with time. Each new optical flow field provides a new measurement for estimating this varying state. The work by Soatto et al. <ref> [SFP94] </ref> addresses the case of estimating the elements of the Fundamental Matrix in a Kalman Filter framework. Although their work was for the full Fundamental Matrix, it is easily adapted to the simpler affine form. <p> Instead there exists the epipolar constraint, equation (11), relating the product of the two. One can linearize this implicit constraint about the predicted state to produce a linear update equation. Details can be found in <ref> [SFP94] </ref>. We will simply state the results for the affine case below. In what Soatto et al. call the Essential Estimator, the state consists of the 5 non-zero elements of the affine Fundamental Matrix, ~ n (t).
Reference: [SP93] <author> Stefano Soatto and Pietro Perona. </author> <title> Three dimensional transparent structure segmentation and multiple 3d motion estimation from monocular perspective image sequences. </title> <type> Technical Report CIT-CNS 33/93, </type> <institution> California Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: The scene structure problem becomes analogous to stereopsis in that object depth is a function of distance along the epipolar constraint. Dense correspondences such as those in optical flow can lead to rich descriptions of the scene geometry. Recent work <ref> [TM94, SP93] </ref> has used the uniqueness of the epipolar constraint to attempt a partition of sparse correspondences. These approaches can only deal with small sets of correspondences and thus will result in sparse recovery of shape information. <p> Thus even under the restrictions of scaled orthography, important motion information can be obtained. The use of the epipolar constraint to associate correspondences to distinct rigid objects has been used by Torr [Tor93, TM94], Nishimura et al. [NXT93] and Soatto and Perona <ref> [SP93] </ref>. Each of these papers form many subsets of the measurements and use a statistical test to determine which subsets are possible correct partitions. This would lead to a combinatorial explosion in hypothesis tests for dense correspondences.
Reference: [SU87] <author> A. Spoerri and S. Ullman. </author> <title> The early detection of motion boundaries. </title> <booktitle> In Proceed--ings of the first ICCV, </booktitle> <pages> pages 209 - 218, </pages> <address> London, </address> <year> 1987. </year>
Reference-contexts: As a result, the first class of algorithms are operating in regions of the image where the optical flow is least accurate. 2.1 Motion Field Segmentation Early work on segmentation via motion looked for discontinuities in one or both components of the displacement field <ref> [SU87, TMB85, BA90, Bla92] </ref>. Since under general perspective projections the motion field is continuous as long as the depth of the viewed surface is continuous, discontinuities in the flow field signal depth discontinuities. Unfortunately, the flow field at discontinuities is difficult to recover.
Reference: [SZB93] <author> Larry S. Shapiro, Andrew P. Zisserman, and Michael Brady. </author> <title> Motion from point matches using affine epipolar geometry. </title> <type> Technical Report OUEL 1994/93, </type> <institution> University of Oxford, </institution> <year> 1993. </year>
Reference-contexts: In addition, the constraint holds for each point on an object, not just at the boundaries. The optical flow can therefore be sparse at the object boundaries. As pointed out by Koenderink and van Doorn [KvD91], and implemented by Shapiro et al. and Cernuschi-Frias et al. <ref> [SZB94, SZB93, CFCHB89] </ref>, under weak perspective projection motion parameters and shape descriptions can be obtained (modulo a relief transformation such as depth scaling) from just two views. Thus even under the restrictions of scaled orthography, important motion information can be obtained. <p> From equation (1) we can write x 0 in terms of the first projected point x, x 0 = sBx + sd ^ Z i + t (9) where t 00 0 sBt and ^ Z i is the scaled depth <ref> [SZB93] </ref> at point x, ^ Z i = f Z i =Z ave with Z i being the true depth. We can eliminate the depth component ^ Z i to get a linear constraint relating x and x 0 . <p> y sin ()) (18) The motion parameters of interest can be obtain from the matrix elements: s = c 2 + d 2 = a 2 + b 2 = arctan (b=a) = arctan (d=c) (19) Equations (18) and (19) are identical to the ones used in Shapiro et al. <ref> [SZB93] </ref>. In their application, they require a test to see if the angle from the arctan function is correct or is too large by a factor of .
Reference: [SZB94] <author> Larry S. Shapiro, Andrew P. Zisserman, and Michael Brady. </author> <title> Motion from point matches using affine epipolar geometry. </title> <booktitle> In Proceedings of the third EECV, </booktitle> <volume> volume II, </volume> <pages> pages 73-84, </pages> <address> Stockholm, </address> <year> 1994. </year>
Reference-contexts: In addition, the constraint holds for each point on an object, not just at the boundaries. The optical flow can therefore be sparse at the object boundaries. As pointed out by Koenderink and van Doorn [KvD91], and implemented by Shapiro et al. and Cernuschi-Frias et al. <ref> [SZB94, SZB93, CFCHB89] </ref>, under weak perspective projection motion parameters and shape descriptions can be obtained (modulo a relief transformation such as depth scaling) from just two views. Thus even under the restrictions of scaled orthography, important motion information can be obtained. <p> The flow itself is subject to error, but not the image location. Therefore it is more appropriate to minimize in velocity space. Equation (21) is similar to the cost function E 2 in Shapiro et al. <ref> [SZB94] </ref>. Adopting their terminology, we write ~ x = (u; v; x; y) and ~ n = (a; b; c 0 ; d 0 ). The minimization is performed by using a Lagrange multiplier, , on the constraint a 2 + b 2 = 1.
Reference: [TH84] <author> R.Y. Tsai and T.S. Huang. </author> <title> Uniqueness and estimation of three-dimensional motion parameters of rigid objects wirth curved surfaces. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 13-27, </pages> <year> 1984. </year>
Reference-contexts: This constraint is unique to each rigid transformation and can be used to identify independently moving objects. The epipolar constraint has been used in a number of structure from motion algorithms <ref> [LH81, TH84, TK92] </ref>. The epipolar constraint can be used even in the case of uncalibrated cameras [LF94, LDFP93]. This allows for segmentation without any priors on shape or scene structure. In addition, the constraint holds for each point on an object, not just at the boundaries.
Reference: [TK92] <author> C. Tomasi and T. Kanade. </author> <title> Shape and motion from image streams under orthography: a factorization method. </title> <journal> International Journal of Computer Vision, </journal> <volume> 9(2) </volume> <pages> 137-154, </pages> <year> 1992. </year>
Reference-contexts: This constraint is unique to each rigid transformation and can be used to identify independently moving objects. The epipolar constraint has been used in a number of structure from motion algorithms <ref> [LH81, TH84, TK92] </ref>. The epipolar constraint can be used even in the case of uncalibrated cameras [LF94, LDFP93]. This allows for segmentation without any priors on shape or scene structure. In addition, the constraint holds for each point on an object, not just at the boundaries.
Reference: [TM94] <author> Phil H. S. Torr and D. W. Murray. </author> <title> Stochastic motion clustering. </title> <booktitle> In Proceedings of the third EECV, </booktitle> <volume> volume II, </volume> <pages> pages 328-337, </pages> <address> Stockholm, </address> <year> 1994. </year>
Reference-contexts: The scene structure problem becomes analogous to stereopsis in that object depth is a function of distance along the epipolar constraint. Dense correspondences such as those in optical flow can lead to rich descriptions of the scene geometry. Recent work <ref> [TM94, SP93] </ref> has used the uniqueness of the epipolar constraint to attempt a partition of sparse correspondences. These approaches can only deal with small sets of correspondences and thus will result in sparse recovery of shape information. <p> Thus even under the restrictions of scaled orthography, important motion information can be obtained. The use of the epipolar constraint to associate correspondences to distinct rigid objects has been used by Torr <ref> [Tor93, TM94] </ref>, Nishimura et al. [NXT93] and Soatto and Perona [SP93]. Each of these papers form many subsets of the measurements and use a statistical test to determine which subsets are possible correct partitions. This would lead to a combinatorial explosion in hypothesis tests for dense correspondences. <p> The black and white colored regions (corresponding to the train, rotating ball and transition regions) were not labeled as affine. 29 Another method of motion segmentation based on the Fundamental Matrix can be found in <ref> [Tor93, TM94] </ref>. In this paper, the displacement vectors are segregated by finding outliers in a robust estimation of the Fundamental Matrix. Clusters of displacements are found through an iterative method. The combinatorial explosion of dense displacement fields would make this method difficult to implement.
Reference: [TMB85] <author> William Thompson, Kathleen Mutch, and Valdis Berzins. </author> <title> Dynamic occlusion analysis in optical flow fields. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(4) </volume> <pages> 374-383, </pages> <year> 1985. </year>
Reference-contexts: As a result, the first class of algorithms are operating in regions of the image where the optical flow is least accurate. 2.1 Motion Field Segmentation Early work on segmentation via motion looked for discontinuities in one or both components of the displacement field <ref> [SU87, TMB85, BA90, Bla92] </ref>. Since under general perspective projections the motion field is continuous as long as the depth of the viewed surface is continuous, discontinuities in the flow field signal depth discontinuities. Unfortunately, the flow field at discontinuities is difficult to recover.
Reference: [Tor93] <author> Phil H. S. Torr. </author> <title> Outlier detection and motion segmentation. </title> <type> Technical Report OUEL 1987/93, </type> <institution> University of Oxford, </institution> <year> 1993. </year>
Reference-contexts: Thus even under the restrictions of scaled orthography, important motion information can be obtained. The use of the epipolar constraint to associate correspondences to distinct rigid objects has been used by Torr <ref> [Tor93, TM94] </ref>, Nishimura et al. [NXT93] and Soatto and Perona [SP93]. Each of these papers form many subsets of the measurements and use a statistical test to determine which subsets are possible correct partitions. This would lead to a combinatorial explosion in hypothesis tests for dense correspondences. <p> The black and white colored regions (corresponding to the train, rotating ball and transition regions) were not labeled as affine. 29 Another method of motion segmentation based on the Fundamental Matrix can be found in <ref> [Tor93, TM94] </ref>. In this paper, the displacement vectors are segregated by finding outliers in a robust estimation of the Fundamental Matrix. Clusters of displacements are found through an iterative method. The combinatorial explosion of dense displacement fields would make this method difficult to implement. <p> Clusters of displacements are found through an iterative method. The combinatorial explosion of dense displacement fields would make this method difficult to implement. We take advantage of the gross number of estimates to gain robustness as opposed to finding specific outliers. However, the eigenvector perturbation method discussed in <ref> [Tor93] </ref> would make an easy test for outliers and could be implemented in our framework to detect and remove outliers. Acknowledgements This research was partially supported by the PATH project MOU 83. The authors wish to thank Paul Debevec for creating the synthetic image sequence.
Reference: [Ull79] <author> Shimon Ullman. </author> <title> The Interpretation of Visual Motion. </title> <publisher> MIT press, </publisher> <address> Cambridge, MA., </address> <year> 1979. </year>
Reference-contexts: The Fundamental Matrix cannot be uniquely determined in these cases. This is a consequence of that fact that a family of motions and scene structures can give rise to the same affine flow. This observation was also made by Ullman <ref> [Ull79] </ref>, but without reference to the Fundamental Matrix. The non-trivial causes of affine flow are either a special arrangement of the points observed (co-planarity) or special motions. The causes are enumerated below. * Case 1: Co-planar points. <p> This comes from the fact that we are viewing the plane from different views, and as a result, it gains depth, as illustrated in Figure 3. In fact under orthographic projection, 4 non-coplanar point correspondences through 3 frames are sufficient to determine both motion and structure in this case <ref> [Ull79] </ref>. Motions consisting of pure translational motion and/or rotation about an axis parallel to the optical axis (Case 2) will always result in affine flow under weak-perspective and orthographic projections. The measurement matrix will not gain rank with multiple frames. When a region 16 time.
Reference: [WA94] <author> John Wang and Edward Adelson. </author> <title> Representing moving images with layers. </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> 3(5) </volume> <pages> 625-638, </pages> <year> 1994. </year>
Reference-contexts: Black and Jepson [BJ94] first find regions of uniform brightness and calculate flow within these regions. In a number of segmentation algorithms, the assumption that the optical flow field was locally affine in image coordinates was used <ref> [Adi85, MW86, NSKO94, RCV92, WA94] </ref>. This assumes that the scene is piecewise-continuous in depth. * Adiv [Adi85] groups optical flow vectors that have similar affine coordinates using a Hough transform. Assuming each cluster represents the motion of a planar object, the object's motion is recovered.
Reference: [WAH93] <author> Juyang Weng, Narendra Ahuja, and Thomas Huang. </author> <title> Optimal motion and structure estimation. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(9) </volume> <pages> 864-884, </pages> <year> 1993. </year> <month> 33 </month>
Reference-contexts: We therefore use the value of trace ( ~v ) as an initial value of w i and update once we find ~ n. Weng et al. <ref> [WAH93] </ref> also calculated weighting factors in this fashion for their non-linear minimization. At each iteration of their algorithm they recalculated the weights. <p> This is in contrast to the weighting factor w i used in Section 4 for the deviation of the estimate in the direction perpendicular to the epipolar constraint. This separation of depth and motion parameters estimation was also pointed out by Weng et al. <ref> [WAH93] </ref>. The final depth estimate Z i is found using a weighted sum of local estimates. Z i = j2 X q j (34) where the region is the intersection of a local region about the point x i and the rigid motion region associated with that point. <p> If a recovery of the full perspective case was required, the same algorithm could be used. However, the calculation of the Fundamental Matrix from small displacements such as found in optical flow is not stable <ref> [WAH93, LDFP93] </ref>. 28 patterns while a toy train traverses the foreground. An example optical flow recovered is also shown. The labeled image is shown below. The background parts (colored grey) were identified as undergoing pure translational motion by the singular value ratio test.
Reference: [Web94] <author> Joseph Weber. </author> <title> Scene partitioning via statistic-based region growing. </title> <type> Techni--cal Report UCB/CSD-94-817, </type> <institution> Computer Science Division (EECS), University of California, Berkeley, </institution> <year> 1994. </year>
Reference-contexts: This is the basis of the Graduated Non-Convexity Algorithm of Blake and Zisserman [BZ87] and the mean field theory approach of Geiger and Girosi [GG91]. Our solution will use the region-growing method described in <ref> [Web94] </ref> to solve for the partition. This method uses a statistic-based region growing algorithm which assumes the solution is piecewise continuous in image coordinates. 3 Pro jections and Rigid Motions This section will describe the weak perspective camera. <p> To solve this partitioning problem we will use the region-growing method described in <ref> [Web94] </ref>. We outline the method below. From the previous section we saw that computing the cost (in terms of deviation of fit from data) of combining information from different regions involves only simple operations. <p> Newly formed regions are tested for affine flow solutions. The value of is increased and all possible mergings are checked again. This continues until we reach the final value of . See <ref> [Web94] </ref> for details and application of the algorithm on a range of scene segmentation problems. The use of a statistical test for flow-based segmentation can also be found in [NSKO94, BR87]. In these cases however, the deviation of the optical flow from affine parameters is tested instead.
Reference: [WM95] <author> Joseph Weber and Jitendra Malik. </author> <title> Robust computation of optical flow in a multi-scale differential framework. </title> <journal> International Journal of Computer Vision, </journal> <volume> 14(1), </volume> <year> 1995. </year>
Reference-contexts: The optical flow was computed using the multi-scale differential method of Weber and Malik <ref> [WM95] </ref>. The algorithm also produced the expected error covariance, ~v , associated with each flow estimate. For each sequence, the segmentation of the scene into distinct objects, the tracked motion of these objects, and the scaled scene structure are recovered.
References-found: 40

