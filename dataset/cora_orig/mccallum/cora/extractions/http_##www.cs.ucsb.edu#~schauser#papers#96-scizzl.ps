URL: http://www.cs.ucsb.edu/~schauser/papers/96-scizzl.ps
Refering-URL: http://www.cs.ucsb.edu/~schauser/papers/
Root-URL: http://www.cs.ucsb.edu
Email: fibel,schauser,chriss,weisg@cs.ucsb.edu  
Title: Implementing Active Messages and Split-C for SCI Clusters and Some Architectural Implications  
Author: Maximilian Ibel, Klaus E. Schauser, Chris J. Scheiman, and Manfred Weis 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: In this paper we discuss our preliminary experience with implementing active messages and Split-C on a cluster of workstations connected via Scalable Coherent Interface (SCI). Our experiments were conducted on a cluster of UltraSparcs connected via SBus-2 SCI adapter cards and an SCI switch. We present performance measurements and discuss several avenues for optimizations. As our experiments show, SCI offers much lower latencies than other cluster of workstation architectures, including a Myrinet based cluster, and other parallel machines like the Meiko CS-2. The simple remote load and store transactions supported by SCI allow for a very low latency implementation of the data transfer. However, active messages additionally require a transfer of control since the receiving processor has to execute the active message handler. Unfortunately, remote load and store transactions are not sufficient to efficiently implement this transfer of control. We propose hardware support for a remote enqueue transaction which would facilitate an efficient implementation. We hope that this architectural support can be included in future SCI based standards such as Serial Express. A remote enqueue transaction would not only support the efficient implementation of active messages, but also of other many-to-one communication primitives, such as RPC or MPI. 
Abstract-found: 1
Intro-found: 1
Reference: [ACK + 95] <author> R. Arpaci, D. Culler, A. Krishnamurthy, S. Stein-berg, and K. Yelick. </author> <title> Empirical evaluation of the CRAY T3D: A compiler perspective. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Over the past several years, active messages have been implemented on many different hardware platforms, including the CM-5 and Ncube/2 [vECGS92], Meiko CS-2 [SS95], Paragon and T3D <ref> [ACK + 95] </ref>, as well as clusters of workstations connected by FDDI [Mar94], ATM [vEBBV95] and Myrinet [CLMY96]. 1.2 Scalable Coherent Interface In this paper we study the issues arising when implementing many-to-one communication on a cluster of workstations connected via SCI.
Reference: [BCL + 95] <author> E. A. Brewer, F. T. Chong, L. T. Liu, S. D. Sharma, and J. Kubiatowicz. </author> <title> Remote Queues: Exposing Message Queues for Optimization and Atomicity. </title> <booktitle> In 7th Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: However, in a non-dedicated workstation polling wastes CPU time available to other processors. The ideal abstract data type for dealing with notification and synchronization is a queue. This simple insight under-lied the experience presented in [SS95] and was formalized in <ref> [BCL + 95] </ref>, where it was named the Remote Queue abstraction. Sending processors just enqueue their message on the remote queue. During a poll, the receiving processor just checks whether something has been enqueued. If so, it removes the message from the queue and processes it.
Reference: [CDG + 93] <author> D. E. Culler, A. Dusseau, S. C. Golstein, A. Krish-namurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proc. of Supercomputing, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: The small overhead and low latency facilitates building more complicated communication layers [TM94] and makes it a desirable target for high-level language compilers [CGSvE93], and forms the basis of the simple parallel language Split-C <ref> [CDG + 93] </ref>. <p> In this section we briefly discuss the key features of Split-C and our implementation on SCI. 3.1 Split-C Split-C is a parallel extension of the C programming language <ref> [CDG + 93] </ref>. Split-C follows the SPMD model of computation: a single thread of computation is started on each processor. Both the parallelism and data layout is explicit and is specified by the programmer.
Reference: [CGSvE93] <author> D. E. Culler, S. C. Goldstein, K. E. Schauser, and T. von Eicken. </author> <title> TAM A Compiler Controlled Threaded Abstract Machine. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18, </volume> <month> July </month> <year> 1993. </year>
Reference-contexts: Although they are quite primitive, active messages have become an important communication layer because of their efficiency. The small overhead and low latency facilitates building more complicated communication layers [TM94] and makes it a desirable target for high-level language compilers <ref> [CGSvE93] </ref>, and forms the basis of the simple parallel language Split-C [CDG + 93].
Reference: [CLMY96] <author> D. Culler, L. T. Liu, R. Martin, and C. Yoshikawa. </author> <title> Logp performance assessment of fast network interfaces. </title> <booktitle> IEEE Micro, </booktitle> <year> 1996. </year>
Reference-contexts: Over the past several years, active messages have been implemented on many different hardware platforms, including the CM-5 and Ncube/2 [vECGS92], Meiko CS-2 [SS95], Paragon and T3D [ACK + 95], as well as clusters of workstations connected by FDDI [Mar94], ATM [vEBBV95] and Myrinet <ref> [CLMY96] </ref>. 1.2 Scalable Coherent Interface In this paper we study the issues arising when implementing many-to-one communication on a cluster of workstations connected via SCI. We focus on active messages; the issues and analysis should apply equally to other forms of many-to-one communication. <p> Each node is equipped with a communications co-processor, which enables direct user-level communication between virtual addresses. Long messages are supported by a DMA engine. Additionally, we use a cluster of Ultra-1 workstations connected by Myrinet <ref> [CLMY96] </ref> located at Berkeley. All CPU's run at 167 MHz.
Reference: [Dol95a] <author> Dolphin. </author> <title> Multiprocessor Systems Design With SCI and Dolphin Technology. Dolphin Interconnect Solutions, </title> <year> 1995. </year>
Reference-contexts: Although the SCI standard provides versatile features to manage incoming messages, the Dolphin SCI adapter cards only support read, write and move transaction (no locking), and the device drivers support only two communication modes: the raw device interface and distributed shared memory <ref> [Dol95a, Dol95b] </ref>. For the raw interface, the device driver accepts conventional read () and write () systems calls to transfer data between nodes. The shared memory interface uses mmap () system calls to map shared segments from other processors directly into the virtual address space of an application.
Reference: [Dol95b] <author> Dolphin. </author> <note> SBus-to-SCI Adapter User's Guide, DIS303 SBus-2. Dolphin Interconnect Solutions A.S, </note> <year> 1995. </year>
Reference-contexts: Although the SCI standard provides versatile features to manage incoming messages, the Dolphin SCI adapter cards only support read, write and move transaction (no locking), and the device drivers support only two communication modes: the raw device interface and distributed shared memory <ref> [Dol95a, Dol95b] </ref>. For the raw interface, the device driver accepts conventional read () and write () systems calls to transfer data between nodes. The shared memory interface uses mmap () system calls to map shared segments from other processors directly into the virtual address space of an application.
Reference: [GL95] <author> D. B. Gustavson and Q. Li. </author> <title> Local-Area MultiProcessor: the Scalable Coherent Interface. </title> <type> Technical report, </type> <institution> SCIzzl, Santa Clara University, Department of Computer Engineering, </institution> <address> Santa Clara, California, </address> <year> 1995. </year>
Reference-contexts: Computational resources were provided by the NSF Instrumentation Grant CDA-9529418 and NSF Infrastructure Grant CDA-9216202. Scalable Coherent Interface (SCI) promise to deliver both <ref> [IEE93, GL95, Oma95, OP96] </ref>. The shared memory principle on which SCI is based allows for a very low latency data transfer since plain load or store operations automatically turn into remote communication, without the need to invoke system calls or program IO devices.
Reference: [HM93] <author> M. Homewood and M. McLaren. </author> <title> Meiko CS-2 Interconnect Elan-Elite Design. </title> <booktitle> In Proc. of Hot Interconnects, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: The SCI links have a nominal bandwidth of 1GBit/s. Our Meiko CS-2 consists of 64 Sparc based nodes connected via a fat tree communication network <ref> [HM93] </ref>. Running a slightly enhanced version of the Solaris 2.3 operating system on every node, the Meiko closely resembles a cluster of workstations connected by a fast network. Each node contains a 40 MHz SuperSparc processor with 1 MB external cache and 32 MB of main memory.
Reference: [IEE93] <editor> IEEE, </editor> <address> 345 East 47th Street, New York. </address> <institution> IEEE Standard for Scalable Coherent Interface (SCI), </institution> <year> 1993. </year>
Reference-contexts: Computational resources were provided by the NSF Instrumentation Grant CDA-9529418 and NSF Infrastructure Grant CDA-9216202. Scalable Coherent Interface (SCI) promise to deliver both <ref> [IEE93, GL95, Oma95, OP96] </ref>. The shared memory principle on which SCI is based allows for a very low latency data transfer since plain load or store operations automatically turn into remote communication, without the need to invoke system calls or program IO devices.
Reference: [IEE96] <author> IEEE. </author> <title> Physical Layer Application Programming Interface for the Scalable Coherent Interface (SCI PHY-API), </title> <note> Std P1596.9 draft 0.20 edition, </note> <month> July </month> <year> 1996. </year>
Reference-contexts: There are control and status registers (CSR) defined at standard addresses within each SCI node that may have special properties, defined in IEEE Std 1212. Unfortunately, it is not clear that every implementation exposes this mechanism to the user (the physical API <ref> [IEE96] </ref>). A natural concern is what happens when the pointer goes beyond the allocated segment. We really would like a circular queue, where the pointer wraps around. It is up to the message library to ensure that this queue never overflows.
Reference: [KSS + 96] <author> A. Krishnamurthy, K. E. Schauser, C. J. Scheiman, R. Y. Wang, D. E. Culler, and K. Yelick. </author> <title> Evaluation of Architectural Support for Global Address-Based Communication in Large-Scale Parallel Machines. </title> <booktitle> In 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: By using the DMA engine, active messages on the Meiko achieve approximately the nominal bandwidth of 40 MB/s. For Myrinet, the raw bandwidth reported in <ref> [KSS + 96] </ref> is approximately 35 MB/s, much lower than the nominal bandwidth of 80MB/s due to SBus limitations. For SCI, the nominal bandwidth of 1GBit/s can not be achieved, probably due to the same reasons. <p> The numbers for the CM-5, Meiko CS-2, Paragon, T3D and Myrinet NOW are from <ref> [KSS + 96] </ref>. Meiko numbers are from [SSFK96] and the Myrinet numbers from [KSS + 96]. 4.5 Split-C Applications To conclude the performance measurements, we run a suite of Split-C applications on the Meiko CS-2, the SCI cluster and the Myrinet cluster at Berkeley. <p> The numbers for the CM-5, Meiko CS-2, Paragon, T3D and Myrinet NOW are from <ref> [KSS + 96] </ref>. Meiko numbers are from [SSFK96] and the Myrinet numbers from [KSS + 96]. 4.5 Split-C Applications To conclude the performance measurements, we run a suite of Split-C applications on the Meiko CS-2, the SCI cluster and the Myrinet cluster at Berkeley.
Reference: [Mar94] <author> R. P. Martin. HPAM: </author> <title> An Active Message Layer for a Network of HP Workstations. </title> <booktitle> In Proc. of Hot Interconnects II, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Over the past several years, active messages have been implemented on many different hardware platforms, including the CM-5 and Ncube/2 [vECGS92], Meiko CS-2 [SS95], Paragon and T3D [ACK + 95], as well as clusters of workstations connected by FDDI <ref> [Mar94] </ref>, ATM [vEBBV95] and Myrinet [CLMY96]. 1.2 Scalable Coherent Interface In this paper we study the issues arising when implementing many-to-one communication on a cluster of workstations connected via SCI. We focus on active messages; the issues and analysis should apply equally to other forms of many-to-one communication.
Reference: [Oma95] <author> K. Omang. </author> <title> Performance results from SALMON, a cluster of Workstations Connected by SCI. </title> <type> Technical report, </type> <institution> Department of Informatics, University of Oslo, Norway, </institution> <year> 1995. </year>
Reference-contexts: Computational resources were provided by the NSF Instrumentation Grant CDA-9529418 and NSF Infrastructure Grant CDA-9216202. Scalable Coherent Interface (SCI) promise to deliver both <ref> [IEE93, GL95, Oma95, OP96] </ref>. The shared memory principle on which SCI is based allows for a very low latency data transfer since plain load or store operations automatically turn into remote communication, without the need to invoke system calls or program IO devices. <p> Our current implementation makes use of the Ul-traSPARC's block store operations which transfer a block of 8 double precision floating point registers (i.e. 64 bytes) from/to memory. Using the SCI DMA engine, <ref> [Oma95] </ref> reports a maximal bandwidth of 26 MB/s, with one SCI network interface and an Ultra-1 workstation.
Reference: [OP96] <author> K. Omang and B. Parady. </author> <title> Performance of Low-Cost UltraSparc Multiprocessors connected by SCI. </title> <type> Technical report, </type> <institution> Department of Informatics, University of Oslo, Norway and Sun Microsystems Inc., </institution> <year> 1996. </year>
Reference-contexts: Computational resources were provided by the NSF Instrumentation Grant CDA-9529418 and NSF Infrastructure Grant CDA-9216202. Scalable Coherent Interface (SCI) promise to deliver both <ref> [IEE93, GL95, Oma95, OP96] </ref>. The shared memory principle on which SCI is based allows for a very low latency data transfer since plain load or store operations automatically turn into remote communication, without the need to invoke system calls or program IO devices. <p> For Myrinet, the raw bandwidth reported in [KSS + 96] is approximately 35 MB/s, much lower than the nominal bandwidth of 80MB/s due to SBus limitations. For SCI, the nominal bandwidth of 1GBit/s can not be achieved, probably due to the same reasons. In <ref> [OP96] </ref>, the maximum achieved bandwidth (for 1 thread and one NIC per node) is approximately 26 MB/s. 4.2.2 Polling Overhead Another important consideration is the time that it takes for polling. In our implementation a poll has to check P incoming request buffers, as well as the reply buffer.
Reference: [SS95] <author> K. E. Schauser and C. J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Over the past several years, active messages have been implemented on many different hardware platforms, including the CM-5 and Ncube/2 [vECGS92], Meiko CS-2 <ref> [SS95] </ref>, Paragon and T3D [ACK + 95], as well as clusters of workstations connected by FDDI [Mar94], ATM [vEBBV95] and Myrinet [CLMY96]. 1.2 Scalable Coherent Interface In this paper we study the issues arising when implementing many-to-one communication on a cluster of workstations connected via SCI. <p> However, in a non-dedicated workstation polling wastes CPU time available to other processors. The ideal abstract data type for dealing with notification and synchronization is a queue. This simple insight under-lied the experience presented in <ref> [SS95] </ref> and was formalized in [BCL + 95], where it was named the Remote Queue abstraction. Sending processors just enqueue their message on the remote queue. During a poll, the receiving processor just checks whether something has been enqueued. <p> Thus, this scheme only works well for a small number of processors. As shown 1 Unfortunately, lock transactions are currently not supported by our Dolphin network cards. 2 Our experience on the Meiko CS-2 has shown <ref> [SS95] </ref> that using many buffers leads to a larger working set and therefore to a higher cache miss rate. Thus, there is a tradeoff between how many communication operations can be overlapped and the efficiency of each. in Figure 5, our implementation does not scale to 1000's of processors. <p> The Meiko measurements are from <ref> [SS95] </ref> and the Myrinet measurements from Myricom. On the SCI cluster, we measured a roundtrip latency of 7:9s for a pair of workstations directly connected, and 9:3s when an intermediate switch is present.
Reference: [SSFK96] <author> K. E. Schauser, C. J. Scheiman, J. M. Ferguson, and P. Z. Kolano. </author> <title> Exploiting the Capabilities of Communications Co-processors. </title> <booktitle> In 10th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: The numbers for the CM-5, Meiko CS-2, Paragon, T3D and Myrinet NOW are from [KSS + 96]. Meiko numbers are from <ref> [SSFK96] </ref> and the Myrinet numbers from [KSS + 96]. 4.5 Split-C Applications To conclude the performance measurements, we run a suite of Split-C applications on the Meiko CS-2, the SCI cluster and the Myrinet cluster at Berkeley.
Reference: [TM94] <author> L. W. Tucker and A. Mainwaring. </author> <title> CMMD: Active messages on the CM-5. </title> <journal> Parallel Computing, </journal> <volume> 20(4), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: As a result, active messages achieve an order of magnitude performance improvement over more traditional communication mechanisms. Although they are quite primitive, active messages have become an important communication layer because of their efficiency. The small overhead and low latency facilitates building more complicated communication layers <ref> [TM94] </ref> and makes it a desirable target for high-level language compilers [CGSvE93], and forms the basis of the simple parallel language Split-C [CDG + 93].
Reference: [vEBBV95] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proc. Symposium on Operating Systems Principles, </booktitle> <year> 1995. </year>
Reference-contexts: Over the past several years, active messages have been implemented on many different hardware platforms, including the CM-5 and Ncube/2 [vECGS92], Meiko CS-2 [SS95], Paragon and T3D [ACK + 95], as well as clusters of workstations connected by FDDI [Mar94], ATM <ref> [vEBBV95] </ref> and Myrinet [CLMY96]. 1.2 Scalable Coherent Interface In this paper we study the issues arising when implementing many-to-one communication on a cluster of workstations connected via SCI. We focus on active messages; the issues and analysis should apply equally to other forms of many-to-one communication.
Reference: [vECGS92] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year> <month> 10 </month>
Reference-contexts: Whether the architecture is based on shared memory or message passing, the machine must support many-to-one communication primitives. 1.1 Active Messages Active messages are probably the simplest form of many-to-one communication <ref> [vECGS92] </ref>. Although usable by application programmers, active messages are mainly targeted at compiler and library writers who want to obtain the utmost in performance. Each active message contains the address of a handler function, which is executed on the receiving processor upon arrival of the message. <p> Over the past several years, active messages have been implemented on many different hardware platforms, including the CM-5 and Ncube/2 <ref> [vECGS92] </ref>, Meiko CS-2 [SS95], Paragon and T3D [ACK + 95], as well as clusters of workstations connected by FDDI [Mar94], ATM [vEBBV95] and Myrinet [CLMY96]. 1.2 Scalable Coherent Interface In this paper we study the issues arising when implementing many-to-one communication on a cluster of workstations connected via SCI.
References-found: 20

