URL: http://www.cs.utexas.edu/users/ajohn/LCPC96.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/code/code-publications/
Root-URL: 
Email: fajohn,browneg@cs.utexas.edu  
Title: Compilation of Constraint Systems to Procedural Parallel Programs  
Author: Ajita John and J. C. Browne 
Address: Austin, TX 78712  
Affiliation: Dept. of Computer Sciences University of Texas,  
Abstract: This paper describes the first results from research 1 on the compilation of constraint systems into task level parallel programs in a procedural language. This is the only research, of which we are aware, which attempts to generate efficient parallel programs for numerical computations from constraint systems. Computations are expressed as constraint systems. A dependence graph is derived from the constraint system and a set of input variables. The dependence graph, which exploits the parallelism in the constraints, is mapped to the target language CODE, which represents parallel computation structures as generalized dependence graphs. Finally, parallel C programs are generated. The granularity of the derived dependence graphs depends upon the complexity of the operations represented in the type system of the constraint specification language. To extract parallel programs of appropriate granularity, the following features have been included: (i) modularity, (ii) operations over structured types as primitives, (iii) definition of sequential C functions. A prototype of the compiler has been implemented. The execution environment or software architecture is specified separately from the constraint system. The domain of matrix computations has been targeted for applications. Some examples have been programmed. Initial results are very encouraging. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Doug Baldwin. </author> <title> A status report on consul. </title> <editor> In Nicolau Gelernter and Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: a set of constraints upon the state variables defining the solution and choosing an appropriate subset of the state variables as the input set is an attractive approach to specification of programs, but there has been little success previously in attaining efficient execution of parallel programs derived from constraint representations <ref> [1] </ref>. There are however, both motivations for continuing research in this direction and reasons for optimism concerning success. Constraint systems have attractive properties for compilation to parallel computation structures. <p> Consul <ref> [1] </ref> is a parallel constraint language that uses an interpretive technique (local propagation) to find satisfying values for the system of constraints. This system offers performance only in the range of logic languages. <p> This condition will be relaxed in later versions of the compiler. An instance of Rule 4 is AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g. This example succinctly represents the constraints A <ref> [1] </ref> == A [0] AND A [2] == A [1] AND B [2] == A [1] AND B [3] == A [2]. <p> An instance of Rule 4 is AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g. This example succinctly represents the constraints A <ref> [1] </ref> == A [0] AND A [2] == A [1] AND B [2] == A [1] AND B [3] == A [2]. <p> An instance of Rule 4 is AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g. This example succinctly represents the constraints A <ref> [1] </ref> == A [0] AND A [2] == A [1] AND B [2] == A [1] AND B [3] == A [2].
Reference: 2. <author> K.M. Chandy and J. Misra. </author> <title> Parallel Program Design : A Foundation. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: There are however, both motivations for continuing research in this direction and reasons for optimism concerning success. Constraint systems have attractive properties for compilation to parallel computation structures. A constraint system gives the minimum specification (See <ref> [2] </ref> for the benefits from postponing imposition of program structure) for a computation, thereby offering the compiler freedom of choice 1 This work was supported in part through a grant from the Advanced Research Projects Office/CSTO, subcontract to Syracuse University #3531427 for derivation of control structure. <p> However, both require the programmer to provide explicit operators for specification of parallelism and the dependence graph structures which could be generated were restricted to trees. Equational specifications of computations is a restriction of constraint specifications. Unity <ref> [2] </ref> is the equational programming representation around which Chandy and Misra have built a powerful paradigm for the design of parallel programs. Again, Unity requires addition of explicit specifications for parallelism. 4 Language Description This section describes the different components of the programming system. <p> This condition will be relaxed in later versions of the compiler. An instance of Rule 4 is AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g. This example succinctly represents the constraints A [1] == A [0] AND A <ref> [2] </ref> == A [1] AND B [2] == A [1] AND B [3] == A [2]. <p> An instance of Rule 4 is AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g. This example succinctly represents the constraints A [1] == A [0] AND A <ref> [2] </ref> == A [1] AND B [2] == A [1] AND B [3] == A [2]. <p> This example succinctly represents the constraints A [1] == A [0] AND A <ref> [2] </ref> == A [1] AND B [2] == A [1] AND B [3] == A [2].
Reference: 3. <author> K.M. Chandy and S. Taylor. </author> <title> An Introduction to Parallel Programming. </title> <editor> Jones and Bartlett, </editor> <year> 1992. </year>
Reference-contexts: Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in [16]. The performance reported for the system is only comparable with commercial Prolog and Lisp systems [13]. Parallel logic programming <ref> [3, 6] </ref> is another area of related work. PCN [3] and Strand [6] are two parallel programming representations with a strong component of logic specification. <p> Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in [16]. The performance reported for the system is only comparable with commercial Prolog and Lisp systems [13]. Parallel logic programming [3, 6] is another area of related work. PCN <ref> [3] </ref> and Strand [6] are two parallel programming representations with a strong component of logic specification. However, both require the programmer to provide explicit operators for specification of parallelism and the dependence graph structures which could be generated were restricted to trees. <p> An instance of Rule 4 is AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g. This example succinctly represents the constraints A [1] == A [0] AND A [2] == A [1] AND B [2] == A [1] AND B <ref> [3] </ref> == A [2].
Reference: 4. <author> T.S. Collins and J.C. Browne. </author> <title> Matrix++: An object-oriented environment for parallel high-perfomance matrix computations. </title> <booktitle> In Proc. of the Hawaii Intl. Conf. on Systems and Software, </booktitle> <year> 1995. </year>
Reference-contexts: We have a built-in matrix type with its associated operations of addition, subtraction, multiplication and inverse. The matrix subtypes currently implemented in our system are lower and upper triangular and dense matrices. We plan to extend the type system to a richer class of matrices including hierarchical matrices <ref> [4] </ref>. Specialized algorithms based on the structure of the matrix can be invoked for the matrix subtypes. <p> There are several promising approaches: object-oriented formulations of data structures are one possibility. A simpler and more algorithmic basis for definition of constraints over partitions of matrices is to utilize a simple version of the hierarchical type theory for matrices of Collins and Browne <ref> [4] </ref>.
Reference: 5. <author> J.J. Dongarra and D.C. Sorenson. </author> <title> Schedule: Tools for developing and analyzing parallel fortran programs. </title> <type> Technical Report 86, </type> <institution> Argonne National Laboratory, </institution> <month> November </month> <year> 1986. </year>
Reference-contexts: The next two subsections describe two examples programmed in our system. 6.1 Block Triangular Solver (BTS) The example chosen is the solution of the AX = B linear algebra problem for a known lower triangular matrix A and vector B. The parallel algorithm <ref> [5] </ref> involves dividing the matrix into blocks and a constraint program (excluding declarations) for a problem instance split into 4 blocks is shown in Figure 8.
Reference: 6. <author> I. Foster and S. Taylor. Strand: </author> <title> New Concepts in Parallel Programming. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in [16]. The performance reported for the system is only comparable with commercial Prolog and Lisp systems [13]. Parallel logic programming <ref> [3, 6] </ref> is another area of related work. PCN [3] and Strand [6] are two parallel programming representations with a strong component of logic specification. <p> The performance reported for the system is only comparable with commercial Prolog and Lisp systems [13]. Parallel logic programming [3, 6] is another area of related work. PCN [3] and Strand <ref> [6] </ref> are two parallel programming representations with a strong component of logic specification. However, both require the programmer to provide explicit operators for specification of parallelism and the dependence graph structures which could be generated were restricted to trees. Equational specifications of computations is a restriction of constraint specifications.
Reference: 7. <author> Ian Foster. </author> <title> Designing and Building Parallel Programs. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: The software architecture or execution environment to which CODE is to compile is separately specified (SMP, DSM, NOW, etc). Finally, sequential and parallel C programs for shared memory machines like the CRAY J90, SPARCcenter 2000, and Sequent and the distributed memory PVM [10] system can be generated. An MPI <ref> [7] </ref> backend for CODE is also available. The granularity of the derived dependence graphs depends upon the types directly represented as primitives in the constraint representation.
Reference: 8. <author> B. Freeman-Benson and Alan Borning. </author> <title> The design and implementation of Kaleidoscope '90, a constraint imperative programming language. </title> <booktitle> In Computer Languages, </booktitle> <pages> pages 174-180. </pages> <publisher> IEEE Computer Society, </publisher> <month> April </month> <year> 1992. </year>
Reference-contexts: HPF does not support task parallelism. Also, existing control flow in its procedural programming style makes analysis for parallelism difficult. Thinglab [9] transforms constraints to a compilable language rather than to an interpretive execution environment as in many constraint systems. Kaleidoscope <ref> [8] </ref> integrates constraints with an object-oriented language and uses partial compilation for the constraints. Neither Thinglab nor Kaleidoscope is concerned with extraction of parallel structures. Vijay Saraswat described a family of concurrent constraint logic programming languages, the cc languages [16].
Reference: 9. <author> Bjorn N. Freeman-Benson. </author> <title> A module compiler for Thinglab II. </title> <booktitle> In Proc. 1989 ACM Conf. on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <month> October </month> <year> 1989. </year>
Reference-contexts: Declarative extensions have been added as part of High-Performance Fortran (HPF) [15], a portable data-parallel language with some optimization directives. HPF does not support task parallelism. Also, existing control flow in its procedural programming style makes analysis for parallelism difficult. Thinglab <ref> [9] </ref> transforms constraints to a compilable language rather than to an interpretive execution environment as in many constraint systems. Kaleidoscope [8] integrates constraints with an object-oriented language and uses partial compilation for the constraints. Neither Thinglab nor Kaleidoscope is concerned with extraction of parallel structures.
Reference: 10. <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy Sunderam. </author> <title> PVM: Parallel Virtual Machine:A Users' Guide and Tutorial for Networked Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The software architecture or execution environment to which CODE is to compile is separately specified (SMP, DSM, NOW, etc). Finally, sequential and parallel C programs for shared memory machines like the CRAY J90, SPARCcenter 2000, and Sequent and the distributed memory PVM <ref> [10] </ref> system can be generated. An MPI [7] backend for CODE is also available. The granularity of the derived dependence graphs depends upon the types directly represented as primitives in the constraint representation.
Reference: 11. <author> Ajita John and J.C. Browne. </author> <title> Extraction of parallelism from constraint specifications. </title> <booktitle> In Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications, volume III, </booktitle> <pages> pages 1501-1512, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Constraint systems offer some unique advan-tages as a representation from which parallel programs are to be derived <ref> [11] </ref>. Both "OR" and "AND" parallelism can be derived. Either effective or complete programs can be derived from constraint systems on demand. Programs for different computations can be derived from the same constraint specification by different choices of the input set of variables. <p> It also supports implementation of data parallelism, if desired <ref> [11] </ref>. The general requirements for a constraint representation which can be compiled to execute efficiently, include: (i) modularity for reusable modules, (ii) definition of sequential functions, and (iii) a rich type set. <p> In a later version of the compiler, provision will be made for user specification of parallelism for operations over structures. For a detailed description of the types of parallelism extracted see <ref> [11] </ref>. Parallelism in AND indexed sets: To extract parallelism, the computations within the compiled loop structures corresponding to AND indexed sets are examined. We discuss the case of loops with a single computation. The discussion can be generalized to the case of loops with multiple computations.
Reference: 12. <author> S. Lakshmivarahan and Sudarshan K. Dhall. </author> <title> Analysis and Design of Parallel Algorithms: Arithmetic and Matrix Problems. </title> <booktitle> Supercomputing and Parallel Processing. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: It is further assumed that the blocks B and C are symmetric and commute. A version of the parallel algorithm (taken from <ref> [12] </ref>) has a reduction phase in which the system is split into two subsystems: one for odd-indexed (reduced system) and another for even-indexed (eliminated system) terms. The reduction process is repeatedly applied to the reduced system. After k 1 iterations the reduced system contains the solution for a single term. <p> After k 1 iterations the reduced system contains the solution for a single term. The rest of the terms can be obtained by back-substitution. The constraint specification for the problem is shown in Figure 10. The variable names, BP, CP, dP correspond to the indexed terms B,C,d in <ref> [12] </ref> and are examples of the hierarchical data type in our system (elements of BP, CP and dP are matrices). The inputs to the system are BP [0], CP [0] and dP [i][0]. pow is a C function implementing the arithmetic power function. <p> Specification of the BOER Algorithm as a Constraint System The resulting dataflow graph is shown in Figure 11 (a) and corresponds to the dataflow in the algorithm in <ref> [12] </ref>. The START and STOP nodes initiate and terminate the program, respectively. A FOR node initiates the different iterations of a loop. The two such nodes in the figure correspond to the two outer indexed sets for index j in the constraint specification. <p> Furthermore, it is capable of detecting the parallelism within the expression 2 fl CP [j 1] fl CP [j 1] BP [j 1] fl BP [j 1] in the computation for BP [j]. The authors in <ref> [12] </ref> have mentioned that the single-solution step is the major bottleneck in the algorithm. But, in our experiments we found the reduction phase resistant to scalability. This is due to the fact that the computation for BP [j] and CP [j] involve matrix-matrix multiplication: an O (n 3 ) operation.
Reference: 13. <author> Michael Mehl, Ralf Scheidhauer, and Christian Schulte. </author> <title> An abstract machine for Oz. In Programming Languages, Implementations, Logics and Programs, </title> <booktitle> Seventh Intl. Symposium, LNCS, number 982. </booktitle> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1995. </year>
Reference-contexts: Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in [16]. The performance reported for the system is only comparable with commercial Prolog and Lisp systems <ref> [13] </ref>. Parallel logic programming [3, 6] is another area of related work. PCN [3] and Strand [6] are two parallel programming representations with a strong component of logic specification.
Reference: 14. <author> P. Newton and J. C. Browne. </author> <title> The CODE 2.0 graphical parallel programming environment. </title> <booktitle> In Proc. of the Intl. Conf. on Supercomputing, </booktitle> <pages> pages 167-177, </pages> <year> 1992. </year>
Reference-contexts: A dependence graph is derived from the program and mapped to the target language CODE <ref> [14] </ref>, which expresses parallel structure over sequential units of computation declaratively as a generalized dependence graph. The software architecture or execution environment to which CODE is to compile is separately specified (SMP, DSM, NOW, etc). <p> Phase 4. Specifications of the execution environment are used to optimally select the communication and synchronization mechanisms to be used by CODE <ref> [14] </ref>. This phase is yet to be completely defined. Phase 5. The dependence graph is mapped to the CODE parallel programming environment to produce sequential and parallel programs in C as executable for different parallel architectures. <p> As of now, there are provisions in the system to select certain program variables as shared variables in a shared memory environment. Also, some operations (e.g. matrix multiplication) can be chosen for parallel execution. 5.5 Phase 5 Our target for executable for constraint programs is the CODE <ref> [14] </ref> parallel programming environment. CODE takes a dependence graph as its input. The form of a node in a CODE dependence graph is given in Figure 7 (a).
Reference: 15. <author> Harvey Richardson. </author> <title> High Performance Fortran: History, overview and current developments. </title> <type> Technical Report TMC 261, </type> <institution> Thinking Machines Corporation. </institution>
Reference-contexts: Consul [1] is a parallel constraint language that uses an interpretive technique (local propagation) to find satisfying values for the system of constraints. This system offers performance only in the range of logic languages. Declarative extensions have been added as part of High-Performance Fortran (HPF) <ref> [15] </ref>, a portable data-parallel language with some optimization directives. HPF does not support task parallelism. Also, existing control flow in its procedural programming style makes analysis for parallelism difficult. Thinglab [9] transforms constraints to a compilable language rather than to an interpretive execution environment as in many constraint systems.
Reference: 16. <author> Vijay A. Saraswat. </author> <title> Concurrent Constraint Programming Languages. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon, School of Computer Science, Pittsburgh, </institution> <year> 1989. </year>
Reference-contexts: Kaleidoscope [8] integrates constraints with an object-oriented language and uses partial compilation for the constraints. Neither Thinglab nor Kaleidoscope is concerned with extraction of parallel structures. Vijay Saraswat described a family of concurrent constraint logic programming languages, the cc languages <ref> [16] </ref>. The logic and constraint portions are explicitly separated with the constraint part acting as an active data store for the logic portion of the program. Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in [16]. <p> logic programming languages, the cc languages <ref> [16] </ref>. The logic and constraint portions are explicitly separated with the constraint part acting as an active data store for the logic portion of the program. Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in [16]. The performance reported for the system is only comparable with commercial Prolog and Lisp systems [13]. Parallel logic programming [3, 6] is another area of related work. PCN [3] and Strand [6] are two parallel programming representations with a strong component of logic specification.
References-found: 16

