URL: http://trantor.cse.psu.edu/~yoo/publications/paging.ps
Refering-URL: http://trantor.cse.psu.edu/~yoo/research.html
Root-URL: http://www.cse.psu.edu
Email: E-mail: fyoo j dasg@cse.psu.edu  
Title: A Fast and Efficient Processor Management Scheme for k-ary n-cubes 1  
Author: Byung S. Yoo and Chita R. Das 
Keyword: Index Terms K-ary n-cube, multiprogramming, paging, processor man agement, virtual cube.  
Address: Park, PA 16802  
Affiliation: Department of Computer Science and Engineering The Pennsylvania State University University  
Abstract: Job scheduling and processor allocation are two key aspects of processor management technique in a multiprocessor operating system. Both of them play a significant role in achieving high system performance. An ideal processor management technique should be fast and efficient. We propose such a processor management technique, called virtual cube (VC), for k-ary n-cubes in this paper. The proposed scheme supports spatial allocation of jobs to the virtual cubes of the system and multiprograms the virtual cubes in a round-robin fashion. The VC scheme uses two efficient allocation algorithms, called enhanced k-ary buddy (EKB) and paged first fit (PFF), for the allocation of cubic and non-cubic (submesh) requests, respectively. A novel approach, called paging, is proposed for fast submesh allocation. When used with the first fit algorithm (hence called PFF), the paging scheme is shown to be extremely fast and efficient compared to other contemporary submesh allocation algorithms for k-ary n-cubes. We also study the impact of page size on performance and illustrate a methodology to compute optimal page size. Simulation results show that the VC scheme with its multiprogramming capability can boost system performance considerably and outperforms all existing policies while incurring minimal run-time overhead. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Al-Dhelaan and B. Bose, </author> <title> "A New Strategy for Processors Allocation in an N-Cube Multiprocessor," </title> <booktitle> Proc. Int'l Phoenix Conf. on Computers and Communications, </booktitle> <pages> pp. 114-118, </pages> <month> Mar. </month> <year> 1989. </year>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes <ref> [1, 3, 5, 8, 10, 13, 16, 20] </ref> and meshes [4, 7, 14, 15, 18, 21, 23]. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance. <p> For a submesh request, each side of the submesh is computed following a uniform, a normal, or a bimodal uniform distribution that is limited by the arity. In a bimodal uniform distribution, each side of a submesh request is uniformly selected from an interval <ref> [1; m] </ref> with a probability ' and selected from an interval [m + 1; k] with a probability (1-'). For simulating the virtual cube scheme, the time quantum for multiprogramming is set to 0.1 time unit and the context switching time is taken as 0.001 time unit.
Reference: [2] <author> B. Bose, B. Broeg, Y. Kwon and Y. Ashir, </author> <title> "Lee Distance and Topological Properties of k-ary n-cubes," </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. 44, </volume> <pages> pp. 1021-1030, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: High efficiency and low complexity are conflicting requirements. Therefore, resource management policies proposed in the literature exhibit various tradeoffs between efficiency and complexity. The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors <ref> [2, 6] </ref> which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes [1, 3, 5, 8, 10, 13, 16, 20] and meshes [4, 7, 14, 15, 18, 21, 23].
Reference: [3] <author> M. S. Chen and K. G. Shin, </author> <title> "Processor Allocation in an N-Cube Multiprocessor Using Gray Code," </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. 36, </volume> <pages> pp. 1396-1407, </pages> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes <ref> [1, 3, 5, 8, 10, 13, 16, 20] </ref> and meshes [4, 7, 14, 15, 18, 21, 23]. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance.
Reference: [4] <author> P. J. Chuang and N. F. Tzeng, </author> <title> "An Efficient Submesh Allocation Strategy for Mesh Computer Systems," </title> <booktitle> Proc. Int'l Conf. on Distributed Computing Systems, </booktitle> <pages> pp. 256-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes [1, 3, 5, 8, 10, 13, 16, 20] and meshes <ref> [4, 7, 14, 15, 18, 21, 23] </ref>. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance. However, these algorithms have the following drawbacks.
Reference: [5] <author> P. J. Chuang and N. F. Tzeng, </author> <title> "A Fast Recognition-Complete Processor Allocation Strategy for Hypercube Computers," </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. 41, </volume> <pages> pp. 467-479, </pages> <month> Apr. </month> <year> 1992. </year> <month> 20 </month>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes <ref> [1, 3, 5, 8, 10, 13, 16, 20] </ref> and meshes [4, 7, 14, 15, 18, 21, 23]. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance.
Reference: [6] <author> W. J. Dally, </author> <title> "Performance Analysis of k-ary n-cube Interconnection Networks," </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. 39, </volume> <pages> pp. 775-785, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: High efficiency and low complexity are conflicting requirements. Therefore, resource management policies proposed in the literature exhibit various tradeoffs between efficiency and complexity. The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors <ref> [2, 6] </ref> which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes [1, 3, 5, 8, 10, 13, 16, 20] and meshes [4, 7, 14, 15, 18, 21, 23]. <p> Low-dimensional k-ary n-cubes with fairly large arities are selected for this simulation because such structures are shown to perform better than high-dimensional k-ary n-cubes with small arities <ref> [6] </ref>. 9 4.1 System Model and Workload Characterization The workload is characterized by the distribution of job interarrival time, the distribution of job service demand, and the distribution of job size. Like prior studies, the job interarrival time is assumed to follow an exponential distribution.
Reference: [7] <author> J. Ding and L. N. Bhuyan, </author> <title> "An Adaptive Submesh Allocation Strategy for Two-Dimensional Mesh Connected Systems," </title> <booktitle> Proc. Int'l Conf. on Parallel Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 193-200, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes [1, 3, 5, 8, 10, 13, 16, 20] and meshes <ref> [4, 7, 14, 15, 18, 21, 23] </ref>. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance. However, these algorithms have the following drawbacks.
Reference: [8] <author> S. Dutt and J. P. Hayes, </author> <title> "Subcube Allocation in Hypercube Computers," </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. 40, </volume> <pages> pp. 341-352, </pages> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes <ref> [1, 3, 5, 8, 10, 13, 16, 20] </ref> and meshes [4, 7, 14, 15, 18, 21, 23]. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance.
Reference: [9] <author> V. Gautam and V. Chaudhary, </author> <title> "Subcube Allocation Strategies in a k-ary n-cube," </title> <booktitle> Sixth ISCA Int'l Conf. on Parallel and Distributed Computing Systems, </booktitle> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: Prior work on multiprocessor resource management has focused mostly on hypercubes [1, 3, 5, 8, 10, 13, 16, 20] and meshes [4, 7, 14, 15, 18, 21, 23]. Recently, allocation algorithms for k-ary n-cubes are reported in the literature <ref> [9, 17, 19] </ref>. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance. However, these algorithms have the following drawbacks. First, except [17], all the schemes support only subcube requests. <p> Submesh allocation is then reduced to the problem of finding the base and end of a free submesh. 2.2 Related Work Two k-ary n-cube allocation policies of interest are briefly described in this subsection. Other allocation schemes can be found in <ref> [9, 19] </ref>. K-ary Buddy: The k-ary buddy algorithm [9], proposed for cubic allocation, is an extension of the binary buddy scheme [11]. In the k-ary buddy strategy, a tree is arranged such that the leaves representing individual processors are ordered in an increasing order from 0 to k n 1. <p> Submesh allocation is then reduced to the problem of finding the base and end of a free submesh. 2.2 Related Work Two k-ary n-cube allocation policies of interest are briefly described in this subsection. Other allocation schemes can be found in [9, 19]. K-ary Buddy: The k-ary buddy algorithm <ref> [9] </ref>, proposed for cubic allocation, is an extension of the binary buddy scheme [11]. In the k-ary buddy strategy, a tree is arranged such that the leaves representing individual processors are ordered in an increasing order from 0 to k n 1. <p> cube is a constant, the complexity of the 3DT scheme is O (L 2 ), where L is the length of the free list. 3 Proposed Processor Management Technique 3.1 Allocation Schemes The primary objective of the previous allocation policies for k-ary n-cubes is to provide better subsystem recognition ability <ref> [9, 17, 19] </ref>. These algorithms involve extensive search for a free subsystem and thus often incur severe run-time overhead. <p> Utilization (Normal job size distribution = N (4:8; 1) for non-cubic jobs and N (1:5; 1) for cubic jobs, Page size = 1 (FCFS) and 2 (VC)) Fig. 11 depicts the response time behavior of the proposed VC scheme compared to that of the other existing policies for k-ary n-cubes <ref> [9, 17, 19] </ref>. These policies differ from each other in terms of the allocation algorithm employed and the types of requests they can handle. All the policies being compared use the FCFS scheduling. Therefore, we refer to them as FCFS policies. <p> All the policies being compared use the FCFS scheduling. Therefore, we refer to them as FCFS policies. The FCFS policies are classified into two classes depending on the types of requests they can process: cubic FCFS <ref> [9, 19] </ref> and non-cubic FCFS [17]. A cubic FCFS policy can handle only cubic requests while a non-cubic FCFS policy can manage any types of requests. It has been shown that all the cubic FCFS policies show very similar performance [19]. <p> A cubic FCFS policy can handle only cubic requests while a non-cubic FCFS policy can manage any types of requests. It has been shown that all the cubic FCFS policies show very similar performance [19]. So, we only simulate the k-ary buddy strategy <ref> [9] </ref> mainly due to its simplicity. Response time behaviors of VC and k-ary buddy with respect to the system load are presented in Fig. 11.a. The simulation results are reported for a 4-ary 5-cube because it is the model simulated in [19].
Reference: [10] <author> J. Kim, C. R. Das and W. Lin, </author> <title> "A Top-Down Processor Allocation Scheme for Hypercube Computers," </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 2, </volume> <pages> pp. 20-30, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes <ref> [1, 3, 5, 8, 10, 13, 16, 20] </ref> and meshes [4, 7, 14, 15, 18, 21, 23]. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance.
Reference: [11] <author> K. C. Knowlton, </author> <title> "A Fast Storage Allocator," </title> <journal> Commun. ACM, </journal> <volume> Vol. 8, </volume> <pages> pp. 623-625, </pages> <month> Oct. </month> <year> 1965. </year>
Reference-contexts: Other allocation schemes can be found in [9, 19]. K-ary Buddy: The k-ary buddy algorithm [9], proposed for cubic allocation, is an extension of the binary buddy scheme <ref> [11] </ref>. In the k-ary buddy strategy, a tree is arranged such that the leaves representing individual processors are ordered in an increasing order from 0 to k n 1.
Reference: [12] <author> D. E. Knuth, </author> <booktitle> The Art of Computer Programming, Vol. 1 Fundamental Algorithms, </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Using an efficient data structure <ref> [12] </ref>, the time complexity of k-ary buddy scheme can be drastically reduced. The idea is to maintain a list, called free list. Each entry of the free list contains a set of free subcubes at the same level of the tree.
Reference: [13] <author> P. Krueger, T. H. Lai and V. A. Radiya, </author> <title> "Job Scheduling Is More Important than Processor Allocation for Hypercube Computers," </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 5, </volume> <pages> pp. 488-497, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes <ref> [1, 3, 5, 8, 10, 13, 16, 20] </ref> and meshes [4, 7, 14, 15, 18, 21, 23]. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance. <p> Second, most allocation algorithms aim at providing better subsystem recognition ability at the expense of excessively high run-time overhead. The last and probably the most crucial problem is that the existing policies use FCFS scheduling. An allocation policy with the FCFS scheduling alone cannot boost the performance significantly <ref> [13, 16, 21] </ref>. The FCFS scheduling policy has what is known as the `blocking' property. A request for a large subsystem that cannot be allocated may block subsequent smaller requests, which are serviceable. This results in high job response time. <p> These algorithms involve extensive search for a free subsystem and thus often incur severe run-time overhead. However, it has been shown that the performance improvement through a better subsystem recognition ability is not significant and that job scheduling plays the more important role in boosting system performance <ref> [13, 16, 21] </ref>. Our main concern for processor allocation in this paper is therefore on minimizing the complexity of the underlying allocation algorithm.
Reference: [14] <author> K. Li and K. H. Cheng, </author> <title> "A Two Dimensional Buddy System for Dynamic Resource Allocation in A Partitionable Mesh Connected System," </title> <booktitle> Proc. ACM Computer Science Conf., </booktitle> <pages> pp. 22-28, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes [1, 3, 5, 8, 10, 13, 16, 20] and meshes <ref> [4, 7, 14, 15, 18, 21, 23] </ref>. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance. However, these algorithms have the following drawbacks.
Reference: [15] <author> T. Liu, W. Huang, F. Lombardi and L. N. Bhuyan, </author> <title> "A Submesh Allocation Scheme for Mesh-Connected Multiprocessor Systems," </title> <booktitle> Proc. Int'l Conf. on Parallel Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 159-163, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes [1, 3, 5, 8, 10, 13, 16, 20] and meshes <ref> [4, 7, 14, 15, 18, 21, 23] </ref>. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance. However, these algorithms have the following drawbacks.
Reference: [16] <author> P. Mohapatra, C. Yu and C. R. Das, </author> <title> "A Lazy Scheduling Scheme for Hypercube Computers," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 27, </volume> <pages> pp. 26-37, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes <ref> [1, 3, 5, 8, 10, 13, 16, 20] </ref> and meshes [4, 7, 14, 15, 18, 21, 23]. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance. <p> Second, most allocation algorithms aim at providing better subsystem recognition ability at the expense of excessively high run-time overhead. The last and probably the most crucial problem is that the existing policies use FCFS scheduling. An allocation policy with the FCFS scheduling alone cannot boost the performance significantly <ref> [13, 16, 21] </ref>. The FCFS scheduling policy has what is known as the `blocking' property. A request for a large subsystem that cannot be allocated may block subsequent smaller requests, which are serviceable. This results in high job response time. <p> These algorithms involve extensive search for a free subsystem and thus often incur severe run-time overhead. However, it has been shown that the performance improvement through a better subsystem recognition ability is not significant and that job scheduling plays the more important role in boosting system performance <ref> [13, 16, 21] </ref>. Our main concern for processor allocation in this paper is therefore on minimizing the complexity of the underlying allocation algorithm.
Reference: [17] <author> W. Qiao and L. M. Ni, </author> <title> "Efficient Processor Allocation in 3D Tori," </title> <type> Technical Report, </type> <institution> MSU-CPS-ACS-98, Michigan State University, </institution> <month> Sep. </month> <year> 1994. </year>
Reference-contexts: Prior work on multiprocessor resource management has focused mostly on hypercubes [1, 3, 5, 8, 10, 13, 16, 20] and meshes [4, 7, 14, 15, 18, 21, 23]. Recently, allocation algorithms for k-ary n-cubes are reported in the literature <ref> [9, 17, 19] </ref>. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance. However, these algorithms have the following drawbacks. First, except [17], all the schemes support only subcube requests. <p> Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance. However, these algorithms have the following drawbacks. First, except <ref> [17] </ref>, all the schemes support only subcube requests. Commercial systems such as the Cray T3D honor both cubic and non-cubic (submesh) requests. While a submesh could be scaled up to the nearest subcube size before being allocated, this will result in large internal fragmentation and poor system utilization. <p> The worst-case time complexity of the k-ary buddy algorithm is O (k n ). 3D Tori Allocation (3DT): An allocation algorithm for three-dimensional tori was proposed in <ref> [17] </ref>. It is the fastest and the most efficient of all the k-ary n-cube allocation policies reported so far in the literature. In addition, unlike other schemes, the 3DT algorithm can allocate both cubic and non-cubic (submesh) requests. This algorithm is based on the concept of free list. <p> cube is a constant, the complexity of the 3DT scheme is O (L 2 ), where L is the length of the free list. 3 Proposed Processor Management Technique 3.1 Allocation Schemes The primary objective of the previous allocation policies for k-ary n-cubes is to provide better subsystem recognition ability <ref> [9, 17, 19] </ref>. These algorithms involve extensive search for a free subsystem and thus often incur severe run-time overhead. <p> Utilization (Normal job size distribution = N (4:8; 1) for non-cubic jobs and N (1:5; 1) for cubic jobs, Page size = 1 (FCFS) and 2 (VC)) Fig. 11 depicts the response time behavior of the proposed VC scheme compared to that of the other existing policies for k-ary n-cubes <ref> [9, 17, 19] </ref>. These policies differ from each other in terms of the allocation algorithm employed and the types of requests they can handle. All the policies being compared use the FCFS scheduling. Therefore, we refer to them as FCFS policies. <p> All the policies being compared use the FCFS scheduling. Therefore, we refer to them as FCFS policies. The FCFS policies are classified into two classes depending on the types of requests they can process: cubic FCFS [9, 19] and non-cubic FCFS <ref> [17] </ref>. A cubic FCFS policy can handle only cubic requests while a non-cubic FCFS policy can manage any types of requests. It has been shown that all the cubic FCFS policies show very similar performance [19]. So, we only simulate the k-ary buddy strategy [9] mainly due to its simplicity. <p> Similar simulation experiments were conducted for a 16fi16fi16 three-dimensional torus using the VC scheme (with page sizes of 1 and 2) and the Best Fit with Lookahead (BFL) <ref> [17] </ref> strategy. The BFL is a free-list based non-cubic FCFS policy. Input parameters are set as described in [17]. <p> Similar simulation experiments were conducted for a 16fi16fi16 three-dimensional torus using the VC scheme (with page sizes of 1 and 2) and the Best Fit with Lookahead (BFL) <ref> [17] </ref> strategy. The BFL is a free-list based non-cubic FCFS policy. Input parameters are set as described in [17]. <p> The processor management policy adopted for the Cray T3D has been compared with the non-cubic FCFS policies in <ref> [17] </ref>, and it has been shown that the non-cubic FCFS policies provide much better performance than the Cray's processor management policy. <p> We propose in this research a versatile and efficient processor management scheme for k-ary n-cubes. 19 The policy is versatile in that it can serve jobs of any size that could be cubic or non-cubic (submesh) in nature. All prior work except <ref> [17] </ref> considered only cubic jobs. The policy is efficient in that it uses simple techniques for job allocation and a time-sharing technique for multiprogramming of the jobs. Cubic jobs are allocated using the simplest EKB scheme, while submesh requests are allocated using the fast paged first fit (PFF) algorithm. <p> When used in conjunction with the first fit (or any other bit-map based) allocation algorithm, the paging scheme can reduce the run-time allocation overhead by more than 85%. Its run-time overhead is significantly less than that of the 3DT scheme <ref> [17] </ref>. We also show an approach to find the optimal page size using system density as the optimization criterion. Regression analysis is used to obtain mathematical expressions for the system density from the simulation results. We use these two allocation techniques in a multiprogramming environment, called virtual cube (VC).
Reference: [18] <author> D. Das Sharma and D. K. Pradhan, </author> <title> "A Fast and Efficient Strategy for Submesh Allocation in Mesh-Connected Parallel Computers," </title> <booktitle> Proc. 5th IEEE Symp. on Parallel and Distributed Processing, </booktitle> <pages> pp. 682-689, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes [1, 3, 5, 8, 10, 13, 16, 20] and meshes <ref> [4, 7, 14, 15, 18, 21, 23] </ref>. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance. However, these algorithms have the following drawbacks.
Reference: [19] <author> K. Windisch, V. Lo and B. Bose, </author> <title> "Contiguous and Non-contiguous Processor Allocation Algorithms for k-ary n-cubes," </title> <booktitle> Proc. Int'l Conf. on Parallel Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 164-168, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Prior work on multiprocessor resource management has focused mostly on hypercubes [1, 3, 5, 8, 10, 13, 16, 20] and meshes [4, 7, 14, 15, 18, 21, 23]. Recently, allocation algorithms for k-ary n-cubes are reported in the literature <ref> [9, 17, 19] </ref>. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance. However, these algorithms have the following drawbacks. First, except [17], all the schemes support only subcube requests. <p> Submesh allocation is then reduced to the problem of finding the base and end of a free submesh. 2.2 Related Work Two k-ary n-cube allocation policies of interest are briefly described in this subsection. Other allocation schemes can be found in <ref> [9, 19] </ref>. K-ary Buddy: The k-ary buddy algorithm [9], proposed for cubic allocation, is an extension of the binary buddy scheme [11]. In the k-ary buddy strategy, a tree is arranged such that the leaves representing individual processors are ordered in an increasing order from 0 to k n 1. <p> cube is a constant, the complexity of the 3DT scheme is O (L 2 ), where L is the length of the free list. 3 Proposed Processor Management Technique 3.1 Allocation Schemes The primary objective of the previous allocation policies for k-ary n-cubes is to provide better subsystem recognition ability <ref> [9, 17, 19] </ref>. These algorithms involve extensive search for a free subsystem and thus often incur severe run-time overhead. <p> Utilization (Normal job size distribution = N (4:8; 1) for non-cubic jobs and N (1:5; 1) for cubic jobs, Page size = 1 (FCFS) and 2 (VC)) Fig. 11 depicts the response time behavior of the proposed VC scheme compared to that of the other existing policies for k-ary n-cubes <ref> [9, 17, 19] </ref>. These policies differ from each other in terms of the allocation algorithm employed and the types of requests they can handle. All the policies being compared use the FCFS scheduling. Therefore, we refer to them as FCFS policies. <p> All the policies being compared use the FCFS scheduling. Therefore, we refer to them as FCFS policies. The FCFS policies are classified into two classes depending on the types of requests they can process: cubic FCFS <ref> [9, 19] </ref> and non-cubic FCFS [17]. A cubic FCFS policy can handle only cubic requests while a non-cubic FCFS policy can manage any types of requests. It has been shown that all the cubic FCFS policies show very similar performance [19]. <p> A cubic FCFS policy can handle only cubic requests while a non-cubic FCFS policy can manage any types of requests. It has been shown that all the cubic FCFS policies show very similar performance <ref> [19] </ref>. So, we only simulate the k-ary buddy strategy [9] mainly due to its simplicity. Response time behaviors of VC and k-ary buddy with respect to the system load are presented in Fig. 11.a. The simulation results are reported for a 4-ary 5-cube because it is the model simulated in [19]. <p> <ref> [19] </ref>. So, we only simulate the k-ary buddy strategy [9] mainly due to its simplicity. Response time behaviors of VC and k-ary buddy with respect to the system load are presented in Fig. 11.a. The simulation results are reported for a 4-ary 5-cube because it is the model simulated in [19]. All other input parameters are the same as before. The results indicate that the VC scheme achieves significant improvement in performance compared to the cubic FCFS policies.
Reference: [20] <author> Q. Yang and H. Wang, </author> <title> "A New Graph Approach to Minimizing Processor Fragmentation in Hypercube Multiprocessors," </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 4, </volume> <pages> pp. 1165-1171, </pages> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes <ref> [1, 3, 5, 8, 10, 13, 16, 20] </ref> and meshes [4, 7, 14, 15, 18, 21, 23]. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance.
Reference: [21] <author> B. S. Yoo, C. R. Das and C. Yu, </author> <title> "Processor Management Techniques for Mesh-Connected Multiprocessors," </title> <booktitle> Proc. Int'l Conf. on Parallel Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 105-112, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes [1, 3, 5, 8, 10, 13, 16, 20] and meshes <ref> [4, 7, 14, 15, 18, 21, 23] </ref>. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance. However, these algorithms have the following drawbacks. <p> Second, most allocation algorithms aim at providing better subsystem recognition ability at the expense of excessively high run-time overhead. The last and probably the most crucial problem is that the existing policies use FCFS scheduling. An allocation policy with the FCFS scheduling alone cannot boost the performance significantly <ref> [13, 16, 21] </ref>. The FCFS scheduling policy has what is known as the `blocking' property. A request for a large subsystem that cannot be allocated may block subsequent smaller requests, which are serviceable. This results in high job response time. <p> Hence, the overhead of the PFF method is much lower than that of any bit-map based allocation algorithms. Finally, we use multiprogramming of jobs allocated on the virtual cubes. The virtual cube scheme is a variation of a generic processor management policy called, multitasking and multiprogramming (M 2 ) <ref> [21] </ref>. Multitasking here refers to allocation and multiprogramming refers to temporal scheduling. This generic scheme uses both space- and time-division multiplexing to assign jobs. With the virtual cube scheme, a k-ary n-cube is considered to consist of multiple virtual k-ary n-cubes. <p> These algorithms involve extensive search for a free subsystem and thus often incur severe run-time overhead. However, it has been shown that the performance improvement through a better subsystem recognition ability is not significant and that job scheduling plays the more important role in boosting system performance <ref> [13, 16, 21] </ref>. Our main concern for processor allocation in this paper is therefore on minimizing the complexity of the underlying allocation algorithm. <p> The actual run-time overhead is discussed in Section 4. 3.2 Virtual Cube (VC) Mechanism A processor management policy that combines the space- and timesharing techniques, called multitasking and multiprogramming (M 2 ), has been proposed and analyzed for hypercube and mesh-connected multiprocessors <ref> [21, 22] </ref>. The main idea of the M 2 policy is to allocate multiple jobs to a system or subsystem (multitasking) and run them in a time-sharing fashion (multiprogramming). Once allocated, the processes of a job are executed in a synchronous or asynchronous manner.
Reference: [22] <author> C. Yu, </author> <title> "Processor Management Policies for Multiprocessors," </title> <type> Ph.D. Dissertation, </type> <address> The Pennsylvania State University, </address> <pages> pp. 25-54, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The actual run-time overhead is discussed in Section 4. 3.2 Virtual Cube (VC) Mechanism A processor management policy that combines the space- and timesharing techniques, called multitasking and multiprogramming (M 2 ), has been proposed and analyzed for hypercube and mesh-connected multiprocessors <ref> [21, 22] </ref>. The main idea of the M 2 policy is to allocate multiple jobs to a system or subsystem (multitasking) and run them in a time-sharing fashion (multiprogramming). Once allocated, the processes of a job are executed in a synchronous or asynchronous manner.
Reference: [23] <author> Y. Zhu, </author> <title> "Efficient Processor Allocation Strategies for Mesh-Connected Parallel Computers," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 16, </volume> <pages> pp. 328-337, </pages> <month> Dec. </month> <year> 1992. </year> <month> 21 </month>
Reference-contexts: The focus of this paper is on developing an efficient processor management policy for k-ary n-cube multiprocessors [2, 6] which have drawn considerable attention in recent years. Prior work on multiprocessor resource management has focused mostly on hypercubes [1, 3, 5, 8, 10, 13, 16, 20] and meshes <ref> [4, 7, 14, 15, 18, 21, 23] </ref>. Recently, allocation algorithms for k-ary n-cubes are reported in the literature [9, 17, 19]. These policies attempt to improve the subsystem recognition ability of the underlying allocation algorithms to obtain better performance. However, these algorithms have the following drawbacks. <p> Else Return. The worst-case time complexity of the EKB scheme is O (kn), and this is the fastest subcube allocation algorithm. 5 Coverage set Allocated submesh 3 x 2 job 22 3.1.2 Paged First Fit (PFF) We use the first fit (FF) strategy <ref> [23] </ref> in this paper for submesh allocation. It is simple and yet provides relatively good submesh recognition ability. The FF algorithm tries to find the base of an available submesh for a job that needs to be allocated. <p> This drastically reduces the search space and thus the allocation overhead 3 of the processor management scheme. The proposed processor allocation scheme is called paged first fit (PFF) and is described below. For the detailed discussion of the FF policy, the reader should refer to <ref> [23] </ref>. Paged First Fit Algorithm: PFF (J i ) Allocation: 1. Let J i be a job to be allocated. 2. Determine the number of pages required by J i . 3. <p> In addition, we need a more complex allocation scheme and data structures to manage two totally different types of requests. By segregating these two types of jobs to different virtual cubes, we can avoid such problems. System performance improves significantly when incoming jobs are small <ref> [23] </ref>. This is because small jobs are more likely to be allocated and thus increase the parallelism of the system. On the other hand, a large job occupies more number of processors and prevents subsequent smaller jobs from being allocated and hence increases the queueing delay.
References-found: 23

