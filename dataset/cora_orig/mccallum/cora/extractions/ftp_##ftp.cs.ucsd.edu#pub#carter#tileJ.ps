URL: ftp://ftp.cs.ucsd.edu/pub/carter/tileJ.ps
Refering-URL: http://www.cs.ucsd.edu/users/ferrante/papers.html
Root-URL: http://www.cs.ucsd.edu
Title: Hierarchical Tiling: A Methodology for High Performance  
Author: Larry Carter Jeanne Ferrantey Susan Flynn Hummel Bowen Alpern Kang-Su Gatlin 
Abstract: Good parallel algorithms are not enough; computer features such as the memory hierarchy and processor architecture need to be exploited to achieve high performance on parallel machines. Hierarchical tiling is a methodology for exploiting parallelism and locality at all levels of the memory/processor hierarchy: functional units, registers, caches, multiple processors, and disks. Hierarchical tiling concentrates on the interaction between multiple levels of tilings. One novel idea of hierarchical tiling is the naming of the values on the surface of a tile. Names determine where values are stored in the memory/processor hierarchy. Storage for the surface of a tile is materialized at that level, while interior elements of the tile only require storage as temporaries at a lower level of hierarchical memory. A second distinctive feature is that hierarchical tiling provides explicit control of all data movement, both within and between the levels memory/processor hierarchy. This is accomplished by using a three-stage tiling discipline to choreograph data movement. It allows for inter- and intra-level overlapping of computations and data movement. The size and the shape of the tiles impact the parallelism and communication to computation ratio at each level. We illustrate guidelines and modeling techniques for selecting tile sizes and shapes, as well as the choreography of data movement, using two examples. In the first, uniprocessor performance was sped up by a factor of three, achieving perfect use of the superscalar processor pipeline, and the cache miss penalty was reduced to an insignificant level. A high level of multiple processor efficiency was also achieved. In the second example, taken from a production code, performance gains up to a factor of two (depending on the machine and the program variant) were seen. These improvements ware largely due to hierarchical tiling's ability to improve instruction-level parallelism.
Abstract-found: 1
Intro-found: 1
Reference: [ACFS94] <author> Alpern, B., L. Carter, E. Feig, and T. Selker, </author> <title> "The Uniform Memory Hierarchy Model of Computation," </title> <journal> Algorithmica, </journal> <month> June </month> <year> 1994. </year>
Reference-contexts: Hierarchical tiling takes responsibility for the entire process, though ultimately the other mechanisms may overrule its choices. Hierarchical tiling proceeds as follows: 1. The processing elements, memory hierarchy, and communication capabilities of the target computer are modeled. We use the Parallel Memory Hierarchy (PMH) model <ref> [ACFS94] </ref>, which is a tree of memory modules with processors at the leaves of the tree. Each module has a separate address space, and all movement of data is explicit. 2.
Reference: [ACF93] <author> B. Alpern, L. Carter and J. Ferrante, </author> <title> "Modeling Parallel Computers as Memory Hierarchies," Proceedings, Programming Models for Massively Parallel Computers (September, </title> <year> 1993). </year>
Reference-contexts: The model has parameters that give the communication costs of each channel, and the memory capacity and number of children of each module. A procedure for deriving a model for given computer is given in <ref> [ACF93] </ref>. In the PMH model, all of the channels can be active at the same time, although two channels cannot simultaneously touch the same block. model the two-stage floating-point pipeline. Each module has a separate name space. Data movement along a channel is controlled by the parent module.
Reference: [ACG95] <author> B. Alpern, L. Carter and K.S. Gatlin, </author> <title> "Microparallelism and High-performance Protein Matching", </title> <booktitle> Supercomputing 95, </booktitle> <month> (December, </month> <year> 1995). </year> <month> 22 </month>
Reference-contexts: For parallelism (as illustrated in section 4.4), data to be communicated can be stored directly in buffers, possibly eliminating copy instructions. In the final fl Preliminary versions of portions of this paper appeared in three conference papers <ref> [CFH95, CFH95, ACG95] </ref>. y UCSD, CS&E Dept., 9500 Gilman Dr., La Jolla, CA 92093-0114, fcarter, ferranteg@cs.ucsd.edu. This work was supported in part by NSF Grant CCR-9504150. z Polytechnic University, Brooklyn, NY 11201 and IBM T.J. Watson Research Center, hummel@mono.poly.edu. <p> For b 1:5, encoding the score s by b s produces nearly-identical scores to the fixed-point variant, and both algorithmic variants select exactly the same set of strings as good matches <ref> [ACG95] </ref>. 15 To be historically accurate, we note that we began our experiments on this application with an implementation that used space-efficient one-dimensional matrices. We wrote the pseudo-code to help us understand the program and facilitate our experiments.
Reference: [B92] <author> Banerjee, U., </author> <title> "Unimodular Transformations of Double Loops", </title> <booktitle> in Advances in Languages and Compilers for Prallel Processing , A. </booktitle> <editor> Nicolau and D. Padua, editors, </editor> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference: [CFH95] <author> Carter, L., J. Ferrante and S. Flynn Hummel, </author> <title> "Efficient Parallelism via Hierarchical Tiling," </title> <booktitle> Proc. of SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> (Feb. </month> <year> 1995). </year>
Reference-contexts: For parallelism (as illustrated in section 4.4), data to be communicated can be stored directly in buffers, possibly eliminating copy instructions. In the final fl Preliminary versions of portions of this paper appeared in three conference papers <ref> [CFH95, CFH95, ACG95] </ref>. y UCSD, CS&E Dept., 9500 Gilman Dr., La Jolla, CA 92093-0114, fcarter, ferranteg@cs.ucsd.edu. This work was supported in part by NSF Grant CCR-9504150. z Polytechnic University, Brooklyn, NY 11201 and IBM T.J. Watson Research Center, hummel@mono.poly.edu.
Reference: [CFH95] <author> Carter, L., J. Ferrante and S. Flynn Hummel, </author> <title> "Hierarchical Tiling for Improved Superscalar Perfomance", </title> <booktitle> Proc. of International Parallel Processing Symposium, </booktitle> <month> (Apr. </month> <year> 1995). </year>
Reference-contexts: For parallelism (as illustrated in section 4.4), data to be communicated can be stored directly in buffers, possibly eliminating copy instructions. In the final fl Preliminary versions of portions of this paper appeared in three conference papers <ref> [CFH95, CFH95, ACG95] </ref>. y UCSD, CS&E Dept., 9500 Gilman Dr., La Jolla, CA 92093-0114, fcarter, ferranteg@cs.ucsd.edu. This work was supported in part by NSF Grant CCR-9504150. z Polytechnic University, Brooklyn, NY 11201 and IBM T.J. Watson Research Center, hummel@mono.poly.edu.
Reference: [TC94] <institution> Cornell Theory Center Online Documentation System, Documentation for the IBM Scalable POWERparallel System SP1 (available vi gopher) (April, </institution> <year> 1994). </year>
Reference-contexts: The program was run on the SP1 to verify the accuracy of our analysis. For the 62.5 MHz/sec SP1, we use the message communication cost estimate given in <ref> [TC94] </ref> of 50 microseconds (3125 cycles) plus .12 microsecond (7.5 cycles) per byte transferred.
Reference: [GJG88] <author> Gannon, D., W. Jalby and K. Gallivan, </author> <title> "Strategies for Cache and Local Memory Management by Global Program Transformation," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 5, No. 5, </volume> <month> October </month> <year> 1988, </year> <pages> pp. 587-616. </pages>
Reference: [HA90] <author> Hudak, D. E. and S. G. Abraham, </author> <title> "Compiler Techniques for Data Partitioning of Sequentially Iterated Parallel Loops," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 2 No. 3, </volume> <month> July, </month> <year> 1991, </year> <pages> pp. 318-328. </pages>
Reference: [H93] <author> Flynn Hummel, S., I. Banicescu, C.-T. Wang and J. Wein, </author> <title> "Load balancing and data locality via fractiling: an experimental study," Languages, Compilers and Run-Time Systems for Scalable Computers, </title> <editor> B. K. Szymanski and B. Sinharoy (Editors), </editor> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1995, </year> <pages> pp. 85-89. </pages>
Reference-contexts: This assumption ignores the 11 It may be that a mixed strategy | using small tiles at the ends of the computation and larger tiles in the middle | would be advantageous. This is closely related to the technique of fractiling <ref> [H93] </ref>. 12 fact that some tiles are not full W fi H rectangles, and that there will certainly be irregularities in the communication times. However, the formula does take into account that full P -way parallelism doesn't begin immediately, using the following reasoning. Let k = P H=W .
Reference: [IT88] <author> Irigoin, F. and R. Triolet, </author> <title> "Supernode Partitioning," </title> <booktitle> Proc. 15th ACM Symp. on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1988, </year> <pages> pp. 319-328. </pages>
Reference: [JM86] <author> Jalby, W. and U. Meier, </author> <title> "Optimizing Matrix Operations on a Parallel Multiprocessor with a Hierarchical Memory System," </title> <booktitle> Proc. International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1986, </year> <pages> pp. 429-432. </pages>
Reference: [KM92] <author> Kennedy, K. and K. S. Mc Kinley, </author> <title> "Optimizing for Paralelism and Data Locality," </title> <booktitle> Proc. International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1992, </year> <pages> pp. 323-334. </pages>
Reference: [LRW91] <author> Lam, M, E. Rothberg, and M. Wolf, </author> <title> "The Cache Performance and Optimization of Blocked Algorithms," </title> <booktitle> Proc. ASPLOS, </booktitle> <month> April </month> <year> 1991, </year> <pages> pp. 63-74. </pages>
Reference: [LCW94] <author> Larus, J. R., S. Chandra and D.A. Wood, "CICO: </author> <title> A Practical Shared-Memory Programming Performance Model", in Portability and Performance for Parallel Processing, </title> <editor> A. Hey and J. Ferrante editors, </editor> <publisher> John Wiley and Sons, </publisher> <year> 1994. </year>
Reference-contexts: Some aspects of this are easy, for instance, message-passing primitives give explicit control over interprocessor communication. Certain language features may help, e.g. CICO directives <ref> [LCW94] </ref> guide cache choices. However, the programming languages may not provide a way to express all the desired data movement. Programs can only indirectly specify register usage or schedule instructions. One must sometimes "trick" the compiler into producing the desired code, or, as a last resort, write assembly-language code.
Reference: [L90] <author> Lee, F. F., </author> <title> "Partitioning of Regular Computation on Multiprocessor Systems," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 9, </volume> <year> 1990, </year> <pages> pp. 312-317. </pages>
Reference: [NJVLL93] <author> Navarro, J. J., A. Juan, M. Valero, J. M. Llaberia, and T. Lang, </author> <title> "Multilevel Orthogonal Blocking for Dense Linear Algebra Computations," </title> <journal> IEEE Technical Committee on Computer Architecture Newsletter, </journal> <month> Fall </month> <year> 1993, </year> <pages> pp. 10-14. </pages>
Reference-contexts: Hierarchical tiling uses an abstract machine model that captures the degree of parallelism and the memory capacity of each level of the processor/memory hierarchy of a machine. Each level is tiled by dividing a tile of a higher level into subtiles. Thus this work (like that of <ref> [NJVLL93] </ref>) explores the interaction of tiling at the different levels. But whereas traditional tiling affects data movement implicitly by altering the order 1 Since the ISG has a distinct node for each value produced, only true dependences need to be represented.
Reference: [PW86] <author> Padua, D. A. and M. J. Wolfe, </author> <title> "Advanced Compiler Optimizations for Supercomputers," </title> <journal> CACM, </journal> <month> December, </month> <year> 1986, </year> <pages> pp. 1184-1201. </pages>
Reference-contexts: If the loop bounds are known, then a finite subset of Z N can be used. The Iteration Space Graph (ISG) [RS91] is a directed acyclic graph whose nodes represent the initial values and computations in the loop body, and the edges represent data dependences <ref> [PW86] </ref> 1 between nodes. An example ISG is given in figure 1. Tiling partitions the iteration space into uniform tiles of a given size and shape (except at boundaries of the iteration space) that tessellate the iteration space.
Reference: [RS91] <author> Ramanujam, J. and P. Sadayappan, </author> <title> "Tiling Multidimensional Iteration Spaces for Nonshared Memory Machines," </title> <booktitle> Proceedings of Supercomputing, </booktitle> <month> November, </month> <year> 1991, </year> <pages> pp. 111-120. </pages>
Reference-contexts: Given such a loop, the iteration space is Z N , where Z represents the set of integers, and N is the depth of nesting. If the loop bounds are known, then a finite subset of Z N can be used. The Iteration Space Graph (ISG) <ref> [RS91] </ref> is a directed acyclic graph whose nodes represent the initial values and computations in the loop body, and the edges represent data dependences [PW86] 1 between nodes. An example ISG is given in figure 1. <p> Some guidelines for creating efficient hierarchical tilings are summarized below. These will be illustrated in the examples. * Use a tile shape that "contains" the dependence vectors from an initial node. As discussed in <ref> [RS91] </ref>, this insures a legal tiling with simple loops. * If parallel execution is possible, generate independent subtiles.
Reference: [RAP87] <author> Reed, D. A., L. M. Adams and M. L. Patrick, </author> <title> "Stencil and Problem Partitionings: Their Influence on the Performance of Multiple Processor Systems," </title> <journal> IEEE Transactions on Computers, </journal> <month> July, </month> <year> 1987, </year> <pages> pp. 845-858. </pages>
Reference: [SSM89] <author> Saavedra-Barrera, R. H., A. J. Smith, and E. Miya, </author> <title> "Machine Characterization Based on an Abstract High-Level Language Machine," </title> <journal> IEEE Transactions on Computers, </journal> <month> December, </month> <year> 1989, </year> <pages> pp. 1659-1679 </pages>
Reference-contexts: T 's input surface is moved from the parent module P into the module C that will execute it. (Any portion of the surface that is already in C need not be moved.) 2 The PMH model may not be ideal. Further work such as <ref> [SSM89] </ref> is needed to understand which machine characteristics are important for performance. 4 2. C executes the tile. During this time, no data of T may be moved between P and C.
Reference: [TAC94] <author> Thomborson, C., B. Alpern and L. Carter, </author> <title> "Rectilinear Steiner Tree Minimization on a Workstation," Computational Support for Discrete Mathematics, </title> <editor> N. Dean and G. Shannon editors, </editor> <booktitle> Volume 15 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science, American Mathematics Society (1994). </booktitle>
Reference-contexts: When this happens, the most expedient solution often is to find a computer with more memory. Although not illustrated in our examples, the techniques described in this paper (both ours and the previous tiling references) can be applied to some problems with great success <ref> [TAC94] </ref>. The standard techniques cited in section 2 can greatly reduce the amount of data that needs to be paged out to disk and brought back into main memory during the course of a computation.
Reference: [WL91] <author> Wolf, M. E. and M. S. Lam, </author> <title> "A Data Locality Optimizing Algorithm," </title> <booktitle> Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June, </month> <year> 1991, </year> <pages> pp. 30-44. </pages>
Reference-contexts: relax interprocessor synchronization. 4 Example 1: PDE-like Code This section will use the SP1 and a simple doubly-nested loop as a running example: for I = 1 to T A (J) = C*(A (J-1) + (A (J) + A (J+1))); This example comes from a paper by Wolf and Lam <ref> [WL91] </ref>. The value computed in a typical loop iteration depends on the old value of the current array element, the value just computed, and the old value of the next element in the array. <p> Therefore if two nodes lie on a line with negative slope, there is no dependence between them, and they can be executed in either order. There are many possible computation orders that respect the data dependences in the ISG. For instance, Wolf and Lam <ref> [WL91] </ref> improve cache usage on a uniprocessor by partitioning the parallelogram into vertical piles of rectangles (and partial rectangles), and evaluating the piles left to right (using a bottom to top order within each strip).
Reference: [WL91b] <author> Wolf, M. E. and M. S. Lam, </author> <title> "A Loop Transformation Theory and an Algorithm to Maximize Parallelism," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 2, No. 4, </volume> <month> October, </month> <year> 1991, </year> <pages> pp. 452-471. </pages>
Reference-contexts: 1 Introduction Obtaining high performance on parallel machines involves not just finding a good parallelization but also optimizing data movement within a processing node and exploiting each processor's capabilities. Hierarchical tiling builds on a large body of literature on tiling, notably that of Wolf and Lam <ref> [WL91b] </ref>. It applies both optimizations of parallelism and locality at all levels of the processing-element/memory hierarchy. Thus, it encompasses many program optimization techniques that are often considered separately.
Reference: [W87] <author> Wolfe, M., </author> <title> "Iteration Space Tiling for Memory Hierarchies," Parallel Processing for Scientific Computing, </title> <editor> G. Rodrigue (Ed), </editor> <booktitle> SIAM 1987, </booktitle> <pages> pp. 357-361. </pages>
Reference: [W89] <author> Wolfe, M., </author> <title> "More Iteration Space Tiling ," Proceedings of Supercomputing, </title> <month> November, </month> <year> 1989, </year> <pages> pp. 655-664. 23 </pages>
References-found: 26

