URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3266/3266.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: (dybbuk@cs.umd.edu) (ravi@npac.syr.edu)  
Title: Run-time and Compile-time Support for Adaptive Irregular Problems  
Author: Shamik D. Sharma Ravi Ponnusamy yz Bongki Moon Yuan-Shin Hwang Raja Das Joel Saltz 
Address: College Park, MD 20742 Syracuse, NY 13244  
Affiliation: UMIACS and Computer Science Dept. Northeast Parallel Architectures Center University of Maryland Syracuse University  
Web: URL ftp://hpsl.cs.umd.edu/pub/papers/sc94-tr.ps.Z.  
Note: CS-TR-3266 and UMIACS-TR-94-55. Also appears in Supercomputing'94. Available at  This work was sponsored in part by ARPA (NAG-1-1485), NSF (ASC 9213821), ONR (SC292-1-22913) and EPRI (RP3103-6).  
Abstract: In adaptive irregular problems the data arrays are accessed via indirection arrays, and data access patterns change during computation. Implementing such problems on distributed memory machines requires support for dynamic data partitioning, efficient preprocessing and fast data migration. This research presents efficient runtime primitives for such problems. This new set of primitives is part of the CHAOS library. It subsumes the previous PARTI library which targeted only static irregular problems. To demonstrate the efficacy of the runtime support, two real adaptive irregular applications have been parallelized using CHAOS primitives: a molecular dynamics code (CHARMM) and a particle-in-cell code (DSMC). The paper also proposes extensions to Fortran D which can allow compilers to generate more efficient code for adaptive problems. These language extensions have been implemented in the Syracuse Fortran 90D/HPF prototype compiler. The performance of the compiler parallelized codes is compared with the hand parallelized versions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Baden. </author> <title> Programming abstractions for dynamically partitioning and coordinating localized scientific calculations running on multiprocessors. </title> <journal> SIAM J. Sci. and Stat. Computation., </journal> <volume> 12(1), </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: Williams [27] describes a programming environment (DIME) for calculations with unstructured triangular meshes using distributed memory machines. Baden <ref> [1] </ref> has developed a programming environment targeting particle computations. This programming environment provides facilities that support dynamic load balancing.
Reference: [2] <author> M.J. Berger and S. H. Bokhari. </author> <title> A partitioning strategy for nonuniform problems on multiprocessors. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(5):570-580, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: As molecules move across cells, the computational load balance deteriorates over time. Performance can be substantially improved by periodically redistributing the cells with the help of CHAOS's parallel partition-ers such as recursive coordinate bisection (RCB) <ref> [2] </ref> and recursive inertial bisection (RIB) [21]. While these partitioners are parallelized, they are still expensive and are affordable only when the load imbalance becomes too severe. <p> Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [25, 27, 2] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution. Fortran D provides the user with a choice of several standard distributions.
Reference: [3] <author> Graeme A. Bird. </author> <title> Molecular Gas Dynamics and the Direct Simulation of Gas Flows. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1994. </year>
Reference-contexts: Preprocessing methods are being developed for a variety of unstructured problems including explicit multi-grid unstructured computational fluid dynamic solvers [18, 11], molecular dynamics codes (CHARMM, AMBER, GROMOS, etc.) [5], diagonal or polynomial preconditioned iterative linear solvers [26], and particle-in-cell (PIC) codes <ref> [3] </ref>. These problems share the characteristics of (1) arrays accessed through one or more levels of indirection, and (2) formulation of the problem as a sequence of loop nests each of which prove to be parallelizable. ia and ib, which are known only at runtime.
Reference: [4] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, S. Ranka, and M.-Y. Wu. </author> <title> Compiling Fortran 90D/HPF for distributed memory MIMD computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 15-26, </pages> <month> April </month> <year> 1994. </year> <month> 24 </month>
Reference-contexts: The runtime support has been incorporated in the Fortran 90D compiler that is being developed at Syracuse University <ref> [4] </ref>. The Fortran 90D compiler transformations generate translated codes which embed calls to CHAOS procedures. The performance of the compiler generated code is compared with that of the hand parallelized versions. <p> Baden [1] has developed a programming environment targeting particle computations. This programming environment provides facilities that support dynamic load balancing. There are a variety of compiler projects targeting at distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse <ref> [9, 4] </ref> and the Vienna Fortran compiler project [7] at the University of Vienna, among others. The Jade project at Stanford [16], the DINO project at Colorado [23], and the CODE project at UT, Austin, provide parallel programming environments.
Reference: [5] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus. Charmm: </author> <title> A program for macromolecular energy, minimization, and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4:187, </volume> <year> 1983. </year>
Reference-contexts: These preprocessing strategies primarily deal with reducing data movement between processor memories. Preprocessing methods are being developed for a variety of unstructured problems including explicit multi-grid unstructured computational fluid dynamic solvers [18, 11], molecular dynamics codes (CHARMM, AMBER, GROMOS, etc.) <ref> [5] </ref>, diagonal or polynomial preconditioned iterative linear solvers [26], and particle-in-cell (PIC) codes [3].
Reference: [6] <author> B. R. Brooks and M. Hodoscek. </author> <title> Parallelization of charmm for mimd machines. Chemical Design Automation News, </title> <address> 7:16, </address> <year> 1992. </year>
Reference-contexts: Overheads of Preprocessing Data and iteration partitioning, remapping, and loop preprocessing must be done at runtime. Preprocessing overheads of the simulation are shown in Table 2. The data partition time is the 1 Estimation done by Brooks and Hodoscek <ref> [6] </ref> 13 Table 1: Performance of Parallel CHARMM on Intel iPSC/860 (in sec.) Number of Processors 1 16 32 64 128 Execution Time 74595.5 1 4356.0 2293.8 1261.4 781.8 Computation Time 74595.5 4099.4 2026.8 1011.2 507.6 Communication Time 0.0 147.1 159.8 181.1 219.2 Load Balance Index 1.00 1.03 1.05 1.06 1.08
Reference: [7] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: At high levels of parallelism the costs of performing the partitioning dominate over the gains in load balance. The chain partitioner, however, provided the better results for this problem. 16 5 Compiling Adaptive Irregular Problems There are a wide range of languages such as Vienna Fortran <ref> [7] </ref>, pC++ [10], Fortran-D [9] and High Performance Fortran (HPF) [13], which provide a rich set of directives allowing users to specify desired data decompositions. With these decomposition directives, compilers can partition loop iterations and generate communication required to parallelize programs. <p> Baden [1] has developed a programming environment targeting particle computations. This programming environment provides facilities that support dynamic load balancing. There are a variety of compiler projects targeting at distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse [9, 4] and the Vienna Fortran compiler project <ref> [7] </ref> at the University of Vienna, among others. The Jade project at Stanford [16], the DINO project at Colorado [23], and the CODE project at UT, Austin, provide parallel programming environments. The Split-C project [15] at Berkeley is targeted towards providing a parallel programming environment on distributed memory machines.
Reference: [8] <author> Raja Das, Mustafa Uysal, Joel Saltz, and Yuan-Shin Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <institution> Technical Report CS-TR-3163 and UMIACS-TR-93-109, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> October </month> <year> 1993. </year> <note> Submitted to Journal of Parallel and Distributed Computing. </note>
Reference-contexts: The number of messages can also be reduced by pre-fetching large quantities of off-processor data in a single message. These optimizations are called software caching and communication vectorization respectively. Such optimizations have been successfully used to parallelize static irregular problems <ref> [8] </ref>, in which array access patterns do not change during computation. For the static irregular problems considered in Das et al. [8], it is enough to perform preprocessing only once to optimize communication. Adaptive irregular problems are more complex. <p> These optimizations are called software caching and communication vectorization respectively. Such optimizations have been successfully used to parallelize static irregular problems <ref> [8] </ref>, in which array access patterns do not change during computation. For the static irregular problems considered in Das et al. [8], it is enough to perform preprocessing only once to optimize communication. Adaptive irregular problems are more complex. In these problems, the data access patterns may change during computation, resulting in complex preprocessing requirements. Consider, for instance, adaptive fluid dynamics and molecular dynamics codes. <p> First, a brief overview of the runtime support is presented; the framework is same as that of PARTI, and has been described in earlier papers <ref> [8, 22] </ref>. We then focus on the inspector, which is a preprocessing stage that must be repeated frequently in adaptive problems. <p> Index translation involves converting the global array indices in indirection arrays into local indices. The purpose of index translation has been discussed in greater detail elsewhere <ref> [8] </ref>. Communication schedule generation involves analyzing data access patterns and performing optimizations such as software caching and communication vectorization. In adaptive problems, data access patterns are modified frequently; hence index translation and schedule regeneration are repeated many times. <p> Data movements occur frequently in adaptive problems, hence it is important to optimize them. Generally, it is possible with existing compiler techniques to compile irregular loops where data access patterns are known only at runtime due to indirections <ref> [8, 22] </ref>. The compiler generates a pre-processing code for such a loop that, at runtime, generates appropriate communication calls and places off-processor data in a pre-determined order. However, this technique does not detect reductions.
Reference: [9] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <institution> Department of Computer Science Rice COMP TR90-141, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: The chain partitioner, however, provided the better results for this problem. 16 5 Compiling Adaptive Irregular Problems There are a wide range of languages such as Vienna Fortran [7], pC++ [10], Fortran-D <ref> [9] </ref> and High Performance Fortran (HPF) [13], which provide a rich set of directives allowing users to specify desired data decompositions. With these decomposition directives, compilers can partition loop iterations and generate communication required to parallelize programs. <p> Baden [1] has developed a programming environment targeting particle computations. This programming environment provides facilities that support dynamic load balancing. There are a variety of compiler projects targeting at distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse <ref> [9, 4] </ref> and the Vienna Fortran compiler project [7] at the University of Vienna, among others. The Jade project at Stanford [16], the DINO project at Colorado [23], and the CODE project at UT, Austin, provide parallel programming environments.
Reference: [10] <author> Dennis Gannon, Shelby Yang, and Peter Beckman. </author> <title> User Guide for a Portable Parallel C++ Programming System, pC++. </title> <institution> Department of Computer Science and CICA, Indiana University, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: At high levels of parallelism the costs of performing the partitioning dominate over the gains in load balance. The chain partitioner, however, provided the better results for this problem. 16 5 Compiling Adaptive Irregular Problems There are a wide range of languages such as Vienna Fortran [7], pC++ <ref> [10] </ref>, Fortran-D [9] and High Performance Fortran (HPF) [13], which provide a rich set of directives allowing users to specify desired data decompositions. With these decomposition directives, compilers can partition loop iterations and generate communication required to parallelize programs.
Reference: [11] <author> S. Hammond and T. Barth. </author> <title> An optimal massively parallel Euler solver for unstructured grids. </title> <journal> AIAA Journal, </journal> <note> AIAA Paper 91-0441, </note> <month> January </month> <year> 1991. </year>
Reference-contexts: These preprocessing strategies primarily deal with reducing data movement between processor memories. Preprocessing methods are being developed for a variety of unstructured problems including explicit multi-grid unstructured computational fluid dynamic solvers <ref> [18, 11] </ref>, molecular dynamics codes (CHARMM, AMBER, GROMOS, etc.) [5], diagonal or polynomial preconditioned iterative linear solvers [26], and particle-in-cell (PIC) codes [3].
Reference: [12] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. In Compilers and Runtime Software for Scalable Multiprocessors, </title> <editor> J. Saltz and P. Mehrotra Editors, </editor> <address> Amsterdam, The Netherlands, </address> <note> To appear 1991. Elsevier. </note>
Reference-contexts: The Split-C project [15] at Berkeley is targeted towards providing a parallel programming environment on distributed memory machines. Runtime compilation methods have been employed in four compiler projects: the Fortran D project <ref> [12] </ref>, the Kali project [14], Marina Chen's work at Yale [17] and the PARTI project [19, 24].
Reference: [13] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The chain partitioner, however, provided the better results for this problem. 16 5 Compiling Adaptive Irregular Problems There are a wide range of languages such as Vienna Fortran [7], pC++ [10], Fortran-D [9] and High Performance Fortran (HPF) <ref> [13] </ref>, which provide a rich set of directives allowing users to specify desired data decompositions. With these decomposition directives, compilers can partition loop iterations and generate communication required to parallelize programs. This paper presents language features required to support adaptive problems within the Fortran D framework. <p> Another example would be to assign consecutively indexed array elements to processors in a round-robin fashion. These two data distribution schemes are often called BLOCK and CYCLIC data distributions <ref> [13] </ref>, respectively. 5.1.1 Irregular Distribution On distributed memory machines, irregular concurrent problems may not run efficiently with standard data distributions such as BLOCK and CYCLIC [25]. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements [25, 27, 2].
Reference: [14] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory architectures. </title> <booktitle> In 2nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 177-186. </pages> <publisher> ACM, </publisher> <month> March </month> <year> 1990. </year>
Reference-contexts: The Split-C project [15] at Berkeley is targeted towards providing a parallel programming environment on distributed memory machines. Runtime compilation methods have been employed in four compiler projects: the Fortran D project [12], the Kali project <ref> [14] </ref>, Marina Chen's work at Yale [17] and the PARTI project [19, 24]. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [14] and the ARF compiler was the first compiler to support irregularly distributed arrays [28]. 7 Conclusions The CHAOS procedures described in this paper can <p> Runtime compilation methods have been employed in four compiler projects: the Fortran D project [12], the Kali project <ref> [14] </ref>, Marina Chen's work at Yale [17] and the PARTI project [19, 24]. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [14] and the ARF compiler was the first compiler to support irregularly distributed arrays [28]. 7 Conclusions The CHAOS procedures described in this paper can be viewed as forming part of a portable, compiler independent, runtime support library.
Reference: [15] <author> A. Krishnamurthy, D.E. Culler, A. Dusseau, S.C. Goldstein, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 262-273. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: The Jade project at Stanford [16], the DINO project at Colorado [23], and the CODE project at UT, Austin, provide parallel programming environments. The Split-C project <ref> [15] </ref> at Berkeley is targeted towards providing a parallel programming environment on distributed memory machines. Runtime compilation methods have been employed in four compiler projects: the Fortran D project [12], the Kali project [14], Marina Chen's work at Yale [17] and the PARTI project [19, 24].
Reference: [16] <author> Monica Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The cache performance and optimizations of block algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 63-74. </pages> <publisher> ACM Press, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: There are a variety of compiler projects targeting at distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse [9, 4] and the Vienna Fortran compiler project [7] at the University of Vienna, among others. The Jade project at Stanford <ref> [16] </ref>, the DINO project at Colorado [23], and the CODE project at UT, Austin, provide parallel programming environments. The Split-C project [15] at Berkeley is targeted towards providing a parallel programming environment on distributed memory machines.
Reference: [17] <author> L. C. Lu and M.C. Chen. </author> <title> Parallelizing loops with indirect array references or pointers. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: The Split-C project [15] at Berkeley is targeted towards providing a parallel programming environment on distributed memory machines. Runtime compilation methods have been employed in four compiler projects: the Fortran D project [12], the Kali project [14], Marina Chen's work at Yale <ref> [17] </ref> and the PARTI project [19, 24].
Reference: [18] <author> D. J. Mavriplis. </author> <title> Three dimensional unstructured multigrid for the Euler equations, paper 91-1549cp. </title> <booktitle> In AIAA 10th Computational Fluid Dynamics Conference, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: These preprocessing strategies primarily deal with reducing data movement between processor memories. Preprocessing methods are being developed for a variety of unstructured problems including explicit multi-grid unstructured computational fluid dynamic solvers <ref> [18, 11] </ref>, molecular dynamics codes (CHARMM, AMBER, GROMOS, etc.) [5], diagonal or polynomial preconditioned iterative linear solvers [26], and particle-in-cell (PIC) codes [3].
Reference: [19] <author> R. Mirchandaney, J. H. Saltz, R. M. Smith, D. M. Nicol, and Kay Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the 1988 ACM International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: MOVE procedure in DSMC is much more complex than the code fragment shown in compiler implementation, we have used a computational template similar to the one shown here. 3 Runtime Support This section describes the principles and functionality of the CHAOS runtime support library, a superset of the PARTI library <ref> [19, 28, 24] </ref>. First, a brief overview of the runtime support is presented; the framework is same as that of PARTI, and has been described in earlier papers [8, 22]. We then focus on the inspector, which is a preprocessing stage that must be repeated frequently in adaptive problems. <p> The Split-C project [15] at Berkeley is targeted towards providing a parallel programming environment on distributed memory machines. Runtime compilation methods have been employed in four compiler projects: the Fortran D project [12], the Kali project [14], Marina Chen's work at Yale [17] and the PARTI project <ref> [19, 24] </ref>.
Reference: [20] <author> David M. Nicol and David R. O'Hallaron. </author> <title> Improved algorithms for mapping pipelined and parallel computations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(3) </volume> <pages> 295-306, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: While these partitioners are parallelized, they are still expensive and are affordable only when the load imbalance becomes too severe. CHAOS also provides a fast one dimensional partitioner, called the chain partitioner <ref> [20] </ref>, which takes advantage of the highly directional nature of particle flow that characterizes many DSMC communication patterns. For instance, in the experiments reported here, more than 70 percent of the molecules were found moving along the positive x-axis.
Reference: [21] <author> B. Nour-Omid, A. Raefsky, and G. Lyzenga. </author> <title> Solving finite element equations on concurrent computers. </title> <booktitle> In Proc. of Symposium on Parallel Computations and theis Impact on Mechanics, </booktitle> <address> Boston, </address> <month> December </month> <year> 1987. </year>
Reference-contexts: As molecules move across cells, the computational load balance deteriorates over time. Performance can be substantially improved by periodically redistributing the cells with the help of CHAOS's parallel partition-ers such as recursive coordinate bisection (RCB) [2] and recursive inertial bisection (RIB) <ref> [21] </ref>. While these partitioners are parallelized, they are still expensive and are affordable only when the load imbalance becomes too severe.
Reference: [22] <author> Ravi Ponnusamy, Joel Saltz, Alok Choudhary, Yuan-Shin Hwang, and Geoffrey Fox. </author> <title> Runtime support and compilation methods for user-specified irregular data distributions. </title> <institution> Technical Report CS-TR-3194 and UMIACS-TR-93-135, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> November </month> <year> 1993. </year> <journal> Appears in IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 6, No. </volume> <pages> 8. </pages>
Reference-contexts: First, a brief overview of the runtime support is presented; the framework is same as that of PARTI, and has been described in earlier papers <ref> [8, 22] </ref>. We then focus on the inspector, which is a preprocessing stage that must be repeated frequently in adaptive problems. <p> Data movements occur frequently in adaptive problems, hence it is important to optimize them. Generally, it is possible with existing compiler techniques to compile irregular loops where data access patterns are known only at runtime due to indirections <ref> [8, 22] </ref>. The compiler generates a pre-processing code for such a loop that, at runtime, generates appropriate communication calls and places off-processor data in a pre-determined order. However, this technique does not detect reductions. <p> Therefore, it is important that the compiler-generated code be able to detect when preprocessing can be reused. An implementation of reusing results of pre-processing in compiler-generated code is described in Ponnusamy et al. <ref> [22] </ref>. In this approach, the compiler-generated code maintains a record of when statements or array intrinsics of loops may have modified indirection arrays.
Reference: [23] <author> Matthew Rosing, Robert B. Schnabel, and Robert P. Weaver. </author> <title> The DINO parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 30-42, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: There are a variety of compiler projects targeting at distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse [9, 4] and the Vienna Fortran compiler project [7] at the University of Vienna, among others. The Jade project at Stanford [16], the DINO project at Colorado <ref> [23] </ref>, and the CODE project at UT, Austin, provide parallel programming environments. The Split-C project [15] at Berkeley is targeted towards providing a parallel programming environment on distributed memory machines.
Reference: [24] <author> Joel Saltz, Harry Berryman, and Janet Wu. </author> <title> Multiprocessors and run-time compilation. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(6) </volume> <pages> 573-592, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: This paper presents a new set of runtime procedures designed to efficiently implement adaptive programs on distributed memory machines. This runtime library is called CHAOS; it subsumes PARTI, a library aimed at static irregular problems <ref> [24] </ref>. CHAOS introduces two new features | light-weight schedules and efficient schedule generation , which are useful in certain types of adaptive problems. We describe these features in Section 3. <p> MOVE procedure in DSMC is much more complex than the code fragment shown in compiler implementation, we have used a computational template similar to the one shown here. 3 Runtime Support This section describes the principles and functionality of the CHAOS runtime support library, a superset of the PARTI library <ref> [19, 28, 24] </ref>. First, a brief overview of the runtime support is presented; the framework is same as that of PARTI, and has been described in earlier papers [8, 22]. We then focus on the inspector, which is a preprocessing stage that must be repeated frequently in adaptive problems. <p> The Split-C project [15] at Berkeley is targeted towards providing a parallel programming environment on distributed memory machines. Runtime compilation methods have been employed in four compiler projects: the Fortran D project [12], the Kali project [14], Marina Chen's work at Yale [17] and the PARTI project <ref> [19, 24] </ref>.
Reference: [25] <author> H. Simon. </author> <title> Partitioning of unstructured mesh problems for parallel processing. </title> <booktitle> In Proceedings of the Conference on Parallel Methods on Large Scale Structural Analysis and Physics Applications. </booktitle> <publisher> Pergamon Press, </publisher> <year> 1991. </year>
Reference-contexts: These two data distribution schemes are often called BLOCK and CYCLIC data distributions [13], respectively. 5.1.1 Irregular Distribution On distributed memory machines, irregular concurrent problems may not run efficiently with standard data distributions such as BLOCK and CYCLIC <ref> [25] </ref>. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements [25, 27, 2]. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. <p> Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [25, 27, 2] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution. Fortran D provides the user with a choice of several standard distributions.
Reference: [26] <author> P. Venkatkrishnan, J. Saltz, and D. Mavriplis. </author> <title> Parallel preconditioned iterative methods for the compressible navier stokes equations. </title> <booktitle> In 12th International Conference on Numerical Methods in Fluid Dynamics, </booktitle> <address> Oxford, England, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: These preprocessing strategies primarily deal with reducing data movement between processor memories. Preprocessing methods are being developed for a variety of unstructured problems including explicit multi-grid unstructured computational fluid dynamic solvers [18, 11], molecular dynamics codes (CHARMM, AMBER, GROMOS, etc.) [5], diagonal or polynomial preconditioned iterative linear solvers <ref> [26] </ref>, and particle-in-cell (PIC) codes [3]. These problems share the characteristics of (1) arrays accessed through one or more levels of indirection, and (2) formulation of the problem as a sequence of loop nests each of which prove to be parallelizable. ia and ib, which are known only at runtime.
Reference: [27] <author> R. Williams. </author> <title> Performance of dynamic load balancing algorithms for unstructured mesh calculations. </title> <journal> Concurrency, Practice and Experience, </journal> <volume> 3(5) </volume> <pages> 457-482, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [25, 27, 2] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution. Fortran D provides the user with a choice of several standard distributions. <p> Hence, the compiler-generated code performs extra communication. (This communication is done by invoking CHAOS procedures.) 6 Related Work Several researchers have developed programming environments that target particular classes of irregular or adaptive problems. Williams <ref> [27] </ref> describes a programming environment (DIME) for calculations with unstructured triangular meshes using distributed memory machines. Baden [1] has developed a programming environment targeting particle computations. This programming environment provides facilities that support dynamic load balancing.
Reference: [28] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 26-30, </pages> <year> 1991. </year>
Reference-contexts: MOVE procedure in DSMC is much more complex than the code fragment shown in compiler implementation, we have used a computational template similar to the one shown here. 3 Runtime Support This section describes the principles and functionality of the CHAOS runtime support library, a superset of the PARTI library <ref> [19, 28, 24] </ref>. First, a brief overview of the runtime support is presented; the framework is same as that of PARTI, and has been described in earlier papers [8, 22]. We then focus on the inspector, which is a preprocessing stage that must be repeated frequently in adaptive problems. <p> The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [14] and the ARF compiler was the first compiler to support irregularly distributed arrays <ref> [28] </ref>. 7 Conclusions The CHAOS procedures described in this paper can be viewed as forming part of a portable, compiler independent, runtime support library.
References-found: 28

