URL: ftp://ftp.cs.columbia.edu/reports/reports-1997/cucs-011-97.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1997.html
Root-URL: http://www.cs.columbia.edu
Email: Email: simonb@cs.columbia.edu  
Title: On the Parameter Estimation Accuracy of Model-Matching Feature Detectors  
Author: Simon Baker 
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Pubnum: (Technical Report CUCS-011-97)  
Abstract-found: 0
Intro-found: 1
Reference: [Abdou and Pratt, 79] <author> I.E. Abdou and W.K. Pratt, </author> <title> "Quantitative Design and Evaluation of Enhancement/Thresholding Edge Detectors," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 67 </volume> <pages> 753-763, </pages> <year> 1979. </year>
Reference-contexts: 1 Introduction Many computer vision algorithms rely upon some form of local feature detection. Moreover, high performance feature detection is nearly always vital to subsequent processing. Although there are several different aspects to the performance of a feature detector <ref> [Abdou and Pratt, 79] </ref>, here we will just consider one of them, that being parameter estimation accuracy. We also only consider a certain type of feature detector. <p> In Cartesian coordinates this corresponds to the weighting function w (x; y) = 1=(x 2 +y 2 ) 1=2 . A final example is <ref> [Abdou and Pratt, 79] </ref> in which it is mentioned that weighting pixels so as to reduce the influence of pixels which are distant from the center of the window improves Pratt's Figure of Merit [Pratt, 90], but few details of this are actually given.
Reference: [Abramatic, 81] <author> J.F. Abramatic, </author> <title> "Why the Simplest `Hueckel' Edge Detector Is a Roberts Operator," </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 17 </volume> <pages> 79-83, </pages> <year> 1981. </year>
Reference-contexts: We note that there has already been some previous work attempting to unify different approaches to edge detection including <ref> [Abramatic, 81] </ref> and [Rosenfeld, 81].
Reference: [Baker et al., 98] <author> S. Baker, S.K. Nayar, and H. Murase, </author> <title> "Parametric Feature Detection," </title> <journal> International Journal of Computer Vision, </journal> <note> to appear in 1998. </note>
Reference-contexts: As far as we can tell, the selection of the fitting function has never before been studied in a systematic manner. In fact, most detectors simply use the Euclidean L 2 norm. Examples include [Hummel, 79], [Rohr, 92], and <ref> [Baker et al., 98] </ref>. The remaining detectors mostly use weighted L 2 norms, but in all cases the weighting function was chosen in an ad-hoc manner. See for example [Hueckel, 71], [Hueckel, 73], and [Hartley, 85]. <p> In Section 2 we introduce the model-fitting approach to feature detection and describe how weighted L 2 norms may be used to measure the degree of fit between the model and image data. We also discuss how the parameter normalization and dimension reduction techniques of <ref> [Baker et al., 98] </ref> can be extended to cope with the weighted L 2 norm case. In Section 3 we begin by deriving our optimality criterion and discussing a couple of simplifying assumptions. <p> Finally, we analyze the effect that the use of a weighted L 2 norm has on the efficiency enhancing techniques of parameter normalization and dimension reduction <ref> [Baker et al., 98] </ref>. 2.1 Parametric Model Fitting The approach we take to feature detection is to fit a parametric model to the image data. <p> See <ref> [Baker et al., 98] </ref> for a description of how this is performed. <p> (17) The designer of a particular feature detector may wish to go even further and combine PM i for i = 1; 2; : : : ; k together with other measures of feature detection performance (for example estimates of the rates of occurrence of false positives and false negatives <ref> [Baker et al., 98] </ref>) to yield a single overall performance metric. Analyzing such a measure analytically would probably be impossible and so optimization would have to be performed numerically. <p> The parametric model of the step edge is presented in Section 2.1. The corresponding models for the line and the corner are the same as those used in <ref> [Baker et al., 98] </ref>. The reader is referred to that paper for a full description of the models. We begin this section by briefly enumerating the weighting functions which we studied. <p> The main cause of errors in estimating the normalized parameters A and B is inaccurate estimates of the other parameters, particularly and but to a lesser extent also. This is because the other 3 parameters are inputs to the procedure to invert the normalization. (See <ref> [Baker et al., 98] </ref> for more details.) In the figure, we see that the results are similar to those for the localization parameter and the blur parameter. For high noise levels all weighting functions perform similarly. Presumably, the performance is limited by the estimate of the blur parameter. <p> The Euclidean L 2 norm performs relatively poorly for all the parameters with the exception of the orientation parameter . 4.3 Corner The Corner model (see <ref> [Baker et al., 98] </ref>) has 5 parameters: the orientation of the corner 1 , the angle subtended by the corner 2 , the blur , the lower intensity level A, and the intensity step B. <p> The Reflected Lenz and Fourth Root weighting functions do a little bit better than Paton's annular stop. 4.4 Line The line model (see <ref> [Baker et al., 98] </ref>) has 6 parameters: the orientation of the line , the localization of the line , the blur parameter , the width of the line w, the base intensity A; and the intensity step B. <p> A Appendix A.1 Experimental Procedure At a number of points we have presented experimental results comparing the parameter estimation performance of several weighted L 2 norms. We now briefly describe the experimental procedure used. This procedure is almost identical to that of <ref> [Baker et al., 98] </ref> and also that of [Nalwa and Binford, 86]. The reader is referred to these papers for a more complete discussion of it. <p> We used the detector proposed in <ref> [Baker et al., 98] </ref> extended to deal with weighted L 2 norms as described in Section 2. The procedure is then as follows: 35 Computing R.M.S. <p> This noise is added to the raw feature F (n; m; q) rather than the normalized one F (n; m; q). Since the feature detection algorithm of <ref> [Baker et al., 98] </ref> works with normalized features we will now analyze the properties of the noise as it affects the normalized feature F (n; m; q). This normalized noise is denoted (n; m; q). <p> w nm 4 F (n; m; q) P X w pq F (p; q; q) 5 3 5 (50) and then projects it into an N 2 dimensional subspace of R N . 4 This is derived from the definition of signal to noise ratio for arbitrary features proposed in <ref> [Baker et al., 98] </ref>: snr = 2fi noise where 2 noise is the variance of the noise. 36 Lemma A.2.1 Suppose that noise (n; m; q) is added to the feature instance F (n; m; q) to give F 0 (n; m; q) = F (n; m; q) + (n; m;
Reference: [Boie et al., 86] <author> R.A. Boie, I.J. Cox, and P. Rehak, </author> <title> "On Optimum Edge Recognition using Matched Filters," </title> <booktitle> in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 100-108, </pages> <year> 1986. </year>
Reference-contexts: The Square Root Ramp function and the Modified Lenz function do slightly better than the Euclidean L 2 norm for low noise levels, and slightly worse for high noise levels. in [Modestino and Fries, 77], [Shanmugam et al., 79], <ref> [Boie et al., 86] </ref>, [Deriche, 87], and [Sarkar and Boyer, 91]. In doing so, we will attempt to characterize the most important similarities and differences between the two major approaches to feature detection as categorized in [Nalwa, 93]: parametric model matching and using differential invariants. <p> We finally note that selecting a uniform weighting function corresponds to choosing a matched-filter. It is then not surprising that the Eu-clidean L 2 norm is optimal for linear manifolds since matched filters are optimal for certain performance measures [Canny, 86] <ref> [Boie et al., 86] </ref>. 5.1.2 Differences The first major difference between the two approaches is the domain over which the maximization is performed.
Reference: [Canny, 86] <author> J. Canny, </author> <title> "A Computational Approach to Edge Detection," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8 </volume> <pages> 679-698, </pages> <year> 1986. </year>
Reference-contexts: In Section 4 we present the results of the empirical comparison of various weighting functions for the step edge, corner, and line. Finally we conclude in Section 5 with a discussion of the relationship between our work and the optimal-filtering approach to edge detection best exemplified by <ref> [Canny, 86] </ref>. We also describe several open problems and opportunities for future work. 4 regions of brightness A and A + B. <p> weighting functions which give the most weight to the pixels midway between the center and the edge of the window. 5 Discussion 5.1 Relationship with Optimal Filtering We now present a brief discussion of the relationship between our work and the optimal filtering approach to edge detection best exemplified by <ref> [Canny, 86] </ref>, but also studied 30 is the Gaussian weighting function. <p> In <ref> [Canny, 86] </ref>, Canny investigates the selection of the filter f c (x) to optimize the performance of a 1-D edge detector which declares edges at local maxima of: Z +W I c (a + x) f c (x) dx (47) where I c (x) is the continuous 1-D input image (signal) <p> If we translate the 1-D continuous setting of <ref> [Canny, 86] </ref> into the 2-D discretized setting which we used in this paper, the integral over the continuous 1-D window [W; W ] becomes a sum over the discrete 2-D window S, and so the expression in Equation (47) becomes: max X f (n + cos ; m + sin ; <p> Taking the maximum over the localization parameter has been introduced to represent the operation of taking a local maxima and taking the maximum over the orientation parameter to denote the fact we have to consider edges in all possible directions. In <ref> [Canny, 86] </ref>, the angle which maximizes the expression is taken to be the direction of the gradient, and then the local maxima is computed by 32 finding the zero crossings of a second-order directional derivative in the direction of the gradient. <p> Then, from Equation (48) we see that in a discrete 2-D setting the equivalent of <ref> [Canny, 86] </ref> would be to study the selection of f (x; y; 0 o ) to optimize a measure of the performance of edge detection. <p> We finally note that selecting a uniform weighting function corresponds to choosing a matched-filter. It is then not surprising that the Eu-clidean L 2 norm is optimal for linear manifolds since matched filters are optimal for certain performance measures <ref> [Canny, 86] </ref> [Boie et al., 86]. 5.1.2 Differences The first major difference between the two approaches is the domain over which the maximization is performed. <p> So long as the feature F (n; m; q) has a localization parameter 2 q and an orientation parameter 2 q, the formulation in this paper can be regarded as a generalization of that in <ref> [Canny, 86] </ref> because there is an implicit spatial local maximization and a implicit maximization over orientation performed when we find the closest manifold point. However, most of the features which we consider have additional parameters. Another difference is that in [Canny, 86] the formulation is 1-D and the extension to 2-D <p> paper can be regarded as a generalization of that in <ref> [Canny, 86] </ref> because there is an implicit spatial local maximization and a implicit maximization over orientation performed when we find the closest manifold point. However, most of the features which we consider have additional parameters. Another difference is that in [Canny, 86] the formulation is 1-D and the extension to 2-D by rotating the filter in the direction of the gradient is somewhat ad-hoc. A better way to extend from 1-D to 2-D is possible when the filter f c (x; y; ) is steerable [Freeman and Adelson, 91]. <p> This function may then be maximized by taking the derivative with respect to . It so happens that the gradient of the (suboptimal) Gaussian filter eventually used in <ref> [Canny, 86] </ref> is steerable [Freeman and Adelson, 91] and the maximum response can be shown to be in the direction of the gradient. The other (optimal) filters which Canny derives are not, in general, steerable. <p> The other (optimal) filters which Canny derives are not, in general, steerable. An important parameter omitted from the maximization in Equation (48) is the blurring or scale parameter which is assumed to be fixed in <ref> [Canny, 86] </ref>. It is suggested there that the Canny edge detector be applied for a number of different values of the blurring (scale) parameter. It is outside the scope of this paper to discuss the merits of 33 such an approach. <p> A simple example is the signal-to-noise ratio, which is a measure often used in the design of optimal-filtering feature detectors <ref> [Canny, 86] </ref>. It is straightforward to derive the weighting function which maximizes the signal-to-noise ratio, but unfortunately it turns out to be degenerate. The optimal weighting function always assigns a weight of 1 to exactly one of the pixels and 0 to the rest.
Reference: [Conway, 85] <author> J.B. Conway, </author> <title> A Course in Functional Analysis, </title> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: class (see Figure 1), and (3) every weighted L 2 norm can be specified by its corresponding weighting function thereby facilitating any optimization process. 2.2 Weighted L 2 Norms Every measure on the set of pixels S defines a different L 2 norm and which is denoted L 2 () <ref> [Conway, 85] </ref> [Halmos, 74]. A measure is defined by the weight (n; m) = w nm 0 that it assigns to each of the pixels (n; m) 2 S. <p> norm is easier than computing the L 2 norm itself, and since the square root function is monotonic, we base feature detection upon the 2 For this to really be a norm, as opposed to a semi-norm, we strictly require w nm &gt; 0 for all (n; m) 2 S <ref> [Conway, 85] </ref>. Similarly, we also require w nm &gt; 0 for Equation (8) to define an inner product rather than a semi-inner product. <p> The d eigenvectors of C with the largest eigenvalues are used as a basis for the reduced dimensional space. Since C is self-adjoint with respect to the inner product (even though it is not symmetric), the eigenvectors will automatically be orthogonal (at least for distinct eigenvalues) <ref> [Conway, 85] </ref>, but orthonormalization is advised since for many symmetric features duplicate eigenvalues are commonplace. 3 Optimal Weighting Functions We begin this section by deriving our optimality criterion. Then, after describing a couple of simplifying assumptions, we finally derive optimal weighting functions assuming that the manifold is linear.
Reference: [Deriche, 87] <author> R. Deriche, </author> <title> "Using Canny's Criteria To Derive A Recursively Implemented Optimal Edge Detector," </title> <journal> International Journal of Computer Vision, </journal> <volume> 1 </volume> <pages> 167-187, </pages> <year> 1987. </year>
Reference-contexts: The Square Root Ramp function and the Modified Lenz function do slightly better than the Euclidean L 2 norm for low noise levels, and slightly worse for high noise levels. in [Modestino and Fries, 77], [Shanmugam et al., 79], [Boie et al., 86], <ref> [Deriche, 87] </ref>, and [Sarkar and Boyer, 91]. In doing so, we will attempt to characterize the most important similarities and differences between the two major approaches to feature detection as categorized in [Nalwa, 93]: parametric model matching and using differential invariants.
Reference: [Freeman and Adelson, 91] <author> W.T. Freeman and E.H. Adelson, </author> <title> "The Design and Use of Steerable Filters," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13 </volume> <pages> 891-906, </pages> <year> 1991. </year>
Reference-contexts: A better way to extend from 1-D to 2-D is possible when the filter f c (x; y; ) is steerable <ref> [Freeman and Adelson, 91] </ref>. In this case, the maximum over can be computed by first correlating with, say, f (n; m; 0 o ) and f (n; m; 90 o ); and then deducing a formula for the correlation with f (n; m; ) as a function of . <p> This function may then be maximized by taking the derivative with respect to . It so happens that the gradient of the (suboptimal) Gaussian filter eventually used in [Canny, 86] is steerable <ref> [Freeman and Adelson, 91] </ref> and the maximum response can be shown to be in the direction of the gradient. The other (optimal) filters which Canny derives are not, in general, steerable.
Reference: [Fukunaga, 90] <author> K. Fukunaga, </author> <title> Introduction to Statistical Pattern Recognition, </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: If we model the noise with the additive random vector , we have: I (a + n; b + m) = F (n; m; q) + (n; m; q) (14) 3 The covariance matrix enters the unweighted minimum mean-square error formulation of the Karhunen-Loeve expansion in <ref> [Fukunaga, 90] </ref> as an instantiation of the linear operator y ! E X [(X E X [X])hX E X [X]; yi]. In this, the inner product is the Euclidean inner product.
Reference: [Halmos, 74] <author> P.R. Halmos, </author> <title> Measure Theory, </title> <publisher> Springer-Verlag, </publisher> <year> 1974. </year>
Reference-contexts: Figure 1), and (3) every weighted L 2 norm can be specified by its corresponding weighting function thereby facilitating any optimization process. 2.2 Weighted L 2 Norms Every measure on the set of pixels S defines a different L 2 norm and which is denoted L 2 () [Conway, 85] <ref> [Halmos, 74] </ref>. A measure is defined by the weight (n; m) = w nm 0 that it assigns to each of the pixels (n; m) 2 S.
Reference: [Hancock, 60] <author> H. Hancock, </author> <title> Theory of Maxima and Minima, </title> <publisher> Dover, </publisher> <year> 1960. </year>
Reference-contexts: Although this does not strictly prove that w nm = 1 2 (n;m) minimizes PM i , it is a necessary condition, and as stated in <ref> [Hancock, 60] </ref> verification of the sufficient conditions for optimality is intractable for almost any non-trivial k-dimensional optimization problem. <p> In the 1-parameter case, the manifold is a parabola. Finding the closest point on a parabola to a fixed point can be solved in at least two ways. One way involves solving a cubic equation. Perhaps a more promising way to proceed is to use constrained optimization <ref> [Hancock, 60] </ref>, since it leads to a linear system of equations. The k-parameter case is even more difficult. 5.2.2 Feature Detection Robustness In this paper we have investigated the selection of the fitting-function which optimizes the parameter estimation accuracy of a model-matching feature detector.
Reference: [Haralick 84] <author> R.M. Haralick, </author> <title> "Digital Step Edges from Zero Crossing of Second Directional Derivatives," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 58-68, </pages> <year> 1984. </year>
Reference-contexts: One way to approximate the partial derivatives needed to compute the differential invariants is to first fit a surface, and to then use the partial derivatives of the surface as estimates of the partial derivatives in the underlying image. One example of such an approach is <ref> [Haralick 84] </ref>. Another example is [Meer and Weiss, 92] where both unweighted and Gaussian weighted L 2 norms are used to define and then find the closest-fitting low-order polynomial surface. 1.2 Summary The remainder of this paper is organized as follows.
Reference: [Hartley, 85] <author> R. </author> <title> Hartley, "A Gaussian-Weighted Multiresolution Edge Detector," Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 30 </volume> <pages> 70-83, </pages> <year> 1985. </year>
Reference-contexts: Examples include [Hummel, 79], [Rohr, 92], and [Baker et al., 98]. The remaining detectors mostly use weighted L 2 norms, but in all cases the weighting function was chosen in an ad-hoc manner. See for example [Hueckel, 71], [Hueckel, 73], and <ref> [Hartley, 85] </ref>. Here, we consider the entire class of weighted L 2 norms as possible fitting functions and investigate the selection of the best one. As can be seen from Figure 1, this class contains functions with vastly different performance levels. <p> However, in the appendix Hueckel suggests that, neglecting computational issues, a Gaussian weighting function may be a more appropriate choice. Besides Hueckel's work, very few other detectors actually use non-uniform weighting functions. One example which follows Hueckel's suggestion of using a Gaussian weighting 3 function is <ref> [Hartley, 85] </ref>. In [Lenz, 87], Lenz concentrates on the Euclidean L 2 norm, but extends some of his results to the L 2 norm in polar coordinates (with no weighting function). <p> weighting functions are scaled so that the most heavily weighted pixel is displayed at the maximum intensity (ie. pure white.) In Figures 8 (a) and (b) we show the Gaussian weighting function: w (x; y) = e 2 2 (37) suggested by Hueckel [Hueckel, 73] and used by Hartley in <ref> [Hartley, 85] </ref>. In Figure 8 (a) the value of for the Gaussian is 1.0 pixels and in Figure 8 (b) it is 2.0 pixels.
Reference: [Healey and Kondepudy, 91] <author> G. Healey and R. </author> <title> Kondepudy "Modeling and Calibrating CCD Cameras for Illumination Insensitive Machine Vision," </title> <booktitle> SPIE Optics, Illumination, and Image Sensing for Machine Vision VI, </booktitle> <volume> 1614 </volume> <pages> 121-132, </pages> <year> 1991. </year>
Reference-contexts: In Equation (3) of Section 2.1 we model the blur introduced by the optics and the averaging performed by the CCD. Unfortunately the operation of a CCD is a noisy process <ref> [Healey and Kondepudy, 91] </ref> and it is this which we model with the noise . We assume that, (a) (n; m; q) has zero mean, (b) it is pairwise independent across the pixels (n; m) 2 S, and (c) it has variance 2 (n; m; q).
Reference: [Hueckel, 71] <author> M.H. Hueckel, </author> <title> "An Operator Which Locates Edges in Digitized Pictures," </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 18 </volume> <pages> 113-125, </pages> <year> 1971. </year>
Reference-contexts: In fact, most detectors simply use the Euclidean L 2 norm. Examples include [Hummel, 79], [Rohr, 92], and [Baker et al., 98]. The remaining detectors mostly use weighted L 2 norms, but in all cases the weighting function was chosen in an ad-hoc manner. See for example <ref> [Hueckel, 71] </ref>, [Hueckel, 73], and [Hartley, 85]. Here, we consider the entire class of weighted L 2 norms as possible fitting functions and investigate the selection of the best one. As can be seen from Figure 1, this class contains functions with vastly different performance levels. <p> Weighted L 2 norms have been used in a small number of previous feature detection algorithms, and date back to the work of Hueckel. In the continuous domain of <ref> [Hueckel, 71] </ref>, Hueckel used the weighting function w (x; y) = [1 (x 2 + y 2 )] 1=2 where (x; y) are coordinates relative to the center of a circular feature window with unit radius. <p> The K-L expansion is used in several model-fitting feature detectors, including [Hummel, 79], [Zucker and Hummel, 81], and [Nayar et al., 96]. Ad-hoc dimension reduction is used in other detectors, such as <ref> [Hueckel, 71] </ref>, [Hueckel, 73], and [Morgenthaler, 81]. Applying dimension reduction when using a weighted L 2 norm is straightforward, as we now show. <p> In Figure 8 (a) the value of for the Gaussian is 1.0 pixels and in Figure 8 (b) it is 2.0 pixels. Next, in w (x; y) = [1 (x 2 + y 2 )] 1=2 (38) which was actually used by Hueckel in <ref> [Hueckel, 71] </ref> and [Hueckel, 73]. The weighting function: w (x; y) = (x 2 + y 2 ) 1=2 (39) which was briefly studied by Lenz in [Lenz, 87] has a discontinuity at the origin, and so we modify it slightly.
Reference: [Hueckel, 73] <author> M.H. Hueckel, </author> <title> "A Local Visual Operator Which Recognizes Edges and Lines," </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 20 </volume> <pages> 634-647, </pages> <year> 1973. </year> <month> 42 </month>
Reference-contexts: In fact, most detectors simply use the Euclidean L 2 norm. Examples include [Hummel, 79], [Rohr, 92], and [Baker et al., 98]. The remaining detectors mostly use weighted L 2 norms, but in all cases the weighting function was chosen in an ad-hoc manner. See for example [Hueckel, 71], <ref> [Hueckel, 73] </ref>, and [Hartley, 85]. Here, we consider the entire class of weighted L 2 norms as possible fitting functions and investigate the selection of the best one. As can be seen from Figure 1, this class contains functions with vastly different performance levels. <p> The informal justifications provided for this choice are: (a) the weighting function should be continuous, including at the periphery of the window, and (b) its value should decrease monotonically moving away from a maximum at the center of the window. In <ref> [Hueckel, 73] </ref> the weighting function remains the same to allow a closed form solution for the parameters in the norm minimization problem. However, in the appendix Hueckel suggests that, neglecting computational issues, a Gaussian weighting function may be a more appropriate choice. <p> The K-L expansion is used in several model-fitting feature detectors, including [Hummel, 79], [Zucker and Hummel, 81], and [Nayar et al., 96]. Ad-hoc dimension reduction is used in other detectors, such as [Hueckel, 71], <ref> [Hueckel, 73] </ref>, and [Morgenthaler, 81]. Applying dimension reduction when using a weighted L 2 norm is straightforward, as we now show. <p> Finally, the weighting functions are scaled so that the most heavily weighted pixel is displayed at the maximum intensity (ie. pure white.) In Figures 8 (a) and (b) we show the Gaussian weighting function: w (x; y) = e 2 2 (37) suggested by Hueckel <ref> [Hueckel, 73] </ref> and used by Hartley in [Hartley, 85]. In Figure 8 (a) the value of for the Gaussian is 1.0 pixels and in Figure 8 (b) it is 2.0 pixels. <p> In Figure 8 (a) the value of for the Gaussian is 1.0 pixels and in Figure 8 (b) it is 2.0 pixels. Next, in w (x; y) = [1 (x 2 + y 2 )] 1=2 (38) which was actually used by Hueckel in [Hueckel, 71] and <ref> [Hueckel, 73] </ref>. The weighting function: w (x; y) = (x 2 + y 2 ) 1=2 (39) which was briefly studied by Lenz in [Lenz, 87] has a discontinuity at the origin, and so we modify it slightly.
Reference: [Hummel, 79] <author> R.A. Hummel, </author> <title> "Feature Detection Using Basis Functions," </title> <booktitle> Computer Graph--ics and Image Processing, </booktitle> <volume> 9 </volume> <pages> 40-55, </pages> <year> 1979. </year>
Reference-contexts: As far as we can tell, the selection of the fitting function has never before been studied in a systematic manner. In fact, most detectors simply use the Euclidean L 2 norm. Examples include <ref> [Hummel, 79] </ref>, [Rohr, 92], and [Baker et al., 98]. The remaining detectors mostly use weighted L 2 norms, but in all cases the weighting function was chosen in an ad-hoc manner. See for example [Hueckel, 71], [Hueckel, 73], and [Hartley, 85]. <p> Most existing detectors simply use the Euclidean L 2 norm, often without any discussion of the decision, including [O'Gorman, 78], <ref> [Hummel, 79] </ref>, [Morgenthaler, 81], [Zucker and Hummel, 81], [Nalwa and Binford, 86], [Rohr, 92], and [Nayar et al., 96]. Weighted L 2 norms have been used in a small number of previous feature detection algorithms, and date back to the work of Hueckel. <p> Since every L 2 norm is derived from an underlying Hilbert space, we can apply dimension reduction techniques, such as the Karhunen-Loeve (K-L) expansion [Oja, 83], to further improve the efficiency. The K-L expansion is used in several model-fitting feature detectors, including <ref> [Hummel, 79] </ref>, [Zucker and Hummel, 81], and [Nayar et al., 96]. Ad-hoc dimension reduction is used in other detectors, such as [Hueckel, 71], [Hueckel, 73], and [Morgenthaler, 81]. Applying dimension reduction when using a weighted L 2 norm is straightforward, as we now show.
Reference: [Knuth, 81] <author> D.E. Knuth, </author> <title> The Art of Computer Programming, Volume II: Seminumerical Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1981. </year>
Reference-contexts: The way to generate points uniformly from a unit ball is to pick the coordi nates independently from the Gaussian distribution and then normalize by dividing each coordinate by the Euclidean L 2 norm of the weight vector <ref> [Knuth, 81] </ref>. In Table 1 we present the results of the experiment. For each of the two manifold parameters, we give the R.M.S. error in its estimate for each of the weighting functions. For the random weighting functions we just present the best-ever and the worst-ever performances.
Reference: [Lenz, 87] <author> R. Lenz, </author> <title> "Optimal Filters for the Detection of Linear Patterns in 2-D and Higher Dimensional Images," </title> <journal> Pattern Recognition, </journal> <volume> 20 </volume> <pages> 163-172, </pages> <year> 1987. </year>
Reference-contexts: However, in the appendix Hueckel suggests that, neglecting computational issues, a Gaussian weighting function may be a more appropriate choice. Besides Hueckel's work, very few other detectors actually use non-uniform weighting functions. One example which follows Hueckel's suggestion of using a Gaussian weighting 3 function is [Hartley, 85]. In <ref> [Lenz, 87] </ref>, Lenz concentrates on the Euclidean L 2 norm, but extends some of his results to the L 2 norm in polar coordinates (with no weighting function). In Cartesian coordinates this corresponds to the weighting function w (x; y) = 1=(x 2 +y 2 ) 1=2 . <p> Next, in w (x; y) = [1 (x 2 + y 2 )] 1=2 (38) which was actually used by Hueckel in [Hueckel, 71] and [Hueckel, 73]. The weighting function: w (x; y) = (x 2 + y 2 ) 1=2 (39) which was briefly studied by Lenz in <ref> [Lenz, 87] </ref> has a discontinuity at the origin, and so we modify it slightly. Instead, we used the weighting function: w (x; y) = (k + x 2 + y 2 ) 1=2 (40) for various values of k.
Reference: [Meer and Weiss, 92] <author> P. Meer and I. Weiss, </author> <title> "Smoothed Differentiation Filters for Images," </title> <journal> Journal of Visual Communication and Image Representation, </journal> <volume> 3 </volume> <pages> 58-72, </pages> <year> 1992. </year>
Reference-contexts: One example of such an approach is [Haralick 84]. Another example is <ref> [Meer and Weiss, 92] </ref> where both unweighted and Gaussian weighted L 2 norms are used to define and then find the closest-fitting low-order polynomial surface. 1.2 Summary The remainder of this paper is organized as follows.
Reference: [Modestino and Fries, 77] <author> J.W. Modestino and R.W. Fries, </author> <title> "Edge Detection in Noisey Images Using Recursive Digital Filtering," </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 6 </volume> <pages> 409-433, </pages> <year> 1977. </year>
Reference-contexts: The Square Root Ramp function and the Modified Lenz function do slightly better than the Euclidean L 2 norm for low noise levels, and slightly worse for high noise levels. in <ref> [Modestino and Fries, 77] </ref>, [Shanmugam et al., 79], [Boie et al., 86], [Deriche, 87], and [Sarkar and Boyer, 91].
Reference: [Morgenthaler, 81] <author> D.G. Morgenthaler, </author> <title> "A New Hybrid Edge Detector," </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 16 </volume> <pages> 166-176, </pages> <year> 1981. </year>
Reference-contexts: Most existing detectors simply use the Euclidean L 2 norm, often without any discussion of the decision, including [O'Gorman, 78], [Hummel, 79], <ref> [Morgenthaler, 81] </ref>, [Zucker and Hummel, 81], [Nalwa and Binford, 86], [Rohr, 92], and [Nayar et al., 96]. Weighted L 2 norms have been used in a small number of previous feature detection algorithms, and date back to the work of Hueckel. <p> The K-L expansion is used in several model-fitting feature detectors, including [Hummel, 79], [Zucker and Hummel, 81], and [Nayar et al., 96]. Ad-hoc dimension reduction is used in other detectors, such as [Hueckel, 71], [Hueckel, 73], and <ref> [Morgenthaler, 81] </ref>. Applying dimension reduction when using a weighted L 2 norm is straightforward, as we now show.
Reference: [Nalwa, 93] <author> V.S. Nalwa, </author> <title> A Guided Tour of Computer Vision, </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: We also only consider a certain type of feature detector. All but a small number of feature detectors can be placed into one of two categories <ref> [Nalwa, 93] </ref>: those based upon differential invariants and those which fit parametric models to the image data. In this paper, we restrict attention to the model-fitting approach best exemplified by [Nalwa and Binford, 86], [Rohr, 92], and [Nayar et al., 96]. <p> In doing so, we will attempt to characterize the most important similarities and differences between the two major approaches to feature detection as categorized in <ref> [Nalwa, 93] </ref>: parametric model matching and using differential invariants. We note that there has already been some previous work attempting to unify different approaches to edge detection including [Abramatic, 81] and [Rosenfeld, 81].
Reference: [Nalwa and Binford, 86] <author> V.S. </author> <title> Nalwa and T.O. Binford, "On detecting edges," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8 </volume> <pages> 699-714, </pages> <year> 1986. </year>
Reference-contexts: All but a small number of feature detectors can be placed into one of two categories [Nalwa, 93]: those based upon differential invariants and those which fit parametric models to the image data. In this paper, we restrict attention to the model-fitting approach best exemplified by <ref> [Nalwa and Binford, 86] </ref>, [Rohr, 92], and [Nayar et al., 96]. Model-fitting feature detectors work by finding the model parameters which yield the closest match between the image data and the feature model. <p> Most existing detectors simply use the Euclidean L 2 norm, often without any discussion of the decision, including [O'Gorman, 78], [Hummel, 79], [Morgenthaler, 81], [Zucker and Hummel, 81], <ref> [Nalwa and Binford, 86] </ref>, [Rohr, 92], and [Nayar et al., 96]. Weighted L 2 norms have been used in a small number of previous feature detection algorithms, and date back to the work of Hueckel. <p> In particular, measures of feature detection robustness (such as rates of occurrence of false-positives and false-negatives) are of fundamental importance. Several technical difficulties arise when analyzing such measures. The most important such difficulty is the selection of a characteristic model for a "not-feature" <ref> [Nalwa and Binford, 86] </ref>. It appears that the only way to proceed is to assume the existence of two parametric manifolds, one characterizing the feature and one characterizing a "not feature." The manifold characterizing the "not feature" would need to depend upon the feature in question. <p> We now briefly describe the experimental procedure used. This procedure is almost identical to that of [Baker et al., 98] and also that of <ref> [Nalwa and Binford, 86] </ref>. The reader is referred to these papers for a more complete discussion of it.
Reference: [Nayar et al., 96] <author> S.K. Nayar, S. Baker, and H. Murase, </author> <title> "Parametric Feature Detection," </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 471-477, </pages> <address> San Francisco, </address> <year> 1996. </year>
Reference-contexts: In this paper, we restrict attention to the model-fitting approach best exemplified by [Nalwa and Binford, 86], [Rohr, 92], and <ref> [Nayar et al., 96] </ref>. Model-fitting feature detectors work by finding the model parameters which yield the closest match between the image data and the feature model. <p> We begin by defining an optimality criterion based upon the assumption that inaccurate parameter estimation is caused by ideal features in the real world being corrupted by noise during the imaging process. Since we will represent features by their manifolds, in the same manner as <ref> [Nayar et al., 96] </ref>, we model this corruption by perturbing ideal 1 for the step edge model introduced in Section 2. One of the fitting functions is the Euclidean L 2 norm and the other is a non-uniformly weighted L 2 norm. <p> Most existing detectors simply use the Euclidean L 2 norm, often without any discussion of the decision, including [O'Gorman, 78], [Hummel, 79], [Morgenthaler, 81], [Zucker and Hummel, 81], [Nalwa and Binford, 86], [Rohr, 92], and <ref> [Nayar et al., 96] </ref>. Weighted L 2 norms have been used in a small number of previous feature detection algorithms, and date back to the work of Hueckel. <p> We follow <ref> [Nayar et al., 96] </ref> and start with a continuous feature model F c (x; y; q c ); where (x; y) 2 S c are points within a compact feature window S c R 2 and q c is a vector containing the feature parameters. <p> The 2-D Gaussian g (x; y; ) models the blurring of the feature by the optical system and the rectangular averaging function a (x; y) accounts for the integration performed by the CCD sensor. See <ref> [Nayar et al., 96] </ref> for more details. If N is the total number of pixels in the discrete feature window S; then each feature instance F (n; m; q) may be regarded as a vector in R N . <p> measuring how well I c (x; y) fits the continuous feature model F c (x; y; q c ): Although all of what follows could be placed in such a continuous framework, we believe that carefully modeling the image formation process is vital for high performance, as is argued in <ref> [Nayar et al., 96] </ref>, and so work with discrete models and data. 6 In the model-fitting approach just described, the performance of feature detection is critically dependent upon the fitting function used to find the closest point on the feature manifold. <p> As we now describe, the Hilbert Space structure allows us to generalize the parameter normalization and dimension reduction procedures proposed in <ref> [Nayar et al., 96] </ref>. As described in [Nayar et al., 96], these two procedures are often needed to enhance the efficiency of detection algorithms. 2.3 Parameter Normalization In [Nayar et al., 96] an important normalization is applied. <p> As we now describe, the Hilbert Space structure allows us to generalize the parameter normalization and dimension reduction procedures proposed in <ref> [Nayar et al., 96] </ref>. As described in [Nayar et al., 96], these two procedures are often needed to enhance the efficiency of detection algorithms. 2.3 Parameter Normalization In [Nayar et al., 96] an important normalization is applied. <p> As we now describe, the Hilbert Space structure allows us to generalize the parameter normalization and dimension reduction procedures proposed in <ref> [Nayar et al., 96] </ref>. As described in [Nayar et al., 96], these two procedures are often needed to enhance the efficiency of detection algorithms. 2.3 Parameter Normalization In [Nayar et al., 96] an important normalization is applied. For each feature instance F (n; m; q) 2 R N the mean coordinate (q)= 1 N (n;m)2S F (n; m; q) and the coordinate variance -(q) = [ P (n;m)2S [F (n; m; q) (q)] 2 ] 1=2 are computed. <p> Since every L 2 norm is derived from an underlying Hilbert space, we can apply dimension reduction techniques, such as the Karhunen-Loeve (K-L) expansion [Oja, 83], to further improve the efficiency. The K-L expansion is used in several model-fitting feature detectors, including [Hummel, 79], [Zucker and Hummel, 81], and <ref> [Nayar et al., 96] </ref>. Ad-hoc dimension reduction is used in other detectors, such as [Hueckel, 71], [Hueckel, 73], and [Morgenthaler, 81]. Applying dimension reduction when using a weighted L 2 norm is straightforward, as we now show.
Reference: [O'Gorman, 78] <author> F. O'Gorman, </author> <title> "Edge Detection Using Walsh Functions," </title> <journal> Artificial Intelligence, </journal> <volume> 10 </volume> <pages> 215-223, </pages> <year> 1978. </year>
Reference-contexts: Most existing detectors simply use the Euclidean L 2 norm, often without any discussion of the decision, including <ref> [O'Gorman, 78] </ref>, [Hummel, 79], [Morgenthaler, 81], [Zucker and Hummel, 81], [Nalwa and Binford, 86], [Rohr, 92], and [Nayar et al., 96]. Weighted L 2 norms have been used in a small number of previous feature detection algorithms, and date back to the work of Hueckel.
Reference: [Oja, 83] <author> E. Oja, </author> <title> Subspace Methods of Pattern Recognition, </title> <publisher> Research Studies Press, </publisher> <year> 1983. </year>
Reference-contexts: Since every L 2 norm is derived from an underlying Hilbert space, we can apply dimension reduction techniques, such as the Karhunen-Loeve (K-L) expansion <ref> [Oja, 83] </ref>, to further improve the efficiency. The K-L expansion is used in several model-fitting feature detectors, including [Hummel, 79], [Zucker and Hummel, 81], and [Nayar et al., 96]. Ad-hoc dimension reduction is used in other detectors, such as [Hueckel, 71], [Hueckel, 73], and [Morgenthaler, 81].
Reference: [Paton, 75] <author> K. Paton, </author> <title> "Picture Description Using Legendre Polynomials," </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 4 </volume> <pages> 40-54, </pages> <year> 1975. </year>
Reference-contexts: Some papers do discuss the possibility of using a weighting function, but end up using the Euclidean L 2 norm. One example is Paton <ref> [Paton, 75] </ref>, who proposes a number of alternatives including a sequence of functions similar to Hueckel's and an annular stop function which interestingly assigns zero weight to the center of the feature window: w (x; y) = &gt; &lt; k if 0 &lt; r 2 x 2 + y 2 1 <p> In Figure 8 (g) we plot this function for k = 0:5, and in Figure 8 (h) for k = 0:25. Next, in Figures 8 (i) and (j) we display the annular stop function suggested by Paton in <ref> [Paton, 75] </ref> for radius values of 0:4 and 0:57. Since this function has a step discontinuity, we also considered smooth weighting functions which increase with the distance from the center of the window.
Reference: [Pratt, 90] <author> W.K. Pratt, </author> <title> Digital Image Processing, </title> <publisher> John Wiley & Sons, </publisher> <year> 1990. </year>
Reference-contexts: A final example is [Abdou and Pratt, 79] in which it is mentioned that weighting pixels so as to reduce the influence of pixels which are distant from the center of the window improves Pratt's Figure of Merit <ref> [Pratt, 90] </ref>, but few details of this are actually given. Some papers do discuss the possibility of using a weighting function, but end up using the Euclidean L 2 norm.
Reference: [Rohr, 92] <author> K. Rohr, </author> <title> "Recognizing Corners by Fitting Parametric Models," </title> <journal> International Journal of Computer Vision, </journal> <volume> 9 </volume> <pages> 213-230, </pages> <year> 1992. </year>
Reference-contexts: In this paper, we restrict attention to the model-fitting approach best exemplified by [Nalwa and Binford, 86], <ref> [Rohr, 92] </ref>, and [Nayar et al., 96]. Model-fitting feature detectors work by finding the model parameters which yield the closest match between the image data and the feature model. <p> As far as we can tell, the selection of the fitting function has never before been studied in a systematic manner. In fact, most detectors simply use the Euclidean L 2 norm. Examples include [Hummel, 79], <ref> [Rohr, 92] </ref>, and [Baker et al., 98]. The remaining detectors mostly use weighted L 2 norms, but in all cases the weighting function was chosen in an ad-hoc manner. See for example [Hueckel, 71], [Hueckel, 73], and [Hartley, 85]. <p> Most existing detectors simply use the Euclidean L 2 norm, often without any discussion of the decision, including [O'Gorman, 78], [Hummel, 79], [Morgenthaler, 81], [Zucker and Hummel, 81], [Nalwa and Binford, 86], <ref> [Rohr, 92] </ref>, and [Nayar et al., 96]. Weighted L 2 norms have been used in a small number of previous feature detection algorithms, and date back to the work of Hueckel.
Reference: [Rosenfeld, 81] <author> A. Rosenfeld, </author> <title> "The Max Roberts Operator is a Hueckel-Type Edge Detector," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 3 </volume> <pages> 101-103, </pages> <year> 1981. </year>
Reference-contexts: We note that there has already been some previous work attempting to unify different approaches to edge detection including [Abramatic, 81] and <ref> [Rosenfeld, 81] </ref>.
Reference: [Sarkar and Boyer, 91] <author> S. Sarkar and K.L. Boyer, </author> <title> "On Optimal Infinite Impulse Response Edge Detection Filters," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13 </volume> <pages> 1154-1171, </pages> <year> 1991. </year>
Reference-contexts: The Square Root Ramp function and the Modified Lenz function do slightly better than the Euclidean L 2 norm for low noise levels, and slightly worse for high noise levels. in [Modestino and Fries, 77], [Shanmugam et al., 79], [Boie et al., 86], [Deriche, 87], and <ref> [Sarkar and Boyer, 91] </ref>. In doing so, we will attempt to characterize the most important similarities and differences between the two major approaches to feature detection as categorized in [Nalwa, 93]: parametric model matching and using differential invariants.
Reference: [Shanmugam et al., 79] <author> K.S. Shanmugam, </author> <title> F.M. Dickey, and J.A. Green, "An Optimal Frequency Domain Filter for Edge Detection in Digital Pictures," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 1 </volume> <pages> 37-49, </pages> <year> 1979. </year>
Reference-contexts: The Square Root Ramp function and the Modified Lenz function do slightly better than the Euclidean L 2 norm for low noise levels, and slightly worse for high noise levels. in [Modestino and Fries, 77], <ref> [Shanmugam et al., 79] </ref>, [Boie et al., 86], [Deriche, 87], and [Sarkar and Boyer, 91]. In doing so, we will attempt to characterize the most important similarities and differences between the two major approaches to feature detection as categorized in [Nalwa, 93]: parametric model matching and using differential invariants.
Reference: [Zucker and Hummel, 81] <author> S.W. Zucker and R.A. Hummel, </author> <title> "A Three-Dimensional Edge Operator," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 3 </volume> <pages> 324-331, </pages> <year> 1981. </year>
Reference-contexts: Most existing detectors simply use the Euclidean L 2 norm, often without any discussion of the decision, including [O'Gorman, 78], [Hummel, 79], [Morgenthaler, 81], <ref> [Zucker and Hummel, 81] </ref>, [Nalwa and Binford, 86], [Rohr, 92], and [Nayar et al., 96]. Weighted L 2 norms have been used in a small number of previous feature detection algorithms, and date back to the work of Hueckel. <p> Since every L 2 norm is derived from an underlying Hilbert space, we can apply dimension reduction techniques, such as the Karhunen-Loeve (K-L) expansion [Oja, 83], to further improve the efficiency. The K-L expansion is used in several model-fitting feature detectors, including [Hummel, 79], <ref> [Zucker and Hummel, 81] </ref>, and [Nayar et al., 96]. Ad-hoc dimension reduction is used in other detectors, such as [Hueckel, 71], [Hueckel, 73], and [Morgenthaler, 81]. Applying dimension reduction when using a weighted L 2 norm is straightforward, as we now show.
References-found: 34

