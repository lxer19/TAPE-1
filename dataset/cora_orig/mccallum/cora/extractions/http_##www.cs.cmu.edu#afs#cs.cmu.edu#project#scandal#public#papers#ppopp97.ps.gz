URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/ppopp97.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/ppopp97.html
Root-URL: 
Email: narlikar@cs.cmu.edu  blelloch@cs.cmu.edu  
Title: Space-Efficient Implementation of Nested Parallelism  
Author: Girija J. Narlikar Guy E. Blelloch 
Keyword: Space efficiency, dynamic scheduling, nested parallelism, multithreading, language implementation.  
Address: 5000 Forbes Avenue Pittsburgh, PA 15232  
Affiliation: CMU School of Computer Science  
Abstract: Many of today's high level parallel languages support dynamic, fine-grained parallelism. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. Hence an efficient scheduling algorithm is required to assign computations to processors at runtime. Besides having low overheads and good load balancing, it is important for the scheduling algorithm to minimize the space usage of the parallel program. This paper presents a scheduling algorithm that is provably space-efficient and time-efficient for nested parallel languages. In addition to proving the space and time bounds of the parallel schedule generated by the algorithm, we demonstrate that it is efficient in practice. We have implemented a runtime system that uses our algorithm to schedule parallel threads. The results of executing parallel programs on this system show that our scheduling algorithm significantly reduces memory usage compared to previous techniques, without compromising performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Arvind, R. S. Nikhil, and K. Pingali. I-structures: </author> <title> Data structures for parallel computing. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(4) </volume> <pages> 598-632, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Many of today's high level parallel programming languages provide constructs to express dynamic, fine-grained parallelism. Such languages include data-parallel languages such as Nesl [2] and HPF [21], as well as control-parallel languages such as ID <ref> [1] </ref>, Cilk [5], CC++ [12], Sisal [17], Pro-teus [28], and C with light-weight thread libraries [25, 30]. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors.
Reference: [2] <author> G. E. Blelloch, S. Chatterjee, J. C. Hardwick, J. Sipel-stein, and M. Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 4-14, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Many of today's high level parallel programming languages provide constructs to express dynamic, fine-grained parallelism. Such languages include data-parallel languages such as Nesl <ref> [2] </ref> and HPF [21], as well as control-parallel languages such as ID [1], Cilk [5], CC++ [12], Sisal [17], Pro-teus [28], and C with light-weight thread libraries [25, 30].
Reference: [3] <author> G. E. Blelloch, P. B. Gibbons, and Y. Matias. </author> <title> Provably efficient scheduling for languages with fine-grained parallelism. </title> <booktitle> In Proc. Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Santa Barbara, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: These ideas are used in the implementation of the Cilk programming language [5]. A recent scheduling algorithm improved these space bounds from a multiplicative factor on the number of processors to an additive factor <ref> [3] </ref>. The algorithm generates a schedule that uses only S 1 + O (p D) space, where D is the depth of the parallel computation (i.e., the longest sequence of dependencies or the critical path in the computation). <p> Moreover, it ignores the issue of locality | a thread may be moved from processor to processor at every timestep. This paper presents a variant on the scheduling algorithm proposed in <ref> [3] </ref> that overcomes the above mentioned problems. The paper then gives experimental results that demonstrate that the algorithm does achieve good performance both in terms of memory and time. <p> Together these are within the S 1 + O (p D) bounds. In addition to proving space-efficiency, as with <ref> [3] </ref> we bound the time required to execute the schedule for a parallel computation in terms of its work (total number of operations) and depth. <p> If the step performs a deallocation, m (v) is negative 4 . For the nested parallelism model described in Section 2, the dynamically unfolding DAG has a series-parallel structure. A series-parallel DAG <ref> [3] </ref> can be defined inductively: the DAG G 0 consisting of a single node (which is both its source and sink) and no edges, is series-parallel. <p> This does not allow a unit computational step within a thread to allocate more than K bytes. We now show how such allocations are handled, similar to the technique suggested in <ref> [3] </ref>. The key idea is to delay the big allocations, so that if threads with lower 1df numbers become ready, they will be executed instead. Consider a thread with a node that allocates m units of space in the original DAG, and m &gt; K. <p> Lemma 1 The asynchronous scheduling algorithm in Figure 4 always maintains the threads in the ready-queue in an increasing order of the 1df-numbers of their leading nodes. Proof: This can be proved by induction. The proof is similar to the one presented in <ref> [3] </ref>, and we present the outline here. When the execution begins, the ready-queue contains just the root thread, and therefore it is ordered by the 1df-numbers.
Reference: [4] <author> R. D. Blumofe, M. Frigo, C. F. Joerg, C. E. Leiserson, and K. H. Randall. </author> <title> An Analysis of Dag-Consistent Distributed Shared-Memory Algorithms. </title> <booktitle> In Proc. Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 297-308, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: However, the algorithm has scheduling overheads that are too high for it to be practical. Since it is synchronous, threads need to be 1 More recent work provides a stronger upper bound than p S 1 for space requirements of regular divide-and-conquer algorithms in Cilk <ref> [4] </ref>. rescheduled after every unit computation to guarantee the space bounds. Moreover, it ignores the issue of locality | a thread may be moved from processor to processor at every timestep. This paper presents a variant on the scheduling algorithm proposed in [3] that overcomes the above mentioned problems.
Reference: [5] <author> R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leis-erson, K. H. Randall, and Y. Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proc. Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 207-216, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Many of today's high level parallel programming languages provide constructs to express dynamic, fine-grained parallelism. Such languages include data-parallel languages such as Nesl [2] and HPF [21], as well as control-parallel languages such as ID [1], Cilk <ref> [5] </ref>, CC++ [12], Sisal [17], Pro-teus [28], and C with light-weight thread libraries [25, 30]. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. <p> If S 1 is the space required by the serial execution, these techniques generate schedules for a multithreaded computation on p processors that require no more than p S 1 space. These ideas are used in the implementation of the Cilk programming language <ref> [5] </ref>. A recent scheduling algorithm improved these space bounds from a multiplicative factor on the number of processors to an additive factor [3]. <p> They are implemented using variants of lock-free algorithms based on atomic fetch-and primitives [27]. The parallel programs executed using this system have been explicitly hand-coded in the continuation-passing style, similar to the code generated by the Cilk preprocessor 5 <ref> [5] </ref>. Each continuation points to a C function representing the next computation of a thread, and a structure containing all its arguments. These continuations are created dynamically and moved between the queues.
Reference: [6] <author> R. D. Blumofe and C. E. Leiserson. </author> <title> Space-efficient scheduling of multithreaded computations. </title> <booktitle> In Proc. 25th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 362-371, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism [10, 16, 32, 36], and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs <ref> [6, 7, 8, 9] </ref>. If S 1 is the space required by the serial execution, these techniques generate schedules for a multithreaded computation on p processors that require no more than p S 1 space. These ideas are used in the implementation of the Cilk programming language [5]. <p> For a parallel computation with D depth and W work, if the total space allocated is O (W ), then the generated schedule runs in O (W=p + D) time, making it time-efficient <ref> [6] </ref>. We note that a bigger K leads to a lower running time since it reduces scheduling costs, but also results in a larger space bound. The K parameter therefore provides a trade-off between the running time and the memory requirement of a parallel computation. <p> Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 14, 19, 23, 32, 36] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n).
Reference: [7] <author> R. D. Blumofe and C. E. Leiserson. </author> <title> Scheduling mul-tithreaded computations by work stealing. </title> <booktitle> In Proc. 35th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 356-368, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: This paper appears in the Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), June 18-21, 1997, Las Vegas. Several systems providing dynamic parallelism have been implemented with efficient runtime schedulers <ref> [7, 11, 13, 18, 19, 22, 23, 33, 34, 35] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration. <p> Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism [10, 16, 32, 36], and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs <ref> [6, 7, 8, 9] </ref>. If S 1 is the space required by the serial execution, these techniques generate schedules for a multithreaded computation on p processors that require no more than p S 1 space. These ideas are used in the implementation of the Cilk programming language [5].
Reference: [8] <author> F. W. Burton. </author> <title> Storage management in virtual tree machines. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 37(3) </volume> <pages> 321-328, </pages> <year> 1988. </year>
Reference-contexts: Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism [10, 16, 32, 36], and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs <ref> [6, 7, 8, 9] </ref>. If S 1 is the space required by the serial execution, these techniques generate schedules for a multithreaded computation on p processors that require no more than p S 1 space. These ideas are used in the implementation of the Cilk programming language [5]. <p> Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 14, 19, 23, 32, 36] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n). <p> Cilk uses less memory than this estimate due to its use of randomization: an idle processor steals the topmost thread (representing the outermost parallelism) from the private queue of a randomly picked processor; this thread may not represent the outermost parallelism in the entire computation. Previous techniques like <ref> [8, 9, 19] </ref> use a strategy similar to that of Cilk. The results show that when big allocations are delayed with dummy nodes, our algorithm results in a with K (in bytes) for multiplying two 1024 fi 1024 matrices using blocked recursive matrix multiplication on 8 processors.
Reference: [9] <author> F. W. Burton and D. J. Simpson. </author> <title> Space efficient execution of deterministic parallel programs. </title> <type> Manuscript, </type> <month> December </month> <year> 1994. </year>
Reference-contexts: Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism [10, 16, 32, 36], and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs <ref> [6, 7, 8, 9] </ref>. If S 1 is the space required by the serial execution, these techniques generate schedules for a multithreaded computation on p processors that require no more than p S 1 space. These ideas are used in the implementation of the Cilk programming language [5]. <p> Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 14, 19, 23, 32, 36] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n). <p> Cilk uses less memory than this estimate due to its use of randomization: an idle processor steals the topmost thread (representing the outermost parallelism) from the private queue of a randomly picked processor; this thread may not represent the outermost parallelism in the entire computation. Previous techniques like <ref> [8, 9, 19] </ref> use a strategy similar to that of Cilk. The results show that when big allocations are delayed with dummy nodes, our algorithm results in a with K (in bytes) for multiplying two 1024 fi 1024 matrices using blocked recursive matrix multiplication on 8 processors.
Reference: [10] <author> F. W. Burton and M. R. Sleep. </author> <title> Executing functional programs on a virtual tree of processors. </title> <booktitle> In Conference on Functional Programming Languages and Computer Architecture, </booktitle> <month> October </month> <year> 1981. </year>
Reference-contexts: Many researchers have addressed this problem in the past. Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism <ref> [10, 16, 32, 36] </ref>, and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs [6, 7, 8, 9].
Reference: [11] <author> R. Chandra, A. Gupta, and J. Hennessy. </author> <title> COOL: An object-based language for parallel programming. </title> <journal> IEEE Computer, </journal> <volume> 27(8) </volume> <pages> 13-26, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: This paper appears in the Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), June 18-21, 1997, Las Vegas. Several systems providing dynamic parallelism have been implemented with efficient runtime schedulers <ref> [7, 11, 13, 18, 19, 22, 23, 33, 34, 35] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [12] <author> K. M. Chandy and C. Kesselman. </author> <title> Compositional c++: compositional parallel programming. </title> <booktitle> In Proc. 5th. Intl. Wkshp. on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 124-144, </pages> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Many of today's high level parallel programming languages provide constructs to express dynamic, fine-grained parallelism. Such languages include data-parallel languages such as Nesl [2] and HPF [21], as well as control-parallel languages such as ID [1], Cilk [5], CC++ <ref> [12] </ref>, Sisal [17], Pro-teus [28], and C with light-weight thread libraries [25, 30]. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors.
Reference: [13] <author> J. S. Chase, F. G. Amador, and E. D. Lazowska. </author> <title> The amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proc. Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: This paper appears in the Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), June 18-21, 1997, Las Vegas. Several systems providing dynamic parallelism have been implemented with efficient runtime schedulers <ref> [7, 11, 13, 18, 19, 22, 23, 33, 34, 35] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [14] <author> J. H. Chow and W. L. Harrison III. Switch-stacks: </author> <title> A scheme for microtasking nested parallel loops. </title> <booktitle> In Proc. Supercomputing, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 14, 19, 23, 32, 36] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n). <p> On the other hand, scheduling the outer parallelism would allocate space for the p branches at the top level, with each processor executing a subtree serially. Hence we use our algorithm without the delay to estimate the memory requirements of previous techniques like <ref> [14, 23] </ref>, which schedule the outer parallelism with higher priority.
Reference: [15] <author> S. A. Cook. </author> <title> A taxonomy of problems with fast parallel algorithms. </title> <journal> Information and Control, </journal> <volume> 64 </volume> <pages> 2-22, </pages> <year> 1985. </year>
Reference-contexts: The space bound presented in this paper is significantly lower than space bounds on existing systems for programs with a sufficient amount of parallelism (D S 1 ). Most parallel programs, including all programs in N C <ref> [15] </ref>, fall in this category. We have built a low-overhead runtime system that schedules parallel threads using our algorithm. The results demonstrate that our approach is more effective in reducing space usage than previous scheduling techniques, and at the same time yields good parallel performance.
Reference: [16] <author> D. E. Culler and Arvind. </author> <title> Resource requirements of dataflow programs. </title> <booktitle> In Proc. Intl. Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1988. </year>
Reference-contexts: However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration. In an attempt to expose a sufficient degree of parallelism to keep all processors busy, schedulers often create many more parallel threads than necessary, leading to excessive memory usage <ref> [16, 32, 36] </ref>. <p> Many researchers have addressed this problem in the past. Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism <ref> [10, 16, 32, 36] </ref>, and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs [6, 7, 8, 9].
Reference: [17] <author> J. T. Feo, D. C. Cann, and R. R. Oldehoeft. </author> <title> A report on the Sisal language project. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10(4) </volume> <pages> 349-366, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Many of today's high level parallel programming languages provide constructs to express dynamic, fine-grained parallelism. Such languages include data-parallel languages such as Nesl [2] and HPF [21], as well as control-parallel languages such as ID [1], Cilk [5], CC++ [12], Sisal <ref> [17] </ref>, Pro-teus [28], and C with light-weight thread libraries [25, 30]. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. The language implementation is responsible for scheduling this parallelism onto the processors.
Reference: [18] <author> V. W. Freeh, D. K. Lowenthal, and G. R. Andrews. </author> <title> Distributed filaments: efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 201-212, </pages> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: This paper appears in the Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), June 18-21, 1997, Las Vegas. Several systems providing dynamic parallelism have been implemented with efficient runtime schedulers <ref> [7, 11, 13, 18, 19, 22, 23, 33, 34, 35] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [19] <author> S. C. Goldstein, D. E. Culler, and K. E. Schauser. </author> <title> Enabling primitives for compiling parallel languages. </title> <booktitle> In Third Workshop on Languages, Compilers, and Run-Time Systems for Scalable Computers, </booktitle> <address> Rochester, NY, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: This paper appears in the Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), June 18-21, 1997, Las Vegas. Several systems providing dynamic parallelism have been implemented with efficient runtime schedulers <ref> [7, 11, 13, 18, 19, 22, 23, 33, 34, 35] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration. <p> Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 14, 19, 23, 32, 36] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n). <p> Cilk uses less memory than this estimate due to its use of randomization: an idle processor steals the topmost thread (representing the outermost parallelism) from the private queue of a randomly picked processor; this thread may not represent the outermost parallelism in the entire computation. Previous techniques like <ref> [8, 9, 19] </ref> use a strategy similar to that of Cilk. The results show that when big allocations are delayed with dummy nodes, our algorithm results in a with K (in bytes) for multiplying two 1024 fi 1024 matrices using blocked recursive matrix multiplication on 8 processors.
Reference: [20] <author> L. Greengard. </author> <title> The rapid evaluation of potential fields in particle systems. </title> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: The sizes of matrices multiplied were the same as for the previous program. * Fast multipole method (FMM). This is an n-body algorithm that calculates the forces between n bodies in O (n) work <ref> [20] </ref>. We have implemented the most time-consuming phases of the algorithm, which are a bottom-up traversal of the octree followed by a top-down traversal.
Reference: [21] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, </title> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Many of today's high level parallel programming languages provide constructs to express dynamic, fine-grained parallelism. Such languages include data-parallel languages such as Nesl [2] and HPF <ref> [21] </ref>, as well as control-parallel languages such as ID [1], Cilk [5], CC++ [12], Sisal [17], Pro-teus [28], and C with light-weight thread libraries [25, 30].
Reference: [22] <author> W. E. Hseih, P. Wang, and W. E. Weihl. </author> <title> Computation migration: enhancing locality for distributed memory parallel systems. </title> <booktitle> In Proc. Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Francisco, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This paper appears in the Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), June 18-21, 1997, Las Vegas. Several systems providing dynamic parallelism have been implemented with efficient runtime schedulers <ref> [7, 11, 13, 18, 19, 22, 23, 33, 34, 35] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [23] <author> S. F. Hummel and E. Schonberg. </author> <title> Low-overhead scheduling of nested parallelsim. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 35(5-6):743-65, </volume> <year> 1991. </year>
Reference-contexts: This paper appears in the Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), June 18-21, 1997, Las Vegas. Several systems providing dynamic parallelism have been implemented with efficient runtime schedulers <ref> [7, 11, 13, 18, 19, 22, 23, 33, 34, 35] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration. <p> Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 14, 19, 23, 32, 36] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n). <p> On the other hand, scheduling the outer parallelism would allocate space for the p branches at the top level, with each processor executing a subtree serially. Hence we use our algorithm without the delay to estimate the memory requirements of previous techniques like <ref> [14, 23] </ref>, which schedule the outer parallelism with higher priority.
Reference: [24] <author> S. F. Hummel, E. Schonberg, and L. E. Flynn. </author> <title> Factoring: a method for scheduling parallel loops. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 90-101, </pages> <month> Aug </month> <year> 1992. </year>
Reference-contexts: These results show that delaying big allocations significantly changes the order of execution of the threads, and results in much lower memory usage, especially as the number of processors increases. 9 into fixed-size chunks. A dynamic, decreasing-size chunking scheme such as <ref> [24, 26, 38] </ref> can be used instead. We are considering ways of automatically introducing such coarsening at runtime through the scheduling algorithm; for example, by allowing the execution order to differ to a limited extent from the order dictated by the 1df-numbers.
Reference: [25] <author> IEEE. </author> <title> Threads extension for portable operating systems (draft 6), </title> <month> Feb </month> <year> 1985. </year> <month> 10 </month>
Reference-contexts: Such languages include data-parallel languages such as Nesl [2] and HPF [21], as well as control-parallel languages such as ID [1], Cilk [5], CC++ [12], Sisal [17], Pro-teus [28], and C with light-weight thread libraries <ref> [25, 30] </ref>. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. The language implementation is responsible for scheduling this parallelism onto the processors.
Reference: [26] <author> C. D. Polychronopoulos; D.J. Kuck. </author> <title> Guided self--scheduling: a practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-39, </volume> <month> Dec </month> <year> 1987. </year>
Reference-contexts: These results show that delaying big allocations significantly changes the order of execution of the threads, and results in much lower memory usage, especially as the number of processors increases. 9 into fixed-size chunks. A dynamic, decreasing-size chunking scheme such as <ref> [24, 26, 38] </ref> can be used instead. We are considering ways of automatically introducing such coarsening at runtime through the scheduling algorithm; for example, by allowing the execution order to differ to a limited extent from the order dictated by the 1df-numbers.
Reference: [27] <author> J. M. Mellor-Crummey. </author> <title> Concurrent queues: Practical Fetch-and- algorithms. </title> <type> Technical Report 229, </type> <institution> University of Rochester, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: The ready-queue is implemented as a simple, singly-linked list. The in-queue and out-queue, which are accessed by the scheduler and the workers, are required to support concurrent enqueue and dequeue operations. They are implemented using variants of lock-free algorithms based on atomic fetch-and primitives <ref> [27] </ref>. The parallel programs executed using this system have been explicitly hand-coded in the continuation-passing style, similar to the code generated by the Cilk preprocessor 5 [5]. Each continuation points to a C function representing the next computation of a thread, and a structure containing all its arguments.
Reference: [28] <author> P. H. Mills, L. S. Nyland, J. F. Prins, J. H. Reif, and R. A. Wagner. </author> <title> Prototyping parallel and distributed programs in Proteus. </title> <type> Technical Report UNC-CH TR90-041, </type> <institution> Computer Science Department, University of North Carolina, </institution> <year> 1990. </year>
Reference-contexts: 1 Introduction Many of today's high level parallel programming languages provide constructs to express dynamic, fine-grained parallelism. Such languages include data-parallel languages such as Nesl [2] and HPF [21], as well as control-parallel languages such as ID [1], Cilk [5], CC++ [12], Sisal [17], Pro-teus <ref> [28] </ref>, and C with light-weight thread libraries [25, 30]. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. The language implementation is responsible for scheduling this parallelism onto the processors.
Reference: [29] <author> G. J. Narlikar and G. E. Blelloch. </author> <title> A framework for space and time efficient scheduling of parallelism. </title> <type> Technical Report CMU-CS-96-197, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <year> 1996. </year>
Reference-contexts: Thus processors can perform fast, non-blocking accesses to these queues. In this paper we use a serial algorithm to move threads between the buffers and the ready-queue. Elsewhere we discuss how this can be done in parallel <ref> [29] </ref>. and the scheduling queues. The in-queue and out-queue are FIFOs, whereas the ready-queue allows insertions and deletions of intermediate nodes. Threads in the ready-queue are always stored in increasing order of the 1df-numbers of their leading nodes (the first nodes to be executed when the thread gets scheduled). <p> Our scheduling algorithm executes this computation on p processors in O (W=p + S a =p + D) timesteps. Our time bounds do not include the cost of the scheduler. We have described a serial scheduler in this paper; however, the scheduler can be parallelized as described in <ref> [29] </ref>, and its time and space costs can be included to execute a schedule in O (W=p+S a =p+D log p) time and S 1 +O (pD log p) space. Such a parallel implementation is important when the number of processors is large, to ensure the scalability of the scheduler. <p> The results show that the percentage of time spent waiting for threads to appear in the out-queue increases as the number of processors increases (since we use a serial scheduler). A parallel implementation of the scheduler, such as one described in <ref> [29] </ref>, will be more efficient for a larger number of processors. matrix multiplication. "Serial work" is the time taken by a single processor executing the equivalent serial C program. For ideal speedups, all the other components would be zero. <p> A more detailed specification of the theoretical framework on which the scheduling algorithm is based, along with a description of an efficient parallelized scheduler, can be found in <ref> [29] </ref>. We are currently considering methods to further improve the scheduling algorithm, particularly to provide better support for fine-grained computations.
Reference: [30] <author> M. L. Powell, S. R. Kleiman, S. Barton, D. Shah, D. Stein, and M. Weeks. </author> <title> SunOS multi-thread architecture. </title> <booktitle> In Proc. USENIX COnference, </booktitle> <year> 1991. </year>
Reference-contexts: Such languages include data-parallel languages such as Nesl [2] and HPF [21], as well as control-parallel languages such as ID [1], Cilk [5], CC++ [12], Sisal [17], Pro-teus [28], and C with light-weight thread libraries <ref> [25, 30] </ref>. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. The language implementation is responsible for scheduling this parallelism onto the processors.
Reference: [31] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: This algorithm by Quinlan <ref> [31] </ref> builds a decision tree from a set of training examples in a top-down manner, using a recursive divide-and-conquer strategy.
Reference: [32] <author> Jr. R. H. Halstead. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <year> 1985. </year>
Reference-contexts: However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration. In an attempt to expose a sufficient degree of parallelism to keep all processors busy, schedulers often create many more parallel threads than necessary, leading to excessive memory usage <ref> [16, 32, 36] </ref>. <p> Many researchers have addressed this problem in the past. Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism <ref> [10, 16, 32, 36] </ref>, and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs [6, 7, 8, 9]. <p> Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 14, 19, 23, 32, 36] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n).
Reference: [33] <author> M. C. Rinard, D. J. Scales, and M. S. Lam. </author> <title> Jade: A high-level, machine-independent language for parallel programming. </title> <booktitle> IEEE Computer, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: This paper appears in the Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), June 18-21, 1997, Las Vegas. Several systems providing dynamic parallelism have been implemented with efficient runtime schedulers <ref> [7, 11, 13, 18, 19, 22, 23, 33, 34, 35] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [34] <author> A. Rogers, M. Carlisle, J. Reppy, and L. Hendren. </author> <title> Supporting dynamic data structures on distributed memory machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17(2) </volume> <pages> 233-263, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: This paper appears in the Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), June 18-21, 1997, Las Vegas. Several systems providing dynamic parallelism have been implemented with efficient runtime schedulers <ref> [7, 11, 13, 18, 19, 22, 23, 33, 34, 35] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [35] <author> R.S.Nikhil. Cid: </author> <title> A parallel, shared-memory c for distributed memory machines. </title> <booktitle> In Proc. 7th. Ann. Wkshp. on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 376-390, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: This paper appears in the Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), June 18-21, 1997, Las Vegas. Several systems providing dynamic parallelism have been implemented with efficient runtime schedulers <ref> [7, 11, 13, 18, 19, 22, 23, 33, 34, 35] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [36] <author> C. A. Rugguero and J. Sargeant. </author> <title> Control of parallelism in the manchester dataflow machine. </title> <booktitle> In Functional Programming Languages and Computer Architecture, volume 174 of Lecture Notes in Computer Science, </booktitle> <pages> pages 1-15. </pages> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration. In an attempt to expose a sufficient degree of parallelism to keep all processors busy, schedulers often create many more parallel threads than necessary, leading to excessive memory usage <ref> [16, 32, 36] </ref>. <p> Many researchers have addressed this problem in the past. Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism <ref> [10, 16, 32, 36] </ref>, and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs [6, 7, 8, 9]. <p> Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 14, 19, 23, 32, 36] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n).
Reference: [37] <author> V. Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Nu-merische Mathematik, </journal> <volume> 13 </volume> <pages> 354-356, </pages> <year> 1969. </year>
Reference-contexts: The DAG for this algorithm is very similar to that of the blocked recursive matrix multiply, but performs only O (n 2:807 ) work and makes seven recursive calls at each step <ref> [37] </ref>. Once again, a simple serial matrix multiply is used at the leaves of the recursion tree. The sizes of matrices multiplied were the same as for the previous program. * Fast multipole method (FMM).
Reference: [38] <author> T. H. Tzen and L. M. Ni. </author> <title> Trapezoid self-scheduling: a practical scheduling scheme for parallel compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 87-98, </pages> <month> Jan </month> <year> 1993. </year>
Reference-contexts: These results show that delaying big allocations significantly changes the order of execution of the threads, and results in much lower memory usage, especially as the number of processors increases. 9 into fixed-size chunks. A dynamic, decreasing-size chunking scheme such as <ref> [24, 26, 38] </ref> can be used instead. We are considering ways of automatically introducing such coarsening at runtime through the scheduling algorithm; for example, by allowing the execution order to differ to a limited extent from the order dictated by the 1df-numbers.
References-found: 38

