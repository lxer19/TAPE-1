URL: ftp://ftp.cse.unsw.edu.au/pub/doc/papers/UNSW/9405.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/research/tr.html
Root-URL: http://www.cse.unsw.edu.au
Title: Teams of Learning Machines  
Author: Sanjay Jain and Arun Sharma 
Affiliation: SCHOOL OF COMPUTER SCIENCE AND ENGINEERING THE UNIVERSITY OF NEW SOUTH WALES  
Note: On Aggregating  
Abstract: SCS&E Report 9405 March, 1994 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. M. Barzdin. </author> <title> Two theorems on the limiting synthesis of functions. In Theory of Algorithms and Programs, Latvian State University, </title> <journal> Riga, </journal> <volume> 210 </volume> <pages> 82-88, </pages> <year> 1974. </year> <note> In Russian. 22 </note>
Reference-contexts: Behaviorally Correct Function Identification Definition 5 [8] M Bc-identifies f (read: f 2 Bc (M)) () ( 1 define the class Bc = fS R j (9M)[S Bc (M)]g. The following proposition summarizes the relationship between the various function learning criteria. Proposition 1 <ref> [8, 1] </ref> Fin ae Ex ae Bc. 2.4 Language Learning A text T for a language L is a mapping from N into (N [ f#g) such that L is the set of natural numbers in the range of T .
Reference: [2] <author> J. M. Barzdin and K. Podnieks. </author> <title> The theory of inductive inference. </title> <booktitle> In Mathematical Foundations of Computer Science, </booktitle> <year> 1973. </year>
Reference-contexts: This notion was studied by Osherson and Weinstein [22] and by Case [5]. It should be noted that in the context of function learning, vacillatory identification turns out to be the same as identification in the limit. This was first shown by Barzdin and Podnieks <ref> [2] </ref> (see also Case and Smith [8]). Let n be a positive integer. A learning machine M is said to TxtFex n -identify a language L just in case M, fed any text for L, converges in the limit to a finite set, with cardinality n, of grammars for L. <p> Thus, M (T ) finitely converges and, for large enough n, M (T [n]) is a grammar for L. (b) For team function learning, we know that Team 1 2 Ex Ex 6= ; [27]. Also, since Fex = Ex <ref> [2, 8] </ref>, we have Team 1 2 Fex Fex 6= ;. Let S 2 (Team 1 2 Fex Fex). Now, it is easy to verify that the collection of single valued total languages represented by each function in S witnesses Team 1 2 TxtFex fl TxtFex fl 6= ;.
Reference: [3] <author> M. Blum. </author> <title> A machine independent theory of the complexity of recursive functions. </title> <journal> Journal of the ACM, </journal> <volume> 14 </volume> <pages> 322-336, </pages> <year> 1967. </year>
Reference-contexts: The letter, p, in some contexts, with or without decorations, ranges over programs; in other contexts p ranges over total functions with its range being construed as programs. By we denote an arbitrary fixed Blum complexity measure <ref> [3, 14] </ref> for the '-system. By W i we denote domain (' i ). W i is, then, the r.e. set/language ( N ) accepted (or equivalently, generated) by the '-program i. Symbol E will denote the set of all r.e. languages.
Reference: [4] <author> J. </author> <title> Case. Periodicity in generations of automata. </title> <journal> Mathematical Systems Theory, </journal> <volume> 8 </volume> <pages> 15-32, </pages> <year> 1974. </year>
Reference-contexts: This result, Theorem 1 (a) below, appeared in [15] and can also easily be argued from a related result of Freivalds [12] about probabilistic finite identification. Theorem 1 (b) shows that 2=3 is the cut-off point for aggregation of Fin-identification; a diagonalization argument using the operator recursion theorem <ref> [4] </ref> suffices to establish this latter result. Theorem 1 [28, 15] (a) (8m; n 2 N + j m=n &gt; 2=3)[Team m n Fin = Fin]. (b) Fin ae Team 2 3 Fin. <p> We now proceed formally. By the (i + 1) fl j-ary recursion theorem <ref> [4] </ref> there exist grammars k 1 ; k 2 ; : : : ; k (i+1)flj such that the languages W k s may be described as follows. <p> Suppose by way of contradiction that M TxtFex 2 - identifies L. We then show that there exists a language in L that M fails to TxtFex 2 - identify. The description of this witness proceeds in stages and uses the operator recursion theorem <ref> [4] </ref>. The construction is somewhat on the lines of the diagonalization argument presented in our proof of Theorem 8 (a). We give an informal description of the idea first. At each Stage s, the construction makes use of initial sequence oe s .
Reference: [5] <author> J. </author> <title> Case. The power of vacillation. </title> <editor> In D. Haussler and L. Pitt, editors, </editor> <booktitle> Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 133-142. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1988. </year> <note> Expanded in [6]. </note>
Reference-contexts: We next consider vacillatory identification of languages from texts in which a machine is required to converge to a finite set of grammars. This notion was studied by Osherson and Weinstein [22] and by Case <ref> [5] </ref>. It should be noted that in the context of function learning, vacillatory identification turns out to be the same as identification in the limit. This was first shown by Barzdin and Podnieks [2] (see also Case and Smith [8]). Let n be a positive integer. <p> We define the class TxtBc = fL E j (9M)[L TxtBc (M)]g. Vacillatory Language Identification We now introduce the notion of a learning machine finitely converging on a text <ref> [5] </ref>. Let M be a learning machine and T be a text. M (T ) finitely-converges (written: M (T )+) () fM (oe) j oe ae T g is finite, otherwise we say that M (T ) finitely-diverges (written: M (T )*). <p> M (T ) finitely-converges (written: M (T )+) () fM (oe) j oe ae T g is finite, otherwise we say that M (T ) finitely-diverges (written: M (T )*). If M (T )+, then M (T ) is defined = fi j ( 1 Definition 9 <ref> [22, 5] </ref> Let b 2 N + [ fflg. M TxtFex b identifies L (read: L 2 TxtFex b (M)) () (8 texts T for L)(9P j card (P ) b ^ (8i 2 P )[W i = L])[M (T )+ ^ M (T ) = P ]. <p> We define the class TxtFex b = fL E j (9M)[L TxtFex b (M)]g. The following proposition summarizes the relationship between the various language learning criteria. Proposition 2 <ref> [22, 7, 5] </ref> TxtFin ae TxtEx = TxtFex 1 ae TxtFex 2 ae ae TxtFex fl ae TxtBc. 2.6 Team Learning A team of learning machines is essentially a multiset of learning machines. Definition 10 introduces team learning of functions and Definition 11 introduces team learning of lan guages.
Reference: [6] <author> J. </author> <title> Case. The power of vacillation in language learning. </title> <type> Technical Report 93-08, </type> <institution> University of Delaware, </institution> <year> 1992. </year> <note> Expands on [5]; journal article under review. </note>
Reference: [7] <author> J. Case and C. Lynes. </author> <title> Machine inductive inference and language identification. </title> <editor> In M. Nielsen and E. M. Schmidt, editors, </editor> <booktitle> Proceedings of the 9th International Colloquium on Automata, Languages and Programming, </booktitle> <volume> volume 140, </volume> <pages> pages 107-115. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1982. </year>
Reference-contexts: A learning machine M is said to TxtEx identify a language L just in case M, fed any text for L, converges to a correct grammar for L. This is essentially the seminal notion of identification in the limit introduced by Gold [13] (see also Case and Lynes <ref> [7] </ref> and Osherson and Weinstein [22]). A learning machine M is said to TxtBc-identify L just in case M, fed any text for L, outputs an infinite sequence of grammars such that after a finite number of incorrect 1 guesses, M outputs only grammars for L. <p> A learning machine M is said to TxtBc-identify L just in case M, fed any text for L, outputs an infinite sequence of grammars such that after a finite number of incorrect 1 guesses, M outputs only grammars for L. This criterion was first studied by Case and Lynes <ref> [7] </ref> and Osherson and Weinstein [22], and is also referred to as "extensional" identification. Osherson, Stob, and Weinstein [20] first observed that for TxtEx-identification, a team can be aggregated if its success ratio is greater than 2=3. <p> We define the class TxtEx = fL E j (9M)[L TxtEx (M)]g. Behaviorally Correct Language Identification Definition 8 <ref> [22, 7] </ref> M TxtBc-identifies L (read: L 2 TxtBc (M)) () (8 texts T for L) ( 8 n)[W M (T [n]) = L]. We define the class TxtBc = fL E j (9M)[L TxtBc (M)]g. <p> We define the class TxtFex b = fL E j (9M)[L TxtFex b (M)]g. The following proposition summarizes the relationship between the various language learning criteria. Proposition 2 <ref> [22, 7, 5] </ref> TxtFin ae TxtEx = TxtFex 1 ae TxtFex 2 ae ae TxtFex fl ae TxtBc. 2.6 Team Learning A team of learning machines is essentially a multiset of learning machines. Definition 10 introduces team learning of functions and Definition 11 introduces team learning of lan guages.
Reference: [8] <author> J. Case and C. Smith. </author> <title> Comparison of identification criteria for machine inductive inference. </title> <journal> Theoretical Computer Science, </journal> <volume> 25 </volume> <pages> 193-220, </pages> <year> 1983. </year>
Reference-contexts: It should be noted that in the context of function learning, vacillatory identification turns out to be the same as identification in the limit. This was first shown by Barzdin and Podnieks [2] (see also Case and Smith <ref> [8] </ref>). Let n be a positive integer. A learning machine M is said to TxtFex n -identify a language L just in case M, fed any text for L, converges in the limit to a finite set, with cardinality n, of grammars for L. <p> Function Identification in the Limit Definition 4 [13] M Ex-identifies f (read: f 2 Ex (M)) () (9i j ' i = f ) ( 1 n)[M (f [n]) = i]. We define the class Ex = fS R j (9M)[S Ex (M)]g. Behaviorally Correct Function Identification Definition 5 <ref> [8] </ref> M Bc-identifies f (read: f 2 Bc (M)) () ( 1 define the class Bc = fS R j (9M)[S Bc (M)]g. The following proposition summarizes the relationship between the various function learning criteria. <p> Behaviorally Correct Function Identification Definition 5 [8] M Bc-identifies f (read: f 2 Bc (M)) () ( 1 define the class Bc = fS R j (9M)[S Bc (M)]g. The following proposition summarizes the relationship between the various function learning criteria. Proposition 1 <ref> [8, 1] </ref> Fin ae Ex ae Bc. 2.4 Language Learning A text T for a language L is a mapping from N into (N [ f#g) such that L is the set of natural numbers in the range of T . <p> Thus, M (T ) finitely converges and, for large enough n, M (T [n]) is a grammar for L. (b) For team function learning, we know that Team 1 2 Ex Ex 6= ; [27]. Also, since Fex = Ex <ref> [2, 8] </ref>, we have Team 1 2 Fex Fex 6= ;. Let S 2 (Team 1 2 Fex Fex). Now, it is easy to verify that the collection of single valued total languages represented by each function in S witnesses Team 1 2 TxtFex fl TxtFex fl 6= ;. <p> Essentially the proof of Team m n TxtBc TxtBc can also be used to show that Team m n InfBc InfBc. Theorem 14 can be proved using techniques similar to that used by Case and Smith to show that Fex = Ex <ref> [8] </ref>. Theorem 14 (8b 2 N + [ fflg)[InfFex b = InfEx]. Hence, Theorem 13 holds for vacillatory identification from informants, too. 4 Conclusion Clearly, aggregation issues for for TxtFex b , where b 6= fl ^ b &gt; 2, are open.
Reference: [9] <author> R. P. Daley. </author> <title> Inductive inference hierarchies: Probabilistic vs pluralistic. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 215 </volume> <pages> 73-82, </pages> <year> 1986. </year>
Reference: [10] <author> R. P. Daley, B. Kalyanasundaram, and M. Velauthapillai. </author> <title> Breaking the probability 1/2 barrier in fin-type learning. </title> <booktitle> In Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 203-217. </pages> <editor> A. C. M. </editor> <publisher> Press, </publisher> <year> 1992. </year>
Reference: [11] <author> R. P. Daley, L. Pitt, M. Velauthapillai, and T. </author> <title> Will. Relations between probabilistic and team one-shot learners. </title> <editor> In L. Valiant and M. Warmuth, editors, </editor> <booktitle> Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 228-239. </pages> <publisher> Morgan Kauf-mann Publishers, Inc., </publisher> <year> 1991. </year>
Reference: [12] <author> R. Freivalds. </author> <title> Functions computable in the limit by probabilistic machines. </title> <booktitle> Mathematical Foundations of Computer Science, </booktitle> <year> 1975. </year>
Reference-contexts: For finite function identification, Fin, it was reported in [15] that a team can be aggregated if the success ratio of the team is greater than 2=3 (this result can also be argued from a result of Freivalds <ref> [12] </ref> about probabilistic finite function identification). The present paper describes aggregation results about language identification from positive data. The main results are in the context of vacillatory identification. To facilitate discussion of these results, we informally present some preliminaries from theory of language learning next. <p> For finite function identification, aggregation takes place at success ratios greater than 2=3. This result, Theorem 1 (a) below, appeared in [15] and can also easily be argued from a related result of Freivalds <ref> [12] </ref> about probabilistic finite identification. Theorem 1 (b) shows that 2=3 is the cut-off point for aggregation of Fin-identification; a diagonalization argument using the operator recursion theorem [4] suffices to establish this latter result.
Reference: [13] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: A learning machine M is said to TxtEx identify a language L just in case M, fed any text for L, converges to a correct grammar for L. This is essentially the seminal notion of identification in the limit introduced by Gold <ref> [13] </ref> (see also Case and Lynes [7] and Osherson and Weinstein [22]). <p> Clearly, f [0] denotes the empty segment. SEG denotes the set of all finite initial segments, ff [n] j f 2 R ^ n 2 N g. Definition 1 <ref> [13] </ref> A function learning machine is an algorithmic device which computes a mapping from SEG into N . We now consider language learning machines. A sequence oe is a mapping from an initial segment of N into (N [ f#g). <p> We define the class Fin = fS R j (9M)[S Fin (M)]g. Function Identification in the Limit Definition 4 <ref> [13] </ref> M Ex-identifies f (read: f 2 Ex (M)) () (9i j ' i = f ) ( 1 n)[M (f [n]) = i]. We define the class Ex = fS R j (9M)[S Ex (M)]g. <p> We define the class TxtFin = fL E j (9M)[L TxtFin (M)]g. 5 2.5 Language Identification in the Limit Definition 7 <ref> [13] </ref> M TxtEx-identifies L (read: L 2 TxtEx (M)) () (8 texts T for L) (9i j W i = L) ( 8 n)[M (T [n]) = i]. We define the class TxtEx = fL E j (9M)[L TxtEx (M)]g. <p> Identification from texts is an abstraction of learning from positive data. Similarly, learning from both positive and negative data can be abstracted as identification from informants. The notion of informants, defined below, was first considered by Gold <ref> [13] </ref>. 20 Definition 18 A text I is called an informant for a language L just in case content (I) = fhx; 1i j x 2 Lg [ fhx; 0i j x 62 Lg. The next definition formalizes identification in the limit from informants.
Reference: [14] <author> J. Hopcroft and J. Ullman. </author> <title> Introduction to Automata Theory Languages and Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1979. </year> <month> 23 </month>
Reference-contexts: Hence, languages may be construed as subsets of N . A grammar for a language is a set of rules that accepts (or equivalently, generates <ref> [14] </ref>) the language. Essentially, any computer program may be viewed as a grammar. Languages for which a grammar exists are called recursively enumerable. A text for a language L is any infinite sequence that lists all and only the elements of L; repetitions are permitted. <p> The letter, p, in some contexts, with or without decorations, ranges over programs; in other contexts p ranges over total functions with its range being construed as programs. By we denote an arbitrary fixed Blum complexity measure <ref> [3, 14] </ref> for the '-system. By W i we denote domain (' i ). W i is, then, the r.e. set/language ( N ) accepted (or equivalently, generated) by the '-program i. Symbol E will denote the set of all r.e. languages.
Reference: [15] <author> S. Jain and A. Sharma. </author> <title> Finite learning by a team. </title> <editor> In M. Fulk and J. </editor> <booktitle> Case, edi-tors, Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <address> Rochester, New York, </address> <pages> pages 163-177. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: For Ex and Bc, Pitt and Smith [24] showed that a team can be aggregated into a single machine if the success ratio of the team is greater than 1=2. For finite function identification, Fin, it was reported in <ref> [15] </ref> that a team can be aggregated if the success ratio of the team is greater than 2=3 (this result can also be argued from a result of Freivalds [12] about probabilistic finite function identification). The present paper describes aggregation results about language identification from positive data. <p> For finite function identification, aggregation takes place at success ratios greater than 2=3. This result, Theorem 1 (a) below, appeared in <ref> [15] </ref> and can also easily be argued from a related result of Freivalds [12] about probabilistic finite identification. Theorem 1 (b) shows that 2=3 is the cut-off point for aggregation of Fin-identification; a diagonalization argument using the operator recursion theorem [4] suffices to establish this latter result. <p> Theorem 1 (b) shows that 2=3 is the cut-off point for aggregation of Fin-identification; a diagonalization argument using the operator recursion theorem [4] suffices to establish this latter result. Theorem 1 <ref> [28, 15] </ref> (a) (8m; n 2 N + j m=n &gt; 2=3)[Team m n Fin = Fin]. (b) Fin ae Team 2 3 Fin.
Reference: [16] <author> S. Jain and A. Sharma. </author> <title> Language learning by a team. </title> <editor> In M. S. Paterson, editor, </editor> <booktitle> Proceedings of the 17th International Colloquium on Automata, Languages and Programming, </booktitle> <pages> pages 153-166. </pages> <publisher> Springer-Verlag, </publisher> <month> July </month> <year> 1990. </year>
Reference-contexts: For language learning, the result is known for TxtEx-identification and TxtBc-identification. It was shown by Osherson, Stob, and Weinstein [20] that aggregation for TxtEx takes place at success ratios greater than 2=3, and 2=3 is also the cut-off point for aggregation of TxtEx-identification (see also <ref> [16, 18, 17] </ref> for extension of this result to anomalies in the final grammar). Theorem 3 (a) (8m; n 2 N + j m=n &gt; 2=3)[Team m n TxtEx = TxtEx] (b) TxtEx ae Team 2 3 TxtEx.
Reference: [17] <author> S. Jain and A. Sharma. </author> <title> Computational limits on team identification of languages. </title> <type> Technical Report 9301, </type> <institution> School of Computer Science and Engineering; University of New S outh Wales, </institution> <year> 1993. </year>
Reference-contexts: For language learning, the result is known for TxtEx-identification and TxtBc-identification. It was shown by Osherson, Stob, and Weinstein [20] that aggregation for TxtEx takes place at success ratios greater than 2=3, and 2=3 is also the cut-off point for aggregation of TxtEx-identification (see also <ref> [16, 18, 17] </ref> for extension of this result to anomalies in the final grammar). Theorem 3 (a) (8m; n 2 N + j m=n &gt; 2=3)[Team m n TxtEx = TxtEx] (b) TxtEx ae Team 2 3 TxtEx.
Reference: [18] <author> S. Jain and A. Sharma. </author> <title> Probability is more powerful than team for language identification. </title> <booktitle> In Proceedings of the Sixth Annual Conference on Computational Learning Theory, </booktitle> <address> Santa Cruz, California, </address> <pages> pages 192-198. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1993. </year>
Reference-contexts: For language learning, the result is known for TxtEx-identification and TxtBc-identification. It was shown by Osherson, Stob, and Weinstein [20] that aggregation for TxtEx takes place at success ratios greater than 2=3, and 2=3 is also the cut-off point for aggregation of TxtEx-identification (see also <ref> [16, 18, 17] </ref> for extension of this result to anomalies in the final grammar). Theorem 3 (a) (8m; n 2 N + j m=n &gt; 2=3)[Team m n TxtEx = TxtEx] (b) TxtEx ae Team 2 3 TxtEx.
Reference: [19] <author> M. Machtey and P. Young. </author> <title> An Introduction to the General Theory of Algorithms. </title> <publisher> North Holland, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Similarly, we can define h; : : : ; i for encoding multiple tuples of natural numbers onto N . By ' we denote a fixed acceptable programming system for the partial computable functions: N ! N <ref> [25, 26, 19] </ref>. By ' i we denote the partial computable function computed by program i in the '-system. The letter, p, in some contexts, with or without decorations, ranges over programs; in other contexts p ranges over total functions with its range being construed as programs.
Reference: [20] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Aggregating inductive expertise. </title> <journal> Information and Control, </journal> <volume> 70 </volume> <pages> 69-95, </pages> <year> 1986. </year>
Reference-contexts: This criterion was first studied by Case and Lynes [7] and Osherson and Weinstein [22], and is also referred to as "extensional" identification. Osherson, Stob, and Weinstein <ref> [20] </ref> first observed that for TxtEx-identification, a team can be aggregated if its success ratio is greater than 2=3. Hence, in matters of aggregation, identification in the limit of languages from positive data turns out to be similar to finite function identification. <p> Theorem 2 Let I 2 fEx; Bcg. (a) (8m; n 2 N + j m=n &gt; 1=2)[Team m n I = I] (b) I ae Team 1 2 I. For language learning, the result is known for TxtEx-identification and TxtBc-identification. It was shown by Osherson, Stob, and Weinstein <ref> [20] </ref> that aggregation for TxtEx takes place at success ratios greater than 2=3, and 2=3 is also the cut-off point for aggregation of TxtEx-identification (see also [16, 18, 17] for extension of this result to anomalies in the final grammar).
Reference: [21] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems that Learn, An Introduction to Learning Theory for Cognitive and Computer Scientists. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: Definition 10 introduces team learning of functions and Definition 11 introduces team learning of lan guages. Definition 10 <ref> [27, 21] </ref> Let I 2 fFin; Ex; Bcg and let m; n 2 N + . (a) A team of n machines, M 1 ; M 2 ; : : : ; M n , is said to Team m n I-identify f (writ ten: f 2 Team m n I
Reference: [22] <author> D. Osherson and S. Weinstein. </author> <title> Criteria of language learning. </title> <journal> Information and Control, </journal> <volume> 52 </volume> <pages> 123-138, </pages> <year> 1982. </year>
Reference-contexts: This is essentially the seminal notion of identification in the limit introduced by Gold [13] (see also Case and Lynes [7] and Osherson and Weinstein <ref> [22] </ref>). A learning machine M is said to TxtBc-identify L just in case M, fed any text for L, outputs an infinite sequence of grammars such that after a finite number of incorrect 1 guesses, M outputs only grammars for L. <p> This criterion was first studied by Case and Lynes [7] and Osherson and Weinstein <ref> [22] </ref>, and is also referred to as "extensional" identification. Osherson, Stob, and Weinstein [20] first observed that for TxtEx-identification, a team can be aggregated if its success ratio is greater than 2=3. <p> Thus, TxtFin-identification shows similar behavior as TxtEx-identification and finite function identification so far as aggregation is concerned. We next consider vacillatory identification of languages from texts in which a machine is required to converge to a finite set of grammars. This notion was studied by Osherson and Weinstein <ref> [22] </ref> and by Case [5]. It should be noted that in the context of function learning, vacillatory identification turns out to be the same as identification in the limit. This was first shown by Barzdin and Podnieks [2] (see also Case and Smith [8]). Let n be a positive integer. <p> We define the class TxtEx = fL E j (9M)[L TxtEx (M)]g. Behaviorally Correct Language Identification Definition 8 <ref> [22, 7] </ref> M TxtBc-identifies L (read: L 2 TxtBc (M)) () (8 texts T for L) ( 8 n)[W M (T [n]) = L]. We define the class TxtBc = fL E j (9M)[L TxtBc (M)]g. <p> M (T ) finitely-converges (written: M (T )+) () fM (oe) j oe ae T g is finite, otherwise we say that M (T ) finitely-diverges (written: M (T )*). If M (T )+, then M (T ) is defined = fi j ( 1 Definition 9 <ref> [22, 5] </ref> Let b 2 N + [ fflg. M TxtFex b identifies L (read: L 2 TxtFex b (M)) () (8 texts T for L)(9P j card (P ) b ^ (8i 2 P )[W i = L])[M (T )+ ^ M (T ) = P ]. <p> We define the class TxtFex b = fL E j (9M)[L TxtFex b (M)]g. The following proposition summarizes the relationship between the various language learning criteria. Proposition 2 <ref> [22, 7, 5] </ref> TxtFin ae TxtEx = TxtFex 1 ae TxtFex 2 ae ae TxtFex fl ae TxtBc. 2.6 Team Learning A team of learning machines is essentially a multiset of learning machines. Definition 10 introduces team learning of functions and Definition 11 introduces team learning of lan guages.
Reference: [23] <author> L. Pitt. </author> <title> A characterization of probabilistic inference. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1984. </year>
Reference-contexts: Hence, in matters of aggregation, identification in the limit of languages from positive data turns out to be similar to finite function identification. On the other hand, for TxtBc-identification, a result from Pitt <ref> [23] </ref> can easily be used to show that a team can be aggregated if its success ratio is greater than 1=2. Thus, TxtEx and TxtBc exhibit different behavior with respect to aggregation. We now present two more criteria of successful language learning, namely, finite identification and vacillatory identification. <p> Theorem 3 (a) (8m; n 2 N + j m=n &gt; 2=3)[Team m n TxtEx = TxtEx] (b) TxtEx ae Team 2 3 TxtEx. Using a result from Pitt <ref> [23] </ref>, it can be shown that aggregation for TxtBc takes place at success ratios greater than 1=2. This is Theorem 4 (a) below.
Reference: [24] <author> L. Pitt and C. Smith. </author> <title> Probability and plurality for aggregations of learning machines. </title> <journal> Information and Computation, </journal> <volume> 77 </volume> <pages> 77-92, </pages> <year> 1988. </year>
Reference-contexts: For the problem of learning recursive functions from graphs, the answer is known for the three popularly investigated criteria of success, namely, Fin (finite identification), Ex (identification in the limit) and Bc (behaviorally correct identification). For Ex and Bc, Pitt and Smith <ref> [24] </ref> showed that a team can be aggregated into a single machine if the success ratio of the team is greater than 1=2. <p> Theorem 1 [28, 15] (a) (8m; n 2 N + j m=n &gt; 2=3)[Team m n Fin = Fin]. (b) Fin ae Team 2 3 Fin. Pitt and Smith <ref> [24] </ref> settled the question for function identification in the limit and behaviorally correct function identification by showing the following Theorem 2 (a) which implies that for both these criteria aggregation takes place at success ratios greater than 1=2.
Reference: [25] <author> H. Rogers. </author> <title> Godel numberings of partial recursive functions. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 23 </volume> <pages> 331-341, </pages> <year> 1958. </year>
Reference-contexts: Similarly, we can define h; : : : ; i for encoding multiple tuples of natural numbers onto N . By ' we denote a fixed acceptable programming system for the partial computable functions: N ! N <ref> [25, 26, 19] </ref>. By ' i we denote the partial computable function computed by program i in the '-system. The letter, p, in some contexts, with or without decorations, ranges over programs; in other contexts p ranges over total functions with its range being construed as programs.
Reference: [26] <author> H. Rogers. </author> <title> Theory of Recursive Functions and Effective Computability. </title> <publisher> McGraw Hill, </publisher> <address> New York, 1967. </address> <publisher> Reprinted, MIT Press 1987. </publisher>
Reference-contexts: We now proceed formally. Section 2 records the notation and describes preliminary notions and definitions from inductive inference literature. Our results are presented in Section 3. 2 Preliminaries 2.1 Notation Any unexplained recursion theoretic notation is from <ref> [26] </ref>. The symbol N denotes the set of natural numbers, f0; 1; 2; 3; : : :g. The symbol N + denotes the set of positive natural numbers, f1; 2; 3; : : :g. <p> C and S, with or without decorations, range over subsets of R. A pair hi; ji stands for an arbitrary, computable, one-to-one encoding of all pairs of natural numbers onto N 3 Decorations are subscripts, superscripts and the like. 3 <ref> [26] </ref>. Similarly, we can define h; : : : ; i for encoding multiple tuples of natural numbers onto N . By ' we denote a fixed acceptable programming system for the partial computable functions: N ! N [25, 26, 19]. <p> Similarly, we can define h; : : : ; i for encoding multiple tuples of natural numbers onto N . By ' we denote a fixed acceptable programming system for the partial computable functions: N ! N <ref> [25, 26, 19] </ref>. By ' i we denote the partial computable function computed by program i in the '-system. The letter, p, in some contexts, with or without decorations, ranges over programs; in other contexts p ranges over total functions with its range being construed as programs.
Reference: [27] <author> C. Smith. </author> <title> The power of pluralism for automatic program synthesis. </title> <journal> Journal of the ACM, </journal> <volume> 29 </volume> <pages> 1144-1165, </pages> <year> 1982. </year>
Reference-contexts: Definition 10 introduces team learning of functions and Definition 11 introduces team learning of lan guages. Definition 10 <ref> [27, 21] </ref> Let I 2 fFin; Ex; Bcg and let m; n 2 N + . (a) A team of n machines, M 1 ; M 2 ; : : : ; M n , is said to Team m n I-identify f (writ ten: f 2 Team m n I <p> Pitt and Smith [24] settled the question for function identification in the limit and behaviorally correct function identification by showing the following Theorem 2 (a) which implies that for both these criteria aggregation takes place at success ratios greater than 1=2. Theorem 2 (b), due to Smith <ref> [27] </ref>, shows that 1=2 is indeed the cut-off point. Theorem 2 Let I 2 fEx; Bcg. (a) (8m; n 2 N + j m=n &gt; 1=2)[Team m n I = I] (b) I ae Team 1 2 I. For language learning, the result is known for TxtEx-identification and TxtBc-identification. <p> Thus, M (T ) finitely converges and, for large enough n, M (T [n]) is a grammar for L. (b) For team function learning, we know that Team 1 2 Ex Ex 6= ; <ref> [27] </ref>. Also, since Fex = Ex [2, 8], we have Team 1 2 Fex Fex 6= ;. Let S 2 (Team 1 2 Fex Fex).
Reference: [28] <author> M. Velauthapillai. </author> <title> Inductive inference with bounded number of mind changes. </title> <booktitle> In Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 200-213, </pages> <year> 1989. </year>
Reference-contexts: Theorem 1 (b) shows that 2=3 is the cut-off point for aggregation of Fin-identification; a diagonalization argument using the operator recursion theorem [4] suffices to establish this latter result. Theorem 1 <ref> [28, 15] </ref> (a) (8m; n 2 N + j m=n &gt; 2=3)[Team m n Fin = Fin]. (b) Fin ae Team 2 3 Fin.
References-found: 28

