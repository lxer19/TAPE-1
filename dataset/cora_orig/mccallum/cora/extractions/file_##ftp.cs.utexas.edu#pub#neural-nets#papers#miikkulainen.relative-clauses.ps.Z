URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/miikkulainen.relative-clauses.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: 
Email: risto@cs.ucla.edu  
Title: A PDP ARCHITECTURE FOR PROCESSING SENTENCES WITH RELATIVE CLAUSES  
Author: Risto Miikkulainen 
Address: Los Angeles, CA 90024  
Affiliation: Artificial Intelligence Laboratory, Computer Science Department University of California,  
Abstract: A modular parallel distributed processing architecture for parsing, representing and paraphrasing sentences with multiple hierarchical relative clauses is presented. A lowel-level network reads the segments of the sentence word by word into partially specified case-role representations of the acts. A higher-level network combines these representations into a list of complete act representations. This internal representation stores the information conveyed by the sentence independent of its linguistic form. The information can be output in natural language in different form or style, e.g. as a sequence of simple sentences or as a complex sentence consisting of relative clauses. Generating output is independent from parsing, and what actually gets generated depends on the training of the generator modules. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Charniak. </author> <title> A neat theory of marker passing. </title> <booktitle> In Proceedings of AAAI-86, </booktitle> <publisher> Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: The advantage of this approach is that sentences with arbitrary complexity can be parsed and represented. However, processing knowledge must be hand-coded with specific examples in mind. Rules for expectations, defaults and generalizations must be explicitly programmed. The localist connectionist models <ref> [2; 20; 5; 1; 19; 7] </ref> provide more general mechanisms for inferencing and give a more plausible account of the parsing process in terms of human performance. However, these networks need to be carefully crafted for each example.
Reference: [2] <author> G. W. Cottrell and S. L. </author> <title> Small. A connectionist scheme for modelling word sense disambiguation. </title> <journal> Cognition and Brain Theory, </journal> <volume> 6(1) </volume> <pages> 89-120, </pages> <year> 1983. </year>
Reference-contexts: The advantage of this approach is that sentences with arbitrary complexity can be parsed and represented. However, processing knowledge must be hand-coded with specific examples in mind. Rules for expectations, defaults and generalizations must be explicitly programmed. The localist connectionist models <ref> [2; 20; 5; 1; 19; 7] </ref> provide more general mechanisms for inferencing and give a more plausible account of the parsing process in terms of human performance. However, these networks need to be carefully crafted for each example.
Reference: [3] <author> M. G. Dyer. </author> <title> In-Depth Understanding. </title> <publisher> MIT Press, </publisher> <year> 1983. </year>
Reference-contexts: 1 Introduction Parsing a sentence means reading the input text into an internal representation, which makes the relations of the constituents explicit. In symbolic parsing, the result is usually a semantic network structure, e.g. a conceptual dependency representation <ref> [17; 3] </ref>, or a syntactic parse tree augmented with semantic restrictions [13; 9]. The advantage of this approach is that sentences with arbitrary complexity can be parsed and represented. However, processing knowledge must be hand-coded with specific examples in mind. Rules for expectations, defaults and generalizations must be explicitly programmed.
Reference: [4] <author> J. L. Elman. </author> <title> Finding Structure in Time. </title> <type> Technical Report 8801, </type> <institution> Center for Research in Language, UCSD, </institution> <year> 1988. </year>
Reference-contexts: The representations for words which are used in similar ways become similar. Recurrent FGREP [11; 12] is an extension of the basic FGREP architecture to sequential input and output, based on <ref> [4] </ref>. A copy of the hidden layer at time step t is saved and used along with the actual input at step t + 1 as input to the hidden layer (figure 3).
Reference: [5] <author> R. H. Granger, K. P. Eiselt, and J. K. Holbrook. </author> <title> Parsing with parallelism. In Kolodner and Riesbeck, Experience, Memory and Reasoning, </title> <publisher> LEA, </publisher> <year> 1986. </year>
Reference-contexts: The advantage of this approach is that sentences with arbitrary complexity can be parsed and represented. However, processing knowledge must be hand-coded with specific examples in mind. Rules for expectations, defaults and generalizations must be explicitly programmed. The localist connectionist models <ref> [2; 20; 5; 1; 19; 7] </ref> provide more general mechanisms for inferencing and give a more plausible account of the parsing process in terms of human performance. However, these networks need to be carefully crafted for each example.
Reference: [6] <author> G. E. Hinton. </author> <title> Representing part-whole hierarchies in connectionist networks. </title> <booktitle> In Proceedings of CogSci-88, </booktitle> <publisher> LEA, </publisher> <year> 1988. </year>
Reference-contexts: The output layer of the network is divided into partitions, each representing a case role, and distributed activity patterns in the assemblies indicate the words filling these roles. Representing complex structures is problematic in the distributed approach <ref> [6; 15] </ref>. The proposed sentence processing architectures can only deal with simple, straightforward sentences. Case-role analysis is feasible only when the sentences consist of single acts, so that unique case role can be assigned for each constituent.
Reference: [7] <author> T. E. Lange and M. G. Dyer. </author> <title> High-level inferencing in a connectionist network. </title> <journal> Connection Science, </journal> <volume> 1(2), </volume> <year> 1989. </year>
Reference-contexts: The advantage of this approach is that sentences with arbitrary complexity can be parsed and represented. However, processing knowledge must be hand-coded with specific examples in mind. Rules for expectations, defaults and generalizations must be explicitly programmed. The localist connectionist models <ref> [2; 20; 5; 1; 19; 7] </ref> provide more general mechanisms for inferencing and give a more plausible account of the parsing process in terms of human performance. However, these networks need to be carefully crafted for each example.
Reference: [8] <author> J. L. McClelland and A. H. Kawamoto. </author> <title> Mechanisms of sentence processing. </title> <editor> In McClelland and Rumel-hart, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: However, these networks need to be carefully crafted for each example. The main advantage of the distributed connectionist approach <ref> [8; 18; 12] </ref> is that processing is learned from examples. Expectations about unspecified constituents arise automatically from the processing mechanism, and generalizations into new inputs result automatically from the representations. <p> The result in distributed parsing at the sentence level is e.g. an assembly-based case-role representation of the sentence <ref> [8; 10] </ref>. The output layer of the network is divided into partitions, each representing a case role, and distributed activity patterns in the assemblies indicate the words filling these roles. Representing complex structures is problematic in the distributed approach [6; 15].
Reference: [9] <author> M. C. McCord. </author> <title> Using slots and modifiers in logic grammars for natural language. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 327-367, </pages> <year> 1982. </year>
Reference-contexts: 1 Introduction Parsing a sentence means reading the input text into an internal representation, which makes the relations of the constituents explicit. In symbolic parsing, the result is usually a semantic network structure, e.g. a conceptual dependency representation [17; 3], or a syntactic parse tree augmented with semantic restrictions <ref> [13; 9] </ref>. The advantage of this approach is that sentences with arbitrary complexity can be parsed and represented. However, processing knowledge must be hand-coded with specific examples in mind. Rules for expectations, defaults and generalizations must be explicitly programmed.
Reference: [10] <author> R. Miikkulainen and M. G. Dyer. </author> <title> Encoding in put/output representations in connectionist cognitive systems. </title> <editor> In Touretzky, Hinton, and Sejnowski, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <publisher> Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: The result in distributed parsing at the sentence level is e.g. an assembly-based case-role representation of the sentence <ref> [8; 10] </ref>. The output layer of the network is divided into partitions, each representing a case role, and distributed activity patterns in the assemblies indicate the words filling these roles. Representing complex structures is problematic in the distributed approach [6; 15]. <p> Each network is a Recurrent FGREP module, i.e. a three-layer backpropagation network with sequential input or output, which develops the word representations automatically while it is learning the processing task. 2.2 Recurrent FGREP Abuilding block The FGREP mechanism (Forming Global Representations with Extended backPropagation) <ref> [10; 12] </ref> is based on a basic three-layer backward error propagation network (figure 3). The network learns the processing task by adapting the connection weights according to the standard backpropagation equations [16, pages 327-329].
Reference: [11] <author> R. Miikkulainen and M. G. Dyer. </author> <title> A modular neural network architecture for sequential paraphrasing of script-based stories. </title> <booktitle> In Proceedings of IJCNN-89, </booktitle> <year> 1989. </year>
Reference-contexts: This representation pattern as a whole is meaningful and can be claimed to code the meaning of that word. The representations for words which are used in similar ways become similar. Recurrent FGREP <ref> [11; 12] </ref> is an extension of the basic FGREP architecture to sequential input and output, based on [4]. A copy of the hidden layer at time step t is saved and used along with the actual input at step t + 1 as input to the hidden layer (figure 3).
Reference: [12] <author> R. Miikkulainen and M. G. Dyer. </author> <title> Natural Language Processing with Modular Neural Networks and Distributed Lexicon. </title> <type> Technical Report UCLA-AI-90-02, </type> <institution> Computer Science Department, UCLA, </institution> <year> 1990. </year>
Reference-contexts: However, these networks need to be carefully crafted for each example. The main advantage of the distributed connectionist approach <ref> [8; 18; 12] </ref> is that processing is learned from examples. Expectations about unspecified constituents arise automatically from the processing mechanism, and generalizations into new inputs result automatically from the representations. <p> Each network is a Recurrent FGREP module, i.e. a three-layer backpropagation network with sequential input or output, which develops the word representations automatically while it is learning the processing task. 2.2 Recurrent FGREP Abuilding block The FGREP mechanism (Forming Global Representations with Extended backPropagation) <ref> [10; 12] </ref> is based on a basic three-layer backward error propagation network (figure 3). The network learns the processing task by adapting the connection weights according to the standard backpropagation equations [16, pages 327-329]. <p> This representation pattern as a whole is meaningful and can be claimed to code the meaning of that word. The representations for words which are used in similar ways become similar. Recurrent FGREP <ref> [11; 12] </ref> is an extension of the basic FGREP architecture to sequential input and output, based on [4]. A copy of the hidden layer at time step t is saved and used along with the actual input at step t + 1 as input to the hidden layer (figure 3). <p> Certain semantic restrictions were imposed on the templates to obtain more meaningful sentences. The restrictions also create enough differences in the usage of the words, so that their representations do not become identical (see <ref> [12] </ref>). A verb could have only specified nouns as its agent and patient, listed in table 2. Sentences with two instances of the same noun were also excluded. With these restrictions, the templates generate a total of 388 sentences. All sentences were used to train the system. <p> Sentences with two instances of the same noun were also excluded. With these restrictions, the templates generate a total of 388 sentences. All sentences were used to train the system. Generalization was not studied in these experiments (for a discussion of the generalization capabilities of FGREP systems see <ref> [12] </ref>). Two different versions of the sentence generator were trained: one to produce the output as a sequence of simple sentences, and another to produce a single sentence with hierarchical relative clauses, i.e. to reproduce the input sentence. <p> Internally, the information can be represented in a parallel, canonical form, which makes all information directly accessible. In communication through narrow channels, i.e. in language, it is necessary to transform the knowledge into a sequential form <ref> [12] </ref>. Parallel dependencies in the knowledge are then coded with recursion. Generating output is seen as a task separate from parsing. Sentence generation is performed by a different module and learned separately. <p> For example, in representing The man, who helped the boy, blamed the man, who hit the girl it is crucial to indicate that the man-who-helped is the same as the man-who-blamed, but different from the man-who-hit. A possible technique for doing this has been proposed in <ref> [12] </ref>. The representation of the word could consist of two parts: the content part, which is developed by FGREP and codes the processing properties of the word, and an ID part, which is unique for each separate instance of the word.
Reference: [13] <author> F. C. N. Pereira and D. H. Warren. </author> <title> Definite clause grammars for language analysis. </title> <journal> Artificial Intelligence, </journal> <volume> 13 </volume> <pages> 231-278, </pages> <year> 1980. </year>
Reference-contexts: 1 Introduction Parsing a sentence means reading the input text into an internal representation, which makes the relations of the constituents explicit. In symbolic parsing, the result is usually a semantic network structure, e.g. a conceptual dependency representation [17; 3], or a syntactic parse tree augmented with semantic restrictions <ref> [13; 9] </ref>. The advantage of this approach is that sentences with arbitrary complexity can be parsed and represented. However, processing knowledge must be hand-coded with specific examples in mind. Rules for expectations, defaults and generalizations must be explicitly programmed.
Reference: [14] <author> J. Pollack. </author> <title> Cascaded back-propagation on dynamic connectionist networks. </title> <booktitle> In Proceedings of CogSci-87, </booktitle> <publisher> LEA, </publisher> <year> 1987. </year>
Reference-contexts: What actually gets generated depends on the connection weights of this module. It would be possible to add a higher-level decision-making network to the system, which controls the connection weight values in the sentence generator network through multiplicative connections <ref> [14] </ref>. A decision about the style, detail etc. of the paraphrase would be made by this module, and its output would assign the appropriate function to the sentence gen erator. 5 The model exhibits certain features of human per-formance.
Reference: [15] <author> J. Pollack. </author> <title> Recursive auto-associative memory. </title> <booktitle> In Proceedings of CogSci-88, </booktitle> <publisher> LEA, </publisher> <year> 1988. </year>
Reference-contexts: The output layer of the network is divided into partitions, each representing a case role, and distributed activity patterns in the assemblies indicate the words filling these roles. Representing complex structures is problematic in the distributed approach <ref> [6; 15] </ref>. The proposed sentence processing architectures can only deal with simple, straightforward sentences. Case-role analysis is feasible only when the sentences consist of single acts, so that unique case role can be assigned for each constituent.
Reference: [16] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart and McClelland, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The network learns the processing task by adapting the connection weights according to the standard backpropagation equations <ref> [16, pages 327-329] </ref>. At the same time, representations for the input data are developed at the input layer according to the error signal extended to the input layer. Input and output layers are divided into assemblies and several items are represented and modified simultaneously.
Reference: [17] <author> R. Schank and R. Abelson. </author> <title> Scripts, Plans, Goals, and Understanding. </title> <publisher> LEA, </publisher> <year> 1977. </year>
Reference-contexts: 1 Introduction Parsing a sentence means reading the input text into an internal representation, which makes the relations of the constituents explicit. In symbolic parsing, the result is usually a semantic network structure, e.g. a conceptual dependency representation <ref> [17; 3] </ref>, or a syntactic parse tree augmented with semantic restrictions [13; 9]. The advantage of this approach is that sentences with arbitrary complexity can be parsed and represented. However, processing knowledge must be hand-coded with specific examples in mind. Rules for expectations, defaults and generalizations must be explicitly programmed.
Reference: [18] <author> M. F. St. John and J. L. McClelland. </author> <title> Learning and applying contextual constraints in sentence comprehension. </title> <booktitle> In Proceedings of CogSci-88, </booktitle> <publisher> LEA, </publisher> <year> 1988. </year>
Reference-contexts: However, these networks need to be carefully crafted for each example. The main advantage of the distributed connectionist approach <ref> [8; 18; 12] </ref> is that processing is learned from examples. Expectations about unspecified constituents arise automatically from the processing mechanism, and generalizations into new inputs result automatically from the representations.
Reference: [19] <author> R. A. Sumida, M. G. Dyer, and M. Flowers. </author> <title> Integrating marker passing and connectionism for handling conceptual and structural ambiguities. </title> <booktitle> In Proceedings of CogSci-88, </booktitle> <publisher> LEA, </publisher> <year> 1988. </year>
Reference-contexts: The advantage of this approach is that sentences with arbitrary complexity can be parsed and represented. However, processing knowledge must be hand-coded with specific examples in mind. Rules for expectations, defaults and generalizations must be explicitly programmed. The localist connectionist models <ref> [2; 20; 5; 1; 19; 7] </ref> provide more general mechanisms for inferencing and give a more plausible account of the parsing process in terms of human performance. However, these networks need to be carefully crafted for each example.
Reference: [20] <author> D. L. Waltz and J. B. Pollack. </author> <title> Massively parallel parsing. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 51-74, </pages> <year> 1985. </year> <month> 6 </month>
Reference-contexts: The advantage of this approach is that sentences with arbitrary complexity can be parsed and represented. However, processing knowledge must be hand-coded with specific examples in mind. Rules for expectations, defaults and generalizations must be explicitly programmed. The localist connectionist models <ref> [2; 20; 5; 1; 19; 7] </ref> provide more general mechanisms for inferencing and give a more plausible account of the parsing process in terms of human performance. However, these networks need to be carefully crafted for each example.
References-found: 20

