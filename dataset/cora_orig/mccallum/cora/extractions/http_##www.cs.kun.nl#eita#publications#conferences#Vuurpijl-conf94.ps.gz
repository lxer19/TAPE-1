URL: http://www.cs.kun.nl/eita/publications/conferences/Vuurpijl-conf94.ps.gz
Refering-URL: http://www.cs.kun.nl/eita/publications/
Root-URL: 
Email: email: louis@cs.kun.nl  
Phone: phone: +31 80 652710, fax: +31 80 553450,  
Title: A Scalable Performance Prediction Method for Parallel Neural Network Simulations  
Author: Louis Vuurpijl, Theo Schouten and Jan Vytopil 
Address: Nijmegen, Toernooiveld 1, 6525 ED Nijmegen, The Netherlands  
Affiliation: University of  
Abstract: A performance prediction method is presented for indicating the performance range of MIMD parallel processor systems for neural network simulations. The total execution time of a parallel application is modeled as the sum of its calculation and communication times. The method is scalable because based on the times measured on one processor and one communication link, the performance, speedup, and efficiency can be predicted for a larger processor system. It is validated quantitatively by applying it to two popular neural networks, backpropagation and the Kohonen self-organizing feature map, decomposed on a GCel-512, a 512 transputer system. Agreement of the model with the measurements is within 9%.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> L-C. Chu and B.W. Wah. </author> <title> Optimal Mapping of Neural Network Learning on Message-Passing Multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 319-339, </pages> <year> 1992. </year>
Reference-contexts: Chu and Wah <ref> [1] </ref>, Witbrock and Zagha [8] and various other authors have discussed the parallel implementation of backpropagation networks. Similar efforts have been reported implementing Kohonen networks by for example Obermayer et al [5]. Various techniques can be used to decompose a neural network over a number of processors.
Reference: 2. <author> A.J.G. Hey. </author> <title> The Genesis Distributed Memory Benchmarks. </title> <journal> Parallel Computing, </journal> <volume> 17(2) </volume> <pages> 1275-1283, </pages> <year> 1991. </year>
Reference-contexts: The needed benchmarks in our method are restricted to measuring the execution time of a small number of kernel functions on one processor and the time needed to communicate a single information unit between two processors. This approach can be classified as kernel benchmarking <ref> [2] </ref>.
Reference: 3. <author> T. Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <note> second edition, </note> <year> 1988. </year>
Reference-contexts: This approach can be classified as kernel benchmarking [2]. In the subsequent sections, our method is validated quantitatively by predicting the performance of the GCel-512 (a 512 multi-transputer system) for backpropagation [6] and Kohonen <ref> [3] </ref> neural networks decomposed via a technique called dataset decomposition. 2 Dataset decomposition of neural network simulations The problem of decomposing a given neural network over a parallel processor system with given topology has often been addressed in the literature. <p> Similar efforts have been reported implementing Kohonen networks by for example Obermayer et al [5]. Various techniques can be used to decompose a neural network over a number of processors. In this paper, the attention is focussed on dataset decomposition of backpropagation [6] and Kohonen <ref> [3] </ref> neural networks. Using this technique, each processor has a copy of the network, initiated with the same parameters and architecture. The parallelism that is exploited in this technique stems from the dataset.
Reference: 4. <author> Inmos/SGS-Thomson Microelectronics. </author> <title> The T9000 Transputer Product Overview, </title> <year> 1991. </year>
Reference-contexts: It is interesting to make some predictions for the T9000 transputer, extrapolating the measured parameters on the T805 by dividing them by the increased computing and communication powers of the T9000. Based on the information in <ref> [4] </ref>, one could roughly say that the T9000 is about 8 times faster than the T805 and has about 6 times higher bandwidth. Furthermore, we expect that when exploiting the new C104 communication chips, it will be cheaper to use all-to-all communications, thus reducing the communication times.
Reference: 5. <author> K. Obermayer, H.Heller, H. Ritter, and K. Schulten. </author> <title> Simulation of Self-Organizing Neural Nets: a Comparison between a Transputer Ring and a Connection Machine CM-2. </title> <booktitle> In Proceedings of the Third Conference of NATUG, </booktitle> <address> Sunnyvale, CA, </address> <year> 1990. </year>
Reference-contexts: Chu and Wah [1], Witbrock and Zagha [8] and various other authors have discussed the parallel implementation of backpropagation networks. Similar efforts have been reported implementing Kohonen networks by for example Obermayer et al <ref> [5] </ref>. Various techniques can be used to decompose a neural network over a number of processors. In this paper, the attention is focussed on dataset decomposition of backpropagation [6] and Kohonen [3] neural networks.
Reference: 6. <author> D.E. Rumelhart and J.L. McClelland. </author> <title> Parallel Distributed Processing: </title> <journal> Explorations in the Microstructure of Cognition, </journal> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This approach can be classified as kernel benchmarking [2]. In the subsequent sections, our method is validated quantitatively by predicting the performance of the GCel-512 (a 512 multi-transputer system) for backpropagation <ref> [6] </ref> and Kohonen [3] neural networks decomposed via a technique called dataset decomposition. 2 Dataset decomposition of neural network simulations The problem of decomposing a given neural network over a parallel processor system with given topology has often been addressed in the literature. <p> Similar efforts have been reported implementing Kohonen networks by for example Obermayer et al [5]. Various techniques can be used to decompose a neural network over a number of processors. In this paper, the attention is focussed on dataset decomposition of backpropagation <ref> [6] </ref> and Kohonen [3] neural networks. Using this technique, each processor has a copy of the network, initiated with the same parameters and architecture. The parallelism that is exploited in this technique stems from the dataset.
Reference: 7. <author> L.G. </author> <title> Vuurpijl and Th.E. Schouten. Performance of MIMD Execution Platforms for PNNs: </title> <type> How many MCUPS? Technical report, </type> <institution> Department of Real-Time Systems, Faculty of Mathematics and Informatics, University of Nijmegen, </institution> <address> Toernooiveld 1, 6525 ED Nijmegen, The Netherlands, </address> <month> August </month> <year> 1993. </year> <note> In progress. </note>
Reference-contexts: In step 3 of algorithm 1, every processor needs the weight changes that are computed on every other processor. In <ref> [7] </ref>, a detailed description of the different patterns of communication required for parallel neural network simulations is given. It is explained that the number of communications can be reduced enormously by using only local communications. This can be established via broadcast and gather operations. <p> All-to-all communications are then implemented by subsequently gathering and broadcasting messages. In the case of dataset decomposition, during the gathering also the accumulation of the weight changes can take place, which can be done in parallel. For these kind of gather-accumulate and broadcast operations, a tree topology is optimal <ref> [7] </ref>. Unfortunately, the GCel does not support the physical configuration of tree topologies. We have tried to use virtual tree topologies by using the Parix MakeTree utility, but the performance of the communication was far worse than using the physical grid topology and communicating locally. <p> For example many parallel image processing applications show this behaviour. We have also applied our method for neural networks using other decomposition techniques than dataset decomposition <ref> [7] </ref>. It is interesting to make some predictions for the T9000 transputer, extrapolating the measured parameters on the T805 by dividing them by the increased computing and communication powers of the T9000.
Reference: 8. <author> M. Witbrock and M. Zagha. </author> <title> An Implementation of Backpropagation Learning on GF11, a Large SIMD Parallel Computer. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 329-346, </pages> <year> 1990. </year>
Reference-contexts: Chu and Wah [1], Witbrock and Zagha <ref> [8] </ref> and various other authors have discussed the parallel implementation of backpropagation networks. Similar efforts have been reported implementing Kohonen networks by for example Obermayer et al [5]. Various techniques can be used to decompose a neural network over a number of processors.
References-found: 8

