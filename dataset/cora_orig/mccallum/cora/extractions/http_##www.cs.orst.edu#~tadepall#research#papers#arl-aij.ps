URL: http://www.cs.orst.edu/~tadepall/research/papers/arl-aij.ps
Refering-URL: 
Root-URL: 
Email: email: tadepall@cs.orst.edu  email: okdo@unitel.co.kr  
Phone: DoKyeong  
Title: Model-based Average Reward Reinforcement Learning  
Author: Prasad Tadepalli 
Note: Corresponding author. Most of the work was done when both the authors were  
Date: January 13, 1998  
Web: http://www.cs.orst.edu/~tadepall  
Address: Corvallis, OR 97331  NonSanSi DuMaMyeon BuNamRi P.O.Box 29 Korea ChungNam 320-919  
Affiliation: Department of Computer Science Oregon State University  Ok Korean Army Computer Center  at Oregon State University.  
Abstract: Reinforcement Learning (RL) is the study of programs that improve their performance by receiving rewards and punishments from the environment. Most RL methods optimize the discounted total reward received by an agent, while, in many domains, the natural criterion is to optimize the average reward per time step. In this paper, we introduce a model-based Average-reward Reinforcement Learning method called H-learning and show that it converges more quickly and robustly than its discounted counterpart in the domain of scheduling a simulated Automatic Guided Vehicle (AGV). We also introduce a version of H-learning that automatically explores the unexplored parts of the state space, while always choosing greedy actions with respect to the current value function. We show that this "Auto-exploratory H-Learning" performs better than the previously studied exploration strategies. To scale H-learning to larger state spaces, we extend it to learn action models and reward functions in the form of dynamic Bayesian networks, and approximate its value function using local linear regression. We show that both of these extensions are effective in significantly reducing the space requirement of H-learning and making it converge faster in some AGV scheduling tasks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.W. Aha, D. Kibler, and M.K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Steps 1 - 7 are executed when the agent is in state i. plars that improve their predictive accuracy and prune the unnecessary ones to keep the size of the representation small <ref> [1, 14, 13] </ref>. One problem with the edited nearest neighbor approaches is that they are highly sensitive to noise, since they tend to store all the noise points, which cannot be interpolated from the remaining points [1]. <p> One problem with the edited nearest neighbor approaches is that they are highly sensitive to noise, since they tend to store all the noise points, which cannot be interpolated from the remaining points <ref> [1] </ref>. Our algorithm does not seem to suffer from this problem, since the target value function is in fact piecewise linear and has no noise.
Reference: [2] <author> C. G. Atkeson, A. W. Moore, and S. Schaal. </author> <title> Locally weighted learning. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11 </volume> <pages> 11-73, </pages> <year> 1997. </year>
Reference-contexts: Our value function approximation is limited to generalizing the values of the k linear features. This is similar to Locally Weighted Regression (LWR) where the nonlinear features are given infinitely large weights <ref> [36, 2, 29] </ref>. Instead of representing the h function with a set of piecewise linear functions, we represent the value function using a set of "exemplars," which are a select set of states and their h-values, picked by the learning algorithm. <p> We showed that the piecewise linearity of the h-function can be effectively exploited using Local Linear Regression (LLR). It is a member of a family of regression techniques that go under the name of Locally Weighted Regression (LWR) <ref> [2, 29] </ref>. LWR is a regression technique with a sound statistical basis that also takes into account the locality of the target function. As a result, it can fit functions that are smooth in some places, but complex in other places.
Reference: [3] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 73(1) </volume> <pages> 81-138, </pages> <year> 1995. </year>
Reference-contexts: RL has been quite successful in automatic learning of good procedures for many tasks, including some real-world tasks such as job-shop scheduling and elevator scheduling [44, 11, 47]. Most approaches to reinforcement learning, including Q-learning [46] and Adaptive Real-Time Dynamic Programming (ARTDP) <ref> [3] </ref>, optimize the total discounted reward the learner receives [18]. In other words, a reward that is received after one time step is considered equivalent to a fraction of the same reward received immediately. <p> This raises the question whether and when discounted RL methods are appropriate to use to optimize the average reward. In this paper, we describe an Average-reward RL (ARL) method called H-learning, which 2 is an undiscounted version of Adaptive Real-Time Dynamic Programming (ARTDP) <ref> [3] </ref>. We compare H-learning with its discounted counterpart, ARTDP, in the task of scheduling a simulated Automatic Guided Vehicle (AGV), a material handling robot used in manufacturing. <p> We show that in the AGV scheduling domain H-learning converges in fewer steps than R-learning and is competitive with it in CPU time. This is consistent with the previous results on the comparisons between model-based and model-free discounted RL <ref> [3, 28] </ref>. Like most other RL methods, H-learning needs exploration to find an optimal policy. A number of exploration strategies have been studied in RL, including occasionally executing random actions, preferring to visit states that are least visited (counter-based) or executing actions that are least recently executed (recency-based) [45]. <p> It can be shown to satisfy the following recurrence relation <ref> [3, 6] </ref>: f fl u2U (i) X p i;j (u)f fl Real-Time Dynamic Programming (RTDP) solves the above recurrence relation by updating the value of the current state i in each step by the right hand side of the above equation. <p> Adaptive Real-time Dynamic Programming (ARTDP) estimates the action model probabilities and reward functions through on-line experience, and uses these estimates as real values while updating the value function of the current state by the right hand side of the recurrence relation above <ref> [3] </ref>. In dynamic programming literature, this method is called the certainty equivalence control [6]. It is a model-based method because it learns the action and reward models explicitly, and uses them to simultaneously learn the value function. <p> GreedyActions in each state are initialized to the set of admissible actions in that state. H-learning can be seen as a cross between Schwartz's R-learning [37], which is a model-free average-reward learning method, and Adaptive RTDP (ARTDP) <ref> [3] </ref>, which is a model-based discounted learning method. Like ARTDP, H-learning computes the probabilities p i;j (a) and rewards r i (a) by straightforward maximum likelihood estimates. <p> Model-free algorithms are easier to implement, because they have simpler update procedures. However, it has been observed that they do need more real-time experience to converge because they do not learn explicit action models, which are independent of the control policy and can be learned fairly quickly <ref> [3, 28] </ref>. Once the action models are learned, they can be used to propagate more information in each update of the value function by considering all possible next states of an action rather than the only next state that was actually reached.
Reference: [4] <author> D. Bertsekas. </author> <title> A new value-iteration method for the average cost dynamic programming problem. </title> <type> Technical Report LIDS-P-2307, </type> <institution> M.I.T., </institution> <address> Boston, MA, </address> <year> 1995. </year>
Reference-contexts: Mahadevan gives a useful survey of this literature from Reinforcement Learning point of view [24]. Schwartz and Singh present model-free RL algorithms for average-reward optimization [37, 38]. There are at least two average-reward reinforcement learning methods that have been proved to converge under suitable conditions <ref> [4, 16] </ref>. Bertsekas's algorithm is based on converting the Average-reward RL problem into a stochastic shortest path algorithm with slowly changing edge costs [4, 6]. <p> There are at least two average-reward reinforcement learning methods that have been proved to converge under suitable conditions [4, 16]. Bertsekas's algorithm is based on converting the Average-reward RL problem into a stochastic shortest path algorithm with slowly changing edge costs <ref> [4, 6] </ref>. The edge costs are essentially the negated average-adjusted immediate rewards, i.e., R i (u), where is the gain of the current greedy policy and R i (u) is the immediate reward of executing action u in state i.
Reference: [5] <author> D. P. Bertsekas. </author> <title> Distributed dynamic programming. </title> <journal> IEEE Transactions in Automatic Control, </journal> <volume> AC-27(3), </volume> <year> 1982. </year>
Reference-contexts: In White's relative value iteration method, the h-value of an arbitrarily chosen reference state is set to 0 and the resulting equations are solved by synchronous successive approximation [6]. Unfortunately, the asynchronous version of this algorithm that updates using 10 Equation (7) does not always converge <ref> [5] </ref>. Hence, instead of using Equation (7) to solve for , H-learning estimates it from on-line rewards (see Figure 4). ' $ 1. Take an exploratory action or a greedy action in the current state i.
Reference: [6] <author> D. P. Bertsekas. </author> <title> Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <address> Bel-mont, MA, </address> <year> 1995. </year> <month> 49 </month>
Reference-contexts: It can be shown to satisfy the following recurrence relation <ref> [3, 6] </ref>: f fl u2U (i) X p i;j (u)f fl Real-Time Dynamic Programming (RTDP) solves the above recurrence relation by updating the value of the current state i in each step by the right hand side of the above equation. <p> In dynamic programming literature, this method is called the certainty equivalence control <ref> [6] </ref>. It is a model-based method because it learns the action and reward models explicitly, and uses them to simultaneously learn the value function. <p> Although lim t!1 * t (s) may not exist for periodic policies, the Cesaro-limit of * t (s), defined as lim l!1 l t=1 * t (s), always exists, and is denoted by h (s) <ref> [6] </ref>. It is called the bias of state s and can be interpreted as the expected long-term advantage in total reward for starting in state s over and above ()t, the expected total reward in time t on the average. <p> Hence, the bias values of state i and j for the policy must satisfy the following equation. () + h (i) = r i ((i)) + j=1 The gain-optimal policy fl maximizes both sides of the above equation for each state i <ref> [6] </ref>. <p> In White's relative value iteration method, the h-value of an arbitrarily chosen reference state is set to 0 and the resulting equations are solved by synchronous successive approximation <ref> [6] </ref>. Unfortunately, the asynchronous version of this algorithm that updates using 10 Equation (7) does not always converge [5]. Hence, instead of using Equation (7) to solve for , H-learning estimates it from on-line rewards (see Figure 4). ' $ 1. <p> have only a limited amount of space and time, use local linear regression * when the structures of the Bayesian network action models are available, use them to learn parameters for the Bayesian network action models There is an extensive body of literature on average-reward optimization using dynamic programming approaches <ref> [15, 34, 6] </ref>. Mahadevan gives a useful survey of this literature from Reinforcement Learning point of view [24]. Schwartz and Singh present model-free RL algorithms for average-reward optimization [37, 38]. There are at least two average-reward reinforcement learning methods that have been proved to converge under suitable conditions [4, 16]. <p> There are at least two average-reward reinforcement learning methods that have been proved to converge under suitable conditions [4, 16]. Bertsekas's algorithm is based on converting the Average-reward RL problem into a stochastic shortest path algorithm with slowly changing edge costs <ref> [4, 6] </ref>. The edge costs are essentially the negated average-adjusted immediate rewards, i.e., R i (u), where is the gain of the current greedy policy and R i (u) is the immediate reward of executing action u in state i.
Reference: [7] <author> C. Boutilier, R. Dearden, and M. Goldszmidt. </author> <title> Exploiting structure in policy construc-tion. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: However, one of the stumbling blocks for the model-based algorithms to be more widely used is that representing them explicitly as transition matrices consumes too much space to be practical. Dynamic Bayesian networks have been used by a number of researchers to represent action models in decision theoretic planning <ref> [12, 30, 19, 7] </ref>. We showed that they can also be useful to compactly represent the action models for reinforcement learning and to shorten the learning time. Our current method uses the structure of the dynamic Bayesian network as prior knowledge and learns only the conditional probability tables.
Reference: [8] <author> J. Boyan and A. W. Moore. </author> <title> Generalizing reinforcement learning:safely approximating the value function. </title> <booktitle> In Proceedings of Neural Information Processing Systems, </booktitle> <year> 1994. </year>
Reference-contexts: This is usually done by finding an approximation for the value function from a hypothesized function space. There have been several function approximation methods studied in the discounted RL literature, including neural network learning <ref> [21, 8] </ref>, clustering [26], memory-based methods [28], and locally weighted regression [36, 29]. Two characteristics of the AGV scheduling domain attracted us to local linear regression as the method of choice. First, the location of the AGV is one of the most important features of the state in this domain.
Reference: [9] <author> L. Brieman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and regression trees. </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, MA, </address> <year> 1984. </year>
Reference-contexts: It may be necessary to combine it with other feature selection methods, or instead use more aggressive approximation methods like the neural networks or regression trees to scale learning to larger domains <ref> [21, 11, 9] </ref>. Another important problem is to extend our work to domains where some features, such as the location of the AGV, can be real-valued. To apply our methods to the full-scale AGV scheduling, we need to be able to handle multiple AGVs.
Reference: [10] <author> G. C. Canavos. </author> <title> Applied Probability and Statistical Methods. Little, </title> <publisher> Brown and Company, </publisher> <address> Boston, MA., </address> <year> 1984. </year>
Reference-contexts: In linear regression, we fit a linear surface in k dimensions to a set of m data points so that the sum of the squares of the errors of these points with respect to the output surface is minimized <ref> [10] </ref>. In local linear regression, the data points are chosen in the neighborhood of the point where a prediction is needed. Let us assume that the state is represented by a set of k "linear" features and n k "nonlinear" features.
Reference: [11] <author> R. H. Crites and A. G. Barto. </author> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 8,, </booktitle> <address> Cambridge, MA, 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: 1 Introduction Reinforcement Learning (RL) is the study of programs that improve their performance at some task by receiving rewards and punishments from the environment. RL has been quite successful in automatic learning of good procedures for many tasks, including some real-world tasks such as job-shop scheduling and elevator scheduling <ref> [44, 11, 47] </ref>. Most approaches to reinforcement learning, including Q-learning [46] and Adaptive Real-Time Dynamic Programming (ARTDP) [3], optimize the total discounted reward the learner receives [18]. <p> It may be necessary to combine it with other feature selection methods, or instead use more aggressive approximation methods like the neural networks or regression trees to scale learning to larger domains <ref> [21, 11, 9] </ref>. Another important problem is to extend our work to domains where some features, such as the location of the AGV, can be real-valued. To apply our methods to the full-scale AGV scheduling, we need to be able to handle multiple AGVs. <p> There have been some positive results in multi-agent reinforcement learning, including Crites's work on scheduling a bank of elevators, and Tan's results in a hunter-prey simulation <ref> [11, 43] </ref>. Our preliminary experiments in a simple domain with 2 AGVs indicate that an optimal policy can be learned as a mapping from global state space to actions of a single AGV. Both AGVs share and update the same value function and follow the same optimal policy.
Reference: [12] <author> T. Dean and K. </author> <title> Kanazawa. A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: To scale ARL to such domains, it is essential to approximate its action models and value function in a more compact form. Dynamic Bayesian networks have been successfully used in the past to represent the action models <ref> [12, 35] </ref>. In many cases, it is possible to design these networks in such a way that a small number of parameters are sufficient to fully specify the domain models. <p> The space requirement for storing the domain model is also exponential in the number of state variables. Dynamic Bayesian networks have been successfully used in the past to represent the domain models <ref> [12, 35] </ref>. <p> However, one of the stumbling blocks for the model-based algorithms to be more widely used is that representing them explicitly as transition matrices consumes too much space to be practical. Dynamic Bayesian networks have been used by a number of researchers to represent action models in decision theoretic planning <ref> [12, 30, 19, 7] </ref>. We showed that they can also be useful to compactly represent the action models for reinforcement learning and to shorten the learning time. Our current method uses the structure of the dynamic Bayesian network as prior knowledge and learns only the conditional probability tables.
Reference: [13] <author> G.W. Gates. </author> <title> The reduced nearest neighbor rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <pages> pages 431-433, </pages> <year> 1972. </year>
Reference-contexts: Steps 1 - 7 are executed when the agent is in state i. plars that improve their predictive accuracy and prune the unnecessary ones to keep the size of the representation small <ref> [1, 14, 13] </ref>. One problem with the edited nearest neighbor approaches is that they are highly sensitive to noise, since they tend to store all the noise points, which cannot be interpolated from the remaining points [1].
Reference: [14] <author> P.E. Hart. </author> <title> The condensed nearest neighbor rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 14 </volume> <pages> 515-516, </pages> <year> 1968. </year>
Reference-contexts: Steps 1 - 7 are executed when the agent is in state i. plars that improve their predictive accuracy and prune the unnecessary ones to keep the size of the representation small <ref> [1, 14, 13] </ref>. One problem with the edited nearest neighbor approaches is that they are highly sensitive to noise, since they tend to store all the noise points, which cannot be interpolated from the remaining points [1].
Reference: [15] <author> R. A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT press and Wiley, </publisher> <address> Cambridge, MA, </address> <year> 1960. </year>
Reference-contexts: have only a limited amount of space and time, use local linear regression * when the structures of the Bayesian network action models are available, use them to learn parameters for the Bayesian network action models There is an extensive body of literature on average-reward optimization using dynamic programming approaches <ref> [15, 34, 6] </ref>. Mahadevan gives a useful survey of this literature from Reinforcement Learning point of view [24]. Schwartz and Singh present model-free RL algorithms for average-reward optimization [37, 38]. There are at least two average-reward reinforcement learning methods that have been proved to converge under suitable conditions [4, 16].
Reference: [16] <author> A. Jalali and M. Ferguson. </author> <title> Computationally efficient adaptive control algorithms for markov chains. </title> <booktitle> In IEEE Proceedings of the 28'th Conference on Decision and Control, </booktitle> <address> Tampa, FL, </address> <year> 1989. </year>
Reference-contexts: Thus, is updated using the following equation, where ff is the learning rate. + ff (r i (u) h (i) + h (j) ) (12) H-learning is very similar to Jalali and Ferguson's Algorithm B <ref> [16] </ref>. This algorithm was proved to converge to the gain-optimal policy for ergodic MDPs. Since most domains that we are interested in are non-ergodic, to apply this algorithm to such domains, we need to add exploration. <p> Mahadevan gives a useful survey of this literature from Reinforcement Learning point of view [24]. Schwartz and Singh present model-free RL algorithms for average-reward optimization [37, 38]. There are at least two average-reward reinforcement learning methods that have been proved to converge under suitable conditions <ref> [4, 16] </ref>. Bertsekas's algorithm is based on converting the Average-reward RL problem into a stochastic shortest path algorithm with slowly changing edge costs [4, 6]. <p> The basic H-learning algorithm is very similar to the Algorithm B of Jalali and Ferguson <ref> [16] </ref>. The main difference is due to exploration, which is ignored by Jalali and Ferguson. In this paper, we have been mainly concerned with average-reward optimality or gain-optimality. Bias-optimality, or Schwartz's T-optimality, is a more refined notion than gain-optimality [34, 37].
Reference: [17] <author> L. P. Kaelbling. </author> <title> Learning in Embedded Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Other methods such as the Interval Estimation (IE) method of Kaelbling and the action-penalty representation of reward functions used by Koenig and Simmons incorporate the idea of "optimism under uncertainty" <ref> [17, 20] </ref>. By initializing the value function of states with high values, and gradually decreasing them, these methods encourage the learner to explore automatically, while always executing greedy actions. <p> Our approach is based on the idea of "optimism under uncertainty," and is similar to Kaelbling's Interval Estimation (IE) algorithm, and Koenig and Simmons's method of representing the reward functions using action-penalty scheme <ref> [17, 20] </ref>. 4.1 Auto-exploratory H-Learning Recall that in ergodic MDPs, every stationary policy is guaranteed to visit all states. <p> This gives the algorithm a chance to execute the other potentially best actions, thus encouraging exploration. 46 Kaelbling's Interval Estimation (IE) method is based on a more sophisticated version of the same idea, and is applicable to stochastic domains <ref> [17] </ref>. It maintains a confidence interval of the value function for each state, and picks actions that maximize the upper-bounds of their confidence intervals.
Reference: [18] <author> L. P. Kaelbling, M. L. Littman, and A. W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Aritificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: Most approaches to reinforcement learning, including Q-learning [46] and Adaptive Real-Time Dynamic Programming (ARTDP) [3], optimize the total discounted reward the learner receives <ref> [18] </ref>. In other words, a reward that is received after one time step is considered equivalent to a fraction of the same reward received immediately. Discounted optimization criterion is motivated by domains in which reward can be interpreted as money that can earn interest in each time step.
Reference: [19] <author> U. Kjaerulff. </author> <title> A computational scheme for reasoning in dynamic probabilistic networks. </title> <booktitle> In Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 121-129, </pages> <year> 1992. </year>
Reference-contexts: However, one of the stumbling blocks for the model-based algorithms to be more widely used is that representing them explicitly as transition matrices consumes too much space to be practical. Dynamic Bayesian networks have been used by a number of researchers to represent action models in decision theoretic planning <ref> [12, 30, 19, 7] </ref>. We showed that they can also be useful to compactly represent the action models for reinforcement learning and to shorten the learning time. Our current method uses the structure of the dynamic Bayesian network as prior knowledge and learns only the conditional probability tables.
Reference: [20] <author> S. Koenig and R. G. Simmons. </author> <title> The effect of representation and knowledge on goal--directed exploration with reinforcement-learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 227-250, </pages> <year> 1996. </year>
Reference-contexts: Other methods such as the Interval Estimation (IE) method of Kaelbling and the action-penalty representation of reward functions used by Koenig and Simmons incorporate the idea of "optimism under uncertainty" <ref> [17, 20] </ref>. By initializing the value function of states with high values, and gradually decreasing them, these methods encourage the learner to explore automatically, while always executing greedy actions. <p> Our approach is based on the idea of "optimism under uncertainty," and is similar to Kaelbling's Interval Estimation (IE) algorithm, and Koenig and Simmons's method of representing the reward functions using action-penalty scheme <ref> [17, 20] </ref>. 4.1 Auto-exploratory H-Learning Recall that in ergodic MDPs, every stationary policy is guaranteed to visit all states. <p> Koenig and Simmons achieve this effect simply by zero-initializing the value function and by giving a negative penalty for each action <ref> [20] </ref>. In deterministic domains, or when the updates are done using the minimax scheme, this ensures that the Q-values of state-action pairs are never less than their true optimal values. In analogy to the A* algorithms, this is called the "admissibility" condition. <p> Since is subtracted in the right hand side of the update equation of AH-learning, it is equivalent to giving a high penalty for each action as in Koenig and Simmons method <ref> [20] </ref>. As the system converges to the optimal policy, converges to the optimal gain, and the system stops exploring states not in the optimal loop. Theoretically characterizing the conditions of convergence of AH-learning is an important open problem.
Reference: [21] <author> L-J. Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 293-321, </pages> <year> 1992. </year>
Reference-contexts: The natural criterion to optimize in such domains is the average reward received per time step. Even so, many people have used discounted reinforcement learning algorithms in such domains, while aiming to optimize the average reward <ref> [21, 26] </ref>. One reason to do this is that the discounted total reward is finite even for an infinite sequence of actions and rewards. Hence, two such action sequences from a state can be compared by this criterion to choose the better one. <p> Hence, using discounted optimization when average-reward optimization is what is required could lead to suboptimal policies. Nevertheless, it can be argued that it is appropriate to optimize discounted total reward if that also nearly optimizes the average reward by using a discount factor which is sufficiently close to 1 <ref> [21, 26] </ref>. This raises the question whether and when discounted RL methods are appropriate to use to optimize the average reward. In this paper, we describe an Average-reward RL (ARL) method called H-learning, which 2 is an undiscounted version of Adaptive Real-Time Dynamic Programming (ARTDP) [3]. <p> This is usually done by finding an approximation for the value function from a hypothesized function space. There have been several function approximation methods studied in the discounted RL literature, including neural network learning <ref> [21, 8] </ref>, clustering [26], memory-based methods [28], and locally weighted regression [36, 29]. Two characteristics of the AGV scheduling domain attracted us to local linear regression as the method of choice. First, the location of the AGV is one of the most important features of the state in this domain. <p> It may be necessary to combine it with other feature selection methods, or instead use more aggressive approximation methods like the neural networks or regression trees to scale learning to larger domains <ref> [21, 11, 9] </ref>. Another important problem is to extend our work to domains where some features, such as the location of the AGV, can be real-valued. To apply our methods to the full-scale AGV scheduling, we need to be able to handle multiple AGVs.
Reference: [22] <author> M. L. Littman, A. Cassandra, and L. P. Kaelbling. </author> <title> Learning policies for partially observable environments: scaling up. </title> <booktitle> In Proceedings of International Machine Learning Conference, </booktitle> <pages> pages 362-370, </pages> <address> San Fransisco, CA, </address> <year> 1995. </year>
Reference-contexts: Another important assumption that we made that needs relaxing is that the state is fully observable. Recently, there has been some work to extend RL to Partially Observable Markov Decision Problems (POMDPs) <ref> [22, 33] </ref>. Unfortunately, even the best algorithms for solving POMDPs currently appear to be impractical for large problems.
Reference: [23] <author> S. Mahadevan. </author> <title> An average reward reinforcement learning algorithm for computing bias-optimal policies. </title> <booktitle> In Proceedings of National Conference on Artificial Intelligence,, </booktitle> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: To find the bias-optimal policies for more general unichains, it is necessary to select bias-optimal actions from among the gain-optimal ones in every state using more refined criteria. Mahadevan extends both H-learning and R-learning to find the bias-optimal policies for general unichains <ref> [23, 25] </ref>. His method is based on solving a set of nested recurrence relations for two values h and W for each state, since the h values alone are not sufficient to determine a bias-optimal policy. <p> Bias optimal policies appear to have a significant advantage in some domains such as the admission control queuing systems studied in operations research literature <ref> [23, 25] </ref>.
Reference: [24] <author> S. Mahadevan. </author> <title> Average reward reinforcement learning: Foundations, algorithms, and empirical results. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 159-195, </pages> <year> 1996. </year>
Reference-contexts: Our results are consistent with those of Mahadevan who compared Q-learning and R-learning in a robot simulator domain and a maze domain and found that R-learning can be tuned to perform better <ref> [24] </ref>. 18 4 Exploration Recall that H-learning needs exploratory actions to ensure that every state is visited infinitely often during training in order to avoid converging to suboptimal policies. <p> Mahadevan gives a useful survey of this literature from Reinforcement Learning point of view <ref> [24] </ref>. Schwartz and Singh present model-free RL algorithms for average-reward optimization [37, 38]. There are at least two average-reward reinforcement learning methods that have been proved to converge under suitable conditions [4, 16].
Reference: [25] <author> S. Mahadevan. </author> <title> Sensitive discount optimality: Unifying discounted and average reward reinforcement learning. </title> <booktitle> In Proceedings of International Machine Learning Conference,, </booktitle> <address> Bari, Italy, </address> <year> 1996. </year>
Reference-contexts: To find the bias-optimal policies for more general unichains, it is necessary to select bias-optimal actions from among the gain-optimal ones in every state using more refined criteria. Mahadevan extends both H-learning and R-learning to find the bias-optimal policies for general unichains <ref> [23, 25] </ref>. His method is based on solving a set of nested recurrence relations for two values h and W for each state, since the h values alone are not sufficient to determine a bias-optimal policy. <p> Bias optimal policies appear to have a significant advantage in some domains such as the admission control queuing systems studied in operations research literature <ref> [23, 25] </ref>.
Reference: [26] <author> S. Mahadevan and J. Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55 </volume> <pages> 311-365, </pages> <year> 1992. </year>
Reference-contexts: The natural criterion to optimize in such domains is the average reward received per time step. Even so, many people have used discounted reinforcement learning algorithms in such domains, while aiming to optimize the average reward <ref> [21, 26] </ref>. One reason to do this is that the discounted total reward is finite even for an infinite sequence of actions and rewards. Hence, two such action sequences from a state can be compared by this criterion to choose the better one. <p> Hence, using discounted optimization when average-reward optimization is what is required could lead to suboptimal policies. Nevertheless, it can be argued that it is appropriate to optimize discounted total reward if that also nearly optimizes the average reward by using a discount factor which is sufficiently close to 1 <ref> [21, 26] </ref>. This raises the question whether and when discounted RL methods are appropriate to use to optimize the average reward. In this paper, we describe an Average-reward RL (ARL) method called H-learning, which 2 is an undiscounted version of Adaptive Real-Time Dynamic Programming (ARTDP) [3]. <p> This is usually done by finding an approximation for the value function from a hypothesized function space. There have been several function approximation methods studied in the discounted RL literature, including neural network learning [21, 8], clustering <ref> [26] </ref>, memory-based methods [28], and locally weighted regression [36, 29]. Two characteristics of the AGV scheduling domain attracted us to local linear regression as the method of choice. First, the location of the AGV is one of the most important features of the state in this domain.
Reference: [27] <author> W. L. Maxwell and J. A. Muckstadt. </author> <title> Design of automatic guided vehicle systems. </title> <journal> Institute of Industrial Engineers Transactions, </journal> <volume> 14(2) </volume> <pages> 114-124, </pages> <year> 1982. </year>
Reference-contexts: proof of convergence is quite involved and its correctness is disputed. 1 In any case, it may be easier to give an independent proof based on stochastic convergence theories. 3.2 AGV Scheduling Automatic Guided vehicles (AGVs) are used in modern manufacturing plants to transport materials from one location to another <ref> [27] </ref>. To compare the performance of various learning algorithms, a small AGV domain called the "Delivery domain" shown in Figure 5 was used. There are two job generators on the left, one AGV, and two destination conveyor belts on the right.
Reference: [28] <author> A. W. Moore and A. G. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less time. </title> <journal> Machine Learning Journal, </journal> <volume> 13 </volume> <pages> 103-130, </pages> <year> 1993. </year>
Reference-contexts: We show that in the AGV scheduling domain H-learning converges in fewer steps than R-learning and is competitive with it in CPU time. This is consistent with the previous results on the comparisons between model-based and model-free discounted RL <ref> [3, 28] </ref>. Like most other RL methods, H-learning needs exploration to find an optimal policy. A number of exploration strategies have been studied in RL, including occasionally executing random actions, preferring to visit states that are least visited (counter-based) or executing actions that are least recently executed (recency-based) [45]. <p> This is usually done by finding an approximation for the value function from a hypothesized function space. There have been several function approximation methods studied in the discounted RL literature, including neural network learning [21, 8], clustering [26], memory-based methods <ref> [28] </ref>, and locally weighted regression [36, 29]. Two characteristics of the AGV scheduling domain attracted us to local linear regression as the method of choice. First, the location of the AGV is one of the most important features of the state in this domain. <p> Model-free algorithms are easier to implement, because they have simpler update procedures. However, it has been observed that they do need more real-time experience to converge because they do not learn explicit action models, which are independent of the control policy and can be learned fairly quickly <ref> [3, 28] </ref>. Once the action models are learned, they can be used to propagate more information in each update of the value function by considering all possible next states of an action rather than the only next state that was actually reached.
Reference: [29] <author> A. W. Moore, C. G. Atkeson, and S. Schaal. </author> <title> Locally weighted learning for control. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11 </volume> <pages> 75-113, </pages> <year> 1997. </year>
Reference-contexts: This is usually done by finding an approximation for the value function from a hypothesized function space. There have been several function approximation methods studied in the discounted RL literature, including neural network learning [21, 8], clustering [26], memory-based methods [28], and locally weighted regression <ref> [36, 29] </ref>. Two characteristics of the AGV scheduling domain attracted us to local linear regression as the method of choice. First, the location of the AGV is one of the most important features of the state in this domain. <p> Our value function approximation is limited to generalizing the values of the k linear features. This is similar to Locally Weighted Regression (LWR) where the nonlinear features are given infinitely large weights <ref> [36, 2, 29] </ref>. Instead of representing the h function with a set of piecewise linear functions, we represent the value function using a set of "exemplars," which are a select set of states and their h-values, picked by the learning algorithm. <p> We showed that the piecewise linearity of the h-function can be effectively exploited using Local Linear Regression (LLR). It is a member of a family of regression techniques that go under the name of Locally Weighted Regression (LWR) <ref> [2, 29] </ref>. LWR is a regression technique with a sound statistical basis that also takes into account the locality of the target function. As a result, it can fit functions that are smooth in some places, but complex in other places.
Reference: [30] <author> A. E. Nicholson and J. M. Brady. </author> <title> The data association problem when monitoring robot vehicles using dynamic belief networks. </title> <booktitle> In ECAI 92: 10th European Conference on Artificial Intelligence Proceedings, </booktitle> <pages> pages 689-693, </pages> <address> Vienna, Austria, 1992. </address> <publisher> Wiley. </publisher>
Reference-contexts: However, one of the stumbling blocks for the model-based algorithms to be more widely used is that representing them explicitly as transition matrices consumes too much space to be practical. Dynamic Bayesian networks have been used by a number of researchers to represent action models in decision theoretic planning <ref> [12, 30, 19, 7] </ref>. We showed that they can also be useful to compactly represent the action models for reinforcement learning and to shorten the learning time. Our current method uses the structure of the dynamic Bayesian network as prior knowledge and learns only the conditional probability tables.
Reference: [31] <author> D. </author> <title> Ok. A Study of Model-based Average Reward Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> Oregon State University, Corvallis, </institution> <address> OR, </address> <year> 1996. </year> <note> Available as Department of Computer Science Technical Report, 96-30-2. 51 </note>
Reference-contexts: We present the result of comparing H-learning with ARTDP, Q-learning, and R-learning in two situations of the AGV domain: K=1 and K=5. We chose these two sets of domain parameters because they illustrate two qualitatively different situations. Experiments on a wider range of domain parameters are reported elsewhere <ref> [31] </ref>. Each experiment was repeated for 30 trials for each algorithm. Every trial started from a random initial state. <p> Increasing fl to 0.99 helped ARTDP find the gain-optimal policy, but much more slowly than H-learning. Increasing fl to 0:999, however, decreased the success rate of ARTDP (in 300,000 steps), because it slowed down the convergence too drastically <ref> [31] </ref>. In summary, our experiments indicate that H-learning is more robust with respect to changes in the domain parameters, and in many cases, converges in fewer steps to the gain-optimal policy than its discounted counterpart.
Reference: [32] <author> D. Ok and P. Tadepalli. </author> <title> Auto-exploratory average reward reinforcement learning. </title> <booktitle> In Proceedings of AAAI-96, </booktitle> <year> 1996. </year>
Reference-contexts: We presented a variety of algorithms based on H-learning, a model-based method designed to optimize the gain or average reward per time step, and demonstrated their usefulness in AGV scheduling tasks. Earlier presentations of parts of this work include [41], <ref> [32] </ref>, and [42]. network model learning, and local linear regression to H-learning. We can choose any combination of these three extensions, depending on the domain, our needs, and the resources and the prior knowledge available.
Reference: [33] <author> R. Parr and S. Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of National Conference On Artificial Intelligence, </booktitle> <pages> pages 1088-1093, </pages> <address> Seattle, WA, </address> <year> 1995. </year>
Reference-contexts: Another important assumption that we made that needs relaxing is that the state is fully observable. Recently, there has been some work to extend RL to Partially Observable Markov Decision Problems (POMDPs) <ref> [22, 33] </ref>. Unfortunately, even the best algorithms for solving POMDPs currently appear to be impractical for large problems.
Reference: [34] <author> M. L. Puterman. </author> <title> Markov Decision Processes: Discrete Dynamic Stochastic Programming. </title> <publisher> John Wiley, </publisher> <year> 1994. </year>
Reference-contexts: Non-recurrent states are called transient. An MDP is ergodic if its states form a single recurrent set under each stationary policy. It is a unichain if every stationary policy gives rise to a single recurrent set of states and possibly some transient states <ref> [34] </ref>. For unichain MDPs the expected long-term average reward per time step for any policy is independent of the starting state s 0 . We call it the "gain" of the policy , denoted by (), and consider the problem of finding a "gain-optimal policy," fl , that maximizes (). <p> Hence, we are primarily interested in non-ergodic domains in this section. Unfortunately, the gain of a stationary policy for a general multichain (non-unichain) MDP is not constant but depends on the initial state <ref> [34] </ref>. Hence we consider some restricted classes of MDPs. An MDP is communicating if for every pair of states i; j, there is a stationary policy under which they communicate. Contrast this with ergodic MDPs, where every pair of states communicate under every stationary policy. <p> Serving only one of the generators all the time prevents the AGV from visiting some states. A weakly communicating MDP is more general than a communicating MDP and also allows a set of states which are transient under every stationary policy <ref> [34] </ref>. Although the gain of a stationary policy for a weakly communicating MDP also depends on the initial state, the gain of an optimal policy does not. AH-learning exploits this fact, and works by using as an upper bound on the optimal gain. <p> have only a limited amount of space and time, use local linear regression * when the structures of the Bayesian network action models are available, use them to learn parameters for the Bayesian network action models There is an extensive body of literature on average-reward optimization using dynamic programming approaches <ref> [15, 34, 6] </ref>. Mahadevan gives a useful survey of this literature from Reinforcement Learning point of view [24]. Schwartz and Singh present model-free RL algorithms for average-reward optimization [37, 38]. There are at least two average-reward reinforcement learning methods that have been proved to converge under suitable conditions [4, 16]. <p> The main difference is due to exploration, which is ignored by Jalali and Ferguson. In this paper, we have been mainly concerned with average-reward optimality or gain-optimality. Bias-optimality, or Schwartz's T-optimality, is a more refined notion than gain-optimality <ref> [34, 37] </ref>. It seeks to find a policy that maximizes the expected total reward obtained before entering a recurrent state, while also being gain-optimal. All gain-optimal policies are not bias-optimal.
Reference: [35] <author> S. Russell and P. Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice-Hall, Inc, </publisher> <year> 1995. </year>
Reference-contexts: To scale ARL to such domains, it is essential to approximate its action models and value function in a more compact form. Dynamic Bayesian networks have been successfully used in the past to represent the action models <ref> [12, 35] </ref>. In many cases, it is possible to design these networks in such a way that a small number of parameters are sufficient to fully specify the domain models. <p> The space requirement for storing the domain model is also exponential in the number of state variables. Dynamic Bayesian networks have been successfully used in the past to represent the domain models <ref> [12, 35] </ref>. <p> The CPT at each node describes the probabilities of different values for a node conditioned on the values of its parents. The probability of any event given some evidence is determined by the network and the associated CPTs, and there are many algorithms to compute this <ref> [35] </ref>. Since the network structure is given as prior knowledge, learning action models reduces to learning the CPTs. We illustrate the dynamic Bayesian network representation using the Slippery-lane domain in Figure 13 (a). There is a job generator on the left and a conveyor-belt on the right.
Reference: [36] <author> S. Schaal and C. Atkeson. </author> <title> Robot juggling: An implementation of memory-based learning. </title> <journal> In IEEE Control Systems, </journal> <volume> volume 14, </volume> <pages> pages 57-71, </pages> <year> 1994. </year>
Reference-contexts: This is usually done by finding an approximation for the value function from a hypothesized function space. There have been several function approximation methods studied in the discounted RL literature, including neural network learning [21, 8], clustering [26], memory-based methods [28], and locally weighted regression <ref> [36, 29] </ref>. Two characteristics of the AGV scheduling domain attracted us to local linear regression as the method of choice. First, the location of the AGV is one of the most important features of the state in this domain. <p> Our value function approximation is limited to generalizing the values of the k linear features. This is similar to Locally Weighted Regression (LWR) where the nonlinear features are given infinitely large weights <ref> [36, 2, 29] </ref>. Instead of representing the h function with a set of piecewise linear functions, we represent the value function using a set of "exemplars," which are a select set of states and their h-values, picked by the learning algorithm. <p> As a result, it can fit functions that are smooth in some places, but complex in other places. There have been many successful applications of LWR in reinforcement learning, including a juggling robot <ref> [36] </ref>. Our results suggest that local linear regression (LLR) is a promising approach to approximation especially for Average-reward RL. We also showed that it synergistically combines with approximating domain models using Bayesian networks.
Reference: [37] <author> A. Schwartz. </author> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning,, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: When short-term and long-term optimal policies are different, ARTDP either fails to converge to the optimal average-reward policy or converges too slowly if the discount factor is high. Like ARTDP, and unlike Schwartz's R-learning <ref> [37] </ref> and Singh's ARL algorithms [38], H-learning is model-based, in that it learns and uses explicit action and reward models. We show that in the AGV scheduling domain H-learning converges in fewer steps than R-learning and is competitive with it in CPU time. <p> As Schwartz pointed out, even researchers who use learning methods that optimize discounted totals in such domains evaluate their systems using a different, but more natural, measure average expected reward per time step <ref> [37] </ref>. Discounting in such domains tends to sacrifice bigger long-term rewards in favor of smaller short-term rewards, which is undesirable in many cases. The following example illustrates this. In the Multi-loop domain shown in Figure 1, there are four loops of different lengths. <p> Before starting, the algorithm initializes ff to 1, and all other variables to 0. GreedyActions in each state are initialized to the set of admissible actions in that state. H-learning can be seen as a cross between Schwartz's R-learning <ref> [37] </ref>, which is a model-free average-reward learning method, and Adaptive RTDP (ARTDP) [3], which is a model-based discounted learning method. Like ARTDP, H-learning computes the probabilities p i;j (a) and rewards r i (a) by straightforward maximum likelihood estimates. <p> Instead, we use a method similar to that of R-learning to estimate the average reward 12 <ref> [37] </ref>. From Equation (7), in any state i, for any greedy action u that maximizes the right hand side, = r i (u) h (i) + P n j=1 p i;j (u)h (j). <p> Mahadevan gives a useful survey of this literature from Reinforcement Learning point of view [24]. Schwartz and Singh present model-free RL algorithms for average-reward optimization <ref> [37, 38] </ref>. There are at least two average-reward reinforcement learning methods that have been proved to converge under suitable conditions [4, 16]. Bertsekas's algorithm is based on converting the Average-reward RL problem into a stochastic shortest path algorithm with slowly changing edge costs [4, 6]. <p> The main difference is due to exploration, which is ignored by Jalali and Ferguson. In this paper, we have been mainly concerned with average-reward optimality or gain-optimality. Bias-optimality, or Schwartz's T-optimality, is a more refined notion than gain-optimality <ref> [34, 37] </ref>. It seeks to find a policy that maximizes the expected total reward obtained before entering a recurrent state, while also being gain-optimal. All gain-optimal policies are not bias-optimal.
Reference: [38] <author> S. P. Singh. </author> <title> Reinforcement learning algorithms for average-payoff markovian decision processes. </title> <booktitle> In Proceedings of AAAI-94. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: When short-term and long-term optimal policies are different, ARTDP either fails to converge to the optimal average-reward policy or converges too slowly if the discount factor is high. Like ARTDP, and unlike Schwartz's R-learning [37] and Singh's ARL algorithms <ref> [38] </ref>, H-learning is model-based, in that it learns and uses explicit action and reward models. We show that in the AGV scheduling domain H-learning converges in fewer steps than R-learning and is competitive with it in CPU time. <p> Mahadevan gives a useful survey of this literature from Reinforcement Learning point of view [24]. Schwartz and Singh present model-free RL algorithms for average-reward optimization <ref> [37, 38] </ref>. There are at least two average-reward reinforcement learning methods that have been proved to converge under suitable conditions [4, 16]. Bertsekas's algorithm is based on converting the Average-reward RL problem into a stochastic shortest path algorithm with slowly changing edge costs [4, 6].
Reference: [39] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: The exploration strategy only effects the speed with which the optimal policy is learned, not the optimality of the learned policy. This is unlike some temporal difference methods such as TD- which are designed to learn the value function for the policy that is executed during learning <ref> [39] </ref>.
Reference: [40] <author> R. S. Sutton. </author> <title> Integrating architectures for learning, planning and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of Machine Learning Conference, </booktitle> <year> 1990. </year>
Reference-contexts: They can also be used to plan and to learn from simulated experience as in the Dyna architecture <ref> [40] </ref>. However, one of the stumbling blocks for the model-based algorithms to be more widely used is that representing them explicitly as transition matrices consumes too much space to be practical.
Reference: [41] <author> P. Tadepalli and D. Ok. H-learning: </author> <title> A reinforcement learning method for optimizing undiscounted average reward. </title> <type> Technical Report 94-30-1, </type> <institution> Dept. of Computer Science, Oregon State University, </institution> <year> 1994. </year>
Reference-contexts: We presented a variety of algorithms based on H-learning, a model-based method designed to optimize the gain or average reward per time step, and demonstrated their usefulness in AGV scheduling tasks. Earlier presentations of parts of this work include <ref> [41] </ref>, [32], and [42]. network model learning, and local linear regression to H-learning. We can choose any combination of these three extensions, depending on the domain, our needs, and the resources and the prior knowledge available.
Reference: [42] <author> P. Tadepalli and D. </author> <title> Ok. Scaling up average reward reinforcement learning by approximating the domain models and the value function. </title> <booktitle> In Proceedings of International Machine Learning Conference, </booktitle> <year> 1996. </year>
Reference-contexts: We presented a variety of algorithms based on H-learning, a model-based method designed to optimize the gain or average reward per time step, and demonstrated their usefulness in AGV scheduling tasks. Earlier presentations of parts of this work include [41], [32], and <ref> [42] </ref>. network model learning, and local linear regression to H-learning. We can choose any combination of these three extensions, depending on the domain, our needs, and the resources and the prior knowledge available.
Reference: [43] <author> M. Tan. </author> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning,, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 52 </pages>
Reference-contexts: There have been some positive results in multi-agent reinforcement learning, including Crites's work on scheduling a bank of elevators, and Tan's results in a hunter-prey simulation <ref> [11, 43] </ref>. Our preliminary experiments in a simple domain with 2 AGVs indicate that an optimal policy can be learned as a mapping from global state space to actions of a single AGV. Both AGVs share and update the same value function and follow the same optimal policy.
Reference: [44] <author> G. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <booktitle> Machine Learning, </booktitle> <address> 8(3--4):257-277, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Reinforcement Learning (RL) is the study of programs that improve their performance at some task by receiving rewards and punishments from the environment. RL has been quite successful in automatic learning of good procedures for many tasks, including some real-world tasks such as job-shop scheduling and elevator scheduling <ref> [44, 11, 47] </ref>. Most approaches to reinforcement learning, including Q-learning [46] and Adaptive Real-Time Dynamic Programming (ARTDP) [3], optimize the total discounted reward the learner receives [18].
Reference: [45] <author> S. Thrun. </author> <title> The role of exploration in learning control. In Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <year> 1994. </year>
Reference-contexts: Like most other RL methods, H-learning needs exploration to find an optimal policy. A number of exploration strategies have been studied in RL, including occasionally executing random actions, preferring to visit states that are least visited (counter-based) or executing actions that are least recently executed (recency-based) <ref> [45] </ref>. Other methods such as the Interval Estimation (IE) method of Kaelbling and the action-penalty representation of reward functions used by Koenig and Simmons incorporate the idea of "optimism under uncertainty" [17, 20]. <p> We use the Delivery domain of We compared AH-learning to four other exploration methods: random exploration, counter-based exploration, Boltzmann exploration, and recency-based exploration <ref> [45] </ref>. In random exploration, a random action is selected uniformly from among the admissible actions with a small probability . With a high probability 1 , in any state i, a greedy action, i.e., one that maximizes R (i; a) over all a, is chosen.
Reference: [46] <author> C. J. C. H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: RL has been quite successful in automatic learning of good procedures for many tasks, including some real-world tasks such as job-shop scheduling and elevator scheduling [44, 11, 47]. Most approaches to reinforcement learning, including Q-learning <ref> [46] </ref> and Adaptive Real-Time Dynamic Programming (ARTDP) [3], optimize the total discounted reward the learner receives [18]. In other words, a reward that is received after one time step is considered equivalent to a fraction of the same reward received immediately. <p> If the learning algorithm uses an exploration strategy that ensures that it executes each admissible action in each state infinitely often, and the learning rate fi is appropriately decayed, Q-learning is guaranteed to converge to an optimal policy <ref> [46] </ref>. 2.2 The problems of discounting Discounted reinforcement learning is well-studied, and methods such as Q-learning and ARTDP are shown to converge under suitable conditions both in theory and in practice. <p> Another possibility is to maintain a confidence interval for and to use the upper bound of the confidence interval to update the R values as in the IE method. Most reinforcement learning work is based on model-free algorithms such as Q-learning <ref> [46] </ref>. Model-free algorithms are easier to implement, because they have simpler update procedures. However, it has been observed that they do need more real-time experience to converge because they do not learn explicit action models, which are independent of the control policy and can be learned fairly quickly [3, 28].

References-found: 46

