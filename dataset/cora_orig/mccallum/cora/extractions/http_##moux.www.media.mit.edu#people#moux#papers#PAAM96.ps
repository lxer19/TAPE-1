URL: http://moux.www.media.mit.edu/people/moux/papers/PAAM96.ps
Refering-URL: http://agents.www.media.mit.edu/groups/agents/publications/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: moux@media.mit.edu  
Title: Amalthaea: Information Discovery and Filtering using a Multiagent Evolving Ecosystem  
Author: Alexandros Moukas 
Keyword: Agents, Information Filtering, Evolution, World-Wide-Web  
Address: 20 Ames street, Cambridge, MA 02139-4307.  
Affiliation: MIT Media Laboratory,  
Abstract: Agents are semi-intelligent programs that assist the user in performing repetitive and time-consuming tasks. Information discovery and information filtering are a suitable domain for applying agent technology. Ideas drawn from the field of autonomous agents and artificial life are combined in the creation of an evolving ecosystem composed of competing and cooperating agents. A co-evolution model of information filtering agents that adapt to the various user's interests and information discovery agents that monitor and adapt to the various on-line information sources is analyzed. Results from a number of experiments are presented and discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> The lycos, </author> <title> the catalog of the internet. </title> <address> http://www.lycos.com. </address>
Reference-contexts: In the domain of the WWW, the human-updated indices like Yahoo [4] at Stanford University were supplemented by indexing engines like the WWWW [3], and Lycos at CMU <ref> [1] </ref>. Metacrawler [2] is an agent that operates at a higher abstraction level by utilizing eight existing WWW index and search engines which was developed at the University of Washington.
Reference: [2] <institution> The metacrawler multi-threaded web search service. </institution> <note> http://www.metacrawler.com. </note>
Reference-contexts: In the domain of the WWW, the human-updated indices like Yahoo [4] at Stanford University were supplemented by indexing engines like the WWWW [3], and Lycos at CMU [1]. Metacrawler <ref> [2] </ref> is an agent that operates at a higher abstraction level by utilizing eight existing WWW index and search engines which was developed at the University of Washington.
Reference: [3] <author> The world-wide-web worm. </author> <note> http://www.cs.colorado.edu/ mcbryan/wwww.html. </note>
Reference-contexts: In the domain of the WWW, the human-updated indices like Yahoo [4] at Stanford University were supplemented by indexing engines like the WWWW <ref> [3] </ref>, and Lycos at CMU [1]. Metacrawler [2] is an agent that operates at a higher abstraction level by utilizing eight existing WWW index and search engines which was developed at the University of Washington.
Reference: [4] <institution> The yahoo index. </institution> <note> http://www.yahoo.com. </note>
Reference-contexts: In the domain of the WWW, the human-updated indices like Yahoo <ref> [4] </ref> at Stanford University were supplemented by indexing engines like the WWWW [3], and Lycos at CMU [1]. Metacrawler [2] is an agent that operates at a higher abstraction level by utilizing eight existing WWW index and search engines which was developed at the University of Washington.
Reference: [5] <author> M. Balabanovic and Y. Shoham. </author> <title> Learning information retrieval agents: Experiments with automated web browsing. </title> <booktitle> In AAAI Technical Report SS-95-08, Proceedings of the 1995 AAAI Spring Symposium Series., </booktitle> <year> 1995. </year>
Reference-contexts: These agents are designed to assist and providing personalization to the user while browsing the WWW by performing a breadth-first search on the links ahead and provide navigation recommendations. More similar to our work in terms of application domain and representation is the system build by Balabanovic and Shoham <ref> [5] </ref>. They introduced a system for WWW document filtering which also utilized the weighted keyword vector representation. In terms of evolutionary filtering systems, NewT [18] developed by Sheth, is a multiagent system that uses evolution and relevance feedback for information filtering.
Reference: [6] <author> N. Belkin and B. Croft. </author> <title> Information filtering and informatiuon retrieval. </title> <journal> Communications of the ACM, </journal> <volume> 35, No. 12 </volume> <pages> 29-37, </pages> <year> 1992. </year>
Reference-contexts: At the same time the system continues to explore the search space for better solutions using evolution techniques such as mutation and crossover for refreshing and specializing the agents population. Our approach to creating an ecosystem is influenced by Belkin and Croft's <ref> [6] </ref> approach to information filtering and information retrieval: filtering and retrieval are in fact two sides of the same coin and should be combined to maximize efficiency in information managing tasks. Significant work has been done on applying artificial intelligence techniques to information filtering.
Reference: [7] <author> B. Grosof et al. </author> <title> Reusable architecture for embedding rule-based intelligence. </title> <booktitle> In CIKM Conference, Workshop on Intelligent Information Agents, </booktitle> <year> 1995. </year>
Reference-contexts: The MACRON multiagent system [8] developed at UMass/Amherst, is built on top of the CIG searchbots and uses a centralized planner to generate sub-goals that are pursued by a group of cooperating agents, using KQML [10], a standardized language for inter-agent communication and negotiation. Another similar system is RAISE <ref> [7] </ref>, developed by IBM. RAISE is a rule-based system that provides a framework for knowledge reuse in different domains (like electronic mail, newsgroups e.t.c.) INFOrmer [16], developed at University of Cork introduces the idea of using associative networks instead of keywords for information retrieval.
Reference: [8] <author> K. Decker V. Lesser et al. Macron: </author> <title> An architecture for multi-agent cooperative information gathering. </title> <booktitle> In CIKM Conference, Workshop on Intelligent Information Agents, </booktitle> <year> 1995. </year>
Reference-contexts: Webcompass is another similar product by Quarterdeck. Webcompass is directed towards off-line search and indexing. It enables the user to generate queries that will search the WWW off-line and presents the results at a later time. The MACRON multiagent system <ref> [8] </ref> developed at UMass/Amherst, is built on top of the CIG searchbots and uses a centralized planner to generate sub-goals that are pursued by a group of cooperating agents, using KQML [10], a standardized language for inter-agent communication and negotiation. Another similar system is RAISE [7], developed by IBM.
Reference: [9] <author> H. Frystyk and H. </author> <title> Lie. Towards a uniform library of common code. </title> <booktitle> In Proceedings of the Second WWW Conference, </booktitle> <year> 1994. </year>
Reference-contexts: stemming, keyword extraction and weighted keyword vector generation engine. * A database of the retrieved documents 3.1 World-Wide-Web Objects Retrieval Engine Since the sources of the information 1 of the system can be accessed through the WorldWide-Web, the main engine for retrieving documents is based on WWW Organization's libwww library <ref> [9] </ref> 2 . This library provides an effective way of accessing and retrieving information from the WWW and is easily interfaced with off-line applications.
Reference: [10] <author> Y. Labrou and T. Finn. </author> <title> A semantics approach for kqml a general purpose communication language for software agents. </title> <booktitle> In Proceedings of Conference on Information and Knowledge Management 1994. </booktitle> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: The MACRON multiagent system [8] developed at UMass/Amherst, is built on top of the CIG searchbots and uses a centralized planner to generate sub-goals that are pursued by a group of cooperating agents, using KQML <ref> [10] </ref>, a standardized language for inter-agent communication and negotiation. Another similar system is RAISE [7], developed by IBM.
Reference: [11] <author> H. Lieberman. Letizia, </author> <title> an agent that assists web browsing. </title> <booktitle> In Proceedings of IJCAI-95. </booktitle> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference-contexts: Another category of WWW agents includes CMU's Webwatcher [14] and MIT Media Laboratory's Letizia <ref> [11] </ref>. These agents are designed to assist and providing personalization to the user while browsing the WWW by performing a breadth-first search on the links ahead and provide navigation recommendations.
Reference: [12] <author> B. Tarry M. Lennon, D. Pierce. </author> <title> An evaluation of some conflation algorithms for information retrieval. </title> <journal> Journal of Information Science, </journal> <pages> pages 177-183, </pages> <year> 1981. </year>
Reference-contexts: The stemmer algorithm used here is a modified version of that introduced by Porter in [15] (a good overview of stemming algorithms can be found in <ref> [12] </ref>). The output of the stemmer is then filtered from all commonly-used English language words (for instance "the", "it", "for" "will" etc). Finally, each keyword is weighted by producing its tfidf measure.
Reference: [13] <author> P. Maes. </author> <title> Agents that reduce work and information overload. </title> <journal> Communications of the ACM, </journal> <volume> 37, No. 7 </volume> <pages> 31-40, </pages> <year> 1994. </year>
Reference-contexts: We are witnessing a paradigm shift in human-computer interaction from direct manipulation of computer systems to indirect management <ref> [13] </ref> in which agents play an important role. The domain we focus on is that of personalized information filtering and information discovery.
Reference: [14] <author> R. Armstrong D. Freitag T. Joachims T. Mitchell. Webwatcher: </author> <title> A learning apprentice for the world wide web. </title> <booktitle> In Proceedings of the Symposium on Information Gathering from Heterogeneous, Distributed Environments. </booktitle> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference-contexts: RAISE is a rule-based system that provides a framework for knowledge reuse in different domains (like electronic mail, newsgroups e.t.c.) INFOrmer [16], developed at University of Cork introduces the idea of using associative networks instead of keywords for information retrieval. Another category of WWW agents includes CMU's Webwatcher <ref> [14] </ref> and MIT Media Laboratory's Letizia [11]. These agents are designed to assist and providing personalization to the user while browsing the WWW by performing a breadth-first search on the links ahead and provide navigation recommendations.
Reference: [15] <author> M. Porter. </author> <title> An algorithm for suffix stripping. </title> <booktitle> Program, </booktitle> <pages> pages 130-138, </pages> <year> 1980. </year>
Reference-contexts: The newly created text file is then passed through a stemmer, which removes the suffix of the words and keeps only their roots (for instance the words "proposing" and "proposal" both become "propos"). The stemmer algorithm used here is a modified version of that introduced by Porter in <ref> [15] </ref> (a good overview of stemming algorithms can be found in [12]). The output of the stemmer is then filtered from all commonly-used English language words (for instance "the", "it", "for" "will" etc). Finally, each keyword is weighted by producing its tfidf measure.
Reference: [16] <author> A. O Riordan and H. Sorensen. </author> <title> An intelligent agent for high-precision information filtering. </title> <booktitle> In Proceedings of the CIKM-95 Conference, </booktitle> <year> 1995. </year>
Reference-contexts: Another similar system is RAISE [7], developed by IBM. RAISE is a rule-based system that provides a framework for knowledge reuse in different domains (like electronic mail, newsgroups e.t.c.) INFOrmer <ref> [16] </ref>, developed at University of Cork introduces the idea of using associative networks instead of keywords for information retrieval. Another category of WWW agents includes CMU's Webwatcher [14] and MIT Media Laboratory's Letizia [11].
Reference: [17] <author> G. Salton and C. Buckley. </author> <title> Text Weighting Approaches in Automatic Text Retrieval. </title> <type> Cornell University Technical Report 87-881, </type> <year> 1987. </year>
Reference-contexts: On top of this library is used for canonicalizing URLs before they are stored in the "already retrieved" database (more on that on section 3.8). 3.2 Document Representation: Keyword Vectoring Amalthaea's internal representation of documents is based on a standard information retrieval technique, weighted vector representation <ref> [17] </ref>. After their retrieval, the doc 1 By "information" we mean documents from various sources like WWW pages, gopher sites, ftp sites, WWW-based information services, newsgroups etc. 2 Alternative ways of creating http connections are explored.
Reference: [18] <author> B. Sheth and P. Maes. </author> <title> Evolving agents for personalized information filtering. </title> <booktitle> In Proceedings of the Ninth Conference on Artificial Intelligence for Applications, 1993. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: More similar to our work in terms of application domain and representation is the system build by Balabanovic and Shoham [5]. They introduced a system for WWW document filtering which also utilized the weighted keyword vector representation. In terms of evolutionary filtering systems, NewT <ref> [18] </ref> developed by Sheth, is a multiagent system that uses evolution and relevance feedback for information filtering. NewT's application domain is structured newsgroups documents (clarinet) and the system is able to adapt successfuly to such a dynamic environment.
References-found: 18

