URL: http://www.cs.ucsd.edu/~calder/pfdc/papers/pfdc-eun-jin-im.ps.Z
Refering-URL: http://www.cs.ucsd.edu/~calder/pfdc/program.html
Root-URL: http://www.cs.ucsd.edu
Title: Model-Based Memory Hierarchy Optimizations for Sparse Matrices  
Author: Eun-Jin Im and Katherine Yelick 
Address: Berkeley  
Affiliation: Computer Science Division University of California,  
Abstract: Sparse matrix-vector multiplication is an important computational kernel used in numerical algorithms. It tends to run much more slowly than its dense counterpart, and its performance depends heavily on both the nonzero structure of the sparse matrix and on the machine architecture. In this paper we address the problem of optimizing sparse matrix-vector multiplication for the memory hierarchies that exist on modern machines and how machine-specific or matrix-specific profiling information can be used to decide which optimizations should be applied and what parameters should be used. We also consider a variation of the problem in which a matrix is multiplied by a set of vectors. Performance is measured on a 167 MHz Ultra-sparc I, 200 MHz Pentium Pro, and 450 MHz DEC Alpha 21164. Experiments show these optimization techniques to have significant payoff, although the effectiveness of each depends on the matrix structure and machine. 
Abstract-found: 1
Intro-found: 1
Reference: [ABB + 95] <author> E. Anderson, Z. Bai, C. Bischof, J. Dem-mel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <note> LAPACK Users' Guide. SIAM, </note> <month> February </month> <year> 1995. </year>
Reference: [BAD + 97] <author> Jeff A Bilmes, Krste Asanovic, Jim Demmel, CheeWhye Chin, and Dominic Lam. </author> <title> Optimizing matrix multiply using PHiPAC: a portable, high-performance, ANSI C coding methodology. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1997. </year>
Reference-contexts: The problem of determining the size of a cache block so that cache conflicts between different array elements is avoided is not easy even for dense matrices and Lam [MSLW91], Cole-man [CM95], and Essenghir [Ess93] gave algorithms to determine blocking sizes using only cache characteristics and matrix size. PHiPAC <ref> [BAD + 97] </ref> approaches this problem in a different way in that blocked code is generated on a target machine using a parameterized code generator and the block size is chosen by measuring the performance of those generated codes with different parameters. <p> Determining block size even for dense codes requires feedback from benchmarking, because register level blocking interacts with instruction scheduling, especially the use of multiple functional units, prefetching and write buffers <ref> [BAD + 97, WD] </ref>. Third, for sparse matrices, there is an additional problem of costs that depend on the sparsity structure of the matrix. Here, we focus on the latter two problems of selecting the block size for the specific problem of sparse matrix vector multiplication.
Reference: [Bik96] <author> Aart J. C. Bik. </author> <title> Compiler Support for Sparse Matrix Computations. </title> <type> PhD thesis, </type> <institution> Leiden University, </institution> <year> 1996. </year>
Reference-contexts: PHiPAC [BAD + 97] approaches this problem in a different way in that blocked code is generated on a target machine using a parameterized code generator and the block size is chosen by measuring the performance of those generated codes with different parameters. For sparse matrices, Bik <ref> [Bik96] </ref> and Kotlyar's [KPS97] work generates a sparse code from dense code with the compiler, in order to relieve the programmer's effort in writing complex sparse code and to increase the flexibility of the code for various stor age formats of sparse matrices.
Reference: [Car94] <author> Steve Carr. </author> <title> Memory-Hierarchy Management. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: The other is from the library developer's perspective, in which several optimized versions are provided and the user selects the routine to use. For dense matrix operations, Wolf [Wol92] and Carr <ref> [Car94] </ref> formulated loop transformation theory for array-based loop nests; it was used to transform loops into blocked loops.
Reference: [CM95] <author> S. Coleman and K. S. McKinley. </author> <title> Tile size selection using cache organization and data layout. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: The problem of determining the size of a cache block so that cache conflicts between different array elements is avoided is not easy even for dense matrices and Lam [MSLW91], Cole-man <ref> [CM95] </ref>, and Essenghir [Ess93] gave algorithms to determine blocking sizes using only cache characteristics and matrix size. <p> This optimization is commonly known as blocking or tiling for dense matrix operations <ref> [MSLW91, CM95] </ref>. Register blocking in sparse matrix codes requires information that is not available at compile time for several reasons. First, the nonzero structure of the sparse matrix is not known statically, so runtime-optimization techniques are needed to determine which transformations are legal [DHU + 93].
Reference: [DCHH88] <author> J. Dongarra, J. Du Croz, S. Hammarling, and R. J. Hanson. </author> <title> An extended set of FORTRAN Basic Linear Algebra Subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 14 </volume> <pages> 1-17, </pages> <month> March </month> <year> 1988. </year>
Reference: [DHU + 93] <author> R. Das, Y.-S. Hwang, M. Uysal, J. Saltz, and A. Sussman. </author> <title> Applying the CHAOS/PARTI library to irregular problems in computational chemistry and computational aerodynamics. </title> <booktitle> In Proceedings of the 1993 Scalable Parallel Libraries Conference, </booktitle> <pages> pages 45-56. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1993. </year>
Reference-contexts: Register blocking in sparse matrix codes requires information that is not available at compile time for several reasons. First, the nonzero structure of the sparse matrix is not known statically, so runtime-optimization techniques are needed to determine which transformations are legal <ref> [DHU + 93] </ref>. Second, the problem of tile-size selection requires information about the machine characteristics which may not be captured in a simple model with the number of registers and memory costs.
Reference: [Ess93] <author> Karim Essenghir. </author> <title> Improving data locality for caches. </title> <type> Master's thesis, </type> <institution> Rice University, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: The problem of determining the size of a cache block so that cache conflicts between different array elements is avoided is not easy even for dense matrices and Lam [MSLW91], Cole-man [CM95], and Essenghir <ref> [Ess93] </ref> gave algorithms to determine blocking sizes using only cache characteristics and matrix size.
Reference: [GU77] <author> G. H. Golub and R. Underwood. </author> <title> The Block Lanczos Method for Computing Eigenvalues. </title> <editor> In J. R. Rice, editor, </editor> <booktitle> Mathematical Sotware III, </booktitle> <pages> pages 361-377. </pages> <publisher> Academic Press, Inc., </publisher> <year> 1977. </year>
Reference-contexts: Algorithms such as block Lanczos <ref> [GU77] </ref>, which compute a set of eigen-values and associated eigenvectors, require multiplication of matrix to a set of vectors.
Reference: [HST95] <author> S. A. Hutchinson, J. N. Shadid, and R. S. Tu-minaro. </author> <title> Aztec user's guide: Version 1.1. </title> <type> Technical Report SAND95-1559, </type> <institution> Sandia National Laboratories, </institution> <year> 1995. </year>
Reference-contexts: As a result of effort to provide a generic sparse matrix operation library, NIST sparseBLAS [PR97] provides generic routines and TNT (Template Numerical Toolkit) [Poz97] provides a generic matrix/vector classes. BlockSolve [PJ95] and Aztec <ref> [HST95] </ref> are parallel iterative solvers that include the implementation of optimized sparse matrix operations.
Reference: [KPS97] <author> Vladimir Kotlyar, Keshav Pingali, and Paul Stodghill. </author> <title> Compiling parallel code for sparse matrix applications. </title> <booktitle> In Supercomputing, </booktitle> <year> 1997. </year>
Reference-contexts: For sparse matrices, Bik [Bik96] and Kotlyar's <ref> [KPS97] </ref> work generates a sparse code from dense code with the compiler, in order to relieve the programmer's effort in writing complex sparse code and to increase the flexibility of the code for various stor age formats of sparse matrices.
Reference: [MSLW91] <author> E. E. Rothberg M. S. Lam and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: The problem of determining the size of a cache block so that cache conflicts between different array elements is avoided is not easy even for dense matrices and Lam <ref> [MSLW91] </ref>, Cole-man [CM95], and Essenghir [Ess93] gave algorithms to determine blocking sizes using only cache characteristics and matrix size. <p> This optimization is commonly known as blocking or tiling for dense matrix operations <ref> [MSLW91, CM95] </ref>. Register blocking in sparse matrix codes requires information that is not available at compile time for several reasons. First, the nonzero structure of the sparse matrix is not known statically, so runtime-optimization techniques are needed to determine which transformations are legal [DHU + 93].
Reference: [PJ95] <author> Paul Plassmann and M. T. Jones. </author> <title> Block-Solve95 users manual: Scalable library software for the parallel solution of sparse linear systems. </title> <type> Technical Report ANL-95/48, </type> <institution> Ar-gonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: As a result of effort to provide a generic sparse matrix operation library, NIST sparseBLAS [PR97] provides generic routines and TNT (Template Numerical Toolkit) [Poz97] provides a generic matrix/vector classes. BlockSolve <ref> [PJ95] </ref> and Aztec [HST95] are parallel iterative solvers that include the implementation of optimized sparse matrix operations.
Reference: [Poz97] <author> Roldan Pozo. </author> <title> Template numerical toolkit (TNT), </title> <note> 1997. http://math.nist.gov/tnt. </note>
Reference-contexts: As a result of effort to provide a generic sparse matrix operation library, NIST sparseBLAS [PR97] provides generic routines and TNT (Template Numerical Toolkit) <ref> [Poz97] </ref> provides a generic matrix/vector classes. BlockSolve [PJ95] and Aztec [HST95] are parallel iterative solvers that include the implementation of optimized sparse matrix operations.
Reference: [PR97] <author> Roldan Pozo and Karin Remington. </author> <title> NIST Sparse BLAS, </title> <note> 1997. http://math.nist.gov/spblas. </note>
Reference-contexts: As a result of effort to provide a generic sparse matrix operation library, NIST sparseBLAS <ref> [PR97] </ref> provides generic routines and TNT (Template Numerical Toolkit) [Poz97] provides a generic matrix/vector classes. BlockSolve [PJ95] and Aztec [HST95] are parallel iterative solvers that include the implementation of optimized sparse matrix operations.
Reference: [WD] <author> R. Clint Whaley and Jack Dongarra. </author> <title> Automatically tuned linear algebra software (ATLAS). </title> <address> http://www.netlib.org/atlas. </address>
Reference-contexts: Determining block size even for dense codes requires feedback from benchmarking, because register level blocking interacts with instruction scheduling, especially the use of multiple functional units, prefetching and write buffers <ref> [BAD + 97, WD] </ref>. Third, for sparse matrices, there is an additional problem of costs that depend on the sparsity structure of the matrix. Here, we focus on the latter two problems of selecting the block size for the specific problem of sparse matrix vector multiplication.
Reference: [Wol92] <author> Michael E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> PhD thesis, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: One is from the compiler writer's perspective, where the focus is on how to automatically generate memory efficient code. The other is from the library developer's perspective, in which several optimized versions are provided and the user selects the routine to use. For dense matrix operations, Wolf <ref> [Wol92] </ref> and Carr [Car94] formulated loop transformation theory for array-based loop nests; it was used to transform loops into blocked loops.
References-found: 17

