URL: ftp://cogsci.indiana.edu/pub/french.tandem-stm-ltm.ps.Z
Refering-URL: http://vita.mines.edu:3857/0/lpratt/transfer.html
Root-URL: 
Email: french@cogsci.indiana.edu  
Title: Interactive tandem networks and the sequential learning problem  
Author: Robert M. French 
Address: Bloomington, IN 47408  
Affiliation: Indiana University,  
Note: Center for Research on Concepts and Cognition  
Abstract: This paper presents a novel connectionist architecture to handle the "sensitivity-stability" problem and, in particular, an extreme manifestation of the problem, catastrophic interference. This architecture, called an interactive tandem-network (ITN) architecture, consists of two continually interacting networks, one the LTM network dynamically storing "prototypes" of the patterns learned, the other the STM network being responsible for "short-term" learning of new patterns. Prototypes stored in the LTM network influence hidden-layer representations in the STM network and, conversely, newly learned representations in the STM network gradually modify the more stable LTM prototypes. As prototypes are learned by the LTM network, they are dynamically constrained to maximize mutual orthogonality. This system of tandem networks performs particularly well on the problem of catastrophic interference. It also produces "long-term" representations that are stable in the face of new input and "short-term" representations that remain sensitive to new input. Justification for this type of architecture is similar to that given recently by McClelland, McNaughton, & O'Reilly (1994) in arguing for the necessary complementarity of the hippocampal (short-term memory) and neocortical (long-term memory) systems. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, N. H. </author> <year> (1981). </year> <title> Foundations of information integration theory. </title> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference-contexts: This method of prototype averaging is patterned after Anderson's weighted averaging model of information integration <ref> (Anderson, 1981) </ref>. Before the LTM-network actually learns this new prototype, it must first be "context-biased" (described below) in order to increase its orthogonality with respect to the other prototypes stored in the LTM-network.
Reference: <author> Elman, J. L. </author> <title> (1990) Finding Structure in time. </title> <booktitle> Cognitive Science, </booktitle> <pages> 14 , 179-211. </pages>
Reference: <author> French, R. M. </author> <title> (1991) Using semi-distributed representations to overcome catastrophic forgetting in connectionist networks. </title> <booktitle> In Proceedings of the 13th Annual Cognitive Science Society Conference. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum, </publisher> <pages> 173-178. </pages>
Reference-contexts: The new set of activations for the biased representation is then "locked into" the input-to-hidden weights by backpropagating from the hidden layer to the input layer an "error signal" consisting of the difference between R natural and R biased . This "locking-in" technique is discussed in <ref> (French 1991) </ref>. It is to be noted that numerous techniques involving dynamically "massaging" the hidden-layer representations in order achieve certain types of representations have been developed (e.g., Kruschke 1989; French 1991; and Murre 1993).
Reference: <author> French, R. M. </author> <title> (1994) Dynamically contraining connectionist networks to produce orthogonal, distributed representations to reduce catastrophic interference. </title> <booktitle> In Proceedings of the 16th Annual Cognitive Science Society Conference. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum, </publisher> <pages> 335-340. </pages>
Reference-contexts: The former stores prototypes that have been gradually built up from representations learned in the STM. These prototypes are distributed and separated in the LTM by a technique called context-biasing <ref> (French 1994) </ref>. The prototypes in LTM also influence new representations in STM.
Reference: <author> French, R. M. </author> <year> (1994). </year> <title> Catastrophic forgetting in connectionist networks: Can it be predicted, can it be prevented? In Cowan, </title> <editor> J.D., Tesauro,G., & Alspector, J. (eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kauffmann, </publisher> <pages> 1176-1177 Hebb, </pages> <address> D. O. </address> <year> (1949). </year> <title> Organization of Behavior. </title> <address> New York, N.Y.: </address> <publisher> Wiley & Sons. </publisher>
Reference-contexts: The former stores prototypes that have been gradually built up from representations learned in the STM. These prototypes are distributed and separated in the LTM by a technique called context-biasing <ref> (French 1994) </ref>. The prototypes in LTM also influence new representations in STM.
Reference: <author> Hetherington, P. A. and Seidenberg, M. S., </author> <year> (1989), </year> <title> Is there 'catastrophic interference' in connectionist networks?, </title> <booktitle> In Proceedings of the 11th Annual Conference of the Cognitive Science Society, </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum, </publisher> <pages> 26-33. </pages>
Reference-contexts: Democrat AAAA AAAA Republican same problem, there is far greater representational overlap, and, as a result, worse catastrophic interference how completely the network had forgotten the original data <ref> (Hetherington & Seidenberg, 1989) </ref>. As can be seen in Figure 2, the ITN developed STM representations that were both well distributed across the hidden layer and were also well separated. Figure 3 shows the considerably greater overlap of representations when a standard backpropagation network is used.
Reference: <author> Hinton, G. E. & Plaut, D. C. </author> <title> (1987) Using Fast Weights to Deblur Old Memories. </title> <booktitle> In Proceedings of the 9th Annual Conference of the Cognitive Science Society, </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum, </publisher> <pages> 177-186. </pages>
Reference: <author> Kaplan, S., Chown, E., & Sonntag, M., </author> <title> (1991) Tracing recurrent activity in cognitive elements: The trace mode of temporal dynamics in a cell-assembly. </title> <journal> Connection Science. </journal>
Reference: <author> Kruschke, J. K. </author> <title> (1989) Distributed bottlenecks for improved generalization in backpropagation networks. </title> <journal> International Journal of Neural Networks Research & Applications, </journal> <volume> 1 , 187193. </volume>
Reference: <author> Kruschke, J. K. </author> <title> (1993) Human Category Learning: Implications for Backpropagation Models. </title> <journal> Connection Science, </journal> <volume> Vol. 5, No. 1, </volume> <year> 1993. </year>
Reference: <author> McClelland, J., McNaughton, B., & O'Reilly, R., </author> <title> Why there are complementary learning systems in the hippocampus and neocortex. </title> <type> CMU Tech Report PDP.CNS.94.1, </type> <month> March </month> <year> 1994. </year>
Reference: <author> McCloskey, M. & Cohen, N. J. </author> <year> (1989). </year> <title> "Catastrophic interference in connectionist networks: The sequential learning problem" The Psychology of Learning and Motivation, </title> <booktitle> 24, </booktitle> <pages> 109-165. </pages>
Reference: <author> McRae, K. & Hetherington, P. </author> <title> (1993) Catastrophic interference is eliminated in pretrained networks. </title> <booktitle> In Proceedings of the 15h Annual Conference of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher> <pages> 723-728. </pages>
Reference: <author> Murphy, P. & Aha, D. </author> <year> (1992). </year> <title> UCI repository of machine learning databases. </title> <institution> Maintained at the Dept. of Information and Computer Science, U.C. </institution> <address> Irvine, Irvine, CA. </address>
Reference-contexts: It is to be noted that numerous techniques involving dynamically "massaging" the hidden-layer representations in order achieve certain types of representations have been developed (e.g., Kruschke 1989; French 1991; and Murre 1993). Results To test this model, I used data from the 1984 Congressional Voting Records <ref> (Murphy & Aha, 1992) </ref>, which gives the voting record and party affiliation (Republican or Democrat) of each member of Congress in 1984. The network was trained to associate 50 different voting patterns with party affiliation.
Reference: <author> Murre, J. </author> <title> (1992) The effects of pattern presentation on interference in backpropagation networks. </title> <booktitle> In Proceedings of the 14th Annual Conference of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher> <pages> 54-59. </pages>
Reference: <author> Ratcliff, R. </author> <title> (1990) Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions, </title> <booktitle> 97, </booktitle> <pages> 285-308. </pages>
Reference: <author> Sharkey, N. & Sharkey, A., </author> <title> (1994) Understanding Catastrophic Interference in Neural Nets, </title> <institution> Computer Science Technical Report CS-94-4, University of Sheffield, Sheffield, </institution> <address> England. </address>
Reference: <author> Tetewsky, S., Shultz, T., & Buckingham, D. </author> <title> Assessing Interference and Savings in Connectionist Models of Human Recognition Memory, </title> <type> Psychology Department technical report, </type> <institution> McGill University, </institution> <note> presented at 1994 Meeting of the Psychonomic Society. </note>
References-found: 18

