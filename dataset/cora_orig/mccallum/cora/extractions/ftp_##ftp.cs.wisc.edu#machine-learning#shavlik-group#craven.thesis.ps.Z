URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/craven.thesis.ps.Z
Refering-URL: http://www.cs.wisc.edu/~craven/craven.html
Root-URL: 
Title: EXTRACTING COMPREHENSIBLE MODELS FROM TRAINED NEURAL NETWORKS  
Author: By Mark W. Craven 
Degree: A thesis submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1996  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: <author> Aha, D., Kibler, D., & Albert, M. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66. </pages>
Reference: <author> Alexander, J. A. & Mozer, M. C. </author> <year> (1995). </year> <title> Template-based algorithms for connectionist rule extraction. </title> <editor> In Tesauro, G., Touretzky, D., & Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 7). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Almuallim, H. & Dietterich, T. G. </author> <year> (1991). </year> <title> Learning with many irrelevant features. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 547-552), </pages> <address> Anaheim, CA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Andrews, R. & Geva, S. </author> <year> (1995). </year> <title> Inserting and extracting knowledge from constrained error backpropagation networks. </title> <booktitle> In Proceedings of the Sixth Australian Conference on Neural Networks, </booktitle> <address> Sydney, Australia. </address>
Reference: <author> Angluin, D. </author> <year> (1988). </year> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342. </pages>
Reference-contexts: The key aspects of Trepan are described in detail below. 3.2.1 Membership Queries and the Oracle The generality of Trepan derives from the fact that its interaction with the network consists solely of membership queries <ref> (Angluin, 1988) </ref>. A membership query is a question to an oracle that consists of an instance from the learner's instance space. Given a membership query, the oracle returns the class label for the instance. <p> My experience with this algorithm led to the adoption of decision trees as the representation language for Trepan. Another notable aspect of this earlier algorithm is that it used subset queries <ref> (Angluin, 1988) </ref> instead of membership queries. A subset query specifies a region of the instance space, and asks an oracle if all of the instances in the region are members of a given class. <p> He proved that decision trees in Boolean domains are exactly learnable when the learner is allowed to make membership and equivalence queries <ref> (Angluin, 1988) </ref>. Recall that a membership query is a question to an oracle that consists of an instance from the learner's instance space. Given a membership query, the oracle returns the class label for the instance.
Reference: <author> Angluin, D. </author> <year> (1993). </year> <title> Learning with queries. </title> <editor> In Baum, E. B., editor, </editor> <booktitle> Computational Learning & Cognition: Proceedings of the Third NEC Research Symposium. </booktitle> <publisher> SIAM Books, </publisher> <address> Philadel-phia, PA. </address>
Reference-contexts: Trepan, for example, is an active learning algorithm in that it selectively draws instances and makes membership queries for these instances. Under the rubric "learning with queries," active learning has been extensively investigated in the computational learning theory community. Angluin (1988) provides a taxonomy of queries, and elsewhere <ref> (Angluin, 1993) </ref> reviews learnability results for algorithms that use queries. These results are compelling because they demonstrate that some learning problems are efficiently learnable only if the learner is allowed to make queries. Hence, in some practical 159 situations the ability to make queries should be a significant advantage.
Reference: <author> Ash, T. </author> <year> (1989). </year> <title> Dynamic node creation in backpropagation networks. </title> <journal> Connection Science, </journal> <volume> 1(4) </volume> <pages> 365-376. </pages>
Reference: <author> Atlas, L., Cole, R., Connor, J., El-Sharkawi, M., Marks II, R., Muthusamy, Y., & Barnard, E. </author> <year> (1989). </year> <title> Performance comparisons between backpropagation networks and classification trees on three real-world applications. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Auer, P., Holte, R. C., & Maass, W. </author> <year> (1995). </year> <title> Theory and applications of agnostic PAC-learning with small decision trees. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> (pp. 21-29), </pages> <address> Tahoe City, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: There is an algorithm called T2 <ref> (Auer et al., 1995) </ref> that is an agnostic PAC-learning algorithm for the class of decision trees that have two levels of internal nodes. Could T2 be profitably used for rule extraction? The answer is a qualified yes.
Reference: <author> Bao, G., Cassandras, C. G., Djaferis, T. E., Gandhi, A. D., & Looze, D. P. </author> <year> (1994). </year> <title> Elevator dispatchers for down peak traffic. </title> <type> Technical report, </type> <institution> ECE Department, University of Massachusetts, </institution> <address> Amherst, MA. </address> <note> 186 187 Baum, </note> <author> E. & Lang, K. </author> <year> (1991). </year> <title> Constructing hidden units using examples and queries. </title> <editor> In Lippmann, R., Moody, J., & Touretzky, D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 3). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Becker, S. & Le Cun, Y. </author> <year> (1988). </year> <title> Improving the convergence of back-propagation learning with second order methods. </title> <editor> In Hinton, G. E., Sejnowski, T. J., & Touretzky, D. S., editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> (pp. 29-37), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Other optimization methods can be used to minimize the cost function. Standard gradient descent augmented with a momentum term is sometimes used, as is the conjugate-gradient method (Kramer & Sangiovanni-Vincentelli, 1989), and even second-order methods <ref> (Becker & Le Cun, 1988) </ref>. Often, network training is stopped before a local minimum in the cost function is reached. The motivation underlying this technique of early stopping is that over-fitting may occur if the network is trained to fit the training data too closely.
Reference: <author> Blasig, R. </author> <year> (1994). </year> <title> GDS: Gradient descent generation of symbolic classification rules. </title> <editor> In Cowan, J., Tesauro, G., & Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 6). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Blum, A. </author> <year> (1995). </year> <title> Empirical support for Winnow and Weighted-Majority based algorithms: results on a calendar scheduling domain. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> (pp. 64-72), </pages> <address> Tahoe City, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Finally, Winnow is selected because, like BBP, it learns hypotheses that are linear-threshold functions, and because it has provably good performance when given irrelevant features. Additionally, Winnow has recently shown its practical value by demonstrating excellent predictive performance in an interesting, real-world application <ref> (Blum, 1995) </ref>. We use the 135 Balanced version of Winnow, rather than one of the other versions (Littlestone, 1989), largely due to its empirical success. We now describe these algorithms in more detail, and discuss how they are applied in the experiments.
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. </author> <year> (1989). </year> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of ACM, </journal> <volume> 36(4) </volume> <pages> 929-965. </pages>
Reference-contexts: While there are already known algorithms for PAC learning the general class of perceptrons <ref> (e.g., Blumer et al., 1989) </ref>, an atypical aspect of the BBP algorithm is that when learning target functions which are in the class of sparse perceptrons, it produces hypotheses that are relatively sparse themselves.
Reference: <author> Breiman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth and Brooks, </publisher> <address> Monterey, CA. </address>
Reference-contexts: When an example reaches a leaf, the class associated with the leaf is the prediction made by the decision tree for that example. 2.2.2 Decision-Tree Learning The two most widely used decision-tree induction algorithms are C4.5 (Quinlan, 1993), which arose in the artificial intelligence community, and CART <ref> (Breiman et al., 1984) </ref> which was developed in the statistics community. C4.5 is the successor to the ID3 algorithm (Quinlan, 1986). These two algorithms, and numerous variants of them, are similar in their overall structure, but differ somewhat in details. <p> Like conventional decision-tree induction algorithms, such as CART <ref> (Breiman et al., 1984) </ref>, and C4.5 (Quinlan, 1993), Trepan builds a decision tree by recursively partitioning the instance space. Unlike these algorithms, however, Trepan constructs a decision tree in a best-first manner (the notion of "best" is described shortly). <p> A regression tree is a tree-structured model like a decision tree, except that its leaves are characterized by real-valued functions as opposed to predicted classes. The CART algorithm <ref> (Breiman et al., 1984) </ref>, for example, learns regression trees in which a linear function is associated with each leaf. Beyond Trees The general approach underlying the Trepan algorithm is to view rule extraction as a learning task.
Reference: <author> Bruck, J. </author> <year> (1990). </year> <title> Harmonic analysis of polynomial threshold functions. </title> <journal> SIAM Journal of Discrete Mathematics, </journal> <volume> 3(2) </volume> <pages> 168-177. </pages>
Reference-contexts: Each function f 2 H r can be defined as a linear threshold over a set R of Boolean features, where jRj r. For a given such R there are at most 2 (r+1) 2 distinct Boolean functions defined by linear thresholds over R <ref> (e.g., Bruck, 1990) </ref>.
Reference: <author> Bshouty, N. H. </author> <year> (1993). </year> <title> Exact learning via the monotone theory. </title> <booktitle> In Proceedings of the 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> (pp. 302-311). </pages> <publisher> IEEE Press. </publisher>
Reference: <author> Buntine, W. & Niblett, T. </author> <year> (1992). </year> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 75-86. </pages>
Reference: <author> Carpenter, G. A., Grossberg, S., Markuzon, N., Reynolds, J. H., & Rosen, D. B. </author> <year> (1992). </year> <title> Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3 </volume> <pages> 698-713. </pages>
Reference-contexts: As with the method of Tresp and Hollatz, extracting rules from these networks involves a straightforward process of translating each local function directly into a rule. Finally, Tan (1994) has presented a rule extraction method for complex neural-network architecture called the Fuzzy ARTMAP <ref> (Carpenter et al., 1992) </ref>. As is the case for networks with local basis functions, rule-extraction in this context involves directly translating parts of the network architecture into rules.
Reference: <author> Caruana, R. </author> <year> (1996). </year> <title> Algorithms and applications for multitask learning. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <address> Bari, Italy. </address>
Reference: <author> Caruana, R. & Freitag, D. </author> <year> (1994). </year> <title> Greedy attribute selection. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> (pp. 28-36), </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Catlett, J. </author> <year> (1992). </year> <title> Peepholing: Choosing attributes efficiently for megainduction. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> (pp. 49-54), </pages> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Chatterjee, S. & Hadi, A. S. </author> <year> (1988). </year> <title> Sensitivity Analysis in Linear Regression. </title> <publisher> Wiley, </publisher> <address> New York, NY. </address> <note> 188 Cherkauer, </note> <author> K. & Shavlik, J. </author> <year> (1996). </year> <title> Growing simpler decision trees to facilitate knowledge discovery. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> (pp. 315-318), </pages> <address> Portland, OR. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Clark, P. & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-284. </pages>
Reference: <author> Cleeremans, A., Servan-Schreiber, D., & McClelland, J. </author> <year> (1989). </year> <title> Finite state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 372-381. </pages>
Reference: <author> Cohn, D., Atlas, L., & Ladner, R. </author> <year> (1994). </year> <title> Improving generalization with active learning. </title> <journal> Machine Learning, </journal> <volume> 15(2) </volume> <pages> 201-221. </pages>
Reference: <author> Cohn, D. A., Ghahramani, Z., & Jordan, M. I. </author> <year> (1996). </year> <title> Active learning with statistical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 129-145. </pages>
Reference: <author> Craven, M. & Shavlik, J. </author> <year> (1992). </year> <title> Visualizing learning and computation in artificial neural networks. </title> <journal> International Journal on Artificial Intelligence Tools, </journal> <volume> 1(2) </volume> <pages> 399-425. </pages>
Reference-contexts: a network's connections (Hinton, 1986; Wejchert & Tesauro, 1989; Craven & Shavlik, 1992; Pomerleau & Touretzky, 1993), * the decision boundaries formed by units in a network (Lang & Witbrock, 1988; Munro, 1991; Pratt et al., 1991), * unit activations and the forward propagation of activation signals through the network <ref> (Craven & Shavlik, 1992) </ref>, * the sensitivity of hidden and output unit activations to smoothly varying inputs (Pomer leau & Touretzky, 1993), * the backward propagation of error signals during learning (Craven & Shavlik, 1992), * the trajectory of units in weight space during learning (Wejchert & Tesauro, 1989). <p> Munro, 1991; Pratt et al., 1991), * unit activations and the forward propagation of activation signals through the network <ref> (Craven & Shavlik, 1992) </ref>, * the sensitivity of hidden and output unit activations to smoothly varying inputs (Pomer leau & Touretzky, 1993), * the backward propagation of error signals during learning (Craven & Shavlik, 1992), * the trajectory of units in weight space during learning (Wejchert & Tesauro, 1989). These visualization methods can sometimes provide insight into the learning and prediction behavior of a network.
Reference: <author> Craven, M. & Shavlik, J. </author> <year> (1993a). </year> <title> Learning symbolic rules using artificial neural networks. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> (pp. 73-80), </pages> <address> Amherst, MA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Since they correspond the symbolic rules, the weights are initially well clustered, and empirical results indicate that the weights of knowledge-based neural networks remain fairly clustered after training. The applicability of this approach was later extended to ordinary neural networks by using a special cost function for network training <ref> (Craven & Shavlik, 1993a) </ref>; this work is described in detail in Chapter 6. 37 Blasig (1994) also developed a method that uses a special cost function to encourage hidden and output units to stay in rule-like states during training. <p> The MofN-sws algorithm, and portions of the experimental evaluation of the algorithm reported in this chapter were previously published in a conference paper <ref> (Craven & Shavlik, 1993a) </ref>. Chapter 7 The Boosting-Based Perceptron Learning Algorithm Unlike the methods presented previously in this thesis, the algorithm introduced in this section is not a method for extracting rules from neural networks. Rather, the Boosting-Based Perceptron (BBP) learning algorithm is a method for learning sparse perceptrons.
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1993b). </year> <title> Learning to predict reading frames in E. coli DNA sequences. </title> <booktitle> In Proceedings of the 26th Hawaii International Conference on System Sciences, </booktitle> <pages> (pp. 773-782), </pages> <address> Wailea, HI. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: They include: recognizing protein-coding regions in E. coli DNA sequences <ref> (Craven & Shavlik, 1993b) </ref>, diagnosing the presence of heart disease in patients (Detrano et al., 1989), mapping English text into its pronunciation (Sejnowski & Rosenberg, 1987), recognizing promoters in E. coli DNA sequences (Towell et al., 1990), diagnosing faults in local telephone loops (Provost & Danyluk, 1995), and predicting the party <p> In cases where multiple trees were extracted for a given domain, I show trees that are typical, in terms of their syntactic complexity, of the trees extracted in the reported experiments. 173 174 A.1 The Coding Domain The features in the coding domain <ref> (Craven & Shavlik, 1993b) </ref> represent the 64 codons (three-letter words) that can be formed from the DNA bases: fa, c, g, tg. Each Boolean feature indicates the presence or absence of a given codon "in-frame" in the sequence being classified.
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1993c). </year> <title> Learning to represent codons: A challenge problem for constructive induction. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 1319-1324), </pages> <address> Chambery, France. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1994). </year> <title> Using sampling and queries to extract rules from trained neural networks. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> (pp. 37-45), </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thus, Trepan does not assume that it is given an exhaustive set of instances, but instead selectively draws instances and makes membership queries as needed. In earlier work <ref> (Craven & Shavlik, 1994) </ref>, I developed a precursor to Trepan that also treated the rule-extraction task as an inductive learning problem. This algorithm, however, extracted inference rules instead of decision trees, and therefore suffered from the limitations of using rules discussed above. <p> Another key aspect of Trepan is that it uses m-of-n expressions as splitting tests in the trees it induces. These expressions often result in more concise and comprehensible trees. The idea of viewing rule extraction as an inductive learning task was introduced in an earlier publication <ref> (Craven & Shavlik, 1994) </ref>. A description of the Trepan algorithm and some of experiments reported in the following chapter were also previously published (Craven & Shavlik, 1996). Chapter 4 Empirical Evaluation of Trepan This chapter provides an empirical evaluation of the Trepan approach.
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1996). </year> <title> Extracting tree-structured representations of trained networks. </title> <editor> In Touretzky, D., Mozer, M., & Hasselmo, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 8). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The idea of viewing rule extraction as an inductive learning task was introduced in an earlier publication (Craven & Shavlik, 1994). A description of the Trepan algorithm and some of experiments reported in the following chapter were also previously published <ref> (Craven & Shavlik, 1996) </ref>. Chapter 4 Empirical Evaluation of Trepan This chapter provides an empirical evaluation of the Trepan approach. The experiments presented here illustrate the application of Trepan to neural networks trained to solve both supervised and reinforcement learning tasks. <p> This chapter is based on joint work with Jeffrey Jackson. The BBP algorithm was co-developed with Jackson, and he is responsible for the technical contributions presented in Section 7.4. A description of the BBP algorithm and some of the experiments reported in this chapter were previously published elsewhere <ref> (Jackson & Craven, 1996) </ref>. 7.1 Hypothesis Boosting and the AdaBoost Algorithm The BBP algorithm is based on a hypothesis-boosting method called AdaBoost (Freund & Schapire, 1995). Recall from Chapter 5 that a hypothesis-boosting algorithm is one that combines the hypotheses produced by a weak learning algorithm into a strong hypothesis.
Reference: <author> Crites, R. H. & Barto, A. G. </author> <year> (1996). </year> <title> Improving elevator performance using reinforcement learning. </title> <editor> In Touretzky, D., Mozer, M., & Hasselmo, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 8). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Similarly, for some reinforcement learning tasks there exists a good computational model of the problem domain that could be used to generate instances <ref> (e.g., Crites & Barto, 1996) </ref>. This situation is discussed further in Chapter 5. <p> This learning agent is especially interesting because it has demonstrated performance that surpasses the best heuristic, elevator-control algorithms. 81 4.4.1 Problem Domain The reinforcement-learning system that is investigated here is a neural network that addresses the real-world problem of elevator dispatching <ref> (Crites & Barto, 1996) </ref>. As with the exchange-rate network investigated in the previous section, this learning system was developed without any intention of later applying a rule-extraction method to it. The elevator-dispatching agent operates in a simulated 10-story building with four elevator cars (Lewis, 1991; Bao et al., 1994). <p> Measures of the Mark-Dollar exchange rate versus Mark futures. vs SFr future A measure of the Swiss Franc-Dollar exchange rate versus Swiss Franc futures. vs Yen future A measure of the Yen-Dollar exchange rate versus Yen futures. 183 184 A.7 The Elevator-Control Domain The following features from the elevator-control domain <ref> (Crites & Barto, 1996) </ref> are in corporated into the depicted tree. The class labels indicate whether the elevator car should continue in its current direction or stop at the next floor. abbreviation description footprint X These features correspond to the ten floors where the other elevator cars may be located.
Reference: <author> Das, S. & Mozer, M. </author> <year> (1994). </year> <title> A unified gradient-descent/clustering architecture for finite state machine induction. </title> <editor> In Cowan, J., Tesauro, G., & Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 6). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <note> 189 Dennis, </note> <author> S. & Phillips, S. </author> <year> (1991). </year> <title> Analysis tools for neural networks. </title> <type> Technical Report 207, </type> <institution> Department of Computer Science, University of Queensland, Queensland, Australia. </institution>
Reference: <author> Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, J., Sandhu, S., Guppy, K., Lee, S., & Froelicher, V. </author> <year> (1989). </year> <title> International application of a new probability algorithm for the diagnosis of coronary artery disease. </title> <journal> American Journal of Cardiology, </journal> <volume> 64 </volume> <pages> 304-310. </pages>
Reference-contexts: They include: recognizing protein-coding regions in E. coli DNA sequences (Craven & Shavlik, 1993b), diagnosing the presence of heart disease in patients <ref> (Detrano et al., 1989) </ref>, mapping English text into its pronunciation (Sejnowski & Rosenberg, 1987), recognizing promoters in E. coli DNA sequences (Towell et al., 1990), diagnosing faults in local telephone loops (Provost & Danyluk, 1995), and predicting the party affiliation of members of the U.S. <p> In the figure, negated features are preceded by a minus sign. The class labels indicate whether a given, fixed-length sequence is predicted to encode a protein or not. 175 A.2 The Heart Domain The 13 features in the heart domain <ref> (Detrano et al., 1989) </ref> represent measured descriptions of the patient under consideration. The table below lists the abbreviations used in the figure, and provides a lengthier description of each feature. The possible values for each feature are also listed.
Reference: <author> Devroye, L. </author> <year> (1983). </year> <title> The equivalence of weak, strong, and complete convergence in L 1 for kernel density estimates. </title> <journal> The Annals of Statistics, </journal> <volume> 11 </volume> <pages> 896-904. </pages>
Reference: <author> Dietterich, T., Kearns, M., & Mansour, Y. </author> <year> (1996). </year> <title> Applying the weak learning framework to understand and improve C4.5. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <address> Bari, Italy. </address>
Reference: <author> Drucker, H. & Cortes, C. </author> <year> (1996). </year> <title> Boosting decision trees. </title> <editor> In Touretzky, D., Mozer, M., & Hasselmo, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 8). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Drucker, H., Schapire, R., & Simard, P. </author> <year> (1993). </year> <title> Improving performance in neural networks using a boosting algorithm. </title> <editor> In Hanson, S. J., Cowan, J. D., & Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 5). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Ehrenfeucht, A. & Haussler, D. </author> <year> (1989). </year> <title> Learning decision trees from random examples. </title> <journal> Information and Computation, </journal> <volume> 82 </volume> <pages> 231-246. </pages>
Reference: <author> Elman, J. L. </author> <year> (1991). </year> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 195-225. </pages>
Reference: <author> Fahlman, S. & Lebiere, C. </author> <year> (1989). </year> <title> The cascade-correlation learning architecture. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2), </booktitle> <pages> (pp. 524-532), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. </author> <year> (1996). </year> <title> From data mining to knowledge discovery in databases. </title> <journal> AI Magazine, </journal> <volume> 17(3). </volume>
Reference-contexts: In many cases the models constructed by inductive learning algorithms are humanly comprehensible and thus can lead to a better understanding of the problem domain. Inductive learning with a focus on comprehensibility is a central activity in the growing field of knowledge discovery in databases and data mining <ref> (Fayyad et al., 1996) </ref>.
Reference: <author> Fayyad, U. M. & Irani, K. B. </author> <year> (1992). </year> <title> On the handling of continuous-valued attributes in decision tree generation. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 87-102. </pages>
Reference: <author> Fisher, D. </author> <year> (1987). </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172. </pages>
Reference-contexts: The nature of the models constructed by unsupervised algorithms varies greatly from method to method. For example, there are unsupervised methods that explain for their training data by estimating probability distribution functions (e.g., Silverman, 1986), constructing hierarchical categorizations <ref> (e.g., Fisher, 1987) </ref>, and reducing the data to a lower dimensional space that accounts for most of its variance (e.g., Jolliffe, 1986). Reinforcement learning involves a task that lies in between supervised and unsupervised learning.
Reference: <author> Fisher, D. & McKusick, K. </author> <year> (1989). </year> <title> An empirical comparison of ID3 and back-propagation. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 788-793), </pages> <address> Detroit, MI. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Flann, N. & Dietterich, T. </author> <year> (1986). </year> <title> Selecting appropriate representations for learning from examples. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 460-466), </pages> <address> Philadelphia, PA. </address> <publisher> Morgan Kaufmann. </publisher> <address> 190 Frean, M. </address> <year> (1990). </year> <title> The upstart algorithm: A method for constructing and training feed-forward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2(2) </volume> <pages> 198-209. </pages>
Reference: <author> Freund, Y. </author> <year> (1990). </year> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> (pp. 202-216), </pages> <address> Rochester, NY. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Freund, Y. </author> <year> (1992). </year> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> (pp. 391-398), </pages> <address> Pittsburgh, PA. </address> <publisher> ACM Press. </publisher>
Reference: <author> Freund, Y. </author> <year> (1993). </year> <title> Data Filtering and Distribution Modeling Algorithms for Machine Learning. </title> <type> PhD thesis, </type> <institution> University of California at Santa Cruz. </institution> <note> Available as technical report UCSC-CRL-93-37. </note>
Reference: <author> Freund, Y. & Schapire, R. E. </author> <year> (1995). </year> <title> A decision-theoretic generalization of on-line learning and its application to boosting. </title> <booktitle> In Proceedings of the Second European Conference on Computational Learning Theory, </booktitle> <address> Barcelona, Spain. </address>
Reference-contexts: A description of the BBP algorithm and some of the experiments reported in this chapter were previously published elsewhere (Jackson & Craven, 1996). 7.1 Hypothesis Boosting and the AdaBoost Algorithm The BBP algorithm is based on a hypothesis-boosting method called AdaBoost <ref> (Freund & Schapire, 1995) </ref>. Recall from Chapter 5 that a hypothesis-boosting algorithm is one that combines the hypotheses produced by a weak learning algorithm into a strong hypothesis. <p> In particular, given a weak learning algorithm for a class F , there are general mechanisms for boosting the weak learner into a strong learner for F . Several such boosting algorithms have been developed (Schapire, 1990; Freund, 1990; Freund, 1992), and one called AdaBoost <ref> (Freund & Schapire, 1995) </ref> is the basis of the BBP algorithm. assume here that the target function we are trying to learn, f , is a mapping from a set of Boolean features to a Boolean output: f0; 1g n ! f1; +1g. <p> f (x) then weight (x) := fi i weight (x) Return: h (x) sign T X ln (fi i ) h i (x) into a strong hypothesis. 131 that has * i ( 1 2 fl) at each stage, then AdaBoost's final hypothesis will be consistent with the training set <ref> (Freund & Schapire, 1995) </ref>. The hypothesis returned by AdaBoost is a thresholded, weighted sum over the outputs of the weak hypotheses. In other words, it is a perceptron over the weak hypotheses. <p> A second significant limitation of the BBP algorithm is that it currently handles only discrete-valued features. An important task for future work is to generalize the algorithm 172 to be applicable to problems with real-valued features. The AdaBoost algorithm <ref> (Freund & Schapire, 1995) </ref> can be applied to domains with real-valued features, so the task would involve generalizing the notion of high-ordered functions over the inputs, and the notion of correlation of a hypothesis with the target function.
Reference: <author> Freund, Y. & Schapire, R. E. </author> <year> (1996). </year> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <address> Bari, Italy. </address>
Reference: <author> Fu, L. </author> <year> (1991). </year> <title> Rule learning by searching on adapted nets. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 590-595), </pages> <address> Anaheim, CA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference: <author> Gallant, S. I. </author> <year> (1993). </year> <title> Neural Network Learning and Expert Systems. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference-contexts: In some domains, it is not necessary to have a complete description of the learning system's induced model, but it is desirable to be able to explain classifications of individual examples <ref> (Gallant, 1993) </ref>. If the learned hypothesis is understandable in such a domain, then it can be used to produce explanations of classifications 5 made for particular cases. * Improving predictive accuracy.
Reference: <author> Garey, M. R. & Johnson, D. S. </author> <year> (1979). </year> <title> Computers and Intractability. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> New York, NY. </address>
Reference-contexts: The primary purpose of this restriction is that it prevents DrawIn-stance from having to solve a difficult satisfiability problem to ensure that all of the m-of-n constraints were satisfied. Trepan avoids this situation for the sake of efficiency since the general satisfiability problem is NP-hard <ref> (Garey & Johnson, 1979) </ref>. Trepan is drawing a sample for the shaded node in the figure, where x 1 is a real-valued feature in the range [0; 1], and x 2 is a Boolean feature.
Reference: <author> Giles, C. L., Miller, C. B., Chen, D., Chen, H. H., Sun, G. Z., & Lee, Y. C. </author> <year> (1992). </year> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 393-405. </pages>
Reference: <author> Goh, T. H. </author> <year> (1993). </year> <title> Semantic extraction using neural network modelling and sensitivity analysis. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Nagoya, Japan. </address>
Reference: <author> Goldmann, M., Hastad, J., & Razborov, A. </author> <year> (1992). </year> <title> Majority gates vs. general weighted threshold gates. </title> <booktitle> In Proceedings of the Seventh IEEE Conference on Structure in Complexity Theory, </booktitle> <pages> (pp. 2-13). </pages>
Reference-contexts: The first step in the proof that the BBP algorithm PAC learns the class P k is to show that the WeakLearn function can weakly learn P k . Following Freund, we begin with the following lemma <ref> (Goldmann et al., 1992) </ref>. 150 Lemma 1 (Goldmann Hastad Razborov) For f : f0; 1g n ! f1; +1g and H, any set of functions with the same domain and range, if f can be represented as f (x) = sign s X h i (x) where h i 2 H,
Reference: <author> Gorman, R. P. & Sejnowski, T. J. </author> <year> (1988). </year> <title> Analysis of hidden units in a layered network trained to classify sonar targets. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 75-89. </pages>
Reference: <author> Hancock, T. R. </author> <year> (1990). </year> <title> Identifying -formula decision trees with queries. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> (pp. 23-37), </pages> <address> Rochester, NY. </address> <publisher> Morgan Kaufmann. </publisher> <editor> 191 Hanson, S. & Burr, D. </editor> <year> (1990). </year> <title> What connectionist models learn: Learning and representation in connectionist networks. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 13 </volume> <pages> 471-518. </pages>
Reference: <author> Haussler, D. </author> <year> (1988). </year> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 177-221. </pages>
Reference-contexts: Often, finding a hypothesis consistent with a large enough set of examples produced by an example oracle EX (f; D) is sufficient to guarantee PAC learnability. For example, given a finite set of functions F , it is straightforward to show the following <ref> (Haussler, 1988) </ref>. 151 Lemma 2 (Haussler) Let F be a finite set of functions over a domain X.
Reference: <author> Haussler, D. </author> <year> (1992). </year> <title> Decision theoretic generalizations of the PAC model for neural nets and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100 </volume> <pages> 78-150. </pages>
Reference: <author> Hayashi, Y. </author> <year> (1991). </year> <title> A neural expert system with automated extraction of fuzzy if-then rules. </title> <editor> In Lippmann, R., Moody, J., & Touretzky, D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 3). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Heckerman, D. </author> <year> (1995). </year> <title> A tutorial on learning Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, </institution> <address> Redmond, WA. </address>
Reference: <author> Hinton, G. </author> <year> (1986). </year> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> (pp. 1-12), </pages> <address> Amherst, MA. </address> <publisher> Erlbaum. </publisher>
Reference-contexts: These nonlinear, non-monotonic relationships are represented by the hidden units in a network which combine the inputs of multiple features thus allowing the model to take advantage of dependencies among the features. Understanding the hidden units themselves is often difficult because these units often learn distributed representations <ref> (Hinton, 1986) </ref>. Hidden units can be thought of as representing higher-level, "derived features." In a distributed representation, however, these derived features may not correspond to well understood features in the problem domain.
Reference: <author> Hinton, G. </author> <year> (1989). </year> <title> Connectionist learning procedures. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 185-234. </pages>
Reference-contexts: Additionally, it may include a complexity term that reflects a prior distribution over the values that the parameters can take (Rumelhart et al., 1995). An appropriate cost function for classification problems is the cross-entropy function <ref> (Hinton, 1989) </ref>: C = i j Here i ranges the examples in the training set, j ranges over the output units of the network, t j is the target value for the jth output unit for a given example, and a j is the activation of the jth output unit in <p> For each training set, networks with 0, 5, 10, 20, and 40 hidden units are tried, and we use 10-fold cross validation within the training set to select the topology to be used for that training set. The networks are trained using the cross-entropy error function <ref> (Hinton, 1989) </ref> and a conjugate-gradient minimization algorithm (Kramer & Sangiovanni-Vincentelli, 1989). They are trained for a maximum of 50 search directions.
Reference: <author> Hogg, R. V. & Tanis, E. A. </author> <year> (1983). </year> <title> Probability and Statistical Inference. </title> <publisher> MacMillan Publishing, </publisher> <address> New York, NY. </address>
Reference-contexts: The value for m L is calculated by considering that a confidence interval around a proportion, ^p, specifies: p ff =2n z ff ^p (1 ^p)=n + z 2 1 + z 2 with 100 (1 ff)% confidence <ref> (Hogg & Tanis, 1983) </ref> 5 . Here, z ff represents the value such that the integral of the standard normal density from z ff to 1 equals ff.
Reference: <author> Hong, Z. Q. & Yang, J. Y. </author> <year> (1991). </year> <title> Optimal discriminant plane for a small number of samples and design method of classifier on the plane. </title> <journal> Pattern Recognition, </journal> <volume> 24(4) </volume> <pages> 317-324. </pages>
Reference-contexts: use the larger feature set because we are interested in testing the ability of the BBP algorithm to sparingly incorporate features into its learned hypotheses. 141 In addition to the coding, promoter, and voting data sets the experiments here also use the splice-junction (Noordewier et al., 1991) and the lung-cancer <ref> (Hong & Yang, 1991) </ref> domains. The splice-junction data set is a three-class problem comprising 3,190 examples represented using 60 features. Like the promoter domain, each feature corresponds to a position in a DNA sequence and thus can take on one of four values.
Reference: <author> Hunter, L. & Klein, T. </author> <year> (1993). </year> <title> Finding relevant biomolecular features. </title> <booktitle> In Proceedings of the First International Conference on Intelligent Systems for Molecular Biology, </booktitle> <pages> (pp. 190-197), </pages> <address> Bethesda, MD. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: A system may discover salient features and relationships in the training data whose importance was not previously recognized. If the hypotheses formed by the learner are comprehensible, then these discoveries can be made accessible to human review <ref> (e.g., Hunter & Klein, 1993) </ref>. * Explanation. In some domains, it is not necessary to have a complete description of the learning system's induced model, but it is desirable to be able to explain classifications of individual examples (Gallant, 1993).
Reference: <author> Jackson, J. C. & Craven, M. W. </author> <year> (1996). </year> <title> Learning sparse perceptrons. </title> <editor> In Touretzky, D., Mozer, M., & Hasselmo, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 8). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: This chapter is based on joint work with Jeffrey Jackson. The BBP algorithm was co-developed with Jackson, and he is responsible for the technical contributions presented in Section 7.4. A description of the BBP algorithm and some of the experiments reported in this chapter were previously published elsewhere <ref> (Jackson & Craven, 1996) </ref>. 7.1 Hypothesis Boosting and the AdaBoost Algorithm The BBP algorithm is based on a hypothesis-boosting method called AdaBoost (Freund & Schapire, 1995). Recall from Chapter 5 that a hypothesis-boosting algorithm is one that combines the hypotheses produced by a weak learning algorithm into a strong hypothesis.
Reference: <author> John, G. H., Kohavi, R., & Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> (pp. 121-129), </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> John, G. H. & Langley, P. </author> <year> (1995). </year> <title> Estimating continuous distributions in Bayesian classifiers. </title> <booktitle> In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> (pp. 338-345), </pages> <address> Montreal, Quebec. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Jolliffe, I. T. </author> <year> (1986). </year> <title> Principal Component Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY. </address>
Reference-contexts: For example, there are unsupervised methods that explain for their training data by estimating probability distribution functions (e.g., Silverman, 1986), constructing hierarchical categorizations (e.g., Fisher, 1987), and reducing the data to a lower dimensional space that accounts for most of its variance <ref> (e.g., Jolliffe, 1986) </ref>. Reinforcement learning involves a task that lies in between supervised and unsupervised learning. A reinforcement learner operates in a dynamic environment, and it may take actions that influence the environment. <p> The next step is to apply a clustering method to these vectors in order to uncover structure in the patterns of hidden-unit activations. Hanson (1990) used a hierarchical-clustering method to identify clusters, while others have used principal-components analysis <ref> (Jolliffe, 1986) </ref> (and the related method of canonical-discriminant analysis) for this task (Dennis & Phillips, 1991; Elman, 1991).
Reference: <author> Jordan, M. </author> <year> (1986). </year> <title> Serial order: A parallel distributed processing approach. </title> <type> Technical Report 8604, </type> <institution> University of California, Institute for Cognitive Science, </institution> <address> San Diego. </address> <note> 192 Kearns, </note> <author> M. J. & Mansour, Y. </author> <year> (1996). </year> <title> On the boosting ability of top-down decision tree learning algorithms. </title> <booktitle> In Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> (pp. 459-468), </pages> <address> Philadelphia, PA. </address> <publisher> ACM Press. </publisher>
Reference: <author> Kearns, M. J., Schapire, R. E., & Sellie, L. M. </author> <year> (1992). </year> <title> Toward efficient agnostic learning. </title> <booktitle> In Proceedings of the Fifth ACM Workshop on Computational Learning Theory, </booktitle> <pages> (pp. 341-352), </pages> <address> Pittsburgh, PA. </address> <publisher> ACM Press. </publisher>
Reference: <author> Kearns, M. J. & Vazirani, U. V. </author> <year> (1994). </year> <title> An Introduction to Computational Learning Theory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Since Trepan treats the rule-extraction task as a learning problem, this section discusses Trepan in the context of analytical frameworks that have developed in the field of computational learning theory <ref> (Kearns & Vazirani, 1994) </ref>.
Reference: <author> Kibler, D. & Langley, P. </author> <year> (1988). </year> <title> Machine learning as an experimental science. </title> <booktitle> In Proceedings of the Third European Working Session on Learning, </booktitle> <address> Glasgow, Scotland. </address> <publisher> Pitman. </publisher>
Reference: <author> Kira, K. & Rendell, L. A. </author> <year> (1992a). </year> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 129-134), </pages> <address> San Jose, CA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference: <author> Kira, K. & Rendell, L. A. </author> <year> (1992b). </year> <title> A practical approach to feature selection. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> (pp. 249-256), </pages> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Filter methods, on the other hand, are often fast but do not take advantage of the learning method's inductive bias when selecting feature subsets. In Chapter 7, I presented a hybrid filter-wrapper method based on the Relief algorithm <ref> (Kira & Rendell, 1992b) </ref>. Relief is a filter method that determines a total ordering over the domain features. My method employs a wrapper procedure to evaluate the possible subsets defined by this ordering.
Reference: <author> Kramer, A. H. & Sangiovanni-Vincentelli, A. </author> <year> (1989). </year> <title> Efficient parallel learning algorithms for neural networks. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 1), </booktitle> <pages> (pp. 40-48), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Other optimization methods can be used to minimize the cost function. Standard gradient descent augmented with a momentum term is sometimes used, as is the conjugate-gradient method <ref> (Kramer & Sangiovanni-Vincentelli, 1989) </ref>, and even second-order methods (Becker & Le Cun, 1988). Often, network training is stopped before a local minimum in the cost function is reached. <p> The hidden and the output units use logistic transfer functions, and the number of hidden units used for each network (0, 5, 10, 20 or 40) is chosen using cross validation within each network's training set. The networks are trained using a conjugate-gradient learning method <ref> (Kramer & Sangiovanni-Vincentelli, 1989) </ref>, and training continues until either (i) all of the training-set examples are correctly classified, (ii) a local minimum in the error surface is reached, or (iii) 50 search 1 The task of learning stresses is of comparable predictive difficulty to the task of learning phonemes. <p> After the number of hidden units is selected for each network, I use a similar cross-validation procedure to determine the parameter for soft weight-sharing. I use a conjugate-gradient learning algorithm <ref> (Kramer & Sangiovanni-Vincentelli, 1989) </ref> to train the weights and the Gaussian parameters of the networks. Each hidden and output unit has five local Gaussians which act on the weights feeding into it. I use the C4.5 system to induce decision trees and to convert the trees into rule sets. <p> Second, for algorithms, such as BBP, that form hypotheses composed of weighted connections, we count the number of connections in the learned hypotheses. 7.3.1 Algorithms The four other learning algorithms to which we compare BBP are: 1. multi-layer neural networks (Rumelhart et al., 1986) trained using a conjugate-gradient method <ref> (Kramer & Sangiovanni-Vincentelli, 1989) </ref>; 2. decision trees induced using C4.5 (Quinlan, 1993); 3. the Relief feature-selection algorithm (Kira & Rendell, 1992a; 1992b) used in conjunc tion with C4.5; 4. the Balanced version of the Winnow algorithm (Littlestone, 1989; 1995). The reasons for selecting each of these algorithms are as follows. <p> The networks are trained using the cross-entropy error function (Hinton, 1989) and a conjugate-gradient minimization algorithm <ref> (Kramer & Sangiovanni-Vincentelli, 1989) </ref>. They are trained for a maximum of 50 search directions. During training, 10% of the training data is held aside as a validation set in order to decide when the weights should be saved (i.e., when training should be effectively stopped).
Reference: <author> Kushilevitz, E. & Mansour, Y. </author> <year> (1991). </year> <title> Learning decision trees using the Fourier spectrum. </title> <booktitle> In Proceedings of the Twenty-third Annual ACM Symposium on Theory of Computing, </booktitle> <pages> (pp. 455-464), </pages> <address> New Orleans, LA. </address> <publisher> ACM Press. </publisher>
Reference: <author> Lang, K. J. & Witbrock, M. J. </author> <year> (1988). </year> <title> Learning to tell two spirals apart. </title> <booktitle> In Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> (pp. 52-59), </pages> <address> Pittsburgh, PA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In particular, I investigate the value of (i) m-of-n splitting tests in Trepan, (ii) Trepan's instance-modeling method, and (iii) Trepan's best-first tree expansion strategy. The methodology used here is to conduct lesion experiments <ref> (Ki-bler & Langley, 1988) </ref>, in which some component of Trepan is left out or modified, and the resulting performance is compared to that of the unmodified algorithm. 4.5.1 The Value of M-of-N Splitting Tests The first experiment investigates the utility of m-of-n tests in trees grown by Trepan.
Reference: <author> Langley, P. & Simon, H. A. </author> <year> (1995). </year> <title> Applications of machine learning and rule induction. </title> <journal> Communications of the ACM, </journal> <volume> 38(11) </volume> <pages> 54-64. </pages>
Reference: <author> Le Cun, Y., Denker, J., & Solla, S. </author> <year> (1989). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2), </booktitle> <pages> (pp. 598-605), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Additionally, Winnow has recently shown its practical value by demonstrating excellent predictive performance in an interesting, real-world application (Blum, 1995). We use the 135 Balanced version of Winnow, rather than one of the other versions <ref> (Littlestone, 1989) </ref>, largely due to its empirical success. We now describe these algorithms in more detail, and discuss how they are applied in the experiments. Multi-layer Neural Networks The multi-layer neural networks used in the experiments have logistic output units and one (or no) layer of logistic hidden units. <p> However, it does enable a 2 It would be possible to augment the neural-network training procedure with a pruning method that trimmed extraneous weights from the network during training <ref> (e.g., Le Cun et al., 1989) </ref>. Such a method might produce networks that do not use all of the features. <p> The AdaBoost algorithm (Freund & Schapire, 1995) can be applied to domains with real-valued features, so the task would involve generalizing the notion of high-ordered functions over the inputs, and the notion of correlation of a hypothesis with the target function. Another limitation of BBP, in comparison to Winnow <ref> (Littlestone, 1989) </ref>, is that it is a batch algorithm (i.e., it looks at all of its training data before updating its hypothesis). Although the capability was not exploited in the experiments, one of the appealing aspects of Winnow is that it is well suited for tasks that involve on-line learning.
Reference: <author> Lewis, J. </author> <year> (1991). </year> <title> A Dynamic Load Balancing Approach to Control of Multiserver Polling Systems with Applications to Elevator System Dispatching. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Littlestone, N. </author> <year> (1989). </year> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of California, Santa Cruz. </institution> <note> Available as Technical Report UCSC-CRL-89-11. 193 Littlestone, </note> <author> N. </author> <year> (1995). </year> <title> Comparing several linear-threshold algorithms on tasks involving superfluous attributes. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> (pp. 353-361), </pages> <address> Tahoe City, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Additionally, Winnow has recently shown its practical value by demonstrating excellent predictive performance in an interesting, real-world application (Blum, 1995). We use the 135 Balanced version of Winnow, rather than one of the other versions <ref> (Littlestone, 1989) </ref>, largely due to its empirical success. We now describe these algorithms in more detail, and discuss how they are applied in the experiments. Multi-layer Neural Networks The multi-layer neural networks used in the experiments have logistic output units and one (or no) layer of logistic hidden units. <p> The AdaBoost algorithm (Freund & Schapire, 1995) can be applied to domains with real-valued features, so the task would involve generalizing the notion of high-ordered functions over the inputs, and the notion of correlation of a hypothesis with the target function. Another limitation of BBP, in comparison to Winnow <ref> (Littlestone, 1989) </ref>, is that it is a batch algorithm (i.e., it looks at all of its training data before updating its hypothesis). Although the capability was not exploited in the experiments, one of the appealing aspects of Winnow is that it is well suited for tasks that involve on-line learning.
Reference: <author> Loh, W.-Y. </author> <year> (1996). </year> <type> Personal communication. </type>
Reference-contexts: The Bonferroni correction is conservative in nature (it makes a worst-case assumption about the outcomes of the tests), and thus it is fairly common to use a somewhat relaxed significance level with it <ref> (Loh, 1996) </ref>. The other parameter of Trepan is min sample, which specifies how many instances must be considered before giving a class label or choosing a splitting test for a node. For all domains, the value of this parameter is set to 10,000.
Reference: <author> MacKay, D. </author> <year> (1992). </year> <title> A practical Bayesian framework for backpropagation networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 448-472. </pages>
Reference: <author> Mangasarian, O. L. & Solodov, M. V. </author> <year> (1993). </year> <title> Nonlinear complementarity as unconstrained and constrained minimization. </title> <journal> Mathematical Programming, Series B, </journal> <volume> 62 </volume> <pages> 277-297. </pages>
Reference: <author> Mangasarian, O. L. & Solodov, M. V. </author> <year> (1994). </year> <title> Serial and parallel backpropagation convergence via nonmonotone perturbed minimization. </title> <journal> Optimization Methods and Software, </journal> <volume> 4(2) </volume> <pages> 103-116. </pages>
Reference: <author> Matheus, C. J. </author> <year> (1990). </year> <title> Feature Construction: An Analytic Framework and an Application to Decision Trees. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign. </institution>
Reference-contexts: For example, the given model might be used to suggest additional, higher-level features that Trepan could use in its description of the model. A number of studies in the conventional learning setting have shown that the predictive performance of a learner can often be improved by constructing new features <ref> (e.g., Matheus, 1990) </ref>. Such features might be discovered either by directly inspecting the structure of the model (i.e., the parameters and topology of a neural network), or by using membership queries to uncover feature interactions.
Reference: <author> McMillan, C., Mozer, M. C., & Smolensky, P. </author> <year> (1992). </year> <title> Rule induction through integrated symbolic and subsymbolic processing. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 4). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Medin, D. L., Wattenmaker, W. D., & Michalski, R. S. </author> <year> (1987). </year> <title> Constraints and preferences in inductive learning: An experimental study of human and machine performance. </title> <journal> Cognitive Science, </journal> <volume> 11 </volume> <pages> 299-339. </pages>
Reference: <author> Michalski, R. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20 </volume> <pages> 111-161. </pages>
Reference: <author> Michalski, R. S. </author> <year> (1986). </year> <title> Understanding the nature of learning: Issues and research directions. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M., editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (volume II). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA. </address>
Reference-contexts: The various representations for expressing learned hypotheses differ greatly in how readily they can be inspected and understood by humans. Hypothesis languages that have a logic-like syntax (e.g., decision trees, inference rules, and decision lists), are often termed symbolic representations <ref> (Michalski, 1986) </ref>. One purported advantage of symbolic representations is that they are usually easily understood. For example, in a decision tree it is easy to see which features are incorporated into the hypothesis, and to see the important relationships among features in the hypothesis.
Reference: <author> Minsky, M. & Papert, S. </author> <year> (1969). </year> <title> Perceptrons: An Introduction to Computational Geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference-contexts: The restricted hypothesis space bias refers to the constraints that a learning algorithm places on the hypotheses that it is able to construct. For example, the hypothesis space of a perceptron is limited to linear discriminant functions <ref> (Minsky & Papert, 1969) </ref>. The preference bias of a learning algorithm refers to the preference ordering it places on the models that are within its hypothesis space. <p> The units of a network are related by weighted connections. A network for classification that has only input units and output units is capable of representing only linear decision boundaries in its input space <ref> (Minsky & Papert, 1969) </ref>. In order to represent more complex boundaries, it necessary to add hidden units to the network. The role of hidden units is to transform the input space into another space in which it is more profitable for the output units to make linear discriminations.
Reference: <author> Mitchell, T., Caruana, R., Freitag, D., McDermott, J., & Zabowski, D. </author> <year> (1994). </year> <title> Experience with a learning personal assistant. </title> <journal> Communications of the ACM, </journal> <volume> 37(7) </volume> <pages> 80-91. </pages>
Reference: <author> Mitchell, T. M. </author> <year> (1980). </year> <title> The need for biases in learning generalizations. </title> <type> Technical Report CBM-TR-117, </type> <institution> Department of Computer Science, Rutgers University, </institution> <address> New Brunswick, NJ. </address>
Reference-contexts: Every learning algorithm has an inductive bias that determines the models that it is likely to return <ref> (Mitchell, 1980) </ref>. There are two aspects of the inductive bias of an algorithm: its restricted hypothesis space bias and its preference bias. The restricted hypothesis space bias refers to the constraints that a learning algorithm places on the hypotheses that it is able to construct.
Reference: <author> Moody, J. & Darken, C. </author> <year> (1988). </year> <title> Learning with localized receptive fields. </title> <editor> In Hinton, G. E., Sejnowski, T. J., & Touretzky, D. S., editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> (pp. 133-143), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. 194 Moore, </publisher> <editor> A. W. & Lee, M. S. </editor> <year> (1994). </year> <title> Efficient algorithms for minimizing cross validation error. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> (pp. 190-198), </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mozer, M. & Smolensky, P. </author> <year> (1988). </year> <title> Skeletonization: A technique for trimming the fat from a network via relevance assessment. </title> <booktitle> In Advances in Neural Information Processing Systems (volume 1), </booktitle> <pages> (pp. 107-115), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Muggleton, S. & DeRaedt, L. </author> <year> (1994). </year> <title> Inductive logic programming: Theory and methods. </title> <journal> Journal of Logic Programming, </journal> <volume> 19 </volume> <pages> 629-679. </pages>
Reference: <author> Munro, P. </author> <year> (1991). </year> <title> Visualizations of 2-D hidden unit space. </title> <type> Technical Report LIS035/IS91003, </type> <institution> School of Library and Information Science, University of Pittsburgh, </institution> <address> Pittsburgh, PA. </address>
Reference: <author> Murphy, P. M. & Pazzani, M. J. </author> <year> (1991). </year> <title> ID2-of-3: Constructive induction of M-of-N concepts for discriminators in decision trees. </title> <booktitle> In Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <pages> (pp. 183-187), </pages> <address> Evanston, IL. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The selected binary test serves as a seed for Trepan's m-of-n search process. This search uses information gain as its heuristic evaluation function, and uses the following two operators <ref> (Murphy & Pazzani, 1991) </ref>: * m-of-n+1 : This operator adds a new literal to the set, holding m constant. <p> As a baseline for evaluating the accuracy and comprehensibility of the trees extracted by Trepan, I also run two conventional decision-tree algorithms: C4.5 (Quinlan, 1993) and an enhanced version of ID2-of-3 <ref> (Murphy & Pazzani, 1991) </ref>. C4.5 (the successor to ID3) is one of the most widely used inductive learning algorithms, and is perhaps the most popular inductive algorithm for learning "symbolic" hypotheses. <p> And finally, it has been shown to produce syntactically simple hypothesis, thereby facilitating human comprehension of what it has learned. Enhancements to the ID2-of-3 Algorithm A minor contribution of this thesis is that an improved version of the ID2-of-3 algorithm <ref> (Murphy & Pazzani, 1991) </ref> was developed. ID2-of-3 is a decision-tree induction method that uses m-of-n tests at internal nodes in the trees it learns.
Reference: <author> Musick, R., Catlett, J., & Russell, S. </author> <year> (1993). </year> <title> Decision theoretic subsampling for induction on large databases. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> (pp. 212-219), </pages> <address> Amherst, MA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The classification algorithms, however, are for learning in neural networks and thus they do not produce comprehensible hypotheses like Trepan. Catlett (1992) and others <ref> (Musick et al., 1993) </ref> have investigated an issue related to active learning: selecting samples for decision-tree induction when there is a wealth of training data. <p> For example, one method developed in this line of work frames the question of when to select one splitting test over another as a decision-theory problem <ref> (Musick et al., 1993) </ref>. This method estimates how much training data is needed to confidently decide that the information gain of one test is greater than the information gain of other candidate tests, and uses such estimates to select samples of appropriate size for making these decisions.
Reference: <author> Neisser, U. & Weene, P. </author> <year> (1962). </year> <title> Hierarchies in concept attainment. </title> <journal> Journal of Experimental Psychology, </journal> <volume> 64 </volume> <pages> 640-645. </pages>
Reference: <author> Noordewier, M., Towell, G., & Shavlik, J. </author> <year> (1991). </year> <title> Training knowledge-based neural networks to recognize genes in DNA sequences. </title> <editor> In Lippmann, R., Moody, J., & Touretzky, D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 3), </booktitle> <pages> (pp. 530-536), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The experiments in this chapter use the larger feature set because we are interested in testing the ability of the BBP algorithm to sparingly incorporate features into its learned hypotheses. 141 In addition to the coding, promoter, and voting data sets the experiments here also use the splice-junction <ref> (Noordewier et al., 1991) </ref> and the lung-cancer (Hong & Yang, 1991) domains. The splice-junction data set is a three-class problem comprising 3,190 examples represented using 60 features. Like the promoter domain, each feature corresponds to a position in a DNA sequence and thus can take on one of four values.
Reference: <author> Nowlan, S. & Hinton, G. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 473-493. </pages>
Reference: <author> Opitz, D. </author> <year> (1995). </year> <title> An Anytime Approach to Connectionist Theory Refinement: Refining the Topologies of Knowledge-Based Neural Networks. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, WI. </institution> <note> Available as CS Technical Report 1281. </note>
Reference-contexts: Some local methods also assume that hidden units in the network are locally meaningful, and thus that rules describing the behavior of individual hidden units will be comprehensible. This assumption is also often not valid. In some cases, very large, complex networks provide the best predictive accuracy <ref> (e.g., Opitz, 1995) </ref>. In such networks, local descriptions of hidden units make for very large, complicated rule sets. Another problem with local descriptions of hidden units is that trained networks often employ a large set of hidden units to represent a smaller set of "derived" features (Weigend, 1994). <p> This was the approach taken by Towell and Shavlik (1993) in their work on extracting rules from knowledge-based networks. In many cases, however, much of the structure of knowledge-based networks after training is as opaque as ordinary networks <ref> (Opitz, 1995) </ref>, and thus a hybrid local-global approach might be most appropriate. Domain-Specific Instance Models A key aspect of Trepan is that it constructs models of the underlying data distribution in order to generate instances for membership queries.
Reference: <author> Opitz, D. W. & Shavlik, J. W. </author> <year> (1996). </year> <title> Generating accurate and diverse members of a neural-network ensemble. </title> <editor> In Touretzky, D., Mozer, M., & Hasselmo, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 8). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: For the telephone domain, Trepan is not applied to individual networks, but instead to neural-network ensembles. An ensemble is a set of separately trained classifiers whose predictions are combined through some weighting scheme. Here, the ensembles are induced using the Addemup algorithm <ref> (Opitz & Shavlik, 1996) </ref>. The Addemup algorithm uses a domain theory and genetic search techniques to generate an ensemble of knowledge-based neural networks (Towell & Shavlik, 1994).
Reference: <author> Ourston, D. & Mooney, R. </author> <year> (1994). </year> <title> Theory refinement combining analytical and empirical methods. </title> <journal> Artificial Intelligence, </journal> <volume> 66(2) </volume> <pages> 273-309. </pages>
Reference: <author> Pazzani, M. & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9(1) </volume> <pages> 57-94. </pages> <note> 195 Pazzani, </note> <author> M. J., Muramatsu, J., & Billsus, D. </author> <year> (1996). </year> <title> Syskill & Webert: Identifying interesting Web sites. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 54-59), </pages> <address> Portland, OR. </address> <publisher> AAAI/MIT Press. </publisher>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabalistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: For example, to make queries to a Bayesian-network instance model Trepan would have to be integrated with an algorithm for inference in Bayesian networks <ref> (Pearl, 1988) </ref> to ensure that it sampled properly from the distribution. Better Theoretical Guarantees Chapter 5 argued that the scalability of rule-extraction methods, and Trepan in particular, should be analyzed in the context of formal models of learning.
Reference: <author> Pineda, F. J. </author> <year> (1987). </year> <title> Generalization of back-propagation to recurrent neural networks. </title> <journal> Physical Review Letters, </journal> <volume> 59 </volume> <pages> 2229-2232. </pages>
Reference: <author> Pinker, S. </author> <year> (1979). </year> <title> Formal models of language learning. </title> <journal> Cognition, </journal> <volume> 7 </volume> <pages> 217-283. </pages>
Reference: <author> Poggio, T. & Girosi, F. </author> <year> (1990). </year> <title> Regularization algorithms for learning that are equivalent to multilayer networks. </title> <journal> Science, </journal> <volume> 247 </volume> <pages> 978-982. </pages>
Reference: <author> Pollack, J. </author> <year> (1991). </year> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 227-252. </pages>
Reference: <author> Pomerleau, D. A. </author> <year> (1993). </year> <title> Neural Network Perception for Mobile Robot Guidance. </title> <publisher> Kluwer, </publisher> <address> Boston, MA. </address>
Reference-contexts: The application of inductive learning algorithms is usually driven by two underlying goals: performance and discovery. In the first case, the goal is to use a learning method to induce a model that can be used to perform some task of interest. For example, the ALVINN system <ref> (Pomerleau, 1993) </ref> has learned to steer a motor vehicle, and the GRAIL system (Uberbacher et al., 1993) has learned to recognize genes in uncharacterized DNA sequences. <p> These visualization methods can sometimes provide insight into the learning and prediction behavior of a network. They seem to be especially effective in tasks where the input to the network is image data <ref> (e.g., Pomerleau & Touretzky, 1993) </ref>.
Reference: <author> Pomerleau, D. A. & Touretzky, D. S. </author> <year> (1993). </year> <title> Understanding neural network internal representations through hidden unit sensitivity analysis. </title> <booktitle> In Proceedings of the International Conference on Intelligent Autonomous Systems, </booktitle> <address> Pittsburgh, PA. </address> <publisher> IOS Publishers. </publisher>
Reference-contexts: The application of inductive learning algorithms is usually driven by two underlying goals: performance and discovery. In the first case, the goal is to use a learning method to induce a model that can be used to perform some task of interest. For example, the ALVINN system <ref> (Pomerleau, 1993) </ref> has learned to steer a motor vehicle, and the GRAIL system (Uberbacher et al., 1993) has learned to recognize genes in uncharacterized DNA sequences. <p> These visualization methods can sometimes provide insight into the learning and prediction behavior of a network. They seem to be especially effective in tasks where the input to the network is image data <ref> (e.g., Pomerleau & Touretzky, 1993) </ref>.
Reference: <author> Pratt, L. Y., Mostow, J., & Kamm, C. A. </author> <year> (1991). </year> <title> Direct transfer of learned information among neural networks. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 584-589), </pages> <address> Anaheim, CA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference: <author> Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. </author> <year> (1992). </year> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> New York, NY, 2nd edition. </address>
Reference-contexts: data came from the same distribution) is rejected with 100 (1 ff)% confidence if: Q KS p 0:11 m e D &gt; ff where m e is the effective number of data points: m e = m A + m B 17 and the function approximating the significance level is <ref> (Press et al., 1992) </ref>: Q KS () = 2 j=1 : 2.2 Decision Trees Decision-tree induction algorithms are among the most widely used methods in machine learning. <p> In order to estimate accuracy for a given feature subset (i.e., to evaluate the function at a given point), we use a 10-fold cross-validation procedure. The optimization method we use is a modified version of golden-section search <ref> (Press et al., 1992) </ref>. Our modification to this method is simple: whereas ordinary golden-section search assumes a continuous domain for the x-axis, this problem involves a discrete-valued domain.
Reference: <author> Provost, F. & Danyluk, A. </author> <year> (1995). </year> <title> Learning from bad data. </title> <booktitle> In Working Notes of the Workshop on Applying Machine Learning in Practice, Twelfth International Conference on Machine Learning, </booktitle> <address> Tahoe City, CA. </address>
Reference-contexts: protein-coding regions in E. coli DNA sequences (Craven & Shavlik, 1993b), diagnosing the presence of heart disease in patients (Detrano et al., 1989), mapping English text into its pronunciation (Sejnowski & Rosenberg, 1987), recognizing promoters in E. coli DNA sequences (Towell et al., 1990), diagnosing faults in local telephone loops <ref> (Provost & Danyluk, 1995) </ref>, and predicting the party affiliation of members of the U.S. House of Representatives given their voting records (Schlimmer & Fisher, 1986). I shall refer to these as the coding, heart, NETtalk, promoter, telephone, and voting domains, respectively.
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: C4.5 is the successor to the ID3 algorithm <ref> (Quinlan, 1986) </ref>. These two algorithms, and numerous variants of them, are similar in their overall structure, but differ somewhat in details. Here I focus mainly on C4.5, since it used extensively in the experiments reported in later chapters. Decision-tree learning involves constructing a tree by recursively partitioning the training examples.
Reference: <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-2666. </pages>
Reference: <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: When an example reaches a leaf, the class associated with the leaf is the prediction made by the decision tree for that example. 2.2.2 Decision-Tree Learning The two most widely used decision-tree induction algorithms are C4.5 <ref> (Quinlan, 1993) </ref>, which arose in the artificial intelligence community, and CART (Breiman et al., 1984) which was developed in the statistics community. C4.5 is the successor to the ID3 algorithm (Quinlan, 1986). <p> Like conventional decision-tree induction algorithms, such as CART (Breiman et al., 1984), and C4.5 <ref> (Quinlan, 1993) </ref>, Trepan builds a decision tree by recursively partitioning the instance space. Unlike these algorithms, however, Trepan constructs a decision tree in a best-first manner (the notion of "best" is described shortly). <p> Like the ID2-of-3 algorithm, Trepan uses a heuristic search process to construct its m-of-n tests. The search process begins by first selecting the best binary test at the current node measured using the information gain criterion <ref> (Quinlan, 1993) </ref> 4 to evaluate candidate tests. For two-valued features, a binary test separates examples according to their values for the feature. For discrete features with more than two values, Trepan considers binary tests based on each allowable value of the feature (e.g., color=red?, color=blue?, ...). <p> This distinction may not seem important, since a decision tree can easily be converted into a set of inference rules <ref> (Quinlan, 1993) </ref>. There are, however, a couple of advantages to using decision trees as the extracted representation, instead of inference rules. First, the decision-tree representation provides the extraction algorithm with some degree of control over the extracted representation's complexity and fidelity. <p> As a baseline for evaluating the accuracy and comprehensibility of the trees extracted by Trepan, I also run two conventional decision-tree algorithms: C4.5 <ref> (Quinlan, 1993) </ref> and an enhanced version of ID2-of-3 (Murphy & Pazzani, 1991). C4.5 (the successor to ID3) is one of the most widely used inductive learning algorithms, and is perhaps the most popular inductive algorithm for learning "symbolic" hypotheses. <p> After trees are grown, they are pruned using the same pruning process as C4.5. The first four of these modifications are common to Trepan also, and are described in detail in Chapter 3. The final enhancement is described in detail elsewhere <ref> (Chapter 4 of Quinlan, 1993) </ref>. The primary parameter of C4.5 that is varied in the experiments is the pruning level. As discussed in Chapter 2, C4.5's pruning method is parameterized by a confidence level that specifies how liberally trees should be pruned. <p> The method I present in this chapter, called MofN-sws, is designed to extend the applicability of the MofN algorithm to ordinary neural networks. I evaluate the accuracy and complexity of rules extracted by the MofN-sws method by comparing them to rules induced by the C4.5 system <ref> (Quinlan, 1993) </ref>. Additionally, I compare MofN-sws, which was developed in the early stages of this thesis, to the more recent Trepan algorithm. 116 117 6.1 The MofN Algorithm Towell and Shavlik's MofN algorithm comprises six steps: 1. Clustering. <p> I evaluate the method by comparing the predictive accuracy and comprehensibility of rules extracted from neural networks by MofN-sws to rules learned directly from the training data using C4.5 <ref> (Quinlan, 1993) </ref>. Additionally, I compare the MofN-sws method to Trepan by applying Trepan to the same networks. Following the experimental design used in Chapter 4, I measure the syntactic complexity of the rule sets and use this as a proxy for comprehensibility. <p> C4.5's rule-generation algorithm is described in detail elsewhere <ref> (Chapter 5 of Quinlan, 1993) </ref>. The key parameter of C4.5's rule-pruning method is a confidence level analogous to the one used for decision-tree pruning (discussed in Chapter 2 of this thesis). I use cross-validation within each training set to determine the confidence levels for both tree pruning and rule pruning. <p> BBP tries to keep a learned hypothesis comprehensible by incorporating as few features as possible into the hypothesis, and by ensuring that the relationship between each feature and the target function is relatively simple. The BBP algorithm is similar to C4.5 <ref> (Quinlan, 1993) </ref>, and other decision-tree learning algorithms, in that it incrementally adds features to its hypothesis during learning. On the other hand, since BBP learns perceptrons, its inductive bias is more similar to that of multilayer neural networks. <p> composed of weighted connections, we count the number of connections in the learned hypotheses. 7.3.1 Algorithms The four other learning algorithms to which we compare BBP are: 1. multi-layer neural networks (Rumelhart et al., 1986) trained using a conjugate-gradient method (Kramer & Sangiovanni-Vincentelli, 1989); 2. decision trees induced using C4.5 <ref> (Quinlan, 1993) </ref>; 3. the Relief feature-selection algorithm (Kira & Rendell, 1992a; 1992b) used in conjunc tion with C4.5; 4. the Balanced version of the Winnow algorithm (Littlestone, 1989; 1995). The reasons for selecting each of these algorithms are as follows. <p> In course of investigating Trepan, I developed a version of ID2-of-3 that incorporates five enhancements not included in the original algorithm: the application of a 2 test, literal pruning, and a beam search 167 when constructing m-of-n tests, the use of C4.5's pruning method <ref> (Quinlan, 1993) </ref> after tree induction, and the generalization of m-of-n tests to real-valued features. The first four enhancements were empirically evaluated in Chapter 4 and found to be of value in improving the concision and predictive accuracy of induced trees. The fifth capability clearly enhances the applicability of the algorithm.
Reference: <author> Quinlan, J. R. </author> <year> (1996). </year> <title> Bagging, boosting, </title> <booktitle> and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 725-730), </pages> <address> Portland, OR. </address> <publisher> AAAI/MIT Press. </publisher>
Reference: <author> Rabiner, L. R. </author> <year> (1989). </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-286. </pages> <note> 196 Rice, </note> <author> J. A. </author> <year> (1995). </year> <title> Mathematical Statistics and Data Analysis. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA. </address>
Reference-contexts: For example, there are learning algorithms that represent their hypotheses as decision trees (Breiman et al., 1984; Quinlan, 1993), decision lists (Rivest, 1987; Clark & Niblett, 1989), inference rules (Michalski, 1983; Quinlan, 1993), neural networks (Rumelhart et al., 1986), hidden Markov models <ref> (Rabiner, 1989) </ref>, Bayesian networks (Heck-erman, 1995), and stored lists of examples (Stanfill & Waltz, 1986; Aha et al., 1991). The various representations for expressing learned hypotheses differ greatly in how readily they can be inspected and understood by humans.
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference-contexts: Although soft weight-sharing was motivated by the desire for better predictive accuracy, it is explored here as a means for facilitating rule extraction. In the spirit of the minimum-description-length principle <ref> (Rissanen, 1978) </ref>, soft weight-sharing uses a cost function that penalizes network complexity. Thus, during training the network tries to find an optimal trade-off between data-misfit (i.e., the error rate on the training examples) and complexity.
Reference: <author> Rissanen, J. </author> <year> (1989). </year> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific. </publisher>
Reference-contexts: The motivation underlying this heuristic is that higher-order conjunctive features make a hypothesis more difficult to understand. This heuristic is related to Rissanen's Minimum Description Length principle <ref> (Rissanen, 1989) </ref> in that it involves trading off a possible gain in accuracy (at least in the weak hypothesis) for reduction in the complexity of the final hypothesis. Because the BBP algorithm uses exhaustive search over all conjunctions of size k, learning time depends exponentially on the choice of k.
Reference: <author> Rivest, R. </author> <year> (1987). </year> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 229-246. </pages>
Reference-contexts: One hypothesis representation that would be a good candidate to consider is the language of decision lists <ref> (Rivest, 1987) </ref>. Algorithms for inducing decision lists are 169 interesting in that they employ a different type of learning strategy, and hence have a different inductive bias, than decision-tree learning methods. Whereas decision-tree learning methods use a divide-and-conquer approach, decision-list algorithms employ a covering (i.e., separate-and-conquer) strategy.
Reference: <author> Rumelhart, D., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. & McClelland, J., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the microstructure of cognition. Volume 1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: For example, there are learning algorithms that represent their hypotheses as decision trees (Breiman et al., 1984; Quinlan, 1993), decision lists (Rivest, 1987; Clark & Niblett, 1989), inference rules (Michalski, 1983; Quinlan, 1993), neural networks <ref> (Rumelhart et al., 1986) </ref>, hidden Markov models (Rabiner, 1989), Bayesian networks (Heck-erman, 1995), and stored lists of examples (Stanfill & Waltz, 1986; Aha et al., 1991). The various representations for expressing learned hypotheses differ greatly in how readily they can be inspected and understood by humans. <p> As can be seen in the figure, they are continuous approximations of a threshold function. 2.3.2 Neural-Network Learning The most widely used neural-network learning method is the backpropagation algorithm <ref> (Rumelhart et al., 1986) </ref>. Learning in a neural network involves modifying the weights and 23 biases of the network in order to minimize a cost function. <p> The method also has a deterministic interpretation (Mangasarian & Solodov, 1993; 1994) in which the cost function does not decrease monotonically. In order to avoid large oscillations, these weight changes usually also incorporate a momentum term <ref> (Rumelhart et al., 1986) </ref>, which is a time-decaying average of previous weight changes. Other optimization methods can be used to minimize the cost function. <p> Second, for algorithms, such as BBP, that form hypotheses composed of weighted connections, we count the number of connections in the learned hypotheses. 7.3.1 Algorithms The four other learning algorithms to which we compare BBP are: 1. multi-layer neural networks <ref> (Rumelhart et al., 1986) </ref> trained using a conjugate-gradient method (Kramer & Sangiovanni-Vincentelli, 1989); 2. decision trees induced using C4.5 (Quinlan, 1993); 3. the Relief feature-selection algorithm (Kira & Rendell, 1992a; 1992b) used in conjunc tion with C4.5; 4. the Balanced version of the Winnow algorithm (Littlestone, 1989; 1995).
Reference: <author> Rumelhart, D. E., Durbin, R., Golden, R., & Chauvin, Y. </author> <year> (1995). </year> <title> Backpropagation: The basic theory. </title> <editor> In Chauvin, Y. & Rumelhart, D. E., editors, </editor> <title> Backpropagation: Theory, Architectures, and Applications. </title> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: The cost function always includes an error term a measure of how close the network's predictions are to the class labels for the examples in the training set. Additionally, it may include a complexity term that reflects a prior distribution over the values that the parameters can take <ref> (Rumelhart et al., 1995) </ref>.
Reference: <author> Sachs, L. </author> <year> (1984). </year> <title> Applied Statistics: A Handbook of Techniques. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, 2nd edition. </address>
Reference-contexts: I use two types of statistical tests to compare the accuracy of one algorithm to another. The first, which is used with cross-validation runs, is the paired-sample t-test <ref> (Sachs, 1984) </ref>. In the paired-sample t-test, we first calculate the average of the differences in accuracy measurements for algorithms A and B for k folds: diff = 1 k X accuracy i A accuracy i B and the standard deviation, s, of this value. <p> Each entry in the table indicates the number of test-set examples for one possible outcome. The other method used in this thesis to test hypotheses about the predictive accuracy of learning algorithms is the McNemar 2 test <ref> (Sachs, 1984) </ref>. This test, which I use when there is only a single test set, involves analyzing a four-entry table like that shown in Figure 1. The rows of the table indicate the correct/incorrect predictions made by one algorithm, and the columns indicate the correct/incorrect predictions made by another algorithm. <p> I test the statistical significance of these accuracy differences using the sign test known as the McNemar 2 test <ref> (Sachs, 1984) </ref>. Because the test set for this problem is small, none of the differences are significant at p 0:05.
Reference: <author> Saito, K. & Nakano, R. </author> <year> (1988). </year> <title> Medical diagnostic expert system based on PDP model. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <pages> (pp. 255-262), </pages> <address> San Diego, CA. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: To address this issue, a number of heuristics have been employed to limit the combinatorics of the rule-exploration process. One of the first rule-extraction methods developed <ref> (Saito & Nakano, 1988) </ref> employs a breadth-first search process to extract conjunctive rules in binary problem domains. To deal with the combinatorics of the problem, Saito and Nakano use two heuristics. The first heuristic limits the number of literals in the antecedents of extracted rules.
Reference: <author> Sanger, D. </author> <year> (1989). </year> <title> Contribution analysis: A technique for assigning responsibilities to hidden units in connectionist networks. </title> <journal> Connection Science, </journal> <volume> 1(2) </volume> <pages> 115-138. </pages>
Reference: <author> Sanger, T. D., Sutton, R. S., & Matheus, C. J. </author> <year> (1992). </year> <title> Iterative construction of sparse polynomial approximations. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 4). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Schapire, R. E. </author> <year> (1990). </year> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227. </pages>
Reference-contexts: It is known that if we have a weak-learning method for some problem, then we can construct an algorithm that combines hypotheses produced the weak learner into a hypothesis that achieves arbitrary accuracy for the problem <ref> (Schapire, 1990) </ref>. An algorithm that combines weak hypotheses in such a way is called a hypothesis-boosting algorithm. More formally, let F be a any class of Boolean functions over the instance space of some problem domain.
Reference: <author> Schlimmer, J. C. & Fisher, D. </author> <year> (1986). </year> <title> A case study of incremental concept induction. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 496-501), </pages> <address> Philadelphia, PA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: House of Representatives given their voting records <ref> (Schlimmer & Fisher, 1986) </ref>. I shall refer to these as the coding, heart, NETtalk, promoter, telephone, and voting domains, respectively. A few notes about the data sets used here are in order. <p> The value of each feature indicates the base that occurs at that position. The allowable values are: f a, c, g, tg. The class labels indicate whether a given sequence is predicted to be a promoter or not. 180 A.5 The Voting Domain The features in the voting domain <ref> (Schlimmer & Fisher, 1986) </ref> represent votes made by members of the U. S. House of Representatives in 1984. The value of each feature indicates whether a given representative voted yes, no or did not vote.
Reference: <author> Sejnowski, T. & Rosenberg, C. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168. </pages>
Reference-contexts: They include: recognizing protein-coding regions in E. coli DNA sequences (Craven & Shavlik, 1993b), diagnosing the presence of heart disease in patients (Detrano et al., 1989), mapping English text into its pronunciation <ref> (Sejnowski & Rosenberg, 1987) </ref>, recognizing promoters in E. coli DNA sequences (Towell et al., 1990), diagnosing faults in local telephone loops (Provost & Danyluk, 1995), and predicting the party affiliation of members of the U.S. House of Representatives given their voting records (Schlimmer & Fisher, 1986). <p> blood sugar &gt; 120mg/dl (yes, no) restecg resting electrocardiographic results (normal, wave abnormality, hypertrophy) sex male or female slope slope of the peak exercise ST segment (upsloping, flat, downsloping) thal exercise test (normal, fixed defect, reversible defect) 176 177 A.3 The NETtalk Domain The seven features in the NETtalk domain <ref> (Sejnowski & Rosenberg, 1987) </ref> represent letters in a seven-letter window of English text. The features are named p1, p2, p3, p4, p5, p6 and p7, indicating their position in the window.
Reference: <author> Sethi, I. K. & Yoo, J. H. </author> <year> (1994). </year> <title> Symbolic approximation of feedforward neural networks. </title> <editor> In Gelsema, E. S. & Kanal, L. N., editors, </editor> <booktitle> Pattern Recognition in Practice (volume 4). </booktitle> <publisher> North-Holland, </publisher> <address> New York, NY. </address>
Reference: <author> Setiono, R. & Liu, H. </author> <year> (1995). </year> <title> Understanding neural networks via rule extraction. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 480-485), </pages> <address> Montreal, Quebec. </address> <publisher> Morgan Kaufmann. 197 Shavlik, </publisher> <editor> J., Mooney, R., & Towell, G. </editor> <year> (1991). </year> <title> Symbolic and neural net learning algorithms: An empirical comparison. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 111-143. </pages>
Reference: <author> Silverman, B. W. </author> <year> (1986). </year> <title> Density Estimation for Statistics and Data Analysis. </title> <publisher> Chapman and Hall, </publisher> <address> New York, NY. </address>
Reference-contexts: The nature of the models constructed by unsupervised algorithms varies greatly from method to method. For example, there are unsupervised methods that explain for their training data by estimating probability distribution functions <ref> (e.g., Silverman, 1986) </ref>, constructing hierarchical categorizations (e.g., Fisher, 1987), and reducing the data to a lower dimensional space that accounts for most of its variance (e.g., Jolliffe, 1986). Reinforcement learning involves a task that lies in between supervised and unsupervised learning. <p> Although Trepan could employ sophisticated domain-specific models for this purpose, by default it uses a fairly simple approach based on modeling the marginal distributions 2 of individual features. Trepan uses empirical distributions to model discrete-valued features, and kernel density estimates <ref> (Silverman, 1986) </ref> to model continuous features. The empirical distribution of a feature is simply the distribution of values that occurs in a given sample of the feature. <p> The final step of the procedure is to draw a random value from the conditional distribution of each feature. This step is trivial for discrete-valued features. For real-valued features, Trepan adapts a standard algorithm for generating values from kernel density estimates <ref> (Silverman, 1986, pg. 143) </ref>. The adaptation of this algorithm ensures that the values it generates fall within the range specified by the conditional distribution of the feature.
Reference: <author> Skalak, D. B. </author> <year> (1994). </year> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> (pp. 293-301), </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Spackman, K. A. </author> <year> (1988). </year> <title> Learning categorical decision criteria. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> (pp. 36-46), </pages> <address> Ann Arbor, MI. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As evidence for this position that sparse perceptrons are comprehensible, consider that linear discriminant functions are commonly used to express domain knowledge in fields such as medicine <ref> (Spackman, 1988) </ref> and molecular biology (Stormo, 1987). In short, BBP is in some sense a hybrid between the decision tree and multi-layer network approaches to machine learning, combining some of the interpretability of one approach with the powerful inductive bias of the other.
Reference: <author> Stanfill, C. & Waltz, D. </author> <year> (1986). </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228. </pages>
Reference: <author> Stone, M. </author> <year> (1974). </year> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 36 </volume> <pages> 111-147. </pages>
Reference-contexts: Such a set is called a test set or a holdout set. Unless the size of the available data set is quite large, or unless the nature of the data somehow precludes it, a preferred method for accuracy estimation is to use cross validation <ref> (Stone, 1974) </ref>. In k-fold cross validation, the available data is partitioned into k separate sets of approximately equal size. The cross-validation procedure involves k iterations in which the learning method is given k 1 of the subsets to use as training data, and is tested on the set left out.
Reference: <author> Stormo, G. </author> <year> (1987). </year> <title> Identifying coding sequences. </title> <editor> In Bishop, M. J. & Rawlings, C. J., editors, </editor> <title> Nucleic Acid and Protein Sequence Analysis: A Practical Approach. </title> <publisher> IRL Press, Oxford, </publisher> <address> England. </address>
Reference-contexts: As evidence for this position that sparse perceptrons are comprehensible, consider that linear discriminant functions are commonly used to express domain knowledge in fields such as medicine (Spackman, 1988) and molecular biology <ref> (Stormo, 1987) </ref>. In short, BBP is in some sense a hybrid between the decision tree and multi-layer network approaches to machine learning, combining some of the interpretability of one approach with the powerful inductive bias of the other. This chapter is based on joint work with Jeffrey Jackson.
Reference: <author> Sutton, R. S. & Matheus, C. J. </author> <year> (1991). </year> <title> Learning polynomial functions by feature construction. </title> <booktitle> In Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <pages> (pp. 208-212), </pages> <address> Evanston, IL. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Tan, A.-H. </author> <year> (1994). </year> <title> Rule learning and extraction with self-organizing neural networks. </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> (pp. 192-199), </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Tchoumatchenko, I. & Ganascia, J.-G. </author> <year> (1994). </year> <title> A Bayesian framework to integrate symbolic and neural learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> (pp. 302-308), </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Thrun, S. </author> <year> (1995). </year> <title> Extracting rules from artificial neural networks with distributed representations. </title> <editor> In Tesauro, G., Touretzky, D., & Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 7). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Thrun, S. </author> <year> (1996). </year> <title> Explanation-Based Neural Network Learning: A Lifelong Approach. </title> <publisher> Kluwer, </publisher> <address> Boston, MA. </address>
Reference: <author> Thrun, S. B. & Moeller, K. </author> <year> (1992). </year> <title> Active exploration in dynamic environments. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 4). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: The heuristic used by Trepan is related to the first three of these: Trepan selects data in the part of the instance space in which it is currently refining its model. In practical settings, active learning has gained the most attention in reinforcement-learning problems <ref> (e.g., Thrun & Moeller, 1992) </ref>. Active learning is a natural aspect of reinforcement learning since, in these problems, the learner can influence its environment, thereby exerting some control over the states (i.e., training examples) it experiences.
Reference: <author> Towell, G. </author> <year> (1991). </year> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement, and Extraction. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, WI. </institution> <note> 198 Towell, </note> <author> G. & Shavlik, J. </author> <year> (1993). </year> <title> Extracting refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 71-101. </pages>
Reference-contexts: Experimental evidence indicates that the weights tend to be fairly well clustered after training as well <ref> (Towell, 1991) </ref>. The applicability of the MofN method might seem to be limited to knowledge-based networks, since in conventional neural networks there is usually not an inductive bias leading weight values to be clustered after training.
Reference: <author> Towell, G. & Shavlik, J. </author> <year> (1994). </year> <booktitle> Knowledge-based artificial neural networks. Artificial Intelligence, </booktitle> <address> 70(1,2):119-165. </address>
Reference-contexts: This clustering reduces the search problem from one defined by n weights to one defined by (c t n) clusters. This approach, which assumes that the weights are fairly well clustered after training, was initially developed for knowledge-based neural networks <ref> (Towell & Shavlik, 1994) </ref>, in which the initial weights of the network are specified by a set of symbolic inference rules. Since they correspond the symbolic rules, the weights are initially well clustered, and empirical results indicate that the weights of knowledge-based neural networks remain fairly clustered after training. <p> An ensemble is a set of separately trained classifiers whose predictions are combined through some weighting scheme. Here, the ensembles are induced using the Addemup algorithm (Opitz & Shavlik, 1996). The Addemup algorithm uses a domain theory and genetic search techniques to generate an ensemble of knowledge-based neural networks <ref> (Towell & Shavlik, 1994) </ref>. As mentioned in Chapter 2, a domain theory is a set of symbolic inference rules that represents an approximately correct solution to the task at hand. A knowledge-based network is a neural network in which the topology and initial weights are specified by a domain theory. <p> The domain theory used in the telephone experiments was derived from the inference rules of an expert system that performs the local-loop diagnosis task. The Addemup algorithm initially maps the rules of a domain theory into a set of networks using the Kbann algorithm <ref> (Towell & Shavlik, 1994) </ref>, and then significantly alters the architecture of the networks by interleaving a genetic search algorithm with training. In the experiments reported here, Addemup produced ensembles consisting of twenty networks each. <p> As discussed in Chapter 2, Towell and Shavlik's method is interesting because it simplifies the combinatorics involved in extracting local rules, and because it tends to produce relatively concise rule sets. Since the MofN algorithm was designed to be applied to knowledge-based neural networks <ref> (Towell & Shavlik, 1994) </ref>, however, it makes certain assumptions about the distribution of weights in trained networks. The method I present in this chapter, called MofN-sws, is designed to extend the applicability of the MofN algorithm to ordinary neural networks. <p> For knowledge-based neural networks, this is a reasonable assumption since the weights are clustered before training. For example, using the Kbann algorithm <ref> (Towell & Shavlik, 1994) </ref> to map a set of symbolic rules into a knowledge-based network, the weights that are specified by the domain theory have values of approximately 4 and -4, whereas the rest of the weights in the network have values near 0. <p> For example, membership queries could be used to determine if a network's predictions were statistically dependent on particular feature expressions. A related issue is how best to apply Trepan to knowledge-based neural networks <ref> (Towell & Shavlik, 1994) </ref>. Recall that a knowledge-based network has its topology and initial weights specified by a formal specification of available background knowledge (i.e., a domain theory). Trepan currently treats a given model as a black box, interacting with it only via membership queries.
Reference: <author> Towell, G., Shavlik, J., & Noordewier, M. </author> <year> (1990). </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 861-866), </pages> <address> Boston, MA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: They include: recognizing protein-coding regions in E. coli DNA sequences (Craven & Shavlik, 1993b), diagnosing the presence of heart disease in patients (Detrano et al., 1989), mapping English text into its pronunciation (Sejnowski & Rosenberg, 1987), recognizing promoters in E. coli DNA sequences <ref> (Towell et al., 1990) </ref>, diagnosing faults in local telephone loops (Provost & Danyluk, 1995), and predicting the party affiliation of members of the U.S. House of Representatives given their voting records (Schlimmer & Fisher, 1986). <p> The scaled-down version used here involves learning only the stresses (but not the phonemes 1 ) from a corpus of the 1,000 most common English words. The promoter data set used here is a larger and more complex set than the original one <ref> (Towell et al., 1990) </ref>. Following Buntine and Niblett (1992), the physician-fee-freeze feature is not used in the voting domain in order to make the problem more difficult. For the heart, NETtalk, telephone, promoter, and voting domains, experiments are conducted using a 10-fold cross validation methodology. <p> Vowels are generally assigned a stress of pri, sec, or none, denoting whether the vowel receives primary, secondary, or no stress, respectively. 178 179 A.4 The Promoter Domain The 57 features in the promoter domain <ref> (Towell et al., 1990) </ref> represent the DNA bases in a 57-base long sequence. Each feature is named pX, where X ranges from -50 to +7 (skipping 0) and indicates the position of the feature in the sequence. The value of each feature indicates the base that occurs at that position.
Reference: <author> Tresp, V., Hollatz, J., & Ahmad, S. </author> <year> (1992). </year> <title> Network structuring and training using rule-based knowledge. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 5). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Uberbacher, E. C., Einstein, J. R., Guan, X., & Mural, R. J. </author> <year> (1993). </year> <title> Gene recognition and assembly in the GRAIL system: Progress and challenges. </title> <editor> In Lim, H., Fickett, J., Cantor, C., & Robbins, R., editors, </editor> <booktitle> Proceedings of the Second International Conference on Bioin-formatics, Supercomputing, and Complex Genome Analysis. </booktitle> <publisher> World Scientific, Singapore. </publisher>
Reference-contexts: In the first case, the goal is to use a learning method to induce a model that can be used to perform some task of interest. For example, the ALVINN system (Pomerleau, 1993) has learned to steer a motor vehicle, and the GRAIL system <ref> (Uberbacher et al., 1993) </ref> has learned to recognize genes in uncharacterized DNA sequences. As illustrated by these cases, inductive learning methods can often learn to perform tasks that we do not know how to program explicitly, or that are too difficult and time-consuming to program explicitly.
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142. </pages>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, Cam-bridge University, </institution> <address> Cambridge, England. </address>
Reference-contexts: In some cases, neural networks have a more appropriate restricted hypothesis space bias than other learning algorithms. For example, the Q-learning method for reinforcement learning problems <ref> (Watkins, 1989) </ref> requires that the learner represent hypotheses as continuous-valued functions, and it requires that these hypotheses be updated after each training example. Few if any symbolic learning algorithms are able to meet both of these requirements. <p> The network is trained using a method called Q-learning <ref> (Watkins, 1989) </ref>. In Q-learning, the learner estimates an evaluation function, Q (s; a), that specifies the discounted, cumulative reinforcement that can be achieved starting from state s, applying a as the first action, and assuming that optimal actions are followed thereafter.
Reference: <author> Watrous, R. L. & Kuhn, G. M. </author> <year> (1992). </year> <title> Induction of finite state languages using second-order neural networks. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 4). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Weigend, A. </author> <year> (1994). </year> <title> On overfitting and the effective number of hidden units. </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> (pp. 335-342), </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: In such networks, local descriptions of hidden units make for very large, complicated rule sets. Another problem with local descriptions of hidden units is that trained networks often employ a large set of hidden units to represent a smaller set of "derived" features <ref> (Weigend, 1994) </ref>. In such cases, local hidden-unit descriptions do not correspond very well to the derived features that the network has learned. 40 Another limitation of many rule-extraction methods is that they are designed for problems that have only discrete-valued features.
Reference: <author> Weigend, A. S., Zimmermann, H. G., & Neuneier, R. </author> <year> (1995). </year> <title> Clearning. </title> <type> Technical Report CU-CS-772-95, </type> <institution> Computer Science Department, University of Colorado. </institution>
Reference-contexts: Specifically, the network is trained to predict the Dollar-Mark exchange rate <ref> (Weigend et al., 1995) </ref>. This network was trained by Weigend et al. without any intention of later applying Trepan to it. <p> The network was trained using the technique of clearning <ref> (Weigend et al., 1995) </ref>. <p> aid to Nicaraguan Contras crime crime education education spending el salvador El Salvador aid exports duty free exports immigration immigration mx missile MX missile synfuels Synfuels corporation cutback religious religious groups in schools satellite anti-satellite test ban 181 182 A.6 The Exchange-Rate Domain The following features from the exchange-rate domain <ref> (Weigend et al., 1995) </ref> are incorpo rated into the depicted tree. There are sets of features in this domain that involve multiple functions computed on the same underlying indicator; these features are indicated by sub scripts in the table and the tree.
Reference: <author> Weiss, S. M. & Kapouleas, I. </author> <year> (1989). </year> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 688-693), </pages> <address> Detroit, MI. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Wejchert, J. & Tesauro, G. </author> <year> (1989). </year> <title> Neural network visualization. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2), </booktitle> <pages> (pp. 465-472), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: activation signals through the network (Craven & Shavlik, 1992), * the sensitivity of hidden and output unit activations to smoothly varying inputs (Pomer leau & Touretzky, 1993), * the backward propagation of error signals during learning (Craven & Shavlik, 1992), * the trajectory of units in weight space during learning <ref> (Wejchert & Tesauro, 1989) </ref>. These visualization methods can sometimes provide insight into the learning and prediction behavior of a network. They seem to be especially effective in tasks where the input to the network is image data (e.g., Pomerleau & Touretzky, 1993).
Reference: <author> White, H. </author> <year> (1989a). </year> <title> Learning in artificial neural networks: a statistical perspective. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 425-461. </pages> <address> 199 White, </address> <publisher> H. </publisher> <year> (1989b). </year> <title> Some asymptotic results for learning in single hidden-layer feedforward network models. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 84(408) </volume> <pages> 1003-1013. </pages>
Reference: <author> Widrow, B., Rumelhart, D. E., & Lehr, M. A. </author> <year> (1994). </year> <title> Neural networks: Applications in industry, business, </title> <journal> and science. Communications of the ACM, </journal> <volume> 37(3) </volume> <pages> 93-105. </pages>
Reference: <author> Wolberg, W. H., Street, W. N., & Mangasarian, O. L. </author> <year> (1994). </year> <title> Machine learning techniques to diagnose breast cancer from fine needle aspirates. </title> <journal> Cancer Letters, </journal> <volume> 77 </volume> <pages> 163-171. </pages>
Reference-contexts: In order to gain confidence in the performance of a learning system, its users often want to know how it arrives at its decisions. The ability to inspect a learned hypothesis is important in such domains. It is an especially important criterion in domains, such as medical diagnosis <ref> (e.g., Wolberg et al., 1994) </ref>, in which the system occupies a position of trust. * Discovery. Learning systems may also play an important role in the process of scientific discovery. A system may discover salient features and relationships in the training data whose importance was not previously recognized.
Reference: <author> Wolpert, D. H. </author> <year> (1995). </year> <title> The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework. </title> <editor> In Wolpert, D. H., editor, </editor> <title> The Mathematics of Generalization. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: For example, most learning algorithms initially try to fit a simple hypothesis to a given training set and then explore progressively more complex hypotheses until they find an acceptable fit. There is no universally "best" learning algorithm <ref> (Wolpert, 1995) </ref>. That is, there is no 7 learning method that provides superior predictive accuracy for all problems. Therefore, often one of the tasks involved in tackling a learning problem is to try to select the algorithm with the most suitable inductive bias for the problem. <p> The two algorithms have different inductive biases, and as stated in Chapter 1, no learning algorithm has a universally superior bias <ref> (Wolpert, 1995) </ref>. A related argument is that there are other learning methods that produce hard-to-understand hypotheses. As argued elsewhere in this thesis, Trepan is general enough that it can be used to understand a wide range of learned models.
Reference: <author> Zadeh, L. A. </author> <year> (1965). </year> <title> Fuzzy sets. </title> <journal> Information and Control, </journal> <volume> 8 </volume> <pages> 338-353. </pages>
Reference-contexts: Hayashi (1991) has described a method for extracting fuzzy rules from a specialized network designed for the task. Hayashi's extraction method is quite similar to the local search procedures described above. The principal difference is that the literals in the rules can represent fuzzy conditions <ref> (Zadeh, 1965) </ref>. A fuzzy condition is one which has graded, as opposed to Boolean, degrees of satisfaction. A local method developed by Towell and Shavlik (1993) searches not for conjunctive rules, but instead for rules that include m-of-n expressions.
Reference: <author> Zeng, Z., Goodman, R. M., & Smyth, P. </author> <year> (1993). </year> <title> Learning finite state machines with self-clustering recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 5(6) </volume> <pages> 976-990. </pages>
References-found: 171

