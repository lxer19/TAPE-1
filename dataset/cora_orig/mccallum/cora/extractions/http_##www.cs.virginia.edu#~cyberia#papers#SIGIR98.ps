URL: http://www.cs.virginia.edu/~cyberia/papers/SIGIR98.ps
Refering-URL: http://www.cs.virginia.edu/~alp4g/publications.html
Root-URL: http://www.cs.virginia.edu
Email: ffrench|alp4gg@cs.virginia.edu  viles@ils.unc.edu  fte3d|kjp4fg@cs.virginia.edu  
Title: Evaluating Database Selection Techniques: A Testbed and Experiment  
Author: James C. French Allison L. Powell Charles L. Viles Travis Emmitt Kevin J. Prey 
Address: Charlottesville, VA  Library Science  Chapel Hill, NC  Charlottesville, VA  
Affiliation: Department of Computer Science University of Virginia  School of Information and  University of North Carolina, Chapel Hill  Department of Computer Science University of Virginia  
Abstract: We describe a testbed for database selection techniques and an experiment conducted using this testbed. The testbed is a decomposition of the TREC/TIPSTER data that allows analysis of the data along multiple dimensions, including collection-based and temporal-based analysis. We characterize the subcollections in this testbed in terms of number of documents, queries against which the documents have been evaluated for relevance, and distribution of relevant documents. We then present initial results from a study conducted using this testbed that examines the effectiveness of the gGlOSS approach to database selection. The databases from our testbed were ranked using the gGlOSS techniques and compared to the gGlOSS Ideal(l) baseline and a baseline derived from TREC relevance judgements. We have examined the degree to which several gGlOSS estimate functions approximate these base-lines. Our initial results confirm that the gGlOSS estimators are excellent predictors of the Ideal(l) ranks but that the Ideal(l) ranks do not estimate relevance-based ranks well. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Nicholas J. Belkin, Paul Kantor, Edward A. Fox, and J. A. Shaw. </author> <title> Combining the Evidence of Multiple Query Representations for Informa tion Retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 31(4) </volume> <pages> 431-448, </pages> <year> 1995. </year>
Reference-contexts: Other large corpora such as USENET news groups have also been used. Distributed IR research encompasses many important problems such as the following: * database or collection selection [3, 7, 6, 9]; * collection fusion or results merging <ref> [1, 3, 4, 15, 14] </ref>; and * dissemination of collection information to increase re trieval effectiveness [5, 11, 10, 12].
Reference: [2] <author> Chris Buckley. </author> <note> SMART version 11.0, 1992. ftp://ftp.cs.cornell.edu/pub/smart. </note>
Reference-contexts: We used the TREC topics 51-150 as the test query set. gGlOSS Estimates We prepared the test collection by using SMART version 11.0 <ref> [2] </ref>.
Reference: [3] <author> James P. Callan, Zhihong Lu, and W. Bruce Croft. </author> <title> Searching Distributed Collections with Inference Networks. </title> <booktitle> In Proceedings of the 18th International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 21-29, </pages> <address> Seattle, WA, </address> <year> 1995. </year>
Reference-contexts: Other large corpora such as USENET news groups have also been used. Distributed IR research encompasses many important problems such as the following: * database or collection selection <ref> [3, 7, 6, 9] </ref>; * collection fusion or results merging [1, 3, 4, 15, 14]; and * dissemination of collection information to increase re trieval effectiveness [5, 11, 10, 12]. <p> Other large corpora such as USENET news groups have also been used. Distributed IR research encompasses many important problems such as the following: * database or collection selection [3, 7, 6, 9]; * collection fusion or results merging <ref> [1, 3, 4, 15, 14] </ref>; and * dissemination of collection information to increase re trieval effectiveness [5, 11, 10, 12]. <p> In addition, the TREC data is often referenced in terms of source : disk number, and has often been subdivided using one or both of those attributes <ref> [3, 16, 4, 13] </ref>. To the extent possible, a candidate partition should not obscure these other, more coarse grained possibilities. * At least 100 subcollections. We feel realistic experiments must at least involve distributed document collections with hundreds of participating subcollec tions. * A temporal dimension.
Reference: [4] <author> Edward A. Fox, M. Prabhakar Koushik, Joseph Shaw, Rus-sell M odlin, and Durgesh Rao. </author> <title> Combining Evidence from Multiple Searches. </title> <booktitle> In The First Text Retrieval Conference (TREC-1), </booktitle> <pages> pages 319-328, </pages> <address> Gaithersburg, MD, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Other large corpora such as USENET news groups have also been used. Distributed IR research encompasses many important problems such as the following: * database or collection selection [3, 7, 6, 9]; * collection fusion or results merging <ref> [1, 3, 4, 15, 14] </ref>; and * dissemination of collection information to increase re trieval effectiveness [5, 11, 10, 12]. <p> In addition, the TREC data is often referenced in terms of source : disk number, and has often been subdivided using one or both of those attributes <ref> [3, 16, 4, 13] </ref>. To the extent possible, a candidate partition should not obscure these other, more coarse grained possibilities. * At least 100 subcollections. We feel realistic experiments must at least involve distributed document collections with hundreds of participating subcollec tions. * A temporal dimension.
Reference: [5] <author> James C. French and Charles L. Viles. </author> <title> Ensuring Retrieval Effectiveness in Distributed Digital Libraries. </title> <journal> Journal of Visual Communication and Image Representation, </journal> <volume> 7(1) </volume> <pages> 61-73, </pages> <year> 1996. </year>
Reference-contexts: Distributed IR research encompasses many important problems such as the following: * database or collection selection [3, 7, 6, 9]; * collection fusion or results merging [1, 3, 4, 15, 14]; and * dissemination of collection information to increase re trieval effectiveness <ref> [5, 11, 10, 12] </ref>.
Reference: [6] <author> Luis Gravano and Hector Garcia-Molina. </author> <title> Generalizing GlOSS to Vector-Space Databases and Broker Hierarchies. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Databases (VLDB), </booktitle> <address> Zurich, Switzerland, </address> <year> 1995. </year>
Reference-contexts: Other large corpora such as USENET news groups have also been used. Distributed IR research encompasses many important problems such as the following: * database or collection selection <ref> [3, 7, 6, 9] </ref>; * collection fusion or results merging [1, 3, 4, 15, 14]; and * dissemination of collection information to increase re trieval effectiveness [5, 11, 10, 12]. <p> Unfortunately, the nature of this comparison also differs from research group to research group. This point will be developed more fully in the section on evaluation below. In our experiments we investigate the gGlOSS <ref> [6] </ref> methodology in a different test environment and compare its performance to the standard proposed by its developers, so-called Ideal (l) ranks, as well as to the standard used by Callan et al.[3], the so-called optimal ranks. <p> These are described more fully later. 3 3.2 gGlOSS Gravano et al.[7] proposed GlOSS, the Glossary-of-Servers Server, as an approach to the database selection problem. GlOSS originally assumed a Boolean retrieval model but was later generalized to gGlOSS <ref> [6] </ref> to handle the vector space information retrieval model. gGlOSS assumes that the group of databases can be characterized according to their goodness with respect to any particular query. gGlOSS 's job is then to estimate the goodness of each candidate database with respect to a particular query and then suggest <p> Goodness and Ideal Ranks In <ref> [6] </ref>, Gravano et al. make the following two assumptions: 1. all the databases in a group of databases employ the same algorithms to compute term weights and simi larities; and 2. given a query q and a document d, d is only useful for q if sim (q; d) &gt; l <p> As we have already noted, gGlOSS creates rankings for each query by estimating the goodness of each database with respect to a query. There are many ways that one might make these estimates; two were reported in <ref> [6] </ref>. 1. High-Correlation Scenario: if two query terms t 1 and t 2 appear in db i and df i1 df i2 then every document in db i that contains t 1 also contains t 2 . This assump tion gives rise to the M ax (l) estimator. 2. <p> This forms the basis for the gGlOSS estimate of the goodness of db. Complete details for calculating the M ax (l) and Sum (l) estimators are given in <ref> [6] </ref> and are not reproduced here.
Reference: [7] <author> Luis Gravano, Hector Garcia-Molina, and Anthony Tomasic. </author> <title> The Effectiveness of GlOSS for the Text Database Discovery Problem. </title> <booktitle> In SIGMOD94, </booktitle> <pages> pages 126-137, </pages> <address> Minneapolis, MN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Other large corpora such as USENET news groups have also been used. Distributed IR research encompasses many important problems such as the following: * database or collection selection <ref> [3, 7, 6, 9] </ref>; * collection fusion or results merging [1, 3, 4, 15, 14]; and * dissemination of collection information to increase re trieval effectiveness [5, 11, 10, 12].
Reference: [8] <author> Donna Harman. </author> <booktitle> Overview of the Fourth Text Retrieval Conference (TREC-4). In Proceedings of the Fourth Text Retrieval Conference (TREC-4), </booktitle> <address> Gaithersburg, MD, </address> <year> 1996. </year>
Reference-contexts: An examination of Table 1 shows the variety of test environments employed by researchers and gives some insight into why it is often impossible to compare findings from different research efforts. 1 A description of this data can be found in Harman <ref> [8] </ref> for example. 1 Group Sources DB Queries Gravano et al.[6] news groups 53 6,800 user Moffat & Zobel [9] TREC (by source, disk) 9 51-150 Viles & French [10] TREC-CatB (random) 20 201-250 Vorhees et al.[14] TREC (by source) 5 1-200 Callan et al.[3] TREC (by source, disk) 17 51-150 <p> This will be particularly helpful for those who are used to the standard TREC referencing that uses source and disk number. We started with the data available to participants in the TREC-4 <ref> [8] </ref> experiments. Gross characteristics of this data appear in Table 2.
Reference: [9] <author> Alistair Moffat and Justin Zobel. </author> <title> Information Retrieval Systems for Large Document Collections. </title> <booktitle> In Proceedings of the Third Text Retrieval Conference (TREC-3), </booktitle> <pages> pages 85-94, </pages> <address> Gaithersburg, MD, </address> <year> 1995. </year>
Reference-contexts: Other large corpora such as USENET news groups have also been used. Distributed IR research encompasses many important problems such as the following: * database or collection selection <ref> [3, 7, 6, 9] </ref>; * collection fusion or results merging [1, 3, 4, 15, 14]; and * dissemination of collection information to increase re trieval effectiveness [5, 11, 10, 12]. <p> employed by researchers and gives some insight into why it is often impossible to compare findings from different research efforts. 1 A description of this data can be found in Harman [8] for example. 1 Group Sources DB Queries Gravano et al.[6] news groups 53 6,800 user Moffat & Zobel <ref> [9] </ref> TREC (by source, disk) 9 51-150 Viles & French [10] TREC-CatB (random) 20 201-250 Vorhees et al.[14] TREC (by source) 5 1-200 Callan et al.[3] TREC (by source, disk) 17 51-150 Walczuch et al.[16] TREC (by source) 5 1-100 Fox et al.[4] TREC (by source) 5 1-100 Vorhees [13] TREC
Reference: [10] <author> Charles L. Viles and James C. </author> <title> French. TREC4 Experiments Using Drift. </title> <booktitle> In Proceedings of the Fourth Text Retrieval Conference (TREC-4), </booktitle> <address> Gaithersburg, MD, </address> <year> 1996. </year>
Reference-contexts: Distributed IR research encompasses many important problems such as the following: * database or collection selection [3, 7, 6, 9]; * collection fusion or results merging [1, 3, 4, 15, 14]; and * dissemination of collection information to increase re trieval effectiveness <ref> [5, 11, 10, 12] </ref>. <p> is often impossible to compare findings from different research efforts. 1 A description of this data can be found in Harman [8] for example. 1 Group Sources DB Queries Gravano et al.[6] news groups 53 6,800 user Moffat & Zobel [9] TREC (by source, disk) 9 51-150 Viles & French <ref> [10] </ref> TREC-CatB (random) 20 201-250 Vorhees et al.[14] TREC (by source) 5 1-200 Callan et al.[3] TREC (by source, disk) 17 51-150 Walczuch et al.[16] TREC (by source) 5 1-100 Fox et al.[4] TREC (by source) 5 1-100 Vorhees [13] TREC (source, month) 98 251-300 Table 1: Summary attributes of distributed
Reference: [11] <author> Charles L. Viles and James C. </author> <title> French. Dissemination of Collection Wide Information in a Distributed Inform ation Retrieval System. </title> <booktitle> In Proceedings of the 18th International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 12-20, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Distributed IR research encompasses many important problems such as the following: * database or collection selection [3, 7, 6, 9]; * collection fusion or results merging [1, 3, 4, 15, 14]; and * dissemination of collection information to increase re trieval effectiveness <ref> [5, 11, 10, 12] </ref>.
Reference: [12] <author> Charles L. Viles and James C. </author> <title> French. On the Update of Term Weights in Dynamic Information Retrieval Syste ms. </title> <booktitle> In Proceedings of the 4th International Conference on Knowledge and Information Management, </booktitle> <pages> pages 167-174, </pages> <address> Baltimore, MD, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: Distributed IR research encompasses many important problems such as the following: * database or collection selection [3, 7, 6, 9]; * collection fusion or results merging [1, 3, 4, 15, 14]; and * dissemination of collection information to increase re trieval effectiveness <ref> [5, 11, 10, 12] </ref>.
Reference: [13] <author> Ellen Vorhees. </author> <title> The TREC-5 Database Merging Track. </title> <booktitle> In Proceedings of the Fifth Text Retrieval Conference (TREC-5), </booktitle> <address> Gaithersburg, MD, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: Zobel [9] TREC (by source, disk) 9 51-150 Viles & French [10] TREC-CatB (random) 20 201-250 Vorhees et al.[14] TREC (by source) 5 1-200 Callan et al.[3] TREC (by source, disk) 17 51-150 Walczuch et al.[16] TREC (by source) 5 1-100 Fox et al.[4] TREC (by source) 5 1-100 Vorhees <ref> [13] </ref> TREC (source, month) 98 251-300 Table 1: Summary attributes of distributed document collections that have been used in a sampling of previous work. 2.1 Test Data For effectiveness experiments, the possible sources of data are relatively limited. <p> In addition, the TREC data is often referenced in terms of source : disk number, and has often been subdivided using one or both of those attributes <ref> [3, 16, 4, 13] </ref>. To the extent possible, a candidate partition should not obscure these other, more coarse grained possibilities. * At least 100 subcollections. We feel realistic experiments must at least involve distributed document collections with hundreds of participating subcollec tions. * A temporal dimension.
Reference: [14] <author> Ellen Vorhees, Narendra K. Gupta, and Ben Johnson-Laird. </author> <title> Learning Collection Fusion Strategies. </title> <booktitle> In Proceedings of the 18th International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 172-179, </pages> <address> Seattle, WA, </address> <year> 1995. </year>
Reference-contexts: Other large corpora such as USENET news groups have also been used. Distributed IR research encompasses many important problems such as the following: * database or collection selection [3, 7, 6, 9]; * collection fusion or results merging <ref> [1, 3, 4, 15, 14] </ref>; and * dissemination of collection information to increase re trieval effectiveness [5, 11, 10, 12].
Reference: [15] <author> Ellen Vorhees, Narendra K. Gupta, and Ben Johnson-Laird. </author> <title> The Collection Fusion Problem. </title> <booktitle> In Proceedings of the Third Text Retrieval Conference (TREC-3), </booktitle> <pages> pages 95-104, </pages> <address> Gaithersburg, MD, </address> <year> 1995. </year>
Reference-contexts: Other large corpora such as USENET news groups have also been used. Distributed IR research encompasses many important problems such as the following: * database or collection selection [3, 7, 6, 9]; * collection fusion or results merging <ref> [1, 3, 4, 15, 14] </ref>; and * dissemination of collection information to increase re trieval effectiveness [5, 11, 10, 12].
Reference: [16] <author> Nikolaus Walczuch, Norbert Fuhr, Michael Pollman, and Birgit Sievers. </author> <title> Routing and Ad-hoc Retrieval with the TREC-3 Collection in a Loosely Federated Environment. </title> <booktitle> In The Third Text REtrieval Conference (TREC-3), </booktitle> <pages> pages 135-144, </pages> <address> Gaithersburg, MD, </address> <month> November </month> <year> 1994. </year> <month> 9 </month>
Reference-contexts: In addition, the TREC data is often referenced in terms of source : disk number, and has often been subdivided using one or both of those attributes <ref> [3, 16, 4, 13] </ref>. To the extent possible, a candidate partition should not obscure these other, more coarse grained possibilities. * At least 100 subcollections. We feel realistic experiments must at least involve distributed document collections with hundreds of participating subcollec tions. * A temporal dimension.
References-found: 16

