URL: http://www.cogs.susx.ac.uk/users/christ/papers/constructive-learners.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Phone: Tel: (44)1273 678856  
Title: What do Constructive Learners Really Learn?  
Author: Chris Thornton 
Date: February 25, 1998  
Web: WWW: http://www.cogs.susx.ac.uk/users/cjt  
Address: Brighton BN1 9QH  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: In constructive induction (CI), the learner's problem representation is modified as a normal part of the learning process. This may be necessary if the initial representation is inadequate or inappropriate. However, the distinction between constructive and non-constructive methods appears to be highly ambiguous. Several conventional definitions of the process of constructive induction appear to include all conceivable learning processes. In this paper I argue that the process of constructive learning should be identified with that of relational learning (i.e., I suggest that 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Michalski, R. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <editor> In R. Michalski, J. Carbonell and T. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Palo Alto: </address> <publisher> Tioga. </publisher>
Reference-contexts: 1 Introduction Constructive induction (CI) is of use when the initial representation for a problem obstructs the application of ordinary inductive methods <ref> [1] </ref>. Wnek and Michalski [2] have divided constructive induction methods into several types including hypothesis-driven (HCI) methods, data-driven (DCI) methods and knowledge-driven (KCI) methods. Practical methods introduced in recent years include FRINGE [3], AQ17-HCI [Wnek and Michalski,AQ17,1994] and CN2-MCI [Kramer, 1994].
Reference: [2] <author> Wnek, J. and Michalski, R. </author> <year> (1994). </year> <title> Hypothesis-driven constructive induction in AQ17-HCI: a method and experiments. </title> <booktitle> Machine Learning, </booktitle> <address> 14 (p. 139). Boston: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: 1 Introduction Constructive induction (CI) is of use when the initial representation for a problem obstructs the application of ordinary inductive methods [1]. Wnek and Michalski <ref> [2] </ref> have divided constructive induction methods into several types including hypothesis-driven (HCI) methods, data-driven (DCI) methods and knowledge-driven (KCI) methods. Practical methods introduced in recent years include FRINGE [3], AQ17-HCI [Wnek and Michalski,AQ17,1994] and CN2-MCI [Kramer, 1994].
Reference: [3] <author> Pagallo, G. </author> <year> (1989). </year> <title> Learning DNF by decision trees. </title> <booktitle> Proceedings of The Eleventh Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 639-644). </pages> <publisher> Morgan Kaufmann. </publisher> <pages> 10 </pages>
Reference-contexts: Wnek and Michalski [2] have divided constructive induction methods into several types including hypothesis-driven (HCI) methods, data-driven (DCI) methods and knowledge-driven (KCI) methods. Practical methods introduced in recent years include FRINGE <ref> [3] </ref>, AQ17-HCI [Wnek and Michalski,AQ17,1994] and CN2-MCI [Kramer, 1994]. Almost all CI methods seek to transform the initial representation space by introducing new features. However, in the literature, the term `feature' has been used ambiguously.
Reference: [4] <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, this usage cannot be taken too literally since all supervised learning algorithms attempt to implement the target partitioning on the space and would thus all be potentially classified as constructive. Explicitly discounting such degenerate cases still leaves us with methods such as C4.5 <ref> [4] </ref>, Backpropagation [5] and CN2 [6, 7] which all make use of intermediate constructs (i.e., constructs not directly involved in production of output) that identify partitions on the representation space. 1 These methods would seem to satisfy the criterion of being `feature generators' in a non-degenerate sense and yet they are
Reference: [5] <author> Rumelhart, D., Hinton, G. and Williams, R. </author> <year> (1986). </year> <title> Learning representations by back-propagating errors. </title> <booktitle> Nature, </booktitle> <pages> 323 (pp. 533-6). </pages>
Reference-contexts: However, this usage cannot be taken too literally since all supervised learning algorithms attempt to implement the target partitioning on the space and would thus all be potentially classified as constructive. Explicitly discounting such degenerate cases still leaves us with methods such as C4.5 [4], Backpropagation <ref> [5] </ref> and CN2 [6, 7] which all make use of intermediate constructs (i.e., constructs not directly involved in production of output) that identify partitions on the representation space. 1 These methods would seem to satisfy the criterion of being `feature generators' in a non-degenerate sense and yet they are typically described
Reference: [6] <author> Clark, P. and Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <booktitle> Machine Learning, </booktitle> <pages> 3 (pp. 261-283). </pages>
Reference-contexts: However, this usage cannot be taken too literally since all supervised learning algorithms attempt to implement the target partitioning on the space and would thus all be potentially classified as constructive. Explicitly discounting such degenerate cases still leaves us with methods such as C4.5 [4], Backpropagation [5] and CN2 <ref> [6, 7] </ref> which all make use of intermediate constructs (i.e., constructs not directly involved in production of output) that identify partitions on the representation space. 1 These methods would seem to satisfy the criterion of being `feature generators' in a non-degenerate sense and yet they are typically described as `selective' and
Reference: [7] <author> Clark, P. and Boswell, R. </author> <year> (1991). </year> <title> Rule induction with CN2: some recent improvements. </title> <editor> In Y. Kodratoff (Ed.), </editor> <booktitle> Proceedings of the Fifth European Working Session on Learning. No. 482 of Lecture Notes in Artificial Intelligence (pp. </booktitle> <pages> 151-163). </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: However, this usage cannot be taken too literally since all supervised learning algorithms attempt to implement the target partitioning on the space and would thus all be potentially classified as constructive. Explicitly discounting such degenerate cases still leaves us with methods such as C4.5 [4], Backpropagation [5] and CN2 <ref> [6, 7] </ref> which all make use of intermediate constructs (i.e., constructs not directly involved in production of output) that identify partitions on the representation space. 1 These methods would seem to satisfy the criterion of being `feature generators' in a non-degenerate sense and yet they are typically described as `selective' and
Reference: [8] <author> Sazonov, V. and Wnek, J. </author> <year> (1994). </year> <title> A hypothesis-driven constructive induction approach to expanding neural networks. </title> <booktitle> Proceedings of ML-COLT'94. </booktitle>
Reference: [9] <author> Pfahringer, B. </author> <year> (1994). </year> <title> Cipf 2.0: a robust constructive induction system. </title> <booktitle> Proceedings of ML-COLT'94. </booktitle>
Reference: [10] <author> Kramer, S. </author> <year> (1994). </year> <title> CN2-MCI: a two-step method for constructive induction. </title> <booktitle> Proceedings of ML-COLT'94. </booktitle>
Reference-contexts: Oliveira and Sangiovanni-Vincentelli [12] describe a system that searches for a minimal 2 It is no surprise to find that practical learning methods tend to be predominantly empirical rather than relational [26]. 7 set of features each of which tests for a particular relationship among the input variables. Kramer <ref> [10] </ref> describes a system that tries to detect and exploit relations over variables which tend to appear together in useful rules. Matheus [27] describes the CITRE system which, to a first approximation, tries to capitalize on the presence of disjunctive regions in decision tree descriptions.
Reference: [11] <author> Japkowicz, N. and Hirsh, H. </author> <year> (1994). </year> <title> Towards a bootstrapping approach to constructive induction. </title> <booktitle> Proceedings of ML-COLT'94. </booktitle>
Reference: [12] <author> Oliveira, A. and Sangiovanni-Vincentelli, A. </author> <year> (1992). </year> <title> Constructive induction using a non-greedy strategy for feature selection. </title> <editor> In D. Sleeman and P. Edwards (Eds.), </editor> <booktitle> Proceedings of the Ninth International Workshop on Machine Machine Learning (ML92) (pp. </booktitle> <pages> 355-360). </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann Publishers. </publisher> <pages> 11 </pages>
Reference-contexts: And there is, in fact, a clear suggestion that we may be able to identify the class of constructive learners with the class of relational learners. Practical CI methods, it turns out, are often based on some form of relational search process. Oliveira and Sangiovanni-Vincentelli <ref> [12] </ref> describe a system that searches for a minimal 2 It is no surprise to find that practical learning methods tend to be predominantly empirical rather than relational [26]. 7 set of features each of which tests for a particular relationship among the input variables.
Reference: [13] <author> Gold, E. </author> <year> (1967). </year> <title> Language identification in the limit. </title> <booktitle> Information and Con--trol, </booktitle> <pages> 10 (pp. 447-74). </pages>
Reference-contexts: I will also show how the process of CI can be properly motivated as a necessary response to hard, relational learning problems. 2 Bayesian analysis of inductive justification In recent years, researchers have made rapid progress in the theoretical analysis of learning. Early work by Gold <ref> [13] </ref> and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension [16] and led to the theoretical advances of Haussler and others, e.g., [17, 18, 19, 20, 21, 21, 22].
Reference: [14] <author> Valiant, L. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <pages> 27 (pp. 1134-42). </pages>
Reference: [15] <author> Valiant, L. </author> <year> (1985). </year> <title> Learning disjunctions of conjunctions. </title> <booktitle> Proceedings of the Ninth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 560-566). </pages> <address> Los Altos: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [16] <author> Vapnik, V. and Chervonenkis, A. </author> <year> (1971). </year> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theor. Probab. Appl., </journal> <volume> 16, No. </volume> <pages> 2 (pp. 264-280). </pages>
Reference-contexts: Early work by Gold [13] and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension <ref> [16] </ref> and led to the theoretical advances of Haussler and others, e.g., [17, 18, 19, 20, 21, 21, 22].
Reference: [17] <author> Haussler, D. </author> <year> (1986). </year> <title> Quantifying the inductive bias in concept learning. </title> <institution> UCSC-CRL-86-25, University of California at Santa Cruz. </institution>
Reference-contexts: Early work by Gold [13] and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension [16] and led to the theoretical advances of Haussler and others, e.g., <ref> [17, 18, 19, 20, 21, 21, 22] </ref>. Much of this work is directed towards the goal of analyzing the complexity of learning but, at the time of writing, measuring the hardness of arbitrary learning problems (e.g., specific training sets) remains problematic [23].
Reference: [18] <author> Blumer, A., Ehrenfeucht, A., Haussler, D. and Warmuth, M. </author> <year> (1987). </year> <title> Oc-cam's razor. </title> <journal> Information Processing Letters, </journal> <pages> 24 (pp. 377-380). </pages>
Reference-contexts: Early work by Gold [13] and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension [16] and led to the theoretical advances of Haussler and others, e.g., <ref> [17, 18, 19, 20, 21, 21, 22] </ref>. Much of this work is directed towards the goal of analyzing the complexity of learning but, at the time of writing, measuring the hardness of arbitrary learning problems (e.g., specific training sets) remains problematic [23].
Reference: [19] <author> Haussler, D. </author> <year> (1988). </year> <title> Quantifying inductive bias: AI learning and valiant's learning framework. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 36 (pp. 177-221). </pages>
Reference-contexts: Early work by Gold [13] and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension [16] and led to the theoretical advances of Haussler and others, e.g., <ref> [17, 18, 19, 20, 21, 21, 22] </ref>. Much of this work is directed towards the goal of analyzing the complexity of learning but, at the time of writing, measuring the hardness of arbitrary learning problems (e.g., specific training sets) remains problematic [23].
Reference: [20] <author> Haussler, D. </author> <year> (1987). </year> <title> Bias, version spaces and valiant's learning framework. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning (pp. </booktitle> <pages> 324-336). </pages> <institution> University of California, Irvine: </institution> <month> (June 22-25). </month>
Reference-contexts: Early work by Gold [13] and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension [16] and led to the theoretical advances of Haussler and others, e.g., <ref> [17, 18, 19, 20, 21, 21, 22] </ref>. Much of this work is directed towards the goal of analyzing the complexity of learning but, at the time of writing, measuring the hardness of arbitrary learning problems (e.g., specific training sets) remains problematic [23].
Reference: [21] <author> Baum, E. and Haussler, D. </author> <year> (1990). </year> <title> What size net gives valid generalization?. </title> <editor> In J.W. Shavlik and T.G. Dietterich (Eds.), </editor> <booktitle> Readings In Machine Learning (pp. </booktitle> <pages> 258-262). </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Early work by Gold [13] and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension [16] and led to the theoretical advances of Haussler and others, e.g., <ref> [17, 18, 19, 20, 21, 21, 22] </ref>. Much of this work is directed towards the goal of analyzing the complexity of learning but, at the time of writing, measuring the hardness of arbitrary learning problems (e.g., specific training sets) remains problematic [23].
Reference: [22] <author> Haussler, D., Kearns, M. and Schapire, R. </author> <year> (1992). </year> <title> Bounds on the sample complexity of bayesian learning using information theory and the VC dimension. </title> <institution> UCSC-CRL-91-44, University of California at Santa Cruz. </institution> <month> 12 </month>
Reference-contexts: Early work by Gold [13] and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension [16] and led to the theoretical advances of Haussler and others, e.g., <ref> [17, 18, 19, 20, 21, 21, 22] </ref>. Much of this work is directed towards the goal of analyzing the complexity of learning but, at the time of writing, measuring the hardness of arbitrary learning problems (e.g., specific training sets) remains problematic [23].
Reference: [23] <author> Kearns, M. </author> <year> (1990). </year> <title> The Computational Complexity of Machine Learning. </title> <publisher> The MIT Press. </publisher>
Reference-contexts: Much of this work is directed towards the goal of analyzing the complexity of learning but, at the time of writing, measuring the hardness of arbitrary learning problems (e.g., specific training sets) remains problematic <ref> [23] </ref>. However, it turns out that a useful, qualitative measure of problem hardness can be obtained via a Bayesian analysis of justification sources for generalisation. Consider the following example. D is the body of data shown in Table 1.
Reference: [24] <editor> Michalski, R., Carbonell, J. and Mitchell, T. (Eds.) </editor> <booktitle> (1983). Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Palo Alto: </address> <publisher> Tioga. </publisher>
Reference-contexts: These groupings correspond roughly to the well established categories of empirical learning and relational learning <ref> [24, 25] </ref>. The distinction is key from the point of view of complexity. Methods in the first (empirical) category confront a much easier learning task than methods in the latter (relational) category.
Reference: [25] <editor> Michalski, R., Carbonell, J. and Mitchell, T. (Eds.) </editor> <booktitle> (1986). Machine Learning: An Artificial Intelligence Approach: Vol II. </booktitle> <address> Los Altos: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: These groupings correspond roughly to the well established categories of empirical learning and relational learning <ref> [24, 25] </ref>. The distinction is key from the point of view of complexity. Methods in the first (empirical) category confront a much easier learning task than methods in the latter (relational) category.
Reference: [26] <author> Stone, J. and Thornton, C. </author> <year> (1995). </year> <title> Can artificial neural networks discover useful regularities?. </title> <booktitle> Proceedings of ICANN-95. </booktitle> <address> Cambridge. </address>
Reference-contexts: Practical CI methods, it turns out, are often based on some form of relational search process. Oliveira and Sangiovanni-Vincentelli [12] describe a system that searches for a minimal 2 It is no surprise to find that practical learning methods tend to be predominantly empirical rather than relational <ref> [26] </ref>. 7 set of features each of which tests for a particular relationship among the input variables. Kramer [10] describes a system that tries to detect and exploit relations over variables which tend to appear together in useful rules.
Reference: [27] <author> Matheus, C. </author> <year> (1990). </year> <title> Adding domain knowledge to SBL through feature construction. </title> <booktitle> Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 803-808). </pages> <address> Boston, MA.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Kramer [10] describes a system that tries to detect and exploit relations over variables which tend to appear together in useful rules. Matheus <ref> [27] </ref> describes the CITRE system which, to a first approximation, tries to capitalize on the presence of disjunctive regions in decision tree descriptions. Other systems involve a search (i.e., operator-based) process working with some sort of relational description language, e.g., [28, 11,9].
Reference: [28] <author> Aronis, J. and Provost, F. </author> <year> (1994). </year> <title> Efficiently constructing relational features&lt;br&gt;from background knowledge for inductive machine learning. </title> <booktitle> Proceedings of ML-COLT'94. </booktitle>
Reference-contexts: Matheus [27] describes the CITRE system which, to a first approximation, tries to capitalize on the presence of disjunctive regions in decision tree descriptions. Other systems involve a search (i.e., operator-based) process working with some sort of relational description language, e.g., <ref> [28, 11,9] </ref>. In several recent cases, this type of approach has focussed on what are known as `counting' or M-of-N features, i.e., features which effectively count the number of occurrences of a particular variable value, cf. [29,30,2,8 31,32].
Reference: [29] <author> Spackman, K. </author> <year> (1988). </year> <title> Learning categorical decision criteria in biomedical domains. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 36-46). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann,. </publisher>
Reference: [30] <author> Fawcett, T. and Utgoff, P. </author> <year> (1991). </year> <title> A hybrid method for feature generation. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning (pp. </booktitle> <pages> 137-141). </pages> <address> Evanston, Ill. </address>
Reference: [31] <author> Seshu, R. </author> <year> (1989). </year> <title> Solving the Parity Problem. </title> <institution> University of Illinois at Urbana-Champaign, Inductive Learning Group. </institution> <month> 13 </month>
Reference: [32] <author> Murphy, P. and Pazzani, M. </author> <year> (1991). </year> <title> ID2-of-3: constructive induction of m--of-n concepts for discriminators in decision trees. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning (ML91). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [33] <author> Dietterich, T. and Michalski, R. </author> <year> (1983). </year> <title> A comparative review of selected methods for learning from examples. </title> <editor> In R. Michalski, J. Carbonell and T. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Palo Alto: </address> <publisher> Tioga. </publisher>
Reference-contexts: But some difficulties remain. It is arguable that the identification of constructive learning with relational learning flies in the face of convention. One of the best known definitions of constructive induction | given in <ref> [33] </ref> | is `con 8 structive induction is any form of induction that generates new descriptors not present in the input data.' And more recently, Tom Mitchell has defined constructive induction as `the process of augmenting the set of predicates, based on background knowledge.' [34].
Reference: [34] <author> Mitchell, T. </author> <year> (1997). </year> <title> Machine Learning. </title> <publisher> McGraw-Hill. </publisher> <pages> 14 </pages>
Reference-contexts: of constructive induction | given in [33] | is `con 8 structive induction is any form of induction that generates new descriptors not present in the input data.' And more recently, Tom Mitchell has defined constructive induction as `the process of augmenting the set of predicates, based on background knowledge.' <ref> [34] </ref>. These definitions refer merely to the production of new predicates rather than to the identification of relational functions (predicates). They therefore provide a definition which is weaker and more general than the one proposed.
References-found: 34

