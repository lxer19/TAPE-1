URL: http://www.eecs.umich.edu/~thalerd/icccn.ps
Refering-URL: http://www.eecs.umich.edu/~thalerd/docs.html
Root-URL: http://www.cs.umich.edu
Email: thalerd@eecs.umich.edu ravi@eecs.umich.edu  
Title: An Architecture for Inter-Domain Troubleshooting  
Author: David G. Thaler and Chinya V. Ravishankar 
Address: Ann Arbor, Michigan 48109-2122  
Affiliation: Electrical Engineering and Computer Science Department The University of Michigan,  
Abstract: In this paper, we explore the constraints of a new problem: that of coordinating network troubleshooting among peer administrative domains and untrusted observers. Allowing untrusted observers permits any entity to report problems, whether it is a Network Operations Center (NOC), end-user, or application. Our goals are to define the inter-domain coordination problem clearly, and to develop an architecture which allows observers to report problems and receive timely feedback, regardless of their own locations and identities. By automating this process, we also relieve human bottlenecks at help desks and NOCs whenever possible. We present a troubleshooting methodology for coordinating problem diagnosis, and describe GDT, a distributed protocol which realizes this methodology. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Shri Goyal and Ralph Worrest. </author> <title> Expert systems in network maintenance and management. </title> <booktitle> In IEEE International Conference on Communications, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: 1 Introduction Work to date in network management has concentrated on effectively managing a single network. To our knowledge, little has been done to address the problem of coordinated network management across administrative domains, although the need for such a global coordination system has long been recognized <ref> [1, 2, 3, 4] </ref>. A recent study of routing instability [5] found that in about 10% of the problems, all parties questioned pointed to another party as the cause. Such problems strongly underscore the need for inter-domain coordination. The true cause of a problem may be distant from its effect.
Reference: [2] <author> Makoto Yoshida, Makoto Kobayashi, and Haruo Ya-maguchi. </author> <title> Customer control of network management from the service provider's perspective. </title> <journal> IEEE Communications Magazine, </journal> <pages> pages 35-40, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Work to date in network management has concentrated on effectively managing a single network. To our knowledge, little has been done to address the problem of coordinated network management across administrative domains, although the need for such a global coordination system has long been recognized <ref> [1, 2, 3, 4] </ref>. A recent study of routing instability [5] found that in about 10% of the problems, all parties questioned pointed to another party as the cause. Such problems strongly underscore the need for inter-domain coordination. The true cause of a problem may be distant from its effect.
Reference: [3] <author> Kraig R. Meyer and Dale S. Johnson. </author> <title> Experience in network management: The Merit network operations center. In Integrated Network Management, II. </title> <booktitle> IFIP TC6/WG6.6, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Work to date in network management has concentrated on effectively managing a single network. To our knowledge, little has been done to address the problem of coordinated network management across administrative domains, although the need for such a global coordination system has long been recognized <ref> [1, 2, 3, 4] </ref>. A recent study of routing instability [5] found that in about 10% of the problems, all parties questioned pointed to another party as the cause. Such problems strongly underscore the need for inter-domain coordination. The true cause of a problem may be distant from its effect.
Reference: [4] <author> Alan Hannan. Inter-provider outage notification. NANOG, </author> <month> May </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Work to date in network management has concentrated on effectively managing a single network. To our knowledge, little has been done to address the problem of coordinated network management across administrative domains, although the need for such a global coordination system has long been recognized <ref> [1, 2, 3, 4] </ref>. A recent study of routing instability [5] found that in about 10% of the problems, all parties questioned pointed to another party as the cause. Such problems strongly underscore the need for inter-domain coordination. The true cause of a problem may be distant from its effect.
Reference: [5] <author> Craig Labovitz. </author> <title> Routing stability analysis. North American Network Operator's Group, </title> <month> October </month> <year> 1996. </year>
Reference-contexts: To our knowledge, little has been done to address the problem of coordinated network management across administrative domains, although the need for such a global coordination system has long been recognized [1, 2, 3, 4]. A recent study of routing instability <ref> [5] </ref> found that in about 10% of the problems, all parties questioned pointed to another party as the cause. Such problems strongly underscore the need for inter-domain coordination. The true cause of a problem may be distant from its effect. <p> Cycles in cause-effect graphs are particularly important. They may lead all administrations involved to conclude it is "somebody else's problem", as observed in the informal routing instability study <ref> [5] </ref>, resulting in no action taken at all.
Reference: [6] <author> Merit/ISI. Inter-provider notification. </author> <note> http://compute.merit.edu/ipn.html. </note>
Reference-contexts: We leave the problems of inter-administration negotiation of repairs, and of notifying organizations of scheduled future downtime to future work. Work such as IPN <ref> [6] </ref> addresses the issue of pre-notification, but pre-notification does not help coordinate troubleshooting, since past announcements may have been lost, ignored, forgotten, and may be inaccessible during the problem. Intra-administration management methods may not apply to the inter-administration case.
Reference: [7] <author> David D. Clark. </author> <title> The design philosophy of the DARPA Internet protocols. </title> <booktitle> Proc. of ACM SIGCOMM '88, </booktitle> <pages> pages 106-114, </pages> <year> 1988. </year>
Reference-contexts: A repair should be performed close to the source of the problem, to avoid reacting to each effect separately. Also, problems reported by untrusted sources must be confirmed before being acted upon. 2.2 Architectural Constraints Our architecture follows the Internet design philosophy described in <ref> [7] </ref>, which we summarize with the following set of constraints ranked in order of importance: high availability, allowing multiple services, networks, and centers of administration, cost-effectiveness, low-effort deployment, and accountability.
Reference: [8] <author> Marshall T. Rose. </author> <title> The Simple Book. </title> <publisher> Prentice Hall, </publisher> <address> 2nd edition, </address> <year> 1994. </year>
Reference-contexts: We paraphrase our foremost constraint (from Rose <ref> [8] </ref>) as follows: Constraint 1 (Reliability): When all else fails, troubleshooting must continue to function, if at all possible. This constraint implies that a global troubleshooting system must require as few other services as possible to be functional.
Reference: [9] <author> Zheng Wang. </author> <title> Model of network faults. In Integrated Network Management, I. </title> <booktitle> IFIP TC6/WG6.6, </booktitle> <month> April </month> <year> 1989. </year>
Reference-contexts: For example, a "TCP session" may be a class, while an instance of that class would be identified by a pair of IP addresses and port numbers. 3.1 Fault-propagation model Faults may propagate both vertically as well as horizontally through the network <ref> [9] </ref>. For the horizontal direction, we use the term downstream to denote the direction of data flow, and upstream to denote the reverse direction. In the vertical direction, up and down are defined with respect to the seven-layer protocol stack defined by the ISO [10].
Reference: [10] <author> ISO. </author> <title> Information processing systems open systems interconnection basic reference model part 4: Management framework, </title> <booktitle> 1989. </booktitle> <pages> ISO 7498-4. </pages>
Reference-contexts: For the horizontal direction, we use the term downstream to denote the direction of data flow, and upstream to denote the reverse direction. In the vertical direction, up and down are defined with respect to the seven-layer protocol stack defined by the ISO <ref> [10] </ref>. Our method uses resource dependency graphs, in which each node represents an object, and directed edges denote dependencies. The arrows show the direction of a demand for the resources of one object by another. <p> This definition allows us to coordinate information relating to both fault management and performance management, as defined by the ISO <ref> [10] </ref>. With the above definitions, we are ready to analyze fault propagation in more detail. We begin with the following observations: Observation 1 High utilization propagates in the direction of resource dependencies. Any object which is highly utilized may consequently impose higher demands on those objects on which it depends.
Reference: [11] <author> German Goldszmidt and Yechiam Yemini. </author> <title> Evaluating management decisions via delegation. In Integrated Network Management, </title> <booktitle> III. IFIP TC6/WG6.6, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: For example, a link's capacity might be measured in Mbps, and a file system's capacity might be measured in Gbytes. Let an object's utilization denote the total amount of its resources in use. We adopt the concept of a "health function" from <ref> [11] </ref>. We let an object's health be a measure of its performance and its ability to adequately meet imposed demands. Low health is thus an indication of degraded performance. We will use the term problem to denote an object experiencing low health.
Reference: [12] <author> Willis Stinson and Shaygan Kheradpir. </author> <title> A state-based approach to real-time telecommunications network management. </title> <booktitle> In NOMS, </booktitle> <year> 1992. </year>
Reference-contexts: Observation 3 High utilization can cause low health, as utilization approaches the object's capacity. Low health may arise from soft failures (congestion) or hard failures (hardware or software faults). 3.2 Cause-effect graphs Previous studies (e.g., <ref> [12] </ref>) have typically only looked at one direction of fault propagation (i.e., "up"). We introduce cause-effect graphs as a more comprehensive model for representing fault propagation. Each node in such a graph represents a problem, and directed edges lead from effects to causes.
Reference: [13] <author> A. Guttman. R-trees: </author> <title> A dynamic index structure for spatial searching. </title> <booktitle> In Proceedings of the 1984 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 47-57, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: This problem is analogous to that of performing a point query in a spatial database to get a list of regions covering the given point. Traditional spatial database techniques such as R-trees <ref> [13] </ref> are not directly applicable, however, since scalability requires that the database of regions be physically distributed. In addition, it doesn't matter whether a region is matched if the associated expert isn't reachable. Thus, there are fundamental differences imposed by our constraints which make traditional approaches less applicable.
Reference: [14] <author> Paul Mockapetris. </author> <title> Domain names concepts and facilities, </title> <month> November </month> <year> 1987. </year> <month> RFC-1034. </month>
Reference-contexts: Maintain low bandwidth and memory overhead (thus trying not to exacerbate congestion problems, and interfering as little as possible with other objects). The first constraint suggests that a hierarchy of servers corresponding to a hierarchy in the namespace (as is used by DNS <ref> [14] </ref>, X.500 [15], etc) will not work, since we must have successful queries even when we are partitioned from a large part of the network. Replicating such servers everywhere will not keep the bandwidth overhead low.
Reference: [15] <author> Gerald Neufeld. </author> <title> Descriptive names in X.500. </title> <booktitle> In Proceedings of the ACM SIGCOMM, </booktitle> <pages> pages 64-70, </pages> <year> 1989. </year>
Reference-contexts: Maintain low bandwidth and memory overhead (thus trying not to exacerbate congestion problems, and interfering as little as possible with other objects). The first constraint suggests that a hierarchy of servers corresponding to a hierarchy in the namespace (as is used by DNS [14], X.500 <ref> [15] </ref>, etc) will not work, since we must have successful queries even when we are partitioned from a large part of the network. Replicating such servers everywhere will not keep the bandwidth overhead low. We also want to avoid mandating a hierarchical namespace to preserve domain autonomy and class independence.
Reference: [16] <author> Larry L. Peterson. </author> <title> The profile naming service. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(4) </volume> <pages> 341-364, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: On the other hand, we desire some structure to the servers so that expert location can provide higher availability, and be easily adapted to changing conditions without manual reconfiguration. Many existing attribute-based naming schemes (e.g., <ref> [16] </ref>) provide no structure to servers and hence rely on manual configuration. The solution we adopt is as follows. Expert location servers (ELS's) are organized into a hierarchy according to their location.
Reference: [17] <author> D. Thaler and C.V. Ravishankar. </author> <title> Distributed top-down hierarchy construction. </title> <note> Submitted to IEEE IN-FOCOM'98. </note>
Reference-contexts: Expert location servers (ELS's) are organized into a hierarchy according to their location. Informally, each ELS is responsible for knowing the namespace regions (areas of expertise) in the subtree rooted at itself. To avoid manual configuration, such a hierarchy may be formed by a self-configuring process such as TDH <ref> [17] </ref>. Experts form the actual leaves of the tree so formed, with each expert's parent being the closest expert location server. We will refer to a server whose children are experts (as opposed to other servers) as a leaf server.
Reference: [18] <author> D. Thaler and C.V. Ravishankar. </author> <title> Using name-based mappings to increase hit rates. </title> <journal> ACM/IEEE Transactions on Networking, </journal> <note> to appear. </note>
Reference-contexts: Prefer experts which match more of the optional attributes. If a region's description does not specify an optional attribute contained in the request, it is not considered to match when counting matched attributes. 5. Finally, to break ties, we use the Highest Random Weight (HRW) algorithm described in <ref> [18] </ref>. If two requests for the same object retrieve the same set S of servers, HRW generates the same ordering of servers in S for both requests.
Reference: [19] <author> D. Eastlake and C. Kaufman. </author> <title> Domain name system security extensions, </title> <month> January </month> <year> 1997. RFC-2065. </year>
Reference-contexts: To ensure integrity of capability advertisements and authentication of their origin, we adopt the current model recommended by the IETF for use with nameservice-like applications, which is known as DNSsec <ref> [19] </ref>. Briefly, a public/private key pair is associated with each domain, and all capabilities are signed with a domain key. To reliably learn the public key of a domain, the key itself must be signed. <p> A resolver must therefore be configured with at least the public key of one domain that it can use to authenticate signatures. It can then securely read the public keys of other domains if the intervening domains in the ELS tree are secure and their signed keys accessible. See <ref> [19, 20] </ref> for a more detailed discussion of the security model and associated concerns. A second security issue is denial-of-service attacks by observers reporting non-existent problems. Such attacks can be combatted in GDT by deferring tests and repairs once such an attack is suspected.
Reference: [20] <author> D. </author> <title> Eastlake. Secure domain name system dynamic update, </title> <month> April </month> <year> 1997. </year> <month> RFC-2137. </month>
Reference-contexts: A resolver must therefore be configured with at least the public key of one domain that it can use to authenticate signatures. It can then securely read the public keys of other domains if the intervening domains in the ELS tree are secure and their signed keys accessible. See <ref> [19, 20] </ref> for a more detailed discussion of the security model and associated concerns. A second security issue is denial-of-service attacks by observers reporting non-existent problems. Such attacks can be combatted in GDT by deferring tests and repairs once such an attack is suspected.
Reference: [21] <author> D. Thaler. Globally-distributed troubleshooting (GDT): </author> <title> Protocol specification. </title> <booktitle> Work in progress, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: If the client never refreshes the deferred state (see Section 4.3), the tests need not be performed. 4.3 Protocol Overview In this section we give a brief overview of the GDT protocol. A detailed specification can be found elsewhere <ref> [21] </ref>. Any entity may report a problem, whether the entity is a client perceiving a problem, or an expert hypothesizing about potential causes of a known problem. To report a problem, an ordered list of experts is first obtained using the method outlined in Section 4.1.
Reference: [22] <institution> Lawrence Berkeley National Labs. ns software. </institution> <note> http://www-nrg.ee.lbl.gov/ns/. </note>
Reference-contexts: down to the same problem, then a cycle must exist, and the cycle is broken by treating its cause as indeterminate, forcing the problem to be treated as a root cause. 4.5 Performance We simulated the performance of GDT by implementing clients and experts using "ns", the LBNL Network Simulator <ref> [22] </ref>. Because of space constraints, we could not include these results here, but they are available in an extended version of this paper [23].
Reference: [23] <author> David Thaler and Chinya V. Ravishankar. </author> <title> An architecture for inter-domain troubleshooting (extended version). </title> <type> Technical Report CSE-TR-344-97, </type> <institution> University of Michigan, </institution> <month> July </month> <year> 1997. </year>
Reference-contexts: Because of space constraints, we could not include these results here, but they are available in an extended version of this paper <ref> [23] </ref>. Briefly, we simulated the performance of GDT in terms of troubleshooting time, number of messages, and amount of state required, while varying the number of clients, the network size, and the degree of message loss.
References-found: 23

