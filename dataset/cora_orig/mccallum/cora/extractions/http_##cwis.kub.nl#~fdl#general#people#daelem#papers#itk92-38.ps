URL: http://cwis.kub.nl/~fdl/general/people/daelem/papers/itk92-38.ps
Refering-URL: http://ilk.kub.nl/papers.html
Root-URL: 
Email: walter@kub.nl  antalb@kub.nl  
Phone: 5000  
Title: GENERALIZATION PERFORMANCE OF BACKPROPAGATION LEARNING ON A SYLLABIFICATION TASK  
Author: Walter Daelemans Antal van den Bosch 
Address: P.O.Box 90153  The Netherlands  
Affiliation: ITK Tilburg University,  LE Tilburg  
Abstract: We investigated the generalization capabilities of backpropagation learning in feed-forward and recurrent feed-forward connectionist networks on the assignment of syllable boundaries to orthographic representations in Dutch (hyphenation). This is a difficult task because phonological and morphological constraints interact, leading to ambiguity in the input patterns. We compared the results to different symbolic pattern matching approaches, and to an exemplar-based generalization scheme, related to a k-nearest neighbour approach, but using a similarity metric weighed by the relative information entropy of positions in the training patterns. Our results indicate that the generalization performance of backpropagation learning for this task is not better than that of the best symbolic pattern matching approaches, and of exemplar-based generalization. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bosch, A. van den and W. Daelemans. </author> <title> Linguistic Pattern Matching Capabilities of Connectionist Networks. In: Daelemans and Powers (eds.) Background and Experiments in Machine Learning of Natural Language. </title> <booktitle> Proceedings First SHOE Workshop. Tilburg: ITK, </booktitle> <pages> 183-196, </pages> <year> 1992. </year>
Reference-contexts: It is consistent with the maximal onset principle. As expected from the analysis of training and test set, window size 7 (3-1-3) produced optimal results. The average phonological syllable length in Dutch is 2.8 phonemes. In a different set of simulations <ref> (van den Bosch and Daelemans, 1992) </ref> in which we tested syllabification of phoneme representations instead of orthographic representations, we even found that window size 5 (2-1-2) turned out to produce better results than window size 7.
Reference: <author> Daelemans, W. GRAFON-D: </author> <title> A Grapheme-to-phoneme Conversion System for Dutch. </title> <type> AI Memo 88-5, </type> <address> AI-LAB Brussels, </address> <year> 1988. </year>
Reference-contexts: It is instructive in this respect to compare the architecture of a typical symbolic system for grapheme-to-phoneme conversion, which "learns by brain surgery" <ref> (Figure 1, from Daelemans, 1988) </ref> to a connectionist solution for the same problem, such as the one by Sejnowski and Rosenberg (1987, Figure 2). the GRAFON grapheme to phoneme con version system. The latter approach can be adapted to different languages simply by changing the training set.
Reference: <author> Daelemans, W. </author> <title> `Automatic Hyphenation: Linguistics versus Engineering.' </title> <editor> In: F.J. Heyvaert and F. Steurs (Eds.), </editor> <title> Worlds behind Words, </title> <publisher> Leuven University Press, </publisher> <pages> 347-364, </pages> <year> 1989. </year>
Reference-contexts: There are also a number of language-dependent spelling hyphenation conventions that override syllabification rules. Simplifying matters slightly <ref> (see Daelemans, 1989 for a full account) </ref>, we can say that the process is guided by a phonotactic maximal onset principle, a principle which states that between two vowels, as many consonants belong to the second syllable as can be pronounced together, and a sonority principle, which states that in general, <p> Because of this, the default phonological principles fail in many cases (we calculated this number to be on average 6 % of word forms for Dutch newspaper text). By incorporating a morphological parser and lexicon, a phonologically guided syllabification algorithm <ref> (as described in Daelemans, 1989) </ref> is able to find the correct syllable boundaries in the complete vocabulary of Dutch (i.e. all existing and all possible words, excluding some loan words and semantically ambiguous word forms like kwarts-lagen (quartz layers) versus kwart-slagen (quarter turns)). <p> to the conclusion that Fritzke and Nasahl would have had a hyphenation error percentage of about 10%. 3.7 Connectionist versus Symbolic Pattern Matching As a final comparison of the performance of connectionist networks to symbolic pattern matching systems, we selected a Dutch text 6 and compared the performance of CHYP <ref> (Daelemans, 1989) </ref>, an approach based on the table look-up method of Weijters (1991) 7 , an (as yet) undocumented algorithm PatHyph (Vosse, p.c.), and our best spelling hyphenation network. The results are summarized in Table 5.
Reference: <author> Durieux, G. </author> <title> Analogical Modelling of Main Stress Assignment in Dutch Simplex Words. </title> <editor> In: Daele-mans and Powers (eds.) </editor> <booktitle> Background and Experiments in Machine Learning of Natural Language. Proceedings First SHOE Workshop. Tilburg: ITK, </booktitle> <pages> 197-204, </pages> <year> 1992. </year>
Reference: <author> Elman, J. </author> <title> Finding Structure in Time. </title> <type> CRL Technical Report 8801, </type> <year> 1988. </year>
Reference: <author> Fritzke, B. and C. Nasahl. </author> <title> A Neural Network that Learns to do Hyphenation. </title> <editor> In: T. Kohonen, </editor> <publisher> K. </publisher>
Reference: <editor> Makisara, O. Simula and J. Kangas (Eds.) </editor> <booktitle> Artificial Neural Networks. </booktitle> <publisher> Elsevier Science Publishers, </publisher> <pages> 1375-1378, </pages> <year> 1991. </year>
Reference: <author> Jordan, M. I. </author> <title> Attractor dynamics and parallelism in a connectionist sequential machine. </title> <booktitle> Proceedings of the Eighth Annual Meeting of the Cognitive Science Society Hillsdale, </booktitle> <address> NJ, </address> <year> 1986. </year>
Reference-contexts: report 96.8% cor-rect generalization on connectionist hyphenation for German (which is similar to Dutch as regards syllabification) with a three-layer feed-forward architecture, a window of 8 letters, a hidden layer size of 80, random encoding of graphemes, and one recurrent (feedback) link from output unit to an extra input unit <ref> (the approach of Jordan, 1986) </ref>. In contradiction with our own results, they noted a slightly better result than a comparable architecture without a feedback connection. The network was trained on 1,000 words and tested on 200 words not present in the training set.
Reference: <author> Lachter, J. and T. Bever. </author> <title> `The relationship between linguistic structure and associative theories of language learning.' </title> <editor> In Pinker and Mehler (eds.) </editor> <title> Connections and Symbols. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: The experiment has been replicated with backpropagation learning in a three-layer network by Plunkett and Marchman (1989). To avoid the legitimate criticism that these approaches only work because of the linguistic knowledge that is implicit in the training data <ref> (Lachter and Bever, 1988) </ref> or don't work because of the wrong linguistic knowledge implicit in the training data (Pinker and Prince, 1988), we performed most of our simulations with random encodings of segments.
Reference: <author> Pinker, S. and A. Prince. </author> <title> `On Language and Connectionism: Analysis of a PDP Model of Language Acquisition.' </title> <editor> In Pinker and Mehler (eds.) </editor> <title> Connections and Symbols. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: To avoid the legitimate criticism that these approaches only work because of the linguistic knowledge that is implicit in the training data (Lachter and Bever, 1988) or don't work because of the wrong linguistic knowledge implicit in the training data <ref> (Pinker and Prince, 1988) </ref>, we performed most of our simulations with random encodings of segments. In the landmark experiments by Sejnowski and Rosenberg (1987) on text-to-speech transformation with NETtalk, they also included syllable boundaries (and stress) in the training material.
Reference: <author> Plunkett, K. and V. Marchman. </author> <title> `Pattern Association in a Back Propagation Network: Implications for Child Language Acquisition.' San Diego, </title> <type> CRL, Technical Report 8902, </type> <year> 1989. </year>
Reference: <author> Quinlan, J. R. </author> <title> Induction of Decision Trees. </title> <booktitle> Machine Learning 1, </booktitle> <pages> 81-106, </pages> <year> 1986. </year>
Reference: <author> Riesbeck, C. K. and R. S. Schank. </author> <title> Inside Case-based Reasoning. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum Assoc., </publisher> <year> 1989. </year>
Reference-contexts: Weiss and Ku-likovsky, 1991), and shares with Case-Based Reasoning <ref> (CBR, e.g. Riesbeck and Schank, 1989) </ref> and Memory-based Reasoning (MBR, Stanfill and Waltz, 1986) the hypothesis that the foundation of intelligence is reasoning on the basis of stored memories rather than the application of (tacit) rules.
Reference: <author> Rumelhart, D. E. and J. McClelland. </author> <title> `On learning the past tense of English verbs.' </title> <editor> In D.E. Rumel-hart and J.L. </editor> <booktitle> McCleland and the PDP Research Group, Parallel Distributed Processing: Explorations in the Microstructure of cognition. Volume 2. </booktitle> <address> Cambridge, MA: </address> <publisher> Bradford Books. </publisher>
Reference: <author> Rumelhart, D.E., G.E. Hinton, and R.J. Williams. </author> <title> Learning Internal Representations by Error Propagation. </title> <editor> In: Rumelhart and McClelland (Eds.) </editor> <booktitle> Parallel Distributed Processing Volume 1, </booktitle> <publisher> MIT Press, </publisher> <pages> 318-362, </pages> <year> 1986. </year>
Reference-contexts: This paper is concerned with a well-defined instance of linguistic pattern matching problems: the assignment of syllable boundaries to orthographic (spelling) representations of word forms in Dutch. We wanted to investigate whether the currently most popular connectionist learning technique, backpropagation of errors <ref> (Rumelhart et al, 1986) </ref> on (recurrent) feed-forward networks, is powerful enough to abstract the regularities governing the segmentation of strings of spelling symbols into syllable representations.
Reference: <author> Sejnowski, T.J. and C.R. Rosenberg. </author> <title> Parallel Networks that Learn to Pronounce English Text. </title> <booktitle> Complex Systems 1, </booktitle> <pages> 145-168, </pages> <year> 1987 </year> <month> Skousen, </month> <title> R. Analogical Modeling of Language. </title> <publisher> Dor-drecht: Kluwer, </publisher> <year> 1989. </year>
Reference: <author> Stanfill, C. and D. L. Waltz. </author> <title> Toward Memory-based Reasoning. </title> <journal> Communications of the ACM, </journal> <volume> Vol. 29, 12, </volume> <year> 1986. </year>
Reference-contexts: Weiss and Ku-likovsky, 1991), and shares with Case-Based Reasoning (CBR, e.g. Riesbeck and Schank, 1989) and Memory-based Reasoning <ref> (MBR, Stanfill and Waltz, 1986) </ref> the hypothesis that the foundation of intelligence is reasoning on the basis of stored memories rather than the application of (tacit) rules.
Reference: <editor> Weijters, A. and G. Hoppenbrouwers. `Net-Spraak: een neuraal netwerk voor grafeem-foneem-omzetting.' </editor> <booktitle> Tabu 20:1, </booktitle> <pages> 1-25, </pages> <year> 1990 </year> <month> Weijters, </month> <title> A. `A simple look-up procedure superior to NETtalk?' In: </title> <editor> T. Kohonen, K. Makisara, </editor> <address> O. </address>
Reference: <editor> Simula and J. Kangas (Eds.) </editor> <booktitle> Artificial Neural Networks. </booktitle> <publisher> Elsevier Science Publishers, </publisher> <year> 1991. </year>
Reference: <author> Weiss, S. M. and C. A. Kulikowsky. </author> <title> Computer Systems that Learn. </title> <address> San Mateo: </address> <publisher> Morgan Kauf-mann, </publisher> <year> 1991. </year>
References-found: 20

