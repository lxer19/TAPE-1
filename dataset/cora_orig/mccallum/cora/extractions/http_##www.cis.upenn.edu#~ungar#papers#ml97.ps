URL: http://www.cis.upenn.edu/~ungar/papers/ml97.ps
Refering-URL: http://www.cis.upenn.edu/~ungar/papers.html
Root-URL: 
Email: Email: daes@linc.cis.upenn.edu  
Phone: Phone: 1-215-573-6285 Fax: 1-215-573-9247  
Title: Characterizing the generalization performance of model selection strategies  
Author: Dale E. Schuurmans Lyle H. Ungar Dean P. Foster 
Keyword: inductive learning, model selection, overfitting, bias/variance decomposition  
Note: Also: NEC Research Institute, Princeton, NJ Mailing address: Dale Schuurmans, Institute for Research in  
Address: Philadelphia, PA 19104  3401 Walnut Street, Suite 400A, Philadelphia, PA 19104-6228.  
Affiliation: Institute for Research in Cognitive Science  Department of Computer and Information Science  Department of Statistics University of Pennsylvania  Cognitive Science, University of Pennsylvania,  
Abstract: We investigate the structure of model selection problems via the bias/variance decomposition. In particular, we characterize the essential structure of a model selection task by the bias and variance profiles it generates over the sequence of hypothesis classes. This leads to a new understanding of complexity-penalization methods: First, the penalty terms in effect postulate a particular profile for the variances as a function of model complexity if the postulated and true profiles do not match, then systematic under-fitting or over-fitting results, depending on whether the penalty terms are too large or too small. Second, it is usually best to penalize according to the true variances of the task, and therefore no fixed penalization strategy is optimal across all problems. We then use this bias/variance characterization to identify the notion of easy and hard model selection problems. In particular, we show that if the variance profile grows too rapidly in relation to the biases then standard model selection techniques become prone to significant errors. This can happen for example in regression when the independent variables are drawn from wide-tailed distributions. Finally, we discuss a new model selection strategy that dramatically outperforms standard complexity-penalization and hold-out methods on these hard tasks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. Cherkassky, F. Mulier, and V. Vapnik. </author> <title> Comparison of VC-method with classical methods for model selection. </title> <type> Preprint, </type> <year> 1996. </year>
Reference-contexts: Following [9] we can write the adjusted error estimate of this strategy as d err GCV (h fl i ; S) + (1 r) 2 err (h fl The other penalization strategy we consider is Vapnik's Structural Risk Minimiza tion procedure SRM [13], which following <ref> [1] </ref> can be formulated d err SRM (h fl i ; S) + ~r 1 ~r + i ; S); (4) where ~r = r (1 + ln 1=r) + (ln t)=2t. <p> In 2 For most natural orderings H 1 H 2 , the complexity level i corresponds to the number of free parameters used in the definition of function class H i . Therefore, intuitively r gives the number of distinct parameters being estimated per training example <ref> [1, 13] </ref>. 5 fact, err (h fl i ; S) is often a much better estimate of err ( h fl i ) than it is of err (h fl i )! 3 Although not often explicitly made, this elementary observation leads to an interesting interpretation of complexity-penalization strategies: if the <p> This distinction is important because difficult model selection problems arise in fairly natural conditions; for example, linear regression on broadly-distributed variables (as we demonstrated), or polynomial curve-fitting <ref> [1, 13] </ref> to name just a few. These observations lead to specific recommendations, the first being that one needs to incorporate as much prior knowledge as possible about the shape of the variance profile in order to choose a model selection policy that works effectively while avoiding disastrous mistakes.
Reference: [2] <author> P. Craven and G. Wahba. </author> <title> Smoothing noisy data with spline functions. </title> <journal> Numer. Math., </journal> <volume> 31:377403, </volume> <year> 1979. </year>
Reference-contexts: There are many variants of this basic approach, including generalized cross validation <ref> [2] </ref>, minimum description length principle [10], structural risk minimization [12, 13], Bayesian maximum a posteriori selection, and regularization [8]. These strategies differ in the specific complexity values they assign and the particular tradeoff function they optimize, but the basic idea is still the same. <p> To describe these strategies, let r = i=t be the number of complexity levels being considered per training example. 2 The first penalization strategy we consider is Generalized Cross Validation GCV <ref> [2] </ref>.
Reference: [3] <author> B. </author> <title> Efron. </title> <journal> Computers and the theory of statistics. SIAM Review, </journal> <volume> 21:46080, </volume> <year> 1979. </year>
Reference-contexts: Again, there are many variants to this basic strategyhaving to do with repeating the pseudo-train pseudo-test split many times and averaging the results to choose the final hypothesis class; e.g., 10-fold cross validation, leave-one-out testing, bootstrapping, etc. <ref> [3, 14] </ref>. The abundance of model selection strategies and different approaches to the problem raises the question of which techniques are best and when. We attempt to answer this question by appealing to the standard bias/variance decomposition of generalization error [4].
Reference: [4] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <booktitle> Neural Comp., </booktitle> <address> 4:158, </address> <year> 1992. </year>
Reference-contexts: The abundance of model selection strategies and different approaches to the problem raises the question of which techniques are best and when. We attempt to answer this question by appealing to the standard bias/variance decomposition of generalization error <ref> [4] </ref>. <p> Of course, the key to making this work is to choose the right hypothesis class H. One way to assess H's suitability is to consider the bias/variance decomposition of the resulting prediction error. Consider a fixed example distribution P XY and training sample size t. Following Geman et al. <ref> [4] </ref> we can decompose the expected error of the empirically optimal hypothesis h fl into bias and variance components as follows: First note that each training set S determines some hypothesis h fl with minimum error on S. <p> ; h fl ); (1) where h fl is the mean hypothesis of the distribution P H , and err ( h fl ; h fl ) 4 R h fl (x)) 2 dP X is the average discrepancy between the empirically optimal hypothesis h fl and h fl (cf. <ref> [4] </ref>). Thus, we decompose the expected hypothesis error into two components: the true error of the mean hypothesis (bias), and the average discrepancy between a random data generated hypothesis and the mean hypothesis (variance). 1 Now consider the model selection task.
Reference: [5] <author> M. Kearns, Y. Mansour, A. Ng, and D. Ron. </author> <title> An experimental and theoretical comparison of model selection methods. </title> <booktitle> In COLT-95, </booktitle> <year> 1995. </year>
Reference-contexts: On the other hand, if the variance profile grows explosively relative to the bias profile, then disaster results for any penalization strategy that does not use the exact variance profile for the task. 8 Note that this is similar to an observation made by Kearns et al. <ref> [5] </ref> in the context of learning classifications. <p> Among the many avenues for future work, we are currently pursuing the same style of bias/variance analysis to classification (as opposed to regression) problems <ref> [5] </ref>. We do not expect the same issues to be important here, since there is no potential for catastrophe in this case (unless one is interested in small relative rather than absolute approximation). The real issue is one of over versus under-penalization, as pointed out by Kearns et al. [5]. <p> problems <ref> [5] </ref>. We do not expect the same issues to be important here, since there is no potential for catastrophe in this case (unless one is interested in small relative rather than absolute approximation). The real issue is one of over versus under-penalization, as pointed out by Kearns et al. [5]. Note however that, for classification, the decomposition of prediction error into additive bias and variance components is not so obvious [7]. We are also pursing more rigorous theoretical analyses of the various questions raised in this paper. 13
Reference: [6] <author> R. Kohavi. </author> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> In IJCAI-95, </booktitle> <year> 1995. </year>
Reference-contexts: increasing these variances, in counterbalance. 12 This problem was defined by setting = 1, fi i = 10 i , as described in Footnote 4. 9 Alternative hold-out methods An obvious idea in these situations is to consider alternative hold-outbased methods, like 10-fold cross-validation (10CV) or some other resampling procedure <ref> [6, 14] </ref>. However, it turns out that these strategies are prone to the very same mistakes suffered by penalty-based methods, as Table 4 clearly demonstrates for 10CV.
Reference: [7] <author> R. Kohavi and D. Wolpert. </author> <title> Bias plus variance decomposition for zero-one loss functions. </title> <booktitle> In ML-96, </booktitle> <pages> pages 275283, </pages> <year> 1996. </year>
Reference-contexts: The real issue is one of over versus under-penalization, as pointed out by Kearns et al. [5]. Note however that, for classification, the decomposition of prediction error into additive bias and variance components is not so obvious <ref> [7] </ref>. We are also pursing more rigorous theoretical analyses of the various questions raised in this paper. 13
Reference: [8] <author> J. Moody. </author> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. </title> <booktitle> In NIPS-4, </booktitle> <year> 1992. </year>
Reference-contexts: There are many variants of this basic approach, including generalized cross validation [2], minimum description length principle [10], structural risk minimization [12, 13], Bayesian maximum a posteriori selection, and regularization <ref> [8] </ref>. These strategies differ in the specific complexity values they assign and the particular tradeoff function they optimize, but the basic idea is still the same. The other most common strategy is hold-out testing.
Reference: [9] <author> J. Moody and J. Utans. </author> <title> Principled architecture selection for neural networks: Application to corporate bond rating prediction. </title> <booktitle> In NIPS-4, </booktitle> <year> 1992. </year>
Reference-contexts: To describe these strategies, let r = i=t be the number of complexity levels being considered per training example. 2 The first penalization strategy we consider is Generalized Cross Validation GCV [2]. Following <ref> [9] </ref> we can write the adjusted error estimate of this strategy as d err GCV (h fl i ; S) + (1 r) 2 err (h fl The other penalization strategy we consider is Vapnik's Structural Risk Minimiza tion procedure SRM [13], which following [1] can be formulated d err SRM
Reference: [10] <author> J. Rissanen. </author> <title> Stochastic complexity and modeling. </title> <journal> Ann. Statist., </journal> <volume> 14:1080100, </volume> <year> 1986. </year>
Reference-contexts: There are many variants of this basic approach, including generalized cross validation [2], minimum description length principle <ref> [10] </ref>, structural risk minimization [12, 13], Bayesian maximum a posteriori selection, and regularization [8]. These strategies differ in the specific complexity values they assign and the particular tradeoff function they optimize, but the basic idea is still the same. The other most common strategy is hold-out testing.
Reference: [11] <author> C Schaffer. </author> <title> Overfitting avoidance as bias. Mach. Learn., </title> <address> 10(2):15378, </address> <year> 1993. </year>
Reference-contexts: This is a somewhat surprising result, but it follows from the fact that VAR does not pay explicit attention to the inter-hypothesis distances, and can therefore be fooled from time to time. Of course, we do not expect a free lunch <ref> [11] </ref> and there are certainly model selection problems where ADJ does not dominate (Table 3). However, the claim is that one should be able to exploit additional information about the task (here knowledge of P X ) to obtain significant improvements across a wide range of problem types and conditions.
Reference: [12] <author> V. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: There are many variants of this basic approach, including generalized cross validation [2], minimum description length principle [10], structural risk minimization <ref> [12, 13] </ref>, Bayesian maximum a posteriori selection, and regularization [8]. These strategies differ in the specific complexity values they assign and the particular tradeoff function they optimize, but the basic idea is still the same. The other most common strategy is hold-out testing. <p> This is in fact quite easy to prove using the uniform convergence results of Vapnik <ref> [12, 13] </ref>. The interesting part of this observation is that err (h fl ; S) evidently con verges much faster to err ( h fl ) than to err (h fl ).
Reference: [13] <author> V. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: There are many variants of this basic approach, including generalized cross validation [2], minimum description length principle [10], structural risk minimization <ref> [12, 13] </ref>, Bayesian maximum a posteriori selection, and regularization [8]. These strategies differ in the specific complexity values they assign and the particular tradeoff function they optimize, but the basic idea is still the same. The other most common strategy is hold-out testing. <p> Following [9] we can write the adjusted error estimate of this strategy as d err GCV (h fl i ; S) + (1 r) 2 err (h fl The other penalization strategy we consider is Vapnik's Structural Risk Minimiza tion procedure SRM <ref> [13] </ref>, which following [1] can be formulated d err SRM (h fl i ; S) + ~r 1 ~r + i ; S); (4) where ~r = r (1 + ln 1=r) + (ln t)=2t. <p> In 2 For most natural orderings H 1 H 2 , the complexity level i corresponds to the number of free parameters used in the definition of function class H i . Therefore, intuitively r gives the number of distinct parameters being estimated per training example <ref> [1, 13] </ref>. 5 fact, err (h fl i ; S) is often a much better estimate of err ( h fl i ) than it is of err (h fl i )! 3 Although not often explicitly made, this elementary observation leads to an interesting interpretation of complexity-penalization strategies: if the <p> This is in fact quite easy to prove using the uniform convergence results of Vapnik <ref> [12, 13] </ref>. The interesting part of this observation is that err (h fl ; S) evidently con verges much faster to err ( h fl ) than to err (h fl ). <p> This distinction is important because difficult model selection problems arise in fairly natural conditions; for example, linear regression on broadly-distributed variables (as we demonstrated), or polynomial curve-fitting <ref> [1, 13] </ref> to name just a few. These observations lead to specific recommendations, the first being that one needs to incorporate as much prior knowledge as possible about the shape of the variance profile in order to choose a model selection policy that works effectively while avoiding disastrous mistakes.

References-found: 13

