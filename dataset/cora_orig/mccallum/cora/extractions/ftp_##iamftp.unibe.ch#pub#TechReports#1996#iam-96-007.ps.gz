URL: ftp://iamftp.unibe.ch/pub/TechReports/1996/iam-96-007.ps.gz
Refering-URL: 
Root-URL: 
Title: On Functional Relation between Recognition Error and Class-Selective Reject  
Author: Thien M. HA 
Keyword: CR Categories and Subject Descriptors: I.5.0 [Pattern Recognition]: General; I.5.1 [Pattern Recognition]: Models; I.5.2 [Pattern Recognition]: Design Methodology; I.5.m [Pattern Recognition]: Decision. Key Words: classification, decision rule, Bayes rule, selective rejection, man-machine interface.  
Address: Neubruckstr. 10, CH-3012 Berne, Switzerland  
Affiliation: University of Berne Institut fur Informatik und Angewandte Mathematik  
Email: E-Mail: haminh@iam.unibe.ch  
Phone: Phone: +41 31 631 86 99 Fax.: +41 31 631 39 65  
Date: March 11, 1996  
Abstract: This report reviews various optimum decision rules for pattern recognition, namely, Bayes rule, Chow's rule (optimum error-reject tradeoff), and a recently proposed class-selective rejection rule. The latter provides an optimum tradeoff between the error rate and the average number of (selected) classes. A new general relation between the error rate and the average number of classes is presented. The error rate can directly be computed from the class-selective reject function, which in turn can be estimated from unlabelled patterns, by simply counting the rejects. Theoretical as well as practical implications are discussed and some future research directions are proposed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Chellappa, C.L. Wilson, and S. Sirohey, </author> <title> "Human and Machine Recognition of Faces: A Survey," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> Vol. 83, No. 5, </volume> <pages> pp. 705-740, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: In contrast, for applications like face identification, humans may not know or remember all (maybe a huge amount of) reference faces. In such an application, a rejection would require the operator to compare the rejected pattern with hundreds, if not thousands, of reference faces <ref> [1] </ref>. Therefore, a useful system should not make a simple rejection, but should provide a (preferably short) list of candidates or classes. For instance, the top-n ranking is such a mechanism. In this context, the error-reject tradeoff becomes error-(number-of-classes) tradeoff. <p> This is obvious for n fl = 1. For n fl &gt; 1, suppose that Q n fl (x) t, then 9k (= n fl 1) such that Q k+1 (=n fl ) t, k (= n fl 1) 1 ) k 2 <ref> [1; ::; N ] </ref>, and k (= n fl 1) &lt; n fl , which means that the optimum decision rule given by Eq. (20) had not been used (n fl is not the minimum value possible). * Case b: Q 1 (x) t: We have Q n fl (x) t <p> Finally, let us consider the range of t 2 [0; 1 2 ]. Since the decision rule involves the comparison between t and posterior probabilities, it makes sense only for t 2 <ref> [0; 1] </ref>. On the other hand, when t 1 2 , it can be easily seen that the rule is identical to the Bayes rule, i.e., choose the single best class.
Reference: [2] <author> C.K. Chow, </author> <title> "An Optimum Character Recognition System Using Decision Func tions," </title> <journal> Institute of Radio Engineers (IRE) Transactions on Electronic Computers, </journal> <volume> Vol. EC-6, No. 4, </volume> <pages> pp. 247-254, </pages> <month> December </month> <year> 1957. </year>
Reference-contexts: With a reject option, the system performance is characterised by the error-reject tradeoff <ref> [2] </ref>. From an application point of view, characterising the system performance by the error-reject tradeoff is appropriate for many tasks, such as those involving optical character recognition (OCR). <p> For instance, the top-n ranking is such a mechanism. In this context, the error-reject tradeoff becomes error-(number-of-classes) tradeoff. Although the optimum error-reject tradeoff has been known for a long time <ref> [2] </ref>, the optimum error-(number-of-classes) was discovered only recently [10]. Few theoretical results on these aspects are available [4, 13]. This report first gives an overview of optimum decision rules. A general relation between error rate and average number of classes is then presented. <p> p (x) is nonzero over the entire pattern space X, otherwise the region over which p (x) is zero is first deleted. 4 decision process are shown on the right side. 5 2.2 Chow's Rule The Bayes rule has also been modified by Chow to cope with a reject option <ref> [2, 3] </ref>. The idea is that when a pattern lies on or near a separation plane between two classes, the assignment to one or the other class is merely a guess.
Reference: [3] <author> C.K. Chow, </author> <title> "On Optimum Recognition Error and Reject Tradeoff," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. IT-16, No. 1, </volume> <pages> pp. 41-46, </pages> <month> January </month> <year> 1970. </year>
Reference-contexts: p (x) is nonzero over the entire pattern space X, otherwise the region over which p (x) is zero is first deleted. 4 decision process are shown on the right side. 5 2.2 Chow's Rule The Bayes rule has also been modified by Chow to cope with a reject option <ref> [2, 3] </ref>. The idea is that when a pattern lies on or near a separation plane between two classes, the assignment to one or the other class is merely a guess. <p> It turns out that it is possible to express the error rate directly as a function of the reject rate via the Stieltjes integral <ref> [3] </ref>. e (t ope ) = 0 where 'ope' stands for operating. (For an introduction to the Stieltjes integral, see [17].) The marvelous feature of the above equation is that it allows the computation of the error rate at any level t from r (t) solely and that the latter can <p> For the optimum class-selective rejection rule, the functional relation between error rate and average number of classes, Eq. (36), is established. It takes the same form as Chow's optimum error-reject tradeoff curve, Eq. (13). The optimum e n curve shares many properties with Chow's optimum error-reject, e r, curve <ref> [3] </ref>. Since the slope of the e n curve is t, the tradeoff, i.e., 12 problem. <p> The use of Eq. (36) in estimating the error rate without having recourse to the true labels of testing patterns should be further investigated. The study should take into account the observations made by Fukunaga and Kessel in [7] on the Stieltjes integral, Eq. (13), proposed by Chow <ref> [3] </ref>. See also [18, 11] for reviews of error estimation methods. Other interesting related papers are [19, 12, 6, 16, 15]. 7 Conclusion We have reviewed various optimum decision rules for pattern recognition, namely, Bayes rule, Chow's rule (optimum error-reject tradeoff), and a recently proposed class-selective rejection rule.
Reference: [4] <author> P.A. Devijver, </author> <title> "Error and Reject Tradeoff for Nearest Neighbor Decision Rules," </title> <editor> in G. Tacconi (Ed.) </editor> <booktitle> Aspects of Signal Processing,, Part 2, </booktitle> <address> D. </address> <publisher> Reidel Publishing Company, Dordrecht-Holland, </publisher> <pages> pp. 525-538, </pages> <year> 1977. </year>
Reference-contexts: For instance, the top-n ranking is such a mechanism. In this context, the error-reject tradeoff becomes error-(number-of-classes) tradeoff. Although the optimum error-reject tradeoff has been known for a long time [2], the optimum error-(number-of-classes) was discovered only recently [10]. Few theoretical results on these aspects are available <ref> [4, 13] </ref>. This report first gives an overview of optimum decision rules. A general relation between error rate and average number of classes is then presented. It will be shown that the error rate can be estimated directly from the empirical number of classes assigned to each unlabelled pattern.
Reference: [5] <author> R.O. Duda and P.E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: through the Bayes formula: P i (x) P (i=x) = p (x) 3 where p (x=i) is the i th class conditional probability density function (p.d.f.), i is the a priori probability of observing the i th class, P N p (x) = j=1 is the absolute probability density function <ref> [5, 8] </ref>. It follows immediately that the posterior probabilities sum up to 1, i.e., N X P i (x) = 1 (3) The connection between classification and decision is illustrated in Fig. 1, for a three-class problem. <p> In most practical applications, fP i (x); i = 1; ::; Ng are unknown but can be estimated from a set of labelled patterns, called training set. Many estimation methods exist, e.g. Parzen estimate, nearest neighbour, potential functions, and neural networks <ref> [5, 8, 14] </ref>.
Reference: [6] <author> G.M. </author> <title> Fitzmaurice and D.J. Hand, "A Comparison of Two Average Conditional Error Rate Estimators," </title> <journal> Pattern Recognition Letters, </journal> <volume> Vol. 6, </volume> <pages> pp. 221-224, </pages> <year> 1987. </year>
Reference-contexts: The study should take into account the observations made by Fukunaga and Kessel in [7] on the Stieltjes integral, Eq. (13), proposed by Chow [3]. See also [18, 11] for reviews of error estimation methods. Other interesting related papers are <ref> [19, 12, 6, 16, 15] </ref>. 7 Conclusion We have reviewed various optimum decision rules for pattern recognition, namely, Bayes rule, Chow's rule (optimum error-reject tradeoff), and a recently proposed class-selective rejection rule. The latter provides an optimum tradeoff between the error rate and the average number of classes.
Reference: [7] <author> K. Fukunaga and D.L. Kessel, </author> <title> "Application of Optimum Error-Reject Func tions," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. IT-18, No. </volume> ??, <pages> pp. 814-817, </pages> <month> November </month> <year> 1972. </year>
Reference-contexts: In other words, the error rate at any level can be estimated without knowing the true classes of the patterns. For a more detailed discussion, see also <ref> [7] </ref>. In particular, the Bayes error rate is given by e Bayes = e (t ope = 1 1 ) = N t dr (t) (14) 2.3 Optimum Class-Selective Rejection Rule Recently, an optimum class-selective rejection rule was proposed [10]. <p> The use of Eq. (36) in estimating the error rate without having recourse to the true labels of testing patterns should be further investigated. The study should take into account the observations made by Fukunaga and Kessel in <ref> [7] </ref> on the Stieltjes integral, Eq. (13), proposed by Chow [3]. See also [18, 11] for reviews of error estimation methods.
Reference: [8] <author> K. Fukunaga, </author> <title> Introduction to Statistical Pattern Recognition, second edition, </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: through the Bayes formula: P i (x) P (i=x) = p (x) 3 where p (x=i) is the i th class conditional probability density function (p.d.f.), i is the a priori probability of observing the i th class, P N p (x) = j=1 is the absolute probability density function <ref> [5, 8] </ref>. It follows immediately that the posterior probabilities sum up to 1, i.e., N X P i (x) = 1 (3) The connection between classification and decision is illustrated in Fig. 1, for a three-class problem. <p> In most practical applications, fP i (x); i = 1; ::; Ng are unknown but can be estimated from a set of labelled patterns, called training set. Many estimation methods exist, e.g. Parzen estimate, nearest neighbour, potential functions, and neural networks <ref> [5, 8, 14] </ref>.
Reference: [9] <author> Thien M. Ha, D. Niggeler, H. Bunke, and J. Clarinval, </author> <title> "Giro Form Reading Machine," </title> <journal> Optical Engineering, </journal> <volume> Vol. 34, No. 8, </volume> <pages> pp. 2277-2288, </pages> <year> 1995. </year>
Reference-contexts: For such a task, the system performance is mainly characterised by its error rate. However, because of noise and other uncertain factors inherent in any real system, the error rate can be excessive for some applications, such as bank check reading <ref> [9] </ref>. Recognition with a reject option provides a means to reduce the error rate through a rejection mechanism, i.e., withhold making a decision if the confidence is not high enough and direct the rejected pattern to an exceptional handling, such as manual inspection.
Reference: [10] <author> Thien M. Ha, </author> <title> "An Optimum Decision Rule for Pattern Recognition," </title> <type> Technical Report IAM-95-009, </type> <institution> Institute of Computer Science and Applied Mathematics, University of Berne, Switzerland, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: For instance, the top-n ranking is such a mechanism. In this context, the error-reject tradeoff becomes error-(number-of-classes) tradeoff. Although the optimum error-reject tradeoff has been known for a long time [2], the optimum error-(number-of-classes) was discovered only recently <ref> [10] </ref>. Few theoretical results on these aspects are available [4, 13]. This report first gives an overview of optimum decision rules. A general relation between error rate and average number of classes is then presented. <p> For a more detailed discussion, see also [7]. In particular, the Bayes error rate is given by e Bayes = e (t ope = 1 1 ) = N t dr (t) (14) 2.3 Optimum Class-Selective Rejection Rule Recently, an optimum class-selective rejection rule was proposed <ref> [10] </ref>. It differs from Chow's in that the outcomes of the decision process are extended to the power set of the set of classes, while excluding the empty set ;. <p> This partition would correspond to a no-decision rule, however. In order to define the optimality of the class-selective rejection rule while avoiding the trivial partition, an additional constraint the average number of classes n 7 was introduced <ref> [10] </ref>. n = X where n (x) is the number of classes assigned to pattern x. <p> If there exist no such classes, the rule simply selects the (a) single best class <ref> [10] </ref>. Notice that the key point in this rule is the choice of the number of best classes, n (x; t), to be assigned to pattern x. <p> The risk defined by Eq. (16) can then be expressed in a more explicit form as follows risk (x; t) = 1 n (x;t) X Q i (x) (19) The optimum class-selective rejection rule is formally given by Decision Rule <ref> [10] </ref>: The optimum class-selective rejection rule assigns to pattern x the n fl (x; t) best classes, where n fl (x; t) = min fk=Q k+1 (x) tg (20) with the convention Q N+1 (x) = 0 (21) and the domain of the pre-specified threshold 0 t 2 Remarks: It is
Reference: [11] <author> D.J. </author> <title> Hand, "Recent Advances in Error Rate Estimation," </title> <journal> Pattern Recognition Letters, </journal> <volume> Vol. 4, </volume> <pages> pp. 335-346, </pages> <year> 1986. </year>
Reference-contexts: The study should take into account the observations made by Fukunaga and Kessel in [7] on the Stieltjes integral, Eq. (13), proposed by Chow [3]. See also <ref> [18, 11] </ref> for reviews of error estimation methods. Other interesting related papers are [19, 12, 6, 16, 15]. 7 Conclusion We have reviewed various optimum decision rules for pattern recognition, namely, Bayes rule, Chow's rule (optimum error-reject tradeoff), and a recently proposed class-selective rejection rule.
Reference: [12] <author> D.J. </author> <title> Hand, "An Optimal Error Rate Estimator Based on Average Conditional Error Rate: Asymptotic Results," </title> <journal> Pattern Recognition Letters, </journal> <volume> Vol. 4, </volume> <pages> pp. 347-350, </pages> <year> 1986. </year>
Reference-contexts: The study should take into account the observations made by Fukunaga and Kessel in [7] on the Stieltjes integral, Eq. (13), proposed by Chow [3]. See also [18, 11] for reviews of error estimation methods. Other interesting related papers are <ref> [19, 12, 6, 16, 15] </ref>. 7 Conclusion We have reviewed various optimum decision rules for pattern recognition, namely, Bayes rule, Chow's rule (optimum error-reject tradeoff), and a recently proposed class-selective rejection rule. The latter provides an optimum tradeoff between the error rate and the average number of classes.
Reference: [13] <author> M.E. Hellman, </author> <title> "The Nearest Neighbor Classification Rule with a Reject Op tion," </title> <journal> IEEE Transactions on Systems, Science, and Cybernetics, </journal> <volume> Vol. SSC-6, No. 3, </volume> <pages> pp. 179-185, </pages> <month> July </month> <year> 1970. </year> <month> 15 </month>
Reference-contexts: For instance, the top-n ranking is such a mechanism. In this context, the error-reject tradeoff becomes error-(number-of-classes) tradeoff. Although the optimum error-reject tradeoff has been known for a long time [2], the optimum error-(number-of-classes) was discovered only recently [10]. Few theoretical results on these aspects are available <ref> [4, 13] </ref>. This report first gives an overview of optimum decision rules. A general relation between error rate and average number of classes is then presented. It will be shown that the error rate can be estimated directly from the empirical number of classes assigned to each unlabelled pattern.
Reference: [14] <editor> C.G.Y. Lau (Editor), </editor> <booktitle> Neural Networks: Theoretical Foundations and Analysis, </booktitle> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: In most practical applications, fP i (x); i = 1; ::; Ng are unknown but can be estimated from a set of labelled patterns, called training set. Many estimation methods exist, e.g. Parzen estimate, nearest neighbour, potential functions, and neural networks <ref> [5, 8, 14] </ref>.
Reference: [15] <author> G. Lugosi and M. Pawlak, </author> <title> "On the Posterior-Probability Estimate of the Error Rate of Nonparametric Classification Rules," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. IT-40, </volume> <pages> No.2, pp. 475-481, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: The study should take into account the observations made by Fukunaga and Kessel in [7] on the Stieltjes integral, Eq. (13), proposed by Chow [3]. See also [18, 11] for reviews of error estimation methods. Other interesting related papers are <ref> [19, 12, 6, 16, 15] </ref>. 7 Conclusion We have reviewed various optimum decision rules for pattern recognition, namely, Bayes rule, Chow's rule (optimum error-reject tradeoff), and a recently proposed class-selective rejection rule. The latter provides an optimum tradeoff between the error rate and the average number of classes.
Reference: [16] <author> M. Pawlak, </author> <title> "On the Asymptotic Properties of Smoothed Estimators of the Classification Error Rate," </title> <journal> Pattern Recognition, </journal> <volume> Vol. 21, No. 5, </volume> <pages> pp. 515-524, </pages> <year> 1988. </year>
Reference-contexts: The study should take into account the observations made by Fukunaga and Kessel in [7] on the Stieltjes integral, Eq. (13), proposed by Chow [3]. See also [18, 11] for reviews of error estimation methods. Other interesting related papers are <ref> [19, 12, 6, 16, 15] </ref>. 7 Conclusion We have reviewed various optimum decision rules for pattern recognition, namely, Bayes rule, Chow's rule (optimum error-reject tradeoff), and a recently proposed class-selective rejection rule. The latter provides an optimum tradeoff between the error rate and the average number of classes.
Reference: [17] <author> S.M. Ross, </author> <title> A First Course in Probability, third edition, </title> <publisher> Macmillan Publishing Company, </publisher> <year> 1988. </year>
Reference-contexts: It turns out that it is possible to express the error rate directly as a function of the reject rate via the Stieltjes integral [3]. e (t ope ) = 0 where 'ope' stands for operating. (For an introduction to the Stieltjes integral, see <ref> [17] </ref>.) The marvelous feature of the above equation is that it allows the computation of the error rate at any level t from r (t) solely and that the latter can be estimated from unlabelled patterns, by just counting the rejects. <p> t from 0 to t ope ('ope' stands for operating) with constant increment t, and summing up all partial variations, we get t n t n e &gt; t n (35) Letting t ! 0, we can drop the second order infinitesimal term t n and get the Stieltjes integral <ref> [17] </ref> e (t ope ) = t=0 Z t ope t dn (t) (36) In particular, the Bayes error rate is given by e Bayes = e (t ope = 1 ) = 2 t dn (t) (37) 4 Properties of Optimum e n Curves Whenever the optimum class-selective rejection rule
Reference: [18] <author> G.T. Toussaint, </author> <title> "Bibliography on Estimation of Misclassifications," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. IT-20, No. </volume> ??, <pages> pp. 472-479, </pages> <month> July </month> <year> 1974. </year>
Reference-contexts: The study should take into account the observations made by Fukunaga and Kessel in [7] on the Stieltjes integral, Eq. (13), proposed by Chow [3]. See also <ref> [18, 11] </ref> for reviews of error estimation methods. Other interesting related papers are [19, 12, 6, 16, 15]. 7 Conclusion We have reviewed various optimum decision rules for pattern recognition, namely, Bayes rule, Chow's rule (optimum error-reject tradeoff), and a recently proposed class-selective rejection rule.
Reference: [19] <author> G.E. Tutz, </author> <title> "Smoothed Additive Estimators for Non-Error Rates in Multiple Discriminant Analysis," </title> <journal> Pattern Recognition, </journal> <volume> Vol. 18, No. 2, </volume> <pages> pp. 151-159, </pages> <year> 1985. </year> <month> 16 </month>
Reference-contexts: The study should take into account the observations made by Fukunaga and Kessel in [7] on the Stieltjes integral, Eq. (13), proposed by Chow [3]. See also [18, 11] for reviews of error estimation methods. Other interesting related papers are <ref> [19, 12, 6, 16, 15] </ref>. 7 Conclusion We have reviewed various optimum decision rules for pattern recognition, namely, Bayes rule, Chow's rule (optimum error-reject tradeoff), and a recently proposed class-selective rejection rule. The latter provides an optimum tradeoff between the error rate and the average number of classes.
References-found: 19

