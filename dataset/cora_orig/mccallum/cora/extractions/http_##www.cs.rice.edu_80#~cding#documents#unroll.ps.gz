URL: http://www.cs.rice.edu:80/~cding/documents/unroll.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~cding/publications.html
Root-URL: 
Title: Improving Software Pipelining With Unroll-and-Jam  
Author: Steve Carr Chen Ding Philip Sweany 
Address: Houghton MI 49931-1295  
Affiliation: Department of Computer Science Michigan Technological University  
Abstract: In this paper, we demonstrate how unroll-and-jam can significantly improve the initiation interval in a software-pipelined loop. Improvements in the initiation interval of greater than 40% are common, while dramatic improvements of a factor of 5 are possible. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. Allan, R. Jones, R. Lee, and S. Allan, </author> <title> "Software Pipelining," </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 27, no. 3, </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: As a result of these improvements, today's microprocessors can perform more operations per machine cycle than their predecessors. To take advantage of these architectural improvements, advanced compiler optimizations such as software pipelining have been developed <ref> [1, 2, 3, 4] </ref>. Software pipelining allows iterations of a loop to be overlapped with one another in order to take advantage of the maximum parallelism in a loop body. <p> POSTLUDE (INNERMOST LOOP): 1. nop POSTLUDE (MIDDLE LOOP): 1. nop 3. nop 5. nop 7. nop 2.2 Modulo Scheduling Allan et al. <ref> [1] </ref> provide an good summary of current software pipelining methods, dividing software pipelining techniques into two general categories 1 Actually, software pipelining could improve upon this by overlapping two iterations of the loop within the 2-instruction loop body, yielding a scheduling requiring only 96 cycles.
Reference: [2] <author> M. Lam, </author> <title> "Software pipelining: An effective scheduling technique for vliw machines," </title> <journal> SIG-PLAN Notices, </journal> <volume> vol. 23, </volume> <pages> pp. 318-328, </pages> <month> July </month> <year> 1988. </year> <booktitle> Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: As a result of these improvements, today's microprocessors can perform more operations per machine cycle than their predecessors. To take advantage of these architectural improvements, advanced compiler optimizations such as software pipelining have been developed <ref> [1, 2, 3, 4] </ref>. Software pipelining allows iterations of a loop to be overlapped with one another in order to take advantage of the maximum parallelism in a loop body. <p> However many instructions are required to "legally" schedule the DDG becomes the actual initiation interval, II. After finding a schedule for the loop body requiring II instructions, it may be necessary to perform modulo variable expansion <ref> [2] </ref> to circumvent inter-interval dependences which can occur due to register reuse. To overcome such inter-interval dependences, the loop body schedule may need to be copied M times, where M is the number of different loop iterations represented within the loop body schedule.
Reference: [3] <author> B. R. Rau, </author> <title> "Iterative modulo scheduling: An algorithm for software pipelining loops," </title> <booktitle> in Proceedings of the 27th International Symposium on Microarchitecture (MICRO-27), </booktitle> <address> (San Jose, CA), </address> <pages> pp. 63-74, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: As a result of these improvements, today's microprocessors can perform more operations per machine cycle than their predecessors. To take advantage of these architectural improvements, advanced compiler optimizations such as software pipelining have been developed <ref> [1, 2, 3, 4] </ref>. Software pipelining allows iterations of a loop to be overlapped with one another in order to take advantage of the maximum parallelism in a loop body. <p> Lam's hierarchical reduction is a modulo scheduling method as is Warter's [13, 4] enhanced modulo scheduling which uses IF-conversion to produce a single super-block to represent a loop. Rau <ref> [3] </ref> provides a detailed discussion of an implementation of modulo scheduling. Since Rocket's software pipelining, patterned after Warter's enhanced modulo scheduling [13], uses a modulo scheduling algorithm, we shall investigate mod ulo scheduling in a bit more detail. <p> Since Rocket's software pipelining, patterned after Warter's enhanced modulo scheduling [13], uses a modulo scheduling algorithm, we shall investigate mod ulo scheduling in a bit more detail. While Warter's method provides a general framework for our software pipelining, the actual modulo scheduling technique implemented in Rocket closely follows Rau <ref> [3] </ref>. Modulo scheduling assumes that a single data-dependence graph (DDG) can be built for a loop. To build a single DDG for a loop requires some method of treating the entire loop as a single basic block. <p> Following Rau <ref> [3] </ref>, to find RecII we iterate on potential values for RecII, building a matrix, MinDist, for each possible II value. M inDist [i; j] is defined to be the minimum interval between loop operations i and j which maintains data dependence integrity.
Reference: [4] <author> N. J. Warter, S. A. Mahlke, W. mei W. Hwu, and B. R. Rau, </author> <title> "Reverse if-conversion," </title> <journal> SIGPLAN Notices, </journal> <volume> vol. 28, </volume> <pages> pp. 290-299, </pages> <month> June </month> <year> 1993. </year> <booktitle> Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: As a result of these improvements, today's microprocessors can perform more operations per machine cycle than their predecessors. To take advantage of these architectural improvements, advanced compiler optimizations such as software pipelining have been developed <ref> [1, 2, 3, 4] </ref>. Software pipelining allows iterations of a loop to be overlapped with one another in order to take advantage of the maximum parallelism in a loop body. <p> Once that minimum initiation interval is determined, instruction scheduling attempts to match that minimum schedule while respecting resource and dependence constraints. Lam's hierarchical reduction is a modulo scheduling method as is Warter's <ref> [13, 4] </ref> enhanced modulo scheduling which uses IF-conversion to produce a single super-block to represent a loop. Rau [3] provides a detailed discussion of an implementation of modulo scheduling.
Reference: [5] <author> D. Callahan, J. Cocke, and K. Kennedy, </author> <title> "Estimating interlock and improving balance for pipelined machines," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 5, </volume> <pages> pp. 334-358, </pages> <year> 1988. </year>
Reference-contexts: Parallelism is normally inhibited by either inner-loop recurrences, or by a mismatch between the resource requirements of a loop and the resources provided by the target architecture. Unroll-and-jam is a transformation that can be used to increase the parallelism in the innermost loop body <ref> [5, 6] </ref>. Unroll-and-jam can reduce the number of memory operations that need to be issued per floating-point operation in order to alleviate resource constraint problems. In addition, unroll-and-jam creates copies of inner-loop recurrences that are parallel with all other copies [5]. <p> Unroll-and-jam can reduce the number of memory operations that need to be issued per floating-point operation in order to alleviate resource constraint problems. In addition, unroll-and-jam creates copies of inner-loop recurrences that are parallel with all other copies <ref> [5] </ref>. These two effects increase the parallelism available to a software pipelining algorithm. This paper measures the effectiveness of unroll-and-jam at improving the initiation interval for software-pipelined loops. <p> fects of modulo variable expansion. 3 Unroll-and-Jam Callahan,et al., have shown that when software pipelining is performed, simple inner loop unrolling does not help in the presence of recurrences nor does it help with a mismatch in the resources demanded by a loop and the resources provided by a machine <ref> [5] </ref>. However, unroll-and-jam, or outer-loop unrolling, can be used to improve the ILP available to a software pipelining algorithm in the above situations [5, 6]. The transformation unrolls an outer loop and then jams the resulting inner loops back together. <p> However, unroll-and-jam, or outer-loop unrolling, can be used to improve the ILP available to a software pipelining algorithm in the above situations <ref> [5, 6] </ref>. The transformation unrolls an outer loop and then jams the resulting inner loops back together. Using unroll-and-jam we can introduce more parallelism into an innermost loop body. <p> In particular, we assume that it performs strength reduction, optimizes for machine addressing modes, allocates registers globally (via a coloring scheme) and schedules the pipelines. To measure the performance of program loops given the above assumptions, we use the notion of balance defined by Callahan, et al. <ref> [5] </ref>. 3.1.1 Machine Balance A computer is balanced when it can operate in a steady state manner with both memory accesses and floating-point operations being performed at peak speed.
Reference: [6] <author> S. Carr and K. Kennedy, </author> <title> "Improving the ratio of memory operations to floating-point operations in loops," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 16, no. 6, </volume> <pages> pp. 1768-1810, </pages> <year> 1994. </year>
Reference-contexts: Parallelism is normally inhibited by either inner-loop recurrences, or by a mismatch between the resource requirements of a loop and the resources provided by the target architecture. Unroll-and-jam is a transformation that can be used to increase the parallelism in the innermost loop body <ref> [5, 6] </ref>. Unroll-and-jam can reduce the number of memory operations that need to be issued per floating-point operation in order to alleviate resource constraint problems. In addition, unroll-and-jam creates copies of inner-loop recurrences that are parallel with all other copies [5]. <p> However, unroll-and-jam, or outer-loop unrolling, can be used to improve the ILP available to a software pipelining algorithm in the above situations <ref> [5, 6] </ref>. The transformation unrolls an outer loop and then jams the resulting inner loops back together. Using unroll-and-jam we can introduce more parallelism into an innermost loop body. <p> Unroll-and-jam moves reuse carried by outer loops into the inner loop to reduce the number of memory references and cache misses, thus, decreasing the memory demands of the loop. 3.1.3 Applying Unroll-and-Jam In A Com piler Previous work has used the following optimization problem to guide unroll-and-jam <ref> [6] </ref>: objective function: min jfi L fi M j 0 constraint: R L R M where R L is the number of registers required by a loop, R M is the register-set size of the target architecture, and jfi L fi M j 0 is the balance norm having the following <p> The solution to the objective function for a particular loop nest can be optimized in O (log R M ) steps when unroll-and-jam is applied to one loop and O (R M ) steps when unroll-and-jam is applied to two loops <ref> [6] </ref>. After unroll-and-jam guided by the above objective function, a loop may still contain pipeline interlock, leaving idle computational cycles. <p> We expect this heuristic to be needed rarely as unrolling for loop balance will likely remove interlock. In previous work, Carr and Kennedy show that unroll-and-jam guided by the previous optimization formula improves the performance on the IBM RS/6000 <ref> [6] </ref>. Their experiment presents execution-time improvements to validate their claims. However, the RS/6000 only has limited hardware instruction scheduling and the full benefit of unroll-and-jam could not be measured.
Reference: [7] <author> S. Carr, </author> <title> Memory-Hierarchy Management. </title> <type> PhD thesis, </type> <institution> Rice University, Department of Computer Science, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: These two effects increase the parallelism available to a software pipelining algorithm. This paper measures the effectiveness of unroll-and-jam at improving the initiation interval for software-pipelined loops. In our experiments, unroll-and-jam is performed by a Fortran source-to-source transformer called Memoria <ref> [7] </ref> that is based upon the Para-Scope programming environment [8]. Software pipelining is performed by a retargetable compiler for ILP (Instruction-Level Parallel) architectures, called Rocket [9].
Reference: [8] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon, </author> <title> "ParaScope: A parallel programming environment," </title> <booktitle> in Proceedings of the First International Conference on Supercomputing, </booktitle> <address> (Athens, Greece), </address> <month> June </month> <year> 1987. </year>
Reference-contexts: These two effects increase the parallelism available to a software pipelining algorithm. This paper measures the effectiveness of unroll-and-jam at improving the initiation interval for software-pipelined loops. In our experiments, unroll-and-jam is performed by a Fortran source-to-source transformer called Memoria [7] that is based upon the Para-Scope programming environment <ref> [8] </ref>. Software pipelining is performed by a retargetable compiler for ILP (Instruction-Level Parallel) architectures, called Rocket [9].
Reference: [9] <author> P. H. Sweany and S. J. Beaty, </author> <title> "Overview of the Rocket retargetable C compiler," </title> <type> Tech. Rep. </type> <institution> CS-94-01, Department of Computer Science, Michi-gan Technological University, Houghton, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: In our experiments, unroll-and-jam is performed by a Fortran source-to-source transformer called Memoria [7] that is based upon the Para-Scope programming environment [8]. Software pipelining is performed by a retargetable compiler for ILP (Instruction-Level Parallel) architectures, called Rocket <ref> [9] </ref>. We present experimental evidence that suggests that Memoria can be quite effective at improving the initiation interval generated by Rocket. 2 Software Pipelining While local and global instruction scheduling can, together, exploit considerable parallelism for non-loop code, to best exploit instruction-level parallelism within loops requires software pipelining.
Reference: [10] <author> A. Aiken and A. Nicolau, </author> <title> "Optimal loop par-allelization," </title> <booktitle> in Conference on Programming Language Design and Implementation, (Atlanta Georgia), </booktitle> <pages> pp. 308-317, </pages> <booktitle> SIGPLAN '88, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: After scheduling the N copies of the loop, some pattern recognition technique is used to identify a repeating kernel within the schedule. Examples of kernel recognition methods are Aiken and Nicolau's perfect pipelining method <ref> [10, 11] </ref> and Al-lan's petri-net pipelining technique [12]. In contrast to kernel recognition methods, modulo scheduling does not schedule multiple iterations of a loop and then look for a pattern.
Reference: [11] <author> A. Aiken and A. Nicolau, </author> <title> "Perfect Pipelin-ing: A New Loop Optimization Technique," </title> <booktitle> in Proceedings of the 1988 European Symposium on Programming, Springer Verlag Lecture Notes in Computer Science, #300, </booktitle> <address> (Atlanta, GA), </address> <pages> pp. 221-235, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: After scheduling the N copies of the loop, some pattern recognition technique is used to identify a repeating kernel within the schedule. Examples of kernel recognition methods are Aiken and Nicolau's perfect pipelining method <ref> [10, 11] </ref> and Al-lan's petri-net pipelining technique [12]. In contrast to kernel recognition methods, modulo scheduling does not schedule multiple iterations of a loop and then look for a pattern.
Reference: [12] <author> V. Allan, M. Rajagopalan, and R. Lee, </author> <title> "Software Pipelining: Petri Net Pacemaker," in Working Conference on Architectures and Compilation Techniques for Fine and Medium Grain Parallelism, </title> <address> (Orlando, FL), </address> <month> January 20-22 </month> <year> 1993. </year>
Reference-contexts: After scheduling the N copies of the loop, some pattern recognition technique is used to identify a repeating kernel within the schedule. Examples of kernel recognition methods are Aiken and Nicolau's perfect pipelining method [10, 11] and Al-lan's petri-net pipelining technique <ref> [12] </ref>. In contrast to kernel recognition methods, modulo scheduling does not schedule multiple iterations of a loop and then look for a pattern. Instead, modulo scheduling selects a schedule for one iteration of the loop such that, when that schedule is repeated, no resource or dependence constraints are violated.
Reference: [13] <author> N. Warter, G. Haab, and J. Bockhaus, </author> <title> "Enhanced Modulo Scheduling for Loops with Conditional Branches," </title> <booktitle> in Proceedings of the 25th Annual International Symposium on Microarchitec-ture (MICRO-25), </booktitle> <address> (Portland, OR), </address> <pages> pp. 170-179, </pages> <month> December 1-4 </month> <year> 1992. </year>
Reference-contexts: Once that minimum initiation interval is determined, instruction scheduling attempts to match that minimum schedule while respecting resource and dependence constraints. Lam's hierarchical reduction is a modulo scheduling method as is Warter's <ref> [13, 4] </ref> enhanced modulo scheduling which uses IF-conversion to produce a single super-block to represent a loop. Rau [3] provides a detailed discussion of an implementation of modulo scheduling. <p> Rau [3] provides a detailed discussion of an implementation of modulo scheduling. Since Rocket's software pipelining, patterned after Warter's enhanced modulo scheduling <ref> [13] </ref>, uses a modulo scheduling algorithm, we shall investigate mod ulo scheduling in a bit more detail. While Warter's method provides a general framework for our software pipelining, the actual modulo scheduling technique implemented in Rocket closely follows Rau [3].
Reference: [14] <author> D. Kuck, </author> <title> The Structure of Computers and Computations Volume 1. </title> <address> New York: </address> <publisher> John Wiley and Sons, </publisher> <year> 1978. </year>
Reference-contexts: Since we intend to overlap different loop iterations we need to consider loop-carried dependence as well as loop-independent dependence, but well-known algorithms provide this information <ref> [14, 15] </ref>. Once the DDG is constructed for the loop, modulo scheduling attempts to identify the smallest number of instructions which might separate different loop iterations. This minimum initiation interval (II min ) represents the shortest time interval between the initiation of consecutive loop iterations.
Reference: [15] <author> G. Goff, K. Kennedy, and C.-W. Tseng, </author> <title> "Practical dependence testing," </title> <journal> SIGPLAN Notices, </journal> <volume> vol. 26, </volume> <pages> pp. 15-29, </pages> <month> June </month> <year> 1991. </year> <booktitle> Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: Since we intend to overlap different loop iterations we need to consider loop-carried dependence as well as loop-independent dependence, but well-known algorithms provide this information <ref> [14, 15] </ref>. Once the DDG is constructed for the loop, modulo scheduling attempts to identify the smallest number of instructions which might separate different loop iterations. This minimum initiation interval (II min ) represents the shortest time interval between the initiation of consecutive loop iterations.
Reference: [16] <author> K. Kennedy and K. McKinley, </author> <title> "Optimizing for parallelism and memory hierarchy," </title> <booktitle> in Proceedings of the 1992 International Conference on Supercomputing, </booktitle> <address> (Washington, DC), </address> <pages> pp. 323-334, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: L = M L where M L is the number of memory operations in loop L and F L is the number of floating-point operations in L. 3 This model assigns a uniform cost to memory references under the assumption that compiler optimizations can be performed to attain cache locality <ref> [16, 17, 18] </ref>. Comparing fi M to fi L can give us a measure of the performance of a loop running on a particular architecture.
Reference: [17] <author> T. C. Mowry, M. S. Lam, and A. Gupta, </author> <title> "Design and evaluation of a compiler algorithm for prefetching," </title> <booktitle> in Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> (Boston, Massachusetts), </address> <pages> pp. 62-75, </pages> <year> 1992. </year>
Reference-contexts: L = M L where M L is the number of memory operations in loop L and F L is the number of floating-point operations in L. 3 This model assigns a uniform cost to memory references under the assumption that compiler optimizations can be performed to attain cache locality <ref> [16, 17, 18] </ref>. Comparing fi M to fi L can give us a measure of the performance of a loop running on a particular architecture.
Reference: [18] <author> M. E. Wolf and M. S. Lam, </author> <title> "A data locality optimizing algorithm," </title> <journal> SIGPLAN Notices, </journal> <volume> vol. 26, </volume> <pages> pp. 30-44, </pages> <month> June </month> <year> 1991. </year> <booktitle> Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: L = M L where M L is the number of memory operations in loop L and F L is the number of floating-point operations in L. 3 This model assigns a uniform cost to memory references under the assumption that compiler optimizations can be performed to attain cache locality <ref> [16, 17, 18] </ref>. Comparing fi M to fi L can give us a measure of the performance of a loop running on a particular architecture.
Reference: [19] <author> D. Callahan, S. Carr, and K. Kennedy, </author> <title> "Improving register allocation for subscripted variables," </title> <journal> SIGPLAN Notices, </journal> <volume> vol. 25, </volume> <pages> pp. 53-65, </pages> <month> June </month> <year> 1990. </year> <booktitle> Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: In our matrix multiply example in Figure 1, the original loop has a balance of 1 because c [i][j] can be allocated to a register using scalar replacement <ref> [19, 20] </ref>.
Reference: [20] <author> S. Carr and K. Kennedy, </author> <title> "Scalar replacement in the presence of conditional control flow," </title> <journal> Software Practice and Experience, </journal> <volume> vol. 24, </volume> <pages> pp. 51-77, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: In our matrix multiply example in Figure 1, the original loop has a balance of 1 because c [i][j] can be allocated to a register using scalar replacement <ref> [19, 20] </ref>.
Reference: [21] <author> P. Kogge, </author> <title> The Architecture of Pipelined Computers. </title> <address> New York: </address> <publisher> McGraw-Hill, </publisher> <year> 1981. </year>
Reference-contexts: Although there are methods other than unroll-and-jam to deal with recurrences <ref> [21] </ref>, unroll-and-jam can additionally bring memory-bound loops into balance. Inner loop unrolling does not improve memory performance as reuse across the inner loop will already be captured by cache and registers [22].
Reference: [22] <author> S. Carr, K. McKinley, and C.-W. Tseng, </author> <title> "Compiler optimizations for improving data locality," </title> <booktitle> in Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> (Santa Clara, California), </address> <year> 1994. </year>
Reference-contexts: Although there are methods other than unroll-and-jam to deal with recurrences [21], unroll-and-jam can additionally bring memory-bound loops into balance. Inner loop unrolling does not improve memory performance as reuse across the inner loop will already be captured by cache and registers <ref> [22] </ref>.
References-found: 22

