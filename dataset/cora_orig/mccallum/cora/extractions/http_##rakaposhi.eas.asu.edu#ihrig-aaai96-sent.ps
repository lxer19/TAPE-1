URL: http://rakaposhi.eas.asu.edu/ihrig-aaai96-sent.ps
Refering-URL: http://rakaposhi.eas.asu.edu/yochan.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: laurie.ihrig@asu.edu, rao@asu.edu  
Title: Design and Implementation of a Replay Framework based on a Partial Order Planner  
Author: Laurie H. Ihrig and Subbarao Kambhampati 
Address: Tempe, AZ 85287  
Affiliation: Department of Computer Science and Engineering Arizona State University,  
Abstract: In this paper we describe the design and implementation of the derivation replay framework, dersnlp+ebl (Derivational snlp+ebl), which is based within a partial order planner. dersnlp+ebl replays previous plan derivations by first repeating its earlier decisions in the context of the new problem situation, then extending the replayed path to obtain a complete solution for the new problem. When the replayed path cannot be extended into a new solution, explanation-based learning (ebl) techniques are employed to identify the features of the new problem which prevent this extension. These features are then added as censors on the retrieval of the stored case. To keep retrieval costs low, dersnlp+ebl normally stores plan derivations for individual goals, and replays one or more of these derivations in solving multi-goal problems. Cases covering multiple goals are stored only when subplans for individual goals cannot be successfully merged. The aim in constructing the case library is to predict these goal interactions and to store a multi-goal case for each set of negatively interacting goals. We provide empirical results demonstrating the effectiveness of dersnlp+ebl in improving planning performance on randomly-generated problems drawn from a complex domain. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barrett, A., and Weld, D. </author> <year> 1994. </year> <title> Partial order planning: evaluating possible efficiency gains. </title> <journal> Artificial Intelligence 67:71--112. </journal>
Reference-contexts: Since a plan failure explanation is a subset of plan constraints, these explanations are represented in the same manner as a partial plan. dersnlp+ebl represents its partial plans as a 6-tuple, hS; O; B; L; E ; Ci, where <ref> (Barrett & Weld 1994) </ref>: S is the set of actions (step-names) in the plan, each of which is mapped onto an operator in the domain theory. S contains two dummy steps: t I whose effects are the initial state conditions, and t G whose preconditions are the input goals, G.
Reference: <author> Hammond, K. </author> <year> 1990. </year> <title> Explaining and repairing plans that fail. </title> <journal> Artificial Intelligence 45:173--228. </journal>
Reference-contexts: Replay failures are explained and used to avoid the retrieval of a case in situations where replay will mislead the planner. Failures are also used to construct repairing cases which are stored as alternatives to be retrieved when a similar failure is predicted. CHEF <ref> (Hammond 1990) </ref> learns to avoid execution-time failures by simulating and analyzing plans derived by reusing old cases. In contrast, our approach attempts to improve planning efficiency by concentrating on search failures encountered in plan generation.
Reference: <author> Ihrig, L., and Kambhampati, S. </author> <year> 1994. </year> <title> Derivation replay for partial-order planning. </title> <booktitle> In Proceedings AAAI-94. </booktitle>
Reference-contexts: One approach to case-based planning is to store plan derivations which are then used as guidance when solving new similar problems (Veloso & Carbonell 1993). Recently we adapted this approach, called derivational replay, to improve the performance of the partial-order planner, snlp <ref> (Ihrig & Kambhampati 1994) </ref>.
Reference: <author> Ihrig, L., and Kambhampati, S. </author> <year> 1995. </year> <title> An explanation-based approach to improve retrieval in case-based planning. </title> <booktitle> In Current Trends in AI Planning: EWSP '95. </booktitle> <publisher> IOS Press. </publisher>
Reference-contexts: In <ref> (Ihrig & Kambhampati 1995) </ref>, the potential effective and replay.
Reference: <author> Ihrig, L. </author> <year> 1996. </year> <title> The Design and Implementation of a Case-based Planning Framework within a Partial Order Planner. </title> <type> Ph.D. Dissertation, </type> <institution> Arizona State University. </institution>
Reference-contexts: Problem test sets increasing in problem size were randomly generated from this domain. The initial state of each problem described the location of 6 packages, and 12 transport devices (6 planes and 6 trucks) within 6 cities, each containing a post office and an airport. See <ref> (Ihrig 1996) </ref> for similar tests on larger problems in a 15 city domain. The experiments were run in six phases. At the start of each phase n the library was cleared and thirty test problems, each with n goals, were randomly generated.
Reference: <author> Kambhampati, S., and Hendler, J. A. </author> <year> 1992. </year> <title> A validation structure based theory of plan modification and reuse. </title> <journal> Artificial Intelligence 55:193--258. </journal>
Reference: <author> Kambhampati, S.; Katukam, S.; and Qu, Y. </author> <year> 1996. </year> <title> Failure driven dynamic search control for partial order planners: An explanation-based approach. </title> <journal> Artificial Intelligence. </journal> <note> To Appear. </note>
Reference-contexts: These include methods for forming explanations of search path failures and regressing these explanations through the planning decisions in the failing paths <ref> (Kambhampati, Katukam, & Qu 1996) </ref>. Here we employ these techniques to construct reasons for case failure, which are then used to annotate the failing cases to constrain their future retrieval. <p> We are now in a position to describe how the planner learns the reasons underlying a case failure. Specifically, we use EBL techniques to accomplish this learning. In the next section, we show how the techniques developed in <ref> (Kambhampati, Katukam, & Qu 1996) </ref> are employed to construct these reasons. <p> A search path experiences an analytical failure when it arrives at a plan which, because it contains a set of inconsistent constraints, cannot be further refined into a solution. EBL techniques are used to form explanations of plan failures in terms of these conflicting constraints <ref> (Kambhampati, Katukam, & Qu 1996) </ref>. dersnlp+ebl constructs explanations for each of the analytical failures that occur in the subtree beneath the skeletal plan 1 . <p> CHEF (Hammond 1990) learns to avoid execution-time failures by simulating and analyzing plans derived by reusing old cases. In contrast, our approach attempts to improve planning efficiency by concentrating on search failures encountered in plan generation. We integrate replay with techniques adopted from the planning framework provided by snlp+ebl <ref> (Kambhampati, Katukam, & Qu 1996) </ref>. This framework includes methods for constructing conditions for predicting analytical failures in its search space. EBL techniques have been previously used to learn from problem-solving failures (Kambhampati, Katukam, & Qu 1996; Minton 1990; Mostow & Bhatnagar 1987).
Reference: <author> Minton, S. </author> <year> 1990. </year> <title> Quantitative results concerning the utility of explanation-based learning. </title> <journal> In Artificial Intelligence, </journal> <volume> volume 42, </volume> <month> 363--392. </month>
Reference: <author> Mostow, J., and Bhatnagar, N. </author> <year> 1987. </year> <title> Failsafe: A floor planner that uses ebg to learn from its failures. </title> <booktitle> In Proceedings IJCAI-87, </booktitle> <address> 249--255. </address>
Reference: <author> Veloso, M., and Carbonell, J. </author> <year> 1993. </year> <title> Toward scaling up machine learning: A case study with derivational analogy in prodigy. </title>
Reference-contexts: One approach to case-based planning is to store plan derivations which are then used as guidance when solving new similar problems <ref> (Veloso & Carbonell 1993) </ref>. Recently we adapted this approach, called derivational replay, to improve the performance of the partial-order planner, snlp (Ihrig & Kambhampati 1994). <p> The new derivation path which leads from the root of the search tree to the final plan in the leaf node thus avoids (or repairs) the failure encountered in replaying the old case. Consider a simple example taken from the logistics transportation domain of <ref> (Veloso & Carbonell 1993) </ref>. Figure 3a illustrates the solution to a simple problem drawn from this domain. The goal is to have package OB1 located at the destination location l d . The package is initially at location l 1 . <p> It is therefore an advantage to store cases covering smaller subsets of goals, and to retrieve and replay multiple cases in solving a single large problem. In implementing this storage strategy, decisions have to be made as to which goal combinations to store. Previous work <ref> (Veloso & Carbonell 1993) </ref> has reduced the size of the library by separating out connected components of a plan, and storing their derivations individually. <p> Empirical Evaluation Experimental Setup: We tested the improvement in planning performance provided by multi-case replay on problems drawn from the logistics transportation domain <ref> (Veloso & Carbonell 1993) </ref>. Problem test sets increasing in problem size were randomly generated from this domain. The initial state of each problem described the location of 6 packages, and 12 transport devices (6 planes and 6 trucks) within 6 cities, each containing a post office and an airport.
Reference: <editor> In Minton, S., ed., </editor> <title> Machine Learning methods for planning. </title> <publisher> Morgan Kaufmann. </publisher>
References-found: 11

