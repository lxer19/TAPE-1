URL: http://www.ai.sri.com/~harabagi/Papers/hs.ps.gz
Refering-URL: http://www.ai.sri.com/~harabagi/
Root-URL: 
Email: moldovan@seas.smu.edu  harabagi@usc.edu  
Title: A MARKER PROPAGATION TEXT UNDERSTANDING AND INFERENCE SYSTEM  
Author: Dan I. Moldovan Sanda M. Harabagiu 
Address: Dallas, TX 75275  Los Angeles, CA 90089-2562  
Affiliation: Southern Methodist University Dept. of Computer Science Engineering  University of Southern California Dept. of Electrical Engineering-Systems  
Abstract: This paper introduces a system intended for deep language understanding and inference. The system uses a large knowledge base structured around WordNet. Semantic paths between text concepts are established in the knowledge base with the help of the marker propagation method. These paths bring forward inferences and implicatures that otherwise are difficult to extract. 
Abstract-found: 1
Intro-found: 1
Reference: [Cha86] <author> E. Charniak. </author> <title> A neat theory of marker passing. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence AAAI-86, </booktitle> <pages> pages 584-588, </pages> <year> 1986. </year>
Reference-contexts: The marker propagation model can be briefly described as a network consisting of nodes and links, and a set of markers moving through the network according to some propagation rules. The marker propagation scheme designed by us is by far more complex than the ones previously proposed by Charniak <ref> [Cha86] </ref>, Hendler [Hen88] and Norvig [Nor87]. One of the differences is that marker propagations are dependent on the processing of their propagation rules, which interact with the predicate values of the concepts they reach in the knowledge base, as well as with their constraint arguments.
Reference: [Cha95] <author> E. Charniak. </author> <title> Natural Language Learning. </title> <journal> ACM Computing Surveys, </journal> <volume> vol 27: No3, </volume> <pages> pages 317-319, </pages> <month> September </month> <year> 1995. </year>
Reference: [COL88] <author> P.R. Cohen and C.L. Loiselle. </author> <title> Beyond ISA: Structures for Plausible Inference in Semantic Networks. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence AAAI-88, </booktitle> <pages> pages 415-420, </pages> <year> 1988. </year>
Reference: [GPW95] <editor> L. Guthrie, J. Pustejowsky, Y. Wilks and B. Slator.. </editor> <booktitle> The Role of Lexicons in Natural Language Processing Communication of the ACM, </booktitle> <volume> vol 39: No1, </volume> <month> January, </month> <year> 1996. </year>
Reference-contexts: We exemplify on the above text some reasoning mechanisms which produce explanations for reference resolution, context recovery and implicature understanding. 4 STRUCTURE OF A VERY LARGE LINGUISTIC KNOWLEDGE BASE WordNet and extensions Realistic natural language processing requires extensive knowledge. While researchers agree that large, scalable knowledge bases are needed <ref> [GPW95] </ref>, [Jac92], [Len95], there is little agreement on how to structure such knowledge bases. We decided to build our knowledge base on top of WordNet1.5, the lexical database developed at Princeton. WordNet was built as a semantic dictionary, in which words are searched based on conceptual affinity with other words.
Reference: [Har96] <author> S.M. Harabagiu. </author> <title> An Application of WordNet to Prepositional Attachment. </title> <booktitle> To apear in the Proceedings of ACL-96, </booktitle> <year> 1996. </year>
Reference-contexts: One of the first tasks is to determine the acceptable lexical and syntactic structures for each sentence. Prepositional attachments are solved immediately after the parse is produced. Our system uses a body of heuristics operating on WordNet, that are synthesized from experiments conducted over the Treebank corpora <ref> [Har96] </ref>. Further on, transformational grammars account for the recognition of semantic case functions. Semantic disambiguation is performed as inference which relates concepts with minimal semantic distance in WordNet [MiT91]. At this point, the system is ready to provide the explicit inferences implied by the text.
Reference: [HM95] <author> S.M. Harabagiu and D.I. Moldovan. </author> <title> A Marker-Propagation Algorithm for Text Coherence. </title> <booktitle> In Working Notes of the Workshop on Parallel Processing for Artificial Intelligence, IJCAI-95, </booktitle> <pages> pages 76-86, </pages> <year> 1995. </year>
Reference-contexts: Attributes connections within glossary phrases are represented as DFC attr relations. Many verbs have the goal of their actions explicitly stated in the glossary. the relations DFC purpose links actions to their goals in the defining features. Logical connections between textual concepts We developed a parallel marker propagation algorithm <ref> [HM95] </ref> that establishes the logical connections between pairs of textual concepts. Filtered by a variety of linguistic constraints [HMY96], the semantic paths bring forward the implicit inferences implied by the text. The textual context is also recovered along this process.
Reference: [HMY96] <author> S.M. Harabagiu, D.I. Moldovan and T.Yukawa. </author> <title> Testing Gricean constraints on a WordNet-based Coherence Evaluaiton System. </title> <booktitle> In Working Notes of the AAAI-96 Spring Symposium on Computational Approaches to Interpreting and Generating Conversational Implicature, </booktitle> <pages> pages 31-38, </pages> <year> 1996. </year>
Reference-contexts: Logical connections between textual concepts We developed a parallel marker propagation algorithm [HM95] that establishes the logical connections between pairs of textual concepts. Filtered by a variety of linguistic constraints <ref> [HMY96] </ref>, the semantic paths bring forward the implicit inferences implied by the text. The textual context is also recovered along this process. <p> We found this to be an ideal setting on which to apply marker propagations to draw inferences. More information about inference rules that may be constructed by chaining relations can be found in <ref> [HMY96] </ref>. The inference method proposed here is still very simplistic because it cannot rank the semantic paths it finds and it does not take into account enough linguistic constraints. These are not inherent limitations and can be resolved.
Reference: [Hen88] <author> J.A. Hendler. </author> <title> Intergating Marker-Passing and Problem Solving : A Spreading Activation Approach to Improve Choice in Planning. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1988. </year>
Reference-contexts: The marker propagation scheme designed by us is by far more complex than the ones previously proposed by Charniak [Cha86], Hendler <ref> [Hen88] </ref> and Norvig [Nor87]. One of the differences is that marker propagations are dependent on the processing of their propagation rules, which interact with the predicate values of the concepts they reach in the knowledge base, as well as with their constraint arguments.
Reference: [Hen95] <author> K.J. Hendrickson. </author> <title> A New Parallel LR Parsing Algorithm. </title> <booktitle> In Proceedings of the 1995 ACM Symposium on Applied Computing, </booktitle> <pages> pages 115-120, </pages> <year> 1995. </year>
Reference-contexts: The preprocessor locates sentence boundaries and inserts sentence boundary markers for the syntactic parser. Phrase tagging at this point is meant to reduce the parser's work load. Our syntactic parser, a parallel version of a LR (0) shift-reduce parser <ref> [Hen95] </ref> recognizes the syntactic structure of an input sentence and produces markers for noun groups, verb groups and prepositional phrases. Many of the lexical ambiguities are resolved in this module. The output consists of larger markers that carry the accepted syntactical structure of a sentence.
Reference: [HSA92] <author> J.R. Hobbs, M. Stickel, D. Applet and P. Martin. </author> <title> Interpretation as Abduction. </title> <journal> Artificial Intelligence, </journal> <volume> vol 63:pages 69-142, </volume> <year> 1993. </year>
Reference-contexts: The main advantages of this approach are: (1) it offers the possibility of exploiting the inherent parallelism in natural language, and (2) the same knowledge base and propagation mechanism are used at all levels of processing i.e., syntactic, semantic, pragmatic and discourse. The approach we use is abductive interpretation <ref> [HSA92] </ref> integrating constraint satisfaction and search in a very large knowledge base. Constraints are implemented as links of the network and as marker processing functions. Markers carrying linguistic information check constraints, and interact with some other markers inside the nodes, and produce new markers that satisfy the constraints.
Reference: [Jac92] <author> P.S. Jacobs. </author> <title> Text Power and Intelligent Systems. </title> <publisher> In Lawrence Erlbaum Associates, Text-Based Intelligent Systems, </publisher> <pages> pages 1-8. </pages> <editor> ,editor Jacobs, P.S., </editor> <address> Hillsdale, N.J., </address> <year> 1992. </year>
Reference-contexts: While researchers agree that large, scalable knowledge bases are needed [GPW95], <ref> [Jac92] </ref>, [Len95], there is little agreement on how to structure such knowledge bases. We decided to build our knowledge base on top of WordNet1.5, the lexical database developed at Princeton. WordNet was built as a semantic dictionary, in which words are searched based on conceptual affinity with other words.
Reference: [Len95] <author> D.B. Lenat. </author> <title> CYC: A Large-Scale Investment in Knowledge Infrastructure. </title> <journal> Communication of the ACM, </journal> <volume> vol 38: No11, </volume> <pages> pages 32-38, </pages> <month> November, </month> <year> 1995. </year>
Reference-contexts: While researchers agree that large, scalable knowledge bases are needed [GPW95], [Jac92], <ref> [Len95] </ref>, there is little agreement on how to structure such knowledge bases. We decided to build our knowledge base on top of WordNet1.5, the lexical database developed at Princeton. WordNet was built as a semantic dictionary, in which words are searched based on conceptual affinity with other words.
Reference: [Mil95] <author> G.A. Miller. </author> <title> WordNet: A Lexical Database. </title> <journal> Communication of the ACM, </journal> <volume> vol 38: No11, </volume> <pages> pages 39-41, </pages> <month> November, </month> <year> 1995. </year>
Reference-contexts: To facilitate the processing of unrestricted text, we use a knowledge base constructed on top of WordNet 1.5, the lexical database for English language developed at Princeton <ref> [Mil95] </ref>. Text understanding and inference The understanding of natural language processing involves complex tasks such as lexical and semantic disambiguation, pragmatic reasoning for coherence and implicature recognition, anaphora and co-reference resolution. Each of these problems are AI-complete.
Reference: [MiT91] <author> G.A. Miller and D.A. Teibel. </author> <title> A Proposal for Lexical Disambiguation. </title> <booktitle> In Proceedings of the ARPA Speech and Language Workshop, </booktitle> <pages> pages 395-399, </pages> <year> February,1991. </year>
Reference-contexts: Our system uses a body of heuristics operating on WordNet, that are synthesized from experiments conducted over the Treebank corpora [Har96]. Further on, transformational grammars account for the recognition of semantic case functions. Semantic disambiguation is performed as inference which relates concepts with minimal semantic distance in WordNet <ref> [MiT91] </ref>. At this point, the system is ready to provide the explicit inferences implied by the text. Nevertheless, they still have to be enforced by the implicit inferences, supporting coherence evaluation and context determination. Almost all inferences, from disambiguation and prepositional attachments to discourse implicatures rely heavily on WordNet information.
Reference: [Nor87] <author> P. Norvig. </author> <title> Inference in text understanding. </title> <booktitle> In National Conference on Artificial Intelligence AAAI-87, </booktitle> <pages> pages 561-565, </pages> <year> 1987. </year>
Reference-contexts: The marker propagation scheme designed by us is by far more complex than the ones previously proposed by Charniak [Cha86], Hendler [Hen88] and Norvig <ref> [Nor87] </ref>. One of the differences is that marker propagations are dependent on the processing of their propagation rules, which interact with the predicate values of the concepts they reach in the knowledge base, as well as with their constraint arguments.
Reference: [Sud93] <author> B. </author> <title> Sundheim. </title> <booktitle> Proceedings. Fifth Message Understanding Conference (MUC-5) August, </booktitle> <year> 1993. </year>
Reference-contexts: In this paper we describe a solution for these problems that uses an encoded common-sense knowledge base which is publicly available. The inferences drawn on such a knowledge base show that human-like text understanding capabilities may be achievable. The evaluation of MUC-3, MUC-4 and MUC-5 results <ref> [Sud93] </ref> demonstrate that higher scores of recall and precision are not achievable without implementing reasoning mechanisms that rely on human common-sense knowledge. We present here our efforts to maximize our system's efficiency by dealing with world knowledge. <p> A large part of this system was developed as an information extraction system for the Message Understanding Conference <ref> [Sud93] </ref>. The current system aims at high level inferences, provided by the last module of the system. The preprocessor locates sentence boundaries and inserts sentence boundary markers for the syntactic parser. Phrase tagging at this point is meant to reduce the parser's work load.
References-found: 16

