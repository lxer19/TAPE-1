URL: http://www.cs.rice.edu:80/~nenad/lcr95.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~nenad/papers.html
Root-URL: 
Title: COMMUNICATION GENERATION FOR CYCLIC(K) DISTRIBUTIONS  
Author: Ken Kennedy, Nenad Nedeljkovic, and Ajay Sethi 
Address: Houston, TX 77005, USA  
Affiliation: Department of Computer Science Rice University,  
Abstract: Communication resulting from references to arrays with general cyclic(k) distributions in data-parallel programs is not amenable to existing analyses developed for block and cyclic distributions. The methods for communication generation presented in this paper are based on exploiting the repetitive nature of array accesses. We represent array access patterns as periodic sequences and use these sequences for efficient communication analysis and code generation. Our approach allows us to incorporate message coalescing optimization and to use overlap areas for shift communication. Experimental results from our prototype implementation demonstrate the validity of the proposed techniques. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Chatterjee, J. Gilbert, F. Long, R. Schreiber, and S. Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In this paper, we present methods for computing communication sets and optimizations that improve the efficiency of the generated code. The starting point for our work are the results by Chatterjee et al. <ref> [1] </ref>. <p> (k B ) distributions: do i = 0, u enddo Chatterjee et al. show that for each reference, the sequence of array elements accessed by any given processor can be computed using a table of local memory gaps whose size does not exceed the block size of the array's distribution <ref> [1] </ref>. Communication Generation for Cyclic (k) Distributions 3 The squares mark the array elements accessed by A (5i) (i 0). In a related work, we present an improved algorithm for constructing the table in time linearly proportional to its size [8]. <p> Assuming that the computation is partitioned using the owner computes rule, the sequence of processors that processor 0 sends data to is PSend (0) = <ref> [0; 2; 2; 0; 1; 2; 0; 2] </ref>. (For any index j in the Global B on a given processor, the corresponding entry in PSend is Owner A (((j l B )=s B ) fi s A + l A ), where Owner A (x) = (x div k A ) <p> Chatterjee et al. show that only one pass over the locally owned elements of array B is needed to build all outgoing messages <ref> [1] </ref>. However, an ownership computation involving integer divisions is performed for each array access, in order to determine the destination processors for the accessed elements. <p> Processor m scans the elements of B it owns using the NextIndex function, which can be implemented using either the table of the local memory gaps between consecutive array accesses <ref> [1] </ref> or our demand-driven address generation method [7]. 6 Chapter 1 index B = start B ; count P = 0 while (index B &lt; end B ) do if (PSend [count P ] 6= m) then fl (buffer [PSend [count P ]])++ = B [index B ] endif count P <p> Neither Chatterjee et al. <ref> [1] </ref> nor Gupta et al. [4] deal with this issue. Stichnoth [9] indicates that combining communication steps would be profitable, but he does not describe the necessary analysis. In contrast, we show how our approach can support message coalescing optimization [5] to reduce the communication cost. <p> In Section 2, we have seen that Global B (7i+2) (0) = [2; 30; 51; 72; 79; 100; 121; 149] with the period 168 and PSend B (7i+2) (0) = <ref> [0; 2; 2; 0; 1; 2; 0; 2] </ref>. Similarly, for B (7i + 5) we can compute Global B (7i+5) (0) = [5; 26; 54; 75; 96; 103; 124; 145] with the period 168, which results in PSend B (7i+5) (0) = [0; 0; 2; 0; 1; 2; 0; 2]. <p> Similarly, for B (7i + 5) we can compute Global B (7i+5) (0) = [5; 26; 54; 75; 96; 103; 124; 145] with the period 168, which results in PSend B (7i+5) (0) = <ref> [0; 0; 2; 0; 1; 2; 0; 2] </ref>.
Reference: [2] <author> R. Das, M. Uysal, J. Saltz, and Y-S. Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <type> Technical Report CS-TR-3163, </type> <institution> Dept. of Computer Science, Univ. of Maryland, College Park, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Our techniques improve on Chatterjee et al.'s FSM approach in several ways. We take advantage of the repetitive memory access patterns to significantly reduce the number of required index translations. Using an idea previously applied in communication generation for irregular problems <ref> [2] </ref>, we allocate overflow areas for non-local array elements which simplifies the generated code. Furthermore, this allows us to execute all loop iterations in the order specified by the original program. <p> In order to compute the sets of data that processor 0 must send to other processors, using the vectors R B and L B we first find the sequence of accesses to array B (in the global index space) that processor 0 owns, Global B (0) = <ref> [2; 30; 51; 72; 79; 100; 121; 149] </ref> with the period 168, and the corresponding iteration sequence Iter B (0) = [0; 4; 7; 10; 11; 14; 17; 21] with the period 24. <p> Assuming that the computation is partitioned using the owner computes rule, the sequence of processors that processor 0 sends data to is PSend (0) = <ref> [0; 2; 2; 0; 1; 2; 0; 2] </ref>. (For any index j in the Global B on a given processor, the corresponding entry in PSend is Owner A (((j l B )=s B ) fi s A + l A ), where Owner A (x) = (x div k A ) <p> do not expect the length of the expanded sequence to exceed the larger of the two block sizes.) After expansion, we get Iter A (0) = [0; 3; 5; 10; 12; 15; 17; 22] with the period 24, and the sequence of corresponding rhs accesses is Image rhs (0) = <ref> [2; 23; 37; 72; 86; 107; 121; 156] </ref> with the period 168. <p> efficient code at the receiving end and find the lengths of the DRecv sequences. 3 CODE GENERATION When generating communication for irregular problems, Das et al. allocate buffers for the non-local data immediately following the space for local array elements and translate the off-processor references to point to buffer addresses <ref> [2] </ref>. Using a similar idea, we allocate the space for the local and non-local rhs references contiguously, but we have a separate, appropriately sized buffer for each processor in PRecv [ PSend. <p> In order be able to pack the messages corresponding to different rhs terms in a single pass over the array, we must find the union of PSend sequences corresponding to different rhs references. In Section 2, we have seen that Global B (7i+2) (0) = <ref> [2; 30; 51; 72; 79; 100; 121; 149] </ref> with the period 168 and PSend B (7i+2) (0) = [0; 2; 2; 0; 1; 2; 0; 2]. <p> In Section 2, we have seen that Global B (7i+2) (0) = [2; 30; 51; 72; 79; 100; 121; 149] with the period 168 and PSend B (7i+2) (0) = <ref> [0; 2; 2; 0; 1; 2; 0; 2] </ref>. Similarly, for B (7i + 5) we can compute Global B (7i+5) (0) = [5; 26; 54; 75; 96; 103; 124; 145] with the period 168, which results in PSend B (7i+5) (0) = [0; 0; 2; 0; 1; 2; 0; 2]. <p> Similarly, for B (7i + 5) we can compute Global B (7i+5) (0) = [5; 26; 54; 75; 96; 103; 124; 145] with the period 168, which results in PSend B (7i+5) (0) = <ref> [0; 0; 2; 0; 1; 2; 0; 2] </ref>. <p> By merging the two Global sequences we get Global B = <ref> [2; 5; 26; 30; : : :; 124; 145; 149] </ref> with the period 168, and the unioned PSend B (0) = [0; 0; 0; 2; 2; 2; 0; 0; 1; 1; 2; 2; 0; 0; 2; 2].
Reference: [3] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: that processor 0 must receive from other processors are computed similarly; using the basis vectors R A and L A we build the table of global indices for accesses made by processor 0, Global A (0) = [0; 15; 25; 50] with the period 60 and Iter A (0) = <ref> [0; 3; 5; 10] </ref> with the period 12. <p> of pk A =gcd (pk A ; s A ) and pk B =gcd (pk B ; s B ), but in practice we do not expect the length of the expanded sequence to exceed the larger of the two block sizes.) After expansion, we get Iter A (0) = <ref> [0; 3; 5; 10; 12; 15; 17; 22] </ref> with the period 24, and the sequence of corresponding rhs accesses is Image rhs (0) = [2; 23; 37; 72; 86; 107; 121; 156] with the period 168. <p> If the arrays are block-distributed, the code can be significantly improved by using the overlap areas <ref> [3] </ref>. The same idea can be applied to the cyclic (k) distribution, but this requires an overlap region for every block of size k.
Reference: [4] <author> S.K.S. Gupta, S.D. Kaushik, C.-H. Huang, and P. Sadayappan. </author> <title> On compiling array expressions for efficient execution on distributed-memory machines. </title> <type> Technical Report OSU-CISRC-4/94-TR19, </type> <institution> Department of Computer and Information Science, The Ohio State University, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: vectors R B and L B we first find the sequence of accesses to array B (in the global index space) that processor 0 owns, Global B (0) = [2; 30; 51; 72; 79; 100; 121; 149] with the period 168, and the corresponding iteration sequence Iter B (0) = <ref> [0; 4; 7; 10; 11; 14; 17; 21] </ref> with the period 24. After multiplying the iteration numbers by the stride s A , we get the global index sequence of corresponding left-hand-side (lhs) accesses, Image lhs (0) = [0; 20; 35; 50; 55; 70; 85; 105] with the period 120. <p> Neither Chatterjee et al. [1] nor Gupta et al. <ref> [4] </ref> deal with this issue. Stichnoth [9] indicates that combining communication steps would be profitable, but he does not describe the necessary analysis. In contrast, we show how our approach can support message coalescing optimization [5] to reduce the communication cost. <p> This allowed us to evaluate the overheads incurred by our techniques, as well as compare them with other existing methods, in particular the virtual processor approach by Gupta et al. <ref> [4] </ref>. Their solution is based on treating a block-cyclic distribution as either a block or cyclic distribution of data onto virtual processors, and mapping of the virtual processors to the physical processors in either a cyclic or block fashion.
Reference: [5] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: that processor 0 must receive from other processors are computed similarly; using the basis vectors R A and L A we build the table of global indices for accesses made by processor 0, Global A (0) = [0; 15; 25; 50] with the period 60 and Iter A (0) = <ref> [0; 3; 5; 10] </ref> with the period 12. <p> of pk A =gcd (pk A ; s A ) and pk B =gcd (pk B ; s B ), but in practice we do not expect the length of the expanded sequence to exceed the larger of the two block sizes.) After expansion, we get Iter A (0) = <ref> [0; 3; 5; 10; 12; 15; 17; 22] </ref> with the period 24, and the sequence of corresponding rhs accesses is Image rhs (0) = [2; 23; 37; 72; 86; 107; 121; 156] with the period 168. <p> Neither Chatterjee et al. [1] nor Gupta et al. [4] deal with this issue. Stichnoth [9] indicates that combining communication steps would be profitable, but he does not describe the necessary analysis. In contrast, we show how our approach can support message coalescing optimization <ref> [5] </ref> to reduce the communication cost. In order be able to pack the messages corresponding to different rhs terms in a single pass over the array, we must find the union of PSend sequences corresponding to different rhs references. <p> Similarly, for B (7i + 5) we can compute Global B (7i+5) (0) = <ref> [5; 26; 54; 75; 96; 103; 124; 145] </ref> with the period 168, which results in PSend B (7i+5) (0) = [0; 0; 2; 0; 1; 2; 0; 2]. <p> By merging the two Global sequences we get Global B = <ref> [2; 5; 26; 30; : : :; 124; 145; 149] </ref> with the period 168, and the unioned PSend B (0) = [0; 0; 0; 2; 2; 2; 0; 0; 1; 1; 2; 2; 0; 0; 2; 2].
Reference: [6] <author> S. D. Kaushik. </author> <title> Private communication, </title> <month> April </month> <year> 1995. </year>
Reference-contexts: Kaushik <ref> [6] </ref>.) The experiments were performed on 32 processors of an Intel iPSC/860 hypercube, using the icc compiler with -O4 optimization level and dclock timer.
Reference: [7] <author> K. Kennedy, N. Nedeljkovic, and A. Sethi. </author> <title> Efficient address generation for block-cyclic distributions. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The content of this paper does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. 1 2 Chapter 1 ods for computing each processor's local addresses for array references with arbitrary affine subscripts <ref> [7] </ref>. In this paper, we present methods for computing communication sets and optimizations that improve the efficiency of the generated code. The starting point for our work are the results by Chatterjee et al. [1]. <p> vectors R B and L B we first find the sequence of accesses to array B (in the global index space) that processor 0 owns, Global B (0) = [2; 30; 51; 72; 79; 100; 121; 149] with the period 168, and the corresponding iteration sequence Iter B (0) = <ref> [0; 4; 7; 10; 11; 14; 17; 21] </ref> with the period 24. After multiplying the iteration numbers by the stride s A , we get the global index sequence of corresponding left-hand-side (lhs) accesses, Image lhs (0) = [0; 20; 35; 50; 55; 70; 85; 105] with the period 120. <p> Not all the tables shown here need to be actually allocated and assigned values. The initial array access sequences for either reference can be traversed using the demand-driven address generation based on the vectors R and L <ref> [7] </ref>. The only table essential for packing outgoing messages is PSend since it eliminates the need for computing the owner processor for every array reference. In addition, this table can be used to compute the lengths of the DSend sequences. <p> Processor m scans the elements of B it owns using the NextIndex function, which can be implemented using either the table of the local memory gaps between consecutive array accesses [1] or our demand-driven address generation method <ref> [7] </ref>. 6 Chapter 1 index B = start B ; count P = 0 while (index B &lt; end B ) do if (PSend [count P ] 6= m) then fl (buffer [PSend [count P ]])++ = B [index B ] endif count P = (count P + 1) mod Length
Reference: [8] <author> K. Kennedy, N. Nedeljkovic, and A. Sethi. </author> <title> A linear-time algorithm for computing the memory access sequence in data-parallel programs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Communication Generation for Cyclic (k) Distributions 3 The squares mark the array elements accessed by A (5i) (i 0). In a related work, we present an improved algorithm for constructing the table in time linearly proportional to its size <ref> [8] </ref>. <p> In contrast, we do not need to unpack the received messages, but our approach requires table lookups in the loop. Since the overheads due to indirect addressing are small <ref> [8] </ref>, the loop execution in our scheme is faster than with the virtual processor method, except in the case when virtual-cyclic view was selected at both the sending and the receiving side.
Reference: [9] <author> J. Stichnoth. </author> <title> Efficient compilation of array statements for private memory mul-ticomputers. </title> <type> Technical Report CMU-CS-93-109, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: While this technique may lead to good data locality, especially during message packing, its disadvantage, as Stichnoth <ref> [9] </ref> points out, is that one ownership computation is required for each array access, and expensive integer divisions can incur substantial overhead. Our techniques improve on Chatterjee et al.'s FSM approach in several ways. <p> Neither Chatterjee et al. [1] nor Gupta et al. [4] deal with this issue. Stichnoth <ref> [9] </ref> indicates that combining communication steps would be profitable, but he does not describe the necessary analysis. In contrast, we show how our approach can support message coalescing optimization [5] to reduce the communication cost.
References-found: 9

