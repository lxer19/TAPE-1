URL: http://www.cs.umn.edu/Users/dept/users/kumar/vineet.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: Parallel Formulations of Inductive Classification Learning Algorithm  
Author: Vineet Singh Eui-Hong (Sam) Han, Anurag Srivastava, Vipin Kumar 
Keyword: Data mining, parallel processing, classification, decision trees.  
Address: 4-192 EECS Bldg., 200 Union St. SE  Minneapolis, MN 55455, USA  
Affiliation: IBM T.J. Watson Research Center  Dept. of Computer Information Sciences  University of Minnesota  
Abstract: One of the important problems in data mining [SAD + 93] is the classification-rule learning. The classification-rule learning involves finding rules or decision trees that partition given data into predefined classes. For any realistic problem domain of the classification-rule learning, the set of possible decision trees is too large to be searched exhaustively. In fact, the computational complexity of finding an optimal classification decision tree is N P - hard. All of the existing algorithms, like C4:5 [Qui93], CDP [AIS93] and SLIQ [MAR96], use local heuristics to handle the computational complexity. The computational complexity of these algorithms ranges from O(AN logN ) to O(AN (logN ) 2 ) with N training data items and A attributes. These algorithms are fast enough for application domains where N is relatively small. However, in the data mining domain where millions of records and a large number of attributes are involved, the execution time of these algorithms can become prohibitive, particularly in interactive applications. In this paper, we present parallel formulations of classification-rule-learning algorithm based on induction. We describe two basic parallel formulation, one is based on Synchronous Tree Construction Approach and the other is based on Partitioned Tree Construction Approach. We discuss the advantages and disadvantages of using these methods and propose a hybrid method that employs the good features of these methods. We also provide the analysis of the cost of computation and communication of the proposed hybrid method. 
Abstract-found: 1
Intro-found: 1
Reference: [AIS93] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engg., </journal> <volume> 5(6) </volume> <pages> 914-925, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: For any realistic problem domain of the classification-rule learning, the set of possible decision trees is too large to be searched exhaustively. In fact, the computational complexity of finding an optimal classification decision tree is NP hard. All of the existing algorithms, like C4:5 [Qui93], CDP <ref> [AIS93] </ref> and SLIQ [MAR96], use local heuristics to handle the computational complexity. The computational complexity of these algorithms ranges from O (AN logN) to O (AN (logN ) 2 ) with N training data items and A attributes. <p> Section 3 describes the two basic parallel formulation. The hybrid method is described in Section 4 and its analysis is shown in Section 5. Section 6 presents our conclusions. 2 Sequential Classification Rule Learning Algorithms Most of the existing induction-based algorithms like C4:5 [Qui93], CDP <ref> [AIS93] </ref> and SLIQ [MAR96], use Hunt's method [Qui93] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g.
Reference: [BFOS84] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, Monterrey, </publisher> <address> CA, </address> <year> 1984. </year>
Reference-contexts: Table 3 shows the class distribution information of data attribute Humidity. Once the class distribution information of all the attributes are gathered, the entropy is calculated based on either information theory [Qui93] or Gini Index <ref> [BFOS84] </ref>. One attribute with the most entropy gain is selected as a test for the node expansion. The C4:5 algorithm generates a classification-decision tree for the given training data set by recursive partitioning of the data. The decision tree is grown using depth-first strategy.
Reference: [Hon94] <author> S.J. Hong. </author> <title> Use of contextual information for feature ranking and discretization. </title> <type> Technical Report RC 19664, </type> <institution> IBM Research Division, </institution> <year> 1994. </year>
Reference: [KGGK94] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Algorithm Design and Analysis. </title> <publisher> Benjamin Cummings/ Addison Wesley, </publisher> <address> Redwod City, </address> <year> 1994. </year>
Reference-contexts: Discrete and continuous attributes are treated differently. For discrete attributes, there are three steps. In the first step, every processor collects the class distribution information of the local data for the discrete attributes only. In the second step, the processors exchange the local class distribution information using global reduction <ref> [KGGK94] </ref>. Finally, each processor can simultaneously compute entropy gains of the discrete attributes and find the best discrete attribute. For continuous attributes, there are three steps for each attribute. <p> attributes in the database C Number of classes in the database M Average number of distinct values in the discrete attributes S Average number of distinct values in the continuous attributes L Level of a decision tree t c Unit computation time t s Start up time of communication latency <ref> [KGGK94] </ref> t w Per-word transfer time of communication latency [KGGK94] Table 4: Symbols used in the analysis. 4.2 Reducing Communication Costs A key element of the algorithm is the criterion that triggers the partitioning of the current set of processors ( and the corresponding frontier of the decision tree ). <p> the database M Average number of distinct values in the discrete attributes S Average number of distinct values in the continuous attributes L Level of a decision tree t c Unit computation time t s Start up time of communication latency <ref> [KGGK94] </ref> t w Per-word transfer time of communication latency [KGGK94] Table 4: Symbols used in the analysis. 4.2 Reducing Communication Costs A key element of the algorithm is the criterion that triggers the partitioning of the current set of processors ( and the corresponding frontier of the decision tree ). <p> We provide a separate analysis for two different data attribute types, discrete and continuous. The analysis of the mixture of discrete and continuous attributes could be done easily based on the analysis provided here. The detailed study of the communication patterns used in this analysis can be found in <ref> [KGGK94] </ref>. Table 4 describes the symbols used in this section. 11 5.1 Case with Discrete Attributes Only Here we give a detailed analysis of splitting for the case when only discrete attributes are present. At each leaf of a level, there are A attribute tables that need to be communicated. <p> Refer to <ref> [KGGK94] </ref> section 3.7 for details. 12 Note that before the load balancing step, the number of items at each processor range from 0 to 2 fl N P and the average number of items desired at the end at each processor is N P . <p> Using parallel sample sort <ref> [KGGK94] </ref>, the overall computation cost of step 4 is: 2 L A (B log B + P (log P ) 2 + P log B) and the overall communication cost is: 2 L A (B + P log P ) The size of the communication buffer for step 9 is 2
Reference: [KK92] <author> G. Karypis and V. Kumar. </author> <title> Unstructured tree search on SIMD parallel computers. </title> <type> Technical Report TR-92-021, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1992. </year>
Reference: [MAR96] <author> M. Mehta, R. Agrawal, and J. Rissanen. SLIQ: </author> <title> A fast scalable classifier for data mining. </title> <booktitle> In Proc. of the Fifth Int'l Conference on Extending Database Technology, </booktitle> <address> Avignon, France, </address> <year> 1996. </year>
Reference-contexts: For any realistic problem domain of the classification-rule learning, the set of possible decision trees is too large to be searched exhaustively. In fact, the computational complexity of finding an optimal classification decision tree is NP hard. All of the existing algorithms, like C4:5 [Qui93], CDP [AIS93] and SLIQ <ref> [MAR96] </ref>, use local heuristics to handle the computational complexity. The computational complexity of these algorithms ranges from O (AN logN) to O (AN (logN ) 2 ) with N training data items and A attributes. These algorithms are fast enough for application domains where N is relatively small. <p> Section 3 describes the two basic parallel formulation. The hybrid method is described in Section 4 and its analysis is shown in Section 5. Section 6 presents our conclusions. 2 Sequential Classification Rule Learning Algorithms Most of the existing induction-based algorithms like C4:5 [Qui93], CDP [AIS93] and SLIQ <ref> [MAR96] </ref>, use Hunt's method [Qui93] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g.
Reference: [Qui93] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year> <month> 16 </month>
Reference-contexts: For any realistic problem domain of the classification-rule learning, the set of possible decision trees is too large to be searched exhaustively. In fact, the computational complexity of finding an optimal classification decision tree is NP hard. All of the existing algorithms, like C4:5 <ref> [Qui93] </ref>, CDP [AIS93] and SLIQ [MAR96], use local heuristics to handle the computational complexity. The computational complexity of these algorithms ranges from O (AN logN) to O (AN (logN ) 2 ) with N training data items and A attributes. <p> Section 3 describes the two basic parallel formulation. The hybrid method is described in Section 4 and its analysis is shown in Section 5. Section 6 presents our conclusions. 2 Sequential Classification Rule Learning Algorithms Most of the existing induction-based algorithms like C4:5 <ref> [Qui93] </ref>, CDP [AIS93] and SLIQ [MAR96], use Hunt's method [Qui93] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g. <p> The hybrid method is described in Section 4 and its analysis is shown in Section 5. Section 6 presents our conclusions. 2 Sequential Classification Rule Learning Algorithms Most of the existing induction-based algorithms like C4:5 <ref> [Qui93] </ref>, CDP [AIS93] and SLIQ [MAR96], use Hunt's method [Qui93] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g. <p> Play overcast 83 78 false Play overcast 64 65 true Play overcast 81 75 false Play rain 71 80 true Don't Play rain 65 70 true Don't Play rain 75 80 false Play rain 68 80 false Play rain 70 96 false Play Table 1: A small training data set <ref> [Qui93] </ref> 2 Attribute Value Class Play Don't Play sunny 2 3 overcast 4 0 rain 3 2 Table 2: Class Distribution Information of Attribute Outlook Attribute Value Binary Test Class Play Don't Play 65 1 0 70 3 1 75 4 1 78 5 1 80 7 2 85 7 3 <p> For a continuous attribute, binary test involving all the distinct values of the attribute is considered. Table 3 shows the class distribution information of data attribute Humidity. Once the class distribution information of all the attributes are gathered, the entropy is calculated based on either information theory <ref> [Qui93] </ref> or Gini Index [BFOS84]. One attribute with the most entropy gain is selected as a test for the node expansion. The C4:5 algorithm generates a classification-decision tree for the given training data set by recursive partitioning of the data. The decision tree is grown using depth-first strategy.
Reference: [SAD + 93] <author> M. Stonebraker, R. Agrawal, U. Dayal, E. J. Neuhold, and A. Reuter. </author> <title> DBMS research at a crossroads: The vienna update. </title> <booktitle> In Proc. of the 19th VLDB Conference, </booktitle> <pages> pages 688-692, </pages> <address> Dublin, Ireland, </address> <year> 1993. </year> <month> 17 </month>
Reference-contexts: 1 Introduction One of the important problems in data mining <ref> [SAD + 93] </ref> is the classification-rule learning. The classification-rule learning involves finding rules or decision trees that partition given data into predefined classes. For any realistic problem domain of the classification-rule learning, the set of possible decision trees is too large to be searched exhaustively.
References-found: 8

