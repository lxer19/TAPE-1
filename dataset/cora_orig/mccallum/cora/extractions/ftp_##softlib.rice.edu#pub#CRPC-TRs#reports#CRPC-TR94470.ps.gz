URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94470.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Derivative-Free Pattern Search Methods for Multidisciplinary Design Problems  
Author: J. E. Dennis* Virginia J. Torczon 
Address: Houston, Texas 77251-1892  
Affiliation: Rice University  Department of Computational and Applied Mathematics  
Abstract: There have been interesting recent developments in methods for solving optimization problems without making use of derivative (sensitivity) information. While calculus based methods that employ derivative information can be extremely efficient and very effective, they are not applicable to all MDO problems, for instance, when the function to be optimized is nondifferentiable, when sensitivity information is not available or is not reliable, or when the function values are inaccurate. In these settings, we have found that the multidirectional search method, a derivate-free method we have developed for solving nonlinear optimization problems, can be used effectively. Our analysis of the multidirectional search algorithm has led us to discover that its algebraic structure and resulting convergence theory can be related to an entire class of derivative-free methods, which we now call pattern search methods, that have been in use for decades. The goal of this paper is to give an introduction to pattern search methods, to describe the features they share by using coordinate search, one of the earliest and best-known (if not as effective) pattern search methods, as an example, and to review some recent developments that suggest that these methods can be extended to handle problems with a mix of continuous and discrete variables|another situation that can arise in MDO problems. Finally, we will discuss when these methods are an appropriate choice for solving MDO problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Mordecai Avriel. </author> <title> Nonlinear Programming: Analysis and Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1976. </year>
Reference-contexts: Second, this bound also deteriorates as the dimension n of the problem increases. This certainly would explain, at least in part, the long-standing observation that the effectiveness of pattern search methods tends to decrease as the dimension of the problem increases <ref> [1] </ref>.
Reference: [2] <author> G. E. P. </author> <title> Box. The exploration and exploitation of response surfaces: Some general considerations and examples. </title> <journal> Biometrics, </journal> <volume> 10 </volume> <pages> 16-60, </pages> <month> March </month> <year> 1954. </year>
Reference-contexts: This loose definition of pattern search methods captures a variety of other long-standing direct search methods. These include optimization methods based on the response surface methodology work first introduced by G. E. P. Box and K. B. Wilson [4] and subsequently developed by Box and other researchers <ref> [2, 3, 5] </ref>, the original pattern search algorithm of Robert Hooke and T. A.
Reference: [3] <author> G. E. P. </author> <title> Box. Evolutionary operation: A method for increasing industrial productivity. </title> <journal> Appl. Statist., </journal> <volume> 6(2) </volume> <pages> 81-101, </pages> <month> June </month> <year> 1957. </year> <month> 9 </month>
Reference-contexts: This loose definition of pattern search methods captures a variety of other long-standing direct search methods. These include optimization methods based on the response surface methodology work first introduced by G. E. P. Box and K. B. Wilson [4] and subsequently developed by Box and other researchers <ref> [2, 3, 5] </ref>, the original pattern search algorithm of Robert Hooke and T. A.
Reference: [4] <author> G. E. P. Box and K. B. Wilson. </author> <title> On the ex-perimental attainment of optimum conditions. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> XIII(1):1-45, </volume> <year> 1951. </year>
Reference-contexts: If we consider the pattern of points from which the function can be sampled (shown in Figure 1 for problems with only two 2 variables), we see something that looks much like a two-factor composite design as described by G. E. P. Box and K. B. Wilson <ref> [4] </ref> on the use of experimental designs to attain optimum conditions. r r r r r 6 search in IR 2 . In fact, the patterns associated with pattern search methods share many features with some of the early orthogonal designs suggested for experimental design. <p> This loose definition of pattern search methods captures a variety of other long-standing direct search methods. These include optimization methods based on the response surface methodology work first introduced by G. E. P. Box and K. B. Wilson <ref> [4] </ref> and subsequently developed by Box and other researchers [2, 3, 5], the original pattern search algorithm of Robert Hooke and T. A.
Reference: [5] <author> M. J. Box, D. Davies, and W. H. Swann. </author> <title> NonLinear Optimization Techniques. </title> <journal> ICI Monograph No. </journal> <volume> 5. </volume> <publisher> Oliver & Boyd, Edinburgh, </publisher> <year> 1969. </year>
Reference-contexts: This loose definition of pattern search methods captures a variety of other long-standing direct search methods. These include optimization methods based on the response surface methodology work first introduced by G. E. P. Box and K. B. Wilson [4] and subsequently developed by Box and other researchers <ref> [2, 3, 5] </ref>, the original pattern search algorithm of Robert Hooke and T. A.
Reference: [6] <author> William C. Davidon. </author> <title> A belated preface for ANL 5990. </title> <journal> SIAM J. Optimization, </journal> <volume> 1(1) </volume> <pages> 1-17, </pages> <note> Febru-ary 1991. The belated preface was received by the editors June 4, 1990; accepted for publication August 10, </note> <year> 1990. </year> <title> The rest of the article was originally published as "Variable Metric Method for Minimization," </title> <institution> Argonne National Laboratory Research and Development Report 5990, </institution> <month> May </month> <year> 1959 </year> <month> (revised November </month> <year> 1959). </year>
Reference-contexts: To simplify the discussion, we will assume that S = IR n (i.e., the problem is unconstrained), though computational efforts have been directed at extending these methods in a reliable fashion to handle problems with constraints [15]. Pattern search methods date back to at least the 1950s <ref> [6] </ref> and have been used for parameter estimation in a wide variety of scientific and engineering applications. <p> consider the method of coordinate search with fixed step length, perhaps the simplest and most obvious of the pattern search methods (used, for example, forty years ago by Enrico Fermi and Nicholas Metropolis to determine which values of certain theoretical parameters (phase shifts) best fit experimental data (scattering cross sections) <ref> [6] </ref>). This simple idea conforms with classical notions of good experimental design: vary one factor at a time and observe the effect of that variation on the result of the associated experiment.
Reference: [7] <author> J. E. Dennis, Jr. and Robert B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1983. </year>
Reference-contexts: The terminology is somewhat unfortunate in that convergence to a global minimizer of the function is not implied. However, "locally convergent" is reserved by tradition for another use <ref> [7] </ref>. The so-called global convergence theory for classical unconstrained minimization methods based on Taylor's series approximations, such as the method of steepest-descent or Newton's method, have been well-studied and are well-understood.
Reference: [8] <author> J. E. Dennis, Jr. and Virginia Torczon. </author> <title> Direct search methods on parallel machines. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(4) </volume> <pages> 448-474, </pages> <month> Novem-ber </month> <year> 1991. </year>
Reference-contexts: Our investigation of pattern search methods was prompted by our interest in developing useful general-purpose parallel optimization methods <ref> [8, 21] </ref>. However, during the course of our investigations we discovered that pattern search methods have many important features that are independent of the computing environment in which they are used. These are the features we will discuss in this paper. <p> E. P. Box and K. B. Wilson [4] and subsequently developed by Box and other researchers [2, 3, 5], the original pattern search algorithm of Robert Hooke and T. A. Jeeves [14], and more recently the multidirectional search algorithm and its variants developed by the authors <ref> [8, 19, 20] </ref>. 2.2 A Formal Abstraction The analysis of the pattern search methods is based on a general unifying abstraction. A simplified version of this abstraction is given here. A complete version, with the accompanying analysis, can be found in [22]. <p> While 8 our examples centered around coordinate search, be-cause it is the simplest and probably best-known of the pattern search methods, our comments on the effectiveness of pattern search methods are based on our extensive use of the more effective multidirec-tional search algorithm and its parallel variants <ref> [8, 20, 21, 22] </ref>. We begin by noting that there is no question but that higher-order methods, when they are applicable, are generally more efficient in their use of function values than pattern search methods.
Reference: [9] <author> Samuel K. Eldersveld. </author> <title> Part nesting for just-in-time manufacturing. TechNet: </title> <journal> Briefs on Computing Technology, </journal> <volume> 7(3) </volume> <pages> 7-8, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The advantage of pattern search methods is that while they are gradient-related, they do not rely explicitly on the gradient and they consider multiple directions. This may not be as efficient, but it can be much more reliable in such settings <ref> [9] </ref>. Pattern search methods can also be used for non-differentiable functions [13].
Reference: [10] <author> P. D. Frank, A. J. Booker, T. P. Caudell, and M. J. Healy. </author> <title> A comparison of optimization and search methods for multidisciplinary design. </title> <booktitle> In Proceedings of the Fourth AIAA/USAF/NASA/OAI Symposium on Multidisciplinary Analysis and Optimization, </booktitle> <address> Cleve-land, OH, </address> <month> September 21-23, </month> <year> 1992. </year>
Reference-contexts: Because the pattern search methods are direct search methods (i.e., methods that neither require nor estimate derivatives to drive the search procedure but rely solely on function values), they are not as efficient as higher-order methods can be when using reliable sensitivity information <ref> [10] </ref>. Furthermore, because pattern search methods can be viewed as sampling methods, they suffer from the so-called "curse of dimensionality," though they have been used successfully on problems with up to twenty parameters [11]. <p> The challenge when analyzing pattern search methods using a similar approach is that we have no explicit approximation of the gradient with which to work. As it turns out, one of the surprising results <ref> [10] </ref> is that pattern search methods can be viewed as gradient-related methods. Thus, some of the analysis techniques made possible by the use of Taylor's series approximations can by applied.
Reference: [11] <author> Roland Glowinski and Anthony J. Kearsley. </author> <title> On the simulation and control of some friction constrained motions. </title> <type> Technical Report 93-21, </type> <institution> Department of Computational and Applied Mathematics, Rice University, Houston, Texas 77251-1892, </institution> <year> 1993. </year> <note> To appear in SIAM Journal on Optimization. </note>
Reference-contexts: Furthermore, because pattern search methods can be viewed as sampling methods, they suffer from the so-called "curse of dimensionality," though they have been used successfully on problems with up to twenty parameters <ref> [11] </ref>. The purpose of this paper is to define pattern search methods and their many features while giving some guidelines as to when their use is most appropriate. 2. Pattern Search Methods We have yet to define pattern search methods. <p> Our more recent efforts have focussed on developing effective extensions to the multidirectional search algorithm and its parallel variants [15] and monitoring the applications of these extensions to optimization problems based on engineering simulations <ref> [11, 12] </ref>. We are also investigating the use of the theory to guide the development of additional robust variants to handle constraints.
Reference: [12] <author> Roland Glowinski, Anthony J. Kearsley, Tsorn-Whay Pan, and Jacques Peraux. </author> <title> Numerical simulation and optimal shape for viscous flow by a fictitious domain method. </title> <note> Special Edition: The International Journal of Numerical Methods in Engineering, 1994. To appear. </note>
Reference-contexts: Our more recent efforts have focussed on developing effective extensions to the multidirectional search algorithm and its parallel variants [15] and monitoring the applications of these extensions to optimization problems based on engineering simulations <ref> [11, 12] </ref>. We are also investigating the use of the theory to guide the development of additional robust variants to handle constraints.
Reference: [13] <author> Nicholas J. Higham. </author> <title> Optimization by direct search in matrix computation. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 14(2) </volume> <pages> 317-333, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: This may not be as efficient, but it can be much more reliable in such settings [9]. Pattern search methods can also be used for non-differentiable functions <ref> [13] </ref>.
Reference: [14] <author> Robert Hooke and T. A. Jeeves. </author> <title> "Direct search" solution of numerical and statistical problems. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 8(2) </volume> <pages> 212-229, </pages> <month> April </month> <year> 1961. </year>
Reference-contexts: These include optimization methods based on the response surface methodology work first introduced by G. E. P. Box and K. B. Wilson [4] and subsequently developed by Box and other researchers [2, 3, 5], the original pattern search algorithm of Robert Hooke and T. A. Jeeves <ref> [14] </ref>, and more recently the multidirectional search algorithm and its variants developed by the authors [8, 19, 20]. 2.2 A Formal Abstraction The analysis of the pattern search methods is based on a general unifying abstraction. A simplified version of this abstraction is given here.
Reference: [15] <author> A. J. Kearsley, R. A. Tapia, and V. Torczon. </author> <title> On the use of parallel direct search methods for nonlinear programming problems. </title> <type> Technical Report 93-33, </type> <institution> Department of Computational and Applied Mathematics, Rice University, </institution> <address> Houston, TX 77251-1892, </address> <year> 1993. </year> <note> In revision. </note>
Reference-contexts: To simplify the discussion, we will assume that S = IR n (i.e., the problem is unconstrained), though computational efforts have been directed at extending these methods in a reliable fashion to handle problems with constraints <ref> [15] </ref>. Pattern search methods date back to at least the 1950s [6] and have been used for parameter estimation in a wide variety of scientific and engineering applications. <p> Our more recent efforts have focussed on developing effective extensions to the multidirectional search algorithm and its parallel variants <ref> [15] </ref> and monitoring the applications of these extensions to optimization problems based on engineering simulations [11, 12]. We are also investigating the use of the theory to guide the development of additional robust variants to handle constraints.
Reference: [16] <author> J. C. Meza and M. L. Martinez. </author> <title> Direct search methods for the molecular conformation problem. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 15(6) </volume> <pages> 627-632, </pages> <year> 1994. </year>
Reference-contexts: Good arguments can also be made for their effectiveness when a problem has many local minimizers <ref> [16] </ref>. This is because pattern search methods search in multiple directions, some of which may be "up-hill" directions, rather than limiting the search to a single descent (down-hill) direction, as is more typical of classical higher-order methods based on Taylor's series approximations to the function to be minimized. <p> nondifferentiable, or, for technical reasons, a point where the gradient exists but is not continuous.) Another feature of pattern search methods is that because they search in multiple directions, some of which may be up-hill directions, they can be useful in situations where there are a great many local minimizers <ref> [16] </ref>. While we can give no theoretical justification for this claim, this is a useful feature of pattern search methods that distinguishes them from methods based on higher-order information. Pattern search methods also make useful exploratory tools, even when it is possible to obtain higher-order information.
Reference: [17] <author> J. A. Nelder and R. Mead. </author> <title> A simplex method for function minimization. </title> <journal> Comput. J., </journal> <volume> 7(4) </volume> <pages> 308-313, </pages> <month> January </month> <year> 1965. </year>
Reference-contexts: Our own work for the multidirectional search algorithm is based on the use of simplex designs first proposed by Spendley, Hext, and Himsworth [18] and subsequently developed by Nelder and Mead <ref> [17] </ref>. (See Figure 2 for a possible pattern for problems with only two variables.) Curiously, neither the simplex r r r tional search in IR 2 . algorithm of Spendley, Hext and Himsworth nor the simplex method of Nelder and Mead satisfy the strict definition of a pattern search method.
Reference: [18] <author> W. Spendley, G. R. Hext, and F. R. Himsworth. </author> <title> Sequential application of simplex designs in op-timisation and evolutionary operation. </title> <journal> Techno-metrics, </journal> <volume> 4(4) </volume> <pages> 441-461, </pages> <month> November </month> <year> 1962. </year>
Reference-contexts: In fact, the patterns associated with pattern search methods share many features with some of the early orthogonal designs suggested for experimental design. Our own work for the multidirectional search algorithm is based on the use of simplex designs first proposed by Spendley, Hext, and Himsworth <ref> [18] </ref> and subsequently developed by Nelder and Mead [17]. (See Figure 2 for a possible pattern for problems with only two variables.) Curiously, neither the simplex r r r tional search in IR 2 . algorithm of Spendley, Hext and Himsworth nor the simplex method of Nelder and Mead satisfy the
Reference: [19] <author> Virginia Torczon. </author> <title> Multi-Directional Search: A Direct Search Algorithm for Parallel Machines. </title> <type> PhD thesis, </type> <institution> Department of Mathematical Sciences, Rice University, Houston, TX, </institution> <note> 1989; also available as Tech. Report 90-7, </note> <institution> Department of Computational and Applied Mathematics, Rice University, </institution> <address> Houston, TX 77251-1892. </address>
Reference-contexts: With suitable modifications, the analysis for pattern search methods can be extended to the simplex algorithm of Spendley, Hext, and Himsworth. However, similar extensions cannot be made for the simplex algorithm of Nelder and Mead and numerical experiments show that the Nelder-Mead algorithm can fail on trivial problems <ref> [19] </ref>. <p> E. P. Box and K. B. Wilson [4] and subsequently developed by Box and other researchers [2, 3, 5], the original pattern search algorithm of Robert Hooke and T. A. Jeeves [14], and more recently the multidirectional search algorithm and its variants developed by the authors <ref> [8, 19, 20] </ref>. 2.2 A Formal Abstraction The analysis of the pattern search methods is based on a general unifying abstraction. A simplified version of this abstraction is given here. A complete version, with the accompanying analysis, can be found in [22].
Reference: [20] <author> Virginia Torczon. </author> <title> On the convergence of the multidirectional search algorithm. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(1) </volume> <pages> 123-145, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: E. P. Box and K. B. Wilson [4] and subsequently developed by Box and other researchers [2, 3, 5], the original pattern search algorithm of Robert Hooke and T. A. Jeeves [14], and more recently the multidirectional search algorithm and its variants developed by the authors <ref> [8, 19, 20] </ref>. 2.2 A Formal Abstraction The analysis of the pattern search methods is based on a general unifying abstraction. A simplified version of this abstraction is given here. A complete version, with the accompanying analysis, can be found in [22]. <p> While 8 our examples centered around coordinate search, be-cause it is the simplest and probably best-known of the pattern search methods, our comments on the effectiveness of pattern search methods are based on our extensive use of the more effective multidirec-tional search algorithm and its parallel variants <ref> [8, 20, 21, 22] </ref>. We begin by noting that there is no question but that higher-order methods, when they are applicable, are generally more efficient in their use of function values than pattern search methods.
Reference: [21] <author> Virginia Torczon. </author> <title> PDS: Direct search methods for unconstrained optimization on either sequential or parallel machines. </title> <type> Technical Report 92-9, </type> <institution> Department of Computational and Applied Mathematics, Rice University, Houston, Texas 77251-1892, </institution> <year> 1992. </year> <note> In revision; to appear in acm Transactions on Mathematical Software. </note>
Reference-contexts: Our investigation of pattern search methods was prompted by our interest in developing useful general-purpose parallel optimization methods <ref> [8, 21] </ref>. However, during the course of our investigations we discovered that pattern search methods have many important features that are independent of the computing environment in which they are used. These are the features we will discuss in this paper. <p> While 8 our examples centered around coordinate search, be-cause it is the simplest and probably best-known of the pattern search methods, our comments on the effectiveness of pattern search methods are based on our extensive use of the more effective multidirec-tional search algorithm and its parallel variants <ref> [8, 20, 21, 22] </ref>. We begin by noting that there is no question but that higher-order methods, when they are applicable, are generally more efficient in their use of function values than pattern search methods.
Reference: [22] <author> Virginia Torczon. </author> <title> On the convergence of pattern search algorithms. </title> <type> Technical Report 93-10, </type> <institution> Department of Computational and Applied Mathematics, Rice University, Houston, Texas 77251-1892, </institution> <year> 1993. </year> <note> In revision; to appear in SIAM Journal on Optimization. </note>
Reference-contexts: Second, recent theoretical developments <ref> [22] </ref> show that given a precise definition of pattern search methods these methods can be related analytically to classical higher-order methods based on a Taylor's series approximation to the function to be minimized. This analysis is significant for several reasons. <p> A simplified version of this abstraction is given here. A complete version, with the accompanying analysis, can be found in <ref> [22] </ref>. The two key components of a pattern search method are the generating matrix and the exploratory moves algorithm. <p> The columns of A k may or may not vary across iterations; however, A k always contains at least one column|the column of all zeros. There are certain restrictions, discussed fully in <ref> [22] </ref>, on the form of the additional columns A k , but there is a great deal of flexibility in the choice of these columns. <p> for the pattern given in Figure 1 is C k = (2) 1 0 1 0 1 1 1 1 0 # while the one associated with the pattern given in C k = 1 0 1 0 0 # Again we suppress some of the many nuances found in <ref> [22] </ref> to simplify the presentation. In its most basic form, the pattern can be represented by the generating matrix C k . <p> There is some flexibility in the way that the step length is updated from iteration to iteration, but again certain restrictions must be satisfied (see <ref> [22] </ref>). In fact, the multidirectional search algorithm of Den-nis and Torczon also allows the step size to be in creased if an iteration has been successful. But, in general, a very simple update algorithm (Algorithm 2) is used. Here the critical point is that the size Algorithm 2. <p> The Convergence Theory We have tried to demonstrate, albeit with a very simple example, that there is a great deal of flexibility for deriving pattern search methods within the restrictions imposed by the definition for pattern search methods found in <ref> [22] </ref>. However, it is important to stress that these restrictions are not arbitrary. The goal is to try to define methods that are robust. Within the numerical optimization community, "robustness" is usually considered analogous to first-order stationary point convergence. <p> Thus, some of the analysis techniques made possible by the use of Taylor's series approximations can by applied. If we assume that the function to be optimized is continuously differentiable, even though no explicit approximation to the directional derivative is constructed, it can be shown <ref> [22] </ref> that the sequence of iterates satisfies both that f (x k+1 ) f (x k ) and that lim inf krf (x k )k = 0: In other words, we can show weak first-order stationary point convergence for these methods. <p> In particular, it gives us a nonzero uniform lower bound on the cosine of the angle between the direction of descent contained in F and the (negative) gradient at x k whenever rf (x k ) 6= 0. We suppress many of the details in this discussion (see <ref> [22] </ref>), but in effect it can be shown that if is the angle between the gradient and a guaranteed direction of descent, then jcos j 1 p ; where () is the condition number of the core matrix . <p> For the simple examples we have shown, the core matrix is the identity matrix, so that () = 1. Thus, for the coordinate search algorithm we have presented, it can be shown <ref> [22] </ref> that jcos j 1 n When n = 2, even in the worst case (shown in Figure 5), at least one of the search directions defined by k F must be within 45 ffi of the negative gradient, a claim easily verified by inspection for this example. <p> While 8 our examples centered around coordinate search, be-cause it is the simplest and probably best-known of the pattern search methods, our comments on the effectiveness of pattern search methods are based on our extensive use of the more effective multidirec-tional search algorithm and its parallel variants <ref> [8, 20, 21, 22] </ref>. We begin by noting that there is no question but that higher-order methods, when they are applicable, are generally more efficient in their use of function values than pattern search methods.
Reference: [23] <author> Garret N. Vanderplaats. </author> <title> Numerical Optimization Techniques for Engineering Design: With Applications. </title> <publisher> McGraw-Hill, Inc., </publisher> <year> 1984. </year> <month> 11 </month>
Reference-contexts: They have remained popular because they are simple, easy to understand, easy to program, widely applicable, and have proven to be robust in practice <ref> [23] </ref>, where by "robust" we mean that the sequence of iterates produced typically converges to a stationary point of the function to be optimized. Our investigation of pattern search methods was prompted by our interest in developing useful general-purpose parallel optimization methods [8, 21].
References-found: 23

