URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-2/schneide/psfiles/iros95.ps
Refering-URL: http://www.cs.cmu.edu/~schneide/papers.html
Root-URL: 
Title: Cooperative Coaching in Robot Learning  
Author: Jeff G. Schneider Christopher M. Brown 
Address: Pittsburgh, PA 15213 Rochester, NY 14627  
Affiliation: Robotics Institute Computer Science Dept. Carnegie Mellon University University of Rochester  
Abstract: Many closed loop learning algorithms perform gradient descent on a cost function with respect to the parameters of a learning controller. We observe that both local closed loop learners, which consider only the cost of the current time step, and optimal control based closed loop learners, which consider the future effects of control actions, can become stuck in sub-optimal local minima in the cost function. We propose the use of "Cooperating Coaches" to deal with this problem. Each coach attempts gradient descent based on its own cost function and they work together to avoid getting stuck in local minima. When one coach has achieved the best result it can (the gradient for its cost function is zero), another coach takes over to guide the search through the parameter space. We demonstrate cooperative coaching on the problem of curve tracking with an inverted pendulum and show that it yields faster, smoother tracking of target curves by combining the best aspects of two different coaches. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Barto, R. Sutton, and C. Anderson. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <year> 1983. </year>
Reference: [2] <author> R. Jacobs, M. Jordan, S. Nowlan, and G. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <booktitle> Neural Compuation, </booktitle> <year> 1991. </year>
Reference-contexts: One possibility is to select the one with the best overall performance. Another is to select different controllers dynamically during execution for different reference curves. This is similar to the "mixture of experts" idea in classification <ref> [2] </ref> or the combination of control laws in fuzzy control. 0 10 20 0 10 20 30 40 50 60 70 Seconds Tip Position Reference Curve 3 Simulation Experiments on Curve Tracing with an Inverted Pendulum This section presents a learning problem, a fixed controller and two simple coaches that satisfy
Reference: [3] <author> M. Jordan and R. Jacobs. </author> <title> Learning to control an unstable system with forward modeling. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <year> 1990. </year>
Reference: [4] <author> M. Jordan and D. Rumelhart. </author> <title> Forward models: Supervised learning with a distal teacher. </title> <journal> Cognitive Science, </journal> <volume> 16, </volume> <year> 1992. </year>
Reference-contexts: Any learning method using gradient descent on a cost function may have problems with local minima, regardless of whether it is local or optimal (Jordan and Rumelhart discuss both methods in their paper on forward modeling <ref> [4] </ref>). Usually, control problems and cost metrics are designed so that there are no spurious local minima or at least they do not interfere with learning.
Reference: [5] <author> M. Kawato, Y. Uno, M. Isobe, and R. Suzuki. </author> <title> A hierarchical neural network model for voluntary movement with application to robotics. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 8, </volume> <year> 1988. </year>
Reference-contexts: Many methods, called local closed loop learners, compute training data at each time step, by considering the state, desired state, and error at the current time only. The training data is then used to modify the parameters of the learning controller. Adaptive control, feedback error learning <ref> [5] </ref>, and the use of modified Kohonen maps for robot control [9] are all examples of local closed loop learners. In any task where changing the controller parameters to reduce tracking error at the current time step is always the right thing to do, local learning performs well. <p> It was proposed by Kawato and his colleagues <ref> [5] </ref>. In our experiments with it we use a function approximator having four input variables. They are the current and previous desired tip position and tip velocity.
Reference: [6] <author> J. Koza and M. Keane. </author> <title> Cart centering and broom balancing by genetically breeding populations of control strategy programs. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <year> 1990. </year>
Reference: [7] <author> F. Lewis. </author> <title> Optimal Control. </title> <publisher> John Wiley and Sons, </publisher> <year> 1986. </year>
Reference-contexts: The use of local learning on these tasks can result in the learning getting stuck at a local minimum that is far from the global optimum. An optimal control <ref> [7] </ref> formulation for learning can be used to address the shortcomings of local closed loop learners. In optimal control, a cost function is specified for the performance of an entire task. The controller chooses controls that minimize the cost in-curred at the current and all future time steps.
Reference: [8] <author> B. Pearlmutter. </author> <title> Learning state space trajectories in recurrent neural networks. </title> <booktitle> Neural Computation, </booktitle> <pages> pages 263-269, </pages> <year> 1989. </year>
Reference-contexts: A controller constructed under this scheme can trade off accuracy for improved overall performance. One way to do this is through the use of the neural net backpropagation algorithm in a scheme called backpropagation-in-time <ref> [8] </ref>. It uses the learning rule from backprop to propagate performance error back in time over an entire task execution. The result is derivatives relating control signals to their effects on the entire task execution. These derivatives are used to update the weights in a neural network controller.
Reference: [9] <author> H. Ritter, T. Martinetz, and K. Schulten. </author> <title> Neural Computation and Self-Organizing Maps. </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: The training data is then used to modify the parameters of the learning controller. Adaptive control, feedback error learning [5], and the use of modified Kohonen maps for robot control <ref> [9] </ref> are all examples of local closed loop learners. In any task where changing the controller parameters to reduce tracking error at the current time step is always the right thing to do, local learning performs well.
Reference: [10] <author> T. Sanger. </author> <title> Neural network learning control of robot manipulators using gradually increasing task difficulty. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10 </volume> <pages> 323-333, </pages> <year> 1994. </year>
Reference-contexts: Usually, control problems and cost metrics are designed so that there are no spurious local minima or at least they do not interfere with learning. Sanger <ref> [10] </ref> shows how bad solutions can be avoided by starting with a simple task that has only one local minimum (thus it is also the global minimum) and slowly increasing its difficulty during learning.
Reference: [11] <author> J. Schneider. </author> <title> Robot Skill Learning Through Intelligent Experimentation. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> De-cember </month> <year> 1994. </year>
Reference-contexts: At each control cycle, the value of interest is the force that will make the system be exactly on the reference curve at the next control cycle. However, use of the instantaneous inverse dynamics alone is insufficient and thus assumption 1 holds for the inverted pendulum <ref> [11] </ref>. 3.1.1 A fixed controller Our closed loop learning algorithms start with a fixed controller that is capable of some minimal level of performance. We use the following nonlinear controller (its derivation can be found in [11]): u = 1:0 1:5 cos 2 (x 0 ) y d = t 2 <p> inverse dynamics alone is insufficient and thus assumption 1 holds for the inverted pendulum <ref> [11] </ref>. 3.1.1 A fixed controller Our closed loop learning algorithms start with a fixed controller that is capable of some minimal level of performance. We use the following nonlinear controller (its derivation can be found in [11]): u = 1:0 1:5 cos 2 (x 0 ) y d = t 2 (4) where t is the length of the control cycle. The controller is derived from the simulation state equations, but without the terms due to friction in the state equation for x 1 . <p> In order to improve the controller, it should consider the future effects of its actions as in optimal control. In <ref> [11] </ref> we describe such a coach for the inverted pendulum. <p> Further details of how such formulations of this problem can encounter sub-optimal local minima, and a demonstration that assumption 2 for cooperative coaching holds can be found in <ref> [11] </ref>. Dynamic programming often solves problems of this type.
References-found: 11

