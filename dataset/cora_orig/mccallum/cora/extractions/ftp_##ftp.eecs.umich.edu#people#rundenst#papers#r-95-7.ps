URL: ftp://ftp.eecs.umich.edu/people/rundenst/papers/r-95-7.ps
Refering-URL: ftp://ftp.eecs.umich.edu/people/rundenst/papers/INDEX.html
Root-URL: http://www.cs.umich.edu
Email: hibino@eecs.umich.edu rundenst@eecs.umich.edu  
Title: Position Paper for IJCAI95 Workshop on Intelligent Multimedia Information Retrieval Interactive Visualizations for Exploration and
Author: Stacie L. Hibino Elke A. Rundensteiner 
Date: February 1995  
Address: 1301 Beal Avenue Ann Arbor, MI 48109-2122  
Affiliation: Electrical Engineering Computer Science Dept. Software Systems Research Laboratory The University of Michigan  
Abstract: As various media formats (e.g., images, video, etc.) become more and more common forms of data collection, researchers need better tools for analyzing such data. They need new forms of analysis which take advantage of the characteristics inherent to the medium in which the data was collected. In the case of multimedia data such as video, simulations, or real-time data, researchers need new tools for exploring and analyzing temporal and/or spatial data trends. Not only do they need to analyze when events take place, but also when certain events take place in relation to other types of events. In this paper, we present a framework for an interactive visualization environment for spatio-temporal video analysis. This environment integrates a spatio-temporal visual query language with user-tailorable data visualizations. The visual query language simplifies the inquiry process for researchers, allowing them to quickly and easily submit data queries to test hypotheses, as well as to explore the temporal and/or spatial relationships (including motion) between different types of objects or events within the video data. The data visualizations present the results of each query in an intuitive, aggregated fashion. These visualizations are dynamically updated as constraints within a query are adjusted, thereby aiding users in the discovery of temporal, spatial, and motion data trends. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ahlberg, C., & Shneiderman, B. </author> <year> (1994). </year> <title> Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield Displays. </title> <booktitle> CHI'94 Conference Proc., </booktitle> <publisher> ACM Press, </publisher> <pages> pp. 619-626. </pages>
Reference-contexts: Research Goals The overall goal of this research is to support spatio-temporal analysis of video data by applying and extending a visual information seeking approach <ref> (Ahlberg and Shneiderman, 1994) </ref> to spatio-temporal annotations. <p> Overall Framework 4.1 The Visual Information Seeking Approach The overall framework to our approach is based on applying and extending a visual information seeking approach, called VIS <ref> (Ahlberg and Shneiderman, 1994) </ref>, to spatio-temporal video annotations. Ahlberg and Shneiderman describe VIS as a process for browsing database information. This process is characterized by rapid filtering, progressive refinement, continuous reformulation of goals, and visual scanning to identify results.
Reference: <author> Allen, J.F. </author> <year> (1983). </year> <title> Maintaining knowledge about temporal intervals. </title> <journal> Communications of the ACM, </journal> <volume> 26(11), </volume> <pages> 832-843. </pages>
Reference-contexts: Designing a relative spatio-temporal visual query language is not a trivial problem. In the case of relative temporal queries, there are 13 primitive temporal relationships <ref> (Allen, 1983) </ref>, leading to 2 13 -1 possible combinations of temporal relationships which users may wish explore. While users could pose relative temporal queries by selecting from a list of the 13 primitives, they would still have to specify temporal parameters for each primitive selected.
Reference: <author> Davenport, G., Smith, T.A., & Pincever, N. </author> <year> (1991). </year> <title> Cinematic Primitives for Multimedia. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 11(4), </volume> <pages> 67-74. </pages> <note> 7 Dimitrova, </note> <author> N. & Golshani, F. </author> <year> (1994). </year> <title> Rx for Semantic Video Database Retrieval. </title> <booktitle> ACM Multimedia94 Proceedings: </booktitle> <publisher> ACM Press, </publisher> <pages> pp. 219-226. </pages>
Reference-contexts: Using Annotations for Object Level Analysis of Video Data In order to explore and analyze video data at an object or event level (vs. at the bit-level), we are using video annotations to represent these objects and events. That is, we are using a stratifications approach <ref> (Davenport et al., 1991) </ref> whereby annotations are used as a form of data abstraction, describing the spatio-temporal and semantic content of the underlying video. This allows us to design an approach to data analysis that operates directly on the annotations, thereby simplifying the analysis process and making it more manageable.
Reference: <author> Freksa, C. </author> <year> (1992). </year> <title> Temporal reasoning based on semi-intervals. </title> <journal> Artificial Intelligence, </journal> <volume> 54(1992), </volume> <pages> 199-227. </pages>
Reference-contexts: this temporal VQL design are 1) users can dynamically and incrementally refine their queries by manipulating the slider thumbs, 2) the sliders provide continuous ranges of values allowing users to easily select a group of temporal primitives which are similar to one another (i.e., allowing users to select temporal neighborhoods <ref> (Freksa, 1992) </ref>), and 3) a dynamic temporal diagram is provided to visually clarify the specified query. The temporal diagram dynamically updates as users adjust slider thumbs, thereby providing a visual indication of the correlation between individual temporal primitives and the numerical ranges specified for the temporal end point relationships. 6.
Reference: <author> Hampapur, A., Weymouth, T., & Jain, R. </author> <year> (1994). </year> <title> Digital Video Segmentation. </title> <booktitle> ACM Multimedia94 Proceedings: </booktitle> <publisher> ACM Press, </publisher> <pages> pp. 357-364. </pages>
Reference: <author> Harrison, B.L., Owen, R., & Baecker, R.M. </author> <year> (1994). </year> <title> Timelines: An Interactive System for the Collection of Visualization of Temporal Data. </title> <booktitle> Proc. of Graphics Interface '94. Canadian Info. Processing Society. </booktitle>
Reference: <author> Hibino, S. & Rundensteiner, E. </author> <month> (Dec. </month> <year> 1994). </year> <title> A Graphical Query Language for Identifying Temporal Trends in Video Data. </title> <institution> University of Michigan, </institution> <note> EECS Technical Report CSE-TR-225-94 (Dec 94). </note>
Reference-contexts: Current Status and Conclusions Specifications of the temporal VQL interface are complete <ref> (Hibino and Rundensteiner, 1994) </ref> and have been implemented in a Windows-based multimedia pc environment. We are currently conducting user testing on this interface to evaluate the learning time and efficiency in which users can specify relative temporal queries using this interface compared to a forms-based interface.
Reference: <author> Mackay, W. E. </author> <year> (1989). </year> <title> EVA: An experimental video annotator for symbolic analysis of video data. </title> <journal> SIGCHI Bulletin, </journal> <volume> 21(2), </volume> <pages> 68-71. </pages>
Reference: <author> Nagasaka, A. and Tanaka, A. </author> <year> (1992). </year> <title> Automatic Video Indexing and Full-Video Search for Object Appearances. Visual Database Systems, </title> <editor> II (E. Knuth and L.M. Wegner, </editor> <booktitle> Eds.), </booktitle> <pages> pp. 113-127. </pages> <publisher> Elsevier Science Publishers. </publisher>
Reference-contexts: Although we are currently requiring researchers to manually input annotations in our current tool, we expect that previous work by others in the area of object extraction <ref> (e.g., Nagasaka and Tanaka, 1992) </ref> will eventually be used to generate annotations automatically. 3 3. Research Goals The overall goal of this research is to support spatio-temporal analysis of video data by applying and extending a visual information seeking approach (Ahlberg and Shneiderman, 1994) to spatio-temporal annotations.
References-found: 9

