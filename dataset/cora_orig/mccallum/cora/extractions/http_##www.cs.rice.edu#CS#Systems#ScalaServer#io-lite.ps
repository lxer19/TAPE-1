URL: http://www.cs.rice.edu/CS/Systems/ScalaServer/io-lite.ps
Refering-URL: http://www.cs.umd.edu/projects/hpssl/Friday2pm/
Root-URL: 
Title: IO-Lite: A unified I/O buffering and caching system  
Author: Vivek S. Pai, Peter Druschel, Willy Zwaenepoel 
Affiliation: Rice University  Rice University  
Pubnum: CS Technical Report TR97-294  
Abstract: This paper presents the design, implementation, and evaluation of IO-Lite, a unified I/O buffering and caching system. IO-Lite unifies all buffering and caching in the system, to the extent permitted by the hardware. In particular, it allows applications, inter-process communication, the filesystem, the file cache, and the network subsystem to share a single physical copy of the data safely and concurrently. Protection and security are maintained through a combination of access control and read-only sharing. The various subsystems use (mutable) buffer aggregates to access the data according to their needs. IO-Lite eliminates all copying and multiple buffering of I/O data, and enables various cross-subsystem optimizations. Performance measurements show significant performance improvements on Web servers and other I/O intensive applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. N. Bershad, S. Savage, P. Pardyak, E. G. Sirer, M. E. Fiuczynski, D. Becker, C. Chambers, and S. Eggers. </author> <title> Extensibility, safety and performance in the SPIN operating system. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating System Principles, </booktitle> <address> Copper Mountain, CO, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Furthermore, IO-Lite fully integrates the file cache, thus avoiding multiple buffering and copying along I/O data paths involving cached files. Unlike approaches based on extensible kernels <ref> [1, 5, 12] </ref>, IO-Lite uses an application-independent paradigm and provides an unrestricted execution environment, benefiting I/O intensive applications across the board. Existing UNIX applications can take advantage of IO-Lite's performance with modest to no changes. <p> Their use as an interprocess communication facility would benefit CGI programs, but with the same restrictions on filesystem access. Approaches Based on Extensible Kernels Recent work has proposed the use of of extensible kernels <ref> [1, 5, 12] </ref> to address a variety of problems associated with existing operating systems. Extensible kernels can potentially address many different OS performance problems, not just the I/O bottleneck that is the focus of our work.
Reference: [2] <author> D. L. Black, R. F. Rashid, D. B. Golub, C. R. Hill, and R. V. Baron. </author> <title> Translation looka-side buffer consistency: A software approach. </title> <booktitle> In Third Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS-III), </booktitle> <pages> pages 113-122, </pages> <address> Boston, Mas-sachusetts (USA), </address> <month> Apr. </month> <year> 1989. </year> <note> ACM. </note>
Reference-contexts: necessary during data transfers. (With an untrusted data source, two map changes are necessary per use of a buffer, independent of the number of transfers.) We expect that the resulting performance difference would be even more pronounced on a multiprocessor, where TLB shootdown adds considerable cost to VM map changes <ref> [2] </ref>. The results of a network loopback test are shown in Figure 3. Two test processes on the same machine exchange data as in the previous test, except INET domain stream sockets were used, i.e., the TCP/IP protocol suite is used for communication.
Reference: [3] <author> J. C. Brustoloni and P. Steenkiste. </author> <title> Effects of buffering semantics on I/O performance. </title> <booktitle> In Proc. 2nd Symp. on Operating Systems Design and Implementation, </booktitle> <address> Seattle WA (USA), </address> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Many other designs have been proposed to improve I/O performance in operating systems. In particular, various designs exist for copy-free I/O along certain data paths <ref> [3, 13, 6, 10, 16, 4] </ref>. IO-Lite distinguishes itself from these approaches by using a single paradigm to unify buffering on all data paths in the system, including inter-application, application-to-kernel, kernel-to-application, and inter-kernel data paths without restrictions on alignment of data. <p> For instance, a caching Web server must retain access to a cached document after it passes the document to the network subsystem or to a local client. IO-Lite uses a mechanism similar to fbufs [4] to achieve this goal. Mechanisms that only allow sequential sharing <ref> [10, 3] </ref> cannot achieve this goal. IO-Lite, like fbufs, combines page remapping and shared memory. Initially, when an (immutable) buffer is transferred, VM mappings are updated to grant the receiving process read access to the buffer's pages. <p> Well-known difficulties with this approach are alignment restrictions and the overhead of VM data structure manipulations. Dealing with VM alignment restrictions leads to the idea of input alignment as used in the emulated copy technique in Genie <ref> [3] </ref>. Here, the idea is to try and align system buffers with the application's data buffers, allowing page swapping even if application buffers are not page-aligned. To allow proper alignment of the system buffers, the application's read operation must be posted before the data arrives in main memory.
Reference: [4] <author> P. Druschel and L. L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 189-202, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Many other designs have been proposed to improve I/O performance in operating systems. In particular, various designs exist for copy-free I/O along certain data paths <ref> [3, 13, 6, 10, 16, 4] </ref>. IO-Lite distinguishes itself from these approaches by using a single paradigm to unify buffering on all data paths in the system, including inter-application, application-to-kernel, kernel-to-application, and inter-kernel data paths without restrictions on alignment of data. <p> For instance, a caching Web server must retain access to a cached document after it passes the document to the network subsystem or to a local client. IO-Lite uses a mechanism similar to fbufs <ref> [4] </ref> to achieve this goal. Mechanisms that only allow sequential sharing [10, 3] cannot achieve this goal. IO-Lite, like fbufs, combines page remapping and shared memory. Initially, when an (immutable) buffer is transferred, VM mappings are updated to grant the receiving process read access to the buffer's pages.
Reference: [5] <author> D. R. Engler, M. F. Kaashoek, and J. O'Toole. Exokernel: </author> <title> An operating system architecture for application-level resource management. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating System Principles, </booktitle> <address> Copper Mountain, CO, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Furthermore, IO-Lite fully integrates the file cache, thus avoiding multiple buffering and copying along I/O data paths involving cached files. Unlike approaches based on extensible kernels <ref> [1, 5, 12] </ref>, IO-Lite uses an application-independent paradigm and provides an unrestricted execution environment, benefiting I/O intensive applications across the board. Existing UNIX applications can take advantage of IO-Lite's performance with modest to no changes. <p> Their use as an interprocess communication facility would benefit CGI programs, but with the same restrictions on filesystem access. Approaches Based on Extensible Kernels Recent work has proposed the use of of extensible kernels <ref> [1, 5, 12] </ref> to address a variety of problems associated with existing operating systems. Extensible kernels can potentially address many different OS performance problems, not just the I/O bottleneck that is the focus of our work.
Reference: [6] <author> C. Maeda and B. Bershad. </author> <title> Protocol service decomposition for high-performance networking. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Many other designs have been proposed to improve I/O performance in operating systems. In particular, various designs exist for copy-free I/O along certain data paths <ref> [3, 13, 6, 10, 16, 4] </ref>. IO-Lite distinguishes itself from these approaches by using a single paradigm to unify buffering on all data paths in the system, including inter-application, application-to-kernel, kernel-to-application, and inter-kernel data paths without restrictions on alignment of data.
Reference: [7] <author> Open Market. </author> <title> FastCGI specification. </title> <address> http://www.fastcgi.com/. </address>
Reference-contexts: In our model, the application uses a simple library to establish a connection and wait on incoming requests, but otherwise behaves like a regular CGI application. Our approach maintains the simplicity of the CGI 1.1 standard [8] while gaining many of the benefits of the non-forking CGI approaches <ref> [7] </ref>. As with regular files, we handle all aspects of CGI processing in the server in a non-blocking manner.
Reference: [8] <author> NCSA. </author> <title> The common gateway interface. </title> <address> http://hoohoo.ncsa.uiuc.edu/cgi/. </address>
Reference-contexts: In our model, the application uses a simple library to establish a connection and wait on incoming requests, but otherwise behaves like a regular CGI application. Our approach maintains the simplicity of the CGI 1.1 standard <ref> [8] </ref> while gaining many of the benefits of the non-forking CGI approaches [7]. As with regular files, we handle all aspects of CGI processing in the server in a non-blocking manner.
Reference: [9] <author> V. Pai. IO-Lite: </author> <title> A copy-free UNIX I/O system. </title> <type> Technical Report TR97-269, </type> <institution> Rice University, </institution> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: At the heart of this API are two operations that replace the conventional UNIX read/write operations on file descriptors. A full discussion of this API is beyond the scope of this paper, due to space limitations. A technical report contains the relevant information <ref> [9] </ref>. We are now ready to summarize the key attributes of IO-Lite's unified representation of I/O data as follows: Immutable buffers I/O data is stored in immutable buffers. This allows safe, concurrent read-only sharing of a single copy of I/O data among all OS subsystems and applications.
Reference: [10] <author> J. Pasquale, E. Anderson, and P. K. Muller. </author> <title> Container Shipping: Operating system support for I/O-intensive applications. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 84-93, </pages> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: Many other designs have been proposed to improve I/O performance in operating systems. In particular, various designs exist for copy-free I/O along certain data paths <ref> [3, 13, 6, 10, 16, 4] </ref>. IO-Lite distinguishes itself from these approaches by using a single paradigm to unify buffering on all data paths in the system, including inter-application, application-to-kernel, kernel-to-application, and inter-kernel data paths without restrictions on alignment of data. <p> For instance, a caching Web server must retain access to a cached document after it passes the document to the network subsystem or to a local client. IO-Lite uses a mechanism similar to fbufs [4] to achieve this goal. Mechanisms that only allow sequential sharing <ref> [10, 3] </ref> cannot achieve this goal. IO-Lite, like fbufs, combines page remapping and shared memory. Initially, when an (immutable) buffer is transferred, VM mappings are updated to grant the receiving process read access to the buffer's pages. <p> This would lead to multiple buffering in a Web server. Moreover, the lack of general copy-free IPC hampers the performance of CGI programs. Copy Avoidance with Handoff Semantics The Container Shipping (CS) I/O system <ref> [10] </ref> and Thadani and Khalidi [16] use I/O read and write operations with handoff (move) semantics. Like IO-Lite, these systems require applications to process I/O data at a given location. Unlike IO-Lite, they allow applications to modify I/O buffers in-place. <p> As expected, the copy-free IO-Lite system outperforms standard UNIX by a considerable margin. Moreover, our results compare favorably with those reported for a similar experiment with the Container Shipping system <ref> [10] </ref> 7 . When transferring 128 KBytes (16 pages) at a time, CS achieves only 88 MBytes/s. The CS API requires that a received buffer is explicitly mapped into the receiver's address space before it can be accessed.
Reference: [11] <author> J. Poskanzer. </author> <note> thttpd tiny/turbo/throttling HTTP server. http://www.acme.com/software/thttpd/. </note>
Reference-contexts: As a basis for comparison, we also provide performance results for the Harvest cache running in httpd accelerator mode and for thttpd, a freeware web server <ref> [11] </ref>.
Reference: [12] <author> M. I. Seltzer, Y. Endo, C. Small, and K. A. Smith. </author> <title> Dealing with disaster: Surviving misbehaved kernel extensions. </title> <booktitle> In Proc. 2nd Symp. on Operating Systems Design and Implementation, </booktitle> <address> Seattle, WA, </address> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Furthermore, IO-Lite fully integrates the file cache, thus avoiding multiple buffering and copying along I/O data paths involving cached files. Unlike approaches based on extensible kernels <ref> [1, 5, 12] </ref>, IO-Lite uses an application-independent paradigm and provides an unrestricted execution environment, benefiting I/O intensive applications across the board. Existing UNIX applications can take advantage of IO-Lite's performance with modest to no changes. <p> Their use as an interprocess communication facility would benefit CGI programs, but with the same restrictions on filesystem access. Approaches Based on Extensible Kernels Recent work has proposed the use of of extensible kernels <ref> [1, 5, 12] </ref> to address a variety of problems associated with existing operating systems. Extensible kernels can potentially address many different OS performance problems, not just the I/O bottleneck that is the focus of our work.
Reference: [13] <author> J. M. Smith and C. B. S. Traw. </author> <title> Giving applications access to Gb/s networking. </title> <journal> IEEE Network, </journal> <volume> 7(4) </volume> <pages> 44-52, </pages> <month> July </month> <year> 1993. </year> <title> TR97-294: IO-Lite: A unified I/O buffering and caching system 17 </title>
Reference-contexts: Many other designs have been proposed to improve I/O performance in operating systems. In particular, various designs exist for copy-free I/O along certain data paths <ref> [3, 13, 6, 10, 16, 4] </ref>. IO-Lite distinguishes itself from these approaches by using a single paradigm to unify buffering on all data paths in the system, including inter-application, application-to-kernel, kernel-to-application, and inter-kernel data paths without restrictions on alignment of data.
Reference: [14] <author> K. L. Swartz. </author> <note> Migrating to a web filer. http://www.netapp.com/technology/level3/- 3012.html. </note>
Reference-contexts: Furthermore, it is desirable to provide CGI programs a rich environment while protecting the server from errant programs. For these reason, highly optimized Web servers (such as NetApp's server <ref> [18, 14] </ref>, and those based on extensible kernels typically revert to an unoptimized, general execution for CGI programs.
Reference: [15] <author> D. L. Tennenhouse. </author> <title> Layered multiplexing considered harmful. </title> <editor> In H. Rudin and R. Williamson, editors, </editor> <booktitle> Protocols for High-Speed Networks, </booktitle> <pages> pages 143-148, </pages> <address> Amsterdam, 1989. </address> <publisher> North-Holland. </publisher>
Reference-contexts: Here, the network driver must perform an early demultiplexing operation to determine the packet's destination. Incidentally, early demulti-plexing has been identified by many researchers as a necessary feature for efficiency and quality of service in high-performance networks <ref> [15] </ref>. With IO-Lite, to avoid copying entirely, early demultiplexing is necessary. pages, buffers, and buffer aggregates. Fbufs are allocated in a region of the virtual called the fbuf window, which appears in the virtual address space of all protection domains, including the kernel.
Reference: [16] <author> M. N. Thadani and Y. A. Khalidi. </author> <title> An efficient zero-copy I/O framework for UNIX. </title> <type> Technical Report SMLI TR-95-39, </type> <institution> Sun Microsystems Laboratories, Inc., </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Many other designs have been proposed to improve I/O performance in operating systems. In particular, various designs exist for copy-free I/O along certain data paths <ref> [3, 13, 6, 10, 16, 4] </ref>. IO-Lite distinguishes itself from these approaches by using a single paradigm to unify buffering on all data paths in the system, including inter-application, application-to-kernel, kernel-to-application, and inter-kernel data paths without restrictions on alignment of data. <p> This would lead to multiple buffering in a Web server. Moreover, the lack of general copy-free IPC hampers the performance of CGI programs. Copy Avoidance with Handoff Semantics The Container Shipping (CS) I/O system [10] and Thadani and Khalidi <ref> [16] </ref> use I/O read and write operations with handoff (move) semantics. Like IO-Lite, these systems require applications to process I/O data at a given location. Unlike IO-Lite, they allow applications to modify I/O buffers in-place.
Reference: [17] <author> G. Trent and M. Sake. WebSTONE: </author> <title> The first generation in HTTP server benchmarking. </title> <address> http://www.sgi.com/Products/WebFORCE/- WebStone/paper.html. </address>
Reference-contexts: Our HTTP client is a process that opens multiple connections to the server and uses a select-based loop to efficiently manage all the connections. We originally tried using the Webstone HTTP benchmark <ref> [17] </ref>, but the load caused the client machines to reach CPU saturation well before the server machine. All tests were conducted using 32 or more concurrent client connections and access logging was disabled in all the servers.
Reference: [18] <author> A. Watson. </author> <title> Multiprotocol data access: </title> <journal> NFS, </journal> <note> CIFS, and HTTP. http://www.netapp.com/technology/level3/- 3014.html. </note>
Reference-contexts: Furthermore, it is desirable to provide CGI programs a rich environment while protecting the server from errant programs. For these reason, highly optimized Web servers (such as NetApp's server <ref> [18, 14] </ref>, and those based on extensible kernels typically revert to an unoptimized, general execution for CGI programs.
References-found: 18

