URL: http://www.research.att.com/~wcohen/postscript/aaai-96.ps
Refering-URL: http://www.research.att.com/~wcohen/ripperd.html
Root-URL: 
Email: wcohen@research.att.com  
Title: Learning Trees and Rules with Set-valued Features  
Author: William W. Cohen 
Address: 600 Mountain Avenue Murray Hill, NJ 07974  
Affiliation: AT&T Laboratories  
Abstract: In most learning systems examples are represented as fixed-length "feature vectors", the components of which are either real numbers or nominal values. We propose an extension of the feature-vector representation that allows the value of a feature to be a set of strings; for instance, to represent a small white and black dog with the nominal features size and species and the set-valued feature color, one might use a feature vector with size=small, species=canis-familiaris and color=fwhite,blackg. Since we make no assumptions about the number of possible set elements, this extension of the traditional feature-vector representation is closely connected to Blum's "infinite attribute" representation. We argue that many decision tree and rule learning algorithms can be easily extended to set-valued features. We also show by example that many real-world learning problems can be efficiently and naturally represented with set-valued features; in particular, text categorization problems and problems that arise in propositionalizing first-order representations lend themselves to set-valued features. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Kamal Ali and Machael Pazzani. HYDRA: </author> <title> A noise-tolerant relational concept learning algorithm. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <address> Chambery, France, </address> <year> 1993. </year>
Reference: <author> Chidanand Apte, Fred Damerau, and Sholom M. Weiss. </author> <title> Automated learning of decision rules for text categorization. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 12(3) </volume> <pages> 233-251, </pages> <year> 1994. </year>
Reference: <author> Avrim Blum. </author> <title> Learning boolean functions in an infinite attribute space. </title> <journal> Machine Learning, </journal> <volume> 9(4) </volume> <pages> 373-386, </pages> <year> 1992. </year>
Reference: <author> L. Brieman, J. H. Friedman, R.A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmon, CA, </address> <year> 1984. </year>
Reference-contexts: Henceforth, we will refer to these tests as monotone element-of tests. Theory, as well as experience on practical problems, indicates that this restriction may be useful. Generality. This routine can be easily adapted to maximize a metric other than entropy, such as the GINI criteria <ref> (Brieman et al. 1984) </ref>, information gain (Quin-lan 1990b), predictive value (Apte et al. 1994), Bayes-Laplace corrected error (Clark and Niblett 1989), or LS-content (Ali and Pazzani 1993). In fact, any metric that depends only on the empirical performance of a condition on a sample can be used.
Reference: <author> Wray Buntine. </author> <title> Generalized subsumption and its application to induction and redundancy. </title> <journal> Artificial Intelligence, </journal> <volume> 36(2) </volume> <pages> 149-176, </pages> <year> 1988. </year>
Reference: <author> Jason Catlett. </author> <title> Megainduction: a test flight. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <address> Ithaca, New York, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(1), </volume> <year> 1989. </year>
Reference-contexts: Generality. This routine can be easily adapted to maximize a metric other than entropy, such as the GINI criteria (Brieman et al. 1984), information gain (Quin-lan 1990b), predictive value (Apte et al. 1994), Bayes-Laplace corrected error <ref> (Clark and Niblett 1989) </ref>, or LS-content (Ali and Pazzani 1993). In fact, any metric that depends only on the empirical performance of a condition on a sample can be used.
Reference: <author> William W. Cohen and Haym Hirsh. </author> <title> Learning the CLASSIC description logic: Theoretical and experimental results. </title> <booktitle> In Principles of Knowledge Representation and Reasoning: Proceedings of the Fourth International Conference (KR94). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference: <author> William W. Cohen and Yoram Singer. </author> <title> Context-sensitive learning methods for text categorization. </title> <note> To appear in SIGIR-96, </note> <year> 1996. </year>
Reference: <author> William W. Cohen. </author> <title> Rapid prototyping of ILP systems using explicit bias. </title> <booktitle> In Proceedings of the 1993 IJCAI Workshop on Inductive Logic Programming, </booktitle> <address> Chambery, France, </address> <year> 1993. </year>
Reference: <author> William W. Cohen. </author> <title> Pac-learning nondeterminate clauses. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <year> 1994. </year>
Reference: <author> William W. Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <address> Lake Taho, California, 1995. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> William W. Cohen. </author> <title> Text categorization and relational learning. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <address> Lake Taho, California, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Saso Dzeroski, Stephen Muggleton, and Stuart Russell. </author> <title> Pac-learnability of determinate logic programs. </title> <booktitle> In Proceedings of the 1992 Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, Pennsylvania, </address> <year> 1992. </year>
Reference: <author> David J. Ittner, David D. Lewis, and David D. Ahn. </author> <title> Text categorization of low quality images. </title> <booktitle> In Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 301-315, </pages> <address> Las Vegas, NV, </address> <year> 1995. </year> <institution> ISRI; Univ. of Nevada, Las Ve-gas. </institution>
Reference: <author> Michael Kearns and Umesh Vazarani. </author> <title> An introduction to computational learning theory. </title> <publisher> The MIT Press, </publisher> <address> Com-bridge, Massachusetts, </address> <year> 1994. </year>
Reference: <author> Nada Lavrac and Saso Dzeroski. </author> <title> Background knowledge and declarative bias in inductive concept learning. </title> <editor> In K. P. Jantke, editor, </editor> <title> Analogical and Inductive Inference: </title> <booktitle> International Workshop AII'92. </booktitle> <publisher> Springer Verlag, </publisher> <address> Daghstuhl Castle, Germany, </address> <year> 1992. </year> <booktitle> Lectures in Artificial Intelligence Series #642. </booktitle>
Reference: <author> Nada Lavrac and Saso Dzeroski. </author> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <address> Chich-ester, England, </address> <year> 1994. </year>
Reference: <author> David Lewis and Jason Catlett. </author> <title> Heterogeneous uncertainty sampling for supervised learning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh Annual Conference, </booktitle> <address> New Brunswick, New Jersey, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> David Lewis and William Gale. </author> <title> Training text classifiers by uncertainty sampling. </title> <booktitle> In Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1994. </year>
Reference: <author> David Lewis and Mark Ringuette. </author> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Symposium on Document Analysis and Information Retrieval, </booktitle> <address> Las Ve-gas, Nevada, </address> <year> 1994. </year>
Reference: <author> David Lewis. </author> <title> Representation and learning in information retrieval. </title> <type> Technical Report 91-93, </type> <institution> Computer Science Dept., University of Massachusetts at Amherst, </institution> <year> 1992. </year> <type> PhD Thesis. </type>
Reference: <author> Nick Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2(4), </volume> <year> 1988. </year>
Reference: <author> Katharina Morik. </author> <title> A bootstrapping approach to conceptual clustering. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> Ithaca, New York, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Stephen H. Muggleton, </author> <title> editor. Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference: <author> Stephen Muggleton. </author> <title> Inverse entailment and Progol. New Generation Computing, </title> <address> 13(3,4):245-286, </address> <year> 1995. </year>
Reference: <author> Michael Pazzani and Clifford Brunk. </author> <title> Detecting and correcting errors in rule-based expert systems: an integration of empirical and explanation-based learning. </title> <journal> Knowledge Acquisition, </journal> <volume> 3 </volume> <pages> 157-173, </pages> <year> 1991. </year>
Reference: <author> Michael Pazzani. </author> <title> Creating a Memory of Causal Relationships. </title> <publisher> Lawrence Erlbaum, </publisher> <year> 1990. </year>
Reference: <author> J. Ross Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <year> 1990. </year>
Reference: <author> J. Ross Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3), </volume> <year> 1990. </year>
Reference: <author> J. Ross Quinlan. C4.5: </author> <title> programs for machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference: <author> C. J. Van Rijsbergen. </author> <title> Information Retrieval. Butterworth, </title> <address> London, </address> <note> second edition, </note> <year> 1979. </year>
Reference-contexts: All entries in the table are averages over all nine problems (equally weighted). So that we can compare earlier work, we also record the value of the F-measure <ref> (Van Rijsbergen 1979, pages 168-176) </ref> at fi = 1. The F-measure is defined as F fi (fi 2 +1)precisionrecall fi 2 precision+recall where fi controls the importance given to precision relative to recall.
References-found: 32

