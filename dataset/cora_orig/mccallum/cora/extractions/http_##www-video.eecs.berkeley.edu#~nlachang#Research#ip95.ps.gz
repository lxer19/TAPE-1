URL: http://www-video.eecs.berkeley.edu/~nlachang/Research/ip95.ps.gz
Refering-URL: http://www-video.eecs.berkeley.edu/~nlachang/Research/pubs.html
Root-URL: 
Title: View Generation for Three-Dimensional Scenes from Video Sequences  
Author: Nelson L. Chang, Student Member, IEEE, and Avideh Zakhor, Member, IEEE 
Date: 4, APRIL 1997  
Note: 584 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 6, NO.  
Abstract: This paper focuses on the representation and view generation of three dimensional (3-D) scenes. In contrast to existing methods that construct a full 3-D model or those that exploit geometric invariants, our representation consists of dense depth maps at several preselected viewpoints from an image sequence. Furthermore, instead of using multiple calibrated stationary cameras or range scanners, we derive our depth maps from image sequences captured by an uncalibrated camera with only approximately known motion. We propose an adaptive matching algorithm which assigns various confidence levels to different regions in the depth maps. Nonuniform bicubic spline interpolation is then used to fill in low confidence regions in the depth maps. Once the depth maps are computed at preselected viewpoints, the intensity and depth at these locations are used to reconstruct arbitrary views of the 3-D scene. Specifically, the depth maps are regarded as vertices of a deformable 2-D mesh which are transformed in 3-D, projected to 2-D, and rendered to generate the desired view. Experimental results are presented to verify our approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Ardizzone, M. A. Palazzo, and F. Sorbello, </author> <title> "3-D scene re CHANG AND ZAKHOR: VIEW GENERATION 597 Fig. 30. Reconstructed view along horizontal trajectory. Fig. 31. Reconstructed "Chess" view from translation along z axis. construction from multiple 2-D views," </title> <booktitle> in Proceedings of the 5th Int'l Conf. on Image Analysis and Processing. </booktitle> <month> 20-22 Sept. </month> <year> 1989, </year> <pages> pp. 394-398, </pages> <address> Positano, Italy. </address>
Reference: [2] <author> J. L. Barron, D. J. Fleet, and S. S. Beauchemin, </author> <title> "Performance of optical flow techniques," </title> <journal> International Journal of Computer Vision, </journal> <volume> vol. 12, no. 1, </volume> <pages> pp. 43-77, </pages> <month> Feb. </month> <year> 1994. </year>
Reference: [3] <author> R. H. Bartels, J. C. Beatty, and B. A. Barsky, </author> <title> An Introduction to Splines for use in Computer Graphics and Geometric Modeling, </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1987. </year>
Reference-contexts: There are however a considerable number of low confidence regions. To fill in these regions and to make the map much denser while not sacrificing too much accuracy, nonuniform cubic B-splines are used [5], <ref> [3] </ref>, [12]. Every depth point in low confidence regions is interpolated by its neighboring high confidence depth vertices 5 Note that other images may be considered as well including those obtained by arbitrary translational motion in the x-y plane. CHANG AND ZAKHOR: VIEW GENERATION 589 Fig. 6.
Reference: [4] <author> C. Braccini, A. Grattarola, and S. Zappatore, </author> <title> "Volumetric and pictorial reconstruction of 3D objects from correspondences in moving 2D views," in Recent Issues in Pattern Analysis and Recognition, </title> <editor> V. Cantoni, R. Creutzburg, S. Levialdi, G. Wolf, Ed. </editor> <publisher> Springer-Verlag, </publisher> <pages> pp. 249-258, </pages> <year> 1989. </year>
Reference: [5] <author> N. L. Chang, </author> <title> "View reconstruction from uncalibrated cameras for three-dimensional scenes," M.S. </title> <type> thesis, </type> <institution> Department of Electrical Engineering and Computer Sciences, University of Califor-nia at Berkeley, </institution> <year> 1994. </year>
Reference-contexts: To avoid too sparse a depth map, we attempt to improve estimates in these regions by proposing an adaptive matching approach <ref> [5] </ref>. The approach consists of essentially dividing the image into CONST and non-CONST regions and finding the best matches for both regions. To match images I 1 and I 2 , all AP points in both images are located first using edge detection. <p> Because of the predominantly bimodal distribution of the data, i.e. foreground and background points, we consider using the median instead of mean to throw out outliers <ref> [5] </ref>. Generally, the depth associated with the cluster consisting of the majority of points is reasonably correct. We found that when dealing with bimodal distributed data, outlier identification was significantly improved by using the median rather than using the mean. <p> Cubic B-Spline Approximation The depth map after the combination stage is fairly accurate in many regions. There are however a considerable number of low confidence regions. To fill in these regions and to make the map much denser while not sacrificing too much accuracy, nonuniform cubic B-splines are used <ref> [5] </ref>, [3], [12]. Every depth point in low confidence regions is interpolated by its neighboring high confidence depth vertices 5 Note that other images may be considered as well including those obtained by arbitrary translational motion in the x-y plane. CHANG AND ZAKHOR: VIEW GENERATION 589 Fig. 6. <p> However, if we consider transforming only this set of points to generate the view estimate, the resulting image may exhibit inconsistencies in the ordering of foreground and background points [6], <ref> [5] </ref>. 590 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 6, NO. 4, APRIL 1997 A better approach is to consider the points of the reference frame arrays as vertices of a deformable 2-D wire mesh.
Reference: [6] <author> N. L. Chang and A. Zakhor, </author> <title> "Arbitrary view generation for three-dimensional scenes from uncalibrated video cameras," </title> <booktitle> in Proceedings of ICASSP. </booktitle> <month> 8-12 May </month> <year> 1995, </year> <journal> vol. </journal> <volume> 4, </volume> <pages> pp. 2455-2458, </pages> <address> Detroit, MI. </address>
Reference-contexts: There are some artifacts inherent both in the algorithm and the problem itself that induce incorrect disparities for certain regions <ref> [6] </ref>. In what follows, we describe four of the most important artifacts and explain techniques of mini mizing their effects. <p> However, if we consider transforming only this set of points to generate the view estimate, the resulting image may exhibit inconsistencies in the ordering of foreground and background points <ref> [6] </ref>, [5]. 590 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 6, NO. 4, APRIL 1997 A better approach is to consider the points of the reference frame arrays as vertices of a deformable 2-D wire mesh.
Reference: [7] <author> N. L. Chang and A. Zakhor, </author> <title> "Intermediate View Reconstruction for Three-Dimensional Scenes," </title> <booktitle> in Proceedings of ICDSP. </booktitle> <month> 14-16 July </month> <year> 1993, </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 636-641, </pages> <address> Nicosia, Cyprus. </address>
Reference: [8] <author> N. L. Chang and A. Zakhor, </author> <title> "Multivalued representations for image reconstruction and new view synthesis," </title> <note> In preparation. </note>
Reference-contexts: Since most of the frames of the original data have been discarded, we do not utilize all the information about the scene with our representation. We are currently examining layered representations [39] and multivalued intensity and depth maps to encapsulate the scene information better <ref> [8] </ref>. Finally, a real-time implementation of the reconstruction algorithm would expedite the development of a virtual environment.
Reference: [9] <author> N. L. Chang and A. Zakhor, </author> <title> "View generation for 3-D scenes from video sequences," </title> <booktitle> in Proceedings of IMDSP Workshop. </booktitle> <month> 3-6 March </month> <year> 1996, </year> <pages> pp. 134-135, </pages> <address> Belize City, Belize. </address>
Reference-contexts: To detect patches along depth discontinuities, we estimate the local variance with a 5 fi 5 window on the depth maps and mark points whose variance is above a certain threshold <ref> [9] </ref>. This technique of searching for large depth variations is similar in nature to a crude intensity-based edge detection algorithm. Patches associated with a depth discontinuity are not rendered to avoid streaking.
Reference: [10] <author> S. E. Chen and L. Williams, </author> <title> "View interpolation for image synthesis," </title> <booktitle> in Proceedings of SIGGRAPH. </booktitle> <month> 1-6 Aug. </month> <year> 1993, </year> <pages> pp. 279-288, </pages> <address> New York. Fig. </address> <month> 32. </month> <title> Reconstructed "Chess" view from translation along +z axis. Fig. 33. Reconstructed "Chess" view from tilt of 10 ffi , translation of 4 units along the x axis, 3 units along the y axis, and 0:04 units along the z axis. </title>
Reference: [11] <author> C. H. Chien and J. K. Aggarwal, </author> <title> "Identification of 3D objects from multiple silhouettes using quadtrees/octrees," Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> vol. 36, no. </volume> <pages> 2-3, pp. 256-273, </pages> <address> Nov.-Dec. </address> <year> 1986. </year>
Reference: [12] <author> C. de Boor, </author> <title> "On calculating with B-splines," </title> <journal> Journal of Approximation Theory, </journal> <volume> vol. 6, no. 1, </volume> <pages> pp. 50-62, </pages> <month> July </month> <year> 1972. </year>
Reference-contexts: There are however a considerable number of low confidence regions. To fill in these regions and to make the map much denser while not sacrificing too much accuracy, nonuniform cubic B-splines are used [5], [3], <ref> [12] </ref>. Every depth point in low confidence regions is interpolated by its neighboring high confidence depth vertices 5 Note that other images may be considered as well including those obtained by arbitrary translational motion in the x-y plane. CHANG AND ZAKHOR: VIEW GENERATION 589 Fig. 6.
Reference: [13] <author> U. R. Dhond and J. K. Aggarwal, </author> <title> "Structure from stereo| a review," </title> <journal> IEEE Trans. Sys. Man Cyber., </journal> <volume> vol. 19, no. 6, </volume> <pages> pp. 1489-1509, </pages> <year> 1989. </year>
Reference: [14] <author> J. D. Foley, A. van Dam, et al., </author> <title> Introduction to Computer Graphics, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1994. </year>
Reference-contexts: With the patches transformed, we are now in a position to generate the view estimate. The set of valid patches are projected onto the image plane and then rendered using a scan-line algorithm <ref> [14] </ref>. For each patch, the four edges are stored in memory and sorted in decreasing v coordinate. Starting at the smallest v coordinate, every scan line is filled in according to an intensity-based interpolation scheme.
Reference: [15] <author> P. Fua, </author> <title> "A parallel stereo algorithm that produces dense depth maps and preserves image features," </title> <journal> Machine Vision and Applications, </journal> <volume> vol. 6, no. 1, </volume> <pages> pp. 35-49, </pages> <month> Winter </month> <year> 1993. </year>
Reference-contexts: Typically, some additional information is furnished to aid in matching such as uniqueness and disparity constraints for random dot stereograms [28], a third view [20] or even more views [32], shading information <ref> [15] </ref>, or different filtered outputs [21]. Other approaches are classified as solving the structure-from-motion (SFM) problem [29], [36], [41], [37]. For these algorithms, a set of features, e.g. edges in [37] and corners in [41], are identified and tracked. <p> A low variance suggests that the block consists of little texture and nearly constant intensity. Matching the images in both directions helps to identify occluded regions and inconsistent matches <ref> [15] </ref>, [40]. Occluded regions (OCCL) are precisely the unmatched points in the images, whereas inconsistent matches (INCONS ) may be found by validating matches in both directions.
Reference: [16] <author> D. Hearn and M. P. Baker, </author> <title> Computer Graphics, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1986. </year>
Reference-contexts: For each patch, the four edges are stored in memory and sorted in decreasing v coordinate. Starting at the smallest v coordinate, every scan line is filled in according to an intensity-based interpolation scheme. In addition, we employ a software z-buffering technique <ref> [16] </ref>, [31] to determine the ordering of patches with respect to the smallest depth. Thus, a given pixel in the view estimate is assigned an intensity corresponding to the patch closest to the camera.
Reference: [17] <author> K. Higuchi, M. Hebert, and K. </author> <title> Ikeuchi, "Building 3-D models from unregistered range images," </title> <type> Tech. Rep. </type> <institution> CMU-CS-93-214, Carnegie Mellon University, </institution> <month> Nov. </month> <year> 1993. </year>
Reference: [18] <author> B. K. P. Horn, </author> <title> Robot Vision, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: However, unlike the approaches which produce depth for a sparse set of points, we recover dense depth information as required by the problem of AVG. Since we have confined the motion to be planar, the depth estimation problem reduces to a 1-D correspondence matching problem <ref> [18] </ref>. In this case, the epipolar lines of the two images are parallel and may be found using the algorithm described in [42]. <p> Clearly, the localization of depth discontinuities depends on the size of the block used for matching|the smaller the block, the better the localization. However, it is widely known that using blocks that are too small produces many false matches, since intensity patterns will be less distinctive <ref> [18] </ref>. An example of all four artifacts is shown in Figure 2. The two images shown are related by horizontal translational motion, i.e. the two optical axes are parallel to each other and perpendicular to the direction of motion and the epipolar lines are coincident with the scan lines. <p> Once all the points of the reference frame have been mapped into 3-D accordingly, they are then transformed according to the appropriate motion parameters. The notion of applying motion parameters to a frame has been addressed in conventional computer vision and robotics literature <ref> [18] </ref>, [30].
Reference: [19] <author> B. K. P. Horn and B. G. Schunck, </author> <title> "Determining optical flow," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 17, </volume> <pages> nos. 1-3, pp. 185-203, </pages> <month> Aug. </month> <year> 1981. </year>
Reference: [20] <author> M. Ito and A. Ishii, </author> <title> "Three-view stereo analysis," </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <volume> vol. PAMI-8, no. 4, </volume> <pages> pp. 524-532, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: Typically, some additional information is furnished to aid in matching such as uniqueness and disparity constraints for random dot stereograms [28], a third view <ref> [20] </ref> or even more views [32], shading information [15], or different filtered outputs [21]. Other approaches are classified as solving the structure-from-motion (SFM) problem [29], [36], [41], [37]. For these algorithms, a set of features, e.g. edges in [37] and corners in [41], are identified and tracked.
Reference: [21] <author> D. G. Jones and J. Malik, </author> <title> "A computational framework for determining stereo correspondence from a set of linear spatial filters," </title> <booktitle> in Proceedings of ECCV. </booktitle> <address> 18-23 May 1992, Santa Margherita Ligure, Italy, </address> <pages> pp. 395-410. </pages> <booktitle> 598 IEEE TRANSACTIONS ON IMAGE PROCESSING, </booktitle> <volume> VOL. 6, NO. 4, </volume> <month> APRIL </month> <year> 1997 </year> <month> Fig. </month> <title> 34. Reconstructed "Chess" view with Laveau and Faugeras's algorithm for translation along z axis. Fig. 35. Reconstructed "Chess" view with Laveau and Faugeras's algorithm for translation along +z axis. </title>
Reference-contexts: Typically, some additional information is furnished to aid in matching such as uniqueness and disparity constraints for random dot stereograms [28], a third view [20] or even more views [32], shading information [15], or different filtered outputs <ref> [21] </ref>. Other approaches are classified as solving the structure-from-motion (SFM) problem [29], [36], [41], [37]. For these algorithms, a set of features, e.g. edges in [37] and corners in [41], are identified and tracked. The motion of the camera and the structure of these features are then computed simultaneously.
Reference: [22] <author> T. Kanade, P. J. Narayanan, and P. W. Rander, </author> <title> "Virtualized reality: Concepts and early results," </title> <booktitle> in IEEE Workshop on Representation of Visual Scenes. </booktitle> <address> June 24 1995, Cambridge, MA, </address> <pages> pp. 69-76. </pages>
Reference-contexts: These low confidence regions will be times intensity edges are related to depth ones. We note that depth discontinuities can be much better localized by human interaction as done in <ref> [22] </ref>. Fig. 4. Example of disparity map using adaptive block size. Legend: blue, CON ST ; red, AP ; yellow, IN CONS; green, OCCL. dealt with in the upcoming sections. B. <p> For example, image mosaics [35], [27] restrict camera motion to be panning, not roughly translational. The approach of Skerjanc and Liu [34] requires a calibrated trinocular setup while that of Kanade et al. <ref> [22] </ref> uses clusters of fixed CHANG AND ZAKHOR: VIEW GENERATION 595 Fig. 22. Reconstructed "Mug" view from x y plane. Fig. 23. Reconstructed "Mug" view from translation along z axis. arrays of cameras.
Reference: [23] <author> J. J. Koenderink and A. J. van Doorn, </author> <title> "Facts on optic flow," </title> <journal> Biological Cybernetics, </journal> <volume> vol. 56, no. 4, </volume> <pages> pp. 247-254, </pages> <year> 1987. </year>
Reference: [24] <author> R. Kumar, P. Anandan, and K. Hanna, </author> <title> "Shape recovery from multiple views: a parallax based approach," </title> <booktitle> in Proceedings of Image Understanding Workshop. </booktitle> <month> 13-16 Nov. </month> <year> 1994, </year> <journal> Monterey, </journal> <volume> vol. 2, </volume> <pages> pp. 947-955. </pages>
Reference-contexts: Translating away from the scene leads to the image shown in Figure 32. We observe that the result consists of the union of the points seen in the two reference images and is similar to a 3-D parallax corrected mosaic <ref> [24] </ref>. Finally, Figure 33 demonstrates an oblique view obtained by tilting the camera 10 ffi and translating along all three axes. The results for each of the views are quite reasonable. C.
Reference: [25] <author> S. Laveau and O. Faugeras, </author> <title> "3-D scene representation as a collection of images and fundamental matrices," </title> <type> Tech. Rep. 2205, </type> <institution> INRIA, </institution> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: Reconstructed "Mug" view from x y plane. Fig. 23. Reconstructed "Mug" view from translation along z axis. arrays of cameras. In the end, we have chosen to compare our results with the approach of Laveau and Faugeras <ref> [25] </ref>. They require only a set of reference frames and a dense disparity map, both consistent with our approach. The algorithms also have similar storage requirements; their approach needs two images and one disparity map whereas the proposed approach requires one additional depth map. <p> In our implementation of Laveau and Faugeras's algorithm, the reference frames in Figures 26 and 27 are the input images while the depth map in Figure 29 serves as the correspondence map. Because of our reference pair configuration, their algorithm exhibits artifacts near the trifocal plane <ref> [25] </ref>. Also, we found that views like Figure 30 lying along the baseline between the two reference images are unattainable due to collinearity of the optical centers. As compared to Figure 31, their algorithm has difficulty Fig. 24. Reconstructed "Mug" view from translation along +z axis. Fig. 25. <p> Our results are comparable to full 3-D modeling techniques yet not as complicated. Moreover, using depth surfaces to estimate scene structure results in recovering uncovered background points in the scene much better and leads to a faster rendering approach. Direct methods based on projective geometry such as <ref> [25] </ref> reconstruct only those points that lie in the intersection 596 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 6, NO. 4, APRIL 1997 Fig. 26. Reference frame #10 (intensity) of "Chess." Fig. 27.
Reference: [26] <author> J. S. Lim, </author> <title> Two-Dimensional Signal and Image Processing, </title> <publisher> Pren-tice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1990. </year>
Reference-contexts: These confidence levels are important for locating the regions to ignore when combining multiple depth maps together. To detect aperture ambiguity (AP ), a gradient-based edge detector <ref> [26] </ref> is used to locate the horizontal edges. 3 Points in the image near these edge pixels are marked as possibly spurious. To identify constant intensity regions (CONST ), a small window is used to find regions where the intensity variance is lower than a prespecified threshold.
Reference: [27] <author> S. Mann and R. W. </author> <title> Picard, "Video orbits of the projective group: A new perspective on image mosaicing," </title> <type> Tech. Rep. 338, </type> <institution> MIT Media Lab Perceptual Computing, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The results for each of the views are quite reasonable. C. Comparison to Previous Work We found that most of the methods described in Section I have different input data requirements from the proposed approach, making it difficult for direct comparison. For example, image mosaics [35], <ref> [27] </ref> restrict camera motion to be panning, not roughly translational. The approach of Skerjanc and Liu [34] requires a calibrated trinocular setup while that of Kanade et al. [22] uses clusters of fixed CHANG AND ZAKHOR: VIEW GENERATION 595 Fig. 22. Reconstructed "Mug" view from x y plane. Fig. 23.
Reference: [28] <author> D. Marr and T. Poggio, </author> <title> "Cooperative computation of stereo disparity," </title> <journal> Science, </journal> <volume> vol. 194, no. 4262, </volume> <pages> pp. 283-287, </pages> <month> Oct. </month> <year> 1976. </year>
Reference-contexts: Typically, some additional information is furnished to aid in matching such as uniqueness and disparity constraints for random dot stereograms <ref> [28] </ref>, a third view [20] or even more views [32], shading information [15], or different filtered outputs [21]. Other approaches are classified as solving the structure-from-motion (SFM) problem [29], [36], [41], [37].
Reference: [29] <author> R. Mohr, F. Veillon, and L. Quan, </author> <title> "Relative 3D reconstruction using multiple uncalibrated images," </title> <booktitle> in Proceedings of IEEE Comp. Vis. and Patt. </booktitle> <address> Recog. </address> <month> 15-18 Jan. </month> <year> 1993, </year> <pages> pp. 543-548, </pages> <address> New York. </address>
Reference-contexts: Typically, some additional information is furnished to aid in matching such as uniqueness and disparity constraints for random dot stereograms [28], a third view [20] or even more views [32], shading information [15], or different filtered outputs [21]. Other approaches are classified as solving the structure-from-motion (SFM) problem <ref> [29] </ref>, [36], [41], [37]. For these algorithms, a set of features, e.g. edges in [37] and corners in [41], are identified and tracked. The motion of the camera and the structure of these features are then computed simultaneously.
Reference: [30] <author> R. M. Murray, Z. Li, and S. S. Sastry, </author> <title> A Mathematical Introduction to Robotic Manipulation, </title> <publisher> CRC Press, </publisher> <address> Boca Raton, </address> <year> 1994. </year>
Reference-contexts: Once all the points of the reference frame have been mapped into 3-D accordingly, they are then transformed according to the appropriate motion parameters. The notion of applying motion parameters to a frame has been addressed in conventional computer vision and robotics literature [18], <ref> [30] </ref>.
Reference: [31] <author> W. M. Newman and R. F. Sproull, </author> <title> Principles of Interactive Computer Graphics, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: For each patch, the four edges are stored in memory and sorted in decreasing v coordinate. Starting at the smallest v coordinate, every scan line is filled in according to an intensity-based interpolation scheme. In addition, we employ a software z-buffering technique [16], <ref> [31] </ref> to determine the ordering of patches with respect to the smallest depth. Thus, a given pixel in the view estimate is assigned an intensity corresponding to the patch closest to the camera.
Reference: [32] <author> M. Okutomi and T. Kanade, </author> <title> "A multiple-baseline stereo," </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <volume> vol. 15, no. 4, </volume> <pages> pp. 353-363, </pages> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: Typically, some additional information is furnished to aid in matching such as uniqueness and disparity constraints for random dot stereograms [28], a third view [20] or even more views <ref> [32] </ref>, shading information [15], or different filtered outputs [21]. Other approaches are classified as solving the structure-from-motion (SFM) problem [29], [36], [41], [37]. For these algorithms, a set of features, e.g. edges in [37] and corners in [41], are identified and tracked.
Reference: [33] <author> A. Shashua, </author> <title> "Projective structure from two uncalibrated images: Structure from motion and recognition," </title> <type> Tech. Rep. 1363, </type> <institution> MIT AI Laboratory, </institution> <month> Sept. </month> <year> 1992. </year>
Reference: [34] <author> R. Skerjanc and J. Liu, </author> <title> "A three camera approach for calculating disparity and synthesizing intermediate pictures," Signal Processing: </title> <journal> Image Communication, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 55-64, </pages> <year> 1991. </year>
Reference-contexts: For example, image mosaics [35], [27] restrict camera motion to be panning, not roughly translational. The approach of Skerjanc and Liu <ref> [34] </ref> requires a calibrated trinocular setup while that of Kanade et al. [22] uses clusters of fixed CHANG AND ZAKHOR: VIEW GENERATION 595 Fig. 22. Reconstructed "Mug" view from x y plane. Fig. 23. Reconstructed "Mug" view from translation along z axis. arrays of cameras.
Reference: [35] <author> R. Szeliski, </author> <title> "Image mosaicing for tele-realityapplications," </title> <type> Tech. Rep. CRL 94/2, </type> <institution> Digital Equipment Corporation: Cambridge Research Lab, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: The results for each of the views are quite reasonable. C. Comparison to Previous Work We found that most of the methods described in Section I have different input data requirements from the proposed approach, making it difficult for direct comparison. For example, image mosaics <ref> [35] </ref>, [27] restrict camera motion to be panning, not roughly translational. The approach of Skerjanc and Liu [34] requires a calibrated trinocular setup while that of Kanade et al. [22] uses clusters of fixed CHANG AND ZAKHOR: VIEW GENERATION 595 Fig. 22. Reconstructed "Mug" view from x y plane.
Reference: [36] <author> R. Szeliski and S. B. Kang, </author> <title> "Recovering 3D shape and motion from image streams using non-linear least squares," </title> <type> Tech. Rep. CRL 93/3, </type> <institution> Digital Equipment Corporation: Cambridge Research Lab, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Typically, some additional information is furnished to aid in matching such as uniqueness and disparity constraints for random dot stereograms [28], a third view [20] or even more views [32], shading information [15], or different filtered outputs [21]. Other approaches are classified as solving the structure-from-motion (SFM) problem [29], <ref> [36] </ref>, [41], [37]. For these algorithms, a set of features, e.g. edges in [37] and corners in [41], are identified and tracked. The motion of the camera and the structure of these features are then computed simultaneously. <p> Once the depth map for each reference frame has undergone spline approximation, we are left with depth estimates at different locations around the scene. The final step in the representation process is to estimate the relative camera motion between reference frames using an approach like <ref> [36] </ref>. Once the relative motion between all reference frames is known, a geometric relationship may be constructed among the different reference frames. This enables us to select the reference frames needed to use in the reconstruction stage.
Reference: [37] <author> C. J. Taylor and D. J. Kriegman, </author> <title> "Structure and motion from line segments in multiple images," </title> <type> Tech. Rep. </type> <institution> 9402b, Yale University, Center for Systems Science, </institution> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: Other approaches are classified as solving the structure-from-motion (SFM) problem [29], [36], [41], <ref> [37] </ref>. For these algorithms, a set of features, e.g. edges in [37] and corners in [41], are identified and tracked. The motion of the camera and the structure of these features are then computed simultaneously. <p> Other approaches are classified as solving the structure-from-motion (SFM) problem [29], [36], [41], <ref> [37] </ref>. For these algorithms, a set of features, e.g. edges in [37] and corners in [41], are identified and tracked. The motion of the camera and the structure of these features are then computed simultaneously. Despite the complexity of solving this nonlinear optimization problem under perspective projection, the SFM algorithms perform reasonably well given two or more arbitrary views.
Reference: [38] <author> S. Ullman and R. Basri, </author> <title> "Recognition by linear combinations of models," </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <volume> vol. 13, no. 10, </volume> <pages> pp. 992-1006, </pages> <month> Oct. </month> <year> 1991. </year>
Reference: [39] <author> J. Y. A. Wang and E. H. Adelson, </author> <title> "Representing moving images with layers," </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> vol. 3, no. 5, </volume> <pages> pp. 625-638, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Since most of the frames of the original data have been discarded, we do not utilize all the information about the scene with our representation. We are currently examining layered representations <ref> [39] </ref> and multivalued intensity and depth maps to encapsulate the scene information better [8]. Finally, a real-time implementation of the reconstruction algorithm would expedite the development of a virtual environment.
Reference: [40] <author> J. Weng, N. Ahuja, and T. S. Huang, </author> <title> "Matching two perspective views," </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <volume> vol. 14, no. 8, </volume> <pages> pp. 806-825, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: A low variance suggests that the block consists of little texture and nearly constant intensity. Matching the images in both directions helps to identify occluded regions and inconsistent matches [15], <ref> [40] </ref>. Occluded regions (OCCL) are precisely the unmatched points in the images, whereas inconsistent matches (INCONS ) may be found by validating matches in both directions.
Reference: [41] <author> J. Weng, N. Ahuja, and T. S. Huang, </author> <title> "Optimal motion and structure estimation," </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <volume> vol. 15, no. 9, </volume> <pages> pp. 864-884, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Other approaches are classified as solving the structure-from-motion (SFM) problem [29], [36], <ref> [41] </ref>, [37]. For these algorithms, a set of features, e.g. edges in [37] and corners in [41], are identified and tracked. The motion of the camera and the structure of these features are then computed simultaneously. <p> Other approaches are classified as solving the structure-from-motion (SFM) problem [29], [36], <ref> [41] </ref>, [37]. For these algorithms, a set of features, e.g. edges in [37] and corners in [41], are identified and tracked. The motion of the camera and the structure of these features are then computed simultaneously. Despite the complexity of solving this nonlinear optimization problem under perspective projection, the SFM algorithms perform reasonably well given two or more arbitrary views.

References-found: 41

