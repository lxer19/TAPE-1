URL: http://www.cs.nyu.edu/phd_students/baratloo/papers/hpdc95.ps.gz
Refering-URL: http://www.cs.nyu.edu/phd_students/baratloo/html/publications.html
Root-URL: http://www.cs.nyu.edu
Title: CALYPSO: A Novel Software System for Fault-Tolerant Parallel Processing on Distributed Platforms  
Author: Arash Baratloo Partha Dasgupta Zvi M. Kedem 
Affiliation: New York University  Arizona State University  New York University  
Note: Appeared the 4th IEEE International Symposium on High Performance Distributed Computing (HPDC), 1995. 1  
Abstract: CALYPSO is a prototype software system for writing and executing parallel programs on non-dedicated platforms, based on COTS networked workstations, operating systems, and compilers. Among notable properties of the system are: (1) simple programming paradigm incorporating shared memory constructs and separating the programming and the execution parallelism, (2) transparent utilization of unreliable shared resources by providing dynamic load balancing and fault tolerance, and (3) effective performance for large classes of coarse-grained computations. We present the system and report our initial experiments and performance results in settings that closely resemble the dynamic behavior of a real network. Under varying work-load conditions, resource availability and process failures, the efficiency of the test program we present ranged from 84% to 94% bench-marked against a sequential program. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Arabe, A. Beguelin, B. Lowekamp, E. Seligman, M. Starkey, and P. Stephan. DOME: </author> <title> Parallel programming in a heterogeneous multi-user environment. </title> <note> Submitted to Supercomputing, </note> <year> 1995. </year>
Reference-contexts: But unlike all of the previous systems, CALYPSO can mask process crashes. There have been several proposals to provide fault tolerance, mostly by augmenting an existing system. They include FT-PVM [32], FT-Linda [5], PLinda [19], and Orca [20]. A notable exception is DOME <ref> [1] </ref> that incorporated fault tolerance and load balancing form the onset. These systems provide fault tolerance by using well known mechanisms: checkpointing the data, logging messages, and using reliable atomic broadcasts. In contrast, CALYPSO uses a unified set of techniques to tolerate failures and slowdowns. <p> Machine C becomes available 60 seconds into the computation, and then disappears 120 seconds later. (See Figure 3.) Each machine profile P , is defined by the function availability P mapping time (in seconds) into the interval <ref> [0; 1] </ref>. Then if the computation lasted for time T , the work that was made available to us is R T t=0 availability P dt. This is the area of the shaded region (for time interval [0; T ]) in the top graphs of Figure 3.
Reference: [2] <author> J. Auerbach, A. Goldberg, G. Goldszmidt, A. Gopal, M. Kennedy, J. Rao, and J. Russell. Concert/C: </author> <title> A language for distributed programming. </title> <booktitle> In Proc. of the Winter 1994 USENIX Conf., </booktitle> <year> 1994. </year>
Reference-contexts: Message passing systems closely resemble the underlying hardware in a portable environment. Popular systems include PVM [16], P4 [9] and MPI [18]. The remote procedure call mechanism adds structure to message passing systems and makes programming a little simpler. Concert/C <ref> [2] </ref> from IBM and DCE-RPC [28] are examples of mature and portable packages. CALYPSO provides a high-level programming model that relieves the programmer from handling the underlying communication layer.
Reference: [3] <author> Y. Aumann, Z. Kedem, K. Palem, and M. Rabin. </author> <title> Highly efficient asynchronous execution of large-grained parallel programs. </title> <booktitle> In Proc. 34th IEEE Ann. Symp. on Foundations of Computer Science, </booktitle> <year> 1993. </year>
Reference: [4] <author> Y. Aumann and M. Rabin. </author> <title> Clock construction in fully asynchronous parallel systems and PRAM simulation. </title> <booktitle> In Proc. 33rd IEEE Ann. Symp. on Foundations of Computer Science, </booktitle> <year> 1992. </year>
Reference: [5] <author> D. Bakken and R. Schlichting. </author> <title> Supporting fault-tolerant parallel programming in Linda. </title> <type> Technical Report TR93-18, </type> <institution> The University of Arizona, </institution> <year> 1993. </year>
Reference-contexts: In fact, our daemons are modeled after Piranha. But unlike all of the previous systems, CALYPSO can mask process crashes. There have been several proposals to provide fault tolerance, mostly by augmenting an existing system. They include FT-PVM [32], FT-Linda <ref> [5] </ref>, PLinda [19], and Orca [20]. A notable exception is DOME [1] that incorporated fault tolerance and load balancing form the onset. These systems provide fault tolerance by using well known mechanisms: checkpointing the data, logging messages, and using reliable atomic broadcasts.
Reference: [6] <author> A. Baratloo, P. Dasgupta, Z. Kedem, and D. Krakovsky. </author> <title> CALYPSO goes to Wall Street: A case study. </title> <booktitle> In Proc. of Third Intl. Conf. on Artificial Intelligence Applications on Wall Street, </booktitle> <year> 1995. </year>
Reference-contexts: Appeared the 4th IEEE International Symposium on High Performance Distributed Computing (HPDC), 1995. 6 5 Experiments Several applications have been implemented in CSL. These include both toy examples and real applications, such as several sorting algorithms, DFT, computation of eigenvalues and eigenvectors, Option-Adjusted-Spread bond indices <ref> [6] </ref>, and modules of the Automatic Target Recognition software. We have tested the performance of some of these applications. For a clear, simple, and complete illustration, we present the exact performance results for the specific program listed in Section 3.
Reference: [7] <author> J. Bennett, J. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proc. 2nd Ann. Symp. on Principles and Practice of Parallel Programming, 1990. Appeared the 4th IEEE International Symposium on High Performance Distributed Computing (HPDC), 1995. 7 Appeared the 4th IEEE International Symposium on High Performance Distributed Computing (HPDC), </booktitle> <year> 1995. </year> <month> 8 </month>
Reference-contexts: Appeared the 4th IEEE International Symposium on High Performance Distributed Computing (HPDC), 1995. 3 Another class of systems for parallel computing focuses on providing DSM (Distributed Shared Memory) across loosely-coupled workstations. IVY [29] was one of the first implementations of DSM. Midway [8], Munin <ref> [7] </ref>, and now TreadMarks [27] and Quarks provide a weaker, and sometimes multiple consistency semantics in order to improve performance. In contrast, CALYPSO provides a simple, and a unified programming model that separates logical and execution parallelism.
Reference: [8] <author> B. Bershad, M. Zekauskas, and W. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In Proc. COMPCON, </booktitle> <year> 1993. </year>
Reference-contexts: Appeared the 4th IEEE International Symposium on High Performance Distributed Computing (HPDC), 1995. 3 Another class of systems for parallel computing focuses on providing DSM (Distributed Shared Memory) across loosely-coupled workstations. IVY [29] was one of the first implementations of DSM. Midway <ref> [8] </ref>, Munin [7], and now TreadMarks [27] and Quarks provide a weaker, and sometimes multiple consistency semantics in order to improve performance. In contrast, CALYPSO provides a simple, and a unified programming model that separates logical and execution parallelism.
Reference: [9] <author> J. Boyle, R. Butler, T. Disz, B. Glickfeld, E. Lusk, R. Over-beek, J. Patterson, and R. Stevens. </author> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <year> 1987. </year>
Reference-contexts: Among the many systems that directly address parallel computing on workstation networks, some focus on providing message passing mechanisms. Message passing systems closely resemble the underlying hardware in a portable environment. Popular systems include PVM [16], P4 <ref> [9] </ref> and MPI [18]. The remote procedure call mechanism adds structure to message passing systems and makes programming a little simpler. Concert/C [2] from IBM and DCE-RPC [28] are examples of mature and portable packages.
Reference: [10] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> C. ACM, </journal> <volume> 32, </volume> <year> 1989. </year>
Reference-contexts: In contrast, CALYPSO provides a simple, and a unified programming model that separates logical and execution parallelism. In addition, load balancing is not left for the programmer, rather provided by the system. Linda <ref> [10] </ref> is a variant of DSM that provides a common global space. Piranha [17] is built on top of Linda and allows workstations to join an ongoing computation as they become idle, and retreat when reclaimed by their owners. In fact, our daemons are modeled after Piranha.
Reference: [11] <author> P. Dasgupta, R. Ananthanarayanan, S. Menon, A. Mohin-dra, and R. Chen. </author> <title> Distributed programming with objects and threads in the Clouds system. </title> <journal> Computing Sytems, </journal> <volume> 4, </volume> <year> 1991. </year>
Reference: [12] <author> P. Dasgupta and R. C. Chen. </author> <title> Memory semantics for large grained persistent objects. </title> <editor> In A. Dearle, G. Shaw, and S. Zdonick, editors, </editor> <title> Implementation of Persistent Object Systems. </title> <publisher> Morgan Kaufman, </publisher> <year> 1990. </year>
Reference: [13] <author> P. Dasgupta, Z. Kedem, and M. Rabin. </author> <title> Parallel processing on networks of workstations: A fault-tolerant, high performance approach. </title> <booktitle> In Proc. of the 15th Intl. Conf. on Distributed Computing Systems, </booktitle> <year> 1995. </year>
Reference-contexts: An outline of a network of workstations-based system for parallel computing based on earlier formal work was presented in <ref> [13] </ref>. CALYPSO is an evolution of this design, and is the result of considerable redesign and extensive experimentation on progressively more and more sophisticated implementations. <p> However, it does not implement a fault-tolerant manager as described in <ref> [13] </ref>, which relied on dispersal and evasion. Among the many systems that directly address parallel computing on workstation networks, some focus on providing message passing mechanisms. Message passing systems closely resemble the underlying hardware in a portable environment. Popular systems include PVM [16], P4 [9] and MPI [18].
Reference: [14] <author> P. Dasgupta, R. J. LeBlanc, M. Ahamad, and U. Ramachan-dran. </author> <title> The Clouds distributed operating system. </title> <journal> IEEE Computer, </journal> <volume> 24, </volume> <year> 1991. </year>
Reference: [15] <author> M. Fu and P. Dasgupta. </author> <title> Programming support for memory mapped persistent objects. </title> <booktitle> In Proc. COMPSAC, </booktitle> <year> 1993. </year>
Reference: [16] <author> G. Geist and V. Sunderam. </author> <title> Network-based concurrent computing on the PVM system. </title> <journal> Concurrency: Practice and experience, </journal> <volume> 4, </volume> <year> 1992. </year>
Reference-contexts: Among the many systems that directly address parallel computing on workstation networks, some focus on providing message passing mechanisms. Message passing systems closely resemble the underlying hardware in a portable environment. Popular systems include PVM <ref> [16] </ref>, P4 [9] and MPI [18]. The remote procedure call mechanism adds structure to message passing systems and makes programming a little simpler. Concert/C [2] from IBM and DCE-RPC [28] are examples of mature and portable packages.
Reference: [17] <author> D. Gelernter, M. Jourdenais, and D. Kaminsky. </author> <title> Piranha scheduling: Strategies and their implementation. </title> <type> Technical Report TR-983, </type> <institution> Yale University Department of Computer Science, </institution> <year> 1993. </year>
Reference-contexts: In contrast, CALYPSO provides a simple, and a unified programming model that separates logical and execution parallelism. In addition, load balancing is not left for the programmer, rather provided by the system. Linda [10] is a variant of DSM that provides a common global space. Piranha <ref> [17] </ref> is built on top of Linda and allows workstations to join an ongoing computation as they become idle, and retreat when reclaimed by their owners. In fact, our daemons are modeled after Piranha. But unlike all of the previous systems, CALYPSO can mask process crashes.
Reference: [18] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing-Interface. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Among the many systems that directly address parallel computing on workstation networks, some focus on providing message passing mechanisms. Message passing systems closely resemble the underlying hardware in a portable environment. Popular systems include PVM [16], P4 [9] and MPI <ref> [18] </ref>. The remote procedure call mechanism adds structure to message passing systems and makes programming a little simpler. Concert/C [2] from IBM and DCE-RPC [28] are examples of mature and portable packages. CALYPSO provides a high-level programming model that relieves the programmer from handling the underlying communication layer.
Reference: [19] <author> K. Jeong and D. Shasha. Plinda 2.0: </author> <title> A transactional/checkpointing approach to fault tolerant linda. </title> <booktitle> In Proc. of the 13th Symp. on Reliable Distributed Systems, </booktitle> <year> 1994. </year>
Reference-contexts: In fact, our daemons are modeled after Piranha. But unlike all of the previous systems, CALYPSO can mask process crashes. There have been several proposals to provide fault tolerance, mostly by augmenting an existing system. They include FT-PVM [32], FT-Linda [5], PLinda <ref> [19] </ref>, and Orca [20]. A notable exception is DOME [1] that incorporated fault tolerance and load balancing form the onset. These systems provide fault tolerance by using well known mechanisms: checkpointing the data, logging messages, and using reliable atomic broadcasts.
Reference: [20] <author> M. Kaashoek, R. Michiels, H. Bal, and A. Tanenbaum. </author> <title> Transparent fault-tolerance in parallel Orca programs. Symp. on Experiences with Distributed and Multiprocessor Systems, </title> <year> 1992. </year>
Reference-contexts: In fact, our daemons are modeled after Piranha. But unlike all of the previous systems, CALYPSO can mask process crashes. There have been several proposals to provide fault tolerance, mostly by augmenting an existing system. They include FT-PVM [32], FT-Linda [5], PLinda [19], and Orca <ref> [20] </ref>. A notable exception is DOME [1] that incorporated fault tolerance and load balancing form the onset. These systems provide fault tolerance by using well known mechanisms: checkpointing the data, logging messages, and using reliable atomic broadcasts.
Reference: [21] <author> Z. Kedem. </author> <title> Methods for handling faults and asynchrony in parallel computations. </title> <booktitle> In Proc. DARPA Software Technology Conf., </booktitle> <year> 1992. </year>
Reference: [22] <author> Z. Kedem and K. Palem. </author> <title> Transformations for the automatic derivation of resilient parallel programs. </title> <booktitle> In Proc. IEEE Workshop on Fault-Tolerant Parallel and Distributed Systems, </booktitle> <year> 1992. </year>
Reference: [23] <author> Z. Kedem, K. Palem, M. Rabin, and A. Raghunathan. </author> <title> Efficient program transformations for resilient parallel computation via randomization. </title> <booktitle> In Proc. 24th ACM Symp. on Theory of Computing, </booktitle> <year> 1992. </year>
Reference: [24] <author> Z. Kedem, K. Palem, A. Raghunathan, and P. Spirakis. </author> <title> Combining tentative and definite algorithms for very fast dependable parallel computing. </title> <booktitle> In Proc. 23rd ACM Symp. on Theory of Computing, </booktitle> <year> 1991. </year>
Reference-contexts: The paradigm that CALYPSO embodies in a working system, was first described in [26]. That paper details a methodology for instrumenting general parallel programs to automatically obtain their fault-tolerant counterparts. While the solutions in [26] were formulated in the context of synchronous faults, they were later applied in <ref> [24] </ref> to a certain variant of asynchronous behavior (as does CALYPSO). A unified set of mechanisms, eager scheduling and collating differential memory, is used to provide the functionality of CALYPSO.
Reference: [25] <author> Z. Kedem, K. Palem, A. Raghunathan, and P. Spirakis. </author> <title> Resilient parallel computing on unreliable parallel machines. </title> <editor> In A. Gibbons and P. Spirakis, editors, </editor> <booktitle> Lectures on Parallel Computation. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1993. </year>
Reference: [26] <author> Z. Kedem, K. Palem, and P. Spirakis. </author> <title> Efficient robust parallel computations. </title> <booktitle> In Proc. 22nd ACM Symp. on Theory of Computing, </booktitle> <year> 1990. </year>
Reference-contexts: The paradigm that CALYPSO embodies in a working system, was first described in <ref> [26] </ref>. That paper details a methodology for instrumenting general parallel programs to automatically obtain their fault-tolerant counterparts. While the solutions in [26] were formulated in the context of synchronous faults, they were later applied in [24] to a certain variant of asynchronous behavior (as does CALYPSO). <p> The paradigm that CALYPSO embodies in a working system, was first described in <ref> [26] </ref>. That paper details a methodology for instrumenting general parallel programs to automatically obtain their fault-tolerant counterparts. While the solutions in [26] were formulated in the context of synchronous faults, they were later applied in [24] to a certain variant of asynchronous behavior (as does CALYPSO). A unified set of mechanisms, eager scheduling and collating differential memory, is used to provide the functionality of CALYPSO. <p> The idempotence property is fundamental in CALYPSO: a code segment can be executed multiple times (with possibly some partial executions), with exactly-once semantics. The importance of idempotence, and the utilization of the eager scheduling to take advantage of it, was discovered in <ref> [26] </ref> in an abstract context. (The term eager scheduling itself was coined recently.) Eager scheduling is a mechanism for assigning concurrently executable tasks to the available machines. Any machine can execute any enabled task independent of whether this task is already under execution by another machine. <p> And finally, any of the machines that are helping out the parallel computation can fail or slow down at any time. The mechanism of collating differential memory provides logical coherence and synchronization while avoiding false sharing. It is an adaption and refinement of the two-phase idempotent execution strategy <ref> [26] </ref> among others. Memory updates are collated to assure exactly-once logical execution, and they are transmitted as bitwise differences, preventing false sharing.
Reference: [27] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed shared memory on standard workstations anperating systemsd. </title> <booktitle> In Proc. of the Winter 94 Usenix Conf., </booktitle> <year> 1994. </year>
Reference-contexts: Appeared the 4th IEEE International Symposium on High Performance Distributed Computing (HPDC), 1995. 3 Another class of systems for parallel computing focuses on providing DSM (Distributed Shared Memory) across loosely-coupled workstations. IVY [29] was one of the first implementations of DSM. Midway [8], Munin [7], and now TreadMarks <ref> [27] </ref> and Quarks provide a weaker, and sometimes multiple consistency semantics in order to improve performance. In contrast, CALYPSO provides a simple, and a unified programming model that separates logical and execution parallelism. In addition, load balancing is not left for the programmer, rather provided by the system.
Reference: [28] <author> N. Leser. </author> <title> The Distributed Computing Environment naming architecture. </title> <address> OpenForum, </address> <year> 1992. </year>
Reference-contexts: Message passing systems closely resemble the underlying hardware in a portable environment. Popular systems include PVM [16], P4 [9] and MPI [18]. The remote procedure call mechanism adds structure to message passing systems and makes programming a little simpler. Concert/C [2] from IBM and DCE-RPC <ref> [28] </ref> are examples of mature and portable packages. CALYPSO provides a high-level programming model that relieves the programmer from handling the underlying communication layer.
Reference: [29] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7, </volume> <year> 1989. </year>
Reference-contexts: CALYPSO provides a high-level programming model that relieves the programmer from handling the underlying communication layer. Appeared the 4th IEEE International Symposium on High Performance Distributed Computing (HPDC), 1995. 3 Another class of systems for parallel computing focuses on providing DSM (Distributed Shared Memory) across loosely-coupled workstations. IVY <ref> [29] </ref> was one of the first implementations of DSM. Midway [8], Munin [7], and now TreadMarks [27] and Quarks provide a weaker, and sometimes multiple consistency semantics in order to improve performance. In contrast, CALYPSO provides a simple, and a unified programming model that separates logical and execution parallelism.
Reference: [30] <author> M. Rabin. </author> <title> Fingerprinting by random polynomials. </title> <type> Technical report, </type> <institution> Harvard University, </institution> <year> 1981. </year>
Reference: [31] <author> M. Rabin. </author> <title> Efficient dispersal of information for security, load balancing and fault tolerance. </title> <journal> J. ACM, </journal> <volume> 36, </volume> <year> 1989. </year>
Reference: [32] <author> V. Sunderam, G. Geist, J. Dongarra, and R. Manchek. </author> <title> The PVM concurrent computing system: Evolution, experiences, and trends. </title> <journal> Parallel Computing, </journal> <volume> 20, </volume> <year> 1994. </year>
Reference-contexts: In fact, our daemons are modeled after Piranha. But unlike all of the previous systems, CALYPSO can mask process crashes. There have been several proposals to provide fault tolerance, mostly by augmenting an existing system. They include FT-PVM <ref> [32] </ref>, FT-Linda [5], PLinda [19], and Orca [20]. A notable exception is DOME [1] that incorporated fault tolerance and load balancing form the onset. These systems provide fault tolerance by using well known mechanisms: checkpointing the data, logging messages, and using reliable atomic broadcasts.
References-found: 32

