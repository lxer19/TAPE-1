URL: http://www.cs.washington.edu/homes/vass/Links/GraphZPL-paper.ps
Refering-URL: http://www.cs.washington.edu/homes/vass/
Root-URL: 
Title: Design of Graph ZPL: Extensions to ZPL to Handle Irregular and Dynamic Data Structures  
Author: Vassily Litvinov 
Date: October 25, 1995  
Abstract: On distributed memory MIMD machines, ZPL is a powerful language for expressing parallel algorithms that can be described with regular parallel arrays. But this is not enough for many data parallel applications that require irregular or dynamic data structures. This paper is a report on the author's Graph ZPL project whose goals have been to select and design new language features to be added to ZPL that would enable handling such irregular and dynamic data structures. We first analyzed the successful design decisions made in ZPL and determined a set of design principles to follow in our design. Then we considered several data-parallel applications and determined a set of capabilities that needed to be added to ZPL. We design in detail language extensions to ZPL to provide these capabilities. They include a graph data type, irregular block partitioning for parallel arrays, and operations for dynamic repartitioning of graphs and parallel arrays. We give the reasoning behind our design choices and illustrate our extensions by writing several applications in Graph ZPL. Implementing Graph ZPL seems feasible with the modern level of technology. In comparison with other approaches to providing support for data-parallel computations, Graph ZPL results in cleaner and less error-prone programs which are easier to write and maintain, at the expense of limiting the class of algorithms that the programmer can express. 
Abstract-found: 1
Intro-found: 1
Reference: [Ashok and Zahorjan 1994] <author> Ashok, I., and Zahorjan, J. Adhara: </author> <title> runtime support for dynamic space-based applications on distributed memory MIMD multiprocessors. </title> <booktitle> Proceedings of the Scalable High-Performance Computing Conference, IEEE Computer Society Technical Committee on Supercomputer Applications, </booktitle> <month> 23-25 May </month> <year> 1994, </year> <pages> 168-175. </pages>
Reference-contexts: They differ in the degree of abstraction and the level of language support they provide, and also in the scope of applications they handle efficiently. Some systems specialize on handling certain classes of applications, and they achieve significant speedups for such applications. For example, Adhara <ref> [Ashok and Zahorjan 1994] </ref> is designed for dynamic space-based applications, such as particle-in-cell problems. It provides limited language support and an efficient runtime system for such applications. <p> Therefore, it is necessary to combine multiple values "assigned" to the node, and a reduction operation (any commutative and associative operation provided by the language) must be specified for that. (A similar idea has been implemented before, for example, in Adhara <ref> [Ashok and Zahorjan 1994] </ref>.) The Accumulate-AT operation on the left-hand side implements the non-owner computes paradigm. Its semantics is best described using a temporary graph of the same structure. First, such graph is substituted for the Accumulate-AT expression.
Reference: [Bodin et al. 1993] <author> Bodin, F., Beckman, P., Gannon D., Yang, S., Kesavan, S., Malony, A., Mohr, B. </author> <title> Imple menting a parallel C++ runtime system for scalable parallel systems. </title> <booktitle> Proceedings of Supercomputing, IEEE, ACM SIGARCH, </booktitle> <month> 5-19 November </month> <year> 1993, </year> <pages> 588-597. </pages>
Reference-contexts: Such libraries could be invoked from a non-parallel language such as C++. This approach has been implemented, for example, in pC++ <ref> [Bodin et al. 1993] </ref>. In comparison with language support, libraries offer greater flexibility at lower development cost. However, they make optimizing the program a much harder task for a general purpose compiler.
Reference: [Edjlali et al. 1995] <author> Edjlali, G., Agrawal, G., Sussman, A., Saltz, J. </author> <title> Data parallel programming in an adap tive environment. </title> <booktitle> Proceedings 9th International Parallel Processing Symposium, IEEE Computer Society Technical Committee on Parallel Processing, </booktitle> <month> 25-28 April </month> <year> 1995, </year> <pages> 827-832. </pages>
Reference-contexts: The compiler could automatically insert calls to the appropriate testing and redistributing routines in the generated code. A detailed description of the problem and a possible implementation of the runtime system can be found, for example, in <ref> [Edjlali et al. 1995] </ref>.
Reference: [HPFF 1993] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> May, </month> <year> 1993. </year>
Reference-contexts: It assumes no language support. The Fortran family of languages and HPF (see, for example, <ref> [HPFF 1993] </ref>) use arrays as the basic data type. To obtain parallel a program, the compiler applies parallelizing techniques to code written mostly in sequential terms. <p> This section briefly describes several existing systems that support data parallel programming and compares them with the proposed approach. 40 10.1 Fortran Family and HPF The Fortran family of languages and HPF (see, for example, <ref> [HPFF 1993] </ref>) encourage expressing a program in sequential terms. Compiler parallelizing techniques are applied to achieve parallelism (for example, paral-lelizing a loop in the absence of loop-carried dependencies). In most programs, the primary aggregate data structure is array with explicit or implicit indexing.
Reference: [Kohn and Baden 1994] <author> Kohn, S., and Baden, S. </author> <title> A Robust Parallel Programming Model for Dynamic Non Uniform Scientific Computations. </title> <type> Technical Report CS94-354, </type> <institution> University of California, </institution> <address> San Diego, </address> <month> March, </month> <year> 1994. </year>
Reference-contexts: In Graph ZPL, the use of parallel arrays or graphs represents parallelism explicitly. The Fortran sequential paradigm hinders development of parallel algorithms. In particular, sequential description of many parallel operations, such as reductions, introduces unnecessary constraints that the compiler may or may not eliminate. 10.2 LPAR In LPAR <ref> [Kohn and Baden 1994] </ref>, arrays with explicit indexing are the primary aggregate data structure for numeric data. LPAR introduces X-arrays the elements of which are to be distributed across the processors. Each element of an X-array is typically a regular array representing the local portion of data for each processor.
Reference: [Lin 1994] <author> Lin, C. </author> <title> ZPL Language Reference Manual. </title> <type> Technical Report UW-CSE-TR 94-10-06, </type> <institution> University of Washington, </institution> <month> October, </month> <year> 1994. </year>
Reference-contexts: However, it is only a very brief overview. The reader is referred to <ref> [Lin 1994] </ref> for further details. 2.1 Data Structures All data structures fall into two categories, scalar and parallel. The scalar data structures are those found in Pascal-like languages (except for pointers), such as integers, reals, arrays, records, etc. These data structures are processed sequentially.
Reference: [Lin and Snyder 1994] <author> Lin, C., and Snyder, L. ZPL: </author> <title> An Array Sublanguage. </title> <editor> In U. Banerjee et al. (eds.), </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Also, lower-level details result in the loss of higher-level information that the compiler might be able to explore to perform a better optimization. The ZPL language <ref> [Lin and Snyder 1994] </ref> provides high-level abstractions of aggregate data types and data-parallel operations on them. The ZPL programmer conveys high-level information to the compiler which enables optimizations, for example, of communication and array accesses. <p> One general observation is that the more high-level information com municated to the compiler, the more opportunities for optimization and achieving better performance it has. In particular, the following optimizations could be performed by the Graph ZPL compiler (most of them are already performed by the UW ZPL compiler <ref> [Lin and Snyder 1994] </ref> for parallel arrays): loop fusion minimizing loop overhead and improving cache locality communication optimizations combining multiple communications where possible and removing unnecessary communications array contraction and copy direction analysis allow potentially to reduce significantly the amount of memory used by the program. <p> An application of this method to solve Laplace's equation on a rectangle has been used to illustrate the capabilities of the ZPL language <ref> [Lin and Snyder 1994] </ref>. Here we are using a similar algorithm on irregular meshes. Each iteration of the method consists of updating the values at the mesh points depending on their neighbors, and finding the maximum, among the mesh points, difference between the new and old values. <p> Mesh.Density := Compute_Density (Mesh, Mesh@East, Mesh@West, Mesh@North, Mesh@South); -- reaction phase increm_load_balance_do (Load, nIter%rebalanceFrequency = 0) begin Compute_Reaction (Mesh, Load); end; end; 9 Implementation Strategies This section sketches the key components in which an implementation of Graph ZPL could differ from the implementation of ZPL at the University of Washington <ref> [Lin and Snyder 1994] </ref>. It discusses primarily the strategies of achieving a Graph ZPL program's semantics at runtime, which include data representations and partitioning and communication functions. Any implementation is likely to proceed incrementally. For Graph ZPL, it seems reasonable to start with implementation of irregular array partitioning and repartitioning.
Reference: [Ponnusamy et al. 1993] <author> Ponnusamy, R., Saltz, J., Choudhary, A., Hwang, Y.-S. </author> <title> Runtime Support and Compilation Methods for User-Specified Irregular Data Distributions. </title> <institution> University of Maryland: Department of Computer Science Technical Report CS-TR-3194 and UMIACS-TR-93-135 </institution>
Reference-contexts: arrays to achieve load balancing, where the per processor load is determined as a sum of per-array element loads defined by the user * allow dynamic repartitioning of parallel arrays * provide the ability to compute new partitionings incrementally 9 Flame Simulation In this application, described in more detail in <ref> [Ponnusamy et al. 1993] </ref>, the space is divided into cells forming a regular mesh. Each cell contains some amount of fluid which participates both in fluid motion and the combustion process. Correspondingly, the computation at each time step consists of a convection phase and a reaction phase. <p> expression as LHS: -- masses attached to springs accumulate the forces Springs +@ m1.force.X := Force1X; Springs +@ m1.force.Y := Force1Y; Springs +@ m2.force.X := Force2X; Springs +@ m2.force.Y := Force2Y; -- Phase 3: updating velocities and positions Compute_Movement (Masses); end; /*for*/ 8.3 Flame Simulation This application is described in <ref> [Ponnusamy et al. 1993] </ref>. Application Description In this application, the space is divided into cells forming a regular mesh. Each cell contains some amount of fluid which participates both in fluid motion and the combustion process.
Reference: [Rault and Woronowicz 1993] <author> Rault, D. F. G., and Woronowicz, M. S. </author> <title> Spacecraft contamination investiga tion by direct simulation Monte Carlo Contamination on UARC/HALOE. </title> <booktitle> Proceedings AIAA 31th Aerospace Sciences Meeting and Exhibit, </booktitle> <month> January </month> <year> 1993. </year>
Reference: [Saltz et al. 1995] <author> Saltz, J., Ponnusammy, R., Sharma, S., Moon, B., Hwang, Y.-S., Uysal, M., Das, </author> <note> R. </note>
Reference-contexts: For example, Adhara [Ashok and Zahorjan 1994] is designed for dynamic space-based applications, such as particle-in-cell problems. It provides limited language support and an efficient runtime system for such applications. The CHAOS library <ref> [Saltz et al. 1995] </ref> contains a set of runtime routines sufficient to implement efficiently a wide variety of algorithms based on irregular and dynamic data structures, such as adaptive computational fluid dynamic solvers, molecular dynamic codes, etc. It assumes no language support. <p> However, they are still distributed across the processors, and the owner-computes rule is applied to all the computations on graphs except for Accumulate-AT operations. To implement many of the operations on graphs, it seems possible to port some part of the CHAOS Runtime Library <ref> [Saltz et al. 1995] </ref>, which has been developed to assist in runtime support for irregular data structures. 9.2.1 Structure Representation The following per-processor data structures could be used to represent the nodes and edges of a graph: * local node array: an array of node IDs that are local to the
References-found: 10

