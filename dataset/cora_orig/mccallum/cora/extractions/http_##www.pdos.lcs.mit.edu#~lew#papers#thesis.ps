URL: http://www.pdos.lcs.mit.edu/~lew/papers/thesis.ps
Refering-URL: http://www.pdos.lcs.mit.edu/~lew/
Root-URL: 
Title: A Case Study of Shared Memory and Message Passing: The Triangle Puzzle  
Author: by Kevin A. Lew Kirk Johnson 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Master of Science at the  All Rights Reserved. Author  Certified by Professor M. Frans Kaashoek Department of Electrical Engineering and Computer Science Thesis Supervisor Certified by  Thesis Co-Supervisor Accepted by Professor Frederic R. Morgenthaler Chair, Department Committee on Graduate Students  
Date: January 20, 1995  January 20, 1995  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Massachusetts Institute of Technology, 1995.  Department of Electrical Engineering and Computer Science  Department of Electrical Engineering and Computer Science  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. Applegate, G. Jacobson, and D. </author> <type> Sleator. </type> <institution> Computer Analysis of Sprouts. Carn-egie Mellon University, School of Computer Science, </institution> <type> TR 91-144, </type> <month> May </month> <year> 1991. </year>
Reference-contexts: When an extension of a position is found in the transposition table, the subtree generated from this extension does not need to be explored again, and we can instead join the extension and the position in the transposition table (this is also known as folding or fusing positions) <ref> [1, 23] </ref>. In the triangle puzzle, because the number of joined positions is large, the size of the search tree is greatly reduced by using a transposition table. For problem sizes 5, 6, and 7, 66%, 83%, and 90% of all positions explored are joined, respectively. <p> These include compression <ref> [1] </ref>, evicting positions from the transposition table when a threshold is reached [4, 18], pre-loading the transposition table [1], pruning heuristics besides exploiting symmetry [10, 21], and move ordering [18]. We did not explore the first two optimizations, and we do not employ the remaining ones. <p> These include compression <ref> [1] </ref>, evicting positions from the transposition table when a threshold is reached [4, 18], pre-loading the transposition table [1], pruning heuristics besides exploiting symmetry [10, 21], and move ordering [18]. We did not explore the first two optimizations, and we do not employ the remaining ones.
Reference: [2] <author> A. Agarwal, R. Bianchini, D. Chaiken, K. Johnson, D. Kranz, J. Kubiatowicz, B.-H. Lim, K. Mackenzie, and D. Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <note> Submitted for publication, </note> <month> December </month> <year> 1994. </year>
Reference-contexts: A shared-memory version can outperform the message-passing implementation (by up to 14% for our application) under low contention because shared memory offers low-overhead data access. Our implementations run on the MIT Alewife multiprocessor <ref> [2] </ref>. The message-passing implementation was ported from a message-passing implementation that runs on Thinking Machines CM-5 family of multicomputers [25]. <p> Each node consists of a Sparcle processor [3] clocked at 20MHz, a 64KB direct-mapped cache with 16-byte cache lines, a communications and memory management unit (CMMU), a oating-point coprocessor, an Elko-series mesh routing chip (EMRC) from Caltech, and 8MB of memory <ref> [2] </ref>. The EMRCs implement a direct network [24] with a two-dimensional mesh topology using wormhole routing [11]. A mesh-connected SCSI disk array provides I/O. Figure 4.1 shows the Alewife architecture. <p> Since the version of the Alewife software system used in this case study does not offer polling (later versions are expected to), polling was not used. For further details on how message passing is implemented, the reader is referred to the papers by Agarwal et al. <ref> [2] </ref> and Kranz et al. [16]. 32 4.2 Shared-Memory Implementations We developed several shared-memory implementations of PBFS, all of which use Mellor-Crummey-Scott queue locks [20]. Our baseline shared-memory implementation (SM) takes the simplest approach by putting the transposition table and the work queues in the shared-memory bank of one processor.
Reference: [3] <author> A. Agarwal, J. Kubiatowicz, D. Kranz, B.-H. Lim, D. Yeung, G. DSouza, and M. Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Large-Scale Multiprocessors. </title> <booktitle> IEEE Micro, </booktitle> <month> June </month> <year> 1993, </year> <pages> pp. 48-61. </pages>
Reference-contexts: Alewife is a distributed shared-memory, cache-coherent multiprocessor. Each processor is tightly coupled with a memory bank. Each node contains its own memory bank, part of which is used as a portion of the single shared address space. Each node consists of a Sparcle processor <ref> [3] </ref> clocked at 20MHz, a 64KB direct-mapped cache with 16-byte cache lines, a communications and memory management unit (CMMU), a oating-point coprocessor, an Elko-series mesh routing chip (EMRC) from Caltech, and 8MB of memory [2].
Reference: [4] <author> M. Bischoff. </author> <title> Approaches for Solving the Tri-Puzzle. </title> <month> November </month> <year> 1993. </year> <note> Available via anonymous ftp from lucy.ifi.unibas.ch as tri-puzzle/michael/ doku.ps. </note>
Reference-contexts: However, doing the same for problem size 7 cuts the number of positions explored by a factor of almost six <ref> [4] </ref>. <p> These include compression [1], evicting positions from the transposition table when a threshold is reached <ref> [4, 18] </ref>, pre-loading the transposition table [1], pruning heuristics besides exploiting symmetry [10, 21], and move ordering [18]. We did not explore the first two optimizations, and we do not employ the remaining ones. <p> For example, for problem size 5 on 64 processors, performance degrades by a factor of three (setup time included). The triangle puzzle has been solved using sequential algorithms. Bischoff <ref> [4] </ref>, who won second place in the Internet contest mentioned in Chapter 1, uses algorithmic techniques similar to ours in a sequential algorithm that runs on various machines.
Reference: [5] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the Twenty-first Annual International Symposium on Computer Architecture, </booktitle> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year> <pages> pp. 142-153. </pages>
Reference-contexts: explicitly request data by sending a message to the remote processor, which then responds with a message containing the requested data. (Of course, a shared-memory programming model (i.e., a single address space) can be implemented using this message-passing architecture; messages are used to keep data values consistent across all processors <ref> [5, 6] </ref>.) Thus, the check described in the previous paragraph is not performed by hardware in message-passing machines. This is the main difference between shared-memory and message-passing machines.
Reference: [6] <author> J. Carter, J. Bennett, W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <address> Pacific Grove, California, </address> <month> October </month> <year> 1991. </year> <pages> pp. 152-164. </pages>
Reference-contexts: explicitly request data by sending a message to the remote processor, which then responds with a message containing the requested data. (Of course, a shared-memory programming model (i.e., a single address space) can be implemented using this message-passing architecture; messages are used to keep data values consistent across all processors <ref> [5, 6] </ref>.) Thus, the check described in the previous paragraph is not performed by hardware in message-passing machines. This is the main difference between shared-memory and message-passing machines.
Reference: [7] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, California, </address> <month> April </month> <year> 1991, </year> <pages> pp. 224-234. </pages>
Reference-contexts: Figure 4.1 shows the Alewife architecture. The CMMU implements shared-memory and message-passing communication interfaces, which will be described next. 313131 4.1.1 Shared-Memory Interface Shared memory is accessed by loads and stores. Caches store recently accessed shared data. These caches are kept coherent by a directory-based protocol called LimitLESS <ref> [7] </ref>. Directory-based protocols were described in Chapter 2.1. LimitLESS implements the directory in both hardware and software.
Reference: [8] <author> S. Chandra, J. Larus, and A. Rogers. </author> <title> Where Is Time Spent in Message-Passing and Shared-Memory Implementations? In Proceedings of Architectural Support for Programming Languages and Operating Systems VI, </title> <address> San Jose, California, </address> <month> October </month> <year> 1994, </year> <pages> pp. 61-73. </pages>
Reference-contexts: In fact, Alewife is the only existing machine of its class, and is thus a unique platform on which to compare shared-memory and message-passing implementations. Previous research comparing shared-memory and message-passing implementations of the same application has resorted to either using the same machine to run different simulators <ref> [8] </ref> or using different machines to run different simulators [19]. The Stanford FLASH multiprocessor [17] also efficiently supports these two programming models, but has yet to be built. <p> For both problem sizes and eight and more processors, 2,000 or fewer cycles are spent executing LimitLESS software. 50 515151 Chapter 5 Related Work Several papers compare shared-memory and message-passing versions of parallel applications. Chandra et al. <ref> [8] </ref> compare shared-memory and message-passing implementations of four parallel applications. However, unlike our comparison, which uses an actual machine to make measurements, they simulate a distributed, cache-coherent shared-memory machine and a message-passing machine by using different simulators that run on a CM-5.
Reference: [9] <author> P.-C. Chen. </author> <title> Heuristic Sampling on Backtrack Trees. </title> <type> Ph.D. thesis. </type> <institution> Stanford University. </institution> <month> May </month> <year> 1989. </year> <note> (Also available as Stanford Technical Report CS 89-1258.) </note>
Reference-contexts: We decided to dynamically compute reections of positions because physical memory per node on Alewife is low (5MB maximum usable memory) and there is no virtual memory yet. The triangle puzzle is similar to other tree-search problems, such as the N-queens problem <ref> [9, 12, 23, 27, 28] </ref>, the Hi-Q puzzle [23], and chess [18].
Reference: [10] <author> L. Crowl, M. Crovella, T. LeBlanc, M. Scott. </author> <title> Beyond Data Parallelism: The Advantages of Multiple Parallelizations in Combinatorial Search. </title> <institution> University of Rochester, Dept. of Computer Science, </institution> <type> TR 451, </type> <month> April </month> <year> 1993. </year>
Reference-contexts: These include compression [1], evicting positions from the transposition table when a threshold is reached [4, 18], pre-loading the transposition table [1], pruning heuristics besides exploiting symmetry <ref> [10, 21] </ref>, and move ordering [18]. We did not explore the first two optimizations, and we do not employ the remaining ones. We cannot pre-load the transposition table because information obtained at one point in the search gives no information about later stages of the search.
Reference: [11] <author> W. Dally. </author> <title> A VLSI Architecture for Concurrent Data Structures. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1987. </year>
Reference-contexts: The EMRCs implement a direct network [24] with a two-dimensional mesh topology using wormhole routing <ref> [11] </ref>. A mesh-connected SCSI disk array provides I/O. Figure 4.1 shows the Alewife architecture. The CMMU implements shared-memory and message-passing communication interfaces, which will be described next. 313131 4.1.1 Shared-Memory Interface Shared memory is accessed by loads and stores. Caches store recently accessed shared data.
Reference: [12] <author> R. Finkel and U. Manber. </author> <title> DIB--A Distributed Implementation of Backtracking. </title> <journal> ACM Transactions on Programming Languages and Systems, April 1987, </journal> <volume> Vol. 9, No. 2. </volume> <pages> pp. 235-256. 64 </pages>
Reference-contexts: We decided to dynamically compute reections of positions because physical memory per node on Alewife is low (5MB maximum usable memory) and there is no virtual memory yet. The triangle puzzle is similar to other tree-search problems, such as the N-queens problem <ref> [9, 12, 23, 27, 28] </ref>, the Hi-Q puzzle [23], and chess [18].
Reference: [13] <author> J. Gittinger; T. Chikayama and K. Kumon. </author> <title> Proofs that there are no solutions for the triangle puzzle for size 3N+1. </title> <note> Available via anonymous ftp from lucy.ifi.unibas.ch in directories tri-puzzle/gitting and tri-puzzle/jm. </note> <month> November </month> <year> 1993. </year>
Reference-contexts: We refer to the number of holes per side as the problem size. We solved problem sizes 5, 6, and 7 on a 32-node Alewife machine. Problem size 5 has 1,550 solutions. Problem size 6 has 29,235,690,234 solutions. Problem size 7 has zero solutions. Gittinger, Chikayama, and Kumon <ref> [13] </ref> show that there are zero solutions for problem sizes 3N+1 (N 2). Solving problem size 8 is expected to require several gigabytes of memory [14]; to the best of our knowledge, no one has solved it yet. The rest of this thesis is organized as follows.
Reference: [14] <author> S. Gutzwiller and G. Haechler. </author> <title> Contest: How to Win a Swiss Toblerone Chocolate! August 1993, </title> <publisher> Usenet newsgroup comp.parallel. </publisher>
Reference-contexts: Our implementations run on the MIT Alewife multiprocessor [2]. The message-passing implementation was ported from a message-passing implementation that runs on Thinking Machines CM-5 family of multicomputers [25]. The original CM-5 implementation written by Kirk Johnson won first place in an Internet newsgroup contest <ref> [14] </ref>, the goal of which was to solve the triangle puzzle in the shortest time. Alewife efficiently supports both message-passing and cache-coherent shared-memory programming models in hardware. <p> Problem size 5 has 1,550 solutions. Problem size 6 has 29,235,690,234 solutions. Problem size 7 has zero solutions. Gittinger, Chikayama, and Kumon [13] show that there are zero solutions for problem sizes 3N+1 (N 2). Solving problem size 8 is expected to require several gigabytes of memory <ref> [14] </ref>; to the best of our knowledge, no one has solved it yet. The rest of this thesis is organized as follows. Chapter 2 provides background on the architectural, programming, and performance issues when using shared-memory and message-passing machines.
Reference: [15] <author> P. Hatcher and M. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> The MIT Press: </publisher> <year> 1991. </year>
Reference-contexts: A solution to the puzzle is a sequence of moves that leaves only one peg on the board. Counting the number of distinct solutions to the triangle puzzle is the goal of the triangle puzzle search problem <ref> [15] </ref>. Because we must find all solutions, solving this search problem involves an exhaustive search. This search problem can be extended for puzzles with boards that have more than five holes on each side. We refer to the number of holes per side as the problem size. <p> However, they use different simulators for their shared-memory and message-passing machines, and run each simulator on a different machine. Kranz et al. [16] evaluate shared-memory and message-passing microbench-marks on Alewife, and one of their results supports our first main conclusion. A noteworthy parallel solution by Hatcher and Quinn <ref> [15] </ref> solves the triangle puzzle on different parallel machines that support either shared memory or message passing in hardware.
Reference: [16] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B.-H. Lim. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year> <pages> pp. 54-63. </pages>
Reference-contexts: P = processor, M = memory bank. Network P P P P ... P = processor, M = memory bank. 191919 specialized hardware. By using this hardware support, shared-memory accesses can be performed using a single instruction <ref> [16] </ref>. To illustrate cache coherence actions, we describe actions that a directory-based cache coherence protocol executes when a processor references remote data that is not present in its cache. <p> We assume that all synchronization and data transfers are achieved by using simple library calls. 2.3 Performance Issues There are certain scenarios in which shared memory can perform worse than message passing <ref> [16] </ref>. First, if the grain size of shared data is larger than a cache line, shared memory may perform worse because a data transfer requires multiple coherence actions, which demand more network bandwidth and increase the latency of the data transfer. <p> For further details on how message passing is implemented, the reader is referred to the papers by Agarwal et al. [2] and Kranz et al. <ref> [16] </ref>. 32 4.2 Shared-Memory Implementations We developed several shared-memory implementations of PBFS, all of which use Mellor-Crummey-Scott queue locks [20]. Our baseline shared-memory implementation (SM) takes the simplest approach by putting the transposition table and the work queues in the shared-memory bank of one processor. <p> Mar-tonosi and Gupta [19] compare shared-memory and message-passing implementations of a standard cell router called LocusRoute. However, they use different simulators for their shared-memory and message-passing machines, and run each simulator on a different machine. Kranz et al. <ref> [16] </ref> evaluate shared-memory and message-passing microbench-marks on Alewife, and one of their results supports our first main conclusion. A noteworthy parallel solution by Hatcher and Quinn [15] solves the triangle puzzle on different parallel machines that support either shared memory or message passing in hardware.
Reference: [17] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the Twenty-first International Symposium on Computer Architecture, </booktitle> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year> <pages> pp. 302-313. </pages>
Reference-contexts: Previous research comparing shared-memory and message-passing implementations of the same application has resorted to either using the same machine to run different simulators [8] or using different machines to run different simulators [19]. The Stanford FLASH multiprocessor <ref> [17] </ref> also efficiently supports these two programming models, but has yet to be built. We chose the triangle puzzle because it is simple enough to understand and solve, yet exhibits many of the characteristics of complex tree-search problems.
Reference: [18] <author> B. Kuszmaul. </author> <title> Synchronized MIMD Computing. </title> <type> Ph.D. thesis. </type> <institution> MIT Laboratory for Computer Science. </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: These include compression [1], evicting positions from the transposition table when a threshold is reached <ref> [4, 18] </ref>, pre-loading the transposition table [1], pruning heuristics besides exploiting symmetry [10, 21], and move ordering [18]. We did not explore the first two optimizations, and we do not employ the remaining ones. <p> These include compression [1], evicting positions from the transposition table when a threshold is reached [4, 18], pre-loading the transposition table [1], pruning heuristics besides exploiting symmetry [10, 21], and move ordering <ref> [18] </ref>. We did not explore the first two optimizations, and we do not employ the remaining ones. We cannot pre-load the transposition table because information obtained at one point in the search gives no information about later stages of the search. <p> The triangle puzzle is similar to other tree-search problems, such as the N-queens problem [9, 12, 23, 27, 28], the Hi-Q puzzle [23], and chess <ref> [18] </ref>. Many of the search techniques, such as exploiting symmetry to reduce the search space and using a transposition table, arise in solving these problems. 535353 Chapter 6 Conclusions We presented parallel shared-memory and message-passing implementations that solve the triangle puzzle.
Reference: [19] <author> M. Martonosi and A. Gupta. </author> <title> Trade-offs in Message Passing and Shared Memory Implementations of a Standard Cell Router. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <institution> Pennsylvania State University Park, Pennsylvania, </institution> <month> August </month> <year> 1989. </year> <pages> pp. </pages> <note> III-88 to III-96. </note>
Reference-contexts: Previous research comparing shared-memory and message-passing implementations of the same application has resorted to either using the same machine to run different simulators [8] or using different machines to run different simulators <ref> [19] </ref>. The Stanford FLASH multiprocessor [17] also efficiently supports these two programming models, but has yet to be built. We chose the triangle puzzle because it is simple enough to understand and solve, yet exhibits many of the characteristics of complex tree-search problems. <p> In addition, their message-passing machine simulator uses Thinking Machines CMMD message-passing library, which has questionable efficiency. Moreover, their simulators assume constant network latency and do not account for network contention. Mar-tonosi and Gupta <ref> [19] </ref> compare shared-memory and message-passing implementations of a standard cell router called LocusRoute. However, they use different simulators for their shared-memory and message-passing machines, and run each simulator on a different machine.
Reference: [20] <author> J.M. Mellor-Crummey and M.L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, February 1991, </journal> <volume> Vol. 9, No. 1. </volume> <pages> pp. 21-65. </pages>
Reference-contexts: For further details on how message passing is implemented, the reader is referred to the papers by Agarwal et al. [2] and Kranz et al. [16]. 32 4.2 Shared-Memory Implementations We developed several shared-memory implementations of PBFS, all of which use Mellor-Crummey-Scott queue locks <ref> [20] </ref>. Our baseline shared-memory implementation (SM) takes the simplest approach by putting the transposition table and the work queues in the shared-memory bank of one processor.
Reference: [21] <author> J. Pearl. </author> <title> Heuristics: Intelligent Search Strategies for Computer Problem Solving. </title> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference-contexts: These include compression [1], evicting positions from the transposition table when a threshold is reached [4, 18], pre-loading the transposition table [1], pruning heuristics besides exploiting symmetry <ref> [10, 21] </ref>, and move ordering [18]. We did not explore the first two optimizations, and we do not employ the remaining ones. We cannot pre-load the transposition table because information obtained at one point in the search gives no information about later stages of the search.
Reference: [22] <author> C. Polychronopoulos and D. Kuck. </author> <title> Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <month> December </month> <year> 1987. </year> <pages> pp. 1425-1439. </pages>
Reference-contexts: This random load distribution policy distributes work fairly evenly. As shown in Table 4.2, the percentage of total execution time a pro cessor on the average spends idle is small. Thus, employing more sophisticated schedul ing techniques, such as self-scheduling <ref> [22] </ref>, is not warranted. 4.2.4 Prefetching In our experiments with prefetching, the execution time of the Improved DTABQ implementation can be improved at most 3.8% for problem size 6. We experimented with prefetching data in three places.
Reference: [23] <author> E. Reingold, J. Nievergelt, N. Deo. </author> <title> Combinatorial Algorithms: Theory and Practice. </title> <publisher> Prentice Hall, </publisher> <year> 1977. </year>
Reference-contexts: When an extension of a position is found in the transposition table, the subtree generated from this extension does not need to be explored again, and we can instead join the extension and the position in the transposition table (this is also known as folding or fusing positions) <ref> [1, 23] </ref>. In the triangle puzzle, because the number of joined positions is large, the size of the search tree is greatly reduced by using a transposition table. For problem sizes 5, 6, and 7, 66%, 83%, and 90% of all positions explored are joined, respectively. <p> We decided to dynamically compute reections of positions because physical memory per node on Alewife is low (5MB maximum usable memory) and there is no virtual memory yet. The triangle puzzle is similar to other tree-search problems, such as the N-queens problem <ref> [9, 12, 23, 27, 28] </ref>, the Hi-Q puzzle [23], and chess [18]. <p> The triangle puzzle is similar to other tree-search problems, such as the N-queens problem [9, 12, 23, 27, 28], the Hi-Q puzzle <ref> [23] </ref>, and chess [18]. Many of the search techniques, such as exploiting symmetry to reduce the search space and using a transposition table, arise in solving these problems. 535353 Chapter 6 Conclusions We presented parallel shared-memory and message-passing implementations that solve the triangle puzzle.
Reference: [24] <author> C. Seitz. </author> <title> Concurrent VLSI Architectures. </title> <journal> IEEE Transactions on Computers, </journal> <month> December </month> <year> 1984. </year> <pages> pp. 1247-1265. </pages>
Reference-contexts: Each node consists of a Sparcle processor [3] clocked at 20MHz, a 64KB direct-mapped cache with 16-byte cache lines, a communications and memory management unit (CMMU), a oating-point coprocessor, an Elko-series mesh routing chip (EMRC) from Caltech, and 8MB of memory [2]. The EMRCs implement a direct network <ref> [24] </ref> with a two-dimensional mesh topology using wormhole routing [11]. A mesh-connected SCSI disk array provides I/O. Figure 4.1 shows the Alewife architecture. The CMMU implements shared-memory and message-passing communication interfaces, which will be described next. 313131 4.1.1 Shared-Memory Interface Shared memory is accessed by loads and stores.
Reference: [25] <institution> Thinking Machines Corporation. </institution> <type> Connection Machine CM-5 Technical Summary. </type> <month> Nov. </month> <year> 1993. </year> <month> 656565 </month>
Reference-contexts: Our implementations run on the MIT Alewife multiprocessor [2]. The message-passing implementation was ported from a message-passing implementation that runs on Thinking Machines CM-5 family of multicomputers <ref> [25] </ref>. The original CM-5 implementation written by Kirk Johnson won first place in an Internet newsgroup contest [14], the goal of which was to solve the triangle puzzle in the shortest time. Alewife efficiently supports both message-passing and cache-coherent shared-memory programming models in hardware.
Reference: [26] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year> <pages> pp. 256-266. </pages>
Reference-contexts: On the other hand, when using message passing, synchronization and data transfer can be coupled. When one processor transfers data to another processor by sending the data in a message, the receiving processor can process the message by atomically executing a block of code, as in active messages <ref> [26] </ref>. Because this processing is performed atomically, this code can be thought of as a critical section; it is if a lock were acquired, the code is executed, and the lock were released. <p> This organization is illustrated in Figure 4.2. When a processor sends a message to another processor, the receiving processor is interrupted and then processes the message. This message-passing model provides the functionality of active messages <ref> [26] </ref>. The end-to-end latency of an active message, in which delivery interrupts the receiving processor, is just over 100 cycles. Since the version of the Alewife software system used in this case study does not offer polling (later versions are expected to), polling was not used.
Reference: [27] <author> C.K. Yuen and M.D. Feng. </author> <title> Breadth-First Search in the Eight Queens Problem. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 29, No. 9, </volume> <month> Sep. </month> <year> 1994. </year> <pages> pp. 51-55. </pages>
Reference-contexts: We decided to dynamically compute reections of positions because physical memory per node on Alewife is low (5MB maximum usable memory) and there is no virtual memory yet. The triangle puzzle is similar to other tree-search problems, such as the N-queens problem <ref> [9, 12, 23, 27, 28] </ref>, the Hi-Q puzzle [23], and chess [18].
Reference: [28] <author> Y. Zhang. </author> <title> Parallel Algorithms for Combinatorial Search Problems. </title> <type> Ph.D. thesis. </type> <institution> UC Berkeley. </institution> <month> November </month> <year> 1989. </year> <note> (Also available as UC Berkeley Computer Science Technical Report 89/543.) 66 </note>
Reference-contexts: We decided to dynamically compute reections of positions because physical memory per node on Alewife is low (5MB maximum usable memory) and there is no virtual memory yet. The triangle puzzle is similar to other tree-search problems, such as the N-queens problem <ref> [9, 12, 23, 27, 28] </ref>, the Hi-Q puzzle [23], and chess [18].
References-found: 28

