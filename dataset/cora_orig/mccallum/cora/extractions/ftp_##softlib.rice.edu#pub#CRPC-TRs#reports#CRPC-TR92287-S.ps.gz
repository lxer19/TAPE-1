URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92287-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: reinhard@rice.edu ken@rice.edu chk@rice.edu  raja@icase.edu jhs@icase.edu  
Title: Compiler Analysis for Irregular Problems in Fortran D  
Author: Reinhard von Hanxleden Ken Kennedy Charles Koelbel Raja Das Joel Saltz 
Address: Houston, TX 77251, USA.  Hampton, VA 23665, USA.  
Affiliation: Center for Research on Parallel Computation, Rice University,  Institute for Computer Applications in Science and Engineering, NASA Langley Research Center,  
Abstract: Many parallel programs require run-time support to implement the communication caused by indirect data references. In previous work, we have developed the inspector-executor paradigm to handle these cases. This paper extends that work by developing a dataflow framework to aid in placing the executor communications calls. Our dataflow analysis determines when it is safe to combine communications statements, move them into less frequently executed code regions, or avoid them altogether in favor of reusing data which are already buffered locally.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. W. Clark, R. v. Hanxleden, K. Kennedy, C. Koelbel, and L. R. Scott. </author> <title> Evaluating parallel languages for molecular dynamics computations. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Such loops are often referred to as data-parallel loops, and are the primary target of the Fortran D compiler [5]. Examples for the types of irregular loops we are trying to handle are found in unstructured mesh solvers and molecular dynamics codes <ref> [1, 2] </ref>. 1.1 The Inspector-Executor Paradigm In distributed memory machines, large data arrays need to be partitioned between processor memories. We call these distributed arrays. The pattern chosen often has a great effect on the efficiency of communication, but that topic is beyond the scope of this paper.
Reference: [2] <author> R. Das, D. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives, </title> <booktitle> AIAA-92-0562. In Proceedings of the 30th Aerospace Sciences Meeting. AIAA, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: Such loops are often referred to as data-parallel loops, and are the primary target of the Fortran D compiler [5]. Examples for the types of irregular loops we are trying to handle are found in unstructured mesh solvers and molecular dynamics codes <ref> [1, 2] </ref>. 1.1 The Inspector-Executor Paradigm In distributed memory machines, large data arrays need to be partitioned between processor memories. We call these distributed arrays. The pattern chosen often has a great effect on the efficiency of communication, but that topic is beyond the scope of this paper. <p> The executor then uses the information from the inspector to perform the actual computation. In the Fortran D compiler project at Rice University, we plan to perform these tasks by calls to the Parti (Parallel Automated Runtime Toolkit at ICASE) library <ref> [2] </ref>. These primitives perform such low-level functions as coordinating interprocessor data movement, managing the storage of and access to copies of off-processor data, and supporting complex data distributions using distributed translation tables [11]. <p> Duplicates are removed by using a hash table. To illustrate the importance of combining and incremental schedules, we summarize some previous hand-coded results. We ported an explicit unstructured mesh solver of the three dimensional Euler equations originally developed by Dimitri Mavriplis to the Touchstone Delta using Parti primitives <ref> [2] </ref>. We present timings from two implementations of the program. In the first, labeled "Without Dataflow," each data-parallel loop was treated separately, resulting in gather and scatter calls before and after every loop.
Reference: [3] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed memory compiler methods for irregular problems | data copy reuse and runtime partitioning. </title> <type> ICASE Report 91-73, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: The routines are designed in the style of collective communication calls in which all processors participate; thus, there is a well-defined point in the program to insert the subroutine calls. 1.2 Optimizing Data Communication The Parti routines can track and reuse off-processor data copies <ref> [3] </ref>. We generate combining communication schedules that combine off-processor data for several indirect references, and incremental schedules that send only data not already requested by previous schedules. This gives the run-time support needed for combining and hoisting gather, scatter, and accumulate operations. Duplicates are removed by using a hash table.
Reference: [4] <author> T. Gross and P. Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> Software|Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: For many programs, this can actually be achieved by forward substituting array indices. For example, the code sequence j=ia (i); x (j)=10 would be treated as x (ia (i))=10. Arrays that are never referenced indirectly are assumed to be analyzed using other methods <ref> [4] </ref> prior to this analysis. References with multiple (but bounded) levels of indirection will require more levels of complexity in the dataflow framework; we do not consider potentially unbounded indirection, as is found in linked lists. <p> In the following, loop refers to an element of N , i.e., it may denote a pad as well. Future work will present a complete framework in which summary information is built in a bottom-up fashion similar to array kill information <ref> [4] </ref>. Furthermore, this paper only discusses the case where the summarized loops have no data dependences, except for commutative and associative reductions that are handled separately. 2.2 Array portions Array portions are a central concept to the framework and best introduced by an example.
Reference: [5] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: This framework applies to collections of loops with no loop-carried data dependences except possibly those used for accumulations. Such loops are often referred to as data-parallel loops, and are the primary target of the Fortran D compiler <ref> [5] </ref>. Examples for the types of irregular loops we are trying to handle are found in unstructured mesh solvers and molecular dynamics codes [1, 2]. 1.1 The Inspector-Executor Paradigm In distributed memory machines, large data arrays need to be partitioned between processor memories. We call these distributed arrays.
Reference: [6] <author> S. Horwitz, T. Reps, and D. Binkley. </author> <title> Interprocedural slicing using dependence graphs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(1) </volume> <pages> 26-60, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: A more general approach could be based on basic blocks combined with slicing <ref> [6] </ref>; we are already exploring this alternative in our implementation. Another extension is to divide the communication calls into matching send/receive pairs, placing these components to overlap communication and computation when possible.
Reference: [7] <author> J. Kam and J. Ullman. </author> <title> Global data flow analysis and iterative algorithms. </title> <journal> Journal of the ACM, </journal> <volume> 23(1) </volume> <pages> 159-171, </pages> <month> January </month> <year> 1976. </year>
Reference-contexts: To construct these bit vectors, an initial pass over the program has to collect all indirect references. The length of the bit vectors is bounded by the number of indirect array references an therefore linear in the program size. All equations given here are rapid <ref> [7] </ref>. Therefore, using bit vectors for the analysis results in good asymptotic running times. However, for our examples (and probably also in a practical implementation), it seems advantageous to represent the different flow variables as bit matrices.
Reference: [8] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Lazy code motion. </title> <booktitle> In Proceedings of the ACM SIG-PLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Another strategy would be to limit communication hoisting to cases where we actually reduce the size or number of messages, which can also be achieved in a straightforward manner 8 similar to Lazy Code Motion <ref> [8] </ref>; this might decrease the live ranges of our communication buffer with a possible savings in overall buffer storage requirements, but at the expense of reduced opportunities for hiding communication delays.
Reference: [9] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory machines. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: In irregular problems, the communications pattern depends on the input data, typically because of some indirection in the code. This implies that messages must be aggregated at run time rather than compile time; we do this by transforming the original program into two constructs called inspector and executor <ref> [9, 10] </ref>. At run time, the inspector examines the data references made by a processor calculating the off-processor data to be received and allocating its storage. The executor then uses the information from the inspector to perform the actual computation.
Reference: [10] <author> R. Mirchandaney, J. Saltz, R. Smith, D. Nicol, and K. Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: In irregular problems, the communications pattern depends on the input data, typically because of some indirection in the code. This implies that messages must be aggregated at run time rather than compile time; we do this by transforming the original program into two constructs called inspector and executor <ref> [9, 10] </ref>. At run time, the inspector examines the data references made by a processor calculating the off-processor data to be received and allocating its storage. The executor then uses the information from the inspector to perform the actual computation.
Reference: [11] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 303-312, </pages> <year> 1990. </year>
Reference-contexts: These primitives perform such low-level functions as coordinating interprocessor data movement, managing the storage of and access to copies of off-processor data, and supporting complex data distributions using distributed translation tables <ref> [11] </ref>. The Parti functions are used by the inspector to produce a schedule describing the communications pattern, and by the executor to perform the actual gather or scatter of data.
References-found: 11

