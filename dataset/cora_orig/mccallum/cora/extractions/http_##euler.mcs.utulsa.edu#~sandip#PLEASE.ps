URL: http://euler.mcs.utulsa.edu/~sandip/PLEASE.ps
Refering-URL: http://euler.mcs.utulsa.edu/~sandip/GAGP.html
Root-URL: 
Email: e-mail: knight@euler.mcs.utulsa.edu  e-mail: sandip@kolkata.mcs.utulsa.edu  
Title: PLEASE: A prototype learning system using genetic algorithms  
Author: Leslie Knight Sandip Sen 
Address: Tulsa  Tulsa  
Affiliation: Dept of Mathematical Computer Sciences, The University of  Dept of Mathematical Computer Sciences, The University of  
Abstract: Prototypes have been proposed as representation of concepts that are used effectively by humans. Developing computational schemes for generating prototypes from examples, however, has proved to be a difficult problem. We present a novel genetic algorithm based prototype learning system, PLEASE, for constructing appropriate prototypes from classified training instances. After constructing a set of prototypes for each of the possible classes, the class of a new input instance is determined by the nearest prototype to this instance. Attributes are assumed to be ordinal in nature and prototypes are represented as sets of feature-value pairs. A genetic algorithm is used to evolve the number of prototypes per class and their positions on the input space. We present experimental results on a series of artificial problems of varying complexity. PLEASE performs competitively with several nearest neighbor classification algorithms on the problem set. An analysis of the strengths and weaknesses of the initial version of our system motivates the need for additional operators. The inclusion of these operators substantially improves the performance of the system on particularly difficult problems.
Abstract-found: 1
Intro-found: 1
Reference: [AKA91] <author> David W. Aha, Dennis Kibler, and Marc K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1), </volume> <year> 1991. </year>
Reference: [Che92] <author> K.J. Cherkauer. </author> <title> Genetic search for nearest-neighbor exemplars. </title> <booktitle> In Proceedings of the Fourth Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 87-91, </pages> <year> 1992. </year>
Reference: [DeJ90] <author> Kenneth A. DeJong. </author> <title> Genetic-algorithm-based learning. </title> <editor> In Y. Kodratoff and R.S. Michalski, editors, </editor> <booktitle> Machine Learning, Volume III. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Alamos, CA, </address> <year> 1990. </year>
Reference: [DS91] <author> Kenneth A. DeJong and William M. Spears. </author> <title> Learning concept classification rules using genetic algorithms. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 651-656, </pages> <year> 1991. </year>
Reference: [Hin86] <author> D.L. Hintzman. </author> <title> "schema abstraction" in a multiple trace memory model. </title> <journal> Psychological Review, </journal> <volume> 93 </volume> <pages> 411-428, </pages> <year> 1986. </year>
Reference: [Hol75] <author> John H. Holland. </author> <booktitle> Adpatation in natural and artificial systems. </booktitle> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI, </address> <year> 1975. </year>
Reference: [Hol86] <author> John H. Holland. </author> <title> Escaping brittleness: the possibilities of general-purpose learning algorithms applied to parallel rule-based systems. In R.S. </title> <editor> Michalski, J.G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning, an artificial intelligence approach: Volume II. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Alamos, CA, </address> <year> 1986. </year>
Reference: [KD91] <author> James D. Kelly and Lawrence Davis. </author> <title> A hybrid genetic algorithm for classification. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 645-650, </pages> <year> 1991. </year>
Reference: [KM86] <author> D. Kahneman and D.T. Miller. </author> <title> Norm theory: Comparing reality to its alternatives. </title> <journal> Psychological Review, </journal> <volume> 93 </volume> <pages> 136-153, </pages> <year> 1986. </year>
Reference: [Mit86] <author> T. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 203-226, </pages> <year> 1986. </year>
Reference: [Qui86] <author> Ross J. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference: [Qui93] <author> Ross J. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference: [Ree72] <author> S.K. Reed. </author> <title> Pattern recognition and categorization. </title> <journal> Cognitive Psychology, </journal> <volume> 3 </volume> <pages> 382-407, </pages> <year> 1972. </year>
Reference: [RHW86] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference: [She74] <author> R.N. Shepard. </author> <title> Representation of structure in similarity data: Problems and prospects. </title> <journal> Psychometrika, </journal> <volume> 39 </volume> <pages> 373-421, </pages> <year> 1974. </year>
Reference: [SK95] <author> Sandip Sen and Leslie Knight. </author> <title> A genetic prototype learner. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference: [Ska94] <author> David B. Skalak. </author> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 293-301, </pages> <year> 1994. </year>
Reference: [SM81] <author> E.E. Smith and D.L. Medin. </author> <title> Categories and concepts. </title> <publisher> Harvard University Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1981. </year>
Reference: [Smi80] <author> Steve F. Smith. </author> <title> A learning system based on genetic adaptive algorithms. </title> <type> PhD thesis, </type> <institution> University of Pittsburgh, </institution> <year> 1980. </year> <note> (Dissertation Abstracts International, 41, 4582B; University Microfilms No. 81-12638). </note>
Reference: [Smi89] <author> Edward E. Smith. </author> <title> Concepts and induction. </title> <editor> In Michael I. Posner, editor, </editor> <booktitle> Foundations of Cognitive Science. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference: [Whi89] <author> D. Whitley. </author> <title> The genitor algorithm and selection pressure: Why rank-based allocation of reproductive trials is best. </title> <booktitle> In Proceedings of the 3rd International Conference on Genetic Algorithms, </booktitle> <pages> pages 116-121, </pages> <address> San Ma-teo, CA, 1989. </address> <publisher> Morgan Kaufman. </publisher>
References-found: 21

