URL: http://www.cs.umd.edu/~aporter/icse16.ps
Refering-URL: http://www.cs.umd.edu/~aporter/html/selected_pubs.html
Root-URL: 
Email: aporter@cs.umd.edu votta@research.att.com  
Title: An Experiment To Assess Different Defect Detection Methods For Software Requirements Inspections  
Author: A.A. Porter L.G. Votta 
Note: This work is supported in part by the National Aeronautics and Space Administration under grant NSG-5123  
Date: January 21, 1994  
Address: College Park, Maryland 20472 Naperville, IL 60566  
Affiliation: Computer Science Department Software Production Research Department University of Maryland AT&T Bell Laboratories Institute for Advanced Computer Science  
Abstract: Software requirements specifications (SRS) are usually validated by inspections, in which several reviewers read all or part of the specification and search for defects. We hypothesize that different methods for conducting these searches may have significantly different rates of success. Therefore, we designed and performed an experiment to assess the effect of different detection methods on the probability of detecting defects in a software specification document. Preliminary results of our first run with 24 reviewers, assembled in 3 member teams, indicate that a scenario-based detection method, in which each reviewer executes a specific procedure to discover a particular class of defects is more effective than either ad hoc or checklist methods. We describe the design, execution, and analysis of the experiment so others may reproduce it and test our results for different kinds of software developments and different populations of software engineers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. R. Basili and D. M. Weiss. </author> <title> Evaluation of a software requirements document by analysis of change data. </title> <booktitle> In Proceedings of the Fifth International Conference on Software Engineering, </booktitle> <pages> pages 314-323, </pages> <address> San Diego, CA, </address> <month> March </month> <year> 1981. </year>
Reference-contexts: This population was defined using a general defect taxonomy. The taxonomy is a composite of two schemes developed by Schneider, et.al. [13] and Basili and Weiss <ref> [1] </ref>. Defects are divided into two broad types: omission in which important information is left unstated and commission in which incorrect, redundant or ambiguous information is placed in the SRS.
Reference: [2] <author> Barry W Boehm. </author> <title> Software Engineering Economics. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: Consequently, we will discuss only work directly related to our current efforts. Fagan [5] defined the basic software inspection process. While most writers have endorsed this approach <ref> [2, 7] </ref>, Parnas and Weiss are more critical [12]. In part they argue that effectiveness suffers because individual reviewers are not assigned specific responsibilities and because they lack specialized techniques for meeting those responsibilities.
Reference: [3] <author> G. E. P. Box, W. G. Hunter, and J. S. Hunter. </author> <title> Statistics for Experimenters. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference: [4] <author> Stephen G Eick, Clive R Loader, M. David Long, Scott A Vander Wiel, and Lawrence G Votta. </author> <title> Estimating software fault content before coding. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 59-65. </pages> <institution> Association for Computing Machinery, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Nevertheless, laboratory experimentation is a necessary first step because it greatly reduces the risk of transferring immature technology. We avoided the third threat by modeling the experiment's inspection process after the design inspection process described in Eick, et. al., <ref> [4] </ref> which is used by several development organizations at AT&T; therefore, we know that at least one professional software development organization practices inspections in this manner. 2.1.5 Analysis Strategy Our analysis strategy has two steps.
Reference: [5] <author> M E Fagan. </author> <title> Design and code inspections to reduce errors in program development. </title> <journal> IBM Syst. J., </journal> <volume> 15(3) </volume> <pages> 182-211, </pages> <year> 1976. </year>
Reference-contexts: Consequently, we will discuss only work directly related to our current efforts. Fagan <ref> [5] </ref> defined the basic software inspection process. While most writers have endorsed this approach [2, 7], Parnas and Weiss are more critical [12]. In part they argue that effectiveness suffers because individual reviewers are not assigned specific responsibilities and because they lack specialized techniques for meeting those responsibilities. <p> Porter. The students were given a series of lectures on software requirements specifications, the SCR tabular requirements notation, inspection procedures, the defect classification scheme and the filling out of data collection forms. The references for these lectures were <ref> [12, 5, 9] </ref>. Each participant filled out a personal experience survey of their academic status, relevant computer science course work, and professional experience.
Reference: [6] <author> Kathryn L. Heninger. </author> <title> Specifying Software Requirements for Complex Systems: New Techniques and their Application. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-6(1):2-13, </volume> <month> January </month> <year> 1980. </year>
Reference-contexts: Each specification has four sections: Overview, Specific functional requirements, External interfaces, and a Glossary. The overview is written in natural language, while the last three sections are specified using the SCR tabular requirements notation <ref> [6] </ref>. For this experiment, all three documents were adapted to adhere to the IEEE suggested format [9]. All defects present in these SRS appear in the original documents or were generated during the adaptation process; no defects were intentionally seeded into the document.
Reference: [7] <author> Watts S. Humphery. </author> <title> Managing the Software Process. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <address> 1989. Reading, Massachusetts. </address>
Reference-contexts: 1 Introduction One of the most common ways of validating a software requirements specification (SRS) is to submit it to an inspection by a team of reviewers. Many organi zations use a three-step inspection procedure: defect 1 detection, collection and repair 2 . <ref> [7, 15] </ref> A team of reviewers reads the SRS, identifying as many defects as possible. Newly identified defects are collected, usually at a team meeting, and then sent to the document's authors for repair. <p> The reason is that fault has a context implying software-only one of the several places where inspections are used. 2 Depending on the exact form of the inspection, they are sometimes called reviews or walkthroughs. For a more thorough description of the taxonomy see <ref> [7] </ref> pp. 171ff and [9]. dards has a specific responsibility. In practice, reviewers often use ad hoc or checklist detection procedures to discharge general reviewer responsibilities. <p> Below we describe the relevant literature, several alternative defect detection methods which motivated our study, our research hypothesis, and our experimental observations, analysis and conclusions. 1.1 Inspection Literature A summary of the origins and current practice of inspections may be found in <ref> [7] </ref>. Consequently, we will discuss only work directly related to our current efforts. Fagan [5] defined the basic software inspection process. While most writers have endorsed this approach [2, 7], Parnas and Weiss are more critical [12]. <p> Consequently, we will discuss only work directly related to our current efforts. Fagan [5] defined the basic software inspection process. While most writers have endorsed this approach <ref> [2, 7] </ref>, Parnas and Weiss are more critical [12]. In part they argue that effectiveness suffers because individual reviewers are not assigned specific responsibilities and because they lack specialized techniques for meeting those responsibilities.
Reference: [8] <institution> IEEE Guide for the Ude of IEEE Standard Dictionary of Measures to Produce Reliable Software. Soft. Eng. Tech. Comm. of the IEEE Computer Society, </institution> <year> 1989. </year> <note> IEEE Std 982.2-1988. </note>
Reference-contexts: concerned with a limited set of issues, say insuring appropriate use of hardware interfaces, identifying untestable requirements or checking conformance to coding stan 1 We have consciously chosen the word defect instead of the word fault even though this does not adhere to the IEEE Standards on Software Engineering Terminology <ref> [8] </ref>. The reason is that fault has a context implying software-only one of the several places where inspections are used. 2 Depending on the exact form of the inspection, they are sometimes called reviews or walkthroughs.
Reference: [9] <institution> IEEE Standard for software reviews and audits. Soft. Eng. Tech. Comm. of the IEEE Computer Society, </institution> <year> 1989. </year> <note> IEEE Std 1028-1988. </note>
Reference-contexts: The reason is that fault has a context implying software-only one of the several places where inspections are used. 2 Depending on the exact form of the inspection, they are sometimes called reviews or walkthroughs. For a more thorough description of the taxonomy see [7] pp. 171ff and <ref> [9] </ref>. dards has a specific responsibility. In practice, reviewers often use ad hoc or checklist detection procedures to discharge general reviewer responsibilities. Some authors, notably Parnas and Weiss [12] have argued that using more systematic detection procedures with selective reviewer responsibilities would be a more effective method. <p> The overview is written in natural language, while the last three sections are specified using the SCR tabular requirements notation [6]. For this experiment, all three documents were adapted to adhere to the IEEE suggested format <ref> [9] </ref>. All defects present in these SRS appear in the original documents or were generated during the adaptation process; no defects were intentionally seeded into the document. The authors discovered forty-two defects in the WLMS SRS; 26 in the CRUISE SRS. <p> Porter. The students were given a series of lectures on software requirements specifications, the SCR tabular requirements notation, inspection procedures, the defect classification scheme and the filling out of data collection forms. The references for these lectures were <ref> [12, 5, 9] </ref>. Each participant filled out a personal experience survey of their academic status, relevant computer science course work, and professional experience.
Reference: [10] <author> Charles M. Judd, Eliot R. Smith, and Louise H. Kidder. </author> <title> Research Methods in Social Relations. </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <address> Fort Worth, TX, sixth edition, </address> <year> 1991. </year>
Reference-contexts: (who is assigned to which team); 3. the specification to be inspected (two are used during the experiment); 4. the order in which specifications are inspected by each team of reviewers; Table 1 shows the settings of the independent variables. 3 The interested reader should consult chapter 4 of reference <ref> [10] </ref> for an excellent discussion of randomized social experimental designs. The dependent variable is the team defect detection rate. That is the number of defects detected by the team divided by the total number of defects known 4 to be in the specification.
Reference: [11] <author> J. Kirby. </author> <title> Example nrl/scr software requirements for an automobile cruise control and monitoring system. </title> <type> Technical Report TR-87-07, </type> <institution> Wang Institute of Graduate Studies, </institution> <month> July </month> <year> 1984. </year>
Reference-contexts: Water Level Monitoring System (WLMS) describes the functional and performance requirements of a system for monitoring the operation of a steam generating system [14]. 24 pages. Automobile Cruise Control System (CRUISE) describes the functional and performance requirements for an automobile cruise control system <ref> [11] </ref>. 31 pages. 2.2.2 Defect Detection Methods To make a fair assessment of the three detection methods (ad hoc, checklist, and scenario) each method needed to search for the same population of defects. This population was defined using a general defect taxonomy.
Reference: [12] <author> Dave L Parnas and Dave M Weiss. </author> <title> Active design reviews: </title> <booktitle> principles and practices. In Proceedings of the 8th International Conference on Software Engineering, </booktitle> <pages> pages 215-222, </pages> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: For a more thorough description of the taxonomy see [7] pp. 171ff and [9]. dards has a specific responsibility. In practice, reviewers often use ad hoc or checklist detection procedures to discharge general reviewer responsibilities. Some authors, notably Parnas and Weiss <ref> [12] </ref> have argued that using more systematic detection procedures with selective reviewer responsibilities would be a more effective method. Until now, however, there have been no reproducible, quantitative studies comparing alternative detection methods for software inspections. <p> Consequently, we will discuss only work directly related to our current efforts. Fagan [5] defined the basic software inspection process. While most writers have endorsed this approach [2, 7], Parnas and Weiss are more critical <ref> [12] </ref>. In part they argue that effectiveness suffers because individual reviewers are not assigned specific responsibilities and because they lack specialized techniques for meeting those responsibilities. To address these concerns at least for software designs they introduced the idea of active design reviews. <p> Porter. The students were given a series of lectures on software requirements specifications, the SCR tabular requirements notation, inspection procedures, the defect classification scheme and the filling out of data collection forms. The references for these lectures were <ref> [12, 5, 9] </ref>. Each participant filled out a personal experience survey of their academic status, relevant computer science course work, and professional experience.
Reference: [13] <author> G. Michael Schnieder, Johnny Martin, and W. T. Tsai. </author> <title> An experimental study of fault detection in user requirements. </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> 1(2) </volume> <pages> 188-204, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: This population was defined using a general defect taxonomy. The taxonomy is a composite of two schemes developed by Schneider, et.al. <ref> [13] </ref> and Basili and Weiss [1]. Defects are divided into two broad types: omission in which important information is left unstated and commission in which incorrect, redundant or ambiguous information is placed in the SRS.
Reference: [14] <author> J. van Schouwen. </author> <title> The a-7 requirements model: Re-examination for real-time systems and an application to monitoring systems. </title> <type> Technical Report TR-90-276, </type> <institution> Queen's University, Kingston, </institution> <address> Ontario, Canada, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Elevator Control System (ELEVATOR) describes the functional and performance requirements of a system for monitoring the operation of a bank of elevators [16]. 16 pages. Water Level Monitoring System (WLMS) describes the functional and performance requirements of a system for monitoring the operation of a steam generating system <ref> [14] </ref>. 24 pages.
Reference: [15] <author> Lawrence G Votta. </author> <booktitle> Does every inspection need a meeting? In Proceedings of ACM SIGSOFT '93 Symposium on Foundations of Software Engineering. Association for Computing Machinery, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: 1 Introduction One of the most common ways of validating a software requirements specification (SRS) is to submit it to an inspection by a team of reviewers. Many organi zations use a three-step inspection procedure: defect 1 detection, collection and repair 2 . <ref> [7, 15] </ref> A team of reviewers reads the SRS, identifying as many defects as possible. Newly identified defects are collected, usually at a team meeting, and then sent to the document's authors for repair. <p> As the collection activity normally occurs as a meeting, we can ask, "Does every inspection need a meeting?" Interestingly, this data is consistent with an industrial study performed by L. Votta <ref> [15] </ref>. * Consider the effectiveness of scenarios as shown in Table 5. More than half of the defects in both specifications are not addressed by the scenarios used in this study.
Reference: [16] <author> William G. Wood. </author> <title> Temporal logic case study. </title> <type> Technical Report CMU/SEI-89-TR-24, </type> <institution> Software Engineering Institute, </institution> <address> Pittsburgh, Pa, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: The authors did not inspect the ELEVATOR SRS since it was only used during training exercises. Elevator Control System (ELEVATOR) describes the functional and performance requirements of a system for monitoring the operation of a bank of elevators <ref> [16] </ref>. 16 pages. Water Level Monitoring System (WLMS) describes the functional and performance requirements of a system for monitoring the operation of a steam generating system [14]. 24 pages.
References-found: 16

