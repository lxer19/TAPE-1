URL: http://cs.ua.edu/~rsun/tp.rbf.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00463.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: ftodd,rsung@cs.ua.edu  
Title: An RBF Network Alternative for a Hybrid Architecture  
Author: Todd Peterson and Ron Sun 
Address: Tuscaloosa, AL 35486  
Affiliation: University of Alabama  
Abstract: Although our previous model CLARION has shown some measure of success in reactive sequential decision making tasks by utilizing a hybrid architecture which uses both procedural and declarative learning, it suffers from a number of problems because of its use of back propagation networks. CLARION-RBF is a more parsimonious architecture that remedies some of the problems exhibited in CLARION by utilizing RBF Networks. CLARION-RBF is also capable of learning reactive procedures, and can have high level symbolic knowledge extracted and applied. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Agre and D. Chapman. </author> <title> What are plans for? In P. </title> <editor> Maes, editor, </editor> <title> Designing Autonomous Agents. </title> <publisher> Elsevier, </publisher> <address> New York, </address> <year> 1990. </year>
Reference: [2] <author> J.A. Boyan and A. W. Moore. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <booktitle> In Advances in Nerual Information Processing Systems 7, </booktitle> <year> 1995. </year>
Reference-contexts: However, they also have several problems: they are slow to converge and can be subject to catastrophic interference [8]. They have additional problems when used as function approximators in a reinforcement learning setting. Many function approximation algorithms have been shown to be problematic in reinforcement learning tasks <ref> [2] </ref>. One of the primary reasons is that they rely on global approximation rather than local. In back-propagation networks, each update of the network may affect inputs that are far away from the current input, thus the updates may have effect over the entire input space.
Reference: [3] <author> Andrew W. Moore Christopher G. Atkeson and Stefan Schaal. </author> <title> Locally weighted learning. </title> <journal> Artificial Intelligence Review, </journal> <pages> pages 11-73, </pages> <month> Feb </month> <year> 1997. </year>
Reference-contexts: When the network encounters a situation that is not specifically covered by one of the stored cases, the network performs a weighted sum of all nearby nodes. This corresponds well to many existing extrapolation algorithms found in locally weighted models <ref> [3] </ref>. VI. Experimental Setup We tested back-propagation Q-learning (Q-BP), Clarion, and Clarion-RBF on the ONR/NRL navigation task as shown in Figure 2. The agent has to navigate an underwater vessel through a minefield to reach a target location.
Reference: [4] <author> J. del R. Millan and C. Torras. </author> <title> A reinforcement connectionist approach to robot path finding in non-maze-like environments. </title> <journal> Machine Learning, </journal> 8(3/4):363-395, May 1992. 
Reference: [5] <author> Kenneth J. Hunt, Roland Haas, and Roderick Murray-Smith. </author> <title> Extending the functional equivalence of radial basis function networks and fuzzy inference systems. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(3) </volume> <pages> 776-781, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: The activation of a particular hidden unit falls off as the distance from input value to the hidden unit center increases. Thus the RBF network, once trained, can be thought of as a set of rules <ref> [5] </ref>, [6]: if the input is sufficiently similar to the center, then do the recommended action. In [5] and [6] it was pointed out that RBF networks map to a fuzzy inference system. However, the rules are learned on-line. <p> Thus the RBF network, once trained, can be thought of as a set of rules <ref> [5] </ref>, [6]: if the input is sufficiently similar to the center, then do the recommended action. In [5] and [6] it was pointed out that RBF networks map to a fuzzy inference system. However, the rules are learned on-line. Any time the agent encounters a situation which is sufficiently different from the current situation, a new RBF node is added and hence, a new rule is extracted.
Reference: [6] <author> J. S. Roger Jang and C.T. Sun. </author> <title> Functional equivalence between radial basis function networks and fuzzy inference systems. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(1) </volume> <pages> 156-159, </pages> <year> 1993. </year>
Reference-contexts: The weight update equation then becomes: w out k )o hid RBF networks are a better choice for function approximation because they learn locally, they are more robust, and the information stored in an RBF network is easier to interpret symbolically <ref> [6] </ref>. IV. Clarion-RBF Clarion-RBF combines the advantages of RBF networks and reinforcement learning. It also simplifies the hybrid architecture of Clarion into a more unified architecture, but achieving the same goals as Clarion. Clarion-RBF uses a reinforcement learning paradigm similar to that of Clarion. <p> The activation of a particular hidden unit falls off as the distance from input value to the hidden unit center increases. Thus the RBF network, once trained, can be thought of as a set of rules [5], <ref> [6] </ref>: if the input is sufficiently similar to the center, then do the recommended action. In [5] and [6] it was pointed out that RBF networks map to a fuzzy inference system. However, the rules are learned on-line. <p> Thus the RBF network, once trained, can be thought of as a set of rules [5], <ref> [6] </ref>: if the input is sufficiently similar to the center, then do the recommended action. In [5] and [6] it was pointed out that RBF networks map to a fuzzy inference system. However, the rules are learned on-line. Any time the agent encounters a situation which is sufficiently different from the current situation, a new RBF node is added and hence, a new rule is extracted.
Reference: [7] <author> L.-J. Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> 8(3/4):293-321, May 1992. 
Reference: [8] <author> M. McCloskey and N.J. Cohen. </author> <title> Catastrophic interference in connectionist networks. In G.H. Bower, editor, The psychology of learning and motivation. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Back-propagation networks as used in Clarion were a natural choice of function approximators because they are well understood and have been shown to be universal ap-proximators. However, they also have several problems: they are slow to converge and can be subject to catastrophic interference <ref> [8] </ref>. They have additional problems when used as function approximators in a reinforcement learning setting. Many function approximation algorithms have been shown to be problematic in reinforcement learning tasks [2]. One of the primary reasons is that they rely on global approximation rather than local.
Reference: [9] <author> J. Moody and C.J. Darken. </author> <title> Fast learning in networks of locally-tuned processing units. </title> <booktitle> Neural Computation, </booktitle> <address> `:281-294, </address> <year> 1989. </year>
Reference-contexts: While we were somewhat successful in overcoming these problems, there might be a better way of handling the problems using local approximation. Radial Basis Function (RBF) networks serve as possible alternatives because they are local <ref> [9] </ref> such that they learn continuously and are not subject to the problems that plague global approximation methods. Sutton [15] has shown that local approximation methods are stable and can be used reliably as function approximators in reinforcement learning. In an RBF network [9], [11], each hidden unit output o hid <p> as possible alternatives because they are local <ref> [9] </ref> such that they learn continuously and are not subject to the problems that plague global approximation methods. Sutton [15] has shown that local approximation methods are stable and can be used reliably as function approximators in reinforcement learning. In an RBF network [9], [11], each hidden unit output o hid j is activated by the following formula: o hid X ff i jh ji x i j r ) q=r ]; (6) where x i is the ith input, c j is the radius of node j, h ji is the ith component <p> Sample Rule Set for Clarion-RBF of 220 indicates no mine in that direction. The last two columns represent the action chosen by the agent for this situation. VIII. Discussion The reason Clarion-RBF has the fastest training time is because it relies on RBF networks for function approximation <ref> [9] </ref>. Clarion and Q-BP rely on back-propagation networks for function approximation, so they have to update two layers of weights. RBF networks have only one layer of weights to update since the radius and dimensional weighting parameters are not updated; thus, Clarion-RBF also trains faster.
Reference: [10] <author> Sebastian Thrun & Joseph O'Sullivan. </author> <title> Discovering structure in multiple learning tasks: The tc algorithm. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <year> 1996. </year>
Reference-contexts: This makes learning global rather than localized. While this may lead to correct learning in areas that are near to the input in question, it has been shown in practice that such updates also modify regions of the input space where the correct action has been previously learned [14], <ref> [10] </ref>, [17]. Back-propagation approximation is unstable with regard to reinforcement learning as success rates tend to drift down towards zero after reaching a high level of performance.
Reference: [11] <author> J. Park and I. W. Sandberg. </author> <title> Universal approximation using radial-basis function networks. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 246-257, </pages> <year> 1991. </year>
Reference-contexts: Sutton [15] has shown that local approximation methods are stable and can be used reliably as function approximators in reinforcement learning. In an RBF network [9], <ref> [11] </ref>, each hidden unit output o hid j is activated by the following formula: o hid X ff i jh ji x i j r ) q=r ]; (6) where x i is the ith input, c j is the radius of node j, h ji is the ith component of <p> The stability is inherent in the network. Clarion-RBF learns locally, thus there is much less change to hidden units that have already converged to an appropriate action. Although they learn locally, RBF networks have the same universal function approximation abilities as back-propagation networks <ref> [11] </ref>. The rule set is typically smaller and more stable than Clarion, and the conditions of the rules do not change as often and as drastically as they do in Clarion.
Reference: [12] <author> Ron Sun & Todd Peterson. </author> <title> A hybrid model for learning sequential decision making. </title> <booktitle> In Proceedings of the International Conference on Neural Networks, </booktitle> <pages> pages 1073-1078, </pages> <year> 1996. </year>
Reference-contexts: If the result is good according to the current criterion, then generalize the matching rules by using expansion. 9. Perform merge to combine existing rules. III. Observations Although Clarion outperformed Q-learning alone in terms of learning <ref> [12] </ref>, there were two problems that needed to be overcome: (1) it was difficult to maintain high performance over time, and (2) it was difficult to maintain a stable rule set, both of which rely critically on parameter settings. <p> A random mine layout is generated for each trial. The time allotted to the agent for each trial is 200 steps. There are 43 binary inputs and thus more than 10 12 states (see <ref> [12] </ref> for details of the input encoding). Each model was trained for 1000 trials. Each model was run 10 times. All results here are averages. All experiments were performed on a Sun SPARC20. VII. Results 1000. Figures 5,6 and 7 show the learning curves of the three models over time.
Reference: [13] <author> Ron Sun & Todd Peterson. </author> <title> A hybrid model for learning sequential navigtion. </title> <booktitle> In Proc. of IEEE International Symposium on Computational Intelligence in Robotics and Automation, </booktitle> <pages> pages 234-239, </pages> <address> Monterey, CA., 1997. </address> <publisher> IEEE Press. </publisher>
Reference: [14] <author> Sebastian Thrun & Anton Schwartz. </author> <title> Issues in using function approximation for reinforcement learning. </title> <booktitle> In Proceedings of the Fourth Connectionist Models Summer School, </booktitle> <year> 1993. </year>
Reference-contexts: This makes learning global rather than localized. While this may lead to correct learning in areas that are near to the input in question, it has been shown in practice that such updates also modify regions of the input space where the correct action has been previously learned <ref> [14] </ref>, [10], [17]. Back-propagation approximation is unstable with regard to reinforcement learning as success rates tend to drift down towards zero after reaching a high level of performance.
Reference: [15] <author> Richard Sutton. </author> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <booktitle> In Advances in Nerual Information Processing Systems 8, </booktitle> <year> 1996. </year>
Reference-contexts: Radial Basis Function (RBF) networks serve as possible alternatives because they are local [9] such that they learn continuously and are not subject to the problems that plague global approximation methods. Sutton <ref> [15] </ref> has shown that local approximation methods are stable and can be used reliably as function approximators in reinforcement learning.
Reference: [16] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <month> June </month> <year> 1990. </year>
Reference: [17] <author> Gerald Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 257-277, </pages> <year> 1992. </year>
Reference-contexts: While this may lead to correct learning in areas that are near to the input in question, it has been shown in practice that such updates also modify regions of the input space where the correct action has been previously learned [14], [10], <ref> [17] </ref>. Back-propagation approximation is unstable with regard to reinforcement learning as success rates tend to drift down towards zero after reaching a high level of performance.
Reference: [18] <author> G. Towell and J. Shavlik. </author> <title> The extraction of refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 71-101, </pages> <year> 1993. </year>
Reference: [19] <author> Christopher J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <year> 1989. </year>
References-found: 19

