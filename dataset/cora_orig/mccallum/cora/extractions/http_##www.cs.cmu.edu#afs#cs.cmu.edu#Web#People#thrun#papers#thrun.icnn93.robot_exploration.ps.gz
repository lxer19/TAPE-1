URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/thrun.icnn93.robot_exploration.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/full.html
Root-URL: http://www.cs.cmu.edu
Email: E-mail: thrun@cs.cmu.edu  
Title: Exploration and Model Building in Mobile Robot Domains  
Author: Sebastian B. Thrun 
Affiliation: School of Computer Science Carnegie Mellon University  
Date: March 28-April 1, 1993  
Address: San Francisco, CA,  Pittsburgh, PA 15213  
Note: In: Proceedings of the IEEE International Conference on Neural Networks  
Abstract: I present first results on COLUMBUS, an autonomous mobile robot. COLUMBUS operates in initially unknown, structured environments. Its task is to explore and model the environment efficiently while avoiding collisions with obstacles. COLUMBUS uses an instance-based learning technique for modeling its environment. Real-world experiences are generalized via two artificial neural networks that encode the characteristics of the robot's sensors, as well as the characteristics of typical environments the robot is assumed to face. Once trained, these networks allow for knowledge transfer across different environments the robot will face over its lifetime. COLUMBUS' models represent both the expected reward and the confidence in these expectations. Exploration is achieved by navigating to low confidence regions. An efficient dynamic programming method is employed in background to find minimal-cost paths that, executed by the robot, maximize exploration. COLUMBUS operates in real-time. It has been operating successfully in an office building environment for periods up to hours.
Abstract-found: 1
Intro-found: 1
Reference: [ Atkeson, 1991 ] <author> Christopher A. Atkeson. </author> <title> Using locally weighted regression for robot learning. </title> <booktitle> In Proceedings of the 1991 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 958-962, </pages> <address> Sacramento, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Modularized, local and instance-based approaches to function approximation have often been reported to generalize better from fewer or ill-distributed examples [ Moore, 1990 ] , [ Nowlan, 1990 ] , <ref> [ Atkeson, 1991 ] </ref> , [ Friedmann, 1991 ] , [ Fox et al., 1991 ] , [ Jacobs and Jordan, 1991 ] . Instance-based curve fitting techniques have also been applied successfully to fairly complex robot control learning problems, including the work by Atkeson and Moore.
Reference: [ Bachrach, 1991 ] <author> Jonathan R. Bachrach. </author> <title> A connectionist learning control architecture for navigation. </title> <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 457-463, </pages> <address> San Mateo, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Many global approaches to function fitting (e.g., with a single monolithic neural network) in mobile robot domains either fail in complex environments due to effects like un-/relearning, or demand many well-distributed training examples and have often been tested in simplified simulations only <ref> [ Bachrach, 1991 ] </ref> , [ Lin, 1991 ] , [ Singh, 1992 ] , [ Thrun and Moller, 1992 ] .
Reference: [ Barto et al., 1991 ] <author> Andy G. Barto, Steven J. Bradtke, and Satinder P. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report COINS 91-57, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> MA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: In order to find low-cost paths to the unexplored, the model is discretized yielding a grid representation of the environment, and dynamic programming is employed to propagate exploration utility through this discretized model <ref> [ Barto et al., 1991 ] </ref> , [ Sutton, 1990 ] . More specifically, this is done in the following way: To each grid point x in the discretized model there is a real-valued exploration utility U (x) associated.
Reference: [ Elves, 1987 ] <author> Alberto Elves. </author> <title> Sonar-based real-world mapping and navigation. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> RA-3(3):249-265, </volume> <month> June </month> <year> 1987. </year>
Reference-contexts: This planner, however, operates on a two-dimensional representation of the environment. In order to scale up to higher dimensional environments the approach taken has to be modified. Future research will also include comparisons to other approaches to model building and path planning, e.g. <ref> [ Elves, 1987 ] </ref> , [ Moravec, 1988 ] .
Reference: [ Fox et al., 1991 ] <author> Dieter Fox, Volker Heinze, Knut Moller, Sebastian B. Thrun, and Gerd Veenker. </author> <title> Learning by error-driven decomposition. </title> <editor> In Simula/Kohonen, editor, </editor> <booktitle> Proceedings of International Conference on Artificial Neural Networks, </booktitle> <address> Amsterdam, 1991. </address> <publisher> Elsevier Publisher. </publisher> <pages> 11 </pages>
Reference-contexts: Modularized, local and instance-based approaches to function approximation have often been reported to generalize better from fewer or ill-distributed examples [ Moore, 1990 ] , [ Nowlan, 1990 ] , [ Atkeson, 1991 ] , [ Friedmann, 1991 ] , <ref> [ Fox et al., 1991 ] </ref> , [ Jacobs and Jordan, 1991 ] . Instance-based curve fitting techniques have also been applied successfully to fairly complex robot control learning problems, including the work by Atkeson and Moore. In COLUMBUS' approach to model building, experiences are remembered explicitly.
Reference: [ Friedmann, 1991 ] <author> Jerome H. Friedmann. </author> <title> Multivariate adaptive regression splines. </title> <journal> Annals of Statistics, </journal> <volume> 19(1) </volume> <pages> 1-141, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Modularized, local and instance-based approaches to function approximation have often been reported to generalize better from fewer or ill-distributed examples [ Moore, 1990 ] , [ Nowlan, 1990 ] , [ Atkeson, 1991 ] , <ref> [ Friedmann, 1991 ] </ref> , [ Fox et al., 1991 ] , [ Jacobs and Jordan, 1991 ] . Instance-based curve fitting techniques have also been applied successfully to fairly complex robot control learning problems, including the work by Atkeson and Moore.
Reference: [ Jacobs and Jordan, 1991 ] <author> Robert A. Jacobs and Michael I. Jordan. </author> <title> A modular connectionist architecture for learning piecewise control strategies. </title> <booktitle> In Proceedings of the American Control Conference. </booktitle> <institution> Dept. of Brain and Cognitive Sciences, MIT, </institution> <year> 1991. </year>
Reference-contexts: Modularized, local and instance-based approaches to function approximation have often been reported to generalize better from fewer or ill-distributed examples [ Moore, 1990 ] , [ Nowlan, 1990 ] , [ Atkeson, 1991 ] , [ Friedmann, 1991 ] , [ Fox et al., 1991 ] , <ref> [ Jacobs and Jordan, 1991 ] </ref> . Instance-based curve fitting techniques have also been applied successfully to fairly complex robot control learning problems, including the work by Atkeson and Moore. In COLUMBUS' approach to model building, experiences are remembered explicitly.
Reference: [ Koenig, 1992 ] <author> Sven Koenig. </author> <title> The complexity of real-time search. </title> <type> Technical Report CMU-CS-92-145, </type> <institution> Carnegie Mellon University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: In contrast, more thoughtful exploration techniques, such as go to the least explored location, have been shown to reduce the complexity to a small polynomial function in the size of the state space [ Thrun, 1992a ] , <ref> [ Koenig, 1992 ] </ref> . While these results may be theoretically significant, their relevance and implications for practical research in robot exploration are unclear.
Reference: [ Lin, 1991 ] <author> Long-Ji Lin. </author> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Proceedings of AAAI-91, </booktitle> <address> Menlo Park, CA, July 1991. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: Many global approaches to function fitting (e.g., with a single monolithic neural network) in mobile robot domains either fail in complex environments due to effects like un-/relearning, or demand many well-distributed training examples and have often been tested in simplified simulations only [ Bachrach, 1991 ] , <ref> [ Lin, 1991 ] </ref> , [ Singh, 1992 ] , [ Thrun and Moller, 1992 ] .
Reference: [ Moore, 1990 ] <author> Andrew W. Moore. </author> <title> Efficient Memory-based Learning for Robot Control. </title> <type> PhD thesis, </type> <institution> Trinity Hall, University of Cambridge, </institution> <address> England, </address> <year> 1990. </year>
Reference-contexts: Modularized, local and instance-based approaches to function approximation have often been reported to generalize better from fewer or ill-distributed examples <ref> [ Moore, 1990 ] </ref> , [ Nowlan, 1990 ] , [ Atkeson, 1991 ] , [ Friedmann, 1991 ] , [ Fox et al., 1991 ] , [ Jacobs and Jordan, 1991 ] .
Reference: [ Moravec, 1988 ] <author> Hans P. Moravec. </author> <title> Sensor fusion in certainty grids for mobile robots. </title> <journal> AI Magazine, </journal> <pages> pages 61-74, </pages> <month> Summer </month> <year> 1988. </year>
Reference-contexts: This planner, however, operates on a two-dimensional representation of the environment. In order to scale up to higher dimensional environments the approach taken has to be modified. Future research will also include comparisons to other approaches to model building and path planning, e.g. [ Elves, 1987 ] , <ref> [ Moravec, 1988 ] </ref> .
Reference: [ Nowlan, 1990 ] <author> Steven J. Nowlan. </author> <title> Competing experts: An experimental investigation of associative mixture models. </title> <type> Technical Report CRG-TR-90-5, </type> <institution> Dept. of Computer Science, University of Toronto, Canada, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Modularized, local and instance-based approaches to function approximation have often been reported to generalize better from fewer or ill-distributed examples [ Moore, 1990 ] , <ref> [ Nowlan, 1990 ] </ref> , [ Atkeson, 1991 ] , [ Friedmann, 1991 ] , [ Fox et al., 1991 ] , [ Jacobs and Jordan, 1991 ] .
Reference: [ Rumelhart et al., 1986 ] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing. </booktitle> <volume> Vol. I + II. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This assumption is relaxed in section 4, where a technique for re-estimating the location of the robot based on sensations is described. 3 (a) neural networks: the sensor interpretation network R and the confidence estimation network C. These networks are trained using the backpropagation training procedure <ref> [ Rumelhart et al., 1986 ] </ref> to encode the specific characteristics of the sensors as well as those of typical environments of a mobile robot.
Reference: [ Simmons, 1992 ] <author> Reid Simmons. </author> <title> Concurrent planning and execution for autonomous robots. </title> <journal> IEEE Control Systems, </journal> <volume> 12(1) </volume> <pages> 46-50, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: COLUMBUS actual implementation is modularized and distributed. Modules (map builder, planner, position controller, central controller and graphical user interface) are connected using the Task Control Architecture <ref> [ Simmons, 1992 ] </ref> , which allows to execute the programs on several SUN SPARC workstations in parallel. Robot actions take usually between 3 and 12 seconds, plus approximately 3 seconds for transmitting sensor and control information by a radio link.
Reference: [ Singh, 1992 ] <author> Satinder P. Singh. </author> <title> The efficient learning of multiple task sequences. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 251-258, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: approaches to function fitting (e.g., with a single monolithic neural network) in mobile robot domains either fail in complex environments due to effects like un-/relearning, or demand many well-distributed training examples and have often been tested in simplified simulations only [ Bachrach, 1991 ] , [ Lin, 1991 ] , <ref> [ Singh, 1992 ] </ref> , [ Thrun and Moller, 1992 ] .
Reference: [ Sutton, 1990 ] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <month> June </month> <year> 1990, </year> <pages> pages 216-224, </pages> <year> 1990. </year>
Reference-contexts: In order to find low-cost paths to the unexplored, the model is discretized yielding a grid representation of the environment, and dynamic programming is employed to propagate exploration utility through this discretized model [ Barto et al., 1991 ] , <ref> [ Sutton, 1990 ] </ref> . More specifically, this is done in the following way: To each grid point x in the discretized model there is a real-valued exploration utility U (x) associated. Initially, the exploration utility of x is set to the negative cumulative confidence c M (x).
Reference: [ Thrun and Moller, 1992 ] <author> Sebastian B. Thrun and Knut Moller. </author> <title> Active exploration in dynamic environments. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 531-538, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: with a single monolithic neural network) in mobile robot domains either fail in complex environments due to effects like un-/relearning, or demand many well-distributed training examples and have often been tested in simplified simulations only [ Bachrach, 1991 ] , [ Lin, 1991 ] , [ Singh, 1992 ] , <ref> [ Thrun and Moller, 1992 ] </ref> .
Reference: [ Thrun, 1992a ] <author> Sebastian B. Thrun. </author> <title> Efficient exploration in reinforcement learning. </title> <type> Technical Report CMU-CS-92-102, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: In contrast, more thoughtful exploration techniques, such as go to the least explored location, have been shown to reduce the complexity to a small polynomial function in the size of the state space <ref> [ Thrun, 1992a ] </ref> , [ Koenig, 1992 ] . While these results may be theoretically significant, their relevance and implications for practical research in robot exploration are unclear.
Reference: [ Thrun, 1992b ] <author> Sebastian B. Thrun. </author> <title> The role of exploration in learning control. </title> <editor> In David A. White and Donald A. Sofge, editors, </editor> <booktitle> Handbook of intelligent control: neural, fuzzy and adaptive approaches, </booktitle> <address> Florence, Kentucky 41022, 1992. </address> <publisher> Van Nostrand Reinhold. </publisher>
Reference-contexts: In contrast, tabula rasa learning methods would result in collisions in any new, unknown environment before learning to avoid them. COLUMBUS top-level goal is efficient exploration. The approach taken in this paper is motivated by earlier research on exploration in the context of reinforcement learning <ref> [ Thrun, 1992b ] </ref> . Theoretical results on the efficiency of exploration indicate the importance of the exploration strategy for the amount of knowledge gained, and for the efficiency of learning control in general.
Reference: [ Whitehead, 1991 ] <author> Steven D. Whitehead. </author> <title> A study of cooperative mechanisms for faster reinforcement learning. </title> <type> Technical Report 365, </type> <institution> University of Rochester, Computer Science Department, Rochester, </institution> <address> NY, </address> <year> 1991. </year> <month> 12 </month>
Reference-contexts: It has been shown that for certain hard deterministic environments, that an autonomous robot can face, exploration strategies such as random walk result in an expected learning time that scales at least exponentially with the number of states the environment can take <ref> [ Whitehead, 1991 ] </ref> . In contrast, more thoughtful exploration techniques, such as go to the least explored location, have been shown to reduce the complexity to a small polynomial function in the size of the state space [ Thrun, 1992a ] , [ Koenig, 1992 ] .
References-found: 20

