URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-95-1285/CS-TR-95-1285.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-95-1285/
Root-URL: http://www.cs.wisc.edu
Title: LEARNING FROM INSTRUCTION AND EXPERIENCE: METHODS FOR INCORPORATING PROCEDURAL DOMAIN THEORIES INTO KNOWLEDGE-BASED NEURAL NETWORKS  
Author: By Richard Frank Maclin 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1995  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: <author> Abu-Mostafa, Y. </author> <year> (1995). </year> <title> Hints. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 639-671. </pages>
Reference: <author> Agre, P. & Chapman, D. </author> <year> (1987). </year> <title> Pengi: An implementation of a theory of activity. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 268-272), </pages> <address> Seattle, WA. </address>
Reference: <author> Anderson, C. </author> <year> (1987). </year> <title> Strategy learning with multilayer connectionist representations. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> (pp. 103-114), </pages> <address> Irvine, CA. </address>
Reference: <author> Angluin, D. </author> <year> (1987). </year> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106. </pages>
Reference: <author> Atlas, L., Cole, R., Muthusamy, Y., Lippman, A., Connor, J., Park, D., El-Sharkawi, M., & Marks, R. </author> <year> (1990). </year> <title> A performance comparison of trained multilayer perceptrons and trained classification trees. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78 </volume> <pages> 1614-1619. </pages>
Reference: <author> Barto, A., Bradtke, S., & Singh, S. </author> <year> (1995). </year> <title> Learning act using realtime dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72 </volume> <pages> 81-138. </pages>
Reference: <author> Barto, A., Sutton, R., & Anderson, C. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 834-846. </pages>
Reference: <author> Barto, A., Sutton, R., & Watkins, C. </author> <year> (1990). </year> <title> Learning and sequential decision making. </title> <editor> In Gabriel, M. & Moore, J., editors, </editor> <title> Learning and Computational Neuroscience. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Berenji, H. & Khedkar, P. </author> <year> (1992). </year> <title> Learning and tuning fuzzy logic controllers through reinforcements. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3 </volume> <pages> 724-740. </pages>
Reference: <author> Brooks, R. </author> <year> (1990). </year> <title> The behavior language; user's guide. </title> <type> AI Memo 1227, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference: <author> Chapman, D. </author> <year> (1991). </year> <title> Vision, Instruction, and Action. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Chou, P. & Fasman, G. </author> <year> (1978). </year> <title> Prediction of the secondary structure of proteins from their amino acid sequence. </title> <booktitle> Advances in Enzymology, </booktitle> <volume> 47 </volume> <pages> 45-148. </pages>
Reference: <author> Cleeremans, A., Servan-Schreiber, D., & McClelland, J. </author> <year> (1989). </year> <title> Finite state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 372-381. </pages> <note> 154 Clouse, </note> <author> J. & Utgoff, P. </author> <year> (1992). </year> <title> A teaching method for reinforcement learning. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> (pp. 92-101), </pages> <address> Aberdeen, Scotland. </address>
Reference: <author> Cohen, B., Presnell, S., Cohen, F., & Langridge, R. </author> <year> (1991). </year> <title> A proposal for feature-based scoring of protein secondary structure predictions. </title> <booktitle> In Proceedings of the AAAI91 Workshop on AI Approaches to Classification and Pattern Recognition in Molecular Biology, </booktitle> <pages> (pp. 5-20), </pages> <address> Anaheim, CA. </address>
Reference-contexts: A region is a consecutive sequence of amino acids with the same secondary structure. I consider regions because the measure of accuracy obtained by comparing the prediction for each amino acid does not adequately capture the notion of secondary structure as biologists view it <ref> (Cohen et al., 1991) </ref>. For biologists, knowing the number of regions and the approximate order of the regions is nearly as important as knowing exactly the structure within which each amino acid lies. Consider the two predictions in Figure 21 (adapted from Cohen et al., 1991). <p> For biologists, knowing the number of regions and the approximate order of the regions is nearly as important as knowing exactly the structure within which each amino acid lies. Consider the two predictions in Figure 21 <ref> (adapted from Cohen et al., 1991) </ref>. The first prediction completely misses the third ff-helix region, so it has four errors. The second prediction is slightly skewed for each ff-helix region and ends up having six errors, even though it appears to be a better answer.
Reference: <author> Cohen, P. & Feigenbaum, E. </author> <year> (1982). </year> <booktitle> The Handbook of Artificial Intelligence (volume 3). </booktitle> <publisher> William Kaufmann, </publisher> <address> Los Altos, CA. </address>
Reference: <author> Cohen, W. </author> <year> (1994). </year> <title> Grammatically biased learning: Learning logic programs using an explicit antecedent description language. </title> <journal> Artificial Intelligence, </journal> <volume> 68 </volume> <pages> 303-366. </pages>
Reference: <author> Cost, S. & Salzberg, S. </author> <year> (1993). </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 57-78. </pages>
Reference-contexts: I also chose the secondary-structure task because a number of machine learning techniques are currently being applied to this task, including neural networks (Holley & Karplus, 1989; Qian & Sejnowski, 1988), inductive logic programming (Muggleton & Feng, 1990), case-based reasoning <ref> (Cost & Salzberg, 1993) </ref>, and multistrategy learning (Zhang et al., 1992). Thus this task provides a good baseline for the effectiveness of my approach. The protein-folding task is an open problem that is becoming increasingly critical as the Human Genome Project (Watson, 1990) proceeds.
Reference: <author> Crangle, C. & Suppes, P. </author> <year> (1994). </year> <title> Language and Learning for Robots. </title> <publisher> CSLI Publications, Stanford, </publisher> <address> CA. </address>
Reference: <author> Craven, M. & Shavlik, J. </author> <year> (1994). </year> <title> Using sampling and queries to extract rules from trained neural networks. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> (pp. 37-45), </pages> <address> New Brunswick, NJ. </address>
Reference: <author> Dent, L., Boticario, J., McDermott, J., Mitchell, T., & Zabowski, D. </author> <year> (1992). </year> <title> A personal learning apprentice. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 96-103), </pages> <address> San Jose, CA. </address>
Reference: <author> Diederich, J. </author> <year> (1989). </year> <title> "Learning by instruction" in connectionist systems. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> (pp. 66-68), </pages> <address> Ithaca, NY. </address>
Reference: <author> Dietterich, T. </author> <year> (1991). </year> <title> Knowledge compilation: Bridging the gap between specification and implementation. </title> <journal> IEEE Expert, </journal> <volume> 6 </volume> <pages> 80-82. </pages>
Reference-contexts: Step 2. Convert the advice to an internal representation. Once the teacher has created a piece of advice, ratle parses it using the tools lex and yacc (Levine et al., 1992). Step 3. Convert the advice into a usable form. Other techniques, such as knowledge compilation <ref> (Dietterich, 1991) </ref>, convert ("operationalize") high-level instructions into a (usually larger) collection of directly interpretable statements (Gordon & Subramanian, 1994; Kaelbling & Rosenschein, 1990; Nilsson, 1994).
Reference: <author> Elman, J. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211. </pages>
Reference: <author> Elman, J. </author> <year> (1991). </year> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 195-225. </pages>
Reference: <author> Fahlman, S. & Lebiere, C. </author> <year> (1990). </year> <title> The cascade-correlation learning architecture. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address>
Reference-contexts: Methods addressing the problem of selecting an appropriate network architecture include techniques for network pruning (Le Cun et al., 1990), genetic search (Opitz & Shavlik, 1993), and cascade correlation <ref> (Fahlman & Lebiere, 1990) </ref>. In this work I rely on the domain theory provided by the teacher to select an appropriate architecture. As will be seen in the next section, the instructions of the teacher result in a corresponding neural-network architecture. <p> I used backpropagation (Rumelhart et al., 1986) to train neural networks for two learning approaches, our FSkbann approach and a standard neural-network approach similar to Qian and Sejnowski's (which I will refer to as standard ANNs). I terminated training using patience as a stopping criterion <ref> (Fahlman & Lebiere, 1990) </ref>. The patience criterion states that training should continue until the error rate on the training set has not decreased for some number of training cycles. For this study I set the number of epochs to be four, a value I determined by empirical testing.
Reference: <author> Fisher, D. & McKusick, K. </author> <year> (1989). </year> <title> An empirical comparison of ID3 and back-propagation. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 788-793), </pages> <address> Detroit, MI. </address> <note> 155 Frasconi, </note> <author> P., Gori, M., Maggini, M., & Soda, G. </author> <year> (1995). </year> <title> Unified integration of explicit knowledge and learning by example in recurrent networks. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 7 </volume> <pages> 340-346. </pages>
Reference: <author> Frasconi, P., Gori, M., & Soda, G. </author> <year> (1992). </year> <title> Local feedback multi-layered networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 120-130. </pages>
Reference-contexts: Frasconi, Gori, Maggini and Soda Frasconi et al. (1995) have also examined an an approach for learning a finite-state automaton where some information about the FSA is known. In their approach, they use Local 133 Feedback Multi-Layered Networks <ref> (Frasconi et al., 1992) </ref>. This type of network has recurrent connections within the hidden layer of the network. They show that this type of network will achieve a stable activation state for each input value in a finite number of steps.
Reference: <author> Fu, L. M. </author> <year> (1989). </year> <title> Integration of neural heuristics into knowledge-based inference. </title> <journal> Connection Science, </journal> <volume> 1 </volume> <pages> 325-340. </pages>
Reference: <author> Garnier, J. & Robson, B. </author> <year> (1989). </year> <title> The GOR method for predicting secondary structures in proteins. </title> <editor> In Fasman, G., editor, </editor> <title> Prediction of Protein Structure and the Principles of Protein Conformation. </title> <publisher> Plenum Press, </publisher> <address> New York. </address>
Reference: <author> Gat, E. </author> <year> (1991). </year> <title> ALFA: A language for programming reactive robotic control systems. </title> <booktitle> In IEEE International Conference on Robotics and Automation (volume 2), </booktitle> <pages> (pp. 1116-1121). </pages>
Reference: <author> Giles, C., Miller, C., Chen, D., Chen, H., Sun, G., & Lee, Y. </author> <year> (1992). </year> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 393-405. </pages>
Reference: <author> Ginsberg, A. </author> <year> (1988). </year> <title> Automatic Refinement of Expert System Knowledge Bases. </title> <publisher> Pitman, London. </publisher>
Reference: <author> Gold, E. M. </author> <year> (1972). </year> <title> System identification via state characterization. </title> <journal> Automatica, </journal> <volume> 8 </volume> <pages> 621-636. </pages>
Reference: <author> Gold, E. M. </author> <year> (1978). </year> <title> Complexity of automaton identification from given data. </title> <journal> Information and Control, </journal> <volume> 37 </volume> <pages> 302-320. </pages>
Reference: <author> Gordon, D. & Subramanian, D. </author> <year> (1994). </year> <title> A multistrategy learning scheme for agent knowledge acquisition. </title> <journal> Informatica, </journal> <volume> 17 </volume> <pages> 331-346. </pages>
Reference: <author> Gruau, F. </author> <year> (1994). </year> <title> Neural Network Synthesis using Cellular Encoding and the Genetic Algorithm. </title> <type> PhD thesis, </type> <institution> Ecole Normale Superieure de Lyon, France. </institution>
Reference: <author> Hayes-Roth, F., Klahr, P., & Mostow, D. J. </author> <year> (1981). </year> <title> Advice-taking and knowledge refinement: An iterative view of skill acquisition. </title> <editor> In Anderson, J., editor, </editor> <title> Cognitive Skills and their Acquisition. </title> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Hinton, G. E. </author> <year> (1986). </year> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> (pp. 1-12), </pages> <address> Amherst, MA. </address>
Reference-contexts: One approach is to introduce a term into the cost function that penalizes a network that is overfitting the data. Such techniques are called regularization methods. One standard method for regularization is called weight decay <ref> (Hinton, 1986) </ref>. Weight decay works as its name suggests: at each step each weight is decayed towards zero by a small amount. This approach is equivalent to adding a penalty term to the cost function proportional to the sum of the squared weights in the network. <p> As the agent's neural network grows, the learning process becomes slower, since a larger network requires more computation. Also, an overly large neural network can often produce overfitting (Holder, 1991). One solution to this problem (see Chapter 2) introduces a penalty term to prevent overfitting, such as weight decay <ref> (Hinton, 1986) </ref>, which can cause the learner to gradually remove unused links. A related approach periodically prunes the network (Le Cun et al., 1990) to find and remove hidden units that are no longer being used. Utilizing these approaches is a subject of future work.
Reference: <author> Holder, L. B. </author> <year> (1991). </year> <title> Maintaining the Utility of Learned Knowledge Using Model-Based Control. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of Illinois at Urbana-Champaign. </institution> <address> 156 Holland, J. </address> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI. </address>
Reference-contexts: As the agent's neural network grows, the learning process becomes slower, since a larger network requires more computation. Also, an overly large neural network can often produce overfitting <ref> (Holder, 1991) </ref>. One solution to this problem (see Chapter 2) introduces a penalty term to prevent overfitting, such as weight decay (Hinton, 1986), which can cause the learner to gradually remove unused links.
Reference: <author> Holland, J. </author> <year> (1986). </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In Michalski, R., Carbonell, J., & Mitchell, T., editors, </editor> <title> Machine Learning: An AI Approach (volume 2). </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: A number of early AI systems made use of reinforcement learning techniques, work such as Samuel's checker-playing program (Samuel, 1959) and Holland's bucket-brigade algorithm <ref> (Holland, 1986) </ref>. In standard RL (see Figure 9), the reinforcement learner (usually called the agent) senses the current state of the world, chooses an action to apply to the world, and occasionally receives rewards and punishments based on its actions and the states it sees.
Reference: <author> Holley, L. & Karplus, M. </author> <year> (1989). </year> <title> Protein structure prediction with a neural network. </title> <booktitle> Proceedings of the National Academy of Sciences (USA), </booktitle> <volume> 86 </volume> <pages> 152-156. </pages>
Reference: <author> Hopcroft, J. & Ullman, J. </author> <year> (1979). </year> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: FSkbann (for Finite-State kbann) translates domain theories that include generalized finite-state automata - FSA <ref> (Hopcroft & Ullman, 1979) </ref>, into corresponding neural networks. FSkbann then refines the resulting networks using backpropagation (Rumelhart et al., 1986) on a set of training examples.
Reference: <author> Huffman, S. & Laird, J. </author> <year> (1993). </year> <title> Learning procedures from interactive natural language instructions. </title> <booktitle> In Machine Learning: Proceedings on the Tenth International Conference, </booktitle> <pages> (pp. 143-150), </pages> <address> Amherst, MA. </address>
Reference: <author> Jensen, K. & Wirth, N. </author> <year> (1975). </year> <title> PASCAL: User Manual and Report. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: While the ratle language is not a panacea for instruction, it does permit the teacher to express a wide range of possible instructions. This chapter presents the complete description of ratle's advice language. Advice in the ratle language consists of a set of simple statements, similar to Pascal <ref> (Jensen & Wirth, 1975) </ref> programming statements. Recall that I use a programming language rather than natural language because this greatly simplifies the task of parsing the language and lets me focus on the process of transferring advice to the agent.
Reference: <author> Jordan, M. </author> <year> (1989). </year> <title> Serial order: A parallel, distributed processing approach. </title> <editor> In Elman, J. & Rumelhart, D., editors, </editor> <booktitle> Advances in Connectionist Theory: Speech. </booktitle> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Kaelbling, L. </author> <year> (1987). </year> <title> REX: A symbolic language for the design and parallel implementation of embedded systems. </title> <booktitle> In Proceedings of the AIAA Conference on Computers in Aerospace, </booktitle> <address> Wakefield, MA. </address>
Reference: <author> Kaelbling, L. & Rosenschein, S. </author> <year> (1990). </year> <title> Action and planning in embedded agents. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 6 </volume> <pages> 35-48. </pages>
Reference: <author> Laird, J., Hucka, M., Yager, E., & Tuck, C. </author> <year> (1990). </year> <title> Correcting and extending domain knowledge using outside guidance. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> (pp. 235-243), </pages> <address> Austin, TX. </address>
Reference: <author> Laird, J., Newell, A., & Rosenbloom, P. </author> <year> (1987). </year> <title> SOAR: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 1-64. </pages>
Reference-contexts: More recently, Laird et al. (1990) created an advice-taking technique called Robo-Soar that is based on the Soar architecture <ref> (Laird et al., 1987) </ref>. Soar uses its knowledge to select operators to achieve goals. When it is unable to select from a set of operators, or no operator is applicable, an impasse arises. When this happens, Soar creates a subgoal and applies itself to that subgoal.
Reference: <author> Lang, K., Waibel, A., & Hinton, G. </author> <year> (1990). </year> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 23-43. </pages>
Reference-contexts: Other approaches to regularization include network pruning (Le Cun et al., 1990) and soft-weight sharing (Nowlan & Hinton, 1992). Another way to prevent overfitting is to use a validation set <ref> (Lang et al., 1990) </ref>.
Reference: <author> Le Cun, Y., Denker, J., & Solla, S. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address> <note> 157 Leng, </note> <author> B., Buchanan, B., & Nicholas, H. </author> <year> (1993). </year> <title> Protein secondary structure prediction using two-level case-based reasoning. </title> <booktitle> In Proceedings of the First International Conference on Intelligent Systems for Molecular Biology, </booktitle> <pages> (pp. 251-259), </pages> <address> Washington, DC. </address>
Reference-contexts: This approach is equivalent to adding a penalty term to the cost function proportional to the sum of the squared weights in the network. Other approaches to regularization include network pruning <ref> (Le Cun et al., 1990) </ref> and soft-weight sharing (Nowlan & Hinton, 1992). Another way to prevent overfitting is to use a validation set (Lang et al., 1990). <p> For example, if the resulting network does not have enough units to solve the problem, there is no easy way to determine this other than having the learner fail to learn a solution. Methods addressing the problem of selecting an appropriate network architecture include techniques for network pruning <ref> (Le Cun et al., 1990) </ref>, genetic search (Opitz & Shavlik, 1993), and cascade correlation (Fahlman & Lebiere, 1990). In this work I rely on the domain theory provided by the teacher to select an appropriate architecture. <p> Methods addressing the problem of selecting an appropriate network architecture include techniques for network pruning (Le Cun et al., 1990), genetic search (Opitz & Shavlik, 1993), and cascade correlation <ref> (Fahlman & Lebiere, 1990) </ref>. In this work I rely on the domain theory provided by the teacher to select an appropriate architecture. As will be seen in the next section, the instructions of the teacher result in a corresponding neural-network architecture. <p> I also chose the secondary-structure task because a number of machine learning techniques are currently being applied to this task, including neural networks (Holley & Karplus, 1989; Qian & Sejnowski, 1988), inductive logic programming <ref> (Muggleton & Feng, 1990) </ref>, case-based reasoning (Cost & Salzberg, 1993), and multistrategy learning (Zhang et al., 1992). Thus this task provides a good baseline for the effectiveness of my approach. The protein-folding task is an open problem that is becoming increasingly critical as the Human Genome Project (Watson, 1990) proceeds. <p> I used backpropagation (Rumelhart et al., 1986) to train neural networks for two learning approaches, our FSkbann approach and a standard neural-network approach similar to Qian and Sejnowski's (which I will refer to as standard ANNs). I terminated training using patience as a stopping criterion <ref> (Fahlman & Lebiere, 1990) </ref>. The patience criterion states that training should continue until the error rate on the training set has not decreased for some number of training cycles. For this study I set the number of epochs to be four, a value I determined by empirical testing. <p> One solution to this problem (see Chapter 2) introduces a penalty term to prevent overfitting, such as weight decay (Hinton, 1986), which can cause the learner to gradually remove unused links. A related approach periodically prunes the network <ref> (Le Cun et al., 1990) </ref> to find and remove hidden units that are no longer being used. Utilizing these approaches is a subject of future work. Other approaches to advice-taking that do not suffer from this problem will be discussed in Chapter 8.
Reference: <author> Levine, J., Mason, T., & Brown, D. </author> <year> (1992). </year> <title> Lex & Yacc. </title> <address> O'Reilly, Sebastopol, CA. </address>
Reference-contexts: Step 2. Convert the advice to an internal representation. Once the teacher has created a piece of advice, ratle parses it using the tools lex and yacc <ref> (Levine et al., 1992) </ref>. Step 3. Convert the advice into a usable form. Other techniques, such as knowledge compilation (Dietterich, 1991), convert ("operationalize") high-level instructions into a (usually larger) collection of directly interpretable statements (Gordon & Subramanian, 1994; Kaelbling & Rosenschein, 1990; Nilsson, 1994).
Reference: <author> Lim, V. </author> <year> (1974). </year> <title> Algorithms for prediction of ff-helical and fi-structural regions in globular proteins. </title> <journal> Journal of Molecular Biology, </journal> <volume> 88 </volume> <pages> 873-894. </pages>
Reference: <author> Lin, L. </author> <year> (1991). </year> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 781-786), </pages> <address> Anaheim, CA. </address>
Reference-contexts: particular Q-function, because of the gradient-based nature of neural networks, a connectionist Q-function is not guaranteed to find an optimal policy function a limitation which will become important later. 24 Chapter 3 A First Approach: FSKBANN This chapter describes my initial approach (called FSkbann) to refining a procedural domain theory <ref> (Maclin & Shavlik, 1991) </ref>. FSkbann (for Finite-State kbann) translates domain theories that include generalized finite-state automata - FSA (Hopcroft & Ullman, 1979), into corresponding neural networks. FSkbann then refines the resulting networks using backpropagation (Rumelhart et al., 1986) on a set of training examples.
Reference: <author> Lin, L. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 293-321. </pages>
Reference-contexts: Thirdly, further tests with the replay technique <ref> (Lin, 1992) </ref> are important, since this technique allows the agent to re-use the experiences it accumulates. This ability is crucial for domains where the cost of exploration is prohibitive.
Reference: <author> Lin, L. </author> <year> (1993). </year> <title> Scaling up reinforcement learning for robot control. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> (pp. 182-189), </pages> <address> Amherst, MA. </address>
Reference-contexts: Both of my approaches for refining procedural domain theories use backpropagation (Rumelhart et al., 1986) on neural networks as their basic learning method and build on work in knowledge-based neural networks (Towell et al., 1990). My preliminary technique (FSkbann) focuses on refining domain theories represented as finite-state automata <ref> (Maclin & Shavlik, 1993) </ref>. Table 2 shows the general input and output behavior of this system. It uses a finite-state domain theory and some training examples to refine the provided domain theory.
Reference: <author> Maclin, R. & Shavlik, J. </author> <year> (1991). </year> <title> Refining domain theories expressed as finite-state automata. </title> <booktitle> In Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <pages> (pp. 524-528), </pages> <address> Evanston, IL. </address>
Reference-contexts: particular Q-function, because of the gradient-based nature of neural networks, a connectionist Q-function is not guaranteed to find an optimal policy function a limitation which will become important later. 24 Chapter 3 A First Approach: FSKBANN This chapter describes my initial approach (called FSkbann) to refining a procedural domain theory <ref> (Maclin & Shavlik, 1991) </ref>. FSkbann (for Finite-State kbann) translates domain theories that include generalized finite-state automata - FSA (Hopcroft & Ullman, 1979), into corresponding neural networks. FSkbann then refines the resulting networks using backpropagation (Rumelhart et al., 1986) on a set of training examples.
Reference: <author> Maclin, R. & Shavlik, J. </author> <year> (1992). </year> <title> Using knowledge-based neural networks to improve algorithms: Refining the Chou-Fasman algorithm for protein folding. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 165-170), </pages> <address> San Jose, CA. </address>
Reference: <author> Maclin, R. & Shavlik, J. </author> <year> (1993). </year> <title> Using knowledge-based neural networks to improve algorithms: Refining the Chou-Fasman algorithm for protein folding. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 195-215. </pages>
Reference-contexts: Both of my approaches for refining procedural domain theories use backpropagation (Rumelhart et al., 1986) on neural networks as their basic learning method and build on work in knowledge-based neural networks (Towell et al., 1990). My preliminary technique (FSkbann) focuses on refining domain theories represented as finite-state automata <ref> (Maclin & Shavlik, 1993) </ref>. Table 2 shows the general input and output behavior of this system. It uses a finite-state domain theory and some training examples to refine the provided domain theory.
Reference: <author> Maclin, R. & Shavlik, J. </author> <year> (1994a). </year> <title> Incorporating advice into agents that learn from reinforcements. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 694-699), </pages> <address> Seattle, WA. </address>
Reference: <author> Maclin, R. & Shavlik, J. </author> <year> (1994b). </year> <title> Refining algorithms with knowledge-based neural networks: Improving the Chou-Fasman algorithm for protein folding. </title> <editor> In Hanson, S., Drastal, G., & Rivest, R., editors, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems (volume 1). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Maclin, R. & Shavlik, J. </author> <year> (1996). </year> <title> Creating advice-taking reinforcement learners. </title> <booktitle> Machine Learning. </booktitle>
Reference-contexts: In my second approach I develop a method that lets a teacher provide instructions to a reinforcement learner <ref> (Maclin & Shavlik, 1996) </ref>. In this approach I use a learner that performs connectionist Q-learning (Lin, 1992; Sutton, 1988; Watkins, 1989). My work allows the reinforcement learner to take instructions in the form of programming-language constructs in a simple, yet expressive, language that I have developed.
Reference: <author> Maes, P. & Kozierok, R. </author> <year> (1993). </year> <title> Learning interface agents. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 459-465), </pages> <address> Washington, DC. </address> <note> 158 Mahadevan, </note> <author> S. & Connell, J. </author> <year> (1992). </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55 </volume> <pages> 311-365. </pages>
Reference: <author> Mataric, M. </author> <year> (1994). </year> <title> Reward functions for accelerated learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> (pp. 181-189), </pages> <address> New Brunswick, NJ. </address>
Reference: <author> Mathews, B. </author> <year> (1975). </year> <title> Comparison of the predicted and observed secondary structure of T4 Phage Lysozyme. </title> <journal> Biochimica et Biophysica Acta, </journal> <volume> 405 </volume> <pages> 442-451. </pages>
Reference-contexts: The apparent gain in accuracy for FSkbann over ANN networks appears fairly small 2 The following formula defines the correlation coefficient for the secondary-structure task <ref> (Mathews, 1975) </ref>: P N F M (P + F )(P + M )(N + F )(N + M ) where C is calculated for each structure separately, and P, N, F, and M are the number of true positives, true negatives, false positives, and misses for each structure, respectively. 40 Table
Reference: <author> McCarthy, J. </author> <year> (1958). </year> <title> Programs with common sense. </title> <booktitle> In Proceedings of the Symposium on the Mechanization of Thought Processes (volume I), </booktitle> <pages> (pp. 77-84). </pages> <note> (Reprinted in M. </note> <editor> Minsky, editor, </editor> <booktitle> 1968, Semantic Information Processing. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press, 403-409.). </publisher>
Reference: <author> Minsky, M. & Papert, S. </author> <year> (1969). </year> <title> Perceptrons. </title> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference: <author> Mitchell, T., Keller, R., & Kedar-Cabelli, S. </author> <year> (1986). </year> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 47-80. </pages>
Reference-contexts: Huffman and Laird (1993) developed another Soar-based method called Instructo-Soar that allows an agent to interpret simple imperative statements such as "Pick up the red block." Instructo-Soar examines these instructions in the context of its current task, and uses Soar's form of explanation-based learning <ref> (Mitchell et al., 1986) </ref> to generalize the instruction into a rule that it uses in similar situations.
Reference: <author> Moody, J. & Darken, C. </author> <year> (1988). </year> <title> Learning with localized receptive fields. </title> <editor> In Hinton, G., Sejnowski, T., & Touretzky, D., editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> (pp. 133-143), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: I chose sigmoidal activation functions mostly for simplicity, since I was already using sigmoidal units for mapping other constructs. Other activation functions might make more sense for capturing fuzzy functions, especially radial basis functions <ref> (Moody & Darken, 1988, 1989) </ref>. A unit using a radial basis function is defined by a prototype point in input space. The unit is more active the closer the current input vector is to the prototype (a second parameter determines how dispersed the activation function is).
Reference: <author> Moody, J. & Darken, C. </author> <year> (1989). </year> <title> Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 281-294. </pages>
Reference: <author> Mostow, D. J. </author> <year> (1982). </year> <title> Transforming declarative advice into effective procedures: A heuristic search example. </title> <editor> In Michalski, R., Carbonell, J., & Mitchell, T., editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (volume 1). </booktitle> <publisher> Tioga Press, </publisher> <address> Palo Alto. </address>
Reference-contexts: Also, ratle agents are able to learn. In foo <ref> (Mostow, 1982) </ref>, general advice is operationalized by reformulating the advice into search heuristics. Foo applies these search heuristics during problem solving.
Reference: <author> Muggleton, S. </author> <year> (1992). </year> <title> Inductive logic programming. </title> <publisher> Academic Press, London. </publisher>
Reference-contexts: I first present a number of alternatives for incorporating advice into a neural network and discuss how these approaches relate to mine. I then give an overview of some work using inductive-logic programming <ref> (Muggleton, 1992) </ref> to refine a domain theory. Next, I present research on inducing finite-state automata. Early work in this field focused on theoretical limits for learning algorithms and methods using constructs such as oracles (i.e., a device that can always produce an answer to a query if an answer exists).
Reference: <author> Muggleton, S. & Feng, C. </author> <year> (1990). </year> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Theory, </booktitle> <pages> (pp. 1-14), </pages> <address> Tokyo, Japan. </address>
Reference-contexts: I also chose the secondary-structure task because a number of machine learning techniques are currently being applied to this task, including neural networks (Holley & Karplus, 1989; Qian & Sejnowski, 1988), inductive logic programming <ref> (Muggleton & Feng, 1990) </ref>, case-based reasoning (Cost & Salzberg, 1993), and multistrategy learning (Zhang et al., 1992). Thus this task provides a good baseline for the effectiveness of my approach. The protein-folding task is an open problem that is becoming increasingly critical as the Human Genome Project (Watson, 1990) proceeds.
Reference: <author> Nilsson, N. </author> <year> (1994). </year> <title> Teleo-reactive programs for agent control. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 139-158. </pages>
Reference: <author> Nishikawa, K. </author> <year> (1983). </year> <title> Assessment of secondary-structure prediction of proteins: Comparison of computerized Chou-Fasman method with others. </title> <journal> Biochimica et Biophysica Acta, </journal> <volume> 748 </volume> <pages> 285-299. </pages>
Reference: <author> Noelle, D. & Cottrell, G. </author> <year> (1994). </year> <title> Towards instructable connectionist systems. In Sun, </title> <editor> R. & Bookman, L., editors, </editor> <booktitle> Computational Architectures Integrating Neural and Symbolic Processes. </booktitle> <publisher> Kluwer Academic, </publisher> <address> Boston. </address> <note> 159 Nowlan, </note> <author> S. & Hinton, G. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 473-493. </pages>
Reference: <author> Omlin, C. & Giles, C. </author> <year> (1992). </year> <title> Training second-order recurrent neural networks using hints. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> (pp. 361-366), </pages> <address> Aberdeen, Scotland. </address>
Reference: <author> Opitz, D. & Shavlik, J. </author> <year> (1993). </year> <title> Heuristically expanding knowledge-based neural networks. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 1360-1365), </pages> <address> Chambery, France. </address>
Reference-contexts: Methods addressing the problem of selecting an appropriate network architecture include techniques for network pruning (Le Cun et al., 1990), genetic search <ref> (Opitz & Shavlik, 1993) </ref>, and cascade correlation (Fahlman & Lebiere, 1990). In this work I rely on the domain theory provided by the teacher to select an appropriate architecture. As will be seen in the next section, the instructions of the teacher result in a corresponding neural-network architecture.
Reference: <author> Ourston, D. & Mooney, R. </author> <year> (1990). </year> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 815-820), </pages> <address> Boston, MA. </address>
Reference: <author> Ourston, D. & Mooney, R. </author> <year> (1994). </year> <title> Theory refinement combining analytical and empirical methods. </title> <journal> Artificial Intelligence, </journal> <volume> 66 </volume> <pages> 273-309. </pages>
Reference: <author> Pazzani, M. & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 57-94. </pages>
Reference: <author> Pineda, F. </author> <year> (1987). </year> <title> Generalization of back-propagation to recurrent neural networks. </title> <journal> Physical Review Letters, </journal> <volume> 59 </volume> <pages> 2229-2232. </pages>
Reference-contexts: During learning we ignore the recurrent links no learning is done on these links. This is the main difference between SRNs and other recurrent techniques such as backpropagation through time (Minsky & Papert, 1969; Rumelhart et al., 1986) and recurrent backpropagation <ref> (Pineda, 1987) </ref>. Most recurrent network techniques are more complicated than SRNs because they have to deal with learning across these recurrent links, but they are also more powerful for this same reason. as dashed lines.
Reference: <author> Qian, N. & Sejnowski, T. </author> <year> (1988). </year> <title> Predicting the secondary structure of globular proteins using neural network models. </title> <journal> Journal of Molecular Biology, </journal> <volume> 202 </volume> <pages> 865-884. </pages>
Reference: <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-2666. </pages>
Reference: <author> Richards, B. & Mooney, R. </author> <year> (1995). </year> <title> Automated refinement of first-order Horn-clause domain theories. </title> <journal> Machine Learning, </journal> <volume> 19 </volume> <pages> 95-131. </pages>
Reference: <author> Richardson, J. & Richardson, D. </author> <year> (1989). </year> <title> Principles and patterns of protein conformation. </title> <editor> In Fasman, G., editor, </editor> <title> Prediction of Protein Structure and the Principles of Protein Conformation. </title> <publisher> Plenum Press, </publisher> <address> New York. </address>
Reference: <author> Riecken, D. </author> <year> (1994). </year> <journal> Special issue on intelligent agents. Communications of the ACM, </journal> <volume> 37(7). </volume>
Reference: <author> Rivest, R. & Schapire, R. </author> <year> (1987). </year> <title> A new approach to unsupervised learning in deterministic environments. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> (pp. 364-375), </pages> <address> Irvine, CA. </address>
Reference: <author> Rost, B. & Sander, C. </author> <year> (1993). </year> <title> Prediction of protein secondary structure at better than 70% accuracy. </title> <journal> Journal of Molecular Biology, </journal> <volume> 232 </volume> <pages> 584-599. </pages> <note> 160 Rumelhart, </note> <author> D., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. & McClelland, J., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the microstructure of cognition. </booktitle> <volume> (volume 1). </volume> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference-contexts: This is borne out by current biological approaches, where researchers look for matches to existing structures in determining new structures. A second point to note is that significant gains have been achieved by reformulating the input information <ref> (as in Rost & Sander, 1993) </ref>. This suggest that the input information we are currently using may be inadequate to produce a good solution.
Reference: <author> Sacerdoti, E. </author> <year> (1974). </year> <title> Planning in a hierarchy of abstraction spaces. </title> <journal> Artificial Intelligence, </journal> <volume> 5 </volume> <pages> 115-135. </pages>
Reference-contexts: Generally these 125 approaches focus on understanding rather than refining the advice provided by the teacher. One early example of an algorithm that makes use of advice for planning is abstrips <ref> (Sacerdoti, 1974) </ref>. In abstrips, a human assigns initial "criticalities" to preconditions to cause the planner to focus on making key planning steps.
Reference: <author> Salzberg, S. & Cost, S. </author> <year> (1992). </year> <title> Predicting protein secondary structure with a nearest-neighbor algorithm. </title> <journal> Journal of Molecular Biology, </journal> <volume> 227 </volume> <pages> 371-374. </pages>
Reference: <author> Samuel, A. </author> <year> (1959). </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal on Research and Development, </journal> <volume> 3 </volume> <pages> 210-229. </pages> <note> (Reprinted in E. Feigenbaum and J. </note>
Reference-contexts: A number of early AI systems made use of reinforcement learning techniques, work such as Samuel's checker-playing program <ref> (Samuel, 1959) </ref> and Holland's bucket-brigade algorithm (Holland, 1986).
Reference: <editor> Feldman, eds., </editor> <booktitle> 1963, Computers and Thought. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York). </address>
Reference: <author> Schoppers, M. </author> <year> (1994). </year> <title> Estimating reaction plan size. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 1238-1244), </pages> <address> Seattle, WA. </address>
Reference-contexts: case we would hope that the learner would alter the current plan and wait until the pedestrian is out of the way, but if the learner's input description is not a complete description of the world, it will be difficult to anticipate all the possible situations that can come up <ref> (Schoppers, 1994) </ref>. A more appealing approach is to treat a sequential task as a series of sub-tasks. In this approach each input vector represents the current description of the task, and the output vector is the next step to be taken.
Reference: <author> Sedgewick, R. </author> <year> (1983). </year> <title> Algorithms. </title> <publisher> Addison Wesley, </publisher> <address> Redding, MA. </address>
Reference-contexts: I continue this process until the grid is at least 35% full of obstacles. I then use a standard algorithm for connected components <ref> (Sedgewick, 1983, pp. 384-385) </ref> to insure that all of the open spaces in the grid are reachable from any other. To do this, I construct the connected components of the grid, and then remove obstacles in a line between the two closest components until they are connected.
Reference: <author> Sejnowski, T. & Rosenberg, C. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168. </pages>
Reference-contexts: The neural networks in these efforts have as input a window of amino acids consisting of the central amino acid whose secondary structure is being predicted, plus some number of the amino acids before and after it in the sequence <ref> (similar to nettalk networks, Sejnowski & Rosenberg, 1987) </ref>. The output of the network is the secondary structure for the central amino acid. Figure 16 shows the general structure of this type of network; Table 10 presents results from these studies.
Reference: <author> Shastri, L. </author> <year> (1988). </year> <title> A connectionist approach to knowledge representation and limited inference. </title> <journal> Cognitive Science, </journal> <volume> 12 </volume> <pages> 331-392. </pages>
Reference: <author> Shavlik, J. & Maclin, R. </author> <year> (1995). </year> <title> Learning from instruction and experience in competitive situations. In Proceedings of the ML95 Workshop on Agents that Learn from Other Agents, </title> <address> Tahoe City, CA. </address>
Reference-contexts: Preliminary tests I have performed in such situations show that agents receiving advice demonstrate initial gains in performance, but that the opposing agents adjust their performance to counter the teacher's instructions <ref> (Shavlik & Maclin, 1995) </ref>. This suggests that the ability of the teacher to give recommendations continuously is crucial in this type of situation. Such a domain would also be interesting in that the teacher could provide instructions about how learners on the same team can cooperate.
Reference: <author> Shavlik, J., Mooney, R., & Towell, G. </author> <year> (1991). </year> <title> Symbolic and neural learning algorithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 111-143. </pages>
Reference-contexts: particular Q-function, because of the gradient-based nature of neural networks, a connectionist Q-function is not guaranteed to find an optimal policy function a limitation which will become important later. 24 Chapter 3 A First Approach: FSKBANN This chapter describes my initial approach (called FSkbann) to refining a procedural domain theory <ref> (Maclin & Shavlik, 1991) </ref>. FSkbann (for Finite-State kbann) translates domain theories that include generalized finite-state automata - FSA (Hopcroft & Ullman, 1979), into corresponding neural networks. FSkbann then refines the resulting networks using backpropagation (Rumelhart et al., 1986) on a set of training examples.
Reference: <author> Shavlik, J. & Towell, G. </author> <year> (1989). </year> <title> An approach to combining explanation-based and neural learning algorithms. </title> <journal> Connection Science, </journal> <volume> 1 </volume> <pages> 233-255. </pages>
Reference: <author> Siegelmann, H. </author> <year> (1994). </year> <booktitle> Neural programming language. In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 877-882), </pages> <address> Seattle, WA. </address>
Reference: <author> Suddarth, S. & Holden, A. </author> <year> (1991). </year> <title> Symbolic-neural systems and the use of hints for developing complex systems. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 35 </volume> <pages> 291-311. </pages>
Reference: <author> Sun, R. </author> <year> (1992). </year> <title> On variable binding in connectionist networks. </title> <journal> Connection Science, </journal> <volume> 4 </volume> <pages> 93-124. </pages>
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages> <note> 161 Sutton, </note> <author> R. </author> <year> (1991). </year> <title> Reinforcement learning architectures for animats. </title> <editor> In Meyer, J. & Wilson, S., editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The third area I will present is simple recurrent neural networks (Elman, 1990; Jordan, 1989), the specific type of neural network I will use in order to deal with the contextual information in procedural domain theories. Finally, I will describe reinforcement learning <ref> (Sutton, 1988) </ref>, the task I investigate in Chapters 4-7. 2.1 Artificial Neural Networks Neural networks are mathematical constructs based loosely on observations of the behavior of human brain cells. <p> It would then connect this unit to the output unit with a strong link. Secondly, in another approach to reinforcement learning, the agent predicts the utility of a state rather than the utility of an action in a state <ref> (Sutton, 1988) </ref>; in this approach the learner has a model of how its actions change the world. In this approach the agent determines the action to take by checking the utility of the states that are reachable from the current state.
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 257-277. </pages>
Reference-contexts: The feedback provided by the environment offers a crude measure of the quality of the instruction. (One can also envision that in some circumstances such as a game-learner that can play against itself <ref> (Tesauro, 1992) </ref> or an agent that builds an internal world model (Sutton, 1991) it would be straightforward to empirically evaluate the new advice.) The teacher judges the value of her statements similarly (i.e., by watching the learner's post-advice behavior).
Reference: <author> Thrun, S. </author> <year> (1994). </year> <type> Personal communication. </type>
Reference: <author> Thrun, S. & Mitchell, T. </author> <year> (1993). </year> <title> Integrating inductive neural network learning and explanation-based learning. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 930-936), </pages> <address> Chambery, France. </address>
Reference: <author> Towell, G. </author> <year> (1991). </year> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement, and Extraction. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, WI. </institution>
Reference: <author> Towell, G. & Shavlik, J. </author> <year> (1993). </year> <title> Extracting refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 71-101. </pages>
Reference: <author> Towell, G. & Shavlik, J. </author> <year> (1994). </year> <title> Knowledge-based artificial neural networks. </title> <journal> Artificial Intelligence, </journal> <volume> 70 </volume> <pages> 119-165. </pages>
Reference: <author> Towell, G., Shavlik, J., & Noordewier, M. </author> <year> (1990). </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 861-866), </pages> <address> Boston, MA. </address>
Reference-contexts: Both of my approaches for refining procedural domain theories use backpropagation (Rumelhart et al., 1986) on neural networks as their basic learning method and build on work in knowledge-based neural networks <ref> (Towell et al., 1990) </ref>. My preliminary technique (FSkbann) focuses on refining domain theories represented as finite-state automata (Maclin & Shavlik, 1993). Table 2 shows the general input and output behavior of this system. It uses a finite-state domain theory and some training examples to refine the provided domain theory. <p> Table 5 shows the type of task to which FSkbann is applicable domain theories for state-based problem solvers. To refine this type of domain theory I extend the kbann algorithm <ref> (Towell et al., 1990) </ref> to theories that include finite-state automata. The resulting domain theory is a hybrid one consisting of finite-state automata, as well as rules that can make reference to the states of the FSAs. <p> Integrate the reformulated advice into the agent's knowledge base. In ratle I employ a connectionist approach to RL (Anderson, 1987; Barto et al., 1983; Lin, 1992). Hence, to incorporate the teacher's advice, the agent's neural network must be updated. As in FSkbann, I expand on the basic kbann algorithm <ref> (Towell et al., 1990) </ref> to install the advice into the agent. action-choosing network. This network computes a function that maps sensor readings to the utility of actions. Incorporating instructions involves adding new hidden units that represent the instructions to the existing neural network. <p> In the following sections, I define how each of the constructs in the ratle language is 81 translated into corresponding neural-network components. My methods are based on the kbann algorithm <ref> (Towell et al., 1990) </ref> which is extended in the following ways: (1) advice can contain multi-step plans, (2) it can contain loops, (3) it can suggest actions to avoid, (4) it can contain fuzzy conditions, and (5) it can be given more than once. <p> One closely related area of theory refinement is knowledge-based neural networks. My work augments the knowledge-based neural network technique kbann <ref> (Towell et al., 1990) </ref>, to translate a much broader instruction language. My work differs from some techniques for producing knowledge-based networks in that I produce networks that can be refined with standard learning algorithms, rather than requiring new learning methods. <p> The states of the FSA act as the learner's "memory" for information from previous steps of the task. FSkbann inserts the knowledge from the propositional rules and FSAs into a knowledge-based neural network. My approach is based on the kbann technique <ref> (Towell et al., 1990) </ref>, which I extend to represent the states of the FSAs. The states of the teacher's 137 FSAs are represented in FSkbann using a Simple Recurrent neural Network (Elman, 1990; Jordan, 1989).
Reference: <author> Utgoff, P. & Clouse, J. </author> <year> (1991). </year> <title> Two kinds of training information for evaluation function learning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 596-600), </pages> <address> Anaheim, CA. </address>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, Cam-bridge. </institution>
Reference-contexts: Based on the reinforcements from the environment, the task of the agent is to discover a policy that indicates the "best" action to take in each state (see Figure 11). In this work, I employ a particular type of RL called Q-learning. In Q-learning <ref> (Watkins, 1989) </ref> the policy is implemented by an action-choosing module that employs a utility function that maps states and actions to a numeric value (the utility).
Reference: <author> Watson, J. </author> <year> (1990). </year> <title> The Human Genome Project: Past, present, and future. </title> <journal> Science, </journal> <volume> 248 </volume> <pages> 44-48. </pages>
Reference-contexts: Thus this task provides a good baseline for the effectiveness of my approach. The protein-folding task is an open problem that is becoming increasingly critical as the Human Genome Project <ref> (Watson, 1990) </ref> proceeds. Proteins are long strings of amino acids, containing several hundred elements on average. There are 20 naturally occurring amino acids, denoted by different capital letters. The string of amino acids making up a given protein constitutes the primary structure of the protein.
Reference: <author> Whitehead, S. </author> <year> (1991). </year> <title> A complexity analysis of cooperative mechanisms in reinforcement learning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 607-613), </pages> <address> Anaheim, CA. </address>
Reference: <author> Wilson, I., Haft, D., Getzoff, E., Tainer, J., Lerner, R., & Brenner, S. </author> <year> (1985). </year> <title> Identical short peptide sequences in unrelated proteins can have different conformations: A testing ground for theories of immune recognition. </title> <booktitle> Proceedings of the National Academy of Sciences (USA), </booktitle> <volume> 82 </volume> <pages> 5255-5259. </pages>
Reference-contexts: Method Accuracy Comments Chou and Fasman (1978) 58% data from Qian and Sejnowski (1988) Lim (1974) 50% from Nishikawa (1983) Garnier and Robson (1989) 58% data from Qian and Sejnowski (1988) 31 researchers believe that algorithms that take into account only local information can achieve only limited accuracy <ref> (Wilson et al., 1985) </ref>, generally believed to be at most 80-90%. I should note that the results in Table 9 are derived from my work on re-implementing these algorithms, as well as work by Nishikawa (1983).
Reference: <author> Zadeh, L. </author> <year> (1965). </year> <title> Fuzzy sets. </title> <journal> Information and Control, </journal> <volume> 8 </volume> <pages> 338-353. </pages> <note> 162 Zhang, </note> <author> X., Mesirov, J., & Waltz, D. </author> <year> (1992). </year> <title> Hybrid system for protein secondary structure prediction. </title> <journal> Journal of Molecular Biology, </journal> 225:1049-1063. 
Reference-contexts: Statements in the ratle advice language specify conditions that must be met in order for certain actions to be taken. Conditions are logical combinations of input and intermediate terms, as well as fuzzy conditions <ref> (Zadeh, 1965) </ref>. The teacher can use these conditions to define particular states of the world to signal actions the agent should take. Actions include simple actions, action prohibitions, and multi-step plans; they can be performed separately or in loops. <p> any logical combination of the current terms and fuzzy conditions. 5.2.3 Conditions: Fuzzy Conditions Object f is j are g Descriptor [T ype1] Quantifier Object f is j are g Properties [T ype2] Zadeh defines a fuzzy set as "a class of objects with a continuum of grades of membership" <ref> (Zadeh, 1965, pp. 338) </ref>. A fuzzy membership function characterizes the fuzzy set by assigning each object a value in the interval [0; 1] that indicates to what extent that object fits into the class (0 being not at all, 1 being a perfect fit).
References-found: 117

