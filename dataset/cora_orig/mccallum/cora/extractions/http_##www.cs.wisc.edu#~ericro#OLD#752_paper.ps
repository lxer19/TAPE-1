URL: http://www.cs.wisc.edu/~ericro/OLD/752_paper.ps
Refering-URL: http://www.cs.wisc.edu/~ericro/OLD/
Root-URL: 
Title: Level Two Translation Lookaside Buffers  
Author: Mark D. Callaghan Mohammed M. Hoque Eric Rotenberg 
Date: March 22, 1995  
Abstract: The Translation Lookaside Buffer (TLB) is a key component of the memory hierarchy in that it hides the overhead of maintaining virtual memory. However, certain trends in computer design are increasing the demand on the TLB. The reach of current TLBs (amount of memory that the TLB maps) is not sufficient to support this demand. The consequence is that an unacceptable fraction of total execution time will be spent servicing TLB misses. Cycle time and area constraints prohibit the brute-force solution of scaling the size of the TLB with greater demand. We begin with an analysis of the technological and functional trends which are creating the need for greater TLB reach. A timing model is then used to quantitatively examine cycle time constraints on the TLB, which results in a strong case for small, simple level one (L1) TLBs. Simulations are performed to study the effectiveness of level two (L2) TLBs as a means of increasing overall TLB reach. Both software L2 TLBs (located in physical memory) and hardware L2 TLBs (located on the processor chip) are considered. Further, some simulations using partial subblock TLBs are performed for comparison purposes. We claim that the L2 TLB can provide a significant performance improvement even if a partial subblock L1 TLB is used. There are two cases in which a partial subblock L1 TLB may not provide enough reach: high rates of multiprogramming, and memory intensive applications with irregular data access patterns. These cases may cause the performance of the partial subblock L1 TLB to degrade rapidly. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Madhusudhan Talluri, Mark D. Hill. </author> <title> Surpassing the TLB performance of Superpages with Less Operating System Support. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: These two factors work in concert to compound the TLB reach problem. TLBs with superpage support, complete subblocks, or partial subblocks offset the performance penalty due to these trends, since each method improves the ability of the TLB to map a process's address space <ref> [1] </ref>. However, there are other functional factors that increase the number of contexts that a TLB must contain, and solutions aimed at the L1 TLB may be insufficient in these cases. <p> Area cost considerations are described in Section 5. Section 6 describes the performance data that has been generated for L2 TLBs. Finally, Section 7 summarizes the results and discusses additional work that is required to support our hypothesis. 2 Previous Work Talluri and Hill <ref> [1] </ref> take an extensive look at the problem of TLB reach and contrast available alternatives, including increasing the size of the L1 TLB, increasing the page size, complete subblocking, and using superpages. <p> this hashed page table looks like an 8-way set-associative L2 TLB. 3 Methodology This section details the simulation environment used to obtain TLB miss counts, workloads measured, the analytical access time and area models, the derivation of miss penalties, and metrics. 3.1 Simulation Environment A modified operating system kernel, Foxtrot <ref> [1] </ref>, was used to count the number of TLB misses. Foxtrot allows TLBs to be simulated that are larger or smaller than the physical TLB of the host machine by trapping to a user handler on TLB misses. <p> The second option has been extensively studied <ref> [1] </ref>. We include measurements for subblock TLBs to show where such TLBs are successful and where the additional reach provided by a subblock TLB is insufficient. <p> Another memory intensive application was run to test the effectiveness of partial subblocking when less regularly accessed data structures were used. A C++ source file was compiled using 11 SPEC92 floating point benchmarks with fully as sociative TLBs. Benchmark Partial Subblock TLB Misses <ref> [1] </ref> nasa7 9000 wave5 33000 spice 5000 64-entry fully-associative partial subblock 16 L1 TLB. the L1 TLB configuration varies. version 2.6.0 of g++. In this case, large TLBs performed much better than partial subblock TLBs.
Reference: [2] <author> Kavita Bala, M. Frans Kaashoek and William E. Weihl. </author> <title> Software Prefetching and Caching for Translation Lookaside Buffers. </title> <booktitle> In Proceedings of the First Symposium on Operating System Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Several papers within the last two years take this approach with some form of L2 TLB. DeLano et al [4] and Bala et al <ref> [2] </ref> both cached PTEs in a software L2 TLB. On an L1 TLB miss, this L2 TLB is searched first before a page table walk is initiated.
Reference: [3] <author> David Nagle, Richard Uhlig, Tim Stanley, Stuart Sechrest, Trevor Mudge and Richard Brown. </author> <title> Design Tradeoffs for Software-Managed TLBs. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 358-369, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference: [4] <author> Eric DeLano, Will Walker, Jeff Yetter and Mark Forsyth. </author> <title> A High Speed Superscalar PA-RISC Processor. </title> <booktitle> In Proceedings of the IEEE Computer Society International Computer Conference, </booktitle> <year> 1992. </year>
Reference-contexts: Several papers within the last two years take this approach with some form of L2 TLB. DeLano et al <ref> [4] </ref> and Bala et al [2] both cached PTEs in a software L2 TLB. On an L1 TLB miss, this L2 TLB is searched first before a page table walk is initiated.
Reference: [5] <author> Jeffrey D. Gee, Mark D. Hill, Dionisios N. Pnevmatikatos and Alan Jay Smith. </author> <title> Cache Performance of the SPEC92 Benchmark Suite. </title> <journal> IEEE Micro, </journal> <note> To Appear. </note>
Reference-contexts: Instruction counts for the SPEC92 benchmarks were obtained from <ref> [5] </ref>. Note that the equation assumes the L2 TLB and page table are not searched in parallel, although this optimization could be used to hide at least some part of L2 TLB misses (assuming the memory system could handle the simultaneous requests).
Reference: [6] <author> Johannes M. Mulder, Nhon T. Quach and Michael J. Flynn. </author> <title> An Area Model for On-Chip Memories and its Application. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <month> February </month> <year> 1991. </year>
Reference-contexts: However, if we assume that a given design style, organization, or optimization is applied in every circuit, then absolute numbers are not as important as relative numbers. This makes comparisons of access times, for example, possible. 4 3.4 Analytical Area Model An area model <ref> [6] </ref> was used to determine the area cost of L1 TLBs as well as to consider the feasibility of hardware L2 TLBs. Since the area model was developed for caches, slight modifications had to be made so that it could be applied to TLBs (similar to the timing model modifications).
Reference: [7] <author> Tomohisa Wada, Suresh Rajan and Steven A. Przybylski. </author> <title> An Analytical Access Time Model for On-Chip Cache Memories. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <month> August </month> <year> 1992. </year>
Reference: [8] <author> Norman P. Jouppi and Steven J.E. Wilton. </author> <title> Tradeoffs in Two-Level On-Chip Caching. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference: [9] <author> Steven J.E. Wilton and Norman P. Jouppi. </author> <title> An Enhanced Access and Cycle Time Model for On-Chip Caches. </title> <institution> DEC Western Research Lab, </institution> <type> Tech Report 93/5, </type> <month> July </month> <year> 1994. </year>
Reference-contexts: Several files from a research project [27] were compiled using g++ [28]. These files used the template features of C++ [29] and caused the compiler to have a much larger working set than the gcc benchmarks in the SPEC92 integer suite. 3.3 Analytical Timing Model An analytical timing model <ref> [9] </ref> was used to generate all cache and TLB timing data. Slight modifications, having to do primarily with tag sizes, were made to the source code to support TLBs and virtual index caches. <p> Note the segments labelled "precharge". Accessing a memory composed of static RAM cells involves a precharge phase in addition to the actual access. Bitlines, comparators, and other elements of the array must be precharged before a new access can be initiated <ref> [9] </ref>. Figure 3 shows the scenario where the TLB has as much time possible to provide the physical page number to the cache comparator: the cache is precharged at the beginning of each cycle, whereas the TLB precharge is overlapped with logic after the cache comparators.
Reference: [10] <author> Steven A. Przybylski. </author> <title> Cache and Memory Hierarchy Design: a Performance-Directed Approach, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference: [11] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: a Quantitative Approach, Second Edition, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction The TLB reach problem can be attributed to both technological and functional trends in computer systems. The technological trends are: * Raw processor speed has increased by a rate of 50% per year since 1987, while DRAM performance is improving by just 7% per year <ref> [11] </ref>. It is expected that this widening gap will soon rival the Federal Deficit. As a consequence, TLB misses are becoming relatively more expensive. * The current trend in DRAM memory capacity is a four-fold increase every three years [11]. <p> 1987, while DRAM performance is improving by just 7% per year <ref> [11] </ref>. It is expected that this widening gap will soon rival the Federal Deficit. As a consequence, TLB misses are becoming relatively more expensive. * The current trend in DRAM memory capacity is a four-fold increase every three years [11]. <p> The factor of 20 disparity between processor and DRAM speed is typical of current technology (the Alpha 21064 has a 5 ns cycle time, DRAMs have a latency on the order of 100 ns <ref> [11] </ref>). Most of the other assumptions, such as fast bypass, split address and data buses, pipelined memory bus, critical word first, and castout buffers, can be categorized as typical performance optimizations. Item 7 determines the number of bus transfers required for a cache block.
Reference: [12] <author> George Paap, Ed Silha. </author> <title> PowerPC: A Performance Architecture. </title> <booktitle> In Proceedings of the IEEE Computer Society International Computer Conference, </booktitle> <year> 1993. </year>
Reference: [13] <author> Michael S. Allen, Michael C. Becker. </author> <title> Multiprocessing Aspects of the PowerPC 601. </title> <booktitle> In Proceedings of the IEEE Computer Society International Computer Conference, </booktitle> <year> 1993. </year>
Reference: [14] <author> Tom Thompson, Bob Ryan. </author> <title> PowerPC 620 Soars. </title> <journal> BYTE, </journal> <month> November </month> <year> 1994. </year>
Reference-contexts: However, there are two major drawbacks to virtual tags (which may explain the prevalence of physically tagged caches): 1. Virtual tags are considerably larger than physical tags. For example, the PowerPC architecture specifies an 80-bit virtual address space, whereas the PowerPC 620 implementation has a 40-bit address bus <ref> [14] </ref>. The graph in Figure 2 shows the effect of using virtual tags versus using physical tags on cache cycle time, assuming a 64-bit virtual space and a 32-bit physical space.
Reference: [15] <author> John M. Borkenhagen, Glen H. Handlogten, John D. Irish, Sheldon B. Levenstein. </author> <title> AS/400 64-bit PowerPC-Compatible Processor Implementation. </title> <note> Submitted to ICCD. 16 </note>
Reference-contexts: Borkenhagen et al <ref> [15] </ref> describe a commercial processor implementation with a hardware L2 TLB (512 entry, 4-way associative), which has the advantage of only a 2-cycle penalty for an L2 hit. Taylor et al [18] implemented a unique form of L1 TLB called the TLB slice.
Reference: [16] <author> Mark D. Hill. </author> <title> A Case for Direct-Mapped Caches. </title> <booktitle> IEEE Computer. </booktitle> <month> December, </month> <year> 1988. </year>
Reference: [17] <author> J. Bradley Chen, Anita Borg, and Norman P. Jouppi. </author> <title> A Simulation Based Study of TLB Performance. </title> <booktitle> In The 19th Annual International Symposium on Computer Architecture. </booktitle> <month> May, </month> <year> 1992. </year>
Reference: [18] <author> George Taylor, Peter Davies and Michael Farmwald. </author> <title> The TLB Slice A Low-Cost High-Speed Address Translation Mechanism. REFERENCE INCOMPLETE. </title>
Reference-contexts: Borkenhagen et al [15] describe a commercial processor implementation with a hardware L2 TLB (512 entry, 4-way associative), which has the advantage of only a 2-cycle penalty for an L2 hit. Taylor et al <ref> [18] </ref> implemented a unique form of L1 TLB called the TLB slice. The cache in this implementation is physically indexed, so the virtual address must be translated fast. This goal is met with the small TLB slice, containing only 8 entries.
Reference: [19] <author> Jerry Huck and Jim Hays. </author> <title> Architectural Support for Translation Table Management in Large Address Space Machines. </title> <booktitle> In Proceedings of the IEEE Annual Symposium on Computer Architecture, </booktitle> <year> 1993. </year>
Reference-contexts: This goal is met with the small TLB slice, containing only 8 entries. As one might expect, the performance of the TLB slice is poor and is consequently backed by a larger secondary TLB. In a Hashed Page Table organization <ref> [19] </ref>, bits of the virtual address are hashed to index into a table which holds page table entries. This scheme has a significant advantage over hierarchical page tables in terms of the number of memory references required to translate.
Reference: [20] <author> Mike Rozier et al. </author> <title> Overview of the CHORUS distributed operating system. </title> <booktitle> In Proceedings of the USENIX Workshop on Micro-kernels and Other Kernel Architectures. USENIX, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: However, there are other functional factors that increase the number of contexts that a TLB must contain, and solutions aimed at the L1 TLB may be insufficient in these cases. The first functional trend is the distribution of operating systems across several contexts due to the use of microkernels <ref> [20] </ref>. This will increase the minimum number of contexts that a TLB must contain to provide the same level of functionality that a monolithic kernel provides. For example, a file system call can involve several processes (the file system server and the pager) in a microkernel based OS.
Reference: [21] <author> David R. Cheriton. </author> <title> The V distributed system. </title> <journal> In Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference: [22] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, Vaidy Sunderam, </author> <title> PVM 3 User's Guide and Reference Manual, </title> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The third functional trend is the increase in the number of context switches due to communication intensive applications. Inexpensive and portable parallelism is being sought by utilizing workstations as the nodes of parallel machines [23] <ref> [22] </ref>. Increases in communication, the number of user processes that the average user requires, and the number of system processes required by the operating system increase both the rate of context switches and the number of contexts that a TLB must concurrently map.
Reference: [23] <author> K. Castagnera, D. Cheng, R. Fatoohi, E. Hook, B. Kramer, C. Manning, J. Musch, C. Nig-gley, W. Saphir, D. Sheppard, M. Smith, I. Stockdale, S. Welch, R. Williams and D. Yip. </author> <title> NAS Experiences with a Prototype Cluster of Workstations. </title> <booktitle> Supercomputing '94 Proceedings, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: The third functional trend is the increase in the number of context switches due to communication intensive applications. Inexpensive and portable parallelism is being sought by utilizing workstations as the nodes of parallel machines <ref> [23] </ref> [22]. Increases in communication, the number of user processes that the average user requires, and the number of system processes required by the operating system increase both the rate of context switches and the number of contexts that a TLB must concurrently map.
Reference: [24] <author> John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham and Michael J. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-61, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: For example, a file system call can involve several processes (the file system server and the pager) in a microkernel based OS. The second functional trend is the distribution of system services across several machines to increase sharing. File systems are no longer restricted to a single machine <ref> [24] </ref>. In this case, a file system read will invoke a file system process on the local machine, and may also require network communication. The effect is a larger imprint in the TLB from the kernel for basic OS services.
Reference: [25] <institution> SPEC. </institution> <note> SPEC Newsletter, 3(4), </note> <month> December </month> <year> 1991. </year>
Reference-contexts: The L2 TLB is assumed to have significantly more entries in this case, and both TLBs use LRU replacement. Trace based simulation using the first one million references from each of the SPEC92 <ref> [25] </ref> benchmarks confirmed the validity of this assumption. 3 3.2 Workloads The SPEC92 floating point benchmarks were used to study memory system performance. These benchmarks were used because of their acceptance by the benchmarking community.
Reference: [26] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Report RNR-91-002 Revision 2, </type> <institution> Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: Most of the floating point benchmarks did not use large amounts of memory. We feel that this type of behavior is not indicative of all workloads for which performance is important. One application from the NAS benchmark <ref> [26] </ref> suite, fftpde, was also used. This application had the largest working set, 58MB, of all the applications run. A workload more similar to a software development workload was also run. Several files from a research project [27] were compiled using g++ [28].
Reference: [27] <author> Barton P. Miller, Mark D. Callaghan, Jonathan M. Cargille, Jeffrey K. Hollingsworth, R. Bruce Irvin, Karen L. Karavanic, Krishna Kunchithapadam, and Tia Newhall. </author> <title> The Paradyn Parallel Performance Measurement Tools, </title> <note> Submitted for publication. </note>
Reference-contexts: One application from the NAS benchmark [26] suite, fftpde, was also used. This application had the largest working set, 58MB, of all the applications run. A workload more similar to a software development workload was also run. Several files from a research project <ref> [27] </ref> were compiled using g++ [28].
Reference: [28] <author> Online manual and reference for g++. </author> <month> December </month> <year> 1994. </year>
Reference-contexts: One application from the NAS benchmark [26] suite, fftpde, was also used. This application had the largest working set, 58MB, of all the applications run. A workload more similar to a software development workload was also run. Several files from a research project [27] were compiled using g++ <ref> [28] </ref>. These files used the template features of C++ [29] and caused the compiler to have a much larger working set than the gcc benchmarks in the SPEC92 integer suite. 3.3 Analytical Timing Model An analytical timing model [9] was used to generate all cache and TLB timing data.
Reference: [29] <author> Stanley B. Lippman. </author> <title> C++ Primer, Second Edition, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> New York, </address> <year> 1994. </year> <month> 17 </month>
Reference-contexts: This application had the largest working set, 58MB, of all the applications run. A workload more similar to a software development workload was also run. Several files from a research project [27] were compiled using g++ [28]. These files used the template features of C++ <ref> [29] </ref> and caused the compiler to have a much larger working set than the gcc benchmarks in the SPEC92 integer suite. 3.3 Analytical Timing Model An analytical timing model [9] was used to generate all cache and TLB timing data.
References-found: 29

