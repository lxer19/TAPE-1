URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3698/3698.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: keleher@cs.umd.edu tseng@cs.umd.edu  
Title: Enhancing Software DSM for Compiler-Parallelized Applications  
Author: Pete Keleher Chau-Wen Tseng 
Address: College Park, MD 20742  
Affiliation: Dept. of Computer Science University of Maryland  
Abstract: Current parallelizing compilers for message-passing machines only support a limited class of data-parallel applications. One method for eliminating this restriction is to combine powerful shared-memory parallelizing compilers with software distributed-shared-memory (DSM) systems. We demonstrate such a system by combining the SUIF parallelizing compiler and the CVM software DSM. Innovations of the system include compiler-directed techniques that: 1) combine synchronization and parallelism information communication on parallel task invocation, 2) employ customized routines for evaluating reduction operations, and 3) select a hybrid update protocol that pre-sends data by flushing updates at barriers. For applications with sufficient granularity of parallelism, these optimizations yield very good speedups eight processors on an IBM SP-2 and DEC Alpha cluster, usually matching or exceeding the speedup of equivalent HPF and message-passing versions of each program. Based on our experimental results, we point out areas where additional compiler analysis and software DSM improvements can be used to achieve good performance on a broader range of applications.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Anderson, S. Amarasinghe, and M. Lam. </author> <title> Data and computation transformation for multiprocessors. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: To improve spatial locality of local data, the compiler may decide to reindex array references to make local sections of each array contiguous. However, scalar optimizations are required to clean up modulo and division operations inserted into array subscripts <ref> [1] </ref>. Since the compiler is already building a structure for all shared variables, it should also attempt to page align shared data to improve spatial locality at the page level. Packing Nonlocal Data Software DSM systems may waste significant communication bandwidth for nonlocal data accesses with poor spatial locality.
Reference: [2] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In Proceedings of the '93 CompCon Conference, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Bershad et al. maintain coherence by using a compiler to update a software dirty bit on shared-memory accesses <ref> [2] </ref>. Scales et al. designed Shasta, a software-only approach that supports fine-grain coherence through binary rewriting [24]. Using a number of optimizations, Shasta limits software overhead to within 5-35% for the Splash benchmarks on a DEC Alpha cluster.
Reference: [3] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: False Sharing False sharing occurs when two or more processors access different variables within a page, with at least one of the accesses being a write. False sharing is problematic for software DSMs because of the large page-size coherence units. Multiple-writer coherence protocols <ref> [3] </ref> such as that implemented by CVM avoid false sharing by allowing two or more processors to simultaneously modify local copies of the same shared page. These concurrent modifications are merged using diffs to summarize the updates. <p> A disadvantage of the distributed approach is that it requires additional messages. In the centralized approach, all reduction traffic is piggybacked on existing barrier messages. More experiments will be needed. 6 Related Work While there has been a large amount of research on software DSMs <ref> [3, 7, 22] </ref>, we are aware of only a few projects combining compilers and software DSMs. Bershad et al. maintain coherence by using a compiler to update a software dirty bit on shared-memory accesses [2].
Reference: [4] <author> S. Chandra and J.R. Larus. </author> <title> HPF on fine-grain distributed shared memory: Early experience. </title> <booktitle> In Proceedings of the Ninth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> San Jose, CA, </address> <month> August </month> <year> 1996. </year> <month> 13 </month>
Reference-contexts: Compared to their system, the SUIF/CVM interface is targeted towards optimizing compiler-parallelized programs which are less tuned for software DSMs. Finally, Chandra and Larus have preliminary results from a system combining the PGI HPF compiler and the Tempest software DSM system <ref> [4] </ref>. The PGI HPF compiler can generate either message-passing code or shared-memory code relying on Tempest. Preliminary results on a network of workstations connected by Myrinet indicates shared-memory versions of dense matrix programs achieve performance close to the message-passing codes generated. <p> However, since CVM supports multiple writers, the main performance advantage is in avoiding page faults traps for shared data. Large units of coherence can exploit spatial locality, so the PGI compiler can actually improve performance by using larger coherence units <ref> [4] </ref>. In comparison to PGI/Tempest, we implement and evaluate enhancements to the software DSM to improve performance.
Reference: [5] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 462-479, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Results show that with suitable extensions to the coherence protocol, the shared-memory program was able to match the performance of the optimized message-passing program utilizing Chaos <ref> [5] </ref>. The authors point out that a compiler like SUIF can take advantage of the extensible coherence protocol to improve performance. Compared with their approach, we use a single general coherence protocol in the CVM 12 for all applications, exploiting compile-time analysis to provide hints to the software DSM.
Reference: [6] <author> S. Dwarkadas, A. Cox, and W. Zwaenepoel. </author> <title> An integrated compile-time/run-time software distributed shared memory system. </title> <booktitle> In Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VIII), </booktitle> <address> Boston, MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Their proposal is similar to portions of our SUIF/CVM interface; we differ in requiring less analysis and by providing a more fine-grained API to control the behavior of individual pages. Dwarkadas et al. recently described applying compiler analysis to explicitly parallel programs to improve the performance of software DSM <ref> [6] </ref>. By combining analysis in the ParaScope programming environment with TreadMarks, they were able to compute data access patterns at compile time and use it to help the runtime system aggregate communication and synchronization. Results for five programs were within 9% of equivalent HPF programs on the IBM SP-2.
Reference: [7] <author> S. Dwarkadas, P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 244-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Processor q then piggybacks on the release-acquire message to p write notices for all intervals named in q's current vector timestamp but not in the vector timestamp it received from p. Experiments show alternative implementations of release consistency generally cause more communication than LRC <ref> [7] </ref>. False Sharing False sharing occurs when two or more processors access different variables within a page, with at least one of the accesses being a write. False sharing is problematic for software DSMs because of the large page-size coherence units. <p> A disadvantage of the distributed approach is that it requires additional messages. In the centralized approach, all reduction traffic is piggybacked on existing barrier messages. More experiments will be needed. 6 Related Work While there has been a large amount of research on software DSMs <ref> [3, 7, 22] </ref>, we are aware of only a few projects combining compilers and software DSMs. Bershad et al. maintain coherence by using a compiler to update a software dirty bit on shared-memory accesses [2].
Reference: [8] <author> Andrew Erlichson, Neal Nuckolls, Greg Chesson, and John Hennessy. SoftFLASH: </author> <title> Analyzing the performance of clustered distributed virtual shared memory. </title> <booktitle> In Proceedings of the 7th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Using more than one processor per node resulted in up to 15% performance degradation relative to a system containing the same number of processors, but only a single processor per node. This degradation arises primarily from contention at the network interface, and has been documented on similar systems elsewhere <ref> [8, 25] </ref>. On the DEC cluster, CVM processes communicate via unreliable UDP sockets over the ATM switch. Simple RPCs take 160 sec, and eight-processor barriers take a minimum of 1836 secs.
Reference: [9] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In order to simplify the comparison process, however, we do not use either of these techniques in this study. Memory Consistency - CVM's primary protocol implements a multiple-writer version of lazy release consistency [16], which is a derivation of release consistency (RC) <ref> [9] </ref>. Release consistency a processor to delay making modifications to shared data visible to other processors until special acquire or release synchronization accesses occur. The propagation of modifications can thus be postponed until the next synchronization operation takes effect. <p> Programs produce the same results for the two memory models provided that (i) all synchronization operations use system-supplied primitives, and (ii) there is a release-acquire pair between conflicting ordinary accesses to the same memory location on different processors <ref> [9] </ref>. In practice, most shared-memory programs require little or no modifications to meet these requirements. Lazy release consistency (LRC) allows the propagation of modifications to be further postponed until the time of the next subsequent acquire of a released synchronization variable.
Reference: [10] <author> E. Granston and H. Wishoff. </author> <title> Managing pages in shared virtual memory systems: Getting the compiler into the game. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Previous researchers have examined shared-memory compilation issues such as improving locality [19] and reducing false sharing [26], but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs <ref> [10] </ref>. These include tiling loop iterations so computation is on partitioned matching page boundaries, aligning arrays to pages, and inserting hints to use weak coherence. No implementation or experiments are provided.
Reference: [11] <author> M. Gupta, S. Midkiff, E. Schonberg, V. Seshadri, D. Shields, K.-Y. Wang, W.-M. Ching, and T. Ngo. </author> <title> An HPF compiler for the IBM SP2. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: High Performance Fortran (HPF) applications were created by manually translating from Fortran to Fortran 90, with HPF data decompositions added for each array. On the IBM SP-2 we used the IBM HPF compiler <ref> [11] </ref> with the -O2 flag. On the DEC cluster we used the DEC HPF compiler f90 version 2.0-1 with the -O2 -wsf -fast flags. Message-passing versions of each program were created using calls to communication routines specified under Message Passing Interface (MPI).
Reference: [12] <author> M. Hall, S. Amarasinghe, B. Murphy, S. Liao, and M. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: We believe that the combination of shared-memory parallelizing compilers and sophisticated runtime systems presents one of the most promising approaches towards addressing this key problem. This paper presents our experience using the CVM [15] software distributed shared memory (DSM) system as a compilation target for the SUIF <ref> [12] </ref> shared-memory compiler. SUIF automatically parallelizes sequential applications and allows users to benefit from sophisticated program analysis. The use of CVM as a compilation target hides the details of the underlying message-passing architecture and allows the compiler-generated code to assume shared memory semantics. <p> When all necessary diffs have been received, they are applied to the page in increasing timestamp order. 3 Compiler/Software DSM Interface Our system consists of the Stanford SUIF parallelizing compiler <ref> [12] </ref> and the CVM software DSM system [15]. A simple interface was produced by porting the SUIF run-time system to the CVM API. <p> The large number of customized coherence protocols they used for each application does not appear to be necessary for compiler-parallelized applications. The SUIF compiler draws on a large body of work on techniques for identifying parallelism <ref> [12] </ref>. Previous researchers have examined shared-memory compilation issues such as improving locality [19] and reducing false sharing [26], but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [10].
Reference: [13] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: On the IBM SP-2 we used the MPL version 2 implementation of MPI; on the DEC Alpha cluster we used the MPICH version 1.0.12 implementation of MPI. MPI versions of ADI, DOT, EXPL, and JACOBI were generated using the Fortran D compiler <ref> [13] </ref>. Previous experiments show the resulting programs achieve performance close to optimized hand-written message-passing programs [14]. MPI versions of MULT and RB were created by hand. These programs represent message-passing programs written with a reasonable amount of effort, not programs highly customized for performance. <p> The compiler must first apply communication analysis to detect nonlocal accesses. If the nonlocal data is not contiguous, then the compiler must insert code to copy the data to contiguous buffers (one for each processor). The placement copy code can be determined by data dependences using message vectorization <ref> [13] </ref>. The compiler must also modify the code so data so nonlocal accesses are made to the buffers. Message Library Support Part of the problem is the underlying communication mechanism. The numbers in this paper reflect using UDP sockets as a communication substrate.
Reference: [14] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Preliminary experiences with the Fortran D compiler. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: MPI versions of ADI, DOT, EXPL, and JACOBI were generated using the Fortran D compiler [13]. Previous experiments show the resulting programs achieve performance close to optimized hand-written message-passing programs <ref> [14] </ref>. MPI versions of MULT and RB were created by hand. These programs represent message-passing programs written with a reasonable amount of effort, not programs highly customized for performance.
Reference: [15] <author> P. Keleher. </author> <title> The relative importance of concurrent writers and weak consistency models. </title> <booktitle> In 16th International Conference on Distributed Computing Systems, </booktitle> <address> Hong Kong, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: We believe that the combination of shared-memory parallelizing compilers and sophisticated runtime systems presents one of the most promising approaches towards addressing this key problem. This paper presents our experience using the CVM <ref> [15] </ref> software distributed shared memory (DSM) system as a compilation target for the SUIF [12] shared-memory compiler. SUIF automatically parallelizes sequential applications and allows users to benefit from sophisticated program analysis. <p> When all necessary diffs have been received, they are applied to the page in increasing timestamp order. 3 Compiler/Software DSM Interface Our system consists of the Stanford SUIF parallelizing compiler [12] and the CVM software DSM system <ref> [15] </ref>. A simple interface was produced by porting the SUIF run-time system to the CVM API.
Reference: [16] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: We therefore expect the performance of the fully functional system to improve over the existing base. In order to simplify the comparison process, however, we do not use either of these techniques in this study. Memory Consistency - CVM's primary protocol implements a multiple-writer version of lazy release consistency <ref> [16] </ref>, which is a derivation of release consistency (RC) [9]. Release consistency a processor to delay making modifications to shared data visible to other processors until special acquire or release synchronization accesses occur. The propagation of modifications can thus be postponed until the next synchronization operation takes effect.
Reference: [17] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Like commercially available systems such as TreadMarks <ref> [17] </ref>, CVM is written entirely as a user-level library and runs on most UNIX-like systems. Unlike TreadMarks, CVM was created specifically as a platform for protocol experimentation. The system is written in C++, and opaque interfaces are strictly enforced between different functional units of the system whenever possible.
Reference: [18] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Increasingly powerful processor and network architectures make so-called meta-computers (loosely-coupled computers communicating via messages) a tempting platform on which to run large parallel and distributed applications. Unfortunately, writing efficient message-passing programs is difficult, error-prone, and tedious, and data-parallel languages such as High Performance Fortran (HPF) <ref> [18] </ref> may prove overly restrictive. We believe that the combination of shared-memory parallelizing compilers and sophisticated runtime systems presents one of the most promising approaches towards addressing this key problem.
Reference: [19] <author> E. Markatos and T. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The large number of customized coherence protocols they used for each application does not appear to be necessary for compiler-parallelized applications. The SUIF compiler draws on a large body of work on techniques for identifying parallelism [12]. Previous researchers have examined shared-memory compilation issues such as improving locality <ref> [19] </ref> and reducing false sharing [26], but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [10].
Reference: [20] <author> R. Mirchandaney, S. Hiranandani, and A. Sethi. </author> <title> Improving the performance of DSM systems via compiler involvement. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <address> Washington, DC, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: No implementation or experiments are provided. CVM uses a multi-writer release consistency protocol, so these optimizations are not as vital as for a sequentially-consistent single-writer protocol. Mirchandaney et al. described the design of a compiler for TreadMarks, a software DSM <ref> [20] </ref>. They propose section locks and broadcast barriers to guide eager updates of data, integrating send, recv and broadcast operations with the software DSM, and reductions based on multiple-writer protocols.
Reference: [21] <author> S. Mukherjee, S. Sharma, M. Hill, J. Larus, A. Rogers, and J. Saltz. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Results, however, show that this is not a problem since the software communication overhead usually dominates the memory management overhead. Mukherjee et al. compared the performance of explicit message-passing programs with shared-memory programs <ref> [21] </ref> on Typhoon, a Flexible-Shared-Memory machine implemented on top of a CM-5 [23]. Results show that with suitable extensions to the coherence protocol, the shared-memory program was able to match the performance of the optimized message-passing program utilizing Chaos [5].
Reference: [22] <author> B. Nitzberg and V. Lo. </author> <title> Distributed shared memory: A survey of issues and algorithms. </title> <journal> IEEE Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: A disadvantage of the distributed approach is that it requires additional messages. In the centralized approach, all reduction traffic is piggybacked on existing barrier messages. More experiments will be needed. 6 Related Work While there has been a large amount of research on software DSMs <ref> [3, 7, 22] </ref>, we are aware of only a few projects combining compilers and software DSMs. Bershad et al. maintain coherence by using a compiler to update a software dirty bit on shared-memory accesses [2].
Reference: [23] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21th International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Results, however, show that this is not a problem since the software communication overhead usually dominates the memory management overhead. Mukherjee et al. compared the performance of explicit message-passing programs with shared-memory programs [21] on Typhoon, a Flexible-Shared-Memory machine implemented on top of a CM-5 <ref> [23] </ref>. Results show that with suitable extensions to the coherence protocol, the shared-memory program was able to match the performance of the optimized message-passing program utilizing Chaos [5]. The authors point out that a compiler like SUIF can take advantage of the extensible coherence protocol to improve performance. <p> Tempest is significantly more efficient than message-passing for programs with irregular access patterns not analyzed at compile time. Unlike CVM, Tempest provides fine-grain access control for units smaller than a page <ref> [23] </ref>. However, since CVM supports multiple writers, the main performance advantage is in avoiding page faults traps for shared data. Large units of coherence can exploit spatial locality, so the PGI compiler can actually improve performance by using larger coherence units [4].
Reference: [24] <author> D. Scales, K. Gharachorloo, and C. Thekkath. </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grained shared memory. </title> <booktitle> In Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VIII), </booktitle> <address> Boston, MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Bershad et al. maintain coherence by using a compiler to update a software dirty bit on shared-memory accesses [2]. Scales et al. designed Shasta, a software-only approach that supports fine-grain coherence through binary rewriting <ref> [24] </ref>. Using a number of optimizations, Shasta limits software overhead to within 5-35% for the Splash benchmarks on a DEC Alpha cluster. In comparison, CVM, like most software DSMs, relies on the virtual memory system to detect shared memory updates.
Reference: [25] <author> Per Stenstrom. </author> <title> MP nodes don't need dedicated protocol processors. </title> <type> DRAFT: </type> <note> submitted for publication, </note> <month> October </month> <year> 1995. </year>
Reference-contexts: Using more than one processor per node resulted in up to 15% performance degradation relative to a system containing the same number of processors, but only a single processor per node. This degradation arises primarily from contention at the network interface, and has been documented on similar systems elsewhere <ref> [8, 25] </ref>. On the DEC cluster, CVM processes communicate via unreliable UDP sockets over the ATM switch. Simple RPCs take 160 sec, and eight-processor barriers take a minimum of 1836 secs.
Reference: [26] <author> J. Torrellas, M. Lam, and J. Hennessy. </author> <title> False sharing and spatial locality in multiprocessor caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 43(6) </volume> <pages> 651-663, </pages> <month> June </month> <year> 1994. </year> <month> 14 </month>
Reference-contexts: The SUIF compiler draws on a large body of work on techniques for identifying parallelism [12]. Previous researchers have examined shared-memory compilation issues such as improving locality [19] and reducing false sharing <ref> [26] </ref>, but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [10]. These include tiling loop iterations so computation is on partitioned matching page boundaries, aligning arrays to pages, and inserting hints to use weak coherence.
References-found: 26

