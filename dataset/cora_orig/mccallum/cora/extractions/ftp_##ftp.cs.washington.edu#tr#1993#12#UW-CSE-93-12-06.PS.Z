URL: ftp://ftp.cs.washington.edu/tr/1993/12/UW-CSE-93-12-06.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: Performance of User-Level Communication on Distributed-Memory Multiprocessors with an Optimistic Protocol  
Author: J. William Lee 
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Technical Report 93-12-06 December 1993 
Abstract-found: 1
Intro-found: 1
Reference: [Ananda et al. 91] <author> A. L. Ananda, B. H. Tay, and E. K. Koh. </author> <title> ASTRA An Asynchronous Remote Procedure Call Facility. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 172-179, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Compared to the request-reply model used in RPC systems, this approach exposes more power <ref> [Lampson 83, Walker et al. 90, Ananda et al. 91] </ref>. * The system maintains the order of messages. Maintaining the order of messages makes it easier to reason about correctness in a distributed application. * In each process, there is a single server thread processing incoming messages one after another.
Reference: [Anderson et al. 91] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> The Interaction of Architecture and Operating System Design. </title> <booktitle> In Proceedings of the 4th International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <year> 1991. </year>
Reference-contexts: Thus processors are likely to have enough buffer space to receive messages. The motivation behind the second idea is that traps, upcalls, and software interrupts are inherently expensive <ref> [Anderson et al. 91, Anderson et al. 92] </ref>. The idea of letting kernel and user communicate asynchronously through shared memory is not new: Marsh et al. used this technique to integrate kernel support with a user-level thread package on NUMA shared memory multiprocessors [Marsh et al. 91]. <p> Second, and more importantly, it can be costly to cross protection boundaries. Conventional communication software crosses protection boundaries through traps, upcalls, or software interrupts. These operations may save registers and flush cache and TLBs; they are inherently expensive <ref> [Anderson et al. 91, Anderson et al. 92, Felten 92a] </ref>. The protection boundaries problem described above exists only on machines that prohibit applications from accessing the network interface directly.
Reference: [Anderson et al. 92] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Thus processors are likely to have enough buffer space to receive messages. The motivation behind the second idea is that traps, upcalls, and software interrupts are inherently expensive <ref> [Anderson et al. 91, Anderson et al. 92] </ref>. The idea of letting kernel and user communicate asynchronously through shared memory is not new: Marsh et al. used this technique to integrate kernel support with a user-level thread package on NUMA shared memory multiprocessors [Marsh et al. 91]. <p> Second, and more importantly, it can be costly to cross protection boundaries. Conventional communication software crosses protection boundaries through traps, upcalls, or software interrupts. These operations may save registers and flush cache and TLBs; they are inherently expensive <ref> [Anderson et al. 91, Anderson et al. 92, Felten 92a] </ref>. The protection boundaries problem described above exists only on machines that prohibit applications from accessing the network interface directly. <p> These applications do not require a general naming and binding facility such as those found in RPC systems. The second reason is performance. User-level threads provide a way to manage parallelism with inexpensive primitives <ref> [Anderson et al. 92] </ref>. User-level threads also provide a way to hide communication latency by allowing the communication of one thread to overlap with the computation of other threads [Felten 92b]. <p> Our lazy scheme is in contrast to the eager recovery schemes used in Scheduler Activations, which always give the interrupted thread a chance to exit critical sections at the time of the upcall <ref> [Anderson et al. 92, Barton-Davis et al. 93] </ref>. Such an eager scheme is necessary on shared-memory multiprocessors, because if a thread is preempted in a critical section, it may impede the progress of other threads on other processors. This problem, however, does not exist on uniprocessors.
Reference: [Barton-Davis et al. 93] <author> Paul Barton-Davis, Dylan McNamee, Raj Vaswani, and Edward D. La zowska. </author> <title> Adding Scheduler Activations to Mach 3.0. </title> <booktitle> In The USENIX Mach III Symposium Proceedings, </booktitle> <address> Santa Fe, New Mexico, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Our lazy scheme is in contrast to the eager recovery schemes used in Scheduler Activations, which always give the interrupted thread a chance to exit critical sections at the time of the upcall <ref> [Anderson et al. 92, Barton-Davis et al. 93] </ref>. Such an eager scheme is necessary on shared-memory multiprocessors, because if a thread is preempted in a critical section, it may impede the progress of other threads on other processors. This problem, however, does not exist on uniprocessors.
Reference: [Bershad et al. 91] <author> Brian N. Bershad, Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> User-Level Interprocess Communication for Shared Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(2), </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: Although our approach shares similar philosophy in protocol design, our techniques and our targeted network environment are very different. Our optimistic protocol is designed to speed up transferring medium messages across reliable networks, and the protocol attempts to avoid sending protocol messages by making optimistic assumptions. URPC <ref> [Bershad et al. 91] </ref> is an RPC system for shared-memory multiprocessors. URPC lets two address spaces on the same machine communicate through a short cut | shared memory regions pairwise mapped into both address spaces. In URPC, each address space polls incoming RPC calls.
Reference: [Bershad et al. 93] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Mid way Distributed Shared Memory System. </title> <booktitle> In Proceedings of the 38th IEEE Computer Society International Conference (COMPCON 93), </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year> <month> 17 </month>
Reference-contexts: Due to this delay of processing incoming messages, the processor that issued the call may idle for a long time waiting for the reply. This is especially a problem in distributed shared memory systems <ref> [Chase et al. 89, Bershad et al. 93] </ref>, in which a request may hop across several processors through a chain of "best guesses" before it fetches some remote data.
Reference: [Brustoloni & Bershad 93] <author> Jose C. Brustoloni and Brian N. Bershad. </author> <title> Simple Protocol Processing for High-Bandwidth Low-Latency Networking. </title> <type> Technical Report CMU-CS-93-132, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: The challenge in buffer management is to reduce overhead and avoid copying messages. To reduce message copying, a communication system may map message buffers into both the kernel and the user-level address space <ref> [Schroeder & Burrows 90, Brustoloni & Bershad 93] </ref>. The challenge of flow control is to deliver messages between user-level processes reliably with a small cost. The cost of flow-control typically includes extra computation, as well as extra protocol messages.
Reference: [Carter & Zwaenepoel 89] <author> John B. Carter and Willy Zwaenepoel. </author> <title> Optimistic Implementation of Bulk Data Transfer Protocols. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 61-69, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: In Express Messages, message handlers execute while interrupts are enabled; message handlers can block; and the flow control in Express Messages frees the programmer from manually buffering and re-sending messages in order to avoid deadlock. Optimistic implementation of bulk data transfer protocols <ref> [Carter & Zwaenepoel 89] </ref> is a technique to speed up transferring bulk data across a packet-based network. Using this technique, a bulk data transfer protocol optimistically predicts that, during receipt of bulk data, the next packet that comes from the network is the next packet of the bulk data transfer.
Reference: [Chase et al. 89] <author> Jeffrey S. Chase, Franz Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Due to this delay of processing incoming messages, the processor that issued the call may idle for a long time waiting for the reply. This is especially a problem in distributed shared memory systems <ref> [Chase et al. 89, Bershad et al. 93] </ref>, in which a request may hop across several processors through a chain of "best guesses" before it fetches some remote data.
Reference: [Felten 92a] <author> Edward W. Felten. </author> <title> The Case for Application-Specific Communication Protocols. </title> <booktitle> In Proceedings of Intel Supercomputer Systems Division Technology Focus Conference, </booktitle> <pages> pages 171-181, </pages> <year> 1992. </year>
Reference-contexts: CCR-8907666, CDA-9123308, and CCR-9200832), the Washington Technology Center, Digital Equipment Corporation, Boeing Computer Services, Intel Corporation, Hewlett-Packard Corporation, and Apple Computer. 1 the user-level application requires 67 microseconds. This communication gap <ref> [Felten 92a] </ref> is caused by the communication software, which typically performs flow control, manages message buffers, crosses protection boundaries, and ensures protection. In this paper, we propose two new ideas to reduce this communication gap. <p> Section 7 concludes. 2 Sources of the Communication Gap In this section, we motivate our approach by describing the sources of software overhead in conventional communication systems on distributed-memory multiprocessors. We can attribute the software overhead to two main reasons: protocol processing and protection boundaries <ref> [Felten 92a] </ref>. 2.1 Protocol Processing Overhead Protocol processing includes two tasks: buffer management and flow control. The challenge in buffer management is to reduce overhead and avoid copying messages. <p> Second, and more importantly, it can be costly to cross protection boundaries. Conventional communication software crosses protection boundaries through traps, upcalls, or software interrupts. These operations may save registers and flush cache and TLBs; they are inherently expensive <ref> [Anderson et al. 91, Anderson et al. 92, Felten 92a] </ref>. The protection boundaries problem described above exists only on machines that prohibit applications from accessing the network interface directly. <p> The protection boundaries problem described above exists only on machines that prohibit applications from accessing the network interface directly. By mapping the network interface into the user level, Thinking Machines' CM-5 eliminates the need to cross protection boundaries in communication systems <ref> [TMC 91, Felten 92a] </ref>.
Reference: [Felten 93] <author> Edward W. Felten. </author> <title> Protocol Compilation: High-Performance Communication for Par allel Programs. </title> <type> PhD thesis, </type> <institution> Department of Computer Science & Engineering, University of Washington, </institution> <type> Technical Report 93-09-09, </type> <year> 1993. </year>
Reference-contexts: Below we compare our work with four other systems that either have similar goals or use similar techniques. Protocol compilation <ref> [Felten 93] </ref> is a technique in which a compiler generates tailored protocols to reduce the cost of protocol processing.
Reference: [Felten 92b] <author> Edward W. Felten and Dylan McNamee. </author> <title> Improving the Performance of Message Passing Applications by Multithreading. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-92), </booktitle> <address> Williamsburg, VA, </address> <year> 1992. </year>
Reference-contexts: The second reason is performance. User-level threads provide a way to manage parallelism with inexpensive primitives [Anderson et al. 92]. User-level threads also provide a way to hide communication latency by allowing the communication of one thread to overlap with the computation of other threads <ref> [Felten 92b] </ref>. Many characteristics of Express Messages are motivated by our desire to support the implementation of efficient distributed applications, especially distributed shared memory systems: * Express Messages invokes message handlers automatically, and message handlers may block.
Reference: [Lampson 83] <author> Butler W. Lampson. </author> <title> Hints for Computer System Design. </title> <booktitle> In Proceedings of the 9th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 33-48, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: Compared to the request-reply model used in RPC systems, this approach exposes more power <ref> [Lampson 83, Walker et al. 90, Ananda et al. 91] </ref>. * The system maintains the order of messages. Maintaining the order of messages makes it easier to reason about correctness in a distributed application. * In each process, there is a single server thread processing incoming messages one after another.
Reference: [Lee 93] <author> J. William Lee. </author> <title> Concord: Re-Thinking the Division of Labor in a Distributed Shared Mem ory System. </title> <institution> Dept. of Computer Science & Engineering, University of Washington. </institution> <note> Submitted for Publication, </note> <month> November </month> <year> 1993. </year>
Reference-contexts: Concord attempts to provide a practical DSM system for real applications through a careful division of responsibilities among the programmer, the compiler, and the runtime system <ref> [Lee 93] </ref>. Currently, the same machine-independent code of Concord has been implemented on top of three message passing platforms: Unix sockets on a network of workstations, the iPSC/2 message passing library on an iPSC/2, and Express Messages on the iPSC/2.
Reference: [Marsh et al. 91] <author> Brian D. Marsh, Michael L. Scott, Thomas J. LeBlanc, and Evangelos P. Markatos. </author> <title> First-Class User-Level Threads. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 110-121, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The idea of letting kernel and user communicate asynchronously through shared memory is not new: Marsh et al. used this technique to integrate kernel support with a user-level thread package on NUMA shared memory multiprocessors <ref> [Marsh et al. 91] </ref>. However, we believe we are the first to propose the extensive use of shared data structures in communication systems on distributed-memory multiprocessors. Our approach focuses on how a communication system may cross protection boundaries asynchronously instead of synchronously, and still maintain the same message passing semantics. <p> other processors be reasonably optimistic. 3.2 Crossing the Protection Boundaries through Kernel/User Shared Data Structures Crossing the protection boundaries through traps, upcalls, or software interrupts is expensive, but there is a short cut: the kernel and the address space may communicate asynchronously through data structures mapped into both address spaces <ref> [Marsh et al. 91] </ref>. The difficulty, however, is to 6 maintain the same message passing semantics when the kernel and the address space communicate asynchronously instead of synchronously. To illustrate, consider a parallel program running on top of an RPC system implemented at the user level.
Reference: [Pierce 88] <author> Paul Pierce. </author> <title> The NX/2 Operating System. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <volume> vol. 1, </volume> <pages> pages 384-390, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: The processor retransmits messages failed due to buffer overflow using a protocol that takes advantage of the reliable network. This optimistic protocol is in contrast to conservative protocols, in which each processor sends messages only after it ensures that the receiver has enough buffer space <ref> [Pierce 88] </ref>. * Crossing the protection boundaries in a communication system through a short cut | shared kernel/user data structures. <p> The cost of flow-control typically includes extra computation, as well as extra protocol messages. For example, the Intel NX/2 operating system, which runs on many of the Intel multi-computers with reliable networks, uses a conservative flow control scheme <ref> [Pierce 88] </ref>: each node sends a message only after it ensures that the destination node has enough buffer space to receive the message. Specifically, NX/2 partitions messages into short and long messages. Each node reserves some short message slots for each other node. <p> We choose a single server thread for simplicity and performance. 4.2 Implementation As mentioned earlier, we implemented Express Messages on an Intel iPSC/2, which runs the native Intel NX/2 operating system and incorporates a reliable hypercube network <ref> [Pierce 88] </ref>. The DMA controller of the network can perform scattered-gather. The NX/2 operating system provides virtual memory, but it does not provide paging. We modified the NX/2 kernel and implemented a user-level thread package.
Reference: [Schroeder & Burrows 90] <author> Michael D. Schroeder and Michael Burrows. </author> <title> Performance of Firefly RPC. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 1-17, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: The challenge in buffer management is to reduce overhead and avoid copying messages. To reduce message copying, a communication system may map message buffers into both the kernel and the user-level address space <ref> [Schroeder & Burrows 90, Brustoloni & Bershad 93] </ref>. The challenge of flow control is to deliver messages between user-level processes reliably with a small cost. The cost of flow-control typically includes extra computation, as well as extra protocol messages.
Reference: [Singh et al. 92] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In this section, we evaluate the effectiveness of Express Messages through two shared memory programs that run on top of Concord. The first application is Red/Black SOR. Red/Black SOR solves the two dimensional Laplace equation using the successive over-relaxation method. The second application is LocusRoute from the SPLASH benchmark <ref> [Singh et al. 92] </ref>. LocusRoute is a commercial quality VLSI standard cell router. The original LocusRoute code consists of 6400 lines of C code. We first compare the performance of the two programs running on top of Express Messages and the iPSC/2 message passing library.
Reference: [TMC 91] <institution> Thinking Machines Corporation. </institution> <type> CM-5 Technical Summary, </type> <year> 1991. </year>
Reference-contexts: The protection boundaries problem described above exists only on machines that prohibit applications from accessing the network interface directly. By mapping the network interface into the user level, Thinking Machines' CM-5 eliminates the need to cross protection boundaries in communication systems <ref> [TMC 91, Felten 92a] </ref>.
Reference: [von Eicken et al. 92] <author> Thorsten von Eicken, David E. Culler, Seth Copen Coldstein, and Klaus Erik Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Our approach also reduces the copying of messages, though we require the programmer to allocate message buffers from dedicated memory regions. Active Messages <ref> [von Eicken et al. 92] </ref> is a communication system that attempts to reduce the cost of protocol processing using a minimum protocol approach. Because Active Messages invokes message handlers while interrupts are masked off, it does not need a flow-control scheme, and it requires only two message buffers.
Reference: [Walker et al. 90] <author> Edward F. Walker, Richard Floyd, and Paul Neves. </author> <title> Asynchronous Remote Op eration Execution in Distributed Systems. </title> <booktitle> In Proceedings of the 10th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 172-179, </pages> <month> May </month> <year> 1990. </year> <month> 19 </month>
Reference-contexts: Compared to the request-reply model used in RPC systems, this approach exposes more power <ref> [Lampson 83, Walker et al. 90, Ananda et al. 91] </ref>. * The system maintains the order of messages. Maintaining the order of messages makes it easier to reason about correctness in a distributed application. * In each process, there is a single server thread processing incoming messages one after another.
References-found: 21

