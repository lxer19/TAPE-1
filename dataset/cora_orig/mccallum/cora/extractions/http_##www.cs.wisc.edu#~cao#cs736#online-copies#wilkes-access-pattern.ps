URL: http://www.cs.wisc.edu/~cao/cs736/online-copies/wilkes-access-pattern.ps
Refering-URL: http://www.cs.wisc.edu/~cao/cs736/online-copies/
Root-URL: http://www.cs.wisc.edu
Title: HP  UNIX disk access patterns  
Author: Laboratories Chris Ruemmler and John Wilkes 
Note: Also published in USENIX Winter 1993 Technical Conference Proceedings  Copyright Hewlett-Packard Company 1992. All rights reserved.  
Date: December 1992  2529 January 1993, pages 405420.  
Address: Palo Alto, CA  (San Diego, CA),  
Affiliation: Computer Systems Laboratory Hewlett-Packard Laboratories,  
Pubnum: Technical Report  HPL92152,  
Abstract-found: 0
Intro-found: 1
Reference: [Baker91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout. </author> <title> Measurements of a distributed file system. </title> <booktitle> Proceedings of 13th ACM Symposium on Operating Systems Principles (Asilomar, </booktitle> <address> Pacific Grove, CA). </address> <note> Published as Operating Systems Review 25(5):198212, </note> <month> 1316 October </month> <year> 1991. </year>
Reference-contexts: Many were taken on non-UNIX systems. For example: IBM mainframes [Procar82, Smith85, Kure88, Staelin88, Staelin90, Staelin91, Bozman91]; Cray supercomputers [Miller91]; Sprite (with no timing data on individual I/Os) <ref> [Baker91] </ref>; DEC VMS [Ramakrishnan92]; TOPS-10 (static analysis only) [Satyanarayanan81]. The UNIX buffer cache means that most accesses never reach the disk, so these studies are not very good models of what happens at the disk. <p> On cello, 10% of the writes occur in bursts of more than 100 requests. I/O placement Previous studies have shown that I/Os at the file system level are highly sequential <ref> [Ousterhout85, Baker91] </ref>. But our data (plotted in Figure 9) shows that by the time these requests reach the disk they are much less so. We define requests to be logically sequential if they are at adjacent disk addresses or disk addresses spaced by the file system interleave factor.
Reference: [Bartlett88] <author> Debra S. Bartlett and Joel D. Tesler. </author> <title> A discless HP-UX file system. </title> <journal> Hewlett-Packard Journal 39(5):1014, </journal> <month> October </month> <year> 1988. </year>
Reference-contexts: The other partitions vary, with the mean being 46% writes. Because of the large activity directed to the news partitions, the system as a whole does more writes (56%) than reads. Snake acted as a file server for an HP-UX cluster <ref> [Bartlett88] </ref> of nine clients at the University of California, Berkeley. Each client was an Hewlett-Packard 9000/720 workstation with 24MB of main memory, 66MB of local swap space, and a 4MB file buffer cache.
Reference: [Bozman91] <author> G. P. Bozman, H. H. Ghannad, and E. D. Weinberger. </author> <title> A trace-driven study of CMS file references. </title> <journal> IBM Journal of Research and Development 35(5/6):81528, </journal> <volume> Sept.Nov. </volume> <year> 1991. </year>
Reference-contexts: Since logging every file system operation (particularly every read and write) generates huge quantities of data, most such studies have produced only summary statistics or made some other compromise such as coalescing multiple I/Os together (e.g., [Ousterhout85, Floyd86, Floyd89]). Many were taken on non-UNIX systems. For example: IBM mainframes <ref> [Procar82, Smith85, Kure88, Staelin88, Staelin90, Staelin91, Bozman91] </ref>; Cray supercomputers [Miller91]; Sprite (with no timing data on individual I/Os) [Baker91]; DEC VMS [Ramakrishnan92]; TOPS-10 (static analysis only) [Satyanarayanan81].
Reference: [Carson90] <author> Scott D. Carson. </author> <title> Experimental performance evaluation of the Berkeley file system. </title> <note> Technical report UMIACSTR905 and CSTR 2387. </note> <institution> Institute for Advanced Computer Studies, University of Maryland, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: There have been a few studies of disk traffic, but most have had aws of one kind or another. For example: poor measurement technology (60 Hz timer) [Johnson87]; short trace periods (75 minutes at a time, no detailed reporting of data, 2ms timer granularity) [Muller91]; limited use patterns <ref> [Carson90] </ref>. Raymie Stata had earlier used the same tracing technology as this study to look at the I/Os in a time-sharing UNIX environment [Stata90]. He discovered skewed device utilization and small average device queue lengths with large bursts.
Reference: [Clegg86] <author> Frederick W. Clegg, Gary Shiu-Fan Ho, Steven R. Kusmer, and John R. Sontag. </author> <title> The HP-UX operating system on HP Precision Architecture computers. </title> <journal> Hewlett-Packard Journal 37(12):422, </journal> <month> December </month> <year> 1986. </year>
Reference-contexts: We were interested in pursuing this path further, and gathering detailed statistics without the limitations of others work. The next section details how we did so. Trace gathering We traced three different Hewlett-Packard computer systems (described in Table 1). All were running release 8 of the HP-UX operating system <ref> [Clegg86] </ref>, which uses a version of the BSD fast file system [McKusick84]. The systems had several different types of disks attached to them, whose properties are summarized in Table 2. Trace collection method All of our data were obtained using a kernel-level trace facility built into HP-UX.
Reference: [English92] <author> Robert M. English and Alexander A. Stepanov. Loge: </author> <title> a self-organizing storage device. </title> <booktitle> USENIX Winter 1992 Technical Conference Proceedings (San Francisco, </booktitle> <address> CA), </address> <pages> pages 23751, </pages> <year> 2024 </year> <month> January </month> <year> 1992. </year> <title> UNIX disk access patterns Ruemmler and Wilkes 16 1993 Winter USENIX January 25-29, </title> <address> 1993 San Diego, CA </address>
Reference: [Floyd86] <author> Rick Floyd. </author> <title> Short-term file reference patterns in a UNIX environment. </title> <type> Technical report 177. </type> <institution> Computer Science Department, University of Rochester, </institution> <address> NY, </address> <month> March </month> <year> 1986. </year>
Reference-contexts: Since logging every file system operation (particularly every read and write) generates huge quantities of data, most such studies have produced only summary statistics or made some other compromise such as coalescing multiple I/Os together (e.g., <ref> [Ousterhout85, Floyd86, Floyd89] </ref>). Many were taken on non-UNIX systems. For example: IBM mainframes [Procar82, Smith85, Kure88, Staelin88, Staelin90, Staelin91, Bozman91]; Cray supercomputers [Miller91]; Sprite (with no timing data on individual I/Os) [Baker91]; DEC VMS [Ramakrishnan92]; TOPS-10 (static analysis only) [Satyanarayanan81].
Reference: [Floyd89] <author> Richard A. Floyd and Carla Schlatter Ellis. </author> <title> Directory reference patterns in hierarchical file systems. </title> <journal> IEEE Transactions on Knowledge and Data Engineering 1(2):23847, </journal> <month> June </month> <year> 1989. </year>
Reference-contexts: Since logging every file system operation (particularly every read and write) generates huge quantities of data, most such studies have produced only summary statistics or made some other compromise such as coalescing multiple I/Os together (e.g., <ref> [Ousterhout85, Floyd86, Floyd89] </ref>). Many were taken on non-UNIX systems. For example: IBM mainframes [Procar82, Smith85, Kure88, Staelin88, Staelin90, Staelin91, Bozman91]; Cray supercomputers [Miller91]; Sprite (with no timing data on individual I/Os) [Baker91]; DEC VMS [Ramakrishnan92]; TOPS-10 (static analysis only) [Satyanarayanan81].
Reference: [Jacobson91] <author> David M. Jacobson and John Wilkes. </author> <title> Disk scheduling algorithms based on rotational position. </title> <type> Technical report HPLCSP917. </type> <institution> Hewlett-Packard Laboratories, </institution> <month> 24 February </month> <year> 1991. </year>
Reference-contexts: The maximum queue lengths seen are much larger: over 1000 requests on cello, over 100 on hplajw and snake. This suggests that improved request-scheduling algorithms may be beneficial <ref> [Seltzer90, Jacobson91] </ref>. The bursty nature of the arrival rate is also shown by Figure 4, which shows the overall arrival rates of requests, and by Figure 6, which shows request inter-arrival times. Many of the I/O operations are issued less than 20ms apart; 1020% less than 1ms apart. <p> Policies we explore here include: no cache at all, immediate reporting, caching with a straight FCFS scheduling algorithm, and caching with a modified shortest access time first scheduling (SATF) algorithm. (SATF is a scheduling algorithm that takes both seek and rotation position into account <ref> [Seltzer90, Jacobson91] </ref>.
Reference: [Johnson87] <author> Thomas D. Johnson, Jonathan M. Smith, and Eric S. Wilson. </author> <title> Disk response time measurements. </title> <booktitle> USENIX Winter 1987 Technical Conference Proceedings (Washington, DC), </booktitle> <pages> pages 14762, </pages> <month> 2123 January </month> <year> 1987. </year> <title> [Kure88] flivind Kure. Optimization of file migration in distributed systems. </title> <type> PhD thesis, </type> <note> published as UCB/CSD 88/413. </note> <institution> Computer Science Division, Department of Electrical Engineering and Computer Science, UC Berkeley, </institution> <month> April </month> <year> 1988. </year>
Reference-contexts: They also ignore the effects of file-system generated traffic, such as for metadata and read-ahead, and the effects of swapping and paging. There have been a few studies of disk traffic, but most have had aws of one kind or another. For example: poor measurement technology (60 Hz timer) <ref> [Johnson87] </ref>; short trace periods (75 minutes at a time, no detailed reporting of data, 2ms timer granularity) [Muller91]; limited use patterns [Carson90]. Raymie Stata had earlier used the same tracing technology as this study to look at the I/Os in a time-sharing UNIX environment [Stata90].
Reference: [McKusick84] <author> Marshall K. McKusick, William N. Joy, Samuel J. Lefer, and Robert S. Fabry. </author> <title> A fast file system for UNIX. </title> <journal> ACM Transactions on Computer Systems 2(3):18197, </journal> <month> August </month> <year> 1984. </year>
Reference-contexts: The next section details how we did so. Trace gathering We traced three different Hewlett-Packard computer systems (described in Table 1). All were running release 8 of the HP-UX operating system [Clegg86], which uses a version of the BSD fast file system <ref> [McKusick84] </ref>. The systems had several different types of disks attached to them, whose properties are summarized in Table 2. Trace collection method All of our data were obtained using a kernel-level trace facility built into HP-UX.
Reference: [Miller91] <author> Ethan L. Miller and Randy H. Katz. </author> <title> Analyzing the I/O behavior of supercomputer applications. </title> <booktitle> Digest of papers, 11th IEEE Symposium on Mass Storage Systems (Monterey, </booktitle> <address> CA), </address> <pages> pages 519, </pages> <month> 710 October </month> <year> 1991. </year>
Reference-contexts: Many were taken on non-UNIX systems. For example: IBM mainframes [Procar82, Smith85, Kure88, Staelin88, Staelin90, Staelin91, Bozman91]; Cray supercomputers <ref> [Miller91] </ref>; Sprite (with no timing data on individual I/Os) [Baker91]; DEC VMS [Ramakrishnan92]; TOPS-10 (static analysis only) [Satyanarayanan81]. The UNIX buffer cache means that most accesses never reach the disk, so these studies are not very good models of what happens at the disk.
Reference: [Muller91] <author> Keith Muller and Joseph Pasquale. </author> <title> A high performance multi-structured file system design. </title> <booktitle> Proceedings of 13th ACM Symposium on Operating Systems Principles (Asilomar, </booktitle> <address> Pacific Grove, CA). </address> <note> Published as Operating Systems Review 25(5):56 67, </note> <month> 1316 October </month> <year> 1991. </year>
Reference-contexts: There have been a few studies of disk traffic, but most have had aws of one kind or another. For example: poor measurement technology (60 Hz timer) [Johnson87]; short trace periods (75 minutes at a time, no detailed reporting of data, 2ms timer granularity) <ref> [Muller91] </ref>; limited use patterns [Carson90]. Raymie Stata had earlier used the same tracing technology as this study to look at the I/Os in a time-sharing UNIX environment [Stata90]. He discovered skewed device utilization and small average device queue lengths with large bursts.
Reference: [Ousterhout85] <author> John K. Ousterhout, Herv Da Costa, David Harrison, John A. Kunze, Mike Kupfer, and James G. Thompson. </author> <title> A trace-driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> Proceedings of 10th ACM Symposium on Operating Systems Principles (Orcas Island, </booktitle> <address> WA). </address> <note> Published as Operating Systems Review 19(5):1524, </note> <month> December </month> <year> 1985. </year>
Reference-contexts: Since logging every file system operation (particularly every read and write) generates huge quantities of data, most such studies have produced only summary statistics or made some other compromise such as coalescing multiple I/Os together (e.g., <ref> [Ousterhout85, Floyd86, Floyd89] </ref>). Many were taken on non-UNIX systems. For example: IBM mainframes [Procar82, Smith85, Kure88, Staelin88, Staelin90, Staelin91, Bozman91]; Cray supercomputers [Miller91]; Sprite (with no timing data on individual I/Os) [Baker91]; DEC VMS [Ramakrishnan92]; TOPS-10 (static analysis only) [Satyanarayanan81]. <p> Hplajw: mean = 4.1; stddev = 7.8b. Snake: mean = 1.7, stddev = 3.5a. Cello: mean = 8.9; stddev = 36.0 I/O types Some previous studies (e.g., <ref> [Ousterhout85] </ref>) assumed that almost all accesses to the disk were for user data and therefore neglected to measure metadata and swap accesses. Our data (presented in Table 6) b. Hplajw a. <p> When cellos buffer cache was increased from 10MB to 30MB, the fraction of reads declined from 49.6% to 43.1%: a surprisingly small reduction. This suggests that predictions that almost all reads would be absorbed by large buffer caches <ref> [Ousterhout85, Ousterhout89, Rosenblum92] </ref> may be overly optimistic. The HP-UX file system generates both synchronous and asynchronous requests. Synchronous I/O operations cause the invoking process to wait until the I/O operation has occurred before proceeding, so delaying synchronous operations can increase the latency associated with an I/O operation. <p> On cello, 10% of the writes occur in bursts of more than 100 requests. I/O placement Previous studies have shown that I/Os at the file system level are highly sequential <ref> [Ousterhout85, Baker91] </ref>. But our data (plotted in Figure 9) shows that by the time these requests reach the disk they are much less so. We define requests to be logically sequential if they are at adjacent disk addresses or disk addresses spaced by the file system interleave factor.
Reference: [Ousterhout89] <author> John Ousterhout and Fred Douglis. </author> <title> Beating the I/O bottleneck: a case for log-structured file systems. </title> <booktitle> Operating Systems Review 23(1):11 27, </booktitle> <month> January </month> <year> 1989. </year>
Reference-contexts: When cellos buffer cache was increased from 10MB to 30MB, the fraction of reads declined from 49.6% to 43.1%: a surprisingly small reduction. This suggests that predictions that almost all reads would be absorbed by large buffer caches <ref> [Ousterhout85, Ousterhout89, Rosenblum92] </ref> may be overly optimistic. The HP-UX file system generates both synchronous and asynchronous requests. Synchronous I/O operations cause the invoking process to wait until the I/O operation has occurred before proceeding, so delaying synchronous operations can increase the latency associated with an I/O operation.
Reference: [Porcar82] <author> Juan M. Porcar. </author> <title> File migration in distributed computer systems. </title> <type> PhD thesis, </type> <note> published as Technical report LBL14763. </note> <institution> Physics, Computer Science and Mathematics Division, Lawrence Berkeley Laboratory, UC Berkeley, </institution> <month> July </month> <year> 1982. </year>
Reference: [Ramakrishnan92] <author> K. K. Ramakrishnan, Prabuddha Biswas, and Ramakrishna Karedla. </author> <title> Analysis of file I/O traces in commercial computing environments. </title> <booktitle> Proceedings of 1992 ACM SIGMETRICS and PERFORMANCE92 International Conference on Measurement and Modeling of Computer Systems (Newport, </booktitle> <address> RI). </address> <note> Published as Performance Evaluation Review 20(1):7890, </note> <month> 15 June </month> <year> 1992. </year>
Reference-contexts: Many were taken on non-UNIX systems. For example: IBM mainframes [Procar82, Smith85, Kure88, Staelin88, Staelin90, Staelin91, Bozman91]; Cray supercomputers [Miller91]; Sprite (with no timing data on individual I/Os) [Baker91]; DEC VMS <ref> [Ramakrishnan92] </ref>; TOPS-10 (static analysis only) [Satyanarayanan81]. The UNIX buffer cache means that most accesses never reach the disk, so these studies are not very good models of what happens at the disk.
Reference: [Rosenblum92] <author> Mendel Rosenblum and John K. Ousterhout. </author> <title> The design and implementation of a log-structured file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1):2652, </volume> <month> February </month> <year> 1992. </year>
Reference-contexts: When cellos buffer cache was increased from 10MB to 30MB, the fraction of reads declined from 49.6% to 43.1%: a surprisingly small reduction. This suggests that predictions that almost all reads would be absorbed by large buffer caches <ref> [Ousterhout85, Ousterhout89, Rosenblum92] </ref> may be overly optimistic. The HP-UX file system generates both synchronous and asynchronous requests. Synchronous I/O operations cause the invoking process to wait until the I/O operation has occurred before proceeding, so delaying synchronous operations can increase the latency associated with an I/O operation.
Reference: [Satyanarayanan81] <author> M. Satyanarayanan. </author> <title> A study of file sizes and functional lifetimes. </title> <booktitle> Proceedings of 8th ACM Symposium on Operating Systems Principles (Asilomar, </booktitle> <address> Ca). </address> <note> Published as Operating Systems Review, 15(5):96108, </note> <month> December </month> <year> 1981. </year>
Reference-contexts: Many were taken on non-UNIX systems. For example: IBM mainframes [Procar82, Smith85, Kure88, Staelin88, Staelin90, Staelin91, Bozman91]; Cray supercomputers [Miller91]; Sprite (with no timing data on individual I/Os) [Baker91]; DEC VMS [Ramakrishnan92]; TOPS-10 (static analysis only) <ref> [Satyanarayanan81] </ref>. The UNIX buffer cache means that most accesses never reach the disk, so these studies are not very good models of what happens at the disk. They also ignore the effects of file-system generated traffic, such as for metadata and read-ahead, and the effects of swapping and paging.
Reference: [Seltzer90] <author> Margo Seltzer, Peter Chen, and John Ousterhout. </author> <title> Disk scheduling revisited. </title> <booktitle> USENIX Winter 1990 Technical Conference Proceedings (Washington, DC), </booktitle> <pages> pages 31323, </pages> <month> 2226 Jan. </month> <year> 1990. </year>
Reference-contexts: The maximum queue lengths seen are much larger: over 1000 requests on cello, over 100 on hplajw and snake. This suggests that improved request-scheduling algorithms may be beneficial <ref> [Seltzer90, Jacobson91] </ref>. The bursty nature of the arrival rate is also shown by Figure 4, which shows the overall arrival rates of requests, and by Figure 6, which shows request inter-arrival times. Many of the I/O operations are issued less than 20ms apart; 1020% less than 1ms apart. <p> Policies we explore here include: no cache at all, immediate reporting, caching with a straight FCFS scheduling algorithm, and caching with a modified shortest access time first scheduling (SATF) algorithm. (SATF is a scheduling algorithm that takes both seek and rotation position into account <ref> [Seltzer90, Jacobson91] </ref>.
Reference: [Smith85] <author> Alan Jay Smith. </author> <title> Disk cachemiss ratio analysis and design considerations. </title> <journal> ACM Transactions on Computer Systems 3(3):161203, </journal> <month> August </month> <year> 1985. </year>
Reference-contexts: Since logging every file system operation (particularly every read and write) generates huge quantities of data, most such studies have produced only summary statistics or made some other compromise such as coalescing multiple I/Os together (e.g., [Ousterhout85, Floyd86, Floyd89]). Many were taken on non-UNIX systems. For example: IBM mainframes <ref> [Procar82, Smith85, Kure88, Staelin88, Staelin90, Staelin91, Bozman91] </ref>; Cray supercomputers [Miller91]; Sprite (with no timing data on individual I/Os) [Baker91]; DEC VMS [Ramakrishnan92]; TOPS-10 (static analysis only) [Satyanarayanan81].
Reference: [Staelin88] <author> Carl Staelin. </author> <title> File access patterns. </title> <type> Technical report CSTR17988. </type> <institution> Department of Computer Science, Princeton University, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Since logging every file system operation (particularly every read and write) generates huge quantities of data, most such studies have produced only summary statistics or made some other compromise such as coalescing multiple I/Os together (e.g., [Ousterhout85, Floyd86, Floyd89]). Many were taken on non-UNIX systems. For example: IBM mainframes <ref> [Procar82, Smith85, Kure88, Staelin88, Staelin90, Staelin91, Bozman91] </ref>; Cray supercomputers [Miller91]; Sprite (with no timing data on individual I/Os) [Baker91]; DEC VMS [Ramakrishnan92]; TOPS-10 (static analysis only) [Satyanarayanan81].
Reference: [Staelin91] <author> Carl Staelin and Hector Garcia-Molina. </author> <title> Smart filesystems. </title> <booktitle> USENIX Winter 1991 Technical Conference Proceedings (Dallas, TX), </booktitle> <pages> pages 4551, </pages> <month> 2125 January </month> <year> 1991. </year>
Reference-contexts: Since logging every file system operation (particularly every read and write) generates huge quantities of data, most such studies have produced only summary statistics or made some other compromise such as coalescing multiple I/Os together (e.g., [Ousterhout85, Floyd86, Floyd89]). Many were taken on non-UNIX systems. For example: IBM mainframes <ref> [Procar82, Smith85, Kure88, Staelin88, Staelin90, Staelin91, Bozman91] </ref>; Cray supercomputers [Miller91]; Sprite (with no timing data on individual I/Os) [Baker91]; DEC VMS [Ramakrishnan92]; TOPS-10 (static analysis only) [Satyanarayanan81].

References-found: 23

