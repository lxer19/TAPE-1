URL: http://svr-www.eng.cam.ac.uk/~srw1001/Papers/nips95_bayes.ps.Z
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Email: mackay@mrao.cam.ac.uk  
Phone: Tel: [+44] 1223 332754  Tel: [+44] 1223 337238  Tel: [+44] 1223 332815  
Title: Bayesian Methods for Mixtures of Experts  
Author: Steve Waterhouse David MacKay Tony Robinson 
Address: Cambridge CB2 1PZ England  Madingley Rd. Cambridge CB3 0HE England  Cambridge CB2 1PZ England.  
Affiliation: Cambridge University Engineering Department  Cavendish Laboratory  Cambridge University Engineering Department  
Note: To appear in Neural Information Processing Systems 8, MIT Press  
Abstract: We present a Bayesian framework for inferring the parameters of a mixture of experts model based on ensemble learning by variational free energy minimisation. The Bayesian approach avoids the over-fitting and noise level under-estimation problems of traditional maximum likelihood inference. We demonstrate these methods on artificial problems and sunspot time series prediction.
Abstract-found: 1
Intro-found: 1
Reference: <author> Dempster, A. P., Laird, N. M. & Rubin, D. B. </author> <year> (1977), </year> <title> `Maximum likelihood from incomplete data via the EM algorithm', </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> Series B 39, </volume> <pages> 1-38. </pages>
Reference: <author> Geman, S., Bienenstock, E. & Doursat, R. </author> <year> (1992), </year> <title> `Neural networks and the bias / variance dilemma', </title> <booktitle> Neural Computation 5, </booktitle> <pages> 1-58. </pages>
Reference: <author> Gull, S. F. </author> <year> (1989), </year> <title> Developments in maximum entropy data analysis, </title> <editor> in J. Skilling, ed., </editor> <title> `Maximum Entropy and Bayesian Methods, </title> <address> Cambridge 1988', </address> <publisher> Kluwer, Dordrecht, </publisher> <pages> pp. 53-71. </pages>
Reference-contexts: Maximising the posterior gives us the most probable parameters q MP . We may then set the hyperparameters either by cross-validation, or by finding the maximum of the posterior distribution of the hyperparameters P (ajD), also known as the "evidence" <ref> (Gull 1989) </ref>.
Reference: <author> Hinton, G. E. & van Camp, D. </author> <year> (1993), </year> <title> Keeping neural networks simple by mini mizing the description length of the weights, </title> <booktitle> in `Proceedings of COLT'. </booktitle>
Reference-contexts: In this paper we describe a method, motivated by the Expectation Maximisation (EM) algorithm of Dempster, Laird & Rubin (1977) and the principle of ensemble learning by variational free energy minimisation <ref> (Hinton & van Camp 1993, Neal & Hinton 1994) </ref> which achieves simultaneous optimisation of the parameters and hyperparameters of the HME. We then demonstrate this algorithm on two simulated examples and a time series prediction task. <p> We now outline an algorithm for the simultaneous optimisation of the parameters q and hyperparameters a and b, using the framework of ensemble learning by variational free energy minimisation <ref> (Hinton & van Camp 1993) </ref>. Rather than optimising a point estimate of q, a and b, we optimise a distribution over these parameters. This builds on Neal & Hinton's (1993) description of the EM algorithm in terms of variational free energy minimisation.
Reference: <author> Jordan, M. I. & Jacobs, R. A. </author> <year> (1994), </year> <title> `Hierarchical Mixtures of Experts and the EM algorithm', </title> <booktitle> Neural Computation 6, </booktitle> <pages> 181-214. </pages>
Reference-contexts: This problem is particularly dominant in models where the ratio of the number of data points in the training set to the number of parameters in the model is low. In this paper we consider inference of the parameters of the hierarchical mixture of experts (HME) architecture <ref> (Jordan & Jacobs 1994) </ref>. This model consists of a series of "experts," each modelling different processes assumed to be underlying causes of the data. Since each expert may focus on a different subset of the data which may be arbitrarily small, the possibility of over-fitting of each process is increased. <p> In each task the use of the Bayesian methods prevents over-fitting of the data and gives better prediction performance. Before we describe this algorithm, we will specify the model and its associated priors. MIXTURES OF EXPERTS The mixture of experts architecture <ref> (Jordan & Jacobs 1994) </ref> consists of a set of "experts" which perform local function approximation. The expert outputs are combined by a "gate" to form the overall output. In the hierarchical case, the experts are themselves mixtures of further experts, thus extending the network in a tree structured fashion.
Reference: <author> MacKay, D. J. C. </author> <year> (1992a), </year> <title> `Bayesian interpolation', </title> <booktitle> Neural Computation 4(3), </booktitle> <volume> 415 447. </volume>
Reference-contexts: Since each expert may focus on a different subset of the data which may be arbitrarily small, the possibility of over-fitting of each process is increased. We use Bayesian methods <ref> (MacKay 1992a) </ref> to avoid over-fitting by specifying prior belief in various aspects of the model and marginalising over parameter uncertainty. The use of regularisation or "weight decay" corresponds to the prior assumption that the model should have smooth outputs.
Reference: <author> MacKay, D. J. C. </author> <year> (1992b), </year> <title> `The evidence framework applied to classification net works', </title> <booktitle> Neural Computation 4(5), </booktitle> <pages> 698-714. </pages>
Reference-contexts: We may also marginalise over the gate parameters <ref> (MacKay 1992b) </ref> to give marginalised outputs for the gates.
Reference: <author> Neal, R. M. & Hinton, G. E. </author> <year> (1994), </year> <title> `A new view of the EM algorithm that justifies incremental and other variants', </title> <journal> Biometrika. </journal> <note> submitted. </note>
Reference: <author> Weigend, A. S., Huberman, B. A. & Rumelhart, D. E. </author> <year> (1990), </year> <title> `Predicting the future: a connectionist approach', </title> <journal> International Journal of Neural Systems 1, </journal> <pages> 193-209. </pages>
References-found: 9

