URL: http://www.cs.washington.edu/research/jair/volume5/litman96a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/litman96a.html
Root-URL: 
Email: diane@research.att.com  
Title: Cue Phrase Classification Using Machine Learning  
Author: Diane J. Litman 
Address: 600 Mountain Avenue Murray Hill, NJ 07974 USA  
Affiliation: AT&T Labs Research,  
Note: Journal of Artificial Intelligence Research 5 (1996) 53-94 Submitted 3/95; published 9/96  
Abstract: Cue phrases may be used in a discourse sense to explicitly signal discourse structure, but also in a sentential sense to convey semantic rather than structural information. Correctly classifying cue phrases as discourse or sentential is critical in natural language processing systems that exploit discourse structure, e.g., for performing tasks such as anaphora resolution and plan recognition. This paper explores the use of machine learning for classifying cue phrases as discourse or sentential. Two machine learning programs (cgrendel and C4.5) are used to induce classification models from sets of pre-classified cue phrases and their features in text and speech. Machine learning is shown to be an effective technique for not only automating the generation of classification models, but also for improving upon previous results. When compared to manually derived classification models already in the literature, the learned models often perform with higher accuracy and contain new linguistic insights into the data. In addition, the ability to automatically construct classification models makes it easier to comparatively analyze the utility of alternative feature representations of the data. Finally, the ease of retraining makes the learning approach more scalable and flexible than manual methods.
Abstract-found: 1
Intro-found: 1
Reference: <author> Altenberg, B. </author> <year> (1987). </year> <title> Prosodic Patterns in Spoken English: Studies in the Correlation between Prosody and Grammar for Text-to-Speech Conversion, </title> <booktitle> Vol. 76 of Lund Studies in English. </booktitle> <institution> Lund University Press, Lund. </institution>
Reference: <author> Aone, C., & Bennett, S. W. </author> <year> (1995). </year> <title> Evaluating automated and manual acquisition of anaphora resolution strategies. </title> <booktitle> In Proceedings of the Thirty-Third Annual Meeting of the Association for Computational Linguistics (ACL). </booktitle>
Reference-contexts: In addition, C4.5 has been used to develop anaphora resolution algorithms, by training on corpora tagged with appropriate discourse information <ref> (Aone & Bennett, 1995) </ref>. Similarly, McCarthy and Lehnert (1995) use C4.5 to learn decision trees to classify pairs of phrases as coreferent or not. Soderland and Lehnert (1994) use the machine learning program ID3 (a predecessor of C4.5) to support corpus-driven knowledge acquisition in information extraction.
Reference: <author> Brieman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Monterey, CA: </address> <publisher> Wadsworth and Brooks. </publisher>
Reference-contexts: Experiments using Machine Learning This section describes experiments that use the machine learning programs C4.5 (Quinlan, 1993) and cgrendel (Cohen, 1992, 1993) to automatically induce cue phrase classification models. cgrendel and C4.5 are similar to each other and to other learning methods such as neural networks and cart <ref> (Brieman, Friedman, Olshen, & Stone, 1984) </ref> in that all induce classification models from preclassified examples. <p> Machine learning has also been used in several other areas of discourse analysis. For example, learning has been used to develop rules for structuring discourse into multi-utterance segments. Grosz and Hirschberg (1992) use the classification and regression tree system cart <ref> (Brieman et al., 1984) </ref> to construct decision trees for classifying aspects of discourse structure from intonational feature values.
Reference: <author> Church, K. W. </author> <year> (1988). </year> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing. </booktitle>
Reference-contexts: The textual features used in the multiple cue phrase study (Hirschberg & Litman, 1993; Litman & Hirschberg, 1990) were extracted automatically from the transcript. The part of speech of each cue phrase was obtained by running a program for tagging words with one of approximately 80 parts of speech <ref> (Church, 1988) </ref> on the transcript. 6 Several characteristics of the cue phrase's immediate context were also noted, in particular, whether it was immediately preceded or succeeded by orthography (punctuation or a paragraph boundary), and whether it was immediately preceded or succeeded by a lexical item corresponding to another cue phrase. <p> This is done because the reliability of coding detailed transcriptions of orthography is not known. Part-of-speech (POS) represents the part of speech assigned to each cue phrase by Church's program for tagging part of speech in unrestricted text <ref> (Church, 1988) </ref>; while the program can assign approximately 80 different values, only the subset of values that were actually assigned to the cue phrases in the transcripts of the corpora are shown in the figure.
Reference: <author> Cohen, R. </author> <year> (1984). </year> <title> A computational theory of the function of clue words in argument understanding. </title> <booktitle> In Proceedings of the Tenth International Conference on Computational Linguistics (COLING). </booktitle>
Reference-contexts: Cue phrases can also be used to reduce the complexity of discourse processing in such areas as argument understanding <ref> (Cohen, 1984) </ref> and plan recognition (Litman & Allen, 1987; Grosz & Sidner, 1986). While the problem of cue phrase classification has often been noted (Grosz & Sidner, 1986), until recently, models for classifying cue phrases were neither developed nor evaluated based on careful empirical analyses.
Reference: <author> Cohen, W. W. </author> <year> (1992). </year> <title> Compiling knowledge into an explicit bias. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning. </booktitle>
Reference-contexts: This paper examines the utility of machine learning for automating the construction of models for classifying cue phrases from such empirical data. A set of experiments are described that use two machine learning programs, cgrendel <ref> (Cohen, 1992, 1993) </ref> and C4.5 (Quinlan, 1993), to induce classification models from sets of pre-classified cue phrases and their features. <p> This methodology of using statistical inference to determine whether differences in error rates are significant is discussed more fully in Section 3.3. 3. Experiments using Machine Learning This section describes experiments that use the machine learning programs C4.5 (Quinlan, 1993) and cgrendel <ref> (Cohen, 1992, 1993) </ref> to automatically induce cue phrase classification models. cgrendel and C4.5 are similar to each other and to other learning methods such as neural networks and cart (Brieman, Friedman, Olshen, & Stone, 1984) in that all induce classification models from preclassified examples. <p> Finally, the ease of retraining makes the learning approach more scalable and extensible than manual methods. 88 Cue Phrase Classification Using Machine Learning A first set of experiments were presented that used the machine learning programs cgrendel <ref> (Cohen, 1992, 1993) </ref> and C4.5 (Quinlan, 1993) to induce classification models from the preclassified cue phrases and their features that were used as training data by Hirschberg and Litman (1993). These results were then evaluated with the same testing data and methodology used by Hirschberg and Litman (1993).
Reference: <author> Cohen, W. W. </author> <year> (1993). </year> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI). </booktitle>
Reference-contexts: greedy strategy in which rules are added to the rule set one by one in an effort to form a small cover of the positive examples; each rule, in turn is created by adding one condition after another to the antecedent until the rule is consistent with the negative data. <ref> (Cohen, 1993) </ref> Although cgrendel is claimed to have two advantages over C4.5, these advantages do not come into play for the experiments reported here. First, if-then rules appear to be easier for people to understand than decision trees (Quinlan, 1993).
Reference: <author> Freedman, D., Pisani, R., & Purves, R. </author> <year> (1978). </year> <title> Statistics. </title> <editor> W. W. </editor> <publisher> Norton and Company. </publisher>
Reference-contexts: Following Hirschberg and Litman (1993), the original 48- and 52-example sets (Hirschberg & Litman, 1987) are combined. 58 Cue Phrase Classification Using Machine Learning Although not computed by Hirschberg and Litman, Table 1 also associates margins of errors with each error percentage, which are used to compute confidence intervals <ref> (Freedman, Pisani, & Purves, 1978) </ref>. (The margin of error is 2 standard errors for a 95% confidence interval using a normal table.) The lower bound of a confidence interval is computed by subtracting the margin of error from the error rate, while the upper bound is computed by adding the margin <p> In particular, confidence intervals for the two error rates are computed, at a 95% confidence level. When an error rate is estimated using only a single error rate on a test set (i.e., the train-and-test methodology), the confidence interval is computed using a normal approximation to the binomial distribution <ref> (Freedman et al., 1978) </ref>. When the error rate is estimated using the average from multiple error rates (i.e., the cross-validation methodology), the confidence interval is computed using a t-Table (Freedman et al., 1978). <p> on a test set (i.e., the train-and-test methodology), the confidence interval is computed using a normal approximation to the binomial distribution <ref> (Freedman et al., 1978) </ref>. When the error rate is estimated using the average from multiple error rates (i.e., the cross-validation methodology), the confidence interval is computed using a t-Table (Freedman et al., 1978).
Reference: <author> Grosz, B., & Hirschberg, J. </author> <year> (1992). </year> <title> Some intonational characteristics of discourse structure. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing (ICSLP). </booktitle>
Reference: <author> Grosz, B. J., & Sidner, C. L. </author> <year> (1986). </year> <title> Attention, intentions, and the structure of discourse. </title> <journal> Computational Linguistics, </journal> <volume> 12 (3), </volume> <month> 175-204. </month> <title> 92 Cue Phrase Classification Using Machine Learning Halliday, </title> <editor> M. A. K., & Hassan, R. </editor> <year> (1976). </year> <title> Cohesion in English. </title> <publisher> Longman. </publisher>
Reference-contexts: Cue phrases can also be used to reduce the complexity of discourse processing in such areas as argument understanding (Cohen, 1984) and plan recognition (Litman & Allen, 1987; Grosz & Sidner, 1986). While the problem of cue phrase classification has often been noted <ref> (Grosz & Sidner, 1986) </ref>, until recently, models for classifying cue phrases were neither developed nor evaluated based on careful empirical analyses.
Reference: <author> Hearst, M. A. </author> <year> (1994). </year> <title> Multi-paragraph segmentation of expository text. </title> <booktitle> In Proceedings of the Thirty-Second Annual Meeting of the Association for Computational Linguistics (ACL). </booktitle>
Reference-contexts: As discussed in Section 1, discourse structure is useful for performing tasks such as anaphora resolution and plan recognition. Recent work has also shown that if discourse structure can be recognized, it can be used to improve retrieval of text <ref> (Hearst, 1994) </ref> and speech (Stifleman, 1995). Although the prosodic features were manually labeled by Hirschberg and Litman, there are recent results suggesting that at least some aspects of prosody can be automatically labeled directly from speech.
Reference: <author> Hindle, D. M. </author> <year> (1989). </year> <title> Acquiring disambiguation rules from text. </title> <booktitle> In Proceedings of the Twenty-Seventh Annual Meeting of the Association for Computational Linguistics (ACL). </booktitle>
Reference-contexts: When a cue phrase is uttered as a single intermediate phrase possibly with other cue phrases (i.e., line (1) in 5. Only the features used in Figure 1 are discussed here. 6. Another syntactic feature dominating constituent was obtained by running the parser Fidditch <ref> (Hindle, 1989) </ref> on the transcript.
Reference: <author> Hirschberg, J. </author> <year> (1990). </year> <title> Accent and discourse context: Assigning pitch accent in synthetic speech. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI). </booktitle>
Reference-contexts: Correctly classifying cue phrases as discourse or sentential is important for other natural language processing tasks as well. The discourse/sentential distinction can be used to improve the naturalness of synthetic speech in text-to-speech systems <ref> (Hirschberg, 1990) </ref>. Text-to-speech systems generate synthesized speech from unrestricted text. If a cue phrase can be classified as discourse or sentential using features of the input text, it can then be synthesized using different intonational models for the discourse and sentential usages. <p> (9) elseif preceding orthography = false then sentential (10) Hirschberg and Litman. different single word cue phrases derived from the literature. 2 Hirschberg and Litman also used the cue phrases in the first 17 minutes of this corpus to develop a complementary cue phrase classification model based on textual features <ref> (Litman & Hirschberg, 1990) </ref>, which they then tested on the full corpus (Hirschberg & Litman, 1993). The first study will be referred to as the "now" study, and the follow-up study as the "multiple cue phrase" study. <p> Only the features used in Figure 1 are discussed here. 6. Another syntactic feature dominating constituent was obtained by running the parser Fidditch (Hindle, 1989) on the transcript. However, since this feature did not appear in any models manually derived from the training data <ref> (Litman & Hirschberg, 1990) </ref>, the feature was not pursued. 57 Litman Model Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495) Prosodic 24.6 3.0 14.7 3.2 Textual 19.9 2.8 16.1 3.4 Default Class 38.8 3.2 40.8 4.4 Table 1: 95% confidence intervals for the error rates (%) of the manually derived classifica tion <p> prosodic model was used to classify each cue phrase in its training data, i.e., the 100 examples of "now" from which the model was developed, the error rate was 2.0%. 8 The error rate of the textual model on the training examples from the multiple cue phrase corpus was 10.6% <ref> (Litman & Hirschberg, 1990) </ref>. The prosodic and textual models were evaluated by quantifying their performance in correctly classifying example cue phrases in two test sets of data, as shown in the rows labeled "Prosodic" and "Textual" in Table 1. <p> This allows a direct comparison of the manual and machine learning approaches. However, only the prosodic experiments conducted by Hirschberg and Litman (1993) are replicated. The textual training and testing conditions are not replicated as the original training corpus (the first 17 minutes of the multiple cue phrase corpus) <ref> (Litman & Hirschberg, 1990) </ref> is a subset of, rather than disjoint from, the test corpus (the full 75 minutes of the multiple cue phrase corpus) (Hirschberg & Litman, 1993). In contrast, each experiment in the second set uses cross-validation to estimate error rate. <p> In particular, the manually derived cue phrase classification models are used to improve the naturalness of the synthetic speech in a text-to-speech system <ref> (Hirschberg, 1990) </ref>. Using the text-based model, the text-to-speech system classifies each cue phrase in a text to be synthesized as either a discourse or sentential usage. Using the prosodic model, the system then conveys this usage by synthesizing the cue phrase with the appropriate type of intonation.
Reference: <author> Hirschberg, J., & Litman, D. </author> <year> (1987). </year> <title> Now let's talk about "now": Identifying cue phrases intonationally. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual Meeting of the Association for Computational Linguistics (ACL). </booktitle>
Reference-contexts: The first study by Hirschberg and Litman investigated usage of the cue phrase "now" by multiple speakers in a radio call-in show <ref> (Hirschberg & Litman, 1987) </ref>. A classification model based on prosodic features was developed based on manual analysis of a "training" set of 48 examples of "now", then evaluated on a previously unseen test set of 52 examples of "now". <p> Furthermore, a model that combined orthography and part-of-speech performed comparably to the simpler orthographic model (Hirschberg & Litman, 1993). Hirschberg and Litman also had preliminary observations suggesting that adjacency of cue phrases might prove useful. 8. Following Hirschberg and Litman (1993), the original 48- and 52-example sets <ref> (Hirschberg & Litman, 1987) </ref> are combined. 58 Cue Phrase Classification Using Machine Learning Although not computed by Hirschberg and Litman, Table 1 also associates margins of errors with each error percentage, which are used to compute confidence intervals (Freedman, Pisani, & Purves, 1978). (The margin of error is 2 standard errors
Reference: <author> Hirschberg, J., & Litman, D. </author> <year> (1993). </year> <title> Empirical studies on the disambiguation of cue phrases. </title> <journal> Computational Linguistics, </journal> <volume> 19 (3), </volume> <pages> 501-530. </pages>
Reference-contexts: A classification model based on prosodic features was developed based on manual analysis of a "training" set of 48 examples of "now", then evaluated on a previously unseen test set of 52 examples of "now". In a follow-up study <ref> (Hirschberg & Litman, 1993) </ref>, Hirschberg and Litman tested this classification model on a larger set of cue phrases, namely all single word cue phrases in a technical keynote address by a single speaker. <p> different single word cue phrases derived from the literature. 2 Hirschberg and Litman also used the cue phrases in the first 17 minutes of this corpus to develop a complementary cue phrase classification model based on textual features (Litman & Hirschberg, 1990), which they then tested on the full corpus <ref> (Hirschberg & Litman, 1993) </ref>. The first study will be referred to as the "now" study, and the follow-up study as the "multiple cue phrase" study. <p> This subset was considered particularly reliable since 97.2% of non-conjuncts were classifiable compared to 92.1% of all example cue phrases. The error rate of the prosodic model was 24.6% for the classifiable cue phrases and 14.7% for the classifiable non-conjuncts <ref> (Hirschberg & Litman, 1993) </ref>. The error rate of the textual model was 19.9% for the classifiable cue phrases and 16.1% for the classifiable non-conjuncts (Hirschberg & Litman, 1993). <p> The error rate of the prosodic model was 24.6% for the classifiable cue phrases and 14.7% for the classifiable non-conjuncts <ref> (Hirschberg & Litman, 1993) </ref>. The error rate of the textual model was 19.9% for the classifiable cue phrases and 16.1% for the classifiable non-conjuncts (Hirschberg & Litman, 1993). The last row of the table shows error rates for a simple "Default Class" baseline model that always predicts the most frequent class in the corpus (sentential). These rates are 38.8% for the classifiable cue phrases and 40.8% for the classifiable non-conjuncts. 7. <p> Furthermore, a model that combined orthography and part-of-speech performed comparably to the simpler orthographic model <ref> (Hirschberg & Litman, 1993) </ref>. Hirschberg and Litman also had preliminary observations suggesting that adjacency of cue phrases might prove useful. 8. <p> Hirschberg and Litman's 3-way classification of cue phrases by 2 judges <ref> (Hirschberg & Litman, 1993) </ref> is transformed into the classifications used by the machine learning programs as shown in Table 2. Recall from Section 2 that each judge classified each cue phrase as discourse, sentential, or ambiguous; these classifications are shown as D, S, and ? in Table 2. <p> Consider the following utterance, taken from the multiple cue phrase corpus <ref> (Hirschberg & Litman, 1993) </ref>: Example 1 [(Now) (now that we have all been welcomed here)] it's time to get on with the business of the conference. This utterance contains two cue phrases, corresponding to the two instances of "now". <p> The textual training and testing conditions are not replicated as the original training corpus (the first 17 minutes of the multiple cue phrase corpus) (Litman & Hirschberg, 1990) is a subset of, rather than disjoint from, the test corpus (the full 75 minutes of the multiple cue phrase corpus) <ref> (Hirschberg & Litman, 1993) </ref>. In contrast, each experiment in the second set uses cross-validation to estimate error rate. Furthermore, both training and testing samples are taken from the multiple cue phrase corpus. <p> In contrast, the prosodic features phrasal composition and accent were previously known to be useful in conjunction with each other and with phrasal position <ref> (Hirschberg & Litman, 1993) </ref>, while part-of-speech was known to be useful only in conjunction with orthography (Hirschberg & Litman, 1993). Length, adjacent cue phrases, and succeeding position were not used in either of the manually derived models (Hirschberg & Litman, 1993) (although length and adjacent cue phrases were shown to be <p> In contrast, the prosodic features phrasal composition and accent were previously known to be useful in conjunction with each other and with phrasal position <ref> (Hirschberg & Litman, 1993) </ref>, while part-of-speech was known to be useful only in conjunction with orthography (Hirschberg & Litman, 1993). Length, adjacent cue phrases, and succeeding position were not used in either of the manually derived models (Hirschberg & Litman, 1993) (although length and adjacent cue phrases were shown to be useful again only in conjunction with other prosodic and textual features in Experiment Set 2). 86 <p> known to be useful in conjunction with each other and with phrasal position <ref> (Hirschberg & Litman, 1993) </ref>, while part-of-speech was known to be useful only in conjunction with orthography (Hirschberg & Litman, 1993). Length, adjacent cue phrases, and succeeding position were not used in either of the manually derived models (Hirschberg & Litman, 1993) (although length and adjacent cue phrases were shown to be useful again only in conjunction with other prosodic and textual features in Experiment Set 2). 86 Cue Phrase Classification Using Machine Learning tokenized models) to additionally specify preceding and succeeding orthography, part-of-speech, and adjacent cue phrases that
Reference: <author> Holte, R. C. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 (1), </volume> <pages> 63-90. </pages>
Reference: <author> Litman, D., & Hirschberg, J. </author> <year> (1990). </year> <title> Disambiguating cue phrases in text and speech. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Computational Linguistics (COLING). </booktitle>
Reference-contexts: (9) elseif preceding orthography = false then sentential (10) Hirschberg and Litman. different single word cue phrases derived from the literature. 2 Hirschberg and Litman also used the cue phrases in the first 17 minutes of this corpus to develop a complementary cue phrase classification model based on textual features <ref> (Litman & Hirschberg, 1990) </ref>, which they then tested on the full corpus (Hirschberg & Litman, 1993). The first study will be referred to as the "now" study, and the follow-up study as the "multiple cue phrase" study. <p> Only the features used in Figure 1 are discussed here. 6. Another syntactic feature dominating constituent was obtained by running the parser Fidditch (Hindle, 1989) on the transcript. However, since this feature did not appear in any models manually derived from the training data <ref> (Litman & Hirschberg, 1990) </ref>, the feature was not pursued. 57 Litman Model Classifiable Cue Phrases (N=878) Classifiable Non-Conjuncts (N=495) Prosodic 24.6 3.0 14.7 3.2 Textual 19.9 2.8 16.1 3.4 Default Class 38.8 3.2 40.8 4.4 Table 1: 95% confidence intervals for the error rates (%) of the manually derived classifica tion <p> prosodic model was used to classify each cue phrase in its training data, i.e., the 100 examples of "now" from which the model was developed, the error rate was 2.0%. 8 The error rate of the textual model on the training examples from the multiple cue phrase corpus was 10.6% <ref> (Litman & Hirschberg, 1990) </ref>. The prosodic and textual models were evaluated by quantifying their performance in correctly classifying example cue phrases in two test sets of data, as shown in the rows labeled "Prosodic" and "Textual" in Table 1. <p> This allows a direct comparison of the manual and machine learning approaches. However, only the prosodic experiments conducted by Hirschberg and Litman (1993) are replicated. The textual training and testing conditions are not replicated as the original training corpus (the first 17 minutes of the multiple cue phrase corpus) <ref> (Litman & Hirschberg, 1990) </ref> is a subset of, rather than disjoint from, the test corpus (the full 75 minutes of the multiple cue phrase corpus) (Hirschberg & Litman, 1993). In contrast, each experiment in the second set uses cross-validation to estimate error rate.
Reference: <author> Litman, D. J. </author> <year> (1994). </year> <title> Classifying cue phrases in text and speech using machine learning. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI). </booktitle>
Reference-contexts: 15.3 13.6 16.8 17.6 Table 15: Error rates (%) of the C4.5 prosodic/textual classification model, testing data. (Training and testing were done from the multiple cue phrase corpus using cross validation.) thank William Cohen, Ido Dagan, Julia Hirschberg, and Eric Siegel for comments on a preliminary version of this paper <ref> (Litman, 1994) </ref>.
Reference: <author> Litman, D. J., & Allen, J. F. </author> <year> (1987). </year> <title> A plan recognition model for subdialogues in conversation. </title> <journal> Cognitive Science, </journal> <volume> 11, </volume> <pages> 163-200. </pages>
Reference-contexts: The first study by Hirschberg and Litman investigated usage of the cue phrase "now" by multiple speakers in a radio call-in show <ref> (Hirschberg & Litman, 1987) </ref>. A classification model based on prosodic features was developed based on manual analysis of a "training" set of 48 examples of "now", then evaluated on a previously unseen test set of 52 examples of "now". <p> Furthermore, a model that combined orthography and part-of-speech performed comparably to the simpler orthographic model (Hirschberg & Litman, 1993). Hirschberg and Litman also had preliminary observations suggesting that adjacency of cue phrases might prove useful. 8. Following Hirschberg and Litman (1993), the original 48- and 52-example sets <ref> (Hirschberg & Litman, 1987) </ref> are combined. 58 Cue Phrase Classification Using Machine Learning Although not computed by Hirschberg and Litman, Table 1 also associates margins of errors with each error percentage, which are used to compute confidence intervals (Freedman, Pisani, & Purves, 1978). (The margin of error is 2 standard errors
Reference: <author> Litman, D. J., & Passonneau, R. J. </author> <year> (1995). </year> <title> Combining multiple knowledge sources for discourse segmentation. </title> <booktitle> In Proceedings of the Thirty-Third Annual Meeting of the Association for Computational Linguistics (ACL). </booktitle>
Reference: <author> McCarthy, J. F., & Lehnert, W. G. </author> <year> (1995). </year> <title> Using decision trees for coreference resolution. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI). </booktitle>
Reference: <author> Moser, M., & Moore, J. D. </author> <year> (1995). </year> <title> Investigating cue selection and placement in tutorial discourse. </title> <booktitle> In Proceedings of the Thirty-Third Annual Meeting of the Association for Computational Linguistics (ACL). </booktitle>
Reference: <author> Ostendorf, M., & Ross, K. </author> <title> (in press). A multi-level model for recognition of intonation labels. </title>
Reference: <editor> In Y. Sagisaka, N. C., & Higuchi, N. (Eds.), </editor> <title> Computing Prosody. </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Passonneau, R. J., & Litman, D. J. </author> <title> (in press). Discourse segmentation by human and automated means. </title> <note> Computational Linguistics, 23. 93 Litman Pierrehumbert, </note> <author> J. B. </author> <year> (1980). </year> <title> The Phonology and Phonetics of English Intonation. </title> <type> Ph.D. thesis, </type> <institution> Massachusetts Institute of Technology. </institution> <note> Distributed by the Indiana University Linguistics Club. </note>
Reference: <author> Pitrelli, J., Beckman, M., & Hirschberg, J. </author> <year> (1994). </year> <title> Evaluation of prosodic transcription labeling reliability in the ToBI framework. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing (ICSLP). </booktitle>
Reference-contexts: Similar manual transcriptions of prosodic phrasing and accent have been shown to be reliable across coders <ref> (Pitrelli, Beckman, & Hirschberg, 1994) </ref>. Once prosody was coded, Hirschberg and Litman represented every cue phrase in terms of the following prosodic features. 5 Accent corresponded to the pitch accent (if any) that was associated with the cue phrase. <p> While manual tran 16. Recall that Experiment Sets 2 and 3 constructed 14 prosodic models, 12 textual models, and 1 prosodic/textual model. 85 Litman scriptions of prosodic features have been shown to be reliable across coders <ref> (Pitrelli et al., 1994) </ref>, there are no corresponding results for the reliability of orthography. Examination of the best performing learned models shows that they are often comparable in content to the relevant portions of the manually derived models.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5 : Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This paper examines the utility of machine learning for automating the construction of models for classifying cue phrases from such empirical data. A set of experiments are described that use two machine learning programs, cgrendel (Cohen, 1992, 1993) and C4.5 <ref> (Quinlan, 1993) </ref>, to induce classification models from sets of pre-classified cue phrases and their features. The features, classes and training examples used in the studies of Hirschberg and Litman (1993), as well as additional features, classes and training examples, are given as input to the machine learning programs. <p> This methodology of using statistical inference to determine whether differences in error rates are significant is discussed more fully in Section 3.3. 3. Experiments using Machine Learning This section describes experiments that use the machine learning programs C4.5 <ref> (Quinlan, 1993) </ref> and cgrendel (Cohen, 1992, 1993) to automatically induce cue phrase classification models. cgrendel and C4.5 are similar to each other and to other learning methods such as neural networks and cart (Brieman, Friedman, Olshen, & Stone, 1984) in that all induce classification models from preclassified examples. <p> First, if-then rules appear to be easier for people to understand than decision trees <ref> (Quinlan, 1993) </ref>. However, for the cue phrase classification task, the decision trees produced by C4.5 are quite compact and thus easily understood. Furthermore, a rule representation can be derived from C4.5 decision trees, using the program C4.5rules. <p> data. (Training data was the "now" corpus; testing data was the multiple cue phrase corpus.) of the pruning process is to take a complex decision tree that may also be overfitted to the training data, and to produce a tree that is more comprehensible and whose accuracy is not comprised <ref> (Quinlan, 1993) </ref>. Since almost all trees are improved by pruning (Quinlan, 1993), only simplified decision trees are considered in this paper. In contrast, cgrendel represents each learned classification model as a set of if-then rules. <p> the multiple cue phrase corpus.) of the pruning process is to take a complex decision tree that may also be overfitted to the training data, and to produce a tree that is more comprehensible and whose accuracy is not comprised <ref> (Quinlan, 1993) </ref>. Since almost all trees are improved by pruning (Quinlan, 1993), only simplified decision trees are considered in this paper. In contrast, cgrendel represents each learned classification model as a set of if-then rules. Each rule specifies a conjunction of tests on various features, and results in the assignment of a class. <p> Finally, the ease of retraining makes the learning approach more scalable and extensible than manual methods. 88 Cue Phrase Classification Using Machine Learning A first set of experiments were presented that used the machine learning programs cgrendel (Cohen, 1992, 1993) and C4.5 <ref> (Quinlan, 1993) </ref> to induce classification models from the preclassified cue phrases and their features that were used as training data by Hirschberg and Litman (1993). These results were then evaluated with the same testing data and methodology used by Hirschberg and Litman (1993).
Reference: <author> Reichman, R. </author> <year> (1985). </year> <title> Getting Computers to Talk Like You and Me: Discourse Context, Focus, and Semantics. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Siegel, E. V. </author> <year> (1994). </year> <title> Competitively evolving decision trees against fixed training cases for natural language processing. </title> <editor> In K. E. Kinnear, J. (Ed.), </editor> <booktitle> Advances in Genetic Programming. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Note that only the features that were coded or discussed by Hirschberg and Litman (1993) were considered in this paper. It may be possible to further lower the error rates by considering new types of prosodic and textual features (e.g., other contextual textual features <ref> (Siegel, 1994) </ref>, or features that have been proposed in connection with the more general topic of discourse structure), and/or by using different kinds of learning methods.
Reference: <author> Siegel, E. V., & McKeown, K. R. </author> <year> (1994). </year> <title> Emergent linguistic rules from the automatic grouping of training examples: Disambiguating clue words with decision trees. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI). </booktitle>
Reference-contexts: Note that only the features that were coded or discussed by Hirschberg and Litman (1993) were considered in this paper. It may be possible to further lower the error rates by considering new types of prosodic and textual features (e.g., other contextual textual features <ref> (Siegel, 1994) </ref>, or features that have been proposed in connection with the more general topic of discourse structure), and/or by using different kinds of learning methods.
Reference: <author> Soderland, S., & Lehnert, W. </author> <year> (1994). </year> <title> Corpus-driven knowledge acquisition for discourse analysis. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI). </booktitle>
Reference: <author> Stifleman, L. J. </author> <year> (1995). </year> <title> A discourse analysis approach to structured speech. </title> <booktitle> In Working Notes of AAAI Spring Symposium Series: Empirical Methods in Discourse Interpretation and Generation. </booktitle>
Reference-contexts: As discussed in Section 1, discourse structure is useful for performing tasks such as anaphora resolution and plan recognition. Recent work has also shown that if discourse structure can be recognized, it can be used to improve retrieval of text (Hearst, 1994) and speech <ref> (Stifleman, 1995) </ref>. Although the prosodic features were manually labeled by Hirschberg and Litman, there are recent results suggesting that at least some aspects of prosody can be automatically labeled directly from speech.
Reference: <author> Weiss, S. M., & Kulikowski, C. </author> <year> (1991). </year> <title> Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, </title> <booktitle> Machine Learning, and Expert Systems. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The error rates of the learned classification models are estimated using two methodologies. Train-and-test error rate estimation <ref> (Weiss & Kulikowski, 1991) </ref> "holds out" a test set of examples, which are not seen until after training is completed. That is, the model is developed by examining only the training examples; the error of the model is then estimated by using the model to classify the test examples. <p> That is, the model is developed by examining only the training examples; the error of the model is then estimated by using the model to classify the test examples. This was the evaluation method used by Hirschberg and Litman. The resampling method of cross-validation <ref> (Weiss & Kulikowski, 1991) </ref> estimates error rate using multiple train-and-test experiments. For example, in 10-fold cross-validation, instead of dividing examples into training and test sets once, 10 runs of the learning program are performed. <p> For sample sizes in the hundreds (the classifiable subset of the multiple cue phrase sample and the classifiable non-conjunct subset provide 878 and 495 examples, respectively) 10-fold cross-validation often provides a better performance estimate than the hold-out method <ref> (Weiss & Kulikowski, 1991) </ref>. The major advantage is that in cross-validation all examples are eventually used for testing, and almost all examples are used in any given training run. <p> The reliability of the testing is not compromised due to the use of cross-validation <ref> (Weiss & Kulikowski, 1991) </ref>. Each experiment in the third set replicates an experiment in the second set, with the exception that the learning program is now allowed to distinguish between cue phrases.
Reference: <author> Wermter, S., Riloff, E., & Scheler, G. </author> <year> (1996). </year> <title> Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing. </title> <address> Berlin, Germany: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: While researchers in other areas of natural language processing have also addressed these issues, they have in addition applied a much wider variety of learning approaches, and have been concerned with the development of learning methods particularly designed for language processing. A recent survey of learning for natural language <ref> (Wermter, Riloff, & Scheler, 1996) </ref> illustrates both the type of learning approaches that have been used and modified (in particular, symbolic, connectionist, statistical, and hybrid approaches), as well as the scope of the problems that have proved amenable to the use of learning techniques (e.g., grammatical inference, syntactic disambiguation, and word
Reference: <author> Wightman, C. W., & Ostendorf, M. </author> <year> (1994). </year> <title> Automatic labeling of prosodic patterns. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 2 (4), </volume> <pages> 469-481. </pages>
Reference-contexts: Similarly, accenting versus deaccenting can be automatically labeled with 88% accuracy <ref> (Wightman & Ostendorf, 1994) </ref>, while a more sophisticated labeling scheme that distinguishes between four types of accent classes (and is somewhat similar to the prosodic feature accent* used in this paper) can be labeled with 85% accuracy (Ostendorf & Ross, in press).
Reference: <author> Zuckerman, I., & Pearl, J. </author> <year> (1986). </year> <title> Comprehension-driven generation of meta-technical utterances in math tutoring. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence (AAAI). </booktitle> <pages> 94 </pages>
References-found: 36

