URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-337.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/classes/cs7322_98_spring/readings.html
Root-URL: 
Email: E-mail: drew, bobick@media.mit.edu  
Title: Visual Behavior for Gesture Analysis  
Author: Andrew Wilson Aaron Bobick 
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: Room E15-383, The Media Laboratory Massachusetts Institute of Technology  
Note: Learning  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 337 Appears in Proceedings of the IEEE Symposium on Computer Vision Coral Gables, Florida, November 19-21, 1995 Abstract A state-based method for learning visual behavior from image sequences is presented. The technique is novel for its incorporation of multiple representations into the Hidden Markov Model framework. Independent representations of the instantaneous visual input at each state of the Markov model are estimated concurrently with the learning of the temporal characteristics. Measures of the degree to which each representation describes the input are combined to determine an input's overall membership to a state. We exploit two constraints allowing application of the technique to view-based gesture recognition: gestures are modal in the space of possible human motion, and gestures are viewpoint-dependent. The recovery of the visual behavior of a number of simple gestures with a small num ber of low resolution image sequences is shown.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. F. Bobick and A. D. Wilson. </author> <title> A state-based technique for the summarization and recognition of gesture. </title> <booktitle> Proc. Int. Conf. Comp. Vis., </booktitle> <year> 1995. </year>
Reference-contexts: Model types useful for characterizing images in an image sequence might include eigenvector decomposition of sets of images [16], orientation histograms [4], peak temporal frequencies [10], tracked position of objects in the frame, and optic flow field summaries. 2.2 State-based descriptions In previous work <ref> [1] </ref> we defined gesture to be a sequence of states in a configuration space. States were defined on some input space (say the joint angles returned by a DataGlove) and were designed to capture the constraints present in a series of training examples.
Reference: [2] <author> Y. Cui and J. Weng. </author> <title> Learning-based hand sign recognition. </title> <booktitle> In Proc. of the Intl. Workshop on Automatic Face- and Gesture-Recognition, </booktitle> <address> Zurich, </address> <year> 1995. </year>
Reference-contexts: Murase and Nayar [8] parameterize multiple eigenspaces over pose and illumination angle for object recognition. Polana and Nelson [10] match low level templates of spa-tiotemporal motion to recognize periodic human motions in image sequences. Cui and Weng <ref> [2] </ref> use learned decision boundaries to recognize sequences of vector-quantized images of hands.
Reference: [3] <author> T.J. Darrell and A.P. Pentland. </author> <title> Space-time gestures. </title> <booktitle> Proc. Comp. Vis. and Pattern Rec., </booktitle> <pages> pages 335-340, </pages> <year> 1993. </year>
Reference-contexts: Visual gestures are therefore viewpoint-dependent. And the task of gesture understanding is particularly suited to a view-based, multiple model approach in which only a small subspace of human motions is represented. In related work, Darrell and Pentland <ref> [3] </ref> use dynamic time warping and normalized correlation to match the interpolated responses of several learned image templates. Murase and Nayar [8] parameterize multiple eigenspaces over pose and illumination angle for object recognition.
Reference: [4] <author> W. T. Freeman and M. Roth. </author> <title> Orientation histograms for hand gesture recognition. </title> <booktitle> In Proc. of the Intl. Workshop on Automatic Face- and Gesture-Recognition, </booktitle> <address> Zurich, </address> <year> 1995. </year>
Reference-contexts: One goal is to develop an approach that can exploit multiple models simultaneously, where the type of models might be quite distinct. Model types useful for characterizing images in an image sequence might include eigenvector decomposition of sets of images [16], orientation histograms <ref> [4] </ref>, peak temporal frequencies [10], tracked position of objects in the frame, and optic flow field summaries. 2.2 State-based descriptions In previous work [1] we defined gesture to be a sequence of states in a configuration space.
Reference: [5] <author> X.D. Huang, Y. Ariki, and M. A. Jack. </author> <title> Hidden Markov Models for Speech Recognition. </title> <publisher> Edinburgh University Press, </publisher> <year> 1990. </year>
Reference-contexts: Starner and Pentland [15] apply continuous HMM's with the orientation and position of both hands wearing colored gloves. HMM's are attractive because they put dynamic time warping on a probabilistic foundation and produce a Markov model of discrete states that codes the temporal structure of the observations <ref> [5] </ref>. Training an HMM involves inferring a first-order Markov model from a set of possibly continuous observation vectors. Each state is associated with the observation probability distribution b j (x). <p> In the conventional HMM framework, the Baum-Welch algorithm is guaranteed to converge <ref> [5] </ref>. By interleaving the estimation of model parameters with the Baum-Welch algorithm, the proof of convergence may be invalidated.
Reference: [6] <author> D. McNeill. </author> <title> Hand and Mind: What Gestures Reveal About Thought. </title> <publisher> Univ. of Chicago Press, </publisher> <address> Chicago, </address> <year> 1992. </year>
Reference-contexts: We take visual behavior to mean the sequence of visual events that makes a complete action or gesture. We assume that two gestures that have the same visual behavior are in fact the same gesture, thus ignoring the delicate problem of relating the form and meaning of natural gesture <ref> [6] </ref>. 1.1 View-based approach The machine understanding of human movement and gesture brings new possibilities to computer-human interaction. Such interest has inspired research into the recovery of the the complete 3-dimensional pose of the body or hand using a 3-dimensional physical model (e.g. [12, 13]).
Reference: [7] <author> B. Moghaddam and A. Pentland. </author> <title> Probabalistic visual learning for object detection. </title> <booktitle> In Proc. Int. Conf. Comp. Vis., </booktitle> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: The distances to each model instance may be combined to give d j (x) = hd A j (x); d B j (x); : : :i: This quantity is similar to the "distance from feature space" derived in <ref> [7] </ref>. Next we consider the observation probability distribution b j (x) which describes the probability of measuring a particular residual for an observation when that observation is really generated by state j. The probability b j (x) may be estimated from the observations, each weighted by fl t (j).
Reference: [8] <author> H. Murase and S. Nayar. </author> <title> Visual learning and recognition of 3-D objects from appearance. </title> <journal> Int. J. of Comp. Vis., </journal> <volume> 14 </volume> <pages> 5-24, </pages> <year> 1995. </year>
Reference-contexts: In related work, Darrell and Pentland [3] use dynamic time warping and normalized correlation to match the interpolated responses of several learned image templates. Murase and Nayar <ref> [8] </ref> parameterize multiple eigenspaces over pose and illumination angle for object recognition. Polana and Nelson [10] match low level templates of spa-tiotemporal motion to recognize periodic human motions in image sequences. Cui and Weng [2] use learned decision boundaries to recognize sequences of vector-quantized images of hands.
Reference: [9] <author> R. W. Picard and T. P. Minka. </author> <title> Vision texture for annotation. </title> <journal> Journal of Multimedia Systems, </journal> <volume> 3 </volume> <pages> 3-14, </pages> <year> 1995. </year>
Reference-contexts: Different models may interpret the same sensor data in different ways or they may take data from different sensors, in which case sensor fusion is the goal. The use of multiple models in a visual classification task is discussed in <ref> [9] </ref>. One goal is to develop an approach that can exploit multiple models simultaneously, where the type of models might be quite distinct.
Reference: [10] <author> R. Polana and R. Nelson. </author> <title> Low level recognition of human motion. </title> <booktitle> In Proc. of the Workshop on Motion of Non-Rigid and Articulated Objects, </booktitle> <pages> pages 77-82, </pages> <address> Austin, Texas, </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: In related work, Darrell and Pentland [3] use dynamic time warping and normalized correlation to match the interpolated responses of several learned image templates. Murase and Nayar [8] parameterize multiple eigenspaces over pose and illumination angle for object recognition. Polana and Nelson <ref> [10] </ref> match low level templates of spa-tiotemporal motion to recognize periodic human motions in image sequences. Cui and Weng [2] use learned decision boundaries to recognize sequences of vector-quantized images of hands. <p> One goal is to develop an approach that can exploit multiple models simultaneously, where the type of models might be quite distinct. Model types useful for characterizing images in an image sequence might include eigenvector decomposition of sets of images [16], orientation histograms [4], peak temporal frequencies <ref> [10] </ref>, tracked position of objects in the frame, and optic flow field summaries. 2.2 State-based descriptions In previous work [1] we defined gesture to be a sequence of states in a configuration space.
Reference: [11] <author> F. Quek. </author> <title> Hand gesture interface for human-machine interaction. </title> <booktitle> In Proc. of Virtual Reality Systems, volume Fall, </booktitle> <year> 1993. </year>
Reference-contexts: Quek <ref> [11] </ref> has observed that it is rare for both the pose and the position of the hand to simultaneously change in a meaningful way during a gesture. 1 Rather than use one model that is only partially effec-tive, the approach here is to allow for multiple models.
Reference: [12] <author> J. M. Rehg and T. Kanade. </author> <title> Visual tracking of high DOF articulated structures: an application to human hand tracking. </title> <booktitle> Proc. European Conf. Comp. Vis., </booktitle> <volume> 2 </volume> <pages> 35-46, </pages> <year> 1994. </year>
Reference-contexts: Such interest has inspired research into the recovery of the the complete 3-dimensional pose of the body or hand using a 3-dimensional physical model (e.g. <ref> [12, 13] </ref>). The presumption behind such work is that a complete kinematic model of the body will be required for useful inferences. We claim that gestures are embedded within communication. As such, the gesturer typically orients the movements towards the recipient of the gesture. Visual gestures are therefore viewpoint-dependent.
Reference: [13] <author> K. Rohr. </author> <title> Towards model-based recognition of human movements in image sequences. Comp. Vis., Graph., </title> <journal> and Img. Proc., </journal> <volume> 59(1) </volume> <pages> 94-115, </pages> <year> 1994. </year>
Reference-contexts: Such interest has inspired research into the recovery of the the complete 3-dimensional pose of the body or hand using a 3-dimensional physical model (e.g. <ref> [12, 13] </ref>). The presumption behind such work is that a complete kinematic model of the body will be required for useful inferences. We claim that gestures are embedded within communication. As such, the gesturer typically orients the movements towards the recipient of the gesture. Visual gestures are therefore viewpoint-dependent.
Reference: [14] <author> J. Schlenzig, E. Hunter, and R. Jain. </author> <title> Vision based hand gesture interpretation using recursive estimation. </title> <booktitle> In Proc. of the Twenty-Eighth Asilomar Conf. on Signals, Systems and Comp., </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Yamato et al. [17] compute a simple region-based statistic from each frame of image sequences of tennis swings. Sequences of the vector-quantized features are then identified by a trained HMM. Schlenzig et al. <ref> [14] </ref> use a rotation invariant representation of binary images and a neural net to quantize the image to a hand pose token before using an HMM. Starner and Pentland [15] apply continuous HMM's with the orientation and position of both hands wearing colored gloves.
Reference: [15] <author> T. E. Starner and A. Pentland. </author> <title> Visual recognition of american sign language using hidden markov models. </title> <booktitle> In Proc. of the Intl. Workshop on Automatic Face-and Gesture-Recognition, </booktitle> <address> Zurich, </address> <year> 1995. </year>
Reference-contexts: Polana and Nelson [10] match low level templates of spa-tiotemporal motion to recognize periodic human motions in image sequences. Cui and Weng [2] use learned decision boundaries to recognize sequences of vector-quantized images of hands. Starner and Pentland <ref> [15] </ref> extract the position and dominant orientation of both hands for the recognition of simple American Sign Language. 2 Representation of gesture 2.1 Multiple models for gesture We claim that gestures are modal in the space of human motion. <p> Sequences of the vector-quantized features are then identified by a trained HMM. Schlenzig et al. [14] use a rotation invariant representation of binary images and a neural net to quantize the image to a hand pose token before using an HMM. Starner and Pentland <ref> [15] </ref> apply continuous HMM's with the orientation and position of both hands wearing colored gloves. HMM's are attractive because they put dynamic time warping on a probabilistic foundation and produce a Markov model of discrete states that codes the temporal structure of the observations [5].
Reference: [16] <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference-contexts: One goal is to develop an approach that can exploit multiple models simultaneously, where the type of models might be quite distinct. Model types useful for characterizing images in an image sequence might include eigenvector decomposition of sets of images <ref> [16] </ref>, orientation histograms [4], peak temporal frequencies [10], tracked position of objects in the frame, and optic flow field summaries. 2.2 State-based descriptions In previous work [1] we defined gesture to be a sequence of states in a configuration space.
Reference: [17] <author> J. Yamato, J. Ohya, and K. Ishii. </author> <title> Recognizing human action in time-sequential images using hidden markov model. </title> <booktitle> Proc. Comp. Vis. and Pattern Rec., </booktitle> <pages> pages 379-385, </pages> <year> 1992. </year>
Reference-contexts: Yamato et al. <ref> [17] </ref> compute a simple region-based statistic from each frame of image sequences of tennis swings. Sequences of the vector-quantized features are then identified by a trained HMM.
References-found: 17

