URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-93-09.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/biblio_ora?sort_by_author=yes&tailor=1&loc=0&format=ml/ml&keyword=Publications&keyword=WWW_ML&relop=/
Root-URL: 
Email: juffi@ai.univie.ac.at  
Title: The Role of Qualitative Knowledge in Machine Learning  
Author: Johannes Furnkranz 
Note: This research is sponsored by the Austrian Fonds zur Forderung der Wissen-schaftlichen Forschung (FWF) under grant P8756-TEC. Financial support for the Aus-trian Research Institute for Artificial Intelligence is provided by the Austrian Federal Ministry for Science and Research.  
Date: November 31, 1992  
Address: Schottengasse 3 A-1010 Vienna Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: This paper analyzes the important role qualitative knowledge plays in Machine Learning. For this purpose important results from research in the fields approximate theory formation, automated qualitative modeling, learning in plausible domain theories and learning with abstractions are reviewed. The analysis of these approaches shows several of the benefits the use of qualitative knowledge can bring to Machine Learning and also points out important problems that have to be dealt with. The need for qualitative knowledge to keep learning tractable is illustrated with examples from the domain of chess. Finally we make some suggestions for further research based on the shortcomings of previous approaches. 
Abstract-found: 1
Intro-found: 1
Reference: [AGA, 1990] <institution> Working Notes of the AAAI Workshop on Automatic Generation of Approximations and Abstractions, </institution> <address> Boston, Massachusettes, </address> <year> 1990. </year>
Reference: [AI-, 1984] <institution> Artificial Intelligence, </institution> <month> 24, </month> <year> 1984. </year> <title> Special Volume: Qualitative Reasoning About Physical Systems. </title>
Reference-contexts: Among the number of qualitative knowledge representation formalisms that have emerged over the years <ref> [Weld and deKleer, 1990, AI-, 1984, AI-, 1991] </ref> three formalisms have now become "standard": * Qualitative Process Theory [Forbus, 1984] * Qualitative Simulation [Kuipers, 1986] * Reasoning with Confluences [deKleer and Brown, 1984] All of the above systems support some kind of qualitative algebra [Williams, 1988] that allows them to reason
Reference: [AI-, 1991] <institution> Artificial Intelligence, </institution> <month> 51, </month> <year> 1991. </year> <title> Special Volume: Qualitative Reasoning About Physical Systems II. </title>
Reference-contexts: Among the number of qualitative knowledge representation formalisms that have emerged over the years <ref> [Weld and deKleer, 1990, AI-, 1984, AI-, 1991] </ref> three formalisms have now become "standard": * Qualitative Process Theory [Forbus, 1984] * Qualitative Simulation [Kuipers, 1986] * Reasoning with Confluences [deKleer and Brown, 1984] All of the above systems support some kind of qualitative algebra [Williams, 1988] that allows them to reason
Reference: [Angluin and Laird, 1988] <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: Errors in discrete sensing devices are even harder to deal with. Mechanisms for representing uncertainty and noise in data are needed. The effects of noise on input data has been widely studied (see e.g. <ref> [Angluin and Laird, 1988, Clark and Niblett, 1987] </ref>), but the problem of distinguishing noise in the data from rare instances of the target concept has been addressed only recently [Srinivasan et al., 1992] and is mostly based 10 2 THE NEED FOR QUALITATIVE KNOWLEDGE on statistical heuristics.
Reference: [Baker et al., 1987] <author> Michelle Baker, Mark H. Burstein, and Allan M. Collins. </author> <title> Implementing a model of human plausible reasoning. </title> <booktitle> In Proceedings of the 10th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 185-188, </pages> <address> Milano, Italy, </address> <year> 1987. </year>
Reference-contexts: The work of Collins and Michalski [Collins and Michalski, 1989] gives a formal framework for human plausible reasoning. Although the presented formalism has been implemented in several versions <ref> [Dontas and Zemankova, 1988, Baker et al., 1987] </ref> it has not yet been directly used in Machine Learning research. 5 Nevertheless, many of the ideas presented in this chapter are directly influenced by or based on ideas of this fundamental work.
Reference: [Bennett, 1989] <author> Scott W. Bennett. </author> <title> Learning approximate plans for use in the real world. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pages 224-228, </pages> <address> Ithaca, New York, </address> <year> 1989. </year>
Reference-contexts: Research on using rule approximations as reported in [Fawcett, 1989, Knoblock, 1989, Keller, 1988, Cohen, 1990, Tadepalli, 1989] and on qualitative extensions of the representation language <ref> [DeJong, 1989, Widmer, 1992a, Mozetic and Holzbaur, 1991, Bennett, 1989] </ref> in traditional Explanation-Based Learning environments shows that a tradeoff between correctness and efficiency is an important issue. Applications of this finding to induction are still waiting to be investigated.
Reference: [Bergadano and Giordana, 1988] <author> F. Bergadano and A. Giordana. </author> <title> A knowledge intensive approach to concept induction. </title> <booktitle> In Proceedings of the 5th International Conference on Machine Learning, </booktitle> <pages> pages 305-317, </pages> <address> Ann Arbor, Michigan, </address> <year> 1988. </year>
Reference-contexts: In this section approximate background knowledge is used for narrowing down the search space for inductive learning algorithms. 16 3 RULE APPROXIMATION ML-SMART <ref> [Bergadano and Giordana, 1988] </ref> is an algorithm for inducing concept description in first-order logic from specified positive or negative instances. In addition background knowledge can be specified that may be incorrect or incomplete. <p> The set of all such descriptors is used as the language in which the target concept will be induced. [Giordana and Saitta, 1990] use this idea in the inductive relational learning system ML-SMART <ref> [Bergadano and Giordana, 1988] </ref>. Here a first-order logic abstraction theory quite similar to [Mozetic and Holzbaur, 1991] can be specified. In the first abstraction phase ML-SMART generates instance representations at different levels of abstraction as an input to the inductive learning algorithm. <p> Well-known systems that enhance Explanation-Based Learning methods with an inductive learning component are UNIMEM [Lebowitz, 1986] and OCCAM [Pazzani, 1990]. Other approaches can be found in [Flann and Dietterich, 1989] and [Widmer, 1989b]. In Inductive Logic Programming, the systems FOCL [Pazzani and Kibler, 1992] and ML-SMART <ref> [Bergadano and Giordana, 1988] </ref> try to integrate background knowledge into inductive learning methods. DISCIPLE [Kodratoff and Tecuci, 1987] is another integration of inductive and deductive learning methods. In subsequent work learning by analogy was added as an additional means of dealing with imperfect domain theories [Tecuci and Kodratoff, 1990].
Reference: [Bergadano et al., 1989] <author> F. Bergadano, A. Giordana, and S. Ponsero. </author> <title> Deduction in top-down inductive learning. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pages 23-25, </pages> <address> Ithaca, New York, </address> <year> 1989. </year>
Reference-contexts: As the ILP algorithm ML-SMART uses its background knowledge "only" as a powerful means for guiding the search for an inductive hypothesis (see section 3.3), the rules specified as background knowledge need not be complete or consistent. For this reason <ref> [Bergadano et al., 1989] </ref> present an extension of the basic algorithm, where in addition to first-order logic rules, so-called dependencies can be specified. Dependencies are nothing else as a method which enables the user to give hints about what predicates might be relevant for a concept definition.
Reference: [Berliner and Campbell, 1984] <author> Hans Berliner and Murray Campbell. </author> <title> Using chunking to solve chess pawn endgames. </title> <journal> Artificial Intelligence, </journal> <volume> 23 </volume> <pages> 97-120, </pages> <year> 1984. </year>
Reference-contexts: This philosophy has also led to several chess programs that tried to make use of explicit representation for plans and goals. But these approaches were only successful in limited subdo-mains of chess, as e.g. tactical combinations [Wilkins, 1982], pawn endgames <ref> [Berliner and Campbell, 1984] </ref> etc. The main problem was the complexity of the domain. First of all it is very hard to formulate precise rules or plans for chess as adding or removing only one piece on the board can have a significant influence on the outcome of the game.
Reference: [Blumer et al., 1987] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year> <note> 52 REFERENCES </note>
Reference-contexts: The principle that simpler rules, all other things being equal, are more likely to be predictive for future data | known as Occam's razor | has been thoroughly examined <ref> [Blumer et al., 1987] </ref> and widely applied as a preference criterion for rule formation in Machine Learning algorithms [Michalski, 1983]. Nevertheless, the importance of this principle is still underestimated. In most Machine Learning algorithms a correct, complicated rule is preferred over a simple, approximate rule.
Reference: [Bratko et al., 1989] <author> I. Bratko, I. Mozetic, and N. Lavrac. KARDIO: </author> <title> a Study in Deep and Qualitative Knowledge for Expert Systems. </title> <publisher> MIT press, </publisher> <year> 1989. </year>
Reference-contexts: Although chess | being the favorite domain of the author | will appear quite often in this paper, we want to stress that the general principles exemplified by this domain are valid for all complex, intractable domains (e.g. reasoning about physical systems [Weld and deKleer, 1990], medical domains <ref> [Bratko et al., 1989] </ref>, music [Widmer, 1992b], chess [Tadepalli, 1989], [Flann, 1989] etc.). The next three chapters give a review of previous work in Machine Learning on the topics of approximation, learning with qualitative and plausible theories and abstraction. <p> In [Mozetic, 1987a] a system is described which learns rules for the function of 4.1 Qualitative Reasoning about Physical Systems 25 the model components from instances of their behavior. The initial background knowledge consists of a structured model of the heart from the KAR-DIO project <ref> [Bratko et al., 1989] </ref> and some instances of component behavior. Initial data-driven learning generalizes these instances into first-order logic rules. It does so by generalizing a true fact and subsequently specializing it by adding literals and instantiating or unifying variables until no negative instance is covered. <p> Preliminary work in this area has again been done in the KARDIO project <ref> [Bratko et al., 1989] </ref>. As we have seen in section 4.1.3 an MIS-like learning algorithm was able to induce and debug component behaviors. <p> Shallow models consist of rules that are directly applicable to the input data and immediately produce an answer. Deep models can be used to generate shallow level knowledge as has been shown in the KARDIO-project <ref> [Bratko et al., 1989] </ref> or in [Pearce, 1988]. In Mozetic's work on learning in the KARDIO project described in section 4.1.4 the qualitative background knowledge is organized into a hierarchical, deep knowledge base.
Reference: [Bratko et al., 1991] <author> Ivan Bratko, Stephen Muggleton, and Alen Varsek. </author> <title> Learning qualitative models of dynamic systems. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 385-388, </pages> <address> Evanston, Illinois, </address> <year> 1991. </year>
Reference-contexts: We will discuss a simplified version of the QSIM approach to qualitative reasoning in a little more detail in the next section, because of its simplicity and its wide use for Machine Learning research (e.g. <ref> [Coiera, 1989a, Bratko et al., 1991, Varsek, 1992] </ref>). 4.1.2 QSIM Qualitative SIMulation (QSIM) [Kuipers, 1986] is the most recent approach to qualitative reasoning. Its main purpose was to allow simulation | envi-sionment | of processes. <p> A well-known ILP system was then actually used in <ref> [Bratko et al., 1991] </ref>. Golem [Muggleton and Feng, 1990] learns first-order clauses in a very efficient way. A definition of the QSIM theory in first order Horn clauses allows it to be used as background knowledge.
Reference: [Buntine, 1988] <author> Wray L. Buntine. </author> <title> Generalized subsumption and its applications to induction and redundancy. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 149-176, </pages> <year> 1988. </year>
Reference-contexts: A more detailed discussion together with a suggestion for the use of negative training examples can be found in chapter 9 of [Coiera, 1989b]. [Coiera, 1993] has pointed out that the clause generation process in Gen-Model is nothing else than constructing the relative least general generalization (rlgg) in ILP terminology <ref> [Plotkin, 1970, Plotkin, 1971, Buntine, 1988] </ref>. A well-known ILP system was then actually used in [Bratko et al., 1991]. Golem [Muggleton and Feng, 1990] learns first-order clauses in a very efficient way.
Reference: [Carbonell, 1986] <author> Jaime G. Carbonell. </author> <title> Derivational analogy: A theory of reconstructive problem solving and expertise acquisition. </title> <editor> In Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. II, </volume> <pages> pages 371-392. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1986. </year>
Reference-contexts: Up to this work analogy has mainly used heuristic similarity measures <ref> [Russell, 1986b, Carbonell, 1986] </ref> for retrieving previous instances and mapping their features to the current situation.
Reference: [Chase et al., 1989] <author> Melissa P. Chase, Monte Zweben, Richard L. Piazza, John D. Burger, Paul P. Maglio, and Haym Hirsh. </author> <title> Approximating learned search control knowledge. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pages 218-220, </pages> <address> Ithaca, New York, </address> <year> 1989. </year>
Reference-contexts: A similar approach to increasing a problem solver's efficiency while maintaining its effectiveness by dropping conditions can be found in the works of [Zweben and Chase, 1988] and <ref> [Chase et al., 1989] </ref>. The ULS system transforms rules generated by EBL into approximate rules using statistical measures. Rules are generalized by dropping conditions that are true most of the time and do not introduce new variable bindings.
Reference: [Chien, 1989] <author> Steve A. Chien. </author> <title> Using and refining simplifications: Explanation-based learning of plans in intractable domains. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 590-595, </pages> <year> 1989. </year>
Reference-contexts: The hope is that the more o-plans are learned the more alternative moves for both players will be suggested and thus the better the play will be. The work described in <ref> [Chien, 1989] </ref> is very similar to Lazy EBL. While Lazy EBL is designed mainly for the intractability arising from the need to consider all the opponent's possible moves in two-person games like chess, Chien's extension to EBL incrementally approximates and corrects plans in the domain of STRIPS-like planning.
Reference: [Christensen, 1990] <author> Jens Christensen. </author> <title> A hierarchical planner that creates its own hierarchies. </title> <booktitle> In Proceedings of the 9th National Conference on Artificial Intelligence, </booktitle> <pages> pages 1004-1009, </pages> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: Experimental results prove the problem-solving using the learned abstractions to solve more problems than versions using EBL or Prodigy. Pablo <ref> [Christensen, 1990] </ref> is | as a hierarchical planner | quite similar to alpine. It also learns a hierarchy of abstraction spaces, but uses a different strategy. It performs a regression of goals through the operators, thus being able to determine the number of steps required to achieve each goal.
Reference: [Church and Church, 1983] <author> Russell M. Church and Kenneth W. Church. </author> <title> Plans, goals, and search strategies for the selection of a move in chess. </title> <booktitle> In Chess Skill in Man and Machine, chapter 6, </booktitle> <pages> pages 131-156. </pages> <publisher> Springer-Verlag, </publisher> <address> 2 edition, </address> <year> 1983. </year>
Reference-contexts: chosen with a very limited look-ahead. 8 2 THE NEED FOR QUALITATIVE KNOWLEDGE Nevertheless, the quality of play in Blitz Chess is not that different from the quality of play in regular chess as one might expect, given the differences in the amount of time spent for analyzing the positions <ref> [Church and Church, 1983] </ref>. The reason for this is that even in regular chess, experienced players neither examine considerably more moves nor calculate deeper variations than beginners.
Reference: [Clark and Matwin, 1993] <author> Peter Clark and Stan Matwin. </author> <title> Using qualitative models to guide inductive learning. </title> <booktitle> In Proceedings of the 10th International Conference on Machine Learning, </booktitle> <address> Amherst, Massachusetts, </address> <year> 1993. </year> <note> Submitted. </note>
Reference-contexts: New rules are learned by using plausible explanation trees which are stored for further use by incremental induction. This process is described in detail in [Widmer, 1991] While the approaches described above extend Explanation-Based Learning with the use of plausible inferences using qualitative background knowledge, <ref> [Clark and Matwin, 1993] </ref> present an approach using directed dependencies to guide the search for correct rules in inductive learning.
Reference: [Clark and Niblett, 1987] <author> Peter Clark and Tim Niblett. </author> <title> Induction in noisy domains. </title> <editor> In Ivan Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning, </booktitle> <address> Wilmslow, UK, 1987. </address> <publisher> Sigma Press. </publisher>
Reference-contexts: Errors in discrete sensing devices are even harder to deal with. Mechanisms for representing uncertainty and noise in data are needed. The effects of noise on input data has been widely studied (see e.g. <ref> [Angluin and Laird, 1988, Clark and Niblett, 1987] </ref>), but the problem of distinguishing noise in the data from rare instances of the target concept has been addressed only recently [Srinivasan et al., 1992] and is mostly based 10 2 THE NEED FOR QUALITATIVE KNOWLEDGE on statistical heuristics. <p> Having a complete and consistent set of examples | all possible faults have been simulated | the induction algorithm AQR <ref> [Clark and Niblett, 1987] </ref> 7 generates a set of rules that covers 100% of the data. Thus learning is once more used for data compression.
Reference: [Clark and Niblett, 1989] <author> Peter Clark and Tim Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 261-283, </pages> <year> 1989. </year> <note> REFERENCES 53 </note>
Reference-contexts: The background knowledge consists of a qualitative model of a physical system represented with a directed graph whose nodes represent numeric parameters and whose arcs represent directed dependencies between the parameters. The inductive learning algorithm CN2 <ref> [Clark and Niblett, 1989] </ref> is then used to learn rules that are consistent with the qualitative model. The conditions of the learned rules are restricted to be tests on the numeric parameters.
Reference: [Cohen, 1990] <author> William W. Cohen. </author> <title> Learning approximate control rules of high utility. </title> <booktitle> In Proceedings of the 7th International Conference on Machine Learning, </booktitle> <pages> pages 268-276, </pages> <address> Austin, Texas, </address> <year> 1990. </year>
Reference-contexts: Rules covering negative examples are eliminated before this process. The approach is successfully applied to the domain of finding suitable opening bids for the domain of Bridge. Subsequent work <ref> [Cohen, 1990] </ref> addressed both, the utility problem (section 3.4) and the multiple inconsistent explanations problem by combining induction with explanation-based learning. <p> But to gain even more benefit from abstraction techniques, these constraints on the representation language and/or the abstraction mapping should be relaxed and other methods for dealing with this problem should be developed. Research on using rule approximations as reported in <ref> [Fawcett, 1989, Knoblock, 1989, Keller, 1988, Cohen, 1990, Tadepalli, 1989] </ref> and on qualitative extensions of the representation language [DeJong, 1989, Widmer, 1992a, Mozetic and Holzbaur, 1991, Bennett, 1989] in traditional Explanation-Based Learning environments shows that a tradeoff between correctness and efficiency is an important issue.
Reference: [Cohen, 1992] <author> William W. Cohen. </author> <title> Abductive explanation-based learning: A solution to the multiple inconsistent explanation problem. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 167-219, </pages> <year> 1992. </year>
Reference-contexts: This experience has motivated his research with Prodigy where he tried to make sure that "the cumulative benefits of applying the knowledge must outweigh the cumulative costs of testing whether the knowledge is applicable" [Minton, 1990]. This has been known as the utility problem (see also <ref> [Cohen, 1992] </ref> and section 3.5.2). If approximations incrementally approach a correct and consistent domain theory as in the last section, mistakes will always be corrected by either learning new rules as in Lazy EBL or by further specializing the preconditions of learned rules as in Incremental EBL. <p> ULS also keeps a copy of the original rule that can be used to test whether the generalization was bad in case the new rule repeatedly misclas-sifies examples. 3.5.2 Using a set of explanations In <ref> [Cohen, 1992] </ref> the multiple inconsistent explanation problem is addressed by repeatedly marking a rule that gives the highest ratio of newly explained examples to rule size, thus performing a greedy search to cover the set of 20 3 RULE APPROXIMATION examples with a minimum set of rules. <p> The system thus chooses the explanation which maximizes the ratio of newly explained examples to the size of the rule. With this method multiple, possibly inconsistent explanations can be dealt with (see <ref> [Cohen, 1992] </ref>). In addition to this heuristic that | quite similarly to the Prodigy system [Minton, 1990] | increases the convergence rate of learning, the system also tries to learn approximations of expensive rules as another method to avoid swamping.
Reference: [Coiera, 1989a] <author> Enrico Coiera. </author> <title> Generating qualitative models from example be-haviours. </title> <type> DCS Report 8901, </type> <institution> School of Electr. Eng. and Computer Sc., Univ. of New South Wales, </institution> <address> Sydney, Australia, </address> <year> 1989. </year>
Reference-contexts: We will discuss a simplified version of the QSIM approach to qualitative reasoning in a little more detail in the next section, because of its simplicity and its wide use for Machine Learning research (e.g. <ref> [Coiera, 1989a, Bratko et al., 1991, Varsek, 1992] </ref>). 4.1.2 QSIM Qualitative SIMulation (QSIM) [Kuipers, 1986] is the most recent approach to qualitative reasoning. Its main purpose was to allow simulation | envi-sionment | of processes. <p> The proposed negative and positive exceptions can be validated by the user. When he thinks that enough exceptions for component behaviors have been accumulated, the incremental learning procedure can be started again to modify the rule set for the components' behavior. GenModel <ref> [Coiera, 1989a] </ref> is another early system for automated qualitative modeling. As a simplified version of the QSIM qualitative domain theory is used as background knowledge, the learning algorithm is again quite similar to recent approaches in ILP. <p> The learned theories are thus evaluated by their ability to predict observed physical phenomena. This verification-based refinement technique supposedly applies equally well to any form of theory formation in which conjectures of uncertain validity are made. In this respect Phineas is quite similar to GenModel <ref> [Coiera, 1989a] </ref> | where inconsistent rules are deleted and a new, correct rule is learned | and to the incremental theory refinement approaches of section 3.2.2. 4.1.6 Directed Dependencies The monotonicity operators M+ and Mused in QPT and QSIM (see section 4.1.2) are very widely used in qualitative knowledge representations.
Reference: [Coiera, 1989b] <author> Enrico Coiera. </author> <title> Reasoning with Qualitative Disease Histories for Diagnostic Patient Monitoring. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of New South Wales, </institution> <year> 1989. </year>
Reference-contexts: This specific-to-general search strategy is dual to Mozetic's general-to-specific search for forming the initial hypotheses as described above. A more detailed discussion together with a suggestion for the use of negative training examples can be found in chapter 9 of <ref> [Coiera, 1989b] </ref>. [Coiera, 1993] has pointed out that the clause generation process in Gen-Model is nothing else than constructing the relative least general generalization (rlgg) in ILP terminology [Plotkin, 1970, Plotkin, 1971, Buntine, 1988]. A well-known ILP system was then actually used in [Bratko et al., 1991].
Reference: [Coiera, 1992] <author> Enrico Coiera. </author> <title> The qualitative representation of physical systems. </title> <journal> The Knowledge Engineering Review, </journal> <volume> 7(1) </volume> <pages> 55-77, </pages> <year> 1992. </year>
Reference-contexts: In physical systems qualitative differential equations (QDEs) are very important and thus form central notions of most representation languages. We will shortly introduce the three systems following the more elaborate discussion in <ref> [Coiera, 1992] </ref>. For a detailed description consultation of the original papers is recommended. The work described in [deKleer and Brown, 1984] is primarily concerned with modeling the structure of complex objects. <p> QPT is a very general approach to qualitative reasoning about physical systems. This of course has the disadvantage of requiring quite some time to specify a model. A discussion of the relation between the formalisms and their advantages and disadvantages can be found in <ref> [Coiera, 1992] </ref>.
Reference: [Coiera, 1993] <author> Enrico Coiera. </author> <title> Qualitative superposition of unmodelled systems. </title> <note> Submitted to IJCAI-93, </note> <year> 1993. </year>
Reference-contexts: This specific-to-general search strategy is dual to Mozetic's general-to-specific search for forming the initial hypotheses as described above. A more detailed discussion together with a suggestion for the use of negative training examples can be found in chapter 9 of [Coiera, 1989b]. <ref> [Coiera, 1993] </ref> has pointed out that the clause generation process in Gen-Model is nothing else than constructing the relative least general generalization (rlgg) in ILP terminology [Plotkin, 1970, Plotkin, 1971, Buntine, 1988]. A well-known ILP system was then actually used in [Bratko et al., 1991].
Reference: [Collins and Michalski, 1989] <author> Allan Collins and Ryszard S. Michalski. </author> <title> The logic of plausible reasoning: A core theory. </title> <journal> Cognitive Science, </journal> <volume> 13 </volume> <pages> 1-49, </pages> <year> 1989. </year>
Reference-contexts: In this chapter we will consider a different approach to the learning of and with qualitative knowledge: the introduction of explicit language constructs to capture weak inferences and approximations to make the "qual-itativeness" of the domain explicit. The work of Collins and Michalski <ref> [Collins and Michalski, 1989] </ref> gives a formal framework for human plausible reasoning. <p> They are also referred to as qualitative proportionalities [Forbus, 1984] or directed dependencies <ref> [Collins and Michalski, 1989] </ref>. 9 For a criticism of this view see [Hammond et al., 1991]. 30 4 PLAUSIBLE THEORIES The use of directed dependencies requires an ordering relation on the set of qualitative values. <p> From this work the Multistrategy Task-adaptive Learning framework (MTL) [Tecuci and Michalski, 1991] has emerged. MTL is a Multistrategy approach to learning in the Plausible Reasoning framework of <ref> [Collins and Michalski, 1989] </ref>. The approach is adaptive in the sense that it is able to apply different learning strategies for different learning 48 6 MULTISTRATEGY LEARNING tasks. <p> The inferences can be formalized by using constructs from the Plausible Reasoning theory by <ref> [Collins and Michalski, 1989] </ref>, as e.g. determinations (see section 4.2). During the construction and the following generalization of the tree, different kinds of new knowledge can be inferred: new concept definitions, new rules, new facts, abstractions etc.
Reference: [Crawford et al., 1990] <author> J. Crawford, A. Farqhuar, and B. Kuipers. QPC: </author> <title> A compiler from physical models into qualitative differential equations. </title> <booktitle> In Proceedings of the 9th National Conference on Artificial Intelligence, </booktitle> <pages> pages 365-372, </pages> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: Thus it is mainly concerned with the time varying behavior of qualitative representations of functions which then can be used for simulating the behavior of the system. The main difference between QPT (developed for modeling) and QSIM (developed for simulation) is stressed by the Qualitative Process Compiler (QPC) <ref> [Crawford et al., 1990] </ref>, an algorithm that allows transformation of QPT influences into QSIM constraints. In QSIM a model of a physical system is described by a set of continuously differentiable, real valued functions of time.
Reference: [Danyluk, 1987] <author> Andrea Pohoreckyj Danyluk. </author> <title> The use of explanations for similarity-based learning. </title> <booktitle> In Proceedings of the 10th International Joint Conference on Artificial Intelligence, </booktitle> <address> Milano, Italy, </address> <year> 1987. </year>
Reference-contexts: Then a new, maximally specific rule covering the training instance is asserted. Some primitive generalizations are performed, but the generated rules for unproved facts should be fed to an inductive generalization procedure, as soon as enough of them have been collected. The Gemini system <ref> [Danyluk, 1989, Danyluk, 1987] </ref> relies on previous experience in completing partial proof trees in incomplete domains. The 3.5 The Multiple Explanation Problem 19 author proposes to use knowledge of previous proof trees to bias the induction needed to fill in the gaps of incomplete proofs.
Reference: [Danyluk, 1989] <author> Andrea Pohoreckyj Danyluk. </author> <title> Finding new rules for incomplete theories: Explicit biases for induction with contextual information. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pages 34-36, </pages> <address> Ithaca, New York, </address> <year> 1989. </year>
Reference-contexts: Then a new, maximally specific rule covering the training instance is asserted. Some primitive generalizations are performed, but the generated rules for unproved facts should be fed to an inductive generalization procedure, as soon as enough of them have been collected. The Gemini system <ref> [Danyluk, 1989, Danyluk, 1987] </ref> relies on previous experience in completing partial proof trees in incomplete domains. The 3.5 The Multiple Explanation Problem 19 author proposes to use knowledge of previous proof trees to bias the induction needed to fill in the gaps of incomplete proofs.
Reference: [Davies and Russell, 1987] <author> T. R. Davies and S. J. Russell. </author> <title> A logical approach to reasoning by analogy. </title> <booktitle> In Proceedings of the 10th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 264-270, </pages> <address> Milano, Italy, </address> <year> 1987. </year> <note> 54 REFERENCES </note>
Reference-contexts: It is easy to see that if P implies Q it also determines Q, but the reverse is not true. Thus logical implication can be viewed as a special case of determination. <ref> [Davies and Russell, 1987] </ref> give the following formal definition of determinations: 4.2 Determinations 33 Definition 4.1 (Determination) A predicate schema P with sets of free variables x and y determines (denoted as ) a predicate schema Q with sets of free variables x and z when the following holds: P (x; <p> derive the following rules after seeing one instance of each of the two cases: classification ( CurrentPosition, won) :- king_pawn_relation ( CurrentPosition, in_front). classification ( CurrentPosition, draw) :- king_pawn_relation ( CurrentPosition, behind). 4.2.3 Learning with Determinations Determinations have originally been proposed as a "a logical approach to reasoning by analogy" <ref> [Davies and Russell, 1987] </ref>. Up to this work analogy has mainly used heuristic similarity measures [Russell, 1986b, Carbonell, 1986] for retrieving previous instances and mapping their features to the current situation. <p> In subsequent work [Russell, 1987] points out that single-instance generalization as in EBL and reasoning by analogy both can have explanation-based as well as determination-based justifications. For this purpose both | single-instance generalization and analogy | are put in a framework using 34 4 PLAUSIBLE THEORIES determinations <ref> [Davies and Russell, 1987] </ref>. It is shown that strong domain theories, as used traditionally in EBL, overly constrain learning. For any implicative rule there is a corresponding, less specific determination which, when combined with an instance, yields the same result.
Reference: [deGroot, 1965] <author> Adriaan D. </author> <title> deGroot. Thought and Choice in Chess. </title> <publisher> Mouton, </publisher> <address> The Hague, </address> <year> 1965. </year>
Reference-contexts: The reason for this is that even in regular chess, experienced players neither examine considerably more moves nor calculate deeper variations than beginners. The difference is that the knowledge they use in the problem solving process allows them to compentently and efficiently select the "right" moves for further examination <ref> [deGroot, 1965] </ref>.
Reference: [DeJong, 1989] <author> G. DeJong. </author> <title> Explanation-based learning with plausible inferencing. </title> <booktitle> In Proceedings of the 4th European Working Session on Learning, </booktitle> <pages> pages 1-10, </pages> <address> Montpellier, France, </address> <year> 1989. </year>
Reference-contexts: PieceAdvantage as well as ChancesToWin are functions over a domain of ordered symbols, such as PawnUp, ExchangeUp or RookDown and +, , =, or +. 10 Plausible Inferences | i.e. educated, abstract guesses at why a given proposition is likely to be true | are proposed in <ref> [DeJong, 1989] </ref>. Two problems appear: Plausible inferences are uncertain and imprecise. The problem of imprecision is solved by a procedure outside of the EBL-scope that watches and remembers the amount of changes in the quantitative values of a qualitative variable and uses them for interpolation in future problems. <p> Research on using rule approximations as reported in [Fawcett, 1989, Knoblock, 1989, Keller, 1988, Cohen, 1990, Tadepalli, 1989] and on qualitative extensions of the representation language <ref> [DeJong, 1989, Widmer, 1992a, Mozetic and Holzbaur, 1991, Bennett, 1989] </ref> in traditional Explanation-Based Learning environments shows that a tradeoff between correctness and efficiency is an important issue. Applications of this finding to induction are still waiting to be investigated.
Reference: [deKleer and Brown, 1984] <author> J. deKleer and J. S. Brown. </author> <title> A qualitative physics based on confluences. </title> <journal> Artificial Intelligence, </journal> <volume> 24 </volume> <pages> 7-83, </pages> <year> 1984. </year>
Reference-contexts: Among the number of qualitative knowledge representation formalisms that have emerged over the years [Weld and deKleer, 1990, AI-, 1984, AI-, 1991] three formalisms have now become "standard": * Qualitative Process Theory [Forbus, 1984] * Qualitative Simulation [Kuipers, 1986] * Reasoning with Confluences <ref> [deKleer and Brown, 1984] </ref> All of the above systems support some kind of qualitative algebra [Williams, 1988] that allows them to reason with qualitative function descriptions. In physical systems qualitative differential equations (QDEs) are very important and thus form central notions of most representation languages. <p> In physical systems qualitative differential equations (QDEs) are very important and thus form central notions of most representation languages. We will shortly introduce the three systems following the more elaborate discussion in [Coiera, 1992]. For a detailed description consultation of the original papers is recommended. The work described in <ref> [deKleer and Brown, 1984] </ref> is primarily concerned with modeling the structure of complex objects. The basic philosophy of this approach is that no information about a model's function should be included into the representation.
Reference: [Dontas and Zemankova, 1988] <author> K. Dontas and M. Zemankova. APPLAUS: </author> <title> An implementation of the collins-michalski theory of plausible reasoning. </title> <booktitle> In Proceedings of the 3rd International Symposium on Methodologies fo Intelligent Systems, </booktitle> <address> Torino, Italy, </address> <year> 1988. </year>
Reference-contexts: The work of Collins and Michalski [Collins and Michalski, 1989] gives a formal framework for human plausible reasoning. Although the presented formalism has been implemented in several versions <ref> [Dontas and Zemankova, 1988, Baker et al., 1987] </ref> it has not yet been directly used in Machine Learning research. 5 Nevertheless, many of the ideas presented in this chapter are directly influenced by or based on ideas of this fundamental work.
Reference: [Doyle, 1986] <author> R. J. Doyle. </author> <title> Constructing and refining causal explanations from an inconsistent domain theory. </title> <booktitle> In Proceedings of the 5th National Conference on Artificial Intelligence, </booktitle> <address> Philadelphia, PA, </address> <year> 1986. </year>
Reference-contexts: The next three chapters give a review of previous work in Machine Learning on the topics of approximation, learning with qualitative and plausible theories and abstraction. Very loosely these chapters correspond to the three different kinds of abstraction as seen in <ref> [Doyle, 1986] </ref>: Approximation is the simplification of the domain theory through assump tions that some condition holds or some constraint is satisfied. Qualitization is the mapping of a continuous description of a domain into a discrete one. Aggregation is the subsumption of complex structures under simpler struc tures. <p> Thus the approximation might be viewed as an exact theory of a subdomain of the original problem where several additional assumptions simplify the problem. Thus most authors <ref> [Doyle, 1986, Ellman, 1988] </ref> define approximation in a way similar to this: Definition 3.1 (Approximation) An approximation T A of a domain theory T may be viewed as making useful simplifying assumptions about the domain of T and then reformulating T in the new simplified subdomain. <p> Explicitly representing weaker forms of inference should increase understandability and expressive power. Analogously to <ref> [Doyle, 1986] </ref> we will call this process qualitization. But while Doyle restricts his definition of the mapping of a continuous domain description onto a discrete representation, we want to stress that our definition is meant to capture all forms of language constructs that capture some kind of intuitive, plausible reasoning. <p> The basic idea behind this is to map the continuous physical systems (mostly modeled by differential equation systems) into a discrete representation that captures the qualitative behavior of the systems, but disregards quantitative details. <ref> [Doyle, 1986] </ref> calls this process qualitization.
Reference: [Drastal et al., 1989] <author> G. Drastal, G. Czako, and S. Raatz. </author> <title> Induction in an abstraction space. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 708-712, </pages> <address> Detroit, Michigan, </address> <year> 1989. </year>
Reference-contexts: Classical EBL falls out as a special case of this framework, as the operational-ity criterion can be formulated by adding a predicate abstraction operator for each operational predicate that renames the predicate to itself and deletes none of the arguments. In <ref> [Drastal et al., 1989] </ref> abstraction is used to bias induction in an attribute-value domain language. <p> Here a first-order logic abstraction theory quite similar to [Mozetic and Holzbaur, 1991] can be specified. In the first abstraction phase ML-SMART generates instance representations at different levels of abstraction as an input to the inductive learning algorithm. This differs from the approach of <ref> [Drastal et al., 1989] </ref> where a set of maximally abstracted descriptors is used. Various experiments with induction on different abstraction levels confirmed that using the most abstract representation level minimizes the combined costs of abstraction and induction. <p> The logical next step is to further extend the representational power of the induced concepts by allowing qualitative language constructs or by relaxing the correctness constraint in order to generate more efficient rules. Preliminary research in [Giordana and Saitta, 1990] and <ref> [Drastal et al., 1989] </ref> has suggested that finding an appropriate level of abstraction for the representation of the target concept can facilitate learning. The false proof problem is an important issue when using abstractions for more efficient problem-solving behavior.
Reference: [Ellman, 1988] <author> Tom Ellman. </author> <title> Approximate theory formation: An explanation-based approach. </title> <booktitle> In Proceedings of the 7th National Conference on Artificial Intelligence, </booktitle> <pages> pages 570-574, </pages> <address> Minneapolis, Minnesota, </address> <year> 1988. </year>
Reference-contexts: Thus the approximation might be viewed as an exact theory of a subdomain of the original problem where several additional assumptions simplify the problem. Thus most authors <ref> [Doyle, 1986, Ellman, 1988] </ref> define approximation in a way similar to this: Definition 3.1 (Approximation) An approximation T A of a domain theory T may be viewed as making useful simplifying assumptions about the domain of T and then reformulating T in the new simplified subdomain. <p> This chapter will be concerned with the learning of approximate rules. As we have seen in the preceding section, we have to identify useful simplifying assumptions for finding good approximate theories. 3.2.1 Approximation Hierarchies Ellman <ref> [Ellman, 1988] </ref> was the first to stress the importance of identifying appropriate simplifying assumptions for approximate theories. He views finding the right simplifying assumptions as the main goal of explanation-based learning of approximate theories. Ellman's system POLYANNA thus considers approximation as a search through the space of possible simplifying assumptions.
Reference: [Falkenhainer et al., 1989] <author> Brian Falkenhainer, Kenneth D. Forbus, and Dedre Genter. </author> <title> The structure mapping engine: Algorithm and examples. </title> <journal> Artificial Intelligence, </journal> <volume> 41(1), </volume> <year> 1989. </year>
Reference-contexts: No information about the goal of the mapping or about the problem to solve is involved. 9 Learning occurs by mapping a qualitative explanation of a physical phenomenon onto a new domain. Falkenhaimer's research on the Structural Mapping Engine (SME) <ref> [Falkenhainer et al., 1989] </ref> and its use in Phineas [Falkenhainer, 1990] is an attempt to implement the Structural Mapping Theory developed in [Forbus and Gentner, 1986]. Phineas is an implemented integration of Gentner's SMT and Forbus' QPT.
Reference: [Falkenhainer, 1987] <author> Brian Falkenhainer. </author> <title> Scientific theory formation through analogical inference. </title> <booktitle> In Proceedings of the 4th International Workshop on Machine Learning, </booktitle> <pages> pages 218-229, </pages> <address> Irvine, California, </address> <year> 1987. </year>
Reference-contexts: This prior situation is explained using the domain theory and the explanation is mapped to the new situation with SME. The consistency of the resulting models can be verified by comparing their predictions to the current situation. In <ref> [Falkenhainer, 1987] </ref> a time based planning system can then be used to extend the consistent theory through further experimentation.
Reference: [Falkenhainer, 1990] <author> Brian Falkenhainer. </author> <title> A unified approach to explanation and theory formation. </title> <editor> In J. Shrager and P. Langley, editors, </editor> <title> Computational Models of Discovery and Theory Formation. </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1990. </year>
Reference-contexts: No information about the goal of the mapping or about the problem to solve is involved. 9 Learning occurs by mapping a qualitative explanation of a physical phenomenon onto a new domain. Falkenhaimer's research on the Structural Mapping Engine (SME) [Falkenhainer et al., 1989] and its use in Phineas <ref> [Falkenhainer, 1990] </ref> is an attempt to implement the Structural Mapping Theory developed in [Forbus and Gentner, 1986]. Phineas is an implemented integration of Gentner's SMT and Forbus' QPT. Whenever the system is not able to explain an encountered physical process it tries to find a solution by analogical inference.
Reference: [Fawcett, 1989] <author> Tom E. Fawcett. </author> <title> Learning from plausible explanations. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pages 37-39, </pages> <address> Ithaca, New York, </address> <year> 1989. </year> <note> REFERENCES 55 </note>
Reference-contexts: Similarly, experiments can be conducted to find the only completion of an incomplete proof. Another approach is tried in <ref> [Fawcett, 1989] </ref>. Even for explanation-based learning in complete and consistent domains, the importance of selecting the "best" explanation to derive the most accurate rule has been recognized [Pazzani, 1988]. Fawcett's work tries to transform this approach to EBL systems in incomplete theory domains. <p> But to gain even more benefit from abstraction techniques, these constraints on the representation language and/or the abstraction mapping should be relaxed and other methods for dealing with this problem should be developed. Research on using rule approximations as reported in <ref> [Fawcett, 1989, Knoblock, 1989, Keller, 1988, Cohen, 1990, Tadepalli, 1989] </ref> and on qualitative extensions of the representation language [DeJong, 1989, Widmer, 1992a, Mozetic and Holzbaur, 1991, Bennett, 1989] in traditional Explanation-Based Learning environments shows that a tradeoff between correctness and efficiency is an important issue.
Reference: [Feng, 1991] <author> Cao Feng. </author> <title> Inducing temporal fault diagnostic rules from a qualitative model. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 403-406, </pages> <address> Evanston, Illinois, </address> <year> 1991. </year>
Reference-contexts: Pearce's method using AQ-type induction, however, was not able to diagnose faults affected by the history of certain components, a type of failure that actually can occur in the satellite's power subsystem. 8 <ref> [Feng, 1991] </ref> shows that this problem can be solved by using learning algorithms with greater expressive power, i.e. algorithms that can induce first-order logic descriptions of the faults. The author uses the ILP algorithm Golem [Muggleton and Feng, 1990] in the task mentioned above.
Reference: [Fikes and Nilsson, 1971] <author> R. E. Fikes and N. Nilsson. </author> <title> STRIPS: A new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2(3-4):189-208, </volume> <year> 1971. </year>
Reference-contexts: The abstraction mapping thus is reminescent of some of the ideas of chapter 3. alpine automatically learns its abstraction hierarchies by generating an ordered graph with the literals used in the STRIPS-like operators <ref> [Fikes and Nilsson, 1971] </ref> as nodes. The directed edges of a graph are inserted among literals of the add- and delete-lists, and between them and the literals of the precondition of the same operator.
Reference: [Flann and Dietterich, 1989] <author> Nicholas S. Flann and Thomas G. Dietterich. </author> <title> A study of explanation-based methods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 187-266, </pages> <year> 1989. </year>
Reference-contexts: Multistrategy Learning has been mostly investigated in the context of integrating inductive and deductive learning methods. Well-known systems that enhance Explanation-Based Learning methods with an inductive learning component are UNIMEM [Lebowitz, 1986] and OCCAM [Pazzani, 1990]. Other approaches can be found in <ref> [Flann and Dietterich, 1989] </ref> and [Widmer, 1989b]. In Inductive Logic Programming, the systems FOCL [Pazzani and Kibler, 1992] and ML-SMART [Bergadano and Giordana, 1988] try to integrate background knowledge into inductive learning methods. DISCIPLE [Kodratoff and Tecuci, 1987] is another integration of inductive and deductive learning methods.
Reference: [Flann, 1989] <author> Nicholas S. Flann. </author> <title> Learning appropriate abstractions for planning in formation problems. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pages 235-239, </pages> <address> Ithaca, New York, </address> <year> 1989. </year>
Reference-contexts: the author | will appear quite often in this paper, we want to stress that the general principles exemplified by this domain are valid for all complex, intractable domains (e.g. reasoning about physical systems [Weld and deKleer, 1990], medical domains [Bratko et al., 1989], music [Widmer, 1992b], chess [Tadepalli, 1989], <ref> [Flann, 1989] </ref> etc.). The next three chapters give a review of previous work in Machine Learning on the topics of approximation, learning with qualitative and plausible theories and abstraction. <p> This approach is further pursued in the system Place described in <ref> [Flann, 1989] </ref>. The author observes several difficulties with common approaches to abstraction via approximation (see section 5.4) in solving formation problems 18 .
Reference: [Flann, 1990] <author> Nicholas S. Flann. </author> <title> Applying abstraction and simplification to learn in intractable domains. </title> <booktitle> In Proceedings of the 7th International Conference on Machine Learning, </booktitle> <pages> pages 277-285, </pages> <address> Austin, Texas, </address> <year> 1990. </year>
Reference-contexts: For these reasons chess has been called "the drosophila 2 of AI" alluding to the important role this comparably simple animal played as the object of early research in genetics [McCarthy, 1990]. 3 In particular chess has become a standard example <ref> [Tadepalli, 1986, Tadepalli, 1989, Flann, 1990] </ref> for an intractable domain. 2.1 Knowledge is intractable Many applications of rule-based production systems assume that the underlying theory is complete and consistent. However, real-world domains usually can't be formalized in a nice and neat way [Rajamoney and DeJong, 1987]. <p> The problem of goal interaction is dealt with through a process called visualization. Place looks for operators that maintain, destroy or achieve a goal. Complex expressions for the achievement of multiple goals are analyzed and compiled by doing an exhaustive case analysis <ref> [Flann, 1990] </ref>.
Reference: [Forbus and Gentner, 1986] <author> Kenneth D. Forbus and Dedre Gentner. </author> <title> Learning physical domains: Toward a theoretical framework. </title> <editor> In Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. II, chapter 12, </volume> <pages> pages 311-348. </pages> <publisher> Morgan Kauf-mann, </publisher> <address> Los Altos, California, </address> <year> 1986. </year>
Reference-contexts: Golem succeeds in 29 out of 33 induction experiments. 4 faults could not be detected by induced rules in certain conditions, e.g. a solar panel in eclipse. 4.1.5 Learning with Qualitative Domain Models Several approaches exist for learning with qualitative background theory. In <ref> [Forbus and Gentner, 1986] </ref> a theoretical framework is given by Ken Forbus and Dedre Gentner as an attempt to combine Qualitative Process Theory [Forbus, 1984] and Structure Mapping Theory [Gentner, 1983]. [Forbus and Gentner, 1986] describe a coupling of the two systems: QPT is used to model portions of people's physical <p> In <ref> [Forbus and Gentner, 1986] </ref> a theoretical framework is given by Ken Forbus and Dedre Gentner as an attempt to combine Qualitative Process Theory [Forbus, 1984] and Structure Mapping Theory [Gentner, 1983]. [Forbus and Gentner, 1986] describe a coupling of the two systems: QPT is used to model portions of people's physical knowledge, and SMT is used for reasoning by analogy, i.e. it gives a theory of how a known solution to a problem can be mapped onto a "similar" problem. <p> Falkenhaimer's research on the Structural Mapping Engine (SME) [Falkenhainer et al., 1989] and its use in Phineas [Falkenhainer, 1990] is an attempt to implement the Structural Mapping Theory developed in <ref> [Forbus and Gentner, 1986] </ref>. Phineas is an implemented integration of Gentner's SMT and Forbus' QPT. Whenever the system is not able to explain an encountered physical process it tries to find a solution by analogical inference. First SME is used to retrieve a previous experience matching the current situation.
Reference: [Forbus, 1984] <author> Kenneth D. Forbus. </author> <title> Qualitative process theory. </title> <journal> Artificial Intelligence, </journal> <volume> 24 </volume> <pages> 85-169, </pages> <year> 1984. </year>
Reference-contexts: Among the number of qualitative knowledge representation formalisms that have emerged over the years [Weld and deKleer, 1990, AI-, 1984, AI-, 1991] three formalisms have now become "standard": * Qualitative Process Theory <ref> [Forbus, 1984] </ref> * Qualitative Simulation [Kuipers, 1986] * Reasoning with Confluences [deKleer and Brown, 1984] All of the above systems support some kind of qualitative algebra [Williams, 1988] that allows them to reason with qualitative function descriptions. <p> While this approach has been proven useful in structured domains as e.g. circuitry, it has its difficulties in representing continuously varying systems. In the Qualitative Process Theory <ref> [Forbus, 1984] </ref>, on the other hand, the central notion is a process. Objects consist of a view | the processes and their relations to each other | and a set of influences that are used to model simple qualitative relationships of functions, such as monotonicity, derivation, addition etc. <p> In [Forbus and Gentner, 1986] a theoretical framework is given by Ken Forbus and Dedre Gentner as an attempt to combine Qualitative Process Theory <ref> [Forbus, 1984] </ref> and Structure Mapping Theory [Gentner, 1983]. [Forbus and Gentner, 1986] describe a coupling of the two systems: QPT is used to model portions of people's physical knowledge, and SMT is used for reasoning by analogy, i.e. it gives a theory of how a known solution to a problem can <p> They are also referred to as qualitative proportionalities <ref> [Forbus, 1984] </ref> or directed dependencies [Collins and Michalski, 1989]. 9 For a criticism of this view see [Hammond et al., 1991]. 30 4 PLAUSIBLE THEORIES The use of directed dependencies requires an ordering relation on the set of qualitative values.
Reference: [Gentner and Landers, 1985] <author> Derdre Gentner and R. Landers. </author> <title> Analogical reminding: A good match is hard to find. </title> <booktitle> In Proceedings of the International Conference on Systems, Man and Cybernetics, </booktitle> <pages> pages 76-79, </pages> <address> Tucson, Arizona, </address> <year> 1985. </year>
Reference-contexts: Dedre Gentner views analogy as a mere comparison between two structural representation without any functionality in the mapping process <ref> [Gentner and Landers, 1985] </ref>. This loosely corresponds to the NFIS principle we have discussed in section 4.1.1.
Reference: [Gentner, 1983] <author> Dedre Gentner. Structure-mapping: </author> <title> A theoretical framework fo analogy. </title> <journal> Cognitive Science, </journal> <volume> 7(2) </volume> <pages> 155-170, </pages> <year> 1983. </year>
Reference-contexts: In [Forbus and Gentner, 1986] a theoretical framework is given by Ken Forbus and Dedre Gentner as an attempt to combine Qualitative Process Theory [Forbus, 1984] and Structure Mapping Theory <ref> [Gentner, 1983] </ref>. [Forbus and Gentner, 1986] describe a coupling of the two systems: QPT is used to model portions of people's physical knowledge, and SMT is used for reasoning by analogy, i.e. it gives a theory of how a known solution to a problem can be mapped onto a "similar" problem.
Reference: [Giordana and Saitta, 1990] <author> A. Giordana and L. Saitta. </author> <title> Abstraction: A general framework for learning. </title> <booktitle> In Working Notes of the AAAI Workshop on Automatic Generation of Approximations and Abstractions, </booktitle> <pages> pages 245-256, </pages> <address> Boston, Massachusettes, </address> <year> 1990. </year>
Reference-contexts: The set of all such descriptors is used as the language in which the target concept will be induced. <ref> [Giordana and Saitta, 1990] </ref> use this idea in the inductive relational learning system ML-SMART [Bergadano and Giordana, 1988]. Here a first-order logic abstraction theory quite similar to [Mozetic and Holzbaur, 1991] can be specified. <p> The logical next step is to further extend the representational power of the induced concepts by allowing qualitative language constructs or by relaxing the correctness constraint in order to generate more efficient rules. Preliminary research in <ref> [Giordana and Saitta, 1990] </ref> and [Drastal et al., 1989] has suggested that finding an appropriate level of abstraction for the representation of the target concept can facilitate learning. The false proof problem is an important issue when using abstractions for more efficient problem-solving behavior.
Reference: [Giordana et al., 1991] <author> A. Giordana, L. Saitta, and D. Roverso. </author> <title> Abstracting concepts with inverse resolution. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 142-146, </pages> <address> Evanston, Illinois, </address> <year> 1991. </year> <note> 56 REFERENCES </note>
Reference-contexts: Various experiments with induction on different abstraction levels confirmed that using the most abstract representation level minimizes the combined costs of abstraction and induction. Besides, the results achieved at the most abstract level appeared to be most understandable for human domain experts. <ref> [Giordana et al., 1991] </ref> put this work into the ILP framework of Inverse Resolution [Muggleton and Buntine, 1988]. A non-generalizing absorption operator, a similar variant of inter-construction, and an additional mechanism, term abstraction, are introduced for this purpose.
Reference: [Giunchiglia and Walsh, 1989] <author> Fausto Giunchiglia and Toby Walsh. </author> <title> Abstract theorem proving. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 372-377, </pages> <year> 1989. </year>
Reference-contexts: Intuitively, abstraction can be described as the process of mapping a representation of a problem into a new representation which reduces complexity and preserves certain desirable properties like e.g. that a solution in the abstract space must be useful for solving the ground problem as well <ref> [Giunchiglia and Walsh, 1989] </ref>. Several formal definitions try to capture this intuitive idea. [Plaisted, 1981] defines a class of functions called abstraction mappings for this purpose. <p> Solutions at abstract levels may not exist though, because they may require some of the details that are ignored at that level. <ref> [Giunchiglia and Walsh, 1989] </ref> show a close relationship between the upward-solution property and the false-proof problem: The authors prove that for every mapping between two theories 13 that has the upward-solution property, there is always a set of consistent axioms whose abstraction is inconsistent, i.e. that they cause the false proof <p> As most systems use some form of first-order predicate calculus as a representation formalisms, Plaisted's definition given above is sufficient for our purposes. For a more general account of abstractions see [Giunchiglia and Walsh, 1992b]. 14 <ref> [Giunchiglia and Walsh, 1989] </ref> propose a solution for this problem by introducing an ordering according to relative "weakness" of abstraction spaces.
Reference: [Giunchiglia and Walsh, 1992a] <author> Fausto Giunchiglia and Toby Walsh. </author> <title> Theories of abstraction: A historical perspective. </title> <booktitle> In AAAI-92 Workshop on Approximation and Abstraction of Computational Theories, </booktitle> <address> San Jose, California, </address> <year> 1992. </year>
Reference-contexts: A historical account of the use of abstractions in various domains can be found in <ref> [Giunchiglia and Walsh, 1992a] </ref>.
Reference: [Giunchiglia and Walsh, 1992b] <author> Fausto Giunchiglia and Toby Walsh. </author> <title> A theory of abstraction. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> pages 323-389, </pages> <year> 1992. </year>
Reference-contexts: As most systems use some form of first-order predicate calculus as a representation formalisms, Plaisted's definition given above is sufficient for our purposes. For a more general account of abstractions see <ref> [Giunchiglia and Walsh, 1992b] </ref>. 14 [Giunchiglia and Walsh, 1989] propose a solution for this problem by introducing an ordering according to relative "weakness" of abstraction spaces.
Reference: [Hammond et al., 1991] <author> Kristian J. Hammond, Colleen M. Seifert, and Kenneth C. Gray. </author> <title> Functionality in analogical transfer: A hard match is good to find. </title> <journal> The Journal of the Learning Sciences, </journal> <volume> 1(2) </volume> <pages> 111-152, </pages> <year> 1991. </year>
Reference-contexts: They are also referred to as qualitative proportionalities [Forbus, 1984] or directed dependencies [Collins and Michalski, 1989]. 9 For a criticism of this view see <ref> [Hammond et al., 1991] </ref>. 30 4 PLAUSIBLE THEORIES The use of directed dependencies requires an ordering relation on the set of qualitative values.
Reference: [Hammond, 1990] <author> Kristian J. Hammond. </author> <title> Explaining and repairing plans that fail. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 45(1-2), </pages> <year> 1990. </year>
Reference-contexts: In their way of incrementally approaching a target concept the systems are quite similar to approaches that repair plans from failure <ref> [Hammond, 1990] </ref>. 3.3 Approximate Background Knowledge The algorithms of the last section try to make simplifying assumptions to speed up the learning process.
Reference: [Hsu et al., 1990] <author> F.-h. Hsu, T. S. Anantharaman, M. S. Campbell, and A. Nowatzyk. </author> <title> Deep thought. </title> <editor> In T. Anthony Marsland and Jonathan Scha-effer, editors, </editor> <title> Computers, Chess, </title> <journal> and Cognition, </journal> <pages> pages 55-78. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: It is significant that none of today's leading programs are able to learn from their faults. 1 1 In Deep Thought the weights of the parameters of the evaluation function have been tuned using a simple reinforcement algorithm <ref> [Hsu et al., 1990] </ref>, but the values of the parameters are not changed during tournament play.
Reference: [Hsu, 1987] <author> F.-h. Hsu. </author> <title> A two-million moves/s CMOS single-chip chess move generator. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 22(5) </volume> <pages> 841-846, </pages> <year> 1987. </year>
Reference-contexts: In this respect the basic ideas of today's chess programs have not changed considerably since the proposal of the MiniMax algorithm by Claude Shannon in 1950 [Shannon, 1950]. Improvements in hardware <ref> [Hsu, 1987] </ref> and in the efficiency of the MiniMaxing method | especially through the use of Alpha-Beta Pruning [Knuth and Moore, 1975] | has led to the chess playing programs of today that are able to challenge Grandmasters and maybe in the near future even the World Champion.
Reference: [Keller, 1987] <author> Richard M. Keller. </author> <title> Concept learning in context. </title> <booktitle> In Proceedings of the 4th International Workshop on Machine Learning, </booktitle> <address> Irvine, California, </address> <year> 1987. </year>
Reference-contexts: Several heuristics are suggested to guide the induction, e.g. eliminating features that already appear in other parts of the proof of an example and eliminating features that generally appear with a high frequency. In [Keller, 1988] the MetaLEX system <ref> [Keller, 1987] </ref> has been extended to deliberately introduce approximations. Two operators | Truify and Falsify | replace arbitrary subexpressions of initially correct concept descriptions with the atoms True and False.
Reference: [Keller, 1988] <author> Richard M. Keller. </author> <title> Learning approximate concept descriptions. </title> <type> Technical Report KSL-88-57, </type> <institution> Stanford University, Knowledge Systems Laboratory, Stanford, California, </institution> <year> 1988. </year> <note> Reprinted in [AGA, </note> <year> 1990]. </year>
Reference-contexts: Several heuristics are suggested to guide the induction, e.g. eliminating features that already appear in other parts of the proof of an example and eliminating features that generally appear with a high frequency. In <ref> [Keller, 1988] </ref> the MetaLEX system [Keller, 1987] has been extended to deliberately introduce approximations. Two operators | Truify and Falsify | replace arbitrary subexpressions of initially correct concept descriptions with the atoms True and False. <p> This is very similar to the approach taken in <ref> [Keller, 1988] </ref> (see section 3.5.1). Learning occurs in SOAR by converting the subgoal-based search into shallow rules [Rosenbloom and Newell, 1986, Rosenbloom and Laird, 1986]. Thus abstracted subgoals are simply ignored and approximate search control rules are learned. 16 . <p> But to gain even more benefit from abstraction techniques, these constraints on the representation language and/or the abstraction mapping should be relaxed and other methods for dealing with this problem should be developed. Research on using rule approximations as reported in <ref> [Fawcett, 1989, Knoblock, 1989, Keller, 1988, Cohen, 1990, Tadepalli, 1989] </ref> and on qualitative extensions of the representation language [DeJong, 1989, Widmer, 1992a, Mozetic and Holzbaur, 1991, Bennett, 1989] in traditional Explanation-Based Learning environments shows that a tradeoff between correctness and efficiency is an important issue.
Reference: [Klein and Finin, 1987] <author> David Klein and Tim Finin. </author> <title> What's in a deep model? A characterization of knowledge depth in intelligent safety systems. </title> <booktitle> In Proceedings of the 10th International Joint Conference on Artificial Intelligence, </booktitle> <address> Milano, Italy, </address> <year> 1987. </year>
Reference-contexts: Deep models on the other hand are highly structured and modular. Reasoning in models of this kind is much more transparent and understandable, but much less efficient as it requires a more sophisticated control structure. <ref> [Klein and Finin, 1987] </ref> give an operational definition of the deeper-than relation for domain models: Definition 5.1 (Deepness) Model M 1 is deeper than model M 2 if there exists some implicit knowledge in M 2 which is explicitly represented or computed in M 1 .
Reference: [Knoblock, 1989] <author> Craig A. Knoblock. </author> <title> Learning hierarchies of abstraction spaces. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pages 241-245, </pages> <address> Ithaca, New York, </address> <year> 1989. </year> <note> REFERENCES 57 </note>
Reference-contexts: concern in the next sections will be the use of abstractions in the context of learning. 5.4 Abstraction by Approximation Knoblock's work on a problem solver that uses hierarchies finally led to the development of an algorithm that allows the system to automatically generate the hierarchies needed for efficient problem-solving <ref> [Knoblock, 1989] </ref>. 13 They do not restrict themselves to predicate mappings, but prove their results for general functions that map representation languages, axioms and/or inference rules. As most systems use some form of first-order predicate calculus as a representation formalisms, Plaisted's definition given above is sufficient for our purposes. <p> But to gain even more benefit from abstraction techniques, these constraints on the representation language and/or the abstraction mapping should be relaxed and other methods for dealing with this problem should be developed. Research on using rule approximations as reported in <ref> [Fawcett, 1989, Knoblock, 1989, Keller, 1988, Cohen, 1990, Tadepalli, 1989] </ref> and on qualitative extensions of the representation language [DeJong, 1989, Widmer, 1992a, Mozetic and Holzbaur, 1991, Bennett, 1989] in traditional Explanation-Based Learning environments shows that a tradeoff between correctness and efficiency is an important issue.
Reference: [Knoblock, 1990a] <author> Craig A. Knoblock. </author> <title> Abstracting the tower of hanoi. </title> <booktitle> In Working Notes of the AAAI Workshop on Automatic Generation of Approximations and Abstractions, </booktitle> <pages> pages 13-23, </pages> <address> Boston, Massachusettes, </address> <year> 1990. </year>
Reference-contexts: a problem in successive abstraction levels by first working on the parts of the problem that require the most steps. alpine on the other hand tries to first solve the parts of the problem that can be left unchanged 15 For more details on the planning aspects of alpine see <ref> [Knoblock, 1990a] </ref>. 42 5 DEEP THEORIES AND ABSTRACTIONS while the remaining subproblems are solved. In [Unruh and Rosenbloom, 1989] possibilities for the use of abstraction in the SOAR system [Laird et al., 1987] are suggested.
Reference: [Knoblock, 1990b] <author> Craig A. Knoblock. </author> <title> Learning abstraction hierarchies for problem solving. </title> <booktitle> In Proceedings of the 9th National Conference on Artificial Intelligence, </booktitle> <pages> pages 923-928, </pages> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: Knoblock calls the latter requirement | that the refinement of an abstract plan leaves the truth value of every literal in the abstract plan unchanged | the ordered monotonicity property <ref> [Knoblock, 1990b] </ref>, which is a special case of the downward solution property. The hierarchy of abstraction spaces in alpine is formed by removing successive classes of literals, such that each abstraction space is an approximation of the original problem space. <p> Many systems try to solve this problem by restricting the representation languages in a way that does not al 50 7 CONCLUSION low the problem to arise. The downward solution property [Tenenberg, 1987] or the ordered monotonicity property <ref> [Knoblock, 1990b] </ref> are examples of this approach (see section 5.2). But to gain even more benefit from abstraction techniques, these constraints on the representation language and/or the abstraction mapping should be relaxed and other methods for dealing with this problem should be developed.
Reference: [Knuth and Moore, 1975] <author> Donald E. Knuth and R. W. Moore. </author> <title> An analysis of alpha-beta pruning. </title> <journal> Artificial Intelligence, </journal> <volume> 6(4) </volume> <pages> 293-326, </pages> <year> 1975. </year>
Reference-contexts: In this respect the basic ideas of today's chess programs have not changed considerably since the proposal of the MiniMax algorithm by Claude Shannon in 1950 [Shannon, 1950]. Improvements in hardware [Hsu, 1987] and in the efficiency of the MiniMaxing method | especially through the use of Alpha-Beta Pruning <ref> [Knuth and Moore, 1975] </ref> | has led to the chess playing programs of today that are able to challenge Grandmasters and maybe in the near future even the World Champion.
Reference: [Kodratoff and Tecuci, 1987] <editor> Yves Kodratoff and Gheorghe Tecuci. DISCIPLE-1: </editor> <title> Interactive apprentice system in weak theory fields. </title> <booktitle> In Proceedings of the 10th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 271-273, </pages> <address> Milano, Italy, </address> <year> 1987. </year>
Reference-contexts: Other approaches can be found in [Flann and Dietterich, 1989] and [Widmer, 1989b]. In Inductive Logic Programming, the systems FOCL [Pazzani and Kibler, 1992] and ML-SMART [Bergadano and Giordana, 1988] try to integrate background knowledge into inductive learning methods. DISCIPLE <ref> [Kodratoff and Tecuci, 1987] </ref> is another integration of inductive and deductive learning methods. In subsequent work learning by analogy was added as an additional means of dealing with imperfect domain theories [Tecuci and Kodratoff, 1990]. From this work the Multistrategy Task-adaptive Learning framework (MTL) [Tecuci and Michalski, 1991] has emerged.
Reference: [Kodratoff and Tecuci, 1989] <editor> Yves Kodratoff and Gheorghe D. Tecuci. </editor> <title> The central role of explanations in DISCIPLE. </title> <editor> In Katharina Morik, editor, </editor> <booktitle> Knowledge Representation and Organization in Machine Learning, </booktitle> <pages> pages 135-147. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: problem by extending 5 Michalski's recent Inferential Theory of Learning [Michalski, 1992] views some of the basic ideas of from the standpoint of Machine Learning. 22 4 PLAUSIBLE THEORIES explanation-based schema formation to a powerful generalization algorithm by replacing implication in the domain theory with some weaker forms of inference <ref> [Kodratoff and Tecuci, 1989, Widmer, 1992a] </ref>. 4.1 Qualitative Reasoning about Physical Systems 4.1.1 Qualitization In the middle of the 80's a new direction of research in qualitative knowledge representation has started: Qualitative Reasoning about Physical Systems.
Reference: [Korf, 1980] <author> Richard E. Korf. </author> <title> Toward a model of representation changes. </title> <journal> Artificial Intelligence, </journal> <volume> 14 </volume> <pages> 41-78, </pages> <year> 1980. </year>
Reference-contexts: that has the upward-solution property, there is always a set of consistent axioms whose abstraction is inconsistent, i.e. that they cause the false proof problem. 14 Abstraction hierarchies have been widely used in Artificial Intelligence research, especially in the fields of problem-solving and planning (see e.g. [Sacerdoti, 1974a], [Sacerdoti, 1974b], <ref> [Korf, 1980] </ref>). A historical account of the use of abstractions in various domains can be found in [Giunchiglia and Walsh, 1992a].
Reference: [Kuipers, 1986] <author> B. J. Kuipers. </author> <title> Qualitative simulation. </title> <journal> Artificial Intelligence, </journal> <volume> 29 </volume> <pages> 289-338, </pages> <year> 1986. </year>
Reference-contexts: Among the number of qualitative knowledge representation formalisms that have emerged over the years [Weld and deKleer, 1990, AI-, 1984, AI-, 1991] three formalisms have now become "standard": * Qualitative Process Theory [Forbus, 1984] * Qualitative Simulation <ref> [Kuipers, 1986] </ref> * Reasoning with Confluences [deKleer and Brown, 1984] All of the above systems support some kind of qualitative algebra [Williams, 1988] that allows them to reason with qualitative function descriptions. <p> We will discuss a simplified version of the QSIM approach to qualitative reasoning in a little more detail in the next section, because of its simplicity and its wide use for Machine Learning research (e.g. [Coiera, 1989a, Bratko et al., 1991, Varsek, 1992]). 4.1.2 QSIM Qualitative SIMulation (QSIM) <ref> [Kuipers, 1986] </ref> is the most recent approach to qualitative reasoning. Its main purpose was to allow simulation | envi-sionment | of processes. Thus it is mainly concerned with the time varying behavior of qualitative representations of functions which then can be used for simulating the behavior of the system.
Reference: [Laird et al., 1987] <author> J. E. Laird, A. Newell, and Paul S. Rosenbloom. </author> <title> SOAR: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 1-64, </pages> <year> 1987. </year>
Reference-contexts: In [Unruh and Rosenbloom, 1989] possibilities for the use of abstraction in the SOAR system <ref> [Laird et al., 1987] </ref> are suggested. As alpine and Pablo, SOAR uses a method for abstracting the search space by simply removing some aspects of the problem. While in the systems discussed above this is done a priori in a static way, in SOAR abstraction occurs dynamically during problem solving. <p> While in the systems discussed above this is done a priori in a static way, in SOAR abstraction occurs dynamically during problem solving. Whenever SOAR encounters and impasse during search, it usually resolves this by proposing a new subgoal <ref> [Laird et al., 1987] </ref>. [Unruh and Rosenbloom, 1989] suggest that instead of further problem solving in another level of subgoals the impasse may as well be removed by ignoring the sub-goal. This is very similar to the approach taken in [Keller, 1988] (see section 3.5.1).
Reference: [Lavrac et al., 1991] <author> N. Lavrac, S. Dzeroski, and M. Grobelnik. </author> <title> Learning non-recursive definitions of relations with linus. </title> <booktitle> In Proceedings of the European Working Session on Learning, Porto, </booktitle> <address> Portugal, </address> <year> 1991. </year>
Reference-contexts: The application of other ILP systems | FOIL [Quinlan, 1990] and LINUS <ref> [Lavrac et al., 1991] </ref> | is reported to be less successful on this particular problem. A very interesting approach to the same problem domain using a genetic algorithm can be found in [Varsek, 1992]. Qualitative Models are represented in hierarchical binary trees.
Reference: [Lebowitz, 1986] <author> M. Lebowitz. </author> <title> Integrated learning: Controlling explanation. </title> <journal> Cognitive Science, </journal> <volume> 10(2) </volume> <pages> 219-240, </pages> <year> 1986. </year>
Reference-contexts: Multistrategy Learning has been mostly investigated in the context of integrating inductive and deductive learning methods. Well-known systems that enhance Explanation-Based Learning methods with an inductive learning component are UNIMEM <ref> [Lebowitz, 1986] </ref> and OCCAM [Pazzani, 1990]. Other approaches can be found in [Flann and Dietterich, 1989] and [Widmer, 1989b]. In Inductive Logic Programming, the systems FOCL [Pazzani and Kibler, 1992] and ML-SMART [Bergadano and Giordana, 1988] try to integrate background knowledge into inductive learning methods.
Reference: [Lenat and Feigenbaum, 1991] <author> D. B. Lenat and E. A. Feigenbaum. </author> <title> On the thresholds of knowledge. </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 185-250, </pages> <year> 1991. </year>
Reference-contexts: Bebe can avoid playing a losing 2 1 INTRODUCTION Later on it was proposed that large amounts of knowledge rather than efficient and sophisticated general reasoning procedures are the key to intelligent performance of a computer system, the so-called Knowledge Principle <ref> [Lenat and Feigenbaum, 1991] </ref>. This philosophy has also led to several chess programs that tried to make use of explicit representation for plans and goals. But these approaches were only successful in limited subdo-mains of chess, as e.g. tactical combinations [Wilkins, 1982], pawn endgames [Berliner and Campbell, 1984] etc.
Reference: [Levy, 1986] <author> David N. Levy, </author> <title> editor. Computer Chess Compendium. </title> <publisher> Batsford Ltd., </publisher> <address> London, </address> <year> 1986. </year> <note> 58 REFERENCES </note>
Reference: [Marsland and Schaeffer, 1990] <author> T. Anthony Marsland and Jonathan Schaeffer, </author> <title> editors. Computers, Chess, and Cognition. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: silicon chess world champion to be just a matter of time, a trend has developed that the game of Go should take up that role, because it is not that accessible to brute-force searching algorithms and thus imposes stronger challenges to the proper formalization of knowledge (see Part V of <ref> [Marsland and Schaeffer, 1990] </ref>). 6 2 THE NEED FOR QUALITATIVE KNOWLEDGE playing system has all the information it would need to determine whether a given position (in particular the starting position) is a win, a loss or a draw.
Reference: [McCarthy, 1990] <author> John McCarthy. </author> <title> Chess as the drosophila of AI. </title> <editor> In T. Anthony Marsland and Jonathan Schaeffer, editors, </editor> <title> Computers, Chess, </title> <journal> and Cognition, </journal> <pages> pages 227-237. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: For these reasons chess has been called "the drosophila 2 of AI" alluding to the important role this comparably simple animal played as the object of early research in genetics <ref> [McCarthy, 1990] </ref>. 3 In particular chess has become a standard example [Tadepalli, 1986, Tadepalli, 1989, Flann, 1990] for an intractable domain. 2.1 Knowledge is intractable Many applications of rule-based production systems assume that the underlying theory is complete and consistent.
Reference: [Michalski and Tecuci, 1992] <editor> Ryszard S. Michalski and Gheorghe D. Tecuci, editors. </editor> <title> Machine Learning: A Multistrategy Approach, </title> <booktitle> Vol. IV. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: Other systems try to integrate Explanation-Based Learning and Neural Nets [Shavlik and Towell, 1989] or EBL and Case-Based Learning [Wilkins, 1990]. For more on multistrategy learning systems and a more detailed account of the general framework see <ref> [Michalski and Tecuci, 1992] </ref>. 7 Conclusion The main purpose of this paper was to analyze the important role that qualitative knowledge in various forms can play in Machine Learning research and give a review of previous research in this field.
Reference: [Michalski et al., 1986] <author> R. S. Michalski, I. Mozetic, J. Hong, and N. Lavrac. </author> <title> The multi-purpose incremental learning system AQ15 and its testing application to three medical domains. </title> <booktitle> In Proceedings of the 5th National Conference on Artificial Intelligence, </booktitle> <pages> pages 1041-1045, </pages> <address> Philadelphia, PA, </address> <year> 1986. </year>
Reference-contexts: These instances are then used as training examples for generating efficient decision rules with a predecessor of the AQ15 <ref> [Michalski et al., 1986] </ref> induction algorithm. The induced rules have proved to be not only efficient, but also quite meaningful to human domain experts. A similar approach has been taken in [Pearce, 1988].
Reference: [Michalski, 1980] <author> Ryszard S. Michalski. </author> <title> Pattern recognition and rule-guided inference. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 2 </volume> <pages> 349-361, </pages> <year> 1980. </year>
Reference-contexts: Initial data-driven learning generalizes these instances into first-order logic rules. It does so by generalizing a true fact and subsequently specializing it by adding literals and instantiating or unifying variables until no negative instance is covered. This procedure is quite similar to systems of the AQ-family <ref> [Michalski, 1980] </ref> with the difference that it is using non-recursive PROLOG-like clauses as a representation language. It can thus be considered as an early Inductive Logic Programming (ILP) system [Muggleton, 1991, Quinlan, 1990], very similar to the Model Inference System (MIS) [Shapiro, 1981]. <p> This loosely corresponds to the NFIS principle we have discussed in section 4.1.1. The process of comparing two knowledge 7 AQR is a variant of Michalski's AQ <ref> [Michalski, 1980] </ref> that is able to deal with conflicting classifications of examples. 8 In [Pearce, 1988] this problem did not arise as examples for failures that could not be distinguished from a normal state with respect to the observable indicators had been removed from the set of example behaviors used for
Reference: [Michalski, 1983] <author> Ryszard S. Michalski. </author> <title> A theory and methodology of inductive learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <year> 1983. </year>
Reference-contexts: The principle that simpler rules, all other things being equal, are more likely to be predictive for future data | known as Occam's razor | has been thoroughly examined [Blumer et al., 1987] and widely applied as a preference criterion for rule formation in Machine Learning algorithms <ref> [Michalski, 1983] </ref>. Nevertheless, the importance of this principle is still underestimated. In most Machine Learning algorithms a correct, complicated rule is preferred over a simple, approximate rule.
Reference: [Michalski, 1992] <author> Ryszard S. Michalski. </author> <title> Inferential theory of learning: Developing foundations for multistrategy learning. </title> <editor> In Ryszard S. Michalski and Gheorghe D. Tecuci, editors, </editor> <title> Machine Learning: A Multistrategy Approach, </title> <booktitle> Vol. IV. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: While the research reported there tried to overcome intractability by allowing inconsistent and incomplete domain theories, other authors deal with the problem by extending 5 Michalski's recent Inferential Theory of Learning <ref> [Michalski, 1992] </ref> views some of the basic ideas of from the standpoint of Machine Learning. 22 4 PLAUSIBLE THEORIES explanation-based schema formation to a powerful generalization algorithm by replacing implication in the domain theory with some weaker forms of inference [Kodratoff and Tecuci, 1989, Widmer, 1992a]. 4.1 Qualitative Reasoning about Physical
Reference: [Minton, 1984] <author> Steve Minton. </author> <title> Constraint-based generalization: Learning game-playing plans from single examples. </title> <booktitle> In Proceedings AAAI-84, </booktitle> <pages> pages 251-254, </pages> <address> Austin, Texas, </address> <year> 1984. </year>
Reference-contexts: patching of the errors. 3.4 The Utility Problem 17 3.4 The Utility Problem Although Tadepalli shows most of the algorithms of the section 3.2 to be probably approximately tractable, it is quite conceivable that most of the systems | when faced a complex learning task | will learn huge theories. <ref> [Minton, 1984] </ref> reports the problem of learning too many too specialized rules with explanation-based learning in several game domains including chess.
Reference: [Minton, 1990] <author> Steve Minton. </author> <title> Quantitative results concerning the utility of explanation-based learning. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 363-392, </pages> <year> 1990. </year>
Reference-contexts: This experience has motivated his research with Prodigy where he tried to make sure that "the cumulative benefits of applying the knowledge must outweigh the cumulative costs of testing whether the knowledge is applicable" <ref> [Minton, 1990] </ref>. This has been known as the utility problem (see also [Cohen, 1992] and section 3.5.2). <p> The system thus chooses the explanation which maximizes the ratio of newly explained examples to the size of the rule. With this method multiple, possibly inconsistent explanations can be dealt with (see [Cohen, 1992]). In addition to this heuristic that | quite similarly to the Prodigy system <ref> [Minton, 1990] </ref> | increases the convergence rate of learning, the system also tries to learn approximations of expensive rules as another method to avoid swamping. For this purpose approximations of rules are formed by throwing out all but a bounded number of literals of each rule.
Reference: [Mitchell et al., 1986] <author> T. M. Mitchell, R. M. Keller, and S. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: Even in domains where it would be possible to have complete and correct knowledge about the environment, the size of the knowledge base or of the implied search space would be too big to be useful. Complex complete information games like chess are good examples of such intractable domains <ref> [Mitchell et al., 1986] </ref>. <p> So among the problems that have to be dealt with when using approximate theories are * finding the right approximations * reasoning and learning in incomplete or inconsistent theories * multiple explanations 3.2 Learning of Approximations Learning approximate theories has mostly been studied in connection with Explanation-Based Learning. In <ref> [Mitchell et al., 1986] </ref> it has already been 3.2 Learning of Approximations 13 noted that imperfect theories are a major drawback of EBL. <p> When the debugging process has come to an end, i.e. the user has enough confidence into the behavior of the learned qualitative model, it can be used for simulation and problem solving. A natural approach would be to operationalize and generalize every problem solution with an Explanation-Based Generalization algorithm <ref> [Mitchell et al., 1986] </ref>. However, in the KARDIO project another approach was used [Mozetic, 1986]: The generated qualitative model is simulated on all possible inputs and thus a complete case base of model behavior is obtained. <p> In classical EBL non-operational predicates are unfolded in the generalized proof tree until the concept is defined in operational predicates only <ref> [Mitchell et al., 1986] </ref>. The authors extend this mechanism by adding the abstraction methods used above. Concrete instantiations of these abstraction schemas can be specified by the user and will be applied to all literals in the proof tree. This mechanism allows abstraction to occur during explanation-based generalization.
Reference: [Mostow et al., 1990] <author> Jack Mostow, Thomas Ellman, and Armand Prieditis. </author> <title> A unified transformational model for discovering heuristics by idealizing intractable problems. </title> <booktitle> In Working Notes of the AAAI Workshop on Automatic REFERENCES 59 Generation of Approximations and Abstractions, </booktitle> <pages> pages 290-301, </pages> <address> Boston, Mas-sachusettes, </address> <year> 1990. </year>
Reference-contexts: Error rates are estimated empirically by using a teacher-provided set of training examples. The system was applied to derive heuristics for the game of Hearts and a uniprocessor scheduling task <ref> [Mostow et al., 1990] </ref>. 14 3 RULE APPROXIMATION 3.2.2 Incremental Approximation Lazy Explanation-Based Learning [Tadepalli, 1989] is a very radical approach to learning in intractable two person games like chess.
Reference: [Mozetic and Holzbaur, 1991] <author> Igor Mozetic and C. Holzbaur. </author> <title> Extending explanation-based generalization by abstraction operators. </title> <booktitle> In Proceedings of the 5th European Working Session on Learning, </booktitle> <pages> pages 282-297, </pages> <address> Porto, Portu-gal, </address> <year> 1991. </year>
Reference-contexts: Thus a hierarchy of representation languages can be formed that will be used for model refinement. 17 Variables and constants are associated with types, such that variables of a certain type can only be instantiated by constants of the same type. 44 5 DEEP THEORIES AND ABSTRACTIONS In subsequent work <ref> [Mozetic and Holzbaur, 1991] </ref> this abstraction mechanism is put into an explanation-based learning framework. In classical EBL non-operational predicates are unfolded in the generalized proof tree until the concept is defined in operational predicates only [Mitchell et al., 1986]. <p> The set of all such descriptors is used as the language in which the target concept will be induced. [Giordana and Saitta, 1990] use this idea in the inductive relational learning system ML-SMART [Bergadano and Giordana, 1988]. Here a first-order logic abstraction theory quite similar to <ref> [Mozetic and Holzbaur, 1991] </ref> can be specified. In the first abstraction phase ML-SMART generates instance representations at different levels of abstraction as an input to the inductive learning algorithm. This differs from the approach of [Drastal et al., 1989] where a set of maximally abstracted descriptors is used. <p> Research on using rule approximations as reported in [Fawcett, 1989, Knoblock, 1989, Keller, 1988, Cohen, 1990, Tadepalli, 1989] and on qualitative extensions of the representation language <ref> [DeJong, 1989, Widmer, 1992a, Mozetic and Holzbaur, 1991, Bennett, 1989] </ref> in traditional Explanation-Based Learning environments shows that a tradeoff between correctness and efficiency is an important issue. Applications of this finding to induction are still waiting to be investigated.
Reference: [Mozetic, 1986] <author> Igor Mozetic. </author> <title> Knowledge extraction through learning from examples. </title> <editor> In Tom M. Mitchell, Jaime G. Carbonell, and Ryszard S. Michalski, editors, </editor> <booktitle> Machine Learning: A Guide to Current Research, </booktitle> <pages> pages 227-231. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1986. </year>
Reference-contexts: A natural approach would be to operationalize and generalize every problem solution with an Explanation-Based Generalization algorithm [Mitchell et al., 1986]. However, in the KARDIO project another approach was used <ref> [Mozetic, 1986] </ref>: The generated qualitative model is simulated on all possible inputs and thus a complete case base of model behavior is obtained. These instances are then used as training examples for generating efficient decision rules with a predecessor of the AQ15 [Michalski et al., 1986] induction algorithm.
Reference: [Mozetic, 1987a] <author> Igor Mozetic. </author> <title> Learning of qualitative models. </title> <booktitle> In Progress in Machine Learning. </booktitle> <publisher> Sigma Press, </publisher> <address> Wilmslow, England, </address> <year> 1987. </year>
Reference-contexts: The overall behavior of the system can then be inferred from that by simulation. A natural place for learning in this framework is to induce the qualitative behavior of the components themselves from provided examples. In <ref> [Mozetic, 1987a] </ref> a system is described which learns rules for the function of 4.1 Qualitative Reasoning about Physical Systems 25 the model components from instances of their behavior.
Reference: [Mozetic, 1987b] <author> Igor Mozetic. </author> <title> The role of abstractions in learning qualitative models. </title> <booktitle> In Proceedings of the 4th International Workshop on Machine Learning, </booktitle> <address> Irvine, California, </address> <year> 1987. </year>
Reference-contexts: If a true fact cannot be derived, the debugger proposes positive, yet uncovered instances for some component behaviors until the model behavior can be derived. Generating positive exceptions for the rules is a non-trivial task which, due to its extensive use of abstractions <ref> [Mozetic, 1987b] </ref>, is described in more detail in section 5.5. The proposed negative and positive exceptions can be validated by the user. <p> Qualitization as introduced in chapter 4 is one form of simplifying learn-ning in many domains. In <ref> [Mozetic, 1987b] </ref> an approach to automated qualitative modeling (see section 4.1.3) is extended with the use of abstractions. In section 4.1.3 we have described how initial hypotheses for the component behavior are formed. The model is then simulated on new instances.
Reference: [Muggleton and Buntine, 1988] <author> Stephen H. Muggleton and Wray L. Buntine. </author> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the 5th International Conference on Machine Learning, </booktitle> <pages> pages 339-352, </pages> <year> 1988. </year>
Reference-contexts: Besides, the results achieved at the most abstract level appeared to be most understandable for human domain experts. [Giordana et al., 1991] put this work into the ILP framework of Inverse Resolution <ref> [Muggleton and Buntine, 1988] </ref>. A non-generalizing absorption operator, a similar variant of inter-construction, and an additional mechanism, term abstraction, are introduced for this purpose.
Reference: [Muggleton and Feng, 1990] <author> Stephen H. Muggleton and Cao Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the 1st Conference on Algorithmic Learning Theory, </booktitle> <pages> pages 1-14, </pages> <address> Tokyo, Japan, </address> <year> 1990. </year>
Reference-contexts: A well-known ILP system was then actually used in [Bratko et al., 1991]. Golem <ref> [Muggleton and Feng, 1990] </ref> learns first-order clauses in a very efficient way. A definition of the QSIM theory in first order Horn clauses allows it to be used as background knowledge. Qualitative constraints are expressed by logic predicates, similar to the approach we have used in section 4.1.1. <p> The author uses the ILP algorithm Golem <ref> [Muggleton and Feng, 1990] </ref> in the task mentioned above.
Reference: [Muggleton, 1991] <author> Stephen H. Muggleton. </author> <title> Inductive logic programming. </title> <journal> New Generation Computing, </journal> <volume> 8 </volume> <pages> 295-318, </pages> <year> 1991. </year>
Reference-contexts: This procedure is quite similar to systems of the AQ-family [Michalski, 1980] with the difference that it is using non-recursive PROLOG-like clauses as a representation language. It can thus be considered as an early Inductive Logic Programming (ILP) system <ref> [Muggleton, 1991, Quinlan, 1990] </ref>, very similar to the Model Inference System (MIS) [Shapiro, 1981]. As in MIS [Shapiro, 1982], the initial hypotheses can be incrementally debugged. For this purpose the model with the induced functions of the components is simulated and the result is compared with the intended model behavior. <p> The relatively new area of Inductive Logic Programming now allows to efficiently induce concept descriptions formulated in first-order logic (see e.g. <ref> [Muggleton, 1991] </ref> or [Quinlan, 1990]). This gain in expressiveness over traditional propositional learning algorithms has already contributed significantly to learning in qualitative domain theories [Giordana et al., 1991, Bratko et al., 1991, Feng, 1991, Coiera, 1993].
Reference: [Newell et al., 1960] <author> A. Newell, J. Shaw, and H. Simon. </author> <title> Report on a general problem-solving program for a computer. </title> <booktitle> In Proceedings of the International Conference on Information Processing, UNESCO, </booktitle> <address> Paris, </address> <year> 1960. </year>
Reference-contexts: The most outstanding example of this optimistic point of view was Newell and Simon's "General Problem Solver" <ref> [Newell et al., 1960] </ref>, which operates in a simple state space representation of the world. Later on research mostly concentrated on trying to find better and more efficient inference algorithms, and the knowledge representation side was more and more neglected.
Reference: [Pazzani and Kibler, 1992] <author> Micheal Pazzani and Dennis Kibler. </author> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 57-94, </pages> <year> 1992. </year>
Reference-contexts: The algorithm can also be reformulated in an incremental way as has been shown in [Widmer, 1989a]. A related approach can be found in <ref> [Pazzani and Kibler, 1992] </ref>. FOCL (First-Order Combined Learner) tries to combine the virtues of the First-Order Inductive Learner FOIL [Quinlan, 1990] and Explanation Based Learning. It allows the user to specify extensionally defined background knowledge as well as intensionally defined training examples. <p> Well-known systems that enhance Explanation-Based Learning methods with an inductive learning component are UNIMEM [Lebowitz, 1986] and OCCAM [Pazzani, 1990]. Other approaches can be found in [Flann and Dietterich, 1989] and [Widmer, 1989b]. In Inductive Logic Programming, the systems FOCL <ref> [Pazzani and Kibler, 1992] </ref> and ML-SMART [Bergadano and Giordana, 1988] try to integrate background knowledge into inductive learning methods. DISCIPLE [Kodratoff and Tecuci, 1987] is another integration of inductive and deductive learning methods.
Reference: [Pazzani, 1988] <author> M. Pazzani. </author> <title> Selecting the best explanation for explanation-based learning. </title> <booktitle> In Proceedings of the 1988 Spring Symposium on Explanation-based Learning, </booktitle> <pages> pages 165-169, </pages> <institution> Stanford University, California, </institution> <year> 1988. </year> <note> 60 REFERENCES </note>
Reference-contexts: Similarly, experiments can be conducted to find the only completion of an incomplete proof. Another approach is tried in [Fawcett, 1989]. Even for explanation-based learning in complete and consistent domains, the importance of selecting the "best" explanation to derive the most accurate rule has been recognized <ref> [Pazzani, 1988] </ref>. Fawcett's work tries to transform this approach to EBL systems in incomplete theory domains. Abduction is used to guess the missing parts of a partial proof. Among all possible abductive completions of the proof the system chooses the most plausible partial explanation for the training instance.
Reference: [Pazzani, 1989] <author> Michael J. Pazzani. </author> <title> Explanation-based learning with weak domain theories. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pages 72-74, </pages> <address> Ithaca, New York, </address> <year> 1989. </year>
Reference-contexts: Russell himself calls this form of knowledge partial determinations [Russell, 1986a]. Directed dependencies (section 4.1.6) can be viewed as a kind of determination as well, only used for ordered sets of qualitative values. The system PostHoc <ref> [Pazzani, 1989] </ref> uses so-called influences. Influences are determinations, that do not necessarily have to be correct. After seeing a training example PostHoc forms a hypothesis by finding an influence on the target concept and assuming that all factors that influence the target imply it.
Reference: [Pazzani, 1990] <author> M. J. Pazzani. </author> <title> Integrating explanation-based and empirical learning methods in OCCAM. </title> <booktitle> In Proceedings of the 3rd European Working Session on Learning, </booktitle> <pages> pages 147-166, </pages> <address> Glasgow, Scotland, </address> <year> 1990. </year>
Reference-contexts: Multistrategy Learning has been mostly investigated in the context of integrating inductive and deductive learning methods. Well-known systems that enhance Explanation-Based Learning methods with an inductive learning component are UNIMEM [Lebowitz, 1986] and OCCAM <ref> [Pazzani, 1990] </ref>. Other approaches can be found in [Flann and Dietterich, 1989] and [Widmer, 1989b]. In Inductive Logic Programming, the systems FOCL [Pazzani and Kibler, 1992] and ML-SMART [Bergadano and Giordana, 1988] try to integrate background knowledge into inductive learning methods.
Reference: [Pearce, 1988] <author> D. A. Pearce. </author> <title> The induction of fault diagnosis systems from qualitative models. </title> <booktitle> In Proceedings of the 7th National Conference on Artificial Intelligence, </booktitle> <pages> pages 353-357, </pages> <address> Minneapolis, Minnesota, </address> <year> 1988. </year>
Reference-contexts: The induced rules have proved to be not only efficient, but also quite meaningful to human domain experts. A similar approach has been taken in <ref> [Pearce, 1988] </ref>. By systematically 6 Similarity of two models is estimated with the number of examples on which the models agree. 28 4 PLAUSIBLE THEORIES failing all model components, a qualitative model is used to induce rules for diagnosing faults. <p> This loosely corresponds to the NFIS principle we have discussed in section 4.1.1. The process of comparing two knowledge 7 AQR is a variant of Michalski's AQ [Michalski, 1980] that is able to deal with conflicting classifications of examples. 8 In <ref> [Pearce, 1988] </ref> this problem did not arise as examples for failures that could not be distinguished from a normal state with respect to the observable indicators had been removed from the set of example behaviors used for induction. 4.1 Qualitative Reasoning about Physical Systems 29 structures in order to find relevant <p> Shallow models consist of rules that are directly applicable to the input data and immediately produce an answer. Deep models can be used to generate shallow level knowledge as has been shown in the KARDIO-project [Bratko et al., 1989] or in <ref> [Pearce, 1988] </ref>. In Mozetic's work on learning in the KARDIO project described in section 4.1.4 the qualitative background knowledge is organized into a hierarchical, deep knowledge base.
Reference: [Plaisted, 1981] <author> D. Plaisted. </author> <title> Theorem proving with abstraction. </title> <journal> Artificial Intelligence, </journal> <volume> 16 </volume> <pages> 47-108, </pages> <year> 1981. </year>
Reference-contexts: Approaches using abstraction hierarchies seem to attack this problem in a similar fashion: Concepts recognized at a high level of abstraction do not necessarily have to be true after examining the situation with a more detailed domain theory. This is known as the "false proof" problem (see <ref> [Plaisted, 1981, Tenenberg, 1987] </ref> and section 5.2). 18 3 RULE APPROXIMATION 3.5 The Multiple Explanation Problem Reasoning with inconsistent theories has not been thoroughly investigated yet. [Roos, 1992] gives a framework for using logic in inconsistent domain theories, but no large-scale implementation of a system for reasoning with inconsistent knowledge is <p> Several formal definitions try to capture this intuitive idea. <ref> [Plaisted, 1981] </ref> defines a class of functions called abstraction mappings for this purpose. Definition 5.2 (Abstraction) An abstraction is a mapping of a clause C onto a set of clauses f (C) such that f | the abstraction mapping | has the properties 5.3 Abstractions 39 1.
Reference: [Plotkin, 1970] <author> G. D. Plotkin. </author> <title> A note on inductive generalisation. </title> <editor> In B. Meltzer and Donald Michie, editors, </editor> <booktitle> Machine Intelligence 5, </booktitle> <pages> pages 153-163. </pages> <publisher> Elsevier North-Holland, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: A more detailed discussion together with a suggestion for the use of negative training examples can be found in chapter 9 of [Coiera, 1989b]. [Coiera, 1993] has pointed out that the clause generation process in Gen-Model is nothing else than constructing the relative least general generalization (rlgg) in ILP terminology <ref> [Plotkin, 1970, Plotkin, 1971, Buntine, 1988] </ref>. A well-known ILP system was then actually used in [Bratko et al., 1991]. Golem [Muggleton and Feng, 1990] learns first-order clauses in a very efficient way.
Reference: [Plotkin, 1971] <author> G. D. Plotkin. </author> <title> A further note on inductive generalisation. </title> <editor> In B. Meltzer and Donald Michie, editors, </editor> <booktitle> Machine Intelligence 6, </booktitle> <pages> pages 101-124. </pages> <publisher> Elsevier North-Holland, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: A more detailed discussion together with a suggestion for the use of negative training examples can be found in chapter 9 of [Coiera, 1989b]. [Coiera, 1993] has pointed out that the clause generation process in Gen-Model is nothing else than constructing the relative least general generalization (rlgg) in ILP terminology <ref> [Plotkin, 1970, Plotkin, 1971, Buntine, 1988] </ref>. A well-known ILP system was then actually used in [Bratko et al., 1991]. Golem [Muggleton and Feng, 1990] learns first-order clauses in a very efficient way.
Reference: [Quinlan, 1990] <author> John Ross Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: The algorithm can also be reformulated in an incremental way as has been shown in [Widmer, 1989a]. A related approach can be found in [Pazzani and Kibler, 1992]. FOCL (First-Order Combined Learner) tries to combine the virtues of the First-Order Inductive Learner FOIL <ref> [Quinlan, 1990] </ref> and Explanation Based Learning. It allows the user to specify extensionally defined background knowledge as well as intensionally defined training examples. <p> This procedure is quite similar to systems of the AQ-family [Michalski, 1980] with the difference that it is using non-recursive PROLOG-like clauses as a representation language. It can thus be considered as an early Inductive Logic Programming (ILP) system <ref> [Muggleton, 1991, Quinlan, 1990] </ref>, very similar to the Model Inference System (MIS) [Shapiro, 1981]. As in MIS [Shapiro, 1982], the initial hypotheses can be incrementally debugged. For this purpose the model with the induced functions of the components is simulated and the result is compared with the intended model behavior. <p> The application of other ILP systems | FOIL <ref> [Quinlan, 1990] </ref> and LINUS [Lavrac et al., 1991] | is reported to be less successful on this particular problem. A very interesting approach to the same problem domain using a genetic algorithm can be found in [Varsek, 1992]. Qualitative Models are represented in hierarchical binary trees. <p> The relatively new area of Inductive Logic Programming now allows to efficiently induce concept descriptions formulated in first-order logic (see e.g. [Muggleton, 1991] or <ref> [Quinlan, 1990] </ref>). This gain in expressiveness over traditional propositional learning algorithms has already contributed significantly to learning in qualitative domain theories [Giordana et al., 1991, Bratko et al., 1991, Feng, 1991, Coiera, 1993].
Reference: [Rajamoney and DeJong, 1987] <author> Shankar Rajamoney and Gerald F. DeJong. </author> <title> The classification, detection and handling of imperfect theory problems. </title> <booktitle> In Proceedings of the 10th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 205-207, </pages> <address> Milano, Italy, </address> <year> 1987. </year>
Reference-contexts: However, real-world domains usually can't be formalized in a nice and neat way <ref> [Rajamoney and DeJong, 1987] </ref>. It is often the case that not all information needed is available or that the system's knowledge is not correct. <p> and must be assumed, or from incorrect domain theories that contain one or more over-general rules. 3.5.1 Selecting the best explanation An obvious way to deal with multiple explanations is to try to rule out impossible or implausible explanations until only one explanation is left which clearly is the best. <ref> [Rajamoney and DeJong, 1987] </ref> have designed a system that is able to design experiments to gather enough information to decide which explanations cannot hold for the given situation. Similarly, experiments can be conducted to find the only completion of an incomplete proof. Another approach is tried in [Fawcett, 1989].
Reference: [Roos, 1992] <author> Nico Roos. </author> <title> A logic for reasoning with inconsistent knowledge. </title> <journal> Artificial Intelligence, </journal> <volume> 57 </volume> <pages> 69-103, </pages> <year> 1992. </year>
Reference-contexts: This is known as the "false proof" problem (see [Plaisted, 1981, Tenenberg, 1987] and section 5.2). 18 3 RULE APPROXIMATION 3.5 The Multiple Explanation Problem Reasoning with inconsistent theories has not been thoroughly investigated yet. <ref> [Roos, 1992] </ref> gives a framework for using logic in inconsistent domain theories, but no large-scale implementation of a system for reasoning with inconsistent knowledge is available so far. Nevertheless several authors have faced the problem of dealing with multiple, possibly inconsistent explanations (see also section 2.2) in Machine Learning.
Reference: [Rosenbloom and Laird, 1986] <author> Paul S. Rosenbloom and J. E. Laird. </author> <title> Mapping explanation-based generalization onto SOAR. </title> <booktitle> In Proceedings of the 5th National Conference on Artificial Intelligence, Proceedings of the 5th National Conference on Artificial Intelligence, </booktitle> <year> 1986. </year>
Reference-contexts: This is very similar to the approach taken in [Keller, 1988] (see section 3.5.1). Learning occurs in SOAR by converting the subgoal-based search into shallow rules <ref> [Rosenbloom and Newell, 1986, Rosenbloom and Laird, 1986] </ref>. Thus abstracted subgoals are simply ignored and approximate search control rules are learned. 16 . This approximation mechanism naturally extends to form an abstraction hierarchy out of the dynamic subgoal hierarchy that SOAR creates during problem solving.
Reference: [Rosenbloom and Newell, 1986] <author> Paul S. Rosenbloom and Allen Newell. </author> <title> The chunking of goal hierarchies: A generalized model of practice. </title> <editor> In Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. II, </volume> <pages> pages 247-288. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1986. </year> <note> REFERENCES 61 </note>
Reference-contexts: This is very similar to the approach taken in [Keller, 1988] (see section 3.5.1). Learning occurs in SOAR by converting the subgoal-based search into shallow rules <ref> [Rosenbloom and Newell, 1986, Rosenbloom and Laird, 1986] </ref>. Thus abstracted subgoals are simply ignored and approximate search control rules are learned. 16 . This approximation mechanism naturally extends to form an abstraction hierarchy out of the dynamic subgoal hierarchy that SOAR creates during problem solving.
Reference: [Russell and Grosof, 1987] <author> S. J. Russell and B. N. Grosof. </author> <title> A declarative approach to bias in concept learning. </title> <booktitle> In Proceedings of the 6th National Conference on Artificial Intelligence, </booktitle> <year> 1987. </year>
Reference-contexts: Representing less restrictive pieces of knowledge than implications, they are also easier to specify. Nevertheless automatic acquisition of determinations is a topic that still has to be explored. In <ref> [Russell and Grosof, 1987] </ref> an example is given where the transitivity relation that holds between determinations is exploited to infer new determinations. In short, if we have P Q and Q R we can infer P R. Chapter 8 of [Russell, 1986a] also attacks the problem of inductively acquiring determinations.
Reference: [Russell, 1986a] <author> S. J. Russell. </author> <title> Analogical and Inductive Reasoning. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1986. </year>
Reference-contexts: Many more examples of determinations in common-sense knowledge can be found in <ref> [Russell, 1986a] </ref>. As we have seen, the mere knowledge that there is a functional relationship between the position of king and pawn and the outcome of the game was sufficient to learn a simple, powerful rule. <p> He might thus generalize to the false rule that endgames always can be won by keeping the king behind the pawn. For this reason many authors use some form of determinations with a weaker semantics. Russell himself calls this form of knowledge partial determinations <ref> [Russell, 1986a] </ref>. Directed dependencies (section 4.1.6) can be viewed as a kind of determination as well, only used for ordered sets of qualitative values. The system PostHoc [Pazzani, 1989] uses so-called influences. Influences are determinations, that do not necessarily have to be correct. <p> In [Russell and Grosof, 1987] an example is given where the transitivity relation that holds between determinations is exploited to infer new determinations. In short, if we have P Q and Q R we can infer P R. Chapter 8 of <ref> [Russell, 1986a] </ref> also attacks the problem of inductively acquiring determinations. The author suggests two methods to learn determinations from either a set of universally quantified expressions or from a set 36 4 PLAUSIBLE THEORIES of instances. The first method inverts the reasoning involved in using determinations as guidelines for analogies. <p> The basic method for this is to randomly select known examples from the joint domain of P and Q and then to compute the proportion of those pairs matching on P that also match on Q <ref> [Russell, 1986a] </ref>. The resulting certainty factor supposedly measures the percentage of examples satisfying the implication that would result by using the induced determination for an analogy. Russell calls these weaker determinations partial determinations.
Reference: [Russell, 1986b] <author> S. J. Russell. </author> <title> A quantitative analysis of analogy by similarity. </title> <booktitle> In Proceedings of the 5th National Conference on Artificial Intelligence, </booktitle> <address> Philadel-phia, PA, </address> <year> 1986. </year>
Reference-contexts: Up to this work analogy has mainly used heuristic similarity measures <ref> [Russell, 1986b, Carbonell, 1986] </ref> for retrieving previous instances and mapping their features to the current situation.
Reference: [Russell, 1987] <author> S. J. Russell. </author> <title> Analogy and single-instance generalization. </title> <booktitle> In Proceedings of the 4th International Workshop on Machine Learning, </booktitle> <address> Irvine, Cali-fornia, </address> <year> 1987. </year>
Reference-contexts: By using determinations a reasoning system now has the chance to find appropriate generalizations from single examples, without having the rules it will generate be implicitly specified from the start as most explanation-based systems do. In subsequent work <ref> [Russell, 1987] </ref> points out that single-instance generalization as in EBL and reasoning by analogy both can have explanation-based as well as determination-based justifications. For this purpose both | single-instance generalization and analogy | are put in a framework using 34 4 PLAUSIBLE THEORIES determinations [Davies and Russell, 1987].
Reference: [Sacerdoti, 1974a] <author> Earl D. Sacerdoti. </author> <title> Planning in a hierarchy of abstraction spaces. </title> <journal> Artificial Intelligence, </journal> <volume> 5(2) </volume> <pages> 115-135, </pages> <year> 1974. </year>
Reference-contexts: between two theories 13 that has the upward-solution property, there is always a set of consistent axioms whose abstraction is inconsistent, i.e. that they cause the false proof problem. 14 Abstraction hierarchies have been widely used in Artificial Intelligence research, especially in the fields of problem-solving and planning (see e.g. <ref> [Sacerdoti, 1974a] </ref>, [Sacerdoti, 1974b], [Korf, 1980]). A historical account of the use of abstractions in various domains can be found in [Giunchiglia and Walsh, 1992a].
Reference: [Sacerdoti, 1974b] <author> Earl D. Sacerdoti. </author> <title> A Structure for Plans and Behavior. </title> <publisher> American Elsevier, </publisher> <address> New York, </address> <year> 1974. </year>
Reference-contexts: theories 13 that has the upward-solution property, there is always a set of consistent axioms whose abstraction is inconsistent, i.e. that they cause the false proof problem. 14 Abstraction hierarchies have been widely used in Artificial Intelligence research, especially in the fields of problem-solving and planning (see e.g. [Sacerdoti, 1974a], <ref> [Sacerdoti, 1974b] </ref>, [Korf, 1980]). A historical account of the use of abstractions in various domains can be found in [Giunchiglia and Walsh, 1992a].
Reference: [Saitta et al., 1991] <author> L. Saitta, M. Botta, S. Ravotto, and S. Sperotto. </author> <title> Improving learning by using deep models. </title> <booktitle> In Proceedings of the First International Workshop on Multistrategy Learning, </booktitle> <pages> pages 131-143, </pages> <address> Harper's Ferry, West Virginia, </address> <year> 1991. </year>
Reference-contexts: Dependencies may be viewed as a formalism to denote possibly over-specialized rules. Too many conditions are specified in the concept descriptions. Induction is used to select the correct set of predicates to form a consistent rule. The system WHY <ref> [Saitta et al., 1991] </ref> on the other hand introduces a special 0-arity predicate that can be used to explicitely specify the incompleteness, i.e. the over-generality, of rules.
Reference: [Scherzer et al., 1990] <author> T. Scherzer, L. Scherzer, and D. Tjaden. </author> <title> Learning in bebe. </title> <editor> In T. Anthony Marsland and Jonathan Schaeffer, editors, </editor> <title> Computers, Chess, </title> <journal> and Cognition, </journal> <pages> pages 197-216. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: that does not always allow a correct and consistent match between the represented objects and the real world, but can nevertheless be used to get approximate characterizations of the behavior of the modeled domain. line twice by saving the board positions occurring in a game in a long term memory <ref> [Scherzer et al., 1990] </ref>, but it does not generalize in any way. It will still make the same type of mistake over and over again. 3 Various fields of research can be labeled as using qualitative knowledge according to this definition.
Reference: [Sebag and Schoenauer, 1990] <author> Michele Sebag and Marc Schoenauer. </author> <title> Incremental learning of rules and meta-rules. </title> <booktitle> In Proceedings of the 7th International Conference on Machine Learning, </booktitle> <pages> pages 49-57, </pages> <address> Austin, Texas, </address> <year> 1990. </year>
Reference-contexts: After all examples have been reduced like this, an inductive learning mechanism uses these new predicates to learn preference rules for the original rules. This process of meta-rule generation can be succesively applied to rules, meta-rules, meta-meta-rules etc. Experiments in a propositional learning framework <ref> [Sebag and Schoenauer, 1990] </ref> have shown that three iterations succesively improve predictive accuracy, while further steps may decrease performance on unseen examples, possibly through overfitting.
Reference: [Sebag and Schoenauer, 1992] <author> Michele Sebag and Marc Schoenauer. </author> <title> Learning to control inconsistent knowledge. </title> <booktitle> In Proceedings of the 10th European Conference on Artificial Intelligence, </booktitle> <pages> pages 479-483, </pages> <address> Vienna, Austria, </address> <year> 1992. </year>
Reference-contexts: As the process is exponential on the literal bound, only few literals can be kept. In all but one test domains substantial performance improvement occurred when learning only rules of a length 2. <ref> [Sebag and Schoenauer, 1992] </ref> propose a different approach to this problem. The basic idea of the authors is to learn meta-rules that control the application of the original rules. In a first phase (possibly inconsistent) rules are induced from a set of examples.
Reference: [Shannon, 1950] <author> Claude E. Shannon. </author> <title> Programming a computer for playing chess. </title> <journal> Philosophical Magazine, </journal> <volume> 41(7) </volume> <pages> 256-275, </pages> <year> 1950. </year> <note> Reprinted in [Levy, </note> <year> 1986]. </year>
Reference-contexts: In this respect the basic ideas of today's chess programs have not changed considerably since the proposal of the MiniMax algorithm by Claude Shannon in 1950 <ref> [Shannon, 1950] </ref>.
Reference: [Shapiro, 1981] <author> Ehud Y. Shapiro. </author> <title> An algorithm that infers theories from facts. </title> <booktitle> In Proceedings of the 7th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 446-451, </pages> <year> 1981. </year> <note> 62 REFERENCES </note>
Reference-contexts: It can thus be considered as an early Inductive Logic Programming (ILP) system [Muggleton, 1991, Quinlan, 1990], very similar to the Model Inference System (MIS) <ref> [Shapiro, 1981] </ref>. As in MIS [Shapiro, 1982], the initial hypotheses can be incrementally debugged. For this purpose the model with the induced functions of the components is simulated and the result is compared with the intended model behavior.
Reference: [Shapiro, 1982] <author> Ehud Y. Shapiro. </author> <title> Algorithmic Program debugging. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, </address> <year> 1982. </year>
Reference-contexts: It can thus be considered as an early Inductive Logic Programming (ILP) system [Muggleton, 1991, Quinlan, 1990], very similar to the Model Inference System (MIS) [Shapiro, 1981]. As in MIS <ref> [Shapiro, 1982] </ref>, the initial hypotheses can be incrementally debugged. For this purpose the model with the induced functions of the components is simulated and the result is compared with the intended model behavior. <p> In section 4.1.3 we have described how initial hypotheses for the component behavior are formed. The model is then simulated on new instances. Whenever an expectation failure occurs, a debugging module similar to MIS <ref> [Shapiro, 1982] </ref> is invoked. In the case of incomplete hypotheses | a true fact is not covered | a new rule is added to the current hypothesis.
Reference: [Shavlik and Towell, 1989] <author> Jude W. Shavlik and Geoffrey G. Towell. </author> <title> An approach to combining explanation-based and neural learning algorithms. </title> <journal> Connection Science, </journal> <volume> 1(3), </volume> <year> 1989. </year>
Reference-contexts: In certain base cases the MTL method reduces to EBL, abductive learning, empirical induction or analogical reasoning. Further research has to be done to allow inconsistent information in the knowledge base. Other systems try to integrate Explanation-Based Learning and Neural Nets <ref> [Shavlik and Towell, 1989] </ref> or EBL and Case-Based Learning [Wilkins, 1990].
Reference: [Srinivasan et al., 1992] <author> A. Srinivasan, S. H. Muggleton, and M. E. Bain. </author> <title> Distinguishing noise from exceptions in non-monotonic learning. </title> <booktitle> In Proceedings of the 2nd International Inductive Logic Programming Workshop, </booktitle> <address> Tokyo, </address> <year> 1992. </year>
Reference-contexts: The effects of noise on input data has been widely studied (see e.g. [Angluin and Laird, 1988, Clark and Niblett, 1987]), but the problem of distinguishing noise in the data from rare instances of the target concept has been addressed only recently <ref> [Srinivasan et al., 1992] </ref> and is mostly based 10 2 THE NEED FOR QUALITATIVE KNOWLEDGE on statistical heuristics. Qualitative knowledge about the approximate be-haviour of the environment might help to recognize noise, because of its lack of plausibility.
Reference: [Tadepalli, 1986] <author> Prasad Tadepalli. </author> <title> Learning in intractable domains. In Machine Learning: A Guide to Current Research. </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, Cali-fornia, </address> <year> 1986. </year>
Reference-contexts: For these reasons chess has been called "the drosophila 2 of AI" alluding to the important role this comparably simple animal played as the object of early research in genetics [McCarthy, 1990]. 3 In particular chess has become a standard example <ref> [Tadepalli, 1986, Tadepalli, 1989, Flann, 1990] </ref> for an intractable domain. 2.1 Knowledge is intractable Many applications of rule-based production systems assume that the underlying theory is complete and consistent. However, real-world domains usually can't be formalized in a nice and neat way [Rajamoney and DeJong, 1987].
Reference: [Tadepalli, 1989] <author> Prasad Tadepalli. </author> <title> Lazy explanation-based learning: A solution to the intractable theory problem. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1989. </year>
Reference-contexts: domain of the author | will appear quite often in this paper, we want to stress that the general principles exemplified by this domain are valid for all complex, intractable domains (e.g. reasoning about physical systems [Weld and deKleer, 1990], medical domains [Bratko et al., 1989], music [Widmer, 1992b], chess <ref> [Tadepalli, 1989] </ref>, [Flann, 1989] etc.). The next three chapters give a review of previous work in Machine Learning on the topics of approximation, learning with qualitative and plausible theories and abstraction. <p> For these reasons chess has been called "the drosophila 2 of AI" alluding to the important role this comparably simple animal played as the object of early research in genetics [McCarthy, 1990]. 3 In particular chess has become a standard example <ref> [Tadepalli, 1986, Tadepalli, 1989, Flann, 1990] </ref> for an intractable domain. 2.1 Knowledge is intractable Many applications of rule-based production systems assume that the underlying theory is complete and consistent. However, real-world domains usually can't be formalized in a nice and neat way [Rajamoney and DeJong, 1987]. <p> Error rates are estimated empirically by using a teacher-provided set of training examples. The system was applied to derive heuristics for the game of Hearts and a uniprocessor scheduling task [Mostow et al., 1990]. 14 3 RULE APPROXIMATION 3.2.2 Incremental Approximation Lazy Explanation-Based Learning <ref> [Tadepalli, 1989] </ref> is a very radical approach to learning in intractable two person games like chess. One of the major problems in these domains is that | in order to prove a move to be correct | all possible moves of the opponent have to be considered. <p> But to gain even more benefit from abstraction techniques, these constraints on the representation language and/or the abstraction mapping should be relaxed and other methods for dealing with this problem should be developed. Research on using rule approximations as reported in <ref> [Fawcett, 1989, Knoblock, 1989, Keller, 1988, Cohen, 1990, Tadepalli, 1989] </ref> and on qualitative extensions of the representation language [DeJong, 1989, Widmer, 1992a, Mozetic and Holzbaur, 1991, Bennett, 1989] in traditional Explanation-Based Learning environments shows that a tradeoff between correctness and efficiency is an important issue.
Reference: [Tadepalli, 1990] <author> Prasad Tadepalli. </author> <title> On quantifying approximation. </title> <booktitle> In Working Notes of the AAAI Workshop on Automatic Generation of Approximations and Abstractions, </booktitle> <pages> pages 257-266, </pages> <address> Boston, Massachusettes, </address> <year> 1990. </year>
Reference-contexts: Theory approximation is thus treated as a means to converging towards a correct and consistent theory. This has led to the definition of Probably Approximately Tractable (PAT) Learning <ref> [Tadepalli, 1990] </ref> which tries to extend the notion of Probably Approximately Correct (PAC) Learning as introduced in [Valiant, 1984] to capture the additional constraint of learning a tractable theory.
Reference: [Tecuci and Kodratoff, 1990] <editor> Gheorghe Tecuci and Yves Kodratoff. </editor> <title> Apprenticeship learning in imperfect domain theories. </title> <editor> In Yves Kodratoff and Ryszard S. Michalski, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. III, </volume> <pages> pages 514-551. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: DISCIPLE [Kodratoff and Tecuci, 1987] is another integration of inductive and deductive learning methods. In subsequent work learning by analogy was added as an additional means of dealing with imperfect domain theories <ref> [Tecuci and Kodratoff, 1990] </ref>. From this work the Multistrategy Task-adaptive Learning framework (MTL) [Tecuci and Michalski, 1991] has emerged. MTL is a Multistrategy approach to learning in the Plausible Reasoning framework of [Collins and Michalski, 1989].
Reference: [Tecuci and Michalski, 1991] <author> Gheorghe D. Tecuci and Ryszard S. Michalski. </author> <title> A method for multistrategy task-adaptive learning based on plausible justifications. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 549-553, </pages> <address> Evanston, Illinois, </address> <year> 1991. </year>
Reference-contexts: DISCIPLE [Kodratoff and Tecuci, 1987] is another integration of inductive and deductive learning methods. In subsequent work learning by analogy was added as an additional means of dealing with imperfect domain theories [Tecuci and Kodratoff, 1990]. From this work the Multistrategy Task-adaptive Learning framework (MTL) <ref> [Tecuci and Michalski, 1991] </ref> has emerged. MTL is a Multistrategy approach to learning in the Plausible Reasoning framework of [Collins and Michalski, 1989]. The approach is adaptive in the sense that it is able to apply different learning strategies for different learning 48 6 MULTISTRATEGY LEARNING tasks.
Reference: [Tenenberg, 1987] <author> J. Tenenberg. </author> <title> Preserving consistency across abstraction mappings. </title> <booktitle> In Proceedings of the 10th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1011-1014, </pages> <address> Milano, Italy, </address> <year> 1987. </year>
Reference-contexts: Approaches using abstraction hierarchies seem to attack this problem in a similar fashion: Concepts recognized at a high level of abstraction do not necessarily have to be true after examining the situation with a more detailed domain theory. This is known as the "false proof" problem (see <ref> [Plaisted, 1981, Tenenberg, 1987] </ref> and section 5.2). 18 3 RULE APPROXIMATION 3.5 The Multiple Explanation Problem Reasoning with inconsistent theories has not been thoroughly investigated yet. [Roos, 1992] gives a framework for using logic in inconsistent domain theories, but no large-scale implementation of a system for reasoning with inconsistent knowledge is <p> A serious problem that may arise with Plaisted's formalization of abstraction is the false proof problem, i.e. consistent theories might become inconsistent after abstraction. <ref> [Tenenberg, 1987] </ref> introduces restricted predicate mappings, a subset of abstraction mappings, to deal with this problem. Predicate mappings are functions that map predicate symbols from one first order language to those of another, where they possibly may result in a common symbol, representing a superconcept of the original predicates. <p> The false proof problem is an important issue when using abstractions for more efficient problem-solving behavior. Many systems try to solve this problem by restricting the representation languages in a way that does not al 50 7 CONCLUSION low the problem to arise. The downward solution property <ref> [Tenenberg, 1987] </ref> or the ordered monotonicity property [Knoblock, 1990b] are examples of this approach (see section 5.2). But to gain even more benefit from abstraction techniques, these constraints on the representation language and/or the abstraction mapping should be relaxed and other methods for dealing with this problem should be developed.
Reference: [Unruh and Rosenbloom, 1989] <author> Amy Unruh and Paul S. Rosenbloom. </author> <title> Abstraction in problem solving and learning. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 681-687, </pages> <year> 1989. </year>
Reference-contexts: In <ref> [Unruh and Rosenbloom, 1989] </ref> possibilities for the use of abstraction in the SOAR system [Laird et al., 1987] are suggested. As alpine and Pablo, SOAR uses a method for abstracting the search space by simply removing some aspects of the problem. <p> While in the systems discussed above this is done a priori in a static way, in SOAR abstraction occurs dynamically during problem solving. Whenever SOAR encounters and impasse during search, it usually resolves this by proposing a new subgoal [Laird et al., 1987]. <ref> [Unruh and Rosenbloom, 1989] </ref> suggest that instead of further problem solving in another level of subgoals the impasse may as well be removed by ignoring the sub-goal. This is very similar to the approach taken in [Keller, 1988] (see section 3.5.1).
Reference: [Valiant, 1984] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year> <note> REFERENCES 63 </note>
Reference-contexts: Theory approximation is thus treated as a means to converging towards a correct and consistent theory. This has led to the definition of Probably Approximately Tractable (PAT) Learning [Tadepalli, 1990] which tries to extend the notion of Probably Approximately Correct (PAC) Learning as introduced in <ref> [Valiant, 1984] </ref> to capture the additional constraint of learning a tractable theory.
Reference: [Varsek, 1992] <author> Alen Varsek. </author> <title> Qualitative model evolution. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1311-1316, </pages> <year> 1992. </year>
Reference-contexts: We will discuss a simplified version of the QSIM approach to qualitative reasoning in a little more detail in the next section, because of its simplicity and its wide use for Machine Learning research (e.g. <ref> [Coiera, 1989a, Bratko et al., 1991, Varsek, 1992] </ref>). 4.1.2 QSIM Qualitative SIMulation (QSIM) [Kuipers, 1986] is the most recent approach to qualitative reasoning. Its main purpose was to allow simulation | envi-sionment | of processes. <p> The application of other ILP systems | FOIL [Quinlan, 1990] and LINUS [Lavrac et al., 1991] | is reported to be less successful on this particular problem. A very interesting approach to the same problem domain using a genetic algorithm can be found in <ref> [Varsek, 1992] </ref>. Qualitative Models are represented in hierarchical binary trees. Genetic operators | crossover and mutation | change the population of these tree structures and only the fittest models survive.
Reference: [Weld and deKleer, 1990] <author> D. S. Weld and J. deKleer. </author> <title> Readings in Qualitative Reasoning about Physical Systems. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: Although chess | being the favorite domain of the author | will appear quite often in this paper, we want to stress that the general principles exemplified by this domain are valid for all complex, intractable domains (e.g. reasoning about physical systems <ref> [Weld and deKleer, 1990] </ref>, medical domains [Bratko et al., 1989], music [Widmer, 1992b], chess [Tadepalli, 1989], [Flann, 1989] etc.). The next three chapters give a review of previous work in Machine Learning on the topics of approximation, learning with qualitative and plausible theories and abstraction. <p> Among the number of qualitative knowledge representation formalisms that have emerged over the years <ref> [Weld and deKleer, 1990, AI-, 1984, AI-, 1991] </ref> three formalisms have now become "standard": * Qualitative Process Theory [Forbus, 1984] * Qualitative Simulation [Kuipers, 1986] * Reasoning with Confluences [deKleer and Brown, 1984] All of the above systems support some kind of qualitative algebra [Williams, 1988] that allows them to reason
Reference: [Widmer, 1989a] <author> Gerhard Widmer. </author> <title> An incremental version of Bergadano & Gior-dana's integrated learning strategy. </title> <booktitle> In Proceedings of the Fourth European Working Session on Learning, </booktitle> <address> Montpellier, France, </address> <year> 1989. </year>
Reference-contexts: The algorithm can also be reformulated in an incremental way as has been shown in <ref> [Widmer, 1989a] </ref>. A related approach can be found in [Pazzani and Kibler, 1992]. FOCL (First-Order Combined Learner) tries to combine the virtues of the First-Order Inductive Learner FOIL [Quinlan, 1990] and Explanation Based Learning.
Reference: [Widmer, 1989b] <author> Gerhard Widmer. </author> <title> A tight integration of deductive and inductive learning. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <address> Ithaca, New York, </address> <year> 1989. </year>
Reference-contexts: Multistrategy Learning has been mostly investigated in the context of integrating inductive and deductive learning methods. Well-known systems that enhance Explanation-Based Learning methods with an inductive learning component are UNIMEM [Lebowitz, 1986] and OCCAM [Pazzani, 1990]. Other approaches can be found in [Flann and Dietterich, 1989] and <ref> [Widmer, 1989b] </ref>. In Inductive Logic Programming, the systems FOCL [Pazzani and Kibler, 1992] and ML-SMART [Bergadano and Giordana, 1988] try to integrate background knowledge into inductive learning methods. DISCIPLE [Kodratoff and Tecuci, 1987] is another integration of inductive and deductive learning methods.
Reference: [Widmer, 1991] <author> Gerhard Widmer. </author> <title> Using plausible explanations to bias empirical generalization in weak theory domains. </title> <booktitle> In Proceedings of the 5th European Working Session on Learning, Porto, </booktitle> <address> Portugal, </address> <year> 1991. </year>
Reference-contexts: New rules are learned by using plausible explanation trees which are stored for further use by incremental induction. This process is described in detail in <ref> [Widmer, 1991] </ref> While the approaches described above extend Explanation-Based Learning with the use of plausible inferences using qualitative background knowledge, [Clark and Matwin, 1993] present an approach using directed dependencies to guide the search for correct rules in inductive learning.
Reference: [Widmer, 1992a] <author> Gerhard Widmer. </author> <title> Learning with a qualitative domain theory by means of plausible explanations. </title> <editor> In Ryszard S. Michalski and Gheorghe D. Tecuci, editors, </editor> <title> Machine Learning: A Multistrategy Approach, </title> <booktitle> Vol. IV. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: problem by extending 5 Michalski's recent Inferential Theory of Learning [Michalski, 1992] views some of the basic ideas of from the standpoint of Machine Learning. 22 4 PLAUSIBLE THEORIES explanation-based schema formation to a powerful generalization algorithm by replacing implication in the domain theory with some weaker forms of inference <ref> [Kodratoff and Tecuci, 1989, Widmer, 1992a] </ref>. 4.1 Qualitative Reasoning about Physical Systems 4.1.1 Qualitization In the middle of the 80's a new direction of research in qualitative knowledge representation has started: Qualitative Reasoning about Physical Systems. <p> One of the shortcomings of DeJong's approach is the lack of ability to rate the plausibility of explanations and that it is unable to incrementally modify learned concepts in the face of new evidence. Based on the plausible EBL approach of DeJong, <ref> [Widmer, 1992a] </ref> uses plausible explanations for better generalizations in plausible proof trees. Widmer's system uses estimated degrees of plausibility to assess the value of an explanation, quite similarly to the approaches of section 3.5.1. <p> Research on using rule approximations as reported in [Fawcett, 1989, Knoblock, 1989, Keller, 1988, Cohen, 1990, Tadepalli, 1989] and on qualitative extensions of the representation language <ref> [DeJong, 1989, Widmer, 1992a, Mozetic and Holzbaur, 1991, Bennett, 1989] </ref> in traditional Explanation-Based Learning environments shows that a tradeoff between correctness and efficiency is an important issue. Applications of this finding to induction are still waiting to be investigated.
Reference: [Widmer, 1992b] <author> Gerhard Widmer. </author> <title> Qualitative perception modeling and intelligent musical learning. </title> <journal> Computer Music Journal, </journal> <volume> 16(2) </volume> <pages> 51-68, </pages> <year> 1992. </year>
Reference-contexts: being the favorite domain of the author | will appear quite often in this paper, we want to stress that the general principles exemplified by this domain are valid for all complex, intractable domains (e.g. reasoning about physical systems [Weld and deKleer, 1990], medical domains [Bratko et al., 1989], music <ref> [Widmer, 1992b] </ref>, chess [Tadepalli, 1989], [Flann, 1989] etc.). The next three chapters give a review of previous work in Machine Learning on the topics of approximation, learning with qualitative and plausible theories and abstraction.
Reference: [Wilkins, 1982] <author> David E. Wilkins. </author> <title> Using patterns and plans in chess. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 1-51, </pages> <year> 1982. </year> <note> Reprinted in [Levy, </note> <year> 1986]. </year>
Reference-contexts: This philosophy has also led to several chess programs that tried to make use of explicit representation for plans and goals. But these approaches were only successful in limited subdo-mains of chess, as e.g. tactical combinations <ref> [Wilkins, 1982] </ref>, pawn endgames [Berliner and Campbell, 1984] etc. The main problem was the complexity of the domain.
Reference: [Wilkins, 1990] <author> David C. Wilkins. </author> <title> Knowledge base refinement as improving an incorrect and incomplete domain theory. </title> <editor> In Yves Kodratoff and Ryszard S. Michalski, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. III, </volume> <pages> pages 493-513. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: Further research has to be done to allow inconsistent information in the knowledge base. Other systems try to integrate Explanation-Based Learning and Neural Nets [Shavlik and Towell, 1989] or EBL and Case-Based Learning <ref> [Wilkins, 1990] </ref>.
Reference: [Williams, 1988] <author> B. Williams. </author> <title> MINIMA | A symbolic approach to qualitative algebraic reasoning. </title> <booktitle> In Proceedings of the 7th National Conference on Artificial Intelligence, </booktitle> <pages> pages 264-269, </pages> <address> Minneapolis, Minnesota, </address> <year> 1988. </year>
Reference-contexts: that have emerged over the years [Weld and deKleer, 1990, AI-, 1984, AI-, 1991] three formalisms have now become "standard": * Qualitative Process Theory [Forbus, 1984] * Qualitative Simulation [Kuipers, 1986] * Reasoning with Confluences [deKleer and Brown, 1984] All of the above systems support some kind of qualitative algebra <ref> [Williams, 1988] </ref> that allows them to reason with qualitative function descriptions. In physical systems qualitative differential equations (QDEs) are very important and thus form central notions of most representation languages. We will shortly introduce the three systems following the more elaborate discussion in [Coiera, 1992].
Reference: [Zweben and Chase, 1988] <author> Monte Zweben and Melissa P. Chase. </author> <title> Improving op-erationality with approximate heuristics. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Explanation-Based Learning, </booktitle> <pages> pages 100-106, </pages> <address> Palo Alto, Califor-nia, </address> <year> 1988. </year>
Reference-contexts: Experiments have shown that MetaLEX was able to improve its efficiency due to its ability to explicitly reason about costs and benefits of approximating. A similar approach to increasing a problem solver's efficiency while maintaining its effectiveness by dropping conditions can be found in the works of <ref> [Zweben and Chase, 1988] </ref> and [Chase et al., 1989]. The ULS system transforms rules generated by EBL into approximate rules using statistical measures. Rules are generalized by dropping conditions that are true most of the time and do not introduce new variable bindings.
References-found: 143

