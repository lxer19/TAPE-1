URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/reinforcement/papers/nrdp.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs/user/awm/web/papers.html
Root-URL: 
Title: Applying Online Search Techniques to Reinforcement Learning  
Author: Scott Davies Andrew Y. Ng Andrew Moore 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie-Mellon University  
Abstract: In reinforcement learning it is frequently necessary to resort to an approximation to the true optimal value function. Here we investigate the benefits of online search in such cases. We examine "local" searches, where the agent performs a finite-depth lookahead search, and "global" searches, where the agent performs a search for a trajectory all the way from the current state to a goal state. The key to the success of these methods lies in taking a value function, which gives a rough solution to the hard problem of finding good trajectories from every single state, and combining that with online search, which then gives an accurate solution to the easier problem of finding a good trajectory specifically from the current state.
Abstract-found: 1
Intro-found: 1
Reference: [ 1 ] <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming and optimal control, volume 1. </title> <publisher> Athena Scientific, </publisher> <year> 1995. </year>
Reference-contexts: During execution, the local search algorithm iteratively finds the best trajectory T with the search algorithm above, executes the first action on that trajectory, and then does a new search from the resulting state. If B is the "parallel backup operator" <ref> [ 1 ] </ref> so that BV (s) = max a2A R (s; a) + V (ffi (s; a)), then executing the full jAj d search is formally equivalent to executing the greedy policy with respect to the value function B d1 V .
Reference: [ 2 ] <author> G. Boone. </author> <title> Minimum-Time Control of the Acrobot. </title> <booktitle> In International Conference on Robotics and Automation, </booktitle> <year> 1997. </year>
Reference-contexts: The computational expense is O (djAj d ). To make deeper searches computationally cheaper, we might consider only a subset of these trajectories. Especially for dynamic control, often an optimal trajectory repeatedly selects and then holds a certain action for some time, such as suggested by <ref> [ 2 ] </ref> ; a natural subset of the jA d j trajectories, therefore, are trajectories that switch their actions rarely.
Reference: [ 3 ] <author> S. Davies. </author> <title> Multidimensional Triangulation and Interpolation for Reinforcement Learning. </title> <booktitle> In Neural Information Processing Systems 9, 1996. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: Otherwise, the search has failed, because our grid was too coarse, our state transition model inaccurate, or the problem insoluble. An example of the search performed by this algorithm on the modified-car domain is shown in Figure 5. The value function was approximated with a simplex-based interpolation <ref> [ 3 ] </ref> on a coarse 7 by 7 grid, with all other parameters the same as in 4 Experiments We tested our algorithms on the following domains 2 : * modified-car (2 dimensional): A car has to reach and park at the top of a one dimensional hill; the car <p> The algorithm we used to learn a value function is the simplex-interpolation algorithm described in <ref> [ 3 ] </ref> .
Reference: [ 4 ] <author> R. E. Korf. </author> <title> Real-Time Heuristic Search. </title> <journal> Artifical Intelligence, </journal> <volume> 42, </volume> <year> 1990. </year>
Reference-contexts: This approach, a form of receding horizon control, has most famously been applied to minimax game playing programs [ 8 ] and has also been used in single-agents on small discrete domains (e.g. <ref> [ 4 ] </ref> ). 3 Global Search Local search is not the only way to improve an approximate value function. Here, we describe global search for solving least-cost-to-goal problems in continuous state spaces with non-negative costs.
Reference: [ 5 ] <author> J. Latombe. </author> <title> Robot Motion Planning. </title> <publisher> Kluwer, </publisher> <year> 1991. </year>
Reference-contexts: In order to deal with this problem, we borrow a technique from robot motion planning <ref> [ 5 ] </ref> . We first divide the state space up into a fine uniform grid. A local search procedure is then used to find paths from one grid element to another.
Reference: [ 6 ] <author> A. W. Moore and C. G. Atkeson. </author> <title> The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-spaces. </title> <journal> Machine Learning, </journal> <volume> 21, </volume> <year> 1995. </year>
Reference-contexts: Note this is slightly more difficult than the normal mountain car, as we require a velocity near 0 at the top of the hill <ref> [ 6 ] </ref> . State consists of x-position and velocity. Actions are accelerate forward or backward. * acrobot (4 dimensional): An acrobot is a two-link planar robot acting in the vertical plane under gravity with only one weak actuator at its elbow joint. <p> in formed search permits us to survive 50 4 grids, but to properly thwart the curse of dimensionality we can conclude that (i) stronger approximation than simplex grids are needed, (ii) Informed Global Search is much more tractable than Uninformed Global Search, and (iii) variable resolution methods (e.g. extensions to <ref> [ 6 ] </ref> ) might be needed.
Reference: [ 7 ] <author> N. J. Nilsson. </author> <booktitle> Problem-solving Methods in Artificial Intelligence. </booktitle> <publisher> McGraw Hill, </publisher> <year> 1971. </year>
Reference-contexts: the area where the actual trajectory diverges from the predicted/planned trajectory and thereby improve its model of the world in that area. 3.2 Informed Global Search We can modify Uninformed Global Search by using an approximated value function to guide the search expansions in the style of A fl search <ref> [ 7 ] </ref> , (written out in detail below). With the perfect value function, this causes the search to traverse exactly the optimal path to the goal; with only an approximation to the value function, it can still dramatically reduce the fraction of the state space that is searched.
Reference: [ 8 ] <author> S. Russell and P. Norvig. </author> <title> Artificial Intelligence A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: Lastly, for well chosen s, we expect the cheaper, restricted search to approximate this full search well. This approach, a form of receding horizon control, has most famously been applied to minimax game playing programs <ref> [ 8 ] </ref> and has also been used in single-agents on small discrete domains (e.g. [ 4 ] ). 3 Global Search Local search is not the only way to improve an approximate value function. <p> This is a phenomenon that often occurs in A fl - like searches when one's heuristic evaluation function is not strictly "optimistic" <ref> [ 8 ] </ref> . This is not a problem for Uninformed Global Search, which is effectively using the maximally optimistic "constant 0" evaluation function. 5 Learning a Model Online Occasionally, the state transition function is not known but rather must be learned online. <p> Lastly, algorithms to learn reasonably accurate yet consistently "optimistic" <ref> [ 8 ] </ref> value functions might be helpful for Informed Global Search. a twenty-step search with at most one switch in actions at upper left. on modified-car with model learning. dot is starting state; the small dots are grid elements' "representative states." modified-car.
Reference: [ 9 ] <author> R. S. Sutton. </author> <title> Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding. </title> <editor> In D. Touretzky, M. Mozer, and M. Hasselmo, editors, </editor> <booktitle> Neural Information Processing Systems 8, </booktitle> <year> 1996. </year>
Reference-contexts: Similarly, if g 0 has 1 This has a flavor not dissimilar to the hashed sparse coarse encodings of Sutton <ref> [ 9 ] </ref> . been visited before, but p (g 0 ) R T (s 0 ; : : : ; s 0 )+fl jT j V (s 0 ), then update p (g 0 ) to the latter quantity and set g 0 's "representative state" to s 0 . <p> Actions are accelerate forward or backward. * acrobot (4 dimensional): An acrobot is a two-link planar robot acting in the vertical plane under gravity with only one weak actuator at its elbow joint. The goal is to raise the hand at least one link's height above the shoulder <ref> [ 9 ] </ref> . State consists of joint angles and angular velocities at the shoulder and elbow.
References-found: 9

