URL: ftp://ftp.cs.umass.edu/pub/mckinley/ieee.ps.gz
Refering-URL: http://spa-www.cs.umass.edu/bibliography.html
Root-URL: 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. Kuck, R. Kuhn, B. Leasure, and M. J. Wolfe, </author> <title> "Analysis and transformation of programs for par-allel computation," </title> <booktitle> in Proceedings of COMPSAC 80, the 4th International Computer Software and Applications Conference, </booktitle> <address> (Chicago, IL), </address> <pages> pp. 709-715, </pages> <month> Oct. </month> <year> 1980. </year>
Reference-contexts: In pursuit of this goal, several research projects and prototypes have been built to investigate automatic parallelization. These include Parafrase at the University of Illinois <ref> [1] </ref>, PFC at Rice University [2], PTRAN at IBM Research [3] and SUIF at Stanford University [4]. In addition, several commercial systems, such as Vast by Pacific Sierra, KAP by Kuck and Associates, and compilers by Alliant, Convex, and Cray, perform automatic paralleliza- tion. <p> A substantial amount of work has been conducted on 15 the viability of this approach by researchers at Rice and elsewhere <ref> [1, 2, 3, 15, 46] </ref>. Ideally, automatically parallelized programs will execute efficiently on the target architecture and users will not need to intervene. Although such systems can effectively parallelize many interesting programs, they have not established an acceptable level of success.
Reference: [2] <author> J. R. Allen and K. Kennedy, </author> <title> "PFC: A program to convert Fortran to parallel form," in Supercomputers: Design and Applications (K. Hwang, </title> <publisher> ed.), </publisher> <pages> pp. 186-203, </pages> <address> Silver Spring, MD: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1984. </year>
Reference-contexts: In pursuit of this goal, several research projects and prototypes have been built to investigate automatic parallelization. These include Parafrase at the University of Illinois [1], PFC at Rice University <ref> [2] </ref>, PTRAN at IBM Research [3] and SUIF at Stanford University [4]. In addition, several commercial systems, such as Vast by Pacific Sierra, KAP by Kuck and Associates, and compilers by Alliant, Convex, and Cray, perform automatic paralleliza- tion. <p> A substantial amount of work has been conducted on 15 the viability of this approach by researchers at Rice and elsewhere <ref> [1, 2, 3, 15, 46] </ref>. Ideally, automatically parallelized programs will execute efficiently on the target architecture and users will not need to intervene. Although such systems can effectively parallelize many interesting programs, they have not established an acceptable level of success.
Reference: [3] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante, </author> <title> "An overview of the PTRAN analysis system for multiprocessing," </title> <booktitle> in Proceedings of the First International Conference on Supercomputing, </booktitle> <address> Athens, Greece: </address> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1987. </year>
Reference-contexts: In pursuit of this goal, several research projects and prototypes have been built to investigate automatic parallelization. These include Parafrase at the University of Illinois [1], PFC at Rice University [2], PTRAN at IBM Research <ref> [3] </ref> and SUIF at Stanford University [4]. In addition, several commercial systems, such as Vast by Pacific Sierra, KAP by Kuck and Associates, and compilers by Alliant, Convex, and Cray, perform automatic paralleliza- tion. <p> A substantial amount of work has been conducted on 15 the viability of this approach by researchers at Rice and elsewhere <ref> [1, 2, 3, 15, 46] </ref>. Ideally, automatically parallelized programs will execute efficiently on the target architecture and users will not need to intervene. Although such systems can effectively parallelize many interesting programs, they have not established an acceptable level of success.
Reference: [4] <author> S. Tjiang, M. E. Wolf, M. S. Lam, K. L. Pieper, and J. L. Hennessy, </author> <title> "Integrating scalar optimization and parallelization," </title> <booktitle> in Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: In pursuit of this goal, several research projects and prototypes have been built to investigate automatic parallelization. These include Parafrase at the University of Illinois [1], PFC at Rice University [2], PTRAN at IBM Research [3] and SUIF at Stanford University <ref> [4] </ref>. In addition, several commercial systems, such as Vast by Pacific Sierra, KAP by Kuck and Associates, and compilers by Alliant, Convex, and Cray, perform automatic paralleliza- tion.
Reference: [5] <author> B. Leasure, ed., </author> <title> PCF Fortran: Language Definition, version 3.1. Champaign, IL: </title> <booktitle> The Parallel Computing Forum, </booktitle> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: Usually, these take the form of extensions to standard Fortran. For shared-memory parallel computers, a typical extension permits the programmer to specify one or more loops that should execute in parallel without synchronization between iterations. Parallel Computer Forum (PCF) Fortran is an example of this kind of extension <ref> [5] </ref>. It lets the programmer asserts that a loop is to run in parallel, even if the compiler's analysis does not support this decision. Unfortunately, programmer-specified parallelism can be incorrect. The programmer can specify parallel execution for a loop that does not have independent iterations.
Reference: [6] <author> B. Shei and D. Gannon, "SIGMACS: </author> <title> A programmable programming environment," </title> <booktitle> in Proceedings of the Third Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> (Irvine, CA), </address> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: It includes a mechanical method for locating all data races in a program with respect to a given data set. It can be used for programs written in a useful subset of PCF Fortran. Although several research and production systems provide some features found in the ParaScope environment <ref> [6, 7, 8] </ref>, ParaScope is unique for its integration of these functions into a unified system. The next three sections of this paper provide an overview of the issues, problems, and methods that have arisen during the implementation of the three ParaScope components described above. <p> Ped has influenced and been influenced by other 22 parallel programming tools and environments, such as MIMDizer and its predecessor Forge by Pacific Sierra [63, 64], Sigmacs in the Faust programming environment <ref> [65, 6] </ref>, and Superb [8]. In addition, Ped has been continually evaluated internally and externally by users, compiler writers, and user interface designers [66, 47, 30, 67].
Reference: [7] <author> K. Smith and W. Appelbe, </author> <title> "PAT an interactive Fortran parallelizing assistant tool," </title> <booktitle> in Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <address> (St. Charles, IL), </address> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: It includes a mechanical method for locating all data races in a program with respect to a given data set. It can be used for programs written in a useful subset of PCF Fortran. Although several research and production systems provide some features found in the ParaScope environment <ref> [6, 7, 8] </ref>, ParaScope is unique for its integration of these functions into a unified system. The next three sections of this paper provide an overview of the issues, problems, and methods that have arisen during the implementation of the three ParaScope components described above.
Reference: [8] <author> H. Zima, H.-J. Bast, and M. Gerndt, </author> <title> "SUPERB: A tool for semi-automatic MIMD/SIMD parallelization," </title> <journal> Parallel Computing, </journal> <volume> vol. 6, </volume> <pages> pp. 1-18, </pages> <year> 1988. </year>
Reference-contexts: It includes a mechanical method for locating all data races in a program with respect to a given data set. It can be used for programs written in a useful subset of PCF Fortran. Although several research and production systems provide some features found in the ParaScope environment <ref> [6, 7, 8] </ref>, ParaScope is unique for its integration of these functions into a unified system. The next three sections of this paper provide an overview of the issues, problems, and methods that have arisen during the implementation of the three ParaScope components described above. <p> Ped has influenced and been influenced by other 22 parallel programming tools and environments, such as MIMDizer and its predecessor Forge by Pacific Sierra [63, 64], Sigmacs in the Faust programming environment [65, 6], and Superb <ref> [8] </ref>. In addition, Ped has been continually evaluated internally and externally by users, compiler writers, and user interface designers [66, 47, 30, 67]. In the summer of 1991, we held a workshop on ParaScope to obtain more feedback from users and to further refine our tool [30].
Reference: [9] <author> U. Banerjee, </author> <title> Dependence Analysis for Supercomputing. </title> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: For this reason, the compiler performs dependence analysis to locate the potential data and control dependences within a program <ref> [9, 10, 11, 12] </ref>. A data dependence exists between two memory references if they can access the same location.
Reference: [10] <author> J. Ferrante, K. Ottenstein, and J. Warren, </author> <title> "The program dependence graph and its use in optimization," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 9, </volume> <pages> pp. 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: For this reason, the compiler performs dependence analysis to locate the potential data and control dependences within a program <ref> [9, 10, 11, 12] </ref>. A data dependence exists between two memory references if they can access the same location.
Reference: [11] <author> D. J. Kuck, </author> <booktitle> The Structure of Computers and Computations, </booktitle> <volume> vol. </volume> <pages> 1. </pages> <address> New York: </address> <publisher> John Wiley and Sons, </publisher> <year> 1978. </year>
Reference-contexts: For this reason, the compiler performs dependence analysis to locate the potential data and control dependences within a program <ref> [9, 10, 11, 12] </ref>. A data dependence exists between two memory references if they can access the same location.
Reference: [12] <author> M. J. Wolfe, </author> <title> Optimizing Supercompilers for Supercomputers. </title> <address> Cambridge, MA: </address> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: For this reason, the compiler performs dependence analysis to locate the potential data and control dependences within a program <ref> [9, 10, 11, 12] </ref>. A data dependence exists between two memory references if they can access the same location.
Reference: [13] <author> G. Goff, K. Kennedy, and C. Tseng, </author> <title> "Practical dependence testing," </title> <booktitle> in ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year> <month> 37 </month>
Reference-contexts: Compilers use control and data dependences to prove the safety of transformations that change the control-flow in a program. To perform dependence analysis, pairs of references are tested to determine if they can access the same memory location. ParaScope's dependence analysis applies a hierarchy of tests on each reference <ref> [13] </ref>. It 7 starts with inexpensive tests that can analyze the most commonly occurring subscript references. More expensive tests are applied when the simpler tests fail to prove or disprove dependence. Automatic parallelizers try to transform the program so that the iterations of a loop execute in parallel. <p> Users may sharpen Ped's dependence analysis by classifying each potential dependence as accepted or rejected . Each dependence starts out as proven or pending. If Ped proves a dependence exists with exact dependence tests <ref> [13] </ref>, the dependence is marked as proven; otherwise it is marked pending. When users mark dependences as rejected, they are asserting that the dependences are not feasible and that the system should ignore them. Therefore, rejected dependences are disregarded when Ped considers the safety of a parallelizing transformation.
Reference: [14] <author> K. D. Cooper, K. Kennedy, and L. Torczon, </author> <title> "The impact of interprocedural analysis and optimization in the R n programming environment," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 8, </volume> <pages> pp. 491-523, </pages> <month> Oct. </month> <year> 1986. </year>
Reference-contexts: More complex annotations include a data-dependence graph for each procedure and information about interprocedural constants. Cooperation between the tools, coordinated through a central repository where the tools store derived information, leads to a more efficient implementation of interprocedural techniques <ref> [14] </ref>. The ParaScope editor derives information about individual procedures. The composition editor lets the user specify the individual procedures to be included in a program; it produces information that describes the set of possible procedure invocations.
Reference: [15] <author> K. S. M c Kinley, </author> <title> Automatic and Interactive Parallelization. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, TX, </institution> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: In the database, program-specific objects must be associated with the program. Only objects and annotations that are independent of any context program are associated with the procedure. Building on this framework, we plan to build a prototype source-to-source parallelizer for shared-memory multiprocessors <ref> [15] </ref>. Using the information produced by the program compiler and a performance estimator, it will apply a combination of interprocedural transformations and parallelism-enhancing transformations to provide an initial parallel program [16, 17, 18]. <p> A substantial amount of work has been conducted on 15 the viability of this approach by researchers at Rice and elsewhere <ref> [1, 2, 3, 15, 46] </ref>. Ideally, automatically parallelized programs will execute efficiently on the target architecture and users will not need to intervene. Although such systems can effectively parallelize many interesting programs, they have not established an acceptable level of success. <p> We plan to combine the machine-dependent knowledge of the parallelizer and deep analysis of the program compiler with the program-specific knowledge of the user. We propose the following approach for ParaScope <ref> [15] </ref>. The program is first transformed using the automatic parallelizer. This source-to-source translation produces a machine-specific source version of the program. If the user is satisfied with the resultant program's performance, then the user need not intervene at all.
Reference: [16] <author> M. W. Hall, K. Kennedy, and K. S. McKinley, </author> <title> "Interprocedural transformations for parallel code gen-eration," </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Building on this framework, we plan to build a prototype source-to-source parallelizer for shared-memory multiprocessors [15]. Using the information produced by the program compiler and a performance estimator, it will apply a combination of interprocedural transformations and parallelism-enhancing transformations to provide an initial parallel program <ref> [16, 17, 18] </ref>. The next two subsections describe how the program compiler computes interprocedural data-flow information and how it applies interprocedural transformations. Recompilation analysis is handled using methods described by Burke and Torczon [19]. 2.3 Interprocedural Analysis Interprocedural data-flow analysis addresses several distinct problems. <p> Inline substitution is a simple form of interprocedural code motion [36]; it replaces a procedure call with a copy of the code for the called procedure. Loop extraction and loop embedding are two other forms of interprocedural code motion that the program compiler will use <ref> [16] </ref>. Loop extraction pulls an outermost enclosing loop from a procedure body into a calling procedure; it is a form of partial inlining. Loop embedding is the inverse operation; it pushes a loop surrounding a call site into the called procedure. <p> Procedure cloning can improve the precision of some kinds of interprocedural data-flow information, particularly interprocedural CONSTANTS sets [37]. Interprocedural code motion and procedure cloning can expose additional parallelism and improve the run- time behavior of the program <ref> [16] </ref>. Unfortunately, these interprocedural techniques can have negative side effects. * A study by Cooper, Hall, and Torczon using commercial Fortran compilers showed that inline substitution often resulted in slower running code [43, 44]. When this happened, secondary effects in the optimizer outweighed any improvements from inlining. <p> We call this strategy goal-directed inter- procedural optimization [45]. The effectiveness of this strategy in the context of parallelizing transformations was tested in an experiment with the PERFECT benchmark suite. A goal-directed approach to parallel code generation was employed to introduce parallelism and increase granularity of parallel loops <ref> [16] </ref>. Because the approach proved to be effective, we plan to use this strategy in the planned prototype source-to-source parallelizer mentioned in Section 2.2. 2.5 Application of Interprocedural Techniques Several of the techniques described are also implemented in the Convex Applications Compiler TM .
Reference: [17] <author> K. Kennedy, N. McIntosh, and K. S. M c Kinley, </author> <title> "Static performance estimation in a parallelizing com-piler," </title> <type> Tech. Rep. </type> <institution> TR91-174, Dept. of Computer Science, Rice University, </institution> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: Building on this framework, we plan to build a prototype source-to-source parallelizer for shared-memory multiprocessors [15]. Using the information produced by the program compiler and a performance estimator, it will apply a combination of interprocedural transformations and parallelism-enhancing transformations to provide an initial parallel program <ref> [16, 17, 18] </ref>. The next two subsections describe how the program compiler computes interprocedural data-flow information and how it applies interprocedural transformations. Recompilation analysis is handled using methods described by Burke and Torczon [19]. 2.3 Interprocedural Analysis Interprocedural data-flow analysis addresses several distinct problems.
Reference: [18] <author> K. Kennedy and K. S. M c Kinley, </author> <title> "Optimizing for parallelism and memory hierarchy," </title> <booktitle> in Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> (Washington, DC), </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Building on this framework, we plan to build a prototype source-to-source parallelizer for shared-memory multiprocessors [15]. Using the information produced by the program compiler and a performance estimator, it will apply a combination of interprocedural transformations and parallelism-enhancing transformations to provide an initial parallel program <ref> [16, 17, 18] </ref>. The next two subsections describe how the program compiler computes interprocedural data-flow information and how it applies interprocedural transformations. Recompilation analysis is handled using methods described by Burke and Torczon [19]. 2.3 Interprocedural Analysis Interprocedural data-flow analysis addresses several distinct problems.
Reference: [19] <author> M. Burke and L. Torczon, </author> <title> "Interprocedural optimization: Eliminating unnecessary recompilation." </title> <note> To appear in ACM Transactions on Programming Languages and Systems. </note>
Reference-contexts: The next two subsections describe how the program compiler computes interprocedural data-flow information and how it applies interprocedural transformations. Recompilation analysis is handled using methods described by Burke and Torczon <ref> [19] </ref>. 2.3 Interprocedural Analysis Interprocedural data-flow analysis addresses several distinct problems.
Reference: [20] <author> M. W. Hall, </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, TX, </institution> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: However, most programming languages include some form of procedure-valued variables; this feature complicates the construction process. The program compiler uses an algorithm due to Hall and Kennedy to construct an approximate call graph for languages with procedure-valued parameters <ref> [20] </ref>. For such languages, the procedure variables may only receive their values through the parameter passing mechanism.
Reference: [21] <author> K. D. Cooper and K. Kennedy, </author> <title> "Interprocedural side-effect analysis in linear time," </title> <booktitle> in Proceedings of the ACM SIGPLAN '88 Conference on Programming Languages Design and Implementation, SIGPLAN Notices 23(7), </booktitle> <pages> pp. 57-66, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: along some path from the call. (In contrast, a flow-sensitive version of MOD would contain only those variables modified along every path from the call.) The program compiler derives MOD and USE sets by solving a set of simultaneous equations posed over the call graph or one of its derivatives <ref> [21] </ref>. Regular section side-effect analysis. Classical summary analysis techniques treat array elements naively | they deal only with the entire array. For example, a 2 MOD (p) asserts that one or more elements of the array a may be modified by a call to p.
Reference: [22] <author> D. Callahan and K. Kennedy, </author> <title> "Analysis of interprocedural side effects in a parallel programming envi-ronment," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 5, </volume> <pages> pp. 517-550, </pages> <year> 1988. </year>
Reference-contexts: Because of this lack of precision, classical MOD and REF sets produce only limited improvement in the precision of dependence information for array subscripts. To provide better information about side effects to arrays, the program compiler can compute a more complex form of summary information known as regular sections <ref> [22] </ref>. The basic idea is to replace the single-bit representation of array side effects with a richer lattice that includes the representation of entire subregions of arrays. A regular section is simply a subregion that has an exact representation in the given lattice. The following example illustrates this idea.
Reference: [23] <author> P. Havlak and K. Kennedy, </author> <title> "An implementation of interprocedural bounded regular section analysis," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: We are implementing regular section side-effect analysis in the program compiler. It has been implemented in the PFC system since 1989; our experience with that implementation shows that the analysis is both efficient and effective <ref> [23] </ref>. Kill analysis. An important part of determining whether a loop can be run in parallel is the identification of variables that can be made private to the loop body. <p> Unfortunately, it is problematic to apply these transformations when the loops lie in different procedures. Thus, the program compiler uses interprocedural code motion to relocate these loops. 2. improving dependence information experience shows that interprocedural information can improve the precision of dependence analysis for loops that contain procedure calls <ref> [23, 39, 40, 41, 42] </ref>. Procedure cloning can improve the precision of some kinds of interprocedural data-flow information, particularly interprocedural CONSTANTS sets [37]. Interprocedural code motion and procedure cloning can expose additional parallelism and improve the run- time behavior of the program [16].
Reference: [24] <author> E. Myers, </author> <title> "A precise inter-procedural data flow algorithm," </title> <booktitle> in Conference Record of the Eighth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: To locate private variables, the compiler can perform KILL analysis. We say that a value is killed if it is redefined along every path that leads to a subsequent reference. For scalar variables, intraprocedural KILL analysis is well understood. The interprocedural problem is intractable in general <ref> [24] </ref>; methods that compute approximate solutions have been proposed and implemented [25]. As with summary analysis, computing KILL information is more complex for arrays than for scalar variables [26, 27, 28]. In practice, some simple cases arise in loops. These are easier to detect than the general case.
Reference: [25] <author> D. Callahan, </author> <title> "The program summary graph and flow-sensitive interprocedural data flow analysis," </title> <booktitle> in Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, SIGPLAN Notices 23(7), </booktitle> <pages> pp. 47-56, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: We say that a value is killed if it is redefined along every path that leads to a subsequent reference. For scalar variables, intraprocedural KILL analysis is well understood. The interprocedural problem is intractable in general [24]; methods that compute approximate solutions have been proposed and implemented <ref> [25] </ref>. As with summary analysis, computing KILL information is more complex for arrays than for scalar variables [26, 27, 28]. In practice, some simple cases arise in loops. These are easier to detect than the general case.
Reference: [26] <author> E. Granston and A. Veidenbaum, </author> <title> "Detecting redundant accesses to array data," </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <address> (Albuquerque, NM), </address> <month> Nov. </month> <year> 1991. </year> <month> 38 </month>
Reference-contexts: For scalar variables, intraprocedural KILL analysis is well understood. The interprocedural problem is intractable in general [24]; methods that compute approximate solutions have been proposed and implemented [25]. As with summary analysis, computing KILL information is more complex for arrays than for scalar variables <ref> [26, 27, 28] </ref>. In practice, some simple cases arise in loops. These are easier to detect than the general case. For example, many initialization loops have the property that they define every element of some array a, and no element is referenced before its definition.
Reference: [27] <author> Z. Li, </author> <title> "Array privatization for parallel execution of loops," </title> <booktitle> in Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> (Washington, DC), </address> <month> July </month> <year> 1992. </year>
Reference-contexts: For scalar variables, intraprocedural KILL analysis is well understood. The interprocedural problem is intractable in general [24]; methods that compute approximate solutions have been proposed and implemented [25]. As with summary analysis, computing KILL information is more complex for arrays than for scalar variables <ref> [26, 27, 28] </ref>. In practice, some simple cases arise in loops. These are easier to detect than the general case. For example, many initialization loops have the property that they define every element of some array a, and no element is referenced before its definition.
Reference: [28] <author> C. Rosene, </author> <title> Incremental Dependence Analysis. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, TX, </institution> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: For scalar variables, intraprocedural KILL analysis is well understood. The interprocedural problem is intractable in general [24]; methods that compute approximate solutions have been proposed and implemented [25]. As with summary analysis, computing KILL information is more complex for arrays than for scalar variables <ref> [26, 27, 28] </ref>. In practice, some simple cases arise in loops. These are easier to detect than the general case. For example, many initialization loops have the property that they define every element of some array a, and no element is referenced before its definition. <p> As described in Section 2.1, the precision of dependence analysis relies on sophisticated data-flow information. These updates may be very expensive in an interactive setting and in some cases, incremental analysis may be more expensive than performing batch analysis for the effected procedures <ref> [28] </ref>. Therefore, after arbitrary program edits, Ped relies on efficient batch algorithms rather than on a general incremental algorithm. Updates following edits must not only recalculate intraprocedural data-flow and dependence information, but must also recompute interprocedural data-flow information.
Reference: [29] <author> R. Eigenmann and W. Blume, </author> <title> "An effectiveness study of parallelizing compiler techniques," </title> <booktitle> in Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> (St. Charles, IL), </address> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: In this case, the compiler can conclude that the loop kills a. If the loop contains a procedure call, interprocedural propagation of KILL information may be necessary. Experience suggests that KILL information on arrays, both interprocedural and intraprocedural, is important in parallelizing existing applications <ref> [29, 30] </ref>. We plan to explore methods for approximating KILL sets for both arrays and scalars. The program compiler will be our implementation vehicle. Alias analysis.
Reference: [30] <author> M. W. Hall, T. Harvey, K. Kennedy, N. McIntosh, K. S. McKinley, J. D. Oldham, M. Paleczny, and G. Roth, </author> <title> "Experiences using the ParaScope Editor," </title> <type> Tech. Rep. </type> <institution> CRPC-TR91173, Center for Research on Parallel Computation, Rice University, </institution> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: In this case, the compiler can conclude that the loop kills a. If the loop contains a procedure call, interprocedural propagation of KILL information may be necessary. Experience suggests that KILL information on arrays, both interprocedural and intraprocedural, is important in parallelizing existing applications <ref> [29, 30] </ref>. We plan to explore methods for approximating KILL sets for both arrays and scalars. The program compiler will be our implementation vehicle. Alias analysis. <p> The editor updates dependence information and source code to reflect any such programmer actions. The compiler may then use these modifications when generating code. Research on Ped began in the late 1980s. Its interface, design and implementation have been discussed in detail in other papers <ref> [47, 30, 48, 49] </ref>. This section describes a new version of the ParaScope editor from the user's point of view and discusses new research directions for Ped. <p> In addition, Ped has been continually evaluated internally and externally by users, compiler writers, and user interface designers <ref> [66, 47, 30, 67] </ref>. In the summer of 1991, we held a workshop on ParaScope to obtain more feedback from users and to further refine our tool [30]. Participants from industry, government laboratories, and supercomputing centers used ParaScope to parallelize programs they brought with them. <p> In addition, Ped has been continually evaluated internally and externally by users, compiler writers, and user interface designers [66, 47, 30, 67]. In the summer of 1991, we held a workshop on ParaScope to obtain more feedback from users and to further refine our tool <ref> [30] </ref>. Participants from industry, government laboratories, and supercomputing centers used ParaScope to parallelize programs they brought with them. For the most part, these scientific programmers, tool designers, and compiler writers reported that the user interface and functionality were appropriate to the task of parallelization.
Reference: [31] <author> J. P. Banning, </author> <title> "An efficient way to find the side effects of procedure calls and the aliases of variables," </title> <booktitle> in Proceedings of the Sixth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <address> San Antonio, TX, </address> <pages> pp. 29-41, </pages> <month> Jan. </month> <year> 1979. </year>
Reference-contexts: The program compiler will be our implementation vehicle. Alias analysis. To give the module compiler more information about the run-time environment in which a procedure will execute, the program compiler annotates each node in the call graph with an ALIAS set <ref> [31] </ref>. For a procedure p, ALIAS (p) contains pairs of names. Each pair (x; y), where x and y are either formal parameters or global variables, is an assertion that x and y may occupy the same storage location on some 12 invocation of p.
Reference: [32] <author> K. D. Cooper and K. Kennedy, </author> <title> "Fast interprocedural alias analysis," </title> <booktitle> in Conference Record of the Sixteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: Because it uses a formulation of the problem that ignores control flow within a procedure, the analysis may include some alias pairs that do not really occur at run-time. The ALIAS computation is formulated as a set of simultaneous equations posed over the call graph and its derivatives <ref> [32] </ref>. Constant propagation. Interprocedural constant propagation tags each procedure p in the call graph with a set CONSTANTS that contains (name, value) pairs. Each pair asserts that name contains value on entry to the procedure, along all paths leading to an invocation of p.
Reference: [33] <author> D. Callahan, K. D. Cooper, K. Kennedy, and L. Torczon, </author> <title> "Interprocedural constant propagation," </title> <booktitle> in Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, SIGPLAN Notices 21(7), </booktitle> <pages> pp. 152-161, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: The complexity of the jump function implementation affects the type of constants actually found by the algorithm <ref> [33] </ref>. Symbolic value analysis. Experience with dependence analyzers embedded in both research and commercial compilers has shown the value of symbolic analysis as a way of improving the precision of dependence analysis.
Reference: [34] <author> A. Aho, R. Sethi, and J. Ullman, </author> <booktitle> Compilers, Principles, Techniques and Tools. </booktitle> <address> Reading, MA: AddisonWesley, </address> <year> 1986. </year>
Reference-contexts: The analysis extends traditional inter- procedural constant propagation to include propagation of symbolic values around the call graph. It uses gated static single assignment graphs to find values that are provably equal | a notion similar to the classic value numbering technique <ref> [34, 35] </ref>. When symbolic analysis is complete, the program compiler will discover variables that have consistent symbolic values. This happens during the interprocedural analysis phase; the results are recorded with the 13 program in the repository.
Reference: [35] <author> B. Alpern, M. N. Wegman, and F. K. Zadeck, </author> <title> "Detecting equality of variables in programs," </title> <booktitle> in Conference Record of the Fifteenth ACM Symposium on Principles of Programming Languages, </booktitle> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: The analysis extends traditional inter- procedural constant propagation to include propagation of symbolic values around the call graph. It uses gated static single assignment graphs to find values that are provably equal | a notion similar to the classic value numbering technique <ref> [34, 35] </ref>. When symbolic analysis is complete, the program compiler will discover variables that have consistent symbolic values. This happens during the interprocedural analysis phase; the results are recorded with the 13 program in the repository.
Reference: [36] <author> F. E. Allen and J. Cocke, </author> <title> "A catalogue of optimizing transformations," in Design and Optimization of Compilers (J. </title> <editor> Rustin, ed.), </editor> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: Interprocedural code motion is a general term used to describe transformations that move code across a procedure boundary. Names are translated to model the effects of parameter binding and to merge the name spaces. Inline substitution is a simple form of interprocedural code motion <ref> [36] </ref>; it replaces a procedure call with a copy of the code for the called procedure. Loop extraction and loop embedding are two other forms of interprocedural code motion that the program compiler will use [16]. <p> Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [36, 54, 55, 56, 49, 57, 58, 59] </ref>. Figure 4 shows a taxonomy of the transformations supported in Ped. Reordering transformations change the order in which statements are executed, either within or across loop iterations. They are safe if all dependences in the original program are preserved.
Reference: [37] <author> K. D. Cooper, M. W. Hall, and K. Kennedy, </author> <title> "Procedure cloning," </title> <booktitle> in Proceedings of the IEEE International Conference on Computer Languages, </booktitle> <pages> pp. 96-105, </pages> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: Loop embedding is the inverse operation; it pushes a loop surrounding a call site into the called procedure. Procedure cloning lets the compiler produce multiple versions of a single procedure <ref> [37, 38] </ref>. Each call site of the procedure is assigned to a specific version of the procedure; that version can be tailored to the calling environment. Call sites with similar calling environments can share a single version. <p> Procedure cloning can improve the precision of some kinds of interprocedural data-flow information, particularly interprocedural CONSTANTS sets <ref> [37] </ref>. Interprocedural code motion and procedure cloning can expose additional parallelism and improve the run- time behavior of the program [16].
Reference: [38] <author> K. D. Cooper, K. Kennedy, and L. Torczon, </author> <title> "The impact of interprocedural analysis and optimization in the IR n programming environment," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 8, </volume> <pages> pp. 491-523, </pages> <month> Oct. </month> <year> 1986. </year>
Reference-contexts: Loop embedding is the inverse operation; it pushes a loop surrounding a call site into the called procedure. Procedure cloning lets the compiler produce multiple versions of a single procedure <ref> [37, 38] </ref>. Each call site of the procedure is assigned to a specific version of the procedure; that version can be tailored to the calling environment. Call sites with similar calling environments can share a single version.
Reference: [39] <author> M. Burke and R. Cytron, </author> <title> "Interprocedural dependence analysis and parallelization," </title> <booktitle> in Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, SIGPLAN Notices 21(7), </booktitle> <pages> pp. 163-275, </pages> <month> July </month> <year> 1986. </year> <month> 39 </month>
Reference-contexts: Unfortunately, it is problematic to apply these transformations when the loops lie in different procedures. Thus, the program compiler uses interprocedural code motion to relocate these loops. 2. improving dependence information experience shows that interprocedural information can improve the precision of dependence analysis for loops that contain procedure calls <ref> [23, 39, 40, 41, 42] </ref>. Procedure cloning can improve the precision of some kinds of interprocedural data-flow information, particularly interprocedural CONSTANTS sets [37]. Interprocedural code motion and procedure cloning can expose additional parallelism and improve the run- time behavior of the program [16].
Reference: [40] <author> Z. Li and P.-C. Yew, </author> <title> "Efficient interprocedural analysis for program restructuring for parallel programs," </title> <booktitle> in Proceedings of the SIGPLAN Symposium on Parallel Programs: Experience with Applications, Languages and Systems, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: Unfortunately, it is problematic to apply these transformations when the loops lie in different procedures. Thus, the program compiler uses interprocedural code motion to relocate these loops. 2. improving dependence information experience shows that interprocedural information can improve the precision of dependence analysis for loops that contain procedure calls <ref> [23, 39, 40, 41, 42] </ref>. Procedure cloning can improve the precision of some kinds of interprocedural data-flow information, particularly interprocedural CONSTANTS sets [37]. Interprocedural code motion and procedure cloning can expose additional parallelism and improve the run- time behavior of the program [16].
Reference: [41] <author> R. Metzger and P. Smith, </author> <title> "The CONVEX application compiler," </title> <journal> Fortran Journal, </journal> <volume> vol. 3, no. 1, </volume> <pages> pp. 8-10, </pages> <year> 1991. </year>
Reference-contexts: Unfortunately, it is problematic to apply these transformations when the loops lie in different procedures. Thus, the program compiler uses interprocedural code motion to relocate these loops. 2. improving dependence information experience shows that interprocedural information can improve the precision of dependence analysis for loops that contain procedure calls <ref> [23, 39, 40, 41, 42] </ref>. Procedure cloning can improve the precision of some kinds of interprocedural data-flow information, particularly interprocedural CONSTANTS sets [37]. Interprocedural code motion and procedure cloning can expose additional parallelism and improve the run- time behavior of the program [16]. <p> It clones procedures to improve the precision of its CONSTANTS sets, and performs limited inline substitution. Metzger and Smith report average improvements of twenty percent, with occasional speedups by a factor of four or more <ref> [41] </ref>. 3 ParaScope Editor Upon completion, the compilation system described in Section 2 will be capable of automatically converting a sequential Fortran program into a parallel version.
Reference: [42] <author> R. Triolet, F. Irigoin, and P. Feautrier, </author> <title> "Direct parallelization of call statements," </title> <booktitle> in Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, SIGPLAN Notices 21(7), </booktitle> <pages> pp. 176-185, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: Unfortunately, it is problematic to apply these transformations when the loops lie in different procedures. Thus, the program compiler uses interprocedural code motion to relocate these loops. 2. improving dependence information experience shows that interprocedural information can improve the precision of dependence analysis for loops that contain procedure calls <ref> [23, 39, 40, 41, 42] </ref>. Procedure cloning can improve the precision of some kinds of interprocedural data-flow information, particularly interprocedural CONSTANTS sets [37]. Interprocedural code motion and procedure cloning can expose additional parallelism and improve the run- time behavior of the program [16].
Reference: [43] <author> K. D. Cooper, M. W. Hall, and L. Torczon, </author> <title> "An experiment with inline substitution," </title> <journal> Software - Practice and Experience, </journal> <volume> vol. 21, </volume> <pages> pp. 581-601, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Unfortunately, these interprocedural techniques can have negative side effects. * A study by Cooper, Hall, and Torczon using commercial Fortran compilers showed that inline substitution often resulted in slower running code <ref> [43, 44] </ref>. When this happened, secondary effects in the optimizer outweighed any improvements from inlining. Inline substitution introduces recompilation dependences and increases average procedure size.
Reference: [44] <author> K. D. Cooper, M. W. Hall, and L. Torczon, </author> <title> "Unexpected side effects of inline substitution: a case study." </title> <note> to appear in Letters on Programming Languages and Systems, </note> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Unfortunately, these interprocedural techniques can have negative side effects. * A study by Cooper, Hall, and Torczon using commercial Fortran compilers showed that inline substitution often resulted in slower running code <ref> [43, 44] </ref>. When this happened, secondary effects in the optimizer outweighed any improvements from inlining. Inline substitution introduces recompilation dependences and increases average procedure size.
Reference: [45] <author> P. Briggs, K. D. Cooper, M. W. Hall, and L. Torczon, </author> <title> "Goal-directed interprocedural optimization," </title> <type> Technical Report TR90-148, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: In applying them, the program compiler should consider both the positive and negative effects. It should only apply interprocedural transformations when the potential profits outweigh the potential problems <ref> [45] </ref>. To implement this notion, the program compiler should: 1. Locate focus points in the program. A focus point must have two properties: (a) a target intraprocedural optimization would be profitable, if only it could be applied, and (b) supplying more context from other procedures would make application possible. 2. <p> Apply the target optimization. Thus, the interprocedural transformations will be used exclusively to enable application of profitable target optimizations | in this case, parallelizing transformations. We call this strategy goal-directed inter- procedural optimization <ref> [45] </ref>. The effectiveness of this strategy in the context of parallelizing transformations was tested in an experiment with the PERFECT benchmark suite. A goal-directed approach to parallel code generation was employed to introduce parallelism and increase granularity of parallel loops [16].
Reference: [46] <author> J. Singh and J. Hennessy, </author> <title> "An empirical investigation of the effectiveness of and limitations of automatic parallelization," </title> <booktitle> in Proceedings of the International Symposium on Shared Memory Multiprocessors, </booktitle> <address> (Tokyo, Japan), </address> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: A substantial amount of work has been conducted on 15 the viability of this approach by researchers at Rice and elsewhere <ref> [1, 2, 3, 15, 46] </ref>. Ideally, automatically parallelized programs will execute efficiently on the target architecture and users will not need to intervene. Although such systems can effectively parallelize many interesting programs, they have not established an acceptable level of success.
Reference: [47] <author> K. Fletcher, K. Kennedy, K. S. M c Kinley, and S. Warren, </author> <title> "The ParaScope Editor: User interface goals," </title> <type> Tech. Rep. </type> <institution> TR90-113, Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: The editor updates dependence information and source code to reflect any such programmer actions. The compiler may then use these modifications when generating code. Research on Ped began in the late 1980s. Its interface, design and implementation have been discussed in detail in other papers <ref> [47, 30, 48, 49] </ref>. This section describes a new version of the ParaScope editor from the user's point of view and discusses new research directions for Ped. <p> In addition, Ped has been continually evaluated internally and externally by users, compiler writers, and user interface designers <ref> [66, 47, 30, 67] </ref>. In the summer of 1991, we held a workshop on ParaScope to obtain more feedback from users and to further refine our tool [30]. Participants from industry, government laboratories, and supercomputing centers used ParaScope to parallelize programs they brought with them.
Reference: [48] <author> K. Kennedy, K. M c Kinley, and C. Tseng, </author> <title> "Analysis and transformation in the ParaScope Editor," </title> <booktitle> in Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> (Cologne, Germany), </address> <month> June </month> <year> 1991. </year>
Reference-contexts: The editor updates dependence information and source code to reflect any such programmer actions. The compiler may then use these modifications when generating code. Research on Ped began in the late 1980s. Its interface, design and implementation have been discussed in detail in other papers <ref> [47, 30, 48, 49] </ref>. This section describes a new version of the ParaScope editor from the user's point of view and discusses new research directions for Ped. <p> and relate how it integrates with the compilation system. 3.1 Ped Functionality Ped supports a paradigm called exploratory parallel programming, in which the user converts an existing sequential Fortran program into an efficient parallel one by repeatedly finding opportunities for parallelism and exploiting them via appropriate changes to the program <ref> [48, 49] </ref>. This process involves a collaboration 16 in which the system performs deep analysis of the program, the user interprets analysis results and the system helps the user carry out changes.
Reference: [49] <author> K. Kennedy, K. M c Kinley, and C. Tseng, </author> <title> "Interactive parallel programming using the ParaScope Editor," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 329-341, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The editor updates dependence information and source code to reflect any such programmer actions. The compiler may then use these modifications when generating code. Research on Ped began in the late 1980s. Its interface, design and implementation have been discussed in detail in other papers <ref> [47, 30, 48, 49] </ref>. This section describes a new version of the ParaScope editor from the user's point of view and discusses new research directions for Ped. <p> and relate how it integrates with the compilation system. 3.1 Ped Functionality Ped supports a paradigm called exploratory parallel programming, in which the user converts an existing sequential Fortran program into an efficient parallel one by repeatedly finding opportunities for parallelism and exploiting them via appropriate changes to the program <ref> [48, 49] </ref>. This process involves a collaboration 16 in which the system performs deep analysis of the program, the user interprets analysis results and the system helps the user carry out changes. <p> Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [36, 54, 55, 56, 49, 57, 58, 59] </ref>. Figure 4 shows a taxonomy of the transformations supported in Ped. Reordering transformations change the order in which statements are executed, either within or across loop iterations. They are safe if all dependences in the original program are preserved.
Reference: [50] <author> N. Yankelovitch, N. Meyrowitz, and A. van Dam, </author> <title> "Reading and writing the electronic book," </title> <journal> IEEE Computer, </journal> <volume> vol. 18, </volume> <pages> pp. 15-29, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: The book metaphor portrays a Fortran program as an electronic book in which analysis results are treated as annotations of the main source text analogous to a book's marginal notes, footnotes, appendices, and indices <ref> [50] </ref>. Progressive disclosure presents details incrementally as they become relevant rather than overwhelming the user with all details at once [51]. View filtering acts as an electronic highlighter to emphasize or conceal parts of the book as specified by the user [52].
Reference: [51] <author> D. C. Smith, C. Irby, R. Kimball, B. Verplank, and E. Harslem, </author> <title> "Designing the star user interface," </title> <journal> BYTE, </journal> <volume> vol. 7, </volume> <pages> pp. 242-282, </pages> <month> Apr. </month> <year> 1982. </year>
Reference-contexts: Progressive disclosure presents details incrementally as they become relevant rather than overwhelming the user with all details at once <ref> [51] </ref>. View filtering acts as an electronic highlighter to emphasize or conceal parts of the book as specified by the user [52]. The user can choose predefined filters or define new ones with boolean expressions over predefined predicates.
Reference: [52] <author> D. C. Engelbart and W. K. </author> <title> English, "A research center for augmenting human intellect," </title> <booktitle> in Proceedings of AFIPS 1968 Fall Joint Computer Conference, </booktitle> <pages> pp. 395-410, </pages> <month> Dec. </month> <year> 1968. </year> <month> 40 </month>
Reference-contexts: Progressive disclosure presents details incrementally as they become relevant rather than overwhelming the user with all details at once [51]. View filtering acts as an electronic highlighter to emphasize or conceal parts of the book as specified by the user <ref> [52] </ref>. The user can choose predefined filters or define new ones with boolean expressions over predefined predicates. Power steering automates repetitive or error-prone tasks, providing unobtrusive assistance while leaving the user in control. The layout of a Ped window is shown in Figure 3.
Reference: [53] <author> R. C. Waters, </author> <title> "Program editors should not abandon text oriented commands," </title> <journal> ACM SIGPLAN Notices, </journal> <volume> vol. 17, </volume> <pages> pp. 39-46, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: The source pane allows arbitrary editing of the Fortran program using mixed text and structure editing techniques <ref> [53] </ref>. The user has the full freedom to edit character by character, or at any time to use the power steering afforded by template-based structure editing. During editing, Ped directly constructs an abstract syntax tree (AST) representation of the procedure; this internal form for procedures is used throughout the environment.
Reference: [54] <author> J. R. Allen and K. Kennedy, </author> <title> "Automatic translation of Fortran programs to vector form," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 9, </volume> <pages> pp. 491-542, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [36, 54, 55, 56, 49, 57, 58, 59] </ref>. Figure 4 shows a taxonomy of the transformations supported in Ped. Reordering transformations change the order in which statements are executed, either within or across loop iterations. They are safe if all dependences in the original program are preserved.
Reference: [55] <author> D. Callahan, S. Carr, and K. Kennedy, </author> <title> "Improving register allocation for subscripted variables," </title> <booktitle> in Proceedings of the ACM SIGPLAN 90 Conference on Programming Language Design and Implementation, SIGPLAN Notices 25(6), </booktitle> <pages> pp. 53-65, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [36, 54, 55, 56, 49, 57, 58, 59] </ref>. Figure 4 shows a taxonomy of the transformations supported in Ped. Reordering transformations change the order in which statements are executed, either within or across loop iterations. They are safe if all dependences in the original program are preserved.
Reference: [56] <author> K. Kennedy and K. S. M c Kinley, </author> <title> "Loop distribution with arbitrary control flow," </title> <booktitle> in Proceedings of Supercomputing '90, </booktitle> <address> (New York, NY), </address> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [36, 54, 55, 56, 49, 57, 58, 59] </ref>. Figure 4 shows a taxonomy of the transformations supported in Ped. Reordering transformations change the order in which statements are executed, either within or across loop iterations. They are safe if all dependences in the original program are preserved.
Reference: [57] <author> D. Kuck, R. Kuhn, B. Leasure, and M. J. Wolfe, </author> <title> "The structure of an advanced retargetable vectorizer," in Supercomputers: </title> <booktitle> Design and Applications, </booktitle> <pages> pp. 163-178, </pages> <address> Silver Spring, MD: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1984. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [36, 54, 55, 56, 49, 57, 58, 59] </ref>. Figure 4 shows a taxonomy of the transformations supported in Ped. Reordering transformations change the order in which statements are executed, either within or across loop iterations. They are safe if all dependences in the original program are preserved.
Reference: [58] <author> D. Loveman, </author> <title> "Program improvement by source-to-source transformations," </title> <journal> Journal of the ACM, </journal> <volume> vol. 17, </volume> <pages> pp. 121-145, </pages> <month> Jan. </month> <year> 1977. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [36, 54, 55, 56, 49, 57, 58, 59] </ref>. Figure 4 shows a taxonomy of the transformations supported in Ped. Reordering transformations change the order in which statements are executed, either within or across loop iterations. They are safe if all dependences in the original program are preserved.
Reference: [59] <author> M. J. Wolfe, </author> <title> "Loop skewing: The wavefront method revisited," </title> <journal> International Journal of Parallel Programming, </journal> <volume> vol. 15, </volume> <pages> pp. 279-293, </pages> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [36, 54, 55, 56, 49, 57, 58, 59] </ref>. Figure 4 shows a taxonomy of the transformations supported in Ped. Reordering transformations change the order in which statements are executed, either within or across loop iterations. They are safe if all dependences in the original program are preserved.
Reference: [60] <author> J. R. Allen, D. Baumgartner, K. Kennedy, and A. Porterfield, </author> <title> "PTOOL: A semi-automatic parallel programming assistant," </title> <booktitle> in Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <address> (St. Charles, IL), </address> <publisher> IEEE Computer Society Press, </publisher> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: The program compiler discovers this type of inconsistency when it is invoked to analyze and compile a program (see Section 2.2). 3.2 Ongoing Research The ParaScope editor was originally inspired by a user workshop on PTOOL, a dependence and parallel program browser also developed at Rice <ref> [60, 61, 62] </ref>. Ped has influenced and been influenced by other 22 parallel programming tools and environments, such as MIMDizer and its predecessor Forge by Pacific Sierra [63, 64], Sigmacs in the Faust programming environment [65, 6], and Superb [8].
Reference: [61] <author> V. Balasundaram, D. Baumgartner, D. Callahan, K. Kennedy, and J. Subhlok, </author> <title> "PTOOL: A system for static analysis of parallelism in programs," </title> <type> Technical Report TR88-71, </type> <institution> Dept. of Computer Science, Rice University, </institution> <year> 1988. </year>
Reference-contexts: The program compiler discovers this type of inconsistency when it is invoked to analyze and compile a program (see Section 2.2). 3.2 Ongoing Research The ParaScope editor was originally inspired by a user workshop on PTOOL, a dependence and parallel program browser also developed at Rice <ref> [60, 61, 62] </ref>. Ped has influenced and been influenced by other 22 parallel programming tools and environments, such as MIMDizer and its predecessor Forge by Pacific Sierra [63, 64], Sigmacs in the Faust programming environment [65, 6], and Superb [8].
Reference: [62] <author> L. Henderson, R. Hiromoto, O. Lubeck, and M. Simmons, </author> <title> "On the use of diagnostic dependencyanalysis tools in parallel programming: Experiences using PTOOL," </title> <journal> The Journal of Supercomputing, </journal> <volume> vol. 4, </volume> <pages> pp. 83-96, </pages> <year> 1990. </year> <title> [63] "The MIMDizer: A new parallelization tool," </title> <booktitle> The Spang Robinson Report on Supercomputing and Parallel Processing, </booktitle> <volume> vol. 4, </volume> <pages> pp. 2-6, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: The program compiler discovers this type of inconsistency when it is invoked to analyze and compile a program (see Section 2.2). 3.2 Ongoing Research The ParaScope editor was originally inspired by a user workshop on PTOOL, a dependence and parallel program browser also developed at Rice <ref> [60, 61, 62] </ref>. Ped has influenced and been influenced by other 22 parallel programming tools and environments, such as MIMDizer and its predecessor Forge by Pacific Sierra [63, 64], Sigmacs in the Faust programming environment [65, 6], and Superb [8].
Reference: [64] <author> D. Cheng and D. Pase, </author> <title> "An evaluation of automatic and interactive parallel programming tools," </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <address> (Albuquerque, NM), </address> <month> Nov. </month> <year> 1991. </year> <month> 41 </month>
Reference-contexts: Ped has influenced and been influenced by other 22 parallel programming tools and environments, such as MIMDizer and its predecessor Forge by Pacific Sierra <ref> [63, 64] </ref>, Sigmacs in the Faust programming environment [65, 6], and Superb [8]. In addition, Ped has been continually evaluated internally and externally by users, compiler writers, and user interface designers [66, 47, 30, 67].
Reference: [65] <author> V. Guarna, D. Gannon, D. Jablonowski, A. Malony, and Y. Gaur, </author> <title> "Faust: An integrated environment for parallel programming," </title> <journal> IEEE Software, </journal> <volume> vol. 6, </volume> <pages> pp. 20-27, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Ped has influenced and been influenced by other 22 parallel programming tools and environments, such as MIMDizer and its predecessor Forge by Pacific Sierra [63, 64], Sigmacs in the Faust programming environment <ref> [65, 6] </ref>, and Superb [8]. In addition, Ped has been continually evaluated internally and externally by users, compiler writers, and user interface designers [66, 47, 30, 67].
Reference: [66] <author> D. Cheng and K. Fletcher, </author> <title> "Private communication," </title> <month> July </month> <year> 1991. </year>
Reference-contexts: In addition, Ped has been continually evaluated internally and externally by users, compiler writers, and user interface designers <ref> [66, 47, 30, 67] </ref>. In the summer of 1991, we held a workshop on ParaScope to obtain more feedback from users and to further refine our tool [30]. Participants from industry, government laboratories, and supercomputing centers used ParaScope to parallelize programs they brought with them.
Reference: [67] <author> J. Stein, </author> <title> "Private communication," </title> <month> July </month> <year> 1991. </year>
Reference-contexts: In addition, Ped has been continually evaluated internally and externally by users, compiler writers, and user interface designers <ref> [66, 47, 30, 67] </ref>. In the summer of 1991, we held a workshop on ParaScope to obtain more feedback from users and to further refine our tool [30]. Participants from industry, government laboratories, and supercomputing centers used ParaScope to parallelize programs they brought with them.
Reference: [68] <author> A. Dinning and E. Schonberg, </author> <title> "An evaluation of monitoring algorithms for access anomaly detection," Ultracomputer Note 163, </title> <institution> Courant Institute, </institution> <address> New York University, </address> <month> July </month> <year> 1989. </year>
Reference-contexts: A promising approach for pinpointing data races is to instrument programs to monitor the logical concur- rency of accesses during execution <ref> [68, 69, 70, 71, 72, 73] </ref>. With such instrumentation, a program can detect and report data races in its own execution. The ParaScope debugging system uses such a strategy to detect data races at run time and supports automatic instrumentation of programs for this purpose.
Reference: [69] <author> A. Dinning and E. Schonberg, </author> <title> "An empirical comparison of monitoring algorithms for access anomaly detection," </title> <booktitle> in Second ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pp. 1-10, </pages> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: A promising approach for pinpointing data races is to instrument programs to monitor the logical concur- rency of accesses during execution <ref> [68, 69, 70, 71, 72, 73] </ref>. With such instrumentation, a program can detect and report data races in its own execution. The ParaScope debugging system uses such a strategy to detect data races at run time and supports automatic instrumentation of programs for this purpose. <p> each of these components of in more detail. 4.1 Run-time Detection of Data Races Run-time techniques for detecting data races fall into two classes: summary methods [74, 72, 73] that report the presence of a data race with incomplete information about the references that caused it, and access history methods <ref> [69, 70, 71] </ref> that can precisely identify each of a pair of accesses involved in a data race. In ParaScope, we use an access history method for detecting data races at run time since precise race reports are more helpful to programmers. <p> j = 1,2 [code block C] enddo endif [code block D] parallel do j=1,i [code block E] enddo [code block F] enddo [code block G] A B,D B,D D F F F 4.1.2 A Protocol for Detecting Data Races In contrast to other access history protocols described in the literature <ref> [69, 71] </ref>, the run-time protocol used by ParaScope to detect data races bounds the length of each variable's access history by a small constant that is program independent [70]. Bounding the length of access histories has two advantages.
Reference: [70] <author> J. M. Mellor-Crummey, </author> <title> "On-the-fly detection of data races for programs with nested fork-join paral-lelism," </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <address> (Albuquerque, NM), </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: A promising approach for pinpointing data races is to instrument programs to monitor the logical concur- rency of accesses during execution <ref> [68, 69, 70, 71, 72, 73] </ref>. With such instrumentation, a program can detect and report data races in its own execution. The ParaScope debugging system uses such a strategy to detect data races at run time and supports automatic instrumentation of programs for this purpose. <p> each of these components of in more detail. 4.1 Run-time Detection of Data Races Run-time techniques for detecting data races fall into two classes: summary methods [74, 72, 73] that report the presence of a data race with incomplete information about the references that caused it, and access history methods <ref> [69, 70, 71] </ref> that can precisely identify each of a pair of accesses involved in a data race. In ParaScope, we use an access history method for detecting data races at run time since precise race reports are more helpful to programmers. <p> For this purpose, the system uses offset-span labeling, an on-line scheme for assigning a label to each thread in an execution of a program with fork-join parallelism <ref> [70] </ref>. <p> D F F F 4.1.2 A Protocol for Detecting Data Races In contrast to other access history protocols described in the literature [69, 71], the run-time protocol used by ParaScope to detect data races bounds the length of each variable's access history by a small constant that is program independent <ref> [70] </ref>. Bounding the length of access histories has two advantages. First, it reduces the asymptotic worst-case space requirements to O (V N ), where V is the number of monitored shared variables, and N is the maximum dynamic nesting depth of parallel constructs. <p> To guarantee that the ParaScope debugging system always reports a data race if any is present in an execution, the race detection protocol used must be insensitive to the interleaving of accesses in an execution. In a previous paper <ref> [70] </ref>, the following assertion is proven about the run-time protocol used by ParaScope: If an execution of a program with nested fork-join parallelism contains one or more data races, at least one will be reported.
Reference: [71] <author> I. Nudler and L. Rudolph, </author> <title> "Tools for efficient development of efficient parallel programs," </title> <booktitle> in First Israeli Conference on Computer Systems Engineering, </booktitle> <year> 1986. </year>
Reference-contexts: A promising approach for pinpointing data races is to instrument programs to monitor the logical concur- rency of accesses during execution <ref> [68, 69, 70, 71, 72, 73] </ref>. With such instrumentation, a program can detect and report data races in its own execution. The ParaScope debugging system uses such a strategy to detect data races at run time and supports automatic instrumentation of programs for this purpose. <p> each of these components of in more detail. 4.1 Run-time Detection of Data Races Run-time techniques for detecting data races fall into two classes: summary methods [74, 72, 73] that report the presence of a data race with incomplete information about the references that caused it, and access history methods <ref> [69, 70, 71] </ref> that can precisely identify each of a pair of accesses involved in a data race. In ParaScope, we use an access history method for detecting data races at run time since precise race reports are more helpful to programmers. <p> j = 1,2 [code block C] enddo endif [code block D] parallel do j=1,i [code block E] enddo [code block F] enddo [code block G] A B,D B,D D F F F 4.1.2 A Protocol for Detecting Data Races In contrast to other access history protocols described in the literature <ref> [69, 71] </ref>, the run-time protocol used by ParaScope to detect data races bounds the length of each variable's access history by a small constant that is program independent [70]. Bounding the length of access histories has two advantages.
Reference: [72] <author> E. Schonberg, </author> <title> "On-the-fly detection of access anomalies," </title> <booktitle> in Proceedings of the ACM SIGPLAN '89 Conference on Programming Language Design and Implementation, SIGPLAN Notices 24(7), </booktitle> <pages> pp. 285297, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: A promising approach for pinpointing data races is to instrument programs to monitor the logical concur- rency of accesses during execution <ref> [68, 69, 70, 71, 72, 73] </ref>. With such instrumentation, a program can detect and report data races in its own execution. The ParaScope debugging system uses such a strategy to detect data races at run time and supports automatic instrumentation of programs for this purpose. <p> In the following sections, we describe each of these components of in more detail. 4.1 Run-time Detection of Data Races Run-time techniques for detecting data races fall into two classes: summary methods <ref> [74, 72, 73] </ref> that report the presence of a data race with incomplete information about the references that caused it, and access history methods [69, 70, 71] that can precisely identify each of a pair of accesses involved in a data race.
Reference: [73] <author> G. L. Steele, Jr., </author> <title> "Making asynchronous parallelism safe for the world," </title> <booktitle> in Conference Record of the Seventeenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: A promising approach for pinpointing data races is to instrument programs to monitor the logical concur- rency of accesses during execution <ref> [68, 69, 70, 71, 72, 73] </ref>. With such instrumentation, a program can detect and report data races in its own execution. The ParaScope debugging system uses such a strategy to detect data races at run time and supports automatic instrumentation of programs for this purpose. <p> In the following sections, we describe each of these components of in more detail. 4.1 Run-time Detection of Data Races Run-time techniques for detecting data races fall into two classes: summary methods <ref> [74, 72, 73] </ref> that report the presence of a data race with incomplete information about the references that caused it, and access history methods [69, 70, 71] that can precisely identify each of a pair of accesses involved in a data race.
Reference: [74] <author> S. L. Min and J. Choi, </author> <title> "An efficient cache-based access anomaly detection scheme," </title> <booktitle> in Proceedings of the 4th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, SIGPLAN Notices 26(4), </booktitle> <pages> pp. 235-244, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: In the following sections, we describe each of these components of in more detail. 4.1 Run-time Detection of Data Races Run-time techniques for detecting data races fall into two classes: summary methods <ref> [74, 72, 73] </ref> that report the presence of a data race with incomplete information about the references that caused it, and access history methods [69, 70, 71] that can precisely identify each of a pair of accesses involved in a data race.
Reference: [75] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu, </author> <title> "Fortran D language specification," </title> <type> Tech. Rep. </type> <institution> TR90-141, Dept. of Computer Science, Rice University, </institution> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: Unless the compiler can choose the right data placement automatically| a formidable task|this model will not be very useful on distributed-memory machines. To address this problem, we have developed an extended version of Fortran, called Fortran D <ref> [75] </ref>, which enables the programmer to explicitly specify data distribution and alignment on a multiprocessor system.
Reference: [76] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng, </author> <title> "An overview of the Fortran D programming system," </title> <booktitle> in Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> (Santa Clara, CA), </address> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Once the data distribution is known, parallelism can automatically be derived by the Fortran D compiler through the use of the "owner computes" rule, which specifies that the owner of a datum computes its value <ref> [76] </ref>. Assignment to all locations in an array distributed across the processors can automatically be performed in parallel by having each processor assign values for its own portion of the array, as long as there are no dependences among the assignments.
Reference: [77] <author> S. Hiranandani, K. Kennedy, and C. Tseng, </author> <title> "Compiler optimizations for Fortran D on MIMD distributed-memory machines," </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <address> (Albuquerque, NM), </address> <month> Nov. </month> <year> 1991. </year> <month> 42 </month>
Reference-contexts: To generate correct and efficient code, the Fortran D compiler must determine which distributions may hold at each point in the program and it must track these distributions interprocedurally. Hence the ParaScope program compiler must be significantly enhanced to accommodate the compilation of Fortran D <ref> [77, 78] </ref>. Intelligent Editor. Most of the functionality in the current ParaScope editor will be useful in constructing Fortran D programs. For example, users will still want to find loop nests that can be run in parallel.
Reference: [78] <author> S. Hiranandani, K. Kennedy, and C. Tseng, </author> <title> "Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines," </title> <booktitle> in Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> (Washington, DC), </address> <month> July </month> <year> 1992. </year>
Reference-contexts: To generate correct and efficient code, the Fortran D compiler must determine which distributions may hold at each point in the program and it must track these distributions interprocedurally. Hence the ParaScope program compiler must be significantly enhanced to accommodate the compilation of Fortran D <ref> [77, 78] </ref>. Intelligent Editor. Most of the functionality in the current ParaScope editor will be useful in constructing Fortran D programs. For example, users will still want to find loop nests that can be run in parallel.
Reference: [79] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer, </author> <title> "A static performance estimator to guide data partitioning decisions," </title> <booktitle> in Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 213-223, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The new ParaScope editor will make this relationship clearer by statically predicting the performance of loop nests under specific collections of Fortran D distributions. To achieve sufficient precision while remaining machine-independent, we will employ a "training set" strategy to produce a performance predictor for each new target machine <ref> [79] </ref>. Debugger. Since Fortran D programs undergo radical transformations during compilation, source-level debugging tools for Fortran D will require significant support to relate the run-time behavior of a transformed program back to its original source, which is considerably more abstract.
Reference: [80] <author> D. A. Reed, R. D. Olson, R. A. Aydt, T. M. Madhyastha, T. Birkett, D. W. Jensen, B. A. A. Nazief, and B. K. Totty, </author> <title> "Scalable performance environments for parallel systems," </title> <booktitle> in Proceedings of Distributed Memory Computing Conference, (Portland), </booktitle> <pages> pp. 562-569, </pages> <month> Apr. </month> <year> 1991. </year> <month> 43 </month>
Reference-contexts: It is imperative that the system provide effective tools to help the programmer understand program performance. We plan to adapt the existing ParaScope analysis tools to provide input to performance visualization systems under development elsewhere, such as the Pablo system at the University of Illinois <ref> [80] </ref>. An important component of performance visualization for Fortran D will be relating performance data for a transformed program back to the more abstract original program source. The final result of this effort will be a collection of tools to support the development of machine- independent data-parallel programs.
References-found: 79

