URL: ftp://ftp.mcs.anl.gov/pub/neos/TechReports/otc95-01.ps.gz
Refering-URL: http://www.mcs.anl.gov/otc/Guide/TechReports/index.html
Root-URL: http://www.mcs.anl.gov
Title: OPTIMIZATION TECHNOLOGY CENTER TOWARDS A DISCRETE NEWTON METHOD WITH MEMORY FOR LARGE-SCALE OPTIMIZATION 1  
Author: by Richard H. Byrd Jorge Nocedal and Ciyou Zhu 
Keyword: Key words: discrete Newton method, variable metric method, truncated Newton method, large scale optimization, nonlinear optimization, limited memory method.  
Date: February 15, 1996  
Pubnum: Technical Report OTC 95/01  
Abstract: A new method for solving large nonlinear optimization problems is outlined. It attempts to combine the best properties of the discrete-truncated Newton method and the limited memory BFGS method, to produce an algorithm that is both economical and capable of handling ill-conditioned problems. The key idea is to use the curvature information generated during the computation of the discrete Newton step to improve the limited memory BFGS approximations. The numerical performance of the new method is studied using a family of functions whose nonlinearity and condition number can be controlled. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.S. Dembo, S.C. Eisenstat, and T. Steihaug, </author> <title> "Inexact Newton methods," </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 19 (1982), </volume> <pages> pp. 400-408. </pages>
Reference: [2] <author> J. E. Dennis, Jr. and R. B. Schnabel, </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations, </title> <address> Englewood Cliffs, N.J., </address> <publisher> Prentice-Hall, </publisher> <year> 1983. </year>
Reference-contexts: After computing the search direction d k ; we define the new iterate by x k+1 = x k + ff k d k ; (1:3) where ff k is a step length parameter. In this paper we will assume that ff k satisfies the strong Wolfe conditions (cf. <ref> [2] </ref> or [4]) f (x k + ff k d k ) f (x k ) + c 1 ff k g (x k ) T d k (1:4) where 0 &lt; c 1 &lt; c 2 &lt; 1 are constants.
Reference: [3] <author> J.C. Gilbert and C. </author> <title> Lemarechal, "Some numerical experiments with variable storage quasi-Newton algorithms," </title> <booktitle> Mathematical Programming 45 (1989), </booktitle> <pages> pp. 407-436. 12 </pages>
Reference-contexts: In summary, limited memory BFGS algorithm always keeps the m most recent pairs fs i ; y i g to define the iteration matrix. This approach is suitable for large problems because it has been observed in practice that small values of m (say m 2 <ref> [3; 20] </ref>) very often give satisfactory results [7], [3]. Let us now describe the updating process in more detail. Suppose that the current iterate is x k and that we have stored the m pairs fs i ; y i g; i = k m; :::; k 1. <p> This approach is suitable for large problems because it has been observed in practice that small values of m (say m 2 [3; 20]) very often give satisfactory results [7], <ref> [3] </ref>. Let us now describe the updating process in more detail. Suppose that the current iterate is x k and that we have stored the m pairs fs i ; y i g; i = k m; :::; k 1.
Reference: [4] <author> P. E. Gill, W Murray and M. H. Wright, </author> <title> Practical Optimization, </title> <publisher> London, Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: In this paper we will assume that ff k satisfies the strong Wolfe conditions (cf. [2] or <ref> [4] </ref>) f (x k + ff k d k ) f (x k ) + c 1 ff k g (x k ) T d k (1:4) where 0 &lt; c 1 &lt; c 2 &lt; 1 are constants.
Reference: [5] <author> G.H. Golub and C.F. Van Loan, </author> <title> Matrix Computations (Second Edition), </title> <publisher> The John Hopkins University Press, </publisher> <address> Baltimore and London, </address> <year> 1989. </year>
Reference-contexts: Therefore an inner iterative method such as the linear conjugate method <ref> [5] </ref> is used to solve the system (1.1) approximately. <p> Let us write the CG iteration at x k as where oe i is the steplength and where the directions fv i g are conjugate with respect to the Hessian matrix r 2 f (x k ); see e.g. <ref> [5] </ref>. The initial guess in the CG iteration is set to be z 1 = 0. Suppose that p iterations of the CG method are performed; then the search direction of the optimization 5 algorithm at x k is given by d k = z p .
Reference: [6] <author> A. Griewank, </author> <title> "On automatic differentiation," Mathematical Programming (M. </title> <editor> Iri and K. Tanabe, eds.), </editor> <publisher> Kluwer Academic Publishers, </publisher> <address> (Tokyo, </address> <year> 1989), </year> <pages> pp. 83-107. </pages>
Reference-contexts: We should note that the product r 2 f (x k )v can be computed by automatic differentiation techniques <ref> [6] </ref> instead of the finite difference (1.2). Automatic differentiation has the important advantage of being accurate. However it is at least as expensive as finite differences in terms of computing time.
Reference: [7] <author> D. C. Liu and J. Nocedal, </author> <title> "On the limited memory BFGS method for large scale optimization methods," </title> <booktitle> Mathematical Programming 45 (1989), </booktitle> <pages> pp. 503-528. </pages>
Reference-contexts: This approach is suitable for large problems because it has been observed in practice that small values of m (say m 2 [3; 20]) very often give satisfactory results <ref> [7] </ref>, [3]. Let us now describe the updating process in more detail. Suppose that the current iterate is x k and that we have stored the m pairs fs i ; y i g; i = k m; :::; k 1. <p> Numerical Investigation We will test three algorithms. The first one is the limited memory BFGS method (L-BFGS) as described in <ref> [7] </ref>. It is used mainly as a benchmark. The other two algorithms combine discrete-truncated Newton and limited memory steps in a regular manner: a discrete Newton step is performed at iterations 6; 16; 26; :::, and all other steps are limited memory BFGS steps.
Reference: [8] <author> J. J. </author> <title> More and D.J. Thuente, "Line search algorithms with guaranteed sufficient decrease," </title> <journal> ACM Transactions on Mathematical Software 20 (1994), </journal> <volume> no. 3, </volume> <pages> pp. 286-307. </pages>
Reference-contexts: We will measure the effect of saving information from the inner CG cycle by comparing DINEMO, which saves this information, with ALTERNATE, which does not. The three methods use the same line search. It is performed by the routine of More and Thuente <ref> [8] </ref> with parameters c 1 = 10 4 and c 2 = 0:9 in (1.4)-(1.5).
Reference: [9] <author> S.G. Nash. </author> <title> User's guide for TN/TNBC: FORTRAN routines for nonlinear optimization, </title> <type> Report 397, </type> <institution> Mathematical Sciences Dept., The Johns Hopkins University, </institution> <year> 1984. </year>
Reference-contexts: A method that uses (1.2) is called a discrete Newton method [13],[10]. The combination of these two ideas gives the discrete-truncated Newton method, which has been implemented, for example, in the codes of Nash <ref> [9] </ref> and Schlick and Fogelson [14]. After computing the search direction d k ; we define the new iterate by x k+1 = x k + ff k d k ; (1:3) where ff k is a step length parameter.
Reference: [10] <author> S.G. Nash, </author> <title> "Preconditioning of truncated-Newton methods," </title> <journal> SIAM Journal on Scientific and Statistical Computing 6 (1985), </journal> <pages> pp. 599-616. </pages>
Reference-contexts: This poses the additional question of what types of pairs should be used in the preconditioner: only the inner pairs fs i ; y i g, only the outer pairs fs k ; y k g, or both? A simple form of preconditioning of this type is used by Nash <ref> [10] </ref>. 4. How long the two blocks information, from the inner and outer iterations, are to be kept in memory must be determined, as discussed above. Also, the order in which limited memory updating is performed has not been specified.
Reference: [11] <author> S.G. Nash and J. Nocedal, </author> <title> "A Numerical Study of the Limited Memory BFGS Method and the Truncated-Newton Method for Large Scale Optimization," </title> <journal> SIAM Journal on Optimization 1 (1991), </journal> <volume> no. 3, </volume> <pages> pp. 358-372. </pages>
Reference-contexts: An alternative is to terminate the inner iteration when sufficient decrease in the quadratic model 1 d T has been obtained <ref> [11] </ref>. When the Hessian r 2 f (x k ) is not positive definite, the inner conjugate gradient iteration may generate a direction v of negative curvature, i.e. a vector v such that v T r 2 f (x k )v &lt; 0.
Reference: [12] <author> J. Nocedal, </author> <title> "Updating quasi-Newton matrices with limited storage," </title> <booktitle> Mathematics of Computation 35 (1980), </booktitle> <pages> pp. 773-782. </pages>
Reference-contexts: can be written as H k = V T km H k (V km V k1 ) i k1 V T j km (V km+1 V k1 ) i k1 V T j km+1 (V km+2 V k1 ) . . . k1 : (2.5) A recursive formula described in <ref> [12] </ref> takes advantage of the symmetry of this expression to compute the product H k g (x k ) efficiently. The numerical performance of the limited memory method L-BFGS is often very good in terms of total computing time.
Reference: [13] <author> D.P. O'Leary, </author> <title> "A discrete Newton algorithm for minimizing a function of many variables," </title> <booktitle> Mathematical Programming 23 (1982), </booktitle> <pages> pp. 20-33. </pages>
Reference: [14] <author> T. Schlick and A. Fogelson, </author> <title> "TNPACK A truncated Newton package for large-scale problems: I. Algorithms and usage," </title> <journal> ACM Transactions on Mathematical Software 18 (1992), </journal> <volume> no. 1, </volume> <pages> pp. 46-70. </pages>
Reference-contexts: A method that uses (1.2) is called a discrete Newton method [13],[10]. The combination of these two ideas gives the discrete-truncated Newton method, which has been implemented, for example, in the codes of Nash [9] and Schlick and Fogelson <ref> [14] </ref>. After computing the search direction d k ; we define the new iterate by x k+1 = x k + ff k d k ; (1:3) where ff k is a step length parameter.
Reference: [15] <author> T. </author> <title> Steihaug (1983), "The conjugate gradient method and trust regions in large scale optimization," </title> <journal> SIAM J. Num. Anal. </journal> <volume> 20 (1983), </volume> <pages> pp. 626-637. 13 </pages>
References-found: 15

