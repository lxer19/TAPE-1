URL: http://www.cs.princeton.edu/~shz/papers/paper.ps
Refering-URL: http://www.cs.princeton.edu/~shz/research.html
Root-URL: http://www.cs.princeton.edu
Email: fdj,shzg@cs.princeton.edu  
Title: Performance Portability of Applications and Optimizations Across Shared Address Space Multiprocessor  
Author: Dongming Jiang and Hongzhang Shan 
Address: NJ 08544  
Affiliation: Department of Computer Science, Princeton University,  
Abstract: 1 Abstract 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Holt C., Singh J. P., and Hennessy j. </author> <title> Application and Architectural Bottlenecks in Large Scale Distributed Shared Memory Machines. </title> <booktitle> In Proceedings of the 23th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 134-145, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Nevertheless, software communication costs and protocol overhead can be high and the performance potential of this approach across a wide range of applications is a topic of current research. Previous research has studied parallel application performance on particular shared memory systems <ref> [1, 9, 4, 7, 3, 11] </ref>. Studies on shared virtual memory have largely used applications as written for hardware cache-coherent machines. <p> the parallel performance on shared virtual memory significantly, and understand what kinds of techniques this takes. 4.2 Methodology While classifying programming improvements is difficult, since they can be quite ad-hoc, we define structured classes of optimizations starting from the simplest to the most challenging, similarly to the structure used in <ref> [1] </ref>. Specifically, we divide optimizations into three classes: * Padding and Alignment is the simplest optimization. This is carried out by distinguishing among different communication and data access granularities|cache line size for cache-coherent machines and page size for page-based systems|and padding and aligning important data structure to these granularities. <p> Some of these optimizations become important on hardware CC-NUMA machines as well, but only at much larger scale <ref> [1] </ref>. 7.1 Guidelines for Programming on SVM Padding and Alignment usually does not help for fine-grained access pattern, and shows the disadvantage of increasing fragmentation and waste system memory. But it is important for coarse-grained.
Reference: [2] <author> Lenoski D., Laudon J., Joe T., Nakahira D., Stevens L., Gupta A., and Hennessy J. </author> <title> The DASH Prototype: Implementation and Performance. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Among our four platforms, we experiment on two real machines: an SGI Challenge and the Stanford DASH <ref> [2] </ref>, representing the bus-based and physically distributed shared address space multiprocessors respectively.
Reference: [3] <author> Agarwal A. et al. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22th International Symposium on Computer Architecuture, </booktitle> <pages> pages 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Nevertheless, software communication costs and protocol overhead can be high and the performance potential of this approach across a wide range of applications is a topic of current research. Previous research has studied parallel application performance on particular shared memory systems <ref> [1, 9, 4, 7, 3, 11] </ref>. Studies on shared virtual memory have largely used applications as written for hardware cache-coherent machines.
Reference: [4] <author> Heinrich M. et al. </author> <title> The Performance Impact of Flexibility in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 6th International Conference on Architecutural Support for Programming Language and Operating Systems, </booktitle> <pages> pages 274-285, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: Nevertheless, software communication costs and protocol overhead can be high and the performance potential of this approach across a wide range of applications is a topic of current research. Previous research has studied parallel application performance on particular shared memory systems <ref> [1, 9, 4, 7, 3, 11] </ref>. Studies on shared virtual memory have largely used applications as written for hardware cache-coherent machines.
Reference: [5] <author> Kuskin J. et al. </author> <title> The Stanford Flash Multiprocessor. </title> <booktitle> In Proceedings of the 21th International Symposium on Computer Architecuture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: However, building efficient cache coherence in hardware has high engineering cost. Many efforts have therefore been made to support a coherent shared address space using commodity-oriented parts with less tight integration for the communication architecture|both the controller and the network <ref> [5, 11] </ref>. One extreme in the spectrum is to support the abstraction entirely in software on networks of commodity workstations and personal computers with no additional hardware cost. This approach, called shared virtual memory (SVM), provides the coherent shared address space at page granularity through virtual memory management.
Reference: [6] <author> Lacroute P. G. </author> <title> Fast Volume Rendering Using a Share-Warp Factorization of the Viewing Transformation. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1995. </year>
Reference-contexts: Third we choose applications from different domains of computations. Our application suite currently contains 7 applications, each with several versions. Six are originally from the SPLASH-2 [12] suite, and one is a recently published parallel shear-warp volume rendering program <ref> [6, 10] </ref>. 3.2.1 Regular Applications LU performs the blocked LU factorization of a dense matrix. We begins with the non-contiguous version of LU, which uses the natural 2-d arrays to represent the 2-d matrix. Its inherent data sharing pattern is one producer with multiple consumers. <p> As figure 1 shows, there are two phases in the rendering <ref> [6] </ref>. First, the run-length encoded volume (not shown) is composited into an intermediate image, by traversing the volume in scanline order slice by slice and writing the image. <p> The read access is coarse grained but the write access is fine grained and scattered. It suffers substantial false sharing. 4 Approaches We begin with the applications as they appear in the SPLASH-2 suite (and for Shear Warp in <ref> [6] </ref>). They are all quite well tuned for hardware cache-coherence, and we perform data distribution on the CC-NUMA and SVM platforms as suggested in SPLASH-2.
Reference: [7] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Nevertheless, software communication costs and protocol overhead can be high and the performance potential of this approach across a wide range of applications is a topic of current research. Previous research has studied parallel application performance on particular shared memory systems <ref> [1, 9, 4, 7, 3, 11] </ref>. Studies on shared virtual memory have largely used applications as written for hardware cache-coherent machines. <p> Network contention is modeled. 3.1.4 Shared Virtual Memory Platform The shared virtual memory (SVM) platform we use simulates an all-software home-based lazy release consistency (HLRC) protocol [8]. This protocol has recently been shown to equal or outperform non home-based LRC protocols such as that in TreadMarks <ref> [7] </ref>. It models an architecture of SMP nodes connected by a commodity interconnect|Myrinet. For our platform, each node has one 200Mhz x86 processors.
Reference: [8] <author> Iftode L., Singh J. P., and Li K. </author> <title> Scope Consistency: a Bridge Between Release Consistency and Entry Consistency. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: The second-level cache line size is 64 bytes. Peak node-to-network communication bandwidth is 400MB/sec. Network contention is modeled. 3.1.4 Shared Virtual Memory Platform The shared virtual memory (SVM) platform we use simulates an all-software home-based lazy release consistency (HLRC) protocol <ref> [8] </ref>. This protocol has recently been shown to equal or outperform non home-based LRC protocols such as that in TreadMarks [7]. It models an architecture of SMP nodes connected by a commodity interconnect|Myrinet. For our platform, each node has one 200Mhz x86 processors.
Reference: [9] <author> Iftode L., Singh J. P., and Li K. </author> <title> Understanding Application Performance on Shared Virtual Memory Systems. </title> <booktitle> In Proceedings of the 23th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 122-133, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Nevertheless, software communication costs and protocol overhead can be high and the performance potential of this approach across a wide range of applications is a topic of current research. Previous research has studied parallel application performance on particular shared memory systems <ref> [1, 9, 4, 7, 3, 11] </ref>. Studies on shared virtual memory have largely used applications as written for hardware cache-coherent machines. <p> First we use both regular and irregular applications. Second, we explore applications with a range of behaviors with different inherent communication and data referencing patterns, and different access granularities to data that interact with SVM page granularity to produce different "induced" sharing patterns <ref> [9] </ref>. Third we choose applications from different domains of computations. Our application suite currently contains 7 applications, each with several versions.
Reference: [10] <author> Singh J. P., Gupta A., and Levoy M. </author> <title> Paralle Visualization Algorithms: Performance and Architectural Implications. </title> <journal> Computer, </journal> <volume> 27 </volume> <pages> 45-55, </pages> <year> 1994. </year>
Reference-contexts: Third we choose applications from different domains of computations. Our application suite currently contains 7 applications, each with several versions. Six are originally from the SPLASH-2 [12] suite, and one is a recently published parallel shear-warp volume rendering program <ref> [6, 10] </ref>. 3.2.1 Regular Applications LU performs the blocked LU factorization of a dense matrix. We begins with the non-contiguous version of LU, which uses the natural 2-d arrays to represent the 2-d matrix. Its inherent data sharing pattern is one producer with multiple consumers.
Reference: [11] <author> Reinhardt S., Larus J., and Wood D. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21th International Symposium on Computer Architecuture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: However, building efficient cache coherence in hardware has high engineering cost. Many efforts have therefore been made to support a coherent shared address space using commodity-oriented parts with less tight integration for the communication architecture|both the controller and the network <ref> [5, 11] </ref>. One extreme in the spectrum is to support the abstraction entirely in software on networks of commodity workstations and personal computers with no additional hardware cost. This approach, called shared virtual memory (SVM), provides the coherent shared address space at page granularity through virtual memory management. <p> Nevertheless, software communication costs and protocol overhead can be high and the performance potential of this approach across a wide range of applications is a topic of current research. Previous research has studied parallel application performance on particular shared memory systems <ref> [1, 9, 4, 7, 3, 11] </ref>. Studies on shared virtual memory have largely used applications as written for hardware cache-coherent machines.
Reference: [12] <author> Woo S.C., Ohara M., Torrie E., Singh J. P., and Gupta A. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Third we choose applications from different domains of computations. Our application suite currently contains 7 applications, each with several versions. Six are originally from the SPLASH-2 <ref> [12] </ref> suite, and one is a recently published parallel shear-warp volume rendering program [6, 10]. 3.2.1 Regular Applications LU performs the blocked LU factorization of a dense matrix. We begins with the non-contiguous version of LU, which uses the natural 2-d arrays to represent the 2-d matrix. <p> To eliminate remote access, an alternative algorithm is to allocate a local copy buffer for each processor, and gather the changes locally first before propagate them to the remote nodes in a less scattered way <ref> [12] </ref>.
References-found: 12

