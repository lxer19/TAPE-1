URL: http://www.isi.edu/soar/johnson/explain.ps
Refering-URL: http://ai.eecs.umich.edu/ifor/papers/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: johnson@isi.edu  
Title: Agents that Learn to Explain Themselves  
Author: W. Lewis Johnson 
Note: Appearing in AAAI-94. c flAmerican Association for Artificial Intelligence.  
Address: 4676 Admiralty Way Marina del Rey, CA 90292-6695  
Affiliation: USC Information Sciences Institute  
Abstract: Intelligent artificial agents need to be able to explain and justify their actions. They must therefore understand the rationales for their own actions. This paper describes a technique for acquiring this understanding, implemented in a multimedia explanation system. The system determines the motivation for a decision by recalling the situation in which the decision was made, and replaying the decision under variants of the original situation. Through experimentation the agent is able to discover what factors led to the decisions, and what alternatives might have been chosen had the situation been slightly different. The agent learns to recognize similar situations where the same decision would be made for the same reasons. This approach is implemented in an artificial fighter pilot that can explain the motivations for its actions, situation assessments, and beliefs. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Calder, R.; Smith, J.; Courtemanche, A.; Mar, J.; and Ceranowicz, A. </author> <year> 1993. </year> <title> ModSAF behavior simulation and control. </title> <booktitle> In Proceedings of the Third Conference on Computer Generated Forces and Behavioral Representation, </booktitle> <pages> 347-359. </pages> <address> Orlando, FL: </address> <institution> Institute for Simulation and Training, University of Central Florida. </institution>
Reference-contexts: The pilot must justify his assessments of the situation, e.g., why the bogey was considered a threat. TacAir-Soar is able to simulate pilots executing missions such as this, and Debrief is able to answer questions about them. TacAir-Soar controls a simulation environment called ModSAF <ref> (Calder et al. 1993) </ref> that simulates the behavior of military platforms. TacAir-Soar receives information from ModSAF about aircraft status and radar information, and issues commands to fly the simulated aircraft and employ weapons. After an engagement users can interact with Debrief to ask questions about the engagement.
Reference: <author> Clancey, W. </author> <year> 1983a. </year> <title> The advantages of abstract control knowledge in expert system design. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 74-78. </pages>
Reference-contexts: They contain too many implementation details, and lack information about the domain and about rationales for the design of the system. More advanced explanation techniques encode domain knowledge and problem-solving strategies and employ them in problem solving either as metarules <ref> (Clancey 1983a) </ref> or in compiled form (Neches, Swartout, & Moore 1985). In the computer-generated forces domain, however, problem-solving strategies and domain knowledge representations are matters of current research.
Reference: <author> Clancey, W. </author> <year> 1983b. </year> <title> The epistemology of a rule-based expert system: A framework for explanation. </title> <booktitle> Artificial Intelligence 20(3) </booktitle> <pages> 215-251. </pages>
Reference: <author> Davis, R. </author> <year> 1976. </year> <title> Applications of Meta-Level Knowledge to the Construction, Maintenance, and Use of Large Knowledge Bases. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University. </institution>
Reference: <author> Doorenbos, R. </author> <year> 1993. </year> <title> Matching 100,000 learned rules. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 290-296. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI. </publisher>
Reference: <author> Gil, Y. </author> <year> 1993. </year> <title> Efficient domain-independent experimentation. Technical Report ISI/RR-93-337, </title> <booktitle> USC / Information Sciences Institute. Appears in the Proceedings of the Tenth International Conference on Machine Learning. </booktitle>
Reference-contexts: But while REX requires one to create a separate knowledge base to support explanation, Debrief automatically learns much of what it needs to know to generate explanations. The approach is related to techniques for acquiring domain models through experimentation <ref> (Gil 1993) </ref>, except that the agent learns to model not the external world, but itself. Debrief is implemented as part of the TacAir-Soar fighter pilot simulation (Jones et al. 1993). Debrief can describe and justify decisions using a combination of natural language and diagrams. <p> This same technique enables Debrief to identify the factors involved. Relationship to other exploratory learning approaches The closest correlate to Debrief's decision evaluation capability is Gil's work on learning by experimentation <ref> (Gil 1993) </ref>. Gil's EXPO system keeps track of operator applications, and the states in which those operators were applied. If an operator is found to have different effects in different situations, EXPO compares the states to determine the differences.
Reference: <author> Hill, R., and Johnson, W. </author> <year> 1994. </year> <title> Situated plan attribution for intelligent tutoring. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: It is written in a domain-independent fashion so that it can be readily incorporated into other intelligent systems. Current plans call for incorporating it into the REACT system, an intelligent assistant for operators of NASA Deep Space Network ground tracking stations <ref> (Hill & Johnson 1994) </ref>. An Example Consider the following scenario. A fighter is assigned a Combat Air Patrol (CAP) mission, i.e., it should fly a loop pattern, scanning for enemy aircraft. During the mission a bogey (an unknown aircraft) is spotted on the radar.
Reference: <author> Johnson, W. </author> <year> 1994. </year> <title> Agents that explain their own actions. </title> <booktitle> In Proc. of the Fourth Conference on Computer Generated Forces and Behavioral Representation. </booktitle> <address> Or-lando, FL: </address> <institution> Institute for Simulation and Training, University of Central Florida. </institution> <note> World Wide Web access: http://www.isi.edu/soar/debriefable.html. </note>
Reference-contexts: It is written in a domain-independent fashion so that it can be readily incorporated into other intelligent systems. Current plans call for incorporating it into the REACT system, an intelligent assistant for operators of NASA Deep Space Network ground tracking stations <ref> (Hill & Johnson 1994) </ref>. An Example Consider the following scenario. A fighter is assigned a Combat Air Patrol (CAP) mission, i.e., it should fly a loop pattern, scanning for enemy aircraft. During the mission a bogey (an unknown aircraft) is spotted on the radar. <p> The follow ing sections describe the system components involved in determining motivations for decisions and beliefs; other parts of the system are described in <ref> (Johnson 1994) </ref>. Memory and Recall In order for Debrief to describe and explain decisions, it must be able to recall the decisions and the situations in which they occurred. In order words, the agent requires an episodic memory. <p> The presentation mechanisms that yield the latter two types of chunks are described in <ref> (Johnson 1994) </ref>. Altogether, these chunks enable Debrief to acquire significant facility in explaining problem solving behavior. These chunks result in speedups during the course of explaining a single mission. Future experiments will determine the transfer effects between missions.
Reference: <author> Jones, R.; Tambe, M.; Laird, J.; and Rosenbloom, P. </author> <year> 1993. </year> <title> Intelligent automated agents for flight training simulators. </title> <booktitle> In Proceedings of the Third Conference on Computer Generated Forces and Behavioral Representation, </booktitle> <pages> 33-42. </pages> <address> Orlando, FL: </address> <institution> Institute for Simulation and Training, University of Central Florida. </institution>
Reference-contexts: This is especially true for computer-generated forces, i.e., computer agents that operate within battlefield simulations. Such simulations are expected to have an increasingly important role in the evaluation of missions, tactics, doctrines, and new weapons systems, and in training <ref> (Jones 1993) </ref>. Validation of such forces is critical|they should behave as humans would in similar circumstances. Yet it is difficult to validate behavior through external observation; behavior depends upon the agent's assessment of the situation and its changing goals from moment to moment. <p> The approach is related to techniques for acquiring domain models through experimentation (Gil 1993), except that the agent learns to model not the external world, but itself. Debrief is implemented as part of the TacAir-Soar fighter pilot simulation <ref> (Jones et al. 1993) </ref>. Debrief can describe and justify decisions using a combination of natural language and diagrams. It is written in a domain-independent fashion so that it can be readily incorporated into other intelligent systems.
Reference: <author> Jones, R. </author> <year> 1993. </year> <title> Using CGF for analysis and combat development. </title> <booktitle> In Proceedings of the Third Conference on Computer Generated Forces and Behavioral Representation, </booktitle> <pages> 209-219. </pages> <address> Orlando, FL: </address> <institution> Institute for Simulation and Training, University of Central Florida. </institution>
Reference-contexts: This is especially true for computer-generated forces, i.e., computer agents that operate within battlefield simulations. Such simulations are expected to have an increasingly important role in the evaluation of missions, tactics, doctrines, and new weapons systems, and in training <ref> (Jones 1993) </ref>. Validation of such forces is critical|they should behave as humans would in similar circumstances. Yet it is difficult to validate behavior through external observation; behavior depends upon the agent's assessment of the situation and its changing goals from moment to moment. <p> The approach is related to techniques for acquiring domain models through experimentation (Gil 1993), except that the agent learns to model not the external world, but itself. Debrief is implemented as part of the TacAir-Soar fighter pilot simulation <ref> (Jones et al. 1993) </ref>. Debrief can describe and justify decisions using a combination of natural language and diagrams. It is written in a domain-independent fashion so that it can be readily incorporated into other intelligent systems.
Reference: <author> Neches, R.; Swartout, W.; and Moore, J. </author> <year> 1985. </year> <title> Enhanced maintenance and explanation of expert systems through explicit models of their development. </title> <journal> IEEE Transactions on Software Engineering SE-11(11):1337-1351. </journal>
Reference-contexts: They contain too many implementation details, and lack information about the domain and about rationales for the design of the system. More advanced explanation techniques encode domain knowledge and problem-solving strategies and employ them in problem solving either as metarules (Clancey 1983a) or in compiled form <ref> (Neches, Swartout, & Moore 1985) </ref>. In the computer-generated forces domain, however, problem-solving strategies and domain knowledge representations are matters of current research.
Reference: <author> Newell, A. </author> <year> 1990. </year> <title> Unified Theories of Cognition. </title> <address> Cam-bridge, MA: </address> <publisher> Harvard University Press. </publisher>
Reference-contexts: Beliefs are explained by recalling the situation in which the beliefs arose, determining what decisions caused the beliefs to be asserted, and determining what factors were responsible for the decisions. Implementation Concerns Debrief is implemented in Soar, a problem-solving architecture that implements a theory of human cognition <ref> (Newell 1990) </ref>. Problems in Soar are represented as goals, and are solved within problem spaces. Each problem space consists of a state, represented as a set of attribute-value pairs, and a set of operators.
Reference: <author> Rajamoney, S. </author> <year> 1993. </year> <title> The design of discrimination experiments. </title> <booktitle> Machine Learning 185-203. </booktitle>
Reference-contexts: This enables it to discover discriminating characteristics within the class. Some exploratory learning systems, such as Raja-money's systems <ref> (Rajamoney 1993) </ref>, invest significant effort to design experiments that provide the maximum amount of information. This is necessary because experiments can be costly and can have persistent effects on the environment. Debrief's chunking-based technique filters out irrelevant experiments automatically, without significant effort.
Reference: <author> Rosenbloom, P.; Laird, J.; and Newell, A. </author> <year> 1987. </year> <title> Knowledge level learning in Soar. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> 618-623. </pages> <address> Menlo Park, CA: </address> <booktitle> American Association for Artificial Intelligence. </booktitle>
Reference-contexts: This result is chunked as well, enabling Debrief immediately to recall the state associated with the event should it need to refer back to it in the future. This process is an instance of data chunking, a common mechanism for knowledge-level learning in Soar systems <ref> (Rosenbloom, Laird, & Newell 1987) </ref>. Debrief thus makes extensive use of Soar's long term memory, i.e., chunks, in constructing its episodic memory. In a typical TacAir-Soar run several hundred such chunks are created.
Reference: <author> Scott, P., and Markovich, S. </author> <year> 1993. </year> <title> Experience selection and problem choice in an exploratory learning system. </title> <booktitle> Machine Learning 49-68. </booktitle>
Reference-contexts: Gil's EXPO system keeps track of operator applications, and the states in which those operators were applied. If an operator is found to have different effects in different situations, EXPO compares the states to determine the differences. Another system by Scott and Markovich <ref> (Scott & Markovich 1993) </ref> performs an operation on instances of a class of objects, to determine whether it has different effects on different members of the class. This enables it to discover discriminating characteristics within the class.
Reference: <author> Swartout, W., and Moore, J. </author> <year> 1993. </year> <title> Explanation in second generation expert systems. </title> <editor> In David, J.-M.; Krivine, J.-P.; and Simmons., R., eds., </editor> <booktitle> Second Generation Expert Systems. </booktitle> <publisher> Springer-Verlag. </publisher> <pages> 543-585. </pages>
Reference: <author> Teach, R., and Shortliffe, E. </author> <year> 1984. </year> <title> An analysis of physicians' attitudes. </title> <editor> In Buchanan, B., and Shortliffe, E., eds., </editor> <title> Rule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuristic Programming Project. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher> <pages> 635-652. </pages>
Reference-contexts: Trainees can greatly benefit from automated forces that can explain their actions, so that the trainees can learn how experts behave in various situations. Potential users of computer-generated forces therefore attach great importance to explanation, just as potential users of computer-based medical consultation systems do <ref> (Teach & Shortliffe 1984) </ref>. Explanations based on traces of rule firings or paraphrases of rules tend not to be successful (Davis 1976; Swartout & Moore 1993; Clancey 1983b). They contain too many implementation details, and lack information about the domain and about rationales for the design of the system.
Reference: <author> Vera, A.; Lewis, R.; and Lerch, F. </author> <year> 1993. </year> <title> Situated decision-making and recognition-based learning: Applying symbolic theories to interactive tasks. </title> <booktitle> In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 84-95. </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Meanwhile, if the variants resulted in different operators being selected, the applicability criteria for these operators are identified in the same manner. This generate-and-test approach has been used in other Soar systems to enlist recognition chunks in service of problem solving <ref> (Vera, Lewis, & Lerch 1993) </ref>, and is similar to Debrief's mechanism for reconstructing states from episodic memory. Since the state representations are hierarchically organized, the significant attributes are found quickly.
Reference: <author> Wick, M., and Thompson, W. </author> <year> 1989. </year> <title> Reconstructive explanation: Explanation as complex problem solving. </title> <booktitle> In Proceedings of the Eleventh Intl. Joint Conf. on Artificial Intelligence, </booktitle> <pages> 135-140. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The relationships between situational factors and decisions are learned so that they can be applied to similar decisions. This approach of basing explanations on abstract associations between decisions and situational factors has similarities to the REX system <ref> (Wick & Thompson 1989) </ref>. But while REX requires one to create a separate knowledge base to support explanation, Debrief automatically learns much of what it needs to know to generate explanations.
References-found: 19

