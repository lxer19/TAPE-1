URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/90.PPOPP.Multi_Model_Psyche.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/scott/pubs.html
Root-URL: 
Email: scott,leblanc,marsh-@cs.rochester.edu  
Title: MULTI-MODEL PARALLEL PROGRAMMING IN PSYCHE  
Author: Michael L. Scott, Thomas J. LeBlanc, and Brian D. Marsh 
Address: Rochester, New York 14627  
Affiliation: Computer Science Department University of Rochester  
Abstract: Many different parallel programming models, including lightweight processes that communicate with shared memory and heavyweight processes that communicate with messages, have been used to implement parallel applications. Unfortunately, operating systems and languages designed for parallel programming typically support only one model. Multi-model parallel programming is the simultaneous use of several different models, both across programs and within a single program. This paper describes multi-model parallel programming in the Psyche multiprocessor operating system. We explain why multi-model programming is desirable and present an operating system interface designed to support it. Through a series of three examples, we illustrate how the Psyche operating system supports different models of parallelism and how the different models are able to interact. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Accetta, R. Baron, W. Bolosky, D. Golub, R. Rashid, A. Tevanian and M. Young, </author> <title> ``Mach: A New Kernel Foundation for UNIX Development,'' </title> <booktitle> Proceedings of the Summer 1986 USENIX Technical Conference and Exhibition, </booktitle> <month> June </month> <year> 1986, </year> <pages> pp. 93-112. </pages>
Reference-contexts: It allows processes to interact through events and certain kinds of stylized memory sharing. Its emphasis is on optimizing static relationships between processes and on providing the illusion of memory sharing through extensive software support. Because it is implemented on top of an existing operating system (Mach <ref> [1] </ref>), Agora must limit process interactions to those that can be represented effectively in terms of encapsulated Mach primitives. Mach is representative of a class of operating systems designed for parallel computing. Other systems in this class include Amoeba [21], Chorus [2], Topaz [27], and V [11].
Reference: [2] <author> R. Armand, M. Gien, R. Herrmann and M. Rozier, </author> <title> ``Revolution 89, or `Distributing UNIX Brings it Back to its Original Virtues','' </title> <booktitle> Proceedings of the First USENIX Workshop on Experiences Building Distributed and Multiprocessor Systems, </booktitle> <month> 5-6 October, </month> <year> 1989, </year> <pages> pp. 153-174. </pages>
Reference-contexts: Mach is representative of a class of operating systems designed for parallel computing. Other systems in this class include Amoeba [21], Chorus <ref> [2] </ref>, Topaz [27], and V [11]. To facilitate parallelism within applications, these systems allow more than one kernel-supported process to run in one address space.
Reference: [3] <author> Y. Artsy, H. Chang and R. Finkel, </author> <title> ``Interprocess Communication in Charlotte,'' </title> <booktitle> IEEE Software 4:1 (January 1987), </booktitle> <pages> pp. 22-28. </pages>
Reference-contexts: Wait blocks the process until some outstanding send or receive request has completed. These semantics are a subset of the message-passing primitives of the Charlotte distributed operating system <ref> [3] </ref>. They can be implemented as follows. Each Charlotte process is implemented as a separate protection domain with a single virtual processor, running a single Psyche process. A diagram of one such protection domain (and its interactions with others) appears in figure 2.
Reference: [4] <institution> BBN Advanced Computers Incorporated, </institution> <note> ``Chrysalis Programmers Manual, Version 4.0,'' </note> <institution> Cambridge, </institution> <address> MA, </address> <month> 10 February </month> <year> 1988. </year>
Reference-contexts: Since 1984 we have explored these issues while developing parallel programming environments for the BBN Butterfly multiprocessor. Using the Chrysalis operating system <ref> [4] </ref> as a low-level interface, we have created several new programming libraries and languages and ported several others [18].
Reference: [5] <author> B. N. Bershad, D. T. Ching, E. D. Lazowska, J. Sanislo and M. Schwartz, </author> <title> ``A Remote Procedure Call Facility for Interconnecting Heterogeneous Computer Systems,'' </title> <journal> IEEE Transactions on Software Engineering SE-13:8 (August 1987), </journal> <pages> pp. 880-894. </pages>
Reference-contexts: Related Work Multi-model programming is related to, but distinct from, the work of several other researchers. Remote procedure call systems have often been designed to work between programs written in multiple languages <ref> [5, 14, 16, 20] </ref>. RPC-based systems support a single style of process interaction, and are usually intended for a distributed environment; there is no obvious way to extend them to fine-grained process interactions.
Reference: [6] <author> B. N. Bershad, E. D. Lazowska, H. M. Levy and D. B. Wagner, </author> <title> ``An Open Environment for Building Parallel Programming Systems,'' </title> <booktitle> Proceedings of the First ACM Conference on Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <month> 19-21 July </month> <year> 1988, </year> <pages> pp. 1-9. </pages> <booktitle> In ACM SIGPLAN Notices 23:9. </booktitle>
Reference-contexts: Protection is provided only when the user indicates a willingness to pay for it; Psyche presents an explicit tradeoff between protection and performance. Washington's Presto system <ref> [6] </ref> is perhaps the closest relative to Psyche, at least from the point of view of an individual application. <p> The following three sections illustrate the advantages of the Psyche approach through a series of examples. 4. Lightweight Processes in a Shared Address Space Programming models in which a single address space is shared by many lightweight processes, such as Presto <ref> [6] </ref> and the Uniform System [28], can be implemented easily and efficiently on Psyche.
Reference: [7] <author> B. N. Bershad, T. E. Anderson, E. D. Lazowska and H. M. Levy, </author> <title> ``Lightweight Remote Procedure Call,'' </title> <journal> ACM Transactions on Computer Systems 8:1 (February 1990), </journal> <pages> pp. 37-55. </pages> <booktitle> Also in ACM SIGOPS Operating Systems Review 23:5; originally presented at the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <month> 3-6 December </month> <year> 1989. </year>
Reference-contexts: RPC-based systems support a single style of process interaction, and are usually intended for a distributed environment; there is no obvious way to extend them to fine-grained process interactions. Synchronization is supported only via client-server rendezvous, and even the most efficient implementations <ref> [7] </ref> cannot compete with the low latency of direct access to shared memory. The Agora project [8] defines additional mechanisms for process interaction in a distributed environment. It allows processes to interact through events and certain kinds of stylized memory sharing.
Reference: [8] <author> R. Bisiani and A. Forim, </author> <title> ``Multilanguage Parallel Programming,'' </title> <booktitle> Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <month> 17-21 August </month> <year> 1987, </year> <pages> pp. 381-384. </pages>
Reference-contexts: Synchronization is supported only via client-server rendezvous, and even the most efficient implementations [7] cannot compete with the low latency of direct access to shared memory. The Agora project <ref> [8] </ref> defines additional mechanisms for process interaction in a distributed environment. It allows processes to interact through events and certain kinds of stylized memory sharing. Its emphasis is on optimizing static relationships between processes and on providing the illusion of memory sharing through extensive software support.
Reference: [9] <author> A. P. Black, </author> <title> ``Supporting Distributed Applications: Experience with Eden,'' </title> <booktitle> Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <month> 1-4 December </month> <year> 1985, </year> <pages> pp. 181-193. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 19:5. </booktitle>
Reference-contexts: These mechanisms are seldom amenable to change, and may not be well-matched to a new model under development, leading to awkward or inefficient implementations. The traditional Unix interface, for example, has been used beneath many new parallel programming models <ref> [9, 19, 26] </ref>. In most cases the implementa-tion has needed to compromise on the semantics of the model (e.g., by blocking all threads in a shared address space when any thread makes a system call) or accept enormous inefficiency (e.g., by using a separate Unix process for every lightweight thread).
Reference: [10] <author> R. Campbell, G. Johnston and V. Russo, </author> <title> ``Choices (Class Hierarchical Open Interface for Custom Embedded Systems),'' </title> <booktitle> ACM SIGOPS Operating Systems Review 21:3 (July 1987), </booktitle> <pages> pp. 9-17. </pages>
Reference-contexts: Applications that need not run concurrently can be supported by separate operating systems. Building these systems from scratch would involve enormous expense and duplication of effort, but the development of customizable operating systems (such as Choices <ref> [10] </ref> or the x-Kernel [15]) may make their construction practical. It may even prove practical to customize operating systems at run time, so that models can be changed without rebooting.
Reference: [11] <author> D. Cheriton, </author> <title> ``The V Kernel A Software Base for Distributed Systems,'' </title> <booktitle> IEEE Software 1:2 (April 1984), </booktitle> <pages> pp. 19-42. </pages>
Reference-contexts: Mach is representative of a class of operating systems designed for parallel computing. Other systems in this class include Amoeba [21], Chorus [2], Topaz [27], and V <ref> [11] </ref>. To facilitate parallelism within applications, these systems allow more than one kernel-supported process to run in one address space. To implement minimal-cost threads of control, however, or to exercise control over the representation and scheduling of threads, coroutine packages must still be used within a single kernel process.
Reference: [12] <author> T. W. Doeppner, Jr., </author> <title> ``Threads: A System for the Support of Concurrent Programming,'' </title> <type> Technical Report CS-87-11, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1987. </year>
Reference-contexts: A second option is to use a coroutine-style package to map many user-level processes onto one or more kernel-level processes <ref> [12] </ref>. This option either forces an entire collection of user-level processes to wait when one of them performs a blocking operation, or else forces the programmer to cast all operations in terms of some non-standard non-blocking subset of the kernel interface.
Reference: [13] <author> D. Gelernter, </author> <title> ``Generative Communication in Linda,'' </title> <journal> ACM Transactions on Programming Languages and Systems 7:1 (January 1985), </journal> <pages> pp. 80-112. </pages>
Reference-contexts: Emerging programming models have provided many different styles of communication and process primitives. Individually-routed synchronous and asynchronous messages, unidirectional and bidirectional message channels, remote procedure calls, global buffers <ref> [13] </ref>, and shared address spaces with semaphores, monitors, or spin locks have all been used for communication and synchronization. Coroutines, lightweight run-to-completion threads, lightweight blocking threads, heavyweight single-threaded processes, and heavyweight multi-threaded processes have been used to express concurrency.
Reference: [14] <author> R. Hayes and R. D. Schlichting, </author> <title> ``Facilitating Mixed Language Programming in Distributed Systems,'' </title> <journal> IEEE Transactions on Software Engineering SE-13:12 (December 1987), </journal> <pages> pp. 1254-1264. </pages>
Reference-contexts: Related Work Multi-model programming is related to, but distinct from, the work of several other researchers. Remote procedure call systems have often been designed to work between programs written in multiple languages <ref> [5, 14, 16, 20] </ref>. RPC-based systems support a single style of process interaction, and are usually intended for a distributed environment; there is no obvious way to extend them to fine-grained process interactions.
Reference: [15] <author> N. C. Hutchinson and L. L. Peterson, </author> <booktitle> ``Design of the x-Kernel,'' Proceedings of the SIGCOMM '88 Symposium, </booktitle> <month> August </month> <year> 1988, </year> <pages> pp. 65-75. </pages>
Reference-contexts: Applications that need not run concurrently can be supported by separate operating systems. Building these systems from scratch would involve enormous expense and duplication of effort, but the development of customizable operating systems (such as Choices [10] or the x-Kernel <ref> [15] </ref>) may make their construction practical. It may even prove practical to customize operating systems at run time, so that models can be changed without rebooting. Even so, it is unlikely that applications will be able to communicate across models supported by disjoint portions of the kernel interface.
Reference: [16] <author> M. B. Jones, R. F. Rashid and M. R. Thompson, ``Matchmaker: </author> <title> An Interface Specification Language for Distributed Processing,'' </title> <booktitle> Conference Record of the Twelfth ACM Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1985, </year> <pages> pp. 225-235. </pages>
Reference-contexts: Related Work Multi-model programming is related to, but distinct from, the work of several other researchers. Remote procedure call systems have often been designed to work between programs written in multiple languages <ref> [5, 14, 16, 20] </ref>. RPC-based systems support a single style of process interaction, and are usually intended for a distributed environment; there is no obvious way to extend them to fine-grained process interactions.
Reference: [17] <author> D. W. Jones, </author> <title> ``Concurrent Operations on Priority Queues,'' </title> <journal> Communications of the ACM 32:1 (January 1989), </journal> <pages> pp. 132-137. </pages>
Reference-contexts: This example illustrates how data structures are shared among different programming models and also how the Psyche interface incorporates cooperation between the operating system and the application in the implementation of scheduling. The algorithm for concurrent priority queues can be adapted from <ref> [17] </ref>. The external interface of the queue includes two operations: insert and deletemin. The implementation uses a heap ordered binary tree. Deletemin removes and returns the smallest element in the queue by deleting the root of the tree and melding the two remaining subtrees.
Reference: [18] <author> T. J. LeBlanc, M. L. Scott and C. M. Brown, </author> <title> ``Large-Scale Parallel Programming: Experience with the BBN Butterfly Parallel Processor,'' </title> <booktitle> Proceedings of the First ACM Conference on Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <month> 19-21 July </month> <year> 1988, </year> <pages> pp. 161-172. </pages>
Reference-contexts: Since 1984 we have explored these issues while developing parallel programming environments for the BBN Butterfly multiprocessor. Using the Chrysalis operating system [4] as a low-level interface, we have created several new programming libraries and languages and ported several others <ref> [18] </ref>. We were able to construct efficient implementations of many different models of parallelism because Chrysalis allows the user to manage memory and address spaces explicitly, and provides efficient low-level mechanisms for communication and synchronization. Chrysalis processes are heavyweight, however, so lightweight processes must be encapsulated inside a heavyweight process.
Reference: [19] <author> B. Liskov, D. Curtis, P. Johnson and R. Scheifler, </author> <title> ``Implementation of Argus,'' </title> <booktitle> Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <month> 8-11 November </month> <year> 1987, </year> <pages> pp. 111-122. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 21:5. </booktitle>
Reference-contexts: These mechanisms are seldom amenable to change, and may not be well-matched to a new model under development, leading to awkward or inefficient implementations. The traditional Unix interface, for example, has been used beneath many new parallel programming models <ref> [9, 19, 26] </ref>. In most cases the implementa-tion has needed to compromise on the semantics of the model (e.g., by blocking all threads in a shared address space when any thread makes a system call) or accept enormous inefficiency (e.g., by using a separate Unix process for every lightweight thread).
Reference: [20] <author> B. Liskov, R. Bloom, D. Gifford, R. Scheifler and W. Weihl, </author> <title> ``Communication in the Mercury System,'' </title> <booktitle> Proceedings of the 21st Annual Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1988, </year> <pages> pp. 178-187. </pages>
Reference-contexts: Related Work Multi-model programming is related to, but distinct from, the work of several other researchers. Remote procedure call systems have often been designed to work between programs written in multiple languages <ref> [5, 14, 16, 20] </ref>. RPC-based systems support a single style of process interaction, and are usually intended for a distributed environment; there is no obvious way to extend them to fine-grained process interactions.
Reference: [21] <author> S. J. Mullender and A. S. Tanenbaum, </author> <title> ``The Design of a Capability-Based Distributed Operating System,'' </title> <journal> The Computer Journal 29:4 (1986), </journal> <pages> pp. 289-299. </pages>
Reference-contexts: Mach is representative of a class of operating systems designed for parallel computing. Other systems in this class include Amoeba <ref> [21] </ref>, Chorus [2], Topaz [27], and V [11]. To facilitate parallelism within applications, these systems allow more than one kernel-supported process to run in one address space.
Reference: [22] <author> M. L. Scott, </author> <title> ``The Interface Between Distributed Operating System and High-Level Programming Language,'' </title> <booktitle> Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <month> 19-22 August </month> <year> 1986, </year> <pages> pp. 242-249. </pages>
Reference-contexts: Modifying the operating system is seldom worthwhile for a single application. Augmenting the system from outside (in a library or language run time package) may be difficult as well; depending on the specific primitives provided and the functionality needed, even minor changes can be surprisingly hard to achieve <ref> [22] </ref>. Moving to another system is often not an option, and even when possible will generally preclude running applications to which the old system was well suited. To meet the needs of different applications, many different models of parallel programming must be implemented on the same underlying hardware. <p> Lynx was first implemented on top of Charlotte at the University of Wisconsin. The implementation was complex and difficult, because the operations provided by the operating system were not quite what the language wanted, and their semantics could not be changed <ref> [22] </ref>. For example, there are times when a Lynx process is only interested in receiving replies to RPCs it has initiated, and is not willing to service RPCs requested by anybody else.
Reference: [23] <author> M. L. Scott, </author> <title> ``Language Support for Loosely-Coupled Distributed Programs,'' </title> <journal> IEEE Transactions on Software Engineering SE-13:1 (January 1987), </journal> <pages> pp. 88-103. </pages>
Reference-contexts: Otherwise, the handler can place them in a queue for synchronous examination later. If we prefer a more sophisticated approach to process management and message passing, the communication library and link realms can easily be modified to provide the communication semantics of the Lynx distributed programming language <ref> [23] </ref>. Lynx subdivides each heavyweight process into lightweight threads of control, not to improve performance through parallelism, but as a program structuring tool. The threads use coroutine semantics: one thread runs until it blocks, then another runs.
Reference: [24] <author> M. L. Scott, T. J. LeBlanc and B. D. Marsh, </author> <title> ``Design Rationale for Psyche, a General-Purpose Multiprocessor Operating System,'' </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, V. II Software, </booktitle> <month> 15-19 August </month> <year> 1988, </year> <pages> pp. 255-262. </pages>
Reference-contexts: In the following section we overview the Psyche kernel interface, explaining how it differs from more conventional systems. (Additional details and design rationale can be found in other papers <ref> [24, 25] </ref>.) Using two different models as examples (lightweight threads in a single shared address space and heavyweight processes that communicate with messages), we show how dissimilar models can be implemented on top of Psyche.
Reference: [25] <author> M. L. Scott, T. J. LeBlanc and B. D. Marsh, </author> <title> ``Evolution of an Operating System for Large-Scale Shared-Memory Multiprocessors,'' </title> <type> TR 309, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: In the following section we overview the Psyche kernel interface, explaining how it differs from more conventional systems. (Additional details and design rationale can be found in other papers <ref> [24, 25] </ref>.) Using two different models as examples (lightweight threads in a single shared address space and heavyweight processes that communicate with messages), we show how dissimilar models can be implemented on top of Psyche.
Reference: [26] <author> M. Shapiro, </author> <title> ``Prototyping a Distributed Object-Oriented OS on UNIX,'' </title> <booktitle> Proceedings of the First USENIX Workshop on Experiences Building Distributed and Multiprocessor Systems, </booktitle> <month> 5-6 October, </month> <year> 1989, </year> <pages> pp. 311-331. </pages>
Reference-contexts: These mechanisms are seldom amenable to change, and may not be well-matched to a new model under development, leading to awkward or inefficient implementations. The traditional Unix interface, for example, has been used beneath many new parallel programming models <ref> [9, 19, 26] </ref>. In most cases the implementa-tion has needed to compromise on the semantics of the model (e.g., by blocking all threads in a shared address space when any thread makes a system call) or accept enormous inefficiency (e.g., by using a separate Unix process for every lightweight thread).
Reference: [27] <author> C. P. Thacker and L. C. Stewart, ``Firefly: </author> <title> A Multiprocessor Workstation,'' </title> <journal> IEEE Transactions on Computers 37:8 (August 1988), </journal> <pages> pp. 909-920. </pages> <booktitle> Originally presented at the Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> 5-8 October </month> <year> 1987. </year>
Reference-contexts: Mach is representative of a class of operating systems designed for parallel computing. Other systems in this class include Amoeba [21], Chorus [2], Topaz <ref> [27] </ref>, and V [11]. To facilitate parallelism within applications, these systems allow more than one kernel-supported process to run in one address space.
Reference: [28] <author> R. H. Thomas and W. Crowther, </author> <title> ``The Uniform System: An Approach to Runtime Support for Large Scale Shared Memory Parallel Processors,'' </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, V. II - Software, </booktitle> <month> 15-19 August </month> <year> 1988, </year> <pages> pp. 245-254. </pages>
Reference-contexts: The following three sections illustrate the advantages of the Psyche approach through a series of examples. 4. Lightweight Processes in a Shared Address Space Programming models in which a single address space is shared by many lightweight processes, such as Presto [6] and the Uniform System <ref> [28] </ref>, can be implemented easily and efficiently on Psyche. A lightweight process scheduler can be implemented as a library package that is linked into an application, creating a single realm and protection domain whose virtual processors share both the scheduling code and a central ready list.
Reference: [29] <author> J. Zahorjan, E. D. Lazowska and D. L. Eager, </author> <title> ``The Effect of Scheduling Discipline on Spin Overhead in Shared Memory Parallel Processors,'' </title> <type> TR 89-07-03, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: The discussion accompanying the original parallel priority queue algorithm pointed out that the most appropriate implementation for mutual exclusion is spin locks, not binary semaphores. Unfortunately, spin locks in the presence of preemption 1 can cause a significant performance penalty <ref> [29] </ref>. A process holding the lock could be preempted, causing other processes to spin on a lock that cannot be released. In addition, pent-up demand for the lock increases contention when it is finally released. <p> As a result, spin locks are rarely used unless the operating system dedicates processors to a single application. Fortunately, Psyche supports the ``close alliance between the system's processor allocator and the application's thread scheduler'' called for in <ref> [29] </ref> to solve the problem of spin locks in the presence of preemption.
References-found: 29

