URL: http://www.cs.nyu.edu/phd_students/ungurean/hipc96-camera.ps
Refering-URL: http://www.cs.nyu.edu/phd_students/ungurean/index.html
Root-URL: http://www.cs.nyu.edu
Email: E-mail: fleunga,palem,ungureang@cs.nyu.edu  
Title: Run-time versus Compile-time Instruction Scheduling in Superscalar (RISC) Processors: Performance and Tradeoffs  
Author: Allen Leung, Krishna V. Palem and Cristian Ungureanu 
Address: New York University, 251 Mercer Street, New York, NY 10012-1185  
Affiliation: Courant Institute of Mathematical Sciences  
Abstract: The RISC revolution has spurred the development of processors with increasing degrees of instruction level parallelism (ILP). In order to realize the full potential of these processors, multiple instructions must continuouslybe issued and executed in a single cycle. Consequently, instruction scheduling plays a crucial role as an optimization in this context. While early attempts at instruction scheduling were limited to compile-time approaches, the current trends are aimed at providing dynamic support in hardware. In this paper, we present the results of a detailed comparative study of the performance advantages to be derived by the spectrum of instruction scheduling approaches: from limited basic-block sched-ulers in the compiler, to novel and aggressive schedulers in hardware. A significant portion of our experimental study via simulations, is devoted to understanding the performance advantages of run-time scheduling. Our results indicate it to be effective in extracting the ILP inherent to the program trace being scheduled, over a wide range of machine and program parameters. Furthermore, we also show that this effectiveness can be further enhanced by a simple basic-block scheduler in the compiler, which optimizes for the presence of the run-time scheduler in the target; current basic-block schedulers are not designed to take advantage of this feature. We demonstrate this fact by presenting a novel basic-block scheduling algorithm that is sensitive to the lookahead hardware in the target processor. fl In Proceedings of the Third International Conference on High Performance Computing, Dec. 1996.
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> F. Allen, B. Rosen, and K. Zadeck (editors). </editor> <booktitle> Optimization in Compilers, </booktitle> <publisher> ACM Press and Addison-Wesley (to appear). </publisher>
Reference-contexts: As the degree of parallelism increased, these approaches to instruction scheduling have ranged from the early local scheduling approaches [18, 24] where the code-motion is limited to within a basic-block boundary <ref> [1] </ref>, to more ambitious (and expensive) global approaches [3, 7, 15, 5] which move instructions across basic-blocks. Typically, the data- and control-dependence relationships in the program limit the number of instructions that can be scheduled concurrently on the target processor.
Reference: [2] <author> D. Bernstein and I. Gertner. </author> <title> Scheduling Expressions on a Pipelined Processor with a Maximal Delay of One Cycle In ACM TOPLAS 11(1), </title> <journal> pp. </journal> <pages> 57-66, </pages> <year> 1989 </year>
Reference: [3] <author> D. Bernstein and M. Rodeh. </author> <title> Global Instruction Scheduling for Superscalar Machines. </title> <booktitle> In Proceedings of SIG-PLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 241-255, </pages> <year> 1991. </year>
Reference-contexts: As the degree of parallelism increased, these approaches to instruction scheduling have ranged from the early local scheduling approaches [18, 24] where the code-motion is limited to within a basic-block boundary [1], to more ambitious (and expensive) global approaches <ref> [3, 7, 15, 5] </ref> which move instructions across basic-blocks. Typically, the data- and control-dependence relationships in the program limit the number of instructions that can be scheduled concurrently on the target processor.
Reference: [4] <author> P. Dubey and G. B. Adams, III and M. Flynn. </author> <title> Instruction Window Size Trade-Offs and Characterization of program Parallelism In IEEE Transactions on Computers 43(4) 1994 </title>
Reference-contexts: The key architectural parameters of interest to us are the number and types of functional units in the target processor, their degree of pipelining, and most significantly, the scope <ref> [4] </ref> of the lookahead hardware used by the run-time scheduler. The scope is the number of instructions that the hardware can analyze and attempts to schedule dynamically. Our experiments ranged over a total of 1344 parametric variations over the spectrum of superscalar processors. <p> Similarly, a lot of interest has gone into the study of available ILP in superscalar RISC processors in isolation, independent of its interaction with instruction scheduling done at compile-time; please see <ref> [13, 4, 22, 21, 23] </ref> for further details. There are also some studies that study the combined effect of having optimizations at both levels by lumping the two together; [5, 16] are typical examples.
Reference: [5] <author> R. Cohn and T. Gross and M. Lam and P. S. Tseng. </author> <title> Architecture and Compiler Trade-offs for a Long Instruc 9 (a) k = 8; m = 2 b=15 403530252015105 95 85 75 65 b=29 b=5 100 99 98 97 96 hardware R E;G for pipelines of depth 8 tion Word Microprocessor In Proceedings of ASPLOS III, </title> <month> Apr. </month> <year> 1989, </year> <pages> pp. 2-14 </pages>
Reference-contexts: As the degree of parallelism increased, these approaches to instruction scheduling have ranged from the early local scheduling approaches [18, 24] where the code-motion is limited to within a basic-block boundary [1], to more ambitious (and expensive) global approaches <ref> [3, 7, 15, 5] </ref> which move instructions across basic-blocks. Typically, the data- and control-dependence relationships in the program limit the number of instructions that can be scheduled concurrently on the target processor. <p> There are also some studies that study the combined effect of having optimizations at both levels by lumping the two together; <ref> [5, 16] </ref> are typical examples. To the best of our knowledge, we are the first to explicitly study the relative merits of instruction scheduling at each of these levels, with emphasis on the interactions and tradeoffs involved.
Reference: [6] <author> Keith Dieterdorf and Rich Oehler and Ron Hochsprung. </author> <title> Evolution of the PowerPC Architecture IEEE Micro pp. </title> <month> 34-49 </month> <year> 1994 </year>
Reference-contexts: For example, the outcome of conditional branches will affect the choice of instructions to be executed next. Consequently, as superscalar processor designs evolved, a significant trend has been to support instruction scheduling directly in hardware <ref> [11, 10, 6] </ref>, i.e. dynamic scheduling. Informally, the run-time hardware has a window of instructions that it looks into, ahead of the current instruction to 1 be executed.
Reference: [7] <author> J. Fisher. </author> <title> Trace Scheduling: A General Technique for Global Microcode Compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(7):478-490, </volume> <year> 1981. </year>
Reference-contexts: As the degree of parallelism increased, these approaches to instruction scheduling have ranged from the early local scheduling approaches [18, 24] where the code-motion is limited to within a basic-block boundary [1], to more ambitious (and expensive) global approaches <ref> [3, 7, 15, 5] </ref> which move instructions across basic-blocks. Typically, the data- and control-dependence relationships in the program limit the number of instructions that can be scheduled concurrently on the target processor. <p> Our results establish the benefits of run-time scheduling in the affirmative for ranges of variations of these parameters. Specifically, with enough ILP in the source program, run-time sched-ulers approach the peak performance benefits afforded by compile time global scheduling trace scheduling <ref> [7] </ref> is our canonical global scheduling scheme used here even when the compile-time approach is assumed to be completely clairvoyant in that it precisely knows the behavior of branches ahead of time.
Reference: [8] <author> W. M. Johnson. </author> <title> Superscalar Microprocessor Design Prentice Hall, </title> <address> Engelwood Cliffs, NY, </address> <year> 1991 </year>
Reference-contexts: 1. Introduction The trend in the design of modern RISC processors <ref> [8] </ref> is to have an increasing amount of instruction level parallelism (ILP) provided within a single processor via multiple pipelined functional units. The expected performance gains of these machines are fully realized by a variety of optimizations, with instruction scheduling (or scheduling for short) playing a significant role.
Reference: [9] <author> Joseph Fisher. </author> <title> Global Code Generation for Instruction level Parallelism: Trace Scheduling-2 1991 </title>
Reference: [10] <author> Linley Gwennap. </author> <title> 620 Fills Out PowerPC Product Line Microprocessor Report, </title> <month> October </month> <year> 1994 </year>
Reference-contexts: For example, the outcome of conditional branches will affect the choice of instructions to be executed next. Consequently, as superscalar processor designs evolved, a significant trend has been to support instruction scheduling directly in hardware <ref> [11, 10, 6] </ref>, i.e. dynamic scheduling. Informally, the run-time hardware has a window of instructions that it looks into, ahead of the current instruction to 1 be executed.
Reference: [11] <author> Linley Gwennap. </author> <title> MIPS R10000 Uses Decoupled Ar chitecture Microprocessor Report, </title> <month> October </month> <year> 1994 </year>
Reference-contexts: For example, the outcome of conditional branches will affect the choice of instructions to be executed next. Consequently, as superscalar processor designs evolved, a significant trend has been to support instruction scheduling directly in hardware <ref> [11, 10, 6] </ref>, i.e. dynamic scheduling. Informally, the run-time hardware has a window of instructions that it looks into, ahead of the current instruction to 1 be executed.
Reference: [12] <author> W.-M. W. Hwu et al. </author> <title> The Superblock: An Effective Technique for VLIW and Superscalar Compilation. </title> <journal> The Journal of Supercomputing, </journal> <volume> Vol. 7 (1993), </volume> <pages> 229-248. </pages>
Reference-contexts: We offer this solution to re-engineering the basic-block sched-uler since global scheduling is quite expensive to implement, and it remains a technically challenging research question as well <ref> [15, 12, 16, 20] </ref>. Furthermore, as we show in this paper, the cumulative performance gains to be derived by a combination of our enhanced algorithm for basic-blocks, and an aggressive run-time scheduler, are over half of those that can be derived by a truly clairvoyant global scheduler.
Reference: [13] <author> Ray A. Kamin III and George B. Adams III and Pradeep K. Dubey. </author> <title> Dynamic Trace Analysis for Analytic Modeling of Superscalar Performance In Performance Eval uation 19 (1994) pp. </title> <type> 259-276, </type> <year> 1994. </year>
Reference-contexts: Similarly, a lot of interest has gone into the study of available ILP in superscalar RISC processors in isolation, independent of its interaction with instruction scheduling done at compile-time; please see <ref> [13, 4, 22, 21, 23] </ref> for further details. There are also some studies that study the combined effect of having optimizations at both levels by lumping the two together; [5, 16] are typical examples.
Reference: [14] <author> M. Lam. </author> <title> Software Pipelining: An Effective Method for VLIW Machines In Proc. </title> <booktitle> ACM SIGLPAN`88 Conference on Programming Language Design and Implemen tation, </booktitle> <pages> pp. 318-327, </pages> <year> 1988. </year>
Reference: [15] <author> S. Moon and K. Ebcioglu. </author> <title> An Efficient Resource-constrained Global Scheduling Technique for Super scalar and VLIW Processors In Proc. </title> <month> MICRO-25 </month> <year> 1992 </year>
Reference-contexts: As the degree of parallelism increased, these approaches to instruction scheduling have ranged from the early local scheduling approaches [18, 24] where the code-motion is limited to within a basic-block boundary [1], to more ambitious (and expensive) global approaches <ref> [3, 7, 15, 5] </ref> which move instructions across basic-blocks. Typically, the data- and control-dependence relationships in the program limit the number of instructions that can be scheduled concurrently on the target processor. <p> We offer this solution to re-engineering the basic-block sched-uler since global scheduling is quite expensive to implement, and it remains a technically challenging research question as well <ref> [15, 12, 16, 20] </ref>. Furthermore, as we show in this paper, the cumulative performance gains to be derived by a combination of our enhanced algorithm for basic-blocks, and an aggressive run-time scheduler, are over half of those that can be derived by a truly clairvoyant global scheduler.
Reference: [16] <author> Toshio Nakatani and Kemal Ebcioglu. </author> <note> Making Compaction-Based Par-allelization Affordable IEEE Transactions on Parallel and Distributed Systems, 4(9) 1993 pp. :1014-1029 </note>
Reference-contexts: We offer this solution to re-engineering the basic-block sched-uler since global scheduling is quite expensive to implement, and it remains a technically challenging research question as well <ref> [15, 12, 16, 20] </ref>. Furthermore, as we show in this paper, the cumulative performance gains to be derived by a combination of our enhanced algorithm for basic-blocks, and an aggressive run-time scheduler, are over half of those that can be derived by a truly clairvoyant global scheduler. <p> There are also some studies that study the combined effect of having optimizations at both levels by lumping the two together; <ref> [5, 16] </ref> are typical examples. To the best of our knowledge, we are the first to explicitly study the relative merits of instruction scheduling at each of these levels, with emphasis on the interactions and tradeoffs involved.
Reference: [17] <author> K. Palem and V. Sarkar. </author> <title> Code Optimization in Modern Compilers Lecture Notes, </title> <institution> Western Institute of Computer Science, Stanford University, </institution> <year> 1995. </year>
Reference-contexts: In this measure, the run-time scheduler is assisted by a conventional basic-block scheduler L, which is typically a part of modern optimizing compilers for superscalar processors <ref> [17] </ref>. Both of these are compared relative to basic-block scheduling in the context of a processor with no hardware sup port for scheduling, V . 2.
Reference: [18] <author> K. Palem and B. Simons. </author> <title> Scheduling Time-critical Instructions on RISC Machines. </title> <journal> ACM TOPLAS, </journal> <volume> 5(3), </volume> <year> 1993. </year>
Reference-contexts: In the early RISC processors, scheduling was typically done at compile-time and statically produced a sequence of instructions with good run-time behavior. As the degree of parallelism increased, these approaches to instruction scheduling have ranged from the early local scheduling approaches <ref> [18, 24] </ref> where the code-motion is limited to within a basic-block boundary [1], to more ambitious (and expensive) global approaches [3, 7, 15, 5] which move instructions across basic-blocks. <p> Phase 1: computing priorities We define the notion of a priority of a node which will be used to build the sorted list L. It has two components: a global priority and a local priority. Each of these components is derived from the rank function defined in <ref> [18] </ref>. For each instruction, the local priority is derived strictly locally from its own basic-block, using the algorithm from [18] (please see Figure 2). <p> It has two components: a global priority and a local priority. Each of these components is derived from the rank function defined in <ref> [18] </ref>. For each instruction, the local priority is derived strictly locally from its own basic-block, using the algorithm from [18] (please see Figure 2). <p> In the second case, the rank of an instruction or node is computed lexicographically. Because these schemes produced very similar results, henceforth we shall not distinguish them. Phase 2: scheduling This phase is a generalization of the well-known greedy scheduling procedure <ref> [18] </ref> (please see In contrast to traditional local scheduling, ready instructions now include instructions from potentially more than one basic block.
Reference: [19] <author> K. Palem and B. Simons. </author> <title> Instruction Scheduling. In Optimization in Compilers, </title> <editor> (eds: F. Allen, B. Rosen and K. Zadeck). </editor> <publisher> ACM Press and Addison-Wesley (to ap pear). </publisher>
Reference: [20] <author> B. R. Rau. </author> <title> Iterative Modulo Scheduling: </title> <booktitle> An Algorithm for Software Pipelining Loops In Proceedings of the 27th Annual Symposium on Microarchitecture (MICRO 27), </booktitle> <month> Nov. </month> <year> 1994 </year>
Reference-contexts: We offer this solution to re-engineering the basic-block sched-uler since global scheduling is quite expensive to implement, and it remains a technically challenging research question as well <ref> [15, 12, 16, 20] </ref>. Furthermore, as we show in this paper, the cumulative performance gains to be derived by a combination of our enhanced algorithm for basic-blocks, and an aggressive run-time scheduler, are over half of those that can be derived by a truly clairvoyant global scheduler.
Reference: [21] <author> G. Sohi and S. </author> <title> Vajapeyam. </title> <booktitle> Instruction Issue Logic in High-performance Interruptible Pipelined Processors In Proceedings 14th Annual ACM Symposium on Com puter Architecture, </booktitle> <pages> pp. 27-34, </pages> <year> 1987. </year>
Reference-contexts: Similarly, a lot of interest has gone into the study of available ILP in superscalar RISC processors in isolation, independent of its interaction with instruction scheduling done at compile-time; please see <ref> [13, 4, 22, 21, 23] </ref> for further details. There are also some studies that study the combined effect of having optimizations at both levels by lumping the two together; [5, 16] are typical examples.
Reference: [22] <author> M. Smith, M. Johnson and M. Horowitz. </author> <booktitle> Limits to Multiple Instruction Issue In Proceedings of 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 27 34, </pages> <year> 1987. </year>
Reference-contexts: Similarly, a lot of interest has gone into the study of available ILP in superscalar RISC processors in isolation, independent of its interaction with instruction scheduling done at compile-time; please see <ref> [13, 4, 22, 21, 23] </ref> for further details. There are also some studies that study the combined effect of having optimizations at both levels by lumping the two together; [5, 16] are typical examples.
Reference: [23] <author> D. </author> <title> Wall. </title> <booktitle> Limits to Instruction Level Parallelism In Proceedings of 3rd International Conference on Architectural Support for Programming Languages and Operat ing Systems, </booktitle> <pages> pp. 290-302, </pages> <year> 1989. </year>
Reference-contexts: Similarly, a lot of interest has gone into the study of available ILP in superscalar RISC processors in isolation, independent of its interaction with instruction scheduling done at compile-time; please see <ref> [13, 4, 22, 21, 23] </ref> for further details. There are also some studies that study the combined effect of having optimizations at both levels by lumping the two together; [5, 16] are typical examples.
Reference: [24] <author> H. Warren. </author> <title> Instruction Scheduling for the IBM RISC System/6K Processors. </title> <journal> IBM Journal of Research and Development, </journal> <pages> 85-92, </pages> <year> 1990. </year> <month> 10 </month>
Reference-contexts: In the early RISC processors, scheduling was typically done at compile-time and statically produced a sequence of instructions with good run-time behavior. As the degree of parallelism increased, these approaches to instruction scheduling have ranged from the early local scheduling approaches <ref> [18, 24] </ref> where the code-motion is limited to within a basic-block boundary [1], to more ambitious (and expensive) global approaches [3, 7, 15, 5] which move instructions across basic-blocks.
References-found: 24

