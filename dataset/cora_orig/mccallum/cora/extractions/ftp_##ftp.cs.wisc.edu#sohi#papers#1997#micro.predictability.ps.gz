URL: ftp://ftp.cs.wisc.edu/sohi/papers/1997/micro.predictability.ps.gz
Refering-URL: http://www.cs.wisc.edu/mscalar/publications.html
Root-URL: 
Email: fyanos,jesg@ece.wisc.edu  
Title: The Predictability of Data Values  
Author: Yiannakis Sazeides and James E. Smith 
Address: Dr. Madison, WI 53706  
Affiliation: Department of Electrical and Computer Engineering University of Wisconsin-Madison 1415 Engr.  
Abstract: Copyright 1997 IEEE. Published in the Proceedings of Micro-30, December 1-3, 1997 in Research Triangle Park, North Carolina. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions IEEE Service Center 445 Hoes Lane P.O. Box 1331 Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. H. Lipasti, C. B. Wilkerson, and J. P. Shen, </author> <title> Value locality and data speculation, </title> <booktitle> in Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 138147, </pages> <month> Oc-tober </month> <year> 1996. </year>
Reference-contexts: An important difference between conditional branch prediction and data value prediction is that data are taken from a much larger range of values. This would appear to severely limit the chances of successful prediction. However, it has been demonstrated recently <ref> [1] </ref> that data values exhibit locality where values computed by some instructions tend to repeat a large fraction of the time. We argue that establishing predictability limits for program values is important for determining the performance potential of processors that use value prediction. <p> Non-Stride (NS) 28 -13 -99 107 23 456... Constant sequences are the simplest, and result from instructions that repeatedly produce the same result. Li-pasti and Shen show that this occurs surprisingly often, and forms the basis for their work reported in <ref> [1] </ref>. A stride sequence has elements that differ by some constant delta. For the example above, the stride is one, which is probably the most common case in programs, but other strides are possible, including negative strides. Constant sequences can be considered stride sequences with a zero delta. <p> Context based predictors learn the value (s) that follow a particular context a finite ordered sequence of values and predict one of the values when the same context repeats. This enables the prediction of any repeated sequence, stride or non-stride. 1.2 Related Work In <ref> [1] </ref>, it was reported that data values produced by instructions exhibit locality and as a result can be predicted. <p> A pronounced difference is observed between the locality with history depth 1 and history depth 16. The mechanism proposed for prediction, however, exploits the locality of history depth 1 and is based on predicting that the most recent value will also be the next. In <ref> [1] </ref>, last value prediction was used to predict load values and in a subsequent work to predict all values produced by instructions and written to registers [2].
Reference: [2] <author> M. H. Lipasti and J. P. Shen, </author> <title> Exceeding the dataflow limit via value prediction, </title> <booktitle> in Proceedings of the 29th Annual ACM/IEEE International Symposium and Workshop on Mi-croarchitecture, </booktitle> <pages> pp. 226237, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: In [1], last value prediction was used to predict load values and in a subsequent work to predict all values produced by instructions and written to registers <ref> [2] </ref>. Address prediction has been used mainly for data prefetching to tolerate long memory latency [3, 4, 5], and has been proposed for speculative execution of load and store instructions [6, 7]. <p> Overall, last value prediction is less accurate than stride prediction, and stride prediction is less accurate than fcm prediction. Last value prediction varies in accuracy from about 23% to 61% with an average of about 40%. This is in agreement with the results obtained in <ref> [2] </ref>. Stride prediction provides accuracy of between 38% and 80% with an average of about 56%. Fcm predictors of orders 1, 2, and 3 all perform better than stride prediction; and the higher the order, the higher the accuracy.
Reference: [3] <author> T. F. Chen and J. L. Baer, </author> <title> Effective hardware-based data prefetching for high performance processors, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 44, </volume> <pages> pp. 609623, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: In [1], last value prediction was used to predict load values and in a subsequent work to predict all values produced by instructions and written to registers [2]. Address prediction has been used mainly for data prefetching to tolerate long memory latency <ref> [3, 4, 5] </ref>, and has been proposed for speculative execution of load and store instructions [6, 7]. Stride prediction for values was proposed in [8] and its prediction and performance potential was compared against last value prediction.
Reference: [4] <author> S. Mehrotra and L. Harrison, </author> <title> Examination of a memory access classification scheme for pointer intensive and numeric programs, </title> <booktitle> in Proceedings of the 10th International Conference on Supercomputing, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: In [1], last value prediction was used to predict load values and in a subsequent work to predict all values produced by instructions and written to registers [2]. Address prediction has been used mainly for data prefetching to tolerate long memory latency <ref> [3, 4, 5] </ref>, and has been proposed for speculative execution of load and store instructions [6, 7]. Stride prediction for values was proposed in [8] and its prediction and performance potential was compared against last value prediction.
Reference: [5] <author> D. Joseph and D. Grunwald, </author> <title> Prefetching using markov predictors, </title> <booktitle> in Proceedings of the 24th International Symposium on Computer Architecture, </booktitle> <pages> pp. 252263, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: In [1], last value prediction was used to predict load values and in a subsequent work to predict all values produced by instructions and written to registers [2]. Address prediction has been used mainly for data prefetching to tolerate long memory latency <ref> [3, 4, 5] </ref>, and has been proposed for speculative execution of load and store instructions [6, 7]. Stride prediction for values was proposed in [8] and its prediction and performance potential was compared against last value prediction.
Reference: [6] <author> R. J. Eickemeyer and S. Vassiliadis, </author> <title> A load instruction unit for pipelined processors, </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 37, </volume> <pages> pp. 547564, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Address prediction has been used mainly for data prefetching to tolerate long memory latency [3, 4, 5], and has been proposed for speculative execution of load and store instructions <ref> [6, 7] </ref>. Stride prediction for values was proposed in [8] and its prediction and performance potential was compared against last value prediction. Value prediction can draw from a wealth of work on the prediction of control dependences [9, 10, 11]. <p> Virtually all proposed schemes perform predictions based on previous architected state and values. Notable exceptions to this are the schemes proposed in <ref> [6] </ref>, where it is predicted that a fetched load instruction has no dependence and the instruction is executed early without dependence checking, and in [21], where it is predicted that the operation required to calculate an effective address using two operands is a logical or instead of a binary addition. <p> In [7] the stride is only changed if a saturating counter that is incremented/decremented on success/failure of the predictions is below a certain threshold. This reduces the number of mispredictions in repeated stride sequences from two per repeated sequence to one. Another policy, the two-delta method, was proposed in <ref> [6] </ref>. In the two-delta method, two strides are maintained. The one stride (s1) is always updated by the difference between the two most recent values, whereas the other (s2) is the stride used for computing the predictions.
Reference: [7] <author> J. Gonzalez and A. Gonzalez, </author> <title> Speculative execution via address prediction and data prefetching, </title> <booktitle> in Proceedings of the 11th International Conference on Supercomputing, </booktitle> <pages> pp. </pages> <address> 196203, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: Address prediction has been used mainly for data prefetching to tolerate long memory latency [3, 4, 5], and has been proposed for speculative execution of load and store instructions <ref> [6, 7] </ref>. Stride prediction for values was proposed in [8] and its prediction and performance potential was compared against last value prediction. Value prediction can draw from a wealth of work on the prediction of control dependences [9, 10, 11]. <p> That is if v n1 and v n2 are the two most recent values, then the predictor computes v n1 + (v n1 - v n2 ). As with the last value predictors, there are important variations that use hysteresis. In <ref> [7] </ref> the stride is only changed if a saturating counter that is incremented/decremented on success/failure of the predictions is below a certain threshold. This reduces the number of mispredictions in repeated stride sequences from two per repeated sequence to one. Another policy, the two-delta method, was proposed in [6].
Reference: [8] <author> A. Mendelson and F. Gabbay, </author> <title> Speculative execution based on value prediction, </title> <type> Tech. Rep. </type> <note> (Available from http://www-ee.technion.ac.il/fredg), Technion, </note> <year> 1997. </year>
Reference-contexts: Address prediction has been used mainly for data prefetching to tolerate long memory latency [3, 4, 5], and has been proposed for speculative execution of load and store instructions [6, 7]. Stride prediction for values was proposed in <ref> [8] </ref> and its prediction and performance potential was compared against last value prediction. Value prediction can draw from a wealth of work on the prediction of control dependences [9, 10, 11]. The majority of improvements in the performance of control flow predictors has been obtained by using correlation.
Reference: [9] <author> J. E. Smith, </author> <title> A study of branch prediction strategies, </title> <booktitle> in Proceedings of the 8th International Symposium on Computer Architecture, </booktitle> <pages> pp. 135148, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: Stride prediction for values was proposed in [8] and its prediction and performance potential was compared against last value prediction. Value prediction can draw from a wealth of work on the prediction of control dependences <ref> [9, 10, 11] </ref>. The majority of improvements in the performance of control flow predictors has been obtained by using correlation. The correlation information that has been proposed includes local and global branch history [10], path address history [11, 12, 13], and path register contents [14].
Reference: [10] <author> T.-Y. Yeh and Y. N. Patt, </author> <title> Alternative implementations of two-level adaptive branch prediction, </title> <booktitle> in Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pp. 124134, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Stride prediction for values was proposed in [8] and its prediction and performance potential was compared against last value prediction. Value prediction can draw from a wealth of work on the prediction of control dependences <ref> [9, 10, 11] </ref>. The majority of improvements in the performance of control flow predictors has been obtained by using correlation. The correlation information that has been proposed includes local and global branch history [10], path address history [11, 12, 13], and path register contents [14]. <p> Value prediction can draw from a wealth of work on the prediction of control dependences [9, 10, 11]. The majority of improvements in the performance of control flow predictors has been obtained by using correlation. The correlation information that has been proposed includes local and global branch history <ref> [10] </ref>, path address history [11, 12, 13], and path register contents [14]. An interesting theoretical observation is the resemblance of the predictors used for control dependence prediction to the prediction models for text compression [15].
Reference: [11] <author> P.-Y. Chang, E. Hao, and Y. N. Patt, </author> <title> Target prediction for indirect jumps, </title> <booktitle> in Proceedings of the 24th International Symposium on Computer Architecture, </booktitle> <pages> pp. 274283, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Stride prediction for values was proposed in [8] and its prediction and performance potential was compared against last value prediction. Value prediction can draw from a wealth of work on the prediction of control dependences <ref> [9, 10, 11] </ref>. The majority of improvements in the performance of control flow predictors has been obtained by using correlation. The correlation information that has been proposed includes local and global branch history [10], path address history [11, 12, 13], and path register contents [14]. <p> The majority of improvements in the performance of control flow predictors has been obtained by using correlation. The correlation information that has been proposed includes local and global branch history [10], path address history <ref> [11, 12, 13] </ref>, and path register contents [14]. An interesting theoretical observation is the resemblance of the predictors used for control dependence prediction to the prediction models for text compression [15].
Reference: [12] <author> C. Young and M. D. Smith, </author> <title> Improving the accuracy of static branch prediction using branch correlation, </title> <booktitle> in Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 232241, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The majority of improvements in the performance of control flow predictors has been obtained by using correlation. The correlation information that has been proposed includes local and global branch history [10], path address history <ref> [11, 12, 13] </ref>, and path register contents [14]. An interesting theoretical observation is the resemblance of the predictors used for control dependence prediction to the prediction models for text compression [15].
Reference: [13] <author> R. Nair, </author> <title> Dynamic path-based branch correlation, </title> <booktitle> in Proceedings of the 28th Annual ACM/IEEE International Symposium and Workshop on Microarchitecture, </booktitle> <pages> pp. 1523, </pages> <month> De-cember </month> <year> 1995. </year>
Reference-contexts: The majority of improvements in the performance of control flow predictors has been obtained by using correlation. The correlation information that has been proposed includes local and global branch history [10], path address history <ref> [11, 12, 13] </ref>, and path register contents [14]. An interesting theoretical observation is the resemblance of the predictors used for control dependence prediction to the prediction models for text compression [15].
Reference: [14] <author> S. Mahlke and B. Natarajan, </author> <title> Compiler synthesized dynamic branch prediction, </title> <booktitle> in Proceedings of the 29th Annual ACM/IEEE International Symposium and Workshop on Microarchitecture, </booktitle> <pages> pp. 153164, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: The majority of improvements in the performance of control flow predictors has been obtained by using correlation. The correlation information that has been proposed includes local and global branch history [10], path address history [11, 12, 13], and path register contents <ref> [14] </ref>. An interesting theoretical observation is the resemblance of the predictors used for control dependence prediction to the prediction models for text compression [15].
Reference: [15] <author> I.-C. K. Cheng, J. T. Coffey, and T. N. Mudge, </author> <title> Analysis of branch prediction via data compression, </title> <booktitle> in Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: The correlation information that has been proposed includes local and global branch history [10], path address history [11, 12, 13], and path register contents [14]. An interesting theoretical observation is the resemblance of the predictors used for control dependence prediction to the prediction models for text compression <ref> [15] </ref>. This is an important observation because it re-enforces the approach used for control flow prediction and also suggests that compression-like methods can also be used for data value prediction. A number of interesting studies report on the importance of predicting and eliminating data dependences.
Reference: [16] <author> A. Moshovos, S. E. Breach, T. J. Vijaykumar, and G. Sohi, </author> <title> Dynamic speculation and synchronization of data dependences, </title> <booktitle> in Proceedings of the 24th International Symposium on Computer Architecture, </booktitle> <pages> pp. 181193, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: This is an important observation because it re-enforces the approach used for control flow prediction and also suggests that compression-like methods can also be used for data value prediction. A number of interesting studies report on the importance of predicting and eliminating data dependences. Moshovos <ref> [16] </ref> proposes mechanisms that reduce misspec-ulation by predicting when dependences exist between store and load instructions. The potential of data dependence elimination using prediction and speculation in combination with collapsing was examined in [17]. Elimination of redundant computation is the theme of a number of software/hardware proposals [18, 19, 20].
Reference: [17] <author> Y. Sazeides, S. Vassiliadis, and J. E. Smith, </author> <title> The performance potential of data dependence speculation & collapsing, </title> <booktitle> in Proceedings of the 29th Annual ACM/IEEE International Symposium and Workshop on Microarchitecture, </booktitle> <pages> pp. 238247, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: A number of interesting studies report on the importance of predicting and eliminating data dependences. Moshovos [16] proposes mechanisms that reduce misspec-ulation by predicting when dependences exist between store and load instructions. The potential of data dependence elimination using prediction and speculation in combination with collapsing was examined in <ref> [17] </ref>. Elimination of redundant computation is the theme of a number of software/hardware proposals [18, 19, 20].
Reference: [18] <author> S. P. Harbison, </author> <title> An architectural alternative to optimizing compilers, </title> <booktitle> in Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 5765, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: Moshovos [16] proposes mechanisms that reduce misspec-ulation by predicting when dependences exist between store and load instructions. The potential of data dependence elimination using prediction and speculation in combination with collapsing was examined in [17]. Elimination of redundant computation is the theme of a number of software/hardware proposals <ref> [18, 19, 20] </ref>. These schemes are similar in that they store in a cache the input and output parameters of a function and when the same inputs are detected the output is used without performing the function. Virtually all proposed schemes perform predictions based on previous architected state and values.
Reference: [19] <author> S. E. Richardson, </author> <title> Caching function results: Faster arithmetic by avoiding unnecessary computation, </title> <type> Tech. Rep. </type> <institution> SMLI TR-92-1, Sun Microsystems Laboratories, </institution> <month> Septem-ber </month> <year> 1992. </year>
Reference-contexts: Moshovos [16] proposes mechanisms that reduce misspec-ulation by predicting when dependences exist between store and load instructions. The potential of data dependence elimination using prediction and speculation in combination with collapsing was examined in [17]. Elimination of redundant computation is the theme of a number of software/hardware proposals <ref> [18, 19, 20] </ref>. These schemes are similar in that they store in a cache the input and output parameters of a function and when the same inputs are detected the output is used without performing the function. Virtually all proposed schemes perform predictions based on previous architected state and values.
Reference: [20] <author> A. Sodani and G. S. Sohi, </author> <title> Dynamic instruction reuse, </title> <booktitle> in Proceedings of the 24th International Symposium on Computer Architecture, </booktitle> <pages> pp. </pages> <address> 194205, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Moshovos [16] proposes mechanisms that reduce misspec-ulation by predicting when dependences exist between store and load instructions. The potential of data dependence elimination using prediction and speculation in combination with collapsing was examined in [17]. Elimination of redundant computation is the theme of a number of software/hardware proposals <ref> [18, 19, 20] </ref>. These schemes are similar in that they store in a cache the input and output parameters of a function and when the same inputs are detected the output is used without performing the function. Virtually all proposed schemes perform predictions based on previous architected state and values.
Reference: [21] <author> T. M. Austin and G. S. Sohi, </author> <title> Zero-cycle loads: Microarchi-tecture support for reducing load latency, </title> <booktitle> in Proceedings of the 28th Annual ACM/IEEE International Symposium and Workshop on Microarchitecture, </booktitle> <pages> pp. 8292, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Virtually all proposed schemes perform predictions based on previous architected state and values. Notable exceptions to this are the schemes proposed in [6], where it is predicted that a fetched load instruction has no dependence and the instruction is executed early without dependence checking, and in <ref> [21] </ref>, where it is predicted that the operation required to calculate an effective address using two operands is a logical or instead of a binary addition. In more theoretical work, Hammerstrom [22] used information theory to study the information content (entropy) of programs.
Reference: [22] <author> D. Hammerstrom and E. Davidson, </author> <title> Information content of cpu memory referencing behavior, </title> <booktitle> in Proceedings of the 4th International Symposium on Computer Architecture, </booktitle> <pages> pp. 184192, </pages> <month> March </month> <year> 1977. </year>
Reference-contexts: In more theoretical work, Hammerstrom <ref> [22] </ref> used information theory to study the information content (entropy) of programs. His study of the information content of address and instruction streams revealed a high degree of redundancy.
Reference: [23] <author> S. McFarling, </author> <title> Combining branch predictors, </title> <type> Tech. Rep. </type> <institution> DEC WRL TN-36, Digital Western Research Laboratory, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Going down this path would lead to large hybrid predictors that combine many special-case computational predictors with a chooser as has been proposed for conditional branches in <ref> [23, 24] </ref>. While hybrid prediction for data values is in general a good idea, a potential pitfall is that it may yield an ever-escalating collection of computational predictors, each of which predicts a diminishing number of additional values not caught by the others.
Reference: [24] <author> M. Evers, P.-Y. Chang, and Y. N. Patt, </author> <title> Using hybrid branch predictors to improve branch prediciton in the presence of context switches, </title> <booktitle> in Proceedings of the 23rd International Symposium on Computer Architecture, </booktitle> <pages> pp. 311, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Going down this path would lead to large hybrid predictors that combine many special-case computational predictors with a chooser as has been proposed for conditional branches in <ref> [23, 24] </ref>. While hybrid prediction for data values is in general a good idea, a potential pitfall is that it may yield an ever-escalating collection of computational predictors, each of which predicts a diminishing number of additional values not caught by the others.
Reference: [25] <author> T. C. Bell, J. G. Cleary, and I. H. Witten, </author> <title> Text Compression. </title> <publisher> Prentice-Hall Inc., </publisher> <address> New Jersey, </address> <year> 1990. </year>
Reference-contexts: An important type of context based predictors is derived from finite context methods used in text compression <ref> [25] </ref>. Finite Context Method Predictors (fcm) rely on mechanisms that predict the next value based on a finite number of preceding values. An order k fcm predictor uses k preceding values. Fcms are constructed with counters that count the occurrences of a particular value immediately following a certain context (pattern). <p> The combination of more than one prediction model is known as blending <ref> [25] </ref>. There are a number of variations of blending algorithms, depending on the information that is updated. Full blending updates all contexts, and lazy exclusion selects the prediction with the longer context match and only updates the counts for the predictions with the longer match or higher.
Reference: [26] <author> D. Burger, T. M. Austin, and S. Bennett, </author> <title> Evaluating future microprocessors: The simplescalar tool set, </title> <type> Tech. Rep. </type> <institution> CS-TR-96-1308, University of Wisconsin-Madison, </institution> <month> July </month> <year> 1996. </year>
Reference-contexts: Fcm predictors are studied for orders 1, 2 and 3. To form a context for the fcm predictor we use full concatenation of history values so there is no aliasing when matching contexts. Trace driven simulation was conducted using the Sim-plescalar toolset <ref> [26] </ref> for the integer SPEC95 benchmarks shown in Table 2 1 . The benchmarks were compiled using the simplescalar compiler with -O3 optimization. Integer benchmarks were selected because they tend to have less data parallelism and may therefore benefit more from data predictions.
References-found: 26

