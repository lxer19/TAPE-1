URL: ftp://ftp.cs.rochester.edu/pub/u/kyros/cvpr97.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/kyros/publicat.htm
Root-URL: 
Email: kyros@cs.rochester.edu  
Title: Shape from the Light Field Boundary  
Author: Kiriakos N. Kutulakos 
Address: Rochester, NY 14627-0226 USA  
Affiliation: Department of Computer Sciences University of Rochester  
Note: Appears in: Proc. CVPR'97, pp.53-59, 1997.  
Abstract: Ray-based representations of shape have received little attention in computer vision. In this paper we show that the problem of recovering shape from silhouettes becomes considerably simplified if it is formulated as a reconstruction problem in the space of oriented rays that intersect the object. The method can be used with both calibrated and uncalibrated cameras, does not rely on point correspondences to compute shape, and does not impose restrictions on object topology or smoothness. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. C. Bolles, H. H. Baker, and D. H. Marimont. </author> <title> Epipolar plane image analysis: An approach to determining structure from motion. </title> <address> IJCV, 1:755, </address> <year> 1987. </year>
Reference-contexts: Our work combines elements from previous work on non-metric scene reconstruction [10], silhouette-based shape recovery [4] and epipolar plane image analysis <ref> [1] </ref>, and motivates the use of oriented projective geometry [8, 12] and convex duality theory [3, 12] for studying shape recovery in ray space.
Reference: [2] <author> R. Cipolla and A. Blake. </author> <title> Surface shape from the deformation of apparent contours. </title> <address> IJCV, 9(2):83112, </address> <year> 1992. </year>
Reference-contexts: Surface-based methods compute local surface shape (e.g., curvature) by establishing correspondences between curves on the silhouette across a small number of frames and by computing the envelope of optical rays through corresponding points [4]. While shape can be recovered accurately under certain conditions <ref> [2, 16] </ref>, surface-based methods pose several difficulties: * Tracking curves across frames: It is impossible to guarantee the validity of inter-frame curve correspondences even for simple shapes (Figure 1 (a)). <p> forced analytic methods to rely on few input images [11] and has motivated the use of voxel-based representations of space that limit reconstruction accuracy to the size of individual voxels [13]. * Recovering unexposed surface regions: Volumetric methods cannot recover the shape of regions that project to occluding contour curves <ref> [2] </ref> that are not part of the silhouette. As in surface-based methods, we recover shape by computing the envelope of families of rays (the light field boundary).
Reference: [3] <author> H. Edelsbrunner. </author> <title> Algorithms in Combinatorial Geometry. </title> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: Our work combines elements from previous work on non-metric scene reconstruction [10], silhouette-based shape recovery [4] and epipolar plane image analysis [1], and motivates the use of oriented projective geometry [8, 12] and convex duality theory <ref> [3, 12] </ref> for studying shape recovery in ray space. Our approach is based on the observation that a planar slice of a 3D object can be represented implicitly by the light field boundary, which is the manifold of rays that graze the slice. <p> We use results from the theory of convex duals to perform this step using a simple convex hull operation on T 2 . See [12] for an introduction to convex duality and <ref> [3] </ref> for applications in computational geometry. The theory of convex duals studies convex sets in R n and T n . <p> We establish segment correspondences in Step 1 by searching for overlapping background segments in consecutive images and by using sufficiently dense sequences. Step 2 is performed by computing the arrangement of N lines on the plane <ref> [3] </ref> using an implementation due to Goldwasser [5]. The complexity of this computation is O (N 2 ). 7.2.
Reference: [4] <author> P. Giblin and R. Weiss. </author> <title> Reconstruction of surfaces from profiles. </title> <booktitle> In Proc. 1st ICCV, </booktitle> <pages> pages 136144, </pages> <year> 1987. </year>
Reference-contexts: Our work combines elements from previous work on non-metric scene reconstruction [10], silhouette-based shape recovery <ref> [4] </ref> and epipolar plane image analysis [1], and motivates the use of oriented projective geometry [8, 12] and convex duality theory [3, 12] for studying shape recovery in ray space. <p> These methods fall in roughly two categories, surface-based and volumetric. Surface-based methods compute local surface shape (e.g., curvature) by establishing correspondences between curves on the silhouette across a small number of frames and by computing the envelope of optical rays through corresponding points <ref> [4] </ref>. While shape can be recovered accurately under certain conditions [2, 16], surface-based methods pose several difficulties: * Tracking curves across frames: It is impossible to guarantee the validity of inter-frame curve correspondences even for simple shapes (Figure 1 (a)). <p> We assume for simplicity that S contains a collection of convex connected regions, S 1 ; : : : ; S N which give rise to N distinct components in the visual hull. Every component of the visual hull is the envelope <ref> [4] </ref> of the family of rays that graze region S i and do not intersect the interior of S. Our goal is to compute this family for every connected component of the visual hull. We therefore need a way to map pixels in the image to rays on .
Reference: [5] <author> M. Goldwasser. </author> <title> An implementation for maintaining arrange ments of polygons. </title> <booktitle> In Proc. 11th Ann. Symp. on Computa tional Geometry, </booktitle> <pages> pages C32C33, </pages> <year> 1995. </year>
Reference-contexts: We establish segment correspondences in Step 1 by searching for overlapping background segments in consecutive images and by using sufficiently dense sequences. Step 2 is performed by computing the arrangement of N lines on the plane [3] using an implementation due to Goldwasser <ref> [5] </ref>. The complexity of this computation is O (N 2 ). 7.2. Group assignment Every object region S i separates the samples of B L (S) into two categories, those that belong to the convex hull of B L (S i ) and those that do not.
Reference: [6] <author> J. Illingworth and J. Kittler. </author> <title> A survey of the Hough transform. </title> <type> CVGIP, </type> <institution> 44:87116, </institution> <year> 1988. </year>
Reference-contexts: Very little attention has been paid in the computer vision literature to ray-based representations of shape. These representations have been studied exclusively in the context of the Hough transform <ref> [6, 15] </ref> and have been traditionally used for detecting shapes in images. Our purpose is to show that these representations become a powerful tool for recovering 3D shape because they describe objects in terms of quantities (optical rays) that can be extracted directly even from a single image.
Reference: [7] <author> A. Laurentini. </author> <title> The visual hull concept for silhouette-based image understanding. </title> <journal> IEEE T-PAMI, </journal> <volume> 16(2):150162, </volume> <year> 1994. </year>
Reference-contexts: For every viewpoint, the optical rays defining the silhouette bound an infinite region on that contains the slice S. The 2D visual hull of S is the intersection of these infinite regions for all viewpoints outside S's convex hull <ref> [7] </ref>. In general, the visual hull is a collection of convex supersets of the connected regions in S and represents the best approximation to S that can be obtained from silhouette images (Figure 1 (b)).
Reference: [8] <author> S. Laveau and O. Faugeras. </author> <title> Oriented projective geometry for computer vision. </title> <booktitle> In Proc. 4th ECCV, </booktitle> <pages> pages 147156. </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: Our work combines elements from previous work on non-metric scene reconstruction [10], silhouette-based shape recovery [4] and epipolar plane image analysis [1], and motivates the use of oriented projective geometry <ref> [8, 12] </ref> and convex duality theory [3, 12] for studying shape recovery in ray space. Our approach is based on the observation that a planar slice of a 3D object can be represented implicitly by the light field boundary, which is the manifold of rays that graze the slice.
Reference: [9] <author> M. Levoy and P. Hanrahan. </author> <title> Light field rendering. </title> <booktitle> In Proc. SIGGRAPH '96, </booktitle> <pages> pages 3142, </pages> <year> 1996. </year>
Reference-contexts: 1. Introduction There has been considerable interest recently in representing 3D objects in terms of the rays of light leaving their surface (e.g., the light field <ref> [9] </ref>). One common feature of these ray-based representations is that they contain sufficient information to synthesize arbitrary views of an object, yet they can be built from multiple images without computing point correspondences or 3D shape.
Reference: [10] <editor> J. L. Mundy and A. Zisserman, editors. </editor> <booktitle> Geometric Invari ance in Computer Vision. </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Our purpose is to show that these representations become a powerful tool for recovering 3D shape because they describe objects in terms of quantities (optical rays) that can be extracted directly even from a single image. Our work combines elements from previous work on non-metric scene reconstruction <ref> [10] </ref>, silhouette-based shape recovery [4] and epipolar plane image analysis [1], and motivates the use of oriented projective geometry [8, 12] and convex duality theory [3, 12] for studying shape recovery in ray space. <p> Reconstruction accuracy can therefore degrade as the density of the image sequence increases. * Using uncalibrated cameras: Surface-based methods have so far been formulated within a Euclidean framework and cannot be used when the camera is affinely- or projectively calibrated <ref> [10] </ref>. * Enforcing global shape constraints: Surface-based methods rely on local shape computations that cannot impose global constraints such as convexity. * Building global surface models: A post-processing step is needed to merge or reconcile local shape estimates [16]. <p> The mapping from pixels to rays depends on the camera model and on whether or not the camera is calibrated or uncalibrated. The specific choices do not affect our method. Here we assume that the camera is orthographic and uncalibrated <ref> [10] </ref>, and determine the pixel-to-ray mapping by tracking the projection of three points on . <p> If 1 ; 2 are vectors defined by the Pixel-to-Ray Mapping, 1 ^ 2 gives the affine coordinates of p with respect to the affine frame of p 1 ; p 2 ; p 3 <ref> [10] </ref>. We assume in the following that the projections of the affine basis points p 1 ; p 2 ; p 3 are known. 3.1. <p> The warped spatiotempo-ral image carves two empty areas on T 2 . The upper area is the convex dual of S (Section 5). (f) Top view of plane . from the Pixel-to-Ray Mapping and is general enough to represent rays with respect to Euclidean, affine, and projective reference frames <ref> [10] </ref>. The main elements of the representation are briefly described next. The Pixel-to-Ray Mapping assigns a vector to every ray through the silhouette.
Reference: [11] <author> P. Srinivasan, P. Liang, and S. Hackwood. </author> <title> Computational geometric methods in volumetric intersection from 3D re construction. </title> <journal> Pattern Recognition, </journal> <volume> 23(8):843857, </volume> <year> 1990. </year>
Reference-contexts: Even though volumetric reconstruction does not involve curve tracking and produces a global object description, current methods require calibrated cameras and raise two additional issues: * Intersecting volumes accurately & efficiently: The difficulty of computing volume intersections has forced analytic methods to rely on few input images <ref> [11] </ref> and has motivated the use of voxel-based representations of space that limit reconstruction accuracy to the size of individual voxels [13]. * Recovering unexposed surface regions: Volumetric methods cannot recover the shape of regions that project to occluding contour curves [2] that are not part of the silhouette.
Reference: [12] <author> J. Stolfi. </author> <title> Oriented Projective Geometry. </title> <publisher> Academic Press, </publisher> <year> 1991. </year>
Reference-contexts: Our work combines elements from previous work on non-metric scene reconstruction [10], silhouette-based shape recovery [4] and epipolar plane image analysis [1], and motivates the use of oriented projective geometry <ref> [8, 12] </ref> and convex duality theory [3, 12] for studying shape recovery in ray space. Our approach is based on the observation that a planar slice of a 3D object can be represented implicitly by the light field boundary, which is the manifold of rays that graze the slice. <p> Our work combines elements from previous work on non-metric scene reconstruction [10], silhouette-based shape recovery [4] and epipolar plane image analysis [1], and motivates the use of oriented projective geometry [8, 12] and convex duality theory <ref> [3, 12] </ref> for studying shape recovery in ray space. Our approach is based on the observation that a planar slice of a 3D object can be represented implicitly by the light field boundary, which is the manifold of rays that graze the slice. <p> Oriented Projective Ray Representation A key step in our shape recovery method is to map the optical rays that define the silhouette at a given viewpoint to points on the oriented projective sphere T 2 <ref> [12] </ref>. The resulting oriented projective ray representation plays a crucial role in our approach because, unlike other line representations commonly used in computer vision (e.g., the (r; ) representation [15]), it guarantees that the light field boundary is always a collection of finite, non-degenerate, and convex curves in ray space. <p> This vector is a signed homogeneous vector: vector k is equivalent to when k is a positive constant, and and are antipodal rays, i.e., rays of opposite orientation. The space T 2 of signed homogeneous vectors is an oriented projective space homeomorphic to the unit sphere <ref> [12] </ref>. Every homogeneous vector can be mapped uniquely onto the unit sphere via the mapping ! kk Together with the Pixel-to-Ray Mapping, Eq. (2) allows us to map pixels in the image to points on T 2 (Figure 2). <p> Convexity in T 2 extends the familiar notion of convexity in R 3 . In particular, is a convex set on T 2 if and only if the conical volume defined by and the origin of the unit sphere is convex in R 3 <ref> [12] </ref> (Figure 2 (c)). We exploit this definition in Section 6 to compute the convex hull of rays on T 2 . 4. The Light Field Boundary The visual hull of a slice S is completely determined by the set of rays that graze S. <p> We use results from the theory of convex duals to perform this step using a simple convex hull operation on T 2 . See <ref> [12] </ref> for an introduction to convex duality and [3] for applications in computational geometry. The theory of convex duals studies convex sets in R n and T n . <p> Theorem 1 shows that the convex duality mapping from sets of points to sets of rays preserves convexity, and Theorem 2 shows that the intersection of a set of half-planes can be expressed as a convex hull operation on T 2 <ref> [12] </ref>: Definition 3 (Convex dual) Let S be a convex region on the plane. The set of all rays whose left half-planes contain S is a set on T 2 and is called the convex dual, (S) fl , of S.
Reference: [13] <author> R. Szeliski. </author> <title> Rapid octree construction from image sequences. </title> <type> CVGIP: </type> <institution> IU, 58(1):2332, </institution> <year> 1993. </year>
Reference-contexts: Volumetric methods, on the other hand, incrementally refine a volumetric object representation by intersecting the volumes bounded by optical rays through each silhouette image <ref> [13] </ref>. <p> calibrated cameras and raise two additional issues: * Intersecting volumes accurately & efficiently: The difficulty of computing volume intersections has forced analytic methods to rely on few input images [11] and has motivated the use of voxel-based representations of space that limit reconstruction accuracy to the size of individual voxels <ref> [13] </ref>. * Recovering unexposed surface regions: Volumetric methods cannot recover the shape of regions that project to occluding contour curves [2] that are not part of the silhouette. As in surface-based methods, we recover shape by computing the envelope of families of rays (the light field boundary).
Reference: [14] <author> R. Vaillant and O. D. Faugeras. </author> <title> Using extremal bound aries for 3-d object modeling. </title> <journal> IEEE T-PAMI, </journal> <volume> 14(2):157173, </volume> <year> 1992. </year>
Reference-contexts: This inevitably leads to wrong reconstructions. * Detecting & handling degenerate cases: Shape computations are degenerate when the surface is flat or has creases. Even though these cases are difficult to detect in practice, accurate reconstruction relies on this ability <ref> [14, 17] </ref>. * Handling dense image sequences: Local shape cannot be computed when optical rays through corresponding curves are almost coincident.
Reference: [15] <author> M. Wright, A. Fitzgibbon, P. J. Giblin, and R. B. Fisher. </author> <title> Convex hulls, occluding contours, aspect graphs and the Hough transform. Image Vision Computing, </title> <address> 14:627634, </address> <year> 1996. </year>
Reference-contexts: Very little attention has been paid in the computer vision literature to ray-based representations of shape. These representations have been studied exclusively in the context of the Hough transform <ref> [6, 15] </ref> and have been traditionally used for detecting shapes in images. Our purpose is to show that these representations become a powerful tool for recovering 3D shape because they describe objects in terms of quantities (optical rays) that can be extracted directly even from a single image. <p> The resulting oriented projective ray representation plays a crucial role in our approach because, unlike other line representations commonly used in computer vision (e.g., the (r; ) representation <ref> [15] </ref>), it guarantees that the light field boundary is always a collection of finite, non-degenerate, and convex curves in ray space. The representation is derived directly (a) (b) (c) Rays on . <p> A fundamental property of convex sets on T 2 is that they cannot span a region greater than a hemisphere. Since is a convex set, the algorithm will never fail due to this condition. The second condition corresponds to the 1 Wright et al <ref> [15] </ref> recently suggested a convex hull algorithm based on the Hough transform. Unfortunately, the non-linearity of the (r; ) line representation used in their approach obscures the global structure of the Hough space.
Reference: [16] <author> C. Zhao and R. Mohr. </author> <title> Global three-dimensional surface reconstruction from occluding contours. </title> <journal> CVIU, </journal> <volume> 64(1):62 96, </volume> <year> 1996. </year>
Reference-contexts: Surface-based methods compute local surface shape (e.g., curvature) by establishing correspondences between curves on the silhouette across a small number of frames and by computing the envelope of optical rays through corresponding points [4]. While shape can be recovered accurately under certain conditions <ref> [2, 16] </ref>, surface-based methods pose several difficulties: * Tracking curves across frames: It is impossible to guarantee the validity of inter-frame curve correspondences even for simple shapes (Figure 1 (a)). <p> and cannot be used when the camera is affinely- or projectively calibrated [10]. * Enforcing global shape constraints: Surface-based methods rely on local shape computations that cannot impose global constraints such as convexity. * Building global surface models: A post-processing step is needed to merge or reconcile local shape estimates <ref> [16] </ref>. Volumetric methods, on the other hand, incrementally refine a volumetric object representation by intersecting the volumes bounded by optical rays through each silhouette image [13]. <p> Rather than relying on successive images to determine (a) not the rays through silhouette points q 1 and q 2 touch the same connected region. If q 1 ; q 2 are always matched, the reconstructed shape may erroneously merge two or more regions (right figure) <ref> [16] </ref>. (b) Geometry of the visual hull. The visual hull's boundary is indicated by a thick solid line. Also shown are rays grazing the slice S.
Reference: [17] <author> J. Y. Zheng. </author> <title> Acquiring 3-D models from sequences of con tours. </title> <journal> IEEE T-PAMI, </journal> <volume> 16(2):163178, </volume> <year> 1994. </year>
Reference-contexts: This inevitably leads to wrong reconstructions. * Detecting & handling degenerate cases: Shape computations are degenerate when the surface is flat or has creases. Even though these cases are difficult to detect in practice, accurate reconstruction relies on this ability <ref> [14, 17] </ref>. * Handling dense image sequences: Local shape cannot be computed when optical rays through corresponding curves are almost coincident.
References-found: 17

