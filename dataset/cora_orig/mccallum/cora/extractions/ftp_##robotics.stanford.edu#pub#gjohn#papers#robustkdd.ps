URL: ftp://robotics.stanford.edu/pub/gjohn/papers/robustkdd.ps
Refering-URL: http://www.robotics.stanford.edu/~gjohn/pubs.html
Root-URL: http://www.robotics.stanford.edu
Email: gjohn@CS.Stanford.EDU  
Title: Robust Decision Trees: Removing Outliers from Databases  
Author: George H. John 
Note: In U. M. Fayyad and R. Uthurusamy, editors, Proceedings of the First International Conference on Knowledge Discovery and Data Mining, pages 174-179, AAAI Press,  
Web: http://robotics.stanford.edu/~gjohn/  
Address: Stanford, CA 94305  Menlo Park, CA, 1995.  
Affiliation: Computer Science Dept. Stanford University  
Abstract: Finding and removing outliers is an important problem in data mining. Errors in large databases can be extremely common, so an important property of a data mining algorithm is robustness with respect to errors in the database. Most sophisticated methods in machine learning address this problem to some extent, but not fully, and can be improved by addressing the problem more directly. In this paper we examine C4.5, a decision tree algorithm that is already quite robust few algorithms have been shown to consistently achieve higher accuracy. C4.5 incorporates a pruning scheme that partially addresses the outlier removal problem. In our Robust-C4.5 algorithm we extend the pruning method to fully remove the effect of outliers, and this results in improvement on many databases. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1991), </year> <title> "Instance-based learning algorithms", </title> <booktitle> Machine Learning 6(1), </booktitle> <pages> 37-66. </pages>
Reference-contexts: The selection of data for training is an ubiquitous problem in KDD. Many methods are passive, accepting data from a training sample in some random order or all at once as a set. Other methods <ref> (Aha 1991) </ref> actively accept or reject training records from a temporal sequence presented to them.
Reference: <author> Barnett, V. & Lewis, T. </author> <year> (1993), </year> <title> Outliers in Statistical Data, third edition, </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: data, Daniel Bernoulli wrote "I see no way of drawing a dividing line between those that are to be utterly rejected and those that are to be wholly retained; it may even happen that the rejected observation is the one that would have supplied the best correction to the others..." <ref> (Barnett & Lewis 1993, p. 27) </ref>. In statistics, an outlier is defined as a "case that does not follow the same model as the rest of the data" (Weisberg 1985). This is broad, including not only erroneous data but also "surprising" veridical data.
Reference: <author> Breiman, L., Friedman, J., Olshen, R. & Stone, C. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Chapman & Hall, </publisher> <address> New York. </address>
Reference-contexts: There are also statistical arguments phrasing this as the bias-variance problem <ref> (Breiman, Friedman, Olshen & Stone 1984, Geman, Bienenstock & Doursat 1992) </ref>, but we will not pursue this further. <p> In general, unless time is a very limited resource, the best results are achieved by starting "big" and then shrinking <ref> (Breiman et al. 1984) </ref>. This should apply to pattern selection as well, and thus we suspect backward pattern selection (i.e., outlier removal) will give greater performance than forward selection.
Reference: <author> Cohn, D. </author> <year> (1994), </year> <title> "Improving generalization with active learning", </title> <booktitle> Machine Learning 15(2), </booktitle> <pages> 201-221. </pages>
Reference: <author> Geman, S., Bienenstock, E. & Doursat, R. </author> <year> (1992), </year> <title> "Neural networks and the bias/variance dilemma", </title> <booktitle> Neural Computation 4, </booktitle> <pages> 1-58. </pages>
Reference: <author> Guyon, I., Boser, B. & Vapnik, V. </author> <year> (1993), </year> <title> Automatic capacity tuning of very large VC-dimension classifiers, </title> <editor> in S. J. Hanson, J. Cowan & C. L. Giles, eds, </editor> <booktitle> "Advances in Neural Information Processing Systems", </booktitle> <volume> Vol. 5, </volume> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 147-154. </pages>
Reference: <author> Hastie, T. </author> <year> (1994), </year> <type> Personal Communication. </type>
Reference: <author> Hastie, T. J. & Tibshirani, R. J. </author> <year> (1990), </year> <title> Generalized Additive Models, </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: This makes explicit the assumption that locally un-informative or harmful records are globally un-informative as well. The regularization algorithm continues pruning and retraining until no further pruning can be done. Though common in regression in the guise of robust (Huber 1977) or resistant <ref> (Hastie & Tibshirani 1990, Chapter 9) </ref> fitting, in the context of classification this appears to be novel.
Reference: <author> Hirsh, H. & Cohen, W., </author> <booktitle> eds (1994), Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Huber, P. </author> <year> (1981), </year> <title> Robust Statistics, </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Huber, P. J. </author> <year> (1977), </year> <title> Robust Statistical Procedures, </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: This makes explicit the assumption that locally un-informative or harmful records are globally un-informative as well. The regularization algorithm continues pruning and retraining until no further pruning can be done. Though common in regression in the guise of robust <ref> (Huber 1977) </ref> or resistant (Hastie & Tibshirani 1990, Chapter 9) fitting, in the context of classification this appears to be novel.
Reference: <author> John, G. H. </author> <year> (1995), </year> <title> Robust linear discriminant trees, </title> <booktitle> in "Fifth International Workshop on Artificial Intelligence and Statistics", </booktitle> <address> Ft. Lauderdale, FL, </address> <pages> pp. 285-291. </pages>
Reference-contexts: Thus the superior performance of Robust-C4.5 does not seem to be due solely to under-pruning on C4.5's part. Related Work Robust-C4.5 is based on our earlier work in decision trees <ref> (John 1995) </ref>, which also rejected misclassified records and retrained.
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <booktitle> in Hirsh & Cohen (1994), </booktitle> <pages> pp. 121-129. </pages>
Reference-contexts: Other methods (Aha 1991) actively accept or reject training records from a temporal sequence presented to them. The difference between outlier removal and active learning (as commonly presented) is somewhat akin to the difference between forward and backward feature subset selection <ref> (John, Kohavi & Pfleger 1994) </ref> or forward and backward (construction/pruning) search methods over neural net or decision tree architectures. In general, unless time is a very limited resource, the best results are achieved by starting "big" and then shrinking (Breiman et al. 1984).
Reference: <author> Michie, D., Spiegelhalter, D. J. & Taylor, C. C. </author> <year> (1994), </year> <title> Machine Learning, Neural and Statistical Classification, </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: Below we first describe the experimental methodology and then present and discuss the results. Method To test the hypotheses, we compared Robust-C4.5 and C4.5 on 21 databases selected from the UCI and Statlog database repositories <ref> (Murphy & Aha 1994, Michie, Spiegelhalter & Taylor 1994) </ref>. The databases are all stored and processed as a single relation (table). The databases were selected with no particular agenda in mind other than convenience. To compare Robust-C4.5 and C4.5 on a given database we used ten-fold cross-validation.
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1994), </year> <note> "UCI repository of machine learning databases", Available by anonymous ftp to ics.uci.edu in the pub/machine-learning-databases directory. </note>
Reference-contexts: Below we first describe the experimental methodology and then present and discuss the results. Method To test the hypotheses, we compared Robust-C4.5 and C4.5 on 21 databases selected from the UCI and Statlog database repositories <ref> (Murphy & Aha 1994, Michie, Spiegelhalter & Taylor 1994) </ref>. The databases are all stored and processed as a single relation (table). The databases were selected with no particular agenda in mind other than convenience. To compare Robust-C4.5 and C4.5 on a given database we used ten-fold cross-validation.
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The only idea that seems to be easily portable from linear regression to classification trees (or decision trees) is the idea of leverage we will next explore this idea in the context of C4.5. C4.5 and Pruning: Model Selection C4.5 <ref> (Quinlan 1993) </ref> is a decision tree induction program. A decision tree predicts unknown field values by asking a series of yes/no or multiple-outcome questions.
Reference: <author> Rousseeuw, P. J. & Leroy, A. M. </author> <year> (1987), </year> <title> Robust Regression and Outlier Detection, </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference: <author> Ullman, J. D. </author> <year> (1988), </year> <title> Principles of Database and Knowledge-Base Systems: Volume 1: Classical Database Systems, </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Introduction As Knowledge Discovery in Databases (KDD) becomes more common in practice, users will apply KDD methods and algorithms to more and larger databases. A well-known property of large databases is the frequency of errors. Although theory and practice of database management has advanced rapidly over the past two decades <ref> (Ullman 1988) </ref>, in the end the data stored in a DBMS is no better than the typing skill of the data-entry clerk and the decisions made by experts that are recorded in databases.
Reference: <author> Utgoff, P. </author> <year> (1994), </year> <title> An improved algorithm for incremental induction of decision trees, </title> <booktitle> in Hirsh & Cohen (1994), </booktitle> <pages> pp. 318-326. </pages>
Reference: <author> Venables, W. N. & Ripley, B. D. </author> <year> (1994), </year> <title> Modern Applied Statistics with S-Plus, </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference: <author> Weisberg, S. </author> <year> (1985), </year> <title> Applied Linear Regression, </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: In statistics, an outlier is defined as a "case that does not follow the same model as the rest of the data" <ref> (Weisberg 1985) </ref>. This is broad, including not only erroneous data but also "surprising" veridical data. Most of the work addressing outliers in statistics assumes either a simple estimation task, such as estimating the mean and covariance matrix of a normal distribution, or a linear regression.
References-found: 21

