URL: ftp://ftp.comlab.ox.ac.uk/pub/Packages/ILP/Papers/indlog.ps
Refering-URL: http://gruffle.comlab.ox.ac.uk/oucl/groups/machlearn/mlg_pub.html
Root-URL: 
Title: Inductive Logic Programming  
Author: Stephen MUGGLETON 
Keyword: Learning, logic programming, induction, predicate invention, inverse resolution, information compression  
Address: 36 North Hanover St., Glasgow G1 2AD, United Kingdom.  
Affiliation: The Turing Institute,  
Abstract: A new research area, Inductive Logic Programming, is presently emerging. While inheriting various positive characteristics of the parent subjects of Logic Programming and Machine Learning, it is hoped that the new area will overcome many of the limitations of its forebears. The background to present developments within this area is discussed and various goals and aspirations for the increasing body of researchers are identified. Inductive Logic Programming needs to be based on sound principles from both Logic and Statistics. On the side of statistical justification of hypotheses we discuss the possible relationship between Algorithmic Complexity theory and Probably-Approximately-Correct (PAC) Learning. In terms of logic we provide a unifying framework for Muggleton and Buntine's Inverse Resolution (IR) and Plotkin's Relative Least General Generali-sation (RLGG) by rederiving RLGG in terms of IR. This leads to a discussion of the feasibility of extending the RLGG framework to allow for the invention of new predicates, previously discussed only within the context of IR. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.B. Banerji. </author> <title> Learning in the limit in a growing language. </title> <booktitle> In IJCAI-87, </booktitle> <pages> pages 280-282, </pages> <address> Los Angeles, CA, 1987. </address> <publisher> Kaufmann. </publisher>
Reference-contexts: Since background knowledge is rarely complete in applications, this constraint is now generally believed to be over-restrictive. * Strong bias of vocabulary. Present inductive systems construct hypotheses within the limits of a fixed vocabulary of propositional attributes. An increasing amount of Machine Learning research <ref> [24, 28, 1, 44, 35, 16, 19] </ref> is concerned with algorithms capable of inventing auxiliary predicates when insufficient background knowledge is provided. 2 Inductive Logic Programming A growing body of researchers have started to work on problems of inductive reasoning within the confines of pure Prolog. <p> A set of more general constraints were derived, and two related operations were named the `V' operators (see Section 4.1). Another pair of operators (`W'), also based on inverting resolution, were shown to be capable of "inventing" predicates which were not available within the background knowledge (Section 5.1). Banerji <ref> [1] </ref>, Wirth [44] Ishizaka [16], Ling and Dawes [19] and Rouveirol and Puget [35] have also described related methods for "inventing" new predicates.
Reference: [2] <author> C. Bennett. </author> <title> Logical depth and physical complexity. </title> <editor> In R. Herken, editor, </editor> <title> The Universal Turing Machine A Half Century Survey, </title> <type> pages 227-257. </type> <institution> Kammerer and Unverzagt, Hamburg, </institution> <year> 1988. </year>
Reference-contexts: Secondly, it is relatively straightforward to interpret textual compression within any formalism, be it propositional logic, first-order logic or context-free grammar rules. Thirdly, this would bring the PAC-learning model into line with other approaches developed from algorithmic complexity theory <ref> [40, 6, 2] </ref> and Bayesian statistics.
Reference: [3] <author> I. Bratko. </author> <title> Prolog for artificial intelligence. </title> <publisher> Addison-Wesley, </publisher> <address> London, </address> <year> 1986. </year>
Reference-contexts: Prolog statements are phrased in a restricted clausal form called Horn clause logic. All computations within Prolog take the form of logical proofs based on the application of the rule of resolution. Since its inception Prolog has developed into a widely used programming language <ref> [3, 41] </ref> and was chosen as the core language for Japan's Fifth Generation Project. Prolog's development has also spawned the rigorous theoretical school of Logic Programming [20]. 1.2 Induction Despite the self-evident success of logical deduction, a certain question has cropped up time and again throughout its development.
Reference: [4] <author> W. Buntine. </author> <title> Generalised subsumption and its applications to induction and redundancy. </title> <journal> Artificial Intelligence, </journal> <volume> 36(2) </volume> <pages> 149-176, </pages> <year> 1988. </year>
Reference-contexts: Quinlan sees his approach as being a natural extension of ID3 [32]. He notes that the search can be highly myopic, and is unable to learn predicates such as list-reversal and integer-multiplication. Attempts have been made recently by Buntine <ref> [4] </ref>, Frisch and Page [11] and Muggleton and Feng [29] to find ways around Plotkin's negative RLGG results. Domain Clause No. of No. of atoms examples in background Qualitative state (la:A/B,lb:C/D,fab:E/B,fba:F/D) 169 5408 modelling deriv (la:A/B,fba:F/D), (U-tube) deriv (lb:C/D,fab:E/B), minus (la:A/B,lb:G/D,[]), minus (la:G/B,lb:C/D,[]). <p> Maher in [21] discusses a number of different notions of equivalence for logic programs including that of Definition 3. See also Buntine <ref> [4] </ref> and Niblett [30] for a detailed discussion of generality. 3.2 A general setting for induction In the general inductive setting we are provided with three languages.
Reference: [5] <author> R. Carnap. </author> <title> The Continuum of Inductive Methods. </title> <institution> Chicago University, Chicago, </institution> <year> 1952. </year>
Reference-contexts: Turing [43] later came to believe that Godel's incompleteness theorem required that intelligent machines be capable of learning from examples. Various logical positivists including Carnap <ref> [5] </ref> have developed statistical theories for confirming scientific hypotheses posed in first order logic. <p> In the worst case there could be an infinite number of contending hypotheses all of which fit the relationship shown above. For this reason we require some additional non-logical constraint to justify any particular hypothesis. In the 1950's Carnap <ref> [5] </ref> and others suggested "confirmation theories" aimed at providing a statistical underpinning to the problem of inductive inference.
Reference: [6] <author> G. Chaitin. </author> <title> Information, Randomness and Incompleteness Papers on Algorithmic Information Theory. </title> <publisher> World Scientific Press, </publisher> <address> Singapore, </address> <year> 1987. </year>
Reference-contexts: Secondly, it is relatively straightforward to interpret textual compression within any formalism, be it propositional logic, first-order logic or context-free grammar rules. Thirdly, this would bring the PAC-learning model into line with other approaches developed from algorithmic complexity theory <ref> [40, 6, 2] </ref> and Bayesian statistics.
Reference: [7] <author> Dietterich. </author> <title> Limitations of inductive learning. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 124-128, </pages> <address> San Mateo, CA, 1989. </address> <publisher> Morgan-Kaufmann. </publisher>
Reference-contexts: This has been shown to be possible when various constraints are placed on the hypothesis language. For instance, one such constraint involves placing a constant bound k on the allowable size of conjunctions within a boolean DNF concept description (called k-DNF). In a recent paper Dietterich <ref> [7] </ref> showed that for the purposes of learning DNF propositional descriptions the class of PAC-learnable concepts is highly restricted. There are 2 2 m different boolean functions of m input values. <p> There are 2 2 m different boolean functions of m input values. However, Dietterich shows on the basis of general Valiant-learnability results due to Ehrenfeucht, Haussler et al [8] that given n examples at most O (2 nm ) of these functions are PAC-learnable. Dietterich <ref> [7] </ref> expresses concern over these results since it would seem that inductive learning algorithms must be restricted to searching highly constrained hypothesis spaces.
Reference: [8] <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <booktitle> In COLT 88: Proceedings of the Conference on Learning, </booktitle> <pages> pages 110-120, </pages> <address> Los Altos, CA, 1988. </address> <publisher> Morgan-Kaufmann. </publisher>
Reference-contexts: In line with its parent subjects it would seem likely that the area will subdivide into three related strands of research: theory, implementation and experimental application. While model-theoretic semantics [20] and PAC-learning <ref> [8] </ref> theory are likely to be influential, neither seem totally adequate for the problems involved. The general goals of research for such an area should be to produce a widely used technology with a firm theoretical foundation. <p> There are 2 2 m different boolean functions of m input values. However, Dietterich shows on the basis of general Valiant-learnability results due to Ehrenfeucht, Haussler et al <ref> [8] </ref> that given n examples at most O (2 nm ) of these functions are PAC-learnable. Dietterich [7] expresses concern over these results since it would seem that inductive learning algorithms must be restricted to searching highly constrained hypothesis spaces.
Reference: [9] <editor> S. Feferman et al., editor. Godel's Collected Works. </editor> <publisher> Oxford University Press, Oxford, </publisher> <year> 1980. </year>
Reference-contexts: According to this view, every mathematical statement can be phrased within the logical language of first order predicate calculus and all valid scientific reasoning is based on logical derivation from a set of pre-conceived axioms. Logical positivism was provided with powerful ammunition by Godel's demonstration <ref> [9] </ref> that a small collection of sound rules of inference was complete for deriving all consequences of formulae in first order predicate calculus. Much later Robinson demonstrated [34] that a single rule of inference, called resolution, is both sound and complete for proving statements within this calculus (see Appendix A).
Reference: [10] <author> M.S. Fox and J. McDermott. </author> <title> The role of databases in knowledge-based systems. </title> <type> Technical Report CMU-RI-TR-86-3, </type> <institution> Carnegie-Mellon University, Robotics Institute, </institution> <address> Pittsburgh, PA, </address> <year> 1986. </year>
Reference-contexts: The major successes here have been in the area of inductive construction of expert systems (see [27]). The properties of various expert systems are given in Figure 1. The first two, MYCIN [38] and XCON <ref> [10] </ref> were built using hand-coding of rules. The second two, GASOIL [39] and BMT [15] were built using software derived from Quinlan's inductive decision tree building algorithm ID3 [32]. It should be noted that the inductively constructed BMT is by far the largest expert system in full-time commercial use.
Reference: [11] <author> A.M. </author> <title> Frisch and C.D. Page. On inductive generalisation with taxonomic background knowledge. </title> <type> Technical report, </type> <institution> University of Illinois, Urbana, </institution> <year> 1989. </year>
Reference-contexts: Quinlan sees his approach as being a natural extension of ID3 [32]. He notes that the search can be highly myopic, and is unable to learn predicates such as list-reversal and integer-multiplication. Attempts have been made recently by Buntine [4], Frisch and Page <ref> [11] </ref> and Muggleton and Feng [29] to find ways around Plotkin's negative RLGG results. Domain Clause No. of No. of atoms examples in background Qualitative state (la:A/B,lb:C/D,fab:E/B,fba:F/D) 169 5408 modelling deriv (la:A/B,fba:F/D), (U-tube) deriv (lb:C/D,fab:E/B), minus (la:A/B,lb:G/D,[]), minus (la:G/B,lb:C/D,[]).
Reference: [12] <author> K. </author> <title> Godel. Uber formal unentscheidbare Satze der Principia Mathematica und verwandter System I. </title> <journal> Monats. Math. Phys., </journal> <volume> 32 </volume> <pages> 173-198, </pages> <year> 1931. </year>
Reference-contexts: In turn, Statistics went on to have a central role in the evaluation of scientific hypotheses. In 1931, a year after Godel's previously mentioned completeness result, Godel <ref> [12] </ref> published his more famous incompleteness theorem.
Reference: [13] <author> E.M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: Given a Prolog program which is either incomplete, incorrect or non-terminating, Shapiro's system diagnoses which clause is faulty and then replaces that clause. Both Plotkin and Shapiro managed to prove that their learning systems could identify first order theories in the limit, according to Gold's definition <ref> [13] </ref>. Nowadays Valiant's definition of PAC-learning [14] (see Section 3.4) is generally agreed to provide a better approach to identification than Gold's methodology.
Reference: [14] <author> D. Haussler. </author> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial intelligence, </journal> <volume> 36:177 - 221, </volume> <year> 1988. </year>
Reference-contexts: Both Plotkin and Shapiro managed to prove that their learning systems could identify first order theories in the limit, according to Gold's definition [13]. Nowadays Valiant's definition of PAC-learning <ref> [14] </ref> (see Section 3.4) is generally agreed to provide a better approach to identification than Gold's methodology. However, PAC methods have largely been applied to propositional-level learning and have as yet to be demonstrated capable of dealing with inductive learning in full first order logic. <p> Various difficulties and paradoxes were encountered with these approaches which meant that they were never applied within machine learning programs [23]. 3.4 PAC-learning One popular machine learning approach to the problem of constructing highly probable hypotheses is the PAC (Probably Approximately Correct) model of learning proposed by Valiant <ref> [14] </ref>. According to Valiant's model a learning agent is not concerned with constructing an exact concept definition. Instead we choose a class of hypotheses that we would like to be able to deal with. We are then given a set of examples of the target concept.
Reference: [15] <author> J. Hayes-Michie. </author> <title> News from Brainware. </title> <journal> Pragmatica, </journal> <volume> 1 </volume> <pages> 10-11, </pages> <year> 1990. </year>
Reference-contexts: The major successes here have been in the area of inductive construction of expert systems (see [27]). The properties of various expert systems are given in Figure 1. The first two, MYCIN [38] and XCON [10] were built using hand-coding of rules. The second two, GASOIL [39] and BMT <ref> [15] </ref> were built using software derived from Quinlan's inductive decision tree building algorithm ID3 [32]. It should be noted that the inductively constructed BMT is by far the largest expert system in full-time commercial use.
Reference: [16] <author> H. Ishizaka. </author> <title> Learning simple deterministic languages. </title> <booktitle> In Computational learning theory: proceedings of the second annual workshop, </booktitle> <address> San Mateo, CA, 1989. </address> <publisher> Kaufmann. </publisher>
Reference-contexts: Since background knowledge is rarely complete in applications, this constraint is now generally believed to be over-restrictive. * Strong bias of vocabulary. Present inductive systems construct hypotheses within the limits of a fixed vocabulary of propositional attributes. An increasing amount of Machine Learning research <ref> [24, 28, 1, 44, 35, 16, 19] </ref> is concerned with algorithms capable of inventing auxiliary predicates when insufficient background knowledge is provided. 2 Inductive Logic Programming A growing body of researchers have started to work on problems of inductive reasoning within the confines of pure Prolog. <p> Another pair of operators (`W'), also based on inverting resolution, were shown to be capable of "inventing" predicates which were not available within the background knowledge (Section 5.1). Banerji [1], Wirth [44] Ishizaka <ref> [16] </ref>, Ling and Dawes [19] and Rouveirol and Puget [35] have also described related methods for "inventing" new predicates. However, although predicate invention has been demonstrated on large scale problems within a propositional setting [24, 25] this is not yet the case for any first order learning systems.
Reference: [17] <author> R.A. Kowalski. </author> <title> Logic for Problem Solving. </title> <publisher> North Holland, </publisher> <year> 1980. </year>
Reference-contexts: For the purposes of applying resolution, formulae are normalised to what is known as clausal form. Robinson's discovery was of fundamental importance to the application of logic within computer science. Throughout the early 1970's Colmerauer and Kowalski <ref> [17] </ref> were instrumental in the development of the logic based programming language Prolog. Prolog statements are phrased in a restricted clausal form called Horn clause logic. All computations within Prolog take the form of logical proofs based on the application of the rule of resolution.
Reference: [18] <author> B. Kuipers. </author> <title> Qualitative simulation. </title> <journal> Artificial Intelligence, </journal> <volume> 29 </volume> <pages> 289-338, </pages> <year> 1986. </year>
Reference-contexts: Figure 2 shows various clauses developed as part of ongoing real-world application projects using Muggleton and Feng's [29] Golem. The first clause represents a qualitative model of a U-tube, developed in a collaborative project between the author and Ivan Bratko. The clause was built using Kuipers' <ref> [18] </ref> theory of qualitative reasoning as background knowledge, encoded as 5408 ground atoms. The clause relates the levels of the two water surfaces, la and lb to the directions of flow fab and fba.
Reference: [19] <author> X. Ling and M. Dawes. </author> <title> Theory reduction with uncertainty: A reason for theoretical terms. </title> <type> Technical Report 271, </type> <institution> University of Western Ontario, </institution> <year> 1990. </year>
Reference-contexts: Since background knowledge is rarely complete in applications, this constraint is now generally believed to be over-restrictive. * Strong bias of vocabulary. Present inductive systems construct hypotheses within the limits of a fixed vocabulary of propositional attributes. An increasing amount of Machine Learning research <ref> [24, 28, 1, 44, 35, 16, 19] </ref> is concerned with algorithms capable of inventing auxiliary predicates when insufficient background knowledge is provided. 2 Inductive Logic Programming A growing body of researchers have started to work on problems of inductive reasoning within the confines of pure Prolog. <p> Another pair of operators (`W'), also based on inverting resolution, were shown to be capable of "inventing" predicates which were not available within the background knowledge (Section 5.1). Banerji [1], Wirth [44] Ishizaka [16], Ling and Dawes <ref> [19] </ref> and Rouveirol and Puget [35] have also described related methods for "inventing" new predicates. However, although predicate invention has been demonstrated on large scale problems within a propositional setting [24, 25] this is not yet the case for any first order learning systems.
Reference: [20] <author> J.W. Lloyd. </author> <title> Foundations of Logic Programming. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: Since its inception Prolog has developed into a widely used programming language [3, 41] and was chosen as the core language for Japan's Fifth Generation Project. Prolog's development has also spawned the rigorous theoretical school of Logic Programming <ref> [20] </ref>. 1.2 Induction Despite the self-evident success of logical deduction, a certain question has cropped up time and again throughout its development. <p> In line with its parent subjects it would seem likely that the area will subdivide into three related strands of research: theory, implementation and experimental application. While model-theoretic semantics <ref> [20] </ref> and PAC-learning [8] theory are likely to be influential, neither seem totally adequate for the problems involved. The general goals of research for such an area should be to produce a widely used technology with a firm theoretical foundation.
Reference: [21] <author> M.J. Maher. </author> <title> Equivalences of logic programs. </title> <booktitle> In Proceedings of Third International Conference on Logic Programming, </booktitle> <address> Berlin, 1986. </address> <publisher> Springer. </publisher>
Reference-contexts: Also Definition 5 Literal l is logically redundant within the clause C _ l in the logic program P ^ (C _ l) if and only if P ^ (C _ l) is logically equivalent to P ^ C. Maher in <ref> [21] </ref> discusses a number of different notions of equivalence for logic programs including that of Definition 3. See also Buntine [4] and Niblett [30] for a detailed discussion of generality. 3.2 A general setting for induction In the general inductive setting we are provided with three languages.
Reference: [22] <author> T.M. Mitchell, R.M. Keller, and S.T. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: Human inductive reasoners make use of vast amounts of background knowledge when learning. Inductive algorithms such as ID3 use only a fixed set of attributes attached to each example. Explanation-based learning (EBL) <ref> [22] </ref> attempted to overcome this limitation by redefining the learning problem. In EBL hypotheses are constrained to being those derivable from background knowledge. Since background knowledge is rarely complete in applications, this constraint is now generally believed to be over-restrictive. * Strong bias of vocabulary.
Reference: [23] <author> H. Mortimer. </author> <title> The Logic of Induction. </title> <publisher> Ellis Horwood, </publisher> <address> Chichester, England, </address> <year> 1988. </year>
Reference-contexts: In the 1950's Carnap [5] and others suggested "confirmation theories" aimed at providing a statistical underpinning to the problem of inductive inference. Various difficulties and paradoxes were encountered with these approaches which meant that they were never applied within machine learning programs <ref> [23] </ref>. 3.4 PAC-learning One popular machine learning approach to the problem of constructing highly probable hypotheses is the PAC (Probably Approximately Correct) model of learning proposed by Valiant [14]. According to Valiant's model a learning agent is not concerned with constructing an exact concept definition.
Reference: [24] <author> S.H. Muggleton. Duce, </author> <title> an oracle based approach to constructive induction. </title> <booktitle> In IJCAI-87, </booktitle> <pages> pages 287-292. </pages> <publisher> Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: Since background knowledge is rarely complete in applications, this constraint is now generally believed to be over-restrictive. * Strong bias of vocabulary. Present inductive systems construct hypotheses within the limits of a fixed vocabulary of propositional attributes. An increasing amount of Machine Learning research <ref> [24, 28, 1, 44, 35, 16, 19] </ref> is concerned with algorithms capable of inventing auxiliary predicates when insufficient background knowledge is provided. 2 Inductive Logic Programming A growing body of researchers have started to work on problems of inductive reasoning within the confines of pure Prolog. <p> Banerji [1], Wirth [44] Ishizaka [16], Ling and Dawes [19] and Rouveirol and Puget [35] have also described related methods for "inventing" new predicates. However, although predicate invention has been demonstrated on large scale problems within a propositional setting <ref> [24, 25] </ref> this is not yet the case for any first order learning systems. Recently Quinlan [33] has described a highly efficient program, called FOIL, which induces first order Horn clauses.
Reference: [25] <author> S.H. Muggleton. </author> <title> Inverting the resolution principle. In Machine Intellience 12 (in press). </title> <publisher> Oxford University Press, </publisher> <year> 1988. </year>
Reference-contexts: Banerji [1], Wirth [44] Ishizaka [16], Ling and Dawes [19] and Rouveirol and Puget [35] have also described related methods for "inventing" new predicates. However, although predicate invention has been demonstrated on large scale problems within a propositional setting <ref> [24, 25] </ref> this is not yet the case for any first order learning systems. Recently Quinlan [33] has described a highly efficient program, called FOIL, which induces first order Horn clauses.
Reference: [26] <author> S.H. Muggleton. </author> <title> A strategy for constructing new predicates in first order logic. </title> <booktitle> In Proceedings of the Third European Working Session on Learning, </booktitle> <pages> pages 123-130. </pages> <publisher> Pitman, </publisher> <year> 1988. </year>
Reference-contexts: H can be constrained to be the most general hypothesis relative B as in Shapiro [37] or the least general relative to B as in Plotkin [31]. Alternatively, H can be chosen to be that which produces the maximum information compression of O relative to B as in <ref> [26] </ref>. The choice of which constraint to apply is necessarily tied to our notions of justification of hypotheses. 3.3 Inductive inference and justification As already stated, inductive inference involves the use of background knowledge to construct an hypothesis which agrees with some set of observations according to relationship (1). <p> The natural interpretation that we choose here is that I (OjH) is the minimal encoding of the derivation or proof of O from H. This interpretation has been shown to give good results elsewhere <ref> [26] </ref>.
Reference: [27] <author> S.H. Muggleton. </author> <title> Inductive Acquisition of Expert Knowledge. </title> <address> Addision-Wesley, Wokingham, England, </address> <year> 1990. </year>
Reference-contexts: The major successes here have been in the area of inductive construction of expert systems (see <ref> [27] </ref>). The properties of various expert systems are given in Figure 1. The first two, MYCIN [38] and XCON [10] were built using hand-coding of rules. The second two, GASOIL [39] and BMT [15] were built using software derived from Quinlan's inductive decision tree building algorithm ID3 [32].
Reference: [28] <author> S.H. Muggleton and W. Buntine. </author> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 339-352. </pages> <publisher> Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Since background knowledge is rarely complete in applications, this constraint is now generally believed to be over-restrictive. * Strong bias of vocabulary. Present inductive systems construct hypotheses within the limits of a fixed vocabulary of propositional attributes. An increasing amount of Machine Learning research <ref> [24, 28, 1, 44, 35, 16, 19] </ref> is concerned with algorithms capable of inventing auxiliary predicates when insufficient background knowledge is provided. 2 Inductive Logic Programming A growing body of researchers have started to work on problems of inductive reasoning within the confines of pure Prolog. <p> The matching facts B are replaced by H within F , where is a ground substitution. This was the sole generalisation operation used. The method was demonstrated on a wide set of problems. Muggleton and Buntine <ref> [28] </ref> later showed that Sammut and Banerji's generalisation operation was a special case of inverting a step in a resolution proof. A set of more general constraints were derived, and two related operations were named the `V' operators (see Section 4.1). <p> Usually, L O is required to contain only ground literals. This still leaves a very unconstrained choice for H. Often H is restricted to being a single clause. This is the case for Plotkin [31] and Shapiro [37] though not the case in Muggleton and Buntine <ref> [28] </ref> in which predicate invention introduces a set of clauses. H can be constrained to be the most general hypothesis relative B as in Shapiro [37] or the least general relative to B as in Plotkin [31]. <p> which require very long proofs to derive the examples on which they are based, thus nicely combining the notions of space and time complexity of hypotheses in terms of a single information measure. 4 The relationship between IR and RLGG In this section we review and extend Muggleton and Buntine's <ref> [28] </ref> Inverse Resolution (IR) techniques leading to a unified framework for IR and Plotkin's RLGG. 4.1 The `V' operators the clause at the base of the `V' given the two clauses on the arms. <p> The absorption operator constructs C 2 given C 1 and C. Conversely, the construction of C 1 from C 2 and C is called the identification operator. Together these operators are called the `V' operators. In <ref> [28] </ref> the `V' operators are derived as a set of constraints from the following equation of resolution (see section A.3). <p> Muggleton and Buntine <ref> [28] </ref> base the algebraic manipulation required to produce constraint equations for the `V' operators on a number of assumptions. Among these is the assumption that the clauses (C 1 fl 1 g) 1 and (C 2 fl 2 g) 2 contain no common literals. This they call the separability assumption. <p> Plotkin [31] showed that rlgg P (C; D) can be infinite even when logically reduced. Note however that whereas Plotkin intended the reduced RLGG to be the hypothesis, within Muggleton and Buntine's <ref> [28] </ref> IR framework the most compact consistent clause which subsumes lgg ( S S V k (P; D)) will be chosen. <p> A similar approach is found to be essential within Muggleton and Feng [29], since RLGG clauses can be very large, even when reduced using Definition 5. 5 Predicate invention In the last section we described a unified framework covering both RLGG and IR. However, in Muggleton and Buntine's <ref> [28] </ref> description of IR a salient feature was the invention of new predicates. Predicate invention within the IR framework is carried out using the `W' operators. <p> It is rather intriguing that IR and RLGG should converge in this way since RLGG was based initially on the idea of inverting unification while IR is based on inverting resolution. The IR-RLGG framework has a distinct advantage over the use of the `V' operator in Muggleton and Buntine <ref> [28] </ref>. Whereas Muggleton and Buntine's method requires heuristic search to choose which `V' operator to apply, the new approach constructs a unique solution for multiple `V' operations without recourse to search. Other systems such as Quinlan's FOIL [33] also use heuristic search.
Reference: [29] <author> S.H. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Learning Theory, </booktitle> <address> Tokyo, 1990. </address> <publisher> Ohmsha. </publisher>
Reference-contexts: Quinlan sees his approach as being a natural extension of ID3 [32]. He notes that the search can be highly myopic, and is unable to learn predicates such as list-reversal and integer-multiplication. Attempts have been made recently by Buntine [4], Frisch and Page [11] and Muggleton and Feng <ref> [29] </ref> to find ways around Plotkin's negative RLGG results. Domain Clause No. of No. of atoms examples in background Qualitative state (la:A/B,lb:C/D,fab:E/B,fba:F/D) 169 5408 modelling deriv (la:A/B,fba:F/D), (U-tube) deriv (lb:C/D,fab:E/B), minus (la:A/B,lb:G/D,[]), minus (la:G/B,lb:C/D,[]). <p> The approach had limited success due to the complexity of constructed clauses. Frisch and Page have tried restricting the hypothesis language to being a strictly sorted logic with more success. Muggleton and Feng <ref> [29] </ref> apply a "determinate" restriction to hypotheses. By this it is meant that there is a unique choice of ground substitution for every resolution involved in the refutation of any ground goal using the hypothesised clause. Muggleton and Feng's learning program, Golem, has been demonstrated [29] to have a level of <p> Muggleton and Feng <ref> [29] </ref> apply a "determinate" restriction to hypotheses. By this it is meant that there is a unique choice of ground substitution for every resolution involved in the refutation of any ground goal using the hypothesised clause. Muggleton and Feng's learning program, Golem, has been demonstrated [29] to have a level of efficiency similar to Quinlan's FOIL but without the accompanying loss of scope. Although predicates such as list-reverse, integer-multiply and quick-sort were reported in [29], the following section provides some details of real world applications of Golem. 2.2 Real-world domains A great deal of research has <p> Muggleton and Feng's learning program, Golem, has been demonstrated <ref> [29] </ref> to have a level of efficiency similar to Quinlan's FOIL but without the accompanying loss of scope. Although predicates such as list-reverse, integer-multiply and quick-sort were reported in [29], the following section provides some details of real world applications of Golem. 2.2 Real-world domains A great deal of research has been invested in inductive logic programming algorithms. <p> A general feeling is emerging that it is time to move on from the toy worlds of arch building and list and number theoretic predicates. Figure 2 shows various clauses developed as part of ongoing real-world application projects using Muggleton and Feng's <ref> [29] </ref> Golem. The first clause represents a qualitative model of a U-tube, developed in a collaborative project between the author and Ivan Bratko. The clause was built using Kuipers' [18] theory of qualitative reasoning as background knowledge, encoded as 5408 ground atoms. <p> The constraint that all variables in the head (l 1 ) of a background clause (C 1 ) be found within the body of the clause (C 1 flg) is used by Muggleton and Feng <ref> [29] </ref> for construction of RLGG's. They call a logic program containing only such clauses syntactically generative and prove that all atoms derivable using resolution are ground. <p> literals we can, using the same reasoning as above, show that the most specific solution for the identification operator is C 1 # = (C [ f:l 2 g 2 ) (6) For the purposes of this paper we use the phrase weakly generative in place of Mug-gleton and Feng's <ref> [29] </ref> syntactically generative. The phrase strongly generative is used to describe sets of clauses in which every variable in every clause is found in at least two literals of that clause. <p> However it is well known that linear derivation is sound and complete. In the case of the depth parameter k, Muggleton and Feng <ref> [29] </ref> describe a method for constructing RLGG's that uses a parameter h in the same way as k is used here. Theorem 10 Equivalence of RLGG and lgg of inverse linear derivations. <p> A similar approach is found to be essential within Muggleton and Feng <ref> [29] </ref>, since RLGG clauses can be very large, even when reduced using Definition 5. 5 Predicate invention In the last section we described a unified framework covering both RLGG and IR. However, in Muggleton and Buntine's [28] description of IR a salient feature was the invention of new predicates. <p> The discovery of a common framework for IR and RLGG (section 4.5) is one step in this direction. Surprisingly both approaches demand that the logic programs involved be generative (see <ref> [29] </ref> and Section 4.2), but for independent reasons. It is rather intriguing that IR and RLGG should converge in this way since RLGG was based initially on the idea of inverting unification while IR is based on inverting resolution.
Reference: [30] <author> T. Niblett. </author> <title> A study of generalisation in logic programs. </title> <booktitle> In EWSL-88, </booktitle> <address> London, 1988. </address> <publisher> Pitman. </publisher>
Reference-contexts: Maher in [21] discusses a number of different notions of equivalence for logic programs including that of Definition 3. See also Buntine [4] and Niblett <ref> [30] </ref> for a detailed discussion of generality. 3.2 A general setting for induction In the general inductive setting we are provided with three languages.
Reference: [31] <author> G.D. Plotkin. </author> <title> Automatic Methods of Inductive Inference. </title> <type> PhD thesis, </type> <institution> Edin-burgh University, </institution> <month> August </month> <year> 1971. </year>
Reference-contexts: Turing [43] later came to believe that Godel's incompleteness theorem required that intelligent machines be capable of learning from examples. Various logical positivists including Carnap [5] have developed statistical theories for confirming scientific hypotheses posed in first order logic. Although Plotkin <ref> [31] </ref> in the 1970's and Shapiro [37] in the 1980's worked on computer-based inductive systems within the framework of full first order logic, most successes within the field of Machine Learning have derived from systems which construct hypotheses within the limits of propositional logic. <p> The general goals of research for such an area should be to produce a widely used technology with a firm theoretical foundation. Ideally a uniform theory, such as that behind Logic Programming will emerge. 2.1 Past and present research In his thesis Plotkin <ref> [31] </ref> laid the foundations for much of the present activity. Plotkin did not restrict himself to Horn clause logic. This is hardly surprising given that logic programming had not yet come into existence. <p> Usually, L O is required to contain only ground literals. This still leaves a very unconstrained choice for H. Often H is restricted to being a single clause. This is the case for Plotkin <ref> [31] </ref> and Shapiro [37] though not the case in Muggleton and Buntine [28] in which predicate invention introduces a set of clauses. H can be constrained to be the most general hypothesis relative B as in Shapiro [37] or the least general relative to B as in Plotkin [31]. <p> for Plotkin <ref> [31] </ref> and Shapiro [37] though not the case in Muggleton and Buntine [28] in which predicate invention introduces a set of clauses. H can be constrained to be the most general hypothesis relative B as in Shapiro [37] or the least general relative to B as in Plotkin [31]. Alternatively, H can be chosen to be that which produces the maximum information compression of O relative to B as in [26]. <p> From Corollary 7 we know that C k -subsumes both S S Plotkin <ref> [31] </ref> investigated the lattice that -subsumption defines over the set of all clauses. <p> Proof. The theorem follows from the definitions of rlgg P (C; D) and lgg ( S S 2 Obviously, the clause lgg ( S S V fl (P; D)) can be infinite. Plotkin <ref> [31] </ref> showed that rlgg P (C; D) can be infinite even when logically reduced.
Reference: [32] <author> J.R. Quinlan. </author> <title> Discovering rules from large collections of examples: a case study. </title> <editor> In D. Michie, editor, </editor> <booktitle> Expert Systems in the Micro-electronic Age, </booktitle> <pages> pages 168-201. </pages> <publisher> Edinburgh University Press, Edinburgh, </publisher> <year> 1979. </year>
Reference-contexts: The properties of various expert systems are given in Figure 1. The first two, MYCIN [38] and XCON [10] were built using hand-coding of rules. The second two, GASOIL [39] and BMT [15] were built using software derived from Quinlan's inductive decision tree building algorithm ID3 <ref> [32] </ref>. It should be noted that the inductively constructed BMT is by far the largest expert system in full-time commercial use. From the perspective of software engineering, it is also worth noting the sizeable reductions in development and maintenance times for inductively constructed systems. <p> Recently Quinlan [33] has described a highly efficient program, called FOIL, which induces first order Horn clauses. The method relies on a general to specific heuristic search which is guided by an information criterion related to entropy. Quinlan sees his approach as being a natural extension of ID3 <ref> [32] </ref>. He notes that the search can be highly myopic, and is unable to learn predicates such as list-reversal and integer-multiplication. Attempts have been made recently by Buntine [4], Frisch and Page [11] and Muggleton and Feng [29] to find ways around Plotkin's negative RLGG results.
Reference: [33] <author> J.R. Quinlan. </author> <title> Learning relations: comparison of a symbolic and a connectionist approach. </title> <type> Technical Report 346, </type> <institution> University of Sydney, </institution> <year> 1989. </year>
Reference-contexts: However, although predicate invention has been demonstrated on large scale problems within a propositional setting [24, 25] this is not yet the case for any first order learning systems. Recently Quinlan <ref> [33] </ref> has described a highly efficient program, called FOIL, which induces first order Horn clauses. The method relies on a general to specific heuristic search which is guided by an information criterion related to entropy. Quinlan sees his approach as being a natural extension of ID3 [32]. <p> Whereas Muggleton and Buntine's method requires heuristic search to choose which `V' operator to apply, the new approach constructs a unique solution for multiple `V' operations without recourse to search. Other systems such as Quinlan's FOIL <ref> [33] </ref> also use heuristic search. The disadvantages of heuristic search are not only that it can be inefficient, but also that solutions can be missed due to local minima in the search space. These problems are avoided within the new IR-RLGG framework.
Reference: [34] <author> J.A. Robinson. </author> <title> A machine-oriented logic based on the resolution principle. </title> <journal> JACM, </journal> <volume> 12(1) </volume> <pages> 23-41, </pages> <month> January </month> <year> 1965. </year>
Reference-contexts: Logical positivism was provided with powerful ammunition by Godel's demonstration [9] that a small collection of sound rules of inference was complete for deriving all consequences of formulae in first order predicate calculus. Much later Robinson demonstrated <ref> [34] </ref> that a single rule of inference, called resolution, is both sound and complete for proving statements within this calculus (see Appendix A). For the purposes of applying resolution, formulae are normalised to what is known as clausal form. <p> That is to say that hA; :A 0 i is a complementary pair. The resolvent of clauses C and D is denoted (C D) when the complementary pair of literals is unspecified. The `' operator is commutative, non-associative and non-distributive. Let T be a clausal theory. Robinson <ref> [34] </ref> defined the function R n (T ) recursively as follows. R 0 (T ) = T . R n (T ) is the set of all resolvents constructed from pairs of clauses in R n1 (T ).
Reference: [35] <author> C. Rouveirol and J-F Puget. </author> <title> A simple and general solution for inverting resolution. </title> <booktitle> In EWSL-89, </booktitle> <pages> pages 201-210, </pages> <address> London, 1989. </address> <publisher> Pitman. </publisher>
Reference-contexts: Since background knowledge is rarely complete in applications, this constraint is now generally believed to be over-restrictive. * Strong bias of vocabulary. Present inductive systems construct hypotheses within the limits of a fixed vocabulary of propositional attributes. An increasing amount of Machine Learning research <ref> [24, 28, 1, 44, 35, 16, 19] </ref> is concerned with algorithms capable of inventing auxiliary predicates when insufficient background knowledge is provided. 2 Inductive Logic Programming A growing body of researchers have started to work on problems of inductive reasoning within the confines of pure Prolog. <p> Another pair of operators (`W'), also based on inverting resolution, were shown to be capable of "inventing" predicates which were not available within the background knowledge (Section 5.1). Banerji [1], Wirth [44] Ishizaka [16], Ling and Dawes [19] and Rouveirol and Puget <ref> [35] </ref> have also described related methods for "inventing" new predicates. However, although predicate invention has been demonstrated on large scale problems within a propositional setting [24, 25] this is not yet the case for any first order learning systems. <p> The disadvantages of heuristic search are not only that it can be inefficient, but also that solutions can be missed due to local minima in the search space. These problems are avoided within the new IR-RLGG framework. Rouveirol and Puget <ref> [35] </ref> describe an operator called saturation which carries out multiple `V' operations. There is a clear relationship between the saturation operator and the function V n (Section 4.3). The definition of [V n can be seen as a formalisation of the saturation operator.
Reference: [36] <author> C. Sammut and R.B Banerji. </author> <title> Learning concepts by asking questions. </title> <editor> In R. Michalski, J. Carbonnel, and T. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <volume> Vol. 2, </volume> <pages> pages 167-192. </pages> <publisher> Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1986. </year>
Reference-contexts: It is also worth noting that despite the identification in the limit guarantees for Plotkin's and Shapiro's systems, both were very inefficient and were in practice only demonstrated on very limited problems. Sammut and Banerji <ref> [36] </ref> describe a system called MARVIN which generalises a single example at a time with reference to a set of background clauses. At each stage a set of ground atoms F representing the example are matched against the body of a background clause H B.
Reference: [37] <author> E.Y. Shapiro. </author> <title> Algorithmic program debugging. </title> <publisher> MIT Press, </publisher> <year> 1983. </year>
Reference-contexts: Turing [43] later came to believe that Godel's incompleteness theorem required that intelligent machines be capable of learning from examples. Various logical positivists including Carnap [5] have developed statistical theories for confirming scientific hypotheses posed in first order logic. Although Plotkin [31] in the 1970's and Shapiro <ref> [37] </ref> in the 1980's worked on computer-based inductive systems within the framework of full first order logic, most successes within the field of Machine Learning have derived from systems which construct hypotheses within the limits of propositional logic. <p> Plotkin's main theoretical result was negative. This was that there is in general no finite relative least general generalisation of two clauses. His implementation of RLGG was thus severely restricted. This negative result of Plotkin's prompted Shapiro <ref> [37] </ref> to investigate an approach to Horn clause induction in which the search for hypotheses is from general to specific, rather than Plotkin's specific to general approach. Shapiro also investigated a technique called algorithmic debugging. <p> Usually, L O is required to contain only ground literals. This still leaves a very unconstrained choice for H. Often H is restricted to being a single clause. This is the case for Plotkin [31] and Shapiro <ref> [37] </ref> though not the case in Muggleton and Buntine [28] in which predicate invention introduces a set of clauses. H can be constrained to be the most general hypothesis relative B as in Shapiro [37] or the least general relative to B as in Plotkin [31]. <p> This is the case for Plotkin [31] and Shapiro <ref> [37] </ref> though not the case in Muggleton and Buntine [28] in which predicate invention introduces a set of clauses. H can be constrained to be the most general hypothesis relative B as in Shapiro [37] or the least general relative to B as in Plotkin [31]. Alternatively, H can be chosen to be that which produces the maximum information compression of O relative to B as in [26].
Reference: [38] <author> E.H. Shortliffe and B. Buchanan. </author> <title> A model of inexact reasoning in medicine. </title> <journal> Mathematical Biosciences, </journal> <volume> 23 </volume> <pages> 351-379, </pages> <year> 1975. </year>
Reference-contexts: The major successes here have been in the area of inductive construction of expert systems (see [27]). The properties of various expert systems are given in Figure 1. The first two, MYCIN <ref> [38] </ref> and XCON [10] were built using hand-coding of rules. The second two, GASOIL [39] and BMT [15] were built using software derived from Quinlan's inductive decision tree building algorithm ID3 [32].
Reference: [39] <author> S. Slocombe, K. Moore, and M. Zelonf. </author> <booktitle> Engineering expert systems applications. In Proceedings of the Annual Conference of the BCS Specialist Group on Expert Systems. </booktitle> <publisher> British Computer Society, </publisher> <address> London, </address> <year> 1986. </year>
Reference-contexts: The major successes here have been in the area of inductive construction of expert systems (see [27]). The properties of various expert systems are given in Figure 1. The first two, MYCIN [38] and XCON [10] were built using hand-coding of rules. The second two, GASOIL <ref> [39] </ref> and BMT [15] were built using software derived from Quinlan's inductive decision tree building algorithm ID3 [32]. It should be noted that the inductively constructed BMT is by far the largest expert system in full-time commercial use.
Reference: [40] <author> R.J. Solomonoff. </author> <title> A formal theory of inductive inference. </title> <journal> J. Comput. Sys., </journal> <volume> 7 </volume> <pages> 376-388, </pages> <year> 1964. </year>
Reference-contexts: Secondly, it is relatively straightforward to interpret textual compression within any formalism, be it propositional logic, first-order logic or context-free grammar rules. Thirdly, this would bring the PAC-learning model into line with other approaches developed from algorithmic complexity theory <ref> [40, 6, 2] </ref> and Bayesian statistics.
Reference: [41] <author> L. Sterling and E. Shapiro. </author> <title> The art of Prolog: advanced programming tech-niques. </title> <publisher> MIT-Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Prolog statements are phrased in a restricted clausal form called Horn clause logic. All computations within Prolog take the form of logical proofs based on the application of the rule of resolution. Since its inception Prolog has developed into a widely used programming language <ref> [3, 41] </ref> and was chosen as the core language for Japan's Fifth Generation Project. Prolog's development has also spawned the rigorous theoretical school of Logic Programming [20]. 1.2 Induction Despite the self-evident success of logical deduction, a certain question has cropped up time and again throughout its development.
Reference: [42] <author> A. </author> <title> Turing. Systems of logic based on ordinals. </title> <booktitle> Proceedings of the London Mathematical Society, </booktitle> <pages> pages 161-228, </pages> <year> 1939. </year>
Reference-contexts: This discovery prompted Turing to attempt to show <ref> [42] </ref> that problems concerning incompleteness of logical theories could be overcome by the use of an oracle capable of verifying underivable statements. Turing [43] later came to believe that Godel's incompleteness theorem required that intelligent machines be capable of learning from examples.
Reference: [43] <author> A. </author> <title> Turing. The automatic computing engine. </title> <booktitle> Lecture to the London Mathematical Society, </booktitle> <year> 1947. </year>
Reference-contexts: This discovery prompted Turing to attempt to show [42] that problems concerning incompleteness of logical theories could be overcome by the use of an oracle capable of verifying underivable statements. Turing <ref> [43] </ref> later came to believe that Godel's incompleteness theorem required that intelligent machines be capable of learning from examples. Various logical positivists including Carnap [5] have developed statistical theories for confirming scientific hypotheses posed in first order logic.
Reference: [44] <author> R. Wirth. </author> <title> Completing logic programs by inverse resolution. </title> <booktitle> In EWSL-89, </booktitle> <pages> pages 239-250, </pages> <address> London, 1989. </address> <publisher> Pitman. </publisher>
Reference-contexts: Since background knowledge is rarely complete in applications, this constraint is now generally believed to be over-restrictive. * Strong bias of vocabulary. Present inductive systems construct hypotheses within the limits of a fixed vocabulary of propositional attributes. An increasing amount of Machine Learning research <ref> [24, 28, 1, 44, 35, 16, 19] </ref> is concerned with algorithms capable of inventing auxiliary predicates when insufficient background knowledge is provided. 2 Inductive Logic Programming A growing body of researchers have started to work on problems of inductive reasoning within the confines of pure Prolog. <p> Another pair of operators (`W'), also based on inverting resolution, were shown to be capable of "inventing" predicates which were not available within the background knowledge (Section 5.1). Banerji [1], Wirth <ref> [44] </ref> Ishizaka [16], Ling and Dawes [19] and Rouveirol and Puget [35] have also described related methods for "inventing" new predicates. However, although predicate invention has been demonstrated on large scale problems within a propositional setting [24, 25] this is not yet the case for any first order learning systems. <p> Rouveirol and Puget [35] describe an operator called saturation which carries out multiple `V' operations. There is a clear relationship between the saturation operator and the function V n (Section 4.3). The definition of [V n can be seen as a formalisation of the saturation operator. Also Wirth <ref> [44] </ref> describes a method in which he takes the lgg of a number of `V' operations. Wirth's method is a special case of lgg ([V 1 (P; C),[V 1 (P; D)). Although the generalised `W' method described in Section 5.2 seems promising, it needs much further investigation.
References-found: 44

