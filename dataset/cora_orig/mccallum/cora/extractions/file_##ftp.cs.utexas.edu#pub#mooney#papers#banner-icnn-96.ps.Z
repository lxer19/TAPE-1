URL: file://ftp.cs.utexas.edu/pub/mooney/papers/banner-icnn-96.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/sowmya/pubs.html
Root-URL: 
Email: sowmya@cs.utexas.edu, mooney@cs.utexas.edu  
Title: Revising Bayesian Network Parameters Using Backpropagation  
Author: Sowmya Ramachandran, Raymond J. Mooney 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences, University of Texas,  
Abstract: The problem of learning Bayesian networks with hidden variables is known to be a hard problem. Even the simpler task of learning just the conditional probabilities on a Bayesian network with hidden variables is hard. In this paper, we present an approach that learns the conditional probabilities on a Bayesian network with hidden variables by transforming it into a multi-layer feedforward neural network (ANN). The conditional probabilities are mapped onto weights in the ANN, which are then learned using standard backpropagation techniques. To avoid the problem of exponentially large ANNs, we focus on Bayesian networks with noisy-or and noisy-and nodes. Experiments on real world classification problems demonstrate the effectiveness of our technique. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Buntine. </author> <title> Theory refinement on Bayesian networks. </title> <booktitle> In Proceedings of the Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 52-60, </pages> <year> 1991. </year>
Reference-contexts: The problem of learning Bayesian networks from data has attracted a lot of attention in the recent years. Many researchers have studied this problem and offered interesting solutions. One approach <ref> [2, 1, 3, 9] </ref> is to use a scoring metric to hill climb through a space of possible Bayesian networks to find one that is most probable given the data.
Reference: [2] <author> G. G. Cooper and E. Herskovits. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: The problem of learning Bayesian networks from data has attracted a lot of attention in the recent years. Many researchers have studied this problem and offered interesting solutions. One approach <ref> [2, 1, 3, 9] </ref> is to use a scoring metric to hill climb through a space of possible Bayesian networks to find one that is most probable given the data.
Reference: [3] <author> David Heckerman, Dan Geiger, and David M. Chikering. </author> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 293-301, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: The problem of learning Bayesian networks from data has attracted a lot of attention in the recent years. Many researchers have studied this problem and offered interesting solutions. One approach <ref> [2, 1, 3, 9] </ref> is to use a scoring metric to hill climb through a space of possible Bayesian networks to find one that is most probable given the data.
Reference: [4] <author> J. J. Mahoney and R. J. Mooney. </author> <title> Combining connectionist and symbolic learning to refine certainty-factor rule-bases. </title> <journal> Connection Science, </journal> <volume> 5 </volume> <pages> 339-364, </pages> <year> 1993. </year> <title> Fig. 4: Splice Junction Prediction Accuracy </title>
Reference-contexts: This, again, enables the learning of conditional probabilities even in the presence of hidden variables. However, by concentrating just on noisy-and and noisy-or nodes, we have avoided the problem of intractably large networks faced by Schwalb [12]. Banner is an extension of Rapture, described in <ref> [4] </ref>. While Rapture is concerned with applying symbolic and connectionist techniques to revise certainty factor rule bases, we address the issue of doing the same with Bayesian networks. <p> The main goal of our research is to address the problem of inductively revising knowledge represented as a Bayesian network. Theory revision is an active area of research in the field of machine learning and various algorithms have been proposed for knowledge refinement <ref> [13, 7, 11, 4] </ref>. We aim to adapt these algorithms to revise Bayesian networks. <p> We also compare the performance of Banner with the performances of other inductive learning and theory-revision systems like Kbann, Id3, Either, Rapture and Backprop. Id3 [10] is a system for inducing decision trees. Either [7] learns and revises propositional Horn-clause theories. Rapture <ref> [4] </ref> is a system for revising certainty-factor rule bases using neural networks. Kbann [13, 6] revises a logical theory using a hybrid of symbolic and connectionist learning methods. DNA Promoter Recognition Figure 2 shows the initial logical theory for recognising a DNA promoter sequence.
Reference: [5] <author> James L. McClelland and David E. Rumelhart. </author> <title> Explorations in Parallel Distributed Processing: A Handbook of Models, Programs, and Exercises. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: By mapping the given Bayesian network onto a neural network with SIGMA-PI nodes, this technique can learn the conditional probabilities associated with the network (represented by link weights in the corresponding neural network) using standard backpropagation techniques <ref> [5] </ref>. This has the advantage that it is able to learn the conditional probabilities even in the presence of hidden variables.
Reference: [6] <author> M. O. Noordewier, G. G. Towell, and J. W. Shavlik. </author> <title> Training knowledge-based neural networks to recognize genes in DNA sequences. </title> <booktitle> In Advances in Neural Information Processing Systems, volume 3, </booktitle> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Once the network has been trained to a desired accuracy, it can be mapped back into a Bayesian network. 5. Experimental Evaluation We have evaluated Banner on two classification problems: DNA promoter recognition <ref> [6] </ref> and Gene Splice Junction recognition [6]. Each of these has associated with it an initial domain theory that does not have a good prediction accuracy on the data and therefore has to be revised. <p> Once the network has been trained to a desired accuracy, it can be mapped back into a Bayesian network. 5. Experimental Evaluation We have evaluated Banner on two classification problems: DNA promoter recognition <ref> [6] </ref> and Gene Splice Junction recognition [6]. Each of these has associated with it an initial domain theory that does not have a good prediction accuracy on the data and therefore has to be revised. For each problem, we created several random splits of the data into training and test sets. <p> Id3 [10] is a system for inducing decision trees. Either [7] learns and revises propositional Horn-clause theories. Rapture [4] is a system for revising certainty-factor rule bases using neural networks. Kbann <ref> [13, 6] </ref> revises a logical theory using a hybrid of symbolic and connectionist learning methods. DNA Promoter Recognition Figure 2 shows the initial logical theory for recognising a DNA promoter sequence. <p> Our technique shows a performance that is better than Either and comparable to both Rapture and Kbann, although its learning curve is not as steep. Splice Junction We also evaluated Banner on the task of learning to recognise the splice junctions in a given DNA sequence <ref> [6] </ref>. There are two kinds of splice junctions: IE sites and EI sites. These form the two output categories. There are 60 input features, each of which can take on the values A, C, G or T.
Reference: [7] <author> D. Ourston and R. J. Mooney. </author> <title> Theory refinement combining analytical and empirical methods. </title> <journal> Artificial Intelligence, </journal> <volume> 66 </volume> <pages> 311-344, </pages> <year> 1994. </year>
Reference-contexts: The main goal of our research is to address the problem of inductively revising knowledge represented as a Bayesian network. Theory revision is an active area of research in the field of machine learning and various algorithms have been proposed for knowledge refinement <ref> [13, 7, 11, 4] </ref>. We aim to adapt these algorithms to revise Bayesian networks. <p> In the following subsections, we present the results of the experiments. We also compare the performance of Banner with the performances of other inductive learning and theory-revision systems like Kbann, Id3, Either, Rapture and Backprop. Id3 [10] is a system for inducing decision trees. Either <ref> [7] </ref> learns and revises propositional Horn-clause theories. Rapture [4] is a system for revising certainty-factor rule bases using neural networks. Kbann [13, 6] revises a logical theory using a hybrid of symbolic and connectionist learning methods.
Reference: [8] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, Inc., </publisher> <address> San Mateo,CA, </address> <year> 1988. </year>
Reference-contexts: 1. Introduction Bayesian networks are increasingly being used for representing probabilistic knowledge <ref> [8] </ref>. Their strong grounding in probability theory makes them a particularly attractive formalism for representing knowledge. However, they suffer from the knowledge acquisition problem. <p> Thus, mapping a general Bayesian network into an ANN by mapping each conditional probability onto a weight in the neural network [12] becomes infeasible for even modestly large Bayesian networks. The noisy-or and the noisy-and models of Bayesian networks <ref> [8] </ref> avoid this problem providing a way to compute the conditional probability of a variable given a combination of values of its parents from just the conditional probabilities of the variable given the value of each of its parents in isolation.
Reference: [9] <author> Gregory M. Provan and Moninder Singh. </author> <title> Learning Bayesian networks using feature selection. </title> <booktitle> In Proceedings of the Workshop on Artificial Intelligence and Statistics, </booktitle> <year> 1994. </year>
Reference-contexts: The problem of learning Bayesian networks from data has attracted a lot of attention in the recent years. Many researchers have studied this problem and offered interesting solutions. One approach <ref> [2, 1, 3, 9] </ref> is to use a scoring metric to hill climb through a space of possible Bayesian networks to find one that is most probable given the data.
Reference: [10] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: In the following subsections, we present the results of the experiments. We also compare the performance of Banner with the performances of other inductive learning and theory-revision systems like Kbann, Id3, Either, Rapture and Backprop. Id3 <ref> [10] </ref> is a system for inducing decision trees. Either [7] learns and revises propositional Horn-clause theories. Rapture [4] is a system for revising certainty-factor rule bases using neural networks. Kbann [13, 6] revises a logical theory using a hybrid of symbolic and connectionist learning methods.
Reference: [11] <author> B. L. Richards and R. J. Mooney. </author> <title> Automated refinement of first-order Horn-clause domain theories. </title> <journal> Machine Learning, </journal> <volume> 19(2) </volume> <pages> 95-131, </pages> <year> 1995. </year>
Reference-contexts: The main goal of our research is to address the problem of inductively revising knowledge represented as a Bayesian network. Theory revision is an active area of research in the field of machine learning and various algorithms have been proposed for knowledge refinement <ref> [13, 7, 11, 4] </ref>. We aim to adapt these algorithms to revise Bayesian networks.
Reference: [12] <author> E. Schwalb. </author> <title> Compiling Bayesian networks into neural networks. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 291-297, </pages> <address> Amherst, MA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: However, a drawback with this approach is that it becomes too expensive when there are hidden variables, i.e. variables that cause some observations but are not themselves observed. This technique also requires that a total ordering on the variables be specified. Schwalb <ref> [12] </ref> proposed using connectionist methods to learn the conditional probabilities on a Bayesian network inductively, given its structure and data. <p> This, again, enables the learning of conditional probabilities even in the presence of hidden variables. However, by concentrating just on noisy-and and noisy-or nodes, we have avoided the problem of intractably large networks faced by Schwalb <ref> [12] </ref>. Banner is an extension of Rapture, described in [4]. While Rapture is concerned with applying symbolic and connectionist techniques to revise certainty factor rule bases, we address the issue of doing the same with Bayesian networks. <p> Thus, for a network where all variables are binary-valued, a variable with n parents would require 2 n conditional probabilities to be specified. Thus, mapping a general Bayesian network into an ANN by mapping each conditional probability onto a weight in the neural network <ref> [12] </ref> becomes infeasible for even modestly large Bayesian networks.
Reference: [13] <author> G. G. Towell, J. W. Shavlik, and Michiel O. Noordewier. </author> <title> Refinement of approximate domain theories by knowledge-based artificial neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 861-866, </pages> <address> Boston, MA, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: The main goal of our research is to address the problem of inductively revising knowledge represented as a Bayesian network. Theory revision is an active area of research in the field of machine learning and various algorithms have been proposed for knowledge refinement <ref> [13, 7, 11, 4] </ref>. We aim to adapt these algorithms to revise Bayesian networks. <p> Id3 [10] is a system for inducing decision trees. Either [7] learns and revises propositional Horn-clause theories. Rapture [4] is a system for revising certainty-factor rule bases using neural networks. Kbann <ref> [13, 6] </ref> revises a logical theory using a hybrid of symbolic and connectionist learning methods. DNA Promoter Recognition Figure 2 shows the initial logical theory for recognising a DNA promoter sequence.
References-found: 13

