URL: http://ei.cs.vt.edu/~succeed/IEEEComputer/abdulla.ps.gz
Refering-URL: http://ei.cs.vt.edu/~succeed/IEEEComputer/
Root-URL: http://www.cs.vt.edu
Email: fabdulla,abrams,foxg@vt.edu  
Phone: +1-540-231-6931  
Title: Scaling the World-Wide Web  
Author: Ghaleb Abdulla Marc Abrams Edward A. Fox 
Date: March 19, 1996  
Address: VA 24061-0106  
Affiliation: Virginia Polytechnic Institute and State University Department of Computer Science Blacksburg,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> T. T. Kwan, R. E. McGrath, and D. A. Reed. </author> <title> User access patterns to NCSA's Worldwide Web server. </title> <type> Technical Report UIUCDCS-R-95-1934, </type> <institution> Dept. of Comp. Sci., Univ. 18 of IL, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: We start by identifying techniques that improve scalability other than the brute force solutions of increasing network bandwidth and server throughput. These techniques are: use of caching on the server side (e.g., <ref> [1] </ref>), on the client side (e.g., caches built into Web browsers), and in the network (known as "Proxy caching") (e.g., [2]), and finally data compression. Caching One impediment to scalability is use of the wrong protocol for document delivery. <p> Cache placement Caching can be implemented in three places: at severs, in the network itself, and at clients. Caching on the server side is implemented by replicating the file system and the HTTP server and connecting the replicated servers with a high speed network <ref> [1] </ref>. This is similar to server mirroring, where the data on the server is copied into several other servers to reduce the load on the network and the original server; however in server mirroring the servers are not placed in one location; they can be separated by long distances. <p> The 14 designers of the network architecture can pick a workstation that can support a request rate= r r and connect more than one workstation with a high speed network such as an FDDI ring as shown in Figure 1. This solution was adopted by NCSA <ref> [1] </ref> to scale up their popular server.
Reference: [2] <author> A. Luotonen and K. Altis. </author> <title> World-Wide Web proxies. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 27(2), </volume> <year> 1994. </year> <note> &lt;URL: http://www1.cern.ch/PapersWWW94/luotonen.ps&gt;. </note>
Reference-contexts: These techniques are: use of caching on the server side (e.g., [1]), on the client side (e.g., caches built into Web browsers), and in the network (known as "Proxy caching") (e.g., <ref> [2] </ref>), and finally data compression. Caching One impediment to scalability is use of the wrong protocol for document delivery. For example, the aforementioned flash crowd phenomena consumes Internet bandwidth and server capacity because HTTP delivers a separate document copy to many readers.
Reference: [3] <author> R. Malpani, J. Lorch, and D. Berger. </author> <title> Making world wide web caching servers cooperate. </title> <booktitle> In 4th International World-wide Web Conference, </booktitle> <pages> pages 107-117, </pages> <address> Boston, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Caching on the network side will reflect the access pattern of a group of users who share the cache. The effectiveness of the network cache can increase by placing it where we know that a group of users have a high degree of locality, and by implementing multiple proxy <ref> [3] </ref> or hierarchical proxy caching. Multiple proxy caching is when many clients share many caches and a cache that misses can query other caches. In two-level caching we have several network caches connected to another network cache with a larger cache size. <p> The value for the proxy cache hit rate was measured from our log files, ranging between 30%-60% [4]. Similar numbers have been reported in the literature <ref> [3] </ref>. In this paper we use the value 40% for the network cache hit rate.
Reference: [4] <author> M. Abrams, S. Williams, G. Abdulla, S. Patel, R. Ribler, and E. A. Fox. </author> <title> Multimedia traffic analysis using Chitra95. </title> <booktitle> In Proc. ACM Multimedia '95, </booktitle> <pages> pages 267-276, </pages> <address> San Francisco, </address> <month> November </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: : average audio file transfer rate needed to achieve the real time playback. r s : average transfer rate for the rest of the files that makes accessing the Web acceptable for the rest of the users. c m : data compression ratio. h r : Proxy cache hit rate <ref> [4] </ref>. c r : client cache hit rate. C t : traffic generated from one client. I c : the enterprise connection to the Internet. <p> The value for r s was chosen by noticing that most Web users are happy when they get a transfer rate of 5 KB/sec. The value for the proxy cache hit rate was measured from our log files, ranging between 30%-60% <ref> [4] </ref>. Similar numbers have been reported in the literature [3]. In this paper we use the value 40% for the network cache hit rate. <p> We have been monitoring the WWW traffic for a year, and we have developed tools to log, analyse, and visualize collected traffic. Currently we are developing a comprehensive tool for WWW traffic monitoring and visualization <ref> [4] </ref>. We invite the other researchers to share their collected data and tools to help the WWW evolve to meet demands of the the future.
Reference: [5] <author> S. Williams, M. Abrams, C. R. Standridge, G. Abdulla, , and E. A. Fox. </author> <title> Removal policies in network caches for world-wide web documents. </title> <note> In submitted for publication, February 1996. &lt;URL: http://ei.cs.vt.edu/~succeed/96sigcomm/96sigcomm.html&gt;. </note>
Reference-contexts: Table 1 lists the parameters and their values. We choose the values for pv, pa, and ps based on the results obtained from log files we collected of and reported in <ref> [5] </ref>. The value of U, was calculated from the log files of a PC machine that is used by several students in a research lab by dividing the number of hours the machine was used on average by 24 hours.
Reference: [6] <author> Michael Blakeley. </author> <title> Webstone performance analysis: Sun netra i20. </title> <address> &lt;URL:http://www.sgi.com/Products/WebFORCE/WebStone/sun-ss20/sun-ss20.html&gt;, </address> <month> December </month> <year> 1995. </year> <pages> SGI. </pages>
Reference-contexts: This number might be very high compared to the client request rate since it represents the aggregation of requests by all clients. For a Web dedicated workstation, such as the SGI WebFORCE, the benchmarks show that it will sustain up to 96 requests/sec <ref> [6] </ref>. We will assume that the server will not do any thing more than retrieving and sending the required documents, and the server will not support querying or searching the data stored on the server since this will add extra load. <p> We will include compression as a factor, and will assume that the compression ratio cm is 0.5 for all data types. We will start by assuming that the server can support 96 requests/sec with no errors <ref> [6] </ref>. For this scenario we will use two cases from the previous scenarios as base cases. The first base case is the same base case for scenario 1A, where we concluded that 1942 simultaneous clients can be supported. This case represent the currently measured values for the previous parameters.
Reference: [7] <author> Mark E. Crovella and Azer Bestavros. </author> <title> Explaining world wide web traffic self-similarity. </title> <type> Technical Report TR-95-015, </type> <institution> Boston University, </institution> <month> October </month> <year> 1995. </year> <month> 19 </month>
Reference-contexts: An interesting problem is to check for self-similarity of the Web traffic in a specific user community and compare that with other studies for different communities <ref> [7] </ref>.
References-found: 7

