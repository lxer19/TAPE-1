URL: http://www.research.microsoft.com/~zorn/pubs/ASPLOS8-Seg-Objects.ps
Refering-URL: http://www.research.microsoft.com/~zorn/
Root-URL: http://www.research.microsoft.com
Email: fseidl, zorng@cs.colorado.edu  
Title: Segregating Heap Objects by Reference Behavior and Lifetime  
Author: Matthew L. Seidl and Benjamin G. Zorn 
Address: Campus Box 430  Boulder, CO 80309-0430 USA  
Affiliation: Department of Computer Science  University of Colorado  
Abstract: Dynamic storage allocation has become increasingly important in many applications, in part due to the use of the object-oriented paradigm. At the same time, processor speeds are increasing faster than memory speeds and programs are increasing in size faster than memories. In this paper, we investigate efforts to predict heap object reference and lifetime behavior at the time objects are allocated. Our approach uses profile-based optimization, and considers a variety of different information sources present at the time of object allocation to predict the object's reference frequency and lifetime. Our results, based on measurements of six allocation intensive programs, show that program references to heap objects are highly predictable and that our prediction methods can successfully predict the behavior of these heap objects. We show that our methods can decrease the page fault rate of the programs measured, sometimes dramatically, in cases where the physical memory available to the program is constrained. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas Ball and James R. Larus. </author> <title> Efficient path profiling. </title> <booktitle> In Proceedings of MICRO-29, </booktitle> <address> Paris, France, </address> <month> December </month> <year> 1996. </year>
Reference-contexts: The stack contents predictor requires that the chain of callers on the stack be determined at runtime. This problem is similar in spirit to the problem of determining a trace of basic blocks (or path) within a procedure. Work in the area of path profiling by Ball and Larus <ref> [1] </ref> may be of use as a starting point for the stack contents implementation. One technique that can be used is called "bit-pushing", where before each call bits are pushed into a shift register that indicate the identity of the call site.
Reference: [2] <author> David Barrett and Benjamin Zorn. </author> <title> Using lifetime predictors to improve memory allocation performance. </title> <booktitle> In SIGPLAN'93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 187-196, </pages> <address> Albuquerque, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: While our goals are similar to theirs, our prediction techniques are very different. Perhaps the work that is closest in spirit to our work is that of Barrett and Zorn, who attempted to predict short-lived object lifetimes using information present at the time of allocation <ref> [2] </ref>. Cohn and Singh also showed that object lifetimes in allocation-intensive programs can be predicted using decision trees to extract relevant static features present at an object's allocation [4]. <p> The CPU overhead of the stack contents predictor, which has the highest CPU overhead of the predictors we describe here, is also considered by Barrett and Zorn <ref> [2] </ref>. They computed the per allocation cost to be between 9 and 94 instructions. The memory layout we chose to use for the segments was dictated in part by using vmalloc. <p> For Vis, it is important to note, that even though the SL segment has a density less than one, its density is still twice that of the "other" segment. At first glance our results appear to contradict Barrett and Zorn's results <ref> [2] </ref> about the number of short-lived objects in a program.
Reference: [3] <author> Brad Calder, Dirk Grunwald, and Benjamin Zorn. </author> <title> Quantifying behavioral differences between C and C++ programs. </title> <journal> Journal of Programming Languages, </journal> <volume> 2(4) </volume> <pages> 313-351, </pages> <year> 1994. </year>
Reference-contexts: As a result, the use of dynamic storage allocation in application programs has increased dramatically. A recent study of C and C++ programs shows that over a range of application domains, heap objects are allocated almost ten times more frequently in C++ than in C <ref> [3] </ref>. Because all objects in Java must be allocated on the heap, dynamic storage allocation in Java is likely to be even more frequent than in C++ [8]. As program sizes have increased, so have main memory sizes.
Reference: [4] <author> David A. Cohn and Satinder Singh. </author> <title> Predicting lifetimes in dynamically allocated memory. </title> <booktitle> In Advances in Neural Information Processing Systems 9, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Cohn and Singh also showed that object lifetimes in allocation-intensive programs can be predicted using decision trees to extract relevant static features present at an object's allocation <ref> [4] </ref>. While the methods used in our paper are similar to this previous work, this paper goes beyond that work in several dimensions. First, we attempt to predict object reference behavior as well as lifetime. Second, we consider new predictors that previous work did not consider.
Reference: [5] <author> Robert Courts. </author> <title> Improving locality of reference in a garbage-collecting memory management system. </title> <journal> Communications of the ACM, </journal> <volume> 31(9) </volume> <pages> 1128-1138, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Their work did not consider the more speculative issue of predicting reference locality as we do in this paper. There have also been a number of papers investigating the effect of heap organization on reference locality in garbage collected languages <ref> [5, 14] </ref>, including several recent papers that specifically consider the effect of garbage collection on cache performance [7, 21, 23]. This work differs from ours in its focus. <p> While much of the related garbage collection work has investigated how generational garbage collection interacts with processor cache architecture, none of the previous work we are aware of has attempted to classify objects using profiles and segregate them as we do. The work of Courts <ref> [5] </ref>, in which the working set for an entire Lisp system is obtained by performing a "training" of the system, is the closest in this group to our work. While our goals are similar to theirs, our prediction techniques are very different.
Reference: [6] <author> David Detlefs, Al Dosser, and Benjamin Zorn. </author> <title> Memory allocation costs in large C and C++ programs. </title> <journal> Software| Practice and Experience, </journal> <volume> 24(6) </volume> <pages> 527-542, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: These C programs were selected because they are all allocate heap objects extensively, and Espresso and Sis have been reported on in previous related work (e.g., see <ref> [6] </ref>). A breakdown of basic information about the programs and the data sets we used for training and testing is presented in Table 2. In addition to the standard size metrics presented, we include some information on the number of allocation sites in the programs.
Reference: [7] <author> Amer Diwan, David Tarditi, and Eliot Moss. </author> <title> Memory subsystem performance of programs using copying garbage collection. </title> <booktitle> In ACM SIGPLAN-SIGACT POPL'94, </booktitle> <pages> pages 1-14, </pages> <address> Portland, Oregon, </address> <month> January 17-21, </month> <year> 1994. </year> <note> ACM Press. </note>
Reference-contexts: There have also been a number of papers investigating the effect of heap organization on reference locality in garbage collected languages [5, 14], including several recent papers that specifically consider the effect of garbage collection on cache performance <ref> [7, 21, 23] </ref>. This work differs from ours in its focus.
Reference: [8] <author> James Gosling, Bill Joy, and Guy Steele. </author> <title> The Java Language Specification. The Java Series. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1997. </year>
Reference-contexts: Because all objects in Java must be allocated on the heap, dynamic storage allocation in Java is likely to be even more frequent than in C++ <ref> [8] </ref>. As program sizes have increased, so have main memory sizes. But, because of rapid changes in computer technology, a larger amount of outdated hardware is currently in Copyright (c) 1998 by the Association for Computing Machinery, Inc.
Reference: [9] <author> David Grove, Jeffrey Dean, Charles Garrett, and Craig Chambers. </author> <title> Profile-guided receiver class prediction. </title> <booktitle> In Proceedings of the 1995 Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <pages> pages 108-123, </pages> <address> Austin, TX, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: If the number of such call chains is small, then the impact on program code size may be negligible. Research by Chambers et al. <ref> [9] </ref> suggests that such selective inlining may be effective for certain optimizations.
Reference: [10] <author> Dirk Grunwald, Benjamin Zorn, and Robert Henderson. </author> <title> Improving the cache locality of memory allocation. </title> <booktitle> In ACM SIGPLAN PLDI'93, </booktitle> <pages> pages 177-186, </pages> <address> Albuquerque, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The poor reference locality characteristics of first-fit storage allocation [12] has prompted the improved "better fit" methods [18] that are now often used. Grunwald et al. surveyed existing malloc implementations with the goal of understanding what techniques they provide to support cache locality <ref> [10] </ref>. Their conclusion was that the existing methods, including eliminating boundary value tags, and providing a fast allocator front end to rapidly reuse freed objects, did provide substantially better reference locality than the simple first-fit algorithm.
Reference: [11] <author> Mark D. Hill. </author> <title> Aspects of Cache Memory and Instruction Buffer Performance. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <month> November </month> <year> 1987. </year> <note> Technical Report UCB/CSD 87/381. </note>
Reference-contexts: This instrumentation was necessary to drive the tycho cache simulator <ref> [11] </ref>, and the basic LRU virtual memory simulator [22] we used to gather the data presented in Section 5. 4 Chameleon Chameleon an N-level channel router.
Reference: [12] <author> Donald E. Knuth. </author> <title> Fundamental Algorithms, </title> <booktitle> volume 1 of The Art of Computer Programming, chapter 2, </booktitle> <pages> pages 435-451. </pages> <publisher> Addison Wesley, </publisher> <address> Reading, MA, 2nd edition, </address> <year> 1973. </year>
Reference-contexts: The issue of locality of reference of heap-allocated objects has been investigated extensively, although more so for garbage-collected languages than for languages with explicit storage allocation. The poor reference locality characteristics of first-fit storage allocation <ref> [12] </ref> has prompted the improved "better fit" methods [18] that are now often used. Grunwald et al. surveyed existing malloc implementations with the goal of understanding what techniques they provide to support cache locality [10].
Reference: [13] <author> S. McFarling. </author> <title> Program optimization for instruction caches. </title> <booktitle> In ASPLOS'89, </booktitle> <pages> pages 183-191, </pages> <address> Boston, MA, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: Section 5 presents our results and Section 6 concludes and suggests directions for future work. 2 Background and Related Work Organizing program code to improve its locality of reference in the virtual memory (e.g., see [15]) and cache (e.g., see <ref> [13] </ref>) has been of interest for many years. These methods work because frequently executed code segments can be readily discovered using program profiling and/or static profile estimation [20]. In this paper, we investigate the analogous problem of predicting heap object behavior at the time objects are allocated.
Reference: [14] <author> David A. Moon. </author> <title> Garbage collection in a large Lisp system. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on LISP and Functional Programming, </booktitle> <pages> pages 235-246, </pages> <address> Austin, Texas, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: Their work did not consider the more speculative issue of predicting reference locality as we do in this paper. There have also been a number of papers investigating the effect of heap organization on reference locality in garbage collected languages <ref> [5, 14] </ref>, including several recent papers that specifically consider the effect of garbage collection on cache performance [7, 21, 23]. This work differs from ours in its focus.
Reference: [15] <author> Judith B. Peachey, Richard B. Bunt, and Charles J. Colburn. </author> <title> Some empirical observations on program behavior with applications to program restructuring. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(2):188-193, </volume> <month> February </month> <year> 1985. </year>
Reference-contexts: Section 5 presents our results and Section 6 concludes and suggests directions for future work. 2 Background and Related Work Organizing program code to improve its locality of reference in the virtual memory (e.g., see <ref> [15] </ref>) and cache (e.g., see [13]) has been of interest for many years. These methods work because frequently executed code segments can be readily discovered using program profiling and/or static profile estimation [20].
Reference: [16] <author> Matthew L. Seidl and Benjamin G. Zorn. </author> <title> Predicting references to dynamically allocated objects. </title> <type> Technical Report CU-CS-826-97, </type> <institution> Department of Computer Science, University of Colorado, Boulder, CO, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: As a result, substantial opportunity exists to increase the spatial reference locality of objects in such systems. In previous work, we showed that for some programs, a small number of objects receive a majority of the references, while other objects receive almost no references <ref> [16] </ref>. That previous result acts as a starting point for the current paper, which goes beyond the previous work by quantifying the performance effect achievable using object segregation. <p> Many programs put in layers of abstraction around malloc, such as C++ object constructors or array allocators. The depth needs to be set deep enough to go up the stack past these layers, and get into the actual meat of the program while avoiding over-specification. In previous work <ref> [16] </ref>, we showed that using the last three entries from the call chain is effective for the programs measured, and so a depth of three is the number used in all the results presented here.
Reference: [17] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In ACM SIG-PLAN PLDI'94, </booktitle> <pages> pages 196-205, </pages> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: In this paper, we describe and evaluate several approaches to predicting which segment an object should be placed in. Our results are based on measurements of six allocation-intensive programs. We evaluate our approach using binary instrumentation based on ATOM <ref> [17] </ref>, and vmalloc, a region-based general purpose storage allocator [19]. Us 1 ing a cross-validated experimental method, we show that our technique is uniformly effective at increasing the spatial locality of program references in the programs measured. <p> Figure 1 shows a hypothetical memory layout before and after optimization. program to be optimized, which can be done either with a special compiler, or as we do, with a executable transformation tool (e.g., ATOM <ref> [17] </ref>). <p> As mentioned, we instrument the programs used in our studies with the ATOM toolkit <ref> [17] </ref>. ATOM allows programmers to instrument existing binaries with additional code to gather runtime data, without interfering with that program's execution. 4.1 Programs Table 1 lists the six programs in our collection, and gives some information about what they do.
Reference: [18] <author> C. J. Stephenson. </author> <title> Fast fits: New methods for dynamic storage allocation. </title> <booktitle> In Proceedings of the Ninth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 30-32, </pages> <address> Bretton Woods, NH, </address> <month> October </month> <year> 1983. </year>
Reference-contexts: The issue of locality of reference of heap-allocated objects has been investigated extensively, although more so for garbage-collected languages than for languages with explicit storage allocation. The poor reference locality characteristics of first-fit storage allocation [12] has prompted the improved "better fit" methods <ref> [18] </ref> that are now often used. Grunwald et al. surveyed existing malloc implementations with the goal of understanding what techniques they provide to support cache locality [10].
Reference: [19] <author> K. Phong Vo. </author> <title> Vmalloc: A general and efficient memory allocator. </title> <journal> Software Practice & Experience, </journal> <year> 1996. </year>
Reference-contexts: In this paper, we describe and evaluate several approaches to predicting which segment an object should be placed in. Our results are based on measurements of six allocation-intensive programs. We evaluate our approach using binary instrumentation based on ATOM [17], and vmalloc, a region-based general purpose storage allocator <ref> [19] </ref>. Us 1 ing a cross-validated experimental method, we show that our technique is uniformly effective at increasing the spatial locality of program references in the programs measured. <p> Instrs Executed refers the the total number of instructions executed. Total Loads/Stores refers to the total number of heap references the programs executed. 5 To implement the multiple segments of memory nec-essary to segregate objects, we used vmalloc <ref> [19] </ref>, a customizable storage allocator implemented by K. Phong Vo. Vmalloc allows its users to define separate regions of storage and manage those regions with different management policies.
Reference: [20] <author> Tim A. Wagner, Vance Maverick, Susan Graham, and Michael Harrison. </author> <title> Accurate static estimators for program optimization. </title> <booktitle> In Proceedings of the SIGPLAN'94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 85-96, </pages> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: These methods work because frequently executed code segments can be readily discovered using program profiling and/or static profile estimation <ref> [20] </ref>. In this paper, we investigate the analogous problem of predicting heap object behavior at the time objects are allocated. Existing malloc implementations allocate memory with little awareness of the other objects that have been allocated on the target page.
Reference: [21] <author> Paul R. Wilson, Michael S. Lam, and Thomas G. Moher. </author> <title> Caching considerations for generational garbage collection. </title> <booktitle> In SIGPLAN Symposium on LISP and Functional Programming, </booktitle> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: There have also been a number of papers investigating the effect of heap organization on reference locality in garbage collected languages [5, 14], including several recent papers that specifically consider the effect of garbage collection on cache performance <ref> [7, 21, 23] </ref>. This work differs from ours in its focus.
Reference: [22] <author> Benjamin G. Zorn. </author> <title> Comparative Performance Evaluation of Garbage Collection Algorithms. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <month> March </month> <year> 1989. </year> <note> Technical Report UCB/CSD 89/544. </note>
Reference-contexts: This instrumentation was necessary to drive the tycho cache simulator [11], and the basic LRU virtual memory simulator <ref> [22] </ref> we used to gather the data presented in Section 5. 4 Chameleon Chameleon an N-level channel router. The training and threshold inputs were two of the inputs provided with the release code (ex4 and ex1 respectively) and the testing input was ex3.
Reference: [23] <author> Benjamin G. Zorn. </author> <title> The effect of garbage collection on cache performance. </title> <institution> Computer Science Technical Report CU-CS-528-91, University of Colorado, </institution> <address> Campus Box 430, Boulder, CO 80309, </address> <month> May </month> <year> 1991. </year> <month> 12 </month>
Reference-contexts: There have also been a number of papers investigating the effect of heap organization on reference locality in garbage collected languages [5, 14], including several recent papers that specifically consider the effect of garbage collection on cache performance <ref> [7, 21, 23] </ref>. This work differs from ours in its focus.
References-found: 23

