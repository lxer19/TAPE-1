URL: http://www-wavelet.eecs.berkeley.edu/~vkgoyal/technical/erl-95-57.ps
Refering-URL: http://www-wavelet.eecs.berkeley.edu/~vkgoyal/technical/erl-95-57.html
Root-URL: http://www.cs.berkeley.edu
Email: vkgoyal@eecs.berkeley.edu  
Title: Quantized Overcomplete Expansions: Analysis, Synthesis and Algorithms  
Author: by Vivek K Goyal 
Address: Berkeley, CA 94720  
Affiliation: ELECTRONICS RESEARCH LABORATORY College of Engineering University of California, Berkeley  
Date: fl1995  1 July 1995  
Note: Copyright c  
Pubnum: Memorandum No. UCB/ERL M95/97  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> K. E. Atkinson, </author> <title> "An Introduction to Numerical Analysis (Second Edition)," </title> <publisher> Wiley, </publisher> <year> 1989. </year>
Reference-contexts: Any real-valued, T -periodic, bandlimited, continuous-time signal can be written in this form. Let M N . Define a sampled version of x c (t) by x d [m] = x c ( mT M ) and y = [x d [0] x d <ref> [1] </ref> x d [M 1]] : Then we have y = F x, where F = 6 6 6 1 2 0 2 0 p p p p . . . . . . . . . p p p p 3 7 7 5 M 0 = M 1, and <p> For example, we can start with an orthonormal basis and increase r by adding copies of vectors already in the frame. Putting aside pathological cases, simulations for quantization of a source uniformly distributed on <ref> [1; 1] </ref> N support this conjecture. Simulations were performed with three types of frame sequences: I. A sequence of frames corresponding to oversampled A/D conversion, as given by (2.9). This is the case in which we have a provable O (1=r 2 ) MSE upper bound. II. <p> In practice, this is an overly broad problem. In the following subsection, we will make several choices, some of them arbitrary. 3.3.2 A Detailed Example Consider quantization of a source with a uniform distribution on <ref> [1; 1] </ref> 2 . <p> Even with a known distribution, it is difficult to find CHAPTER 3. ADAPTIVE EXPANSIONS 31 (a) (b) <ref> [1; 1] </ref> 2 . (a) Fixed rate for c ff 1 . (b) Rate for c ff 1 conditioned on c ff 0 . quantization assumption is used. CHAPTER 3. ADAPTIVE EXPANSIONS 32 analytical expressions for optimal quantizers without using a fine quantization assumption.
Reference: [2] <author> A. Buzo, A. H. Gray, Jr. R. M. Gray, and J. D. Markel, </author> <title> "Speech coding based upon vector quantization," </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> Vol. 28, </volume> <month> October </month> <year> 1980, </year> <pages> pp. 562-574. </pages>
Reference-contexts: This is illustrated by the following example. Example 4. Suppose sequences of frames in R 2 is generated by choosing vectors ' k = [cos sin ] , where is uniformly distributed on <ref> [0; 2 ] </ref>. Then 1 F M " 2 2 2 2 elementwise as M ! 1: Thus the sequence of frames does not approach a tight frame. We can make a few additional observations. <p> A single iteration of matching pursuit is very similar to shape-gain VQ, which was introduced in <ref> [2] </ref>. In shape-gain VQ, a vector x 2 R N is separated into a gain, g = kxk and a shape, s = x=g. A shape ^s is chosen from a shape codebook C s to maximize hx; ^si.
Reference: [3] <author> J. H. Conway and N. J. A. Sloane, </author> <title> "Sphere Packings, Lattices and Groups," </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Using Z sin 2n d = 2 4 (2n) Z sin 2n+1 d = 2 1 3 5 (2n + 1) we can simplify (A.8) to get the following familiar result <ref> [3, x1.4] </ref>: c N = (N=2)! N 2 N (N1)=2 ( N1 N! Using (A.6), we can make the following calculation: 2 = E ' 2 i h N APPENDIX A. <p> Through Frame Operations A lattice fl is a set of points consisting of sums of the form P N k=1 ` k v k , where the ` k are integers and the vectors v 1 , : : : , v N are called a basis of the lattice <ref> [3] </ref>. 1 A lattice vector quantizer is a nearest-neighbor quantizer whose reproduction values form a lattice. This appendix establishes a relationship between lattice vector quantization and quantized frame representations. <p> It is equivalent to choosing a basis which minimizes the surface area of the fundamental parallelotope. (The volume of the fundamental parallelotope is fixed by fl.) See <ref> [3, x1.2 of Ch. 1] </ref>. 3 The boundaries can be arbitrarily defined. 56 APPENDIX C. LATTICE QUANTIZATION THROUGH FRAME OPERATIONS 57 depends on the lengths of the basis vectors; enforcing (C.1) minimizes the number of hyper-plane constraints. Denote the minimum number of half-space constraints to describe R 0 by L.
Reference: [4] <author> Z. Cvetkovic and M. Vetterli, </author> <title> "Error Analysis in Oversampled A/D Conversion and Quantization of Weyl-Heisenberg Frame Expansions," </title> <note> submitted to IEEE Transactions on Information Theory. </note>
Reference-contexts: For example, Munch [22] considered a particular type of frame and assumed the coefficients were subject to a stationary noise. This report, on the other hand, is in the same spirit as <ref> [4, 32, 33, 35] </ref> in that it utilizes the deterministic qualities of quantization. 2.1 Frames 2.1.1 Definitions and Basics The material in this subsection is largely adapted from [6, Ch. 3]. We are limiting our attention to Hilbert spaces H of dimension N . Definition.
Reference: [5] <author> I. Daubechies, </author> <title> "The Wavelet Transform, Time-Frequency Localization and Signal Analysis," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 36, No. 5, </volume> <month> September </month> <year> 1990, </year> <pages> pp. 961-1005. </pages>
Reference-contexts: This chapter describes frames, which provide a general framework for understanding non-orthogonal transforms. Frames were introduced by Duffin and Schaeffer [10] in the context of non-harmonic Fourier series. Recent interest in frames has been spurred by its utility in analyzing discrete wavelet transforms <ref> [5, 6, 15] </ref> and time-frequency decompositions [22]. We are motivated by a desire to understand quantization effects and efficient representations in a general framework. To put this chapter in context, we will give a particular interpretation of Fourier analysis and discuss a sense in which it can be generalized.
Reference: [6] <author> I. Daubechies, </author> <title> "Ten Lectures on Wavelets," </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: This chapter describes frames, which provide a general framework for understanding non-orthogonal transforms. Frames were introduced by Duffin and Schaeffer [10] in the context of non-harmonic Fourier series. Recent interest in frames has been spurred by its utility in analyzing discrete wavelet transforms <ref> [5, 6, 15] </ref> and time-frequency decompositions [22]. We are motivated by a desire to understand quantization effects and efficient representations in a general framework. To put this chapter in context, we will give a particular interpretation of Fourier analysis and discuss a sense in which it can be generalized. <p> This report, on the other hand, is in the same spirit as [4, 32, 33, 35] in that it utilizes the deterministic qualities of quantization. 2.1 Frames 2.1.1 Definitions and Basics The material in this subsection is largely adapted from <ref> [6, Ch. 3] </ref>. We are limiting our attention to Hilbert spaces H of dimension N . Definition. <p> Heuristically, we expect tight frames to have a certain degree of uniformity or regularity. This is illustrated by the following examples. Example 1 <ref> [6] </ref>. In H = R 2 , let ' 1 = [0 1] T , ' 2 = [ p 2 1 p 2 1 These are vectors on the unit circle uniformly spaced by 120 ffi . <p> CHAPTER 2. NON-ADAPTIVE EXPANSIONS 9 CHAPTER 2. NON-ADAPTIVE EXPANSIONS 10 In x2.2.1, we review the basic properties of reconstructing from (unquantized) frame coefficients. This material is adapted from <ref> [6] </ref>. The subsequent sections consider the problem of reconstructing an estimate of an original signal from quantized frame coefficients. Classical methods are limited by the assumption that the quantization noise is white. Our approach uses deterministic qualities of quantization to arrive at the concept of consistent reconstruction. <p> Also, e F F fl = F e F fl is the orthogonal projection operator, in C M , onto Ran (F ) = Ran ( e F ). Proof: See <ref> [6, p. 59] </ref>. ffi A consequence of e F fl e F = (F fl F ) 1 is that the dual of e is . <p> Proposition 2.4. If f = P k2K c k ' k for some set of coefficients fc k g k2K , then X jc k j k2K 2 with equality only if c k = hf; f ' k i for all k 2 K. Proof: See <ref> [6, p. 61] </ref>. ffi The norm-minimizing property of (2.15) holds in the "dual" sense also: If f = k2K then X k2K 2 X jh f ' k ; gij for all g 2 H.
Reference: [7] <author> G. Davis, </author> <title> "Adaptive Nonlinear Approximations," </title> <type> Ph.D. dissertation, </type> <institution> Mathematics Department, NYU, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: We expect to find more efficient representations by forming a linear expansion with respect to a subset of the original frame. This problem is formalized below. Definition <ref> [7, Ch. 2] </ref>. Let a dictionary D be a frame in H. Let * &gt; 0 and L 2 Z + . <p> Nevertheless, we are discouraged 19 CHAPTER 3. ADAPTIVE EXPANSIONS 20 from attempting to find optimal quantized representations by the following theorem. Theorem 3.1: Intractability of Optimal Approximation <ref> [7] </ref> Let k 1 and let D be a dictionary that contains O (N k ) vectors. Let 0 &lt; fl 1 &lt; fl 2 &lt; 1 and let L 2 Z + such that fl 1 N L fl 2 N . <p> For any given * &gt; 0 and f 2 H, determining whether an (*; L)-approximation exists is NP-complete. Finding the L-optimal approximation is NP-hard. Proof: See <ref> [7, Ch. 2] </ref>. ffi 3.2 Matching Pursuit The intractability of L-optimal approximation stems from the number of ways to choose L dictionary elements. The complexity is reduced if the dictionary elements are chosen one at a time instead of L at once. <p> It progressively refines a signal estimate instead of finding L components jointly. Matching pursuit was introduced to the signal processing community in the context of time-frequency analysis by Mallat and Zhang [20]. Mallat and his students have uncovered many of its properties <ref> [7, 8, 9, 40] </ref>. 3.2.1 Algorithm Let D = f' k g M k=1 H be a frame. We impose the additional constraint that k' k k = 1 for all k. We will call D our dictionary of vectors. <p> The proof of the convergence of projection pursuits given in [18] can be used to prove the convergence of matching pursuit in infinite dimensional spaces. In infinite dimensional spaces, the convergence can be quite slow. However, the convergence is exponential in finite dimensional spaces <ref> [7, x3:1] </ref>. Since ff i is determined by projection, ff i ' k i ? R i+1 f. <p> In fact, if no two elements of the dictionary are orthogonal, matching pursuit expansions are not only not optimal, but they do not converge in a finite number of steps except on a set of measure zero <ref> [7, x3:1] </ref>. In the following, detailed operation counts and other measures of complexity will not be given since the emphasis is not on implementation details. <p> A better orthogonalization method is presented by Kalker and Vetterli in [19]. It has been noted by several authors <ref> [7, 19, 36] </ref> that for a small number of iterations, orthogonal matching pursuit does not converge significantly faster than the non-orthogonalized version. <p> Looking at the dictionary design problem from a VQ standpoint, the first impulse is to train the dictionary using given training data. Davis <ref> [7, Ch. 8] </ref> has applied a Lloyd-type algorithm to optimize a dictionary to minimize D = E 4 fl fl f i=0 fl fl fl 5 for some fixed L.
Reference: [8] <author> G. Davis, S. Mallat and Z. Zhang, </author> <title> "Adaptive Time-Frequency Approximations with Matching Pursuits," </title> <type> Technical Report 657, </type> <institution> Computer Science Department, NYU, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: It progressively refines a signal estimate instead of finding L components jointly. Matching pursuit was introduced to the signal processing community in the context of time-frequency analysis by Mallat and Zhang [20]. Mallat and his students have uncovered many of its properties <ref> [7, 8, 9, 40] </ref>. 3.2.1 Algorithm Let D = f' k g M k=1 H be a frame. We impose the additional constraint that k' k k = 1 for all k. We will call D our dictionary of vectors.
Reference: [9] <author> G. Davis, S. Mallat and M. Avenaleda, </author> <title> "Chaos in Adaptive Approximations," </title> <type> Technical Report, </type> <institution> Computer Science Department, NYU, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: It progressively refines a signal estimate instead of finding L components jointly. Matching pursuit was introduced to the signal processing community in the context of time-frequency analysis by Mallat and Zhang [20]. Mallat and his students have uncovered many of its properties <ref> [7, 8, 9, 40] </ref>. 3.2.1 Algorithm Let D = f' k g M k=1 H be a frame. We impose the additional constraint that k' k k = 1 for all k. We will call D our dictionary of vectors.
Reference: [10] <author> R. J. Duffin and A. C. Schaeffer, </author> <title> "A class of nonharmonic Fourier series," </title> <journal> Transactions of the American Mathematical Society, </journal> <volume> Vol. 72, </volume> <pages> pp. 341-366, </pages> <year> 1952. </year>
Reference-contexts: For electrical engineers, frequency domain techniques based on Fourier transforms and Fourier series are second-nature. This chapter describes frames, which provide a general framework for understanding non-orthogonal transforms. Frames were introduced by Duffin and Schaeffer <ref> [10] </ref> in the context of non-harmonic Fourier series. Recent interest in frames has been spurred by its utility in analyzing discrete wavelet transforms [5, 6, 15] and time-frequency decompositions [22]. We are motivated by a desire to understand quantization effects and efficient representations in a general framework.
Reference: [11] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness, </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1979. </year>
Reference: [12] <author> A. Gersho, </author> <title> "On the Structure of Vector Quantizers," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 28, No. 2, </volume> <pages> pp. 157-166, </pages> <month> March </month> <year> 1982. </year> <note> 59 BIBLIOGRAPHY 60 </note>
Reference-contexts: Nearest-neighbor encoding implies that the region mapped to the origin is 3 R 0 = x 2 R N : kxk &lt; kx k 8 2 fl n f0g : (C.2) This is an infinite number of half-space constraints. It is shown in <ref> [12, xVI. A.] </ref> that by removing redundant constraints (those corresponding to hyperplanes far from the origin), (C.2) can be replaced by a finite number of constraints. <p> Figure C.1 shows the lattice generated by v 1 = [ 3 1] T and v 2 = [0 2] T . In this case, discarding remote hyperplanes as in <ref> [12, xVI. A.] </ref> leaves six half-space constraints for R 0 .
Reference: [13] <author> A. Gersho and R. M. Gray, </author> <title> "Vector Quantization and Signal Compression," </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: The computational aspects make transform coding very attractive. For this reason, transform coding is ubiquitous in image compression. For fine quantization of a Gaussian signal with known statistics, the Karhunen-Loeve transform (KLT) is optimal for transform coding <ref> [13] </ref>. In general, signal statistics are changing or not known a priori. Thus, one must either estimate the KLT from finite length blocks of the signal or use a fixed, signal-independent transform. <p> Approximating the KLT from data is essentially the same as principal component analysis [17]. It is well known that the KLT is the optimal transform for transform coding. Since the limitations to this result are not as well known, we state the following theorem paraphrased from <ref> [13] </ref>: Theorem 3.2: Optimality of the Karhunen-Lo eve Transform Consider the transform coding of a jointly Gaussian random process. Suppose the quantization is fine enough to use high resolution approximations, and that arbitrary real (non-integer) values can be allocated to the resolution of each (scalar) quantizer. <p> Suppose the quantization is fine enough to use high resolution approximations, and that arbitrary real (non-integer) values can be allocated to the resolution of each (scalar) quantizer. Then the KLT achieves the lowest overall distortion of any orthogonal transform. Proof: See <ref> [13, x8:6] </ref>. ffi Two properties of the KLT that make it good for transform coding are qualitatively mimicked by matching pursuit: 1. <p> Since we are assuming fine quantization, the best codebook constrained quantizer for ff 0 can be found analytically using a compandor model <ref> [13] </ref>. The optimal quantizer is c ff 0 = G 1 ffi Q u ffi G (ff 0 ); where G (y) = &gt; &gt; &lt; 2 1=3 p p p 2 q 2 2 1=3 p q 2 otherwise ; and Q u is a uniform quantizer.
Reference: [14] <author> V. K. Goyal, M. Vetterli and N. T. Thao, </author> <title> "Quantization of Overcomplete Expansions," </title> <booktitle> Proceedings of Data Compression Conference (DCC) 1995, </booktitle> <pages> pp. 13-22. </pages>
Reference-contexts: Since it does not depend on distributional knowledge, matching pursuit can be viewed as a "universal transform" for transform coding. 2 Some of the results of this report appeared earlier in <ref> [14] </ref>. 2 The phrase "universal lossy coder" is avoided because we assume a separation into a transform, followed by scalar quantization and universal lossless coding. This separation is not necessarily optimal but is motivated by complexity considerations. CHAPTER 1.
Reference: [15] <author> C. E. Heil and D. F. Walnut, </author> <title> "Continuous and Discrete Wavelet Transforms," </title> <journal> SIAM Review, </journal> <volume> Vol. 31, No. 4, </volume> <month> December </month> <year> 1989, </year> <pages> pp. 628-666. </pages>
Reference-contexts: This chapter describes frames, which provide a general framework for understanding non-orthogonal transforms. Frames were introduced by Duffin and Schaeffer [10] in the context of non-harmonic Fourier series. Recent interest in frames has been spurred by its utility in analyzing discrete wavelet transforms <ref> [5, 6, 15] </ref> and time-frequency decompositions [22]. We are motivated by a desire to understand quantization effects and efficient representations in a general framework. To put this chapter in context, we will give a particular interpretation of Fourier analysis and discuss a sense in which it can be generalized.
Reference: [16] <author> R. H. Hardin, N. J. A. Sloane and W. D. Smith, </author> <title> "Library of best ways known to us to pack n points on sphere so that minimum separation is maximized," </title> <address> URL: ftp://netlib.att.com/netlib/att/math/sloane/packings/ </address>
Reference-contexts: II. For N = 3, 4, and 5, Hardin, Sloane and Smith have numerically found arrangements of up to 130 points on N -dimensional unit spheres that maximize the minimum Euclidean norm separation <ref> [16] </ref>. III. Frames generated by randomly choosing points on the unit sphere according to a uniform distribution. Simulation results are given in Figure 2.4. The dashed, dotted, and solid curves correspond to frame types I, II, and III, respectively. <p> ADAPTIVE EXPANSIONS 24 * An uncorrelated zero-mean Gaussian source X ~ N (0; I). * A correlated zero-mean Gaussian source formed by placing a first-order autoregressive source with correlation 0.9 in blocks of length 4. Dictionaries were generated from maximally spaced points on the unit sphere <ref> [16] </ref>. The results are given in Figure 3.2. As expected, the energy compaction in the first component increases with r, ranging from about 0:55 to 0:96. <p> The averaging method gives MSEs that are lower by about a factor of ten. Simulations were also conducted with the same R 4 -valued autoregressive source as before. A dictionary of 130 maximally spaced unit vectors from <ref> [16] </ref> was used. The results are shown in Figure 3.5. The three pairs of curves correspond to estimating the first three principal axes of the distribution. The solid and dashed curves correspond to the averaging and histogram peak methods, respectively. <p> Dictionary sizes of M = 25, 50, 75, 100, and 125 were used. The results are shown in Figures 3.16 and 3.17. In Figure 3.16, the dictionaries used are those corresponding to oversampled A/D conversion as given in (2.9). Figure 3.17 was generated using dictionaries of maximally spaced points <ref> [16] </ref>. For both types of dictionaries, the probability of inconsistency goes to one for very coarse quantization and goes to zero as ! 0. <p> In our earlier examples, three methods for generating dictionaries have been used. In R 2 , dictionaries were formed from roots of unity as in (3.7) and (3.8). In higher dimensions, dictionaries were formed from sets of maximally spaced points on the unit sphere <ref> [16] </ref> or from a Fourier transform-like set as in (2.9). We introduce one more method for generating dictionaries. <p> II. Maximally spaced points on the unit sphere from <ref> [16] </ref> Disadvantages: * Dictionary must be stored. * Known only for N = 3, 4, 5, and M 130. III.
Reference: [17] <author> I. T. Jolliffe, </author> <title> "Principal Component Analysis," </title> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: Note that determining the KLT requires knowledge of the distribution of X. Approximating the KLT from data is essentially the same as principal component analysis <ref> [17] </ref>. It is well known that the KLT is the optimal transform for transform coding.
Reference: [18] <author> L. K. Jones, </author> <title> "On a conjecture of Huber concerning the convergence of projection pursuit regression," </title> <journal> The Annals of Statistics, </journal> <volume> Vol. 15, No. 2, </volume> <pages> pp. 880-882. </pages>
Reference-contexts: CHAPTER 3. ADAPTIVE EXPANSIONS 21 3.2.2 Discussion Matching pursuit is similar to a class of algorithms used in statistics called projection pursuits. The proof of the convergence of projection pursuits given in <ref> [18] </ref> can be used to prove the convergence of matching pursuit in infinite dimensional spaces. In infinite dimensional spaces, the convergence can be quite slow. However, the convergence is exponential in finite dimensional spaces [7, x3:1].
Reference: [19] <author> T. Kalker and M. Vetterli, </author> <title> "Projection Methods in Motion Estimation and Compensation", </title> <booktitle> Proceedings of IS&T/SPIE 1995. </booktitle>
Reference-contexts: A better orthogonalization method is presented by Kalker and Vetterli in <ref> [19] </ref>. It has been noted by several authors [7, 19, 36] that for a small number of iterations, orthogonal matching pursuit does not converge significantly faster than the non-orthogonalized version. <p> A better orthogonalization method is presented by Kalker and Vetterli in [19]. It has been noted by several authors <ref> [7, 19, 36] </ref> that for a small number of iterations, orthogonal matching pursuit does not converge significantly faster than the non-orthogonalized version. <p> So as log r is increased, there are diminishing returns in energy compaction, but the cost increases linearly. 3.3 Quantized Matching Pursuit Although matching pursuit has been applied to low bit rate compression problems <ref> [19, 23, 24, 25, 36] </ref>, which inherently require coarse coefficient quantization, little work has been CHAPTER 3. ADAPTIVE EXPANSIONS 27 CHAPTER 3. ADAPTIVE EXPANSIONS 28 done to understand the qualitative effects of coefficient quantization in matching pursuit. In this section we explore some of these effects.
Reference: [20] <author> S. Mallat and Z. Zhang, </author> <title> "Matching pursuits with time-frequency dictionaries," </title> <type> Technical Report 619, </type> <institution> Computer Science Department, NYU, </institution> <month> August </month> <year> 1993. </year> <title> (Also, </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> Vol. 41, No. 12, </volume> <pages> pp. 3397-3415, </pages> <month> December </month> <year> 1993.) </year>
Reference-contexts: However, we are not adapting in the traditional sense of making fine adjustments depending on an error signal. Instead, our basic tool is the matching pursuit algorithm of <ref> [20] </ref> in which the adaptation is in the choice of basis functions from a fixed dictionary (frame). In x3.1, we introduce the optimal approximation problem in order to establish its computational intractability. <p> Matching pursuit is a greedy algorithm for finding approximate solutions to the L-optimal approximation problem. It progressively refines a signal estimate instead of finding L components jointly. Matching pursuit was introduced to the signal processing community in the context of time-frequency analysis by Mallat and Zhang <ref> [20] </ref>. Mallat and his students have uncovered many of its properties [7, 8, 9, 40]. 3.2.1 Algorithm Let D = f' k g M k=1 H be a frame. We impose the additional constraint that k' k k = 1 for all k.
Reference: [21] <author> H. S. Malvar, </author> <title> "Signal Processing with Lapped Transforms," </title> <publisher> Artech House, </publisher> <year> 1992. </year>
Reference-contexts: All varieties of transform coding represent a signal vector as a linear combination of 1 In practical adaptive transform coding system, 20 to 40 percent of the available bit rate is assigned to side information <ref> [21, x2:3] </ref>. 1 CHAPTER 1. INTRODUCTION 2 basis vectors. Notice that in Figure 1.1, y is a representation of x in terms of the rows of F . But ^y is generally not an efficient representation of x. <p> For a stationary, vector-valued random process X, the Karhunen-Loeve transform is the unique orthogonal transform U such that Y = U X has a diagonal covariance matrix with the eigenvalues appearing in descending order on the diagonal <ref> [21, x1:2:4] </ref>. Note that determining the KLT requires knowledge of the distribution of X. Approximating the KLT from data is essentially the same as principal component analysis [17]. It is well known that the KLT is the optimal transform for transform coding.
Reference: [22] <author> N. J. Munch, </author> <title> "Noise Reduction In Tight Weyl-Heisenberg Frames," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 38, No. 2, </volume> <month> March </month> <year> 1992, </year> <pages> pp. 608-616. </pages>
Reference-contexts: This chapter describes frames, which provide a general framework for understanding non-orthogonal transforms. Frames were introduced by Duffin and Schaeffer [10] in the context of non-harmonic Fourier series. Recent interest in frames has been spurred by its utility in analyzing discrete wavelet transforms [5, 6, 15] and time-frequency decompositions <ref> [22] </ref>. We are motivated by a desire to understand quantization effects and efficient representations in a general framework. To put this chapter in context, we will give a particular interpretation of Fourier analysis and discuss a sense in which it can be generalized. <p> The remainder of the section gives new results on reconstruction from quantized frame coefficients. Most previous work on frame expansions is predicated either on exact knowledge of coefficients or on coefficient degradation by white additive noise. For example, Munch <ref> [22] </ref> considered a particular type of frame and assumed the coefficients were subject to a stationary noise.
Reference: [23] <author> R. Neff, A. Zakhor and M. Vetterli, </author> <title> "Very Low Bit Rate Video Coding Using Matching Pursuits," </title> <booktitle> Proceedings of SPIE Conference on Visual Communication and Image Processing (VCIP) 1994, </booktitle> <volume> Vol. 2308, No. 1, </volume> <pages> pp. 47-60. </pages>
Reference-contexts: So as log r is increased, there are diminishing returns in energy compaction, but the cost increases linearly. 3.3 Quantized Matching Pursuit Although matching pursuit has been applied to low bit rate compression problems <ref> [19, 23, 24, 25, 36] </ref>, which inherently require coarse coefficient quantization, little work has been CHAPTER 3. ADAPTIVE EXPANSIONS 27 CHAPTER 3. ADAPTIVE EXPANSIONS 28 done to understand the qualitative effects of coefficient quantization in matching pursuit. In this section we explore some of these effects.
Reference: [24] <author> R. Neff, </author> <title> "Very Low Bit Rate Video Coding Using Matching Pursuits," </title> <type> Masters Thesis, </type> <institution> University of California, Berkeley, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: So as log r is increased, there are diminishing returns in energy compaction, but the cost increases linearly. 3.3 Quantized Matching Pursuit Although matching pursuit has been applied to low bit rate compression problems <ref> [19, 23, 24, 25, 36] </ref>, which inherently require coarse coefficient quantization, little work has been CHAPTER 3. ADAPTIVE EXPANSIONS 27 CHAPTER 3. ADAPTIVE EXPANSIONS 28 done to understand the qualitative effects of coefficient quantization in matching pursuit. In this section we explore some of these effects.
Reference: [25] <author> R. Neff and A. Zakhor, </author> <title> "Matching Pursuit Video Coding at Very Low Bit Rates," </title> <booktitle> Proceedings of Data Compression Conference 1995, </booktitle> <pages> pp. 411-420. </pages>
Reference-contexts: So as log r is increased, there are diminishing returns in energy compaction, but the cost increases linearly. 3.3 Quantized Matching Pursuit Although matching pursuit has been applied to low bit rate compression problems <ref> [19, 23, 24, 25, 36] </ref>, which inherently require coarse coefficient quantization, little work has been CHAPTER 3. ADAPTIVE EXPANSIONS 27 CHAPTER 3. ADAPTIVE EXPANSIONS 28 done to understand the qualitative effects of coefficient quantization in matching pursuit. In this section we explore some of these effects.
Reference: [26] <author> A. V. Oppenheim and R. W. Schafer, </author> <title> "Discrete-Time Signal Processing," </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: Consider the space of continuous-time signals that are bandlimited to [; ]. This is a subspace of the Hilbert space L 2 (R). By the Nyquist Sampling Theorem CHAPTER 2. NON-ADAPTIVE EXPANSIONS 7 <ref> [26, x3:2] </ref>, S 1 = fsinc (t k)g k2Z , where sinc (t) = sin (t) ; forms a basis for this space. Notice that S 1 is the basis set for ideal -bandlimited interpo lation.
Reference: [27] <author> A. Papoulis, </author> <title> "Probability, Random Variables, and Stochastic Processes (Third Edition)," </title> <publisher> McGraw-Hill, </publisher> <year> 1991. </year>
Reference-contexts: PROOFS 50 where 2 = E [(' k ) 2 i ] and 4 = E [(' k ) 4 i ] <ref> [27, x8-1] </ref>. <p> ( M Var ( M 1 E (' k ) 2 j : (A.5) Noting that 2 and 4 are independent of M , (A.3) shows that Var h M F fl F ) ii ! 0 as M F fl F ) ii ! 2 in the mean-squared sense <ref> [27, x8-4] </ref>. Similarly, (A.4) and (A.5) show that for i 6= j, ( 1 M F fl F ) ij ! 0 in the mean-squared sense. This completes the proof, provided 2 = 1 N .
Reference: [28] <author> T. C. Pati, R. Rezahfar and P. S. Krishnaprasad, </author> <title> "Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition," </title> <booktitle> Proceedings of the 27th Asilomar Conference on Signals, Systems and Computers, </booktitle> <pages> pp. 40-44, </pages> <address> Novem-ber 1993. BIBLIOGRAPHY 61 </address>
Reference-contexts: Convergence in N steps is then guaranteed. A simple method of accelerating convergence through orthogonal-ization is described below <ref> [28] </ref>. The selection of dictionary elements is the same as before. CHAPTER 3.
Reference: [29] <author> K. Ramchandran and M. Vetterli, </author> <title> "Best wavelet packet bases in a rate-distortion sense," </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> Vol. 2, No. 2, </volume> <month> April </month> <year> 1993, </year> <pages> pp. 160-175. </pages>
Reference-contexts: As with any fixed transform, the DCT is nearly optimal for only a certain set of possible signals. There has been considerable work in the area of adaptively choosing a transform from a library of orthogonal transforms, for example, using wavelet packets <ref> [29] </ref>. All varieties of transform coding represent a signal vector as a linear combination of 1 In practical adaptive transform coding system, 20 to 40 percent of the available bit rate is assigned to side information [21, x2:3]. 1 CHAPTER 1. INTRODUCTION 2 basis vectors.
Reference: [30] <author> Selby, S. M. </author> <title> editor, "Standard Mathematical Tables (Eighteenth Edition)," </title> <publisher> CRC Press, </publisher> <year> 1970. </year>
Reference-contexts: The simplification (A.12) is due to a standard integration formula <ref> [30, #323] </ref>.
Reference: [31] <author> G. Strang, </author> <title> "Introduction to Applied Mathematics," </title> <publisher> Wellesley-Cambridge Press, </publisher> <year> 1986. </year>
Reference-contexts: These inequalities can CHAPTER 2. NON-ADAPTIVE EXPANSIONS 15 be combined into " F # " 2 + ^y 2 ^y : (2.22) The formulation (2.22) shows that ^x can be determined through linear programming <ref> [31] </ref>. The feasible set of the linear program is exactly the set of consistent estimates, so an arbitrary cost function can be used. <p> The feasible set of the linear program is exactly the set of consistent estimates, so an arbitrary cost function can be used. A linear program always returns a corner of the feasible set <ref> [31, x8:1] </ref>, so this type of reconstruction will not be close to the centroid of the partition cell. Since the cells are convex, one could use several cost functions to (presumably) get different corners of the feasible set and average the results.
Reference: [32] <author> N. T. Thao (Truong-Thao Nguyen), </author> <title> "Deterministic Analysis of Oversampled A/D Conversion and Sigma-Delta Modulation, and Decoding Improvements using Consistent Estimates," </title> <type> Ph.D. dissertation, </type> <institution> Department of Electrical Engineering, Columbia University, </institution> <year> 1993. </year>
Reference-contexts: For example, Munch [22] considered a particular type of frame and assumed the coefficients were subject to a stationary noise. This report, on the other hand, is in the same spirit as <ref> [4, 32, 33, 35] </ref> in that it utilizes the deterministic qualities of quantization. 2.1 Frames 2.1.1 Definitions and Basics The material in this subsection is largely adapted from [6, Ch. 3]. We are limiting our attention to Hilbert spaces H of dimension N . Definition.
Reference: [33] <author> N. T. Thao and M. Vetterli, </author> <title> "Reduction of the MSE in R-times oversampled A/D conversion from O(1=R) to O(1=R 2 )," IEEE Transactions on Signal Processing, </title> <journal> Vol. </journal> <volume> 42, No. 1, </volume> <pages> pp. 200-203, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: For example, Munch [22] considered a particular type of frame and assumed the coefficients were subject to a stationary noise. This report, on the other hand, is in the same spirit as <ref> [4, 32, 33, 35] </ref> in that it utilizes the deterministic qualities of quantization. 2.1 Frames 2.1.1 Definitions and Basics The material in this subsection is largely adapted from [6, Ch. 3]. We are limiting our attention to Hilbert spaces H of dimension N . Definition. <p> Oversampling of a periodic, bandlimited signal can be viewed as a frame operator applied to the signal, where the frame operator is associated with a tight frame. If the samples are quantized, this is exactly the situation of oversampled A/D conversion <ref> [33] </ref>. Let x = [X 1 X 2 X N ] T 2 R N , with N odd. Define a corresponding continuous-time signal by x c (t) = X 1 + k=1 X 2k 2 cos T p 2kt # where W = N1 2 . <p> As a result, the reconstruction may have a different quantized value than the original. Using the term introduced by Thao and Vetterli <ref> [33] </ref>, we say that the reconstruction may be inconsistent. Definition. We say that ^x is a consistent estimate of x or a consistent reconstruction if Q (F ^x) = Q (F x). A reconstruction that is not consistent is said to be inconsistent. <p> This point is not emphasized in [35]. 4 This is to eliminate degenerate distributions for x. CHAPTER 2. NON-ADAPTIVE EXPANSIONS 16 quantized frame expansion of x is equivalent to oversampled A/D conversion of x c (t). According to Thao and Vetterli <ref> [33, Thm. 4.1] </ref>, the MSE can be upper bounded by an O (1=r 2 ) expression. One requirement in applying their result is that x c (t) must have sufficient quantization threshold crossings.
Reference: [34] <author> N. T. Thao and M. Vetterli, </author> <title> "Deterministic analysis of oversampled A/D conversion and decoding improvement based on consistent estimates," </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> Vol. 42, No. 3, </volume> <pages> pp. 519-531, </pages> <month> March </month> <year> 1994. </year>
Reference: [35] <author> N. T. Thao and M. Vetterli, </author> <title> "Lower Bound on the Mean Squared Error in Oversampled Quantization of Periodic Signals Using Vector Quantization Analysis," </title> <note> submitted to IEEE Transactions on Information Theory. </note>
Reference-contexts: For example, Munch [22] considered a particular type of frame and assumed the coefficients were subject to a stationary noise. This report, on the other hand, is in the same spirit as <ref> [4, 32, 33, 35] </ref> in that it utilizes the deterministic qualities of quantization. 2.1 Frames 2.1.1 Definitions and Basics The material in this subsection is largely adapted from [6, Ch. 3]. We are limiting our attention to Hilbert spaces H of dimension N . Definition. <p> Theorem 2.6: MSE Lower Bound For any set of quantized frame expansions, any reconstruction algorithm will yield an MSE that can be lower bounded by an O (1=r 2 ) expression. 3 Proof: The proof of this general result is given under the guise of a more restricted result in <ref> [35] </ref>. There it is proven that when the frame operators correspond to oversampled A/D conversion (see x2.1.2), any reconstruction algorithm will yield an MSE that can be lower bounded by an O (1=r 2 ) expression. <p> Let x c (t) be defined as in (2.8), where T is arbitrary. Then 3 Actually, we must exclude the case where x has a degenerate distribution that allows for perfect reconstruction. This point is not emphasized in <ref> [35] </ref>. 4 This is to eliminate degenerate distributions for x. CHAPTER 2. NON-ADAPTIVE EXPANSIONS 16 quantized frame expansion of x is equivalent to oversampled A/D conversion of x c (t). <p> Thus quantization of y k gives a set of parallel hyperplanes spaced by , called a hyperplane single wave. The M hyperplane single waves give a partition with a particular structure called a hyperplane wave partition <ref> [35] </ref>. Examples of hyperplane wave partitions are shown in Figure B.1. Figure B.1 (a) shows a frame in R 2 composed of three vectors. Suppose x 2 R 2 is specified by quantized inner products with each of the three frame vectors.
Reference: [36] <author> M. Vetterli and T. Kalker, </author> <title> "Matching Pursuit for Compression and Application to Motion Compensated Video Coding," </title> <booktitle> Proceedings of International Conference on Image Processing (ICIP) 1994. </booktitle>
Reference-contexts: A better orthogonalization method is presented by Kalker and Vetterli in [19]. It has been noted by several authors <ref> [7, 19, 36] </ref> that for a small number of iterations, orthogonal matching pursuit does not converge significantly faster than the non-orthogonalized version. <p> So as log r is increased, there are diminishing returns in energy compaction, but the cost increases linearly. 3.3 Quantized Matching Pursuit Although matching pursuit has been applied to low bit rate compression problems <ref> [19, 23, 24, 25, 36] </ref>, which inherently require coarse coefficient quantization, little work has been CHAPTER 3. ADAPTIVE EXPANSIONS 27 CHAPTER 3. ADAPTIVE EXPANSIONS 28 done to understand the qualitative effects of coefficient quantization in matching pursuit. In this section we explore some of these effects.
Reference: [37] <author> M. Vetterli and J. Kovacevic, </author> <title> "Wavelets and Subband Coding," </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: 2 fi fi fi + fi fi p 2 1 f 2 fi fi 2 3 h 1 + f 2 i 3 kf k 2 : Thus f' 1 ; ' 2 ; ' 3 g is a tight frame with frame bound 3 2 = M Example 2 <ref> [37] </ref>. Consider the space of continuous-time signals that are bandlimited to [; ]. This is a subspace of the Hilbert space L 2 (R). By the Nyquist Sampling Theorem CHAPTER 2.
Reference: [38] <author> R. Zamir and M. Feder, </author> <title> "Rate-Distortion Performance in Coding Bandlimited Sources by Sampling and Dithered Quantization," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 41, No. 1, </volume> <pages> pp. 141-154, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: CHAPTER 2. NON-ADAPTIVE EXPANSIONS 18 Also, if r is doubled, storing the result as 2M B-bit values is far from the best thing to do. This is because many of the M additional numbers give little or no information on x. A conclusion of Zamir and Feder <ref> [38] </ref> was described by Zamir as, "one good measurement is better than many noisy ones" [39]. It is important to note that although they consider quantization noise, they do not consider consistency.
Reference: [39] <author> R. Zamir, </author> <type> personal communication, </type> <month> March 29, </month> <year> 1995. </year>
Reference-contexts: This is because many of the M additional numbers give little or no information on x. A conclusion of Zamir and Feder [38] was described by Zamir as, "one good measurement is better than many noisy ones" <ref> [39] </ref>. It is important to note that although they consider quantization noise, they do not consider consistency. These topics are discussed further in Appendix B; Appendix C explores the use of quantized frame expansion as the first stage in a lattice quantizer.
Reference: [40] <author> Z. Zhang, </author> <title> "Matching Pursuit," </title> <type> Ph.D. dissertation, </type> <institution> NYU, </institution> <year> 1993. </year>
Reference-contexts: It progressively refines a signal estimate instead of finding L components jointly. Matching pursuit was introduced to the signal processing community in the context of time-frequency analysis by Mallat and Zhang [20]. Mallat and his students have uncovered many of its properties <ref> [7, 8, 9, 40] </ref>. 3.2.1 Algorithm Let D = f' k g M k=1 H be a frame. We impose the additional constraint that k' k k = 1 for all k. We will call D our dictionary of vectors.
References-found: 40

