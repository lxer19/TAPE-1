URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P362.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts93.htm
Root-URL: http://www.mcs.anl.gov
Email: rbutler@sinkhole.unf.edu  lusk@mcs.anl.gov  
Title: Monitors, Messages, and Clusters: the p4 Parallel Programming System  
Author: Ralph M. Butler Ewing L. Lusk 
Address: Jacksonville, FL 32224  Argonne, IL 60439  
Affiliation: College of Computing Sciences and Engineering University of North Florida  Mathematics and Computer Science Division Argonne National Laboratory  
Abstract: p4 is a portable library of C and Fortran subroutines for programming parallel computers. It is the current version of a system that has been in use since 1984. It includes features for explicit parallel programming of shared-memory machines, distributed-memory machines (including heterogeneous networks of workstations), and clusters, by which we mean shared-memory multiprocessors communicating via message passing. We discuss here the design goals, history, and system architecture of p4 and describe briefly a diverse collection of applications that have demonstrated the utility of p4. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bob Beck. </author> <title> Shared-memory parallel programming in C++ (rational parmacs). </title> <booktitle> In Proceedings of the 2nd Annual Meeting, Sequent User's Resource Forum, </booktitle> <pages> pages 187-205, </pages> <year> 1988. </year> <month> 14 </month>
Reference-contexts: The version included in the book [4] had only rudimentary support for message passing in Fortran, so that was added by Dave Liebfritz at 3 Argonne. Bob Beck at Sequent did a C++ version of the monitors part <ref> [1] </ref>. Rolf Hempel at GMD greatly improved the Fortran interface and added extensive functionality on top of it for grid-based computation. He borrowed the word PARMACS, and what is now called PARMACS is his system, widely used in Europe [3].
Reference: [2] <author> M. Berry, D. Chen, P. Koss, D. Kuck, L. Pointer, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin. </author> <title> The perfect club benchmarks: Effective performance evaluation of supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <year> 1989. </year>
Reference-contexts: Portable Application Benchmarks. A number of groups are beginning to assemble collections of "real" application programs that can be used to measure performance on parallel machines. Two 11 of these such efforts are the SPLASH project at Stanford [25] and the Perfect Club Benchmarks <ref> [2] </ref>. Portable versions of the codes make them much more useful.
Reference: [3] <author> L. Bomans, D. Roose, and R. Hempel. </author> <title> The Argonne/GMD macros in FORTRAN for portable parallel programming and their implementation on the Intel iPSC/2. </title> <journal> Parallel Computing, </journal> <volume> 15 </volume> <pages> 119-132, </pages> <year> 1990. </year>
Reference-contexts: Rolf Hempel at GMD greatly improved the Fortran interface and added extensive functionality on top of it for grid-based computation. He borrowed the word PARMACS, and what is now called PARMACS is his system, widely used in Europe <ref> [3] </ref>. Robert Harrison of Argonne's Chemistry Division re-implemented the message-passing subsystem to provide more efficiency, better error handling, and a library of global operations in his system TCGMSG [13]. Many of his enhancements were later incorporated into p4.
Reference: [4] <author> James Boyle, Ralph Butler, Terrence Disz, Barnett Glickfeld, Ewing Lusk, Ross Overbeek, James Patterson, and Rick Stevens. </author> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart, and Winston, </publisher> <year> 1987. </year>
Reference-contexts: It was at the end of this period that those who had collaborated on the implementation wrote the book Portable Programs for Parallel Processors <ref> [4] </ref>, which served to publicize the system widely. The code itself was distributed with the book and also over the network. <p> The version included in the book <ref> [4] </ref> had only rudimentary support for message passing in Fortran, so that was added by Dave Liebfritz at 3 Argonne. Bob Beck at Sequent did a C++ version of the monitors part [1]. <p> There were also a great variety of new parallel machines and workstations to support. This effort, concentrated in 1990 and 1991, produced the current p4, which takes its name from the title of the 1987 book <ref> [4] </ref> and its functionality from all previous versions, going all the way back to the HEP. The interface has changed little since p4 was first released, although many performance improvements have been made, and programs written for the p4 of 1989 run essentially unchanged on machines only released in 1993.
Reference: [5] <author> Ralph Butler and Ewing Lusk. </author> <title> User's guide to the p4 parallel programming system. </title> <type> Technical Report ANL-92/17, </type> <institution> Argonne National Laboratory, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: The p4 askfor and its related routines p4 update, p4 probend, and p4 progend, provide sufficient functionality that a user may write a customized, general dispatching algorithm for a particular application. The p4 User's Guide <ref> [5] </ref> provides details and examples. Note that p4 does not implement monitors on distributed-memory machines, since then the computational model would be too far away from the hardware for the efficiency that p4 aims for. <p> It is also available through netlib. The distribution contains all the source code, a meta-makefile to build p4 on any of the machines described below, a set of examples, and a User's Guide <ref> [5] </ref>, which can be installed as an on-line help system as well, via the Gnu Emacs info mechanism.
Reference: [6] <author> Ralph M. Butler, Alan L. Leveton, and Ewing Lusk. p4-Linda: </author> <title> A portable implementation of Linda. </title> <editor> In C. S. Rajhavendra and Salim Hariri, editors, </editor> <booktitle> Proceedings of the Second International Symposium on High-Performance Distributed Computing. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year> <note> (to appear). </note>
Reference-contexts: An implementation of the Linda programming model has been done, using both shared-memory and distributed-memory models for the the underlying hardware <ref> [6] </ref>. BlockComm and Chameleon. p4 is one of the "machines" on which Bill Gropp's Chameleon system [11] runs. Thus it is possible to run a program that has been coded for the Intel iPSC/860, say, and run it on a workstation network using p4, without changing source code.
Reference: [7] <author> Tom Canfield, Mark Jones, Paul Plassmann, and Michael Tang. </author> <title> Thermal effects on the frequency response of piezoelectric crystals. In New Methods in Transient Analysis, </title> <journal> PVP-Vol. 246 and AMD-Vol. </journal> <volume> 143, </volume> <pages> pages 103-108, </pages> <address> New York, 1992. </address> <publisher> ASME. </publisher>
Reference-contexts: Heterogeneous Computing: Piezoelectric Crystals. A large finite-element code for computing resonances in piezoelectric crystals was written by Mark Jones and Paul Plassmann in conjunction with Motorola <ref> [7] </ref>. The primary computation was done on the Delta, and p4 was used for communication with the file server (a large-memory Solbourne located 2000 miles away), and a Stardent Titan for displaying graphical output. Invisible Usage: Superconductivity.
Reference: [8] <author> John Gabriel, Tim Lindholm, E. L. Lusk, and R. A. Overbeek. </author> <title> Logic programming on the HEP. </title> <editor> In Janusz S. Kowalik, editor, </editor> <title> Parallel MIMD Computation: </title> <booktitle> The HEP Supercomputer and Its Applications, </booktitle> <pages> pages 367-411. </pages> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: This system worked well, and we were able to use the HEP quite productively <ref> [8, 18] </ref>. 2.2 Monmacs As parallel processing entered the commercial marketplace, the ACRF at Argonne expanded in 1985 to include machines from Alliant, Encore, Sequent, and Intel. The HEP macros for monitors were ported to the Alliant, Encore, and Sequent and were expanded to include C as well as Fortran.
Reference: [9] <author> N. Galbreath, W. Gropp, D. Gunter, D. Levine, and G. Leaf. </author> <title> Parallel solution of the three-dimensional, time-dependent Ginzburg-Landau equation. </title> <booktitle> In Proceedings of the SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1993. </year>
Reference-contexts: Invisible Usage: Superconductivity. An example of usage of p4 without the user "knowing it" is the work reported in <ref> [9] </ref>. Here, the users are modeling vortex dynamics in high-temperature superconductors using the three-dimensional, time-dependent Ginzburg-Landau equation as a phenomenological model. The data movement between processors is done by using the the BlockComm [10] system.
Reference: [10] <author> William Gropp. </author> <title> Blockcomm for fortran. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <year> 1993. </year> <note> (to appear). </note>
Reference-contexts: An example of usage of p4 without the user "knowing it" is the work reported in [9]. Here, the users are modeling vortex dynamics in high-temperature superconductors using the three-dimensional, time-dependent Ginzburg-Landau equation as a phenomenological model. The data movement between processors is done by using the the BlockComm <ref> [10] </ref> system. The development and debugging of this code were done on Sun workstations and then moved into production on the Intel DELTA. Large Workstation Networks: Test Pattern Generation. <p> Thus it is possible to run a program that has been coded for the Intel iPSC/860, say, and run it on a workstation network using p4, without changing source code. Chameleon is the foundation for the BlockComm communication library <ref> [10] </ref>, which allows users to avoid explicit construction of messages when the messages consist of matrix subblocks. MPI. Many vendors, users, and authors of message-passing systems have organized to try to define a standard message-passing interface.
Reference: [11] <author> William Gropp and Barry Smith. </author> <title> Parallel programming tools user's manual. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <year> 1993. </year> <note> (to appear). </note>
Reference-contexts: An implementation of the Linda programming model has been done, using both shared-memory and distributed-memory models for the the underlying hardware [6]. BlockComm and Chameleon. p4 is one of the "machines" on which Bill Gropp's Chameleon system <ref> [11] </ref> runs. Thus it is possible to run a program that has been coded for the Intel iPSC/860, say, and run it on a workstation network using p4, without changing source code.
Reference: [12] <author> Per Brinch Hansen. </author> <title> The Architecture of Concurrent Programs. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1977. </year>
Reference-contexts: A monitor is an abstract data type encapsulating shared data, initialization instructions, and critical code sections. Monitors were extensively studied for their theoretical properties in the early seventies in the context of a scientific approach to operating systems <ref> [12, 15] </ref>. We have found them a durable construct for shared-memory programming.
Reference: [13] <author> R. J. Harrison. </author> <title> Portable tools and applications for parallel computers. </title> <journal> Intern. J. Quantum Chem., </journal> <volume> 40(847), </volume> <year> 1991. </year>
Reference-contexts: He borrowed the word PARMACS, and what is now called PARMACS is his system, widely used in Europe [3]. Robert Harrison of Argonne's Chemistry Division re-implemented the message-passing subsystem to provide more efficiency, better error handling, and a library of global operations in his system TCGMSG <ref> [13] </ref>. Many of his enhancements were later incorporated into p4.
Reference: [14] <author> Virginia Herrarte and Ewing Lusk. </author> <title> Studying parallel program behavior with upshot. </title> <type> Technical Report ANL-91/15, </type> <institution> Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: Parallel program visualization. Distributed with p4 is a portable library called alog for producing log files of user-specified events. These files can then be examined with a variety of tools. One that we have used extensively for some time for studying the behavior of p4 programs is upshot <ref> [14, 19] </ref>. In Figure 5 we see upshot being used to display details of a run made on the parallel automated reasoning system described above. alog is also used to instrument p4 internally.
Reference: [15] <author> C. A. R. Hoare. </author> <title> Monitors: An operating system structuring concept. </title> <booktitle> Comunications of the ACM, </booktitle> <pages> pages 549-557, </pages> <month> October </month> <year> 1974. </year>
Reference-contexts: Department of Energy, under contract W-31-109-Eng-38. The most distinguishing feature of p4 is its support for multiple models of parallel computa-tion. For the shared-memory MIMD model, it provides the monitor paradigm <ref> [15] </ref> for coordinating access to shared data, and runs on "true" shared-memory computers such as the Sequent symmetry and Alliant FX/2800, as well as NUMA (non-uniform memory access) machines that provide a shared-memory computational model, like the BBN TC-2000 and Kendall Square KSR-1. <p> A monitor is an abstract data type encapsulating shared data, initialization instructions, and critical code sections. Monitors were extensively studied for their theoretical properties in the early seventies in the context of a scientific approach to operating systems <ref> [12, 15] </ref>. We have found them a durable construct for shared-memory programming. <p> The lack of memory-management functions in 5 Fortran makes it difficult to describe monitors in a portable way, and so we dropped support for monitors in Fortran in p4, at least temporarily. They may resurface eventually (see Section 8). The fundamental monitor operations (see <ref> [15] </ref>) are provided by p4 menter, p4 mexit, p4 delay, and p4 continue. With these, users can define their own monitors in a completely portable way.
Reference: [16] <editor> Janusz S. Kowalik, editor. </editor> <title> Parallel MIMD Computation: The HEP Supercomputer and Its Applications. Scientific Computation Series. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: HEP Fortran Argonne m4 macros for monitors & messages "monmacs" PPPP book C++ version (Sequent) Argonne Fortran messages p4 Stanford SPLASH extensions TCGMSG message-passing GMD macros (Reactor sim.) (Chemistry) (grid problems) (Memory tracing) (object-oriented) 2 2.1 The Beginning In 1984 Argonne National Laboratory acquired a Denelcor HEP (Heterogeneous Element Processor) <ref> [16] </ref>, the first commercial multiprocessor and the first machine in what would become Argonne's Advanced Computing Research Facility (ACRF).
Reference: [17] <author> Peter A. Krauss and Kurt J. Antreich. </author> <title> Application of fault parallelism to the automatic test pattern generation for sequential circuits, </title> <note> 1993. (to appear in Springer LNCS). </note>
Reference-contexts: The development and debugging of this code were done on Sun workstations and then moved into production on the Intel DELTA. Large Workstation Networks: Test Pattern Generation. Peter Krauss at the Technical University of Munich has been using p4 to do test pattern generation <ref> [17] </ref> on a homogeneous network of Hewlett-Packard workstations. He has run his p4 application on as many as 102 workstations at the same time.
Reference: [18] <author> E. L. Lusk and R. A. Overbeek. </author> <title> Use of monitors in FORTRAN: a tutorial on the barrier, </title> <editor> self-scheduling DO-loop, and askfor monitors. In Janusz S. Kowalik, editor, </editor> <title> Parallel MIMD Computation: </title> <booktitle> The HEP Supercomputer and Its Applications, </booktitle> <pages> pages 367-411. </pages> <publisher> MIT Press, </publisher> <year> 1985. </year> <month> 15 </month>
Reference-contexts: This system worked well, and we were able to use the HEP quite productively <ref> [8, 18] </ref>. 2.2 Monmacs As parallel processing entered the commercial marketplace, the ACRF at Argonne expanded in 1985 to include machines from Alliant, Encore, Sequent, and Intel. The HEP macros for monitors were ported to the Alliant, Encore, and Sequent and were expanded to include C as well as Fortran.
Reference: [19] <author> Ewing L. Lusk. </author> <title> Visualizing parallel program behavior. </title> <editor> In Adrian Tentner, editor, </editor> <booktitle> High Per--formance Computing 1993: Grand Challenges in Computer Simulation, </booktitle> <pages> pages 209-213. </pages> <booktitle> The Society for Computer Simulation Simulation, </booktitle> <year> 1993. </year>
Reference-contexts: Parallel program visualization. Distributed with p4 is a portable library called alog for producing log files of user-specified events. These files can then be examined with a variety of tools. One that we have used extensively for some time for studying the behavior of p4 programs is upshot <ref> [14, 19] </ref>. In Figure 5 we see upshot being used to display details of a run made on the parallel automated reasoning system described above. alog is also used to instrument p4 internally.
Reference: [20] <author> Ewing L. Lusk and William W. McCune. </author> <title> Experiments with ROO, a parallel automated deduction system. </title> <editor> In B. Fronhoefer and G. Wrightson, editors, </editor> <booktitle> Parallelization in Inference Systems (Springer Lecture Notes in Artificial Intelligence 590), </booktitle> <pages> pages 139-162. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: The tight interweaving of indices and shared substructures makes this an intrinsically shared-memory application. We used p4's shared-memory operations, in particular the p4 askfor monitor, to implement the algorithm presented in [26] and got excellent results <ref> [20] </ref>, even for the very fine-grained parallelism necessary for this application. We developed it on a Sequent Symmetry and ran it for peak performance on an Alliant FX/2800.
Reference: [21] <author> Ewing L. Lusk and Ross A. Overbeek. </author> <title> Implementation of monitors with macros: A programming aid for the HEP and other parallel processors. </title> <type> Technical Report ANL-83-97, </type> <institution> Argonne National Laboratory, </institution> <month> December </month> <year> 1983. </year>
Reference-contexts: We have found them a durable construct for shared-memory programming. To implement monitors on the Fortran-only HEP, we defined macros for monitor operations and used the m4 macro processor to expand them on our VAX into HEP Fortran, and then transferred the code to the HEP for compilation <ref> [21] </ref>. This system worked well, and we were able to use the HEP quite productively [8, 18]. 2.2 Monmacs As parallel processing entered the commercial marketplace, the ACRF at Argonne expanded in 1985 to include machines from Alliant, Encore, Sequent, and Intel.
Reference: [22] <author> Ewing L. Lusk and Ross A. Overbeek. </author> <title> A minimalist approach to portable, parallel programming. </title> <editor> In Leah H. Jamieson, Dennis B. Gannon, and Robert J. Douglass, editors, </editor> <booktitle> The Characteristics of Parallel Algorithms, </booktitle> <pages> pages 351-362. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Ewing Lusk and Ross Overbeek chose monitors as their central paradigm for controlling access to shared data by multiple processors <ref> [22] </ref>. A monitor is an abstract data type encapsulating shared data, initialization instructions, and critical code sections. Monitors were extensively studied for their theoretical properties in the early seventies in the context of a scientific approach to operating systems [12, 15].
Reference: [23] <author> Gary Olsen, Carl Woese, Ray Hagstrom, Hideo Matsuda, and Ross Overbeek. </author> <title> Inference of phy-logenetic trees using maximum likelihood. </title> <booktitle> In Proceedings of the First Intel Delta Applications Workshop, </booktitle> <pages> pages 247-262, </pages> <year> 1992. </year>
Reference-contexts: We developed it on a Sequent Symmetry and ran it for peak performance on an Alliant FX/2800. Portable Message-Passing: Phylogenetic Trees. p4 was used to develop a parallel version of the "maximum likelihood" method of computing phylogenetic trees from RNA sequence data <ref> [23] </ref>. The largest such tree ever computed was derived using p4 and the Intel Touchstone Delta.
Reference: [24] <author> John Osterhout. </author> <title> An X11 toolkit based on the Tcl language. </title> <booktitle> In Proceedings of the Winter 1991 USENIX Conference, </booktitle> <pages> pages 105-115. </pages> <publisher> USENIX Association, </publisher> <month> January </month> <year> 1991. </year>
Reference-contexts: It is shown in Figure 2. It dynamically locates workstations on the network and makes it easy to construct a valid procgroup file and then run a p4 application with just a few mouse clicks. This system was easily built using Tcl/Tk <ref> [24] </ref>. 3.2 Shared Memory and Monitors The most primitive components of a shared-memory computational model are semaphores and locks, and these are what the vendor libraries typically supply. One can argue that programming with these concepts is somewhat like programming with branch instructions.
Reference: [25] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year> <note> Also Stanford University Technical Report No. CSL-TR-92-526, </note> <month> June </month> <year> 1992. </year>
Reference-contexts: Robert Harrison of Argonne's Chemistry Division re-implemented the message-passing subsystem to provide more efficiency, better error handling, and a library of global operations in his system TCGMSG [13]. Many of his enhancements were later incorporated into p4. The SPLASH group at Stanford built their collection of shared-memory applications <ref> [25] </ref> on monmacs, adding instrumentation macros to support assorted research projects in parallel programming. 2.4 The Current System In 1989 Ralph Butler and Ewing Lusk began a complete rewrite of the entire system, with the goal of producing a very robust system for wide distribution that included features present from the <p> Portable Application Benchmarks. A number of groups are beginning to assemble collections of "real" application programs that can be used to measure performance on parallel machines. Two 11 of these such efforts are the SPLASH project at Stanford <ref> [25] </ref> and the Perfect Club Benchmarks [2]. Portable versions of the codes make them much more useful.
Reference: [26] <author> J. Slaney and E. Lusk. </author> <title> Parallelizing the closure computation in automated deduction. </title> <editor> In M. Stickel, editor, </editor> <booktitle> Proceedings of the 10th International Conference on Automated Deduction, Lecture Notes in Artificial Intelligence, </booktitle> <volume> Vol. 449, </volume> <pages> pages 28-39, </pages> <address> New York, July 1990. </address> <publisher> Springer-Verlag. </publisher> <pages> 16 </pages>
Reference-contexts: The parallelization of our primary theorem prover presented a particular problem because of its central shared data structures. The tight interweaving of indices and shared substructures makes this an intrinsically shared-memory application. We used p4's shared-memory operations, in particular the p4 askfor monitor, to implement the algorithm presented in <ref> [26] </ref> and got excellent results [20], even for the very fine-grained parallelism necessary for this application. We developed it on a Sequent Symmetry and ran it for peak performance on an Alliant FX/2800.
References-found: 26

