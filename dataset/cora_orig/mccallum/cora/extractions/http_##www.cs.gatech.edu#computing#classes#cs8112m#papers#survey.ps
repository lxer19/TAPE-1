URL: http://www.cs.gatech.edu/computing/classes/cs8112m/papers/survey.ps
Refering-URL: http://www.cs.gatech.edu/computing/classes/cs8112m/wi95/
Root-URL: 
Title: Transport System Architecture Services for High-Performance Communications Systems  
Author: Douglas C. Schmidt and Tatsuya Suda 
Address: Irvine, CA 92717, U.S.A.  
Affiliation: Department of Information and Computer Science, University of California, Irvine  
Abstract: Providing end-to-end gigabit communication support for high-bandwidth multimedia applications requires transport systems that transfer data efficiently via network protocols such as TCP, TP4, XTP, and ST-II. This paper describes and classifies transport system services that integrate operating system resources such as CPU(s), virtual memory, and I/O devices together with network protocols to support distributed multimedia applications running on local and wide area networks. A taxonomy is presented that compares and evaluates four commercial and experimental transport systems in terms of their protocol processing support. The systems covered in this paper include System V UNIX STREAMS, the BSD UNIX networking subsystem, the x-kernel, and the Choices Conduit system. This paper is intended to navigate researchers and developers through the transport system design space by describing alternative approaches for key transport system services.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Crowcroft, I. Wakeman, Z. Wang, and D. Sirovica, </author> <title> "Is Layering Harmful?," </title> <journal> IEEE Network Magazine, </journal> <month> January </month> <year> 1992. </year>
Reference-contexts: In general, the transport system overhead is not decreasing as rapidly as the network channel speed is increasing. This results from improperly layered transport system architectures <ref> [1, 2] </ref>, and is also exacerbated by the widespread use of operating systems such as UNIX, which are not well-suited for asynchronous, interrupt-driven network communication. For example, many operating system network-to-host interfaces generate interrupts for every transmitted and received packet, which increases the amount of CPU context switching [3, 4]. <p> Layer-to-layer flow control has a significant impact on protocol performance. For example, empirical studies demonstrate the importance of matching buffer sizes and flow control strategies at each layer in the protocol family architecture <ref> [1] </ref>.
Reference: [2] <author> N. C. Hutchinson and L. L. Peterson, </author> <title> "The x-kernel: An Architecture for Implementing Network Protocols," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 17, </volume> <pages> pp. 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: In general, the transport system overhead is not decreasing as rapidly as the network channel speed is increasing. This results from improperly layered transport system architectures <ref> [1, 2] </ref>, and is also exacerbated by the widespread use of operating systems such as UNIX, which are not well-suited for asynchronous, interrupt-driven network communication. For example, many operating system network-to-host interfaces generate interrupts for every transmitted and received packet, which increases the amount of CPU context switching [3, 4]. <p> The taxonomy is used to compare and contrast four general-purpose commercial and experimental transport systems (System V UNIX STREAMS [7], the BSD UNIX network subsystem [8], the x-kernel <ref> [2] </ref>, and the Choices Conduit system [9]). The intent of the paper is to explore various design alternatives that enable system architectures to develop transport systems that support high-bandwidth applications effectively. <p> BSD UNIX sockets, System V UNIX TLI, and the V kernel UIO [13] are examples of application interfaces. Performance measurements indicate that traditional application interfaces account for approximately 30 to 40 percent of the overall transport system overhead <ref> [2, 14] </ref>. <p> Inter-protocol services involve message management, multiplexing and demultiplexing, and layer-to-layer flow control. The difference between the session architecture and protocol family architecture services corresponds 4 roughly to the x-kernel's distinction between session objects and protocol objects <ref> [2] </ref>. Session architecture services manage the end-to-end processing activities for network connections, whereas protocol family architecture services manage the layer-to-layer processing activities that occur in multi-layered protocol graphs. <p> However, protocol and session architecture components may actually reside inside the OS kernel (BSD UNIX [8], and System V UNIX [7]), in user-space (Mach [20] and the Conduit [9]), in either location (e.g., the x-kernel <ref> [2] </ref>), or in off-board processors (Nectar [21] and VMP [4]). As shown by the shaded portions of Figure 2, this paper focuses on the protocol and kernel architectures. <p> In order to produce efficient transport systems, it is important to match the selected process architecture with the appropriate concurrency model. In particular, it appears that LWPs are a more appropriate mechanism for implementing process architectures than are HWPs, since minimizing context switching overhead is essential for high-performance <ref> [2] </ref>. (2) Process Architecture Models: Process architectures are implemented using the concurrency models described above. Figure 3 illustrates two basic process architecture models: horizontal and vertical. 2 These two models differ in terms of their structure and their performance. <p> In this vertical approach, a separate process is associated with each incoming and outgoing message <ref> [2, 5] </ref>. Each process escorts its message through the protocol graph within a single address space, delivering the message "down" to a network interface or "up" to an application. <p> REQ. APPLICATION INTERFACE (2) TASK PARALLELISM NETWORK INTERFACE CHECKSUM MESSAGE QUEUES CONTROL FLOW PROCESSING ELEMENTS between layers in different processes incurs additional context switch overhead <ref> [2] </ref>. Finally, vertical process architectures do not impose a total ordering on messages bound for the same session. <p> This is particularly problematic when system communication loads are high and message arrival and departure times are close together. One solution is to cache processes in a "process pool" and recycle them for subsequent messages <ref> [2] </ref>. However, these cached processes may sit idle when overall system communication activity is light. This "ties up" OS resources like memory buffers and process table entries, which may be needed by other OS subsystems and applications. <p> For instance, the BSD message management facility divides its buffers into 112 byte and 1,024 byte blocks. This implementation leads to non-uniform performance behavior when incoming and outgoing messages vary in size between small and large blocks. As discussed in <ref> [2] </ref>, more uniform performance curves may occur if message managers support a wide range of message sizes as efficiently as they do large and/or small messages. (2) Data Copy Avoidance: As mentioned in Section 3.1.3, memory-to-memory copying is a significant source of transport system overhead. <p> In general, the choice of search algorithms and caching optimizations impact overall transport system and protocol performance significantly. Hashing, combined with caching, produces a measurable improvement when searching large lists of control blocks corresponding to active network connections <ref> [2] </ref>. 3.2.3 The Layer-to-Layer Flow Control Dimension Layer-to-layer flow control occurs between various levels in a transport system. <p> However, since the BSD kernel is single-threaded, only one process is permitted to run as incoming messages are passed up to "higher" protocol layers. 4.1.3 x-kernel The x-kernel is a modular, extensible transport system kernel architecture designed to support prototyping and experimentation with alternative protocol and session architectures <ref> [2] </ref>. It was developed to demonstrate that layering and modularity is not inherently detrimental to network protocol performance [24]. The x-kernel supports protocol graphs that implement a wide range of standard and experimental protocol families, including TCP/IP, Sun RPC, Sprite RCP, VMTP, NFS, and Psync [26]. <p> Table 4 illustrates the transport system profile for the x-kernel. The x-kernel's protocol family architecture provides highly uniform interfaces to its services, which manage three communication abstractions that comprise protocol graphs <ref> [2] </ref>: protocol objects, session objects, and message objects.
Reference: [3] <author> Z. Haas, </author> <title> "A Protocol Structure for High-Speed Communication Over Broadband ISDN," </title> <journal> IEEE Network Magazine, </journal> <pages> pp. 64-70, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: For example, many operating system network-to-host interfaces generate interrupts for every transmitted and received packet, which increases the amount of CPU context switching <ref> [3, 4] </ref>. Moreover, despite increasing total MIPS, RISC CPUs often penalize interrupt-driven network communication by exhibiting high context switching overhead. This overhead results from the cost of flushing pipelines, storing and retrieving register windows, and invalidating instruction/data caches and virtual memory "translation-lookaside buffers" [5]. <p> In this model, each protocol layer is encapsulated by one or more lightweight or heavy-weight processes (depicted as dotted-lines that surround the protocol components). These processes send and receive messages in a "pipelined" 2 Different authors use these two terms in different ways. For example, <ref> [3] </ref> uses the terms in a nearly opposite sense to describe the HOPS (horizontally oriented protocols) architecture. <p> Various alternatives have been proposed for mapping horizontal and vertical process architectures onto multiple PEs <ref> [3, 5, 27, 28] </ref>. Several criteria must be considered when mapping a process architecture onto multiple PEs [5]. First, the selected process architecture should support a large degree of paralleliza-tion. <p> Therefore, it is difficult to completely eliminate the overhead from memory contention and synchronization. Proposed strategies for handling these interdependencies include pipelining the message processing [27, 28] and/or computing multiple tasks in parallel, discarding the final results if errors are detected at intermediate stages <ref> [3, 29] </ref>. A variation of layer and task parallelism is called directional parallelism. This approach dedicates two PEs per-protocol layer, one for sending messages and another for receiving messages. Although this model is also relatively simple to design, it provides only a multiplicative increase in parallelism compared to layer parallelism.
Reference: [4] <author> H. Kanakia and D. R. Cheriton, </author> <title> "The VMP Network Adapter Board (NAB): High-Performance Network Communication for Multiprocessors," </title> <booktitle> in SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <address> (Stanford, CA), </address> <pages> pp. 175-187, </pages> <publisher> ACM, </publisher> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: For example, many operating system network-to-host interfaces generate interrupts for every transmitted and received packet, which increases the amount of CPU context switching <ref> [3, 4] </ref>. Moreover, despite increasing total MIPS, RISC CPUs often penalize interrupt-driven network communication by exhibiting high context switching overhead. This overhead results from the cost of flushing pipelines, storing and retrieving register windows, and invalidating instruction/data caches and virtual memory "translation-lookaside buffers" [5]. <p> However, protocol and session architecture components may actually reside inside the OS kernel (BSD UNIX [8], and System V UNIX [7]), in user-space (Mach [20] and the Conduit [9]), in either location (e.g., the x-kernel [2]), or in off-board processors (Nectar [21] and VMP <ref> [4] </ref>). As shown by the shaded portions of Figure 2, this paper focuses on the protocol and kernel architectures.
Reference: [5] <author> J. Jain, M. Schwartz, and T. Bashkow, </author> <title> "Transport Protocol Processing at GBPS Rates," </title> <booktitle> in SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <address> (Philadelphia, PA), </address> <pages> pp. 188-199, </pages> <publisher> ACM, </publisher> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: Moreover, despite increasing total MIPS, RISC CPUs often penalize interrupt-driven network communication by exhibiting high context switching overhead. This overhead results from the cost of flushing pipelines, storing and retrieving register windows, and invalidating instruction/data caches and virtual memory "translation-lookaside buffers" <ref> [5] </ref>. Ameliorating the throughput preservation problem and providing very high data transfer rates to applications and protocols requires an integrated solution that involves redesigning transport system architectures [6]. <p> In this vertical approach, a separate process is associated with each incoming and outgoing message <ref> [2, 5] </ref>. Each process escorts its message through the protocol graph within a single address space, delivering the message "down" to a network interface or "up" to an application. <p> Note that there are other ways to organize vertical process architectures such as connectional parallelism shown in Vertical process architectures have several advantages compared to horizontal approaches. First, there is greater potential for exploiting available parallelism, since every arriving and departing message may be associated with its own process <ref> [5] </ref>. This increased parallelism also facilitates load balancing, which may improve overall transport system throughput. For example, if processes are implemented carefully on a multiprocessor, each incoming message may be dispatched to an available processing element. <p> Various alternatives have been proposed for mapping horizontal and vertical process architectures onto multiple PEs <ref> [3, 5, 27, 28] </ref>. Several criteria must be considered when mapping a process architecture onto multiple PEs [5]. First, the selected process architecture should support a large degree of paralleliza-tion. <p> Various alternatives have been proposed for mapping horizontal and vertical process architectures onto multiple PEs [3, 5, 27, 28]. Several criteria must be considered when mapping a process architecture onto multiple PEs <ref> [5] </ref>. First, the selected process architecture should support a large degree of paralleliza-tion. All other factors held equal, a process architecture that utilizes only two PEs is not likely to scale up as well as one that utilizes dozens of PEs effectively. <p> This synchronization overhead results from mutual exclusion primitives that serialize access to shared resources (such as memory buffers and session control blocks used to reassemble protocol segments bound for the same higher-layer session). Additional overhead may also occur from memory and bus contention between the multiple PEs <ref> [5] </ref>. from "coarse-grained" to "fine-grained") and their process architecture (i.e., horizontal vs. vertical). Granularity is a function of both the task size associated with each PE and the number of PEs involved. <p> as they are received from network interfaces, (2) adding and/or removing headers and trailers from messages as they flow through the protocol graph, (3) fragmenting and reassembling messages to fit into network maximum transmission units, (4) storing messages in buffers for transmission or retransmission, and (5) reordering messages received out-of-sequence <ref> [5] </ref>.
Reference: [6] <author> J. Sterbenz and G. Parulkar, "AXON: </author> <title> Application-Oriented Lightweight Transport Protocol Design," </title> <booktitle> in ICCC'90, </booktitle> <address> (New Delhi, India), </address> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Ameliorating the throughput preservation problem and providing very high data transfer rates to applications and protocols requires an integrated solution that involves redesigning transport system architectures <ref> [6] </ref>. To help system developers and researchers navigate through the transport system design space, this paper presents a taxonomy of six key transport system services.
Reference: [7] <author> UNIX Software Operations, </author> <title> UNIX System V Release 4 Programmer's Guide: STREAMS. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: To help system developers and researchers navigate through the transport system design space, this paper presents a taxonomy of six key transport system services. The taxonomy is used to compare and contrast four general-purpose commercial and experimental transport systems (System V UNIX STREAMS <ref> [7] </ref>, the BSD UNIX network subsystem [8], the x-kernel [2], and the Choices Conduit system [9]). The intent of the paper is to explore various design alternatives that enable system architectures to develop transport systems that support high-bandwidth applications effectively. <p> Note that the term "kernel architecture" is used in this paper to identify services that form the "nucleus" of the transport system. However, protocol and session architecture components may actually reside inside the OS kernel (BSD UNIX [8], and System V UNIX <ref> [7] </ref>), in user-space (Mach [20] and the Conduit [9]), in either location (e.g., the x-kernel [2]), or in off-board processors (Nectar [21] and VMP [4]). As shown by the shaded portions of Figure 2, this paper focuses on the protocol and kernel architectures.
Reference: [8] <author> S. J. Leffler, M. McKusick, M. Karels, and J. Quarter-man, </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: To help system developers and researchers navigate through the transport system design space, this paper presents a taxonomy of six key transport system services. The taxonomy is used to compare and contrast four general-purpose commercial and experimental transport systems (System V UNIX STREAMS [7], the BSD UNIX network subsystem <ref> [8] </ref>, the x-kernel [2], and the Choices Conduit system [9]). The intent of the paper is to explore various design alternatives that enable system architectures to develop transport systems that support high-bandwidth applications effectively. <p> Note that the term "kernel architecture" is used in this paper to identify services that form the "nucleus" of the transport system. However, protocol and session architecture components may actually reside inside the OS kernel (BSD UNIX <ref> [8] </ref>, and System V UNIX [7]), in user-space (Mach [20] and the Conduit [9]), in either location (e.g., the x-kernel [2]), or in off-board processors (Nectar [21] and VMP [4]). As shown by the shaded portions of Figure 2, this paper focuses on the protocol and kernel architectures. <p> invokes the associated event handler. 11 HORIZONTAL VERTICAL C A S G A N PROCESS ARCHITECTURE GRANULARITY OF PROCESSES AND TASKS LAYER PARALLELISM CONNECTIONAL PARALLELISM MESSAGE PARALLELISM TASK PARALLELISM F I N G A Different event management mechanisms include delta lists [30], timing wheels [31], and heap-based [32] and list-based <ref> [8] </ref> callout queues. The following two dimensions differentiate these mechanisms: (1) Search Structure ADT: Several data structures are commonly used to implement the different event management ADT mechanisms. One simple approach sorts the events by their time-to-execute value and stores them in an array [33]. <p> Moreover, remapping may not be useful if the sender or receiver writes on the page immediately, since a separate copy must be generated anyway <ref> [8] </ref>. <p> Virtual Memory Integration none Message Buffering (1) non-uniform, (2) list-based Mulitplexing/Demultiplexing (1) hybrid, (2) layered, (3) sequential-search, (4) single-item Flow Control ND Table 3: BSD UNIX Profile 4.1.2 BSD UNIX BSD UNIX provides a transport system framework that supports multiple protocol families such as the Internet, XNS, and OSI protocols <ref> [8] </ref>. BSD provides a general-purpose application interface called sockets. Unlike the standard UNIX pipe and signal IPC mechanisms, sockets allow bi-directional communication of arbitrary amounts of data between unrelated processes on local and remote hosts. Table 3 illustrates the transport system profile for BSD UNIX.
Reference: [9] <author> J. M. Zweig, </author> <title> "The Conduit: a Communication Abstraction in C++," </title> <booktitle> in USENIX C++ Conference Proceedings, </booktitle> <pages> pp. 191-203, </pages> <publisher> USENIX Association, </publisher> <month> April </month> <year> 1990. </year>
Reference-contexts: The taxonomy is used to compare and contrast four general-purpose commercial and experimental transport systems (System V UNIX STREAMS [7], the BSD UNIX network subsystem [8], the x-kernel [2], and the Choices Conduit system <ref> [9] </ref>). The intent of the paper is to explore various design alternatives that enable system architectures to develop transport systems that support high-bandwidth applications effectively. <p> However, protocol and session architecture components may actually reside inside the OS kernel (BSD UNIX [8], and System V UNIX [7]), in user-space (Mach [20] and the Conduit <ref> [9] </ref>), in either location (e.g., the x-kernel [2]), or in off-board processors (Nectar [21] and VMP [4]). As shown by the shaded portions of Figure 2, this paper focuses on the protocol and kernel architectures.
Reference: [10] <author> S. W. O'Malley and L. L. Peterson, </author> <title> "A Dynamic Network Architecture," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 10, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: This framework coordinates both the hardware resources and software abstractions that implement protocol graphs <ref> [10] </ref>. A protocol graph expresses the hierarchical relations between protocols in protocol families such as the Internet, OSI, XNS, and SNA. For example, Figure 1 depicts a protocol graph containing certain Internet and OSI protocols. <p> vertical (process-per-msg) Event Management (1) relative, (2) linked list Virtual Memory Remapping complete Message Buffering (1) uniform, (2) DAG-based Multiplexing/Demultiplexing (1) synchronous, (2) layered, (3) hashing, (4) single-item Flow Control per-process Table 4: x-kernel Profile request-response RPC mechanisms, and a "blast" algorithm that uses selective retransmission to reduce channel utilization <ref> [10] </ref>. The following paragraphs describe the x-kernel's primary software components: * Protocol Objects: Protocol objects are software abstractions that represent network protocols in the x-kernel. Protocol objects belong to one of two "realms," either the asynchronous realm (e.g., TCP, IP, UDP) or the synchronous realm (e.g., RPC). <p> However, the x-kernel's protocol family architecture framework does not provide standard mechanisms for "end-to-end" session architecture activities such as connection management, error detection, or end-to-end flow control. A related project, Avoca, builds upon the basic x-kernel facilities to provide these end-to-end services <ref> [10] </ref>. * Message Objects: Message objects encapsulate control information and user data that flows "upwards" or "downwards" through the graph of session and protocol objects. In order to decrease memory-to-memory copying and to implement message operations efficiently, message objects are implemented using a "directed-acyclic-graph" (DAG)-based data structure.
Reference: [11] <author> D. D. Clark and D. L. Tennenhouse, </author> <title> "Architectural Considerations for a New Generation of Protocols," </title> <booktitle> in SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <address> (Philadelphia, PA), </address> <pages> pp. 200-208, </pages> <publisher> ACM, </publisher> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: Each virtual machine level is characterized by the services it exports to the surrounding levels. For example, the model depicted in Figure 2 represents an abstraction of hardware and software services common to existing transport systems. Although certain transport systems bypass or combine adjacent levels for performance reasons <ref> [11, 12] </ref>, Figure 2 provides a concise model of the relationships between major transport system services. <p> This is advantageous for network protocols that (1) require only partial orderings between messages (such as the Psync protocol [26] that uses partial orderings to implement efficient "many-to-many" group communication) or (2) utilize application level framing <ref> [11] </ref>, which maintains application data unit boundaries throughout layered protocol processing stages. Vertical process architectures also possess several disadvantages, however. First, performance may suffer if the OS kernel is unable to efficiently associate a process with each message. <p> In general, the cost of copying data provides an upper bound on user application throughput <ref> [11] </ref>. Selecting an efficient message management mechanism is one method for reducing copying overhead (see Section 3.2.1 below). A related approach uses virtual memory optimizations to avoid copying data altogether. <p> For example, most remapping schemes align data in contiguous buffers that begin on page boundaries. Ensuring this alignment may be complicated by certain protocol operations such as message de-encapsulation (i.e., stripping headers 12 and trailers as messages migrate up a protocol graph), presentation layer expansion <ref> [11] </ref> (e.g., uncompressing or decrypting an incoming message), and variable-size header options (such as those proposed to handle TCP options for long-delay paths [16]). Moreover, remapping may not be useful if the sender or receiver writes on the page immediately, since a separate copy must be generated anyway [8]. <p> Demultiplexing is typically more complicated than multiplexing since senders possess knowledge of their entire transfer context such as message destination address (es) (e.g., connection identifiers, port numbers, and/or Internet IP addresses) and which network interfaces to use <ref> [11] </ref>. On the other hand, when a network controller receives an incoming message, it generally has no prior knowledge of the message's validity or eventual destination. To obtain this information, the receiver must inspect the message header and perform demultiplexing operations that select which higher-layer protocols should receive the message.
Reference: [12] <author> D. L. Tennenhouse, </author> <title> "Layered Multiplexing Considered Harmful," </title> <booktitle> in Proceedings of the 1st International Workshop on High-Speed Networks, </booktitle> <month> May </month> <year> 1989. </year>
Reference-contexts: Each virtual machine level is characterized by the services it exports to the surrounding levels. For example, the model depicted in Figure 2 represents an abstraction of hardware and software services common to existing transport systems. Although certain transport systems bypass or combine adjacent levels for performance reasons <ref> [11, 12] </ref>, Figure 2 provides a concise model of the relationships between major transport system services. <p> The sender typically performs multiplexing, which directs outgoing messages emanating from some number of higher-layer sessions onto a smaller number of lower-layer sessions <ref> [12] </ref>. The receiver typically performs demultiplexing, which directs incoming messages up to their associated sessions. Multiplexing and demultiplexing are orthogonal to data copying; depending on the message management scheme, messages need not be copied as they multiplex and demultiplex throughout the protocol graph [35]. <p> This layered approach differs from the de-layered approach shown in Figure 7 (2). In the de-layered approach, multiplexing and/or demultiplexing is performed only once, usually at either the highest- or lowest-layer of the protocol graph. Layered multiplexing and demultiplexing has several advantages <ref> [12] </ref>. First, it promotes modularity, since layers interoperate only at well-defined "service access points" (SAPs). This enables services offered at one layer to be developed independently from other layers. Second, it conserves lower-layer resources like active virtual circuits by sharing them among higher-layer sessions. <p> De-layered multiplexing and demultiplexing is an alternative approach that generally decreases jitter since there is less contention for transport system resources at a single lower-layer SAP from multiple higher-layer data streams <ref> [12] </ref>. De-layered approaches are well-suited for both vertical process architectures and connectional parallelism. Likewise, in a horizontal process architecture, the de-layered approach reduces both the number of processes and protocol layers, which may reduce the context switching overhead. However, the de-layered approach also has several disadvantages. <p> Second, the amount of context information stored within every intermediate protocol layer increases since sessions are replicated and not shared <ref> [12] </ref>. (3) Searching: Implementing multiplexing and demultiplexing schemes involves some type of searching. The search key represents an external identifier (such as a network address, port number, or type-of-service field).
Reference: [13] <author> D. R. Cheriton, "UIO: </author> <title> A Uniform I/O System Interface for Distributed Systems," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 5, </volume> <pages> pp. 12-46, </pages> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: The application interface exchanges data and control information with session architecture services that perform connection management, option negotiation, data transmission control, and error protection. BSD UNIX sockets, System V UNIX TLI, and the V kernel UIO <ref> [13] </ref> are examples of application interfaces. Performance measurements indicate that traditional application interfaces account for approximately 30 to 40 percent of the overall transport system overhead [2, 14]. <p> END POINTS, SEND, RECEIVE, AND BUFFER DATA AND CONTROL INFO. SESSION ARCHITECTURE CONNECTION MANAGEMENT, RELIABILITY MANAGEMENT, END-TO-END FLOW CONTROL, PRESENTATION SERVICES level transport system services. <ref> [13, 15] </ref> evaluate the functionality and performance of several application interfaces. 2.2 Session Architecture The second outer-most level of the transport system is the session architecture. The session architecture provides "end-to-end" network services to applications. Session architecture services are associated with protocol sessions, which are local end-points of distributed communication.
Reference: [14] <author> M. S. Atkins, S. T. Chanson, and J. B. Robinson, </author> <title> "LNTP An Efficient Transport Protocol for Local Area Networks," </title> <booktitle> in Proceedings of the Conference on Global Communications (GLOBECOM), </booktitle> <pages> pp. 705-710, </pages> <year> 1988. </year>
Reference-contexts: BSD UNIX sockets, System V UNIX TLI, and the V kernel UIO [13] are examples of application interfaces. Performance measurements indicate that traditional application interfaces account for approximately 30 to 40 percent of the overall transport system overhead <ref> [2, 14] </ref>.
Reference: [15] <author> M. D. Maggio and D. W. Krumme, </author> <title> "A Flexible System Call Interface for Interprocess Communication in a Distributed Memory Multicomputer," </title> <journal> Operating Systems Review, </journal> <volume> vol. 25, </volume> <pages> pp. 4-21, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: END POINTS, SEND, RECEIVE, AND BUFFER DATA AND CONTROL INFO. SESSION ARCHITECTURE CONNECTION MANAGEMENT, RELIABILITY MANAGEMENT, END-TO-END FLOW CONTROL, PRESENTATION SERVICES level transport system services. <ref> [13, 15] </ref> evaluate the functionality and performance of several application interfaces. 2.2 Session Architecture The second outer-most level of the transport system is the session architecture. The session architecture provides "end-to-end" network services to applications. Session architecture services are associated with protocol sessions, which are local end-points of distributed communication.
Reference: [16] <author> V. Jacobson, R. Braden, and D. </author> <title> Borman, "TCP Extensions for High Performance," </title> <booktitle> Network Information Center RFC 1323, </booktitle> <pages> pp. 1-37, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Session architecture services help satisfy end-to-end application quality-of-service requirements. These requirements involve levels of throughput, latency, and reliability that are essential for efficient application performance over high-performance networks <ref> [16] </ref>. <p> may be complicated by certain protocol operations such as message de-encapsulation (i.e., stripping headers 12 and trailers as messages migrate up a protocol graph), presentation layer expansion [11] (e.g., uncompressing or decrypting an incoming message), and variable-size header options (such as those proposed to handle TCP options for long-delay paths <ref> [16] </ref>). Moreover, remapping may not be useful if the sender or receiver writes on the page immediately, since a separate copy must be generated anyway [8].
Reference: [17] <author> T. F. L. Porta and M. Schwartz, </author> <title> "Architectures, Features, and Implementation of High-Speed Transport Protocols," </title> <journal> IEEE Network Magazine, </journal> <pages> pp. 14-22, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In addition, session architecture services also manage protocol interpreters (e.g., controlling transitions in a transport protocol's state machine) and presentation services (e.g., encryption, compression, and network byte-ordering conversions). <ref> [17, 18, 19] </ref> survey various session architecture issues in greater detail. 2.3 Protocol Family Architecture The protocol family architecture provides intra-protocol and inter-protocol services that commonly occur between and within nodes in a protocol graph.
Reference: [18] <author> W. A. Doeringer, D. Dykeman, M. Kaiserswerth, B. W. Meister, H. Rudin, and R. Williamson, </author> <title> "A Survey of Light-Weight Transport Protocols for High-Speed Networks," </title> <journal> IEEE Transactions on Communication, </journal> <volume> vol. 38, </volume> <pages> pp. 2025-2039, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: In addition, session architecture services also manage protocol interpreters (e.g., controlling transitions in a transport protocol's state machine) and presentation services (e.g., encryption, compression, and network byte-ordering conversions). <ref> [17, 18, 19] </ref> survey various session architecture issues in greater detail. 2.3 Protocol Family Architecture The protocol family architecture provides intra-protocol and inter-protocol services that commonly occur between and within nodes in a protocol graph.
Reference: [19] <author> L. Svobodova, </author> <title> "Implementing OSI Systems," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. SAC-7, </volume> <pages> pp. 1115-1130, </pages> <month> Sept. </month> <year> 1989. </year>
Reference-contexts: In addition, session architecture services also manage protocol interpreters (e.g., controlling transitions in a transport protocol's state machine) and presentation services (e.g., encryption, compression, and network byte-ordering conversions). <ref> [17, 18, 19] </ref> survey various session architecture issues in greater detail. 2.3 Protocol Family Architecture The protocol family architecture provides intra-protocol and inter-protocol services that commonly occur between and within nodes in a protocol graph.
Reference: [20] <author> M. Accetta, R. Baron, D. Golub, R. Rashid, A. Teva-nian, and M. Young, </author> <title> "Mach: A New Kernel Foundation for UNIX Development," </title> <booktitle> in Proceedings of the Summer 1986 USENIX Technical Conference and Exhibition, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: Note that the term "kernel architecture" is used in this paper to identify services that form the "nucleus" of the transport system. However, protocol and session architecture components may actually reside inside the OS kernel (BSD UNIX [8], and System V UNIX [7]), in user-space (Mach <ref> [20] </ref> and the Conduit [9]), in either location (e.g., the x-kernel [2]), or in off-board processors (Nectar [21] and VMP [4]). As shown by the shaded portions of Figure 2, this paper focuses on the protocol and kernel architectures. <p> A process consists of a collection of resources, along with one or more threads of control <ref> [20] </ref>. Process resources include CPU (s), virtual memory, file and I/O device descriptors, and access rights to other OS resources. Threads of control act as separate instruction pointers within a shared address space.
Reference: [21] <author> E. C. Cooper, P. A. Steenkiste, R. D. Sansom, and B. D. Zill, </author> <title> "Protocol Implementation on the Nectar Communication Processor," </title> <booktitle> in SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <address> (Philadelphia, PA), </address> <pages> pp. 135-144, </pages> <publisher> ACM, </publisher> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: However, protocol and session architecture components may actually reside inside the OS kernel (BSD UNIX [8], and System V UNIX [7]), in user-space (Mach [20] and the Conduit [9]), in either location (e.g., the x-kernel [2]), or in off-board processors (Nectar <ref> [21] </ref> and VMP [4]). As shown by the shaded portions of Figure 2, this paper focuses on the protocol and kernel architectures.
Reference: [22] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy, </author> <title> "Scheduler Activiation: Effective Kernel Support for the User-Level Management of Parallelism," </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pp. 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Therefore, HWPs may not be an effective choice for executing multiple fine-grain protocol processing activities concurrently. * Light-Weight Processes: Light-weight processes (LWPs) are often referred to as threads. LWPs may be implemented in kernel-space, user-space, or some hybrid configuration <ref> [22] </ref>. Unlike HWPs, multiple LWPs usually share an address space.
Reference: [23] <author> D. D. Clark, </author> <title> "The Structuring of Systems Using Up-calls," </title> <booktitle> in Proceedings of the Tenth Symposium on Operating Systems Principles, (Shark Is., </booktitle> <address> WA), </address> <year> 1985. </year>
Reference-contexts: These processes send and receive messages in a "pipelined" 2 Different authors use these two terms in different ways. For example, [3] uses the terms in a nearly opposite sense to describe the HOPS (horizontally oriented protocols) architecture. Our use of the horizontal/vertical distinction is consistent with <ref> [23, 24, 25] </ref>. 7 P RPC ETHERNET DRIVER UDP (1) HORIZONTAL FDDI DRIVER P A A (2) VERTICAL APPLICATIONS PRESENTATION AND SESSION PROTOCOLS TRANSPORT PROTOCOLS NETWORK PROTOCOLS DEVICE DRIVERS LOGICAL AND/OR PHYSICAL PROCESSES PROTOCOLS AND SESSIONS MESSAGE QUEUES SUBROUTINE CALLS LEGEND A A A 2 fashion. <p> The x-kernel also supports other context switch optimizations that (1) allow user processes to transform into kernel processes via system calls when sending message and (2) allow kernel processes to transform into user processes via upcalls when receiving messages <ref> [23] </ref>. 21 Process Architecture (1) LWP, (2) hybrid (process-per-buffer) Event Management ND Virtual Memory Integration none Message Buffering (1) uniform, (2) list-based Mulitplexing/Demultiplexing (1) ND, (2) layered, (3) ND, (4) ND Flow Control ND Table 5: Conduit Profile 4.1.4 The Choices Conduit The Conduit provides the protocol family architecture, session architecture,
Reference: [24] <author> S. W. O'Malley and L. L. Peterson, </author> <title> "A Highly Layered Architecture for High-Speed Networks," </title> <booktitle> in Proceedings of the 2nd International Workshop on High-Speed Networks, </booktitle> <month> November </month> <year> 1990. </year>
Reference-contexts: These processes send and receive messages in a "pipelined" 2 Different authors use these two terms in different ways. For example, [3] uses the terms in a nearly opposite sense to describe the HOPS (horizontally oriented protocols) architecture. Our use of the horizontal/vertical distinction is consistent with <ref> [23, 24, 25] </ref>. 7 P RPC ETHERNET DRIVER UDP (1) HORIZONTAL FDDI DRIVER P A A (2) VERTICAL APPLICATIONS PRESENTATION AND SESSION PROTOCOLS TRANSPORT PROTOCOLS NETWORK PROTOCOLS DEVICE DRIVERS LOGICAL AND/OR PHYSICAL PROCESSES PROTOCOLS AND SESSIONS MESSAGE QUEUES SUBROUTINE CALLS LEGEND A A A 2 fashion. <p> It was developed to demonstrate that layering and modularity is not inherently detrimental to network protocol performance <ref> [24] </ref>. The x-kernel supports protocol graphs that implement a wide range of standard and experimental protocol families, including TCP/IP, Sun RPC, Sprite RCP, VMTP, NFS, and Psync [26].
Reference: [25] <author> M. S. Atkins, </author> <title> "Experiments in SR with Different Upcall Program Structures," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 6, </volume> <pages> pp. 365-392, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: These processes send and receive messages in a "pipelined" 2 Different authors use these two terms in different ways. For example, [3] uses the terms in a nearly opposite sense to describe the HOPS (horizontally oriented protocols) architecture. Our use of the horizontal/vertical distinction is consistent with <ref> [23, 24, 25] </ref>. 7 P RPC ETHERNET DRIVER UDP (1) HORIZONTAL FDDI DRIVER P A A (2) VERTICAL APPLICATIONS PRESENTATION AND SESSION PROTOCOLS TRANSPORT PROTOCOLS NETWORK PROTOCOLS DEVICE DRIVERS LOGICAL AND/OR PHYSICAL PROCESSES PROTOCOLS AND SESSIONS MESSAGE QUEUES SUBROUTINE CALLS LEGEND A A A 2 fashion. <p> Memory-to-memory copying should be minimized to improve performance when passing messages between processes. Horizontal process architectures have several advantages. First, their close correspondence to layered protocol specifications simplifies their design and implementation <ref> [25] </ref>. Second, each protocol component manages its active sessions within one process address space. This reduces synchronization complexity when multiple messages are bound for the same session, since only a single process controls the internal data structures of a particular protocol component. However, horizontal process architectures also have several disadvantages.
Reference: [26] <author> L. L. Peterson, N. Buchholz, and R. D. Schlicht-ing, </author> <title> "Preserving and Using Context Information in Interprocess Communication," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 7, </volume> <pages> pp. 217-246, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Finally, vertical process architectures do not impose a total ordering on messages bound for the same session. This is advantageous for network protocols that (1) require only partial orderings between messages (such as the Psync protocol <ref> [26] </ref> that uses partial orderings to implement efficient "many-to-many" group communication) or (2) utilize application level framing [11], which maintains application data unit boundaries throughout layered protocol processing stages. Vertical process architectures also possess several disadvantages, however. <p> It was developed to demonstrate that layering and modularity is not inherently detrimental to network protocol performance [24]. The x-kernel supports protocol graphs that implement a wide range of standard and experimental protocol families, including TCP/IP, Sun RPC, Sprite RCP, VMTP, NFS, and Psync <ref> [26] </ref>. Unlike BSD UNIX, whose protocol family architecture is characterized by a static, relatively monolithic protocol graph, the x-kernel supports dynamic, highly-layered protocol graphs. Table 4 illustrates the transport system profile for the x-kernel.
Reference: [27] <author> M. Zitterbart, </author> <title> "High-Speed Transport Components," </title> <journal> IEEE Network Magazine, </journal> <pages> pp. 54-63, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Various alternatives have been proposed for mapping horizontal and vertical process architectures onto multiple PEs <ref> [3, 5, 27, 28] </ref>. Several criteria must be considered when mapping a process architecture onto multiple PEs [5]. First, the selected process architecture should support a large degree of paralleliza-tion. <p> In practice, however, many protocol processing tasks are interdependent, particularly those involving error detection. Therefore, it is difficult to completely eliminate the overhead from memory contention and synchronization. Proposed strategies for handling these interdependencies include pipelining the message processing <ref> [27, 28] </ref> and/or computing multiple tasks in parallel, discarding the final results if errors are detected at intermediate stages [3, 29]. A variation of layer and task parallelism is called directional parallelism. This approach dedicates two PEs per-protocol layer, one for sending messages and another for receiving messages.
Reference: [28] <author> D. Giarrizzo, M. Kaiserswerth, T. Wicki, and R. Williamson, </author> <title> "High-Speed Parallel Protocol Implementations," </title> <booktitle> in Proceedings of the 1st International Workshop on High-Speed Networks, </booktitle> <pages> pp. 165-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Various alternatives have been proposed for mapping horizontal and vertical process architectures onto multiple PEs <ref> [3, 5, 27, 28] </ref>. Several criteria must be considered when mapping a process architecture onto multiple PEs [5]. First, the selected process architecture should support a large degree of paralleliza-tion. <p> In practice, however, many protocol processing tasks are interdependent, particularly those involving error detection. Therefore, it is difficult to completely eliminate the overhead from memory contention and synchronization. Proposed strategies for handling these interdependencies include pipelining the message processing <ref> [27, 28] </ref> and/or computing multiple tasks in parallel, discarding the final results if errors are detected at intermediate stages [3, 29]. A variation of layer and task parallelism is called directional parallelism. This approach dedicates two PEs per-protocol layer, one for sending messages and another for receiving messages. <p> Although this model is also relatively simple to design, it provides only a multiplicative increase in parallelism compared to layer parallelism. Another disadvantage occurs for protocols that require cooperation between their sending and receiving PEs <ref> [28] </ref>. For example, TCP "piggy-backs" acknowledgments for incoming segments on outgoing data segments and control messages, thereby introducing communication overhead between the sending and receiving PEs.
Reference: [29] <author> T. F. L. Porta and M. Schwartz, </author> <title> "Design, Verification, and Analysis of a High Speed Protocol Parallel Implementation Architecture," </title> <booktitle> in Proceedings of the IEEE Workshop on the Architecture and Implementation of High Performance Communication Subsystems, </booktitle> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: Therefore, it is difficult to completely eliminate the overhead from memory contention and synchronization. Proposed strategies for handling these interdependencies include pipelining the message processing [27, 28] and/or computing multiple tasks in parallel, discarding the final results if errors are detected at intermediate stages <ref> [3, 29] </ref>. A variation of layer and task parallelism is called directional parallelism. This approach dedicates two PEs per-protocol layer, one for sending messages and another for receiving messages. Although this model is also relatively simple to design, it provides only a multiplicative increase in parallelism compared to layer parallelism.
Reference: [30] <author> D. E. Comer, </author> <title> Internetworking with TCP/IP Vol II: Design, Implementation, and Internals. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1991. </year> <month> 26 </month>
Reference-contexts: or more events must be run, the event manager invokes the associated event handler. 11 HORIZONTAL VERTICAL C A S G A N PROCESS ARCHITECTURE GRANULARITY OF PROCESSES AND TASKS LAYER PARALLELISM CONNECTIONAL PARALLELISM MESSAGE PARALLELISM TASK PARALLELISM F I N G A Different event management mechanisms include delta lists <ref> [30] </ref>, timing wheels [31], and heap-based [32] and list-based [8] callout queues. The following two dimensions differentiate these mechanisms: (1) Search Structure ADT: Several data structures are commonly used to implement the different event management ADT mechanisms. <p> One simple approach sorts the events by their time-to-execute value and stores them in an array [33]. A variant on this approach (used by delta lists and list-based callout queues) replaces the array with a sorted linked list, thereby reducing the overhead of adding or deleting an arbitrary event <ref> [30] </ref>. A third approach (used by heap-based callout queues) employs a priority queue ADT called a "heap" [32]. Using a heap instead of a sorted list or array reduces the average- and worst-case time complexity for inserting or deleting an entry from O (n) to O (lg n).
Reference: [31] <author> G. Varghese and T. Lauck, </author> <title> "Hashed and Hierarchical Timing Wheels: Data Structures for the Efficient Implementation of a Timer Facility," </title> <booktitle> in The Proceedings of the 11th Symposium on Operating System Principles, </booktitle> <month> November </month> <year> 1987. </year>
Reference-contexts: must be run, the event manager invokes the associated event handler. 11 HORIZONTAL VERTICAL C A S G A N PROCESS ARCHITECTURE GRANULARITY OF PROCESSES AND TASKS LAYER PARALLELISM CONNECTIONAL PARALLELISM MESSAGE PARALLELISM TASK PARALLELISM F I N G A Different event management mechanisms include delta lists [30], timing wheels <ref> [31] </ref>, and heap-based [32] and list-based [8] callout queues. The following two dimensions differentiate these mechanisms: (1) Search Structure ADT: Several data structures are commonly used to implement the different event management ADT mechanisms. <p> These abstractions are supported by several other reusable software components, including a message manager (an abstract data type that encapsulates messages exchanged between session and protocol objects), a map manager (used for demultiplexing incoming messages between adjacent protocols and sessions), and an event manager (based upon timing wheels <ref> [31] </ref> and used for timer-driven activities like TCP's adaptive retransmission algorithm). In addition, the x-kernel provides a standard library of micro-protocols, which are reusable, modular software components that implement services common to many network protocols. <p> System V, on the other hand, maintains a heap-based callout table, rather than a sorted list or array. The heap-based implementation outperforms the linked-list approach under heavy loads [32]. The x-kernel uses timing wheels <ref> [31] </ref> instead of either callout lists or heaps. The Virtual Memory Remapping Dimension: Recent versions of x-kernel provide virtual memory remapping [35] for transferring messages between application process and the kernel.
Reference: [32] <author> R. E. Barkley and T. P. Lee, </author> <title> "A Heap-Based Call-out Implementation to Meet Real-Time Needs," </title> <booktitle> in Usenix 1988 Summer Conference, </booktitle> <pages> pp. 213-222, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: the event manager invokes the associated event handler. 11 HORIZONTAL VERTICAL C A S G A N PROCESS ARCHITECTURE GRANULARITY OF PROCESSES AND TASKS LAYER PARALLELISM CONNECTIONAL PARALLELISM MESSAGE PARALLELISM TASK PARALLELISM F I N G A Different event management mechanisms include delta lists [30], timing wheels [31], and heap-based <ref> [32] </ref> and list-based [8] callout queues. The following two dimensions differentiate these mechanisms: (1) Search Structure ADT: Several data structures are commonly used to implement the different event management ADT mechanisms. One simple approach sorts the events by their time-to-execute value and stores them in an array [33]. <p> A variant on this approach (used by delta lists and list-based callout queues) replaces the array with a sorted linked list, thereby reducing the overhead of adding or deleting an arbitrary event [30]. A third approach (used by heap-based callout queues) employs a priority queue ADT called a "heap" <ref> [32] </ref>. Using a heap instead of a sorted list or array reduces the average- and worst-case time complexity for inserting or deleting an entry from O (n) to O (lg n). <p> System V, on the other hand, maintains a heap-based callout table, rather than a sorted list or array. The heap-based implementation outperforms the linked-list approach under heavy loads <ref> [32] </ref>. The x-kernel uses timing wheels [31] instead of either callout lists or heaps. The Virtual Memory Remapping Dimension: Recent versions of x-kernel provide virtual memory remapping [35] for transferring messages between application process and the kernel.
Reference: [33] <author> M. J. Bach, </author> <title> The Design of the UNIX Operating System. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1986. </year>
Reference-contexts: The following two dimensions differentiate these mechanisms: (1) Search Structure ADT: Several data structures are commonly used to implement the different event management ADT mechanisms. One simple approach sorts the events by their time-to-execute value and stores them in an array <ref> [33] </ref>. A variant on this approach (used by delta lists and list-based callout queues) replaces the array with a sorted linked list, thereby reducing the overhead of adding or deleting an arbitrary event [30].
Reference: [34] <author> R. W. Watson and S. A. Mamrak, </author> <title> "Gaining Efficiency in Transport Services by Appropriate Design and Implementation Choices," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 5, </volume> <pages> pp. 97-120, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: is typically used in a heap-based implementation, due to the "partially-ordered, almost-complete binary tree" properties necessary to maintain the heap ADT. 3.1.3 The Virtual Memory Remapping Dimension Regardless of the process architecture, minimizing the amount of memory-to-memory copying in a transport system is essential to achieve efficient network protocol performance <ref> [34] </ref>. In general, the cost of copying data provides an upper bound on user application throughput [11]. Selecting an efficient message management mechanism is one method for reducing copying overhead (see Section 3.2.1 below). A related approach uses virtual memory optimizations to avoid copying data altogether.
Reference: [35] <author> S. W. O'Malley, M. B. Abbott, N. C. Hutchinson, and L. L. Peterson, </author> <title> "A Transparent Blast Facility," </title> <journal> Journal of Internetworking, </journal> <volume> vol. 1, </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: Page remapping techniques are particularly useful for transferring large quantities of data between separate address spaces on the same host machine. A common transport system operation that benefits from this technique involves transferring messages between user-space and kernel-space at the application interface level <ref> [35] </ref>. However, page remapping schemes are often difficult to implement effectively. For example, most remapping schemes align data in contiguous buffers that begin on page boundaries. <p> Naive message managers that physically copy messages between each protocol layer are prohibitively expensive. Therefore, more sophisticated implementations avoid data copying via techniques such as buffer-cut-through [36, 37] and copy-on-write optimizations <ref> [35] </ref>. Buffer-cut-through passes messages "by reference" through multiple protocol layers to avoid excessive memory-to-memory copying. Likewise, copy-on-write techniques use lazy evaluation, reference counting, and buffer-sharing to avoid making unnecessary copies. Moreover, these schemes may be combined with the virtual memory remapping optimizations described in Section 3.1.3. <p> Adding data to the front or rear of a buffer list only involves relinking pointers, and does not require any data copying. The x-kernel employs an alternative approach using a directed-acyclic-graph (DAG)-based data structure <ref> [35] </ref>. <p> The receiver typically performs demultiplexing, which directs incoming messages up to their associated sessions. Multiplexing and demultiplexing are orthogonal to data copying; depending on the message management scheme, messages need not be copied as they multiplex and demultiplex throughout the protocol graph <ref> [35] </ref>. Demultiplexing is typically more complicated than multiplexing since senders possess knowledge of their entire transfer context such as message destination address (es) (e.g., connection identifiers, port numbers, and/or Internet IP addresses) and which network interfaces to use [11]. <p> In order to decrease memory-to-memory copying and to implement message operations efficiently, message objects are implemented using a "directed-acyclic-graph" (DAG)-based data structure. This DAG-based scheme uses "lazy-evaluation" to avoid unnecessary data copying when passing messages between protocol layers <ref> [35] </ref>. It also stores message headers in a separate "header stack" and uses pointer arithmetic on this stack to reduce the cost of prepending or stripping message headers. The x-kernel employs a "process-per-message" vertical process architecture that resides in either the OS kernel or in user-space. <p> The heap-based implementation outperforms the linked-list approach under heavy loads [32]. The x-kernel uses timing wheels [31] instead of either callout lists or heaps. The Virtual Memory Remapping Dimension: Recent versions of x-kernel provide virtual memory remapping <ref> [35] </ref> for transferring messages between application process and the kernel.
Reference: [36] <author> C. M. Woodside and J. R. Montealegre, </author> <title> "The Effect of Buffering Strategies on Protocol Execution Performance," </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. 37, </volume> <pages> pp. 545-554, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Naive message managers that physically copy messages between each protocol layer are prohibitively expensive. Therefore, more sophisticated implementations avoid data copying via techniques such as buffer-cut-through <ref> [36, 37] </ref> and copy-on-write optimizations [35]. Buffer-cut-through passes messages "by reference" through multiple protocol layers to avoid excessive memory-to-memory copying. Likewise, copy-on-write techniques use lazy evaluation, reference counting, and buffer-sharing to avoid making unnecessary copies.
Reference: [37] <author> X. Zhang and A. P. Seneviratne, </author> <title> "An Efficient Implementation of High-Speed Protocol without Data Copying," </title> <booktitle> in Proceedings of the 15 th Conference on Local Computer Networks, </booktitle> <address> (Minneapolis, MN), </address> <pages> pp. 443-450, </pages> <publisher> IEEE, </publisher> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: Naive message managers that physically copy messages between each protocol layer are prohibitively expensive. Therefore, more sophisticated implementations avoid data copying via techniques such as buffer-cut-through <ref> [36, 37] </ref> and copy-on-write optimizations [35]. Buffer-cut-through passes messages "by reference" through multiple protocol layers to avoid excessive memory-to-memory copying. Likewise, copy-on-write techniques use lazy evaluation, reference counting, and buffer-sharing to avoid making unnecessary copies.
Reference: [38] <author> A. Fraser, </author> <title> "The Universal Receiver Protocol," </title> <booktitle> in Proceedings of the 1st International Workshop on High-Speed Networks, </booktitle> <pages> pp. 19-25, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Depending on the process architecture, demultiplexing activities may also incur high synchronization and context switching overhead, since multiple processes may need to be awakened, scheduled, and executed. These factors help explain why receivers, rather than senders, are often performance bottlenecks in distributed systems <ref> [38, 39] </ref> As described below, four key multiplexing and demultiplexing dimensions include synchronization, layering, searching, and caching. (1) Synchronization: Multiplexing and demultiplexing may occur either synchronously or asynchronously, depending primarily on whether a horizontal or vertical process architecture is employed (see Figure 3).
Reference: [39] <author> G. J. Holzmann, </author> <title> Design and Validation of Computer Protocols. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: Depending on the process architecture, demultiplexing activities may also incur high synchronization and context switching overhead, since multiple processes may need to be awakened, scheduled, and executed. These factors help explain why receivers, rather than senders, are often performance bottlenecks in distributed systems <ref> [38, 39] </ref> As described below, four key multiplexing and demultiplexing dimensions include synchronization, layering, searching, and caching. (1) Synchronization: Multiplexing and demultiplexing may occur either synchronously or asynchronously, depending primarily on whether a horizontal or vertical process architecture is employed (see Figure 3).
Reference: [40] <author> D. F. Box, D. C. Schmidt, and T. Suda, </author> <title> "Alternative Approaches to ATM/Internet Interoperation," </title> <booktitle> in IEEE Workshop on the Architecture and Implementation of High Performance Communication Subsystems, </booktitle> <pages> pp. 1-5, </pages> <publisher> IEEE, </publisher> <year> 1992. </year>
Reference-contexts: Second, it conserves lower-layer resources like active virtual circuits by sharing them among higher-layer sessions. Such sharing may be useful for high-volume, wide-area, leased-line communication links, where it is expensive to reestablish a dedicated virtual circuit for each transmitted message <ref> [40] </ref>. Finally, layered approaches may be useful for coordinating related streams in multimedia applications (such as synchronized voice and video), since messages synchronize at each SAP boundary. The main disadvantages of layered approaches arise from the additional multiplexing and demultiplexing processing incurred at each layer.
Reference: [41] <author> P. E. McKenney and K. F. Dove, </author> <title> "Efficient Demulti-plexing of Incoming TCP Packets," </title> <type> Tech. Rep. </type> <institution> SQN TR92-01, Sequent Computer Systems, Inc., </institution> <month> Decem-ber </month> <year> 1991. </year>
Reference-contexts: If applications form "message-trains" (where a sequence of back-to-back messages are destined for the same higher-level session), then a single-item control block cache is a relatively efficient, straight-forward implementation <ref> [41] </ref>. However, single-item caching is insufficient when applications do not form message-trains. <p> On the other hand, its TCP implementation uses sequential search with a one-item cache to demultiplex incoming messages to the appropriate connection session. As described in Section 3.2.2, this implementation is inefficient when application data arrival patterns do not form message-trains <ref> [41] </ref>. The Layer-to-Layer Flow Control Dimension: With the exception of System V STREAMS, the surveyed transport systems do not provide uniform layer-to-layer flow control mechanisms. Each STREAM module contains high- and low-watermarks that manage flow control between adjacent modules.
Reference: [42] <author> D. Ritchie, </author> <title> "A Stream Input-Output System," </title> <journal> AT&T Bell Labs Technical Journal, </journal> <volume> vol. 63, </volume> <pages> pp. 311-324, </pages> <month> Oct. </month> <year> 1984. </year>
Reference-contexts: protocols and local 16 Process Architecture (1) coroutines, (2) horizontal (process-per-module) Event Management (1) absolute, (2) heap Virtual Memory Remapping none Message Buffering (1) uniform, (2) list-based Multiplexing/Demultiplexing (1) asynchronous, (2) layered, (3) ND, (4) ND Flow Control per-queue Table 2: STREAMS Profile IPC via multiplexor drivers and STREAM pipes <ref> [42] </ref>. The Table 2 illustrates the transport system profile for System V STREAMS.
Reference: [43] <author> R. Campbell, V. Russo, and G. Johnson, </author> <title> "The Design of a Multiprocessor Operating System," </title> <booktitle> in USENIX C++ Conference Proceedings, </booktitle> <pages> pp. 109-126, </pages> <publisher> USENIX Association, </publisher> <month> November </month> <year> 1987. </year>
Reference-contexts: Event Management ND Virtual Memory Integration none Message Buffering (1) uniform, (2) list-based Mulitplexing/Demultiplexing (1) ND, (2) layered, (3) ND, (4) ND Flow Control ND Table 5: Conduit Profile 4.1.4 The Choices Conduit The Conduit provides the protocol family architecture, session architecture, and application interface for the Choices operating system <ref> [43] </ref>.
Reference: [44] <author> J. M. Zweig and R. Johnson, </author> <title> "Delegation in C++," </title> <journal> Journal of Object-Oriented Programming, </journal> <pages> pp. 31-34, </pages> <month> November/December </month> <year> 1991. </year>
Reference-contexts: Choices is being developed to study the characteristics of object-oriented techniques for the design and implementation of OS kernel and networking services. 3 For example, the design of ZOOT (the Choices TCP/IP implementation) uses object-oriented language constructs and design methods such as inheritance, dynamic binding, and delegation <ref> [44] </ref> to implement the TCP state machine in a highly modular fashion. Together, Choices and the Conduit provide a general-purpose transport system. Table 5 illustrates the transport system profile for the Choices Conduit.
Reference: [45] <author> D. C. Schmidt, D. F. Box, and T. Suda, </author> <title> "ADAPTIVE: </title>
Reference-contexts: Our research group at University of California, Irvine is currently using this taxonomy to guide our research on highly modular and efficient transport system designs <ref> [45] </ref>. We are building an environment for developing, experimenting with, and analyzing various strategies that associate network protocol and session configurations with high-performance networks.
References-found: 45

