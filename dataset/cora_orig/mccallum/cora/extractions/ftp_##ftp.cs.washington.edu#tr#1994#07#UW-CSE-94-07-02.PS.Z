URL: ftp://ftp.cs.washington.edu/tr/1994/07/UW-CSE-94-07-02.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: System Support for Efficient Network Communication  
Author: Chandramohan A. Thekkath 
Date: July 17, 1994  
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  
Pubnum: Technical Report 94-07-02  
Abstract: This technical report is an adaptation of C. Thekkath's Ph.D. dissertation. Our research program is supported in part by the National Science Foundation under Grants No. CCR-8703049, CCR-8619663, and CCR-8907666, CDA-9123308, and CCR-9200832, the Washington Technology Center, Digital Equipment Corporation, Boeing Computer Services, Intel Corporation, Hewlett-Packard Corporation, and Apple Computer. C. Thekkath was supported in part by a fellowship from Intel Corporation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Advanced Micro Devices. </author> <title> Am7990 Local Area Network Controller for Ethernet (LANCE), </title> <year> 1986. </year>
Reference-contexts: Some well-known examples of controllers include the LANCE <ref> [1] </ref>, the VMP-NAB [38], and the Nectar CAB [5]. Typically, controllers buffer network packets, provide checksum and encryption, and have sophisticated data transfer capabilities between the network and host memory (such as scatter-write and gather-read). Some even have on-board implementations of entire protocol stacks. <p> Each network is accessed using a different type of controller. The networks are driven using DECstation 5000/200s and SparcStation I workstations. 10 2.1.1 Ethernet The Ethernet is a 10 Mbit/sec CSMA/CD local-area network, which is accessed on the DECstations and SparcStations by a LANCE controller <ref> [1] </ref>. The controller is packaged differently on the two machines. On the DECstations, the controller cannot do DMA to or from host memory; instead, it contains a 128 Kbyte on-board packet buffer memory into which the host places a correctly formatted packet for transmission [27]. <p> Even with a hardware implementation of the model, this functionality can be provided. In fact, even very early network controllers, such as the Ethernet LANCE, have a facility to swap bytes during data transfers between host memory and the network <ref> [1] </ref>. Other kinds of heterogeneity, such as different word sizes or different floating point formats, are more difficult to deal with.
Reference: [2] <author> Anant Agarwal, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, Dan Nussbaum, Mike Parkin, and Donald Yeung. </author> <title> The MIT Alewife machine: A large-scale distributed-memory multiprocessor. </title> <type> Technical Report MIT/LCS Memo TM-454, </type> <institution> Laboratory for Computer Science, MIT, </institution> <year> 1991. </year>
Reference-contexts: Software emulation has a flexibility advantage over a pure hardware approach because application-specific operations can be easily added. Additional performance gains are also possible by redesigning the network to use the memory or cache bus instead, as is done in dedicated multiprocessors such as Alewife <ref> [2] </ref>. In the final analysis though, the network access model is only an intermediate step. That is, the performance of the network access model is only important in terms of its contribution to the overall performance of applications that use it.
Reference: [3] <author> Thomas E. Anderson, Henry M. Levy, Brian N. Bershad, and Edward D. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Interrupt handling cost has two components: (1) the CPU-dependent cost of vectoring the interrupt and (2) the controller-dependent cost of servicing the interrupt. Previous research has studied interrupt vectoring costs on RISC processors <ref> [3, 55] </ref> and so we shall examine only the second component. The objective is to compare a simple FIFO interface such as that found 20 in the ATM controller with a more elaborate descriptor interface such as that found in the Ethernet or the FDDI controllers. <p> Finally, it is clear that as ATM networks speed up into the gigabit range, the cost of transferring data is going to be significantly lowered. However, the cost of control transfer, which involves context switching overheads, does not appear to scale with processor speeds <ref> [3] </ref>. Thus, a network access model such as ours, which separates the notion of control transfer from data transfer, is a key advantage in next-generation workstation cluster environments.
Reference: [4] <author> Thomas E. Anderson, Susan S. Owicki, James B. Saxe, and Charles P. Thacker. </author> <title> High-speed switch scheduling for local-area networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 319-352, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: This meshes well with the notion of loads and stores supported by our network access model. Second, switch-based local-area ATM networks are getting extremely reliable. For instance, there are ATM networks currently in existence that guarantee against packet loss once a packet is injected into the network <ref> [4] </ref>. This implies, for example, that a set of cooperating applications distributed on a local-area network can use the network access model as though they were cooperating processes sharing memory on a single processor. <p> In our prototype, PVCs are set up to reserve 50% of the link bandwidth. We expect this level of bandwidth reservation to provide adequate protection against cell loss due to congestion. In more advanced switches, e.g., AN2 <ref> [4] </ref>, with hardware flow and congestion control mechanisms, bandwidth reservation may not be needed for protection against cell loss. <p> We wish to build tightly-integrated distributed system clusters, consisting of a modest number of high-performance workstations communicating within a single LAN-connected administrative domain. Newer LAN technologies include hardware flow-control and bandwidth reservation schemes that can guarantee that data packets are delivered reliably <ref> [4, 5] </ref>. We therefore feel justified in treating data loss within the cluster as an extremely rare occurrence, and regard it as a catastrophic event.
Reference: [5] <author> Emmanuel A. Arnould, Francois J. Bitz, Eric C. Cooper, H.T. Kung, Robert D. Sansom, and Peter A. Steenkiste. </author> <title> The design of Nectar: A network backplane for heterogeneous multicomputers. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 205-216, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Some well-known examples of controllers include the LANCE [1], the VMP-NAB [38], and the Nectar CAB <ref> [5] </ref>. Typically, controllers buffer network packets, provide checksum and encryption, and have sophisticated data transfer capabilities between the network and host memory (such as scatter-write and gather-read). Some even have on-board implementations of entire protocol stacks. In recent years, technology has improved the performance of the network. <p> Typically, controllers provide support for checksumming and data transfer mechanisms, e.g., direct memory access (DMA) between the network and host memory. In some cases, controllers have embedded processors that can be used to implement entire transport protocols <ref> [5, 38] </ref>. In this chapter we evaluate the overhead added to cross-machine communication from two important sources: (1) the network controller hardware and (2) low level controller software, memory, and CPU interfaces. <p> Given different network and processor technologies, the relative importance of these components may change; by isolating the components, we can see how the performance of each scales with technology change. Controllers targeted for high throughput have been well studied in the past <ref> [5, 24, 38, 68] </ref>. However, latency is often the overriding concern in distributed systems, and consequently, for the most part, this chapter considers the impact of the controller on this aspect of communication performance. <p> We wish to build tightly-integrated distributed system clusters, consisting of a modest number of high-performance workstations communicating within a single LAN-connected administrative domain. Newer LAN technologies include hardware flow-control and bandwidth reservation schemes that can guarantee that data packets are delivered reliably <ref> [4, 5] </ref>. We therefore feel justified in treating data loss within the cluster as an extremely rare occurrence, and regard it as a catastrophic event. <p> This led to several studies that tried to reduce the cost of software processing, e.g., [17], [28], [72] and many others. Reduced protocol costs led to research on the structure of network controllers and other host-architectural effects, e.g., <ref> [5] </ref>, [23], [38], [66] to name a few. The advent of a new generation of ATM networks has, in turn, resulted in many new research efforts, including ours, aimed at higher performance network communication. The work described in this thesis shares similarities with many previous studies.
Reference: [6] <author> Brian N. Bershad. </author> <title> High Performance Cross-Address Space Communication. </title> <type> Ph.D. thesis, </type> <institution> University of Washington, </institution> <month> June </month> <year> 1990. </year> <institution> Department of Computer Science and Engineering Technical Report 90-06-02. </institution>
Reference-contexts: Ultrix RPC 3 software has not been particularly optimized and thus, on a network that is over ten times faster, the performance of small packet exchanges does not change significantly. Unfortunately, small data exchange is a common occurrence in distributed systems <ref> [6] </ref>. In short, the advantages gained from increased network hardware performance can be almost completely masked without proper software design. Designing software mechanisms so that applications can efficiently use the next generation of high-speed networks is a difficult challenge.
Reference: [7] <author> Brian N. Bershad, Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> Lightweight remote procedure call. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 37-55, </pages> <month> February </month> <year> 1990. </year> <month> 104 </month>
Reference-contexts: The kernel directly supports simple-valued types such as words, bytes, halfwords, and pointers to bytes. Using the template, the kernel synthesizes a marshaling procedure. In many cases, marshaling is typically simple and involves only assignments and byte copying <ref> [7] </ref>. Thus, the task of synthesizing a procedure is nothing more than assembling the right sequence of primitive instructions. The marshaling procedure contains code to check the validity of each input argument passed at run time. <p> Whenever possible, this communication is done using the remote read/write data transfer mechanism. that for the most part, control transfers are restricted between a client and its server-clerk. That is, control transfers are primarily intra-node cross-domain calls, which have been shown to be amenable to high-performance implementation <ref> [7, 42] </ref>. Notice also that our organization maintains the firewalls between untrusted clients and services and the abstractional convenience of procedural interfaces, without relying on conventional mechanisms like RPC for cross-machine communication.
Reference: [8] <author> Brian N. Bershad, Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> User-level interprocess communication for shared memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(2) </volume> <pages> 175-198, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: An advantage of the remote memory model, which provides protected, remote memory that can be shared between a client and server, is that it is natural to extend to the cross-machine case the optimization techniques used for high-performance same-machine RPC, such as URPC <ref> [8] </ref>. Following the URPC approach, in our system the server exports stacks that are then imported by clients at bind time. On an RPC call, the client stub picks an available stack for the server and builds a call frame on that stack using the remote write operation.
Reference: [9] <author> Andrew Birrell, Greg Nelson, Susan Owicki, and Edward Wobber. </author> <title> Network objects. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 217-230, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Further, the SHRIMP design does not focus on issues of separating control and data transfer in distributed systems. 8.9 Channel Model, V System, Network Objects The distributed system structure we propose is loosely related to the Channel Model [31], Network Objects <ref> [9] </ref>, and other systems, like V [16], that use RPC for small data and a separate bulk data transport mechanism. Unlike most of these systems, in our model, there is no explicit activity or thread of control at the destination process to handle an incoming stream of data.
Reference: [10] <author> Andrew D. Birrell. </author> <title> Secure communication using remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(1) </volume> <pages> 1-14, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: For instance, many environments routinely share files through NFS servers with exactly the same guarantees. However, there are environments where such trust may not be warranted. Distributed systems running in these environments have traditionally used encryption techniques to ensure authentication and security <ref> [10, 63] </ref>. The underpinning for such schemes is that data is encrypted and decrypted using secret or public key schemes. The choice of the particular communication primitiveRPC or remote memoryis irrelevant. With the memory-based access model, this implies that each read and write has to be encrypted and decrypted.
Reference: [11] <author> Andrew D. Birrell, Roy Levin, Roger M. Needham, and Michael D. Schroeder. Grapevine: </author> <title> An exercise in distributed computing. </title> <journal> Communications of the ACM, </journal> <volume> 25(4) </volume> <pages> 260-274, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Our use of clerks has much in common with earlier systems, e.g., Grapevine <ref> [11] </ref>. For example, the client is not aware that it is talking to a server's clerk. It sees the same abstract RPC interface, albeit local RPC. However, there are some important differences in our use of clerks. <p> Second, it argues that distributed systems can be structured differently by separating the notions of control transfer and data transfer. 8.2 Grapevine In its use of clerks, our distributed system design has something in common with Grapevine <ref> [11] </ref>. We have already discussed the relationship between the two systems in Chapter 7, so we only mention the important differences here. Our clerks and servers are more tightly integrated, they are part of the same application, and they trust each other.
Reference: [12] <author> Andrew D. Birrell and Bruce Jay Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: High-speed networks and fast processors are necessary but not sufficient by themselves. We illustrate this point below with a simple example. Distributed systems are typically structured using remote procedure call (RPC) as the cross-machine communication mechanism <ref> [12, 51] </ref>. Figure 1.1 shows the performance of the Ultrix remote procedure call system between a pair of otherwise idle 25 MHz DECstations making small packet RPCs. <p> Conventional RPC design is layered. Even high performance RPC implementations adhere to this layering principle (except, perhaps, to do upcalls from the device level directly to the higher levels) <ref> [12, 58, 66] </ref>. For example, the stubs view the RPC transport as a provider of reliable packets or linear memory buffers. The stubs translate between user specified entities (integer, records, arrays) and the packets using marshaling code. <p> In the absence of processor-initiated DMA to the FIFO, this is the best possible scenario. 5.3 Transport RPC systems have typically relied on reliable packet exchange protocol similar to the one first used at Xerox <ref> [12] </ref>. The packet exchange protocol is layered directly over network datagrams for communicating within the same physical network, or on top of internetwork datagrams such as UDP/IP for communicating across multiple networks. <p> Traditional RPC transports were designed for the Ethernet, where typical RPC call arguments and results fit into a single packet (about 1500 bytes). Consequently RPC protocols have been optimized for single packet exchanges <ref> [12] </ref>. 59 In the standard RPC packet exchange protocol, the client-side transport sends a network packet containing its arguments to the server. (In the common case, RPC arguments occupy less space than what the network packet can support.) In the absence of any errors, the server's response packet containing the call
Reference: [13] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Zezary Dubnicki, and Edward W. Felten. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: This is a clear tradeoff that allows us to use commodity parts at low cost and with high flexibility. In contrast, machines like SHRIMP 1 provide uniform local and remote access instructions, but at the expense of customized hardware <ref> [13] </ref>. Second, our model does not provide automatic memory coherence as most shared-memory machines do. Once again, we believe this is a reasonable tradeoff of performance for functionality. <p> make a quantitative comparison of the two designs because, at the time of writing, there is no implementation nor any simulation results based on the paper design described in [74]. 8.8 SHRIMP The network interface for the SHRIMP multicomputer being designed at Princeton also proposes direct remote writes to memory <ref> [13] </ref>. Like Hamlyn and our work, the SHRIMP approach shares a common ancestry with Spector and VAXclusters. However, the main focus of SHRIMP is (1) to provide hardware-supported memory coherence between multi-computer nodes and (2) permit the overlap of communication and computation at the level of individual instructions.
Reference: [14] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The effectiveness of this scheme is highly influenced by the communication performance between client and server. As another example, better communication would greatly facilitate the use of networked workstations as a loosely-coupled multiprocessor or shared virtual memory system <ref> [14, 41] </ref>. Careful software design is essential if we are to exploit the advances in hardware. High-speed networks and fast processors are necessary but not sufficient by themselves. We illustrate this point below with a simple example.
Reference: [15] <author> David R. Cheriton and Carey L. Williamson. </author> <title> VMTP as the transport layer for high-performance distributed systems. </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(6) </volume> <pages> 37-44, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: This scheme is based on the 60 notion of blast protocols and selective acknowledgements <ref> [15, 18] </ref>, and is used in RAPID. A fixed number of cells (say N ) are grouped into an acknowledgment unit. These cells are written to the remote site without waiting for an acknowledgement.
Reference: [16] <author> David R. Cheriton. </author> <title> The V kernel: A software base for distributed systems. </title> <journal> IEEE Software, </journal> <volume> 1(2) </volume> <pages> 19-42, </pages> <month> April </month> <year> 1984. </year> <month> 105 </month>
Reference-contexts: Distributed services are programmed using various distributed programming models. Perhaps the most common example of a distributed programming model is remote procedure call (RPC). Variations of RPC include message passing as seen in the V system <ref> [16] </ref> and CSP [33]. Another example of a distributed programming model is distributed shared memory (DSM) such as Ivy [41]. Distributed programming models are implemented on top of a lower level abstraction referred to, in this thesis, as a network access model. <p> Further, the SHRIMP design does not focus on issues of separating control and data transfer in distributed systems. 8.9 Channel Model, V System, Network Objects The distributed system structure we propose is loosely related to the Channel Model [31], Network Objects [9], and other systems, like V <ref> [16] </ref>, that use RPC for small data and a separate bulk data transport mechanism. Unlike most of these systems, in our model, there is no explicit activity or thread of control at the destination process to handle an incoming stream of data.
Reference: [17] <author> David D. Clark, Van Jacobson, John Romkey, and Howard Salwen. </author> <title> An analysis of TCP processing overhead. </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(6) </volume> <pages> 23-36, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: For example, through software design and increased processor speeds, it should be possible to reduce the software protocol processing time. In fact, even early studies of the Internet TCP protocol determined that protocol processing per se was not an insurmountable source of latency <ref> [17] </ref>. Further, with the increased network speed, it should be possible to rapidly transfer data between the source and destination machines. However, this hypothesis is too simplistic because it ignores an important component of potential cross-machine communication overheadthe network controller. <p> The decision to copy or map in will depend on whether the processor is required to touch every byte of the packet or not. For instance, if it is necessary to calculate a software checksum, then an integrated copy and checksum loop (as proposed in <ref> [17, 19] </ref>) would suggest that mapping is of limited benefit. However, on next-generation networks under certain conditions, there is evidence to suggest that a software checksum may be of limited use [75]. <p> We view this as an application of the principle of Integrated Layer Processing (ILP) that can be used to structure communication protocols for better performance [19]. Some other examples of ILP are combining data copying and checksum calculations in TCP/IP <ref> [17, 75] </ref>, and exploiting 55 application semantics to avoid extra copies of data for UDP/IP [44]. There are many similarities between the structure of cross-machine RPC using the memory-based network access model and the structure of same-machine RPC on shared memory (shown in Figure 5.1c). <p> At one stage in the state-of-the-art, software protocols were considered the key factor in performance. This led to several studies that tried to reduce the cost of software processing, e.g., <ref> [17] </ref>, [28], [72] and many others. Reduced protocol costs led to research on the structure of network controllers and other host-architectural effects, e.g., [5], [23], [38], [66] to name a few.
Reference: [18] <author> David D. Clark, Mark L. Lambert, and Lixia Zhang. NETBLT: </author> <title> A high throughput transport protocol. </title> <booktitle> In Proceedings of the 1987 SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 353-359, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: This scheme is based on the 60 notion of blast protocols and selective acknowledgements <ref> [15, 18] </ref>, and is used in RAPID. A fixed number of cells (say N ) are grouped into an acknowledgment unit. These cells are written to the remote site without waiting for an acknowledgement.
Reference: [19] <author> David D. Clark and David L. Tennenhouse. </author> <title> Architectural considerations for a new generation of protocols. </title> <booktitle> In Proceedings of the 1990 SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 200-208, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The decision to copy or map in will depend on whether the processor is required to touch every byte of the packet or not. For instance, if it is necessary to calculate a software checksum, then an integrated copy and checksum loop (as proposed in <ref> [17, 19] </ref>) would suggest that mapping is of limited benefit. However, on next-generation networks under certain conditions, there is evidence to suggest that a software checksum may be of limited use [75]. <p> In particular, addressing and demultiplexing are done in a uniform fashion (e.g., using &lt;segments, offset&gt; pairs). We view this as an application of the principle of Integrated Layer Processing (ILP) that can be used to structure communication protocols for better performance <ref> [19] </ref>. Some other examples of ILP are combining data copying and checksum calculations in TCP/IP [17, 75], and exploiting 55 application semantics to avoid extra copies of data for UDP/IP [44].
Reference: [20] <author> Douglas Comer and James Griffioen. </author> <title> A new design for distributed systems: The remote memory model. </title> <booktitle> In Proceedings of the Summer 1990 USENIX Conference, </booktitle> <pages> pages 127-135, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This change, in turn, should enable us to build a new generation of 2 novel, high-performance distributed applications. Consider the following two motivating examples. In a cluster of networked workstations, paging to spare memory lying idle on other nodes could be much faster than paging to disk <ref> [20] </ref>. A distributed server could manage the idle memory in the network; on a page fault, the page fault handler would communicate with the server and transfer pages to or from remote nodes. The effectiveness of this scheme is highly influenced by the communication performance between client and server.
Reference: [21] <author> Michael D. Dahlin, Clifford J. Mather, Randolph Y. Wang, Thomas E. Anderson, and David A. Patterson. </author> <title> A quantitative analysis of cache policies for scalable network file systems. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS and PERFORMANCE Conference, </booktitle> <pages> pages 150-160, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: are in use in current distributed file systems, e.g., the Spritely NFS system [62], Sprite [52], and AFS [34] all use cache-consistency schemes. (Even NFS has a cache-consistency policy, albeit a weaker one.) Traditional multiprocessor cache-coherency schemes can also be used, as suggested by the work on the xFS protocol <ref> [21] </ref>. Although our file system model does not explicitly account for coherency traffic, we believe coherency schemes can be built using our communication primitives and our file system model. For example, workstation-cluster file system designs such as Calypso [50] use an RPC-based distributed token management scheme to handle cache coherence.
Reference: [22] <author> William J. Dally, Linda Chao, Andrew Chien, Soha Hassoun, Waldemar Horwat, Jon Kaplan, Paul Song, Brian Totty, and Scott Wills. </author> <title> Architecture of a message-driven processor. </title> <booktitle> In Proceedings of the 14th International Symposium on Computer Architecture, </booktitle> <pages> pages 189-196, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: The proposed structure is applicable in environments other than distributed systems, e.g., in large-scale dedicated multiprocessors, where the cost of control transfer is high relative to that of data transfer. Traditionally, these multiprocessor systems, e.g., the J-Machine <ref> [22] </ref>, *T [53], etc., have opted for a single primitive that unifies remote transfer of data and control, in contrast to our approach. 93 It is important to make two final points in passing.
Reference: [23] <author> Chris Dalton, Greg Watson, David Banks, Costas Calamvokis, Aled Edwards, and John Lumley. </author> <title> Afterburner. </title> <journal> IEEE Network, </journal> <volume> 7(4) </volume> <pages> 36-43, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: This led to several studies that tried to reduce the cost of software processing, e.g., [17], [28], [72] and many others. Reduced protocol costs led to research on the structure of network controllers and other host-architectural effects, e.g., [5], <ref> [23] </ref>, [38], [66] to name a few. The advent of a new generation of ATM networks has, in turn, resulted in many new research efforts, including ours, aimed at higher performance network communication. The work described in this thesis shares similarities with many previous studies.
Reference: [24] <author> Bruce S. Davie. </author> <title> A host-network interface architecture for ATM. </title> <booktitle> In Proceedings of the 1991 SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 307-315, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Given different network and processor technologies, the relative importance of these components may change; by isolating the components, we can see how the performance of each scales with technology change. Controllers targeted for high throughput have been well studied in the past <ref> [5, 24, 38, 68] </ref>. However, latency is often the overriding concern in distributed systems, and consequently, for the most part, this chapter considers the impact of the controller on this aspect of communication performance. <p> As an extreme example, on a 155 Mbit/sec ATM network, individual cells, which are fixed size 53 byte packets, can arrive at the host once every 2.7 microseconds. In order to isolate the host from frequent interrupts and protocol processing overheads, some designs migrate functionality into the controller <ref> [24, 68] </ref>. However, the migration of too much functionality into the controller can impact the latency of processing network messages. In addition, if the controller presents a complex interface to the host, there could be additional software latency introduced by the host device driver in managing the controller.
Reference: [25] <author> Gary Delp. </author> <title> The Architecture and Implementation of Memnet: A High-Speed Shared Memory Computer Communication Network. </title> <type> Ph.D. thesis, </type> <institution> University of Delaware, </institution> <year> 1988. </year> <month> 106 </month>
Reference-contexts: Previous work has also investigated support for network-wide coherent shared virtual memory either in hardware <ref> [25] </ref> or in software [41]. The network access model we propose here is quite different from these efforts. Our goal is to provide efficient primitives to access the network so that distributed programming models, such as shared virtual memory, can be implemented efficiently on top of it.
Reference: [26] <institution> Digital Equipment Corporation. TURBOChannel Hardware Specification, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: The DECstations were connected in turn to an Ethernet, an FDDI ring and an ATM network. The SparcStations were connected to an Ethernet. The DECstation network devices are connected on the 25 MHz TURBOChannel <ref> [26] </ref> while SparcStations use the 25 MHz SBus [64]. The performance of each configuration in sending a source packet from one node to the other and receiving a packet in response was measured. Measurements were averaged over at least 10,000 successful repetitions.
Reference: [27] <author> Digital Equipment Corporation, </author> <title> Workstation Systems Engineering. PMADD-AA TURBOChannel Ethernet Module Functional Specification, </title> <journal> Rev 1.2., </journal> <month> August </month> <year> 1990. </year>
Reference-contexts: The controller is packaged differently on the two machines. On the DECstations, the controller cannot do DMA to or from host memory; instead, it contains a 128 Kbyte on-board packet buffer memory into which the host places a correctly formatted packet for transmission <ref> [27] </ref>. Similarly, the controller places incoming packets in its buffer memory for the host to copy. In the case of the SparcStation, the controller transfers data to and from host memory using DMA.
Reference: [28] <author> Willibald A. Doeringer, Doug Dykeman, Matthias Kaiserwerth, Bernd Werner Meis-ter, Harry Rudin, and Robin Williamson. </author> <title> A survey of light-weight transport protocols for high-speed networks. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 38(11) </volume> <pages> 2025-2039, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: At one stage in the state-of-the-art, software protocols were considered the key factor in performance. This led to several studies that tried to reduce the cost of software processing, e.g., [17], <ref> [28] </ref>, [72] and many others. Reduced protocol costs led to research on the structure of network controllers and other host-architectural effects, e.g., [5], [23], [38], [66] to name a few.
Reference: [29] <author> Edward W. Felten. </author> <title> Protocol Compilation: High-Performance Communication for Parallel Programs. </title> <type> Ph.D. thesis, </type> <institution> University of Washington, </institution> <month> September </month> <year> 1993. </year> <institution> Department of Computer Science and Engineering Technical Report 93-09-09. </institution>
Reference-contexts: Thus, we believe that with little additional mechanism, it is possible to adequately support the needs of parallel applications. Ideally, we do not expect programmers to directly use the network access model to write their parallel applications on the cluster. That is the task of a protocol compiler <ref> [29] </ref>. This notion is similar in spirit to the idea of using RPC stub generators to hide the details of marshaling from users. In general terms, a protocol compiler analyses a parallel application for communication and computation phases.
Reference: [30] <author> FORE Systems, </author> <title> 1000 Gamma Drive, Pittsburgh PA 15238. TCA-100 TURBOchannel ATM Computer Interface, User's Manual, </title> <year> 1992. </year>
Reference-contexts: In an ATM network, user-level data is segmented into cells, routed, and then reassembled at the destination using header information contained in the cells. The particular ATM used has 140 Mbit/sec fiber optic links and cell sizes of 53 bytes, and is accessed using FORE Systems' ATM controller <ref> [30] </ref>. The controllers on the two DECstation hosts were directly connected without going through a switch; thus there is no switch delay. Unlike the Ethernet and FDDI controllers, the ATM controller uses two FIFOs, one for transmit and the other for receive. The controller has no DMA facilities.
Reference: [31] <author> David K. Gifford and Nathan Glasser. </author> <title> Remote pipes and procedures for efficient distributed communication. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(3) </volume> <pages> 258-283, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Also, unlike our approach, there are no read operations. Further, the SHRIMP design does not focus on issues of separating control and data transfer in distributed systems. 8.9 Channel Model, V System, Network Objects The distributed system structure we propose is loosely related to the Channel Model <ref> [31] </ref>, Network Objects [9], and other systems, like V [16], that use RPC for small data and a separate bulk data transport mechanism.
Reference: [32] <author> Daniel H. Greene and J. Bryan Lyles. </author> <title> Reliability of adaptation layers. </title> <booktitle> In Protocols for High-Speed Networks (Proceedings of the IFIP 6.1/6.4 Workshop), </booktitle> <pages> pages 185-201, </pages> <year> 1993. </year>
Reference-contexts: AALs have two sublayers called the segmentation and reassembly (SAR) layer, and the convergence (CS) layer. Typically, the function of the SAR layer is to handle the reassembly of cells, while the CS layer checks the final result <ref> [32] </ref>. There is hardware support for some AAL 3/4 functions, such as CRC generation, in the FORE controller. However, there is no support for the SAR or CS layers in hardware. The device driver is capable of handling reassembly and fragmentation in software.
Reference: [33] <author> C. A. R. Hoare. </author> <title> Communicating sequential processes. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 666-677, </pages> <month> August </month> <year> 1978. </year>
Reference-contexts: Distributed services are programmed using various distributed programming models. Perhaps the most common example of a distributed programming model is remote procedure call (RPC). Variations of RPC include message passing as seen in the V system [16] and CSP <ref> [33] </ref>. Another example of a distributed programming model is distributed shared memory (DSM) such as Ivy [41]. Distributed programming models are implemented on top of a lower level abstraction referred to, in this thesis, as a network access model.
Reference: [34] <author> John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satya-narayanan, Robert M. Sidebotham, and Michael J. West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: However, these issues are independent of whether we use our structure or a traditional RPC-based structure. For example, many file system designs, e.g., Sprite [52], SNFS [62], and AFS <ref> [34] </ref>, which use RPC, require mechanisms for recoverability and cache maintenance. In traditional RPC-based distributed systems, the RPC runtime and transport implement timeout and exception mechanisms to automatically notify the user of remote machine failures. It might appear that in our organization, fault-tolerance might be a difficult goal to achieve. <p> However, we are not directly concerned with the particular choice of protocol that is used and our system model does not implement one. Many coherency protocols are well known and are in use in current distributed file systems, e.g., the Spritely NFS system [62], Sprite [52], and AFS <ref> [34] </ref> all use cache-consistency schemes. (Even NFS has a cache-consistency policy, albeit a weaker one.) Traditional multiprocessor cache-coherency schemes can also be used, as suggested by the work on the xFS protocol [21].
Reference: [35] <author> Wilson C. Hsieh, M. Frans Kaashoek, and William E. Weihl. </author> <title> The persistent relevance of IPC performance: New techniques for reducing the IPC penalty. </title> <booktitle> In Proceedings 107 of the Fourth Workshop on Workstation Operating Systems, </booktitle> <pages> pages 186-190, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Workstation hosts on general purpose networks do not have such mechanisms. Thus, implementing the Active Message model in this environment may require additional functionality, e.g., hardware to allow multiple users to map in the network controller or compiler techniques to ensure that the upcall handlers that execute are well-behaved <ref> [35] </ref>. 8.7 Hamlyn Wilkes describes the design of a hardware controller for a reliable multicomputer interconnection network [74]. Some of the Hamlyn ideas are similar to the ones described in this thesis.
Reference: [36] <institution> Intel Supercomputer Systems Division. Paragon XP/S Product Overview, </institution> <year> 1991. </year>
Reference-contexts: Second, it also becomes increasingly attractive to purchase new commodity workstation and networking gear for dedicated use, rather than a tightly-coupled system. Such loosely-coupled structures built from commodity parts offer advantages over dedicated multicomputers, such as the Intel Paragon <ref> [36] </ref> and Thinking Machines CM-5 [67]: commodity parts are lower in cost and they can be flexibly scaled and upgraded. Workstation clusters used for distributed applications rely on heavyweight client/server models and message-based (RPC) communication while parallel applications favor simpler models that involve more direct inter-processor data access.
Reference: [37] <author> David B. Johnson and Willy Zwaenepoel. </author> <title> The Peregrine high-performance RPC system. </title> <journal> Software Practice and Experience, </journal> <volume> 23(2) </volume> <pages> 201-221, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: There is really no marshaling or demarshaling per se; data moves directly from source memory to destination memory without unnecessary copying or buffering. (Using writable stacks to simplify demarshaling can be done even without the remote memory model <ref> [37] </ref>, however doing so is more complex.) Once the stack is ready, the client activates the server by writing a flag word in the server, for which the server polls. Call-by-reference is straightforward to provide through the remote read and write primitives. <p> For this reason, an enormous amount of energy has been devoted to increasing its performance <ref> [37, 58, 66, 70] </ref>. Still, RPC times are substantial compared to the raw hardware speed. While this cost is due in part to the latency of network controllers and the software protocols used for network transfer, it is due as well to the semantics of RPC.
Reference: [38] <author> Hemant Kanakia and David R. Cheriton. </author> <title> The VMP network adapter board (NAB): High-performance network communications for multiprocessors. </title> <booktitle> In Proceedings of the 1988 SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 175-187, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Some well-known examples of controllers include the LANCE [1], the VMP-NAB <ref> [38] </ref>, and the Nectar CAB [5]. Typically, controllers buffer network packets, provide checksum and encryption, and have sophisticated data transfer capabilities between the network and host memory (such as scatter-write and gather-read). Some even have on-board implementations of entire protocol stacks. <p> Typically, controllers provide support for checksumming and data transfer mechanisms, e.g., direct memory access (DMA) between the network and host memory. In some cases, controllers have embedded processors that can be used to implement entire transport protocols <ref> [5, 38] </ref>. In this chapter we evaluate the overhead added to cross-machine communication from two important sources: (1) the network controller hardware and (2) low level controller software, memory, and CPU interfaces. <p> Given different network and processor technologies, the relative importance of these components may change; by isolating the components, we can see how the performance of each scales with technology change. Controllers targeted for high throughput have been well studied in the past <ref> [5, 24, 38, 68] </ref>. However, latency is often the overriding concern in distributed systems, and consequently, for the most part, this chapter considers the impact of the controller on this aspect of communication performance. <p> Thus, it might be beneficial to support both forms on the same controller. To our knowledge, the only controller which has multiple host interfaces on board is the VMP-NAB <ref> [38] </ref>. Our experiments also suggest that, in the absence of memory system support, DMA may incur the cost of additional copies and/or cache flushing overheads. <p> This led to several studies that tried to reduce the cost of software processing, e.g., [17], [28], [72] and many others. Reduced protocol costs led to research on the structure of network controllers and other host-architectural effects, e.g., [5], [23], <ref> [38] </ref>, [66] to name a few. The advent of a new generation of ATM networks has, in turn, resulted in many new research efforts, including ours, aimed at higher performance network communication. The work described in this thesis shares similarities with many previous studies.
Reference: [39] <author> David Keppel. </author> <title> A portable interface for on-the-fly instruction space modification. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 86-95, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: To do this, code is synthesized on the fly, which is then linked into the kernel and executed. Code synthesis has been used in the past to generate optimized routines for specific situations to achieve high performance <ref> [39, 45] </ref>. The focus here is slightly different: it is more concerned with avoiding the copy cost rather than generating extremely efficient code for a special situation. At bind time, when the client imports the server's interface, the client calls into the kernel with a template of the marshaling procedure.
Reference: [40] <author> Nancy P. Kronenberg, Henry M. Levy, and William D. Strecker. VAXclusters: </author> <title> A closely-coupled distributed system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(2) </volume> <pages> 130-146, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: They do not use RPC for communication, but rely on direct access to each other's memory. 8.3 VAXclusters DEC's VAXclusters system used a set of communication controllers to provide a variety of message operations on a cluster of computers <ref> [40] </ref>. The controller supported, in hardware, reliable message sends, unreliable datagrams, and block data transfers. The block transfer primitive is similar to a remote write operation, in that the sender specifies where in destination memory the data is to be deposited.
Reference: [41] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The effectiveness of this scheme is highly influenced by the communication performance between client and server. As another example, better communication would greatly facilitate the use of networked workstations as a loosely-coupled multiprocessor or shared virtual memory system <ref> [14, 41] </ref>. Careful software design is essential if we are to exploit the advances in hardware. High-speed networks and fast processors are necessary but not sufficient by themselves. We illustrate this point below with a simple example. <p> Perhaps the most common example of a distributed programming model is remote procedure call (RPC). Variations of RPC include message passing as seen in the V system [16] and CSP [33]. Another example of a distributed programming model is distributed shared memory (DSM) such as Ivy <ref> [41] </ref>. Distributed programming models are implemented on top of a lower level abstraction referred to, in this thesis, as a network access model. An example of a network access model is the familiar byte-stream abstraction to the network. <p> Distributed programming models, e.g., RPC in Unix, are layered on top of this byte-stream abstraction. Other distributed programming models such as distributed shared memory can also be implemented on top of the byte-stream abstraction <ref> [41] </ref>. It is also important to note that similar abstractions, albeit with different performance or guarantees, might be implemented at different levels. <p> Previous work has also investigated support for network-wide coherent shared virtual memory either in hardware [25] or in software <ref> [41] </ref>. The network access model we propose here is quite different from these efforts. Our goal is to provide efficient primitives to access the network so that distributed programming models, such as shared virtual memory, can be implemented efficiently on top of it.
Reference: [42] <author> Jochen Liedtke. </author> <title> Improving IPC by kernel design. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 175-188, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Whenever possible, this communication is done using the remote read/write data transfer mechanism. that for the most part, control transfers are restricted between a client and its server-clerk. That is, control transfers are primarily intra-node cross-domain calls, which have been shown to be amenable to high-performance implementation <ref> [7, 42] </ref>. Notice also that our organization maintains the firewalls between untrusted clients and services and the abstractional convenience of procedural interfaces, without relying on conventional mechanisms like RPC for cross-machine communication.
Reference: [43] <author> Richard J. Littlefield. </author> <title> Characterizing and tuning communication performance on the Touchstone DELTA and the iPSC/860. </title> <booktitle> In Proceedings of the 1992 Intel User's Group Meeting, </booktitle> <pages> pages 4-7, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: We expect that as the technology matures, switching times can be brought down to a few hundred nanoseconds with faster datapaths and virtual cut-through. We did not perform measurements on the CM-5 and the Intel machines, but use values reported elsewhere <ref> [43, 69, 73] </ref>. Row two shows the one-way cost of sending data to a pre-allocated buffer using the CMMD scopy routine, which is comparable in functionality to a remote write [69]. Row three represents the performance of send/receive using the facilities of the PUMA operating system [73]. <p> Row three represents the performance of send/receive using the facilities of the PUMA operating system [73]. At the time of writing, this represents the best performance reported in the literature. Row four represents send/receive performance reported by Littlefield <ref> [43] </ref>. Send/receives have an overhead cost for buffer setup and flow control associated with them. The table indicates the performance including the startup cost that is needed for the 3-phase protocol to set up buffers. Table 6.2 compares the sustained throughput achieved by the various systems.
Reference: [44] <author> Chris Maeda and Brian N. Bershad. </author> <title> Protocol service decomposition for high performance internetworking. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 244-255, </pages> <month> December </month> <year> 1993. </year> <month> 108 </month>
Reference-contexts: Some other examples of ILP are combining data copying and checksum calculations in TCP/IP [17, 75], and exploiting 55 application semantics to avoid extra copies of data for UDP/IP <ref> [44] </ref>. There are many similarities between the structure of cross-machine RPC using the memory-based network access model and the structure of same-machine RPC on shared memory (shown in Figure 5.1c). Consequently, techniques that yield good performance for same-machine RPCs yield good performance in the cross-machine case as well.
Reference: [45] <author> Henry Massalin and Calton Pu. </author> <title> Threads and input/output in the Synthesis kernel. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 191-201, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: To do this, code is synthesized on the fly, which is then linked into the kernel and executed. Code synthesis has been used in the past to generate optimized routines for specific situations to achieve high performance <ref> [39, 45] </ref>. The focus here is slightly different: it is more concerned with avoiding the copy cost rather than generating extremely efficient code for a special situation. At bind time, when the client imports the server's interface, the client calls into the kernel with a template of the marshaling procedure.
Reference: [46] <author> R.M. Metcalfe and D.R. Boggs. </author> <title> Ethernet: Distributed packet switching for local computer networks. </title> <journal> Communications of the ACM, </journal> <volume> 19(7) </volume> <pages> 395-404, </pages> <month> July </month> <year> 1976. </year>
Reference-contexts: Some recent developments, however, offer the potential to change the way workstation clusters can be used: * New switch-based network technologies, such as Asynchronous Transfer Mode (ATM), offer more than a 10-fold bandwidth improvement over Ethernet <ref> [46] </ref>. Another order-of-magnitude improvement to one gigabit per second seems close at hand.
Reference: [47] <author> Ronald G. Minnich and David J. Farber. </author> <title> Reducing host load, network load, and latency in a distributed shared memory. </title> <booktitle> In Proceedings of the Tenth IEEE Distributed Computing Systems Conference, </booktitle> <year> 1990. </year>
Reference-contexts: For example, the major use of the block transfer primitive was to transfer data from the disk server directly to file buffers in kernel memory. 96 8.4 Mether and Memnet Mether is another system that provides remote memory accesses on a network of workstations <ref> [47] </ref>. The primary focus of this system is to provide distributed shared memory (DSM) using the page fault handlers in the VM system. To gain performance, the system has mechanisms to avoid data shipment, leaving the sharing and consistency semantics to the application.
Reference: [48] <author> Steven E. Minzer. </author> <title> Broadband ISDN and Asynchronous Transfer Mode (ATM). </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(9) 17-24,57, </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: Chapter 7 RESTRUCTURING SERVICES IN A DISTRIBUTED SYSTEM As we have mentioned earlier, the hardware base for distributed systems has changed significantly over the last decade. Advances in processor architecture and technology have resulted in workstations in the 100+ MIPS range. As well, newer local-area networks such as ATM <ref> [48] </ref> promise a ten- to hundred-fold increase in throughput, much reduced latency, greater scalability, and greatly increased reliability, when compared to current LANs such as Ethernet. Previous chapters have considered a new network access model tuned for next-generation networks.
Reference: [49] <author> Jeffrey C. Mogul and Anita Borg. </author> <title> The effect of context switches on cache performance. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 75-84, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: cells) as opposed to the 2N cells that would be required if each cell were separately acknowledged. 5.4 Control Transfer Context switching causes a significant portion of the overhead in RPC [58]; in addition, there is a substantial impact on processor performance due to cache misses after a context switch <ref> [49] </ref>. An RPC call typically requires four context switches: switching the client out, switching the server in, switching the server out, and finally switching the client back in. Two of theseswitching the client or the server outcan be overlapped with the transmission of the packet.
Reference: [50] <author> Ajay Mohindra and Murthy Devarakonda. </author> <title> Distributed token management in a cluster file system. </title> <booktitle> In Proceedings of the Symposium on Parallel and Distributed Processing, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Although our file system model does not explicitly account for coherency traffic, we believe coherency schemes can be built using our communication primitives and our file system model. For example, workstation-cluster file system designs such as Calypso <ref> [50] </ref> use an RPC-based distributed token management scheme to handle cache coherence. This scheme can be extended to use our communication primitives without involving control transfers in most cases. Token acquire and release can be implemented using compare-and-swap operations. Token revocation is trickier.
Reference: [51] <author> Bruce Jay Nelson. </author> <title> Remote procedure call. </title> <type> Technical Report CSL-81-9, </type> <institution> Xerox Palo Alto Research Center, </institution> <month> May </month> <year> 1981. </year> <title> (Also, </title> <type> Ph.D. thesis, </type> <institution> Carnegie-Mellon University, CMU-CS-81-119). </institution>
Reference-contexts: High-speed networks and fast processors are necessary but not sufficient by themselves. We illustrate this point below with a simple example. Distributed systems are typically structured using remote procedure call (RPC) as the cross-machine communication mechanism <ref> [12, 51] </ref>. Figure 1.1 shows the performance of the Ultrix remote procedure call system between a pair of otherwise idle 25 MHz DECstations making small packet RPCs.
Reference: [52] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout. </author> <title> Caching in the Sprite network file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: However, these issues are independent of whether we use our structure or a traditional RPC-based structure. For example, many file system designs, e.g., Sprite <ref> [52] </ref>, SNFS [62], and AFS [34], which use RPC, require mechanisms for recoverability and cache maintenance. In traditional RPC-based distributed systems, the RPC runtime and transport implement timeout and exception mechanisms to automatically notify the user of remote machine failures. <p> However, we are not directly concerned with the particular choice of protocol that is used and our system model does not implement one. Many coherency protocols are well known and are in use in current distributed file systems, e.g., the Spritely NFS system [62], Sprite <ref> [52] </ref>, and AFS [34] all use cache-consistency schemes. (Even NFS has a cache-consistency policy, albeit a weaker one.) Traditional multiprocessor cache-coherency schemes can also be used, as suggested by the work on the xFS protocol [21].
Reference: [53] <author> R. S. Nikhil, G.M. Papadopoulos, and Arvind. </author> <title> *T: A multithreaded massively parallel architecture. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The proposed structure is applicable in environments other than distributed systems, e.g., in large-scale dedicated multiprocessors, where the cost of control transfer is high relative to that of data transfer. Traditionally, these multiprocessor systems, e.g., the J-Machine [22], *T <ref> [53] </ref>, etc., have opted for a single primitive that unifies remote transfer of data and control, in contrast to our approach. 93 It is important to make two final points in passing.
Reference: [54] <author> J. M. Ortega and R. G. Voight. </author> <title> Solution of partial differential equations on vector and parallel computers. </title> <journal> SIAM Review, </journal> <volume> 27(149), </volume> <year> 1985. </year> <month> 109 </month>
Reference-contexts: The problem is known to be NP hard, so our particular solution uses a heuristic. Our program has an optimizer that scans the grid and changes spins if doing so would lower the total energy. The optimizer contains two loops that are written using red/black coloring <ref> [54] </ref>, a common parallelization technique used in scientific applications. The grid is spatially distributed amongst the processors' memories such that each processor has two nearest neighbors. The boundaries of the grid contain data that is shared between neighboring processors.
Reference: [55] <author> John K. Ousterhout. </author> <booktitle> Why aren't operating systems getting faster as fast as hardware? In Proceedings of the Summer 1990 USENIX Conference, </booktitle> <pages> pages 247-256, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Interrupt handling cost has two components: (1) the CPU-dependent cost of vectoring the interrupt and (2) the controller-dependent cost of servicing the interrupt. Previous research has studied interrupt vectoring costs on RISC processors <ref> [3, 55] </ref> and so we shall examine only the second component. The objective is to compare a simple FIFO interface such as that found 20 in the ATM controller with a more elaborate descriptor interface such as that found in the Ethernet or the FDDI controllers.
Reference: [56] <author> Russel Sandberg, David Goldberg, Steve Kleiman, Dan Walsh, and Bob Lyon. </author> <title> Design and implemention of the Sun network filesystem. </title> <booktitle> In Proceedings of the Summer 1985 USENIX Conference, </booktitle> <pages> pages 119-130, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: The layers are interdependent, 4 i.e., a particular layer provides services to the layer above it and uses the services of the layer below it. The uppermost layer of the system represents distributed services. Familiar examples of distributed services are distributed file systems such as NFS <ref> [56] </ref>, name servers such as the Internet Domain Name Service DNS, electronic mail services, and others. Distributed services are programmed using various distributed programming models. Perhaps the most common example of a distributed programming model is remote procedure call (RPC). <p> Next, we discuss the alternative structure for distributed services. The alternative structure is based on the underlying network access model described in Chapter 3. We present data 72 from a simple name server application and from measurements of NFS <ref> [56] </ref>. 7.1 The Trouble with RPC RPC is the predominant communication mechanism between the components of contemporary distributed systems 1 . For this reason, an enormous amount of energy has been devoted to increasing its performance [37, 58, 66, 70].
Reference: [57] <author> Michael D. Schroeder, Andrew D. Birrell, Michael Burrows, Hal Murray, Roger M. Needham, Thomas L. Rodeheffer, Edwin H. Satterthwaite, and Charles P. Thacker. Autonet: </author> <title> A high-speed, self-configuring local area network using point-to-point links. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 9(8) </volume> <pages> 1318-1335, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The software emulation technique that we use in our implementation will not provide adequate performance in this case. However, it is feasible to do encryption and 80 decryption in hardware. In fact, the AN1 controller <ref> [57] </ref> has mechanisms to decrypt and encrypt data using different keys as data is transmitted or received. <p> Like Mether, the focus of this system is on coherent memory abstractions rather than efficient data movement per se. 8.5 DEC AN1 The AN1 network designed at the DEC System Research Center is a high-speed point-to-point network <ref> [57] </ref>. The network controller for AN1 has hardware support that permits some control over the placement of data at the destination. In the AN1 network, a single field, called the BQI, in the link-level packet header provides a level of indirection into a table kept in the controller.
Reference: [58] <author> Michael D. Schroeder and Michael Burrows. </author> <title> Performance of Firefly RPC. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 1-17, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Conventional RPC design is layered. Even high performance RPC implementations adhere to this layering principle (except, perhaps, to do upcalls from the device level directly to the higher levels) <ref> [12, 58, 66] </ref>. For example, the stubs view the RPC transport as a provider of reliable packets or linear memory buffers. The stubs translate between user specified entities (integer, records, arrays) and the packets using marshaling code. <p> One approach to reducing this cost, for example, the one used in the high-performance Firefly RPC system <ref> [58] </ref>, is to relax kernel/user protection and permanently map all network buffers into user space and allow the user direct access. This is a viable technique if the applications are trusted, or for inter-kernel RPCs. Kernel Level Marshaling For general-purpose time sharing systems, though, kernel/user protection is an important consideration. <p> Thus in the case where no cells are dropped, every group of N cells requires a constant overhead (2-3 cells) as opposed to the 2N cells that would be required if each cell were separately acknowledged. 5.4 Control Transfer Context switching causes a significant portion of the overhead in RPC <ref> [58] </ref>; in addition, there is a substantial impact on processor performance due to cache misses after a context switch [49]. An RPC call typically requires four context switches: switching the client out, switching the server in, switching the server out, and finally switching the client back in. <p> For this reason, an enormous amount of energy has been devoted to increasing its performance <ref> [37, 58, 66, 70] </ref>. Still, RPC times are substantial compared to the raw hardware speed. While this cost is due in part to the latency of network controllers and the software protocols used for network transfer, it is due as well to the semantics of RPC. <p> 3602730 12 Read Symbolic Link 1628256 6 Read Directory Contents 981345 3 Read File System Stats. 149142 0.5 Write File Data 109712 0.4 Other 109986 0.3 Total 28860744 100 and no results, and 7 percent of the overall time for a call with no arguments and a 1440 byte result <ref> [58] </ref>. Spin waiting, e.g., as done in our prototype FRPC and RAPID systems, may not be a feasible option in many environments. Given this problem, we should ask whether distributed applications require a single primitive that unifies data and control transfer.
Reference: [59] <author> Richard L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <address> One Burlington Woods Drive, Burlington, MA 01803, </address> <year> 1992. </year>
Reference-contexts: And in fact, the situation is even worse than the table suggests, because, in addition to the 19 costs of executing additional cache flush instructions, cache flushes have a negative impact on performance by destroying locality. Newer processor/cache designs recognize this problem and provide memory coherence for DMA <ref> [59] </ref>. <p> Our implementation experience confirms our belief that it is useful and feasible to support the notion of remote memory on modern workstation clusters. Given next generation processors, with demultiplexed I/O interrupts and special support for emulation code, as found in the DEC Alpha <ref> [59] </ref>, the cost of software emulation could be substantially lowered. Software emulation has a flexibility advantage over a pure hardware approach because application-specific operations can be easily added.
Reference: [60] <editor> SPEC newsletter benchmark results. </editor> <booktitle> Systems Performance Evaluation Cooperative, </booktitle> <year> 1990. </year>
Reference-contexts: The testbed hardware consists of two workstations (either two DECstations or two SparcStations) connected through an isolated network. The DECstation uses a 25 MHz MIPS R3000 processor rated at 18.5 SPECmarks, and the SparcStation I uses a Sparc processor rated at 24.4 SPECmarks <ref> [60] </ref>. The DECstations were connected in turn to an Ethernet, an FDDI ring and an ATM network. The SparcStations were connected to an Ethernet. The DECstation network devices are connected on the 25 MHz TURBOChannel [26] while SparcStations use the 25 MHz SBus [64].
Reference: [61] <author> Alfred Z. Spector. </author> <title> Performing remote operations efficiently on a local computer network. </title> <journal> Communications of the ACM, </journal> <volume> 25(4) </volume> <pages> 246-260, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Second, it proposes a network access model based on remote memory. The thesis demonstrates the effectiveness of the model in getting higher performance in diverse situations such as RPC and workstation-based multicomputing. The model proposed here goes beyond earlier memory-based approaches, such as that proposed by Spector <ref> [61] </ref>, by incorporating notions of virtual memory, protected sharing of the network, and separate data and control transfer. Separating data and control transfer allows function shipping mechanisms (e.g., RPC) and pure data shipping mechanisms to be optimized separately. <p> The notion of remote memory access on a local-area network has been suggested before. Spector's work on the experimental Ethernet is perhaps the earliest example <ref> [61] </ref>. He suggested that by implementing a set of operations that could be performed efficiently over the network, we could lower communication overheads. The network access model we propose here is influenced by his work, but has several significant extensions to it. <p> The following sections compare our work with previous approaches. For the most part, the discussion below is organized in rough chronological order. 8.1 Spector's Remote References As far as I am aware, the notion of remote memory access was first suggested by Spector <ref> [61] </ref>.
Reference: [62] <author> V. Srinivasan and Jeffrey C. Mogul. Spritely NFS: </author> <title> Experiments with cache-consistency protocols. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 45-57, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: However, these issues are independent of whether we use our structure or a traditional RPC-based structure. For example, many file system designs, e.g., Sprite [52], SNFS <ref> [62] </ref>, and AFS [34], which use RPC, require mechanisms for recoverability and cache maintenance. In traditional RPC-based distributed systems, the RPC runtime and transport implement timeout and exception mechanisms to automatically notify the user of remote machine failures. <p> However, we are not directly concerned with the particular choice of protocol that is used and our system model does not implement one. Many coherency protocols are well known and are in use in current distributed file systems, e.g., the Spritely NFS system <ref> [62] </ref>, Sprite [52], and AFS [34] all use cache-consistency schemes. (Even NFS has a cache-consistency policy, albeit a weaker one.) Traditional multiprocessor cache-coherency schemes can also be used, as suggested by the work on the xFS protocol [21].
Reference: [63] <author> Jennifer G. Steiner, Clifford Neuman, and Jeffrey I. Schiller. </author> <title> Kerberos: An authentication service for open network systems. </title> <booktitle> In Proceedings of the Winter 1988 USENIX Conference, </booktitle> <pages> pages 191-202, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: For instance, many environments routinely share files through NFS servers with exactly the same guarantees. However, there are environments where such trust may not be warranted. Distributed systems running in these environments have traditionally used encryption techniques to ensure authentication and security <ref> [10, 63] </ref>. The underpinning for such schemes is that data is encrypted and decrypted using secret or public key schemes. The choice of the particular communication primitiveRPC or remote memoryis irrelevant. With the memory-based access model, this implies that each read and write has to be encrypted and decrypted.
Reference: [64] <institution> Sun Microsystems Inc., </institution> <address> 2550 Garcia Avenune, Mountain View CA 94043. SBus Specification B.0, </address> <year> 1990. </year> <month> 110 </month>
Reference-contexts: The DECstations were connected in turn to an Ethernet, an FDDI ring and an ATM network. The SparcStations were connected to an Ethernet. The DECstation network devices are connected on the 25 MHz TURBOChannel [26] while SparcStations use the 25 MHz SBus <ref> [64] </ref>. The performance of each configuration in sending a source packet from one node to the other and receiving a packet in response was measured. Measurements were averaged over at least 10,000 successful repetitions.
Reference: [65] <author> Charles P. Thacker, Edward M. McCreight, Butler W. Lampson, Robert F. Sproull, and David R. Boggs. </author> <title> Alto: A personal computer. </title> <editor> In Daniel P. Siewiorek, C. Gordon Bell, and Allen Newell, </editor> <booktitle> Computer Structures: Principles and Examples, chapter 33, </booktitle> <pages> pages 549-572. </pages> <publisher> McGraw-Hill Book Company, </publisher> <year> 1982. </year>
Reference-contexts: Distributed programming models, such as RPC and many of the structuring principles embodied in current systems, date back to the time of 0.5 MIPS Xerox Alto workstations interconnected by the 3 Mb/sec experimental Ethernet <ref> [65] </ref>. For instance, traditional distributed systems have been structured around the client/server model, in which clients access services using message passing or RPC. This model has the advantage of providing a convenient and familiar programming abstraction to users. <p> The network access model we propose here is influenced by his work, but has several significant extensions to it. These are related to sharing, protection, independent time-slicing among nodes, and others. Spector's work, done on the early Xerox Alto workstations <ref> [65] </ref>, left unaddressed important issues regarding protection and virtual memory, which are common concerns in current and next-generation workstations but were not supported by the Alto. Previous work has also investigated support for network-wide coherent shared virtual memory either in hardware [25] or in software [41]. <p> The model presented in this thesis draws heavily on Spector's approach but makes significant extensions to support virtual memory, protection, and timeslicing. These issues were largely unaddressed in Spector's work on the Xerox Altos <ref> [65] </ref>, which neither had virtual memory nor address protection among a set of applications. 95 Spector implemented a small subset of the possible remote access types from his taxonomy. The subset consisted of processor synchronous remote reads, writes, and test-and-set.
Reference: [66] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Limits to low-latency communication on high-speed networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: To this end, we compare the design and performance of RAPID with another RPC system that we built, called FRPC <ref> [66] </ref>. We designed FRPC about three years ago to study the limitations to latency on high-speed networks like ATM and FDDI compared to traditional networks like Ethernet. <p> Conventional RPC design is layered. Even high performance RPC implementations adhere to this layering principle (except, perhaps, to do upcalls from the device level directly to the higher levels) <ref> [12, 58, 66] </ref>. For example, the stubs view the RPC transport as a provider of reliable packets or linear memory buffers. The stubs translate between user specified entities (integer, records, arrays) and the packets using marshaling code. <p> For this reason, an enormous amount of energy has been devoted to increasing its performance <ref> [37, 58, 66, 70] </ref>. Still, RPC times are substantial compared to the raw hardware speed. While this cost is due in part to the latency of network controllers and the software protocols used for network transfer, it is due as well to the semantics of RPC. <p> This led to several studies that tried to reduce the cost of software processing, e.g., [17], [28], [72] and many others. Reduced protocol costs led to research on the structure of network controllers and other host-architectural effects, e.g., [5], [23], [38], <ref> [66] </ref> to name a few. The advent of a new generation of ATM networks has, in turn, resulted in many new research efforts, including ours, aimed at higher performance network communication. The work described in this thesis shares similarities with many previous studies.
Reference: [67] <institution> Thinking Machines Corporation. </institution> <type> CM-5 Technical Summary, </type> <year> 1991. </year>
Reference-contexts: Second, it also becomes increasingly attractive to purchase new commodity workstation and networking gear for dedicated use, rather than a tightly-coupled system. Such loosely-coupled structures built from commodity parts offer advantages over dedicated multicomputers, such as the Intel Paragon [36] and Thinking Machines CM-5 <ref> [67] </ref>: commodity parts are lower in cost and they can be flexibly scaled and upgraded. Workstation clusters used for distributed applications rely on heavyweight client/server models and message-based (RPC) communication while parallel applications favor simpler models that involve more direct inter-processor data access. <p> In particular, as mentioned in previous chapters, our model explicitly separates the notion of data transfer from control transfer. In Active Messages, the upcall handler refers, in general, to user code. Dedicated multicomputers like the CM-5 <ref> [67] </ref>, on which Active Messages has been implemented, typically have hardware and network support to ensure that the upcalled user code cannot impact performance of other jobs by misbehaving. Workstation hosts on general purpose networks do not have such mechanisms.
Reference: [68] <author> C. Brendan S. Traw and Jonathan M. Smith. </author> <title> A high-performance host interface for ATM networks. </title> <booktitle> In Proceedings of the 1991 SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 317-325, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Given different network and processor technologies, the relative importance of these components may change; by isolating the components, we can see how the performance of each scales with technology change. Controllers targeted for high throughput have been well studied in the past <ref> [5, 24, 38, 68] </ref>. However, latency is often the overriding concern in distributed systems, and consequently, for the most part, this chapter considers the impact of the controller on this aspect of communication performance. <p> As an extreme example, on a 155 Mbit/sec ATM network, individual cells, which are fixed size 53 byte packets, can arrive at the host once every 2.7 microseconds. In order to isolate the host from frequent interrupts and protocol processing overheads, some designs migrate functionality into the controller <ref> [24, 68] </ref>. However, the migration of too much functionality into the controller can impact the latency of processing network messages. In addition, if the controller presents a complex interface to the host, there could be additional software latency introduced by the host device driver in managing the controller.
Reference: [69] <author> Lewis W. Tucker and Alan Mainwaring. </author> <title> CMMD: Active Messages on the CM-5. </title> <journal> Parallel Computing, </journal> <volume> 20 </volume> <pages> 481-496, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: We expect that as the technology matures, switching times can be brought down to a few hundred nanoseconds with faster datapaths and virtual cut-through. We did not perform measurements on the CM-5 and the Intel machines, but use values reported elsewhere <ref> [43, 69, 73] </ref>. Row two shows the one-way cost of sending data to a pre-allocated buffer using the CMMD scopy routine, which is comparable in functionality to a remote write [69]. Row three represents the performance of send/receive using the facilities of the PUMA operating system [73]. <p> We did not perform measurements on the CM-5 and the Intel machines, but use values reported elsewhere [43, 69, 73]. Row two shows the one-way cost of sending data to a pre-allocated buffer using the CMMD scopy routine, which is comparable in functionality to a remote write <ref> [69] </ref>. Row three represents the performance of send/receive using the facilities of the PUMA operating system [73]. At the time of writing, this represents the best performance reported in the literature. Row four represents send/receive performance reported by Littlefield [43].
Reference: [70] <author> R. van Renesse, H. van Staveren, and A. S. Tanenbaum. </author> <title> Performance of the world's fastest distributed operating system. </title> <journal> ACM Operating Systems Review, </journal> <volume> 22(4) </volume> <pages> 25-34, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: For this reason, an enormous amount of energy has been devoted to increasing its performance <ref> [37, 58, 66, 70] </ref>. Still, RPC times are substantial compared to the raw hardware speed. While this cost is due in part to the latency of network controllers and the software protocols used for network transfer, it is due as well to the semantics of RPC.
Reference: [71] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: There are similarities in both approaches, to the extent that both try to provide efficient demultiplexing mechanisms for incoming data. 97 8.6 Active Messages Active Messages is a low-level mechanism that has been proposed for communicating between nodes in a dedicated, closely-coupled multicomputer <ref> [71] </ref>. The key idea in this design is that an incoming message carries with it an upcall address of a handler that integrates the message into the computation stream for the node. Thus, using Active Messages, data and control are very closely-coupled.
Reference: [72] <author> Richard W. Watson and Sandy A. Mamrak. </author> <title> Gaining efficiency in transport services by appropriate design and implementation choices. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(2) </volume> <pages> 97-120, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: At one stage in the state-of-the-art, software protocols were considered the key factor in performance. This led to several studies that tried to reduce the cost of software processing, e.g., [17], [28], <ref> [72] </ref> and many others. Reduced protocol costs led to research on the structure of network controllers and other host-architectural effects, e.g., [5], [23], [38], [66] to name a few.
Reference: [73] <author> Stephen R. Wheat, Arthur B. Maccabe, Rolf Riesen, David W. van Dresser, and T. Mack Stallcup. PUMA: </author> <title> An operating system for massively parallel systems. </title> <booktitle> In Proceedings of the Twenty-Seventh Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 56-65, </pages> <year> 1994. </year> <month> 111 </month>
Reference-contexts: We expect that as the technology matures, switching times can be brought down to a few hundred nanoseconds with faster datapaths and virtual cut-through. We did not perform measurements on the CM-5 and the Intel machines, but use values reported elsewhere <ref> [43, 69, 73] </ref>. Row two shows the one-way cost of sending data to a pre-allocated buffer using the CMMD scopy routine, which is comparable in functionality to a remote write [69]. Row three represents the performance of send/receive using the facilities of the PUMA operating system [73]. <p> Row two shows the one-way cost of sending data to a pre-allocated buffer using the CMMD scopy routine, which is comparable in functionality to a remote write [69]. Row three represents the performance of send/receive using the facilities of the PUMA operating system <ref> [73] </ref>. At the time of writing, this represents the best performance reported in the literature. Row four represents send/receive performance reported by Littlefield [43]. Send/receives have an overhead cost for buffer setup and flow control associated with them.
Reference: [74] <author> John Wilkes. </author> <title> Hamlynan interface for sender-based communications. </title> <type> Technical Report HPL-OSR-92-13, </type> <institution> Hewlett Packard Laboratories, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: model in this environment may require additional functionality, e.g., hardware to allow multiple users to map in the network controller or compiler techniques to ensure that the upcall handlers that execute are well-behaved [35]. 8.7 Hamlyn Wilkes describes the design of a hardware controller for a reliable multicomputer interconnection network <ref> [74] </ref>. Some of the Hamlyn ideas are similar to the ones described in this thesis. For example, fundamental to the design is the concept of sender-controlled data transfer where the sender specifies the location at the receiver where data is to be deposited. <p> Many of the design choices are therefore different in the two approaches. It is difficult to make a quantitative comparison of the two designs because, at the time of writing, there is no implementation nor any simulation results based on the paper design described in <ref> [74] </ref>. 8.8 SHRIMP The network interface for the SHRIMP multicomputer being designed at Princeton also proposes direct remote writes to memory [13]. Like Hamlyn and our work, the SHRIMP approach shares a common ancestry with Spector and VAXclusters.
Reference: [75] <author> Alec Wolman, Geoff Voelker, and Chandramohan A. Thekkath. </author> <title> Latency analysis of TCP on an ATM network. </title> <booktitle> In Proceedings of the Winter 1994 USENIX Conference, </booktitle> <pages> pages 167-179, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: However, on next-generation networks under certain conditions, there is evidence to suggest that a software checksum may be of limited use <ref> [75] </ref>. In this case, the processor does not need to mediate the transfer, and I/O-initiated data moves would be of benefit if the memory system provides adequate support for cache consistency. Fortunately, newer processor and memory architectures are providing these support mechanisms. <p> We view this as an application of the principle of Integrated Layer Processing (ILP) that can be used to structure communication protocols for better performance [19]. Some other examples of ILP are combining data copying and checksum calculations in TCP/IP <ref> [17, 75] </ref>, and exploiting 55 application semantics to avoid extra copies of data for UDP/IP [44]. There are many similarities between the structure of cross-machine RPC using the memory-based network access model and the structure of same-machine RPC on shared memory (shown in Figure 5.1c).
References-found: 75

