URL: ftp://publications.ai.mit.edu/ai-publications/1000-1499/AIM-1390.ps.Z
Refering-URL: http://www.ai.mit.edu/people/girosi/home-page/memos.html
Root-URL: 
Title: Some Extensions of the K-Means Algorithm for Image Segmentation and Pattern Classification  
Author: Jose L. Marroquin and Federico Girosi 
Note: Copyright c Massachusetts Institute of Technology, 1993  
Date: 1390 January, 1993  079  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY  
Pubnum: A.I. Memo No.  C.B.C.L. Paper No.  
Abstract: In this paper we present some extensions to the k-means algorithm for vector quantization that permit its efficient use in image segmentation and pattern classification tasks. It is shown that by introducing state variables that correspond to certain statistics of the dynamic behavior of the algorithm, it is possible to find the representative centers of the lower dimensional manifolds that define the boundaries between classes, for clouds of multi-dimensional, multi-class data; this permits one, for example, to find class boundaries directly from sparse data (e.g., in image segmentation tasks) or to efficiently place centers for pattern classification (e.g., with local Gaussian classifiers). The same state variables can be used to define algorithms for determining adaptively the optimal number of centers for clouds of data with space-varying density. Some examples of the application of these extensions are also given. This report describes research done within CIMAT (Guanajuato, Mexico), the Center for Biological and Computational Learning in the Department of Brain and Cognitive Sciences, and at the Artificial Intelligence Laboratory. This research is sponsored by grants from the Office of Naval Research under contracts N00014-91-J-1270 and N00014-92-J-1879; by a grant from the National Science Foundation under contract ASC-9217041; and by a grant from the National Institutes of Health under contract NIH 2-S07-RR07047. Additional support is provided by the North Atlantic Treaty Organization, ATR Audio and Visual Perception Research Laboratories, Mitsubishi Electric Corporation, Sumitomo Metal Industries, and Siemens AG. Support for the A.I. Laboratory's artificial intelligence research is provided by ONR contract N00014-91-J-4038. J.L. Marroquin was supported in part by a grant from the Consejo Nacional de Ciencia y Tecnologia, Mexico. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M.R. Anderberg. </author> <title> Cluster Analysis for Applications. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: one starts with a random center configuration m (0) which is then updated using the rule: m (t+1) P N k P N k (2) where S k = fx i : kx i m k k &lt; kx i m j k; j 6= kg It has been shown <ref> [1, 17] </ref> that this algorithm in fact converges to a local minimum of (2). For our purposes, however, it is more convenient to use a different form of this algorithm, in which each data point is used in turn to update the corresponding center location [15]. <p> Specifically: For each center k: if h k &lt; i N; supress the center. (18) 3 else if h k &gt; s N , generate a new center at a loca-tion corresponding to one the data points inside the current Voronoi polytope of center k. where i ; s 2 <ref> [0; 1] </ref> are two suitably chosen thresholds such that ( s i )N &gt; 1. <p> In this case, its performance may become worse than that of other classifiers. The training data set consisted of 1000 data points in the unit 5 dimensional cube <ref> [0; 1] </ref> 5 . Of these points, 2 3 were of class 1, and 1 3 of class 2. Half of the points of class 1 were inside a 5 dimensional hypersphere of radius 0.07 and the other half are outside a 5 dimensional hypersphere of radius 0.2.
Reference: [2] <author> M. Benaim and L. Tomasini. </author> <title> Competitive and self-organizing algorithms based on the minimization of an information criterion. </title> <editor> In T. Kohonen, K. Mak-isara, O. Simula, and J. Kangas, editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <pages> pages 391-396. </pages> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <year> 1991. </year>
Reference-contexts: A similar result may be obtained by using, instead of (6), an information-related distorsion measure <ref> [2] </ref>: E fi (m) = N i=1 " k=1 exp [fikx i m k k 2 ] # This error measure also corresponds to the log likelihood function of a Gaussian mixture model for the data distribution [10] [23], where: the means of the Gaussians correspond to the center locations; the <p> Note that in the particular case of cliques of size 2, and quadratic potentials of the form: V jk (m) = km j m k k 2 the posterior energy corresponds to the composite cost function discussed in <ref> [2] </ref>, and the corresponding update equation, for first order (nearest-neighbor) systems, reduces to the algorithm proposed by Durbin and Mitchi-son [6] for the development of cortical maps.
Reference: [3] <author> J. Besag. </author> <title> Spatial interaction and the statistical analysis of lattice systems (with discussion). </title> <journal> J. Royal Statist. Soc., </journal> <volume> 36(B):192-326, </volume> <year> 1974. </year>
Reference-contexts: The basic idea in this approach is to express the prior constraint on the organization of the centers in probabilistic terms, specifically, in the form of a discrete Markov Random Field (MRF) model <ref> [20, 13, 3, 26, 7] </ref>, in which the center locations correspond to the state variables associated with the nodes of a graph whose topology is related |but not necessarily identical| to the desired neighborhood structure of the centers; in fact this structure, as well as the smoothness of the locations of
Reference: [4] <author> R. Brunelli and T. Poggio. </author> <title> HyperBF networks for gender recognition. </title> <booktitle> In Proceedings Image Understanding Workshop. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: In order to test the LGC on a set of real data we considered the same task of gender classification that has been considered by Brunelli and Poggio in <ref> [4] </ref>. Brunelli and Poggio had a data set consisting of 168 digitized pictures of frontal views of people without facial hair, and the task was the classification of the gender. There were 21 male and 21 female subjects in the data set, and 4 pictures per subject. <p> This was repeated 10 times and the average training and testing errors computed; 2. the leave-one-out procedure described by Brunelli 7 and Poggio in <ref> [4] </ref>.
Reference: [5] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: 1 Introduction Finding a set of representative vectors for clouds of multi-dimensional data is an important issue in data compression [9], signal coding [9, 8], pattern classification <ref> [5] </ref> and function approximation tasks [22, 24]. <p> With straightforward modifications, this scheme may be used for finding: multiple closed boundaries (fig. 7-a and 7-c); open curves that go from one border of the image to another (fig. 7-b and 7-d), etc. 3.2 Local Gaussian Classifiers Gaussian Classifiers <ref> [5] </ref> are a well known class of procedures for the segmentation of multi-class, multidimensional data.
Reference: [6] <author> R. Durbin and G. Mitchison. </author> <title> A dimension reduction framework for understanding cortical maps. </title> <journal> Nature, </journal> <volume> 343 </volume> <pages> 644-647, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: These maps are interesting because they provide a model for the topology-preserving mappings that are known to exist between sensory inputs and the brain cortex <ref> [6, 14] </ref>. <p> of size 2, and quadratic potentials of the form: V jk (m) = km j m k k 2 the posterior energy corresponds to the composite cost function discussed in [2], and the corresponding update equation, for first order (nearest-neighbor) systems, reduces to the algorithm proposed by Durbin and Mitchi-son <ref> [6] </ref> for the development of cortical maps.
Reference: [7] <author> S. Geman and D. Geman. </author> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-6:721-741, </volume> <year> 1984. </year>
Reference-contexts: The basic idea in this approach is to express the prior constraint on the organization of the centers in probabilistic terms, specifically, in the form of a discrete Markov Random Field (MRF) model <ref> [20, 13, 3, 26, 7] </ref>, in which the center locations correspond to the state variables associated with the nodes of a graph whose topology is related |but not necessarily identical| to the desired neighborhood structure of the centers; in fact this structure, as well as the smoothness of the locations of
Reference: [8] <author> A. Gersho. </author> <title> On the structure of vector quantiz-ers. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 28(2) </volume> <pages> 157-166, </pages> <year> 1982. </year>
Reference-contexts: 1 Introduction Finding a set of representative vectors for clouds of multi-dimensional data is an important issue in data compression [9], signal coding <ref> [9, 8] </ref>, pattern classification [5] and function approximation tasks [22, 24]. <p> : ; m M g, m k 2 R D ; k = 1; : : :; M , partitions X into M sets fS 1 ; : : : ; S M g , where each set S k corresponds to those points in X inside the Voronoi polytope <ref> [8] </ref> of center m k : S k = fx i : kx i m k k &lt; kx i m j k; j 6= kg where k k is the Euclidean norm.
Reference: [9] <author> A. Gersho and R.M. Gray. </author> <title> Vector quantization and signal compression. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Finding a set of representative vectors for clouds of multi-dimensional data is an important issue in data compression <ref> [9] </ref>, signal coding [9, 8], pattern classification [5] and function approximation tasks [22, 24]. <p> 1 Introduction Finding a set of representative vectors for clouds of multi-dimensional data is an important issue in data compression [9], signal coding <ref> [9, 8] </ref>, pattern classification [5] and function approximation tasks [22, 24].
Reference: [10] <author> McLachlan G.J. and Basford K.E. </author> <title> Mixture Models: Inference and Applications to Clustering. </title> <publisher> Marcel Dekker, </publisher> <address> New York and Basel, </address> <year> 1988. </year>
Reference-contexts: result may be obtained by using, instead of (6), an information-related distorsion measure [2]: E fi (m) = N i=1 " k=1 exp [fikx i m k k 2 ] # This error measure also corresponds to the log likelihood function of a Gaussian mixture model for the data distribution <ref> [10] </ref> [23], where: the means of the Gaussians correspond to the center locations; the proportions are all equal to 1=N , and all the covariance matrices are equal to fi 1 I.
Reference: [11] <author> T. Inouye. </author> <title> Computer processing of scintillation camera gas. Nucl. Instrum. </title> <journal> Meth., </journal> <volume> 124 </volume> <pages> 215-219, </pages> <year> 1975. </year>
Reference-contexts: cases the training and test error were less than 5% with 3 centers (3 Gaussians per class), and less than 8% with one Gaussian per class. 3.3 Image Segmentation As a final example, we consider an image segmentation problem that arises in the processing of certain biomedical images: scintigraphic images <ref> [11, 19] </ref>, which are obtained by counting the number of radioactive particles that incide on each cell of a receptor array. The goal of the processing step is to obtain from these measurements an estimate of the radioisotope distribution in specific organs within the human body.
Reference: [12] <author> M. Kass, A. Witkin, and D. Terzopoulos. Snakes: </author> <title> Active contour models. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1 </volume> <pages> 321-331, </pages> <year> 1988. </year>
Reference-contexts: In this sense, it may be said that this algorithm finds the initial position, the number of knots and the final configura tion of a "snake" <ref> [12] </ref> that approximates the inter-class boundary.
Reference: [13] <author> R. Kinderman and J.L. Snell. </author> <title> Markov Random Fields and their applications. </title> <publisher> Amer. Math. Soc., </publisher> <address> Providence, RI, </address> <year> 1980. </year>
Reference-contexts: The basic idea in this approach is to express the prior constraint on the organization of the centers in probabilistic terms, specifically, in the form of a discrete Markov Random Field (MRF) model <ref> [20, 13, 3, 26, 7] </ref>, in which the center locations correspond to the state variables associated with the nodes of a graph whose topology is related |but not necessarily identical| to the desired neighborhood structure of the centers; in fact this structure, as well as the smoothness of the locations of
Reference: [14] <author> T. Kohonen. </author> <title> Self-organization and associative memory. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: These maps are interesting because they provide a model for the topology-preserving mappings that are known to exist between sensory inputs and the brain cortex <ref> [6, 14] </ref>.
Reference: [15] <author> T. Kohonen. </author> <title> The self-organizing map. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(9) </volume> <pages> 1464-1480, </pages> <year> 1990. </year>
Reference-contexts: For our purposes, however, it is more convenient to use a different form of this algorithm, in which each data point is used in turn to update the corresponding center location <ref> [15] </ref>. If at time t point x i is selected, we put: m k = m k + a t (x i m k ); if i 2 S k (3) k ; otherwise where fa t g is a non-increasing sequence of scalars. <p> In section 2 we present an extended version of this scheme which is more amenable to formal analysis. A straightforward generalization of this technique may be also used for classification purposes <ref> [15] </ref>: if one is given for each data point x i an associated class C (x i ) 2 fc 1 : : : ; c L g, one may find M j optimal centers for each class j : fm jk ; j = 1; : : :; L; k <p> This procedure is similar to the LVQ1 classification method <ref> [15] </ref> (except that in that case, one introduces a repulsion term from those points with C (x i ) 6= j that are inside S jk to push the centers away from the decision boundaries) and is equivalent to the use of update rule (3) for each class in a decoupled <p> This will be discussed in the following section. Another modification of the basic procedure (3) allows one to give a neighborhood structure to the set of centers. Thus, Kohonen's Self Organizing Maps <ref> [15] </ref> show how a 1 or 2-dimensional lattice structure may be imposed to the set of centers, and how this structure may, in many cases, reflect the internal organization of the data. <p> Note that with this procedure, the potentials are always of the form (23) with k = 1. The final configurations obtained in this way (see figure 4-b) are very similar to those obtained with Koho-nen's algorithm <ref> [15] </ref> (which also incorporates long range interactions), but since the neighborhood size remains fixed, the computational complexity is lower, and since the new centers are already close to their correct (globally ordered) positions, the convergence rate is significantly faster.
Reference: [16] <author> H. Kushner and D. Clark. </author> <title> Stochastic approximation methods for constrained and unconstrained systems, </title> <booktitle> volume 26 of Applied Mathematical Sciences. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: This fact is stated in lemma 2.1 (see section 2), which is an application of the following theorem <ref> [16] </ref>: Theorem A.1 (Kushner and Clark, 1978) Consider the following sequence in R n : y n+1 = y n + a n h (y n ; ~ n ) + a n n (31) where f~ n g is a sequence of random variables that do not depend on the
Reference: [17] <author> Y. Linde, A. Buzo, and R. Gray. </author> <title> An algorithm for vector quantizer design. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> Com-28(1):84-95, </address> <month> January </month> <year> 1980. </year> <month> 10 </month>
Reference-contexts: one starts with a random center configuration m (0) which is then updated using the rule: m (t+1) P N k P N k (2) where S k = fx i : kx i m k k &lt; kx i m j k; j 6= kg It has been shown <ref> [1, 17] </ref> that this algorithm in fact converges to a local minimum of (2). For our purposes, however, it is more convenient to use a different form of this algorithm, in which each data point is used in turn to update the corresponding center location [15].
Reference: [18] <author> J. MacQueen. </author> <title> Some methods of classification and analysis of multivariate observations. </title> <editor> In L.M. LeCam and J. Neyman, editors, </editor> <booktitle> Proc. 5th Berkeley Symposium on Math., Stat., and Prob., </booktitle> <pages> page 281. </pages> <address> U. </address> <publisher> California Press, </publisher> <address> Berkeley, CA, </address> <year> 1967. </year>
Reference-contexts: A widely used technique for finding a locally optimal set of M centers is the k-means algorithm <ref> [18] </ref>: one starts with a random center configuration m (0) which is then updated using the rule: m (t+1) P N k P N k (2) where S k = fx i : kx i m k k &lt; kx i m j k; j 6= kg It has been shown
Reference: [19] <author> J. Maeda and K. Murata. </author> <title> Digital restoration of scintigraphic images by a two-step procedure. </title> <journal> IEEE Trans. on Medical Imaging, </journal> <volume> MI-6(4):320-324, </volume> <month> De-cember </month> <year> 1987. </year>
Reference-contexts: cases the training and test error were less than 5% with 3 centers (3 Gaussians per class), and less than 8% with one Gaussian per class. 3.3 Image Segmentation As a final example, we consider an image segmentation problem that arises in the processing of certain biomedical images: scintigraphic images <ref> [11, 19] </ref>, which are obtained by counting the number of radioactive particles that incide on each cell of a receptor array. The goal of the processing step is to obtain from these measurements an estimate of the radioisotope distribution in specific organs within the human body.
Reference: [20] <author> J. L. Marroquin, S. Mitter, and T. Poggio. </author> <title> Probabilistic solution of ill-posed problems in computational vision. </title> <journal> J. Amer. Stat. Assoc., </journal> <volume> 82 </volume> <pages> 76-89, </pages> <year> 1987. </year>
Reference-contexts: The basic idea in this approach is to express the prior constraint on the organization of the centers in probabilistic terms, specifically, in the form of a discrete Markov Random Field (MRF) model <ref> [20, 13, 3, 26, 7] </ref>, in which the center locations correspond to the state variables associated with the nodes of a graph whose topology is related |but not necessarily identical| to the desired neighborhood structure of the centers; in fact this structure, as well as the smoothness of the locations of <p> it were possible to find the boundaries of the organ in question (e.g., the heart), the problem would reduce to filtering a smooth function within a given domain, for which effective methods are available (for example, Bayesian estimation methods with MRF priors and quadratic potentials to model the smoothness constraint <ref> [20] </ref>). In the example that we give here, we show that it is possible to adapt the methods that we have presented to classify the pixels of a scintigraphic image of the heart in such a way that one class corresponds approximately to the interior of the organ.
Reference: [21] <author> T. Martinez and K. Schulten. </author> <title> A "Neural Gas" Network Learns Topology. </title> <editor> In T. Kohonen, K. Mak-isara, O. Simula, and J. Kangas, editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <pages> pages 397-402. </pages> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <year> 1991. </year>
Reference-contexts: suffers from some limitations: in the first place, it is difficult to analyze (except in some particular cases [25]), and thus to understand its performance in a precise way; besides, the neighborhood structure is imposed rather than found from the data (although some modifications have been proposed to this end <ref> [21] </ref>), which limits its usefulness in unsupervised clustering tasks. <p> and the "natural" neighborhood structure for a given data set by the average l jk of l (i) jk over all i: two centers (j; k) may be considered neighbors if l jk &gt; , for some appropriate threshold (a similar, although computa-tionally more expensive scheme may be found in <ref> [21] </ref>). several two-dimensional data sets. As one can see, it represents adequately the inner structure of the data, and so, it may be used for unsupervised clustering tasks. This soft-WTA algorithm may also be useful for improving the performance of the LKMA with clustered data.
Reference: [22] <author> J. Moody and C. Darken. </author> <title> Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 281-294, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Finding a set of representative vectors for clouds of multi-dimensional data is an important issue in data compression [9], signal coding [9, 8], pattern classification [5] and function approximation tasks <ref> [22, 24] </ref>. <p> (soft)" and "RBF (fixed) " correspond to the classifiers that are obtained by approximating the indicator function of one of the classes (say, class 2) with a linear combination of Gaussians with fixed covariance I (in all cases we used = 5:0), and centered at a fixed set of points <ref> [22] </ref>.
Reference: [23] <author> S.J. Nowlan. </author> <title> Soft competitive adaptation. </title> <type> Ph.D. Thesis CMU-CS-91-126, </type> <institution> Carnegie Mellon Univ., </institution> <address> Pittsburgh, PA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: may be obtained by using, instead of (6), an information-related distorsion measure [2]: E fi (m) = N i=1 " k=1 exp [fikx i m k k 2 ] # This error measure also corresponds to the log likelihood function of a Gaussian mixture model for the data distribution [10] <ref> [23] </ref>, where: the means of the Gaussians correspond to the center locations; the proportions are all equal to 1=N , and all the covariance matrices are equal to fi 1 I. <p> As one can see, the performance is significantly improved in the latter case, due to the fact that the center distribution is more uniform. A similar improvement has been reported if the standard k-means algorithm is replaced by the "soft-WTA" version (14) <ref> [23] </ref>. The performance of this scheme is included in the rows labeled "RBF (soft)", for comparison. As one can see, the local Gaussian classifier has the best performance on the test set.
Reference: [24] <author> T. Poggio and F. Girosi. </author> <title> Networks for approximation and learning. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(9), </volume> <month> September </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Finding a set of representative vectors for clouds of multi-dimensional data is an important issue in data compression [9], signal coding [9, 8], pattern classification [5] and function approximation tasks <ref> [22, 24] </ref>.
Reference: [25] <author> H. Ritter and K. Schulten. </author> <title> On the stationary state of kohonen's self-organizing sensory mapping. </title> <journal> Biological Cybernetics, </journal> <volume> 54 </volume> <pages> 99-106, </pages> <year> 1986. </year>
Reference-contexts: In Kohonen's work, the neighborhoods fN k (t)g are initially very large and shrink slowly to their final desired size (e.g., a nearest neighbor structure). 1 This scheme suffers from some limitations: in the first place, it is difficult to analyze (except in some particular cases <ref> [25] </ref>), and thus to understand its performance in a precise way; besides, the neighborhood structure is imposed rather than found from the data (although some modifications have been proposed to this end [21]), which limits its usefulness in unsupervised clustering tasks.
Reference: [26] <author> I.A. Rozanov. </author> <title> Markov random fields. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: The basic idea in this approach is to express the prior constraint on the organization of the centers in probabilistic terms, specifically, in the form of a discrete Markov Random Field (MRF) model <ref> [20, 13, 3, 26, 7] </ref>, in which the center locations correspond to the state variables associated with the nodes of a graph whose topology is related |but not necessarily identical| to the desired neighborhood structure of the centers; in fact this structure, as well as the smoothness of the locations of
Reference: [27] <author> A.N. Shiriaev. </author> <title> Probability, volume 95 of Graduate texts in mathematics. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: It is sufficient to show that the series P m i=n s i converges with probability one (Kushner and Clark, page 32). We use the following theorem by Kolmogorov and Khinchin (see <ref> [27] </ref>, page 359): 9 Theorem A.2 Let fz n g be a sequence of independent random variables with zero mean. Then, if 1 X E [z 2 the series P 1 n=1 z n converges with probability 1.
Reference: [28] <author> H. White. </author> <title> Learning in artificial neural networks: a statistical perspective. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 425-464, </pages> <year> 1989. </year>
Reference-contexts: In the limit as fi ! 1, (10) becomes precisely the LKMA (3). The convergence of (10) to a local minimum of E fi follows from the following lemma (the proof is given in the appendix; see also <ref> [28] </ref> for closely related results): Lemma 2.1 Let F (y) : R k 7! R be of the form: F (y) = f 0 (y) + N i=1 where f i are differentiable functions whose gradient is bounded and satisfies the following Lipschiz condition: krf i (y) rf i (y 0

References-found: 28

