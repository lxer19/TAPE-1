URL: http://www.deas.harvard.edu/~beimel/Papers/multiplicity.ps.gz
Refering-URL: http://www.deas.harvard.edu/~beimel/pub.html
Root-URL: 
Email: E-mail: beimel@deas.harvard.edu.  E-mail: bergadan@di.unito.it.  E-mail: bshouty@cpsc.ucalgary.ca.  E-mail: eyalk@cs.technion.ac.il.  E-mail: varricch@univaq.it.  
Title: Learning Functions Represented as Multiplicity Automata class of disjoint DNF, and more generally satisfy-O(1) DNF.
Author: Amos Beimel Francesco Bergadano Nader H. Bshouty Eyal Kushilevitz Stefano Varricchio k 
Web: http://www.deas.harvard.edu/~beimel.  http://www.cpsc.ucalgary.ca/~bshouty.  http://www.cs.technion.ac.il/~eyalk.  
Address: 40 Oxford st., Cambridge MA 02139, USA.  Torino.  Calgary, Calgary, Alberta, Canada.  32000, Israel.  L'Aquila.  
Affiliation: Division of Engineering Applied Sciences, Harvard University,  Technion. Universita di  Department of Computer Science, University of  Department of Computer Science, Technion, Haifa  Universita di  
Note: The  Part of this research was done while the author was a Ph.D. student at the  This research was funded in part by the European Community under grant 20237 (ILP2).  This research was supported by Technion V.P.R. Fund 120-872 and by Japan Technion Society Research Fund. k  
Date: April 23, 1998  
Abstract: We study the learnability of multiplicity automata in Angluin's exact learning model, and we investigate its applications. Our starting point is a known theorem from automata theory relating the number of states in a minimal multiplicity automaton for a function to the rank of its Hankel matrix. With this theorem in hand we present a new simple algorithm for learning multiplicity automata with improved time and query complexity, and we prove the learnability of various concept classes. These include (among others): fl This paper contains results from 3 conference papers. It is mostly based on the FOCS '96 paper by the authors. The rest of the results are from the STOC '96 paper of Bergadano, Catalano and Varricchio, and the EuroCOLT '97 paper of Beimel and Kushilevitz. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Aizenstein and L. Pitt. </author> <title> Exact learning of read-twice DNF formulas. </title> <booktitle> In Proc. of the 32nd Annu. IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 170-179, </pages> <year> 1991. </year>
Reference-contexts: Much work was therefore devoted to learning subclasses of DNF formulae <ref> [1, 2, 3, 6, 15, 16, 18, 20, 31, 38] </ref>. These subclasses are obtained by restricting the DNF formulae in various ways; e.g., by limiting the number of terms in the target formula or by limiting the number of appearances of each variable. <p> Read-once DNF. 5. k-term DNF, for k = !(log n). 6. Satisfy-s DNF, for s = !(1). 7. Read-j satisfy-s DNF, for j = !(1) and s = (log n). Some of these classes are known to be learnable by other methods (monotone DNF [5], read-once DNF <ref> [6, 1, 47] </ref> and 2-DNF [56]), some are natural generalizations of classes known to be learnable as automata (O (log n)-term DNF [16, 18, 20, 38], and satisfy-s DNF for s = O (1) (Corollary 4.7)) or by other methods (read-j satisfy-s for js = O (log n= log log n)
Reference: [2] <author> H. Aizenstein and L. Pitt. </author> <title> Exact learning of read-k disjoint DNF and not-so-disjoint DNF. </title> <booktitle> In Proc. of 5th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 71-76, </pages> <year> 1992. </year>
Reference-contexts: Much work was therefore devoted to learning subclasses of DNF formulae <ref> [1, 2, 3, 6, 15, 16, 18, 20, 31, 38] </ref>. These subclasses are obtained by restricting the DNF formulae in various ways; e.g., by limiting the number of terms in the target formula or by limiting the number of appearances of each variable. <p> A DNF formula with this property is called disjoint DNF. This class of formulae was the subject of previous research but, once again, only sub-classes of disjoint DNF were known to be learnable prior to our work <ref> [2, 15] </ref>. Automata: One of the first classes shown to be learnable in the exact learning model is that of deterministic automata [4]. <p> As mentioned, this class includes as a special case the class of decision trees. These results improve over previous results of <ref> [18, 2, 15] </ref>. Results on Geometric Boxes: An important generalization of DNF formulae is that of boxes over a discrete domain of points (i.e., f0; : : : ; ` 1g n ). Such boxes were considered in many works (e.g., [43, 44, 24, 7, 29, 33, 45]).
Reference: [3] <author> D. Angluin. </author> <title> Learning k-term DNF formulas using queries and counterexamples. </title> <type> Technical Report YALEU/DCS/RR-559, </type> <institution> Department of Computer Science, Yale University, </institution> <year> 1987. </year>
Reference-contexts: Much work was therefore devoted to learning subclasses of DNF formulae <ref> [1, 2, 3, 6, 15, 16, 18, 20, 31, 38] </ref>. These subclasses are obtained by restricting the DNF formulae in various ways; e.g., by limiting the number of terms in the target formula or by limiting the number of appearances of each variable.
Reference: [4] <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <year> 1987. </year>
Reference-contexts: This class of formulae was the subject of previous research but, once again, only sub-classes of disjoint DNF were known to be learnable prior to our work [2, 15]. Automata: One of the first classes shown to be learnable in the exact learning model is that of deterministic automata <ref> [4] </ref>. <p> They are also widely used in the theory of rational series in non-commuting variables. Multiplicity automata are a generalization of deterministic automata, and the algorithms that learn this class [12, 46] are generalizations of Angluin's algorithm for deterministic automata <ref> [4] </ref>. 1.1 Our Results In this work we find connections between the learnability questions of some of the abovementioned classes and other classes of interest. More precisely, we show that the learnability of multiplicity automata implies the learnability of many other important classes of functions.
Reference: [5] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: This task, in spite of enormous efforts, is still far from being accomplished under any of the major learning models. This work presents techniques and results which are targeted towards this goal, with respect to the important exact learning model. The exact learning model was introduced by Angluin <ref> [5] </ref> and since then attracted a lot of attention. <p> Therefore, learnability in the exact learning model also implies learnability in the PAC model with membership queries <ref> [56, 5] </ref>. Attempts to prove learnability of various classes in the exact learning model were made in several directions. <p> Since F x j (w) = f (x j ffiw), then by induction hypothesis this equals r X [ ] i;j [(w)] j ~fl = [() (w)] i ~fl = [(ffiw)] i ~fl; as needed. 8 2.2 The Learning Model The learning model we use is the exact learning model <ref> [5] </ref>: Let f be a target function. A learning algorithm may propose, in each step, a hypothesis function h by making an equivalence query (EQ) to an oracle. <p> Monotone DNF. 3. 2-DNF. 4. Read-once DNF. 5. k-term DNF, for k = !(log n). 6. Satisfy-s DNF, for s = !(1). 7. Read-j satisfy-s DNF, for j = !(1) and s = (log n). Some of these classes are known to be learnable by other methods (monotone DNF <ref> [5] </ref>, read-once DNF [6, 1, 47] and 2-DNF [56]), some are natural generalizations of classes known to be learnable as automata (O (log n)-term DNF [16, 18, 20, 38], and satisfy-s DNF for s = O (1) (Corollary 4.7)) or by other methods (read-j satisfy-s for js = O (log n=
Reference: [6] <author> D. Angluin, L. Hellerstein, and M. Karpinski. </author> <title> Learning read-once formulas with queries. </title> <journal> J. of the ACM, </journal> <volume> 40 </volume> <pages> 185-210, </pages> <year> 1993. </year>
Reference-contexts: Much work was therefore devoted to learning subclasses of DNF formulae <ref> [1, 2, 3, 6, 15, 16, 18, 20, 31, 38] </ref>. These subclasses are obtained by restricting the DNF formulae in various ways; e.g., by limiting the number of terms in the target formula or by limiting the number of appearances of each variable. <p> Read-once DNF. 5. k-term DNF, for k = !(log n). 6. Satisfy-s DNF, for s = !(1). 7. Read-j satisfy-s DNF, for j = !(1) and s = (log n). Some of these classes are known to be learnable by other methods (monotone DNF [5], read-once DNF <ref> [6, 1, 47] </ref> and 2-DNF [56]), some are natural generalizations of classes known to be learnable as automata (O (log n)-term DNF [16, 18, 20, 38], and satisfy-s DNF for s = O (1) (Corollary 4.7)) or by other methods (read-j satisfy-s for js = O (log n= log log n)
Reference: [7] <author> P. Auer. </author> <title> On-line learning of rectangles in noisy environments. </title> <booktitle> In Proc. of 6th Annu. ACM Conf. on Comput. Learning Theory, </booktitle> <pages> pages 253-261, </pages> <year> 1993. </year>
Reference-contexts: These results improve over previous results of [18, 2, 15]. Results on Geometric Boxes: An important generalization of DNF formulae is that of boxes over a discrete domain of points (i.e., f0; : : : ; ` 1g n ). Such boxes were considered in many works (e.g., <ref> [43, 44, 24, 7, 29, 33, 45] </ref>).
Reference: [8] <author> A. Beimel, F. Bergadano, N. H. Bshouty, E. Kushilevitz, and S. Varricchio. </author> <title> On the applications of multiplicity automata in learning. </title> <booktitle> In Proc. of the 37th Annu. IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 349-358, </pages> <year> 1996. </year> <month> 26 </month>
Reference: [9] <author> A. Beimel and E. Kushilevitz. </author> <title> Learning boxes in high dimension. </title> <editor> In S. Ben-David, editor, </editor> <booktitle> 3rd European Conf. on Computational Learning Theory (EuroCOLT '97), volume 1208 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 3-15. </pages> <publisher> Springer, </publisher> <year> 1997. </year> <note> Journal version to appear in Algorithmica. </note>
Reference-contexts: However, they are also of independent interest. In particular, the 2 These automata are known in the literature under various names. In this paper we refer to them as multiplicity automata. The functions computed by these automata are usually referred to as recognizable series. 3 In <ref> [9] </ref>, using additional machinery, the dependency on ` was improved. 4 question of learning polynomials from membership queries only is just the fundamental in-terpolation problem which was studied in numerous papers (see, e.g., [60]). <p> Methods that do use the specific properties of geometric boxes were developed in <ref> [9] </ref> and they lead to improved results. 4.3 Classes of DNF formulae In this section we present several results for classes of DNF formulae and some related classes.
Reference: [10] <author> M. Ben-Or and P. Tiwari. </author> <title> A deterministic algorithm for sparse multivariate polynomial interpolation. </title> <booktitle> In Proc. of the 20th Annu. ACM Symp. on the Theory of Computing, </booktitle> <pages> pages 301-309, </pages> <year> 1988. </year>
Reference-contexts: The algorithm fails if the simulation of one of the tn equivalence queries returned YES although the hypothesis is not equivalent to the target function. This happens with probability at most ". An algorithm which learns multivariate polynomials using only membership queries is called an interpolation algorithm (e.g. <ref> [10, 30, 59, 25, 51, 21, 32] </ref>; for more background and references see [60]). In [10] it is shown how to interpolate polynomials over infinite fields using only 2t membership queries. <p> This happens with probability at most ". An algorithm which learns multivariate polynomials using only membership queries is called an interpolation algorithm (e.g. [10, 30, 59, 25, 51, 21, 32]; for more background and references see [60]). In <ref> [10] </ref> it is shown how to interpolate polynomials over infinite fields using only 2t membership queries.
Reference: [11] <author> F. Bergadano, D. Catalano, and S. Varricchio. </author> <title> Learning sat-k-DNF formulas from membership queries. </title> <booktitle> In Proc. of the 28th Annu. ACM Symp. on the Theory of Computing, </booktitle> <pages> pages 126-130, </pages> <year> 1996. </year>
Reference: [12] <author> F. Bergadano and S. Varricchio. </author> <title> Learning behaviors of automata from multiplicity and equivalence queries. </title> <booktitle> In Proc. of 2nd Italian Conf. on Algorithms and Complexity, volume 778 of Lecture Notes in Computer Science, </booktitle> <pages> pages 54-62, </pages> <year> 1994. </year> <title> Journal version: </title> <journal> SIAM Journal on Computing, </journal> <volume> 25(6) </volume> <pages> 1268-1280, </pages> <year> 1996. </year>
Reference-contexts: This result of Angluin was extended by <ref> [12] </ref> and [46] where the class of multiplicity automata was shown to be learnable in the exact learning model. Multiplicity automata are essentially nondeterministic automata with weights from a field K on the edges. <p> They are also widely used in the theory of rational series in non-commuting variables. Multiplicity automata are a generalization of deterministic automata, and the algorithms that learn this class <ref> [12, 46] </ref> are generalizations of Angluin's algorithm for deterministic automata [4]. 1.1 Our Results In this work we find connections between the learnability questions of some of the abovementioned classes and other classes of interest. <p> Using this theorem, and ideas from the algorithm of [50] (for learning deterministic automata), we develop a new algorithm for learning multiplicity automata which is more efficient than the algorithms of <ref> [12, 46] </ref>. In particular we give a more refined analysis for the complexity of our algorithm when learning functions f with finite domain. <p> All together, the running time is at most O (jj r M (r) + m r 3 ). The complexity of our algorithm should be compared to the complexity of the algorithm of <ref> [12] </ref> which uses r equivalence queries, O (jjmr 2 ) membership queries, and runs in time O (jjm 2 r 5 ). The algorithm of [46] uses r +1 equivalence queries, O ((jj+m)r 2 ) membership queries, and runs in time O ((jj + m)r 4 ). <p> We show that our algorithm, as well as any algorithm whose complexity is polynomial in the size of the automaton (such as the algorithms in <ref> [12, 46] </ref>), does not efficiently learn several important classes of functions. More precisely, we show that these classes contain functions f that have no "small" automaton. By Theorem 2.4, it is enough to prove that the rank of the corresponding Hankel matrix F is "large" over every field K.
Reference: [13] <author> F. Bergadano and S. Varricchio. </author> <title> Learning behaviors of automata from shortest counterexamples. </title> <booktitle> In EuroCOLT '95, volume 904 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 380-391, </pages> <year> 1996. </year>
Reference: [14] <author> J. Berstel and C. Reutenauer. </author> <title> Rational Series and Their Languages, </title> <booktitle> volume 12 of EATCS monograph on Theoretical Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: This approach is based on a fundamental theorem in the theory of multiplicity automata. The theorem relates the size of a smallest automaton for a function f to the rank (over K) of the so-called Hankel matrix of f [23, 28] (see also <ref> [27, 14] </ref> for background on multiplicity automata). Using this theorem, and ideas from the algorithm of [50] (for learning deterministic automata), we develop a new algorithm for learning multiplicity automata which is more efficient than the algorithms of [12, 46].
Reference: [15] <author> A. Blum, R. Khardon, E. Kushilevitz, L. Pitt, and D. Roth. </author> <booktitle> On learning read-k-satisfy-j DNF. In Proc. of 7th Annu. ACM Conf. on Comput. Learning Theory, </booktitle> <pages> pages 110-117, </pages> <year> 1994. </year>
Reference-contexts: Much work was therefore devoted to learning subclasses of DNF formulae <ref> [1, 2, 3, 6, 15, 16, 18, 20, 31, 38] </ref>. These subclasses are obtained by restricting the DNF formulae in various ways; e.g., by limiting the number of terms in the target formula or by limiting the number of appearances of each variable. <p> A DNF formula with this property is called disjoint DNF. This class of formulae was the subject of previous research but, once again, only sub-classes of disjoint DNF were known to be learnable prior to our work <ref> [2, 15] </ref>. Automata: One of the first classes shown to be learnable in the exact learning model is that of deterministic automata [4]. <p> As mentioned, this class includes as a special case the class of decision trees. These results improve over previous results of <ref> [18, 2, 15] </ref>. Results on Geometric Boxes: An important generalization of DNF formulae is that of boxes over a discrete domain of points (i.e., f0; : : : ; ` 1g n ). Such boxes were considered in many works (e.g., [43, 44, 24, 7, 29, 33, 45]). <p> 1, 47] and 2-DNF [56]), some are natural generalizations of classes known to be learnable as automata (O (log n)-term DNF [16, 18, 20, 38], and satisfy-s DNF for s = O (1) (Corollary 4.7)) or by other methods (read-j satisfy-s for js = O (log n= log log n) <ref> [15] </ref>), and the learnability of some of the others is still an open problem. Proof: Observe that f n;n=2 belongs to each of the classes DNF, monotone DNF, 2-DNF, read-once DNF and that by the above argument every automaton for it has size at least 2 n=2 .
Reference: [16] <author> A. Blum and S. Rudich. </author> <title> Fast learning of k-term DNF formulas with queries. </title> <journal> J. of Computer and System Sciences, </journal> <volume> 51(3) </volume> <pages> 367-373, </pages> <year> 1995. </year>
Reference-contexts: Much work was therefore devoted to learning subclasses of DNF formulae <ref> [1, 2, 3, 6, 15, 16, 18, 20, 31, 38] </ref>. These subclasses are obtained by restricting the DNF formulae in various ways; e.g., by limiting the number of terms in the target formula or by limiting the number of appearances of each variable. <p> Some of these classes are known to be learnable by other methods (monotone DNF [5], read-once DNF [6, 1, 47] and 2-DNF [56]), some are natural generalizations of classes known to be learnable as automata (O (log n)-term DNF <ref> [16, 18, 20, 38] </ref>, and satisfy-s DNF for s = O (1) (Corollary 4.7)) or by other methods (read-j satisfy-s for js = O (log n= log log n) [15]), and the learnability of some of the others is still an open problem.
Reference: [17] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: Moreover, it might as well be the case that the whole class of DNF formulae is not at all learnable. Another class whose learnability is of a wide interest in practice is that of (boolean) decision trees <ref> [17, 48, 49] </ref>. This class was shown to be learnable in the exact learning model in [18]. It is not hard to see that every decision tree can be transformed into a DNF formulae of essentially the same size.
Reference: [18] <author> N. H. Bshouty. </author> <title> Exact learning via the monotone theory. </title> <booktitle> In Proc. of the 34th Annu. IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 302-311, </pages> <year> 1993. </year> <title> Journal version: </title> <journal> Information and Computation, </journal> <volume> 123(1) </volume> <pages> 146-153, </pages> <year> 1995. </year>
Reference-contexts: Much work was therefore devoted to learning subclasses of DNF formulae <ref> [1, 2, 3, 6, 15, 16, 18, 20, 31, 38] </ref>. These subclasses are obtained by restricting the DNF formulae in various ways; e.g., by limiting the number of terms in the target formula or by limiting the number of appearances of each variable. <p> Another class whose learnability is of a wide interest in practice is that of (boolean) decision trees [17, 48, 49]. This class was shown to be learnable in the exact learning model in <ref> [18] </ref>. It is not hard to see that every decision tree can be transformed into a DNF formulae of essentially the same size. Hence, this class can also be considered as a subclass of DNF formulae. <p> As mentioned, this class includes as a special case the class of decision trees. These results improve over previous results of <ref> [18, 2, 15] </ref>. Results on Geometric Boxes: An important generalization of DNF formulae is that of boxes over a discrete domain of points (i.e., f0; : : : ; ` 1g n ). Such boxes were considered in many works (e.g., [43, 44, 24, 7, 29, 33, 45]). <p> This in particular includes the class of Decision-trees. By using our algorithm, decision trees of size t on n variables are learnable using O (tn) equivalence queries and O (t 2 n log n) membership queries. This is better than the best known algorithm for decision trees <ref> [18] </ref> (which uses O (t 2 ) equivalence queries and O (t 2 n 2 ) membership queries). In what follows we consider more general classes of decision trees. <p> Then g ` = Q n j=1 p `;j . By Corollary 4.2 the result follows. 20 The above result implies as a special case the learnability of decision trees with "greater--than" queries in the nodes. This is an open problem of <ref> [18] </ref>. Note that every decision tree with "greater-than" queries that computes a boolean function can be expressed as the union of disjoint boxes. Hence, this case can also be derived from Corollary 4.5. The next theorem will be used to learn more classes of decision trees. <p> Some of these classes are known to be learnable by other methods (monotone DNF [5], read-once DNF [6, 1, 47] and 2-DNF [56]), some are natural generalizations of classes known to be learnable as automata (O (log n)-term DNF <ref> [16, 18, 20, 38] </ref>, and satisfy-s DNF for s = O (1) (Corollary 4.7)) or by other methods (read-j satisfy-s for js = O (log n= log log n) [15]), and the learnability of some of the others is still an open problem.
Reference: [19] <author> N. H. Bshouty. </author> <title> A note on learning multivariate polynomials under the uniform distribution. </title> <booktitle> In Proc. of 8th Annu. ACM Conf. on Comput. Learning Theory, </booktitle> <pages> pages 79-82, </pages> <year> 1995. </year>
Reference-contexts: We further show the learnability of the class of XOR of terms, which is an open problem in [52], the class of polynomials over finite fields, which is an open problem in <ref> [52, 19] </ref>, and the class of bounded-degree polynomials over infinite fields (as well as other classes of functions over finite and infinite fields). Techniques: We use an algebraic approach for learning multiplicity automata, similar to [46]. This approach is based on a fundamental theorem in the theory of multiplicity automata.
Reference: [20] <author> N. H. Bshouty. </author> <title> Simple learning algorithms using divide and conquer. </title> <booktitle> In Proc. of 8th Annu. ACM Conf. on Comput. Learning Theory, </booktitle> <pages> pages 447-453, </pages> <year> 1995. </year> <note> Journal version: Computational Complexity, 6 174-194,1997. </note>
Reference-contexts: Much work was therefore devoted to learning subclasses of DNF formulae <ref> [1, 2, 3, 6, 15, 16, 18, 20, 31, 38] </ref>. These subclasses are obtained by restricting the DNF formulae in various ways; e.g., by limiting the number of terms in the target formula or by limiting the number of appearances of each variable. <p> Some of these classes are known to be learnable by other methods (monotone DNF [5], read-once DNF [6, 1, 47] and 2-DNF [56]), some are natural generalizations of classes known to be learnable as automata (O (log n)-term DNF <ref> [16, 18, 20, 38] </ref>, and satisfy-s DNF for s = O (1) (Corollary 4.7)) or by other methods (read-j satisfy-s for js = O (log n= log log n) [15]), and the learnability of some of the others is still an open problem.
Reference: [21] <author> N. H. Bshouty and Y. Mansour. </author> <title> Simple learning algorithms for decision trees and multivariate polynomials. </title> <booktitle> In Proc. of the 36th Annu. IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 304-311, </pages> <year> 1995. </year>
Reference-contexts: The algorithm fails if the simulation of one of the tn equivalence queries returned YES although the hypothesis is not equivalent to the target function. This happens with probability at most ". An algorithm which learns multivariate polynomials using only membership queries is called an interpolation algorithm (e.g. <ref> [10, 30, 59, 25, 51, 21, 32] </ref>; for more background and references see [60]). In [10] it is shown how to interpolate polynomials over infinite fields using only 2t membership queries. <p> In [10] it is shown how to interpolate polynomials over infinite fields using only 2t membership queries. Algorithms for interpolating polynomials over finite fields are given in <ref> [21, 32] </ref> provided that the fields are "big" enough (in [32] the field must contain (t 2 k + tkn 2 ) elements and in [21] the field must contain (kn= log kn) elements). We require that the number of elements in the field is at least kn + 1. <p> Algorithms for interpolating polynomials over finite fields are given in [21, 32] provided that the fields are "big" enough (in [32] the field must contain (t 2 k + tkn 2 ) elements and in <ref> [21] </ref> the field must contain (kn= log kn) elements). We require that the number of elements in the field is at least kn + 1.
Reference: [22] <author> N. H. Bshouty, C. Tamon, and D. K. Wilson. </author> <title> Learning matrix functions over rings. </title> <booktitle> In Proc. of 3rd EuroCOLT, </booktitle> <year> 1997. </year>
Reference-contexts: In particular we give a more refined analysis for the complexity of our algorithm when learning functions f with finite domain. A different algorithm with similar complexity to ours was found by <ref> [22] </ref>. 4 Negative Results: While multiplicity automata are proved to be useful to solve many open problems regarding the learnability of subclasses of DNF and other classes of polynomials and decision trees, we study the limitations of this method. <p> These negative results are proven by exhibiting functions in the above classes that require multiplicity automata with super-polynomial number of states. For proving these results we use, again, the relation between multiplicity automata and Hankel matrices. 4 In fact, <ref> [22] </ref> show that the algorithm can be generalized to K which is not necessarily a field but rather a certain type of ring. 5 1.2 Organization In Section 2 we present some background on multiplicity automata, as well as the definition of the learning model.
Reference: [23] <author> J. W. Carlyle and A. Paz. </author> <title> Realization by stochastic finite automaton. </title> <journal> J. of Computer and System Sciences, </journal> <volume> 5 </volume> <pages> 26-40, </pages> <year> 1971. </year>
Reference-contexts: This approach is based on a fundamental theorem in the theory of multiplicity automata. The theorem relates the size of a smallest automaton for a function f to the rank (over K) of the so-called Hankel matrix of f <ref> [23, 28] </ref> (see also [27, 14] for background on multiplicity automata). Using this theorem, and ideas from the algorithm of [50] (for learning deterministic automata), we develop a new algorithm for learning multiplicity automata which is more efficient than the algorithms of [12, 46]. <p> The following theorem of <ref> [23, 28] </ref> is a fundamental theorem from the theory of formal series. It relates the size of the minimal automaton for f to the rank of F . Theorem 2.4 ([23, 28]) Let f : fl ! K such that f 6 0 and let F be the corresponding Hankel matrix.
Reference: [24] <author> Z. Chen and W. Maass. </author> <title> On-line learning of rectangles. </title> <booktitle> In Proc. of 5th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <year> 1992. </year>
Reference-contexts: These results improve over previous results of [18, 2, 15]. Results on Geometric Boxes: An important generalization of DNF formulae is that of boxes over a discrete domain of points (i.e., f0; : : : ; ` 1g n ). Such boxes were considered in many works (e.g., <ref> [43, 44, 24, 7, 29, 33, 45] </ref>).
Reference: [25] <author> M. Clausen, A. Dress, J. Grabmeier, and M. Karpinski. </author> <title> On zero-testing and interpolation of k-sparse multivariate polynomials over finite fields. </title> <journal> Theoretical Computer Science, </journal> <volume> 84(2) </volume> <pages> 151-164, </pages> <year> 1991. </year>
Reference-contexts: The algorithm fails if the simulation of one of the tn equivalence queries returned YES although the hypothesis is not equivalent to the target function. This happens with probability at most ". An algorithm which learns multivariate polynomials using only membership queries is called an interpolation algorithm (e.g. <ref> [10, 30, 59, 25, 51, 21, 32] </ref>; for more background and references see [60]). In [10] it is shown how to interpolate polynomials over infinite fields using only 2t membership queries. <p> k while in the previous papers each monomial is a product of univariate polynomials of degree k with only one term. 12 To complete the discussion we should mention that if the number of elements in the field is less than k then every efficient algorithm must use equivalence queries <ref> [25, 51] </ref>. 4.2 Classes of Boxes In this section we consider unions of n-dimensional boxes in [`] n (where [`] denotes the set f0; 1; : : : ; ` 1g).
Reference: [26] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to algorithms. </title> <publisher> MIT Press and McGraw-Hill Book Company, </publisher> <year> 1990. </year>
Reference-contexts: Therefore, finding b can be done with one matrix inversion, whose cost is also O (M (r)) (see, e.g., <ref> [26, Theorem 31.11] </ref>), and one matrix multiplication. Hence the complexity of Step 2 is O (jj M (r)). In Step 3 the difficult part is to compute the value of [ b (z)] 1 for 12 the counterexample z.
Reference: [27] <author> S. Eilenberg. </author> <title> Automata, Languages and Machines, volume A. </title> <publisher> Academic Press, </publisher> <year> 1974. </year>
Reference-contexts: This approach is based on a fundamental theorem in the theory of multiplicity automata. The theorem relates the size of a smallest automaton for a function f to the rank (over K) of the so-called Hankel matrix of f [23, 28] (see also <ref> [27, 14] </ref> for background on multiplicity automata). Using this theorem, and ideas from the algorithm of [50] (for learning deterministic automata), we develop a new algorithm for learning multiplicity automata which is more efficient than the algorithms of [12, 46].
Reference: [28] <author> M. Fliess. </author> <title> Matrices de Hankel. </title> <journal> J. Math. Pures Appl., </journal> <volume> 53 </volume> <pages> 197-222, </pages> <year> 1974. </year> <note> Erratum in vol. 54. </note>
Reference-contexts: This approach is based on a fundamental theorem in the theory of multiplicity automata. The theorem relates the size of a smallest automaton for a function f to the rank (over K) of the so-called Hankel matrix of f <ref> [23, 28] </ref> (see also [27, 14] for background on multiplicity automata). Using this theorem, and ideas from the algorithm of [50] (for learning deterministic automata), we develop a new algorithm for learning multiplicity automata which is more efficient than the algorithms of [12, 46]. <p> The following theorem of <ref> [23, 28] </ref> is a fundamental theorem from the theory of formal series. It relates the size of the minimal automaton for f to the rank of F . Theorem 2.4 ([23, 28]) Let f : fl ! K such that f 6 0 and let F be the corresponding Hankel matrix.
Reference: [29] <author> P. W. Goldberg, S. A. Goldman, and H. D. Mathias. </author> <title> Learning unions of boxes with membership and equivalence queries. </title> <booktitle> In Proc. of 7th Annu. ACM Conf. on Comput. Learning Theory, </booktitle> <year> 1994. </year>
Reference-contexts: These results improve over previous results of [18, 2, 15]. Results on Geometric Boxes: An important generalization of DNF formulae is that of boxes over a discrete domain of points (i.e., f0; : : : ; ` 1g n ). Such boxes were considered in many works (e.g., <ref> [43, 44, 24, 7, 29, 33, 45] </ref>).
Reference: [30] <author> D. Y. Grigoriev, M. Karpinski, and M. F. Singer. </author> <title> Fast parallel algorithms for sparse multivariate polynomial interpolation over finite fields. </title> <journal> SIAM Journal on Computing, </journal> <volume> 19(6) </volume> <pages> 1059-1063, </pages> <year> 1990. </year>
Reference-contexts: The algorithm fails if the simulation of one of the tn equivalence queries returned YES although the hypothesis is not equivalent to the target function. This happens with probability at most ". An algorithm which learns multivariate polynomials using only membership queries is called an interpolation algorithm (e.g. <ref> [10, 30, 59, 25, 51, 21, 32] </ref>; for more background and references see [60]). In [10] it is shown how to interpolate polynomials over infinite fields using only 2t membership queries.
Reference: [31] <author> T. R. Hancock. </author> <title> Learning 2 DNF formulas and k decision trees. </title> <booktitle> In Proc. of 4th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 199-209, </pages> <year> 1991. </year> <month> 28 </month>
Reference-contexts: Much work was therefore devoted to learning subclasses of DNF formulae <ref> [1, 2, 3, 6, 15, 16, 18, 20, 31, 38] </ref>. These subclasses are obtained by restricting the DNF formulae in various ways; e.g., by limiting the number of terms in the target formula or by limiting the number of appearances of each variable.
Reference: [32] <author> M. A. Huang and A. J. Rao. </author> <title> Interpolation of sparse multivariate polynomials over large finite fields with applications. </title> <booktitle> In Proc. of the 7th Annu. ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 508-517, </pages> <year> 1996. </year>
Reference-contexts: The algorithm fails if the simulation of one of the tn equivalence queries returned YES although the hypothesis is not equivalent to the target function. This happens with probability at most ". An algorithm which learns multivariate polynomials using only membership queries is called an interpolation algorithm (e.g. <ref> [10, 30, 59, 25, 51, 21, 32] </ref>; for more background and references see [60]). In [10] it is shown how to interpolate polynomials over infinite fields using only 2t membership queries. <p> In [10] it is shown how to interpolate polynomials over infinite fields using only 2t membership queries. Algorithms for interpolating polynomials over finite fields are given in <ref> [21, 32] </ref> provided that the fields are "big" enough (in [32] the field must contain (t 2 k + tkn 2 ) elements and in [21] the field must contain (kn= log kn) elements). We require that the number of elements in the field is at least kn + 1. <p> In [10] it is shown how to interpolate polynomials over infinite fields using only 2t membership queries. Algorithms for interpolating polynomials over finite fields are given in [21, 32] provided that the fields are "big" enough (in <ref> [32] </ref> the field must contain (t 2 k + tkn 2 ) elements and in [21] the field must contain (kn= log kn) elements). We require that the number of elements in the field is at least kn + 1.
Reference: [33] <author> J. C. Jackson. </author> <title> An efficient membership-query algorithm for learning DNF with respect to the uniform distribution. </title> <journal> J. of Computer and System Sciences, </journal> <volume> 55(3) </volume> <pages> 414-440, </pages> <year> 1997. </year>
Reference-contexts: The value f (x) computed by the automaton is essentially the sum of the weights of the paths consistent with the input string x (this sum 1 With the exception of <ref> [33] </ref> who showed the learnability of this class using membership queries with respect to the uniform distribution. 3 is a value in K). 2 Multiplicity automata were first introduced by Shutzenberger [54], and have been used as a mathematical tool for modeling probabilistic automata and ambiguity in nondeterministic automata. <p> These results improve over previous results of [18, 2, 15]. Results on Geometric Boxes: An important generalization of DNF formulae is that of boxes over a discrete domain of points (i.e., f0; : : : ; ` 1g n ). Such boxes were considered in many works (e.g., <ref> [43, 44, 24, 7, 29, 33, 45] </ref>).
Reference: [34] <author> M. J. Kearns and L. G. Valiant. </author> <title> Cryptographic limitations on learning boolean formula and finite automata. </title> <journal> Journal of the ACM, </journal> <volume> 41(1) </volume> <pages> 67-95, </pages> <year> 1994. </year>
Reference-contexts: This class is interesting both from the practical point of view, since finite state machines model many behaviors which we wish to learn in practice (see [55, 41] and references therein), and since this particular class in known to be hard to learn from examples only, under common cryptographic assumptions <ref> [34] </ref>. This result of Angluin was extended by [12] and [46] where the class of multiplicity automata was shown to be learnable in the exact learning model. Multiplicity automata are essentially nondeterministic automata with weights from a field K on the edges.
Reference: [35] <author> M. J. Kearns and U. V. Vazirani. </author> <title> An Introduction to Computational Learning Theory. </title> <publisher> MIT press, </publisher> <year> 1994. </year>
Reference: [36] <author> M. Kharitonov. </author> <title> Cryptographic hardness of distribution-specific learning. </title> <booktitle> In Proc. of the 25th Annu. ACM Symp. on the Theory of Computing, </booktitle> <pages> pages 372-381, </pages> <year> 1993. </year>
Reference-contexts: Attempts to prove learnability of various classes in the exact learning model were made in several directions. In particular, the learnability of DNF formulae and various kinds of automata attracted a lot of attention. 2 DNF Formulae: While general formulae are not learnable in this model <ref> [36] </ref>, as well as in the other major models of learning, researchers concentrated on the question of learning the class of DNF (Disjunctive Normal Form) formulae.
Reference: [37] <author> D. E. Knuth. </author> <booktitle> The Art of Computer Programming, Volume 2: Seminumerical Algorithms. </booktitle> <publisher> Addison-Wesley, </publisher> <address> third edition, </address> <year> 1998. </year>
Reference-contexts: We summarize the analysis of the algorithm by the following theorem. Let m denote the size of the longest counterexample z obtained during the execution of the algorithm. Denote by M (r) = O (r 2:376 ) the complexity of multiplying two r fi r matrices (see, e.g., <ref> [37, pages 499-501] </ref> for discussion on matrix multiplication). Theorem 3.3 Let K be a field, and f : fl ! K be a function such that r = rank (F ) (over K).
Reference: [38] <author> E. Kushilevitz. </author> <title> A simple algorithm for learning O(log n)-term DNF. </title> <booktitle> In Proc. of 9th Annu. ACM Conf. on Comput. Learning Theory, </booktitle> <pages> pages 266-269, </pages> <year> 1996. </year> <title> Journal version: </title> <journal> Inform. Process. Lett., </journal> <volume> 61(6) </volume> <pages> 289-292, </pages> <year> 1997. </year>
Reference-contexts: Much work was therefore devoted to learning subclasses of DNF formulae <ref> [1, 2, 3, 6, 15, 16, 18, 20, 31, 38] </ref>. These subclasses are obtained by restricting the DNF formulae in various ways; e.g., by limiting the number of terms in the target formula or by limiting the number of appearances of each variable. <p> More precisely, we show that the learnability of multiplicity automata implies the learnability of many other important classes of functions. In <ref> [38] </ref> it is shown how the learnability of deterministic automata can be used to learn certain classes of functions. These classes however are much more restricted and hence it yields much weaker results. Below we give a detailed account of our results. <p> This defines a multiplicity automaton which computes the characteristic function of the language of the deterministic (or unambiguous) automaton. 11 By <ref> [38] </ref>, the class of deterministic automata contains the class of O (log n)-term DNF and in fact the class of all boolean functions over O (log n) terms. Hence, all these classes can be learned by our algorithm. <p> Some of these classes are known to be learnable by other methods (monotone DNF [5], read-once DNF [6, 1, 47] and 2-DNF [56]), some are natural generalizations of classes known to be learnable as automata (O (log n)-term DNF <ref> [16, 18, 20, 38] </ref>, and satisfy-s DNF for s = O (1) (Corollary 4.7)) or by other methods (read-j satisfy-s for js = O (log n= log log n) [15]), and the learnability of some of the others is still an open problem.
Reference: [39] <author> E. Kushilevitz and Y. Mansour. </author> <title> Learning decision trees using the Fourier spectrum. </title> <journal> SIAM J. on Computing, </journal> <volume> 22(6) </volume> <pages> 1331-1348, </pages> <year> 1993. </year>
Reference-contexts: The corollary follows. The above class contains, for example, all the decision trees of depth O (log n) that contain in each node a term or a XOR of a subset of variables as defined in <ref> [39] </ref> (the fact that r max 2 for XOR of a subset of variables follows from the proof of Corollary 4.11). 5 Negative Results The purpose of this section is to study some limitation of the learnability via the automaton representation.
Reference: [40] <author> E. Kushilevitz and N. Nisan. </author> <title> Communication Complexity. </title> <publisher> Cambridge university Press, </publisher> <year> 1997. </year>
Reference-contexts: The technique we use next is also similar to methods used in variable partition communication complexity. For background see, e.g., <ref> [42, 40] </ref>. 24 of t. For this, look at the order that induces on z 0 ; : : : ; z 2k1 (ignoring w 0 ; : : : ; w k1 ).
Reference: [41] <author> K. Lang. </author> <title> Random DFA's can be approximately learned from sparse uniform examples. </title> <booktitle> In Proc. of 5th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 45-52, </pages> <year> 1992. </year>
Reference-contexts: Automata: One of the first classes shown to be learnable in the exact learning model is that of deterministic automata [4]. This class is interesting both from the practical point of view, since finite state machines model many behaviors which we wish to learn in practice (see <ref> [55, 41] </ref> and references therein), and since this particular class in known to be hard to learn from examples only, under common cryptographic assumptions [34].
Reference: [42] <author> T. Lengauer. </author> <title> VLSI theory. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume A, chapter 16, </booktitle> <pages> pages 835-868. </pages> <publisher> Elsevier and The MIT press, </publisher> <year> 1990. </year>
Reference-contexts: The technique we use next is also similar to methods used in variable partition communication complexity. For background see, e.g., <ref> [42, 40] </ref>. 24 of t. For this, look at the order that induces on z 0 ; : : : ; z 2k1 (ignoring w 0 ; : : : ; w k1 ).
Reference: [43] <author> W. Maass and G. Turan. </author> <title> On the complexity of learning from counterexamples. </title> <booktitle> In Proc. of the 30th Annu. IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 262-273, </pages> <year> 1989. </year>
Reference-contexts: These results improve over previous results of [18, 2, 15]. Results on Geometric Boxes: An important generalization of DNF formulae is that of boxes over a discrete domain of points (i.e., f0; : : : ; ` 1g n ). Such boxes were considered in many works (e.g., <ref> [43, 44, 24, 7, 29, 33, 45] </ref>).
Reference: [44] <author> W. Maass and G. Turan. </author> <title> Algorithms and lower bounds for on-line learning of geometrical concepts. </title> <journal> Machine Learning, </journal> <volume> 14:251 - 269, </volume> <year> 1994. </year> <month> 29 </month>
Reference-contexts: These results improve over previous results of [18, 2, 15]. Results on Geometric Boxes: An important generalization of DNF formulae is that of boxes over a discrete domain of points (i.e., f0; : : : ; ` 1g n ). Such boxes were considered in many works (e.g., <ref> [43, 44, 24, 7, 29, 33, 45] </ref>).
Reference: [45] <author> W. Maass and M. K. Warmuth. </author> <title> Efficient learning with virtual threshold gates. </title> <journal> Infor--mation and Computation, </journal> <volume> 141(1) </volume> <pages> 66-83, </pages> <year> 1998. </year>
Reference-contexts: These results improve over previous results of [18, 2, 15]. Results on Geometric Boxes: An important generalization of DNF formulae is that of boxes over a discrete domain of points (i.e., f0; : : : ; ` 1g n ). Such boxes were considered in many works (e.g., <ref> [43, 44, 24, 7, 29, 33, 45] </ref>).
Reference: [46] <author> H. Ohnishi, H. Seki, and T. Kasami. </author> <title> A polynomial time learning algorithm for recognizable series. </title> <journal> IEICE Transactions on Information and Systems, </journal> <volume> E77-D(10)(5):1077-1085, </volume> <year> 1994. </year>
Reference-contexts: This result of Angluin was extended by [12] and <ref> [46] </ref> where the class of multiplicity automata was shown to be learnable in the exact learning model. Multiplicity automata are essentially nondeterministic automata with weights from a field K on the edges. <p> They are also widely used in the theory of rational series in non-commuting variables. Multiplicity automata are a generalization of deterministic automata, and the algorithms that learn this class <ref> [12, 46] </ref> are generalizations of Angluin's algorithm for deterministic automata [4]. 1.1 Our Results In this work we find connections between the learnability questions of some of the abovementioned classes and other classes of interest. <p> Techniques: We use an algebraic approach for learning multiplicity automata, similar to <ref> [46] </ref>. This approach is based on a fundamental theorem in the theory of multiplicity automata. <p> Using this theorem, and ideas from the algorithm of [50] (for learning deterministic automata), we develop a new algorithm for learning multiplicity automata which is more efficient than the algorithms of <ref> [12, 46] </ref>. In particular we give a more refined analysis for the complexity of our algorithm when learning functions f with finite domain. <p> The complexity of our algorithm should be compared to the complexity of the algorithm of [12] which uses r equivalence queries, O (jjmr 2 ) membership queries, and runs in time O (jjm 2 r 5 ). The algorithm of <ref> [46] </ref> uses r +1 equivalence queries, O ((jj+m)r 2 ) membership queries, and runs in time O ((jj + m)r 4 ). Our algorithm is similar to the algorithm of [46], however we use a binary search in Step 3 and we implement the algorithm more efficiently. 3.1 The Case of <p> The algorithm of <ref> [46] </ref> uses r +1 equivalence queries, O ((jj+m)r 2 ) membership queries, and runs in time O ((jj + m)r 4 ). Our algorithm is similar to the algorithm of [46], however we use a binary search in Step 3 and we implement the algorithm more efficiently. 3.1 The Case of Functions f : n ! K In many cases of interest the domain of the target function f is not fl but rather n for some value n. <p> We show that our algorithm, as well as any algorithm whose complexity is polynomial in the size of the automaton (such as the algorithms in <ref> [12, 46] </ref>), does not efficiently learn several important classes of functions. More precisely, we show that these classes contain functions f that have no "small" automaton. By Theorem 2.4, it is enough to prove that the rank of the corresponding Hankel matrix F is "large" over every field K.
Reference: [47] <author> K. Pillaipakkamnatt and V. Raghavan. </author> <title> Read-twice DNF formulas are properly learnable. </title> <journal> Information and Computation, </journal> <volume> 122(2) </volume> <pages> 236-267, </pages> <year> 1995. </year>
Reference-contexts: Read-once DNF. 5. k-term DNF, for k = !(log n). 6. Satisfy-s DNF, for s = !(1). 7. Read-j satisfy-s DNF, for j = !(1) and s = (log n). Some of these classes are known to be learnable by other methods (monotone DNF [5], read-once DNF <ref> [6, 1, 47] </ref> and 2-DNF [56]), some are natural generalizations of classes known to be learnable as automata (O (log n)-term DNF [16, 18, 20, 38], and satisfy-s DNF for s = O (1) (Corollary 4.7)) or by other methods (read-j satisfy-s for js = O (log n= log log n)
Reference: [48] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Moreover, it might as well be the case that the whole class of DNF formulae is not at all learnable. Another class whose learnability is of a wide interest in practice is that of (boolean) decision trees <ref> [17, 48, 49] </ref>. This class was shown to be learnable in the exact learning model in [18]. It is not hard to see that every decision tree can be transformed into a DNF formulae of essentially the same size.
Reference: [49] <author> J. R. Quinlan. C4.5: </author> <title> Programs for machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Moreover, it might as well be the case that the whole class of DNF formulae is not at all learnable. Another class whose learnability is of a wide interest in practice is that of (boolean) decision trees <ref> [17, 48, 49] </ref>. This class was shown to be learnable in the exact learning model in [18]. It is not hard to see that every decision tree can be transformed into a DNF formulae of essentially the same size.
Reference: [50] <author> R. L. Rivest and R. E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <journal> Information and Computation, </journal> <volume> 103 </volume> <pages> 299-347, </pages> <year> 1993. </year>
Reference-contexts: The theorem relates the size of a smallest automaton for a function f to the rank (over K) of the so-called Hankel matrix of f [23, 28] (see also [27, 14] for background on multiplicity automata). Using this theorem, and ideas from the algorithm of <ref> [50] </ref> (for learning deterministic automata), we develop a new algorithm for learning multiplicity automata which is more efficient than the algorithms of [12, 46]. In particular we give a more refined analysis for the complexity of our algorithm when learning functions f with finite domain.
Reference: [51] <author> R. M. Roth and G. M. Benedek. </author> <title> Interpolation and approximation of sparse multivariate polynomials over GF (2). </title> <journal> SIAM Journal on Computing, </journal> <volume> 20(2) </volume> <pages> 291-314, </pages> <year> 1991. </year>
Reference-contexts: The algorithm fails if the simulation of one of the tn equivalence queries returned YES although the hypothesis is not equivalent to the target function. This happens with probability at most ". An algorithm which learns multivariate polynomials using only membership queries is called an interpolation algorithm (e.g. <ref> [10, 30, 59, 25, 51, 21, 32] </ref>; for more background and references see [60]). In [10] it is shown how to interpolate polynomials over infinite fields using only 2t membership queries. <p> k while in the previous papers each monomial is a product of univariate polynomials of degree k with only one term. 12 To complete the discussion we should mention that if the number of elements in the field is less than k then every efficient algorithm must use equivalence queries <ref> [25, 51] </ref>. 4.2 Classes of Boxes In this section we consider unions of n-dimensional boxes in [`] n (where [`] denotes the set f0; 1; : : : ; ` 1g).
Reference: [52] <author> R. E. Schapire and L. M. Sellie. </author> <title> Learning sparse multivariate polynomials over a field with queries and counterexamples. </title> <journal> J. of Computer and System Sciences, </journal> <volume> 52(2) </volume> <pages> 201-213, </pages> <year> 1996. </year>
Reference-contexts: Since learning polynomials over small fields with membership queries only is not possible, it raises the question whether with the help of equivalence queries this becomes possible. Before the current work, it was known how to learn the class of multi-linear polynomials and polynomials over GF (2) <ref> [52] </ref>. We further show the learnability of the class of XOR of terms, which is an open problem in [52], the class of polynomials over finite fields, which is an open problem in [52, 19], and the class of bounded-degree polynomials over infinite fields (as well as other classes of functions <p> Before the current work, it was known how to learn the class of multi-linear polynomials and polynomials over GF (2) <ref> [52] </ref>. We further show the learnability of the class of XOR of terms, which is an open problem in [52], the class of polynomials over finite fields, which is an open problem in [52, 19], and the class of bounded-degree polynomials over infinite fields (as well as other classes of functions over finite and infinite fields). Techniques: We use an algebraic approach for learning multiplicity automata, similar to [46]. <p> We further show the learnability of the class of XOR of terms, which is an open problem in [52], the class of polynomials over finite fields, which is an open problem in <ref> [52, 19] </ref>, and the class of bounded-degree polynomials over infinite fields (as well as other classes of functions over finite and infinite fields). Techniques: We use an algebraic approach for learning multiplicity automata, similar to [46]. This approach is based on a fundamental theorem in the theory of multiplicity automata. <p> The above corollary implies as a special case the learnability of polynomials over GF (p). This extends the result of <ref> [52] </ref> from multi-linear polynomials to arbitrary polynomials. Our algorithm (see Corollary 3.4), for polynomials with n variables and t terms, uses O (nt) equivalence queries and O (t 2 n log n) membership queries. <p> Our algorithm (see Corollary 3.4), for polynomials with n variables and t terms, uses O (nt) equivalence queries and O (t 2 n log n) membership queries. The special case of the above class the class of polynomials over GF (2) was known to be learnable before <ref> [52] </ref>. Their algorithm uses O (nt) equivalence queries and O (t 3 n) membership queries (which is worse than ours for "most" values of t). <p> We first consider the following special case of Corollary 4.2 that solves an open problem of <ref> [52] </ref>: 19 Corollary 4.6 The class of functions that can be expressed as exclusive-OR of t (not nec-essarily monotone) monomials is learnable in time poly (n; t). While Corollary 4.6 does not refer to a subclass of DNF, it already implies the learnability of disjoint (i.e., satisfy-1) DNF.
Reference: [53] <author> J. T. Schwartz. </author> <title> Fast probabilistic algorithms for verification of polynomial identities. </title> <journal> J. of the ACM, </journal> <volume> 27 </volume> <pages> 701-717, </pages> <year> 1980. </year>
Reference-contexts: For a given ", the algorithm fails with probability ", and uses poly (n; t; log 1=") membership queries. We simulate each equivalence query in the previous algorithm using membership queries. To prove the correctness of the simulation, we use the Schwartz-Zippel Lemma <ref> [53, 58] </ref> which guarantees that two different polynomials in z 1 ; : : : ; z n of degree k (in each variable) can agree on at most knjLj n1 assignments in L n .
Reference: [54] <author> M. P. Shutzenberger. </author> <title> On the definition of a familiy of automata. </title> <journal> Information and Control, </journal> <volume> 4 </volume> <pages> 245-270, </pages> <year> 1961. </year>
Reference-contexts: of the weights of the paths consistent with the input string x (this sum 1 With the exception of [33] who showed the learnability of this class using membership queries with respect to the uniform distribution. 3 is a value in K). 2 Multiplicity automata were first introduced by Shutzenberger <ref> [54] </ref>, and have been used as a mathematical tool for modeling probabilistic automata and ambiguity in nondeterministic automata. They are also widely used in the theory of rational series in non-commuting variables.
Reference: [55] <author> B. A. Trakhtenbrot and Y. M. Barzdin. </author> <title> Finite Automata: Behavior and Synthesis. </title> <publisher> North-Holland, </publisher> <year> 1973. </year>
Reference-contexts: Automata: One of the first classes shown to be learnable in the exact learning model is that of deterministic automata [4]. This class is interesting both from the practical point of view, since finite state machines model many behaviors which we wish to learn in practice (see <ref> [55, 41] </ref> and references therein), and since this particular class in known to be hard to learn from examples only, under common cryptographic assumptions [34].
Reference: [56] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: It represents situations in which a learner who tries to learn some target function f is allowed to actively collect information regarding f ; this is in contrast to other major learning models, such as the Probably Approximately Correct (PAC) model of Valiant <ref> [56] </ref>, in which the learner tries to "learn" f by passively observing examples (of the form x; f (x)). <p> Therefore, learnability in the exact learning model also implies learnability in the PAC model with membership queries <ref> [56, 5] </ref>. Attempts to prove learnability of various classes in the exact learning model were made in several directions. <p> Satisfy-s DNF, for s = !(1). 7. Read-j satisfy-s DNF, for j = !(1) and s = (log n). Some of these classes are known to be learnable by other methods (monotone DNF [5], read-once DNF [6, 1, 47] and 2-DNF <ref> [56] </ref>), some are natural generalizations of classes known to be learnable as automata (O (log n)-term DNF [16, 18, 20, 38], and satisfy-s DNF for s = O (1) (Corollary 4.7)) or by other methods (read-j satisfy-s for js = O (log n= log log n) [15]), and the learnability of
Reference: [57] <author> L. G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proc. of the International Joint Conf. of Artificial Intelligence, </booktitle> <pages> pages 560-566, </pages> <year> 1985. </year>
Reference-contexts: of the major models of learning. 1 The attraction of this class stems from several facts: on one hand it seems like a simple, natural class that is only "slightly above" our current state of knowledge, and on the other hand it appears that people like it for representing knowledge <ref> [57] </ref>. Much work was therefore devoted to learning subclasses of DNF formulae [1, 2, 3, 6, 15, 16, 18, 20, 31, 38].
Reference: [58] <author> R. E. Zippel. </author> <title> Probabilistic algorithms for sparse polynomials. </title> <booktitle> In Proc. of the International Symp. on Symbolic and Algebraic Manipulation (EUROSAM '79), volume 72 of Lecture Notes in Computer Science, </booktitle> <pages> pages 216-226. </pages> <publisher> Springer-Verlag, </publisher> <year> 1979. </year> <month> 30 </month>
Reference-contexts: For a given ", the algorithm fails with probability ", and uses poly (n; t; log 1=") membership queries. We simulate each equivalence query in the previous algorithm using membership queries. To prove the correctness of the simulation, we use the Schwartz-Zippel Lemma <ref> [53, 58] </ref> which guarantees that two different polynomials in z 1 ; : : : ; z n of degree k (in each variable) can agree on at most knjLj n1 assignments in L n .
Reference: [59] <author> R. E. Zippel. </author> <title> Interpolating polynomials from their values. </title> <journal> J. of Symbolic Comp., </journal> <volume> 9:375--403, </volume> <year> 1990. </year>
Reference-contexts: The algorithm fails if the simulation of one of the tn equivalence queries returned YES although the hypothesis is not equivalent to the target function. This happens with probability at most ". An algorithm which learns multivariate polynomials using only membership queries is called an interpolation algorithm (e.g. <ref> [10, 30, 59, 25, 51, 21, 32] </ref>; for more background and references see [60]). In [10] it is shown how to interpolate polynomials over infinite fields using only 2t membership queries.
Reference: [60] <author> R. E. Zippel. </author> <title> Efficient Polynomial Computation. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year> <month> 31 </month>
Reference-contexts: The functions computed by these automata are usually referred to as recognizable series. 3 In [9], using additional machinery, the dependency on ` was improved. 4 question of learning polynomials from membership queries only is just the fundamental in-terpolation problem which was studied in numerous papers (see, e.g., <ref> [60] </ref>). Since learning polynomials over small fields with membership queries only is not possible, it raises the question whether with the help of equivalence queries this becomes possible. Before the current work, it was known how to learn the class of multi-linear polynomials and polynomials over GF (2) [52]. <p> This happens with probability at most ". An algorithm which learns multivariate polynomials using only membership queries is called an interpolation algorithm (e.g. [10, 30, 59, 25, 51, 21, 32]; for more background and references see <ref> [60] </ref>). In [10] it is shown how to interpolate polynomials over infinite fields using only 2t membership queries.
References-found: 60

