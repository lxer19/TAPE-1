URL: http://www-csag.cs.uiuc.edu/individual/achien/cs433/papers/logp.ps
Refering-URL: http://www-csag.cs.uiuc.edu/individual/achien/cs433/reading.html
Root-URL: http://www.cs.uiuc.edu
Title: LogP: Towards a Realistic Model of Parallel Computation  
Author: David Culler, Richard Karp David Patterson, Abhijit Sahay, Klaus Erik Schauser, Eunice Santos, Ramesh Subramonian, and Thorsten von Eicken 
Keyword: massively parallel processors, parallel models, complexity analysis, parallel algo rithms, PRAM  
Address: Berkeley  
Affiliation: Computer Science Division, University of California,  
Abstract: A vast body of theoretical research has focused either on overly simplistic models of parallel computation, notably the PRAM, or overly specific models that have few representatives in the real world. Both kinds of models encourage exploitation of formal loopholes, rather than rewarding development of techniques that yield performance across a range of current and future parallel machines. This paper offers a new parallel machine model, called LogP, that reflects the critical technology trends underlying parallel computers. It is intended to serve as a basis for developing fast, portable parallel algorithms and to offer guidelines to machine designers. Such a model must strike a balance between detail and simplicity in order to reveal important bottlenecks without making analysis of interesting problems intractable. The model is based on four parameters that specify abstractly the computing bandwidth, the communication bandwidth, the communication delay, and the efficiency of coupling communication and computation. Portable parallel algorithms typically adapt to the machine configuration, in terms of these parameters. The utility of the model is demonstrated through examples that are implemented on the CM-5. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggarwal, A. K. Chandra, and M. Snir. </author> <title> On Communication Latency in PRAM Computation. </title> <booktitle> In Proceedings of the ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 11-21. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: Surprisingly fast algorithms can be developed by exploiting these loopholes, but in many cases the algorithms perform poorly under more realistic assumptions [30]. Several variations on the PRAM have attempted to identify restrictions that would make it more practical while preserving much of its simplicity <ref> [1, 2, 14, 19, 24, 25] </ref>. <p> A different model that also addresses communication latency is the Block Parallel Random Access Machine (BPRAM) in which block transfers are allowed <ref> [1] </ref>. Bandwidth: A model that deals with communication bandwidth is the Local-Memory Parallel Random Access Machine (LPRAM)[2]. This is a CREW PRAM in which each processor is provided with an unlimited amount of local memory and where accesses to global memory are more expensive.
Reference: [2] <author> A. Aggarwal, A. K. Chandra, and M. Snir. </author> <title> Communication Complexity of PRAMs. </title> <booktitle> In Theoretical Computer Science, </booktitle> <pages> pages 3-28, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Surprisingly fast algorithms can be developed by exploiting these loopholes, but in many cases the algorithms perform poorly under more realistic assumptions [30]. Several variations on the PRAM have attempted to identify restrictions that would make it more practical while preserving much of its simplicity <ref> [1, 2, 14, 19, 24, 25] </ref>.
Reference: [3] <author> B. Alpern, L. Carter, E. Feig, and T. Selker. </author> <title> The Uniform Memory Hierarchy Model of Computation. </title> <address> Algorith-mica, </address> <year> 1993. </year> <note> (to appear). </note>
Reference: [4] <author> A. Bar-Noy and S. Kipnis. </author> <title> Designing broadcasting algorithms in the postal model for message-passing systems. </title> <booktitle> In Proceedings of the ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 11-22, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: This produces the communication and computation 3 A special case of this algorithm with o = 0 and g = 1 appears in <ref> [4] </ref>. 6 over time (right). The number shown for each node is the time at which it has received the datum and can begin sending it on. The last value is received at time 24. schedule for the summation problem.
Reference: [5] <author> G. Bell. </author> <title> Ultracomputers: A Teraflop Before Its Time. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 35(8) </volume> <pages> 26-47, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The effort needed to reach such high levels of performance combined with the relatively low cost of purchasing such microprocessors led Intel, Thinking Machines, Meiko, Convex, IBM and even Cray Research to use off-the-shelf microprocessors in their new parallel machines <ref> [5] </ref>. The technological opportunities suggest that parallel machines in the 1990s and beyond are much more likely to aim at thousands of 64-bit, off-the-shelf processors than at a million custom 1-bit processors.
Reference: [6] <author> G. E. Blelloch. </author> <title> Scans as Primitive Parallel Operations. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 355-362, </pages> <year> 1987. </year>
Reference-contexts: A multiprocessor is represented by a tree of modules with processors at the leaves. This model is based on the observation that the techniques used for tuning code for the memory hierarchy are similar to those for developing parallel algorithms. Other primitive parallel operations: The scan-model <ref> [6] </ref> is an EREW PRAM model extended with unit-time scan operations (data independent prefix operations), i.e., it assumes that certain scan operations can be executed as fast as parallel memory references. For integer scan operations this is approximately the case on the CM-2 and CM-5.
Reference: [7] <author> G. E. Blelloch, C. E. Leiserson, B. M. Maggs, C. G. Plaxton, S. J. Smith, and M. Zagha. </author> <title> A Comparison of Sorting Algorithms for the Connection Machine CM-2. </title> <booktitle> In Proceedings of the Symposium on Parallel Architectures and Algorithms, </booktitle> <year> 1990. </year>
Reference-contexts: For example, column sort consists of a series of local sorts and remap steps, similar to our FFT algorithm. An interesting recent algorithm, called splitter sort <ref> [7] </ref>, follows this compute-remap-compute pattern even more closely. A fast global step identifies P 1 values that split the data into P almost equal chunks.
Reference: [8] <author> R. Cole and O. Zajicek. </author> <title> The APRAM: Incorporating asynchrony into the PRAM model. </title> <booktitle> In Proceedings of the Symposium on Parallel Architectures and Algorithms, </booktitle> <pages> pages 169-178, </pages> <year> 1989. </year>
Reference-contexts: To balance the cost of 18 synchronization with the time spent computing, Gibbons proposes to have a single processor of a phase PRAM simulate several PRAM processors. Other proposals for asynchrony include <ref> [8, 18, 21] </ref>. Latency: The delay model of Papadimitriou and Yannakakis [25] accounts for communication latency, i.e. it realizes there is a delay between the time some information is produced at a processor and the time it can be used by another.
Reference: [9] <author> J. M. Cooley and J. W. Tukey. </author> <title> An algorithm for the machine calculation of complex Fourier series. </title> <journal> Math. Comp, </journal> <volume> 19 </volume> <pages> 297-301, </pages> <year> 1965. </year>
Reference-contexts: We discuss the key aspects of the algorithm and then an implementation that achieves near peak performance on the Thinking Machines CM-5. We focus on the butterfly algorithm <ref> [9] </ref> for the discrete FFT problem, most easily described in terms of its computation graph. The n-input (n a power of 2) butterfly is a directed acyclic graph with n (log n + 1) nodes viewed as n rows of (log n + 1) columns each.
Reference: [10] <author> W. J. Dally. </author> <title> Performance Analysis of k-ary n-cube Interconnection Networks. </title> <journal> IEEE Transaction on Computers, </journal> <volume> 39(6) </volume> <pages> 775-785, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Studies 16 such as <ref> [10] </ref> show that there is typically a saturation point at which the latency increases sharply; below the saturation point the latency is fairly insensitive to the load.
Reference: [11] <author> W. Dally et al. </author> <title> The J-Machine: A Fine-Grain Concurrent Computer. </title> <booktitle> In IFIP Congress, </booktitle> <year> 1989. </year>
Reference-contexts: Machine Network Cycle w T snd + T rcv r avg. H T (M =160) ns bits cycles cycles (1024 Proc.) (1024 Proc.) nCUBE/2 Hypercube 25 1 6400 40 5 6760 CM-5 Fattree 25 4 3600 8 9.3 3714 Dash [22] Torus 30 16 30 2 6.8 53 J-Machine <ref> [11] </ref> 3d Mesh 31 8 16 2 12.1 60 Monsoon [26] Butterfly 20 16 10 2 5 30 nCUBE/2 (AM) Hypercube 25 1 1000 40 5 1360 CM-5 (AM) Fattree 25 4 132 8 9.3 246 Table 1: Network timing parameters for a one-way message without contention on several current commercial <p> The final two rows show the inherent hardware overheads of these machines, as revealed by the Active Message layer [33]. The table also includes three research machines that have focused on optimizing the processor network interface: Dash [22] for a shared memory model, J-machine <ref> [11] </ref> for a message driven model, and Monsoon [26] for a dataflow model.
Reference: [12] <author> J. J. Dongarra, R. van de Geijn, and D. W. Walker. </author> <title> A Look at Scalable Dense Linear Algebra Libraries. </title> <editor> In J. Saltz, editor, </editor> <booktitle> Proceedings of the 1992 Scalable High Performance Computing Conference. </booktitle> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: Similarly, the elimination step involves multiplication of sub-matrices or rank-r updates where r is the side of the sub-matrices. Blocked decomposition has been found to outperform scalar decomposition on several machines. (See, for example, <ref> [12] </ref>.) The main reason for this is the extensive use of Level 3 BLAS (which are based on matrix-matrix operations and re-use cache contents optimally) in the blocked decomposition algorithm. 14 severe contention problem of naive implementations can be considerably mitigated and that for sufficiently dense graphs our connected components algorithm
Reference: [13] <author> S. Fortune and J. Wyllie. </author> <title> Parallelism in Random Access Machines. </title> <booktitle> In Proceedings of the 10th Annual Symposium on Theory of Computing, </booktitle> <pages> pages 114-118, </pages> <year> 1978. </year>
Reference-contexts: Others can be classified as overly specialized, in that they are tailored to the idiosyncrasies of a single machine, such as a particular interconnect topology. The most widely used parallel model, the PRAM <ref> [13] </ref>, is unrealistic because it assumes that all processors work synchronously and that interprocessor communication is free. Surprisingly fast algorithms can be developed by exploiting these loopholes, but in many cases the algorithms perform poorly under more realistic assumptions [30]. <p> In this section we briefly review some of the existing computational models and explain why they fail to fully capture the essential features of the coming generation of machines. 6.1 PRAM models The PRAM <ref> [13] </ref> is the most popular model for representing and analyzing the complexity of parallel algorithms.
Reference: [14] <author> P. B. Gibbons. </author> <title> A More Practical PRAM Model. </title> <booktitle> In Proceedings of the ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 158-168. </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: Surprisingly fast algorithms can be developed by exploiting these loopholes, but in many cases the algorithms perform poorly under more realistic assumptions [30]. Several variations on the PRAM have attempted to identify restrictions that would make it more practical while preserving much of its simplicity <ref> [1, 2, 14, 19, 24, 25] </ref>. <p> This model is suitable for handling memory contention at the module level, but does not address issues of bandwidth and network capacity. Asynchrony: Gibbons <ref> [14] </ref> proposed the Phase PRAM, an extension of the PRAM in which computation is divided into phases. All processors run asynchronously within a phase, and synchronize explicitly at the end of each phase.
Reference: [15] <author> J. L. Hennessy. </author> <title> MIPS R4000 Overview. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: This tremendous evolution comes at an equally astounding cost: estimates of the cost of developing the recent MIPS R4000 are 30 engineers for three years, requiring about $30 million to develop the chip, another $10 million to fabricate it, and one million hours of computer time for simulations <ref> [15] </ref>. This cost is borne by the extremely large market for commodity uniprocessors. To remain viable, parallel machines must be on the same technology growth curve, with the added degree of freedom being the number of processors in the system.
Reference: [16] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Memory capacity is increasing at a rate comparable to the increase in capacity of DRAM chips: quadrupling in size every three years <ref> [16] </ref>. Today's personal computers typically use 8 MB of memory and workstations use about 32 MB. By the turn of the century the same number of DRAM chips will offer 64 times the capacity of current machines.
Reference: [17] <editor> IEEE. </editor> <booktitle> Symposium Record Hot Chips IV, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Variations on this structure will involve clustering of localized collections of processors and the details of the interface between the processor and the communication network. The key technological justifications for this outlook are discussed below. Microprocessor performance is advancing at a rate of 50 to 100% per year <ref> [17] </ref>, as indicated by Figure 2. <p> Cache-like structures may be incorporated into the memory chips themselves, as in emerging RAM-bus and synchronous DRAM technology <ref> [17] </ref>. Multiprocessors will need to incorporate state-of-the-art memory systems to remain competitive. Since the parallel machine nodes are very similar to the core of a workstation, the cost of a node is 3 comparable to the cost of a workstation.
Reference: [18] <author> P. Kanellakis and A. Shvartsman. </author> <title> Efficient parallel algorithms can be made robust. </title> <booktitle> In Proceedings of the 8th Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 211-221, </pages> <year> 1989. </year> <month> 23 </month>
Reference-contexts: To balance the cost of 18 synchronization with the time spent computing, Gibbons proposes to have a single processor of a phase PRAM simulate several PRAM processors. Other proposals for asynchrony include <ref> [8, 18, 21] </ref>. Latency: The delay model of Papadimitriou and Yannakakis [25] accounts for communication latency, i.e. it realizes there is a delay between the time some information is produced at a processor and the time it can be used by another.
Reference: [19] <author> R. M. Karp, M. Luby, and F. Meyer auf der Heide. </author> <title> Efficient PRAM Simulation on a Distributed Memory Machine. </title> <booktitle> In Proceedings of the Twenty-Fourth Annual ACM Symposium of the Theory of Computing, </booktitle> <pages> pages 318-326, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Surprisingly fast algorithms can be developed by exploiting these loopholes, but in many cases the algorithms perform poorly under more realistic assumptions [30]. Several variations on the PRAM have attempted to identify restrictions that would make it more practical while preserving much of its simplicity <ref> [1, 2, 14, 19, 24, 25] </ref>. <p> Memory Contention: The Module Parallel Computer <ref> [19, 24] </ref> differs from the PRAM by assuming that the memory is divided into modules, each of which can process one access request at a time. This model is suitable for handling memory contention at the module level, but does not address issues of bandwidth and network capacity.
Reference: [20] <author> R. M. Karp, A. Sahay, E. Santos, and K. E. Schauser. </author> <title> Optimal Broadcast and Summation in the LogP Model. </title> <type> Technical Report UCB/CSD 92/721, </type> <institution> UC Berkeley, </institution> <year> 1992. </year>
Reference-contexts: The last value is received at time 24. schedule for the summation problem. The pattern of communication among the processors again forms a tree; in fact, the tree has the same shape as an optimal broadcast tree <ref> [20] </ref>. Each processor has the task of summing a set of the elements and then (except for the root processor) transmitting the result to its parent.
Reference: [21] <author> Z. M. Kedem, K. V. Palem, and P. G. Spirakis. </author> <title> Efficient robust parallel computations. </title> <booktitle> In Proceedings of the 22nd Annual Symposium on Theory of Computing, </booktitle> <pages> pages 138-148, </pages> <year> 1990. </year>
Reference-contexts: To balance the cost of 18 synchronization with the time spent computing, Gibbons proposes to have a single processor of a phase PRAM simulate several PRAM processors. Other proposals for asynchrony include <ref> [8, 18, 21] </ref>. Latency: The delay model of Papadimitriou and Yannakakis [25] accounts for communication latency, i.e. it realizes there is a delay between the time some information is produced at a processor and the time it can be used by another.
Reference: [22] <author> D. Lenoski et al. </author> <title> The Stanford Dash Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Although the model is stated in terms of primitive message events, we do not assume that algorithms must be described in terms of explicit message passing operations, such as send and receive. Shared memory models are implemented on distributed memory machines through an implicit exchange of messages <ref> [22] </ref>. Under LogP, reading a remote location requires time 2L + 4o. Prefetch operations, which initiate a read and continue, can be issued every g cycles and cost 2o units of processing time. <p> Machine Network Cycle w T snd + T rcv r avg. H T (M =160) ns bits cycles cycles (1024 Proc.) (1024 Proc.) nCUBE/2 Hypercube 25 1 6400 40 5 6760 CM-5 Fattree 25 4 3600 8 9.3 3714 Dash <ref> [22] </ref> Torus 30 16 30 2 6.8 53 J-Machine [11] 3d Mesh 31 8 16 2 12.1 60 Monsoon [26] Butterfly 20 16 10 2 5 30 nCUBE/2 (AM) Hypercube 25 1 1000 40 5 1360 CM-5 (AM) Fattree 25 4 132 8 9.3 246 Table 1: Network timing parameters for <p> The final two rows show the inherent hardware overheads of these machines, as revealed by the Active Message layer [33]. The table also includes three research machines that have focused on optimizing the processor network interface: Dash <ref> [22] </ref> for a shared memory model, J-machine [11] for a message driven model, and Monsoon [26] for a dataflow model.
Reference: [23] <author> C. U. Martel and A. Raghunathan. </author> <title> Asynchronous PRAMs with memory latency. </title> <type> Technical report, </type> <institution> University of California, Davis, Division of Computer Science, </institution> <year> 1991. </year>
Reference-contexts: This is a CREW PRAM in which each processor is provided with an unlimited amount of local memory and where accesses to global memory are more expensive. An asynchronous variant which differs in that it allows more than one outstanding memory request has been studied in <ref> [23] </ref>.
Reference: [24] <author> K. Mehlhorn and U. Vishkin. </author> <title> Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel memories. </title> <journal> Acta Informatica, </journal> <volume> 21 </volume> <pages> 339-374, </pages> <year> 1984. </year>
Reference-contexts: Surprisingly fast algorithms can be developed by exploiting these loopholes, but in many cases the algorithms perform poorly under more realistic assumptions [30]. Several variations on the PRAM have attempted to identify restrictions that would make it more practical while preserving much of its simplicity <ref> [1, 2, 14, 19, 24, 25] </ref>. <p> Memory Contention: The Module Parallel Computer <ref> [19, 24] </ref> differs from the PRAM by assuming that the memory is divided into modules, each of which can process one access request at a time. This model is suitable for handling memory contention at the module level, but does not address issues of bandwidth and network capacity.
Reference: [25] <author> C. H. Papadimitriou and M. Yannakakis. </author> <title> Towards an Architecture-Independent Analysis of Parallel Algorithms. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium of the Theory of Computing, </booktitle> <pages> pages 510-513. </pages> <publisher> ACM, </publisher> <year> 1988. </year>
Reference-contexts: Surprisingly fast algorithms can be developed by exploiting these loopholes, but in many cases the algorithms perform poorly under more realistic assumptions [30]. Several variations on the PRAM have attempted to identify restrictions that would make it more practical while preserving much of its simplicity <ref> [1, 2, 14, 19, 24, 25] </ref>. <p> Our algorithm is a special case of the layered FFT algorithm proposed in <ref> [25] </ref> and adapted for the BSP model [32]. These earlier models do not emphasize the communication schedule: [25] has no bandwidth limitations and hence no contention, whereas [32] places the scheduling burden on the router which is assumed to be capable of routing any balanced pattern in the desired amount of <p> Our algorithm is a special case of the layered FFT algorithm proposed in <ref> [25] </ref> and adapted for the BSP model [32]. These earlier models do not emphasize the communication schedule: [25] has no bandwidth limitations and hence no contention, whereas [32] places the scheduling burden on the router which is assumed to be capable of routing any balanced pattern in the desired amount of time. <p> To balance the cost of 18 synchronization with the time spent computing, Gibbons proposes to have a single processor of a phase PRAM simulate several PRAM processors. Other proposals for asynchrony include [8, 18, 21]. Latency: The delay model of Papadimitriou and Yannakakis <ref> [25] </ref> accounts for communication latency, i.e. it realizes there is a delay between the time some information is produced at a processor and the time it can be used by another.
Reference: [26] <author> G. M. Papadopoulos and D. E. Culler. Monsoon: </author> <title> an Explicit Token-Store Architecture. </title> <booktitle> In Proc. of the 17th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: H T (M =160) ns bits cycles cycles (1024 Proc.) (1024 Proc.) nCUBE/2 Hypercube 25 1 6400 40 5 6760 CM-5 Fattree 25 4 3600 8 9.3 3714 Dash [22] Torus 30 16 30 2 6.8 53 J-Machine [11] 3d Mesh 31 8 16 2 12.1 60 Monsoon <ref> [26] </ref> Butterfly 20 16 10 2 5 30 nCUBE/2 (AM) Hypercube 25 1 1000 40 5 1360 CM-5 (AM) Fattree 25 4 132 8 9.3 246 Table 1: Network timing parameters for a one-way message without contention on several current commercial and research multiprocessors. <p> The table also includes three research machines that have focused on optimizing the processor network interface: Dash [22] for a shared memory model, J-machine [11] for a message driven model, and Monsoon <ref> [26] </ref> for a dataflow model.
Reference: [27] <author> A. G. Ranade. </author> <title> How to emulate shared memory. </title> <booktitle> In Proceedings of the 28th IEEE Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 185-194, </pages> <year> 1987. </year>
Reference-contexts: Moreover the capacity constraint allows multithreading to be employed only up to a limit of L=g virtual processors. Under LogP, multithreading represents a convenient technique which simplifies analysis, as long as these constraints are met, rather than a fundamental requirement <ref> [27, 32] </ref>. On the other hand, LogP encourages techniques that work well in practice, such as coordinating the assignment of work with data placement, so as to reduce the communication bandwidth requirement and the frequency of remote references. <p> It has been suggested that the PRAM can serve as a good model for expressing the logical structure of parallel algorithms, and that implementation of these algorithms can be achieved by general-purpose simulations of the PRAM on distributed-memory machines <ref> [27] </ref>.
Reference: [28] <author> A. Sahay. </author> <title> Hiding Communication Costs in Bandwidth-Limited Parallel FFT Computation. </title> <type> Technical Report UCB/CSD 92/722, </type> <institution> UC Berkeley, </institution> <year> 1992. </year>
Reference-contexts: If o is small compared to g, each processor idles for g 2o cycles between successive transmissions during the remap phase. The remap can be merged into the computation phases, as in the optimal algorithms <ref> [28] </ref>. The initial portion of the remap is interleaved with the pre-remap computation, while the final portions can be interleaved with the post-remap computation.
Reference: [29] <author> Y. Shiloach and U. Vishkin. </author> <title> An O(log n) Parallel Conectivity Algorithm. </title> <journal> Journal of Algorithms, </journal> <volume> 3 </volume> <pages> 57-67, </pages> <year> 1982. </year>
Reference-contexts: For example, in <ref> [29] </ref> each component is represented by one node in the component and processors owning" such nodes are the target of increasing numbers of pointer-jumping" queries as the algorithm progresses. This leads to high contention, which the CRCW PRAM ignores, but LogP makes apparent.
Reference: [30] <author> L. Snyder. </author> <title> Type Architectures, Shared Memory, and the Corollary of Modest Potential. </title> <journal> In Ann. Rev. Comput. Sci., </journal> <pages> pages 289-317. </pages> <publisher> Annual Reviews Inc., </publisher> <year> 1986. </year>
Reference-contexts: The most widely used parallel model, the PRAM [13], is unrealistic because it assumes that all processors work synchronously and that interprocessor communication is free. Surprisingly fast algorithms can be developed by exploiting these loopholes, but in many cases the algorithms perform poorly under more realistic assumptions <ref> [30] </ref>. Several variations on the PRAM have attempted to identify restrictions that would make it more practical while preserving much of its simplicity [1, 2, 14, 19, 24, 25]. <p> For integer scan operations this is approximately the case on the CM-2 and CM-5. The observation of the deficiences of the PRAM led Snyder to conclude it was unrealizable and to develop the Candidate Type Architecture (CTA) as an alternative <ref> [30] </ref>. The CTA is a finite set of sequential computers connected in a fixed, bounded degree graph. The CTA is essentially a formal description of a parallel machine; it requires the communication cost of an algorithm to be analyzed for each interconnection network used.
Reference: [31] <author> R. Subramonian. </author> <title> The influence of limited bandwidth on algorithm design and implementation. </title> <booktitle> In Dartmouth Institute for Advanced Graduate Studies in Parallel Computation (DAGS/PC), </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: This leads to high contention, which the CRCW PRAM ignores, but LogP makes apparent. We consider a randomized PRAM algorithm given in <ref> [31] </ref> and adapt it to the LogP model. <p> For details of the analysis and the implementation, see <ref> [31] </ref>. 5 Matching the Model to Real Machines The LogP model abstracts the communication network into three parameters.
Reference: [32] <author> L. G. Valiant. </author> <title> A Bridging Model for Parallel Computation. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 33(8) </volume> <pages> 103-11, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Several variations on the PRAM have attempted to identify restrictions that would make it more practical while preserving much of its simplicity [1, 2, 14, 19, 24, 25]. The bulk-synchronous parallel model (BSP) developed by Valiant <ref> [32] </ref> attempts to bridge theory and practice fl A version of this report appears in the Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, May 1993, San Diego, CA. y Also affiliated with International Computer Science Institute, Berkeley. with a more radical departure from the <p> Moreover the capacity constraint allows multithreading to be employed only up to a limit of L=g virtual processors. Under LogP, multithreading represents a convenient technique which simplifies analysis, as long as these constraints are met, rather than a fundamental requirement <ref> [27, 32] </ref>. On the other hand, LogP encourages techniques that work well in practice, such as coordinating the assignment of work with data placement, so as to reduce the communication bandwidth requirement and the frequency of remote references. <p> Our algorithm is a special case of the layered FFT algorithm proposed in [25] and adapted for the BSP model <ref> [32] </ref>. These earlier models do not emphasize the communication schedule: [25] has no bandwidth limitations and hence no contention, whereas [32] places the scheduling burden on the router which is assumed to be capable of routing any balanced pattern in the desired amount of time. <p> Our algorithm is a special case of the layered FFT algorithm proposed in [25] and adapted for the BSP model <ref> [32] </ref>. These earlier models do not emphasize the communication schedule: [25] has no bandwidth limitations and hence no contention, whereas [32] places the scheduling burden on the router which is assumed to be capable of routing any balanced pattern in the desired amount of time. A naive schedule would have each processor send data starting with its first row and ending with its last row. <p> This facilitates the development of portable algorithms. 6.3 Bulk Synchronous Parallel Model Valiant's Bulk Synchronous Parallel (BSP) model is closely aligned with our goals, as it seeks to bridge the gap between theoretical work and practical machines <ref> [32] </ref>. In the BSP model a distributed-memory multiprocessor is described in terms of three elements: 1. processor/memory modules, 2. an interconnection network, and 3. a synchronizer which performs barrier synchronization. A computation consists of a sequence of supersteps.
Reference: [33] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year> <month> 24 </month>
Reference-contexts: Thus, the computational model should apply regardless of programming style. The technological factors discussed above make this goal tractable as most recent parallel machines support a range of programming styles using roughly similar hardware mechanisms <ref> [33] </ref>. The essential message is clear: technological forces are leading to massively parallel machines constructed from at most a few thousand nodes, each containing a powerful processor and substantial memory, interconnected by networks with limited bandwidth and significant latency. <p> At an average of 2.2 Mflops and 10 floating-point operations per butterfly, a cycle corresponds to 4:5s, or 150 clock ticks (we use cycles to refer to the time unit in the model and ticks to refer to the 33 Mhz hardware clock). In previous experiments on the CM-5 <ref> [33] </ref> we have determined that o 2s (0.44 cycles, 56 ticks) and, on an unloaded network, L 6s (1.3 cycles, 200 ticks). Furthermore, the 4 The implementation does not use the vector accelerators which are not available at the time of writing. 10 the time spent computing locally. <p> Large overheads such as these have led many to conclude that large messages are essential. For the nCUBE/2, the bulk of this cost is due to buffer management and copying associated with the asynchronous send/receive communication model <ref> [33] </ref>. This is more properly viewed as part of the computational work of an algorithm using that style of communication. For the CM-5, the bulk of the cost is due to the protocol associated with the synchronous send/receive, which involves a pair of messages before transmitting the first data element. <p> This protocol is easily modeled in terms of our parameters as 3 (L + 2o) + ng, where n is the number of words sent. The final two rows show the inherent hardware overheads of these machines, as revealed by the Active Message layer <ref> [33] </ref>. The table also includes three research machines that have focused on optimizing the processor network interface: Dash [22] for a shared memory model, J-machine [11] for a message driven model, and Monsoon [26] for a dataflow model.
References-found: 33

