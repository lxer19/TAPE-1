URL: http://www.cs.mu.oz.au/tr_db/mu_96_04.ps.gz
Refering-URL: http://www.cs.mu.oz.au/tr_db/TR.html
Root-URL: 
Title: CiFi: An Intelligent Agent for Citation Finding on the World-Wide Web  
Address: Parkville, Victoria 3052 Australia  
Affiliation: Department of Computer Science The University of Melbourne  
Abstract: Seng Wai Loke, Andrew Davison, and Leon Sterling Technical Report 96/4 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Armstrong, D. Freitag, T. Joachims, and T. Mitchell. WebWatcher: </author> <title> A Learning Apprentice for the World-Wide Web. </title> <booktitle> In On-line Working Notes of the AAAI Spring Symposium Series on Information Gathering from Distributed, Heterogeneous Environments, </booktitle> <address> http://www.isi.edu/sims/knoblock/sss95/mitchell.ps, 1995. </address>
Reference-contexts: WebWatcher <ref> [9, 1] </ref> uses a description of user interests to highlight interesting hyperlinks, and records hyperlinks to related pages. It also remembers the user's interests, based on the pages selected.
Reference: [2] <author> Marko Balabanovic and Yoav Shoham. </author> <title> Learning Information Retrieval Agents: Experiments with Automated Web Browsing. </title> <booktitle> In On-line Working Notes of the AAAI Spring Symposium Series on Information Gathering from Distributed, Heterogeneous Environments, </booktitle> <address> http://www.isi.edu/sims/knoblock/sss95/balabanovic.ps, 1995. </address>
Reference-contexts: Most of these perform more general searches, rather than targeting a particular domain, such as papers. Also, most learn user interests or permit user feedback. For instance, in <ref> [2] </ref>, user feedback on intermediate search results is used to change the link selection heuristic, improving subsequent search results. Fish-Search [3, 4], traverses the Web looking for particular documents. The user specifies either keywords, a regular expression, or external filters that the contents of the document must match.
Reference: [3] <author> P.M.E. De Bra and R.D.J. Post. </author> <title> Searching for Arbitrary Information in the WWW: the Fish-Search for Mosaic. </title> <booktitle> 2nd International World-Wide Web Conference, </booktitle> <address> http://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/Searching/debra/article.html. </address>
Reference-contexts: Most of these perform more general searches, rather than targeting a particular domain, such as papers. Also, most learn user interests or permit user feedback. For instance, in [2], user feedback on intermediate search results is used to change the link selection heuristic, improving subsequent search results. Fish-Search <ref> [3, 4] </ref>, traverses the Web looking for particular documents. The user specifies either keywords, a regular expression, or external filters that the contents of the document must match.
Reference: [4] <author> P.M.E. De Bra and R.D.J. Post. </author> <title> Information Retrieval in the World-Wide Web: Making client-based searching feasible. </title> <booktitle> Computer Networks and ISDN Systems, </booktitle> <pages> pages 183 - 192, </pages> <year> 1994. </year>
Reference-contexts: Most of these perform more general searches, rather than targeting a particular domain, such as papers. Also, most learn user interests or permit user feedback. For instance, in [2], user feedback on intermediate search results is used to change the link selection heuristic, improving subsequent search results. Fish-Search <ref> [3, 4] </ref>, traverses the Web looking for particular documents. The user specifies either keywords, a regular expression, or external filters that the contents of the document must match.
Reference: [5] <author> O. Etzioni and D. Weld. </author> <title> Intelligent Agents on the Internet: Fact, Fiction, and Forecast. </title> <journal> IEEE Expert, </journal> <volume> 10(4):44 - 49, </volume> <month> August </month> <year> 1995. </year>
Reference-contexts: There is, therefore, a need to locate the desired citations or papers on the Web. Intelligent software agents <ref> [5, 7] </ref>, which perform tasks on behalf of the user, with a degree of autonomy and reasoning capability, are an important paradigm for information discovery and retrieval on the Web. The rapidly expanding Web, which makes browsing very time-consuming, has motivated the need for programs that automatically search for information.
Reference: [6] <author> Kristian Hammond, Robin Burke, and Charles Martin. </author> <title> A Case-Based Approach to Knowledge Navigation. </title> <booktitle> AAAI Workshop on Indexing and Reuse in Multimedia Systems, </booktitle> <pages> pages 46 - 57, </pages> <year> 1994. </year>
Reference-contexts: Letizia [10] is an agent that infers user interests from browsing behaviour, and explores links using a best-first search with heuristics utilising the inferred user interests. Based on its exploration, Letizia can recommend links to follow. A case-based approach for information retrieval is utilised in <ref> [6] </ref>. Past user feedback on example items allows it to suggest potentially relevant new items to the user. CiFi does not require feedback, and does not learn from user interests because there is a specific target (e.g., a citation, or a HTML paper).
Reference: [7] <author> S.R. Hedberg. </author> <title> Intelligent Agents: The First Harvest of Softbots Looks Promising. </title> <journal> IEEE Expert, </journal> <volume> 10(4):6 - 9, </volume> <month> August </month> <year> 1995. </year>
Reference-contexts: There is, therefore, a need to locate the desired citations or papers on the Web. Intelligent software agents <ref> [5, 7] </ref>, which perform tasks on behalf of the user, with a degree of autonomy and reasoning capability, are an important paradigm for information discovery and retrieval on the Web. The rapidly expanding Web, which makes browsing very time-consuming, has motivated the need for programs that automatically search for information.
Reference: [8] <author> Inktomi. </author> <note> http://inktomi.berkeley.edu/. </note>
Reference-contexts: The indices are generated by indexing agents that traverse the Web "off-line", indexing Web pages based on the words they contain. Keywords can be submitted to these search engines to recover links to documents indexed by the keywords. Inktomi <ref> [8] </ref>, Alta Vista [15], and Lycos [12] are examples of this approach. Indexing agents aim to index as much of the Web as possible. For instance, Lycos claims more than 90 percent coverage of the Web.
Reference: [9] <author> T. Joachims, T. Mitchell, D. Freitag, and R. Armstrong. WebWatcher: </author> <title> Machine Learning and Hypertext. </title> <note> http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/webwatcher/mltagung-e.ps.Z. To appear in Fachgruppentreffen Maschinelles Lernen, </note> <institution> Dortmund, Germany, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: WebWatcher <ref> [9, 1] </ref> uses a description of user interests to highlight interesting hyperlinks, and records hyperlinks to related pages. It also remembers the user's interests, based on the pages selected.
Reference: [10] <author> H. Lieberman. Letizia: </author> <title> An Agent That Assists Web Browsing. </title> <booktitle> International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, </address> <year> 1995. </year> <month> 10 </month>
Reference-contexts: WebWatcher [9, 1] uses a description of user interests to highlight interesting hyperlinks, and records hyperlinks to related pages. It also remembers the user's interests, based on the pages selected. Letizia <ref> [10] </ref> is an agent that infers user interests from browsing behaviour, and explores links using a best-first search with heuristics utilising the inferred user interests. Based on its exploration, Letizia can recommend links to follow. A case-based approach for information retrieval is utilised in [6].
Reference: [11] <author> S.W. Loke and A. Davison. </author> <title> Logic Programming with the World-Wide Web. </title> <booktitle> http://www.cs.mu.oz.au/~swloke/papers/paper1.ps.gz. To appear in Proceedings of the 7th. ACM Conference on Hypertext (Hypertext '96). </booktitle> <publisher> ACM Press, </publisher> <year> 1996. </year>
Reference-contexts: To express the rules, we utilise the treatment of a Web page as a logic program module called LogicWeb, as introduced in <ref> [11] </ref>. A LogicWeb page or module is a Web page which has rules and facts. In the rest of the paper, we first determine the effectiveness of Lycos and Alta Vista for finding citations by attempting a set of queries. <p> In presenting the rules, we will assume the reader is acquainted with Prolog. Accessing the Web from Prolog. In formulating the rules, Web pages are treated as logic program modules termed as LogicWeb pages <ref> [11] </ref>. Besides its HTML source, a LogicWeb page can have link/2 facts representing out-links from the page, generated by parsing the page. A LogicWeb module can utilise the rules and facts of another by an operator similar to object-oriented message passing, denoted as #&gt;. <p> The results are checked for the citation required using a method like the one described above. Queries to search engines are constructed using the search engine URL appropriately appended with search keywords. 3.5 Current Implementation CiFi is implemented in Prolog (with LogicWeb extensions <ref> [11] </ref>) for easier writing of rules, and the use of backtracking and pattern matching. The rules presented in x3.2 are a slight simplification of the actual Prolog clauses. The program that interacts with the technical report archive search engines is also implemented in Prolog.
Reference: [12] <author> Lycos. </author> <note> http://www.lycos.com/. </note>
Reference-contexts: The indices are generated by indexing agents that traverse the Web "off-line", indexing Web pages based on the words they contain. Keywords can be submitted to these search engines to recover links to documents indexed by the keywords. Inktomi [8], Alta Vista [15], and Lycos <ref> [12] </ref> are examples of this approach. Indexing agents aim to index as much of the Web as possible. For instance, Lycos claims more than 90 percent coverage of the Web.
Reference: [13] <author> A.E. Monge and C.P. Elkan. </author> <title> Integrating External Information Sources to Guide WorldWide Web Information Retrieval. </title> <note> http://www-cse.ucsd.edu/users/amonge/papers/ai96.ps, Submitted to AI'96. </note>
Reference-contexts: string matching could be used to avoid this problem. * If a link to the (HTML) paper is part of a clickable map, the paper will not be found. 5 Related Work 5.1 Agents for Paper Search A tool called WebFind for automatically searching for scientific papers was introduced in <ref> [13] </ref>. The main idea of WebFind is to use other information sources (non-Web) to help retrieve information from the Web.
Reference: [14] <author> D. Rus and D. Subramaniam. </author> <title> Information Retrieval, Information Structure, and Information Agents. </title> <type> Technical report, </type> <institution> Department of Computer Science, Dartmouth, PCS-TR96-255. </institution> <note> To appear as Customizing Multimedia Information Access in ACM Computing Surveys, </note> <month> February </month> <year> 1996. </year>
Reference-contexts: In contrast to WebFind, CiFi looks for a citation to the paper which may, or may not, have a link to the actual paper. Also, the strategies that CiFi uses are a mixture of Web-based search engines, on-line databases, and heuristics, without depending on external information sources. BibAgent <ref> [14] </ref> semi-autonomously navigates over ftp directories (using the Alex file-system that integrates ftp directories) looking for a specified article. BibAgent examines readme files to aid its navigation, prioritises the directories to follow, and can retrieve the actual paper, or a completed bibliographic reference, from bibliographic files (e.g.,.bib files).
Reference: [15] <institution> Alta Vista. </institution> <note> http://www.altavista.digital.com/. 11 </note>
Reference-contexts: The indices are generated by indexing agents that traverse the Web "off-line", indexing Web pages based on the words they contain. Keywords can be submitted to these search engines to recover links to documents indexed by the keywords. Inktomi [8], Alta Vista <ref> [15] </ref>, and Lycos [12] are examples of this approach. Indexing agents aim to index as much of the Web as possible. For instance, Lycos claims more than 90 percent coverage of the Web.
References-found: 15

