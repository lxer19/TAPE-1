URL: http://www.cs.indiana.edu/pub/bramley/papers/RowProj.ps.Z
Refering-URL: http://www.cs.indiana.edu/pub/bramley/papers/
Root-URL: http://www.cs.indiana.edu
Title: ROW PROJECTION METHODS FOR LARGE NONSYMMETRIC LINEAR SYSTEMS  
Author: R. BRAMLEY AND A. SAMEH 
Keyword: Key Words. iterative methods, nonsymmetric linear systems, Kaczmarz, Cimmino  
Address: URBANA  
Affiliation: CENTER FOR SUPERCOMPUTING RESEARCH AND DEVELOPMENT UNVERSITY OF ILLINOIS  
Abstract: Three conjugate gradient accelerated row projection (RP) methods for nonsymmetric linear systems are presented and their properties described. One method is based on Kaczmarz's method and has an iteration matrix that is the product of orthogonal projectors; another is based on Cimmino's method and has an iteration matrix that is the sum of orthogonal projectors. A new RP method which requires fewer matrix-vector operations, explicitly reduces the problem size, is error reducing in the 2-norm, and consistently produces better solutions than other RP algorithms is also introduced. Using comparisons with the method of conjugate gradient applied to the normal equations, the properties of RP methods are explained. A row partitioning approach is described which yields parallel implementations suitable for a wide range of computer architectures, requires only a few vectors of extra storage, and allows computing the necessary projections with small errors. Numerical testing verifies the robustness of this approach and shows that the resulting algorithms are competitive with other nonsymmetric solvers in speed and efficiency. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Ansorge, </author> <title> Connections between the Cimmino-methods and the Kaczmarz-methods for the solution of singular and regular systems of equations, </title> <booktitle> Computing, </booktitle> <pages> 367-375, 33(1984). </pages>
Reference-contexts: The advantage of this approach is that the projections can be computed in parallel and then added. In 1939 G. Cimmino [9] first proposed an iteration related to (18), and since then it has been examined by several others <ref> [1, 11, 12, 15, 25, 26, 38] </ref>. Later we will show how the individual projections can, for a wide class of problems, be computed with parallelism. <p> seven point centered difference operator [35] for elliptic partial differential equations au xx + bu yy + cu zz + du x + eu y + f u z + gu = F (24) where a g are functions of (x; y; z) and the domain is the unit cube <ref> [0; 1] </ref> fi [0; 1] fi [0; 1]. Dirichlet boundary conditions are imposed in a manner described later. When discretized using n grid points in each direction the resulting system is of order N = n 3 . This class of test problems is chosen for four reasons. <p> difference operator [35] for elliptic partial differential equations au xx + bu yy + cu zz + du x + eu y + f u z + gu = F (24) where a g are functions of (x; y; z) and the domain is the unit cube <ref> [0; 1] </ref> fi [0; 1] fi [0; 1]. Dirichlet boundary conditions are imposed in a manner described later. When discretized using n grid points in each direction the resulting system is of order N = n 3 . This class of test problems is chosen for four reasons. <p> for elliptic partial differential equations au xx + bu yy + cu zz + du x + eu y + f u z + gu = F (24) where a g are functions of (x; y; z) and the domain is the unit cube <ref> [0; 1] </ref> fi [0; 1] fi [0; 1]. Dirichlet boundary conditions are imposed in a manner described later. When discretized using n grid points in each direction the resulting system is of order N = n 3 . This class of test problems is chosen for four reasons.
Reference: [2] <author> M. Arioli, I. Duff, J. Noailles, and D. Ruiz, </author> <title> A block projection method for general sparse matrices, </title> <note> submitted to SIAM J. </note> <institution> Sci. Stat. Comp., </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: More generally, if the longest line in the computational star has l nodes, the Cholesky factors require only lN storage. The scheme used here for row partitioning can thus be extended to other, less simple, problems. 5.4. Other Approaches. Recently, M. Arioli, I. Duff, J. Noailles, and D. Ruiz <ref> [2] </ref> experimented with a block Cimmino method for more general sparse systems. Their approach is to reorder the matrix into a block tridiagonal form, and then to use a row partitioning into two block rows. <p> Although the testing results of this paper are for the structured sparse systems arising from seven point centered differences, Arioli et. al. <ref> [2] </ref> have recently applied block Cimmino to more general sparse systems and introduced an innovative preconditioner with good results. In [2] they report on this approach, and include tests on some of the problems from this paper. <p> Although the testing results of this paper are for the structured sparse systems arising from seven point centered differences, Arioli et. al. <ref> [2] </ref> have recently applied block Cimmino to more general sparse systems and introduced an innovative preconditioner with good results. In [2] they report on this approach, and include tests on some of the problems from this paper. In summary CG accelerated row projection methods can have a robustness unmatched by other nonsymmetric solvers, and successfully solve large systems with indefinite real parts and eigenvalues arbitrarily distributed in the complex plane.
Reference: [3] <author> S. Ashby, Chebycode: </author> <title> A Fortran implementation of Manteuffel's adaptive Chebyshev algorithm, </title> <type> Report No. </type> <institution> UIUCDCS-R-85-1203, Department of Computer Science, University of Illinois - Urbana, </institution> <month> May </month> <year> 1985. </year>
Reference-contexts: A third category, residual polynomial methods, directly uses this idea by finding a convex region in the complex plane containing the eigenvalues of A. Iterates are then formed that satisfy (1) for some class of polynomials. Chebyshev polynomials <ref> [3, 27, 28] </ref> use ellipses for the convex region and Chebyshev polynomials to define p k ().
Reference: [4] <author> A. Bjorck, T. Elfving, </author> <title> Accelerated projection methods for computing pseudo-inverse solutions of systems of linear equations, </title> <journal> BIT, </journal> <pages> 145-163, </pages> <year> 19(1979). </year>
Reference-contexts: However, as with any linear stationary process the rate of convergence is determined by the spectral radius of Q u and can be arbitrarily slow. For this reason Elfving [12] and Bjorck and Elfving <ref> [4] </ref> proposed symmetrizing Q u by following a forward sweep through the rows with a backward sweep, and introduced an iteration parameter to get x k+1 = Q (!)x k + T (!)b;(4) with T (!) defined by (8). <p> premultiplying by A T , or equivalently applying a similarity transformation with W = A T to the coefficient matrix in (11) to yield [A T (D + L) T D (D + L) 1 A]x = A T (D + L) T D (D + L) 1 b:(12) In <ref> [4] </ref> applying a similarity transformation using W = D 1=2 (D + L) T was proposed; when CG acceleration is used this has the desirable property of being an error reducing process in the two-norm, instead of reducing the error in an elliptic norm. <p> CG Acceleration. Although the CG algorithm can be applied directly to the row projection systems, special properties allow a reduction in the amount of work required by KACZ and provide another advantage of V-RP. CG acceleration for RP methods was proposed in <ref> [4] </ref> and tested in [7, 21, 22]. The reason that a reduction in work is possible and A T 1 x k = b 1 on every iteration of CG applied to KACZ follows from: Theorem 3.1. <p> x k1 ] In contrast CG acceleration in the KACZ system minimizes k S T V (x k x fl ) k over the subspace S V S T V fl span [x 0 x fl ; : : : ; x k1 x fl ], a result established in <ref> [4] </ref>. In summary, CG acceleration for KACZ allows one projection per iteration to be omitted and one block of equations to be kept exactly satisfied, provided that x 0 = ~ b is used. CG acceleration for V-RP minimizes the two-norm rather than the A-norm of the error. 4.
Reference: [5] <author> A. Bjorck, G. Golub, </author> <title> Numerical methods for computing angles between linear subspaces, </title> <journal> Math. Comp. </journal> <pages> 579-594, </pages> <month> 27 </month> <year> (1973). </year>
Reference-contexts: The definition presented here follows that of A. Bjorck and G. Golub in <ref> [5] </ref>, but for convenience L 1 and L 2 are assumed to have the same dimension.
Reference: [6] <author> R. Bramley, </author> <title> Row projection methods for linear systems, </title> <type> CSRD Tech. </type> <institution> Rept. 881, Center for Supercomputing Research and Development, Univ. Illinois - Urbana, </institution> <year> (1989). </year>
Reference-contexts: Surprisingly, the third requirement that each subproblem be well-conditioned also is generally satisfied by the m = 9 partitioning. The details can be found in <ref> [6] </ref>, but this can be seen intuitively because C T C consists of the normal equations of T in (25), with the squares of the diagonal blocks D i added to the main diagonal of T T T , making it strongly diagonally dominant. <p> This section describes the other nonsymmetric solvers to which the RP methods are compared, the stopping tests used, and briefly summarizes the results comparing the methods. Fuller details of the results, including tables showing numbers of iterations, times, and residual and error norms are provided in <ref> [6] </ref>. 6.1. Description of Other Methods Tested. The RP algorithms are compared to two other basic methods, GMRES (10) (see [31] for references to this and the other Krylov subspace methods of this section) and CGNE, CG applied to A T Ax = A T b.
Reference: [7] <author> R. Bramley, A. Sameh, </author> <title> A robust parallel solver for block tridiagonal systems, </title> <type> CSRD Tech. </type> <institution> Rept. 806, Center for Supercomputing Research and Development, Univ. Illinois - Urbana, </institution> <year> (1988). </year>
Reference-contexts: CG Acceleration. Although the CG algorithm can be applied directly to the row projection systems, special properties allow a reduction in the amount of work required by KACZ and provide another advantage of V-RP. CG acceleration for RP methods was proposed in [4] and tested in <ref> [7, 21, 22] </ref>. The reason that a reduction in work is possible and A T 1 x k = b 1 on every iteration of CG applied to KACZ follows from: Theorem 3.1. <p> When combined with this precondi-tioner CGNE is denoted as ILCG. This is not the same as forming the normal equations A T A and then performing an incomplete Cholesky factorization of A T A; earlier work <ref> [7] </ref> with such a preconditioner for CGNE applied to two dimensional problems showed that it also suffered robustness problems. 6.2. Stopping Tests. The primary stopping test used is k Ax k b k 2 10 9 :(27) For KACZ, the algorithm checks the pseudo-residual ~ b (I Q)x k .
Reference: [8] <author> R. Bramley, A. Sameh, </author> <title> Row projection methods for large nonsymmetric linear systems, </title> <type> CSRD Tech. </type> <note> Rept. 957 (revised), </note> <institution> Center for Supercomputing Research and Development, Univ. Illinois - Urbana, </institution> <year> (1990). </year>
Reference: [9] <author> G. </author> <type> Cimmino, </type> <institution> Calcolo approssimato per le soluzioni dei sistemi di equazioni lineari, Ric. Sci. Progr. tecn. econom. </institution> <month> naz., </month> <pages> 326-333, 9(1939). </pages>
Reference-contexts: For nonsingular A this system is symmetric positive definite and the CG algorithm can be applied. The advantage of this approach is that the projections can be computed in parallel and then added. In 1939 G. Cimmino <ref> [9] </ref> first proposed an iteration related to (18), and since then it has been examined by several others [1, 11, 12, 15, 25, 26, 38]. Later we will show how the individual projections can, for a wide class of problems, be computed with parallelism.
Reference: [10] <author> P. Eggermont, G. Herman, A. Lent, </author> <title> Iterative algorithms for large partitioned linear systems, with applications to image reconstruction, </title> <journal> Lin. Alg. Appl., </journal> <pages> 37-67, 40(1881). </pages>
Reference-contexts: Kaczmarz [20] proposed iteration (3) and proved convergence for the m = N case where each block row A T i consists of a single row of A. Since then many authors <ref> [10, 11, 19, 29, 30, 37] </ref> have examined the convergence of related iterative methods. The theoretical robustness of (3) is remarkable and the iteration converges even when A is singular or rectangular.
Reference: [11] <author> T. Elfving, </author> <title> Group-iterative methods for consistent and inconsistent linear equations, </title> <institution> Rept. LITH-MAT-R-1977-11, Dept. Math. Linkoping Univ., </institution> <year> (1977). </year>
Reference-contexts: Kaczmarz [20] proposed iteration (3) and proved convergence for the m = N case where each block row A T i consists of a single row of A. Since then many authors <ref> [10, 11, 19, 29, 30, 37] </ref> have examined the convergence of related iterative methods. The theoretical robustness of (3) is remarkable and the iteration converges even when A is singular or rectangular. <p> 2 ) 1 ; : : : ; A m (A T m A m ) 1 ] to obtain (P 1 + P 2 + + P m )x = ~ Ab:(18) This system can also be derived as a block Jacobi method applied to the system (7); see <ref> [11] </ref>. For nonsingular A this system is symmetric positive definite and the CG algorithm can be applied. The advantage of this approach is that the projections can be computed in parallel and then added. In 1939 G. <p> The advantage of this approach is that the projections can be computed in parallel and then added. In 1939 G. Cimmino [9] first proposed an iteration related to (18), and since then it has been examined by several others <ref> [1, 11, 12, 15, 25, 26, 38] </ref>. Later we will show how the individual projections can, for a wide class of problems, be computed with parallelism.
Reference: [12] <author> T. Elfving, </author> <title> Block iterative methods for consistent and inconsistent linear equations, </title> <journal> Nu-mer. Math., </journal> <pages> 1-12, 35(1980). </pages>
Reference-contexts: The theoretical robustness of (3) is remarkable and the iteration converges even when A is singular or rectangular. However, as with any linear stationary process the rate of convergence is determined by the spectral radius of Q u and can be arbitrarily slow. For this reason Elfving <ref> [12] </ref> and Bjorck and Elfving [4] proposed symmetrizing Q u by following a forward sweep through the rows with a backward sweep, and introduced an iteration parameter to get x k+1 = Q (!)x k + T (!)b;(4) with T (!) defined by (8). <p> (!)) lies in the interval (0; 1] and so the CG method can be applied to solve (I Q (!))x = T (!)b:(6) Also, (4) is equivalent to applying block SSOR to ( x = A T y where the blocking is that induced by the row partitioning of A <ref> [12] </ref>. This gives a simple expression for T (!): T (!) = A T (D + !L) T D (D + !L) 1 ;(8) where AA T = L + D + L T is the usual splitting into block lower triangular, diagonal, and upper triangular parts. <p> The advantage of this approach is that the projections can be computed in parallel and then added. In 1939 G. Cimmino [9] first proposed an iteration related to (18), and since then it has been examined by several others <ref> [1, 11, 12, 15, 25, 26, 38] </ref>. Later we will show how the individual projections can, for a wide class of problems, be computed with parallelism. <p> Doing so and minimizing with respect to ff gives ! min = 1 ff min = 2=(1 + s 1 ) and a spectral radius of (1 s 1 )=(1 + s 1 ). The same result was derived in <ref> [12] </ref> using classical SOR theory. The benefit of the CS decomposition is that the full spectrum of Q u is given and not simply the eigenvalue of largest modulus.
Reference: [13] <author> H. Elman, G. Golub, </author> <title> Iterative methods for cyclically reduced non-self-adjoint linear systems, </title> <institution> UMIACS-TR-88-87, CS-TR-2145, Dept. of Comp. Sci., University of Maryland, </institution> <year> (1988). </year>
Reference-contexts: a variety of sources 13 Problem: 1 2 3 4 5 6 Re () &gt; 0 Yes Yes No Yes Yes Yes A + A T Definite Yes No No No No No Estimated (A) 9 57 40000 4786 36 77 Table 2 Properties of A for N = 1728 <ref> [13, 22, 23, 34] </ref>. Each test problem has boundary conditions chosen to give a predetermined solution to the partial differential equation, so that the norm of the error vector as well as that of the residual vector can be checked.
Reference: [14] <author> H. Elman, </author> <title> Iterative methods for large, sparse, nonsymmetric systems of linear equations, </title> <institution> Dept. of Comp. Sci., Yale Univ. Res. Rept. </institution> <month> 229, </month> <year> (1982). </year>
Reference-contexts: Since computation and storage grow with the iteration number for these methods, they are most often used in a trun fl Work supported by grants NSF-MIP-8410110,DOE DE-FG02-85ER25001, AT&T-AFFL-67-SAMEH, NSF-CCR-8717942,AFOSR-85-0211, NSF CCR-900000N (NCSA), and Digital Equipment Corp. 1 cated or restarted form. In 1982 H. Elman <ref> [14] </ref> proved that restarted versions of many of these methods converge provided that the symmetric part (A + A T )=2 of A is positive definite. In spite of this restriction, the restarted GMRES (k) algorithm in particular is currently one of the more popular nonsymmetric solvers.
Reference: [15] <author> P. Gilbert, </author> <title> Iterative methods for the three-dimensional reconstruction of an object from projections, </title> <journal> J. Theor. Biol., </journal> <pages> 105-117, 36(1972). </pages>
Reference-contexts: The advantage of this approach is that the projections can be computed in parallel and then added. In 1939 G. Cimmino [9] first proposed an iteration related to (18), and since then it has been examined by several others <ref> [1, 11, 12, 15, 25, 26, 38] </ref>. Later we will show how the individual projections can, for a wide class of problems, be computed with parallelism.
Reference: [16] <author> G. Golub, C. Van Loan, </author> <title> Matrix Computations, </title> <publisher> John Hopkins University Press, </publisher> <address> Baltimore (1983). </address>
Reference-contexts: From this one can obtain the CS decomposition <ref> [16, 36] </ref>, which is stated below in terms of the projectors P i . 10 Theorem 4.1 (CS Decomposition). Let P i 2 &lt; NfiN be the orthogonal projectors onto the subspaces L i , for i = 1; 2.
Reference: [17] <author> I. Gustafsson, </author> <title> Modified incomplete Cholesky methods, Preconditioning Methods: Theory and Applications, </title> <editor> ed. D. Evans, </editor> <address> 265-293, </address> <publisher> Gordon and Breach, </publisher> <address> New York, </address> <year> (1983). </year>
Reference-contexts: Whenever GMRES is refered to without an argument, GMRES (10) is understood. The reasons for selecting GMRES instead of another Krylov subspace method are the popularity of GMRES, and that Orthomin and GCR give similar results. GMRES is also implemented with ILU and MILU preconditioners <ref> [17] </ref>. Because only GMRES is implemented with these preconditioners, the combination of GMRES with ILU preconditioning is abbreviated ILU, and similarly for MILU.
Reference: [18] <author> L. Hageman, D. Young, </author> <title> Applied Iterative Methods, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> (1981). </year>
Reference-contexts: This category includes the Jacobi, Gauss-Seidel, and successive overrelaxation methods, and convergence is assured if the spectral radius (M 1 N ) is less than 1. This condition can often be shown to hold if, e.g., A is irreducible and diagonally dominant. Hageman and Young <ref> [18] </ref> describe many of these splitting techniques and provide other conditions on A that imply convergence. The second category, CG-like methods, was motivated by the success of the conjugate gradient (CG) method for symmetric positive definite systems. <p> Note that P i = Q i Q T i in this case. Let AA T = L+D +L T be the decomposition of AA T into the strictly lower block triangular, block diagonal, and strictly upper block triangular parts induced by the block row partitioning. As shown in <ref> [18, p.31] </ref>, the block SSOR iteration for AA T y = b can be written as y k+1 = [I !(2 !)(D + !L) T D (D + !L) 1 AA T ]y k For ! = 1 this can be viewed as an iteration to solve the system [(D +
Reference: [19] <author> G. Herman, A. Lent, P. Lutz, </author> <title> Relaxation methods for image reconstruction, </title> <journal> Comm. ACM, </journal> <volume> 152-158, 21(2), </volume> <year> (1978). </year>
Reference-contexts: Kaczmarz [20] proposed iteration (3) and proved convergence for the m = N case where each block row A T i consists of a single row of A. Since then many authors <ref> [10, 11, 19, 29, 30, 37] </ref> have examined the convergence of related iterative methods. The theoretical robustness of (3) is remarkable and the iteration converges even when A is singular or rectangular.
Reference: [20] <author> S. Kaczmarz, Angenaherte Auflosung von Systemen linearer Gleichungen, Bull. </author> <note> intern. </note> <institution> Acad. polonaise Sci. lettres (Cracouie); Class sci. math. natur.: Seira A. Sci. Math., </institution> <month> 355-357 </month> <year> (1939). </year>
Reference-contexts: Kaczmarz <ref> [20] </ref> proposed iteration (3) and proved convergence for the m = N case where each block row A T i consists of a single row of A. Since then many authors [10, 11, 19, 29, 30, 37] have examined the convergence of related iterative methods.
Reference: [21] <author> C. Kamath, </author> <title> Solution of nonsymmetric systems of equations on a multiprocessor, </title> <type> CSRD Tech. </type> <institution> Rept. 591, Center for Supercomputing Research and Development, Univ. Illinois - Urbana, </institution> <year> (1986). </year>
Reference-contexts: Although Bjorck and Elfving tested an accelerated RP algorithm, their work concentrated on the single row (m = N ) case applied to weighted least squares problems. Previous work dealing with the use of block forms is given by C. Kamath and A. Sameh <ref> [21, 22] </ref>. Using sample problems drawn from two dimensional nonself-adjoint elliptic partial differential equations, they numerically examined the issues of suitable row par-titionings and methods for the numerical solution of the induced projections, primarily for the m = 2 or 3 case. <p> CG Acceleration. Although the CG algorithm can be applied directly to the row projection systems, special properties allow a reduction in the amount of work required by KACZ and provide another advantage of V-RP. CG acceleration for RP methods was proposed in [4] and tested in <ref> [7, 21, 22] </ref>. The reason that a reduction in work is possible and A T 1 x k = b 1 on every iteration of CG applied to KACZ follows from: Theorem 3.1.
Reference: [22] <author> C. Kamath, A. Sameh, </author> <title> A projection method for solving nonsymmetric linear systems on multiprocessors, </title> <booktitle> Parallel Computing, </booktitle> <pages> 291-312, 9(1988/1989). </pages>
Reference-contexts: Although Bjorck and Elfving tested an accelerated RP algorithm, their work concentrated on the single row (m = N ) case applied to weighted least squares problems. Previous work dealing with the use of block forms is given by C. Kamath and A. Sameh <ref> [21, 22] </ref>. Using sample problems drawn from two dimensional nonself-adjoint elliptic partial differential equations, they numerically examined the issues of suitable row par-titionings and methods for the numerical solution of the induced projections, primarily for the m = 2 or 3 case. <p> The first implementation issue is the choice of ! in (6). Normally the `optimal' ! is defined as the ! min that minimizes the spectral radius of Q (!). In <ref> [22] </ref> ! min = 1 is proven for the case where A is partitioned into two block rows, i.e., m = 2. <p> CG Acceleration. Although the CG algorithm can be applied directly to the row projection systems, special properties allow a reduction in the amount of work required by KACZ and provide another advantage of V-RP. CG acceleration for RP methods was proposed in [4] and tested in <ref> [7, 21, 22] </ref>. The reason that a reduction in work is possible and A T 1 x k = b 1 on every iteration of CG applied to KACZ follows from: Theorem 3.1. <p> The 11 previous section, however, showed that when ! = 1 KACZ can be implemented with only two projections per iteration. When m = 2 the value ! = 1 minimizes the spectral radius of Q (!) as was shown in <ref> [22] </ref>. This result can be obtained from the representation (22) in the same way as (20) was obtained. The CS decomposition also allows the construction of an example showing that no direct relationship need exist between the singular value distribution of A and its related RP matrices. <p> a variety of sources 13 Problem: 1 2 3 4 5 6 Re () &gt; 0 Yes Yes No Yes Yes Yes A + A T Definite Yes No No No No No Estimated (A) 9 57 40000 4786 36 77 Table 2 Properties of A for N = 1728 <ref> [13, 22, 23, 34] </ref>. Each test problem has boundary conditions chosen to give a predetermined solution to the partial differential equation, so that the norm of the error vector as well as that of the residual vector can be checked. <p> One way to achieve this is through parallelism: if A T i is the direct sum of blocks C j for j 2 S i , then P i is block diagonal <ref> [22] </ref>. The computation of P i x can then be done by assigning each block of P i to a different processor on a multiprocessor machine, or by vectorizing the computations across the blocks on a vector machine. The second criterion is storage efficiency. <p> Row Partitioning of Test Problems. Row partitionings schemes ranging from m = 4 up to m = 27 have been tested on the test problems. For brevity only one, a m = 9 partitioning generalized from a scheme in <ref> [22] </ref>, is presented. Consider an n fi n fi n mesh, and for convenience assume that n is a multiple of 3 and that lines in the mesh are numbered within each plane in the natural order.
Reference: [23] <author> D. Kincaid, D. Young, </author> <title> Adapting iterative algorithms developed for symmetric systems to nonsymmetric systems, Elliptic Problem Solvers, 353-359, </title> <editor> ed. M. Schultz, </editor> <publisher> Academic 28 Press, </publisher> <address> New York, </address> <year> (1981). </year>
Reference-contexts: a variety of sources 13 Problem: 1 2 3 4 5 6 Re () &gt; 0 Yes Yes No Yes Yes Yes A + A T Definite Yes No No No No No Estimated (A) 9 57 40000 4786 36 77 Table 2 Properties of A for N = 1728 <ref> [13, 22, 23, 34] </ref>. Each test problem has boundary conditions chosen to give a predetermined solution to the partial differential equation, so that the norm of the error vector as well as that of the residual vector can be checked.
Reference: [24] <author> D. Kuck, E. Davidson, D. Lawrie, A. Sameh, </author> <title> Parallel supercomputing today and the Cedar approach, </title> <booktitle> Science, </booktitle> <pages> 967-974, 231(1986). </pages>
Reference-contexts: Later we will show how the individual projections can, for a wide class of problems, be computed with parallelism. In this case, the Cimmino method allows computations to proceed with two levels of parallelism, making it especially suitable for hierarchical memory machines such as Cedar <ref> [24] </ref>. 2.4. Connection Between RP Systems and the Normal Equations.
Reference: [25] <author> A. Kydes, R. Tewarson, </author> <title> An iterative method for solving partitioned linear equations, </title> <journal> Computing 357-363, </journal> <volume> 15(1975). </volume>
Reference-contexts: The advantage of this approach is that the projections can be computed in parallel and then added. In 1939 G. Cimmino [9] first proposed an iteration related to (18), and since then it has been examined by several others <ref> [1, 11, 12, 15, 25, 26, 38] </ref>. Later we will show how the individual projections can, for a wide class of problems, be computed with parallelism.
Reference: [26] <author> A. Lakshminarayanan, A. Lent, </author> <title> Methods of least squares and SIRT in reconstruction, </title> <journal> J. Theor. Biol., </journal> <pages> 267-295, 76(1979). </pages>
Reference-contexts: The advantage of this approach is that the projections can be computed in parallel and then added. In 1939 G. Cimmino [9] first proposed an iteration related to (18), and since then it has been examined by several others <ref> [1, 11, 12, 15, 25, 26, 38] </ref>. Later we will show how the individual projections can, for a wide class of problems, be computed with parallelism.
Reference: [27] <author> T. Manteuffel, </author> <title> The Tchebychev iteration for nonsymmetric linear systems, </title> <journal> Numer. Math., </journal> <pages> 307-327, 28(1977). </pages>
Reference-contexts: A third category, residual polynomial methods, directly uses this idea by finding a convex region in the complex plane containing the eigenvalues of A. Iterates are then formed that satisfy (1) for some class of polynomials. Chebyshev polynomials <ref> [3, 27, 28] </ref> use ellipses for the convex region and Chebyshev polynomials to define p k ().
Reference: [28] <author> T. Manteuffel, </author> <title> Adaptive procedure for estimating parameters for the nonsymmetric Tchebychev iteration, </title> <journal> Numer. Math., </journal> <pages> 183-208, 31(1978). </pages>
Reference-contexts: A third category, residual polynomial methods, directly uses this idea by finding a convex region in the complex plane containing the eigenvalues of A. Iterates are then formed that satisfy (1) for some class of polynomials. Chebyshev polynomials <ref> [3, 27, 28] </ref> use ellipses for the convex region and Chebyshev polynomials to define p k ().
Reference: [29] <author> S. Nelson, M. Neumann, </author> <title> Generalizations of the projection method with applications to SOR theory for Hermitian positive semidefinite linear systems, </title> <journal> Numer. Math. </journal> <pages> 123-141, </pages> <month> 51 </month> <year> (1987). </year>
Reference-contexts: Kaczmarz [20] proposed iteration (3) and proved convergence for the m = N case where each block row A T i consists of a single row of A. Since then many authors <ref> [10, 11, 19, 29, 30, 37] </ref> have examined the convergence of related iterative methods. The theoretical robustness of (3) is remarkable and the iteration converges even when A is singular or rectangular.
Reference: [30] <author> W. Peters, </author> <title> Losung linearer Gleichungssysteme durch Projektion auf Schnittraume von Hyperebenen und Berechnung einer verallgemeinerten Inversen, </title> <journal> Beit. Numer. Math., </journal> <pages> 129-146, 5(1976). </pages>
Reference-contexts: Kaczmarz [20] proposed iteration (3) and proved convergence for the m = N case where each block row A T i consists of a single row of A. Since then many authors <ref> [10, 11, 19, 29, 30, 37] </ref> have examined the convergence of related iterative methods. The theoretical robustness of (3) is remarkable and the iteration converges even when A is singular or rectangular.
Reference: [31] <author> Y. Saad, M. Schultz, </author> <title> Conjugate gradient-like algorithms for solving nonsymmetric linear systems, </title> <journal> Math. Comp, </journal> <pages> 417-424, 44(1985). </pages>
Reference-contexts: The second category, CG-like methods, was motivated by the success of the conjugate gradient (CG) method for symmetric positive definite systems. Generalized conjugate residuals (GCR), Orthomin, generalized minimum residual (GMRES), Ax-elsson's method, and Orthodir are included in this category (see Saad <ref> [31] </ref> for a summary of these methods). <p> Fuller details of the results, including tables showing numbers of iterations, times, and residual and error norms are provided in [6]. 6.1. Description of Other Methods Tested. The RP algorithms are compared to two other basic methods, GMRES (10) (see <ref> [31] </ref> for references to this and the other Krylov subspace methods of this section) and CGNE, CG applied to A T Ax = A T b.
Reference: [32] <author> P. </author> <title> Saylor, Leapfrog variants of iterative methods for linear algebraic equations, </title> <journal> J. Comp. Appl. Math., </journal> <pages> 169-193, 24(1988). </pages>
Reference-contexts: Iterates are then formed that satisfy (1) for some class of polynomials. Chebyshev polynomials [3, 27, 28] use ellipses for the convex region and Chebyshev polynomials to define p k (). Least squares polynomial methods <ref> [32] </ref> use an approximation of the convex hull cvx ((A)) of the extremal eigenvalues of A for the enclosing region and polynomials p k () that minimize a weighted sum of squares on the boundary of cvx ((A)) .
Reference: [33] <institution> Scientific Computing Associates, Inc., PCGPAK User's Guide, </institution> <address> New Haven, CT (1987). </address>
Reference-contexts: GMRES (10) is from the PCGPAK library and has been optimized for the Cray-X/MP by Scientific Computing Associates <ref> [33] </ref>, with portions written in Cray Assembly Language.
Reference: [34] <author> A. Sherman, </author> <title> An empirical investigation of methods for nonsymmetric linear systems, Elliptic Problem Solvers, 429-434, </title> <editor> ed. M. Schultz, </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> (1981). </year>
Reference-contexts: a variety of sources 13 Problem: 1 2 3 4 5 6 Re () &gt; 0 Yes Yes No Yes Yes Yes A + A T Definite Yes No No No No No Estimated (A) 9 57 40000 4786 36 77 Table 2 Properties of A for N = 1728 <ref> [13, 22, 23, 34] </ref>. Each test problem has boundary conditions chosen to give a predetermined solution to the partial differential equation, so that the norm of the error vector as well as that of the residual vector can be checked.
Reference: [35] <author> G. Smith, </author> <title> Numerical Solution of Partial Differential Equations: Finite Difference Methods, </title> <publisher> Clarendon Press, </publisher> <address> Oxford (1978). </address>
Reference-contexts: This section describes the set of test problems, outlines the row partitioning criteria, and presents the row partitioning used in the testing. 5.1. Test Problems. Test problems are obtained from the seven point centered difference operator <ref> [35] </ref> for elliptic partial differential equations au xx + bu yy + cu zz + du x + eu y + f u z + gu = F (24) where a g are functions of (x; y; z) and the domain is the unit cube [0; 1] fi [0; 1] fi
Reference: [36] <author> G. Stewart, </author> <title> On the perturbation of pseudo-inverses, projections, and linear least squares problems, </title> <journal> SIAM Rev., </journal> <pages> 634-662, </pages> <year> 19(1977). </year>
Reference-contexts: From this one can obtain the CS decomposition <ref> [16, 36] </ref>, which is stated below in terms of the projectors P i . 10 Theorem 4.1 (CS Decomposition). Let P i 2 &lt; NfiN be the orthogonal projectors onto the subspaces L i , for i = 1; 2.
Reference: [37] <author> K. Tanabe, </author> <title> A projection method for solving a singular system of linear equations, </title> <journal> Numer. Math., </journal> <pages> 203-214 17(1971). </pages>
Reference-contexts: Kaczmarz [20] proposed iteration (3) and proved convergence for the m = N case where each block row A T i consists of a single row of A. Since then many authors <ref> [10, 11, 19, 29, 30, 37] </ref> have examined the convergence of related iterative methods. The theoretical robustness of (3) is remarkable and the iteration converges even when A is singular or rectangular.
Reference: [38] <author> T. Whitney, R. Meany, </author> <title> Two algorithms related to the method of steepest descent, </title> <journal> SIAM J. Numer. Anal.,109-118, </journal> <volume> 4(1967). </volume>
Reference-contexts: The advantage of this approach is that the projections can be computed in parallel and then added. In 1939 G. Cimmino [9] first proposed an iteration related to (18), and since then it has been examined by several others <ref> [1, 11, 12, 15, 25, 26, 38] </ref>. Later we will show how the individual projections can, for a wide class of problems, be computed with parallelism.
Reference: [39] <author> J. Wilkinson, </author> <title> Modern error analysis, </title> <journal> SIAM Rev., </journal> <volume> 548-568, 13(1971). </volume> <pages> 29 </pages>
References-found: 39

