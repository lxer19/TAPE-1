URL: ftp://ftp.cis.upenn.edu/pub/msingh/aaai97.ps.Z
Refering-URL: http://www.cis.upenn.edu/~msingh/frames/papers_list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: msingh@gradient.cis.upenn.edu  
Title: Learning Bayesian Networks from Incomplete Data  
Author: Moninder Singh 
Address: Philadelphia, PA 19104-6389  
Affiliation: Department of Computer and Information Science University of Pennsylvania  
Abstract: Much of the current research in learning Bayesian Networks fails to effectively deal with missing data. Most of the methods assume that the data is complete, or make the data complete using fairly ad-hoc methods; other methods do deal with missing data but learn only the conditional probabilities, assuming that the structure is known. We present a principled approach to learn both the Bayesian network structure as well as the conditional probabilities from incomplete data. The proposed algorithm is an iterative method that uses a combination of Expectation-Maximization (EM) and Imputation techniques. Results are presented on synthetic data sets which show that the performance of the new algorithm is much better than ad-hoc methods for handling missing data. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, S.; Olesen, K.; Jensen, F.; and Jensen, F. </author> <year> 1989. </year> <title> HUGIN A Shell for building Bayesian Belief Universes for Expert Systems. </title> <booktitle> In Procs. IJCAI'89, </booktitle> <pages> 1080-1085. </pages>
Reference-contexts: We used the ALARM network (Beinlich et al. 1989) as the gold-standard for our experiments. We generated a database of 10000 complete cases from this network using the case-generation facility of HUGIN <ref> (Anderson et al. 1989) </ref>. We then, systematically, re moved data from the database in one of two ways: 1. MCAR: To remove data under the MCAR assump tion, we did the following: 1. Let ffi be the required percentage of missing data. 2.
Reference: <author> Beinlich, I.; Suermondt, H.; Chavez, R.; and Cooper, G. </author> <year> 1989. </year> <title> The ALARM monitoring system: A Case Study with Two probabilistic inference techniques for belief networks. </title> <booktitle> In Procs. Second European Conference on AI in Medicine, </booktitle> <pages> 247-256. </pages>
Reference-contexts: Finally, we measure the performance of the algorithm by comparing the true and the learned joint probability distributions. We used the ALARM network <ref> (Beinlich et al. 1989) </ref> as the gold-standard for our experiments. We generated a database of 10000 complete cases from this network using the case-generation facility of HUGIN (Anderson et al. 1989). We then, systematically, re moved data from the database in one of two ways: 1.
Reference: <author> Cooper, G. </author> <year> 1995. </year> <title> A Bayesian method for learning Belief networks that contain hidden variables. </title> <journal> Journal of Intelligent Systems 4 </journal> <pages> 71-88. </pages>
Reference: <author> Cooper, G., and Herskovits, E. </author> <year> 1992. </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <booktitle> Machine Learning 9 </booktitle> <pages> 309-347. </pages>
Reference-contexts: Note that the log-likelihood function can be evaluated without any extra work by simply summing over all cases the log of the normalization constant obtained after propagating the evidence in that case. For inducing Bayesian networks from complete-data (step 2a), we used the K2 algorithm <ref> (Cooper & Her-skovits 1992) </ref>, with a total ordering on the attributes, consistent with the gold-standard network, as input.
Reference: <author> Dempster, A.; Laird, N.; and Rubin, D. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the em algorithm. </title> <journal> Journal of the Royal Statistical Society (Series B) 39 </journal> <pages> 1-38. </pages>
Reference-contexts: A number of methods have also been developed that define a model for the partially missing data and base inferences on the likelihood under that model. Two commonly used techniques are the EM algorithm <ref> (Dempster, Laird, & Rubin 1977) </ref> and Gibbs sampling (Geman & Geman 1984).
Reference: <author> Geman, S., and Geman, D. </author> <year> 1984. </year> <title> Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. </title> <journal> IEEE Trans. on PAMI 6 </journal> <pages> 721-741. </pages>
Reference-contexts: Research on handling data with missing values has been going on within the Statistics community for over three decades. Several techniques have been developed to handle such data, the principal ones being the Expectation-Maximization or EM algorithm (Demp-ster, Laird, & Rubin 1977), Gibbs sampling <ref> (Geman & Geman 1984) </ref> and Multiple Imputation (Rubin 1987). However, within the Machine Learning community, relatively little attention has been paid to the development of effective techniques for handling incomplete data. This paper deals with the task of learning Bayesian networks from data with missing values. <p> A number of methods have also been developed that define a model for the partially missing data and base inferences on the likelihood under that model. Two commonly used techniques are the EM algorithm (Dempster, Laird, & Rubin 1977) and Gibbs sampling <ref> (Geman & Geman 1984) </ref>. Of these, the EM algorithm is especially appealing having the advantages of being conceptually easy to design, and more importantly, having the property of converging relatively quickly; however, it suffers from the disadvantage of often finding only local maxima.
Reference: <author> Ghahramani, Z., and Jordan, M. </author> <year> 1994. </year> <title> Learning from incomplete data. A.I. </title> <type> Memo 1509, </type> <institution> MIT AI Lab. </institution>
Reference-contexts: reflect the fact that the missing value actually came from the original multinomial value set; thus, for a dataset where the value of a particular attribute is often missing, an algorithm may form a class based on that attribute value being unknown, which may not be desirable in a classifier <ref> (Ghahramani & Jordan 1994) </ref>. Other commonly used techniques to handle incomplete data involve replacing each missing value by a single value (Single Imputation) computed by some method.
Reference: <author> Heckerman, D. </author> <year> 1995 </year> <month> (revised Nov. </month> <year> 1996). </year> <title> A tutorial on learning with Bayesian networks. </title> <type> Technical report MSR-TR-95-06, </type> <institution> Microsoft Research, </institution> <address> Redmond, WA. </address>
Reference-contexts: Of these, the EM algorithm is especially appealing having the advantages of being conceptually easy to design, and more importantly, having the property of converging relatively quickly; however, it suffers from the disadvantage of often finding only local maxima. These methods are reviewed in detail in <ref> (Heckerman 1995) </ref>. Heckerman (1995) also discusses an approximation method for computing the likelihood of a Bayesian network structure given the data; however, using this technique to compare various structures and determine the most likely network can be computationally inefficient.
Reference: <author> Heckerman, D.; Geiger, D.; and Chickering, M. </author> <year> 1995. </year> <title> Learning Bayesian networks-the combination of knowledge and statistical data. </title> <booktitle> Machine Learning 20(3) </booktitle> <pages> 197-243. </pages>
Reference-contexts: Of these, the EM algorithm is especially appealing having the advantages of being conceptually easy to design, and more importantly, having the property of converging relatively quickly; however, it suffers from the disadvantage of often finding only local maxima. These methods are reviewed in detail in <ref> (Heckerman 1995) </ref>. Heckerman (1995) also discusses an approximation method for computing the likelihood of a Bayesian network structure given the data; however, using this technique to compare various structures and determine the most likely network can be computationally inefficient.
Reference: <author> Kullback, S., and Leibler, R. </author> <year> 1951. </year> <title> Information and sufficiency. </title> <journal> Ann. Math. Statistics 22. </journal>
Reference-contexts: To measure the quality of the learned network, we computed the cross-entropy <ref> (Kullback & Leibler 1951) </ref> from the actual distribution (represented by the gold-standard) to the distribution represented by the learned network. The cross-entropy measure is defined as follows. Let P denote the joint distribution represented by the gold-standard network and Q the joint distribution represented by the learned network.
Reference: <author> Lauritzen, S. </author> <year> 1995. </year> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis 19 </journal> <pages> 191-201. </pages>
Reference: <author> Little, R. J. A., and Rubin, D. B. </author> <year> 1987. </year> <title> Statistical Analysis with Missing Data. </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: Thus, the parameters of the missing data mechanism can be ignored for the purposes of estimating <ref> (Little & Rubin 1987) </ref>. It is precisely these kinds of problems (where the missing data is either MAR or MCAR) that we are interested in. <p> Some of the frequently used procedures involve replacing a missing value by the mean of the observed values for that attribute (mean imputation) or, more generally, by sampling from the unconditional distribution of that attribute estimated from the observed values (hot-deck imputation) <ref> (Little & Rubin 1987) </ref>. These methods, though simple, can lead to serious biases since a single value can in no way convey the uncertainty about the true value; rather, it depicts (incorrectly) that the imputed value is in fact the `actual' missing value.
Reference: <author> Matzkevich, I., and Abramson, B. </author> <year> 1993. </year> <title> Deriving a minimal i-map of a belief network relative to a target ordering of its nodes. </title> <booktitle> In Procs. UAI'93, </booktitle> <pages> 159-165. </pages>
Reference: <author> Rubin, D. </author> <year> 1987. </year> <title> Multiple Imputation for Nonresponse in Surveys. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: Several techniques have been developed to handle such data, the principal ones being the Expectation-Maximization or EM algorithm (Demp-ster, Laird, & Rubin 1977), Gibbs sampling (Geman & Geman 1984) and Multiple Imputation <ref> (Rubin 1987) </ref>. However, within the Machine Learning community, relatively little attention has been paid to the development of effective techniques for handling incomplete data. This paper deals with the task of learning Bayesian networks from data with missing values. <p> Thus, the parameters of the missing data mechanism can be ignored for the purposes of estimating <ref> (Little & Rubin 1987) </ref>. It is precisely these kinds of problems (where the missing data is either MAR or MCAR) that we are interested in. <p> Some of the frequently used procedures involve replacing a missing value by the mean of the observed values for that attribute (mean imputation) or, more generally, by sampling from the unconditional distribution of that attribute estimated from the observed values (hot-deck imputation) <ref> (Little & Rubin 1987) </ref>. These methods, though simple, can lead to serious biases since a single value can in no way convey the uncertainty about the true value; rather, it depicts (incorrectly) that the imputed value is in fact the `actual' missing value. <p> These methods, though simple, can lead to serious biases since a single value can in no way convey the uncertainty about the true value; rather, it depicts (incorrectly) that the imputed value is in fact the `actual' missing value. A much better technique is to use Multiple imputation methods <ref> (Rubin 1987) </ref> | that impute more than one value for the missing data points. Each missing value is replaced by an ordered vector of M 2 imputed values thus giving M completed data sets, each of which can be analyzed using standard complete-data methods.
Reference: <author> Singh, M., and Valtorta, M. </author> <year> 1995. </year> <title> Construction of Bayesian network structures from data: a brief survey and an efficient algorithm. </title> <journal> International Journal of Approximate Reasoning 12 </journal> <pages> 111-131. </pages>
Reference-contexts: Second, further experiments are needed on real-world datasets. For such domains, it may not be possible to get a good ordering on the domain attributes. In such cases, it is possible to incorporate algorithms for learning Bayesian networks that do not require a node ordering e.g. <ref> (Singh & Valtorta 1995) </ref> with the proposed algorithm to learn Bayesian networks from incomplete data. These are both important issues which we hope to address in the future. Acknowledgments. This research is funded by an IBM Cooperative Fellowship. The author is thankful to Dr.
References-found: 15

