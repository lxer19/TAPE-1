URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/opitz.nips96.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/opitz.nips96.ps.abstract.html
Root-URL: 
Email: opitz@d.umn.edu  shavlik@cs.wisc.edu  
Title: Generating Accurate and Diverse Members of a Neural-Network Ensemble  
Author: David S. Touretzky, Michael C. Mozer and Michael E. Hasselmo, David W. Opitz Jude W. Shavlik 
Address: Duluth, MN 55812  Madison, WI 53706  
Affiliation: Computer Science Department University of Minnesota  Computer Sciences Department University of Wisconsin  
Note: Appears in Advances in Neural Information Processing Systems 8,  eds., MIT Press: Cambridge, MA.  
Abstract: Neural-network ensembles have been shown to be very accurate classification techniques. Previous work has shown that an effective ensemble should consist of networks that are not only highly correct, but ones that make their errors on different parts of the input space as well. Most existing techniques, however, only indirectly address the problem of creating such a set of networks. In this paper we present a technique called Addemup that uses genetic algorithms to directly search for an accurate and diverse set of trained networks. Addemup works by first creating an initial population, then uses genetic operators to continually create new networks, keeping the set of networks that are as accurate as possible while disagreeing with each other as much as possible. Experiments on three DNA problems show that Addemup is able to generate a set of trained networks that is more accurate than several existing approaches. Experiments also show that Addemup is able to effectively incorporate prior knowledge, if available, to improve the quality of its ensemble.
Abstract-found: 1
Intro-found: 1
Reference: <author> Alpaydin, E. </author> <year> (1993). </year> <title> Multiple networks for function learning. </title> <booktitle> In Proceedings of the 1993 IEEE International Conference on Neural Networks, </booktitle> <volume> vol I, </volume> <pages> pages 27-32, </pages> <address> San Fransisco. </address>
Reference: <author> Breiman, L. </author> <year> (1994). </year> <title> Bagging predictors. </title> <type> Technical Report 421, </type> <institution> Department of Statistics, University of California, Berkeley. </institution>
Reference-contexts: In fact, when using domain-specific rules, our algorithm showed statistically significant improvements over (a) the single best network seen during the search, (b) a previously proposed ensemble method called bagging <ref> (Breiman, 1994) </ref>, and (c) a similar algorithm whose objective function is simply the validation-set correctness of the network. In summary, Addemup is successful in generating a set of neural networks that work well together in producing an accurate prediction. Acknowledgements This work was supported by Office of Naval Research grant N00014-93-1-0998.
Reference: <author> Clemen, R. </author> <year> (1989). </year> <title> Combining forecasts: A review and annotated bibliography. </title> <journal> International Journal of Forecasting, </journal> <volume> 5 </volume> <pages> 559-583. </pages>
Reference-contexts: Thus we define our weights for combining the networks as follows: w i = k (1 E k ) While simply averaging the outputs generates a good composite model <ref> (Clemen, 1989) </ref>, we include the predicted accuracy in our weights since one should believe accurate models more than inaccurate ones. 4 Experimental Study The genetic algorithm we use for generating new network topologies is the Regent algorithm (Opitz and Shavlik, 1994).
Reference: <author> Drucker, H., Cortes, C., Jackel, L., LeCun, Y., and Vapnik, V. </author> <year> (1994). </year> <title> Boosting and other machine learning algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 53-61, </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hansen, L. and Salamon, P. </author> <year> (1990). </year> <title> Neural network ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 993-1001. </pages>
Reference: <author> Hashem, S., Schmeiser, B., and Yih, Y. </author> <year> (1994). </year> <title> Optimal linear combinations of neural networks: An overview. </title> <booktitle> In Proceedings of the 1994 IEEE International Conference on Neural Networks, </booktitle> <address> Orlando, FL. </address>
Reference: <author> Krogh, A. and Vedelsby, J. </author> <year> (1995). </year> <title> Neural network ensembles, cross validation, and active learning. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> vol 7, </volume> <publisher> Cambridge, </publisher> <address> MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Maclin, R. and Shavlik, J. </author> <year> (1995). </year> <title> Combining the predictions of multiple classifiers: Using competitive learning to initialize neural networks. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Canada. </address>
Reference: <author> Mani, G. </author> <year> (1991). </year> <title> Lowering variance of decisions by using artificial neural network portfolios. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 484-486. </pages>
Reference: <author> Opitz, D. </author> <year> (1995). </year> <title> An Anytime Approach to Connectionist Theory Refinement: Refining the Topologies of Knowledge-Based Neural Networks. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, WI. </institution>
Reference-contexts: Each of these domains is accompanied by a set of approximately correct rules describing what is currently known about the task <ref> (see Opitz, 1995 or Opitz and Shavlik, 1994 for more details) </ref>. Our experiments measure the test-set error of Addemup on these tasks. Each ensemble consists of 20 networks, and the Regent and Addemup algorithms considered 250 networks during their genetic search.
Reference: <author> Opitz, D. and Shavlik, J. </author> <year> (1994). </year> <title> Using genetic search to refine knowledge-based neural networks. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 208-216, </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: E k ) While simply averaging the outputs generates a good composite model (Clemen, 1989), we include the predicted accuracy in our weights since one should believe accurate models more than inaccurate ones. 4 Experimental Study The genetic algorithm we use for generating new network topologies is the Regent algorithm <ref> (Opitz and Shavlik, 1994) </ref>. Regent uses genetic algorithms to search through the space of knowledge-based neural network (KNN) topologies. KNNs are networks whose topologies are determined as a result of the direct mapping of a set of background rules that represent what we currently know about our task.
Reference: <author> Perrone, M. </author> <year> (1992). </year> <title> A soft-competitive splitting rule for adaptive tree-structured neural networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <pages> pages 689-693, </pages> <address> Baltimore, MD. </address>
Reference: <author> Towell, G. and Shavlik, J. </author> <year> (1994). </year> <booktitle> Knowledge-based artificial neural networks. Artificial Intelligence, </booktitle> <address> 70(1,2):119-165. </address>
Reference-contexts: Regent uses genetic algorithms to search through the space of knowledge-based neural network (KNN) topologies. KNNs are networks whose topologies are determined as a result of the direct mapping of a set of background rules that represent what we currently know about our task. Kbann <ref> (Towell and Shavlik, 1994) </ref>, for instance, translates a set of propositional rules into a neural network, then refines the resulting network's weights using backpropagation.
Reference: <author> Wolpert, D. </author> <year> (1992). </year> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259. </pages>
References-found: 14

