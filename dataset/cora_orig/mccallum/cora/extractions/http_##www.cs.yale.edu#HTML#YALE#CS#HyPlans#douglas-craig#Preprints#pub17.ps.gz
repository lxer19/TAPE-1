URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/douglas-craig/Preprints/pub17.ps.gz
Refering-URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/douglas-craig/ccd-preprints.html
Root-URL: http://www.cs.yale.edu
Title: THE DOMAIN REDUCTION METHOD: HIGH WAY REDUCTION IN THREE DIMENSIONS AND CONVERGENCE WITH INEXACT SOLVERS  
Author: CRAIG C. DOUGLAS AND JAN MANDEL 
Abstract: We study a method for parallel solution of elliptic partial differential equations which decomposes the problem into a number of independent subproblems on subspaces of the underlying solution space. Using symmetries of the domain, we obtain up to 64 such subproblems for a 3 dimensional cube and the method reduces to a direct solver. In the general case, or when the subproblems are solved only approximately, the method becomes an iterative method or can be used as a preconditioner. Bounds on the resulting convergence factors and condition numbers are given. 1. Introduction. In this paper, we approximate the solution to the elliptic 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. B. Belford and J. H. E. Kaufman, </author> <title> An application of approximation theory to an error estimate in linear algebra, </title> <journal> Math. Comp., </journal> <volume> 28 (1974), </volume> <pages> pp. 711-712. </pages>
Reference-contexts: To apply conjugate gradients, we need that the matrix C is symmetric, positive definite, or, equivalently, that the mapping C 1 A be 158 self-adjoint and positive definite relative to a (u; v). Then (see <ref> [1, 12] </ref>) the convergence rate of the conjugate gradient method can be bounded in terms of the condition number = min (C 1 A) Theorem 3. Let (4.7) hold. Then the mapping C 1 A is self-adjoint in the inner product a (u; v) .
Reference: [2] <author> P. E. Bjtrstadt and J. Mandel, </author> <title> Spectra of sums of orthogonal projections with applications to parallel computing. </title> <note> In preparation. </note>
Reference-contexts: Similarly, if c 2 P n i=1 ku i k 2 kuk 2 for all u, then max c 2 (cf., <ref> [2] </ref>). 159 Acknowledgements. We would like to thank Franco Brezzi for discussions about x3. The parallel computers mentioned in this paper belong to the Computer Science Department, Yale University, where the first author is a research affiliate.
Reference: [3] <author> F. Brezzi, C. C. Douglas, and L. D. Marini, </author> <title> A parallel domain reduction method, Numer. Meth. for PDE, </title> <booktitle> 5 (1989), </booktitle> <pages> pp. 195-202. </pages>
Reference-contexts: The applicability to shared or distributed memory, coarse or fine grained parallel computers was discussed informally in [9]. The elliptic partial differential equation case (with an emphasis on one and two dimensions) was analyzed in [10], and a technique for doubling the basic parallelism was introduced in <ref> [3] </ref>. In this paper, we first extend the idea in [3] to three dimensional problems, where up to an eight fold increase in the basic parallelism can be accomplished. We are once again interested in the case where smoothing is unnecessary, making this a projection method. <p> The elliptic partial differential equation case (with an emphasis on one and two dimensions) was analyzed in [10], and a technique for doubling the basic parallelism was introduced in <ref> [3] </ref>. In this paper, we first extend the idea in [3] to three dimensional problems, where up to an eight fold increase in the basic parallelism can be accomplished. We are once again interested in the case where smoothing is unnecessary, making this a projection method. <p> High Way Domain Reduction. In this section, we construct a different eight way reduction of the 3 dimensional cube, based on the number of Neumann and Dirichlet boundary faces. This work is an extension of the two dimensional case in <ref> [3] </ref>. By translating the coordinates and composing simple operators, we extend the eight way domain reduction in x2 to a 60 or 64 way domain reduction.
Reference: [4] <author> N. Carriero and D. Gelernter, </author> <title> The S/Net's Linda kernel, </title> <journal> ACM Trans. Comp. Sys., </journal> <year> (1986). </year>
Reference-contexts: This style of domain reduction has been coded and run on both shared memory (Encore Multimax and Sequent Balance) and distributed memory (Intel iPSC-2) parallel computers using MADPACK and the LINDA system (see <ref> [4, 5, 6, 7] </ref>) for elliptic boundary value problems in two and three dimensions resulting in a five or seven point stencil, respectively. Some numerical experiments from the Encore machine are contained in Table 2.1.
Reference: [5] <author> C. C. Douglas, </author> <title> Madpack (version 2) users' guide. Contact the author for availability information. [6] , Multi-grid algorithms for elliptic boundary-value problems, </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> May </month> <year> 1982. </year> <note> Also, </note> <institution> Computer Science Department, Yale University, </institution> <type> Technical Report 223. </type> <note> [7] , Multi-grid algorithms with applications to elliptic boundary-value problems, SIAM J. Numer. Anal., </note> <month> 21 </month> <year> (1984), </year> <pages> pp. 236-254. </pages>
Reference-contexts: This style of domain reduction has been coded and run on both shared memory (Encore Multimax and Sequent Balance) and distributed memory (Intel iPSC-2) parallel computers using MADPACK and the LINDA system (see <ref> [4, 5, 6, 7] </ref>) for elliptic boundary value problems in two and three dimensions resulting in a five or seven point stencil, respectively. Some numerical experiments from the Encore machine are contained in Table 2.1.
Reference: [8] <author> C. C. Douglas and W. L. Miranker, </author> <title> Constructive interference in parallel algorithms, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 25 (1988), </volume> <pages> pp. </pages> <month> 376-398. </month> <title> [9] , Some nontelescoping parallel algorithms based on serial multigrid/aggregation/disaggre-gation techniques, in Multigrid Methods: Theory, Applications, and Supercomputing, </title> <editor> S. F. McCormick, ed., </editor> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1988, </year> <pages> pp. 167-176. </pages>
Reference-contexts: Each subproblem should be solved in parallel using the fastest known solution method appropriate. The basic method was first derived in <ref> [8] </ref> as a parallel multilevel method for linear systems of equations where smoothing was unnecessary. The applicability to shared or distributed memory, coarse or fine grained parallel computers was discussed informally in [9].
Reference: [10] <author> C. C. Douglas and B. F. Smith, </author> <title> Using symmetries and antisymmetries to analyze a parallel multigrid algorithm, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 26 (1989), </volume> <pages> pp. 1439-1461. </pages>
Reference-contexts: The applicability to shared or distributed memory, coarse or fine grained parallel computers was discussed informally in [9]. The elliptic partial differential equation case (with an emphasis on one and two dimensions) was analyzed in <ref> [10] </ref>, and a technique for doubling the basic parallelism was introduced in [3]. In this paper, we first extend the idea in [3] to three dimensional problems, where up to an eight fold increase in the basic parallelism can be accomplished. <p> We are once again interested in the case where smoothing is unnecessary, making this a projection method. We then extend the analysis in <ref> [10] </ref> to the case when subproblems are solved inexactly and for the case when the method is inherently iterative. In x2, we define the algorithm as it pertains to this paper in a very abstract manner. <p> the problem (2.3) as Lu = f , we find that the corresponding operators for the subproblems are then defined in the Galerkin sense: L i = R i LP i : Examples of several choices of restriction and prolongation operators, and the resulting L i , are given in <ref> [10] </ref>. We complete this section with a simple example where the domain reduction method uses symmetries in (1.1) to split the problem into several subproblems. <p> This is, for example, the case in the "robust multigrid method" by Hackbusch [11], where the subspaces are defined as ranges of suitable prolongation operators on a uniform grid. A convergence result for this method and comparisons with other prolongations operators for a model problem is in <ref> [10] </ref>. Define the energy norm by kuk = a (u; u); and equip the space V with the inner product a (u; v). The direct method of Section 2 now becomes the following iterative algorithm: 156 Algorithm 2.
Reference: [11] <author> W. Hackbusch, </author> <title> A new approach to robust multi-grid solvers, </title> <booktitle> in ICIAM'87: Proceedings of the First International Conference on Industrial and Applied Mathematics, Society for Industrial and Applied Mathematics, </booktitle> <address> Philadelphia, </address> <year> 1988, </year> <pages> pp. 114-126. </pages>
Reference-contexts: Iterative Algorithm and Inexact Solvers. In practice, we often solve the subproblems (2.3) only approximately by some iterative method. In addition, the orthogonality relations V i ? V j in (2.2) may hold also only approximately. This is, for example, the case in the "robust multigrid method" by Hackbusch <ref> [11] </ref>, where the subspaces are defined as ranges of suitable prolongation operators on a uniform grid. A convergence result for this method and comparisons with other prolongations operators for a model problem is in [10].
Reference: [12] <author> S. Kaniel, </author> <title> Estimates for some computational techniques in linear algebra, </title> <journal> Math. Comp., </journal> <year> (1966), </year> <pages> pp. 369-378. </pages>
Reference-contexts: To apply conjugate gradients, we need that the matrix C is symmetric, positive definite, or, equivalently, that the mapping C 1 A be 158 self-adjoint and positive definite relative to a (u; v). Then (see <ref> [1, 12] </ref>) the convergence rate of the conjugate gradient method can be bounded in terms of the condition number = min (C 1 A) Theorem 3. Let (4.7) hold. Then the mapping C 1 A is self-adjoint in the inner product a (u; v) .
Reference: [13] <author> P. L. Lions, </author> <title> On the Schwarz alternating method I, in Domain Decomposition Methods for Partial Differential Equations, </title> <editor> R. Glowinski, G. H. Golub, G. A. Meurant, and J. Periaux, eds., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <address> Philadelphia, </address> <year> 1988, </year> <pages> pp. 1-42. </pages>
Reference-contexts: We note that trivially, we always have max n. A lower bound on min follows from the following result (see <ref> [13, p. 7] </ref> or [16, Lemma 4]): if there is a constant c 1 &gt; 0 such that for all u 2 V , u = P n i=1 u i , u i 2 V i , it holds that c 1 P n min c 1 .
Reference: [14] <author> J. Mandel and S. F. McCormick, </author> <title> Iterative solution of elliptic equations with refinement: the model multi-level case, in Domain Decomposition Methods for Partial Differential Equations II, </title> <editor> T. Chan, R. Glowinski, G. A. Meurant, J. Periaux, and O. Widlund, eds., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <address> Philadelphia, </address> <year> 1989, </year> <pages> pp. 93-102. </pages>
Reference-contexts: In the case of two subspaces (n = 2), we know (see <ref> [14] </ref>) that min = 1 cos (V 1 ; V 2 ) and max = 1 + cos (V 1 ; V 2 ); where the cosine of the two subspaces is defined in the inner product a (u; v).
Reference: [15] <author> H. A. Schwarz, </author> <title> Uber einige Abbildungsaufgaben, </title> <journal> Ges. Math. Abh., </journal> <volume> 11 (1869), </volume> <pages> pp. 65-83. </pages>
Reference-contexts: If more than one iterative method is used, then " is the maximum of the convergence factors. If the subproblems are solved exactly, we get a method related to the well-known Schwarz alternating method (see <ref> [15] </ref>). The following theorem gives the convergence factor of Algorithm 2 in terms of the (maximum) convergence factor of the iterative method (or methods) used to solve the subproblems and a measure of orthogonality of the subspaces V i : Theorem 1.
Reference: [16] <author> O. B. Widlund, </author> <title> Optimal iterative refinement methods, in Domain Decomposition Methods for Partial Differential Equations II, </title> <editor> T. Chan, R. Glowinski, G. A. Meurant, J. Periaux, and O. Widlund, eds., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <address> Philadelphia, </address> <year> 1989, </year> <pages> pp. 114-126. </pages>
Reference-contexts: We note that trivially, we always have max n. A lower bound on min follows from the following result (see [13, p. 7] or <ref> [16, Lemma 4] </ref>): if there is a constant c 1 &gt; 0 such that for all u 2 V , u = P n i=1 u i , u i 2 V i , it holds that c 1 P n min c 1 .
References-found: 13

