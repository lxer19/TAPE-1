URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3304/3304.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Human Emotion Recognition from Motion Using a Radial Basis Function Network Architecture  
Author: Mark Rosenblum Yaser Yacoob Larry S. Davis 
Address: College Park, MD 20742-3275  
Affiliation: Computer Vision Laboratory Center for Automation Research University of Maryland  
Date: June 1994  
Pubnum: CAR-TR-721 DACA76-92-C-0009  
Abstract: In this paper a radial basis function network architecture is developed that learns the correlation between facial feature motion patterns and human emotions. We describe a hierarchical approach which at the highest level identifies emotions, at the mid level determines motions of facial features, and at the low level recovers motion directions. Individual emotion networks were trained to recognize the "smile" and "surprise" emotions. Each network was trained by viewing a set of sequences of one emotion for many subjects. The trained neural network was then tested for retention, extrapolation and rejection ability. Success The support of the Advanced Research Projects Agency (ARPA Order No. 8459) and the U.S. Army Topographic Engineering Center under Contract DACA76-92-C-0009 is gratefully acknowledged, as is the help of Sandy German in preparing this paper. rates were about 88% for retention, 73% for extrapolation, and 79% for rejection.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Abdel-Mottaleb, R. Chellappa, and A. Rosenfeld, </author> <title> "Binocular motion stereo using MAP estimation", </title> <booktitle> Procceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> 321-327, </pages> <year> 1993. </year>
Reference-contexts: The system is composed of the following components: 4 * Optical flow computation: Optical flow is computed at the points with high gradient at each frame. Our algorithm for flow computation is based on a correlation approach proposed by Abdel-Mottaleb et al. <ref> [1] </ref>. It computes subpixel flow assuming that the motion between two consecutive images is bounded within an n fi n window. * Region tracking: Accurate localization of facial features is both difficult and compu-tationally expensive if performed for each frame. <p> work unifies the face features described by Ekman and Friesen and the general motion patterns suggested by Bassili into one spatio-temporal system that allows facial expression recognition from dynamic images. 4 Optical Flow Computation The approach we use for optical flow computation is one recently proposed by Abdel-Mottaleb et al. <ref> [1] </ref>. Assume that a pixel's displacement (x; y) between frame t and frame t + 1 is at most n pixels and is expressed in terms of an integer and a fraction part, i.e., (x; y) = (i x + f x ; i y + f y ).
Reference: [2] <author> J.N. Bassili, </author> <title> "Emotion recognition: The role of facial movement and the relative importance of upper and lower areas of the face", </title> <journal> Journal of Personality and Social Psychology, </journal> <volume> Vol. 37, </volume> <pages> 2049-2059, </pages> <year> 1979. </year>
Reference-contexts: These pictures allow subjects to detect the presence of static cues (such as wrinkles) as well as the position and shape of the facial features. Few studies have directly investigated the influence of the motion and deformation of facial features on the interpretation of facial expressions. Bassili <ref> [2] </ref> suggested that motion in the image of a face would allow emotions to be identified even with minimal information about the spatial arrangement of features. <p> Whereas all expressions were recognized at above chance levels in dynamic images, only happiness and sadness were recognized at above chance level in static images. Building on the results of Bassili <ref> [2] </ref> we explore the potential of motion analysis in an autonomous system for recognition of emotions in facial images. We focus our attention on the appearance of the face from its near frontal image projection, without considering the underlying anatomic and musculature models and actions.
Reference: [3] <author> V. Bruce, </author> <title> Recognizing Faces, </title> <publisher> Lawrence Erlbaum Assoc., </publisher> <address> London, </address> <year> 1988. </year>
Reference-contexts: Furthermore, the approach builds on a model that is suitable for synthesizing facial expressions but remains untested in analysis of facial expressions (for more details see <ref> [3] </ref>). The bottom-up approach covered the area of the face with evenly divided rectangular regions over which feature vectors derived from an optical flow computation were computed. The feature vectors are defined over a 15-dimensional space that is computed based on the means and variances of the optical flow.
Reference: [4] <author> C. Darwin, </author> <title> The Expression of Emotions in Man and Animals, </title> <publisher> John Murray, 1872, reprinted by University of Chicago Press, </publisher> <year> 1965. </year>
Reference-contexts: Developing such methods would contribute to human-computer interaction and to other applications, such as low-bandwidth transmission of facial data and face recognition from dynamic imagery. The study of emotion in human facial expressions was pioneered by Darwin's work <ref> [4] </ref> and has been extensively studied in psychology during the last thirty years [25]. This research has indicated that at least six emotions are universally associated with distinct facial expressions. Several other emotions, and many combinations of emotions, have been studied but remain unconfirmed as universally distinguishable.
Reference: [5] <author> P. Ekman and W. Friesen, </author> <title> Unmasking the Face, </title> <publisher> Prentice-Hall, </publisher> <year> 1975. </year>
Reference-contexts: In this paper we focus on the connectionist approach to solving this problem. 3 Psychological basis for recognizing facial expressions Table 1 summarizes the results of Ekman and Friesen <ref> [5] </ref> on the universal cues for recognizing the six principal emotions. These cues describe the peak of each expression and thus they provide a human interpretation of the static appearance of the facial feature.
Reference: [6] <author> P. Ekman and W. Friesen, </author> <title> The Facial Action Coding System, </title> <publisher> Consulting Psychologists Press, </publisher> <address> San Francisco, CA, </address> <year> 1978. </year>
Reference-contexts: Four facial expressions were studied: surprise, anger, happiness, and disgust. The top-down approach assumed that the face's image can be divided into muscle units that correspond to the Action Units (AUs) suggested by Ekman and Friesen <ref> [6] </ref>. Optical flow is computed within rectangles that include these muscle units, which in turn can be related to facial expression. However, Mase did not report any results on mapping the optical motion results into facial expressions. <p> The image sequence is densely sampled in time. * The subjects wear no eyeglasses. We chose not to model or analyze facial muscle actions, setting our work apart from [15, 16, 21], as well as not to use models for muscle actions <ref> [6] </ref>. Instead, we focus on the motions associated with the edges of the mouth, eyes, and eyebrows.
Reference: [7] <author> E.B. Goldstein, </author> <title> Sensation and Perception, </title> <publisher> Wadsworth Publishing Co., </publisher> <year> 1989. </year>
Reference-contexts: This relates to the across-fiber-pattern theory used to describe sensory coding in humans and animals <ref> [7] </ref>. <p> This is similar to the theory in psychology and neuroscience, across-pattern-fiber-theory, used to explain how ensembles of neurons in the brain are used to collectively generate a response while overcoming the ambiguities that occur at individual 19 neurons <ref> [7] </ref>. 9 Implementation of Training In order to ensure randomness in our pattern presentation during training, we present the network with all of the images from all of the sequences in a random order, removing the notion of an individual sequence.
Reference: [8] <author> W.E.L. Grimson, B.K.P. Horn and T. Poggio, </author> <title> "Progress in image understanding at MIT", </title> <booktitle> Proceedings of the DARPA Image Understanding Workshop, </booktitle> <year> 1992. </year> <month> 28 </month>
Reference-contexts: The first two phases of learning are optional, but the last phase of learning is mandatory. We will explain the algorithm for each phase of learning and how it improves the network's performance. The training of the receptive field centers is done using task-dependent clustering techniques <ref> [8, 18] </ref>. An input pattern to the network corresponds to an N -component vector in the N -dimensional input space, and the receptive fields in the hidden layer act as N - dimensional Gaussian response regions in the input space.
Reference: [9] <author> J. Hertz, A. Krogh and R.G. Palmer, </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Past information is incorporated into the input vector by using feedback from the previous state of the input vector multiplied by a decay constant. Input units that use self feedback are called "context units" <ref> [9] </ref>.
Reference: [10] <author> J.A. Howell, C.W. Barnes, S.K. Brown, G.W. Flake, R.D. Jones, Y.C. Lee, S. Qian and R.M. Wright, </author> <title> "Control of a negative-ion accelerator source using neural networks", </title> <booktitle> Proceedings of the International Conference on Accelerator and Large Experimental Physics Control Systems, </booktitle> <address> Vancouver, BC, Canada, </address> <year> 1989. </year>
Reference-contexts: modify equations (5) and (6) to i (~x) = j exp (fi j (~x ~a j ) T (~x ~a j )) 14 i (~x) = j These enhancements to the original RBFN improve the interpolation ability of the network and reduce the amount of training necessary for accurate learning <ref> [10, 12-14] </ref>. In the RBFN there are three phases of learning. Learning can be performed on: 1) the center locations of the receptive fields in input space, 2) the widths of the receptive fields, and 3) the layer of weights between the hidden layer and the output layer.
Reference: [11] <author> R.D. Jones, C.W. Barnes, G.W. Flake, Y.C. Lee, P.S. Lewis, M.K. O'Rourke and S. Qian, </author> <title> "Prediction and control of chaotic processes using nonlinear adaptive networks", Proceedings for Nonlinear and Chaotic Processes in Plasmas, Fluids, and Solids, </title> <address> Ed-monton, Alberta, Canada, </address> <year> 1990. </year>
Reference-contexts: Several improved versions of the simple RBFN have been made; one of these is called the Connectionist Normalized Linear Spline Network (CNLS) <ref> [11] </ref>. The CNLS differs from the simple RBFN in two ways: 1. In the CNLS network a linear term has been added to the activation function for an output unit. 2. The responses of the receptive fields have been normalized.
Reference: [12] <author> R.D. Jones, </author> <title> "Nonlinear adaptive networks: A little theory, a few applications", </title> <booktitle> Proceedings of the First Los Alamos Workshop on Cognitive Modeling in System Control: Theoretical Foundations and Prospects for Applications, </booktitle> <address> Santa Fe, NM, </address> <year> 1990. </year>
Reference: [13] <author> R.D. Jones, Y.C. Lee, C.W. Barnes, G.W. Flake, K. Lee, P.S. Lewis and S. Qian, </author> <title> "Function approximation and time series prediction with neural networks", </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> San Diego, CA, </address> <year> 1990. </year>
Reference: [14] <author> Y.C. Lee, </author> <title> "Neural networks with memory for intelligent computations", </title> <booktitle> Proceedings of the 13th Conference on the Numerical Simulation of Plasmas, </booktitle> <address> Santa Fe, NM, </address> <year> 1989. </year>
Reference: [15] <author> H. Li, P. Roivainen, and R. Forcheimer, </author> <title> "3-D motion estimation in model-based facial image coding", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 15, </volume> <pages> 545-555, </pages> <year> 1993. </year>
Reference-contexts: We focus our attention on the appearance of the face from its near frontal image projection, without considering the underlying anatomic and musculature models and actions. In doing so we depart from the current trend in which facial expression analysis is based on recovering muscle actions <ref> [15, 16, 21] </ref>. Before proceeding, we introduce some terminology. Face region motion refers to the changes in images of facial features caused by facial actions corresponding to physical feature deformations on the 3-D surface of the face. <p> It remains to be determined whether the 3 computation of muscle contractions would be useful for facial expression recognition (the authors did not explore this aspect in [21]). Li et al. <ref> [15] </ref> proposed an approach that focuses on analyzing facial images for the purpose of resynthesizing these images. Their approach does not attempt to classify or reason about facial expressions and actions, although some aspects of their work might potentially achieve that. <p> The image sequence is densely sampled in time. * The subjects wear no eyeglasses. We chose not to model or analyze facial muscle actions, setting our work apart from <ref> [15, 16, 21] </ref>, as well as not to use models for muscle actions [6]. Instead, we focus on the motions associated with the edges of the mouth, eyes, and eyebrows.
Reference: [16] <author> K. Mase, </author> <title> "Recognition of facial expression from optical flow", </title> <journal> IEICE Transactions, </journal> <volume> Vol. E 74, </volume> <pages> 3474-3483, </pages> <year> 1991. </year>
Reference-contexts: We focus our attention on the appearance of the face from its near frontal image projection, without considering the underlying anatomic and musculature models and actions. In doing so we depart from the current trend in which facial expression analysis is based on recovering muscle actions <ref> [15, 16, 21] </ref>. Before proceeding, we introduce some terminology. Face region motion refers to the changes in images of facial features caused by facial actions corresponding to physical feature deformations on the 3-D surface of the face. <p> Their approach lays a grid over the face and warps it based on the gradient magnitude using a physical model. The amount of warping is represented in a multi-variate vector that is compared to learned vectors of four facial expressions (happiness, sadness, anger, and surprise). Mase <ref> [16] </ref> used optical flow computation for recognizing and analyzing facial expressions in both a top-down and bottom-up approach. In both cases, the focus was on computing the motion of facial muscles rather than of facial features. Four facial expressions were studied: surprise, anger, happiness, and disgust. <p> The image sequence is densely sampled in time. * The subjects wear no eyeglasses. We chose not to model or analyze facial muscle actions, setting our work apart from <ref> [15, 16, 21] </ref>, as well as not to use models for muscle actions [6]. Instead, we focus on the motions associated with the edges of the mouth, eyes, and eyebrows.
Reference: [17] <author> K. Matsuno, C. Lee, and S. Tsuji, </author> <title> "Recognition of human facial expressions without feature extraction", </title> <booktitle> Proceedings of the European Conference on Computer Vision, </booktitle> <pages> 513-520, </pages> <year> 1994. </year>
Reference-contexts: This approach could replace the expert rules developed in [23], and may allow the development of person-specific 2 learning capabilities. Such systems could be used in applications in which interactions are limited to a specific individual. Matsuno et al. <ref> [17] </ref> proposed an approach for recognizing facial expressions from static images based on a pre-computed parameterization of facial expressions. Their approach lays a grid over the face and warps it based on the gradient magnitude using a physical model.
Reference: [18] <author> J. Moody and C. Darken, </author> <title> "Learning with localized receptive fields", </title> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <editor> eds. Touretzky, Hinton, and Sejnowski, </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: This relates to the across-fiber-pattern theory used to describe sensory coding in humans and animals [7]. Goldstein remarks "This theory states that different qualities are signaled to the brain by the pattern of activity across a large number of neurons." The simple RBFN of Moody and Darken <ref> [18] </ref> consists of an input layer, a hidden layer, and an output layer (see clamped to the input vector. The hidden layer is composed of units which are driven by radial basis activation functions; we will refer to these units as receptive fields. <p> The activation function for an output unit is i (~x) = j where i (~x) is the activation level for the i th output unit with input vector ~x, and f ij is the weight of the link from the j th receptive field to the i th output unit <ref> [18] </ref>. Several improved versions of the simple RBFN have been made; one of these is called the Connectionist Normalized Linear Spline Network (CNLS) [11]. The CNLS differs from the simple RBFN in two ways: 1. <p> The first two phases of learning are optional, but the last phase of learning is mandatory. We will explain the algorithm for each phase of learning and how it improves the network's performance. The training of the receptive field centers is done using task-dependent clustering techniques <ref> [8, 18] </ref>. An input pattern to the network corresponds to an N -component vector in the N -dimensional input space, and the receptive fields in the hidden layer act as N - dimensional Gaussian response regions in the input space. <p> 15 the fi's: 1 X 2 X i (~a j )(~a i ~a j ) 2 fi i 5 (10) The adjustment of receptive field widths using this technique is only dependent on the locations of receptive field centers and widths, and does not depend directly on input training vectors <ref> [18] </ref>. Also note that this approach considers the widths along each dimension to be equal, resulting in a Gaussian hyper-sphere response from each receptive field. The training of the receptive field widths optimizes the specialization of receptive fields.
Reference: [19] <author> D.A. Pomerleau, </author> <title> "Neural Network Perception for Mobile Robot Guidance", </title> <type> Ph.D. thesis, </type> <institution> Carnegie Mellon University, Department of Computer Science, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Each output unit is associated with a stage of the emotion sequence. The activation of an intermediate output unit in this representation corresponds to the network's confidence that the emotion of the current subject in the sequence is in the stage corresponding to the particular output unit. Pomerleau <ref> [19] </ref> found that when there exists a proximal relation between output units, the supervised learning unit activations should reflect this relation. In our application, an output unit represents a stage of an emotion. <p> It also provides us with a relative means of comparison between networks to determine which network is the most confident. We made use of a technique called Output Appearance Reliability Estimation (OARE) to provide this confidence measure <ref> [19] </ref>. This technique of determining network reliability is based on comparing the shape of the network generated output distribution with the ideal shape of the training vector distribution; the farther the actual distribution is from the ideal distribution the less reliable the network is.
Reference: [20] <author> M. Rosenblum and L. Davis, </author> <title> "The Use of a Radial Basis Function Network for Visual Autonomous Road Following", </title> <type> Technical Report CAR-TR-666, </type> <institution> Center for Automation Research, University of Maryland, College Park, MD, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: We chose to use as a building block a modified version of the radial basis function network (RBFN), based on its ability to represent prototypical visual templates from the application domain <ref> [20] </ref>. In the rest of this section we discuss the basic RBFN architecture, and the enhancements we made to the basic RBFN architecture to handle the temporal relations associated with this problem. 8.1 The General RBFN Architecture Neural networks are used to approximate multivariate nonlinear functions using sparsely sampled data.
Reference: [21] <author> D. Terzopoulos and K. Waters, </author> <title> "Analysis and synthesis of facial image sequences using physical and anatomical models", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 15, </volume> <pages> 569-579, </pages> <year> 1993. </year> <month> 29 </month>
Reference-contexts: We focus our attention on the appearance of the face from its near frontal image projection, without considering the underlying anatomic and musculature models and actions. In doing so we depart from the current trend in which facial expression analysis is based on recovering muscle actions <ref> [15, 16, 21] </ref>. Before proceeding, we introduce some terminology. Face region motion refers to the changes in images of facial features caused by facial actions corresponding to physical feature deformations on the 3-D surface of the face. <p> Our goal is to develop computational methods that use such motions as cues for action recovery. Recent research in computer vision The problem of recognizing facial expressions has recently attracted attention in the computer vision community <ref> [15-17, 21, 23] </ref>. <p> Furthermore, the optical flow was treated on a per-frame basis without considering the time-sequence of frames. The experiments considered the expressions of just one face and the results were compared with the performance of human subjects that were asked to classify the displayed emotions. Terzopoulos and Waters <ref> [21] </ref> proposed an approach to synthesis and analysis of facial expressions based on physical modeling of the muscles of the face. <p> It remains to be determined whether the 3 computation of muscle contractions would be useful for facial expression recognition (the authors did not explore this aspect in <ref> [21] </ref>). Li et al. [15] proposed an approach that focuses on analyzing facial images for the purpose of resynthesizing these images. Their approach does not attempt to classify or reason about facial expressions and actions, although some aspects of their work might potentially achieve that. <p> The image sequence is densely sampled in time. * The subjects wear no eyeglasses. We chose not to model or analyze facial muscle actions, setting our work apart from <ref> [15, 16, 21] </ref>, as well as not to use models for muscle actions [6]. Instead, we focus on the motions associated with the edges of the mouth, eyes, and eyebrows. <p> Their approach does not quantitatively recover the facial features but it designates rectangles that enclose the features. The only dynamic tracking algorithm for facial features we are aware of is that of Ter-zopoulos and Waters <ref> [21] </ref>. Eleven contours were manually located in the first image of the sequence and then tracked using deformable contour models that are attracted by the local minima of the intensity image.
Reference: [22] <author> Y. Yacoob and L.S. Davis, </author> <title> "Labeling of human face components from range data", </title> <booktitle> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> 592-593, </pages> <year> 1993. </year>
Reference-contexts: We assume that, for each feature, we can initially compute a rectangular region that encloses it. Such an algorithm has been recently proposed for range data by Yacoob and Davis <ref> [22] </ref> and a similar algorithm could be developed for intensity images (or stereo images), a problem we are currently working on. Our algorithm tracks these regions through the remainder of the sequence. <p> This approach requires an accurate initial estimate of the location of the features, and might not be easily applied to facial features such as the eye when it is being closed or opened, since the deformations of the template could become quite complex. Yacoob and Davis <ref> [22] </ref> present an approach for qualitatively localizing and labeling natural facial components from range data. They used a multi-stage diffusion process that classifies range points into relative convexities and concavities.
Reference: [23] <editor> Y. Yacoob and L.S. Davis, </editor> <booktitle> "Computing spatio-temporal representations of human faces" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <year> 1994. </year>
Reference-contexts: Our goal is to develop computational methods that use such motions as cues for action recovery. Recent research in computer vision The problem of recognizing facial expressions has recently attracted attention in the computer vision community <ref> [15-17, 21, 23] </ref>. <p> With the increase of interest in human-computer interaction and related applications, the need arises to analyze human messages that are communicated through body part gestures and facial expressions. 1 Yacoob and Davis <ref> [23] </ref> proposed an approach to analyzing and representing the dynamics of facial expressions from image sequences. <p> Blinking detection success rate was 65%. The work reported here explores the use of a connectionist learning architecture for identifying the motion patterns characteristic of facial expressions. This approach could replace the expert rules developed in <ref> [23] </ref>, and may allow the development of person-specific 2 learning capabilities. Such systems could be used in applications in which interactions are limited to a specific individual. Matsuno et al. [17] proposed an approach for recognizing facial expressions from static images based on a pre-computed parameterization of facial expressions. <p> Instead, we focus on the motions associated with the edges of the mouth, eyes, and eyebrows. These edges allow us to refer to the face features using natural linguistic terminology (for a detailed list of considerations see [24]). is similar to <ref> [23] </ref> in the tracking and optical flow computation but differs in the analysis and interpretation of motion patterns. The system is composed of the following components: 4 * Optical flow computation: Optical flow is computed at the points with high gradient at each frame.
Reference: [24] <author> Y. Yacoob and L.S. Davis, </author> <title> "Recognizing human facial expression", </title> <type> Technical Report CAR-TR-706, </type> <institution> Center for Automation Research, University of Maryland, College Park, MD, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Instead, we focus on the motions associated with the edges of the mouth, eyes, and eyebrows. These edges allow us to refer to the face features using natural linguistic terminology (for a detailed list of considerations see <ref> [24] </ref>). is similar to [23] in the tracking and optical flow computation but differs in the analysis and interpretation of motion patterns. The system is composed of the following components: 4 * Optical flow computation: Optical flow is computed at the points with high gradient at each frame.
Reference: [25] <author> A.W. Young and H.D. Ellis (eds.), </author> <title> Handbook of Research on Face Processing, </title> <publisher> Elsevier Science Publishers, </publisher> <year> 1989. </year>
Reference-contexts: The study of emotion in human facial expressions was pioneered by Darwin's work [4] and has been extensively studied in psychology during the last thirty years <ref> [25] </ref>. This research has indicated that at least six emotions are universally associated with distinct facial expressions. Several other emotions, and many combinations of emotions, have been studied but remain unconfirmed as universally distinguishable. <p> The six principle emotions are: happiness, sadness, surprise, fear, anger, and disgust (see Figure 1), and these are the emotions we focus on in this paper. Most psychological research on facial expression has been conducted on "mug-shot" pictures that capture the subject's expression at its peak <ref> [25] </ref>. These pictures allow subjects to detect the presence of static cues (such as wrinkles) as well as the position and shape of the facial features. Few studies have directly investigated the influence of the motion and deformation of facial features on the interpretation of facial expressions.
Reference: [26] <author> A.L. Yuille, D.S. Cohen, and P.W. Hallinan, </author> <title> "Feature extraction from faces using de-formable templates", </title> <booktitle> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> 104-109, </pages> <year> 1989. </year> <month> 30 </month>
Reference-contexts: displacement is arg max p (dje w ) = arg max p (e w jd)p (d) (4) where p (e w jd) = n j=1 p (e j jd) and e j is the error term at pixel j in frame i. 8 5 Tracking face regions Yuille et al. <ref> [26] </ref> used deformable templates and an energy minimization approach to localize and approximate the eyes and mouth in intensity face images.
References-found: 26

