URL: http://www.almaden.ibm.com/cs/quest/papers/sprint_smp.ps
Refering-URL: http://www.almaden.ibm.com/cs/quest/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Parallel Classification for Data Mining on Shared-Memory Multiprocessors  
Author: Mohammed J. Zaki Ching-Tien Ho Rakesh Agrawal 
Note: 5.98 for the total time on an 8-processor SMP.  
Address: San Jose, CA 95120  
Affiliation: IBM Almaden Research Center,  
Abstract: We present parallel algorithms for building decision-tree classifiers on shared-memory multiprocessor (SMP) systems. The proposed algorithms span the gamut of data and task parallelism. The data parallelism is based on attribute scheduling among processors. This basic scheme is extended with task pipelining and dynamic load balancing to yield faster implementations. The task parallel approach uses dynamic subtree partitioning among processors. We evaluate the performance of these algorithms on two machine configurations: one in which data is too large to fit in memory and must be paged from a local disk as needed and the other in which memory is sufficiently large to cache the whole data. This performance evaluation shows that the construction of a decision-tree classifier can be effectively parallelized on an SMP machine with good speedup. For the local disk configuration, the speedup ranged from 2.97 to 3.86 for the build phase and from 2.20 to 3.67 for the total time on a 4-processor SMP. For the large memory configuration, the range of speedup was from 5.36 to 6.67 for the build phase and from 3.07 to 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rakesh Agrawal, Sakti Ghosh, Tomasz Imielinski, Bala Iyer, and Arun Swami. </author> <title> An interval classifier for database mining applications. </title> <booktitle> In Proc. of the VLDB Conference, </booktitle> <pages> pages 560-573, </pages> <address> Vancouver, British Columbia, Canada, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Decision trees can be constructed relatively fast compared to other methods and they are easy to interpret [17]. Moreover, trees can be converted into SQL statements that can be used to access databases efficiently <ref> [1] </ref>. Finally, decision-tree classifiers obtain similar, and often better, accuracy when compared to other classification methods [15]. Prior to interest in classification for database-centric data mining, it was tacitly assumed that the training sets could fit in memory. <p> Datasets An often used classification benchmark is STATLOG [15]. Its largest dataset contains about 57,000 records. In our performance study we are interested in evaluating the SMP algorithms on large out-of-core data. We therefore use the synthetic benchmark proposed in <ref> [1] </ref> and used in several past studies. Example tuples in this benchmark have both continuous and categorical attributes. The benchmark gives several classification functions of varying complexity to generate synthetic databases. We present results for two of these functions, which are at the two ends of the complexity spectrum.
Reference: [2] <author> Rakesh Agrawal, Tomasz Imielinski, and Arun Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(6) </volume> <pages> 914-925, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Classification is recognized to be a primary data mining task <ref> [2] </ref> [9]. The input to a classification system consists of a set of example tuples, called a training set, where each tuple consists of several attributes. Attributes can be continuous, coming from an ordered domain, or categorical, coming from an unordered domain.
Reference: [3] <author> Dina Bitton, David DeWitt, David K. Hsiao, and Jaishankar Menon. </author> <title> A taxonomy of parallel sorting. </title> <journal> ACM Computing Surveys, </journal> <volume> 16(3) </volume> <pages> 287-318, </pages> <month> September </month> <year> 1984. </year>
Reference-contexts: For simple datasets such as F2, it can be significant, whereas it is negligible for complex datasets such as F7. We have not focussed on parallelizing these phases, concentrating instead on the more challenging build phase. There is much existing research in parallelizing sort including on SMP machines <ref> [3] </ref>. The creation of attribute lists can be speeded up by essentially using multiple input streams and merging this phase with the sort phase. In our implementation, the data scan to create attribute lists is sequential, although we write attribute lists in parallel.
Reference: [4] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: The algorithms required processor communication to evaluate any test condition for a decision-tree node, limiting the number of possible tests the algorithm could consider. The Darwin toolkit from Thinking Machines, a distributed-memory parallel machine, also contained a parallel implementation of the CART decision-tree classifier <ref> [4] </ref>. The details of this parallelization are not available in published literature. The SPRINT algorithm mentioned earlier was also parallelized on the IBM SP2 parallel distributed-memory machine and was shown to achieve good scaleup and speedup [18]. Parallel distributed-memory machines are essential for scalable massive parallelism. <p> Tid Age Car Type Risk 0 23 family High 1 17 sports High 2 43 sports High 3 68 family Low 4 32 truck Low 5 20 family High (a) Training Set (b) Decision Tree A decision-tree classifier is usually built in two phases <ref> [4] </ref> [17]: a growth phase and a prune phase. The tree is grown using a divide and conquer approach. Given a set of training examples S as input, there are two cases to consider. If all examples in S belong to a single class, then S becomes a leaf. <p> On the other hand, if S contains a mixture of examples from different classes, the input is partitioned into subsets that tend towards a single class. The partitioning is based on a test on an attribute. The splits are usually binary as they lead to more accurate trees <ref> [4] </ref>. The above process is recursively applied using the partitioned datasets as inputs. The tree thus built can "overfit" the data. The goal of classification is to predict new unseen cases. <p> We now describe how the above two steps are handled in serial SPRINT [18]. It builds the tree in a breadth-first order and uses a one-time pre-sorting technique to reduce the cost of continuous attribute evaluation. In contrast, the older, well-known CART <ref> [4] </ref> and C4.5 [17] classifiers grow trees depth-first and repeatedly sort the data at every node. <p> SPRINT uses the gini index for this purpose. For a data set S containing examples from n classes, gini (S) is defined to be gini (S) = 1 P j where p j is the relative frequency of class j in S <ref> [4] </ref>. For continuous attributes, the candidate split points are mid-points between every two consecutive attribute values in the training data. <p> The accuracy and tree size characteristics of our SMP classifier are identical to SLIQ/SPRINT since they consider the same splits and use the same pruning algorithm. SLIQ's accuracy, execution time, and tree size have been compared with other classifiers such as CART <ref> [4] </ref> and C4 (a predecessor of C4.5 [17]). This performance evaluation, available in [14], shows that compared to other classifiers SLIQ achieves comparable or better classification accuracy, but produces small decision trees and has small execution times.
Reference: [5] <author> Jason Catlett. </author> <title> Megainduction: Machine Learning on Very Large Databases. </title> <type> PhD thesis, </type> <institution> University of Sydney, </institution> <year> 1991. </year>
Reference-contexts: Recent work has targeted the massive training sets usual in fl Current Affiliation: Department of Computer Science, University of Rochester, Rochester, NY 14627, zaki@cs.rochester.edu. 1 data mining. Developing classification models using larger training sets can enable the development of higher accuracy models. Various studies have confirmed this hypothesis <ref> [5] </ref> [6] [7]. Examples of recent classification systems that can handle disk-resident data include SLIQ [14] and its extension SPRINT [18]. A continuing trend in data management is the rapid and inexorable growth in the data that is being collected.
Reference: [6] <author> Philip K. Chan and Salvatore J. Stolfo. </author> <title> Experiments on multistrategy learning by meta-learning. </title> <booktitle> In Proc. Second Intl. Conference on Info. and Knowledge Mgmt., </booktitle> <pages> pages 314-323, </pages> <year> 1993. </year>
Reference-contexts: Recent work has targeted the massive training sets usual in fl Current Affiliation: Department of Computer Science, University of Rochester, Rochester, NY 14627, zaki@cs.rochester.edu. 1 data mining. Developing classification models using larger training sets can enable the development of higher accuracy models. Various studies have confirmed this hypothesis [5] <ref> [6] </ref> [7]. Examples of recent classification systems that can handle disk-resident data include SLIQ [14] and its extension SPRINT [18]. A continuing trend in data management is the rapid and inexorable growth in the data that is being collected.
Reference: [7] <author> Philip K. Chan and Salvatore J. Stolfo. </author> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Workshop on Multistrategy Learning, </booktitle> <pages> pages 150-165, </pages> <year> 1993. </year>
Reference-contexts: Developing classification models using larger training sets can enable the development of higher accuracy models. Various studies have confirmed this hypothesis [5] [6] <ref> [7] </ref>. Examples of recent classification systems that can handle disk-resident data include SLIQ [14] and its extension SPRINT [18]. A continuing trend in data management is the rapid and inexorable growth in the data that is being collected.
Reference: [8] <author> D. J. DeWitt, J. F. Naughton, and D. A. Schneider. </author> <title> Parallel sorting on a shared-nothing architecture using probabilistic splitting. </title> <booktitle> In Proc. of the 1st Int'l Conf. on Parallel and Distributed Information Systems, </booktitle> <pages> pages 280-291, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: First, the training examples are distributed equally among all the processors. Each processor then generates its own attribute list partitions in parallel by projecting each attribute from training examples it was assigned. Lists for continuous attributes are further globally sorted using a parallel sort <ref> [8] </ref> and repartitioned amongst processors into equal-sized contiguous sorted sections. Lists for categorical attributes are not sorted/repartitioned. Note that the different attribute lists belonging to a processor no longer correspond to the same set of tids.
Reference: [9] <editor> Usama M. Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth, and Ramasamy Uthurusamy, editors. </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Classification is recognized to be a primary data mining task [2] <ref> [9] </ref>. The input to a classification system consists of a set of example tuples, called a training set, where each tuple consists of several attributes. Attributes can be continuous, coming from an ordered domain, or categorical, coming from an unordered domain.
Reference: [10] <author> D. J. Fifield. </author> <title> Distributed tree construction from large data-sets. </title> <type> Bachelor's Honours Thesis, </type> <institution> Australian National University, </institution> <year> 1992. </year>
Reference-contexts: Past research on parallel classification has been focussed on distributed-memory (also called shared-nothing) parallel machines. In such a machine, each processor has private memory and local disks, and communicates with other processors only via passing messages. An early work in this area examined parallelizing the ID3 decision-tree classifier <ref> [10] </ref>. It assumed that the entire dataset could fit in memory and did not address issues such as disk I/O. The algorithms required processor communication to evaluate any test condition for a decision-tree node, limiting the number of possible tests the algorithm could consider.
Reference: [11] <institution> IBM Corp. </institution> <note> See http://www.rs6000.ibm.com/hardware/largescale/index.html, IBM RS/6000 SP System. </note>
Reference-contexts: Individual nodes of even parallel distributed-memory machines are increasingly being designed to be SMP nodes. For example, an IBM SP2 parallel system may consist of up to 64 high nodes, where each high node is an 8-way SMP system with PowerPC 604e processors <ref> [11] </ref>. A shared-memory system offers a single memory address space that all processors can access. Processors communicate through shared variables in memory and are capable of accessing any memory location. Synchronization is used to co-ordinate processes. Any processor can also access any disk attached to the system.
Reference: [12] <author> Bil Lewis and Daniel J. Berg. </author> <title> Threads Primer. </title> <publisher> Prentice Hall, </publisher> <address> New Jersey, </address> <year> 1996. </year>
Reference-contexts: Our parallel schemes will be described in terms of these steps. Our prototype implementation of these schemes uses the POSIX threads (pthread) standard <ref> [12] </ref>. A thread is a light weight process. It is a schedulable entity that has only those properties that are required to ensure its independent flow of control, including the stack, scheduling properties, set 7 of pending and blocking signals, and some thread-specific data.
Reference: [13] <author> E.P. Markatos and T.J. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: For thousands of attributes self-scheduling can generate too much unnecessary synchronization. The latter can be addressed by using guided self-scheduling [16] or its extensions, where a processor grabs a dynamically shrinking chunk of remaining attributes, thus minimizing the synchronization. Another possibility would be to use affinity scheduling <ref> [13] </ref>, where 8 Finding split points (E ) Since each attribute has its own set of four reusable attribute files, as long as no two processors work on the same attribute at the same time, there is no need for file access synchronization.
Reference: [14] <author> Manish Mehta, Rakesh Agrawal, and Jorma Rissanen. SLIQ: </author> <title> A fast scalable classifier for data mining. </title> <booktitle> In Proc. of the Fifth Int'l Conference on Extending Database Technology (EDBT), </booktitle> <address> Avignon, France, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: Classification has applications in diverse fields such as retail target marketing, customer retention, fraud detection, and medical diagnosis [15]. Amongst the several classification methods proposed over the years [20] [15], decision trees are particularly suited for data mining applications <ref> [14] </ref>. Decision trees can be constructed relatively fast compared to other methods and they are easy to interpret [17]. Moreover, trees can be converted into SQL statements that can be used to access databases efficiently [1]. <p> Developing classification models using larger training sets can enable the development of higher accuracy models. Various studies have confirmed this hypothesis [5] [6] [7]. Examples of recent classification systems that can handle disk-resident data include SLIQ <ref> [14] </ref> and its extension SPRINT [18]. A continuing trend in data management is the rapid and inexorable growth in the data that is being collected. The development of high-performance scalable data mining tools must necessarily rely on parallel computing techniques. <p> Previous studies from SLIQ suggest that usually less than 1% of the total time needed to build a classifier was spent in the prune phase <ref> [14] </ref>. The performance of the tree growth phase depends on two factors: 1) how to find split points that define node tests, and 2) having chosen a split point, how to partition the data. We now describe how the above two steps are handled in serial SPRINT [18]. <p> We will only discuss the tree growth phase due to its compute and data-intensive nature. Tree pruning is relatively inexpensive <ref> [14] </ref>, as it requires access to only the decision-tree grown in the training phase. 3.1 SMP Schemes While building a decision-tree, there are three main steps that must be performed for each node at each level of the tree: (i) Evaluate split points for each attribute (denoted as step E); (ii) <p> SLIQ's accuracy, execution time, and tree size have been compared with other classifiers such as CART [4] and C4 (a predecessor of C4.5 [17]). This performance evaluation, available in <ref> [14] </ref>, shows that compared to other classifiers SLIQ achieves comparable or better classification accuracy, but produces small decision trees and has small execution times.
Reference: [15] <author> D. Michie, D. J. Spiegelhalter, and C. C. Taylor. </author> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year> <month> 21 </month>
Reference-contexts: Classification has applications in diverse fields such as retail target marketing, customer retention, fraud detection, and medical diagnosis <ref> [15] </ref>. Amongst the several classification methods proposed over the years [20] [15], decision trees are particularly suited for data mining applications [14]. Decision trees can be constructed relatively fast compared to other methods and they are easy to interpret [17]. <p> Classification has applications in diverse fields such as retail target marketing, customer retention, fraud detection, and medical diagnosis <ref> [15] </ref>. Amongst the several classification methods proposed over the years [20] [15], decision trees are particularly suited for data mining applications [14]. Decision trees can be constructed relatively fast compared to other methods and they are easy to interpret [17]. Moreover, trees can be converted into SQL statements that can be used to access databases efficiently [1]. <p> Moreover, trees can be converted into SQL statements that can be used to access databases efficiently [1]. Finally, decision-tree classifiers obtain similar, and often better, accuracy when compared to other classification methods <ref> [15] </ref>. Prior to interest in classification for database-centric data mining, it was tacitly assumed that the training sets could fit in memory. Recent work has targeted the massive training sets usual in fl Current Affiliation: Department of Computer Science, University of Rochester, Rochester, NY 14627, zaki@cs.rochester.edu. 1 data mining. <p> Machine Number Main Disk Space Access Operating Name Processors Memory Available Type System Machine A 4 128 MB 300 MB local disk AIX 4.1.4 Machine B 8 1 GB 2 GB main-memory (cached) AIX 4.1.5 Table 1: Machine configurations. Datasets An often used classification benchmark is STATLOG <ref> [15] </ref>. Its largest dataset contains about 57,000 records. In our performance study we are interested in evaluating the SMP algorithms on large out-of-core data. We therefore use the synthetic benchmark proposed in [1] and used in several past studies. Example tuples in this benchmark have both continuous and categorical attributes.
Reference: [16] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self-scheduling: a practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: For thousands of attributes self-scheduling can generate too much unnecessary synchronization. The latter can be addressed by using guided self-scheduling <ref> [16] </ref> or its extensions, where a processor grabs a dynamically shrinking chunk of remaining attributes, thus minimizing the synchronization.
Reference: [17] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufman, </publisher> <year> 1993. </year>
Reference-contexts: Amongst the several classification methods proposed over the years [20] [15], decision trees are particularly suited for data mining applications [14]. Decision trees can be constructed relatively fast compared to other methods and they are easy to interpret <ref> [17] </ref>. Moreover, trees can be converted into SQL statements that can be used to access databases efficiently [1]. Finally, decision-tree classifiers obtain similar, and often better, accuracy when compared to other classification methods [15]. <p> Tid Age Car Type Risk 0 23 family High 1 17 sports High 2 43 sports High 3 68 family Low 4 32 truck Low 5 20 family High (a) Training Set (b) Decision Tree A decision-tree classifier is usually built in two phases [4] <ref> [17] </ref>: a growth phase and a prune phase. The tree is grown using a divide and conquer approach. Given a set of training examples S as input, there are two cases to consider. If all examples in S belong to a single class, then S becomes a leaf. <p> We now describe how the above two steps are handled in serial SPRINT [18]. It builds the tree in a breadth-first order and uses a one-time pre-sorting technique to reduce the cost of continuous attribute evaluation. In contrast, the older, well-known CART [4] and C4.5 <ref> [17] </ref> classifiers grow trees depth-first and repeatedly sort the data at every node. Our SMP classifier uses the same basic computations as serial SPRINT, but the details of how computations are orchestrated are different. 2.1 Attribute Lists SPRINT initially creates a disk-based attribute list for each attribute in the data. <p> The accuracy and tree size characteristics of our SMP classifier are identical to SLIQ/SPRINT since they consider the same splits and use the same pruning algorithm. SLIQ's accuracy, execution time, and tree size have been compared with other classifiers such as CART [4] and C4 (a predecessor of C4.5 <ref> [17] </ref>). This performance evaluation, available in [14], shows that compared to other classifiers SLIQ achieves comparable or better classification accuracy, but produces small decision trees and has small execution times.
Reference: [18] <author> John Shafer, Rakesh Agrawal, and Manish Mehta. SPRINT: </author> <title> A scalable parallel classifier for data mining. </title> <booktitle> In Proc. of the 22nd Int'l Conference on Very Large Databases, </booktitle> <address> Bombay, India, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: Developing classification models using larger training sets can enable the development of higher accuracy models. Various studies have confirmed this hypothesis [5] [6] [7]. Examples of recent classification systems that can handle disk-resident data include SLIQ [14] and its extension SPRINT <ref> [18] </ref>. A continuing trend in data management is the rapid and inexorable growth in the data that is being collected. The development of high-performance scalable data mining tools must necessarily rely on parallel computing techniques. Past research on parallel classification has been focussed on distributed-memory (also called shared-nothing) parallel machines. <p> The details of this parallelization are not available in published literature. The SPRINT algorithm mentioned earlier was also parallelized on the IBM SP2 parallel distributed-memory machine and was shown to achieve good scaleup and speedup <ref> [18] </ref>. Parallel distributed-memory machines are essential for scalable massive parallelism. However, shared-memory multiprocessor systems (SMPs), often called shared-everything systems, are also capable of delivering high performance for low to medium degree of parallelism at an economically attractive price. <p> Finally, the SMP architecture offers new challenges and trade-offs that are worth investigating in their own right. The rest of the paper is organized as follows. In Section 2 we review how a decision-tree classifier, specifically SPRINT, is built on a uniprocessor machine. This section is adapted from <ref> [18] </ref>. Section 3 describes our new SMP algorithms based on various data and task parallelization schemes. We give experimental results in Section 4 and conclude with a summary in Section 5. 2 Serial Classification A decision tree contains tree-structured nodes. <p> The performance of the tree growth phase depends on two factors: 1) how to find split points that define node tests, and 2) having chosen a split point, how to partition the data. We now describe how the above two steps are handled in serial SPRINT <ref> [18] </ref>. It builds the tree in a breadth-first order and uses a one-time pre-sorting technique to reduce the cost of continuous attribute evaluation. In contrast, the older, well-known CART [4] and C4.5 [17] classifiers grow trees depth-first and repeatedly sort the data at every node. <p> This scheme is illustrated in Figure 3. In fact, it is possible to combine the records of different attribute lists into one physical file, thus requiring a total of 4 physical files. 2.4 Parallel SPRINT on a Distributed-Memory Machine The parallel implementation of SPRINT on IBM SP2 distributed-memory machine <ref> [18] </ref> is based on what can be called record data parallelism. Briefly, each processor is responsible for processing roughly 1=P fraction of each attribute list. All processors synchronously construct one global decision tree, one node at a time. First, the training examples are distributed equally among all the processors.
Reference: [19] <author> P. Tang and P.-C. Yew. </author> <title> Processor self-scheduling for multiple nested parallel loops. </title> <booktitle> In International Conference On Parallel Processing, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: Attribute scheduling Attributes are scheduled dynamically by using an attribute counter and locking. A processor acquires the lock, grabs an attribute, increments the counter, and releases the lock. This method achieves the same effect as self-scheduling <ref> [19] </ref>, i.e., there is lock synchronization per attribute. 1 1 For typical classification problems with up to a few hundred attributes, this approach works fine. For thousands of attributes self-scheduling can generate too much unnecessary synchronization.
Reference: [20] <author> Sholom M. Weiss and Casimir A. </author> <title> Kulikowski. Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems. </title> <publisher> Morgan Kaufman, </publisher> <year> 1991. </year> <month> 22 </month>
Reference-contexts: Classification has applications in diverse fields such as retail target marketing, customer retention, fraud detection, and medical diagnosis [15]. Amongst the several classification methods proposed over the years <ref> [20] </ref> [15], decision trees are particularly suited for data mining applications [14]. Decision trees can be constructed relatively fast compared to other methods and they are easy to interpret [17]. Moreover, trees can be converted into SQL statements that can be used to access databases efficiently [1].
References-found: 20

