URL: ftp://ftp.cs.kuleuven.ac.be/pub/logic-prgm/ilp/dlab/dlab_ismis.ps.gz
Refering-URL: http://www.cs.kuleuven.ac.be/~wimv/ICL/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email Luc.Dehaspe,Luc.DeRaedt@cs.kuleuven.ac.be  
Phone: fax 32 16 32 79 96; telephone 32 16 32 75 50  
Title: Dlab: A Declarative Language Bias Formalism  
Author: Luc Dehaspe and Luc De Raedt 
Address: Celestijnenlaan 200A, B-3001 Heverlee, Belgium  
Affiliation: Katholieke Universiteit Leuven, Department of Computer Science,  
Abstract: We describe the principles and functionalities of Dlab (Declarative LAnguage Bias). Dlab can be used in inductive learning systems to define syntactically and traverse efficiently finite subspaces of first order clausal logic, be it a set of propositional formulae, association rules, Horn clauses, or full clauses. A Prolog implementation of Dlab is available by ftp access. Keywords: declarative language bias, concept learning, knowledge dis covery
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> H. Ade, L. De Raedt, and M. Bruynooghe. </author> <title> Declarative Bias for Specific-to-General ILP Systems. </title> <journal> Machine Learning, </journal> <volume> 20(1/2):119 - 154, </volume> <year> 1995. </year>
Reference-contexts: We conclude with a brief situation of Dlab against some alternative declarative 6 language bias formalisms. Closest to Dlab are the clausemodels proposed in <ref> [1] </ref>. Generally speaking, clausemodels are special cases of Dlab templates in which nesting and the choice of M in and M ax is restricted. As discussed in [1], schemata [10] and predicate sets [2] as used in MOBAL and the FILP system respectively, are special cases of clausemodels, and thus indirectly <p> Closest to Dlab are the clausemodels proposed in <ref> [1] </ref>. Generally speaking, clausemodels are special cases of Dlab templates in which nesting and the choice of M in and M ax is restricted. As discussed in [1], schemata [10] and predicate sets [2] as used in MOBAL and the FILP system respectively, are special cases of clausemodels, and thus indirectly of Dlab templates.
Reference: 2. <author> F. Bergadano and D. Gunetti. </author> <title> An interactive system to learn functional logic programs. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1044-1049. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Closest to Dlab are the clausemodels proposed in [1]. Generally speaking, clausemodels are special cases of Dlab templates in which nesting and the choice of M in and M ax is restricted. As discussed in [1], schemata [10] and predicate sets <ref> [2] </ref> as used in MOBAL and the FILP system respectively, are special cases of clausemodels, and thus indirectly of Dlab templates. An antecedent description grammar, as used by Cohen in GRENDEL [4], is in essence a definite clause grammar that generates the antecedents of clauses in the hypothesis space.
Reference: 3. <author> W.F. Clocksin and C.S. Mellish. </author> <title> Programming in Prolog. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: This idea can be elegantly formalised and implemented using the Definite Clause Grammar (DCG) notation, which is an extension of Prolog (cf. <ref> [3, 19] </ref>) 3 . Definition 3 (Dlab semantics).
Reference: 4. <author> W.W. Cohen. </author> <title> Grammatically biased learning: learning logic programs using an explicit antecedent description language. </title> <journal> Artificial Intelligence, </journal> <volume> 68 </volume> <pages> 303-366, </pages> <year> 1994. </year>
Reference-contexts: As discussed in [1], schemata [10] and predicate sets [2] as used in MOBAL and the FILP system respectively, are special cases of clausemodels, and thus indirectly of Dlab templates. An antecedent description grammar, as used by Cohen in GRENDEL <ref> [4] </ref>, is in essence a definite clause grammar that generates the antecedents of clauses in the hypothesis space. In general a conversion of antecedent description grammars to Dlab is not always possible 7 .
Reference: 5. <author> L. De Raedt. </author> <title> Interactive Theory Revision: an Inductive Logic Programming Approach. </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Timothy Chow for their elegant solutions of the dlab size combinatorics problem, and Hendrik Blockeel for his helpful comments on this paper. 6 More procedural approaches to syntactic bias specifications use parameters such as the maximal variable depth or term level to control the complexity of the concept language, cf. <ref> [5, 16] </ref>. Parametrized languages should be considered complementary to Dlab, in the sense that the same parameters trivially define (a series of) Dlab grammars. 7 A clear case where this conversion is impossible occurs when the antecedent descrip tion grammar generates an infinite language.
Reference: 6. <author> L. Dehaspe and L. De Raedt. DLAB: </author> <title> a declarative language bias for concept learning and knowledge discovery engines. </title> <type> Technical Report CW-214, </type> <institution> Department of Computer Science, Katholieke Universiteit Leuven, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: When using the Dlab path we can achieve optimality for instance if we never expand 0's to the left of already expanded 0's. As fully described in <ref> [6] </ref>, the Dlab refinement operator sketched above can be implemented in seventeen DCG rules. A more sophisticated Prolog implementation (as well as [6]) is available by anonymous ftp access to ftp:cs:kuleuven:ac:be. <p> When using the Dlab path we can achieve optimality for instance if we never expand 0's to the left of already expanded 0's. As fully described in <ref> [6] </ref>, the Dlab refinement operator sketched above can be implemented in seventeen DCG rules. A more sophisticated Prolog implementation (as well as [6]) is available by anonymous ftp access to ftp:cs:kuleuven:ac:be. The relevant directory is pub=logic prgm=ilp=dlab. 3 Dlab Extended: Dlab In an extended version Dlab mainly two features have been added to improve readability of more complex grammars: second order variables, and subsets on the term level.
Reference: 7. <author> B. Dolsak, I. Bratko, and A. Jezernik. </author> <title> Finite element mesh design: An engineering domain for ilp application. </title> <editor> In S. Wrobel, editor, </editor> <booktitle> Proceedings of the 4th International Workshop on Inductive Logic Programming, volume 237 of GMD-Studien, </booktitle> <address> Sankt Augustin, Germany, </address> <year> 1994. </year> <institution> Gesellschaft fur Mathematik und Datenverar-beitung MBH. </institution>
Reference-contexts: Encoding taxonomies: Dlab grammar G9 We now step through the development of a Dlab grammar for the finite element mesh design application frequently used to evaluate relational learning algorithms (see e. g. <ref> [7, 11] </ref>). The training examples in this domain are "hand-constructed" approximations of physical structures. Such an approximation is a set of finite elements called a mesh model, divided into a collection of edges.
Reference: 8. <author> M. Genesereth and N. Nilsson. </author> <booktitle> Logical foundations of artificial intelligence. </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: 1 Introduction The notion bias, generally circumscribed as "a tendency to show prejudice against one group and favouritism towards another" <ref> (Collins Cobuild, 1987) </ref>, has been adapted to the field of computational inductive reasoning to become a generic term for "any basis for choosing one generalization over another, other than strict consistency with the instances" (Mitchell [14]). We borrow a more fine-tuned definition of inductive bias from Utgoff [20]. <p> We first give a recursive syntactic definition of the Dlab formalism. 2 We assume familiarity with first order logic (see <ref> [12, 8] </ref> for an introduction), but briefly review the basic relevant concepts. A first order alphabet is a set of predicate symbols, constant symbols and functor symbols.
Reference: 9. <editor> D. Gordon and M. desJardins. </editor> <title> Evaluation and selection of biases in machine learning. </title> <journal> Machine Learning, </journal> 20(1/2):5-22, 1995. 
Reference-contexts: As the factors that influence hypothesis selection were further charted the idea grew to take them out of the hands of programmers, promote them to parameters in learning systems, and thus make way for the 1 An alternative framework <ref> [9] </ref> divides bias into representational (cf. items 1 and 2) and procedural (cf. items 2 and 3) components. specification and modification of previously unexploited a priori knowledge. For this type of explicit input parameters, Russell and Grosof [18] introduced the concept declarative bias.
Reference: 10. <author> J-U. Kietz and S. Wrobel. </author> <title> Controlling the complexity of learning in logic through syntactic and task-oriented models. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive logic programming, </booktitle> <pages> pages 335-359. </pages> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Closest to Dlab are the clausemodels proposed in [1]. Generally speaking, clausemodels are special cases of Dlab templates in which nesting and the choice of M in and M ax is restricted. As discussed in [1], schemata <ref> [10] </ref> and predicate sets [2] as used in MOBAL and the FILP system respectively, are special cases of clausemodels, and thus indirectly of Dlab templates.
Reference: 11. <author> N. Lavrac and S. Dzeroski. </author> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: Encoding taxonomies: Dlab grammar G9 We now step through the development of a Dlab grammar for the finite element mesh design application frequently used to evaluate relational learning algorithms (see e. g. <ref> [7, 11] </ref>). The training examples in this domain are "hand-constructed" approximations of physical structures. Such an approximation is a set of finite elements called a mesh model, divided into a collection of edges.
Reference: 12. <author> J.W. Lloyd. </author> <title> Foundations of logic programming. </title> <publisher> Springer-Verlag, </publisher> <address> 2nd edition, </address> <year> 1987. </year>
Reference-contexts: We first give a recursive syntactic definition of the Dlab formalism. 2 We assume familiarity with first order logic (see <ref> [12, 8] </ref> for an introduction), but briefly review the basic relevant concepts. A first order alphabet is a set of predicate symbols, constant symbols and functor symbols.
Reference: 13. <author> I.G. MacDonald. </author> <title> Symmetric functions and Hall polynomials. </title> <publisher> Clarendon Oxford, </publisher> <year> 1979. </year>
Reference-contexts: Therefore we need e k (s 1 ; : : : ; s n ), where e k is the elementary symmetric function <ref> [13] </ref> of degree k and the s i are the numbers of marbles in each urn. The first base case of this recursive function accounts for the fact that there is only one way to select 0 objects.
Reference: 14. <author> T.M. Mitchell. </author> <title> The need for biases in learning generalizations. </title> <type> Technical Report CBM-TR-117, </type> <institution> Department of Computer Science, Rutgers University, </institution> <year> 1980. </year>
Reference-contexts: generally circumscribed as "a tendency to show prejudice against one group and favouritism towards another" (Collins Cobuild, 1987), has been adapted to the field of computational inductive reasoning to become a generic term for "any basis for choosing one generalization over another, other than strict consistency with the instances" (Mitchell <ref> [14] </ref>). We borrow a more fine-tuned definition of inductive bias from Utgoff [20]. Definition 1 (inductive bias). Except for the presented examples and counterexamples of the concept being learned, all factors that influence hypothesis selection constitute bias. These factors include the following: 1.
Reference: 15. <author> T.M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: Ease of navigation: it should suggest a strategy for exploring the search space The latter concern is inspired by the classical machine learning view on induction as a search process through a partially ordered space induced by the generalization relation, cf. <ref> [15] </ref>. Machine learning systems typically search the space specific-to-general or general-to-specific. The features of Dlab make it especially compatible with the latter class of systems. We will introduce a so-called refinement operator for Dlab that calculates the maximally general specializations of any clause in the hypothesis space.
Reference: 16. <author> S. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the 1st conference on algorithmic learning theory, </booktitle> <pages> pages 368-381. </pages> <address> Ohmsma, Tokyo, Japan, </address> <year> 1990. </year>
Reference-contexts: Timothy Chow for their elegant solutions of the dlab size combinatorics problem, and Hendrik Blockeel for his helpful comments on this paper. 6 More procedural approaches to syntactic bias specifications use parameters such as the maximal variable depth or term level to control the complexity of the concept language, cf. <ref> [5, 16] </ref>. Parametrized languages should be considered complementary to Dlab, in the sense that the same parameters trivially define (a series of) Dlab grammars. 7 A clear case where this conversion is impossible occurs when the antecedent descrip tion grammar generates an infinite language.
Reference: 17. <author> C. Nedellec, H. Ade, and B. Bergadano, F. </author> <title> a nd Tausend. Declarative bias in ILP. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> Advances in Inductive Logic Programming, volume 32 of Frontiers in Artificial Intelligence and Applica tions, </booktitle> <pages> pages 82-103. </pages> <publisher> IOS Press, </publisher> <year> 1996. </year>
Reference-contexts: The acceptance criteria that define whether a search procedure may stop with a given hypothesis or should continue searching for a better choice Utgoff's definition of bias has further developed into a typology which distinguishes three different categories <ref> [17] </ref>: language bias roughly combines Utgoff's factors 1 and 2, and search bias and validation bias roughly correspond to items 3 and 4 respectively 1 .
Reference: 18. <author> S. Russell and B. Grosof. </author> <title> A Declarative Approach to Bias in Concept Learning. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI87), </booktitle> <pages> pages 505-510, </pages> <year> 1987. </year>
Reference-contexts: For this type of explicit input parameters, Russell and Grosof <ref> [18] </ref> introduced the concept declarative bias. In this paper, we present a new formalism for the declarative representation of language bias. The formalism is called, somewhat opportunistically, Dlab (Declarative LAnguage Bias).
Reference: 19. <author> Leon Sterling and Ehud Shapiro. </author> <title> The art of Prolog. </title> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This idea can be elegantly formalised and implemented using the Definite Clause Grammar (DCG) notation, which is an extension of Prolog (cf. <ref> [3, 19] </ref>) 3 . Definition 3 (Dlab semantics).
Reference: 20. <author> P.E. Utgoff. </author> <title> Shift of bias for inductive concept-learning. </title> <editor> In R.S Michalski, J.G. Carbonell, and T.M. Mitchell, editors, </editor> <booktitle> Machine Learning: an artificial intelligence approach, </booktitle> <pages> pages 107-148. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: We borrow a more fine-tuned definition of inductive bias from Utgoff <ref> [20] </ref>. Definition 1 (inductive bias). Except for the presented examples and counterexamples of the concept being learned, all factors that influence hypothesis selection constitute bias. These factors include the following: 1. The language in which hypotheses are described 2. The space of hypotheses that the program can consider 3.
References-found: 20

