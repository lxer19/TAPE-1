URL: http://www.cs.princeton.edu/~ristad/papers/pu-533-96.ps.gz
Refering-URL: http://www.cs.princeton.edu/~ristad/papers/pu-533-96.html
Root-URL: http://www.cs.princeton.edu
Title: Finite Growth Models FGM framework one may direct learning by criteria beyond simple maximum-likelihood. The
Author: Eric Sven Ristad Peter N. Yianilos 
Note: In the  
Date: December, 1996 (revised April 1997)  
Pubnum: Research Report CS-TR-533-96  
Abstract: Finite growth models (FGM) are nonnegative functionals that arise from parametrically-weighted directed acyclic graphs and a tuple observation that affects these weights. The weight of a source-sink path is the product of the weights along it. The functional's value is the sum of the weights of all such paths. The mathematical foundations of hidden Markov modeling (HMM) and expectation maximization (EM) are generalized to address the problem of functional maximization given an observation. Probability models such as HMMs and stochastic context free grammars are examples that satisfy a particular constraint: that of summing or integrating to one. The FGM framework, algorithms, and data structures describe these and other similar stochastic models while providing a unified and natural way for computer scientists to learn and reason about them and their many variations. Restricted to probabilistic form, FGMs correspond to stochastic automata that allow observations to be processed in many orderings and groupings not just one-by-one in sequential order. As a result the parameters of a highly general form of stochastic transducer can be learned from examples, and the particular case of string edit distance is developed. fl Both authors are with the Department of Computer Science, Princeton University, 35 Olden Street, Princeton, NJ 08544. The second author is also with the NEC Research Institute, 4 Independence Way, Princeton, NJ 08540. The first author is partially supported by Young Investigator Award IRI-0258517 from the National Science Foundation. Email: fristad,pnyg@cs.princeton.edu. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. E. Baum, </author> <title> An inequality and associated maximization technique in statistical estimatation of probabilistic functions of Markov processes, Inequalities, </title> <booktitle> 3 (1972), </booktitle> <pages> pp. 1-8. </pages>
Reference-contexts: The result remains an FGM on one therefore the ability to easily improve costs given a training corpus is retained. Several kinds of hidden discrete-state stochastic models are in widespread use today. Hidden Markov Models (HMM) were introduced in the 1960's <ref> [2, 3, 1] </ref> and are typically applied in time-series settings when it is reasonable to assume that the observed data are generated or approximated well by the output of an automaton which changes state and generates output values stochastically. <p> The complete dynamic program is then given by the following simple algorithm: Algorithm 1 procedure forward ff 1 = 1 ff i = e2I (v i ) ff s (e) C (ej) M (e; xj) This may be recognized as the forward step of the Baum-Welch HMM forward-backward algorithm <ref> [2, 1, 21] </ref> adapted to the more general FGM setting. Following execution ff n has value F (xj). Algorithm 1 is said to evaluate the FGM using its current set of parameters. <p> The weights are fl e = where denotes the sum of all fl e associated with M i . In general, any density for which a maximum-likelihood estimate is readily available may be used as an observation model, e.g. beta densities for 20 random variables confined to <ref> [0; 1] </ref>. See [14] for a treatment of HMMs including a compact discussion of the discrete and continuous optimization problems discussed above. 4 Beyond Finite Probability Models SFGMs as defined in the previous section are probability functions on an associated finite dimensional observation space.
Reference: [2] <author> L. E. Baum and J. E. Eagon, </author> <title> An inequality with application to statistical estimation for probabalistic functions of a Markov process and to models for ecology, </title> <journal> Bull. AMS, </journal> <volume> 73 (1967), </volume> <pages> pp. 360-363. </pages>
Reference-contexts: The result remains an FGM on one therefore the ability to easily improve costs given a training corpus is retained. Several kinds of hidden discrete-state stochastic models are in widespread use today. Hidden Markov Models (HMM) were introduced in the 1960's <ref> [2, 3, 1] </ref> and are typically applied in time-series settings when it is reasonable to assume that the observed data are generated or approximated well by the output of an automaton which changes state and generates output values stochastically. <p> The complete dynamic program is then given by the following simple algorithm: Algorithm 1 procedure forward ff 1 = 1 ff i = e2I (v i ) ff s (e) C (ej) M (e; xj) This may be recognized as the forward step of the Baum-Welch HMM forward-backward algorithm <ref> [2, 1, 21] </ref> adapted to the more general FGM setting. Following execution ff n has value F (xj). Algorithm 1 is said to evaluate the FGM using its current set of parameters.
Reference: [3] <author> L. E. Baum, T. Petrie, G. Soules, and N. Weiss, </author> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains, </title> <journal> Ann. Math Stat., </journal> <volume> 41 (1970), </volume> <pages> pp. 164-171. </pages>
Reference-contexts: Our view is that the essential content of these ideas is one of decomposability of certain optimization problems that are captured in a somewhat general way by our definition of an FGM. The early literature, especially theorem 2.1 of <ref> [3] </ref>, reveals an awareness that the mathematics of HMMs apply beyond strictly probabalistic settings. This direction of generalization does not however appear to have been followed until now. Thinking seems to have been dominated by the motivating problem: the optimization of Markovian stochastic models. <p> The result remains an FGM on one therefore the ability to easily improve costs given a training corpus is retained. Several kinds of hidden discrete-state stochastic models are in widespread use today. Hidden Markov Models (HMM) were introduced in the 1960's <ref> [2, 3, 1] </ref> and are typically applied in time-series settings when it is reasonable to assume that the observed data are generated or approximated well by the output of an automaton which changes state and generates output values stochastically. <p> This is relevant to FGMs because they may be regarded as immense mixtures of contributions from every source-sink path. We follow earlier lines in its proof without requiring proper densities. The lemma may also be derived starting from theorem 2.1 of <ref> [3] </ref> in its most general form. <p> Lemma 1 and theorem 1 may then be strengthened to say that strict progress is made unless one is already at a critical point in parameter space. Even this does not imply parametric convergence however. See <ref> [3, 11, 22] </ref> for discussion of the convergence and other properties of the EM algorithm. These mathematical issues including rate of convergence, while important, are not our focus and we will not consider them further. <p> A trivial reduction to FGMs results in a simple iterative algorithm. This problem is considered in [9] where the same algorithm, essentially a rediscovery of Baum-Welch/EM, is described. The multiplicative update perspective of [9] is essentially the growth transform of <ref> [3] </ref>. We describe this problem using the notation of [9] and reduce it to an FGM. A stock which appreciates by 25% in say one month, is said to have a return of 1:25. The returns of each stock over time period i forms a vector denoted X i .
Reference: [4] <author> Y. Bengio and P. Frasconi, </author> <title> Input/output hmms for sequence processing, </title> <journal> IEEE Transactions on Neural Networks, </journal> <month> 7 </month> <year> (1996). </year>
Reference-contexts: Later work within the syntactic pattern recognition outlook addressed the induction problem for a particular subclass of finite transducer [19]. More recently a stochastic approach was taken in <ref> [4] </ref> 24 where hidden Markov models were used to capture an input-output relationship between synchronous time series. Finite growth models can be used to derive Baum-Welch/EM based learning algorithms for stochastic transducers. This interesting class of stochastic models capture in a somewhat general way the relationship between dependent symbol streams.
Reference: [5] <author> C. Berrou, A. Glavieux, and P. Thitimajshima, </author> <title> Near shannon limit error-correcting coding and decoding: Turbo codes (1), </title> <booktitle> in Proc. </booktitle> <address> ICC'93, Geneva, Switzerland, </address> <year> 1993, </year> <pages> pp. 1064-1070. </pages>
Reference-contexts: The relationship between these networks and hidden Markov models is elucidated in [27]. Exploring the relationship of FGMs to error correcting codes such as those based on trellises, and the recently developed class of turbo codes <ref> [5] </ref>, is a subject for future work. We wonder broadly whether our framework's generality might lead to better codes, and in particular whether DAGs that encode nonlinear time orderings might have value. Such a possibility is suggested by [29, 28].
Reference: [6] <author> J. Berstel, </author> <title> Transductions and Context-Free Languages, </title> <publisher> Teubner, Stuttgart, </publisher> <year> 1979. </year>
Reference-contexts: We remark, however, that useful models may exist that violate this assumption and therefore generate values that without normalization do not represent probabilities. 5 Stochastic Transducers The notion of transduction has its roots in classical formal language theory (e.g. <ref> [6] </ref>) and represents the formalization of the idea of a machine that converts an input sequence into an output sequence. Later work within the syntactic pattern recognition outlook addressed the induction problem for a particular subclass of finite transducer [19].
Reference: [7] <author> P. F. Brown, </author> <title> Acoustic modeling problem in automatic speech recognition, </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, Department of Computer Science, </institution> <year> 1987. </year>
Reference-contexts: Section 4 observes that conditioning on earlier portions of the observation sequence does not violate this assumption and demonstrates that using the FGM framework, it is easy to see that parameter reestimation for such context dependent HMMs decomposes into primitive context optimization problems. This generalizes Brown's use <ref> [7] </ref> of the previous speech window to condition generation of the current one. Thus the power of causal context models such as statistical language models may be added to the HMM framework and may lead to improved performance. <p> Formally the observation models are then conditional probability 23 functions where the conditioning is on earlier observation components not earlier edge sequences. Each source-sink path then corresponds to a causal chain of conditional probabilities so that the FGM's value is a probability. Brown observes in his thesis <ref> [7] </ref> that the HMM output independence assumption may be relaxed to allow generation of the current speech sample window to depend on the previous one. He then uses Baum-Welch reestimation without reproof.
Reference: [8] <author> R. Chellappa and A. Jain, eds., </author> <title> Markov Random Fields: Theory and Application, </title> <publisher> Academic Press, </publisher> <year> 1993. </year>
Reference-contexts: If the causality restriction is discarded each pixel's model is strengthened but their product is no longer a probability. This corresponds to relaxing the requirement that the projection functions in an SFGM correspond to a permutation of the observation dimensions. Noncausal neighborhood systems have proven useful in image processing <ref> [8] </ref>. Since reestimation must nevertheless climb we have the result that even a noncausal unnormalized models like that sketched above may be improved within the FGM framework. 4.6 Dynamic Choice Function As remarked earlier we might have allowed choice functions to depend on the observation x.
Reference: [9] <author> T. M. </author> <title> Cover, An algorithm for maximizing expected log investment return, </title> <journal> IEEE Transactions on Information Theory, </journal> <year> (1984). </year>
Reference-contexts: Section 6 relates the models above to FGMs by describing a reduction for each. As further illustration it considers the portfolio optimization problem and shows that the simple iterative algorithm reported in <ref> [9] </ref> for optimization arises by reducing the problem to an FGM. The work described in this paper began in 1993 with our efforts to model handwriting. The focus then was on transduction, in an attempt to evaluate and learn a notion for similarity for the off-line case. <p> A trivial reduction to FGMs results in a simple iterative algorithm. This problem is considered in <ref> [9] </ref> where the same algorithm, essentially a rediscovery of Baum-Welch/EM, is described. The multiplicative update perspective of [9] is essentially the growth transform of [3]. We describe this problem using the notation of [9] and reduce it to an FGM. <p> A trivial reduction to FGMs results in a simple iterative algorithm. This problem is considered in <ref> [9] </ref> where the same algorithm, essentially a rediscovery of Baum-Welch/EM, is described. The multiplicative update perspective of [9] is essentially the growth transform of [3]. We describe this problem using the notation of [9] and reduce it to an FGM. A stock which appreciates by 25% in say one month, is said to have a return of 1:25. <p> A trivial reduction to FGMs results in a simple iterative algorithm. This problem is considered in <ref> [9] </ref> where the same algorithm, essentially a rediscovery of Baum-Welch/EM, is described. The multiplicative update perspective of [9] is essentially the growth transform of [3]. We describe this problem using the notation of [9] and reduce it to an FGM. A stock which appreciates by 25% in say one month, is said to have a return of 1:25. The returns of each stock over time period i forms a vector denoted X i . <p> This matches the equation given in <ref> [9] </ref>.
Reference: [10] <author> T. M. Cover and J. A. Thomas, </author> <title> Elements of Information Theory, </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: This can also be seen by recognizing it as [D (k 0 ) + H ()], where D (k) is the Kullbak-Leibler distance, and H () denotes entropy (see <ref> [10] </ref>). Thus maximizing the first term, written Q (; 0 ) in the literature, will surely increase log f (xj 0 ) and hence f (xj 0 ).
Reference: [11] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin, </author> <title> Maximum-likelihood from incomplete data via the EM algorithm, </title> <journal> J. Royal Statistical Society Ser. B (methodological), </journal> <volume> 39 (1977), </volume> <pages> pp. 1-38. </pages>
Reference-contexts: Our approach to this problem extends the idea of Baum-Welch to FGMs. Several years following the development of HMMs, essentially the same mathematical ideas expressed in the their algorithm were rediscovered as the expectation maximization (EM) algorithm for mixture densities <ref> [11, 22] </ref>. This work focused on parameter estimation for mixture densities, and has had considerable influence on a somewhat independent community. In what follows we will use the phrase Baum-Welch/EM to refer to the adaptation of these results to the FGM setting. <p> Lemma 1 and theorem 1 may then be strengthened to say that strict progress is made unless one is already at a critical point in parameter space. Even this does not imply parametric convergence however. See <ref> [3, 11, 22] </ref> for discussion of the convergence and other properties of the EM algorithm. These mathematical issues including rate of convergence, while important, are not our focus and we will not consider them further. <p> The result is a conceptually clear and computationally tractable approach to implementing more sophisticated estimation schemes for both existing and new stochastic models. That EM could accomplish this in general was observed in <ref> [11] </ref> but does not appear to have impacted the use of EM in practice. We consider two approaches: maximum a posteriori parameter (MAP) estimation, and minimum description length (MDL). Our discussion begins with MAP.
Reference: [12] <author> P. A. V. Hall and G. R. Dowling, </author> <title> Approximate string matching, </title> <journal> Computing Surveys, </journal> <volume> 12 (1980), </volume> <pages> pp. 381-402. </pages>
Reference-contexts: The development in section 5.1 begins with a natural reformulation into stochastic terms first noted by Hall and Dowling in <ref> [12, P.390-1] </ref>. We refer to the result as stochastic edit distance [24]. Our focus is on the interesting problem of learning insert, delete, and substitute costs from a training corpus (s 1 ; t 1 ); : : :; (s n ; t n ) of string pairs. <p> These costs are represented as a table c i;j of size jj + 1 fi jj + 1, where row and column 0 correspond to an additional alphabet member * representing the null character. See [26] for review or <ref> [12] </ref> for a compact discussion. The entry in table position i; j gives the cost of substituting symbol j of s for symbol i of t. The first row gives insertion costs, the first column gives deletion costs, and the remaining entries specify substitution costs. <p> This outlook was first introduced in section 3.2.2 of <ref> [12] </ref>. Our contribution is its further development to include the learning of costs and the construction of more sophisticated distance functions as described later in this section. In [24] the authors implement the learning of costs and give experimental results. <p> The logarithm E of the joint probability of s and t is returned rather than the probability itself, so that the caller need not deal with extended range floating point values. As observed by <ref> [12] </ref>, the algorithm's structure resembles that of the standard dynamic program for edit distance. 29 Algorithm 7 procedure Evaluate (s, t) for j = 1; : : : ; jtj + 1 ` = 1 + j mod 2 if i &gt; 1 _ j &gt; 1 else ff 1;1 =
Reference: [13] <author> J. E. Hopcroft and J. D. Ullman, </author> <title> Introduction fo Automata Theory, Languages, and Computation, </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference-contexts: This modified machine will impose the same extrinsic distribution as the original but is free of nonemitting cycles and may be reduced to an FGM. 6.2 Stochastic Context Free Grammars A stochastic context free grammar (SCFG) is a conventional context free grammar <ref> [13] </ref> with a probability attached to each production such that for every nonterminal A the probabilities attached to all productions "A ! : : :" sum to unity. Applied to a sentence of finite length these grammars have an interesting recursive FGM structure.
Reference: [14] <author> X. D. Huang, Y. Ariki, and M. A. Jack, </author> <title> Hidden Markov Models for Speech Recognition, </title> <publisher> Edinburgh University Press, </publisher> <year> 1990. </year>
Reference-contexts: A matrix-based approach is typically used to formalize this notion, and [21] and <ref> [14] </ref> provide nice overviews. Normal (Gaussian) mixtures find application in statistical pattern recognition where items to be classified are represented as real-valued vectors. Stochastic context free grammars (SCFG) lead to a natural probabilistic interpretation of natural language parsing. <p> In general, any density for which a maximum-likelihood estimate is readily available may be used as an observation model, e.g. beta densities for 20 random variables confined to [0; 1]. See <ref> [14] </ref> for a treatment of HMMs including a compact discussion of the discrete and continuous optimization problems discussed above. 4 Beyond Finite Probability Models SFGMs as defined in the previous section are probability functions on an associated finite dimensional observation space. <p> See <ref> [21, 14] </ref> for reviews. <p> This suggests that FGMs may be more expressive than HMMs. We now discuss an important variation: time duration HMMs [17, 25] and sketch their rederiv-ation in FGM terms illustrating the utility of parameterized choice models. See <ref> [14] </ref> pages 218-221 for a brief review. If in a conventional HMM a state transitions to itself with probability p, then duration in that state is geometrically distributed, i.e. as p t1 (1 p).
Reference: [15] <author> X. D. Huang and M. A. Jack, </author> <title> Unified modeling of vector quantization and hidden markov models using semi-continuous hidden markov models, </title> <booktitle> in Proc. ICASSP, </booktitle> <year> 1989, </year> <pages> pp. 639-642. </pages>
Reference-contexts: Our earlier reduction of HMMs to FGMs then results in the mixture-density HMMs widely used in speech recognition. To illustrate the variations possible we observe that tying every component with the same index to a single density yields the semi-continuous variation <ref> [15] </ref>. 36 6.4 Portfolio Optimization Our final example consists of a problem that is not strictly speaking a stochastic probability model but nevertheless fits easily into the FGM formalism.
Reference: [16] <author> S. L. Lauritzen, </author> <title> Graphical Models, </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1996. </year> <month> 38 </month>
Reference-contexts: This matches the equation given in [9]. Assuming the X j occur with equal probability then maximizing the training set's likelihood also maximizes our expected log return from the portfolio. 6.5 Other Applications Pearl's Bayes nets [20], also known as graphical models <ref> [16] </ref>, can be represented in their simplest forms as FGMs in which vertices correspond to variables, edge weights to conditional probabilities, and vertex a posteriori probabilities (fl) to belief. More complex networks can still be represented as FGMs although the reduction is more involved.
Reference: [17] <author> S. E. Levinson, </author> <title> Continuously variable duration hidden markov models for automatic speech recognition, </title> <booktitle> Computer Speech and Language, 1 (1986), </booktitle> <pages> pp. 29-45. </pages>
Reference-contexts: Finally, an FGM that generates joint continuous observations will correspond to an HMM only if these joint densities are trivial, i.e. a product of independent terms. This suggests that FGMs may be more expressive than HMMs. We now discuss an important variation: time duration HMMs <ref> [17, 25] </ref> and sketch their rederiv-ation in FGM terms illustrating the utility of parameterized choice models. See [14] pages 218-221 for a brief review.
Reference: [18] <author> R. J. McEliece, D. J. C. MacKay, and J. Cheng, </author> <title> Turbo decoding as an instance of pearl's `belief propagation' algorithm, </title> <journal> Submitted to IEEE Journal on Selected Areas in Communication, </journal> <year> (1996). </year>
Reference-contexts: We wonder broadly whether our framework's generality might lead to better codes, and in particular whether DAGs that encode nonlinear time orderings might have value. Such a possibility is suggested by [29, 28]. The relationship between Turbo codes and Bayes nets is examined in <ref> [18] </ref> and understanding the relationship of FGMs to this specific outlook is another interesting area for future work. Acknowledgments We thank Andrew Appel, Vince Poor, Bob Sedgewick, and Bob Tarjan for their comments on this work, and Joe Kupin for helpful discussions regarding stochastic transduction. 37
Reference: [19] <author> J. Oncina, P. Garc ia, and E. Vidal, </author> <title> Learning subsequential transducers for pattern recognition interpretation tasks, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <month> 15 </month> <year> (1993). </year>
Reference-contexts: Later work within the syntactic pattern recognition outlook addressed the induction problem for a particular subclass of finite transducer <ref> [19] </ref>. More recently a stochastic approach was taken in [4] 24 where hidden Markov models were used to capture an input-output relationship between synchronous time series. Finite growth models can be used to derive Baum-Welch/EM based learning algorithms for stochastic transducers.
Reference: [20] <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Graphical models <ref> [20, 27] </ref> represent a branch of related work in which the independence structure of a set of random variables is the main focus. <p> This matches the equation given in [9]. Assuming the X j occur with equal probability then maximizing the training set's likelihood also maximizes our expected log return from the portfolio. 6.5 Other Applications Pearl's Bayes nets <ref> [20] </ref>, also known as graphical models [16], can be represented in their simplest forms as FGMs in which vertices correspond to variables, edge weights to conditional probabilities, and vertex a posteriori probabilities (fl) to belief. More complex networks can still be represented as FGMs although the reduction is more involved.
Reference: [21] <author> A. B. Poritz, </author> <title> Hidden Markov models: a guided tour, </title> <booktitle> in Proc. </booktitle> <address> ICASSP-88, </address> <year> 1988, </year> <pages> pp. 7-13. </pages>
Reference-contexts: A matrix-based approach is typically used to formalize this notion, and <ref> [21] </ref> and [14] provide nice overviews. Normal (Gaussian) mixtures find application in statistical pattern recognition where items to be classified are represented as real-valued vectors. Stochastic context free grammars (SCFG) lead to a natural probabilistic interpretation of natural language parsing. <p> The complete dynamic program is then given by the following simple algorithm: Algorithm 1 procedure forward ff 1 = 1 ff i = e2I (v i ) ff s (e) C (ej) M (e; xj) This may be recognized as the forward step of the Baum-Welch HMM forward-backward algorithm <ref> [2, 1, 21] </ref> adapted to the more general FGM setting. Following execution ff n has value F (xj). Algorithm 1 is said to evaluate the FGM using its current set of parameters. <p> See <ref> [21, 14] </ref> for reviews. <p> Applied to a sentence of finite length these grammars have an interesting recursive FGM structure. Baum-Welch/EM may then be used to learn the attached probabilities from a corpus of training sentences. The result is essentially the same as the inside-outside algorithm described in <ref> [21] </ref>. We assume the grammar G is in Chomsky normal form and use t to denote the input sentence. Viewing G as a stochastic generator of strings each nonterminal A may expand into sentences of many lengths.
Reference: [22] <author> R. A. Redner and H. F. Walker, </author> <title> Mixture densities, maximum likelihood, and the EM algorithm, </title> <journal> SIAM Review, </journal> <volume> 26 (1984), </volume> <pages> pp. 195-239. </pages>
Reference-contexts: Our approach to this problem extends the idea of Baum-Welch to FGMs. Several years following the development of HMMs, essentially the same mathematical ideas expressed in the their algorithm were rediscovered as the expectation maximization (EM) algorithm for mixture densities <ref> [11, 22] </ref>. This work focused on parameter estimation for mixture densities, and has had considerable influence on a somewhat independent community. In what follows we will use the phrase Baum-Welch/EM to refer to the adaptation of these results to the FGM setting. <p> Lemma 1 and theorem 1 may then be strengthened to say that strict progress is made unless one is already at a critical point in parameter space. Even this does not imply parametric convergence however. See <ref> [3, 11, 22] </ref> for discussion of the convergence and other properties of the EM algorithm. These mathematical issues including rate of convergence, while important, are not our focus and we will not consider them further.
Reference: [23] <author> E. S. Ristad and P. N. Yianilos, </author> <title> Probability value library, </title> <type> tech. rep., </type> <institution> Princeton University, Department of Computer Science, </institution> <year> 1994. </year> <title> [24] , Learning string edit distance, </title> <type> tech. rep., </type> <institution> Princeton University, Department of Computer Science, </institution> <year> 1996. </year>
Reference-contexts: For complex problems this behavior is the rule not the exception since element probabilities in general decline exponentially with dimension. Logarithmic representation is one solution but the authors have also explored a floating point representation with extended exponent range as reported in <ref> [23] </ref>. Because fl variables are normalized by F (x) they may be adequately represented using standard floating point. We now briefly discuss the computational complexity of FGM maximization. A restricted class of FGMs is identified for which maximization is shown to be NP-complete.
Reference: [25] <author> M. J. Russell and A. E. Cook, </author> <title> Experimental evaluation of duration modelling techniques for automatic speech recognition, </title> <booktitle> in Proc. </booktitle> <address> ICASSP-85, </address> <year> 1985, </year> <pages> pp. 5-8. </pages>
Reference-contexts: Finally, an FGM that generates joint continuous observations will correspond to an HMM only if these joint densities are trivial, i.e. a product of independent terms. This suggests that FGMs may be more expressive than HMMs. We now discuss an important variation: time duration HMMs <ref> [17, 25] </ref> and sketch their rederiv-ation in FGM terms illustrating the utility of parameterized choice models. See [14] pages 218-221 for a brief review.
Reference: [26] <author> D. Sankoff and J. B. Kruskal, </author> <title> Macromolecules: The Theory and Practice of Sequence Comparison, </title> <publisher> Addison-Wesley, </publisher> <year> 1983, 1983. </year>
Reference-contexts: The non-negative costs of these primitive edit operations are parameters to the algorithm. These costs are represented as a table c i;j of size jj + 1 fi jj + 1, where row and column 0 correspond to an additional alphabet member * representing the null character. See <ref> [26] </ref> for review or [12] for a compact discussion. The entry in table position i; j gives the cost of substituting symbol j of s for symbol i of t. The first row gives insertion costs, the first column gives deletion costs, and the remaining entries specify substitution costs.
Reference: [27] <author> P. Smith, D. Heckerman, and M. I. Jordan, </author> <title> Probabilistic independence networks for hidden probability models, </title> <type> Tech. Rep. </type> <institution> MSR-TR-93-03, Microsoft Research, </institution> <year> 1996. </year>
Reference-contexts: Graphical models <ref> [20, 27] </ref> represent a branch of related work in which the independence structure of a set of random variables is the main focus. <p> More complex networks can still be represented as FGMs although the reduction is more involved. The relationship between these networks and hidden Markov models is elucidated in <ref> [27] </ref>. Exploring the relationship of FGMs to error correcting codes such as those based on trellises, and the recently developed class of turbo codes [5], is a subject for future work.
Reference: [28] <author> N. Wiberg, </author> <title> Codes and Decoding on General Graphs, </title> <type> PhD thesis, </type> <institution> Linkoping Studies in Science and Technology, Sweden, </institution> <year> 1996. </year> <note> No. 440. </note>
Reference-contexts: We wonder broadly whether our framework's generality might lead to better codes, and in particular whether DAGs that encode nonlinear time orderings might have value. Such a possibility is suggested by <ref> [29, 28] </ref>. The relationship between Turbo codes and Bayes nets is examined in [18] and understanding the relationship of FGMs to this specific outlook is another interesting area for future work.
Reference: [29] <author> N. Wiberg, H. Loeliger, and R. Kotter, </author> <title> Codes and iterative decoding on general graphs, </title> <journal> European Transactions on Telecommunications, </journal> <volume> 6 (1995), </volume> <pages> pp. 513-526. 39 </pages>
Reference-contexts: We wonder broadly whether our framework's generality might lead to better codes, and in particular whether DAGs that encode nonlinear time orderings might have value. Such a possibility is suggested by <ref> [29, 28] </ref>. The relationship between Turbo codes and Bayes nets is examined in [18] and understanding the relationship of FGMs to this specific outlook is another interesting area for future work.
References-found: 28

