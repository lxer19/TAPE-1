URL: http://www.cs.berkeley.edu/~ghorm/papers/glunix-spe.ps
Refering-URL: http://www.cs.berkeley.edu/~ghorm/resume.html
Root-URL: http://www.cs.berkeley.edu
Title: GLUnix: a Global Layer Unix for a Network of Workstations  Summary  
Author: Douglas P. Ghormley David Petrou Steven H. Rodrigues Amin M. Vahdat Thomas E. Anderson 
Keyword: Key words: Distributed Operating Systems, Networks of Workstations, Single System Image, Transparent Remote Execution.  
Address: Berkeley  Berkeley  Washington  
Affiliation: University of California at  Carnegie Mellon University  Network Appliance Corporation  University of California at  University of  
Note: To appear in Software, Practice and Experience, 1998.  
Email: ghorm@cs.berkeley.edu  dpetrou@cs.cmu.edu  steverod@netapp.com  vahdat@cs.berkeley.edu  tom@cs.washington.edu  
Date: February 23, 1998  
Abstract: Recent improvements in network and workstation performance have made workstation clusters an attractive architecture for diverse workloads, including interactive sequential and parallel applications. Although viable hardware solutions are available today, the largest challenge in making such a cluster usable lies in the system software. This paper describes the design and implementation of GLUnix, operating system middleware for a cluster of workstations. GLUnix was designed to provide transparent remote execution, support for interactive parallel and sequential jobs, load balancing, and backward compatibility for existing application binaries. GLUnix was constructed to be easily portable to a number of platforms. GLUnix has been in daily use for over two and a half years and is currently running on a 100-node cluster of Sun UltraSPARCs. This paper relates our experiences with designing, building, and operating GLUnix. We discuss three important design tradeoffs faced by any cluster system and present the reasons for our choices. Each of these design decisions is then reevaluated in light of both our experience and recent technological advancements. We then describe the user-level, centralized, event-driven architecture of GLUnix and highlight a number of aspects of the implementation. Performance and scalability measurements of the system indicate that a centralized, user-level design can scale gracefully to significant cluster sizes, incurring only an additional 220 msec of overhead per node for remote execution. The discussion focuses on the successes and failures we encountered while building and maintaining the system, including a characterization of the limitations of a user-level implementation and various features that were added to satisfy the user community. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas Anderson, Brian Bershad, Edward Lazowska, and Henry Levy. </author> <title> Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism. </title> <journal> In ACM Transactions on Computer Systems, </journal> <pages> pages 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Alternatively, the system need not kill the entire parallel program whenever a single process in it dies. Although not currently implemented, the master could notify the remaining processes of the failure, using a mechanism such as Scheduler Activations <ref> [1] </ref>, and continue normal operation. Currently the system does not provide any support for check-pointing or restarting applications, though applications are free to use check-pointing packages such as libckpt [34] if necessary.
Reference: [2] <author> Thomas E. Anderson, David E. Culler, David A. Patterson, </author> <title> and the NOW Team. A Case for NOW (Networks of Workstations). </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: The narrowing performance gap between commodity workstations and supercomputers as well as the availability of commodity high-speed local area networks [10, 18, 5] has led to the concept that networks of workstations (NOWs) can support all three types of workloads <ref> [2] </ref>. By leveraging commodity high-performance workstations and networks, the primary challenge in building a NOW shifts to the system software needed to manage, control, and access the cluster.
Reference: [3] <author> Thomas E. Anderson, Michael D. Dahlin, Jeanna M. Neefe, David A. Patterson, Drew S. Roselli, and Randolph Y. Wang. </author> <title> Serverless Network File Systems. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 109-126, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: We then share some software engineering lessons we learned from constructing the system. Assumptions In constructing GLUnix, we made two primary assumptions concerning the target environment: shared filesystems and homogeneous operating systems. Since the Berkeley NOW project included a separate group implementing a high performance global file system <ref> [3] </ref>, GLUnix assumes that each node of the cluster shares the same file systems, making pathnames valid on all nodes. Further, GLUnix uses a shared file to store the network address of the centralized GLUnix master for bootstrapping. Our cluster currently uses an NFS-mounted file system for this purpose. <p> Modifying the Makefile to add dependencies between the source files and the executable can resolve this problem for a given Makefile. However, requiring users to modify Makefiles to avoid consistency problems in the filesystem is neither transparent nor reasonable. This argues for a strongly-consistent cluster file system <ref> [24, 3] </ref>. Social Considerations Over the past few years we have learned a number of things from supporting an active user community.
Reference: [4] <author> Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau and David E. Culler and Joseph M. Hellerstein and David A. Patterson. </author> <title> High-Performance Sorting on Networks of Workstations. </title> <booktitle> In SIGMOD '97, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: Implementations of the parallel programming language Split-C [17] and the Message Passing Interface (MPI) [39] use the GLUnix library routines to run on the Berkeley NOW. A number of interesting parallel programs have been implemented using these facilities, including: NOW-Sort <ref> [4] </ref>, which set a record for the world's fastest sort time (8.4 GB in under one minute), p-murphi [19], a parallel version of the popular protocol verification tool, and a 32K by 32K LINPACK implementation that achieved 10.1 GFlops on 100 UltraSPARC-I processors, placing the Berkeley NOW 345th on the list <p> The second most common use of the cluster has been as a parallel compilation server via the glumake utility. The cluster has been used to achieve a world record in disk-to-disk sorting <ref> [4] </ref>, for simulations of advanced log structured file system research [30], two-dimensional particle in cell plasma [41], three-dimensional fast Fourier transforms, and genetic inference algorithms. GLUnix was also found to be extremely useful for testing and system administration. <p> The slope of the line is 220 msec/node. Performance Tuning In this section, we will describe some of our experiences with improving the performance of remote execution. These changes were driven by the needs of the parallel NOW-Sort application <ref> [4] </ref>, which set a record for the fastest disk-to-disk sort. To set the record, NOW-Sort had to sort as much data as possible in 60 seconds. Unfortunately, initial runs revealed that it took more than 60 seconds simply to start the program on 100 nodes.
Reference: [5] <author> Anonymous. </author> <title> Gigabit Ethernet. </title> <journal> Computer, </journal> <volume> 29(2), </volume> <month> February </month> <year> 1996. </year>
Reference-contexts: However, technology trends of the past two decades have steadily eroded the boundaries of these paradigms. The narrowing performance gap between commodity workstations and supercomputers as well as the availability of commodity high-speed local area networks <ref> [10, 18, 5] </ref> has led to the concept that networks of workstations (NOWs) can support all three types of workloads [2]. By leveraging commodity high-performance workstations and networks, the primary challenge in building a NOW shifts to the system software needed to manage, control, and access the cluster.
Reference: [6] <author> Remzi Arpaci, Andrea Dusseau, Amin Vahdat, Lok Liu, Thomas Anderson, and David Patterson. </author> <title> The Interaction of Parallel and Sequential Workload on a Network of Workstations. </title> <booktitle> In Proceedings of Performance/Sigmetrics, </booktitle> <month> May </month> <year> 1995. </year> <month> 22 </month>
Reference-contexts: A NOW should guarantee one workstation's worth of performance to each user at all times, with the possibility of recruiting all available cluster resources (CPU, disk, memory, network) for parallel programs when needed. The feasibility of supporting integrated parallel and sequential workloads was established by an early simulation study <ref> [6] </ref> of workloads measured from a 32-node CM-5 at Los Alamos National Laboratory and a 70-node workstation cluster at U.C. Berkeley. The study concluded that harvesting idle cycles on a 60-node workstation cluster could support both the parallel and sequential workloads studied with minimal slowdown. <p> The system should also dynamically migrate jobs either as cluster load becomes unbalanced or as nodes are dynamically added to or removed from the system. * Coscheduling of Parallel Jobs: Efficient execution of communicating parallel jobs requires that the individual processes be scheduled at roughly the same time <ref> [33, 6, 21] </ref>. Architecture Alternatives We faced three major design decisions in the development of GLUnix: kernel level vs. user level, centralized vs. decentralized, and multi-threaded vs. event-driven. Kernel Level vs.
Reference: [7] <author> Remzi H. Arpaci, David E. Culler, Arvind Krishnamurthy, Steve Steinberg, and Kathy Yelick. </author> <title> Empirical Evaluation of the CRAY--T3D: A Compiler Perspective. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: In the absence of coscheduling, the individual operating systems in the NOW would independently schedule the processes of a parallel application. User-level cluster systems typically take this approach (e.g., PVM [38]). However, a number of studies <ref> [16, 22, 7, 21] </ref> demonstrate that the lack of coscheduling leads to unacceptable execution times for frequently-communicating processes. This slowdown occurs when a process stalls while attempting to communicate or synchronize with a currently unscheduled process.
Reference: [8] <author> Andrew Birrell and Bruce Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Trans. Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: As development of GLUnix proceeded, maintaining all event handlers as non-blocking routines became increasingly frustrating and considerably reduced code readability, maintainability, and correctness. Distributed GLUnix components often interact in a remote procedure call (RPC) <ref> [8] </ref> like fashion, issuing requests and needing replies. To maintain system responsiveness, GLUnix must return to the main event loop after issuing a request to a remote node.
Reference: [9] <author> Andrew D. Birrell. </author> <title> An introduction to programming with threads. </title> <type> Research Report 35, </type> <institution> Digital Equipment Corporation Systems Research Center, </institution> <address> Palo Alto, CA, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: Multi-threading vs. Event-driven Our final design decision concerned whether to use a multi-threaded or event-driven programming model for individual GLU-nix components. Multi-threaded designs have two primary advantages over event-driven programming: increased concur-rency with the potential for improved performance, and the relative simplicity of dealing with blocking operations <ref> [9, 32] </ref>. Maintaining performance in a distributed system with event-driven programming requires that network message handlers use only non-blocking communication operations, treating reply messages as separate events. Event-driven architectures, however, are simpler to design and debug, since they avoid the deadlocks and race conditions of multi-threaded programming.
Reference: [10] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. L. Seitz, J. N. Seizovic, and Wen-King Su. Myrinet: </author> <title> A gigabit-per-second Local Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: However, technology trends of the past two decades have steadily eroded the boundaries of these paradigms. The narrowing performance gap between commodity workstations and supercomputers as well as the availability of commodity high-speed local area networks <ref> [10, 18, 5] </ref> has led to the concept that networks of workstations (NOWs) can support all three types of workloads [2]. By leveraging commodity high-performance workstations and networks, the primary challenge in building a NOW shifts to the system software needed to manage, control, and access the cluster. <p> Current scalability bottlenecks of the system are BSD sockets over TCP/IP and the 10 Mb/sec switched Ethernet used by GLUnix. Performance improvements can be expected by porting GLUnix to use higher performance communication layers such as Active Messages [29] and faster networks such as the 1.28 Gb/s Myrinet <ref> [10] </ref>. As development of GLUnix proceeded, maintaining all event handlers as non-blocking routines became increasingly frustrating and considerably reduced code readability, maintainability, and correctness. Distributed GLUnix components often interact in a remote procedure call (RPC) [8] like fashion, issuing requests and needing replies. <p> Second, many-to-one communication patterns using TCP/IP over the switched Ethernet exhibit poor scalability, as demonstrated by Figures 6 and 8. Many of these network contention effects can be alleviated by modifying the GLUnix Comm module to use Active Messages [29] across the Myrinet <ref> [10] </ref>. 14 various parallel degrees. This time does not include any time spent in the startup or daemon processes or in the network. The slope of the line is 220 msec/node. Performance Tuning In this section, we will describe some of our experiences with improving the performance of remote execution.
Reference: [11] <author> Allan Bricker, Michael Litzkow, and Miron Livny. </author> <title> Condor Technical Summary. </title> <type> Technical Report 1069, </type> <institution> University of Wisconsin-Madison, Computer Science Department, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: PVM [38] supported parallel jobs but lacked the gang scheduling necessary to run closely synchronized parallel jobs efficiently [21]. PVM also did not integrate the parallel job environment with interactive sequential jobs or provide dynamic load balancing. Condor <ref> [11] </ref> could dynamically balance cluster load through job migration, but did not transparently support existing binaries: jobs were restricted to a limited set of operating system features and had to be re-linked with a special library. Further, Condor only operated in batch mode. <p> However, middleware layers are limited by the functionality and performance of the API provided by the underlying operating system. In particular, we were concerned that the limitations of previous user-level systems <ref> [11, 47, 34] </ref> were an indication that the functionality required for transparent remote execution was simply not available at the user level. We concluded that the advantages of portability and timeliness made a user-level middleware layer the best option at the time. <p> In contrast, only LSF batch jobs can be manipulated from any node in the system. Thus, the interface provided by GLUnix is closer to the SSI of SMPs than the interface provided by LSF. Condor <ref> [11] </ref> and libckpt [34] provide migration and check-pointing, respectively, at the user level. Both systems require application modification or relinking and therefore do not work with existing application binaries.
Reference: [12] <author> D. Brownbridge, L. Marshall, and B. Randell. </author> <title> The Newcastle Commection. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 12 </volume> <pages> 1147-62, </pages> <year> 1982. </year>
Reference-contexts: Two related projects which have taken the approach of minor modifications to existing systems are the Newcastle Connection and Solaris MC. Newcastle Connection <ref> [12] </ref> was an early distributed system constructed by interposing a distributed software layer on top of the Vax VMS operating system. It required very few changes to the underlying operating system, but intercepted application/kernel interactions and forwarded them to remote nodes to provide remote execution.
Reference: [13] <author> J. Chapin, M. Rosenblum, S. Devine, T. Lahiri, D. Teodosiu, and A. Gupta. Hive: </author> <title> Fault Containment for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 12-25, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Both systems were successful at providing remote execution and process migration, though at an admittedly high implementation cost in the Sprite case [20]. More recently, the Hive <ref> [13] </ref> project modified Irix to improve the failure characteristics of distributed shared memory machines by tolerating certain hardware failures (fault-containment).
Reference: [14] <author> David R. Cheriton. </author> <title> The V Distributed System. </title> <booktitle> In Communications of the ACM, </booktitle> <pages> pages 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Fully decentralized (peer-to-peer) systems do not depend on any single node and consequently are more resilient to failure and can scale more gracefully to large clusters. Decentralization, however, complicates state management and synchronization, making such systems more difficult to design and debug. The scalability of previous centralized systems <ref> [40, 14] </ref> led us to decide to begin with a simple, centralized design and to later decentralize system components as needed. Multi-threading vs. Event-driven Our final design decision concerned whether to use a multi-threaded or event-driven programming model for individual GLU-nix components. <p> For example: (myrinet2.3^solaris2.6)desktop. 20 system supports all of our desired functionality for a NOW operating system. In this section we briefly list some of the projects with goals similar to our own. Both V <ref> [14] </ref> and Sprite [31] are research operating systems written from scratch, providing in-kernel support for remote execution and dynamic process migration. Both systems were successful at providing remote execution and process migration, though at an admittedly high implementation cost in the Sprite case [20].
Reference: [15] <author> Brent Chun, Alan Mainwaing, and David Culler. </author> <title> Virtual Network Transport Protocols for Myrinet. </title> <booktitle> In Proceedings of the 5th Hot Interconnects Conference, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: Furthermore, since the GLUnix master maintains a connection to each GLUnix daemon and startup process in the system, this file descriptor limit also restricts the total number of GLUnix processes in the system at any one time. Active Messages provides a light-weight connection establishment abstraction called an endpoint <ref> [15] </ref>. Porting GLUnix to use Active Messages would dramatically reduce the I/O connection establishment costs and would remove the limit on the number of connections. However, using Active Message endpoints for the I/O connections would require application modification, since endpoints cannot be accessed through native UNIX file descriptors.
Reference: [16] <author> Mark Crovella, Prakash Das, Czarek Dubnicki, Thomas LeBlanc, and Evangelos Markatos. </author> <title> Multiprogramming on multiprocessors. </title> <type> Technical Report 385, </type> <institution> University of Rochester, Computer Science Department, </institution> <month> February </month> <year> 1991. </year> <note> Revised May. </note>
Reference-contexts: In the absence of coscheduling, the individual operating systems in the NOW would independently schedule the processes of a parallel application. User-level cluster systems typically take this approach (e.g., PVM [38]). However, a number of studies <ref> [16, 22, 7, 21] </ref> demonstrate that the lack of coscheduling leads to unacceptable execution times for frequently-communicating processes. This slowdown occurs when a process stalls while attempting to communicate or synchronize with a currently unscheduled process.
Reference: [17] <author> David E. Culler, Andrea Dusseau, Seth C. Goldstein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <year> 1993. </year>
Reference-contexts: GLUnix has enabled or assisted systems research and program development in a number of areas. GLUnix has been used by the graduate Parallel Architecture and Advanced Architecture classes and by other research departments here at Berkeley. Implementations of the parallel programming language Split-C <ref> [17] </ref> and the Message Passing Interface (MPI) [39] use the GLUnix library routines to run on the Berkeley NOW.
Reference: [18] <author> Martin de Prycker. </author> <title> Asynchronous Transfer Mode: Solution for Broadband ISDN. </title> <publisher> Ellis Horwood Publishers, </publisher> <address> second edition, </address> <year> 1993. </year>
Reference-contexts: However, technology trends of the past two decades have steadily eroded the boundaries of these paradigms. The narrowing performance gap between commodity workstations and supercomputers as well as the availability of commodity high-speed local area networks <ref> [10, 18, 5] </ref> has led to the concept that networks of workstations (NOWs) can support all three types of workloads [2]. By leveraging commodity high-performance workstations and networks, the primary challenge in building a NOW shifts to the system software needed to manage, control, and access the cluster.
Reference: [19] <author> D.L. Dill, A. Drexler, A.J. Hu, and C.H. Yang. </author> <title> Protocol Verification as a Hardware Design Aid. </title> <booktitle> In International Conference on Computer Design: VLSI in Computers and Processors, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: A number of interesting parallel programs have been implemented using these facilities, including: NOW-Sort [4], which set a record for the world's fastest sort time (8.4 GB in under one minute), p-murphi <ref> [19] </ref>, a parallel version of the popular protocol verification tool, and a 32K by 32K LINPACK implementation that achieved 10.1 GFlops on 100 UltraSPARC-I processors, placing the Berkeley NOW 345th on the list of the world's 500 fastest supercomputers at the time 5 .
Reference: [20] <author> Fred Douglis and John Ousterhout. </author> <title> Transparent Process Migration: Design Alternatives and the Sprite Implementation. </title> <journal> Software Practice and Experience, </journal> <volume> 21(8) </volume> <pages> 757-85, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Further, Condor only operated in batch mode. Sprite [31] provided dynamic load balancing through process migration <ref> [20] </ref> and maintained full UNIX I/O and job control semantics for remote jobs, but did not support parallel jobs. In addition, it was a complete operating system, requiring significant effort to keep up with workstation evolution. <p> Both V [14] and Sprite [31] are research operating systems written from scratch, providing in-kernel support for remote execution and dynamic process migration. Both systems were successful at providing remote execution and process migration, though at an admittedly high implementation cost in the Sprite case <ref> [20] </ref>. More recently, the Hive [13] project modified Irix to improve the failure characteristics of distributed shared memory machines by tolerating certain hardware failures (fault-containment).
Reference: [21] <author> Andrea C. Dusseau, Remzi H. Arpaci, and David E. Culler. </author> <title> Effective Distributed Scheduling of Parallel Workloads. </title> <booktitle> In Proceedings of the 1996 ACM SIGMETRICS Conference, </booktitle> <year> 1996. </year>
Reference-contexts: LSF [47] and Utopia [46] provided support for remote execution of interactive sequential jobs, but did not support parallel jobs or dynamic load balancing. PVM [38] supported parallel jobs but lacked the gang scheduling necessary to run closely synchronized parallel jobs efficiently <ref> [21] </ref>. PVM also did not integrate the parallel job environment with interactive sequential jobs or provide dynamic load balancing. <p> The system should also dynamically migrate jobs either as cluster load becomes unbalanced or as nodes are dynamically added to or removed from the system. * Coscheduling of Parallel Jobs: Efficient execution of communicating parallel jobs requires that the individual processes be scheduled at roughly the same time <ref> [33, 6, 21] </ref>. Architecture Alternatives We faced three major design decisions in the development of GLUnix: kernel level vs. user level, centralized vs. decentralized, and multi-threaded vs. event-driven. Kernel Level vs. <p> In the absence of coscheduling, the individual operating systems in the NOW would independently schedule the processes of a parallel application. User-level cluster systems typically take this approach (e.g., PVM [38]). However, a number of studies <ref> [16, 22, 7, 21] </ref> demonstrate that the lack of coscheduling leads to unacceptable execution times for frequently-communicating processes. This slowdown occurs when a process stalls while attempting to communicate or synchronize with a currently unscheduled process. <p> The GLUnix batch facility (see the section on Social Considerations, Batch Jobs) has been used for managing very large numbers of short and medium simulation runs, such as the thousands of simulations needed during the investigation of implicit 5 See http://netlib.cs.utk.edu/benchmark/top500.html. 6 See http://http.cs.berkeley.edu/~eanders/pictures/index.html 11 coscheduling <ref> [21] </ref>. Performance and Scalability This section evaluates the performance and scalability of GLUnix and relates some of our experiences in performance tuning. All of the data presented in this section are measured with GLUnix running on 167MHz Sun UltraSPARCs running Solaris 2.5.1. <p> Future Work There are two further research efforts that have arisen out of GLUnix. The first is an effort called Implicit Coscheduling <ref> [21] </ref> which enables communicating processes to coschedule themselves using various heuristics. One such heuristic is adaptive two-phase blocking. When a process would normally block, waiting for a communication event, it instead spins for an adaptive period of time.
Reference: [22] <author> Dror G. Feitelson and Larry Rudolph. </author> <title> Gang Scheduling Performance Benefits for Fine-Grained Synchronization. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16(4) </volume> <pages> 306-18, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: In the absence of coscheduling, the individual operating systems in the NOW would independently schedule the processes of a parallel application. User-level cluster systems typically take this approach (e.g., PVM [38]). However, a number of studies <ref> [16, 22, 7, 21] </ref> demonstrate that the lack of coscheduling leads to unacceptable execution times for frequently-communicating processes. This slowdown occurs when a process stalls while attempting to communicate or synchronize with a currently unscheduled process.
Reference: [23] <author> Douglas P. Ghormley, Steven H. Rodrigues, David Petrou, and Thomas E. Anderson. SLIC: </author> <title> An Extensibility System for Commodity Operating Systems. </title> <booktitle> In Proceedings of the 1998 USENIX Conference, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: For reasons discussed in this paper, GLUnix was unable to completely fulfill this vision while remaining a portable, user-level solution. To overcome these limitations, we are developing a non-intrusive kernel extension system called SLIC <ref> [23] </ref> which will enable GLUnix to fulfill its design potential. The current implementation of GLUnix performs remote execution, but lacks complete transparency. It maintains most, but not all, UNIX I/O semantics and provides intelligent load balancing at job startup, but does not migrate processes. <p> To remove these limitations and enable GLUnix to achieve full functionality while retaining the advantages of a user-level solution, we are developing SLIC, a system to enable the transparent interposition of trusted, portable extension code into existing commodity operating system kernels <ref> [23] </ref>. By overcoming the limitations of developing at the user level, SLIC will enable GLUnix to provide a transparent single system image. A second technological advancement that has occurred in the past four years is that kernel modification has become considerably simpler. <p> In this way, the constituent processes of a parallel program dynamically adjust themselves to run at approximately the same time despite independent local operating system schedulers. Experimentation reveals that implicit coscheduling performs within +/-35% of coscheduling without the need for global coordination. The second research effort is SLIC <ref> [23] </ref> which is a method for alleviating the restrictions of implementing system facilities at the user level. SLIC enables trusted extensions in commodity operating systems by transparently interposing on common UNIX interfaces such as system calls, signals, and virtual memory events. <p> By loading trusted extensions into a commodity kernel, system software developers can eliminate many of the user-level limitations discussed earlier while retaining the majority of advantages that user-level software development affords. 21 The second research effort is SLIC <ref> [23] </ref>, a system designed to alleviate the restrictions typically associated with system facilities implemented at the user level. SLIC enables developers to easily insert trusted extensions into commodity operating systems by transparently interposing on common UNIX interfaces such as system calls, signals, and virtual memory events.
Reference: [24] <author> J. Howard, M. Kazar, S. Menees, D. Nichols, M. Satyanarayanan, R. Sidebotham, and M. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-82, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Modifying the Makefile to add dependencies between the source files and the executable can resolve this problem for a given Makefile. However, requiring users to modify Makefiles to avoid consistency problems in the filesystem is neither transparent nor reasonable. This argues for a strongly-consistent cluster file system <ref> [24, 3] </ref>. Social Considerations Over the past few years we have learned a number of things from supporting an active user community.
Reference: [25] <author> Yousef A. Khalidi, Jose M. Bernabeu, Vlada Matena, Ken Shirriff, and Moti Thadani. </author> <title> Solaris MC: A multi computer OS. </title> <booktitle> In Proceedings of the 1996 USENIX Conference, </booktitle> <pages> pages 191-203. </pages> <publisher> USENIX, </publisher> <month> January </month> <year> 1996. </year>
Reference-contexts: It required very few changes to the underlying operating system, but intercepted application/kernel interactions and forwarded them to remote nodes to provide remote execution. The Solaris MC <ref> [25] </ref> group made small modifications to the Solaris kernel to implement cluster facilities, similar to the Newcastle Connection. Solaris MC provides execution-time remote execution and a SSI, but does not do dynamic process migration. A number of other projects have attempted to provide cluster facilities at the user level.
Reference: [26] <author> James R. Larus and Eric Schnarr. EEL: </author> <title> Machine-independent Executable Editing. </title> <booktitle> In ACM SIGPLAN '95 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <volume> volume 30(6), </volume> <pages> pages 291-300, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: We concluded that the advantages of portability and timeliness made a user-level middleware layer the best option at the time. To alleviate the limitations of a user-level implementation while retaining compatibility with existing binaries, we planned to explore binary rewriting techniques <ref> [43, 26] </ref> or develop novel kernel extension techniques. Given the choice to implement a user-level solution, we then considered the question of whether all programs on a node would run under GLUnix or only GLUnix-aware programs.
Reference: [27] <author> Steven S. Lumetta and David E. Culler. </author> <title> The mantis parallel debugger. </title> <booktitle> In Proceedings of the SIGMETRICS Symposium on Parallel and Distributed Tools, </booktitle> <pages> pages 118-26, </pages> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Glib Spawn () is the basic function to run a program under GLUnix. Glib CoSpawn () is used to place an application on the same nodes as a currently running application; it is used by the Mantis <ref> [27] </ref> parallel debugger. Glib GetMyNpid () and Glib GetMyVNN () return the NPID and VNN, respectively, of the requesting application. Glib Signal () is used to send a signal to one or all of the VNNs of a program running under GLUnix.
Reference: [28] <author> Steve Maguire. </author> <title> Writing Solid Code. </title> <publisher> Microsoft Press, </publisher> <year> 1993. </year>
Reference-contexts: This technique actually had the opposite effect. Rather than making the system more stable, masking faults in this way separated the point of fault origin from the point of failure, making bugs more difficult to locate and fix. The book Writing Solid Code <ref> [28] </ref> led us to a adopt a less tolerant internal structure. System debugging became significantly easier and proceeded more quickly. The GLUnix library and the communication protocols used between GLUnix components both remain tolerant to errors in order to maintain overall system stability.
Reference: [29] <author> Alan Mainwaring and David Culler. </author> <title> Active Message Applications Programming Interface and Communication Subsystem Organization. </title> <type> Technical Report CSD-96-918, </type> <institution> University of California at Berkeley, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: Current scalability bottlenecks of the system are BSD sockets over TCP/IP and the 10 Mb/sec switched Ethernet used by GLUnix. Performance improvements can be expected by porting GLUnix to use higher performance communication layers such as Active Messages <ref> [29] </ref> and faster networks such as the 1.28 Gb/s Myrinet [10]. As development of GLUnix proceeded, maintaining all event handlers as non-blocking routines became increasingly frustrating and considerably reduced code readability, maintainability, and correctness. <p> Second, many-to-one communication patterns using TCP/IP over the switched Ethernet exhibit poor scalability, as demonstrated by Figures 6 and 8. Many of these network contention effects can be alleviated by modifying the GLUnix Comm module to use Active Messages <ref> [29] </ref> across the Myrinet [10]. 14 various parallel degrees. This time does not include any time spent in the startup or daemon processes or in the network. The slope of the line is 220 msec/node.
Reference: [30] <author> Jeanna Neefe Matthews, Drew Roselli, Adam M. Costello, Randolph Y. Wang, and Thomas E. Anderson. </author> <title> Improving the Performance of Log-structured File Systems with Adaptive Methods. </title> <booktitle> In Proceedingsof the 16th ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1997. </year>
Reference-contexts: The second most common use of the cluster has been as a parallel compilation server via the glumake utility. The cluster has been used to achieve a world record in disk-to-disk sorting [4], for simulations of advanced log structured file system research <ref> [30] </ref>, two-dimensional particle in cell plasma [41], three-dimensional fast Fourier transforms, and genetic inference algorithms. GLUnix was also found to be extremely useful for testing and system administration. Initially, parallel stress testing of the Myrinet network and Active Messages was accomplished using scripts built around rsh.
Reference: [31] <author> J. Ousterhout, A. Cherenson, F. Douglis, M. Nelson, and B. Welch. </author> <title> The Sprite Network Operating System. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 23-36, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Condor [11] could dynamically balance cluster load through job migration, but did not transparently support existing binaries: jobs were restricted to a limited set of operating system features and had to be re-linked with a special library. Further, Condor only operated in batch mode. Sprite <ref> [31] </ref> provided dynamic load balancing through process migration [20] and maintained full UNIX I/O and job control semantics for remote jobs, but did not support parallel jobs. In addition, it was a complete operating system, requiring significant effort to keep up with workstation evolution. <p> For example: (myrinet2.3^solaris2.6)desktop. 20 system supports all of our desired functionality for a NOW operating system. In this section we briefly list some of the projects with goals similar to our own. Both V [14] and Sprite <ref> [31] </ref> are research operating systems written from scratch, providing in-kernel support for remote execution and dynamic process migration. Both systems were successful at providing remote execution and process migration, though at an admittedly high implementation cost in the Sprite case [20].
Reference: [32] <author> John Ousterhout. </author> <title> Why Threads Are A Bad Idea (for most purposes). </title> <booktitle> Invited Talk at the 1996 USENIX Technical Conference, </booktitle> <month> January </month> <year> 1996. </year> <note> See http://www.sunlabs.com/ouster. </note>
Reference-contexts: Multi-threading vs. Event-driven Our final design decision concerned whether to use a multi-threaded or event-driven programming model for individual GLU-nix components. Multi-threaded designs have two primary advantages over event-driven programming: increased concur-rency with the potential for improved performance, and the relative simplicity of dealing with blocking operations <ref> [9, 32] </ref>. Maintaining performance in a distributed system with event-driven programming requires that network message handlers use only non-blocking communication operations, treating reply messages as separate events. Event-driven architectures, however, are simpler to design and debug, since they avoid the deadlocks and race conditions of multi-threaded programming.
Reference: [33] <author> John K. Ousterhout. </author> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> In Third International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> May </month> <year> 1982. </year> <month> 23 </month>
Reference-contexts: The system should also dynamically migrate jobs either as cluster load becomes unbalanced or as nodes are dynamically added to or removed from the system. * Coscheduling of Parallel Jobs: Efficient execution of communicating parallel jobs requires that the individual processes be scheduled at roughly the same time <ref> [33, 6, 21] </ref>. Architecture Alternatives We faced three major design decisions in the development of GLUnix: kernel level vs. user level, centralized vs. decentralized, and multi-threaded vs. event-driven. Kernel Level vs. <p> Each line of output from remote processes of a parallel program may be prepended with the VNN of the process which generated that line. Parallel Program Support Just like MPP operating systems, GLUnix provides specialized scheduling support for parallel programs in the form of barriers and coscheduling <ref> [33] </ref>. Since individual parallel program instructions are not executed in lock-step in the SPMD model, barriers are necessary to synchronize cooperating processes, enabling programmers to make assumptions about the state of computation and data in other processes. <p> At this point the GLUnix Barrier () library call will return and the process will continue executing. GLUnix implements an approximation of coscheduling through a simple user-level strategy. The GLUnix master uses a matrix algorithm <ref> [33] </ref> to determine a time-slice order for all runnable parallel programs. Periodically, the master sends a message to each daemon identifying which NPID should be scheduled on that node. Each daemon maintains a NPID to local UNIX pid mapping in its process database. <p> Consequently, GLUnix provides a closer approximation of a SSI than does PVM. Interaction with PVM is done through a special shell whereas GLUnix jobs can be run from unmodified UNIX shells. PVM also has only recently begun to support coscheduling <ref> [33] </ref> which GLUnix was designed to do from the start. LSF (Load Sharing Facility) [47] provides command line integration with a load balancing and queuing facility.
Reference: [34] <author> James S. Plank, Micah Beck, Gerry Kingsley, and Kai Li. Libckpt: </author> <title> Transparent Checkpointing under Unix. </title> <booktitle> In Proceedings of the 1995 USENIX Summer Conference, </booktitle> <pages> pages 213-223, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: However, middleware layers are limited by the functionality and performance of the API provided by the underlying operating system. In particular, we were concerned that the limitations of previous user-level systems <ref> [11, 47, 34] </ref> were an indication that the functionality required for transparent remote execution was simply not available at the user level. We concluded that the advantages of portability and timeliness made a user-level middleware layer the best option at the time. <p> Currently the system does not provide any support for check-pointing or restarting applications, though applications are free to use check-pointing packages such as libckpt <ref> [34] </ref> if necessary. Transparent Remote Execution When users run GLUnix programs from the command-line, the process exec'ed by the user's shell calls the Glib Spawn () routine in the GLUnix library. <p> In contrast, only LSF batch jobs can be manipulated from any node in the system. Thus, the interface provided by GLUnix is closer to the SSI of SMPs than the interface provided by LSF. Condor [11] and libckpt <ref> [34] </ref> provide migration and check-pointing, respectively, at the user level. Both systems require application modification or relinking and therefore do not work with existing application binaries.
Reference: [35] <author> Dennis M Ritchie and Ken Thompson. </author> <title> The UNIX Time-Sharing System. </title> <journal> Communications of the ACM, </journal> <volume> 17(7) </volume> <pages> 365-375, </pages> <year> 1974. </year>
Reference-contexts: the signaling of individual processes of a parallel program. 3 GLUnix treats sequential programs as parallel programs with a parallel degree of 1. 5 I/O Redirection GLUnix extends the standard UNIX abstractions of input and output byte streams to remotely running sequential and parallel programs, including file redirection and pipes <ref> [35] </ref>. Input given to a GLUnix program, either from the console keyboard, a file, or a pipe, is replicated by default to all processes of a parallel program. Analogously, output from network programs is multiplexed to the user's console, a file, or a pipe in the normal UNIX manner.
Reference: [36] <author> Steven H. Rodrigues, Thomas E. Anderson, and David E. Culler. </author> <title> High-performance local area communication with fast sockets. </title> <booktitle> In Proceedings of the 1997 USENIX Conference, </booktitle> <pages> pages 257-274, </pages> <month> January </month> <year> 1997. </year>
Reference-contexts: Porting GLUnix to use Active Messages would dramatically reduce the I/O connection establishment costs and would remove the limit on the number of connections. However, using Active Message endpoints for the I/O connections would require application modification, since endpoints cannot be accessed through native UNIX file descriptors. The FastSockets <ref> [36] </ref> library provides a way of mapping standard UNIX sockets onto Active Messages, but requires applications to be relinked, which violates the GLUnix goal of binary compatibility.
Reference: [37] <author> Sun Microsystems. XDR: </author> <title> External Data Representation Standard. </title> <type> Technical Report RFC-1014, </type> <institution> Sun Microsystems, Inc., </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: Enabling multi-platform support for GLUnix would require porting to a network data transport mechanism such as XDR <ref> [37] </ref> and developing a mechanism for supporting multi-platform binaries. System Architecture Overview GLUnix consists of three components: a per-cluster master, a per-node daemon, and a per-application library (see Figure 1).
Reference: [38] <author> V. Sunderam. </author> <title> PVM: A Framework for Parallel Distributed Computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> De-cember </month> <year> 1990. </year>
Reference-contexts: LSF [47] and Utopia [46] provided support for remote execution of interactive sequential jobs, but did not support parallel jobs or dynamic load balancing. PVM <ref> [38] </ref> supported parallel jobs but lacked the gang scheduling necessary to run closely synchronized parallel jobs efficiently [21]. PVM also did not integrate the parallel job environment with interactive sequential jobs or provide dynamic load balancing. <p> In the absence of coscheduling, the individual operating systems in the NOW would independently schedule the processes of a parallel application. User-level cluster systems typically take this approach (e.g., PVM <ref> [38] </ref>). However, a number of studies [16, 22, 7, 21] demonstrate that the lack of coscheduling leads to unacceptable execution times for frequently-communicating processes. This slowdown occurs when a process stalls while attempting to communicate or synchronize with a currently unscheduled process. <p> UNIX provides two distinct output streams, stdout and stderr. While stderr data is usually directed to a user's console, stdout can be easily redirected to a file or to another program using standard shell redirection facilities. Many remote execution facilities such as rsh and PVM <ref> [38] </ref> do not keep these two I/O streams separate and thus do not maintain proper redirection and piping semantics. GLUnix maintains two separate output streams. Barriers and Coscheduling The GLUnix barrier is implemented using a standard binary-tree algorithm. <p> Solaris MC provides execution-time remote execution and a SSI, but does not do dynamic process migration. A number of other projects have attempted to provide cluster facilities at the user level. PVM (Parallel Virtual Machine) <ref> [38] </ref> provides support for parallel programs on clusters of distributed workstations. Whereas PVM supports heterogeneous clusters, GLUnix is currently constrained to homogeneous cluster. PVM was initially designed for wide-area distributed applications across different administrative domains and was not designed to provide the same level of node coupling that GLU-nix does.
Reference: [39] <author> The MPI Forum. </author> <title> MPI: A Message Passing Interface. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 878-883, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: GLUnix has been used by the graduate Parallel Architecture and Advanced Architecture classes and by other research departments here at Berkeley. Implementations of the parallel programming language Split-C [17] and the Message Passing Interface (MPI) <ref> [39] </ref> use the GLUnix library routines to run on the Berkeley NOW.
Reference: [40] <author> Marvin M. </author> <title> Theimer. </title> <type> Personal communication, </type> <month> June </month> <year> 1994. </year>
Reference-contexts: Fully decentralized (peer-to-peer) systems do not depend on any single node and consequently are more resilient to failure and can scale more gracefully to large clusters. Decentralization, however, complicates state management and synchronization, making such systems more difficult to design and debug. The scalability of previous centralized systems <ref> [40, 14] </ref> led us to decide to begin with a simple, centralized design and to later decentralize system components as needed. Multi-threading vs. Event-driven Our final design decision concerned whether to use a multi-threaded or event-driven programming model for individual GLU-nix components.
Reference: [41] <author> J. P. Verboncoeur, A. B. Langdon, and N. T. Gladd. </author> <title> An object-oriented electromagnetic PIC code. </title> <journal> In Computer Physics Communications, </journal> <pages> pages 199-211, </pages> <year> 1995. </year>
Reference-contexts: The second most common use of the cluster has been as a parallel compilation server via the glumake utility. The cluster has been used to achieve a world record in disk-to-disk sorting [4], for simulations of advanced log structured file system research [30], two-dimensional particle in cell plasma <ref> [41] </ref>, three-dimensional fast Fourier transforms, and genetic inference algorithms. GLUnix was also found to be extremely useful for testing and system administration. Initially, parallel stress testing of the Myrinet network and Active Messages was accomplished using scripts built around rsh.
Reference: [42] <author> Thorsten von Eicken, David E. Culler, Steh C. Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: A second user has executed a sequential job B which GLUnix has placed on othello. The I/O connections from the startup to the remote processes are not represented. though it is designed to be easily replaceable by a module using faster communication primitives, such as Active Messages <ref> [42] </ref>. The main event loop is located in the Comm module. GLUnix events consist of both messages and signals. When a message arrives, the Msg module unpacks the message into a local data structure and then invokes the appropriate message handler.
Reference: [43] <author> Robert Wahbe, Steven Lucco, Thomas Anderson, and Susan Graham. </author> <title> Efficient Software-Based Fault Isolation. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 203-216, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: We concluded that the advantages of portability and timeliness made a user-level middleware layer the best option at the time. To alleviate the limitations of a user-level implementation while retaining compatibility with existing binaries, we planned to explore binary rewriting techniques <ref> [43, 26] </ref> or develop novel kernel extension techniques. Given the choice to implement a user-level solution, we then considered the question of whether all programs on a node would run under GLUnix or only GLUnix-aware programs.
Reference: [44] <author> D. Walsh, B. Lyon, G. Sager, J. M. Chang, D. Goldberg, S. Kleiman, T. Lyon, R. Sandberg, and P. Weiss. </author> <title> Overview of the Sun Network File System. </title> <booktitle> In Proceedings of the 1985 USENIX Winter Conference, </booktitle> <pages> pages 117-124, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: NFS The weak cache consistency policy of NFS <ref> [44] </ref> for file attributes limits the utility of the glumake program. To improve performance, NFS clients cache recently accessed file attributes. Elements in this cache are flushed after a timeout period, eliminating most stale accesses in traditional distributed computing environments.
Reference: [45] <author> Neil Webber. </author> <title> Operating System Support for Portable Filesystem Extensions. </title> <booktitle> In Proceedings of the 1993 USENIX Winter Conference, </booktitle> <pages> pages 219-228, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Existing applications can transparently take advantage of these modifications. The tradeoff is that making kernel modifications requires an understanding of an existing kernela large and complex system containing hundreds of thousands of lines of code. Kernel modifications are often not portable between successive releases of the same operating system <ref> [45] </ref>, and not at all between different operating systems. In addition, operating system kernels typically lack state-of-the-art debugging and development tools, hindering development. Finally, understanding and modifying operating system kernels is a time-intensive task 1 .
Reference: [46] <author> Sognian Zhou, Jingwen Wang, Xiaohn Zheng, and Pierre Delisle. </author> <title> Utopia: A Load Sharing Facility for Large, Heterogeneous Distributed Computing Systems. </title> <type> Technical Report CSRI-257, </type> <institution> University of Toronto, </institution> <year> 1992. </year>
Reference-contexts: When we set out in 1993 to select the system software for the Berkeley NOW, some of the features listed above were present in a few existing systems, but none provided a complete solution. LSF [47] and Utopia <ref> [46] </ref> provided support for remote execution of interactive sequential jobs, but did not support parallel jobs or dynamic load balancing. PVM [38] supported parallel jobs but lacked the gang scheduling necessary to run closely synchronized parallel jobs efficiently [21].
Reference: [47] <author> Songnian Zhou. </author> <title> LSF: load sharing in large-scale heterogeneous distributed systems. </title> <booktitle> In Proceedings of the Workshop on Cluster Computing, </booktitle> <month> December </month> <year> 1992. </year> <month> 24 </month>
Reference-contexts: When we set out in 1993 to select the system software for the Berkeley NOW, some of the features listed above were present in a few existing systems, but none provided a complete solution. LSF <ref> [47] </ref> and Utopia [46] provided support for remote execution of interactive sequential jobs, but did not support parallel jobs or dynamic load balancing. PVM [38] supported parallel jobs but lacked the gang scheduling necessary to run closely synchronized parallel jobs efficiently [21]. <p> However, middleware layers are limited by the functionality and performance of the API provided by the underlying operating system. In particular, we were concerned that the limitations of previous user-level systems <ref> [11, 47, 34] </ref> were an indication that the functionality required for transparent remote execution was simply not available at the user level. We concluded that the advantages of portability and timeliness made a user-level middleware layer the best option at the time. <p> Interaction with PVM is done through a special shell whereas GLUnix jobs can be run from unmodified UNIX shells. PVM also has only recently begun to support coscheduling [33] which GLUnix was designed to do from the start. LSF (Load Sharing Facility) <ref> [47] </ref> provides command line integration with a load balancing and queuing facility. Like GLUnix, when running remote processes, LSF recreates a substantial portion of the user's runtime environment, including current working directory, file creation mask, resource limits, etc.
References-found: 47

