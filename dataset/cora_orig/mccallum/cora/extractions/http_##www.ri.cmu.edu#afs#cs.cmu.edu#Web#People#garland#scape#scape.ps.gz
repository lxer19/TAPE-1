URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/Web/People/garland/scape/scape.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/Web/People/garland/scape/index.html
Root-URL: 
Title: Fast Polygonal Approximation of Terrains and Height Fields  
Author: Michael Garland and Paul S. Heckbert 
Note: This work was supported by ARPA contract F19628-93-C-0171 and NSF Young Investigator award CCR-9357763. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of ARPA, NSF, or the U.S. government.  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: September 19, 1995  
Pubnum: CMU-CS-95-181  
Abstract: email: fgarland,phg@cs.cmu.edu tech report & C++ code: http://www.cs.cmu.edu/~garland/scape 
Abstract-found: 1
Intro-found: 1
Reference: <institution> Several papers are accessible on the World Wide Web using the Uniform Resource Locators (URL's) shown. </institution>
Reference: [1] <author> Xin Chen and Francis Schmitt. </author> <title> Adaptive range data approximation by constrained surface triangulation. </title> <editor> In B. Falcidieno and T. Kunii, editors, </editor> <booktitle> Modeling in Computer Graphics: Methods and Applications, </booktitle> <pages> pages 95-113. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: If this is not sufficient, the triangulator can be replaced with a constrained Delaunay triangulator [2], thus allowing arbitrary edges to be preinserted and left untouched by later phases of the simplification process. Ridge lines, valley lines, roads, load module boundaries, 30 and range data discontinuities <ref> [1] </ref> are examples of such features. In addition to specifying critical features which will be maintained in the approximation, the user can also be given discretionary control over point selection. <p> This problem is definitely caused by the short-sightedness of greedy insertion. One, somewhat ad hoc, solution is to find all cliffs and constrain the triangulation to follow them <ref> [1] </ref>. Dealing With Noise and High Frequencies. The greedy insertion algorithms we have described will work on noisy or high frequency data, but they will not do a very good job. There are two causes of this problem.
Reference: [2] <author> L. Paul Chew. </author> <title> Constrained Delaunay triangulations. </title> <journal> Algorithmica, </journal> <volume> 4 </volume> <pages> 97-108, </pages> <year> 1989. </year>
Reference-contexts: For instance, a particular application might wish to guarantee that certain features of a terrain are preserved. It is trivial to preinsert critical points into the approximation mesh. If this is not sufficient, the triangulator can be replaced with a constrained Delaunay triangulator <ref> [2] </ref>, thus allowing arbitrary edges to be preinserted and left untouched by later phases of the simplification process. Ridge lines, valley lines, roads, load module boundaries, 30 and range data discontinuities [1] are examples of such features.
Reference: [3] <author> James H. Clark. </author> <title> Hierarchical geometric models for visible surface algorithms. </title> <journal> CACM, </journal> 19(10) 547-554, Oct. 1976. 
Reference-contexts: To render a height field quickly, we can use multiresolution modeling, preprocessing it to construct approximations of the surface at various levels of detail <ref> [3, 16] </ref>. When rendering the height field, we can choose an approximation with an appropriate level of detail and use it in place of the original. The various levels of detail can be combined into a hierarchical triangulation [6, 5].
Reference: [4] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Greedy Insertion We call refinement algorithms that insert the point (s) of highest error on each pass greedy insertion algorithms, "greedy" because they make irrevocable decisions as they go <ref> [4] </ref>, and "insertion" because on each pass they insert one or more vertices into the triangulation. Methods that insert a single point in each pass we call sequential greedy insertion and methods that insert multiple points in parallel on each pass we call parallel greedy insertion. <p> Faster Selection This leads us to the next avenue of optimization. The asymptotic expected cost of our algorithm is being held up by the cost of scanning the entire input grid to select the best point. This could be done more quickly with a heap <ref> [4] </ref> or other fast priority queue. We define the candidate point of a triangle to be the grid point within the triangle that has the highest error in the current approximation. Each triangle can have zero or one candidate point.
Reference: [5] <author> Mark de Berg and Katrin Dobrindt. </author> <title> On levels of detail in terrains. </title> <booktitle> In Proc. 11th Annual ACM Symp. on Computational Geometry, </booktitle> <address> Vancouver, B.C., </address> <month> June </month> <year> 1995. </year> <note> Also available as Utrecht University tech report UU-CS-1995-12, URL=ftp://ftp.cs.ruu.nl/pub/RUU/CS/techreps/CS-1995/. </note>
Reference-contexts: When rendering the height field, we can choose an approximation with an appropriate level of detail and use it in place of the original. The various levels of detail can be combined into a hierarchical triangulation <ref> [6, 5] </ref>. In some applications, such as flight simulators, the speed of simplification is unimportant, because database preparation is done off-line, once, while rendering of the simplified terrain is done thousands of times.
Reference: [6] <author> Leila De Floriani. </author> <title> A pyramidal data structure for triangle-based surface description. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 9(2) </volume> <pages> 67-78, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: When rendering the height field, we can choose an approximation with an appropriate level of detail and use it in place of the original. The various levels of detail can be combined into a hierarchical triangulation <ref> [6, 5] </ref>. In some applications, such as flight simulators, the speed of simplification is unimportant, because database preparation is done off-line, once, while rendering of the simplified terrain is done thousands of times. <p> That is, the xy projection of the input points need not form a rectangular grid, but could be any finite point set. This change would require each triangle to store a set of points <ref> [6, 18, 11] </ref>. During re-triangulation, these sets would be merged and split. Instead of scan converting a triangle, one would visit all the points in that triangle's point set.
Reference: [7] <author> Leila De Floriani, Bianca Falcidieno, and Caterina Pienovi. </author> <title> A Delaunay-based method for surface approximation. </title> <booktitle> In Eurographics '83, </booktitle> <pages> pages 333-350. </pages> <publisher> Elsevier Science, </publisher> <year> 1983. </year> <month> 34 </month>
Reference-contexts: The words "sequential" and "parallel" here refer to the selection and re-evaluation process, not to the architecture of the machine. Many variations on the greedy insertion algorithm have been explored over the years; apparently the algorithm has been reinvented many times <ref> [10, 7, 18, 31, 28, 11, 30] </ref>. We now explore four variants of the sequential greedy insertion algorithm. Our first method, 7 algorithm I, is a brute force implementation of sequential greedy insertion with Delaunay triangu-lation. This algorithm is quite slow, so we present two optimizations. <p> and interpolate there Insert (Point p ): mark p as used Mesh Insert (p) Error (Point p ): % Returns the error at a point (our importance measure) return jH (p) Locate and Interpolate (p)j The heart of the algorithm is sequential greedy insertion, much as described in earlier work <ref> [7, 31, 11] </ref>. It is simple and unoptimized. We build an initial approximation of two triangles using the corner points of H. Then we repeatedly scan the unused points to find the one with the largest error and call Insert to add it to the current approximation. <p> From a 100 fi 100 grid, 99 vertices were selected to achieve zero error; 8 vertices suffice. in the accuracy of the model" <ref> [7, p. 342] </ref>. 6. Ideas for Future Research Our experimentation has suggested several avenues for further research. Using Extra Grid Information. The sequential greedy insertion algorithms we have described could easily be generalized to accommodate and exploit additional information about the height field being approximated. <p> Compared to previous work, our Delaunay greedy insertion algorithm yields nearly identical approximations to those of De Floriani et al., Rippa, and Franklin <ref> [7, 31, 11] </ref>, but from the information available, it appears that our algorithm is the fastest both in theory and in practice. Part of Heller's adaptive triangular mesh filtering technique is a greedy insertion algorithm 33 employing heaps and local recalculation.
Reference: [8] <author> Nira Dyn, David Levin, and Shmuel Rippa. </author> <title> Data dependent triangulations for piecewise linear inter-polation. </title> <journal> IMA J. Numer. Anal., </journal> <volume> 10(1) </volume> <pages> 137-154, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: This helps to minimize the occurrence of very thin sliver triangles. Data-dependent triangulation, in contrast, uses the heights of points in addition to their x and y coordinates <ref> [8, 31] </ref>. It can achieve lower error approximations than Delaunay triangulation, but it generates more slivers. 2. Statement of Problem and Approach We assume that a discrete two-dimensional set of samples H of some underlying surface is provided. <p> To achieve a given error threshold, data-dependent greedy insertion takes about 3-4 times as long as Delaunay greedy insertion, but it generates a smaller mesh, which will display faster. Data-dependent triangulation does dramatically better than Delaunay triangulation on certain surfaces <ref> [8] </ref>. The optimal case for data-dependent triangulation is a ruled surface with zero curvature in one direction and nonzero curvature in another. Examples are cylinders, cones, and height fields of the form H (x; y) = f (x) + ay.
Reference: [9] <author> James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. </author> <title> Computer Graphics: </title> <booktitle> Principles and Practice, 2nd ed. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading MA, </address> <year> 1990. </year>
Reference-contexts: These extensions allow our algorithm to be used to simplify terrains with color texture or planar color images [35]. The output of such a simplification is a triangulated surface with linear interpolation of color across each triangle. Such models are ideally suited for hardware-assisted Gouraud shading <ref> [9] </ref> on most graphics workstations, and are a possible substitute for texture mapping when that is not supported by hardware. 5. Results We have implemented all four algorithms above. Our combined implementation of algorithms III and IV consists of about 5,200 lines of C++.
Reference: [10] <author> Robert J. Fowler and James J. Little. </author> <title> Automatic extraction of irregular network digital terrain models. </title> <booktitle> Computer Graphics (SIGGRAPH '79 Proc.), </booktitle> <volume> 13(2) </volume> <pages> 199-207, </pages> <month> Aug. </month> <year> 1979. </year>
Reference-contexts: The words "sequential" and "parallel" here refer to the selection and re-evaluation process, not to the architecture of the machine. Many variations on the greedy insertion algorithm have been explored over the years; apparently the algorithm has been reinvented many times <ref> [10, 7, 18, 31, 28, 11, 30] </ref>. We now explore four variants of the sequential greedy insertion algorithm. Our first method, 7 algorithm I, is a brute force implementation of sequential greedy insertion with Delaunay triangu-lation. This algorithm is quite slow, so we present two optimizations. <p> Looking at the error graphs in Figures 17 and 18, we can see that as ff increases towards 1, the approximations become more accurate and converge to sequential insertion. The second insertion variant is the rule used by the latter passes of Fowler-Little and by Puppo et al. <ref> [10, 30] </ref>. We call it constant threshold parallel insertion. In this case, * is the constant error threshold provided by the user. Thus, on each pass we select and insert all candidate points that do not meet the error tolerance. <p> One approach is to choose initial points using variable-window feature selection [18]. Another is to initialize a candidate point to the triangle's centroid, then use a few iterations of hill climbing to move it to the point of locally maximum error <ref> [10] </ref>. A third approach is to scan convert with sub-sampling, only examining every kth point in x and y. The relative quality of the approximations resulting from these optimizations is not known. Combating Slivers. Pure data-dependent triangulation generates too many slivers. <p> Because the initial pass of his algorithm uses feature selection, we suspect, however, that his could be faster but that our method will produce somewhat higher quality approximations. We have tested our sequential greedy insertion algorithm against the parallel greedy insertion algorithms of Fowler-Little and Puppo et al. <ref> [10, 30] </ref>, and found that sequential greedy insertion yields superior approximations in all cases tested. It appears that the sequential method uses vertices more carefully and leads to less unnecessary subdivision in the mesh.
Reference: [11] <author> W. Randolph Franklin. tin.c, </author> <year> 1993. </year> <title> C code, </title> <publisher> URL=ftp://ftp.cs.rpi.edu/pub/franklin/tin.tar.gz. </publisher>
Reference-contexts: The words "sequential" and "parallel" here refer to the selection and re-evaluation process, not to the architecture of the machine. Many variations on the greedy insertion algorithm have been explored over the years; apparently the algorithm has been reinvented many times <ref> [10, 7, 18, 31, 28, 11, 30] </ref>. We now explore four variants of the sequential greedy insertion algorithm. Our first method, 7 algorithm I, is a brute force implementation of sequential greedy insertion with Delaunay triangu-lation. This algorithm is quite slow, so we present two optimizations. <p> and interpolate there Insert (Point p ): mark p as used Mesh Insert (p) Error (Point p ): % Returns the error at a point (our importance measure) return jH (p) Locate and Interpolate (p)j The heart of the algorithm is sequential greedy insertion, much as described in earlier work <ref> [7, 31, 11] </ref>. It is simple and unoptimized. We build an initial approximation of two triangles using the corner points of H. Then we repeatedly scan the unused points to find the one with the largest error and call Insert to add it to the current approximation. <p> That is, the xy projection of the input points need not form a rectangular grid, but could be any finite point set. This change would require each triangle to store a set of points <ref> [6, 18, 11] </ref>. During re-triangulation, these sets would be merged and split. Instead of scan converting a triangle, one would visit all the points in that triangle's point set. <p> Compared to previous work, our Delaunay greedy insertion algorithm yields nearly identical approximations to those of De Floriani et al., Rippa, and Franklin <ref> [7, 31, 11] </ref>, but from the information available, it appears that our algorithm is the fastest both in theory and in practice. Part of Heller's adaptive triangular mesh filtering technique is a greedy insertion algorithm 33 employing heaps and local recalculation.
Reference: [12] <author> P. J. Green and R. Sibson. </author> <title> Computing Dirichlet tesselations in the plane. </title> <journal> The Computer Journal, </journal> <volume> 21(2) </volume> <pages> 168-173, </pages> <year> 1978. </year>
Reference-contexts: For point location, we can use the simple "walking method" due to Guibas-Stolfi, Green-Sibson, and Lawson [13, p. 121], <ref> [12] </ref>, [21]. This algorithm starts on an edge of the mesh, and walks through the mesh toward the target point until it arrives at the target. <p> Randomization fixes the problem. 4.5.3. A Problem with Location In the process of testing this algorithm, we discovered that on some of our data-dependent triangulations, Guibas and Stolfi's walking method for point location [13, p. 121], <ref> [12] </ref>, [21] would occasionally loop forever. This was not an implementation error, but a fundamental bug in that algorithm for certain triangulations (see Figure 6). We have never seen this problem arise in algorithms I-III, so we conjecture that it only occurs for non-Delaunay triangulations.
Reference: [13] <author> Leonidas Guibas and Jorge Stolfi. </author> <title> Primitives for the manipulation of general subdivisions and the computation of Voronoi diagrams. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 4(2) </volume> <pages> 75-123, </pages> <year> 1985. </year>
Reference-contexts: Two such general triangulation methods are Delaunay triangulation and data-dependent triangulation. Delaunay triangulation is a purely two-dimensional method; it uses only the xy projections of the input points. It finds a triangulation that maximizes the minimum angle of all triangles, among all triangulations of a given point set <ref> [13, 21] </ref>. This helps to minimize the occurrence of very thin sliver triangles. Data-dependent triangulation, in contrast, uses the heights of points in addition to their x and y coordinates [8, 31]. It can achieve lower error approximations than Delaunay triangulation, but it generates more slivers. 2. <p> The routine Mesh Insert locates the triangle containing a given point, splits the triangle into three, and then recursively checks each of the outer edges of these triangles, flipping them if necessary to maintain a Delaunay triangulation (Figure 4) <ref> [13] </ref>. <p> For point location, we can use the simple "walking method" due to Guibas-Stolfi, Green-Sibson, and Lawson <ref> [13, p. 121] </ref>, [12], [21]. This algorithm starts on an edge of the mesh, and walks through the mesh toward the target point until it arrives at the target. <p> So the expected cost is a much more important measure of the practical speed of the algorithm. The expected cost for random access location queries in a Delaunay mesh with v vertices for typical point distributions is only L = O ( p v) <ref> [13, 14] </ref>. However, if successive location queries are close together, then the location procedure can start its search at the triangle returned by the previous call so that very few steps will be needed to find the next target point. <p> In this situation, the expected cost of location is L = O (1) <ref> [13] </ref>. In the algorithm above, point location queries almost always occur near one another. All but a few of them are made in the process of scanning the unused points. <p> Fortunately, we can improve upon our original naive algorithm considerably by exploiting the locality of the changes to the approximation during incremental Delaunay triangulation. 4.2.1. Delaunay Triangulation The incremental Delaunay triangulation algorithm is illustrated in Figure 4, and works as follows <ref> [13, 21] </ref>: To insert a vertex A, locate its containing triangle, or, if it lies on an edge, delete that edge and find its containing quadrilateral. Add "spoke" edges from A to the vertices of this containing polygon. <p> The process continues until no suspect edges remain. The resulting 10 triangulation is Delaunay. 4.2.2. Exploiting Locality of Changes After insertion, the point inserted will have edges emanating from it to the corners of a surrounding polygon <ref> [13] </ref>. This polygon defines the area in which the triangulation has been altered, and hence, it defines the area in which the approximation has changed (see Figure 4c). We call this polygon the update region. <p> For the triangulation, we use a slight modification to Guibas and Stolfi's quad-edge data structure <ref> [13] </ref>, consisting of 2-D points, directed edges, and triangles in an interconnected graph. Each edge points to neighboring edges and to a neighboring triangle. <p> Randomization fixes the problem. 4.5.3. A Problem with Location In the process of testing this algorithm, we discovered that on some of our data-dependent triangulations, Guibas and Stolfi's walking method for point location <ref> [13, p. 121] </ref>, [12], [21] would occasionally loop forever. This was not an implementation error, but a fundamental bug in that algorithm for certain triangulations (see Figure 6). We have never seen this problem arise in algorithms I-III, so we conjecture that it only occurs for non-Delaunay triangulations.
Reference: [14] <author> Leonidas J. Guibas, Donald E. Knuth, and Micha Sharir. </author> <title> Randomized incremental construction of Delaunay and Voronoi diagrams. </title> <booktitle> In Proc. 17th Intl. Colloq. | Automata, Languages, and Programming, </booktitle> <publisher> volume 443 of Springer-Verlag LNCS, </publisher> <pages> pages 414-431, </pages> <address> Berlin, 1990. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: So the expected cost is a much more important measure of the practical speed of the algorithm. The expected cost for random access location queries in a Delaunay mesh with v vertices for typical point distributions is only L = O ( p v) <ref> [13, 14] </ref>. However, if successive location queries are close together, then the location procedure can start its search at the triangle returned by the previous call so that very few steps will be needed to find the next target point.
Reference: [15] <author> Bernd Hamann and Jiann-Liang Chen. </author> <title> Data point selection for piecewise trilinear approximation. </title> <booktitle> Computer-Aided Geometric Design, </booktitle> <volume> 11 </volume> <pages> 477-489, </pages> <year> 1994. </year>
Reference-contexts: Data-dependent variants of the greedy insertion algorithms described can be created by replacing Delaunay triangulation with data-dependent triangulation, as discussed by Rippa and Hamann-Chen <ref> [31, 15] </ref>. The vertex to insert in the triangulation on each pass is chosen as before, but the triangulation is done differently. The incremental Delaunay triangulation algorithm of section 4.2.1 tested suspect edges using a purely two-dimensional geometric test involving circumcircles. <p> During re-triangulation, these sets would be merged and split. Instead of scan converting a triangle, one would visit all the points in that triangle's point set. Generalization of these techniques from piecewise-planar approximations of functions of two variables to curved surface approximations and higher dimensional spaces <ref> [15] </ref> is fairly straightforward. Multiresolution Modeling. Further work is needed to build on the algorithms described here to create multiresolution databases for fast terrain rendering.
Reference: [16] <author> Paul S. Heckbert and Michael Garland. </author> <title> Multiresolution modeling for fast rendering. </title> <booktitle> In Proc. Graphics Interface '94, </booktitle> <pages> pages 43-50, </pages> <address> Banff, Canada, </address> <month> May </month> <year> 1994. </year> <institution> Canadian Inf. </institution> <note> Proc. Soc. URL=http:// www.cs.cmu.edu/~ph. </note>
Reference-contexts: To render a height field quickly, we can use multiresolution modeling, preprocessing it to construct approximations of the surface at various levels of detail <ref> [3, 16] </ref>. When rendering the height field, we can choose an approximation with an appropriate level of detail and use it in place of the original. The various levels of detail can be combined into a hierarchical triangulation [6, 5].
Reference: [17] <author> Paul S. Heckbert and Michael Garland. </author> <title> Survey of surface approximation algorithms. </title> <type> Technical report, </type> <institution> CS Dept., Carnegie Mellon U., </institution> <year> 1995. </year> <note> CMU-CS-95-194, URL=http://www.cs.cmu.edu/~garland/scape. </note>
Reference-contexts: We explore the use of both Delaunay triangulation and data-dependent triangulation. The paper concludes with a discussion of empirical results, ideas for future work, and a summary. 1.1. Background Our companion survey paper <ref> [17] </ref> contains a thorough review of surface simplification methods. <p> In one dimension, jH 00 j is a good curvature measure. Since H is a discrete function, we estimate its derivatives numerically using central differences. Note that this measure of importance is independent of the current approximation; it is essentially a feature method <ref> [17] </ref>. Hence, it lends itself to a one pass approach: compute values for jH 00 j at all points and select the m points with the highest values. However, the method is over-sensitive to high frequency variations.
Reference: [18] <author> Martin Heller. </author> <title> Triangulation algorithms for adaptive terrain modeling. </title> <booktitle> In Proc. 4th Intl. Symp. on Spatial Data Handling, </booktitle> <volume> volume 1, </volume> <pages> pages 163-174, </pages> <address> Zurich, </address> <year> 1990. </year>
Reference-contexts: The words "sequential" and "parallel" here refer to the selection and re-evaluation process, not to the architecture of the machine. Many variations on the greedy insertion algorithm have been explored over the years; apparently the algorithm has been reinvented many times <ref> [10, 7, 18, 31, 28, 11, 30] </ref>. We now explore four variants of the sequential greedy insertion algorithm. Our first method, 7 algorithm I, is a brute force implementation of sequential greedy insertion with Delaunay triangu-lation. This algorithm is quite slow, so we present two optimizations. <p> This cost can be eliminated if the vertex set is seeded in a hybrid feature/refinement approach, or if exhaustive scan conversion is avoided. The challenge is to speed the algorithm without sacrificing approximation quality. One approach is to choose initial points using variable-window feature selection <ref> [18] </ref>. Another is to initialize a candidate point to the triangle's centroid, then use a few iterations of hill climbing to move it to the point of locally maximum error [10]. A third approach is to scan convert with sub-sampling, only examining every kth point in x and y. <p> That is, the xy projection of the input points need not form a rectangular grid, but could be any finite point set. This change would require each triangle to store a set of points <ref> [6, 18, 11] </ref>. During re-triangulation, these sets would be merged and split. Instead of scan converting a triangle, one would visit all the points in that triangle's point set. <p> Part of Heller's adaptive triangular mesh filtering technique is a greedy insertion algorithm 33 employing heaps and local recalculation. That portion of his algorithm appears nearly identical to ours in quality and asymptotic complexity <ref> [18, p. 168] </ref>. Because the initial pass of his algorithm uses feature selection, we suspect, however, that his could be faster but that our method will produce somewhat higher quality approximations.
Reference: [19] <author> Hugues Hoppe, Tony DeRose, Tom Duchamp, John McDonald, and Werner Stuetzle. </author> <title> Mesh optimization. </title> <booktitle> In SIGGRAPH '93 Proc., </booktitle> <pages> pages 19-26, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Pavlidis employed a similar split-and-merge method for curves [27, p. 181], while Schmitt and Chen used a related method for surfaces [34]. This approach would eliminate the unnecessary vertices from 31 algorithm of Hoppe et al. <ref> [19] </ref>, it should be quite a bit faster since we already know how to do many of the steps quickly.
Reference: [20] <author> Michael Jones. </author> <title> Lessons learned from visual simulation. </title> <booktitle> In SIGGRAPH '94 Course Notes CD-ROM, Course 14: Designing Real-Time Graphics for Entertainment, </booktitle> <pages> pages 39-71. </pages> <publisher> ACM SIGGRAPH, </publisher> <month> July </month> <year> 1994. </year>
Reference-contexts: Such a system would need to support division of a huge terrain into blocks, precomputation of multiple levels of detail, adaptive selection of level of detail during display, and blending between levels of detail. Much of this work has already been done by others <ref> [28, 20] </ref>. Applications in Computer Vision. Finally, these algorithms can be used for the simplification of range data in computer vision.
Reference: [21] <author> Charles L. Lawson. </author> <title> Software for C 1 surface interpolation. </title> <editor> In John R. Rice, editor, </editor> <booktitle> Mathematical Software III, </booktitle> <pages> pages 161-194. </pages> <publisher> Academic Press, </publisher> <address> NY, </address> <year> 1977. </year> <booktitle> (Proc. of symp., </booktitle> <address> Madison, WI, </address> <month> Mar. </month> <year> 1977). </year>
Reference-contexts: Two such general triangulation methods are Delaunay triangulation and data-dependent triangulation. Delaunay triangulation is a purely two-dimensional method; it uses only the xy projections of the input points. It finds a triangulation that maximizes the minimum angle of all triangles, among all triangulations of a given point set <ref> [13, 21] </ref>. This helps to minimize the occurrence of very thin sliver triangles. Data-dependent triangulation, in contrast, uses the heights of points in addition to their x and y coordinates [8, 31]. It can achieve lower error approximations than Delaunay triangulation, but it generates more slivers. 2. <p> Worst Case Cost. The cost of location, L, and the cost of insertion, I, are mesh-dependent. A planar triangulation with v vertices total, v b of them on the boundary, will have 3vv b 3 edges and 2vv b 2 triangles <ref> [21] </ref>. Typically, v b = O ( p v), in which case the number of edges is approximately 3v and the number of triangles is about 2v, but in any case, the number of edges and the number of triangles are each O (v). <p> For point location, we can use the simple "walking method" due to Guibas-Stolfi, Green-Sibson, and Lawson [13, p. 121], [12], <ref> [21] </ref>. This algorithm starts on an edge of the mesh, and walks through the mesh toward the target point until it arrives at the target. <p> Fortunately, we can improve upon our original naive algorithm considerably by exploiting the locality of the changes to the approximation during incremental Delaunay triangulation. 4.2.1. Delaunay Triangulation The incremental Delaunay triangulation algorithm is illustrated in Figure 4, and works as follows <ref> [13, 21] </ref>: To insert a vertex A, locate its containing triangle, or, if it lies on an edge, delete that edge and find its containing quadrilateral. Add "spoke" edges from A to the vertices of this containing polygon. <p> The vertex to insert in the triangulation on each pass is chosen as before, but the triangulation is done differently. The incremental Delaunay triangulation algorithm of section 4.2.1 tested suspect edges using a purely two-dimensional geometric test involving circumcircles. A generalization of this approach, Lawson's local optimization procedure <ref> [21] </ref>, uses other tests. For data-dependent triangulation, instead of checking the validity of an edge with the circle test, the rule we adopt is that an edge is swapped if the change decreases the error of the approximation. We defer the definition of "error" until later. <p> Randomization fixes the problem. 4.5.3. A Problem with Location In the process of testing this algorithm, we discovered that on some of our data-dependent triangulations, Guibas and Stolfi's walking method for point location [13, p. 121], [12], <ref> [21] </ref> would occasionally loop forever. This was not an implementation error, but a fundamental bug in that algorithm for certain triangulations (see Figure 6). We have never seen this problem arise in algorithms I-III, so we conjecture that it only occurs for non-Delaunay triangulations.
Reference: [22] <author> Jay Lee. </author> <title> A drop heuristic conversion method for extracting irregular network for digital elevation models. </title> <booktitle> In GIS/LIS '89 Proc., </booktitle> <volume> volume 1, </volume> <pages> pages 30-39. </pages> <booktitle> American Congress on Surveying and Mapping, </booktitle> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: Decimation Algorithms We next investigated decimation variants of two of these measures. The curvature measure has no decimation variant since it is not iterative, and decimation with the local error measure is essentially the same as Lee's drop heuristic algorithm <ref> [22] </ref>. We tested decimation variants of the global error measure and the product measure on curves. The results were interesting, but ultimately poorer than other methods. We found that the global error measure produced more accurate approximations when used in a decimation algorithm than in a refinement algorithm. <p> A technique that might permit better approximations of cliffs and better selection in the presence of noise is to alternate refinement and decimation passes, inserting several vertices with the greedy insertion approach, and then deleting a few vertices that appear the least important, using Lee's drop heuristic approach <ref> [22] </ref>. Pavlidis employed a similar split-and-merge method for curves [27, p. 181], while Schmitt and Chen used a related method for surfaces [34].
Reference: [23] <author> Dani Lischinski. </author> <title> Incremental Delaunay triangulation. </title> <editor> In Paul Heckbert, editor, </editor> <booktitle> Graphics Gems IV, </booktitle> <pages> pages 47-59. </pages> <publisher> Academic Press, </publisher> <address> Boston, </address> <year> 1994. </year>
Reference-contexts: Results We have implemented all four algorithms above. Our combined implementation of algorithms III and IV consists of about 5,200 lines of C++. The incremental Delaunay triangulation module is adapted from Lischinski's code <ref> [23] </ref>. Figures 7-10 are a demonstration of algorithm III on a digital elevation model (DEM) for the western half of Crater Lake. Figure 7 shows the full DEM dataset (a rectangular grid with each quadrilateral split into two triangles).
Reference: [24] <author> David Marr. </author> <title> Vision. </title> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1982. </year>
Reference-contexts: The filtering pass removed small fluctuations but left large features intact <ref> [24] </ref>. This scheme appeared promising in several respects. First, the algorithm was much faster than iterative refinement techniques because the final approximation can be constructed in a single pass. Second, by increasing the standard deviation of the Gaussian kernel, the smoothing preprocess could smooth away progressively larger details.
Reference: [25] <author> Donald E. McClure and S. C. Shwartz. </author> <title> A method of image representation based on bivariate splines. </title> <type> Technical report, </type> <institution> Center for Intelligent Control Systems, MIT, </institution> <month> Mar. </month> <year> 1989. </year> <month> CICS-P-113. </month>
Reference-contexts: Similar curvature measures have been explored for approximation purposes by others <ref> [25, 33, 32] </ref>. We do not expect such measures to yield high quality approximations, however, since the curvature approach did not work well for curves. 3.3. Global Error Measure We next tested the global error, or sum of errors over all points, as an importance measure.
Reference: [26] <author> Edmond Nadler. </author> <title> Piecewise linear best L 2 approximation on triangulations. </title> <editor> In C. K. Chui et al., editors, </editor> <booktitle> Approximation Theory V, </booktitle> <pages> pages 499-502, </pages> <address> Boston, 1986. </address> <publisher> Academic Press. </publisher>
Reference-contexts: At a coarse level, the RMS error decreases quite rapidly initially and then slowly approaches 0. In the limit as m ! 1, the error of the L 2 -optimal triangulation converges as m 1 <ref> [26] </ref>, but this empirical data is better fit by the function m :7 . In the early phases of the algorithm, the error fluctuates rather chaotically, but it settles into a more stable decline.
Reference: [27] <author> Theodosios Pavlidis. </author> <title> Structural Pattern Recognition. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1977. </year>
Reference-contexts: Pavlidis employed a similar split-and-merge method for curves <ref> [27, p. 181] </ref>, while Schmitt and Chen used a related method for surfaces [34]. This approach would eliminate the unnecessary vertices from 31 algorithm of Hoppe et al. [19], it should be quite a bit faster since we already know how to do many of the steps quickly.
Reference: [28] <author> Michael F. Polis and David M. McKeown, Jr. </author> <title> Issues in iterative TIN generation to support large scale simulations. </title> <booktitle> In Proc. of Auto-Carto 11 (Eleventh Intl. Symp. on Computer-Assisted Cartography), </booktitle> <pages> pages 267-277, </pages> <month> November </month> <year> 1993. </year> <note> URL=http://www.cs.cmu.edu/~MAPSLab. 35 </note>
Reference-contexts: The words "sequential" and "parallel" here refer to the selection and re-evaluation process, not to the architecture of the machine. Many variations on the greedy insertion algorithm have been explored over the years; apparently the algorithm has been reinvented many times <ref> [10, 7, 18, 31, 28, 11, 30] </ref>. We now explore four variants of the sequential greedy insertion algorithm. Our first method, 7 algorithm I, is a brute force implementation of sequential greedy insertion with Delaunay triangu-lation. This algorithm is quite slow, so we present two optimizations. <p> Crater Lake DEM. Data-dependent triangulation is slower, but higher quality. Data points are marked with m, the number of ver tices selected. to the approach of Polis et al. <ref> [28] </ref>). If ff = 0, fractional thresholding becomes highly aggressive and selects every triangle candidate. Looking at the error graphs in Figures 17 and 18, we can see that as ff increases towards 1, the approximations become more accurate and converge to sequential insertion. <p> In addition to specifying critical features which will be maintained in the approximation, the user can also be given discretionary control over point selection. The system could be trivially extended to support selective fidelity <ref> [28] </ref>, allowing an importance weight to be assigned to each vertex. Importance values computed at a vertex would be scaled by the specified weight. Weight might also be represented as a function of some sort; for instance, weight might be a function of height or slope. <p> Such a system would need to support division of a huge terrain into blocks, precomputation of multiple levels of detail, adaptive selection of level of detail during display, and blending between levels of detail. Much of this work has already been done by others <ref> [28, 20] </ref>. Applications in Computer Vision. Finally, these algorithms can be used for the simplification of range data in computer vision.
Reference: [29] <author> Franco P. Preparata and Michael I. Shamos. </author> <title> Computational Geometry: an Introduction. </title> <publisher> Springer--Verlag, </publisher> <address> New York, NY, </address> <year> 1985. </year>
Reference-contexts: Guibas and Stolfi recommended this method for point location in the context of Delaunay triangulation, so perhaps they had never tested it on more general triangulations. Of course, there are algorithms for point location among m triangles that operate in O (log m) time <ref> [29] </ref> that we could use instead, but since the walking method requires only a few lines of code and no preprocessing or extra memory, and it has expected cost of O (1) with the query patterns typical of our algorithms, there is little practical reason to abandon it.
Reference: [30] <author> Enrico Puppo, Larry Davis, Daniel DeMenthon, and Y. Ansel Teng. </author> <title> Parallel terrain triangulation. </title> <journal> Intl. J. of Geographical Information Systems, </journal> <volume> 8(2) </volume> <pages> 105-128, </pages> <year> 1994. </year>
Reference-contexts: The words "sequential" and "parallel" here refer to the selection and re-evaluation process, not to the architecture of the machine. Many variations on the greedy insertion algorithm have been explored over the years; apparently the algorithm has been reinvented many times <ref> [10, 7, 18, 31, 28, 11, 30] </ref>. We now explore four variants of the sequential greedy insertion algorithm. Our first method, 7 algorithm I, is a brute force implementation of sequential greedy insertion with Delaunay triangu-lation. This algorithm is quite slow, so we present two optimizations. <p> show the results obtained by the sequential and the parallel algorithm : : : Because of the more even refinement of the TIN, which is due to the introduction of many points before the Delaunay optimization, our [parallel] approach needs considerably fewer points to achieve the same level of precision" <ref> [30, p. 123] </ref>. We tested this claim by comparing our sequential greedy insertion algorithm against two variants of parallel insertion. Both variants select and insert all candidate points p such that Error (p) *, where * is a threshold value. <p> Looking at the error graphs in Figures 17 and 18, we can see that as ff increases towards 1, the approximations become more accurate and converge to sequential insertion. The second insertion variant is the rule used by the latter passes of Fowler-Little and by Puppo et al. <ref> [10, 30] </ref>. We call it constant threshold parallel insertion. In this case, * is the constant error threshold provided by the user. Thus, on each pass we select and insert all candidate points that do not meet the error tolerance. <p> Because the initial pass of his algorithm uses feature selection, we suspect, however, that his could be faster but that our method will produce somewhat higher quality approximations. We have tested our sequential greedy insertion algorithm against the parallel greedy insertion algorithms of Fowler-Little and Puppo et al. <ref> [10, 30] </ref>, and found that sequential greedy insertion yields superior approximations in all cases tested. It appears that the sequential method uses vertices more carefully and leads to less unnecessary subdivision in the mesh.
Reference: [31] <author> Shmuel Rippa. </author> <title> Adaptive approximation by piecewise linear polynomials on triangulations of subsets of scattered data. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 13(5) </volume> <pages> 1123-1141, </pages> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: This helps to minimize the occurrence of very thin sliver triangles. Data-dependent triangulation, in contrast, uses the heights of points in addition to their x and y coordinates <ref> [8, 31] </ref>. It can achieve lower error approximations than Delaunay triangulation, but it generates more slivers. 2. Statement of Problem and Approach We assume that a discrete two-dimensional set of samples H of some underlying surface is provided. <p> The words "sequential" and "parallel" here refer to the selection and re-evaluation process, not to the architecture of the machine. Many variations on the greedy insertion algorithm have been explored over the years; apparently the algorithm has been reinvented many times <ref> [10, 7, 18, 31, 28, 11, 30] </ref>. We now explore four variants of the sequential greedy insertion algorithm. Our first method, 7 algorithm I, is a brute force implementation of sequential greedy insertion with Delaunay triangu-lation. This algorithm is quite slow, so we present two optimizations. <p> and interpolate there Insert (Point p ): mark p as used Mesh Insert (p) Error (Point p ): % Returns the error at a point (our importance measure) return jH (p) Locate and Interpolate (p)j The heart of the algorithm is sequential greedy insertion, much as described in earlier work <ref> [7, 31, 11] </ref>. It is simple and unoptimized. We build an initial approximation of two triangles using the corner points of H. Then we repeatedly scan the unused points to find the one with the largest error and call Insert to add it to the current approximation. <p> Data-dependent variants of the greedy insertion algorithms described can be created by replacing Delaunay triangulation with data-dependent triangulation, as discussed by Rippa and Hamann-Chen <ref> [31, 15] </ref>. The vertex to insert in the triangulation on each pass is chosen as before, but the triangulation is done differently. The incremental Delaunay triangulation algorithm of section 4.2.1 tested suspect edges using a purely two-dimensional geometric test involving circumcircles. <p> But sometimes these slivers do not fit the data well, and lead to globally inaccurate approximations. One approach which has been used to combat slivers is a hybrid of data-dependent and Delaunay triangulation <ref> [31] </ref>. We wished to find a more elegant solution, however. <p> Compared to previous work, our Delaunay greedy insertion algorithm yields nearly identical approximations to those of De Floriani et al., Rippa, and Franklin <ref> [7, 31, 11] </ref>, but from the information available, it appears that our algorithm is the fastest both in theory and in practice. Part of Heller's adaptive triangular mesh filtering technique is a greedy insertion algorithm 33 employing heaps and local recalculation.
Reference: [32] <author> Lori Scarlatos. </author> <title> Spatial Data Representations for Rapid Visualization and Analysis. </title> <type> PhD thesis, </type> <institution> CS Dept, State U. of New York at Stony Brook, </institution> <year> 1993. </year>
Reference-contexts: Similar curvature measures have been explored for approximation purposes by others <ref> [25, 33, 32] </ref>. We do not expect such measures to yield high quality approximations, however, since the curvature approach did not work well for curves. 3.3. Global Error Measure We next tested the global error, or sum of errors over all points, as an importance measure.
Reference: [33] <author> Lori L. Scarlatos and Theo Pavlidis. </author> <title> Optimizing triangulations by curvature equalization. </title> <booktitle> In Proc. Visualization '92, </booktitle> <pages> pages 333-339. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1992. </year>
Reference-contexts: Similar curvature measures have been explored for approximation purposes by others <ref> [25, 33, 32] </ref>. We do not expect such measures to yield high quality approximations, however, since the curvature approach did not work well for curves. 3.3. Global Error Measure We next tested the global error, or sum of errors over all points, as an importance measure.
Reference: [34] <author> Francis Schmitt and Xin Chen. </author> <title> Fast segmentation of range images into planar regions. </title> <booktitle> In Conf. on Computer Vision and Pattern Recognition (CVPR '91), </booktitle> <pages> pages 710-711. </pages> <publisher> IEEE Comp. Soc. Press, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: Cliffs similar to this arise when computer vision range data is approximated with this algorithm (this can be seen in the pictures of Schmitt and Chen <ref> [34] </ref>). This problem is definitely caused by the short-sightedness of greedy insertion. One, somewhat ad hoc, solution is to find all cliffs and constrain the triangulation to follow them [1]. Dealing With Noise and High Frequencies. <p> Pavlidis employed a similar split-and-merge method for curves [27, p. 181], while Schmitt and Chen used a related method for surfaces <ref> [34] </ref>. This approach would eliminate the unnecessary vertices from 31 algorithm of Hoppe et al. [19], it should be quite a bit faster since we already know how to do many of the steps quickly.
Reference: [35] <author> David A. Southard. </author> <title> Piecewise planar surface models from sampled data. </title> <editor> In N. M. Patrikalakis, editor, </editor> <booktitle> Scientific Visualization of Physical Phenomena, </booktitle> <pages> pages 667-680, </pages> <address> Tokyo, 1991. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: These extensions allow our algorithm to be used to simplify terrains with color texture or planar color images <ref> [35] </ref>. The output of such a simplification is a triangulated surface with linear interpolation of color across each triangle. Such models are ideally suited for hardware-assisted Gouraud shading [9] on most graphics workstations, and are a possible substitute for texture mapping when that is not supported by hardware. 5.
Reference: [36] <author> Michael Spivak. </author> <title> A Comprehensive Introduction to Differential Geometry. </title> <address> Publish or Perish, </address> <publisher> Inc., </publisher> <year> 1979. </year>
Reference-contexts: A better measure is the sum of the squares of the principal curvatures [37], which can be computed as the square of the Frobenius norm of the Hessian matrix (known as the second fundamental form in differential geometry <ref> [36] </ref>): (@ 2 H=@x 2 ) 2 + 2 (@ 2 H=@x@y) 2 + (@ 2 H=@y 2 ) 2 . Similar curvature measures have been explored for approximation purposes by others [25, 33, 32].
Reference: [37] <author> William Welch. </author> <title> Serious Putty: Topological Design for Variational Curves and Surfaces. </title> <type> PhD thesis, </type> <institution> CS Dept, Carnegie Mellon U., </institution> <year> 1995. </year> <month> 36 </month>
Reference-contexts: Consider H (x; y) = ax 2 ay 2 , for any a, for example. A better measure is the sum of the squares of the principal curvatures <ref> [37] </ref>, which can be computed as the square of the Frobenius norm of the Hessian matrix (known as the second fundamental form in differential geometry [36]): (@ 2 H=@x 2 ) 2 + 2 (@ 2 H=@x@y) 2 + (@ 2 H=@y 2 ) 2 .
References-found: 38

