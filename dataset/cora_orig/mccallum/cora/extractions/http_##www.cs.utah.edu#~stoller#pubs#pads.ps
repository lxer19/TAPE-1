URL: http://www.cs.utah.edu/~stoller/pubs/pads.ps
Refering-URL: http://www.cs.utah.edu/~stoller/pubs/pubs.html
Root-URL: 
Email: E-mail: fswanson,stollerg@cs.utah.edu  
Title: Shared Memory as a Basis for Conservative Distributed Architectural Simulation 1  
Author: Mark R. Swanson Leigh B. Stoller 
Affiliation: Department of Computer Science University of Utah  
Pubnum: UUCS-97-005  
Abstract: This paper describes experience in parallelizing an execution-driven architectural simulation system used in the development and evaluation of the Avalanche distributed architecture. It reports on a specific application of conservative distributed simulation on a shared memory platform. Various communication-intensive synchronization algorithms are described and evaluated. Performance results on a bus-based shared memory platform are reported, and extension and scalability of the implementation to larger distributed shared memory configurations are discussed. Also addressed are specific characteristics of architectural simulations that contribute to decisions relating to the conservatism of the approach and to the achievable performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brewer, E., Dellarocas, C., Colbrook, A., and Weihl, W. PROTEUS: </author> <title> A High-Performance Parallel Architecture Simulator. </title> <type> Tech. Rep. </type> <institution> MIT/LCS/TR-516, Massachusetts Institute of Technology, </institution> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: The main disadvantages of the WWT is the dependence of the simulation environment on the CM-5 hardware and the lack of flexibility in modifying many aspects of the architecture due to its direct execution nature. Parallel Proteus <ref> [1] </ref> performs direct execution simulation, using a conservative time window approach. To overcome a small lookahead size resulting from switch level simulation, they use local barriers and predictive barrier scheduling. Local barriers use a nearest neighbor approach to reduce the number of nodes each processor must synchronize with.
Reference: [2] <author> Chandra, S., Larus, J. R., and Rogers, A. </author> <title> Where is Time Spend in Message Passing and Shared-Memory Programs? In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems (Nov. </title> <booktitle> 1994), </booktitle> <pages> pp. 61-75. </pages>
Reference-contexts: These are modified versions of programs used by <ref> [2] </ref>. The modifications consisted of replacing the message passing libraries with ones based on Direct Deposit [8], a protocol suite developed for the Avalanche system. SOR-sync performs two basic kinds of communication at each time step: a global reduction and accumulation and broadcast of a solution vector.
Reference: [3] <author> Chandrasekaran, S., and Hill, M. D. </author> <title> Optimistic Simulation of Parallel Architectures Using Program Executables. </title> <booktitle> In Proceedings of the 10th Workshop on Parallel and Distributed Simulation (May 1996), </booktitle> <pages> pp. 143-150. </pages>
Reference-contexts: Little effort was made in this direction in the work reported here; it will be undertaken when availability of a larger platform makes it practical. 6 Related Work Numerous groups have developed simulators for multiprocessor architectures. A few of them are surveyed here. The Wisconsin Wind Tunnel <ref> [3] </ref> uses direct execution of instrumented programs on a CM-5, resulting in very fast and accurate simulation. Interactions between nodes occur only when programs access shared memory locations, which are translated by the underlying simulation system into message passing events between CM-5 nodes.
Reference: [4] <author> Dickens, P. M., Heidelberger, P., and Nicol, D. M. </author> <title> Parallelized network simulators for message-passing parallel programs. </title> <booktitle> In MASCOTS 95 (Jan. </booktitle> <year> 1995). </year>
Reference-contexts: This is another example of increased lookahead, and works well when the simulated processes engage in long computational periods between communication. Both compile-time and runtime analysis are employed to predict when simulated processors are going to communicate. LAPSE <ref> [4] </ref> is another simulator that performs direct execution of instrumented programs, in this 11 case message passing programs. The granularity of synchronization is larger since there are larger periods of execution between message events. Parallel Embra [10] is the simulator most closely resembling PPAint.
Reference: [5] <author> Hewlett-Packard Co. </author> <title> PA-RISC 1.1 Architecture and Instruction Set Reference Manual, </title> <month> February </month> <year> 1994. </year>
Reference-contexts: To expose these effects, a detailed and quite fine grained simulation is required. 2 The Base Simulation Environment The base uniprocessor simulation environment developed by the Avalanche project is comprised of a simulator for the HP PA-RISC architecture <ref> [5] </ref>, including an instruction set interpreter, and detailed simulation modules for the first level cache, the system bus, the memory controller, the network interconnect, and the communications device which is the focus of the Avalanche project's research.
Reference: [6] <author> Stoller, L. B., and Swanson, M. R. PAINT: </author> <title> PA Instruction Set Interpreter. </title> <type> Tech. Rep. </type> <institution> UUCS-96-009, University of Utah, </institution> <month> March </month> <year> 1996. </year>
Reference: [7] <author> Swanson, M. R., Davis, A., and Parker, M. </author> <title> Efficient Communication Mechanisms for Cluster Based Parallel Computing. </title> <booktitle> In Workshop on Communication and Architectural Support for Network-based Parallel Computing (CANPC 97) (February 1997), vol. 1199 of Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 1-15. 12 </pages>
Reference-contexts: This paper reports on such a paralleliza-tion effort and its unusual approach of performing distributed simulation within a shared memory model. 1.1 The Avalanche Architecture The Avalanche distributed system will be a cluster or network of 32 to 64 workstations <ref> [7] </ref> interconnected with a Myrinet network. Its unique aspects lie in providing a communications interface supporting extremely efficient message passing and distributed shared memory (DSM) and designing that interface to plug in to commodity workstations. All interactions between processors occur as communications over the Myrinet via this interface.
Reference: [8] <author> Swanson, M. R., and Stoller, L. B. </author> <title> Direct Deposit: A Basic User-Level Protocol for Carpet Clusters. </title> <type> Tech. Rep. </type> <institution> UUCS-95-003, University of Utah, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: These are modified versions of programs used by [2]. The modifications consisted of replacing the message passing libraries with ones based on Direct Deposit <ref> [8] </ref>, a protocol suite developed for the Avalanche system. SOR-sync performs two basic kinds of communication at each time step: a global reduction and accumulation and broadcast of a solution vector. The reduction uses a tree rooted at simulated node 0, while the solution vector operation involves all-to-all communication.
Reference: [9] <author> Veenstra, J., and Fowler, R. MINT: </author> <title> A Front End for Efficient Simulation of Shared-Memory Multiprocessors. </title> <booktitle> In MASCOTS 1994 (Durham, </booktitle> <address> NC, </address> <month> Jan. </month> <year> 1994), </year> <pages> pp. 201-207. </pages>
Reference-contexts: This environment is called PAint (PA-interpreter)[6] and is derived from the Mint simulator <ref> [9] </ref>. The simulator is designed to model multiple nodes, consisting of the modules listed above, and the interactions between nodes, with emphasis on the effects of communication on the memory hierarchies.
Reference: [10] <author> Witchel, E., and Rosenblum, M. Embra: </author> <title> Fast and Flexible Machine Simulation. </title> <booktitle> In Proceedings of the 1996 International Conference on Parallel Processing (Aug. </booktitle> <year> 1996), </year> <pages> pp. 99-107. </pages>
Reference-contexts: LAPSE [4] is another simulator that performs direct execution of instrumented programs, in this 11 case message passing programs. The granularity of synchronization is larger since there are larger periods of execution between message events. Parallel Embra <ref> [10] </ref> is the simulator most closely resembling PPAint. It, too, executes on a shared memory platform. It differs from PPAint in using a largely direct-execution model, though mechanisms are provided to alter the model of most architectural features.
References-found: 10

