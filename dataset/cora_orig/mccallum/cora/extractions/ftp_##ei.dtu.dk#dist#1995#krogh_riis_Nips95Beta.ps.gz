URL: ftp://ei.dtu.dk/dist/1995/krogh_riis_Nips95Beta.ps.gz
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Title: Prediction of Beta Sheets in Proteins  
Author: Anders Krogh Stren Kamaric Riis 
Address: Hinxton, Cambs CB10 1RQ, UK.  Building 349  2800 Lyngby, Denmark  
Affiliation: The Sanger Centre  Electronics Institute,  University of Denmark  
Note: To appear in Advances in Neural Information Processing Systems 8 Edited by D S Touretzky, M C Mozer and M E Hasselmo. MIT Press,  
Pubnum: Technical  
Email: Email: krogh@sanger.ac.uk  Email: riis@ei.dtu.dk  
Date: 1996  
Abstract: Most current methods for prediction of protein secondary structure use a small window of the protein sequence to predict the structure of the central amino acid. We describe a new method for prediction of the non-local structure called fi-sheet, which consists of two or more fi-strands that are connected by hydrogen bonds. Since fi-strands are often widely separated in the protein chain, a network with two windows is introduced. After training on a set of proteins the network predicts the sheets well, but there are many false positives. By using a global energy function the fi-sheet prediction is combined with a local prediction of the three secondary structures ff-helix, fi-strand and coil. The energy function is minimized using simulated annealing to give a final prediction.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Branden and J. Tooze, </author> <title> Introduction to Protein Structure (Garland Publishing, </title> <publisher> Inc., </publisher> <address> New York, </address> <year> 1991). </year>
Reference-contexts: 1 INTRODUCTION Proteins are long sequences of amino acids. There are 20 different amino acids with varying chemical properties, e.g., some are hydrophobic (dislikes water) and some are hydrophilic <ref> [1] </ref>. It is convenient to represent each amino acid by a letter and the sequence of amino acids in a protein (the primary structure) can be written as a string with a typical length of 100 to 500 letters.
Reference: [2] <author> N. Qian and T. Sejnowski, </author> <note> Journal of Molecular Biology 202, 865 (1988). </note>
Reference-contexts: As an important step on the way a lot of work has been devoted to predicting the local conformation of the protein chain, which is called the secondary structure. Neural network methods are currently the most successful for predicting secondary structure. The approach was pioneered by Qian and Sejnowski <ref> [2] </ref> and Bohr et al. [3], but later extended in various ways, see e.g. [4] for an overview. In most of this work, only the two regular secondary structure elements ff-helix and fi-strand are being distinguished, and everything else is labeled coil. Thus, the methods based of the protein.
Reference: [3] <author> H. Bohr et al., </author> <note> FEBS Letters 241, 223 (1988). </note>
Reference-contexts: Neural network methods are currently the most successful for predicting secondary structure. The approach was pioneered by Qian and Sejnowski [2] and Bohr et al. <ref> [3] </ref>, but later extended in various ways, see e.g. [4] for an overview. In most of this work, only the two regular secondary structure elements ff-helix and fi-strand are being distinguished, and everything else is labeled coil. Thus, the methods based of the protein.
Reference: [4] <author> S. Riis and A. Krogh, </author> <note> Nordita Preprint 95/34 S, submitted to J. Comp. Biol. </note>
Reference-contexts: Neural network methods are currently the most successful for predicting secondary structure. The approach was pioneered by Qian and Sejnowski [2] and Bohr et al. [3], but later extended in various ways, see e.g. <ref> [4] </ref> for an overview. In most of this work, only the two regular secondary structure elements ff-helix and fi-strand are being distinguished, and everything else is labeled coil. Thus, the methods based of the protein. <p> This coding ensures that the input representations are uncorrelated, but it is a very inefficient coding, since 20 amino acids could in principle be represented by only 5 bit. Therefore, we use weight sharing [8] to learn a better encoding <ref> [4] </ref>. The 20 input units corresponding to one window position are fully connected to three hidden units. <p> Of course the number of units for this encoding can be varied, but initial experiments showed that 3 was optimal <ref> [4] </ref>. The two windows of the network are made the same way with the same number of inputs etc.. The first layer of hidden units in the two windows are fully connected to a hidden layer which is fully connected to the output unit, see fig. 2. <p> In order to combine local methods with the non-local fi-sheet prediction, we introduce a global energy function as described below. 3 A GLOBAL ENERGY FUNCTION We use a newly developed local neural network method based on one input window <ref> [4] </ref> to give an initial prediction of the three possible structures. The output from this network is constrained by softmax [11], and can thus be interpreted as the probabilities for each of the three structures. <p> The fi-sheets are predicted by ^ T a and ^ T p . Using the above scheme, an average secondary structure accuracy of 66.5% is obtained by seven-fold cross validation. This should be compared to 66.3% obtained by the local neural network based method <ref> [4] </ref> on the same data set. Although these preliminary results do not represent a significant improvement, we consider them very encouraging for future work.
Reference: [5] <author> B. Rost, C. Sander, and R. Schneider, </author> <title> J Mol. </title> <journal> Biol. </journal> <volume> 235, </volume> <month> 13 </month> <year> (1994). </year>
Reference-contexts: Current predictions of secondary structure based on single sequences as input have accuracies of about 65-66%. It is widely believed that this accuracy is close to the limit of what can be done from a local window (using only single sequences as input) <ref> [5] </ref>, because interactions between amino acids far apart in the protein chain are important to the structure. A good example of such non-local interactions are the fi-sheets consisting of two or more fi-strands interconnected by H-bonds, see fig. 1.
Reference: [6] <author> T. </author> <title> Hubbard, </title> <booktitle> in Proc. of the 27th HICSS, edited by R. </booktitle> <publisher> Lathrop (IEEE Computer Soc. Press, </publisher> <year> 1994), </year> <pages> pp. 336-354. </pages>
Reference-contexts: The aim of this work is to improve prediction of secondary structures by combining local predictions of ff-helix, fi-strand and coil with a non-local method predicting fi-sheets. Other work along the same directions include <ref> [6] </ref> in which fi-sheet predictions are done by linear methods and [7] where a so-called density network is applied to the problem. 2 A NEURAL NETWORK WITH TWO WINDOWS We aim at capturing correlations in the fi-sheets by using a neural network with two windows, see fig. 2.
Reference: [7] <author> D. J. C. MacKay, </author> <title> in Maximum Entropy and Bayesian Methods, </title> <address> Cambridge 1994, </address> <note> edited by J. </note> <editor> Skilling and S. </editor> <publisher> Sibisi (Kluwer, </publisher> <address> Dordrecht, </address> <year> 1995). </year>
Reference-contexts: The aim of this work is to improve prediction of secondary structures by combining local predictions of ff-helix, fi-strand and coil with a non-local method predicting fi-sheets. Other work along the same directions include [6] in which fi-sheet predictions are done by linear methods and <ref> [7] </ref> where a so-called density network is applied to the problem. 2 A NEURAL NETWORK WITH TWO WINDOWS We aim at capturing correlations in the fi-sheets by using a neural network with two windows, see fig. 2.
Reference: [8] <editor> Y. Le Cun et al., </editor> <booktitle> Neural Computation 1, </booktitle> <month> 541 </month> <year> (1989). </year>
Reference-contexts: This coding ensures that the input representations are uncorrelated, but it is a very inefficient coding, since 20 amino acids could in principle be represented by only 5 bit. Therefore, we use weight sharing <ref> [8] </ref> to learn a better encoding [4]. The 20 input units corresponding to one window position are fully connected to three hidden units.
Reference: [9] <author> B. Rost and C. Sander, </author> <type> Proteins 19, </type> <month> 55 </month> <year> (1994). </year>
Reference-contexts: Furthermore, two structurally identical networks are used: one for parallel and one for anti-parallel fi-sheets. The basis for the training set in this study is the set of 126 non-homologous protein chains used in <ref> [9] </ref>, but chains forming fi-sheets with other chains are excluded. This leaves us with 85 proteins in our data set. For a protein of length L only a very small fraction of the L (L 1)=2 pairs are positive examples of fi-sheet pairs.
Reference: [10] <author> F. Bernstein et al., </author> <title> J Mol. </title> <journal> Biol. </journal> <volume> 112, </volume> <month> 535 </month> <year> (1977). </year>
Reference-contexts: Notice that the correct structure are lines parallel to the diagonal, whereas they are perpendicular for anti-parallel sheets. For both cases the network was trained on a training set that did not contain the protein for which the result is shown. <ref> [10] </ref>. First of all, one notices the checker board structure of the prediction of fi-sheets. This is related to the structure of fi-sheets. Many sheets are hydrophobic on one side and hydrophilic on the other.
Reference: [11] <author> J. Bridle, </author> <booktitle> in Neural Information Processing Systems 2, edited by D. </booktitle> <publisher> Touretzky (Mor-gan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990), </year> <pages> pp. 211-217. </pages>
Reference-contexts: The output from this network is constrained by softmax <ref> [11] </ref>, and can thus be interpreted as the probabilities for each of the three structures.
Reference: [12] <author> K. Fisher and J. </author> <title> Hertz, </title> <publisher> Spin glasses (Cambridge University Press, </publisher> <year> 1991). </year>
Reference-contexts: The total energy function (4) has some resemblance with a so-called Potts glass in an external field <ref> [12] </ref>. The crucial difference is that the couplings between the `spins' s i are dependent on the dynamic variables T .
Reference: [13] <author> D. Ackley, G. Hinton, and T. Sejnowski, </author> <booktitle> Cognitive Science 9, </booktitle> <month> 147 </month> <year> (1985). </year>
Reference-contexts: These parameters were estimated by a method inspired by Boltzmann learning <ref> [13] </ref>. In the Boltzmann machine the estimation of the weights can be formulated as a minimization of the difference between the free energy of the `clamped' system and that of the `free-running' system [14].
Reference: [14] <author> J. Hertz, A. Krogh, and R. Palmer, </author> <title> Introduction to the Theory of Neural Computation (Addison-Wesley, </title> <address> Redwood City, </address> <year> 1991). </year>
Reference-contexts: These parameters were estimated by a method inspired by Boltzmann learning [13]. In the Boltzmann machine the estimation of the weights can be formulated as a minimization of the difference between the free energy of the `clamped' system and that of the `free-running' system <ref> [14] </ref>.
Reference: [15] <author> R. Frost, SDSC EBSA, </author> <title> C Library Documentation, </title> <note> version 2.1. SDSC Techreport. </note>
Reference-contexts: This was done using simulated annealing and the EBSA package <ref> [15] </ref>. The total procedure for prediction is, 1. A neural net predicts ff-helix, fi-strand or coil. The logarithm of these predictions give all the h i;n for that protein. 2. The two-window neural networks predict the fi-sheets.
References-found: 15

