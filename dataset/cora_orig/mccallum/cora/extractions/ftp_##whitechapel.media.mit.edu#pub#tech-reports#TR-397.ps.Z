URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-397.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Title: Augmented Reality Through Wearable Computing  
Author: Thad Starner, Steve Mann, Bradley Rhodes, Jeffrey Levine, Jennifer Healey, Dana Kirsch, Rosalind W. Picard, and Alex Pentland 
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: Room E15-383, The Media Laboratory Massachusetts Institute of Technology  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 397 To appear: Presence, Special Issue on Augmented Reality, 1997 Abstract Wearable computing moves computation from the desktop to the user. We are forming a community of networked wearable computer users to explore, over a long period, the augmented realities that these systems can provide. By adapting its behavior to the user's changing environment, a body-worn computer can assist the user more intelligently, consistently, and continuously than a desktop system. A text-based augmented reality, the Remembrance Agent, is presented to illustrate this approach. Video cameras are used both to warp the visual input (mediated reality) and to sense the user's world for graphical overlay. With a camera, the computer tracks the user's finger, which acts as the system's mouse; performs face recognition; and detects passive objects to overlay 2.5D and 3D graphics onto the real world. Additional apparatus such as audio systems, infrared beacons for sensing location, and biosensors for learning about the wearer's affect are described. Using the input from these interface devices and sensors, a long term goal of this project is to model the user's actions, anticipate his or her needs, and perform a seamless interaction between the virtual and physical en vironments.
Abstract-found: 1
Intro-found: 1
Reference: <author> Azarbayejani, A. and Pentland, A. </author> <year> (1995). </year> <title> Recursive estimation of motion, structure, and focal length. </title> <journal> IEEE PAMI, </journal> <volume> 17(6). </volume>
Reference-contexts: to reveal the presence of a user at a particular node. 3.2 3D Graphical Overlays and Active Tags When three or more tags are used on a rigid object, and the relative positions of the tags are known, 3D information about the object can be recovered using techniques developed in <ref> (Azarbayejani and Pentland, 1995) </ref>. Registered 3D graphics can be then be overlaid on the real object. Such registered graphics can be very useful in the maintenance of machinery.
Reference: <author> Baker, F. </author> <year> (1994). </year> <title> Assistive technology for the visually impaired. </title> <note> http:// www.wilmer.jhu.edu/ low vis/ lves.htm. </note>
Reference-contexts: This allows individual letters to be magnified so as to be recognizable while still providing the context cues of the surrounding imagery. Figure 14 shows how the same technique can be used to map around scotomas ("blind spots"). Until self-contained systems such as <ref> (Baker, 1994) </ref> can include the processing power necessary to perform this amount of computation, such systems can provide a general experimental platform for testing theories of low vision aids.
Reference: <author> Baum, W., Ettinger, G., White, S., Lozano-Perez, T., Wells, W., and Kikinis, R. </author> <year> (1996). </year> <title> An automatic registration method for frameless stereotaxy, image guided surgery, and enhanced reality visualization. </title> <journal> IEEE Trans. Medical Imaging, </journal> <volume> 15(2) </volume> <pages> 129-140. </pages>
Reference: <author> Damasio, A. </author> <year> (1994). </year> <title> Descartes' Error. G.P. </title> <publisher> Putnam's Sons, </publisher> <address> New York. </address>
Reference-contexts: However, a more personal and striking interface may be possible if the user's emotional affect can be sensed as well. Emotional affect plays a large part in everyday life. In fact, there is evidence that without affect, even rational intellect is impaired <ref> (Damasio, 1994) </ref>. To date, computer interfaces have mostly ignored human affect. However, wearable computers, which are in contact with their users in many different contexts, allow an unprecedented opportunity for affect sensing.
Reference: <author> Feiner, S., MacIntyre, B., and Seligmann, D. </author> <year> (1993). </year> <title> Knowledge-based augmented reality. </title> <journal> Communications of the ACM, </journal> <volume> 36(7) </volume> <pages> 52-62. </pages>
Reference-contexts: Registered 3D graphics can be then be overlaid on the real object. Such registered graphics can be very useful in the maintenance of machinery. Extending a concept by <ref> (Feiner et al., movie sequences are displayed. 1993) </ref>, Figure 10 shows 3D animated images demonstrating repair instructions for a laser printer. The registration method becomes increasingly stable with additional known feature points.
Reference: <author> Huang, T. and Netravali, A. </author> <year> (1984). </year> <title> Motion and structure from feature correspondences: a review. </title> <booktitle> Proc. of IEEE. </booktitle>
Reference-contexts: Using the "pencigraphic" imag information about the conversant can be overlaid on the visual field. to properly register, keystone, and chirp a shopping list in space using the video flowfield. ing approach (Mann, 1995), the virtual image of a rigid planar patch <ref> (Huang and Netravali, 1984) </ref> may be superimposed on to the wearer's real world visual field, creating the illusion of the virtual image floating in 3D space. Figure 12 shows six frames of video from a processed image sequence.
Reference: <author> Kraut, R., Miller, M., and Siegel, J. </author> <year> (1996). </year> <title> Collaboration in performance of physical tasks: Effects on outcomes and communication. </title> <booktitle> In forthcoming ACM Conference on Computer Supported Cooperative Work (CSCW), </booktitle> <address> Boston, MA. </address>
Reference: <author> Lamming, M. and Flynn, M. </author> <year> (1994). </year> <title> Forget-me-not: </title> <booktitle> Intimate computing in support of human memory. In FRIEND21: Inter. Symp. on Next Generation Human Interface, </booktitle> <pages> pages 125-128, </pages> <address> Meguro Gajoen, Japan. </address>
Reference-contexts: While the Remembrance Agent gains most of its contextual information from typed text, wearable computers have the potential to provide a wealth of contextual features similar to those discussed in <ref> (Lamming and Flynn, 1994) </ref>. With the systems described in later sections, additional sources of user context information may include time of day, location, biosignals, face recognition, and visual object tags. Thus, the RA begins to show the advantages of wearable, context-driven augmented reality.
Reference: <author> Mann, S. </author> <year> (1994). </year> <title> Mediated reality. </title> <type> Technical Report 260, </type> <institution> MIT Media Lab, Perceptual Computing Group. </institution>
Reference-contexts: To do this remapping, a camera is mounted on an opaque head-mounted display. The image is wirelessly transmitted to a remote computer, processed, and sent back to the head-mounted display. The process of completely controlling the user's visual field has been termed "mediated reality" <ref> (Mann, 1994) </ref> to distinguish it from the "see-through" effect generally associated with augmented reality. With off-the-shelf SGI 6 hardware, the incoming video may be remapped arbitrarily in real time. Figure 13 shows how text can be magnified by applying a simple 2D `hyper-fisheye' coordinate transformation. <p> Video is transmitted from the head mounted camera, analyzed remotely, and returned to the user's display with appropriate graphics overlaid on the image <ref> (Mann, 1994) </ref>. This method emulates the experience of having significant processing power locally and allows rapid prototyping. As techniques are proven, the code is optimized and moved to the processor (s) of the local wearable computer, if possible. Thus, concept demonstrations are gradually integrated into everyday use.
Reference: <author> Mann, S. </author> <year> (1995). </year> <title> Video orbits of the projective group; a new perspective on image mosaicing. </title> <type> Technical Report 338, </type> <institution> MIT Media Lab, Perceptual Computing Group. </institution>
Reference-contexts: Without visual tags, other methods of aligning the real and virtual are necessary. Using the "pencigraphic" imag information about the conversant can be overlaid on the visual field. to properly register, keystone, and chirp a shopping list in space using the video flowfield. ing approach <ref> (Mann, 1995) </ref>, the virtual image of a rigid planar patch (Huang and Netravali, 1984) may be superimposed on to the wearer's real world visual field, creating the illusion of the virtual image floating in 3D space. Figure 12 shows six frames of video from a processed image sequence.
Reference: <author> Nagao, K. and Rekimoto, J. </author> <year> (1995). </year> <title> Ubiquitous talker: Spoken language interaction with real world objects. </title> <booktitle> In Proc. of Inter. Joint Conf. on Artifical Intelligence (IJCAI), </booktitle> <pages> pages 1284-1290, </pages> <address> Montreal. </address>
Reference-contexts: Visual "tags" uniquely identify each active object. These tags consist of two red squares bounding a pattern of green squares representing a binary number unique to that room. A similar identification system has been demonstrated by <ref> (Nagao and Rekimoto, 1995) </ref> for a tethered, hand-held system. These visual patterns are robust in the presence of similar background colors and can be distinguished from each other in the same visual field.
Reference: <author> Nagel, D. and Lovette, J. </author> <year> (1995). </year> <title> Petition for rulemak-ing: NII band. Open letter to the FCC: </title> <note> http:// www.research.apple.com/ research/ proj/ niiband/ petition/ NII Petition.html. </note>
Reference-contexts: Until self-contained systems such as (Baker, 1994) can include the processing power necessary to perform this amount of computation, such systems can provide a general experimental platform for testing theories of low vision aids. If considerable wireless bandwidth is made available to the public, as per <ref> (Nagel and Lovette, 1995) </ref>, then this system may become practical. Since only cameras, a HMD, and a transmitter/receiver pair are needed, the apparatus can be made lightweight from off-the-shelf components.
Reference: <author> Orwant, J. </author> <year> (1993). </year> <title> Doppelganger goes to school: Machine learning for user modeling. </title> <type> Master's thesis, </type> <institution> MIT, Media Laboratory. </institution>
Reference-contexts: While simply providing a body "status line" overlay for the user's benefit is an interesting application, we hope to create a sophisticated model of the user by combining affect and environment sensing as well as pattern recognition techniques similar to <ref> (Orwant, 1993) </ref>. Through sensor data and the user model, the wearable computer can track the state of its user and adjust its behavior accordingly. For example, suppose the user is attending an important business lunch.
Reference: <author> Pentland, A., Moghaddam, B., and Starner, T. </author> <year> (1994). </year> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In IEEE Conference on Computer Vision & Pattern Recognition (CVPR'94), </booktitle> <address> Seattle, WA. </address>
Reference: <author> Picard, R. </author> <year> (1995). </year> <title> Affective computing. </title> <type> Technical Report 321, </type> <institution> MIT Media Lab, Perceptual Computing Group. </institution>
Reference-contexts: To date, computer interfaces have mostly ignored human affect. However, wearable computers, which are in contact with their users in many different contexts, allow an unprecedented opportunity for affect sensing. Picard discusses ways in which computers might recognize affect <ref> (Picard, 1995) </ref>, as well as a number of potential applications of affective wearables. To this end, we have begun to interface temperature, blood volume pressure, galvanic skin response, foot pressure, and electromyogram biosensors with our wearable computers.
Reference: <author> Poor, R. </author> <year> (1996). </year> <note> iRX 2.0. http:// ttt.www.media.mit.edu/ pia/ Research/ iRX2/ index.html. </note>
Reference-contexts: To avoid running out of identifiers for objects, an additional sense of location is needed. Outdoors, the Global Positioning System can be used to subdivide the space. However, for indoor use, a system of low-cost, infrared, light-powered beacons was developed to serve this purpose (Figure 9) <ref> (Poor, 1996) </ref>. Each of these beacons consists of a low-power microprocessor, an infrared transmitter, and an infrared receiver. A solar cell is used to avoid constant battery replacement. These systems are typically mounted under fluorescent light fixtures where they can draw power and effectively cover a region.
Reference: <author> Rhodes, B. and Starner, T. </author> <year> (1996). </year> <title> Remembrance agent: A continuously running automated information retreival system. </title> <booktitle> In Proc. of Pract. App. of Intelligent Agents and Multi-Agent Tech. </booktitle> <address> (PAAM), London. </address>
Reference-contexts: Thus, the agent can act as a memory aid. Even if the user mostly ignores the agent, he will still tend to glance at it whenever there is a short break in his work. In order to explore such a work environment, the Remembrance Agent <ref> (Rhodes and Starner, 1996) </ref> was created. 2.1.1 The Remembrance Agent The benefits of the Remembrance Agent (RA) are many. First, the RA provides timely information. If the user is writing a paper, the RA might suggest relevant references.
Reference: <author> Salton, G. </author> <year> (1971). </year> <title> The SMART Retrieval System - experiments in automatic document processing. </title> <publisher> Perntice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: Each user may customize the number of suggestions displayed, the type of documents referenced, and the update frequency of the display. The current implementation uses Savant, an in-house information retrieval system, which is similar to the SMART information retrieval program <ref> (Salton, 1971) </ref>. This search engine determines document similarity based on the frequency of words common to both the query and reference documents. In order to improve its speed, Savant creates an index of all words in each document source nightly.
Reference: <author> Schmandt, C. </author> <year> (1994). </year> <title> Voice Communication with Computers. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York. </address>
Reference-contexts: However, in the case of an emergency message, the computer should understand enough of the context to grab the user's attention immediately. The computer should also be able to identify urgent or time-critical messages <ref> (Schmandt, 1994) </ref> and wait for a break in the conversation to post a summary message discreetly onto the user's heads-up display. A user model should also predict the user's next action or state. Such information can be used to allocate resources pre-emptively.
Reference: <author> Starner, T., Mann, S., and Rhodes, B. </author> <year> (1995). </year> <note> The MIT wearable computing web page. http:// wear-ables.www.media.mit.edu/ projects/ wearables. </note>
Reference-contexts: The result is a constantly changing and improving environment. This section gives a brief overview of the systems used for the work in this paper, but for detailed or recent information, the reader is encouraged to visit the MIT wearable computing web site <ref> (Starner et al., 1995) </ref>. A standard system consists of a Private Eye display, Twiddler one-handed keyboard, PC-104 based 50MHz 486 computer, 16M of RAM, and approximately 1G of hard disk.
Reference: <author> Turk, M. and Pentland, A. </author> <year> (1991). </year> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 71-86. </pages>
Reference: <author> Uenohara, M. and Kanade, T. </author> <year> (1994). </year> <title> Vision-based object registration for real-time image overlay. </title> <type> Technical report, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Weiser, M. </author> <year> (1991). </year> <title> The computer for the 21st century. </title> <journal> Scientific American, </journal> <volume> 265(3) </volume> <pages> 94-104. 9 </pages>
Reference-contexts: For example, the plant in Figure 5 may "ask" passers-by for water based on a time schedule maintained by its virtual representation. This method is an effective way to gain the benefits of ubiquitous computing <ref> (Weiser, 1991) </ref> with 4 labels are displayed. a sparse infrastructure. Unfortunately, the visual tag system described above has a limited number of unique codes. To avoid running out of identifiers for objects, an additional sense of location is needed.
References-found: 23

