URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/dai-90.ps
Refering-URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/
Root-URL: http://www.csc.ncsu.edu
Email: msingh@cs.utexas.edu msingh@mcc.com  
Title: Group Intentions  
Author: Munindar P. Singh 
Address: Austin, TX 78712-1188 USA  Austin, TX 78759 USA  
Affiliation: Dept of Computer Sciences University of Texas  and Artificial Intelligence Lab MCC  
Date: October 1990  
Note: In 10th Workshop on Distributed Artificial Intelligence,  I am indebted to Michael Huhns and to three anonymous referees for comments.  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Philip Agre and David Chapman. Pengi: </author> <title> An implementation of a theory of activity. </title> <booktitle> In AAAI, </booktitle> <pages> pages 268-272, </pages> <year> 1987. </year>
Reference-contexts: Other desiderata for a theory of group intentions are the following. A theory of intentions should not be committed to a plan-based architecture of intelligent agency, since intelligence is not solely a matter of explicitly representing and interpreting symbolic structures <ref> [1, 25] </ref>. A good theory would accommodate the idea of situated action, and would consider the interactions among a group's members as they emerge from collective action. This idea is motivated and used in several recent papers; e.g., [11, 17]. <p> (x; *, skip) M j= w;t performs (G; s; Y ) iff (8r : r 2 (I S [ I R ) ! meets (r; s)) ^ (9s 0 ; s 00 : s 0 6= * ^ s = s 0 ffi s 00 ^ (8i : i 2 <ref> [1; . . . ; n] </ref>! M j= w;t follows (x i ; s 0 i , current (Y i ))) ^ (8S : S 2 S w;t ^ (9t 0 : hS; t; t 0 i 2 [[s 0 ]])! (9t 0 : hS; t; t 0 i 2 [[s
Reference: [2] <author> John L. Austin. </author> <title> How to do Things with Words. </title> <publisher> Clarendon, Oxford, </publisher> <address> UK, </address> <year> 1962. </year>
Reference-contexts: The interactions among the members of a group can be seen as objectively determining their respective "roles" in the group. 1. Strategic Interactions: Some of these abstract interactions among agents involve illocutionary acts <ref> [2] </ref>; e.g., assertions and commands. Others involve the establishment of various conditions in the world by some members' strategies that other members' strategies rely on.
Reference: [3] <author> Miroslav Benda, V. Jaganathan, and Rajendra Dodhiawala. </author> <title> On optimal cooperation of knowledge sources. </title> <type> Technical report, </type> <institution> Boeing Advanced Technology Center, Boeing Computer Services, </institution> <address> Seattle, WA, </address> <month> September </month> <year> 1986. </year>
Reference-contexts: I now apply it to the analysis of a well-known problem in DAI: the pursuit problem. This problem was introduced by Benda et al. in 1986 <ref> [3] </ref>, but has been extensively studied by others since then [9, 27]. This problem has been analyzed from the perspective of discussing the impact and costs of different mechanisms of cooperation among the Blue agents, and from the perspective of the demands that declarative representations of this problem make.
Reference: [4] <author> Michael E. Bratman. </author> <title> Intention, Plans, and Practical Reason. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: For a group that 1 It might have done that action intentionally, but need not have had the prior intention to pick it up|this distinction is described by Bratman <ref> [4, p. 119] </ref> and [5]. 15 did not have the abovementioned habit, the same strategy would not yield the latter intention. <p> However, it is not always appropriate, since intentions often have consequences that are not intended <ref> [4, p. 140] </ref>. This inference can be prevented by including a direct notion of what agents (and groups) intend. The new predicate, `intends-for,' tells us which strategy an agent has and what condition that strategy is meant to achieve.
Reference: [5] <author> Michael E. Bratman. </author> <title> What is intention? Technical Report 69, Center for the Study of Language and Information, </title> <publisher> Stanford, </publisher> <address> CA, </address> <month> March </month> <year> 1987. </year>
Reference-contexts: For a group that 1 It might have done that action intentionally, but need not have had the prior intention to pick it up|this distinction is described by Bratman [4, p. 119] and <ref> [5] </ref>. 15 did not have the abovementioned habit, the same strategy would not yield the latter intention.
Reference: [6] <author> Philip R. Cohen and Hector J. Levesque. </author> <title> On acting together: </title> <booktitle> Joint intentions for intelligent agents. In Workshop on Distributed Artificial Intelligence, </booktitle> <year> 1988. </year>
Reference-contexts: 1 Introduction Several subareas of Distributed Artificial Intelligence (DAI) are concerned with groups of intelligent agents who share a part of the world, and who affect one another through their actions. These subareas include autonomous agents, multiagent planning and action, discourse understanding, and cooperative work <ref> [6, 7, 13, 16, 18] </ref>. In recent work, I have developed a formal theory of the ability of a group (as ascribed objectively) that accounts for its internal structure [24] and, with Nicholas Asher, a theory of intentions [26]. <p> It aims to make as few stipulations as possible about the architecture of individual agents, and the specific ways in which they may interact. In particular, this theory, unlike the theories of Grosz and Sidner [13] and Cohen and Levesque <ref> [6] </ref>, does not attempt to reduce social structure to psychological notions like mutual belief. Thus it is both intuitively plausible, uses a fairly simple formal model, and is applicable to a wide variety of multiagent systems. <p> This strategy is itself seen as a set of strategies of its members. Following [24], the structure of a group is captured in terms of the "strategic" and "reactive" interactions of its members as they follow their respective strategies. As a consequence, this theory, unlike <ref> [6, 13] </ref>, does not require mutual beliefs among the members of a group. Mutual beliefs are impossible to achieve in most realistic scenarios; e.g., where communication delay is not bounded or channels not reliable [12, 14]. <p> In x6, I describe some of the logical consequences of the theory presented here. 2 Groups Traditional theories of multiagent intentions and action tend to ignore the structure of the group of agents being considered <ref> [6, 10, 13, 22] </ref>, and assume that all agents are equally capable (in terms of the "basic" actions they can 3 do|they are usually allowed differing knowledge of facts and differences in capabilities that occur solely as a consequence of differences in knowledge of facts). <p> theory also gives an account of the psychological phenomena that connects them directly to behavior and architecture, rather than requiring them to be independently attributed. 5 3 The Traditional Theories The most well-known AI theories of group intentions are the ones of Grosz and Sidner [13], and Cohen and Levesque <ref> [6] </ref>. These theories seem to have been developed for the domain of discourse understanding, although they have been proposed as theories of the intentions of multiagent systems at large. <p> Validity is indicated by `j=.' 1. j= intends (x; p)$ intends (hhxi; ;; ;i; p) Valid when has 1 and has 2 are used in the definition of `intends.' Unlike in the theories of Grosz and Sidner [13], and Cohen and Levesque <ref> [6] </ref>, here the group containing just x (and therefore having no restrictions), namely hhxi; ;; ;i, has the same intentions as x. 2. j= [intends e (x; p) ^ intends e (y; q) ^ (x; y 2 G)]! intends e (G; [(p ^ q) _ (p ^ Pq) _ (q ^
Reference: [7] <editor> Y. Demazeau and J-P. Muller, editors. </editor> <booktitle> Decentralized Artificial Intelligence, </booktitle> <address> Amsterdam, Holland, 1990. </address> <publisher> Elsevier Science Publishers B.V. / North-Holland. </publisher>
Reference-contexts: 1 Introduction Several subareas of Distributed Artificial Intelligence (DAI) are concerned with groups of intelligent agents who share a part of the world, and who affect one another through their actions. These subareas include autonomous agents, multiagent planning and action, discourse understanding, and cooperative work <ref> [6, 7, 13, 16, 18] </ref>. In recent work, I have developed a formal theory of the ability of a group (as ascribed objectively) that accounts for its internal structure [24] and, with Nicholas Asher, a theory of intentions [26].
Reference: [8] <author> Daniel C. Dennett. </author> <title> The Intentional Stance. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Folk psychological notions such as belief and intention provide powerful abstractions with which to specify and model the behavior of complex systems <ref> [8, 20] </ref>. However, for these concepts to be effectively used in DAI (both science and engineering), they must be given an objective grounding in terms of the architectures that different kinds of systems have and the actions they do.
Reference: [9] <author> Edmund H. Durfee and Thomas A. Montgomery. </author> <title> MICE: a flexible testbed for intelligent coordination experiments. </title> <booktitle> In Proc. 9th Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 25-40, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: I now apply it to the analysis of a well-known problem in DAI: the pursuit problem. This problem was introduced by Benda et al. in 1986 [3], but has been extensively studied by others since then <ref> [9, 27] </ref>. This problem has been analyzed from the perspective of discussing the impact and costs of different mechanisms of cooperation among the Blue agents, and from the perspective of the demands that declarative representations of this problem make.
Reference: [10] <author> Ronald Fagin and Joseph Y. Halpern. </author> <title> Belief, awareness, and limited reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 34 </volume> <pages> 39-76, </pages> <year> 1988. </year>
Reference-contexts: In x6, I describe some of the logical consequences of the theory presented here. 2 Groups Traditional theories of multiagent intentions and action tend to ignore the structure of the group of agents being considered <ref> [6, 10, 13, 22] </ref>, and assume that all agents are equally capable (in terms of the "basic" actions they can 3 do|they are usually allowed differing knowledge of facts and differences in capabilities that occur solely as a consequence of differences in knowledge of facts).
Reference: [11] <author> Jacques Ferber. </author> <title> The framework of eco-problem solving. </title> <booktitle> In 2nd Euro-pean Workshop on the Modeling of Autonomous Agents in a Multi-Agent World, </booktitle> <month> August </month> <year> 1990. </year> <month> 24 </month>
Reference-contexts: A good theory would accommodate the idea of situated action, and would consider the interactions among a group's members as they emerge from collective action. This idea is motivated and used in several recent papers; e.g., <ref> [11, 17] </ref>. The attribution of intentions to a group of agents depends not only on their psychological state, but also on their habits and skills, as well as the social interactions they have among themselves. These are aspects that cannot be easily reduced to psychological concepts.
Reference: [12] <author> Michael J. Fischer and Neil Immerman. </author> <title> Foundations of knowledge for distributed systems. </title> <editor> In Joseph Y. Halpern, editor, </editor> <booktitle> Theoretical Aspects of Reasoning About Knowledge, </booktitle> <pages> pages 171-185, </pages> <year> 1986. </year>
Reference-contexts: As a consequence, this theory, unlike [6, 13], does not require mutual beliefs among the members of a group. Mutual beliefs are impossible to achieve in most realistic scenarios; e.g., where communication delay is not bounded or channels not reliable <ref> [12, 14] </ref>. I ought to clarify at the outset the meanings of two terms I use often in this paper: "internal" and "external." By the former I mean any theory or approach that takes the viewpoint of the agent being analyzed or specified. <p> In the general case, mutual beliefs are impossible to establish using asynchronous communications <ref> [12, 14] </ref>. In practice, they can be established only if certain conventions are stipulated. Most importantly, however, the mutual belief requirement makes these analyses applicable to a concept different, and more complex, than simple intentions.
Reference: [13] <author> Barbara Grosz and Candace Sidner. </author> <title> Plans for discourse. </title> <editor> In P. Cohen, J. Morgan, and M. Pollack, editors, </editor> <title> SDF Benchmark Series: Intentions in Communication. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction Several subareas of Distributed Artificial Intelligence (DAI) are concerned with groups of intelligent agents who share a part of the world, and who affect one another through their actions. These subareas include autonomous agents, multiagent planning and action, discourse understanding, and cooperative work <ref> [6, 7, 13, 16, 18] </ref>. In recent work, I have developed a formal theory of the ability of a group (as ascribed objectively) that accounts for its internal structure [24] and, with Nicholas Asher, a theory of intentions [26]. <p> It aims to make as few stipulations as possible about the architecture of individual agents, and the specific ways in which they may interact. In particular, this theory, unlike the theories of Grosz and Sidner <ref> [13] </ref> and Cohen and Levesque [6], does not attempt to reduce social structure to psychological notions like mutual belief. Thus it is both intuitively plausible, uses a fairly simple formal model, and is applicable to a wide variety of multiagent systems. <p> This strategy is itself seen as a set of strategies of its members. Following [24], the structure of a group is captured in terms of the "strategic" and "reactive" interactions of its members as they follow their respective strategies. As a consequence, this theory, unlike <ref> [6, 13] </ref>, does not require mutual beliefs among the members of a group. Mutual beliefs are impossible to achieve in most realistic scenarios; e.g., where communication delay is not bounded or channels not reliable [12, 14]. <p> In x6, I describe some of the logical consequences of the theory presented here. 2 Groups Traditional theories of multiagent intentions and action tend to ignore the structure of the group of agents being considered <ref> [6, 10, 13, 22] </ref>, and assume that all agents are equally capable (in terms of the "basic" actions they can 3 do|they are usually allowed differing knowledge of facts and differences in capabilities that occur solely as a consequence of differences in knowledge of facts). <p> Such a theory also gives an account of the psychological phenomena that connects them directly to behavior and architecture, rather than requiring them to be independently attributed. 5 3 The Traditional Theories The most well-known AI theories of group intentions are the ones of Grosz and Sidner <ref> [13] </ref>, and Cohen and Levesque [6]. These theories seem to have been developed for the domain of discourse understanding, although they have been proposed as theories of the intentions of multiagent systems at large. <p> Validity is indicated by `j=.' 1. j= intends (x; p)$ intends (hhxi; ;; ;i; p) Valid when has 1 and has 2 are used in the definition of `intends.' Unlike in the theories of Grosz and Sidner <ref> [13] </ref>, and Cohen and Levesque [6], here the group containing just x (and therefore having no restrictions), namely hhxi; ;; ;i, has the same intentions as x. 2. j= [intends e (x; p) ^ intends e (y; q) ^ (x; y 2 G)]! intends e (G; [(p ^ q) _ (p
Reference: [14] <author> Joseph Y. Halpern and Yoram O. Moses. </author> <title> Knowledge and common knowledge in a distributed environment (revised version). </title> <type> Technical Report RJ 4421, </type> <institution> IBM, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: As a consequence, this theory, unlike [6, 13], does not require mutual beliefs among the members of a group. Mutual beliefs are impossible to achieve in most realistic scenarios; e.g., where communication delay is not bounded or channels not reliable <ref> [12, 14] </ref>. I ought to clarify at the outset the meanings of two terms I use often in this paper: "internal" and "external." By the former I mean any theory or approach that takes the viewpoint of the agent being analyzed or specified. <p> In the general case, mutual beliefs are impossible to establish using asynchronous communications <ref> [12, 14] </ref>. In practice, they can be established only if certain conventions are stipulated. Most importantly, however, the mutual belief requirement makes these analyses applicable to a concept different, and more complex, than simple intentions.
Reference: [15] <editor> C. L. Hamblin. Imperatives. </editor> <publisher> Basil Blackwell Ltd., Oxford, </publisher> <address> UK, </address> <year> 1987. </year>
Reference-contexts: Two observations, made in [24], are also in order here. 1. A group (e.g., a sports team) may be considered as a single unstruc 4 tured monolithic agent from without; i.e., groups are "Hobbesian cor-porate persons," in Hamblin's term <ref> [15, pp. 60, 240] </ref>. The intention of a group of agents is the same kind of entity as the intention of individual agents: the only difference is one of extent|one would typically expect a cooperative group to have an intention that no proper subgroups of it could succeed with.
Reference: [16] <author> Carl Hewitt. </author> <title> Organizational knowledge processing. </title> <booktitle> In Workshop on Distributed Artificial Intelligence, </booktitle> <year> 1988. </year>
Reference-contexts: 1 Introduction Several subareas of Distributed Artificial Intelligence (DAI) are concerned with groups of intelligent agents who share a part of the world, and who affect one another through their actions. These subareas include autonomous agents, multiagent planning and action, discourse understanding, and cooperative work <ref> [6, 7, 13, 16, 18] </ref>. In recent work, I have developed a formal theory of the ability of a group (as ascribed objectively) that accounts for its internal structure [24] and, with Nicholas Asher, a theory of intentions [26].
Reference: [17] <author> Steve Hickman and Martin Shiels. </author> <title> Situated action as a basis for cooperation. </title> <booktitle> In 2nd European Workshop on the Modeling of Autonomous Agents in a Multi-Agent World, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: A good theory would accommodate the idea of situated action, and would consider the interactions among a group's members as they emerge from collective action. This idea is motivated and used in several recent papers; e.g., <ref> [11, 17] </ref>. The attribution of intentions to a group of agents depends not only on their psychological state, but also on their habits and skills, as well as the social interactions they have among themselves. These are aspects that cannot be easily reduced to psychological concepts.
Reference: [18] <author> Kurt G. Konolige. </author> <title> A first-order formalism of knowledge and action for multi-agent planning. </title> <editor> In J. E. Hayes, D. Mitchie, and Y. Pao, editors, </editor> <booktitle> Machine Intelligence 10, </booktitle> <pages> pages 41-73. </pages> <publisher> Ellis Horwood Ltd., </publisher> <address> Chichester, UK, </address> <year> 1982. </year>
Reference-contexts: 1 Introduction Several subareas of Distributed Artificial Intelligence (DAI) are concerned with groups of intelligent agents who share a part of the world, and who affect one another through their actions. These subareas include autonomous agents, multiagent planning and action, discourse understanding, and cooperative work <ref> [6, 7, 13, 16, 18] </ref>. In recent work, I have developed a formal theory of the ability of a group (as ascribed objectively) that accounts for its internal structure [24] and, with Nicholas Asher, a theory of intentions [26].
Reference: [19] <author> Dexter Kozen and Jerzy Tiurzyn. </author> <title> Logics of program. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science. </booktitle> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1990. </year>
Reference-contexts: It can easily be seen that relative to the standard semantics for the constructs introduced above (e.g., see <ref> [19] </ref>), `Y ' is equivalent to `current (Y ); rest (Y ).' Another obvious, but useful consequence of this is that `current (Y )' is always of the form `skip' or `do (A)' or `wait (A).' But since `skip' is the empty strategy, `follows' is invoked only for cases (1) and
Reference: [20] <author> John McCarthy. </author> <title> Ascribing mental qualities to machines. </title> <editor> In Martin Ringle, editor, </editor> <booktitle> Philosophical Perspectives in Artificial Intelligence. </booktitle> <publisher> Harvester Press, </publisher> <year> 1979. </year> <note> Page nos. from a revised version, issued as a report in 1987. </note>
Reference-contexts: Folk psychological notions such as belief and intention provide powerful abstractions with which to specify and model the behavior of complex systems <ref> [8, 20] </ref>. However, for these concepts to be effectively used in DAI (both science and engineering), they must be given an objective grounding in terms of the architectures that different kinds of systems have and the actions they do.
Reference: [21] <author> Stanley J. Rosenschein. </author> <title> Formal theories of knowledge in AI and robotics. </title> <journal> New Generation Computing, </journal> <volume> 3(4), </volume> <year> 1985. </year>
Reference-contexts: In fact, these theories identify a group with the set of its members. They are also committed to a plan-based view of intelligence, a view that has come under much criticism recently <ref> [21, 23, 25] </ref>. I shall argue that the traditional theories apply to what is merely a special case of the general concept of group intentions. Mutual belief is a requirement that, if it applies at all, applies only to groups in some particular kinds of multi-agent systems. <p> While this task is not attempted in this paper, it is one that has been worked on by others <ref> [21, 23] </ref>. Briefly, these approaches relate the ascription of beliefs to the agent's actions; some further work is needed to make the connection with internal structure explicit.
Reference: [22] <author> John R. Searle. </author> <title> Collective intentions and actions. </title> <editor> In P. Cohen, J. Mor-gan, and M. Pollack, editors, </editor> <title> SDF Benchmark Series: Intentions in Communication. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year> <month> 25 </month>
Reference-contexts: In x6, I describe some of the logical consequences of the theory presented here. 2 Groups Traditional theories of multiagent intentions and action tend to ignore the structure of the group of agents being considered <ref> [6, 10, 13, 22] </ref>, and assume that all agents are equally capable (in terms of the "basic" actions they can 3 do|they are usually allowed differing knowledge of facts and differences in capabilities that occur solely as a consequence of differences in knowledge of facts).
Reference: [23] <author> Nigel Seel. </author> <title> Formalising first-order intentional systems theory. </title> <type> Technical report, </type> <institution> STC Technology Ltd., Harlow, Essex, UK, </institution> <year> 1989. </year>
Reference-contexts: In fact, these theories identify a group with the set of its members. They are also committed to a plan-based view of intelligence, a view that has come under much criticism recently <ref> [21, 23, 25] </ref>. I shall argue that the traditional theories apply to what is merely a special case of the general concept of group intentions. Mutual belief is a requirement that, if it applies at all, applies only to groups in some particular kinds of multi-agent systems. <p> While this task is not attempted in this paper, it is one that has been worked on by others <ref> [21, 23] </ref>. Briefly, these approaches relate the ascription of beliefs to the agent's actions; some further work is needed to make the connection with internal structure explicit.
Reference: [24] <author> Munindar P. Singh. </author> <title> Group ability and structure. </title> <booktitle> In 2nd European Workshop on the Modeling of Autonomous Agents in a Multi-Agent World, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: These subareas include autonomous agents, multiagent planning and action, discourse understanding, and cooperative work [6, 7, 13, 16, 18]. In recent work, I have developed a formal theory of the ability of a group (as ascribed objectively) that accounts for its internal structure <ref> [24] </ref> and, with Nicholas Asher, a theory of intentions [26]. Here I plan to relate these interests, and motivate and present a formal theory of the intentions of a group of agents. <p> Briefly, in this paper, the intentions of a group are described in terms of its internal structure, and the abstract strategy it may be said to be 2 following. This strategy is itself seen as a set of strategies of its members. Following <ref> [24] </ref>, the structure of a group is captured in terms of the "strategic" and "reactive" interactions of its members as they follow their respective strategies. As a consequence, this theory, unlike [6, 13], does not require mutual beliefs among the members of a group. <p> In x3, I briefly review the traditional theories of group actions and intentions, and point out some conceptual problems with them relative to the intuitions of x2. In x4, I review some relevant parts of <ref> [24] </ref> extending it as needed for the purposes of this paper. <p> Similarly, the United Nations Security Council intends to censure a nation if and only if a majority of its members do, and none of its five permanent members object. Two observations, made in <ref> [24] </ref>, are also in order here. 1. A group (e.g., a sports team) may be considered as a single unstruc 4 tured monolithic agent from without; i.e., groups are "Hobbesian cor-porate persons," in Hamblin's term [15, pp. 60, 240]. <p> While this section is essential for a thorough understanding of the definitions to follow, it may safely be skimmed over on a first reading. 4.1 The Formal Model The formal model here is quite close to the ones in <ref> [25, 24] </ref>; an important difference is that the interpretation assigns strategies (that are `had'), conditions (that are `believed') and pairs of strategies and conditions (in which the strategies are `intended-for' the conditions) to agents. Some new predicates used in this paper are also defined. <p> S 2 S w;t ^ (9t 0 : hS; t; t 0 i 2 [[seq]])! (9t 0 : hS; t; t 0 i 2 [[seq]] ^ (M j= w;t 0 A) ^ (8t 00 : t t 00 t 0 ! M j= w;t 00 Constr))) 4.3 Group Structure Following <ref> [24] </ref>, strategies are used here to abstractly characterize the behavior of groups. We can be agnostic about whether they are hard-wired, or obtained through planning, or explicitly represented. A group strategy is treated as a set of strategies of its members. <p> The group strategy, Y , is an ordered set of member strategies, hY 1 ; . . . ; Y n i. The restrictions in I S [ I R must be `met' as each x i follows Y i <ref> [24] </ref>. This ensures that the actions being done are being done by the agents as members of the given group, since each agent plays the appropriate role. The actual restrictions that one needs depend on the application domain; some important categories are described in [24]. 4.4 Performing Strategies I now introduce <p> each x i follows Y i <ref> [24] </ref>. This ensures that the actions being done are being done by the agents as members of the given group, since each agent plays the appropriate role. The actual restrictions that one needs depend on the application domain; some important categories are described in [24]. 4.4 Performing Strategies I now introduce a predicate, `continuation' on strategies. Intuitively, if the `current' part of a strategy is in progress, its continuation is the entire strategy; if the `current' part of a strategy is over, its continuation is just the `rest' of it. <p> The predicate, `meets,' is meant to accommodate the restrictions on the interactions among agents, and has to be defined for each kind of restriction in I S [ I R . Please see <ref> [24] </ref> for details. Now the predicate `leads-to' may be defined as follows. For a group, a strategy leads to a condition iff that condition obtains over all sequences over which that group `performs' that strategy. <p> Y 2 ; Y 3 and Y 4 could then simply be to any location about Red. The group would impose the strategic restriction that all commands from B 1 are obeyed <ref> [24] </ref>.
Reference: [25] <author> Munindar P. Singh. </author> <title> Towards a theory of situated know-how. </title> <booktitle> In 9th European Conference on Artificial Intelligence, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: Other desiderata for a theory of group intentions are the following. A theory of intentions should not be committed to a plan-based architecture of intelligent agency, since intelligence is not solely a matter of explicitly representing and interpreting symbolic structures <ref> [1, 25] </ref>. A good theory would accommodate the idea of situated action, and would consider the interactions among a group's members as they emerge from collective action. This idea is motivated and used in several recent papers; e.g., [11, 17]. <p> In fact, these theories identify a group with the set of its members. They are also committed to a plan-based view of intelligence, a view that has come under much criticism recently <ref> [21, 23, 25] </ref>. I shall argue that the traditional theories apply to what is merely a special case of the general concept of group intentions. Mutual belief is a requirement that, if it applies at all, applies only to groups in some particular kinds of multi-agent systems. <p> While this section is essential for a thorough understanding of the definitions to follow, it may safely be skimmed over on a first reading. 4.1 The Formal Model The formal model here is quite close to the ones in <ref> [25, 24] </ref>; an important difference is that the interpretation assigns strategies (that are `had'), conditions (that are `believed') and pairs of strategies and conditions (in which the strategies are `intended-for' the conditions) to agents. Some new predicates used in this paper are also defined. <p> The semantics is given relative to intensional models: it is standard for the predicate calculus; the predicate intends is considered below. Some auxiliary predicates are defined where needed. It is assumed throughout that operators for quoting and dequoting can be inserted where necessary. 4.2 Strategies Following <ref> [25] </ref>, I use "strategies" as abstract specifications of the behavior of an agent or a group. Strategies as defined here merely characterize an agent's behavior, possibly in quite coarse terms.
Reference: [26] <author> Munindar P. Singh and Nicholas M. Asher. </author> <title> Towards a formal theory of intentions. </title> <booktitle> In European Workshop on Logics in Artificial Intelligence, </booktitle> <month> September </month> <year> 1990. </year>
Reference-contexts: In recent work, I have developed a formal theory of the ability of a group (as ascribed objectively) that accounts for its internal structure [24] and, with Nicholas Asher, a theory of intentions <ref> [26] </ref>. Here I plan to relate these interests, and motivate and present a formal theory of the intentions of a group of agents. <p> The new predicate, `intends-for,' tells us which strategy an agent has and what condition that strategy is meant to achieve. The second part of the meaning of `intends-for' acts like a syntactic filter here. It is at this point that the power of the approach of <ref> [26] </ref> would be useful. That approach allows a semantically motivated definition of structural subsumption between the objects that intentions are defined over, but is too complex to be included here.
Reference: [27] <author> Larry M. Stephens and Matthais Merx. </author> <title> The effect of agent organization on the performance of DAI systems. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <note> 1989. Submitted. 26 </note>
Reference-contexts: I now apply it to the analysis of a well-known problem in DAI: the pursuit problem. This problem was introduced by Benda et al. in 1986 [3], but has been extensively studied by others since then <ref> [9, 27] </ref>. This problem has been analyzed from the perspective of discussing the impact and costs of different mechanisms of cooperation among the Blue agents, and from the perspective of the demands that declarative representations of this problem make. <p> Here my aim is simply to analyze the intentions of the team of Blue agents in terms of the intentions of the individual Blue agents. The version here is taken from <ref> [27] </ref>. Briefly, the problem is as follows. We are 16 given a finite two-dimensional grid of points (see Figure 1). Each point may be occupied by either an agent called `Red' (the "adversary") or up to four `Blue' agents.
References-found: 27

