URL: http://theory.lcs.mit.edu/~cc/papers/unknow.ps
Refering-URL: http://theory.lcs.mit.edu/~cc/publications.html
Root-URL: 
Email: cachin@acm.org  
Title: Hashing a Source With an Unknown Probability Distribution  
Author: Christian Cachin 
Keyword: Universal Hashing, Privacy Amplification, Guessing, Inaccuracy, Renyi Entropy.  
Date: May 10, 1998  
Address: 545 Technology Square Cambridge, MA 02139, USA  
Affiliation: MIT Laboratory for Computer Science  
Abstract: Renyi entropy of order 2 characterizes how many almost uniform random bits can be extracted from a distribution by universal hashing by a technique known as "privacy amplification" in cryptography. We generalize this result and show that if P S is the assumed distribution of a random variable with true distribution P X , then the amount of extractable almost uniform randomness corresponds to log P[X = S], when X and S are interpreted as independent random variables. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. H. Bennett, G. Brassard, C. Crepeau, and U. M. Maurer, </author> <title> "Generalized privacy amplifica-tion," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 41, </volume> <pages> pp. 1915-1923, </pages> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: X kP U ) = log jX j: (1) 2 Privacy Amplification by Universal Hashing Entropy smoothing by universal hashing is a widely-used technique in cryptography and theoretical computer science to concentrate the randomness inherent in a probability distribution, known in different contexts as entropy smoothing [7] or privacy amplification <ref> [2, 1] </ref>. The amount of extractable almost uniform randomness is closely related to the Renyi entropy [8] of the distribution. The Renyi entropy of order 2 of a random variable X is defined as H 2 (X) = log x2X Universal hashing was introduced by Carter and Wegman [4]. <p> The size of the largest Y for which hashing of X yields an almost uniform output is characterized by H 2 (X). (The theorem can be extended to Renyi entropy of order ff for any ff &gt; 1 [3].) Theorem 1 (Privacy Amplification <ref> [1] </ref>). Let X be a random variable over the alphabet X with Renyi entropy H 2 (X), let G be the random variable corresponding to the random choice (with uniform distribution) of a member of a universal hash function G : X ! Y, and let Y = G (X).
Reference: [2] <author> C. H. Bennett, G. Brassard, and J.-M. Robert, </author> <title> "How to reduce your enemy's information," </title> <booktitle> in Advances in Cryptology: CRYPTO '85 (H. </booktitle> <editor> C. Williams, ed.), </editor> <volume> vol. </volume> <booktitle> 218 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 468-476, </pages> <publisher> Springer, </publisher> <year> 1986. </year>
Reference-contexts: X kP U ) = log jX j: (1) 2 Privacy Amplification by Universal Hashing Entropy smoothing by universal hashing is a widely-used technique in cryptography and theoretical computer science to concentrate the randomness inherent in a probability distribution, known in different contexts as entropy smoothing [7] or privacy amplification <ref> [2, 1] </ref>. The amount of extractable almost uniform randomness is closely related to the Renyi entropy [8] of the distribution. The Renyi entropy of order 2 of a random variable X is defined as H 2 (X) = log x2X Universal hashing was introduced by Carter and Wegman [4].
Reference: [3] <author> C. Cachin, </author> <title> Entropy Measures and Unconditional Security in Cryptography, </title> <booktitle> vol. 1 of ETH Series in Information Security and Cryptography. </booktitle> <address> Konstanz, Germany: </address> <publisher> Hartung-Gorre Verlag, </publisher> <year> 1997. </year> <note> ISBN 3-89649-185-7 (Reprint of Ph.D. dissertation No. 12187, </note> <institution> ETH Zurich). </institution>
Reference-contexts: The size of the largest Y for which hashing of X yields an almost uniform output is characterized by H 2 (X). (The theorem can be extended to Renyi entropy of order ff for any ff &gt; 1 <ref> [3] </ref>.) Theorem 1 (Privacy Amplification [1]).
Reference: [4] <author> J. L. Carter and M. N. Wegman, </author> <title> "Universal classes of hash functions," </title> <journal> Journal of Computer and System Sciences, </journal> <volume> vol. 18, </volume> <pages> pp. 143-154, </pages> <year> 1979. </year>
Reference-contexts: The amount of extractable almost uniform randomness is closely related to the Renyi entropy [8] of the distribution. The Renyi entropy of order 2 of a random variable X is defined as H 2 (X) = log x2X Universal hashing was introduced by Carter and Wegman <ref> [4] </ref>. It is a randomized hashing technique involving a family of functions from which one is chosen randomly and applied to the source.
Reference: [5] <author> T. M. Cover and J. A. Thomas, </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year> <month> 4 </month>
Reference-contexts: The quantity H (X) + D (P X kP S ) is also called inaccuracy [6]. Inaccuracy measures the uncertainty about a source with unknown distribution similar to entropy (as demonstrated by the following example from <ref> [5] </ref>). Consider the construction of an optimal binary prefix-free code for a random variable X. If the distribution of X is known, the optimal code has average length between H (X) and H (X)+1, which is one justification of entropy as a fundamental measure of uncertainty.
Reference: [6] <author> I. Csiszar and J. Korner, </author> <title> Information Theory: Coding Theorems for Discrete Memoryless Sys-tems. </title> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: The quantity H (X) + D (P X kP S ) is also called inaccuracy <ref> [6] </ref>. Inaccuracy measures the uncertainty about a source with unknown distribution similar to entropy (as demonstrated by the following example from [5]). Consider the construction of an optimal binary prefix-free code for a random variable X.
Reference: [7] <author> R. Impagliazzo, L. A. Levin, and M. Luby, </author> <title> "Pseudo-random generation from one-way functions," </title> <booktitle> in Proc. 21st Annual ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pp. 12-24, </pages> <year> 1989. </year>
Reference-contexts: (X) + D (P X kP U ) = log jX j: (1) 2 Privacy Amplification by Universal Hashing Entropy smoothing by universal hashing is a widely-used technique in cryptography and theoretical computer science to concentrate the randomness inherent in a probability distribution, known in different contexts as entropy smoothing <ref> [7] </ref> or privacy amplification [2, 1]. The amount of extractable almost uniform randomness is closely related to the Renyi entropy [8] of the distribution.
Reference: [8] <author> A. Renyi, </author> <title> "On measures of entropy and information," </title> <booktitle> in Proc. 4th Berkeley Symposium on Mathematical Statistics and Probability, </booktitle> <volume> vol. 1, </volume> <pages> pp. 547-561, </pages> <institution> Univ. of Calif. Press, </institution> <year> 1961. </year> <month> 5 </month>
Reference-contexts: The amount of extractable almost uniform randomness is closely related to the Renyi entropy <ref> [8] </ref> of the distribution. The Renyi entropy of order 2 of a random variable X is defined as H 2 (X) = log x2X Universal hashing was introduced by Carter and Wegman [4].
References-found: 8

