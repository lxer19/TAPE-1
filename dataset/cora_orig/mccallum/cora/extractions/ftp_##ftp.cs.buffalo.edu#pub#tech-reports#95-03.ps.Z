URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/95-03.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Email: jagota@next1.msci.memst.edu  giri@next1.msci.memst.edu  regan@cs.buffalo.edu  
Title: Information Capacity of Binary Weights Associative Memories  
Author: Arun Jagota Giri Narasimhan Kenneth W. Regan 
Date: January 24, 1995  
Address: Memphis TN 38152  Memphis TN 38152  Buffalo NY 14260  
Affiliation: Mathematical Sciences University of Memphis  Mathematical Sciences University of Memphis  University at Buffalo  
Abstract: We study the amount of information stored in the fixed points of random instances of two binary weights associative memory models: the Willshaw Model (WM) and the Inverted Neural Network (INN). For these models, we show divergences between the information capacity (IC) as defined by Abu-Mostafa and Jacques, and information calculated from the standard notion of storage capacity by Palm and Grossman respectively. We prove that the WM has asymptotically optimal IC for nearly the full range of threshold values, the INN likewise for constant threshold values, and both over all degrees of sparseness of the stored vectors. This is contrasted with the result by Palm, which required stored random vectors to be logarithmically sparse to achieve good storage capacity for the WM, and with that of Grossman, which showed that the INN has poor storage capacity for random vectors. We propose Q-state versions of the WM and the INN, and show that they retain asymptotically optimal IC while guaranteeing stable storage. By contrast, we show that the Q-state INN has poor storage capacity for random vectors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y.S. Abu-Mostafa and J.S. Jacques. </author> <title> Information capacity of the Hopfield model. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 31(4) </volume> <pages> 461-464, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: 1 Introduction Abu-Mostafa and Jacques <ref> [1] </ref> introduced the following concept. Definition 1 The Information Capacity (IC) of a memory model is the logarithm (base 2) of the number of its instances that store distinct collections of memories. <p> The IC measures the entropy of the memory model under the uniform distribution on its instances, that is, the number of bits of information in a random instance of the memory. As one example <ref> [1] </ref>, consider an n-bit memory model in which each location is independent of others. All 2 n instances of this model store distinct n-bit vectors, hence its information capacity equals n, which is the intuitive answer. <p> This is the definition of Information Capacity we employ in the current paper. Such a calculation is not necessarily easy, since many instances may realize the same collection of fixed points. Relationship to Other Measures of Capacity. The capacities of various neural associative memories have been studied extensively <ref> [25, 1, 23, 20, 10, 2, 6, 9] </ref>. <p> That this distinction is a useful one is given by the fact that, for the discrete Hopfield network, the information realizable by the Hebb rule, as noted earlier, is of the order n 2 = log n, whereas the upper bound given by the IC is n 2 <ref> [1] </ref>. Second, the former calculation gives the information stored in an instance arising from a random collection of stored vectors; the IC on the other hand gives the information stored in a random instance of the memory. For some memory models, both calculations give identical results. <p> Earlier results on stable storage, basins, or information capacity have relied on statistical [25, 2], coding theory [23], or threshold function counting <ref> [1] </ref> arguments. By contrast, all our results in the current paper are based on characterizations of the fixed points and graph counting. In the case of the Willshaw model, we characterize the fixed points as certain kinds of induced subgraphs. <p> By contrast, there are collections of at most n vectors (in fact at most 5 three [10]) that cannot be stored stably in an n-unit network (no matter what the storage rule is and even if the weights are real-valued) <ref> [1] </ref>. Define the Q-state INN model as one composed of N = Q fi n neurons, arranged in a grid, in which every instance of the model arises from storing some collection of Q-state vectors of length n using storage rule (5).
Reference: [2] <author> S. Amari. </author> <title> Characteristics of sparsely encoded associative memory. </title> <booktitle> Neural Networks, </booktitle> <volume> 2(6) </volume> <pages> 451-457, </pages> <year> 1989. </year>
Reference-contexts: This is the definition of Information Capacity we employ in the current paper. Such a calculation is not necessarily easy, since many instances may realize the same collection of fixed points. Relationship to Other Measures of Capacity. The capacities of various neural associative memories have been studied extensively <ref> [25, 1, 23, 20, 10, 2, 6, 9] </ref>. <p> Such a calculation is not necessarily easy, since many instances may realize the same collection of fixed points. Relationship to Other Measures of Capacity. The capacities of various neural associative memories have been studied extensively [25, 1, 23, 20, 10, 2, 6, 9]. The capacity definitions employed most frequently <ref> [25, 23, 2, 9] </ref> are instances of the following general one, which is, loosely-stated: Definition 2 The storage capacity is the largest number of randomly selected memories (binary vectors, or vector pairs) that can be stored so that, for sufficiently large number of neurons n, the probability that all (or nearly <p> Let us refer to these definitions as stable storage capacity and basins capacity respectively. The importance of calculating the stable storage and basins capacities of memory models cannot be overstated. These calculations are often done for specific storage rules for the memories <ref> [26, 23, 2] </ref> (for example, the Hebb rule). From these results, the amount of information stored in an instance of the model, arising from storing random vectors using a particular storage rule, can also often be deduced. <p> In particular he showed that order of n 2 = log 2 n random vectors of such sparseness can be stored usefully in this memory model. An n-bit random vector containing fi (log n) ones has fi (log 2 n) bits of information <ref> [2] </ref>. The amount of information in a resulting instance is thus asymptotically of the order n 2 bits [25]. Palm's analysis [25] appears to implicitly indicate that the amount of information drops significantly if the stored random vectors are denser. Amari [2] extended the results of Palm as follows. <p> ones has fi (log 2 n) bits of information <ref> [2] </ref>. The amount of information in a resulting instance is thus asymptotically of the order n 2 bits [25]. Palm's analysis [25] appears to implicitly indicate that the amount of information drops significantly if the stored random vectors are denser. Amari [2] extended the results of Palm as follows. He replaced the binary-weights Willshaw model with the real-valued weights discrete Hopfield model, and employed the Hebb rule for storage. <p> Amari <ref> [2] </ref> calculated the information stored in the resulting network instance as of the order C I (*) = &lt; n 2 = log n if the stored vectors have order of n ones (1 *)n 2 if the stored vectors have order of n * ones, 0 &lt; * &lt; 1 <p> This result contrasts with that of Palm [25], who required the stored random vectors to be logarithmically sparse, in order for the stored information to be asymptotically optimal. Earlier results on stable storage, basins, or information capacity have relied on statistical <ref> [25, 2] </ref>, coding theory [23], or threshold function counting [1] arguments. By contrast, all our results in the current paper are based on characterizations of the fixed points and graph counting. In the case of the Willshaw model, we characterize the fixed points as certain kinds of induced subgraphs. <p> How to exploit this is a separate issue. 6 Sparse Coding and Information Capacity Sparse coding has been suggested as a mechanism to alleviate the poor storage capacity of neural associative memories <ref> [29, 25, 2, 5] </ref>. Though our previous results in this paper indicate that sparse coding is not necessary to retain high information capacity, by our definition of information capacity, it is useful to calculate whether sparse coding is sufficient. 6.1 The INN Model Consider first the INN model. <p> In particular, the sparse recoding of a a collection of binary vectors of length n as Q-state vectors, as described in Section 5, neither helps nor hurts, asymptotically. Recall, from Section 1, Amari's result <ref> [2] </ref>, given by (1), that if random vectors of sparseness k = o (n) are stored in the real-valued weights associative memory model, then order of n 2 bits of information can be stored; if k = fi (n), then n 2 = log n bits of information can be stored.
Reference: [3] <author> J.A. Anderson. </author> <title> A simple neural network generating interactive memory. </title> <journal> Mathematical Biosciences, </journal> <volume> 14 </volume> <pages> 197-220, </pages> <year> 1972. </year>
Reference-contexts: The proofs that rely on graph-theoretic arguments and notation are postponed to a later section, where some useful concepts from graph theory are introduced first, and then the proofs given. 3 2 The Associative Memory Models The Associative Memory Models that we study here have their roots in <ref> [29, 3, 21, 24, 15] </ref>. The two models we study in this paper employ binary-valued weights [29, 28], and are restricted to the auto-associative case. Both may be described as special cases of the Hopfield model [15].
Reference: [4] <author> R. Bar-Yehuda, </author> <year> 1993. </year> <type> Personal Communication. </type>
Reference-contexts: This opens the question of whether recoding binary vectors (Q=2) of length n as Q-state vectors, Q large, increases the number of vectors stored before saturation occurs. The following recoding ideas came from discussion with Bar-Yehuda <ref> [4] </ref>. Define a k-recoding of a binary vector of length n as a recoding to a Q-state vector, Q = 2 k , of length n=k by dividing the n bits of the binary vector into n=k blocks of k bits each.
Reference: [5] <author> Y. Baram. </author> <title> Corrective memory by a symmetric sparsely encoded network. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 40(2) </volume> <pages> 429-438, </pages> <year> 1994. </year>
Reference-contexts: How to exploit this is a separate issue. 6 Sparse Coding and Information Capacity Sparse coding has been suggested as a mechanism to alleviate the poor storage capacity of neural associative memories <ref> [29, 25, 2, 5] </ref>. Though our previous results in this paper indicate that sparse coding is not necessary to retain high information capacity, by our definition of information capacity, it is useful to calculate whether sparse coding is sufficient. 6.1 The INN Model Consider first the INN model.
Reference: [6] <author> S. Biswas and S.S. Venkatesh. </author> <title> Codes, sparsity, and capacity in neural associative memory. </title> <type> Technical report, </type> <institution> Department of Electrical Engineering, University of Pennsylvania, </institution> <year> 1990. </year>
Reference-contexts: This is the definition of Information Capacity we employ in the current paper. Such a calculation is not necessarily easy, since many instances may realize the same collection of fixed points. Relationship to Other Measures of Capacity. The capacities of various neural associative memories have been studied extensively <ref> [25, 1, 23, 20, 10, 2, 6, 9] </ref>.
Reference: [7] <author> J.A. Bondy and U.S.R Murty. </author> <title> Graph Theory with Applications. </title> <publisher> North-Holland, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: We define the terminology we use in this paper here. For a more extensive introduction, the reader is refered to <ref> [7] </ref>. A graph G is denoted by a pair (V; E) where V is the set of vertices and E the set of edges. Two vertices u; v are called adjacent in G if they are connected by an edge.
Reference: [8] <author> S. Chaudhari and J. Radhakrishnan, </author> <year> 1991. </year> <type> Personal Communication. </type>
Reference-contexts: Figure 1 illustrates, for Q = n = 3, the complement G c of the graph G formed after storing (0; 0; 0); (0; 1; 2), and (2; 1; 0). Proof of Lemma 6. The proof is based on a construction due to <ref> [8] </ref> for Q = 2, that we have extended to arbitrary Q. Construct a family of (Q fi n)-vertex graphs as follows. The vertices are arranged into a grid of Q rows 0; . . . ; Q 1 and n columns 1; . . . ; n.
Reference: [9] <author> T. Chiueh and R.M. Goodman. </author> <title> Recurrent correlation associative memories. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(2) </volume> <pages> 275-284, </pages> <year> 1991. </year>
Reference-contexts: This is the definition of Information Capacity we employ in the current paper. Such a calculation is not necessarily easy, since many instances may realize the same collection of fixed points. Relationship to Other Measures of Capacity. The capacities of various neural associative memories have been studied extensively <ref> [25, 1, 23, 20, 10, 2, 6, 9] </ref>. <p> Such a calculation is not necessarily easy, since many instances may realize the same collection of fixed points. Relationship to Other Measures of Capacity. The capacities of various neural associative memories have been studied extensively [25, 1, 23, 20, 10, 2, 6, 9]. The capacity definitions employed most frequently <ref> [25, 23, 2, 9] </ref> are instances of the following general one, which is, loosely-stated: Definition 2 The storage capacity is the largest number of randomly selected memories (binary vectors, or vector pairs) that can be stored so that, for sufficiently large number of neurons n, the probability that all (or nearly
Reference: [10] <author> A. Dembo. </author> <title> On the capacity of associative memories with linear threshold functions. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 35(4) </volume> <pages> 709-720, </pages> <year> 1989. </year>
Reference-contexts: This is the definition of Information Capacity we employ in the current paper. Such a calculation is not necessarily easy, since many instances may realize the same collection of fixed points. Relationship to Other Measures of Capacity. The capacities of various neural associative memories have been studied extensively <ref> [25, 1, 23, 20, 10, 2, 6, 9] </ref>. <p> Notice that when Q = 2, this result states that any collection of n-bit binary vectors can be stored stably in a 2n-unit network. By contrast, there are collections of at most n vectors (in fact at most 5 three <ref> [10] </ref>) that cannot be stored stably in an n-unit network (no matter what the storage rule is and even if the weights are real-valued) [1].
Reference: [11] <author> G.H. Godbeer, J. Lipscomb, and M. Luby. </author> <title> On the computational complexity of finding stable state vectors in connectionist models (Hopfield nets). </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Toronto, Toronto, </institution> <address> Ontario, </address> <year> 1988. </year>
Reference-contexts: We use the set notation from here on. 7.1 Proofs of Section 3 We first characterize the fixed points of WM (t) and INN (t) as certain induced subgraphs of their underlying graphs. Inaccurate versions of these characterizations are in <ref> [11] </ref>. Proposition 13 For integer t, U is a fixed point of an WM (t) instance with underlying graph G if and only if (i) ffi (G [U ]) t and (ii) 8v 62 U : d U (v) &lt; t. (If U = ;, (i) is assumed to hold.
Reference: [12] <author> T. Grossman. </author> <title> The INN model as an associative memory. </title> <type> Technical Report LA-UR-93-4149, </type> <institution> Los Alamos National Laboratory, </institution> <address> Los Alamos, NM 87545, </address> <year> 1993. </year> <month> 13 </month>
Reference-contexts: The exact optimality of the result for t = 0 is significant because realizable networks are clearly finite (usually small). Our result contrasts with that of Grossman <ref> [12] </ref>, who showed that the stable storage capacity (and the resulting information derived from it) of INN for t = 0 was poor. <p> To store X , for all i 6= j: w ij otherwise (3) Grossman has shown that the stable storage capacity of this storage rule for INN (0), for random sparse vectors, is at most log 2 n <ref> [12] </ref>. This puts a (weak) upper bound of n log 2 n on the number of bits of information stored in such a network.
Reference: [13] <author> T. Grossman and A. Jagota. </author> <title> On the equivalence of two Hopfield-type networks. </title> <booktitle> In IEEE International Conference on Neural Networks, </booktitle> <pages> pages 1063-1068. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference: [14] <author> J. Hertz, A. Krogh, and R.G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: From these results, the amount of information stored in an instance of the model, arising from storing random vectors using a particular storage rule, can also often be deduced. For example, it is well known (see <ref> [23, 14] </ref>) that order of n= log n n-bit random vectors can be stored in the discrete Hopfield memory using the Hebb rule so that, with probability tending to one, every stored vector is stable and has a large basin of attraction.
Reference: [15] <author> J.J. </author> <title> Hopfield. Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <address> USA, 79, </address> <year> 1982. </year>
Reference-contexts: In this sense, the IC is to such memory models what VC-dimension is to feedforward neural networks: both measure the intrinsic "richness" of a particular architecture. As an example, consider the discrete Hopfield neural network <ref> [15] </ref>. For the purposes of associative memories, it is normal to consider the memories in this model to be stored only in its fixed points. <p> Overview of Results. In this paper, we study two binary-weights neural associative memory models: the Willshaw Model (WM) [29], and the Inverted Neural Network (INN) [28]. Both may be viewed as special cases of the discrete Hopfield model <ref> [15] </ref>. From each model, we define a family of models indexed by the threshold value t to a neuron (the same for each neuron). We call these families WM (t) and INN (t) respectively. <p> The proofs that rely on graph-theoretic arguments and notation are postponed to a later section, where some useful concepts from graph theory are introduced first, and then the proofs given. 3 2 The Associative Memory Models The Associative Memory Models that we study here have their roots in <ref> [29, 3, 21, 24, 15] </ref>. The two models we study in this paper employ binary-valued weights [29, 28], and are restricted to the auto-associative case. Both may be described as special cases of the Hopfield model [15]. <p> The two models we study in this paper employ binary-valued weights [29, 28], and are restricted to the auto-associative case. Both may be described as special cases of the Hopfield model <ref> [15] </ref>. This model is composed of n McCulloch-Pitts formal neurons 1; . . .; n, connected pair-wise by weights w ij . Throughout this paper we will assume that the weights are symmetric, that is, w ij = w ji .
Reference: [16] <author> A. Jagota. </author> <title> Information capacity of a Hopfield-style memory. </title> <booktitle> In World Congress on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 220-223, </pages> <address> New York, July 1993. Portland, </address> <publisher> IEEE. </publisher>
Reference-contexts: This gives the IC of INN (t) as n=(t+1) , which is fi (n 2 ), asymptotically optimal for constant t, and 2 , exactly optimal for t = 0 (the t = 0 result is also in <ref> [16] </ref>). In earlier work [19], we came up with a different complicated expression for a lower bound on the IC of INN (t), whose value decreased monotonically with t.
Reference: [17] <author> A. Jagota. </author> <title> A Hopfield-style network with a graph-theoretic characterization. </title> <journal> Journal of Artificial Neural Networks, </journal> <volume> 1(1) </volume> <pages> 145-166, </pages> <year> 1994. </year>
Reference-contexts: We have shown earlier that, for any positive integer Q 2, an arbitrary collection of Q-state vectors can be stored in a binary-weights Q-state extension of INN (0), so that all vectors are fixed points <ref> [17] </ref>. In the current paper, we show that the IC of the Q-state INN (0) model remains asymptotically optimal, that is order of N 2 , where N = Q fi n is the number of neurons, in the entire range of admissible values of Q. <p> We now examine these results in the context of particular storage rules for the Willshaw Model and INN. Consider the following storage rule for the INN (0) model <ref> [28, 17] </ref>. Initially, w ij = 1 for all i 6= j. A sequence X 1 ; . . . ; X m of binary vectors of length n is stored as follows. <p> Q-state vectors are stored in a neural grid of N = Q fi n neurons. A neuron is indexed as (q; i) where q 2 f0; . . . ; Q 1g and i 2 f1; . . . ; ng. The storage rule is a generalization of (3) <ref> [17] </ref>. Initially, w (q 1 ;i 1 );(q 2 ;i 2 ) = 1 for all (q 1 ; i 1 ) 6= (q 2 ; i 2 ). <p> Consider any (Q fi n)-unit Q-state INN (0) instance. Every fixed point in such an instance has cardinality at most n <ref> [17] </ref>. Thus every such instance is also a (Q fi n)-unit n-sparse INN (0) instance.
Reference: [18] <author> A. Jagota and G. Narasimhan. </author> <title> A generalization of maximal independent sets, </title> <note> 1994. Submitted. </note>
Reference-contexts: A Maximal degree-0 subgraph is a Maximal Independent Set. We will obtain a lower bound on the number of n-vertex labeled graphs with different collections of Maximal degree-t subgraphs. For this purpose, we employ the following reduction of the Maximum Independent Set problem to the Maximum degree-t subgraph problem <ref> [18] </ref>. Given a graph G = (V; E), with V = fv 1 ; . . . ; v n g, construct a graph G t = (V t ; E t ) as follows.
Reference: [19] <author> A. Jagota, A. Negatu, and D. Kaznachey. </author> <title> Information capacity and fault tolerance of binary weights Hopfield nets. </title> <booktitle> In IEEE International Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 1044-1049, </pages> <address> New York, 1994. Orlando, FL, </address> <publisher> IEEE. </publisher>
Reference-contexts: This gives the IC of INN (t) as n=(t+1) , which is fi (n 2 ), asymptotically optimal for constant t, and 2 , exactly optimal for t = 0 (the t = 0 result is also in [16]). In earlier work <ref> [19] </ref>, we came up with a different complicated expression for a lower bound on the IC of INN (t), whose value decreased monotonically with t. <p> This gave the IC of INN (1) as (n log n), and that of INN (n=2 1) as (log n p n), Lemma 3 is an improvement; for n = o (t), the result in <ref> [19] </ref> is better. Overall, the lower bound given by Lemma 3 decreases more gracefully when t increases from 0. (Since two of the authors of [19] are not amongst those of the current paper, it is useful to note that, other than reference to the above result, none of the new <p> n), and that of INN (n=2 1) as (log n p n), Lemma 3 is an improvement; for n = o (t), the result in <ref> [19] </ref> is better. Overall, the lower bound given by Lemma 3 decreases more gracefully when t increases from 0. (Since two of the authors of [19] are not amongst those of the current paper, it is useful to note that, other than reference to the above result, none of the new ideas, results, or techniques from [19] are used in the current paper.) 4 3.1 Particular Storage Rules The above IC results of WM (t) and <p> bound given by Lemma 3 decreases more gracefully when t increases from 0. (Since two of the authors of <ref> [19] </ref> are not amongst those of the current paper, it is useful to note that, other than reference to the above result, none of the new ideas, results, or techniques from [19] are used in the current paper.) 4 3.1 Particular Storage Rules The above IC results of WM (t) and INN (t) are independent of the storage rule. We now examine these results in the context of particular storage rules for the Willshaw Model and INN.
Reference: [20] <author> J.D. Keeler. </author> <title> Capacity for patterns and sequences in Kanerva's SDM as compared to other associative memory models. </title> <type> Technical report, </type> <institution> Research Institute for Advanced Computer Science: RIACS TR 87.29, NASA Ames Research Center, </institution> <year> 1987. </year>
Reference-contexts: This is the definition of Information Capacity we employ in the current paper. Such a calculation is not necessarily easy, since many instances may realize the same collection of fixed points. Relationship to Other Measures of Capacity. The capacities of various neural associative memories have been studied extensively <ref> [25, 1, 23, 20, 10, 2, 6, 9] </ref>.
Reference: [21] <author> T. Kohonen. </author> <title> Correlation matrix memories. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-21:353-359, </volume> <year> 1972. </year>
Reference-contexts: The proofs that rely on graph-theoretic arguments and notation are postponed to a later section, where some useful concepts from graph theory are introduced first, and then the proofs given. 3 2 The Associative Memory Models The Associative Memory Models that we study here have their roots in <ref> [29, 3, 21, 24, 15] </ref>. The two models we study in this paper employ binary-valued weights [29, 28], and are restricted to the auto-associative case. Both may be described as special cases of the Hopfield model [15].
Reference: [22] <author> G.A. Kohring. </author> <title> On the problems of neural networks with multi-state neurons. </title> <journal> Journal De Physique I, </journal> <volume> 2 </volume> <pages> 1549-1552, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Thus, information calculated from two different definitions gives strikingly different results. In particular, a random instance of the Q-state INN (0) model has much more information than an instance emerging from storing random Q-state vectors. Q-state associative memories have been studied in the past <ref> [27, 22] </ref>. Rieger [27] extended the two-state Hopfield model to Q-states, by using Q-state neurons. <p> He showed that the stable storage capacity, for random Q-state vectors of length n, dropped to fi (n)=Q 2 ; hence the information stored in such a network to fi (n) log 2 Q=Q 2 . Kohring <ref> [22] </ref> modified Rieger's Q-state model and improved the stable storage capacity to fi (n)= log 2 Q; hence the information stored in such a network to fi (n) log 2 Q= log 2 Q, that is, order of n 2 bits.
Reference: [23] <author> R.J. McEliece, E.C. Posner, E.R. Rodemich, and S.S. Venkatesh. </author> <title> The capacity of the Hopfield associative memory. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 33 </volume> <pages> 461-482, </pages> <year> 1987. </year>
Reference-contexts: This is the definition of Information Capacity we employ in the current paper. Such a calculation is not necessarily easy, since many instances may realize the same collection of fixed points. Relationship to Other Measures of Capacity. The capacities of various neural associative memories have been studied extensively <ref> [25, 1, 23, 20, 10, 2, 6, 9] </ref>. <p> Such a calculation is not necessarily easy, since many instances may realize the same collection of fixed points. Relationship to Other Measures of Capacity. The capacities of various neural associative memories have been studied extensively [25, 1, 23, 20, 10, 2, 6, 9]. The capacity definitions employed most frequently <ref> [25, 23, 2, 9] </ref> are instances of the following general one, which is, loosely-stated: Definition 2 The storage capacity is the largest number of randomly selected memories (binary vectors, or vector pairs) that can be stored so that, for sufficiently large number of neurons n, the probability that all (or nearly <p> Let us refer to these definitions as stable storage capacity and basins capacity respectively. The importance of calculating the stable storage and basins capacities of memory models cannot be overstated. These calculations are often done for specific storage rules for the memories <ref> [26, 23, 2] </ref> (for example, the Hebb rule). From these results, the amount of information stored in an instance of the model, arising from storing random vectors using a particular storage rule, can also often be deduced. <p> From these results, the amount of information stored in an instance of the model, arising from storing random vectors using a particular storage rule, can also often be deduced. For example, it is well known (see <ref> [23, 14] </ref>) that order of n= log n n-bit random vectors can be stored in the discrete Hopfield memory using the Hebb rule so that, with probability tending to one, every stored vector is stable and has a large basin of attraction. <p> This result contrasts with that of Palm [25], who required the stored random vectors to be logarithmically sparse, in order for the stored information to be asymptotically optimal. Earlier results on stable storage, basins, or information capacity have relied on statistical [25, 2], coding theory <ref> [23] </ref>, or threshold function counting [1] arguments. By contrast, all our results in the current paper are based on characterizations of the fixed points and graph counting. In the case of the Willshaw model, we characterize the fixed points as certain kinds of induced subgraphs.
Reference: [24] <author> K. Nakano. </author> <title> Associatron|a model of associative memory. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-2:381-38, </volume> <year> 1972. </year>
Reference-contexts: The proofs that rely on graph-theoretic arguments and notation are postponed to a later section, where some useful concepts from graph theory are introduced first, and then the proofs given. 3 2 The Associative Memory Models The Associative Memory Models that we study here have their roots in <ref> [29, 3, 21, 24, 15] </ref>. The two models we study in this paper employ binary-valued weights [29, 28], and are restricted to the auto-associative case. Both may be described as special cases of the Hopfield model [15].
Reference: [25] <author> G. Palm. </author> <title> On associative memory. </title> <journal> Biological Cybernetics, </journal> <volume> 36 </volume> <pages> 19-31, </pages> <year> 1980. </year>
Reference-contexts: This is the definition of Information Capacity we employ in the current paper. Such a calculation is not necessarily easy, since many instances may realize the same collection of fixed points. Relationship to Other Measures of Capacity. The capacities of various neural associative memories have been studied extensively <ref> [25, 1, 23, 20, 10, 2, 6, 9] </ref>. <p> Such a calculation is not necessarily easy, since many instances may realize the same collection of fixed points. Relationship to Other Measures of Capacity. The capacities of various neural associative memories have been studied extensively [25, 1, 23, 20, 10, 2, 6, 9]. The capacity definitions employed most frequently <ref> [25, 23, 2, 9] </ref> are instances of the following general one, which is, loosely-stated: Definition 2 The storage capacity is the largest number of randomly selected memories (binary vectors, or vector pairs) that can be stored so that, for sufficiently large number of neurons n, the probability that all (or nearly <p> Each random n-bit vector has order of n bits of information. Since the size of the collection is small, and the vectors independent, the information can be added to give 1 the cumulative information in the collection as n 2 = log n. As a second example, Palm <ref> [25] </ref> showed that the amount of information stored in the n-unit binary (0=1) weights Willshaw model, using a particular storage rule [29], is maximized when each of the stored random vectors contains order of log n ones. <p> An n-bit random vector containing fi (log n) ones has fi (log 2 n) bits of information [2]. The amount of information in a resulting instance is thus asymptotically of the order n 2 bits <ref> [25] </ref>. Palm's analysis [25] appears to implicitly indicate that the amount of information drops significantly if the stored random vectors are denser. Amari [2] extended the results of Palm as follows. <p> An n-bit random vector containing fi (log n) ones has fi (log 2 n) bits of information [2]. The amount of information in a resulting instance is thus asymptotically of the order n 2 bits <ref> [25] </ref>. Palm's analysis [25] appears to implicitly indicate that the amount of information drops significantly if the stored random vectors are denser. Amari [2] extended the results of Palm as follows. He replaced the binary-weights Willshaw model with the real-valued weights discrete Hopfield model, and employed the Hebb rule for storage. <p> This contrasts with the results of Palm, who required that t be of the order log n, for the stored information derived from storage capacity calculations to be asymptotically optimal <ref> [25] </ref>. How the asymptotically optimal IC we get for non-logarithmic t can be exploited in practice is a separate question. 2 We show that INN (t) has optimal IC of n at t = 0. <p> We give a similar result for the Willshaw model: the IC remains asymptotically optimal for almost all degrees of sparseness. This result contrasts with that of Palm <ref> [25] </ref>, who required the stored random vectors to be logarithmically sparse, in order for the stored information to be asymptotically optimal. Earlier results on stable storage, basins, or information capacity have relied on statistical [25, 2], coding theory [23], or threshold function counting [1] arguments. <p> This result contrasts with that of Palm [25], who required the stored random vectors to be logarithmically sparse, in order for the stored information to be asymptotically optimal. Earlier results on stable storage, basins, or information capacity have relied on statistical <ref> [25, 2] </ref>, coding theory [23], or threshold function counting [1] arguments. By contrast, all our results in the current paper are based on characterizations of the fixed points and graph counting. In the case of the Willshaw model, we characterize the fixed points as certain kinds of induced subgraphs. <p> By Lemma 3, the IC of INN (0), even for this particular storage rule, is n an optimal result that contrasts strikingly with the stable storage capacity result of Grossman. Consider the following storage rule for the WM (t) model <ref> [29, 25] </ref>. Initially, w ij = 0 for all i 6= j. A sequence X 1 ; . . . ; X m of binary vectors of length n is stored as follows. <p> showed that, when random vectors are stored in the network using storage rule (4), order of n 2 bits of information are stored in the resulting network only if the vectors contain order of log 2 n ones, and the threshold t is set to order of log 2 n <ref> [25] </ref>. <p> How to exploit this is a separate issue. 6 Sparse Coding and Information Capacity Sparse coding has been suggested as a mechanism to alleviate the poor storage capacity of neural associative memories <ref> [29, 25, 2, 5] </ref>. Though our previous results in this paper indicate that sparse coding is not necessary to retain high information capacity, by our definition of information capacity, it is useful to calculate whether sparse coding is sufficient. 6.1 The INN Model Consider first the INN model.
Reference: [26] <author> E.M. Palmer. </author> <title> Graphical evolution. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Let us refer to these definitions as stable storage capacity and basins capacity respectively. The importance of calculating the stable storage and basins capacities of memory models cannot be overstated. These calculations are often done for specific storage rules for the memories <ref> [26, 23, 2] </ref> (for example, the Hebb rule). From these results, the amount of information stored in an instance of the model, arising from storing random vectors using a particular storage rule, can also often be deduced.
Reference: [27] <author> H. Rieger. </author> <title> Storing an extensive number of grey-toned patterns in a neural network using multi-state neurons. </title> <journal> Journal of Physics A, </journal> <volume> 23:L1273-L1280, </volume> <year> 1990. </year>
Reference-contexts: Thus, information calculated from two different definitions gives strikingly different results. In particular, a random instance of the Q-state INN (0) model has much more information than an instance emerging from storing random Q-state vectors. Q-state associative memories have been studied in the past <ref> [27, 22] </ref>. Rieger [27] extended the two-state Hopfield model to Q-states, by using Q-state neurons. <p> Thus, information calculated from two different definitions gives strikingly different results. In particular, a random instance of the Q-state INN (0) model has much more information than an instance emerging from storing random Q-state vectors. Q-state associative memories have been studied in the past [27, 22]. Rieger <ref> [27] </ref> extended the two-state Hopfield model to Q-states, by using Q-state neurons. He showed that the stable storage capacity, for random Q-state vectors of length n, dropped to fi (n)=Q 2 ; hence the information stored in such a network to fi (n) log 2 Q=Q 2 .
Reference: [28] <author> I. Shariv, T. Grossman, E. Domany, and A.A. Friesem. </author> <title> All-optical implementation of the inverted neural network model. </title> <journal> In Optics in Complex Systems, </journal> <volume> volume 1319. </volume> <booktitle> SPIE, </booktitle> <year> 1990. </year>
Reference-contexts: For some memory models, both calculations give identical results. For others they give widely different results, as we show in this paper. Overview of Results. In this paper, we study two binary-weights neural associative memory models: the Willshaw Model (WM) [29], and the Inverted Neural Network (INN) <ref> [28] </ref>. Both may be viewed as special cases of the discrete Hopfield model [15]. From each model, we define a family of models indexed by the threshold value t to a neuron (the same for each neuron). We call these families WM (t) and INN (t) respectively. <p> The two models we study in this paper employ binary-valued weights <ref> [29, 28] </ref>, and are restricted to the auto-associative case. Both may be described as special cases of the Hopfield model [15]. This model is composed of n McCulloch-Pitts formal neurons 1; . . .; n, connected pair-wise by weights w ij . <p> In the Willshaw Model (WM), for all i 6= j, w ij 2 f0; 1g. In the Inverted Neural Network (INN), for all i 6= j, w ij 2 f1; 0g. (This is an equivalent reformulation of the INN description in <ref> [28] </ref>, to make INN use the same activation function as WM.) Though the WM and INN architectures are very similar, they will turn out to have Information Capacities that differ at certain extremes of their parameters. 3 Information Capacity of WM and INN In this Section, n is the number of <p> We now examine these results in the context of particular storage rules for the Willshaw Model and INN. Consider the following storage rule for the INN (0) model <ref> [28, 17] </ref>. Initially, w ij = 1 for all i 6= j. A sequence X 1 ; . . . ; X m of binary vectors of length n is stored as follows.

References-found: 28

