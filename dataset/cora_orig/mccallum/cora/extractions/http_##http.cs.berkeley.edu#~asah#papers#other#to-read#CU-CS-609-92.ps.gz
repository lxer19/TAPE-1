URL: http://http.cs.berkeley.edu/~asah/papers/other/to-read/CU-CS-609-92.ps.gz
Refering-URL: http://http.cs.berkeley.edu/~asah/papers/other/to-read/
Root-URL: http://www.cs.berkeley.edu
Title: Portable, Efficient Futures  
Author: David B. Wagner Bradley G. Calder 
Date: August 1992  
Address: Campus Box #430  Boulder 80309-0430  Boulder  
Affiliation: Department of Computer Science  University of Colorado,  ffi University of Colorado at  
Pubnum: CU-CS-609-92  
Abstract: Technical Report CU-CS-609-92 Department of Computer Science Campus Box 430 University of Colorado Boulder, Colorado 80309 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Chatterjee, A., Khanna, A., and Hung, Y. ES-Kit: </author> <title> an object-oriented distributed system. </title> <journal> Concurrency: Practice and Experience 3, </journal> <month> 6 (Dec. </month> <year> 1991), </year> <pages> 525-539. </pages>
Reference-contexts: The syntax used by our interface certainly could be improved. We believe it would increase the transparency and flexibility of the interface to overload the C++ member selection operator, as is done in the ES-kit system <ref> [1] </ref>. We have refrained from doing so because the compiler we are using does not follow the current standard for this operator [4]. This option requires further exploration. Another option that we are considering is writing a simple preprocessor.
Reference: [2] <author> Cooper, E., and Draves, R. C-Threads. </author> <type> Tech. Rep. </type> <institution> CMU-CS-88-154, Carnegie-Mellon University, </institution> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: Our implementation is C++-based, although that is strictly a matter of convenience rather than a limitation of our techniques. The crucial point is that our implementation provides efficiency similar to that of ultra-specialized implementations such as [9], but is immediately portable to any machine that provides a Cthreads <ref> [2] </ref> runtime environment, and can be ported with very little effort to any system that provides process creation, shared memory, and lock synchronization primitives. A secondary contribution of our work is that we explore in detail the problem of applying the futures paradigm to non-recursive applications with complex precedence graphs. <p> Suppose that a worker, X, creates a future F1 and begins executing its continuation K1 (Figure 3 (a)). Now worker Y steals F1 and the scenario becomes that of Figure 3 (b). (In the 1 Our current implementation is built on top of a Cthreads <ref> [2] </ref> runtime library, so each worker is implemented as a Cthread. Ideally each Cthread runs on a different processor, but the implementation works correctly in any case. 2 The user can optionally specify a particular work queue in which to insert the new Future, to aid in load-balancing. <p> We have refrained from doing so because the compiler we are using does not follow the current standard for this operator [4]. This option requires further exploration. Another option that we are considering is writing a simple preprocessor. Our system is currently implemented using Cthreads <ref> [2] </ref> to manage parallelism and synchronization. However, the implementation is easily portable to any platform that provides thread creation primitives, shared memory, and mutual-exclusion locks.
Reference: [3] <author> Eager, D. L., and Zahorjan, J. Chores: </author> <title> Enhanced run-time support for shared-memory parallel computing. </title> <type> Tech. rep., </type> <institution> University of Washington, </institution> <year> 1991. </year>
Reference-contexts: The precedence graph shown in Figure 19 gives no clue how to do this. Our solution is shown in The block-deal algorithm is tricky because it requires the dependences of the program to be known, as in <ref> [3] </ref>. A deadlock will result if an iteration that is currently being executed is dependent on an iteration later in the same worker's block of work. The most straightforward way to insure the correct ordering is to embed the original iteration-spawning code in every BlockFuture.
Reference: [4] <author> Ellis, M. A., and Stroustrup, B. </author> <title> The Annotated C++ Reference Manual. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1990. </year> <month> 35 </month>
Reference-contexts: This would enable the runtime system to track every call to the function, whether inlined or futurized. We have chosen not to pursue this at present because the version of C++ that we are using (Gnu 1.40.0) does not implement member selection in accordance with the current C++ standard <ref> [4] </ref>. 13 * If q.state is NE, the worker finds the work queue in which q resides using the q.creatorid field, and locks that work queue. <p> We believe it would increase the transparency and flexibility of the interface to overload the C++ member selection operator, as is done in the ES-kit system [1]. We have refrained from doing so because the compiler we are using does not follow the current standard for this operator <ref> [4] </ref>. This option requires further exploration. Another option that we are considering is writing a simple preprocessor. Our system is currently implemented using Cthreads [2] to manage parallelism and synchronization. However, the implementation is easily portable to any platform that provides thread creation primitives, shared memory, and mutual-exclusion locks.
Reference: [5] <author> Gabriel, R., and McCarthy, J. </author> <booktitle> Queue-based multi-processing lisp. Proc. 1984 ACM Symp. Lisp and Functional Programming (Aug. </booktitle> <year> 1984), </year> <pages> 25-44. </pages>
Reference-contexts: However, whereas Fortran compiler directives are naturally suited for exposing parallelism in looping constructs, futures are naturally suited for exposing parallelism in recursive constructs. Thus, the bulk of research on futures has been in the context of functional-style languages such as Lisp <ref> [5, 9] </ref>. The following example from [9] shows how futures can be used in Mul-T, a dialect of Scheme. The algorithm recursively sums the leaf values of a binary tree. Figure 1 shows the sequential version of this algorithm, and Figure 2 shows the "futurized" version.
Reference: [6] <author> Goldman, R., and Gabriel, R. </author> <title> Preliminary results with the initial implementation of Qlisp. </title> <booktitle> In Proc. 1989 Conf. on Lisp and Functional Programming (July 1989), ACM SIGPLAN, ACM, </booktitle> <pages> pp. 143-152. </pages>
Reference-contexts: Below that depth the algorithm would execute the recursive calls sequentially, rather than futurizing them. In this way, the programmer takes responsibility for matching the granularity of the tasks to the overheads of the runtime system. This is the approach taken by the Qlisp system <ref> [6, 7] </ref>. In our opinion, this is not as desirable as having the system do it for the programmer automatically. <p> The idea is to dynamically monitor the load and create a new task to compute a future only when processors are idle. Otherwise, the future is executed inline by the task that creates it. This keeps the number of tasks created close to the number of processors available. Qlisp <ref> [6, 7] </ref> provides the programmer with primitives that inspect the state of the system, and allows the programmer to specify an arbitrary predicate to control the inlining of futures.
Reference: [7] <author> Goldman, R., and Gabriel, R. </author> <title> Qlisp: </title> <booktitle> Parallel procesing in Lisp. IEEE Software (July 1989), </booktitle> <pages> 51-59. </pages>
Reference-contexts: Below that depth the algorithm would execute the recursive calls sequentially, rather than futurizing them. In this way, the programmer takes responsibility for matching the granularity of the tasks to the overheads of the runtime system. This is the approach taken by the Qlisp system <ref> [6, 7] </ref>. In our opinion, this is not as desirable as having the system do it for the programmer automatically. <p> The idea is to dynamically monitor the load and create a new task to compute a future only when processors are idle. Otherwise, the future is executed inline by the task that creates it. This keeps the number of tasks created close to the number of processors available. Qlisp <ref> [6, 7] </ref> provides the programmer with primitives that inspect the state of the system, and allows the programmer to specify an arbitrary predicate to control the inlining of futures.
Reference: [8] <author> Halstead, Jr., R. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems 7, </journal> <month> 4 (Oct. </month> <year> 1985), </year> <pages> 501-538. </pages>
Reference-contexts: This reportedly causes blowups of the execution times of their sequential programs in the range of 1.4 to 2.2 [9, Table 1], which tips the balance strongly in favor of our system. 4.2 Recursive computations Futures were originally designed and used in a recursive language <ref> [8] </ref>, so naturally futures are well suited to recursive computations. The type of recursive computations futures can be applied to are those with two or more recursive calls; these recursive calls must operate on disjoint data.
Reference: [9] <author> Mohr, E., Kranz, D., and Halstead, Jr., R. </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 2, </journal> <month> 3 (July </month> <year> 1991), </year> <pages> 264-280. </pages>
Reference-contexts: The principal design rationale behind futures is that "the programmer takes on the burden of identifying what can be computed safely in parallel, leaving the decision of exactly how the division [of work] will take place to the runtime system" <ref> [9] </ref>. This is very similar to parallelizing a Fortran loop by inserting DOALL-style compiler directives. However, whereas Fortran compiler directives are naturally suited for exposing parallelism in looping constructs, futures are naturally suited for exposing parallelism in recursive constructs. <p> However, whereas Fortran compiler directives are naturally suited for exposing parallelism in looping constructs, futures are naturally suited for exposing parallelism in recursive constructs. Thus, the bulk of research on futures has been in the context of functional-style languages such as Lisp <ref> [5, 9] </ref>. The following example from [9] shows how futures can be used in Mul-T, a dialect of Scheme. The algorithm recursively sums the leaf values of a binary tree. Figure 1 shows the sequential version of this algorithm, and Figure 2 shows the "futurized" version. <p> However, whereas Fortran compiler directives are naturally suited for exposing parallelism in looping constructs, futures are naturally suited for exposing parallelism in recursive constructs. Thus, the bulk of research on futures has been in the context of functional-style languages such as Lisp [5, 9]. The following example from <ref> [9] </ref> shows how futures can be used in Mul-T, a dialect of Scheme. The algorithm recursively sums the leaf values of a binary tree. Figure 1 shows the sequential version of this algorithm, and Figure 2 shows the "futurized" version. <p> Even worse, a careless implementation can lead to very subtle deadlock problems in realistic (not contrived) computations. A very elegant, but complicated, solution to these problems was presented by Mohr, Krantz and Halstead <ref> [9] </ref>. Their system, which we will explore in more detail in the next section, is based on continuations. Our main concern with their method is that it seems to be very "wedded" to a particular compiler and architecture, which raises some concerns about portability. <p> Our implementation is C++-based, although that is strictly a matter of convenience rather than a limitation of our techniques. The crucial point is that our implementation provides efficiency similar to that of ultra-specialized implementations such as <ref> [9] </ref>, but is immediately portable to any machine that provides a Cthreads [2] runtime environment, and can be ported with very little effort to any system that provides process creation, shared memory, and lock synchronization primitives. <p> The ideal approach to parallelizing the sum tree algorithm would be to expand the tree "breadth-first by spawning tasks until all processors are busy, and then expanding the tree depth-first within the task on each processor" <ref> [9] </ref>, which is abbreviated "BUSD." The BUSD approach is optimal for a balanced computation tree; however, optimal load balancing is much harder to realize in the general case. In order to achieve BUSD execution, the programmer could limit the creation of tasks to a fixed depth in the algorithm itself. <p> Thus, we conclude that a naive WorkCrew approach would be considerably faster than the one-task-per-future approach for balanced computations, but could be arbitrarily bad for unbalanced computations. 2.3 Lazy Task Creation Mohr et al. <ref> [9] </ref> have devised a clever scheme that they call Lazy Task Creation that solves the problems just discussed. Their technique is built into a compiler for Mul-T, a parallel dialect of Scheme. <p> In Mul-T, (K (future X)) is interpreted to mean, "Start evaluating X in the current task, but save enough information so that its continuation K can be moved to a separate task if another processor becomes idle" <ref> [9] </ref>. This effectively makes the decision to inline any future a revocable one, so the default behavior is that every future is inlined. <p> When such a case arises, the scheduler selects another continuation from the blocked task's stack to work on and cuts the stack again. The authors claim that the costs of doing this are low enough to meet their performance goals. For more details see <ref> [9] </ref>. Obviously, Lazy Task Creation is a completely general approach with the potential for very good performance. If the load is well-balanced, continuation-stealing will happen infrequently, if at all, and the computation will proceed much faster than if the futures had not been inlined. <p> Unfortunately, f2 cannot finish executing because f3 is on top of it on Worker1's stack. Therefore, the computation is deadlocked. We call the dilemma in which Worker1 finds itself a stack dependence. Clearly, the leapfrog depth rule makes stack dependences impossible. Note that Lazy Task Creation <ref> [9] </ref> would solve the analogous problem by allowing Worker1 to dissect its own stack in order to extract the continuation for f2. Thus LTC takes a detection and recovery approach to the deadlock problem, whereas our system practices deadlock avoidance. <p> Unless the granularity of the futurized function is larger than the overhead of creating a future, efficiency will suffer. In order to test out how our futures perform with different granularities we ran a synthetic benchmark, similar to the one in <ref> [9] </ref>. The synthetic benchmark is a modification of the psum-tree program shown in Figure 5. The only difference is that before a leaf returns its value, a for loop is executed that delays the leaf node for a specified number of machine instructions. <p> However, efficiency exceeds 90% for granularities larger than about 750 instructions. Our performance is comparable to but not quite as good as the Encore implementation of Lazy Task Creation reported in <ref> [9] </ref>. However, this is somewhat misleading, because certain futures-related overheads, which are built into the Mul-T compiler, are present in the sequential times reported in that paper. <p> However, this is somewhat misleading, because certain futures-related overheads, which are built into the Mul-T compiler, are present in the sequential times reported in that paper. This reportedly causes blowups of the execution times of their sequential programs in the range of 1.4 to 2.2 <ref> [9, Table 1] </ref>, which tips the balance strongly in favor of our system. 4.2 Recursive computations Futures were originally designed and used in a recursive language [8], so naturally futures are well suited to recursive computations.
Reference: [10] <author> Reiser, M., and Lavenberg, S. </author> <title> Mean value analysis of closed multichain queueing networks. </title> <type> JACM 27, </type> <month> 2 (Apr. </month> <year> 1980), </year> <pages> 313-322. </pages>
Reference-contexts: An example of an iterative computation with a rich dependence set is two-class Mean Value Analysis (2cMVA) <ref> [10] </ref>. 2cMVA has the prototypical structure of a dynamic programming problem; Figure 19 shows an example of a 2cMVA precedence graph. <p> matter in what order the main routine binds the futures to iterations, because of the following two facts: 1. a reference to an unbound future will block until a computation is bound to it. 12 We shall not attempt to define there terms here; the interested reader is referred to <ref> [10] </ref>. 28 QLengths Q [N0][N1]; QLengths* dopop (int n0, int n1) - if (n0 &gt; 0) - QLengths &prevpop = Q [n0-1][n1]; ... = prevpop ... - QLengths &prevpop = Q [n0][n1-1]; ... = prevpop ... - ... computation with local variables ...
Reference: [11] <author> Vandevoorde, M. T., and Roberts, E. S. Workcrews: </author> <title> An abstraction for controlling parallelism. </title> <journal> Int. Journal of Parallel Programming 17, </journal> <volume> 4 (1988), </volume> <pages> 347-366. </pages>
Reference-contexts: Instead, a WorkCrew-style approach can be adopted <ref> [11] </ref>: a future is implemented as a passive object that contains enough information to carry out the computation. These passive objects are then picked up by worker tasks and executed. The number of workers is fixed and is equal to the number of processors.
Reference: [12] <author> Wolfe, M. </author> <title> More iteration space tiling. </title> <booktitle> In Proc. of Supercomputing '89 (Reno, </booktitle> <address> NV, </address> <month> Nov. </month> <year> 1989), </year> <pages> pp. 655-664. </pages>
Reference-contexts: Re-running the computations using manifest constants in place of array accesses eliminated the superlinear speedup, confirming our hypothesis. In order to level the playing field, so to speak, we tiled <ref> [12] </ref> the sequential program (Figure 16) and the matrix-row program (Figure 17). The tiled programs were run using a value of 16 for Gran. In Figure 15, the curves labeled (T) represent the tiled versions of the programs.
Reference: [13] <author> Wolfe, M. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Also note that all nodes within a level are independent of each other. This technique, which is generally referred to as loop skewing <ref> [13, Ch. 8] </ref>, is the key to parallelizing this type of computation. Using our C++ futures it is quite straightforward to parallelize this computation. Figure 22 shows the code for dopop and main.
References-found: 13

