URL: http://www-csag.cs.uiuc.edu/papers/concert-overview.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Email: fachien,vijayk,jplevyakg@cs.uiuc.edu  
Title: The Concert System Compiler and Runtime Support for Efficient, Fine-Grained Concurrent Object-Oriented Programs  
Author: Andrew A. Chien Vijay Karamcheti John Plevyak 
Keyword: Indexing Keywords: Concurrent object-oriented languages, Multicomputers, Parallel Com puting, Compilation, Runtime Systems.  
Date: June 11, 1993  
Address: 1304 W. Springfield Avenue Urbana, IL 61801  
Affiliation: Department of Computer Science  
Abstract: The introduction of concurrency complicates the already difficult task of large-scale programming. Concurrent object-oriented languages provide a mechanism, encapsulation, for managing the increased complexity of large-scale concurrent programs, thereby reducing the difficulty of large scale concurrent programming. In particular, fine-grained object-oriented approaches provide modularity through encapsulation while exposing large degrees of concurrency. Though fine-grained concurrent object-oriented languages are attractive from a programming perspective, they have historically suffered from poor efficiency. The goal of the Concert project is to develop portable, efficient implementations of fine-grained concurrent object-oriented languages. Our approach incorporates careful program analysis and information management at every stage from the compiler to the runtime system. In this document, we outline the basic elements of the Concert approach. In particular, we discuss program analyses, program transformations, their potential payoff, and how they will be embodied in the Concert system. Initial performance results and specific plans for demonstrations and system development are also detailed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gul Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Synchronization constraints are enforced by selective processing of messages. Objects may send messages, create additional objects, and modify their local state in response to a message. A theoretical basis for reasoning about concurrent object oriented programs can be found in <ref> [1] </ref>. In summary, the programmer specifies only the essential aspects of the computation: concurrency control and enabling of computations, but does not manage location, storage, or detailed scheduling explicitly.
Reference: [2] <author> Pierre America. POOL-T: </author> <title> A parallel object-oriented language. </title> <editor> In Aki Yonezawa and Mario Tokoro, editors, </editor> <booktitle> Object-Oriented Concurrent Programming, </booktitle> <pages> pages 199-220. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Our work also differs in emphasis from a wide variety of work in the area of concurrent object-oriented languages. We are primarily concerned with efficiency, while a variety of projects are concerned primarily with language features <ref> [28, 3, 2, 43] </ref>. The most closely related work in this area is the ABCL project [42, 37, 43] which is also pursuing efficient implementations. A recently published description of their runtime parallels many of the techniques found in our optimized runtime system.
Reference: [3] <author> W. C. Athas and C. L. Seitz. </author> <title> Cantor User Report Version 2.0. </title> <type> CalTech Internal Report, </type> <month> January </month> <year> 1987. </year>
Reference-contexts: Our work also differs in emphasis from a wide variety of work in the area of concurrent object-oriented languages. We are primarily concerned with efficiency, while a variety of projects are concerned primarily with language features <ref> [28, 3, 2, 43] </ref>. The most closely related work in this area is the ABCL project [42, 37, 43] which is also pursuing efficient implementations. A recently published description of their runtime parallels many of the techniques found in our optimized runtime system.
Reference: [4] <author> T. Blank. </author> <title> The MasPar MP-1 Architecture. </title> <booktitle> In Proceedings of COMPCON, </booktitle> <pages> pages 20-4. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: We relate the analogous work on compiling for efficiency in each of these models to grain-size tuning in concurrent object-oriented languages. A subset of other approaches to parallel programming can be loosely classified as data parallel, functional, and committed choice. Data parallel approaches <ref> [38, 19, 9, 4] </ref> express parallelism across arrays, collections, or program constructs such as loops in the context of a single control flow model. Data parallel programs admit a degree of grain size tuning, operations in a data parallel operation can be grouped and scheduled together.
Reference: [5] <author> C. Chambers and D. Ungar. </author> <title> Customization: Optimizing compiler technology for self, a dynamically-typed object-oriented programming language. </title> <booktitle> In Proceedings of SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 146-60, </pages> <year> 1989. </year>
Reference-contexts: In particular, many of the optimizations we have discussed have their inspiration in the techniques developed by the SELF compiler group <ref> [5] </ref>. Of course, the major distinction is that our work focuses on the problems associated with concurrency, distribution, and data parallelism. Our work also differs in emphasis from a wide variety of work in the area of concurrent object-oriented languages. <p> This second generation compiler uses relatively traditional internal data structures based on the program dependence graph (PDG) in Static Single Assignment form (SSA). Novel aspects of the compiler include a constraint-based type inference system and an attributed value system similar to that used in the SELF compiler <ref> [5] </ref>. We are currently developing the structure analysis and optimizations subsystems. The second generation runtime system which supports the full range of runtime operations described in this document and in [25] has been operational since March 1993.
Reference: [6] <author> Rohit Chandra, Anoop Gupta, and John L. Hennessy. </author> <title> Data locality and load balancing in COOL. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1993. </year>
Reference-contexts: Consequently, they appear to be worthwhile only when many objects on a particular node exhibit affinity for another object. The effectiveness of hints is currently unknown, but some related studies show that locality hints can improve performance <ref> [6] </ref>. While speculative optimizations can dramatically increase the opportunities for optimization, they are limited to cases where the possibilities can be narrowed and specialized code generated for each.
Reference: [7] <author> K. Mani Chandy and Carl Kesselman. </author> <title> Compositional C++: Compositional parallel programming. </title> <booktitle> In Proceedings of the Fifth Workshop on Compilers and Languages for Parallel Computing, </booktitle> <address> New Haven, Connecticut, </address> <year> 1992. </year> <note> YALEU/DCS/RR-915, Springer-Verlag Lecture Notes in Computer Science, </note> <year> 1993. </year>
Reference-contexts: Further, to date, the ABCL group has focused primarily on runtime techniques and not compiler analysis and optimization. Recently, a great deal of attention has been focused on concurrent languages based on C++ [35] extensions. ESKit C++ [34], Mentat [18], CHARM++ [24], and Compositional C++ <ref> [7] </ref> are medium-grained languages in which the programmer supplies grain-size information. These languages integrate concurrency and object-orientation, but requiring the programmer to specify a grain size for efficiency limits program scalability and portability. Typically, the specified grain size is large and limits scalability.
Reference: [8] <author> D. Chase, M. Wegman, and F. Zadeck. </author> <title> Analysis of pointers and structures. </title> <booktitle> In Proceedings of SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 296-310, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The Concert system uses a structure analysis based on Abstract Storage Graphs (ASG's) which is capable of precise analysis of complex dynamic structures such as singly and doubly linked lists, object hierarchies, and octrees. The Abstract Storage Graph [30] is an enhancement of Chase's Storage Shape Graph (SSG) <ref> [8] </ref>, which adds distinct node and reference types as well as identity paths to achieve more precise analysis. As with SSG's, the analysis is based on a data flow framework. Within the framework, we define the lattice of solutions to be an abstraction of the program store.
Reference: [9] <author> Chen and Cowie. </author> <title> Prototyping FORTRAN-90 compilers for massively parallel machines. </title> <booktitle> In Proceedings of SIGPLAN PLDI, </booktitle> <year> 1992. </year>
Reference-contexts: We relate the analogous work on compiling for efficiency in each of these models to grain-size tuning in concurrent object-oriented languages. A subset of other approaches to parallel programming can be loosely classified as data parallel, functional, and committed choice. Data parallel approaches <ref> [38, 19, 9, 4] </ref> express parallelism across arrays, collections, or program constructs such as loops in the context of a single control flow model. Data parallel programs admit a degree of grain size tuning, operations in a data parallel operation can be grouped and scheduled together.
Reference: [10] <author> A. A. Chien and W. J. Dally. </author> <title> Concurrent Aggregates (CA). </title> <booktitle> In Proceedings of Second Symposium on Principles and Practice of Parallel Programming. ACM, </booktitle> <month> March </month> <year> 1990. </year>
Reference-contexts: The pC++ [27] language supports data parallel operations, but the object-oriented framework of the language allow encapsulation. This means that pC++ programs can express restricted het-erogenous concurrency within collections. However, true task level parallelism cannot be expressed. The parallel collections in pC++ are similar to aggregates in Concurrent Aggregates <ref> [10] </ref>. In the Concert system, data parallelism is expressed as task level concurrency, providing greater programming power, but making efficient implementation significantly more difficult. Effective grain size tuning must be achieved to make data parallel operations efficient.
Reference: [11] <author> Andrew A. Chien. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively-Parallel Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: In fact, we are considering building compiler front-ends for several different source languages. This would demonstrate the genericity of the optimizations, as well as allow direct comparisons isolating the cost of programming language features. The specific language we are supporting initially is an extended version of Concurrent Aggregates (CA) <ref> [11] </ref>. As with most concurrent object-oriented languages, CA augments the basic asynchronous message passing model with common idioms (such as RPC and tail forwarding [21]), inheritance (a mechanism for code reuse), and some concurrency control constructs. <p> The Concert system has been operational on both sequential and parallel platforms since Oc-tober 1992. The system includes an optimizing compiler for an extended version of Concurrent Aggregates <ref> [11] </ref> and a high performance runtime system which runs on both Sun workstations and the Thinking Machines CM5 [25]. We are currently building a second generation system with much greater analysis and optimization capability as well as much greater overall performance.
Reference: [12] <author> Intel Corporation. </author> <title> Paragon XP/S product overview. Product Overview, </title> <year> 1991. </year>
Reference-contexts: In particular, we make no assumptions about special mechanisms for local synchronization, global synchronization, or a global address space. This model is compatible with a broad range of existing and announced multicomputers <ref> [39, 13, 12] </ref>. In such machines, the fundamental performance issues are balancing the level of concurrency exploited against the cost of scheduling and context switching, exploiting data locality within and between nodes and achieving a reasonably balanced work distribution throughout the machine.
Reference: [13] <institution> Cray Research, Inc., Eagan, Minnesota 55121. CRAY T3D Software Overview Technical Note, </institution> <year> 1992. </year>
Reference-contexts: In particular, we make no assumptions about special mechanisms for local synchronization, global synchronization, or a global address space. This model is compatible with a broad range of existing and announced multicomputers <ref> [39, 13, 12] </ref>. In such machines, the fundamental performance issues are balancing the level of concurrency exploited against the cost of scheduling and context switching, exploiting data locality within and between nodes and achieving a reasonably balanced work distribution throughout the machine.
Reference: [14] <author> D. Culler, A. Sah, K. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages an Operating Systems, </booktitle> <pages> pages 164-75, </pages> <year> 1991. </year>
Reference-contexts: However, data parallel languages have difficulty expressing task level concurrency or irregular concurrency. Further, all of the data parallel languages provide essentially no support for encapsulation or modularity. Functional programming approaches <ref> [41, 14, 23] </ref> have the advantage of determinacy, but have limited expressive power due to the absence of state. If laziness or non-strictness is incorporated, efficient compilation becomes difficult.
Reference: [15] <author> W. J. Dally, A. Chien, S. Fiske, W. Horwat, J. Keen, M. Larivee, R. Lethin, P. Nuth, S. Wills, P. Carrick, and G. Fyler. </author> <title> The J-Machine: A fine-grain concurrent computer. </title> <booktitle> In Information Processing 89, Proceedings of the IFIP Congress, </booktitle> <pages> pages 1147-1153, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: While attractive for programmability reasons, the primary drawback of fine-grained, concurrent object-oriented languages to date has been their inefficiency (compared to their competitors such as parallel FORTRAN dialects). In addition, the most efficient implementations of such languages have relied on specialized hardware to achieve high performance <ref> [15, 36, 42] </ref>. The primary goal of the Concert project is to develop compiler and runtime techniques to make fine-grained concurrent object-oriented languages portable and efficient. By portable and efficient, we mean that the programs should run efficiently both on uniprocessors and on parallel computers built from stock microprocessors. <p> Efficient concurrent object-oriented language implementations must provide a global object namespace, communication services for remote method invocation, and support for scheduling method invocations. Though implementations on custom hardware <ref> [15, 36, 42] </ref> focus on providing a few general-purpose primitives, runtime systems on stock hardware require a different approach. The hardware structure of such systems necessarily implies a hierarchy of costs for many basic runtime operations.
Reference: [16] <author> L. Peter Deutsch and Allan M. Schiffman. </author> <title> Efficient implementation of the smalltalk-80 system. </title> <booktitle> In Eleventh Symposium on Principles of Programming Languages, </booktitle> <pages> pages 297-302. </pages> <publisher> ACM, </publisher> <year> 1984. </year>
Reference-contexts: The ultimate goal of such studies is to show that COOP languages are a viable and attractive basis for efficient parallel computation. The Concert System 17 5 Background and Related Work The Concert system is related to prior work on efficient implementation of both object-oriented <ref> [22, 31, 16] </ref> and parallel systems [20, 26]. In particular, many of the optimizations we have discussed have their inspiration in the techniques developed by the SELF compiler group [5]. Of course, the major distinction is that our work focuses on the problems associated with concurrency, distribution, and data parallelism.
Reference: [17] <author> I. Foster and S. Taylor. Strand: </author> <title> New Concepts in Parallel Programming. </title> <publisher> Prentice-Hall, </publisher> <year> 1990. </year> <title> The Concert System 20 </title>
Reference-contexts: The particular problem is a similar one of grain size tuning, but under much more difficult circumstances where little synchronization and data reference information may be available at compile time. Concurrent logic programming approaches, particularly those based on the committed-choice The Concert System 18 model <ref> [40, 17, 33, 32] </ref> are similar to concurrent object-oriented languages. However, they have little support for encapsulation and parallel collections. In committed choice languages, the emphasis is on task parallelism which is often expressed as operations on a stream.
Reference: [18] <author> A. Grimshaw. </author> <title> Easy-to-use object-oriented parallel processing with Mentat. Overview for Mentat system, </title> <year> 1992. </year>
Reference-contexts: Further, to date, the ABCL group has focused primarily on runtime techniques and not compiler analysis and optimization. Recently, a great deal of attention has been focused on concurrent languages based on C++ [35] extensions. ESKit C++ [34], Mentat <ref> [18] </ref>, CHARM++ [24], and Compositional C++ [7] are medium-grained languages in which the programmer supplies grain-size information. These languages integrate concurrency and object-orientation, but requiring the programmer to specify a grain size for efficiency limits program scalability and portability. Typically, the specified grain size is large and limits scalability.
Reference: [19] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler Optimizations for FORTRAN D on MIMD Distributed-Memory Machines. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 86-100, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: We relate the analogous work on compiling for efficiency in each of these models to grain-size tuning in concurrent object-oriented languages. A subset of other approaches to parallel programming can be loosely classified as data parallel, functional, and committed choice. Data parallel approaches <ref> [38, 19, 9, 4] </ref> express parallelism across arrays, collections, or program constructs such as loops in the context of a single control flow model. Data parallel programs admit a degree of grain size tuning, operations in a data parallel operation can be grouped and scheduled together.
Reference: [20] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler optimizations for FORTRAN D on mimd distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <month> August </month> <year> 1992. </year>
Reference-contexts: The Concert System 17 5 Background and Related Work The Concert system is related to prior work on efficient implementation of both object-oriented [22, 31, 16] and parallel systems <ref> [20, 26] </ref>. In particular, many of the optimizations we have discussed have their inspiration in the techniques developed by the SELF compiler group [5]. Of course, the major distinction is that our work focuses on the problems associated with concurrency, distribution, and data parallelism.
Reference: [21] <author> W. Horwat, A. Chien, and W. Dally. </author> <title> Experience with cst: </title> <booktitle> Programming and implementation. In Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 101-9. </pages> <booktitle> ACM SIGPLAN, </booktitle> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: The specific language we are supporting initially is an extended version of Concurrent Aggregates (CA) [11]. As with most concurrent object-oriented languages, CA augments the basic asynchronous message passing model with common idioms (such as RPC and tail forwarding <ref> [21] </ref>), inheritance (a mechanism for code reuse), and some concurrency control constructs. Other novel aspects of Concurrent Aggregates include aggregates (parallel collections), and meta-level structures for parallel composition, first class messages and continuations.
Reference: [22] <author> R. E. Johnson, J. O. Graver, and L. W. Zurawski. </author> <title> Ts: An optimizing compiler for smalltalk. </title> <booktitle> In OOPSLA '88 Proceedings, </booktitle> <pages> pages 18-26, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: The ultimate goal of such studies is to show that COOP languages are a viable and attractive basis for efficient parallel computation. The Concert System 17 5 Background and Related Work The Concert system is related to prior work on efficient implementation of both object-oriented <ref> [22, 31, 16] </ref> and parallel systems [20, 26]. In particular, many of the optimizations we have discussed have their inspiration in the techniques developed by the SELF compiler group [5]. Of course, the major distinction is that our work focuses on the problems associated with concurrency, distribution, and data parallelism.
Reference: [23] <editor> Simon L Peyton Jones. </editor> <booktitle> The Implementation of Functional Programming Languages. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1987. </year>
Reference-contexts: However, data parallel languages have difficulty expressing task level concurrency or irregular concurrency. Further, all of the data parallel languages provide essentially no support for encapsulation or modularity. Functional programming approaches <ref> [41, 14, 23] </ref> have the advantage of determinacy, but have limited expressive power due to the absence of state. If laziness or non-strictness is incorporated, efficient compilation becomes difficult.
Reference: [24] <author> L. V. Kale and Sanjeev Krishnan. CHARM++: </author> <title> A portable concurrent object oriented system based on C++. </title> <booktitle> In Proceedings of OOPSLA'93, </booktitle> <year> 1993. </year>
Reference-contexts: Further, to date, the ABCL group has focused primarily on runtime techniques and not compiler analysis and optimization. Recently, a great deal of attention has been focused on concurrent languages based on C++ [35] extensions. ESKit C++ [34], Mentat [18], CHARM++ <ref> [24] </ref>, and Compositional C++ [7] are medium-grained languages in which the programmer supplies grain-size information. These languages integrate concurrency and object-orientation, but requiring the programmer to specify a grain size for efficiency limits program scalability and portability. Typically, the specified grain size is large and limits scalability.
Reference: [25] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Concert efficient runtime support for concurrent object-oriented programming languages on stock hardware. </title> <note> Submitted to SUPERCOMPUTING'93. </note>
Reference-contexts: Object merging and grouping optimizations can reduce execution overhead significantly. Tiling pairs of elements in a list can produce a 50% reduction in overhead. Consider that in the Concert runtime <ref> [25] </ref>, the most general message send operation requires 44:2s, and a simple procedure call requires only 0:15s. This means traversing each pair of list elements requires 88:4s for the List1 implementation, 44:35s for the List2 implementation, and 44:2s for the List3 implementation. <p> Basic operations such as communication, invocation, and name service can have dramatically different cost, depending on the generality of the operation required. On stock hardware in particular, these differences can be greater than two orders of magnitude. These differences are illustrated in Table 1 and discussed in detail in <ref> [25] </ref>. The ability to choose the cheapest appropriate mechanism is essential to achieving good performance. <p> The Concert system has been operational on both sequential and parallel platforms since Oc-tober 1992. The system includes an optimizing compiler for an extended version of Concurrent Aggregates [11] and a high performance runtime system which runs on both Sun workstations and the Thinking Machines CM5 <ref> [25] </ref>. We are currently building a second generation system with much greater analysis and optimization capability as well as much greater overall performance. This second generation Concert system consists of a new implementation of the compiler (in progress) and a new implementation of the runtime system (already complete). <p> We are currently developing the structure analysis and optimizations subsystems. The second generation runtime system which supports the full range of runtime operations described in this document and in <ref> [25] </ref> has been operational since March 1993. Novel aspects include: 1) providing a hierarchy of functionality and cost for each runtime operation, allowing selection of the cheapest version and 2) providing support for speculative and dynamic compilation.
Reference: [26] <author> D. J. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 207-18, </pages> <year> 1981. </year>
Reference-contexts: The Concert System 17 5 Background and Related Work The Concert system is related to prior work on efficient implementation of both object-oriented [22, 31, 16] and parallel systems <ref> [20, 26] </ref>. In particular, many of the optimizations we have discussed have their inspiration in the techniques developed by the SELF compiler group [5]. Of course, the major distinction is that our work focuses on the problems associated with concurrency, distribution, and data parallelism.
Reference: [27] <author> J. Lee and D. Gannon. </author> <title> Object oriented parallel programming. </title> <booktitle> In Proceedings of the ACM/IEEE Conference on Supercomputing. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Typically, the specified grain size is large and limits scalability. Automatic parallelization of large grains is known to be quite difficult. Further, to date, none of them has focused on developing the compiler support necessary to automatically adjust grain size, the primary focus of the Concert system. The pC++ <ref> [27] </ref> language supports data parallel operations, but the object-oriented framework of the language allow encapsulation. This means that pC++ programs can express restricted het-erogenous concurrency within collections. However, true task level parallelism cannot be expressed. The parallel collections in pC++ are similar to aggregates in Concurrent Aggregates [10].
Reference: [28] <author> Carl R. Manning. Acore: </author> <title> The design of a core actor language and its compiler. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: Our work also differs in emphasis from a wide variety of work in the area of concurrent object-oriented languages. We are primarily concerned with efficiency, while a variety of projects are concerned primarily with language features <ref> [28, 3, 2, 43] </ref>. The most closely related work in this area is the ABCL project [42, 37, 43] which is also pursuing efficient implementations. A recently published description of their runtime parallels many of the techniques found in our optimized runtime system.
Reference: [29] <author> N. Oxhtj, J. Palsberg, and M. Schwartzbach. </author> <title> Making type inference practical. </title> <booktitle> In Proceedings of OOPSLA '92, </booktitle> <year> 1992. </year>
Reference-contexts: The type inference system is an extension of that described in <ref> [29] </ref>. Local constraints are established and propagated according to a set of rules for primitive operations such as object creation, assignment, and usage. A continuous approximation of the control flow is maintained, and constraints are built for the paths along this flow. <p> The available type information is used to determine the interprocedural control flow graph required for a variety of traditional program analyses. The Concert type inference system extends the capability of of traditional constraint based type systems for object-oriented languages <ref> [29] </ref> in four ways. First, the extended inference system allows variable precision based on control flow and object creation points. This allows compiler effort to be focused where it will be most productive.
Reference: [30] <author> John Plevyak, Vijay Karamcheti, and Andrew Chien. </author> <title> Analysis of dynamic structures for efficient parallel execution. </title> <note> Revised Paper, submitted to LCPM '93. </note>
Reference-contexts: The Concert system uses a structure analysis based on Abstract Storage Graphs (ASG's) which is capable of precise analysis of complex dynamic structures such as singly and doubly linked lists, object hierarchies, and octrees. The Abstract Storage Graph <ref> [30] </ref> is an enhancement of Chase's Storage Shape Graph (SSG) [8], which adds distinct node and reference types as well as identity paths to achieve more precise analysis. As with SSG's, the analysis is based on a data flow framework.
Reference: [31] <author> A. D. Samples, D. Ungar, and P. Hilfinger. </author> <title> Soar: Smalltalk without bytecodes. </title> <booktitle> In OOPSLA '86 Prodeedings, </booktitle> <pages> pages 107-18, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: The ultimate goal of such studies is to show that COOP languages are a viable and attractive basis for efficient parallel computation. The Concert System 17 5 Background and Related Work The Concert system is related to prior work on efficient implementation of both object-oriented <ref> [22, 31, 16] </ref> and parallel systems [20, 26]. In particular, many of the optimizations we have discussed have their inspiration in the techniques developed by the SELF compiler group [5]. Of course, the major distinction is that our work focuses on the problems associated with concurrency, distribution, and data parallelism.
Reference: [32] <author> V. Saraswat. </author> <title> Concurrent Constraint Programming Languages. </title> <publisher> MIT Press, </publisher> <year> 1992. </year> <note> To appear, also available as Technical Report from Carnegie-Mellon University as Technical Report CMU-CS-89-108. </note>
Reference-contexts: The particular problem is a similar one of grain size tuning, but under much more difficult circumstances where little synchronization and data reference information may be available at compile time. Concurrent logic programming approaches, particularly those based on the committed-choice The Concert System 18 model <ref> [40, 17, 33, 32] </ref> are similar to concurrent object-oriented languages. However, they have little support for encapsulation and parallel collections. In committed choice languages, the emphasis is on task parallelism which is often expressed as operations on a stream.
Reference: [33] <author> V. Saraswat, K. Kahn, and J. Levy. </author> <title> Janus: A step towards distributed constraint programming. </title> <booktitle> In Proceedings of the North American Conference on Logic Programming, </booktitle> <address> Austin, Texas, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: The particular problem is a similar one of grain size tuning, but under much more difficult circumstances where little synchronization and data reference information may be available at compile time. Concurrent logic programming approaches, particularly those based on the committed-choice The Concert System 18 model <ref> [40, 17, 33, 32] </ref> are similar to concurrent object-oriented languages. However, they have little support for encapsulation and parallel collections. In committed choice languages, the emphasis is on task parallelism which is often expressed as operations on a stream.
Reference: [34] <author> K. Smith and R. Smith II. </author> <title> The experimental systems project at the microelectronics and computer technology corporation. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercube Computers, </booktitle> <year> 1989. </year>
Reference-contexts: Further, to date, the ABCL group has focused primarily on runtime techniques and not compiler analysis and optimization. Recently, a great deal of attention has been focused on concurrent languages based on C++ [35] extensions. ESKit C++ <ref> [34] </ref>, Mentat [18], CHARM++ [24], and Compositional C++ [7] are medium-grained languages in which the programmer supplies grain-size information. These languages integrate concurrency and object-orientation, but requiring the programmer to specify a grain size for efficiency limits program scalability and portability.
Reference: [35] <author> Bjarne Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison Wesley, </publisher> <address> second edition, </address> <year> 1991. </year>
Reference-contexts: Further, to date, the ABCL group has focused primarily on runtime techniques and not compiler analysis and optimization. Recently, a great deal of attention has been focused on concurrent languages based on C++ <ref> [35] </ref> extensions. ESKit C++ [34], Mentat [18], CHARM++ [24], and Compositional C++ [7] are medium-grained languages in which the programmer supplies grain-size information. These languages integrate concurrency and object-orientation, but requiring the programmer to specify a grain size for efficiency limits program scalability and portability.
Reference: [36] <author> T. Baba, et al. </author> <title> A parallel object-oriented total architecture: </title> <booktitle> A-NET. In Proceedings of IEEE Supercomputing '90, </booktitle> <pages> pages 276-285. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1990. </year>
Reference-contexts: While attractive for programmability reasons, the primary drawback of fine-grained, concurrent object-oriented languages to date has been their inefficiency (compared to their competitors such as parallel FORTRAN dialects). In addition, the most efficient implementations of such languages have relied on specialized hardware to achieve high performance <ref> [15, 36, 42] </ref>. The primary goal of the Concert project is to develop compiler and runtime techniques to make fine-grained concurrent object-oriented languages portable and efficient. By portable and efficient, we mean that the programs should run efficiently both on uniprocessors and on parallel computers built from stock microprocessors. <p> Efficient concurrent object-oriented language implementations must provide a global object namespace, communication services for remote method invocation, and support for scheduling method invocations. Though implementations on custom hardware <ref> [15, 36, 42] </ref> focus on providing a few general-purpose primitives, runtime systems on stock hardware require a different approach. The hardware structure of such systems necessarily implies a hierarchy of costs for many basic runtime operations.
Reference: [37] <author> K. Taura, S. Matsuoka, and A. Yonezawa. </author> <title> An efficient implementation scheme of concurrent object-oriented languages on stock multicomputers. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming, </booktitle> <year> 1993. </year>
Reference-contexts: We are primarily concerned with efficiency, while a variety of projects are concerned primarily with language features [28, 3, 2, 43]. The most closely related work in this area is the ABCL project <ref> [42, 37, 43] </ref> which is also pursuing efficient implementations. A recently published description of their runtime parallels many of the techniques found in our optimized runtime system. While our research goals are similar, ABCL is significantly different from our source language as it has no support for parallel collections.
Reference: [38] <author> Thinking Machines Corporation. </author> <title> Getting Started in CM Fortran, </title> <booktitle> 1990. The Concert System 21 </booktitle>
Reference-contexts: We relate the analogous work on compiling for efficiency in each of these models to grain-size tuning in concurrent object-oriented languages. A subset of other approaches to parallel programming can be loosely classified as data parallel, functional, and committed choice. Data parallel approaches <ref> [38, 19, 9, 4] </ref> express parallelism across arrays, collections, or program constructs such as loops in the context of a single control flow model. Data parallel programs admit a degree of grain size tuning, operations in a data parallel operation can be grouped and scheduled together.
Reference: [39] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, Massachusets. </address> <note> CM5 Technical Summary, </note> <month> October </month> <year> 1991. </year>
Reference-contexts: In particular, we make no assumptions about special mechanisms for local synchronization, global synchronization, or a global address space. This model is compatible with a broad range of existing and announced multicomputers <ref> [39, 13, 12] </ref>. In such machines, the fundamental performance issues are balancing the level of concurrency exploited against the cost of scheduling and context switching, exploiting data locality within and between nodes and achieving a reasonably balanced work distribution throughout the machine.
Reference: [40] <author> K. Ueda and M Morita. </author> <title> A new implementation technique for flat GHC. </title> <booktitle> In Proceedings Seventh International Conference on Logic Programming, </booktitle> <pages> pages 3-17. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> Revised version to appear in New Generation Computing. </note>
Reference-contexts: The particular problem is a similar one of grain size tuning, but under much more difficult circumstances where little synchronization and data reference information may be available at compile time. Concurrent logic programming approaches, particularly those based on the committed-choice The Concert System 18 model <ref> [40, 17, 33, 32] </ref> are similar to concurrent object-oriented languages. However, they have little support for encapsulation and parallel collections. In committed choice languages, the emphasis is on task parallelism which is often expressed as operations on a stream.
Reference: [41] <institution> Yale University, </institution> <address> New Haven, Connecticut. </address> <booktitle> Report on the Programming Language Haskell, </booktitle> <address> 1.0 edition, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: However, data parallel languages have difficulty expressing task level concurrency or irregular concurrency. Further, all of the data parallel languages provide essentially no support for encapsulation or modularity. Functional programming approaches <ref> [41, 14, 23] </ref> have the advantage of determinacy, but have limited expressive power due to the absence of state. If laziness or non-strictness is incorporated, efficient compilation becomes difficult.
Reference: [42] <author> M. Yasugi, S. Matsuoka, and A. Yonezawa. ABCL/onEM-4: </author> <title> A new software/hardware architecture for object-oriented concurrent computing on an extended dataflow supercomputer. </title> <booktitle> In Proceedings of the ACM Conference on Supercomputing '92, </booktitle> <year> 1992. </year>
Reference-contexts: While attractive for programmability reasons, the primary drawback of fine-grained, concurrent object-oriented languages to date has been their inefficiency (compared to their competitors such as parallel FORTRAN dialects). In addition, the most efficient implementations of such languages have relied on specialized hardware to achieve high performance <ref> [15, 36, 42] </ref>. The primary goal of the Concert project is to develop compiler and runtime techniques to make fine-grained concurrent object-oriented languages portable and efficient. By portable and efficient, we mean that the programs should run efficiently both on uniprocessors and on parallel computers built from stock microprocessors. <p> Efficient concurrent object-oriented language implementations must provide a global object namespace, communication services for remote method invocation, and support for scheduling method invocations. Though implementations on custom hardware <ref> [15, 36, 42] </ref> focus on providing a few general-purpose primitives, runtime systems on stock hardware require a different approach. The hardware structure of such systems necessarily implies a hierarchy of costs for many basic runtime operations. <p> We are primarily concerned with efficiency, while a variety of projects are concerned primarily with language features [28, 3, 2, 43]. The most closely related work in this area is the ABCL project <ref> [42, 37, 43] </ref> which is also pursuing efficient implementations. A recently published description of their runtime parallels many of the techniques found in our optimized runtime system. While our research goals are similar, ABCL is significantly different from our source language as it has no support for parallel collections.
Reference: [43] <editor> Akinori Yonezawa, editor. </editor> <title> ABCL: An Object-Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> ISBN 0-262-24029-7. The Concert System 22 </note>
Reference-contexts: Our work also differs in emphasis from a wide variety of work in the area of concurrent object-oriented languages. We are primarily concerned with efficiency, while a variety of projects are concerned primarily with language features <ref> [28, 3, 2, 43] </ref>. The most closely related work in this area is the ABCL project [42, 37, 43] which is also pursuing efficient implementations. A recently published description of their runtime parallels many of the techniques found in our optimized runtime system. <p> We are primarily concerned with efficiency, while a variety of projects are concerned primarily with language features [28, 3, 2, 43]. The most closely related work in this area is the ABCL project <ref> [42, 37, 43] </ref> which is also pursuing efficient implementations. A recently published description of their runtime parallels many of the techniques found in our optimized runtime system. While our research goals are similar, ABCL is significantly different from our source language as it has no support for parallel collections.
References-found: 43

