URL: http://www.cs.unc.edu/~billmark/i3dwww/i3dpaper-web.ps.gz
Refering-URL: http://www.cs.unc.edu/~ibr/pubs.html
Root-URL: http://www.cs.unc.edu
Title: Post-Rendering 3D Warping  
Author: William R. Mark Leonard McMillan Gary Bishop 
Keyword: CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/Image Generation Display Algorithms; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Virtual Reality; I.3.2 [Computer Graphics]: Graphics Systems Distributed/Network Graphics. Additional Keywords: image-based rendering, post-rendering warping, image compositing and reconstruction, remote display, 3D warp.  
Address: Chapel Hill  
Affiliation: Department of Computer Science University of North Carolina at  
Note: In Proceedings of 1997 Symposium on Interactive 3D Graphics, Providence, RI, April 27-30, 1997, pp. 7-16.  
Abstract: A pair of rendered images and their Z-buffers contain almost all of the information necessary to re-render from nearby viewpoints. For the small changes in viewpoint that occur in a fraction of a second, this information is sufficient for high quality re-rendering with cost independent of scene complexity. Re-rendering from previously computed views allows an order-of-magnitude increase in apparent frame rate over that provided by conventional rendering alone. It can also compensate for system latency in local or remote display. We use McMillan and Bishop's image warping algorithm to re-render, allowing us to compensate for viewpoint translation as well as rotation. We avoid occlusion-related artifacts by warping two different reference images and compositing the results. This paper explains the basic design of our system and provides details of our reconstruction and multi-image compositing algorithms. We present our method for selecting reference image locations and the heuristic we use for any portions of the scene which happen to be occluded in both reference images. We also discuss properties of our technique which make it suitable for real-time implementation, and briefly describe our simpler real-time remote display system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Stephen J. Adelson and Larry F. Hodges. </author> <title> Stereoscopic ray-tracing. </title> <journal> The Visual Computer, </journal> <volume> 10(3) </volume> <pages> 127-144, </pages> <year> 1993. </year>
Reference-contexts: The algorithm is demonstrated on pre-rendered synthetic imagery. A later system [28] used a 3D cylindrical-to-planar warp for acquired imagery. Similar warping calculations have been used to accelerate ray tracing of animations and stereo pairs <ref> [1, 2, 7] </ref>. Greene and Kass [15] generated simplified image-like representations of a scene by retaining only those polygons which were visible in a Z-buffered image from the reference viewpoint.
Reference: [2] <author> Stephen J. Adelson and Larry F. Hodges. </author> <title> Generating exact ray-traced animation frames by reprojection. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 15(3) </volume> <pages> 43-52, </pages> <year> 1995. </year>
Reference-contexts: The algorithm is demonstrated on pre-rendered synthetic imagery. A later system [28] used a 3D cylindrical-to-planar warp for acquired imagery. Similar warping calculations have been used to accelerate ray tracing of animations and stereo pairs <ref> [1, 2, 7] </ref>. Greene and Kass [15] generated simplified image-like representations of a scene by retaining only those polygons which were visible in a Z-buffered image from the reference viewpoint.
Reference: [3] <author> Daniel G. Aliaga. </author> <title> Visualization of complex models using dynamic texture-based simplification. </title> <booktitle> In Proceedings of IEEE Visualization 96, </booktitle> <pages> pages 101-106, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: this transform in hardware, with the goal of both reducing latency and (in conjunction with their priority rendering technique) increasing frame rate [29, 30]. 2D projective transforms can also be applied to selected parts of a scene by mapping recently rendered imagery onto large textured polygons before performing final rendering <ref> [3, 32] </ref>. 3.2 Warping From Stored Images Image warping can be used to display previously stored imagery. This imagery can be either rendered off-line, or acquired from the real world by cameras. Lippman's movie maps system [20] allows virtual movement through a city along controlled routes. <p> This last system actually renders the near geometry in the standard fashion into every derived frameonly the far geometry is represented in image form. The texture-based simplification systems mentioned earlier <ref> [3, 32] </ref> can also be considered to be layered systems. 3.5 Predictive Tracking Prediction of future viewpoint and view direction can be used in place of, or in conjunction with, image warping to compensate for latency. Predictive tracking has been combined with image shifting by [9, 25, 31, 33].
Reference: [4] <author> Ronald Azuma. </author> <title> Predictive Tracking for Augmented Reality. </title> <type> PhD thesis, </type> <institution> University of North Carolina at Chapel Hill, </institution> <year> 1995. </year> <note> Available as UNC-CH Computer Science TR95-007, at http://www.cs.unc.edu/Research/tech-reports.html. </note>
Reference-contexts: In an actual system, the extra size needed in the reference frames would depend primarily on the maximum rate of head rotation and on the ability of head rotation to be predicted. Typical rates of head rotation are less than 100 ffi ffi /400 msec) <ref> [4] </ref>. 6.2 Anti-aliasing Chen and Williams [12] pointed out that reference frames should not be anti-aliased, because the blending of foreground and background colors at silhouette edges is view dependent. Furthermore, the 3D warp requires a single disparity value, and the disparity value of a blended pixel is ambiguous.
Reference: [5] <author> Ronald Azuma and Gary Bishop. </author> <title> Improving static and dynamic registration in an optical see-through hmd. </title> <booktitle> In Computer Graphics Annual Conference Series (Proceedings of SIGGRAPH 94), </booktitle> <pages> pages 197-204, </pages> <address> Orlando, Florida, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Predictive tracking has been combined with image shifting by [9, 25, 31, 33]. Predictive tracking alone has been used by many systems; see <ref> [5] </ref> for an example and references to other work. <p> Prediction for our application may be able to achieve lower average error than systems which use motion prediction for latency reduction alone (without any form of post-rendering warp). The reason is that there is a tradeoff in motion prediction between reducing average error and reducing perceptually disturbing high-frequency jitter <ref> [5] </ref>. A post-rendering warp can eliminate the jitter, and thus the predicter can be optimized for low average error. Our rendered images have a large field-of-view to allow for head rotation and pixel movement due to translation.
Reference: [6] <author> Ronald Azuma and Gary Bishop. </author> <title> A frequency-domain analysis of head-motion prediction. </title> <booktitle> In Computer Graphics Annual Conference Series (Proceedings of SIGGRAPH 95), </booktitle> <pages> pages 401-408, </pages> <address> Los Angeles, CA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Predictive tracking has been combined with image shifting by [9, 25, 31, 33]. Predictive tracking alone has been used by many systems; see [5] for an example and references to other work. Predictive tracking becomes less accurate as the prediction interval increases <ref> [6] </ref>, and thus becomes less usable (especially by itself) as latencies increase. 4 Compositing and Reconstruction 4.1 Reconstruction The straightforward reconstruction technique of writing a single derived-image pixel for each reference-image sample does not produce images of adequate quality. We have explored two reconstruction techniques that are more sophisticated. <p> Our accompanying videotape shows video sequences using average per-axis perturbations of 2 cm and 5 cm for the simulated prediction interval of 400 msec. In an actual head motion prediction system with a shorter prediction interval of 100 msec, Azuma and Bishop <ref> [6] </ref> cited an average per-axis error of 0.36 centimeters, although their peak error (over the entire sequence) was 15 centimeters.
Reference: [7] <author> Sig Badt, Jr. </author> <title> Two algorithms for taking advantage of temporal coherence in ray tracing. </title> <journal> The Visual Computer, </journal> <volume> 4(3) </volume> <pages> 123-131, </pages> <year> 1988. </year>
Reference-contexts: The algorithm is demonstrated on pre-rendered synthetic imagery. A later system [28] used a 3D cylindrical-to-planar warp for acquired imagery. Similar warping calculations have been used to accelerate ray tracing of animations and stereo pairs <ref> [1, 2, 7] </ref>. Greene and Kass [15] generated simplified image-like representations of a scene by retaining only those polygons which were visible in a Z-buffered image from the reference viewpoint.
Reference: [8] <author> Denis R. Breglia, A. Michael Spooner, and Dan Lobb. </author> <booktitle> Helmet mounted laser projector. In The 1981 Image Generation/Display Conference II, </booktitle> <pages> pages 241-258, </pages> <address> Scottsdale, Arizona, </address> <month> Jun </month> <year> 1981. </year> <booktitle> 9 In Proceedings of 1997 Symposium on Interactive 3D Graphics, </booktitle> <address> Providence, RI, </address> <month> April 27-30, </month> <year> 1997, </year> <pages> pp. 7-16. </pages>
Reference: [9] <author> Dick Burbidge and Paul M. Murray. </author> <title> Hardware improvements to the helmet mounted projector on the visual display research tool (VDRT) at the naval training systems center. </title> <booktitle> In Proceedings SPIE, </booktitle> <volume> volume 1116, </volume> <pages> pages 52-60, </pages> <address> Orlando, Florida, </address> <month> Mar </month> <year> 1989. </year>
Reference-contexts: Predictive tracking has been combined with image shifting by <ref> [9, 25, 31, 33] </ref>. Predictive tracking alone has been used by many systems; see [5] for an example and references to other work.
Reference: [10] <institution> CAE Electronics, Ltd. </institution> <month> Wide-field-of-view, </month> <title> helmet-mounted infinity display system development. </title> <type> interim report AFHRL-TR-84-27, </type> <institution> US Air Force Human Resoures Laboratory, Operations Training Division, </institution> <month> Dec </month> <year> 1984. </year>
Reference: [11] <author> Shenchang Eric Chen. </author> <title> QuickTime VR an image-based approach to virtual environment navigation. </title> <booktitle> In Computer Graphics Annual Conference Series (Proceedings of SIG-GRAPH 95), </booktitle> <pages> pages 29-38, </pages> <address> Los Angeles, California, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Lippman's movie maps system [20] allows virtual movement through a city along controlled routes. He proposes the use of both image scaling and 2D projective transforms to interpolate between stored images. QuickTime VR's panoramic player <ref> [11] </ref> employs a 2D cylindrical-to-planar warp to allow arbitrary view directions from a single viewpoint. The Lumigraph [14] and Light Field Rendering systems [19] take a very different approach to image-based rendering from the systems we have just discussed.
Reference: [12] <author> Shenchang Eric Chen and Lance Williams. </author> <title> View interpolation for image synthesis. </title> <booktitle> In Computer Graphics Annual Conference Series (Proceedings of SIGGRAPH 93), </booktitle> <pages> pages 279-288, </pages> <address> Anaheim, California, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: To minimize cracking, polygons which occupied a pixel in this image but whose true projected screen-space area was smaller than a pixel were enlarged to fill the pixel. Chen and Williams' work on view interpolation <ref> [12] </ref> was the first to discuss many of the problems with which we are concerned in our current work. Their system uses a 3D warp in a pre-processing step to compute the movement of pixels between reference frame viewpoints. The run-time warp is not a full 3D warp. <p> Typical rates of head rotation are less than 100 ffi ffi /400 msec) [4]. 6.2 Anti-aliasing Chen and Williams <ref> [12] </ref> pointed out that reference frames should not be anti-aliased, because the blending of foreground and background colors at silhouette edges is view dependent. Furthermore, the 3D warp requires a single disparity value, and the disparity value of a blended pixel is ambiguous. <p> Furthermore, the 3D warp requires a single disparity value, and the disparity value of a blended pixel is ambiguous. We implement the same approach to this problem suggested by <ref> [12] </ref>: Our system generates reference frames at high resolution, warps them, then averages groups of warped samples to produce the derived frame. In other words, we are performing super-sampled anti-aliasing where the averaging is deferred until after the warp.
Reference: [13] <author> John P. Costella. </author> <title> Motion extrapolation at the pixel level. </title> <note> Unpublished paper available from http://www.ph. unimelb.edu.au/ jpc, </note> <month> January </month> <year> 1993. </year>
Reference-contexts: Our technique also has problems with scenes which are not static. In such scenes the moving objects will move in a jerky manner. We believe that we could incorporate moving objects by augmenting the reference frames with per-pixel motion vectors, similar those proposed by <ref> [13] </ref>. Occlusion artifacts would probably be more pronounced with this strategy, because the reference-frame viewpoint choice algorithm can not be guaranteed to work for even a single moving occluder. It would be interesting to explore the use of a 3D warp in conjunction with image-layering techniques.
Reference: [14] <author> Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. </author> <booktitle> The lumigraph. In Computer Graphics Annual Conference Series (Proceedings of SIGGRAPH 96), </booktitle> <pages> pages 43-54, </pages> <address> New Orleans, Louisiana, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: He proposes the use of both image scaling and 2D projective transforms to interpolate between stored images. QuickTime VR's panoramic player [11] employs a 2D cylindrical-to-planar warp to allow arbitrary view directions from a single viewpoint. The Lumigraph <ref> [14] </ref> and Light Field Rendering systems [19] take a very different approach to image-based rendering from the systems we have just discussed.
Reference: [15] <author> Ned Greene and Michael Kass. </author> <title> Approximating visibility with environment maps. </title> <type> Technical Report #41, </type> <institution> Apple Computer, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: The algorithm is demonstrated on pre-rendered synthetic imagery. A later system [28] used a 3D cylindrical-to-planar warp for acquired imagery. Similar warping calculations have been used to accelerate ray tracing of animations and stereo pairs [1, 2, 7]. Greene and Kass <ref> [15] </ref> generated simplified image-like representations of a scene by retaining only those polygons which were visible in a Z-buffered image from the reference viewpoint. <p> Different layers can also then be re-rendered at different rates. This technique is an alternative to a full 3D warp. Variations on it are used by Talisman [36], Regan and Pose (priority rendering) [30], and by Greene and Kass <ref> [15] </ref>. This last system actually renders the near geometry in the standard fashion into every derived frameonly the far geometry is represented in image form. <p> Figure 10b shows such a case. This figure depicts an extreme example because the viewpoint-to-viewpoint distance is large relative to the viewpoint-to-object distance. Greene and Kass <ref> [15] </ref> discuss a similar two-occluder case. The question of how to choose reference frame viewpoints is then reduced to attempting to insure satisfaction of this viewpoint-on-line condition. The problem can be exactly solved if viewpoint motion is linear and perfectly predictable.
Reference: [16] <author> Georg Rainer Hofmann. </author> <title> The calculus of the non-exact perspective projection. </title> <booktitle> In Proceedings of the European Computer Graphics Conference and Exhibition (Eurograph-ics '88), </booktitle> <pages> pages 429-442, </pages> <address> Nice, France, </address> <month> Sep </month> <year> 1988. </year>
Reference-contexts: Hofmann proposed that 2D affine transforms could be used to avoid re-rendering parts of a scene, and discusses the conditions under which these transforms provide an adequate approximation to the desired image <ref> [16] </ref>. Microsoft's Talisman architecture [36] composites independent image layers at video rates in front-to-back order using a per-layer affine transform. Any given image layer is re-rendered only when the residual error after applying its affine transform exceeds a desired threshold.
Reference: [17] <author> S. Laveau and O. D. Faugeras. </author> <title> 3-D scene representation as a collection of images. </title> <booktitle> In Proc. of 12th IAPR Intl. Conf. on Pattern Recognition, </booktitle> <volume> volume 1, </volume> <pages> pages 689-691, </pages> <address> Jerusalem, Israel, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: This post-process does not fix cases where background objects are visible through missing pixels in foreground objects. There are several systems which use 3D warps similar to those developed by McMillan and Bishop. None of these systems were concerned with real-time performance. Laveau and Faugeras <ref> [17, 18] </ref> explore the use of a partially inverse-mapped 3D warp. Max and Ohsaki [24] use several multi-layered reference images to to minimize occlusion artifacts. Their system uses deferred shading to correctly compute view-dependent shading. It is also different from most other systems because it uses parallel-projection reference images.
Reference: [18] <author> Stephane Laveau and Olivier Faugeras. </author> <title> 3-D scene representation as a collection of images and fundamental ma-tricies. </title> <type> Technical Report RR #2205, </type> <institution> INRIA, </institution> <month> February </month> <year> 1994. </year> <note> Available from ftp://ftp.inria.fr/INRIA/tech-reports/RR/RR-2205.ps.gz. </note>
Reference-contexts: This post-process does not fix cases where background objects are visible through missing pixels in foreground objects. There are several systems which use 3D warps similar to those developed by McMillan and Bishop. None of these systems were concerned with real-time performance. Laveau and Faugeras <ref> [17, 18] </ref> explore the use of a partially inverse-mapped 3D warp. Max and Ohsaki [24] use several multi-layered reference images to to minimize occlusion artifacts. Their system uses deferred shading to correctly compute view-dependent shading. It is also different from most other systems because it uses parallel-projection reference images.
Reference: [19] <author> Marc Levoy and Pat Hanrahan. </author> <title> Light field rendering. </title> <booktitle> In Computer Graphics Annual Conference Series (Proceedings of SIGGRAPH 96), </booktitle> <pages> pages 31-42, </pages> <address> New Orleans, Louisiana, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: He proposes the use of both image scaling and 2D projective transforms to interpolate between stored images. QuickTime VR's panoramic player [11] employs a 2D cylindrical-to-planar warp to allow arbitrary view directions from a single viewpoint. The Lumigraph [14] and Light Field Rendering systems <ref> [19] </ref> take a very different approach to image-based rendering from the systems we have just discussed. Rather than storing a limited number of images and then warping to interpolate between them, they store a dense, regular sampling of the set of all possible light rays.
Reference: [20] <author> Andrew Lippman. Movie-maps: </author> <title> An application of the optical videodisc to computer graphics. </title> <booktitle> Computer Graphics (Proceedings of SIGGRAPH 80), </booktitle> <volume> 14(3) </volume> <pages> 32-42, </pages> <month> July </month> <year> 1980. </year>
Reference-contexts: This imagery can be either rendered off-line, or acquired from the real world by cameras. Lippman's movie maps system <ref> [20] </ref> allows virtual movement through a city along controlled routes. He proposes the use of both image scaling and 2D projective transforms to interpolate between stored images. QuickTime VR's panoramic player [11] employs a 2D cylindrical-to-planar warp to allow arbitrary view directions from a single viewpoint.
Reference: [21] <author> William R. Mark. </author> <title> Efficient two-phase architecture for disparity-based image warping: Overview and memory bandwidth analysis. UNC COMP 290-012 (Graphics Architecture) final project writeup, </title> <month> May </month> <year> 1996. </year>
Reference-contexts: The particular location that is written depends upon the disparity value stored with the reference-frame pixel. An appropriately designed system can maintain this working set in a small cache, and use only large block transfers to main memory. <ref> [21] </ref> 1 This is not strictly true for our current test-bed system, since rapidly alternating disparity values could cause many large triangles to be rendered as part of the reconstruction process. The modifications of the reconstruction technique that we are investigating would eliminate this possibility. 3.
Reference: [22] <author> William R. Mark, Gary Bishop, and Leonard McMillan. </author> <title> Post-rendering image warping for latency compensation. </title> <type> Technical Report UNC-CH TR96-020, </type> <institution> Univ. of North Carolina at Chapel Hill, Dept. of Computer Science, </institution> <month> January </month> <year> 1996. </year> <note> Available at http://www.cs.unc.edu/Research/tech-reports.html. </note>
Reference-contexts: The derived frame quality is also poorer because the system uses a simple splat-like reconstruction algorithm rather than the mesh-based algorithm used by our test-bed system. Further details about this system, including its reconstruction and clipping algorithms, can be found in <ref> [22] </ref>. The remainder of the paper is organized as follows: The next section discusses previous work. Section 4 provide details of our multi-image compositing and reconstruction techniques. In section 5 we describe how reference-frame viewpoints are chosen. Section 6 discusses our test-bed system.
Reference: [23] <author> Nelson Max. </author> <title> Hierarchical rendering of trees from precom-puted multi-layer z-buffers. </title> <editor> In Xavier Pueyo and Peter Schr oder, editors, </editor> <booktitle> Rendering Techniques '96: Proceedings of the Eurographics Rendering Workshop 1996, </booktitle> <pages> pages 165-174, </pages> <address> Porto, Portugal, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Max and Ohsaki [24] use several multi-layered reference images to to minimize occlusion artifacts. Their system uses deferred shading to correctly compute view-dependent shading. It is also different from most other systems because it uses parallel-projection reference images. More recent work by Max <ref> [23] </ref> uses a hierarchy of reference images of differing spatial resolution. <p> In other words, we are performing super-sampled anti-aliasing where the averaging is deferred until after the warp. Max <ref> [23] </ref> uses a similar technique, but with a coverage mask rather than full super-sampling, so Z's and normals are not super-sampled. The super-sampling in the derived frame is on a 2x2 grid.
Reference: [24] <author> Nelson Max and Keiichi Ohsaki. </author> <title> Rendering trees from pre-computed z-buffer views. </title> <editor> In Patrick M. Hanrahan and Werner Purgathofer, editors, </editor> <booktitle> Rendering Techniques '95: Proceedings of the Eurographics Rendering Workshop 1995, </booktitle> <pages> pages 45-54, </pages> <address> Dublin, Ireland, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: There are several systems which use 3D warps similar to those developed by McMillan and Bishop. None of these systems were concerned with real-time performance. Laveau and Faugeras [17, 18] explore the use of a partially inverse-mapped 3D warp. Max and Ohsaki <ref> [24] </ref> use several multi-layered reference images to to minimize occlusion artifacts. Their system uses deferred shading to correctly compute view-dependent shading. It is also different from most other systems because it uses parallel-projection reference images. <p> We discuss the topic of prediction error in more detail in the next section, which describes our test-bed system. Max's multi-layered Z-buffer <ref> [24] </ref> is an alternative to our technique of choosing two different reference frame viewpoints. Our two-viewpoint technique is particularly well suited to post-rendering warping, because it takes advantage of the system's knowledge about likely derived-frame viewpoints to select which surfaces in the scene to sample. <p> These supplementary frames would have a good view of objects in adjacent rooms which are likely to be occluded in the standard reference frames. Our technique does not correctly handle view dependent shadingspecular highlights will jump around at the reference frame rate. Max <ref> [24] </ref> has already solved this problem in a 3D warp by using deferred shading [38]. It might make sense to implement partially deferred shading, in which only the most strongly view dependent parts of the shading calculation are deferred. Our technique also has problems with scenes which are not static.
Reference: [25] <author> Tomasz Mazuryk and Michael Gervautz. </author> <title> Two-step prediction and image deflection for exact head tracking in virtual environments. </title> <booktitle> Computer Graphics Forum (Eurographics '95), </booktitle> <address> 14(3):C29-C41, </address> <year> 1995. </year>
Reference-contexts: The earliest work focused on latency reduction and used image shifting (i.e. translation in image X and Y). The rendered image is shifted either electro-optically [8-10, 31, 33], or in the frame-buffer just prior to scan-out <ref> [25] </ref>. A 2D affine transform is more general than image shifting, because it allows for scaling and image-plane rotation as well as image-plane translation. <p> Predictive tracking has been combined with image shifting by <ref> [9, 25, 31, 33] </ref>. Predictive tracking alone has been used by many systems; see [5] for an example and references to other work.
Reference: [26] <author> Leonard McMillan. </author> <title> A list-priority rendering algorithm for redisplaying projected surfaces. </title> <type> Technical Report UNC-CH TR95-005, </type> <institution> University of North Carolina at Chapel Hill, Dept. of Computer Science, </institution> <year> 1995. </year> <note> Available at http://www.cs.unc.edu/Research/tech-reports.html. </note>
Reference-contexts: The vast majority of the mesh triangles cover only one or two derived-frame pixels. For these triangles, the full triangle rendering machinery is not necessary. These extremely small triangles can be handled by a less expensive, more splat-like, approach. Alternatively, they could be rendered using table lookup <ref> [26] </ref>. The much less numerous large triangles can still be rendered using standard techniques. Even greater efficiency and better bounds on computation cost can be obtained if the large triangles are not rendered at all. These large triangles are the low-connectedness rubber-sheet triangles that stretch between foreground and background objects.
Reference: [27] <author> Leonard McMillan and Gary Bishop. </author> <title> Head-tracked stereoscopic display using image warping. </title> <editor> In S. Fisher, J. Merritt, and B. Bolas, editors, </editor> <booktitle> Proceedings SPIE, </booktitle> <volume> volume 2409, </volume> <pages> pages 21-30, </pages> <address> San Jose, CA, </address> <month> Feb </month> <year> 1995. </year>
Reference-contexts: The two uses of post-rendering warping are very closely related. In either case, image warping must compute derived frames at new viewpoints by extrapolation from reference frames rendered at other viewpoints. We use McMillan and Bishop's planar-to-planar, forward mapped image warping algorithm <ref> [27] </ref> to compute derived frames from reference frames. This warp uses a per-pixel disparity value as part of the warp computation. The disparity value is a form of depth information that is easily computed from the 1/Z values in a standard Z-buffer. <p> Our 3D warping algorithm is an extension of the planar-to-planar 3D image warp <ref> [27] </ref> which was developed earlier by two of this paper's authors. This earlier paper uses incremental evaluation of the 3D warp equations and an occlusion-compatible warping order to achieve real time performance, but does not address reconstruction issues in detail. The algorithm is demonstrated on pre-rendered synthetic imagery. <p> The modifications of the reconstruction technique that we are investigating would eliminate this possibility. 3. The computations involved are very regular. Many of the necessary expressions in the warp can be evaluated incrementally (for example, replacing multiplies with adds). <ref> [27] </ref> The regular memory access pattern of the 3D warp is very different from that encountered in standard rendering, where (without pre-sorting) any triangle can touch any pixel in the frame-buffer. 7.2 Limitations & Future Work There are a number of areas besides reconstruction efficiency in which we believe we can
Reference: [28] <author> Leonard McMillan and Gary Bishop. </author> <title> Plenoptic modeling: An image-based rendering system. </title> <booktitle> In Computer Graphics Annual Conference Series (Proceedings of SIGGRAPH 95), </booktitle> <pages> pages 39-46, </pages> <address> Los Angeles, CA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: This earlier paper uses incremental evaluation of the 3D warp equations and an occlusion-compatible warping order to achieve real time performance, but does not address reconstruction issues in detail. The algorithm is demonstrated on pre-rendered synthetic imagery. A later system <ref> [28] </ref> used a 3D cylindrical-to-planar warp for acquired imagery. Similar warping calculations have been used to accelerate ray tracing of animations and stereo pairs [1, 2, 7].
Reference: [29] <author> Matthew Regan and Ronald Pose. </author> <title> An interactive graphics display architecture. </title> <booktitle> In Proceedings of IEEE Virtual Reality Annual International Symposium, </booktitle> <pages> pages 293-299, </pages> <address> Seattle, Washington, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: To compensate for arbitrary changes in view direction, a 2D projective transform is required. Regan and Pose's address recalculation pipeline implements this transform in hardware, with the goal of both reducing latency and (in conjunction with their priority rendering technique) increasing frame rate <ref> [29, 30] </ref>. 2D projective transforms can also be applied to selected parts of a scene by mapping recently rendered imagery onto large textured polygons before performing final rendering [3, 32]. 3.2 Warping From Stored Images Image warping can be used to display previously stored imagery.
Reference: [30] <author> Matthew Regan and Ronald Pose. </author> <title> Priority rendering with a virtual reality address recalculation pipeline. </title> <booktitle> In Computer Graphics Annual Conference Series (Proceedings of SIGGRAPH 94), </booktitle> <pages> pages 155-162, </pages> <address> Orlando, Florida, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: To compensate for arbitrary changes in view direction, a 2D projective transform is required. Regan and Pose's address recalculation pipeline implements this transform in hardware, with the goal of both reducing latency and (in conjunction with their priority rendering technique) increasing frame rate <ref> [29, 30] </ref>. 2D projective transforms can also be applied to selected parts of a scene by mapping recently rendered imagery onto large textured polygons before performing final rendering [3, 32]. 3.2 Warping From Stored Images Image warping can be used to display previously stored imagery. <p> Different layers can also then be re-rendered at different rates. This technique is an alternative to a full 3D warp. Variations on it are used by Talisman [36], Regan and Pose (priority rendering) <ref> [30] </ref>, and by Greene and Kass [15]. This last system actually renders the near geometry in the standard fashion into every derived frameonly the far geometry is represented in image form.
Reference: [31] <author> Bruce Riner and Blair Browder. </author> <title> Design guidelines for a carrier-based training system. </title> <booktitle> In Proceedings of IMAGE VI Conference, </booktitle> <pages> pages 65-73, </pages> <address> Scottsdale, Arizona, </address> <month> Jul </month> <year> 1992. </year>
Reference-contexts: The earliest work focused on latency reduction and used image shifting (i.e. translation in image X and Y). The rendered image is shifted either electro-optically <ref> [8-10, 31, 33] </ref>, or in the frame-buffer just prior to scan-out [25]. A 2D affine transform is more general than image shifting, because it allows for scaling and image-plane rotation as well as image-plane translation. <p> Predictive tracking has been combined with image shifting by <ref> [9, 25, 31, 33] </ref>. Predictive tracking alone has been used by many systems; see [5] for an example and references to other work.
Reference: [32] <author> Johnathan Shade, Dani Lischinski, David H. Salesin, Tony DeRose, and John Snyder. </author> <title> Hierarchical image caching for accelerated walkthroughs of complex environments. </title> <booktitle> In Computer Graphics Annual Conference Series (Proceedings of SIGGRAPH 96), </booktitle> <pages> pages 75-82, </pages> <address> New Orleans, Louisiana, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: this transform in hardware, with the goal of both reducing latency and (in conjunction with their priority rendering technique) increasing frame rate [29, 30]. 2D projective transforms can also be applied to selected parts of a scene by mapping recently rendered imagery onto large textured polygons before performing final rendering <ref> [3, 32] </ref>. 3.2 Warping From Stored Images Image warping can be used to display previously stored imagery. This imagery can be either rendered off-line, or acquired from the real world by cameras. Lippman's movie maps system [20] allows virtual movement through a city along controlled routes. <p> This last system actually renders the near geometry in the standard fashion into every derived frameonly the far geometry is represented in image form. The texture-based simplification systems mentioned earlier <ref> [3, 32] </ref> can also be considered to be layered systems. 3.5 Predictive Tracking Prediction of future viewpoint and view direction can be used in place of, or in conjunction with, image warping to compensate for latency. Predictive tracking has been combined with image shifting by [9, 25, 31, 33].
Reference: [33] <author> Richard H. Y. So and Michael J. Griffin. </author> <title> Compensating lags in head-coupled displays using head position prediction and and image deflection. </title> <journal> Journal of Aircraft, </journal> <volume> 29(6) </volume> <pages> 1064-1068, </pages> <month> Nov-Dec </month> <year> 1992. </year>
Reference-contexts: The earliest work focused on latency reduction and used image shifting (i.e. translation in image X and Y). The rendered image is shifted either electro-optically <ref> [8-10, 31, 33] </ref>, or in the frame-buffer just prior to scan-out [25]. A 2D affine transform is more general than image shifting, because it allows for scaling and image-plane rotation as well as image-plane translation. <p> Predictive tracking has been combined with image shifting by <ref> [9, 25, 31, 33] </ref>. Predictive tracking alone has been used by many systems; see [5] for an example and references to other work.
Reference: [34] <author> Richard Szeliski. </author> <title> Image mosaicing for tele-reality. </title> <type> Technical Report CRL 94/2, </type> <institution> Digital Equipment Corp. Cam-bridge Research Lab, </institution> <month> May </month> <year> 1994. </year> <note> Available at http:// www.research.digital.com/CRL/publications/crl-rr.html. </note>
Reference-contexts: Their system uses deferred shading to correctly compute view-dependent shading. It is also different from most other systems because it uses parallel-projection reference images. More recent work by Max [23] uses a hierarchy of reference images of differing spatial resolution. Szelinski <ref> [34, 35] </ref> used equations similar to the 3D warp equations as part of a technique to extract depth from acquired imagery. 3.4 Layering Techniques for 2D Warps Affine and 2D projective warps can not by themselves compensate for viewpoint translation if objects in the scene are not all coplanar. 3 In
Reference: [35] <author> Richard Szeliski. </author> <title> Video mosaics for virtual environments. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 16(2) </volume> <pages> 22-30, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: Their system uses deferred shading to correctly compute view-dependent shading. It is also different from most other systems because it uses parallel-projection reference images. More recent work by Max [23] uses a hierarchy of reference images of differing spatial resolution. Szelinski <ref> [34, 35] </ref> used equations similar to the 3D warp equations as part of a technique to extract depth from acquired imagery. 3.4 Layering Techniques for 2D Warps Affine and 2D projective warps can not by themselves compensate for viewpoint translation if objects in the scene are not all coplanar. 3 In
Reference: [36] <author> Jay Torborg and James T. Kajiya. Talisman: </author> <title> Commodity realtime 3D graphics for the PC. </title> <booktitle> In Computer Graphics Annual Conference Series (Proceedings of SIGGRAPH 96), </booktitle> <pages> pages 353-364, </pages> <address> New Orleans, Louisiana, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Hofmann proposed that 2D affine transforms could be used to avoid re-rendering parts of a scene, and discusses the conditions under which these transforms provide an adequate approximation to the desired image [16]. Microsoft's Talisman architecture <ref> [36] </ref> composites independent image layers at video rates in front-to-back order using a per-layer affine transform. Any given image layer is re-rendered only when the residual error after applying its affine transform exceeds a desired threshold. To compensate for arbitrary changes in view direction, a 2D projective transform is required. <p> Different layers can also then be re-rendered at different rates. This technique is an alternative to a full 3D warp. Variations on it are used by Talisman <ref> [36] </ref>, Regan and Pose (priority rendering) [30], and by Greene and Kass [15]. This last system actually renders the near geometry in the standard fashion into every derived frameonly the far geometry is represented in image form.
Reference: [37] <author> Lee Westover. </author> <title> Footprint evaluation for volume rendering. </title> <booktitle> Computer Graphics (Proceedings of SIGGRAPH 90), </booktitle> <volume> 24(4) </volume> <pages> 367-376, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: We have explored two reconstruction techniques that are more sophisticated. The first (Figure 5a) treats each reference pixel independently, but varies the size of the reconstruction kernel depending on the disparity and normal-vector orientation of the reference pixel. This approach is a form of splatting <ref> [37] </ref>. We use this technique in our real-time remote display system, but rough edges on under-sampled surfaces and occasional pinholes harm the visual quality of the derived frame. The second technique (Figure 5b) treats the reference frame as a mesh.
Reference: [38] <author> Turner Whitted and David M. Weimer. </author> <title> A software testbed for the development of 3D raster graphics systems. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 1(1) </volume> <pages> 43-58, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: Our technique does not correctly handle view dependent shadingspecular highlights will jump around at the reference frame rate. Max [24] has already solved this problem in a 3D warp by using deferred shading <ref> [38] </ref>. It might make sense to implement partially deferred shading, in which only the most strongly view dependent parts of the shading calculation are deferred. Our technique also has problems with scenes which are not static. In such scenes the moving objects will move in a jerky manner.
Reference: [39] <author> George Wolberg. </author> <title> Digital Image Warping. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1992. </year> <month> 10 </month>
Reference-contexts: Finally, section 7 considers some limitations of our current work and discusses promising directions for future work. 3 Previous Work In this section we discuss the previous work that our current efforts build upon. A good overview of the mathematical aspects of image warping is provided by Wolberg's book <ref> [39] </ref> on 2D image warps. 3.1 Post-Rendering Warping Previous researchers have investigated a variety of different post-rendering warping algorithms to decrease latency and/or increase frame rate. The earliest work focused on latency reduction and used image shifting (i.e. translation in image X and Y).
References-found: 39

