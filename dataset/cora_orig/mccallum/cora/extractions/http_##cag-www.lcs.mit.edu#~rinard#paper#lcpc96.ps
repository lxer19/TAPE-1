URL: http://cag-www.lcs.mit.edu/~rinard/paper/lcpc96.ps
Refering-URL: http://cag-www.lcs.mit.edu/~rinard/paper/index.html
Root-URL: 
Email: fpedro,marting@cs.ucsb.edu  
Title: Lock Coarsening: Eliminating Lock Overhead in Automatically Parallelized Object-Based Programs  
Author: Pedro Diniz and Martin Rinard 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: Atomic operations are a key primitive in parallel computing systems. The standard implementation mechanism for atomic operations uses mutual exclusion locks. In an object-based programming system the natural granularity is to give each object its own lock. Each operation can then make its execution atomic by acquiring and releasing the lock for the object that it accesses. But this fine lock granularity may have high synchronization overhead. To achieve good performance it may be necessary to reduce the overhead by coarsening the granularity at which the computation locks objects. In this paper we describe a static analysis technique | lock coarsening | designed to automatically increase the lock granularity in object-based programs with atomic operations. We have implemented this technique in the context of a parallelizing compiler for irregular, object-based programs. Experiments show these algorithms to be effective in reducing the lock overhead to negligible levels.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(NlogN) force-calculation algorithm. </title> <booktitle> Nature, </booktitle> <pages> pages 446-449, </pages> <month> December </month> <year> 1976. </year>
Reference-contexts: source-to-source translator, we use a standard C++ compiler to generate object code for the automatically generated parallel programs. 2 The sequential source codes and automatically generated parallel codes can be found at http://www.cs.ucsb.edu/~pedro/CA/apps/LockCoarsening. 7.2 Barnes-Hut The Barnes-Hut application simulates the trajectories of a set of interacting bodies under Newtonian forces <ref> [1] </ref>. It uses a sophisticated pointer-based data structure: a space subdivision tree that dramatically improves the efficiency of a key phase in the algorithm. The application consists of approximately 1500 lines of serial C++ code.
Reference: 2. <author> P. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: In fact, database researchers have identified lock granularity as a key issue in the implementation of atomic transactions, and found that excessive lock overhead can be a significant problem if the lock granularity is too fine <ref> [2, 4] </ref>. The proposed solution to the problem of excessive lock overhead in the context of database concurrency control is to dynamically coarsen the lock granularity using a technique called lock escalation.
Reference: 3. <author> R. Cytron. </author> <title> Doacross: Beyond vectorization for multiprocessors. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: The majority of synchronization optimization research has concentrated on removing barriers or converting barrier synchronization constructs to more efficient synchronization constructs such as counters [10]. Several researchers have also explored optimizations geared towards exploiting more fine grained concur-rency available within loops <ref> [3] </ref>. These optimizations automatically insert one-way synchronization constructs such as post and wait to implement loop-carried data dependences. The research presented in this paper investigates synchronization optimizations for a compiler designed to parallelize object-based programs, not loop nests that manipulate dense arrays using affine access functions.
Reference: 4. <author> U. Herrmann, P. Dadam, K. Kuspert, E. Roman, and G Schlageter. </author> <title> A lock technique for disjoint and non-disjoint complex objects. </title> <booktitle> In Proceedings of the International Conference on Extending Database Technology (EDBT'90), </booktitle> <pages> pages 219-235, </pages> <address> Venice, Italy, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: In fact, database researchers have identified lock granularity as a key issue in the implementation of atomic transactions, and found that excessive lock overhead can be a significant problem if the lock granularity is too fine <ref> [2, 4] </ref>. The proposed solution to the problem of excessive lock overhead in the context of database concurrency control is to dynamically coarsen the lock granularity using a technique called lock escalation.
Reference: 5. <author> G. Kane and J. Heinrich. </author> <title> MIPS Risc Architecture. </title> <publisher> Prentice-Hall, </publisher> <year> 1992. </year>
Reference-contexts: This property makes it possible for the implementation to use an extremely efficient lock implementation. Modern processors have synchronization instructions that make it possible to implement the required lock acquire and release constructs efficiently <ref> [5] </ref>. 8.3 Efficient Synchronization Algorithms Other researchers have addressed the issue of synchronization overhead reduction. This work has concentrated on the development of more efficient implementations of synchronization primitives using various protocols and waiting mechanisms [7]. The research presented in this paper is orthogonal to and synergistic with this work.
Reference: 6. <author> D. Lenoski. </author> <title> The Design and Analysis of DASH: A Scalable Directory-Based Multiprocessor. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: The generated code for each application differs only in the lock coarsening policy used to reduce the lock overhead. 2 We evaluated the performance of each version by running it on a 32-processor Stan-ford DASH machine <ref> [6] </ref>.
Reference: 7. <author> B-H. Lim and A. Agarwal. </author> <title> Reactive synchronization algorithms for multiprocessors. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: This work has concentrated on the development of more efficient implementations of synchronization primitives using various protocols and waiting mechanisms <ref> [7] </ref>. The research presented in this paper is orthogonal to and synergistic with this work.
Reference: 8. <author> S. Midkiff and D. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36(12) </volume> <pages> 1485-1495, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: Aggressive policy dramatically reduces the number of executed locking operations, the introduced lock contention almost completely serializes the execution. 8 Related Work 8.1 Automatically Parallelized Scientific Computations Previous parallelizing compiler research in the area of synchronization optimization has focused almost exclusively on synchronization optimizations for parallel loops in scientific computations <ref> [8] </ref>. The natural implementation of a parallel 3 The speedup curves are given relative to the sequential C++ version. We have also obtained sequential C and sequential Fortran versions of this application.
Reference: 9. <author> M. Rinard and P. Diniz. </author> <title> Commutativity analysis: A new analysis framework for parallelizing compilers. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Program Language Design and Implementation, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year> <note> (http://www.cs.ucsb.edu/~martin/pldi96.ps). </note>
Reference-contexts: An atomic operation first acquires the lock for the data that it manipulates, accesses the data, then releases the lock. We have implemented a compiler designed to automatically parallelize object-based computations that manipulate irregular, pointer-based data structures. This compiler uses commutativity analysis <ref> [9] </ref> as its primary analysis paradigm. For the generated program to execute correctly, each operation in the generated parallel code must execute atomically. The automatically generated code therefore contains mutual exclusion locks and constructs that acquire and release these locks. <p> This paper presents the analysis algorithms and program transformations that the compiler uses to automatically coarsen the lock granularity. We have implemented these algorithms in the context of a parallelizing compiler for object-based programs <ref> [9] </ref>. This paper also presents experimental results that characterize the performance impact of using the lock coarsening algorithms in the compiler. The results show the algorithms to be effective in reducing the lock overhead to negligible levels. <p> We have found that inlined methods and the extension described in the previous paragraph, eliminate virtually all of this overhead. The serial C++ versions of our two benchmark applications, for example, perform slightly better than the C versions from the SPLASH benchmark set <ref> [9] </ref>. 5 Data Lock Coarsening The data lock coarsening algorithm starts with a computation in which each object has its own mutual exclusion lock. The basic idea is to increase the lock granularity by giving multiple objects the same lock. <p> If the original version does not deadlock, the transformed version can not deadlock. 7 Experimental Results We have implemented the lock coarsening algorithms described in Sections 5 and 6 and integrated them into a prototype compiler that uses commutativity analysis <ref> [9] </ref> as its primary analysis paradigm. In this section we present experimental results that characterize the performance impact of using the lock coarsening algorithms in a parallelizing compiler.
Reference: 10. <author> C. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 144-155, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The majority of synchronization optimization research has concentrated on removing barriers or converting barrier synchronization constructs to more efficient synchronization constructs such as counters <ref> [10] </ref>. Several researchers have also explored optimizations geared towards exploiting more fine grained concur-rency available within loops [3]. These optimizations automatically insert one-way synchronization constructs such as post and wait to implement loop-carried data dependences.
Reference: 11. <author> S. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22th International Symposium on Computer Architecture, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: In this section we present experimental results that characterize the performance impact of using the lock coarsening algorithms in a parallelizing compiler. We report performance results for two automatically parallelized scientific applications: the Barnes-Hut hierarchical N-body solver and the Water code <ref> [11] </ref>. 7.1 Methodology To evaluate the impact of the lock coarsening policy on the overall performance, we implemented the three lock coarsening policies described in Section 6. We then built three versions of the prototype compiler. The versions are identical except that each uses a different lock coarsening policy.
References-found: 11

