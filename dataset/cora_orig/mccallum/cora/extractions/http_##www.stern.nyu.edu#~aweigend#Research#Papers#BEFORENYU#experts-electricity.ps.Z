URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/experts-electricity.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: mangeas@cs.colorado.edu  andreas@cs.colorado.edu  Corinne.Muller@der.edf.fr  
Title: Forecasting electricity demand using nonlinear mixture of experts  
Author: Morgan Mangeas Andreas S. Weigend Corinne Muller , av. du g 
Note: This method proposed here gives the best performance on this series to date.  
Address: Boulder, CO 80309-0430  Boulder, CO 80309-0430  Gaulle, 92141 Clamart, France  
Affiliation: Department of Computer Science and Institute of Cognitive Science University of Colorado  Department of Computer Science and Institute of Cognitive Science University of Colorado  Electricit e de France, Direction des Etudes et Recherches  en eral de  
Abstract: In this paper we study a forecasting model based on mixture of experts for predicting the French electric daily consumption energy. We split the task into two parts. Using mixture of experts, a first model predicts the electricity demand from the exogenous variables (such as temperature and degree of cloud cover) and can be viewed as a nonlinear regression model of mixture of Gaussians. Using a single neural network, a second model predicts the evolution of the residual error of the first one, and can be viewed as an nonlinear autoregression model. We analyze the splitting of the input space generated by the mixture of experts model, and compare the performance to models presently used. 
Abstract-found: 1
Intro-found: 1
Reference: [CGG + 95] <author> M. Cottrell, B. Girard, Y. Girard, M. Mangeas, and C. Muller. </author> <title> Neural modeling for time series: a statistical stepwise method for weight elimination. In print, </title> <booktitle> IEEE Transaction on Neural Networks (Volume 6), </booktitle> <year> 1995. </year>
Reference-contexts: Beside these linear models, we have already applied different models using neural network [MCG + 93] and <ref> [CGG + 95] </ref> based on the same principle (using global information via exogenous variable for a single model) with significantly better results. <p> We use 10 lags of the residual errors series for forecasting the next point. In order to avoid overfitting, we use the Statiscal Stepwise Method introduced by <ref> [CGG + 95] </ref> for pruning the irrelevant weights 2 . 5 Performance and analysis We compare the performance of the global model (model 1 + model 2) to an ARX (autoregressive model using exogenous variables) and a common single neural network with two hidden layers, which takes into accounts the exogenous <p> Performance and analysis We compare the performance of the global model (model 1 + model 2) to an ARX (autoregressive model using exogenous variables) and a common single neural network with two hidden layers, which takes into accounts the exogenous variables between the input layer and the first hidden layer <ref> [CGG + 95] </ref>. The global model based on the ME model gives the best performance on this series to date (Table 5). We can observe from the Fig. 5 that the 6 th input (characterizing, via an indicator variable, the holidays), is highly correlated with the gate 2.
Reference: [Fri91] <author> J. H. Friedman. </author> <title> Multivariate adaptive regression splines. </title> <journal> Annals of Statistics, </journal> <volume> 19 </volume> <pages> 1-142, </pages> <year> 1991. </year>
Reference-contexts: During the training, modules compete for the right to learn the data. The fittest ones specialize over certain local regions determined by the gating network. This strategy is not new (e.g., Threshold autoregressive (TAR) model [TL80] and Multivariate Adaptive Regression Splines (MARS) <ref> [Fri91] </ref> applied to prediction of financial data by [LRS94])), but the mixture of experts (ME) model ([JJNH91, JX93, JJ94, WM95] fl http://www.cs.colorado.edu/~andreas/Home.html has a number of potentially promising advantages: * gating and experts are trained simultaneously * both splitting and prediction model (gate and experts) can be non-linear * there is
Reference: [JJ94] <author> M. I. Jordan and R. A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: We apply now in this paper the mixture of experts (ME) model <ref> [JJNH91, JX93, JJ94] </ref> to the daily electricity consumption. The ME model is based on the principle to divide a global difficult problem into a number of simpler sub-problems. <p> prediction model (gate and experts) can be non-linear * there is a natural smooth transition between regions * in the gaussian case, the variances assigned to each expert are independent (so the noise can have different level of variance) For more details about this model and the learning algorithm, see <ref> [JJ94] </ref> and [JX93]. 2 Data Each value of the daily electricity series is the total sum of energy used over the metropolitan France, from commercial and private use. Beside the values of this consumption (since 1986), we know some highly correlated variables such temperature and degree of cloud cover.
Reference: [JJNH91] <author> R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87, </pages> <year> 1991. </year>
Reference-contexts: We apply now in this paper the mixture of experts (ME) model <ref> [JJNH91, JX93, JJ94] </ref> to the daily electricity consumption. The ME model is based on the principle to divide a global difficult problem into a number of simpler sub-problems.
Reference: [JX93] <author> M. I. Jordan and L. Xu. </author> <title> Convergence results for the EM approach to mixtures of experts architectures. Submitted to Neural Networks, </title> <year> 1993. </year>
Reference-contexts: We apply now in this paper the mixture of experts (ME) model <ref> [JJNH91, JX93, JJ94] </ref> to the daily electricity consumption. The ME model is based on the principle to divide a global difficult problem into a number of simpler sub-problems. <p> (gate and experts) can be non-linear * there is a natural smooth transition between regions * in the gaussian case, the variances assigned to each expert are independent (so the noise can have different level of variance) For more details about this model and the learning algorithm, see [JJ94] and <ref> [JX93] </ref>. 2 Data Each value of the daily electricity series is the total sum of energy used over the metropolitan France, from commercial and private use. Beside the values of this consumption (since 1986), we know some highly correlated variables such temperature and degree of cloud cover. <p> The usual way is to maximize the log likelihood with respect to the parameters. However, the log of the likelihood (Eq. 2) is quite complicated, and it is difficult for common optimization (such as gradient descent) to find the splittings. Following <ref> [JX93] </ref>, we use the Expectation-Maximization (EM) algorithm. This algorithm is based on the assumption that some binary variables are missing (the variables characterize the hypothetis that one and only one expert is used for each pattern).
Reference: [Koh89] <author> T. Kohonen. </author> <title> Self Organization and Associative Memory. </title> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: In order to improve our knowledge about this series, we also performed a unsupervised classification based on the Kohonen maps <ref> [Koh89] </ref> on the half-hour electrical power consumption series (the daily consumption is obtained by averaging the 48 points per day). We apply now in this paper the mixture of experts (ME) model [JJNH91, JX93, JJ94] to the daily electricity consumption.
Reference: [LRS94] <author> P. A. W. Lewis, B. K. Ray, and J. G. Stevens. </author> <title> Modeling time series using multivariate adaptive regression splines (MARS). </title> <editor> In A. S. Weigend and N. A. Gershenfeld, editors, </editor> <title> Time Series Prediction: </title> <booktitle> Forecasting the Future and Understanding the Past, </booktitle> <pages> pages 296-318, </pages> <address> Reading, MA, 1994. </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The fittest ones specialize over certain local regions determined by the gating network. This strategy is not new (e.g., Threshold autoregressive (TAR) model [TL80] and Multivariate Adaptive Regression Splines (MARS) [Fri91] applied to prediction of financial data by <ref> [LRS94] </ref>)), but the mixture of experts (ME) model ([JJNH91, JX93, JJ94, WM95] fl http://www.cs.colorado.edu/~andreas/Home.html has a number of potentially promising advantages: * gating and experts are trained simultaneously * both splitting and prediction model (gate and experts) can be non-linear * there is a natural smooth transition between regions * in
Reference: [MCG + 93] <author> M. Mangeas, M. Cottrell, M. Girard, B. Girard, and C. Muller. </author> <title> Advantages of the multilayer perceptron for modeling and forecasting time series: application to the daily electrical consumption in france. </title> <booktitle> In Proceedings of Neuronmes'93, </booktitle> <address> Nmes, France, </address> <year> 1993. </year>
Reference-contexts: The classical method, for forecasting the electricity demand, is basically based on linear regression on the relevant exogenous variables and on the periodicities, anytime during the year, and for any type of day. Beside these linear models, we have already applied different models using neural network <ref> [MCG + 93] </ref> and [CGG + 95] based on the same principle (using global information via exogenous variable for a single model) with significantly better results.
Reference: [MCG + 94] <author> C. Muller, M. Cottrell, M. Girard, B. Girard, and M. Mangeas. </author> <title> A neural network tool for forecasting french electricity consumption. </title> <booktitle> In Proceedings of WCNN'94, </booktitle> <volume> volume 1, </volume> <pages> pages 360-365, </pages> <address> San Diego, California, USA, </address> <year> 1994. </year>
Reference-contexts: We use the prediction of these exogenous variables in our model, assuming that these predictions are accurate. consumption energy. Beside the periodicities of this series (of 7 and 365 days), we can easily see clusters like week-days and Saturdays and Fridays, or August or Christmas holidays. A Kohonen classification <ref> [MCG + 94] </ref> (based on the half-hour series) yields two different breakdowns. The first one distinguishes two kinds of days: Saturdays, Sundays, public holiday and special days on the one hand; and Monday, Tuesday, Wednesday, Thursday and Friday on the other hand.
Reference: [TL80] <author> H. Tong and K. S. Lim. </author> <title> Threshold autoregression, limit cycles and cyclical data. </title> <journal> J. Roy. Stat. Soc. B, </journal> <volume> 42 </volume> <pages> 245-292, </pages> <year> 1980. </year>
Reference-contexts: During the training, modules compete for the right to learn the data. The fittest ones specialize over certain local regions determined by the gating network. This strategy is not new (e.g., Threshold autoregressive (TAR) model <ref> [TL80] </ref> and Multivariate Adaptive Regression Splines (MARS) [Fri91] applied to prediction of financial data by [LRS94])), but the mixture of experts (ME) model ([JJNH91, JX93, JJ94, WM95] fl http://www.cs.colorado.edu/~andreas/Home.html has a number of potentially promising advantages: * gating and experts are trained simultaneously * both splitting and prediction model (gate and
Reference: [WM95] <author> A. S. Weigend and M. Mangeas. </author> <title> Experts for prediction: discovering regimes and avoiding overfitting. </title> <type> Technical Report CU-CS-764-95, </type> <institution> University of Colorado at Boulder, ftp://ftp.cs.colorado.edu/pub/Time-Series/MyPapers/experts.ps.Z, </institution> <year> 1995. </year>
Reference-contexts: We use the patterns from 1988 to 1992 as learning set and from January 1, 1993 to March 1, 1994 as test set. The performance is 5% better than the single neural network model, and there is a poor overfitting due to the splitting and a adaptive variance <ref> [WM95] </ref>. The best run gave 6 active experts and 2 unused experts (their associated gating output are zero for every pattern).
References-found: 11

