URL: http://www.ai.mit.edu/~mpf/desktop/RevComp/pend/SLPE96/main.ps
Refering-URL: 
Root-URL: 
Title: A Reversible Architecture for Low-Energy Computing  
Abstract: This paper presents a radically different approach to energy reduction, based on the result from thermodynamics that information destruction causes unavoidable energy dissipation. Computing engines can be designed that do not require energy dissipation, but only if the computation is logically reversible and performed by a physically reversible implementation technology. In a logically reversible computing engine, the current machine state at any point in time is sufficient to determine the previous state. We have developed a logically reversible computer architecture. The novel aspect of the Pendulum reversible processor is that all computation is reversible; the processor saves enough information to execute programs in reverse. At any point in the computation the processor direction may be reversed and any intermediate results will be "un-computed." This paper discusses a complete instruction set and RTL datapath design for a reversible processor architecture, and issues related to programming this processor. High level hardware description language simulations have demonstrated the functionality of the design and its ability to execute an instruction stream forward and backward.
Abstract-found: 1
Intro-found: 1
Reference: [Bak92a] <author> H. G. Baker. </author> <title> NREVERSAL of fortune | the thermodynamics of garbage collection. </title> <editor> In Y. Bekkers, editor, </editor> <booktitle> International Workshop on Memory Management, </booktitle> <pages> pages 507-524. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: This design had a number of drawbacks, including a monotonically increasing storage requirement for garbage information while running programs forward. The current design allows the programmer to explicitly manage garbage information in a non-monotonically increasing way. Baker <ref> [Bak92a] </ref> covers a wide range of topics related to reversible computing from the thermodynamics of bit erasure to garbage collection and programming subtleties. <p> In that case, garbage data kept under programmer control enables the programmer to control garbage storage and accessibility. Henry Baker has designed a reversible language called -lisp <ref> [Bak92a] </ref>. It has some unusual restrictions, such as that every argument to a function must be used exactly once within the body of the function.
Reference: [Bak92b] <author> Henry G. Baker. </author> <title> Lively linear lisp|`look ma, no garbage!'. </title> <journal> ACM Sigplan Notices, </journal> <volume> 27(8) </volume> <pages> 89-98, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: It is a "linear" lisp, meaning that the reference counts to all cons cells are always exactly 1, which eliminates the possibility of producing the traditional Lisp kind of garbage (meaning inaccessible data) as well <ref> [Bak92b] </ref>. 13 4.4 Applications of Pure Reversibility Although programming for pure reversibility may indeed have long-term benefits for low-power computing, we are also interested in reversible programming because of benefits gained through the reversibility itself.
Reference: [Ben73] <author> C. H. Bennett. </author> <title> Logical reversibility of computation. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 6 </volume> <pages> 525-532, </pages> <year> 1973. </year>
Reference-contexts: 1 Introduction This paper presents a radically different approach to energy reduction, based on the result from thermodynamics that information destruction causes unavoidable energy dissipation. Computing engines can be designed that do not require energy dissipation <ref> [Ben73, Lan61] </ref>, but only if the computation is logically reversible and performed by a physically reversible implementation technology. In a logically reversible computing engine, the current machine state at any point in time is sufficient to determine the previous state. We have developed a logically reversible computer architecture. <p> In the 1950s, this connection was popularly interpreted to mean that computation must dissipate some minimum amount of energy during every elemental act of computation. Landauer [Lan61] later recognized that (in the limit of an ideal implementation) energy dissipation is only unavoidable when information is destroyed. Bennett <ref> [Ben73] </ref> and Fredkin [FT82] first realized that a reversible computation 1 , in 1 Landauer at first believed that logical irreversibility was required for useful computation, and therefore that reversible computation was impossible, but Bennett convincingly proved otherwise. 2 which no information is destroyed, may dissipate arbitrarily small amounts of energy. <p> The program's function is then emulated through a reversible interpreter running on Pendulum. The interpreter implements a specific program-independent policy for managing garbage data produced during emulation of the irreversible program. This approach was suggested in Bennett's work <ref> [Ben73] </ref>, which showed that reversible machines could straightforwardly interpret irreversible machine programs, and that this emulation could be done using only logarithmically-increasing space over time for storing extra garbage data [Ben89] at the cost of a polynomial increase in run-time.
Reference: [Ben89] <author> C. H. Bennett. </author> <title> Time/space trade-offs for reversible computation. </title> <journal> Society for Industrial and Applied Mathematics Journal on Computing, </journal> <volume> 18(4) </volume> <pages> 766-776, </pages> <year> 1989. </year>
Reference-contexts: This approach was suggested in Bennett's work [Ben73], which showed that reversible machines could straightforwardly interpret irreversible machine programs, and that this emulation could be done using only logarithmically-increasing space over time for storing extra garbage data <ref> [Ben89] </ref> at the cost of a polynomial increase in run-time. Bennett's later technique involves periodically saving "checkpoints" of the state of the emulated machine and running the emulated machine backwards to erase accumulated garbage data and earlier-saved checkpoints.
Reference: [FT82] <author> E. F. Fredkin and T. Toffoli. </author> <title> Conservative logic. </title> <journal> International Journal of Theoretical Physics, </journal> 21(3/4):219-253, 1982. 
Reference-contexts: Previous work concerning reversible computer architecture has been either impractical or incomplete. Ressler's work [Res81] is significant in that it is the earliest work which is directly relevant to architecting fully reversible computers, but it is limited in its exclusive use of the Fredkin <ref> [FT82] </ref> gate and its neglect of key control flow issues. Hall's work [Hal94], while correct in many high level issues, is incomplete, suggesting no mapping between instruction set architecture (ISA) and register transfer level (RTL) implementation. <p> Landauer [Lan61] later recognized that (in the limit of an ideal implementation) energy dissipation is only unavoidable when information is destroyed. Bennett [Ben73] and Fredkin <ref> [FT82] </ref> first realized that a reversible computation 1 , in 1 Landauer at first believed that logical irreversibility was required for useful computation, and therefore that reversible computation was impossible, but Bennett convincingly proved otherwise. 2 which no information is destroyed, may dissipate arbitrarily small amounts of energy. <p> The kT ln 2 limit on dissipation assumes an ideal implementation structure. Also, while logical irreversibility of computation implies physical irreversibility, logical reversibility does not imply physical reversibility. The work of other researchers <ref> [FT82, YK94, KA92] </ref> has focused on physically reversible implementation techniques, and that work is beyond the scope of this paper. <p> The first to be proposed were a series of hypothetical mechanical constructions. Fredkin <ref> [FT82] </ref> 3 proposed the billiard ball model which uses collisions between hard spheres and mirrors to perform reversible computations. Fredkin demonstrated that such collisions can simulate any logic function but require perfect spheres and isolation from friction, thermal noise, and other imperfections.
Reference: [Hal94] <author> J. Storrs Hall. </author> <title> A reversible instruction set architecture and algorithms. </title> <booktitle> In Physics and Computation, </booktitle> <pages> pages 128-134, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Ressler's work [Res81] is significant in that it is the earliest work which is directly relevant to architecting fully reversible computers, but it is limited in its exclusive use of the Fredkin [FT82] gate and its neglect of key control flow issues. Hall's work <ref> [Hal94] </ref>, while correct in many high level issues, is incomplete, suggesting no mapping between instruction set architecture (ISA) and register transfer level (RTL) implementation. This paper discusses a complete instruction set and RTL datapath design for a reversible processor architecture, and issues related to programming this processor. <p> He discussed control flow issues and the concept of a garbage stack to retain extra information required for reversibility. Hall, building on his work with retractile cascades, discussed a reversible processor architecture and algorithms <ref> [Hal94] </ref> 4 based on the PDP-10 instruction set. He included instructions to "undo" arithmetic and logical operations and a set of backward branching instructions to undo control flow operations. Our work supports and builds on many of his findings.
Reference: [KA92] <author> J. G. Koller and W. C. Athas. </author> <title> Adiabatic switching, low energy computing, and the physics of storing and erasing information. </title> <booktitle> In Physics of Computation Workshop, </booktitle> <year> 1992. </year>
Reference-contexts: The kT ln 2 limit on dissipation assumes an ideal implementation structure. Also, while logical irreversibility of computation implies physical irreversibility, logical reversibility does not imply physical reversibility. The work of other researchers <ref> [FT82, YK94, KA92] </ref> has focused on physically reversible implementation techniques, and that work is beyond the scope of this paper.
Reference: [Kni90] <author> Tom Knight. </author> <title> An architecture for mostly functional languages. </title> <editor> In Patrick Henry Winston and Sarah Alexandra Shellard, editors, </editor> <booktitle> Artificial Intelligence at MIT: Expanding Frontiers, </booktitle> <volume> volume 1, chapter 19, </volume> <pages> pages 500-519. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Serial programs may be automatically parallelized by performing speculative execution of program fragments, as suggested by Knight <ref> [Kni90] </ref>. In a reversible processor, a program fragment can be reversed if it is found to have accessed the wrong value for a datum.
Reference: [Lan61] <author> R. Landauer. </author> <title> Irreversibility and heat generation in the computing process. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 5 </volume> <pages> 183-191, </pages> <year> 1961. </year>
Reference-contexts: 1 Introduction This paper presents a radically different approach to energy reduction, based on the result from thermodynamics that information destruction causes unavoidable energy dissipation. Computing engines can be designed that do not require energy dissipation <ref> [Ben73, Lan61] </ref>, but only if the computation is logically reversible and performed by a physically reversible implementation technology. In a logically reversible computing engine, the current machine state at any point in time is sufficient to determine the previous state. We have developed a logically reversible computer architecture. <p> In the 1950s, this connection was popularly interpreted to mean that computation must dissipate some minimum amount of energy during every elemental act of computation. Landauer <ref> [Lan61] </ref> later recognized that (in the limit of an ideal implementation) energy dissipation is only unavoidable when information is destroyed. <p> Szilard attempted to resolve the paradox by arguing that the process of measurement required dissipation. But the dissipation arises from erasure of information <ref> [Lan61] </ref>. When the demon measures a particle, it must set a bit indicating the particle's speed. The destination of the particle is controlled by the state of this bit. Once a particle has been redirected, the demon must reset the bit in preparation for the next measurement.
Reference: [LS90] <author> Robert Y. Levine and Alan T. Sherman. </author> <title> A note on Bennett's time-space tradeoff for reversible computation. </title> <journal> Society for Industrial and Applied Mathematics Journal on Computing, </journal> <volume> 19(4) </volume> <pages> 673-677, </pages> <year> 1990. </year> <month> 17 </month>
Reference-contexts: Bennett's later technique involves periodically saving "checkpoints" of the state of the emulated machine and running the emulated machine backwards to erase accumulated garbage data and earlier-saved checkpoints. Levine presents a precise complexity analysis of Bennett's technique <ref> [LS90] </ref>, and Li and Vitanyi show that this analysis provides the optimal space-time tradeoff among algorithms that work by saving checkpoints [LV96].
Reference: [LV96] <author> Ming Li and Paul M. B. Vitanyi. </author> <title> Reversible simulation of irreversible computation. </title> <booktitle> In Proc. 11th IEEE Conference on Computational Complexity, </booktitle> <address> Philadelphia, Pennsylvania, </address> <month> May 24-27, </month> <year> 1996. </year>
Reference-contexts: Levine presents a precise complexity analysis of Bennett's technique [LS90], and Li and Vitanyi show that this analysis provides the optimal space-time tradeoff among algorithms that work by saving checkpoints <ref> [LV96] </ref>. Note that the Bennett-style technique of simply interpreting irreversible programs is completely general in that it does not require the interpreter to know anything about the structure of the 11 interpreted program.
Reference: [Res81] <author> A. L. Ressler. </author> <title> The design of a conservative logic computer and a graphical editor simulator. </title> <type> Master's thesis, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <year> 1981. </year>
Reference-contexts: This technique restores the processor to its original state and requires dissipation only during output of a result. Previous work concerning reversible computer architecture has been either impractical or incomplete. Ressler's work <ref> [Res81] </ref> is significant in that it is the earliest work which is directly relevant to architecting fully reversible computers, but it is limited in its exclusive use of the Fredkin [FT82] gate and its neglect of key control flow issues. <p> He measured an energy savings of over 99% of the power used in conventional CMOS implementations (modulo certain power supply issues) of the same circuits when run at speeds below 1 MHz. 2.3 Architecture Ressler <ref> [Res81] </ref> appears to have been the first to investigate the requirements of a reversible computer in the design of a simple accumulator-based machine. He discussed control flow issues and the concept of a garbage stack to retain extra information required for reversibility.
Reference: [SF95] <author> Paul M. Solomon and David J. Frank. </author> <title> Power measurements of adiabatic circuits by thermoelectric technique. </title> <booktitle> In Symposium on Low Power Electronics, </booktitle> <pages> pages 18-19, </pages> <year> 1995. </year>
Reference: [SYR95] <author> Dinesh Somasekhar, Yibin Ye, and Kaushik Roy. </author> <title> An energy recovery static RAM memory core. </title> <booktitle> In Symposium on Low Power Electronics, </booktitle> <pages> pages 62-63, </pages> <year> 1995. </year>
Reference-contexts: During an exchange with memory, some implicit intermediate storage contains the register file and memory values, and reading the value from the register file clears that location. When the memory value is written, it overwrites that constant zero. This does not entail dissipation. Energy recovery memory structures <ref> [SYR95] </ref> have been developed. 3.3 Control Flow Control flow operations in the context of a conventional processor allow program execution paths to coalesce in a manner which may be irreversible.
Reference: [Szi29] <author> L. </author> <title> Szilard. On the decrease of entropy in a thermodynamic system by the intervention of intelligent beings. </title> <journal> Zeitschrift fur Physik, </journal> <volume> 53 </volume> <pages> 840-852, </pages> <year> 1929. </year> <booktitle> English translation in Behavioral Science, </booktitle> 9:301-310, 1964. 
Reference-contexts: High level hardware description language simulations have demonstrated the functionality of the design and its ability to execute an instruction stream forward and backward. 2 Background and Previous Work 2.1 Physics Maxwell's demon and Szilard's analysis <ref> [Szi29] </ref> of the demon first suggested the connection between a single degree of freedom (one bit) and a minimum quantity of entropy. In the 1950s, this connection was popularly interpreted to mean that computation must dissipate some minimum amount of energy during every elemental act of computation.
Reference: [YK93] <author> S. G. Younis and T. F. Knight, Jr. </author> <title> Practical implementation of charge recovering asymptotically zero power CMOS. </title> <booktitle> In Proceedings of the 1993 Symposium in Integrated Systems, </booktitle> <pages> pages 234-250. </pages> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: The emergence of practical reversible implementation techniques provided much of the motivation for this research. The most highly developed energy recovering reversible logic family is probably the Split-level Charge Recovery Logic (SCRL) of Younis and Knight <ref> [YK93, YK94] </ref>. Instead of constant power supply voltage rails, SCRL uses a series of clock signals which gradually swing from a midpoint voltage to the logic high or low voltage.
Reference: [YK94] <author> S. G. Younis and T. F. Knight, Jr. </author> <title> Asymptotically zero energy split-level charge recovery logic. </title> <booktitle> In International Workshop on Low Power Design, </booktitle> <pages> pages 177-182, </pages> <year> 1994. </year>
Reference-contexts: The kT ln 2 limit on dissipation assumes an ideal implementation structure. Also, while logical irreversibility of computation implies physical irreversibility, logical reversibility does not imply physical reversibility. The work of other researchers <ref> [FT82, YK94, KA92] </ref> has focused on physically reversible implementation techniques, and that work is beyond the scope of this paper. <p> The emergence of practical reversible implementation techniques provided much of the motivation for this research. The most highly developed energy recovering reversible logic family is probably the Split-level Charge Recovery Logic (SCRL) of Younis and Knight <ref> [YK93, YK94] </ref>. Instead of constant power supply voltage rails, SCRL uses a series of clock signals which gradually swing from a midpoint voltage to the logic high or low voltage.
Reference: [YK95] <author> S. G. Younis and T. F. Knight, Jr. </author> <title> Harmonic resonant rail drivers for adiabatic logic. </title> <booktitle> In Proceedings of the 1995 Symposium on Advanced Research in VLSI. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year> <month> 18 </month>
Reference-contexts: Multi-frequency resonators or transmission line techniques can be used to resonate this power with arbitrary waveform voltages as described in <ref> [YK95] </ref>. The energy dissipated per operation in SCRL falls linearly as the computation delay increases, as compared to conventional CMOS circuits which have a relatively constant energy dissipation per operation.
References-found: 18

