URL: ftp://speech.cse.ogi.edu/pub/isgw97/feldman.ps
Refering-URL: 
Root-URL: 
Title: ISGW '97 Area 2 Speech and Natural Language Understanding Feldman 89 LANGUAGE LEARNING AND ITS APPLICATIONS  
Author: Jerome Feldman George Lakoff 
Keyword: PROGRAM AREA Speech and Natural Language Understanding. KEYWORDS Learning, semantics, structured, connectionist, neural, metaphor, language. PROJECT SUMMARY  
Address: 1947 Center St. Berkeley, CA 94704  
Affiliation: 1 International Computer Science Institute CONTACT INFORMATION  
Email: Email: jfeldman@icsi.berkeley.edu  
Phone: Phone: (510) 643-9153 Fax (510) 643-7684  
Web: WWW PAGE http://www.icsi.berkeley.edu/LZERO  
Abstract: The question of how human beings learn natural languages is one of abiding scientific interest, and computational models have begun to play a fruitful role in understanding how language might be acquired. Computational studies of grammar and language learning have also resulted in a wide range of applications. Both of the PIs (JF and GL) have worked on the problem for some time. The collaboration arises from two articulating insights. GL has been involved for 20 years in the study of how human conceptual systems are structured in terms of the details of the functioning of our bodies and brains and how language reflects that bodily structure (Feldman 89, Lakoff 87). Over roughly the same period, JF came to believe that structured connectionism provides the only known computational formalism adequate for the fine-grained modeling of human intelligence (Shastri 93). The joint effort began in 1988 when JF came to UC Berkeley and ICSI and with GL founded the L-zero project. The group seeks to develop structured connectionist models that can learn and use both natural conceptual systems and the languages that express them. A current overview of our efforts found on the NTL web page. After some preliminary explorations, we were able to formulate a version of the language acquisition problem that was small enough to be tractable, but seemed to address most of the important issues. We repeat the initial manifesto below. Pursuing this path, our early efforts were quite productive and well received, but there are a number of limitations that we found no way to surmount. Over the last year, we have developed an extended version of the task specification and made significant changes in our representation and learning methods. For comparison, the 1990 challenge (Feldman et al 1990) was: The system is given examples of pictures paired with true statements about those pictures in an arbitrary natural language. The system is to learn the relevant portion of the language well enough so that given a novel sentence of that language, it can determine whether or not the sentence is true of the accompanying picture. The task was extraordinarily difficult, since the conceptual systems for spatial relations concepts differ markedly from language to language. The problem was solved by making use of results from cognitive linguistics, neuroscience, and psychophysics in the design of a structured connectionist acquisition model that worked for a wide range of languages. As cognitive science, the result is a theory of how spatial relations concepts arise from the structure of the visual system. The result from the perspective of connectionist computation is that structured connectionism can be employed in learning a very important segment of human conceptual systems and the language that expresses them. A detailed description of the system can be found in Terry Regier's recently published book entitled "The Human Semantic Potential" from MIT Press, 1996. Although both the grammar learning and concept learning projects were quite successful in their own terms, each exhibits limitations and this has led us to reformulate the original problem (Feldman et al 1996). In requiring only recognition, the original task fell far short of the human capabilities, particularly in concept acquisition and use. Much of our proposed work involves reformulating the language learning task and developing representations adequate for the extended problem. Of the many shortcomings of our concept learning system we believe that the most important are three: invert-ibility, inference, and abstract concepts. The connectionst networks used in Regier's work were feedforward networks trained by back-propagation. This is fine for most applied tasks, but is inadequate as a model of human concept or word learning or inference. For example, feedforward networks have no structure that could produce an example of a concept that it recognizes perfectly. Its inferential inadequacies arise in similar ways. The network has no way to infer that A above B and B above C suggest that A is above C. After exploring a wide range of modifications to the back-prop structure, we conclude that a radically different representation was called for. The problem of learning words for higher level and abstract concepts is not, like invertibility and inference, primarily computational. The very idea of learning labels for direct bodily experience breaks down for concepts like "sell", and even more so for "inflation", etc. GL (along with many others) has been working for decades on how such abstract target concepts map to more concrete source domains (Lakoff 1987). Such "metaphoric" 
Abstract-found: 1
Intro-found: 1
Reference: <editor> Badler, N., Phillips, C. and Webber, B.: </editor> <publisher> Simulating Humans , Oxford University Press, </publisher> <address> NY, </address> <year> 1993. </year>
Reference-contexts: We need a robot or simulated robot to carry out the motions so they can be labelled by native speakers of various languages. We have decided to use the Jack simulator from U. Penn <ref> (Badler et al 1993) </ref> and have made considerable progress on getting it to run in our environment and to communicate with the learning code. * Program Construction and Testing Even after all the ideas are understood, it is a difficult task to build a system that will appropriately interface with the
Reference: <author> Feldman, J.A.: </author> <title> "Neural Representation Of Conceptual Knowledge," Neural Connections, Mental Computation, </title> <editor> eds. Lynn Nadel and others, </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, pgs. 68-103, </address> <year> 1989. </year>
Reference-contexts: The collaboration arises from two articulating insights. GL has been involved for 20 years in the study of how human conceptual systems are structured in terms of the details of the functioning of our bodies and brains and how language reflects that bodily structure <ref> (Feldman 89, Lakoff 87) </ref>. Over roughly the same period, JF came to believe that structured connectionism provides the only known computational formalism adequate for the fine-grained modeling of human intelligence (Shastri 93).
Reference: <editor> Feldman, J., et. al. </editor> <booktitle> "Miniature Language Acquisition: </booktitle>
References-found: 3

