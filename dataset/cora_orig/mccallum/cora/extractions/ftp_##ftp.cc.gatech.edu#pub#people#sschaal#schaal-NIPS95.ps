URL: ftp://ftp.cc.gatech.edu/pub/people/sschaal/schaal-NIPS95.ps
Refering-URL: ftp://ftp.cc.gatech.edu/pub/people/sschaal/schaal-NIPS95.html
Root-URL: 
Email: sschaal@cc.gatech.edu  cga@cc.gatech.edu  
Title: From Isolation to Cooperation: An Alternative View of a System of Experts  
Author: Schaal. S. Atkeson, C. G. (). In: D. S. Touretzky, M. C. Mozer, M. E. Hasselmo (eds.): Ad- Stefan Schaal Christopher C. Atkeson 
Address: 801 Atlantic Drive, Atlanta, GA 30332-0280  
Affiliation: College of Computing, Georgia Tech,  
Web: http://www.cc.gatech.edu/fac/Stefan.Schaal  http://www.cc.gatech.edu/fac/Chris.Atkeson  
Note: vances in Neural Information Processing Systems 8, pp. 605-611. Cambridge, MA: MIT Press.  ATR Human Information Processing, 2-2 Hikaridai, Seiko-cho, Soraku-gun, 619-02 Kyoto  
Abstract: We introduce a constructive, incremental learning system for regression problems that models data by means of locally linear experts. In contrast to other approaches, the experts are trained independently and do not compete for data during learning. Only when a prediction for a query is required do the experts cooperate by blending their individual predictions. Each expert is trained by minimizing a penalized local cross val i- dation error using second order methods. In this way, an expert is able to find a local distance metric by adjusting the size and shape of the rece p- tive field in which its predictions are valid, and also to detect relevant i n- put features by adjusting its bias on the importance of individual input dimensions. We derive asymptotic results for our method. In a variety of simulations the properties of the algorithm are demonstrated with respect to interference, learning speed, prediction accuracy, feature detection, and task oriented incremental learning.
Abstract-found: 1
Intro-found: 1
Reference: <author> Atkeson, C. G., Moore, A. W., & Schaal, S. </author> <note> (submitted). "Locally weighted learning." Artificial I n- telligence Review. </note>
Reference: <author> Atkeson, C. G. </author> <year> (1992). </year> <title> "Memory-based approaches to approximating continuous functions." </title> <editor> In: Casdagli, M., & Eubank, S. (Eds.), </editor> <title> Nonlinear Modeling and Fore- casting, </title> <publisher> pp.503-521. Addison Wesley. </publisher>
Reference: <author> Belsley, D. A., Kuh, E., & Welsch, R. E. </author> <year> (1980). </year> <title> Re- gression diagnostics: Identifying influential data and sources of collinearity. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Cleveland, W. S. </author> <year> (1979). </year> <title> "Robust locally weighted regression and smoothing scatterplots." </title> <journal> J. American Stat. Association, </journal> <volume> 74, </volume> <editor> pp.829-836. de Boor, C. </editor> <year> (1978). </year> <title> A practical guide to splines. </title> <address> New York: </address> <publisher> Springer. </publisher>
Reference: <author> Hastie, T. J., & Tibshirani, R. J. </author> <year> (1990). </year> <title> Generalized additive models. </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. </author> <year> (1991). </year> <title> "Adaptive mixtures of local experts." </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <month> pp.79-87. </month>
Reference-contexts: This classifier determines which expert models are used in which part of the input space. For incremental learning, competitive learn ing methods are usually applied. Here the e x- perts compete for data such that they change their domains of expertise until a stable configuration is achieved <ref> (e.g., Jacobs, Jordan, Nowlan, & Hinton, 1991) </ref>. The advantage of local experts is that they can have simple parameterizations, such as locally constant or locally linear models. This offers benefits in terms of analyzability, learning speed, and robustness (e.g., Jordan & Jacobs, 1994).
Reference: <author> Jordan, M. I., & Jacobs, R. </author> <year> (1994). </year> <title> "Hierarchical mi x- tures of experts and the EM algorithm." </title> <journal> Neural Co m- putation, </journal> <volume> 6, 2, </volume> <month> pp.181-214. </month>
Reference-contexts: The advantage of local experts is that they can have simple parameterizations, such as locally constant or locally linear models. This offers benefits in terms of analyzability, learning speed, and robustness <ref> (e.g., Jordan & Jacobs, 1994) </ref>. For simple experts, however, a large number of experts is necessary to model a function. As a result, the expert selection sys tem has to be more complicated and, thus, has a higher risk of getting stuck in local minima and/or of learning rather slowly.
Reference: <author> Ljung, L., & S _derstr_m, T. </author> <year> (1986). </year> <title> Theory and pra c- tice of recursive identification. </title> <publisher> Cambridge, MIT Press. </publisher>
Reference-contexts: This will be analyzed further in Section 2.2. The update equations for the linear subnet are the standard weighted recursive least squares equation with forgetting factor l <ref> (Ljung & Sderstrm, 1986) </ref>: b b b cv n T n T n w + + + + = -( ) P x P P x P x where and This is a Newton method, and it requires maintaining the matrix P , which is size 0 5 1. d d
Reference: <author> McLachlan, G. J., & Basford, K. E. </author> <year> (1988). </year> <title> Mixture models. </title> <address> New York: </address> <publisher> Marcel Dekker. </publisher>
Reference-contexts: In incremental learning, another potential danger arises when the input distribution of the data changes. The expert selection system usually makes either 2 implicit or explicit prior assumptions about the input data distribution. For example, in the classical mixture model <ref> (McLachlan & Basford, 1988) </ref> which was employed in several local expert approaches, the prior probabilities of each mixture model can be interpreted as the fraction of data points each expert expects to experience.
Reference: <author> Nadaraya, E. A. </author> <year> (1964). </year> <title> "On estimating regression." </title> <journal> Theor. Prob. Appl., </journal> <volume> 9, </volume> <month> pp.141-142. </month>
Reference: <author> Schaal, S., & Atkeson, C. G. </author> <year> (1994b). </year> <title> "Assessing the quality of learned local models." </title> <editor> In: Cowan, J. , T e- sauro, G., & Alspector, J. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Scott, D. W. </author> <year> (1992). </year> <title> Multivariate Density Estimation . New York: </title> <publisher> Wiley. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1992). </year> <title> "Gain adaptation beats least squares." </title> <booktitle> In: Proc. of 7th Yale Workshop on Adaptive and Learning Systems, </booktitle> <address> New Haven, CT. </address>
Reference: <author> Wolpert, D. H. </author> <year> (1990). </year> <title> "Stacked genealization." Los Alamos Technical Report LA-UR-90-3460 . -0.4 -0.2 0 0.2 0.4 0 0.1 0.2 0.3 0.4 0.5 Gravity (a) (b) (c) arm: (a) Performance of a PID controller b efore learning (the dimmed lines denote the desired trajectories, the solid lines the actual perfor mance); (b) Perfor- mance after learning using a PD controller with feed- forward commands from the learned inverse model; (c) Performance of the learned controller after training on the upper 8 of (b) (see text for more explanations). </title>
Reference-contexts: 1. INTRODUCTION Distributing a learning task among a set of experts has become a popular method in computational learning. One approach is to employ several experts, each with a global domain of expertise <ref> (e.g., Wolpert, 1990) </ref>. When an output for a given input is to be predicted, every expert gives a prediction together with a confidence measure. The individual predictions are combined into a single result, for instance, based on a confidence weighted average.
References-found: 14

