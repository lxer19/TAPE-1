URL: ftp://archive.cis.ohio-state.edu/pub/neuroprose/jung.selection_examples.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00005.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail georgju@physik.uni-wuerzburg.de  
Title: Selection of Examples for a Linear Classifier  
Author: Georg Jung and Manfred Opper 
Web: ftp ftp.physik.uni-wuerzburg.de  
Note: Germany  
Address: Am Hubland, D-97074 Wurzburg,  WUE-ITP-95-022  
Affiliation: Universitat Wurzburg Institut fur Theoretische Physik  Ref.:  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Gardner E 1988 J. </author> <title> Phys. A: </title> <journal> Math. Gen. </journal> <volume> 21 257 </volume>
Reference-contexts: The generalization error can be similarly calculated: * g = p ! t Z Dh 4 2 @ q 1 3 5. Statistical Mechanics: A Lagrangean Approach Following the approach of Elizabeth Gardner <ref> [1, 2] </ref>, the application of statistical mechanics to network learning (for a review see [6, 7, 8]) is often based on the fact, that the network configurations J j obtained from a learning algorithm are minima of a suitable training energy E.
Reference: [2] <author> Gardner E and Derrida B 1988 J. </author> <title> Phys. A: </title> <journal> Math. Gen. </journal> <volume> 21 271 </volume>
Reference-contexts: The generalization error can be similarly calculated: * g = p ! t Z Dh 4 2 @ q 1 3 5. Statistical Mechanics: A Lagrangean Approach Following the approach of Elizabeth Gardner <ref> [1, 2] </ref>, the application of statistical mechanics to network learning (for a review see [6, 7, 8]) is often based on the fact, that the network configurations J j obtained from a learning algorithm are minima of a suitable training energy E.
Reference: [3] <author> Opper M, Kinzel W, Kleinz J and Nehl R 1990 J. </author> <title> Phys. A: </title> <journal> Math. Gen. </journal> <volume> 23 L581 </volume>
Reference-contexts: Section five contains a new approach to the statistical mechanics of the problem. Finally, in sections six and seven, the results of our calculations are presented and discussed. 3 2. Adaline Learning As has been shown e.g. in <ref> [9, 3, 22, 10] </ref>, the effect of overfitting can already be observed for the case of a simple linear classifier, the so called Adaline model [13, 12, 21], which we will discuss in the following sections. <p> In this case, by introducing a canonical ensemble of networks 7 at temperature fi 1 , the desired configuration appears as the one with maximal weight in the partition function Z = R dJ e fiE , in the limit fi ! 1. This procedure works fine <ref> [3] </ref> e.g. in order to determine the order parameters of the standard Adaline rule, which are explicit functions of the couplings. However it does not give us any direct information on the embedding strenghts, which are only implicitely related to the couplings. <p> It is interesting to note that both ~ R and ~q 0 diverge for ff ! 1 as in the case of normal Adaline learning <ref> [3] </ref>. The ratio ~ R= p however, which enters the formulae for " g , remains finite. For the same task, Figure 7 shows the result for the modified Hebb method f (x) = fi (x). <p> Assuming binary outputs, jg B (h)j = 1 this finally yields P (x) = q +1 Z Dh exp 2 ! The order parameters R and q 0 can be obtained by the techniques introduced in <ref> [3, 10] </ref> and read R = ff 1 ff 1 B (h)R 2 (A3) for ff &lt; 1. Explicit results for continuous functions g B (h) are given in [10].
Reference: [4] <institution> Hebb D O 1949 The organization of behaviour, </institution> <address> (New York Wiley) </address>
Reference-contexts: Weighting of Examples Our basic strategy for the selection of examples is based on the fact that most iterative learning algorithms for a single layer net result in a coupling vector which has the form of a weighted Hebbian rule <ref> [4, 9] </ref>. To see this, consider a learning algorithm of the backpropagation type which is based on the minimization of a quadratic training energy E = 2 =1 by gradient descent.
Reference: [5] <editor> Anlauf J and Biehl M 1989 Europhys. Lett. </editor> <volume> 10 687 </volume>
Reference-contexts: The latter ones would have an output that has already the correct sign without being learnt. This strategy may also be understood as an approximation to the AdaTron algorithm <ref> [5] </ref> which, by construction, allows for nonnegative embedding strengths only. Another possibility was discussed in a paper by Garces [18] for the case of Adatron learning.
Reference: [6] <author> Seung H S, </author> <title> Sompolinsky H and Tishby N 1992 Phys. </title> <note> Rev. A 45 6056 </note>
Reference-contexts: The generalization error can be similarly calculated: * g = p ! t Z Dh 4 2 @ q 1 3 5. Statistical Mechanics: A Lagrangean Approach Following the approach of Elizabeth Gardner [1, 2], the application of statistical mechanics to network learning (for a review see <ref> [6, 7, 8] </ref>) is often based on the fact, that the network configurations J j obtained from a learning algorithm are minima of a suitable training energy E.
Reference: [7] <author> Watkin T L H, </author> <title> Rau A and Biehl M 1993 Rev. Mod. </title> <journal> Phys. </journal> <volume> 65 499 </volume>
Reference-contexts: The generalization error can be similarly calculated: * g = p ! t Z Dh 4 2 @ q 1 3 5. Statistical Mechanics: A Lagrangean Approach Following the approach of Elizabeth Gardner [1, 2], the application of statistical mechanics to network learning (for a review see <ref> [6, 7, 8] </ref>) is often based on the fact, that the network configurations J j obtained from a learning algorithm are minima of a suitable training energy E.
Reference: [8] <editor> Opper M and Kinzel W Statistical Mechanics of Generalization in: </editor> <title> Physics of Neural Networks III, edited by van Hemmen J S, Domany E and Schulten K (Berlin: Springer, </title> <note> to be published). </note>
Reference-contexts: The generalization error can be similarly calculated: * g = p ! t Z Dh 4 2 @ q 1 3 5. Statistical Mechanics: A Lagrangean Approach Following the approach of Elizabeth Gardner [1, 2], the application of statistical mechanics to network learning (for a review see <ref> [6, 7, 8] </ref>) is often based on the fact, that the network configurations J j obtained from a learning algorithm are minima of a suitable training energy E.
Reference: [9] <institution> Vallet F 1989 Europhys. </institution> <type> Lett. 8 747 </type>
Reference-contexts: Section five contains a new approach to the statistical mechanics of the problem. Finally, in sections six and seven, the results of our calculations are presented and discussed. 3 2. Adaline Learning As has been shown e.g. in <ref> [9, 3, 22, 10] </ref>, the effect of overfitting can already be observed for the case of a simple linear classifier, the so called Adaline model [13, 12, 21], which we will discuss in the following sections. <p> Weighting of Examples Our basic strategy for the selection of examples is based on the fact that most iterative learning algorithms for a single layer net result in a coupling vector which has the form of a weighted Hebbian rule <ref> [4, 9] </ref>. To see this, consider a learning algorithm of the backpropagation type which is based on the minimization of a quadratic training energy E = 2 =1 by gradient descent.
Reference: [10] <author> Boes S, </author> <title> Kinzel W and Opper M 1993 Phys. </title> <journal> Rev. </journal> <volume> E 47 1384 </volume>
Reference-contexts: Section five contains a new approach to the statistical mechanics of the problem. Finally, in sections six and seven, the results of our calculations are presented and discussed. 3 2. Adaline Learning As has been shown e.g. in <ref> [9, 3, 22, 10] </ref>, the effect of overfitting can already be observed for the case of a simple linear classifier, the so called Adaline model [13, 12, 21], which we will discuss in the following sections. <p> Assuming binary outputs, jg B (h)j = 1 this finally yields P (x) = q +1 Z Dh exp 2 ! The order parameters R and q 0 can be obtained by the techniques introduced in <ref> [3, 10] </ref> and read R = ff 1 ff 1 B (h)R 2 (A3) for ff &lt; 1. Explicit results for continuous functions g B (h) are given in [10]. <p> Explicit results for continuous functions g B (h) are given in <ref> [10] </ref>.
Reference: [11] <editor> Kohonen T 1988 Self-Organisation and associative memory (Berlin: </editor> <publisher> Springer). </publisher> <pages> 14 </pages>
Reference-contexts: An explicit solution for such a weight vector is given by the pseudo-inverse-solution (PSI) <ref> [11] </ref>: ffN X S B (C 1 ) - ~ ; (3) where C - = 1 N j ~ j ~ - j is the correlation matrix of the training patterns.
Reference: [12] <author> Diederich S and Opper M 1987 Phys. </author> <title> Rev. </title> <journal> Lett. </journal> <volume> 59, </volume> <pages> 949 </pages>
Reference-contexts: Adaline Learning As has been shown e.g. in [9, 3, 22, 10], the effect of overfitting can already be observed for the case of a simple linear classifier, the so called Adaline model <ref> [13, 12, 21] </ref>, which we will discuss in the following sections.
Reference: [13] <institution> Widrow B and Hoff M E 1960 WESCON Convention Report IV 96 </institution>
Reference-contexts: Adaline Learning As has been shown e.g. in [9, 3, 22, 10], the effect of overfitting can already be observed for the case of a simple linear classifier, the so called Adaline model <ref> [13, 12, 21] </ref>, which we will discuss in the following sections.
Reference: [14] <author> Kinzel W and Rujan P 1990 Europhys. Lett. </author> <month> 13 473 </month>
Reference-contexts: From an analytical viewpoint, it is simple enough to be free of the problems of replica symmetry breaking. This approach should not be confused with the well studied problem of learning with queries <ref> [14, 15, 16] </ref>. In the latter case, one selects inputs with respect to some criterion, before their outputs are observed. In our case, all training examples, i.e. both inputs and outputs are known to the learner.
Reference: [15] <author> Watkin T L H and Rau A 1992 J. </author> <title> Phys. A: </title> <journal> Math. Gen. </journal> <volume> 25 113 </volume>
Reference-contexts: From an analytical viewpoint, it is simple enough to be free of the problems of replica symmetry breaking. This approach should not be confused with the well studied problem of learning with queries <ref> [14, 15, 16] </ref>. In the latter case, one selects inputs with respect to some criterion, before their outputs are observed. In our case, all training examples, i.e. both inputs and outputs are known to the learner.
Reference: [16] <author> Seung H S, </author> <title> Opper M and Sompolinsky H 1992 contribution to the Vth Annual Workshop on Computational Learning Theory (COLT92) (Pittsburgh 1992); pages 287- 294, published by the Ass. for Computing Machinery, </title> <address> New York </address>
Reference-contexts: From an analytical viewpoint, it is simple enough to be free of the problems of replica symmetry breaking. This approach should not be confused with the well studied problem of learning with queries <ref> [14, 15, 16] </ref>. In the latter case, one selects inputs with respect to some criterion, before their outputs are observed. In our case, all training examples, i.e. both inputs and outputs are known to the learner.
Reference: [17] <institution> Opper M 1988 Phys. </institution> <note> Rev. A 38 3824 </note>
Reference: [18] <editor> Garces R Scheme to improve the generalization error in: </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School pages 358 - 363, edited by Mozer M, </booktitle> <editor> Smolensky P, Touretzky D, </editor> <title> Elman J and Weigend A </title>
Reference-contexts: This means that an increase of the number of examples can in some regions lead to a decrease in generalization abilities. Recently, Garces has found in numerical studies that overfitting can be avoided by deleting examples from the training set <ref> [18] </ref>. In this respect, it is interesting to find out from which selection of examples the performance of the network will benefit most. In this article, we will study a heuristic strategy for the selection of examples in the case of a linear classifier applied to two toy problems. <p> The latter ones would have an output that has already the correct sign without being learnt. This strategy may also be understood as an approximation to the AdaTron algorithm [5] which, by construction, allows for nonnegative embedding strengths only. Another possibility was discussed in a paper by Garces <ref> [18] </ref> for the case of Adatron learning. Only examples which are not too hard to learn, i.e. which have positive embedding strengths below a certain value, were left in the training set To find an expression for the total embedding strengths, it is not necessary to solve the dynamics (5). <p> Deleting the examples with negative weights from the training set avoids the overfitting phenomenon. By using different weight functions f (x), our analysis might be extended to other types of example selections e.g. erasing the ones which are too hard to learn <ref> [18] </ref>. In this case, we expect that the network's performance will benefit mostly from such a procedure, when ff is sufficiently larger than 1. However, such an analysis requires a different mathematical treatment than that of section 5.
Reference: [19] <institution> Watkin T L H and Rau A 1992 Phys. </institution> <note> Rev. A 45 4102 </note>
Reference-contexts: The generalization error can be easily calculated to be: * g = p ! t Z Dh 4 2 @ q 1 3 where Dh is the gaussian measure: Dh = dh p x R Dh: (ii) The second rule is the so-called reversed-wedge-problem <ref> [19] </ref>: g B (h B ) = sign (h B (h B t ) 2 ): The shaded regions in Figure 2 display in the same way as before the fraction of input space, where the answers from teacher and student differ.
Reference: [20] <author> Garces R, Kuhlmann P and Eissfeller H 1992 J. </author> <title> Phys. A: </title> <journal> Math. Gen. </journal> <volume> 25 L1335 </volume>
Reference: [21] <editor> Kinzel W and Opper M 1991 Dynamics of Learning; in: </editor> <title> Physics of Neural Networks, </title> <editor> ed. by van Hemmen J L, Domany E and Schulten K; published by Springer Verlag, p. </editor> <volume> 149 </volume>
Reference-contexts: Adaline Learning As has been shown e.g. in [9, 3, 22, 10], the effect of overfitting can already be observed for the case of a simple linear classifier, the so called Adaline model <ref> [13, 12, 21] </ref>, which we will discuss in the following sections.
Reference: [22] <editor> Krogh A and Hertz J 1991 in: </editor> <booktitle> Advances in Neural Information Processing Systems III (San Mateo: </booktitle> <publisher> Morgan Kaufmann) </publisher>
Reference-contexts: Section five contains a new approach to the statistical mechanics of the problem. Finally, in sections six and seven, the results of our calculations are presented and discussed. 3 2. Adaline Learning As has been shown e.g. in <ref> [9, 3, 22, 10] </ref>, the effect of overfitting can already be observed for the case of a simple linear classifier, the so called Adaline model [13, 12, 21], which we will discuss in the following sections.
Reference: [23] <author> LeCun Y, </author> <title> Denker J S and Solla S 1990 Optimal Brain Damage, </title> <booktitle> in: Advances in Neural Information Processing Systems II (Denver 1989), </booktitle> <editor> ed. Touretzky D, </editor> <address> 598 (San Mateo: </address> <publisher> Morgan Kaufmann) </publisher>
Reference: [24] <author> Bouten M, </author> <title> Engel A, </title> <journal> Komoda A and Serneels R 1990 J. Phys. A: Math. Gen. </journal> <volume> 23 4643 </volume>

References-found: 24

