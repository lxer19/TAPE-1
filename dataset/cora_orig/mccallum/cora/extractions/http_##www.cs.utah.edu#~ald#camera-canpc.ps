URL: http://www.cs.utah.edu/~ald/camera-canpc.ps
Refering-URL: http://www.cs.utah.edu/~ald/
Root-URL: 
Email: ald@cs.utah.edu, swanson@cs.utah.edu, map@cs.utah.edu  
Title: Efficient Communication Mechanisms for Cluster Based Parallel Computing  
Author: Al Davis, Mark Swanson, Mike Parker 
Keyword: NOW-based architectures.  
Address: Salt Lake City, UT 84112, USA  
Affiliation: Department of Computer Science University of Utah  
Abstract: The key to crafting an effective scalable parallel computing system lies in minimizing the delays imposed by the system. Of particular importance are communications delays, since parallel algorithms must communicate frequently. The communication delay is a system-imposed latency. The existence of relatively inexpensive high performance workstations and emerging high performance interconnect options provide compelling economic motivation to investigate NOW/COW (network/cluster of workstation) architectures. However, these commercial components have been designed for generality. Cluster nodes are connected by longer physical wire paths than found in special-purpose supercomputer systems. Both effects tend to impose intractable latencies on communication. Even larger system-imposed delays result from the overhead of sending and receiving messages. This overhead can come in several forms, including CPU occupancy by protocol and device code as well as interference with CPU access to various levels of the memory hierarchy. Access contention becomes even more onerous when the nodes in the system are themselves symmetric multiprocessors. Additional delays are incurred if the communication mechanism requires processes to run concurrently in order to communicate with acceptable efficiency. This paper presents the approach taken by the Utah Avalanche project which spans user level code, operating system support, and network interface hardware. The result minimizes the constraining effects of latency, overhead, and loosely coupled scheduling that are common characteristics in 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Agarwal, A., Bianchini, R., Chaiken, D., and Johnson, K. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture (June 1995), </booktitle> <pages> pp. 2-13. </pages>
Reference-contexts: Stanford's FLASH [13] architecture places the network interface on the memory bus by replacing the standard memory controller with the MAGIC (Memory And General Interconnect Controller) chip. The MIT Alewife <ref> [1] </ref> also places the network interface on the memory bus but connects to a custom memory controller. Both preclude the use of commercial workstation boards.
Reference: 2. <author> Basu, A., Buch, V., Vogels, W., and von Eicken, T. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles (December 1995). </booktitle>
Reference-contexts: Illinois Fast Messages [16] is another I/O-bus resident system, in this case using SPARCstations and Myrinet. Their protocol uses a more traditional buffering approach. Their use of the I/O bus LaNai interface results in a 32 microsecond latency for small messages. U-Net <ref> [2] </ref> attempts to achieve many of the same goals as Avalanche, but using completely off-the-shelf hardware. They arrive at many of the same conclusions, such as the retention of system call interface for message transmission.
Reference: 3. <author> Blumrich, M., et al. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (April 1994), </booktitle> <pages> pp. 142-153. </pages>
Reference-contexts: Hewlett Packard's Afterburner [10] efforts have shown that the bottleneck for communication between machines is due to copy overhead. Sender Based protocols used in the Avalanche work are designed to eliminate any unnecessary copies of message data. Both the Princeton SHRIMP <ref> [11, 3] </ref> group and Digital Equipment Corporation with its Memory Channel [12] provide user-level messaging by mapping local memory pages to receiver memory pages.
Reference: 4. <author> Boden, N., et al. </author> <title> Myrinet A Gigabit-per-second Local-Area Network. </title> <booktitle> IEEE MICRO 15, </booktitle> <month> 1 (February </month> <year> 1995), </year> <pages> 29-36. </pages>
Reference-contexts: For HP7200 based systems, the clock speed is 120 MHz with a maximum sustainable bandwidth of 768 megabytes/second in 4-way systems. The results presented in this paper are based on the HP7200. The Myrinet-II <ref> [4] </ref> fabric runs at 160 MHz and uses bidirectional, byte-wide, data paths in each link to provide a source routed, wormhole, point to point interconnect fabric.
Reference: 5. <author> Bryg, W., Chan, K., and Fiduccia, N. </author> <title> A High-Performance, Low-Cost Multiprocessor Bus for Workstations and Midrange Servers. </title> <journal> Hewlett-Packard Journal 47, </journal> <month> 1 (February </month> <year> 1996), </year> <pages> 18-24. </pages>
Reference-contexts: Hewlett-Packard provides one to four processor workstation platforms based on their HP-7200 or PA-8000 microprocessors [7, 15]. While these processors vary significantly in their internal architecture, they both use the Runway <ref> [5] </ref> main memory bus. The Runway bus is a 64-bit, split transaction bus which supports cache coherence for SMP configurations. For HP7200 based systems, the clock speed is 120 MHz with a maximum sustainable bandwidth of 768 megabytes/second in 4-way systems.
Reference: 6. <editor> Buzzard, G., et al. </editor> <booktitle> An implementation of the Hamlyn sender-managed interface architecture. In Proceedings of the Second Symposium on Operating System Design and Implementation (October 1996). </booktitle>
Reference-contexts: The lower latency afforded by the Widget cache clearly enhances the scalability range of the program. 6 Related Work Many groups are currently researching efficient messaging for workstations. The Hamlyn <ref> [6] </ref> effort at Hewlett-Packard Labs is similar in many respects to the Avalanche approach. It utilizes a sender-based protocol, HP Runway-based workstations, and the Myrinet interconnect. Their interface uses an I/O controller LaNai rather than the system bus.
Reference: 7. <author> Chan, K., et al. </author> <title> Design of the HP PA 7200 CPU. </title> <journal> Hewlett-Packard Journal 47, </journal> <month> 1 (February </month> <year> 1996), </year> <pages> 25-33. </pages>
Reference-contexts: The Avalanche approach is to use Hewlett-Packard's symmetric multiprocessor (SMP) workstations and Myricom's Myrinet-II interconnect fabric coupled by a custom network interface called the Widget. Hewlett-Packard provides one to four processor workstation platforms based on their HP-7200 or PA-8000 microprocessors <ref> [7, 15] </ref>. While these processors vary significantly in their internal architecture, they both use the Runway [5] main memory bus. The Runway bus is a 64-bit, split transaction bus which supports cache coherence for SMP configurations.
Reference: 8. <author> Chandra, S., Larus, J., and Rogers, A. </author> <title> Where is Time Spent in Message-Passing and Shared-Memory Programs? In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems (Oct. </title> <booktitle> 1994), </booktitle> <pages> pp. 61-73. </pages>
Reference: 9. <editor> Culler, D. E., et al. </editor> <booktitle> Parallel Programming in Split-C. In Proceedings of Supercomputing '93 (Nov. </booktitle> <year> 1993), </year> <pages> pp. 262-273. </pages>
Reference-contexts: These delays are imposed by systems which only support synchronous mes-sage transactions. The extremely efficient polling mechanism and low-latency for small messages make DD an excellent target for languages such as Split-C <ref> [9] </ref>, and for message passing libraries such as Active Messages [21] and Thinking Machine Corporation's CMMD, that depend on such efficiency.
Reference: 10. <editor> Dalton, C., et al. Afterburner: </editor> <title> A Network-Independent Card Provides Architectural Support for High-Performance Protocols. </title> <journal> IEEE Network (July 1993), </journal> <pages> 36-43. </pages>
Reference-contexts: Positioning the network interface on an I/O bus and the use of a processor in the fabric inter-face limits their latency to approximately 32 microseconds for a one cell ATM message. Hewlett Packard's Afterburner <ref> [10] </ref> efforts have shown that the bottleneck for communication between machines is due to copy overhead. Sender Based protocols used in the Avalanche work are designed to eliminate any unnecessary copies of message data.
Reference: 11. <author> Dubnicki, C., Iftode, L., Felten, E., and Li, K. </author> <title> Software Support of Vir--tual Memory Mapped Communication. </title> <booktitle> In 10th International Parallel Processing Symposium (Apr. </booktitle> <year> 1996). </year>
Reference-contexts: Hewlett Packard's Afterburner [10] efforts have shown that the bottleneck for communication between machines is due to copy overhead. Sender Based protocols used in the Avalanche work are designed to eliminate any unnecessary copies of message data. Both the Princeton SHRIMP <ref> [11, 3] </ref> group and Digital Equipment Corporation with its Memory Channel [12] provide user-level messaging by mapping local memory pages to receiver memory pages.
Reference: 12. <author> Gillett, R., and Kaufmann, R. </author> <title> Experience Using the First-Generation Memory Channel for PCI Network. </title> <booktitle> In HOT Interconnects Symposium IV (Aug. </booktitle> <year> 1996). </year>
Reference-contexts: Sender Based protocols used in the Avalanche work are designed to eliminate any unnecessary copies of message data. Both the Princeton SHRIMP [11, 3] group and Digital Equipment Corporation with its Memory Channel <ref> [12] </ref> provide user-level messaging by mapping local memory pages to receiver memory pages. Message transmission becomes a series of simple stores to mapped pages, relying either on non-cacheable pages or the availability of a write-through mode in the cache to ensure that stores become visible to the network interface.
Reference: 13. <author> Heinrich, M., et al. </author> <title> The Performance Impact of Flexibility in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems (Oct. </booktitle> <year> 1994), </year> <pages> pp. 274-285. </pages>
Reference-contexts: Neither provides the DMA capability to avoid CPU cycle stealing to support bulk transfers. Neither has the capability of serving incoming data to cache, or achieves the latency and bus utilization benefits exhibited by the Avalanche Widget. Stanford's FLASH <ref> [13] </ref> architecture places the network interface on the memory bus by replacing the standard memory controller with the MAGIC (Memory And General Interconnect Controller) chip. The MIT Alewife [1] also places the network interface on the memory bus but connects to a custom memory controller.
Reference: 14. <author> Hewlett-Packard Co. </author> <title> PA-RISC 1.1 Architecture and Instruction Set Reference Manual, </title> <month> February </month> <year> 1994. </year>
Reference-contexts: The results were obtained from execution-driven simulations using the Paint simulator [17]. Paint simulates the HP PA RISC 1.1 <ref> [14] </ref> architecture and includes an instruction set interpreter and detailed cycle-level models of a first level cache, system bus, and memory system similar to those found in HP J-class systems.
Reference: 15. <author> Hunt, D. </author> <title> Advanced Performance Features of the 64-bit PA-8000. </title> <booktitle> In COMPCON '95 (1995), </booktitle> <pages> pp. 123-128. </pages>
Reference-contexts: The Avalanche approach is to use Hewlett-Packard's symmetric multiprocessor (SMP) workstations and Myricom's Myrinet-II interconnect fabric coupled by a custom network interface called the Widget. Hewlett-Packard provides one to four processor workstation platforms based on their HP-7200 or PA-8000 microprocessors <ref> [7, 15] </ref>. While these processors vary significantly in their internal architecture, they both use the Runway [5] main memory bus. The Runway bus is a 64-bit, split transaction bus which supports cache coherence for SMP configurations.
Reference: 16. <author> Paikin, S., Lauria, and Chien, A. </author> <title> High Performance Messaging on Workstations: Illinois Fast Messages (FM) for Myrinet. </title> <booktitle> In Proceedings of Supercomputing '88 (1995). </booktitle>
Reference-contexts: Their interface uses an I/O controller LaNai rather than the system bus. They report a one-way message latency of 12.7 microseconds with a large portion of the time spent in the interface processor and in high-latency bus translation hardware. Illinois Fast Messages <ref> [16] </ref> is another I/O-bus resident system, in this case using SPARCstations and Myrinet. Their protocol uses a more traditional buffering approach. Their use of the I/O bus LaNai interface results in a 32 microsecond latency for small messages.
Reference: 17. <author> Stoller, L., Kuramkote, R., and Swanson, M. </author> <title> PAINT- PA Instruction Set Interpreter. </title> <type> Tech. Rep. </type> <institution> UUCS-96-009, University of Utah Computer Science Department, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: The results were obtained from execution-driven simulations using the Paint simulator <ref> [17] </ref>. Paint simulates the HP PA RISC 1.1 [14] architecture and includes an instruction set interpreter and detailed cycle-level models of a first level cache, system bus, and memory system similar to those found in HP J-class systems.
Reference: 18. <author> Stoller, L., and Swanson, M. </author> <title> Direct Deposit: A Basic User-Level Protocol for Carpet Clusters. </title> <type> Tech. Rep. </type> <institution> UUCS-95-003, University of Utah Computer Science Department, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: For some communication patterns, standard techniques such as piggy-backed ACKs might be used. For others, buffer state is implicit in synchronization. For example, an RPC ACK implies that the request has been consumed. The Avalanche implementation of SBPs is called Direct Deposit <ref> [18] </ref> (DD); it provides system call-based connection establishment and message transmission. Much of the security and safety overhead is isolated to connection setup time and is provided by going through the kernel.
Reference: 19. <author> Swanson, M., and Stoller, L. </author> <title> Low Latency Workstation Cluster Communications Using Sender-Based Protocols Computer Science Department. </title> <type> Tech. Rep. </type> <institution> UUCS-96-001, University of Utah, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: Additional latency and overhead improvement is achieved by connecting the network interface directly to the memory bus rather than via an I/O adapter. 4.1 Sender-Based Protocols The key concept of sender-based protocols <ref> [19] </ref> (SBPs) is a connection based mechanism that enables the sender to manage a reserved receive buffer within the receiving process' address space that is obtained when the connection is established. The sender directs placement of messages within that buffer via an offset within each packet header.
Reference: 20. <author> Thekkath, A., and Levy, H. </author> <title> Limits to Low-Latency Communications on High-Speed Networks. </title> <journal> acm Transactions on Computer Systems 11, </journal> <month> 2 (May </month> <year> 1993), </year> <pages> 179-203. </pages> <note> 21. </note> <author> von Eicken, T., Culler, D. E., Goldstein, S. C., and Schauser, K. E. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation,. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture (May 1992), </booktitle> <pages> pp. 256-266. </pages>
Reference-contexts: The LaNai interface also precludes coherent fabric transactions and does not efficiently support the improved protocols described in this paper. The design of the fabric interface can have a dramatic effect on both latency and overhead <ref> [20] </ref>. The overhead incurred by the CPU interactions to send or receive a message are usually difficult or impossible to eliminate from the basic latency. The Avalanche choice has been to create a custom network interface capable of directly coupling the Myrinet to the Runway bus.
Reference: 22. <author> Wilkes, J. </author> <title> Hamlyn an interface for sender-based communication. </title> <type> Tech. Rep. </type> <institution> HPL-OSR-92-13, Hewlett-Packard Research Laboratory, </institution> <month> Nov. </month> <year> 1992. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Finally, this capability must be provided in a fashion that does not incur additional scheduling overhead elsewhere in the system. 4 Message Passing in Avalanche Message passing in Avalanche takes a complete system approach to minimizing latency and overhead, and reduces the importance of coordinated scheduling. Lightweight, sender-based protocols <ref> [22] </ref> are used to reduce the software components of latency and overhead, and to allow simple hardware acceleration of common operations. Operating system involvement is retained in connection establishment and in message transmission. This kernel mediation on the send side permits protected use of the fabric by multiple processes.
References-found: 21

