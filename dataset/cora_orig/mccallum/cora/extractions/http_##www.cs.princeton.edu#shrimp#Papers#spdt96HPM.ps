URL: http://www.cs.princeton.edu/shrimp/Papers/spdt96HPM.ps
Refering-URL: http://www.cs.princeton.edu/shrimp/html/papers_stack_13.html
Root-URL: http://www.cs.princeton.edu
Email: martonosi@ee.princeton.edu doug@cs.princeton.edu mesarina@cs.princeton.edu  
Title: The SHRIMP Performance Monitor: Design and Applications  
Author: Margaret Martonosi Douglas W. Clark Malena Mesarina 
Affiliation: Departments of Electrical Engineering and Computer Science Princeton University  
Abstract: Growing complexity in many current computers makes performance evaluation and characterization both increasingly difficult and increasingly important. For parallel systems, performance characterizations can be especially difficult to obtain, since the hardware is more complex, and the simulation time can be prohibitive. The challenge is to design a low-cost and yet flexible and powerful performance monitoring system to provide systems implementers and application programmers with detailed performance information. This paper describes a performance monitoring system for the SHRIMP multicomputer. The system's core is a hardware monitor with several novel features including multi-dimensional histograms, page tags, histogram categories, and a threshold interrupt mechanism. We also describe software applications that make use of these features. These applications range from fairly simple code-oriented or data-oriented performance tools, to more complicated on-the-fly use of the monitor to improve the performance of a shared virtual memory system. We have found that the concurrent development of the hardware and software portions of the system has led to a novel design that supports a wide range of hardware and software uses. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Bershad, D. Lee, T. H. Romer, and J. B. Chen. </author> <title> Avoiding Conflict Misses Dynamically in Large Direct-Mapped Caches. </title> <booktitle> In Proc. 6th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 158-170, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Some researchers have examined using monitoring information to guide operating system policy decisions. For example, Bershad et al. proposed a special-purpose hardware monitor (Cache Miss Lookaside Buffer) that would keep per-page statistics on memory behavior in order to guide operating system decisions about virtual-to-physical page mappings <ref> [1] </ref>. Chandra et al. investigated the potential of dynamically using measured data from a more general purpose hardware performance monitor to guide operating system scheduling and page migration decisions [6].
Reference: [2] <author> M. A. Blumrich, C. Dubnicki, E. Felten, K. Li, and M. Mesarina. </author> <title> Virtual Memory Mapped Network Interfaces. </title> <booktitle> IEEE MICRO, </booktitle> <pages> pages 21-28, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: We discuss related work in Section 5 and give conclusions in Section 6. 2 SHRIMP: An Overview The SHRIMP (Scalable High-performance Really Inexpensive Multi-Processor) project at Princeton studies how to provide high-performance communication mechanisms in order to integrate commodity desktop computers such as PCs and workstations into inexpensive, high-performance multicomputers <ref> [2, 3] </ref>. SHRIMP nodes are unmodified Pentium PC systems, each configured with disk and standard I/O devices such as tape drives, monitors, keyboards and LAN adaptors. The network is the Intel Paragon mesh routing backplane [26].
Reference: [3] <author> M. A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. W. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Mul-ticomputer. </title> <booktitle> In Proc. 21st Annual Int'l. Symp. on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Over the past decade, parallel computers have come into much more widespread use. In particular, a current focus of research in the parallel community has been on "convergence machines" [13], which are created by using high-performance networks and specially-designed network interfaces to interconnect commodity PCs or workstations <ref> [3, 16, 24] </ref>. In convergence machines, the design effort is often focused on supporting fast data This paper appeared in the 1996 ACM SIGMETRICS Symposium on Parallel and Distributed Tools. access and communication amongst an application's cooperating processes. <p> about how to adapt to observed behavior. (For example, a parallel application might migrate a page of data to the processor that references it most often, based on performance numbers available during execution.) This paper describes the design and uses of the hardware performance monitor for the Princeton SHRIMP multicomputer <ref> [3] </ref>. The main focus of the SHRIMP project has been on designing hardware and software support for low-cost, user-level communication mechanisms. Thus, we have designed hardware and software to monitor system behavior, with a particular emphasis on monitoring the inter-processor communication statistics. <p> We discuss related work in Section 5 and give conclusions in Section 6. 2 SHRIMP: An Overview The SHRIMP (Scalable High-performance Really Inexpensive Multi-Processor) project at Princeton studies how to provide high-performance communication mechanisms in order to integrate commodity desktop computers such as PCs and workstations into inexpensive, high-performance multicomputers <ref> [2, 3] </ref>. SHRIMP nodes are unmodified Pentium PC systems, each configured with disk and standard I/O devices such as tape drives, monitors, keyboards and LAN adaptors. The network is the Intel Paragon mesh routing backplane [26]. <p> The 4-bit tags are stored in SHRIMP's incoming page table entries on the network interface board; they are read out at the same time as the page-mapping information <ref> [3] </ref>. Updating page tags requires (protected) memory-mapped I/O writes of the appropriate locations in the page tables. Once calculated, all five quantities are fed into the address multiplexer, which generates the memory address of the histogram bin that is to be incremented. <p> Since the writes occur at user-level, both the runtime overhead and program perturbation incurred by them can be relatively low. Page tags are stored as part of an existing page-table in each receiving node's network interface <ref> [3] </ref>. In SHRIMP, all arriving packets require a page-table lookup anyway; for monitoring, the extra page tag bits are sent over from the network interface board to the monitor as each packet arrives. Like the category register, page tag bits are also set by software.
Reference: [4] <author> W. Brantley, K. McAuliffe, and T. Ngo. </author> <title> RP3 Performance Monitoring Hardware. </title> <editor> In Simmons, Koskela, and Bucher, editors, </editor> <booktitle> Instrumentation for Future Parallel Computing Systems, </booktitle> <pages> pages 35-43. </pages> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: To our knowledge, however, the TRAMS system's memory design is not intended to provide general multidimensional histograms as in the SHRIMP monitor. The performance monitoring system for Cedar used simple histograms [15], while IBM RP3 used a small set of hardware event counters <ref> [4] </ref>. The Intel Paragon includes rudimentary per-node counters [23], but cannot measure message latency. Histogram-based hardware monitors were also used to measure uniprocessor performance in the VAX models 11/780 and 8800 [7, 9].
Reference: [5] <author> R. Carpenter. </author> <title> Performance Measurement Instrumentation at NBS. </title> <editor> In M. Simmons, R. Koskela, and I. Bucher, editors, </editor> <booktitle> Instrumentation for Future Parallel Computing Systems, </booktitle> <pages> pages 159-184. </pages> <year> 1989. </year>
Reference-contexts: The earlier TRAMS system connects to the interconnect of an MIMD machine, and counts events filtered by pattern-matching hardware <ref> [5, 21] </ref>. To our knowledge, however, the TRAMS system's memory design is not intended to provide general multidimensional histograms as in the SHRIMP monitor. The performance monitoring system for Cedar used simple histograms [15], while IBM RP3 used a small set of hardware event counters [4].
Reference: [6] <author> R. Chandra, S. Devine, B. Verghese, A. Gupta, and M. Rosenblum. </author> <title> Scheduling and Page Migration for Multiprocessor Compute Servers. </title> <booktitle> In Proc. 6th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 12-24, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Chandra et al. investigated the potential of dynamically using measured data from a more general purpose hardware performance monitor to guide operating system scheduling and page migration decisions <ref> [6] </ref>. This approach is closer to ours, but they used an existing performance monitor [12] and focused their attention mainly on determining appropriate operating system policies. 6 Conclusions This paper has described the design of the hardware and software that make up SHRIMP's performance monitoring system.
Reference: [7] <author> D. Clark, P. J. Bannon, and J. B. Keller. </author> <title> Measuring VAX 8800 Performance with a Histogram Hardware Monitor. </title> <booktitle> In Proc. 15th Annual Symp. on Computer Architecture, </booktitle> <pages> pages 176-185, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: The Intel Paragon includes rudimentary per-node counters [23], but cannot measure message latency. Histogram-based hardware monitors were also used to measure uniprocessor performance in the VAX models 11/780 and 8800 <ref> [7, 9] </ref>. These monitors offered less flexible histogramming, and could not categorize statistics based on data regions or interrupt the processor based on a user-set threshold. On-chip performance monitors are becoming more common for CPU chips. For example, Intel's Pentium CPU incorporates extensive on-chip monitoring [18].
Reference: [8] <author> Digital Equipment Corporation. </author> <title> DECChip 21064 RISC Microprocessor Preliminary Data Sheet. </title> <type> Technical report, </type> <year> 1992. </year>
Reference-contexts: Here also, there is no support for categorization of statistics or for selective CPU notification. In contrast, the Alpha 21064 does provide some base level of performance monitoring with selective CPU notification <ref> [8] </ref>. Its on-chip cache performance counter is used by initializing it to a particular value, and then decrementing it whenever a cache miss occurs; when the counter value reaches zero, the CPU is interrupted. Some researchers have examined using monitoring information to guide operating system policy decisions.
Reference: [9] <author> J. Emer and D. Clark. </author> <title> A Characterization of Processor Performance in the VAX-11/780. </title> <booktitle> In Proc. 11th Annual Int'l. Symp. on Computer Architecture, </booktitle> <pages> pages 301-310, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: The Intel Paragon includes rudimentary per-node counters [23], but cannot measure message latency. Histogram-based hardware monitors were also used to measure uniprocessor performance in the VAX models 11/780 and 8800 <ref> [7, 9] </ref>. These monitors offered less flexible histogramming, and could not categorize statistics based on data regions or interrupt the processor based on a user-set threshold. On-chip performance monitors are becoming more common for CPU chips. For example, Intel's Pentium CPU incorporates extensive on-chip monitoring [18].
Reference: [10] <author> S. R. Goldschmidt. </author> <title> Simulation of Multiprocessors, Speed and Accuracy. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Since a 16-node machine and the performance monitor boards are not completed yet, we gathered this data using a behavioral simulator of the performance monitor, that is integrated into a Tango-Lite simulator <ref> [10] </ref> of a Shared Virtual Memory (SVM) system being built as part of the SHRIMP project [14]. The performance monitor simulator is a set of C routines that the SVM simulator calls, and that mimic the functionality of the hardware monitor. on SHRIMP's Shared Virtual Memory system.
Reference: [11] <author> S. L. Graham, P. B. Kessler, and M. K. McKu-sick. </author> <title> An Execution Profiler for Modular Programs. </title> <journal> Software- Practice and Experience, </journal> <volume> 13 </volume> <pages> 671-685, </pages> <month> Aug. </month> <year> 1983. </year>
Reference-contexts: Monitoring methods based entirely on software are valuable in some cases. Some performance information (such as instruction counts or explicit synchronization delays) may be both sufficiently coarse-grained and software-visible that it can be monitored simply by instrumenting software or by using techniques like program counter sampling <ref> [11] </ref>. Many current parallel machines, however, allow implicit communication that is hard to monitor without hardware support. In cache coherent, shared-address-space machines, much of the interprocessor communication occurs via the hardware cache-coherence mechanism. With communication potentially occurring on any read or write, monitoring must be very fine-grained.
Reference: [12] <author> M. A. Heinrich. </author> <title> DASH Performance Monitor Hardware Documentation. </title> <institution> Stanford University, </institution> <note> Unpublished Memo, </note> <year> 1993. </year>
Reference-contexts: It then updates its statistics memory appropriately, checks if any new EISA commands have been issued, and waits for the next packet. Like some previous performance monitors (e.g. <ref> [12] </ref>) we have a flexible hardware design based on FPGAs, but we have designed mechanisms into the monitor for runtime flexibility as well. The subsections below discuss some of the key features in more detail. 3.1 Histogram and Trace Modes The monitor has two modes: histogram mode and trace mode. <p> Clearly, the migration overhead must be balanced against its benefits; this trade-off is the subject of ongoing work. 5 Related Work This section discusses the relationship of SHRIMP's performance monitor to several previously developed projects. For example, the Stanford DASH multiprocessor [17] included a per-cluster histogram-based performance monitor <ref> [12] </ref>. In the DASH monitor, histogram-ming is fixed at the time the FPGA was compiled. The histograms allow statistics to be categorized into two user and two operating system categories, or by subsets of the data address bits. <p> Chandra et al. investigated the potential of dynamically using measured data from a more general purpose hardware performance monitor to guide operating system scheduling and page migration decisions [6]. This approach is closer to ours, but they used an existing performance monitor <ref> [12] </ref> and focused their attention mainly on determining appropriate operating system policies. 6 Conclusions This paper has described the design of the hardware and software that make up SHRIMP's performance monitoring system.
Reference: [13] <author> J. L. Hennessy. </author> <title> Distributed Shared Memory: Perspectives on its Development and its Future. </title> <booktitle> In First ARPA Conference on HPCC, </booktitle> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Over the past decade, parallel computers have come into much more widespread use. In particular, a current focus of research in the parallel community has been on "convergence machines" <ref> [13] </ref>, which are created by using high-performance networks and specially-designed network interfaces to interconnect commodity PCs or workstations [3, 16, 24].
Reference: [14] <author> L. Iftode et al. </author> <title> Improving Release-Consistent Shared Virtual Memory Using Automatic Update. </title> <booktitle> In 10th International Parallel Processing Symposium, </booktitle> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: Since a 16-node machine and the performance monitor boards are not completed yet, we gathered this data using a behavioral simulator of the performance monitor, that is integrated into a Tango-Lite simulator [10] of a Shared Virtual Memory (SVM) system being built as part of the SHRIMP project <ref> [14] </ref>. The performance monitor simulator is a set of C routines that the SVM simulator calls, and that mimic the functionality of the hardware monitor. on SHRIMP's Shared Virtual Memory system. <p> For example, in a shared virtual memory (SVM) system built on top of SHRIMP, we can use threshold interrupts and histogram mode to help support adaptive page migration. The SVM system uses SHRIMP's automatic update facility <ref> [14] </ref>, which allows for fine-grained communication between a pair of processors.
Reference: [15] <author> D. Kuck, E. Davidson, D. Lawrie, et al. </author> <title> The Cedar System and an Initial Performance Study. </title> <booktitle> In Proc. 20th Int'l Symp. on Computer Architecture, </booktitle> <pages> pages 213-223, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: To our knowledge, however, the TRAMS system's memory design is not intended to provide general multidimensional histograms as in the SHRIMP monitor. The performance monitoring system for Cedar used simple histograms <ref> [15] </ref>, while IBM RP3 used a small set of hardware event counters [4]. The Intel Paragon includes rudimentary per-node counters [23], but cannot measure message latency. Histogram-based hardware monitors were also used to measure uniprocessor performance in the VAX models 11/780 and 8800 [7, 9].
Reference: [16] <author> J. Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proc. 21st Int'l Symp. on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <address> Chicago, IL, </address> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Over the past decade, parallel computers have come into much more widespread use. In particular, a current focus of research in the parallel community has been on "convergence machines" [13], which are created by using high-performance networks and specially-designed network interfaces to interconnect commodity PCs or workstations <ref> [3, 16, 24] </ref>. In convergence machines, the design effort is often focused on supporting fast data This paper appeared in the 1996 ACM SIGMETRICS Symposium on Parallel and Distributed Tools. access and communication amongst an application's cooperating processes.
Reference: [17] <author> D. Lenoski, J. Laudon, et al. </author> <title> The DASH Prototype: Logic Overhead and Performance. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <pages> pages 41-61, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Clearly, the migration overhead must be balanced against its benefits; this trade-off is the subject of ongoing work. 5 Related Work This section discusses the relationship of SHRIMP's performance monitor to several previously developed projects. For example, the Stanford DASH multiprocessor <ref> [17] </ref> included a per-cluster histogram-based performance monitor [12]. In the DASH monitor, histogram-ming is fixed at the time the FPGA was compiled. The histograms allow statistics to be categorized into two user and two operating system categories, or by subsets of the data address bits.
Reference: [18] <author> T. Mathisen. </author> <title> Pentium secrets. </title> <journal> Byte, </journal> <pages> pages 191-192, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: These monitors offered less flexible histogramming, and could not categorize statistics based on data regions or interrupt the processor based on a user-set threshold. On-chip performance monitors are becoming more common for CPU chips. For example, Intel's Pentium CPU incorporates extensive on-chip monitoring <ref> [18] </ref>. The Pentium performance counters include information on the number of reads and writes, the number of read misses and write misses, pipeline stalls, TLB misses, etc. Here also, there is no support for categorization of statistics or for selective CPU notification.
Reference: [19] <author> A. Mink. </author> <title> Operating Principles of Multikron II Performance Instrumentation for MIMD Computers. </title> <type> Technical Report NISTIR 5571, </type> <institution> National Institute of Science and Technology, </institution> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: In contrast, our hardware monitor supports both such specific studies as well as more general monitoring. Performance monitoring work by Mink et al. shows some similarities in approach <ref> [19, 21, 22] </ref>. Their Mul-tikron and Multikron II hardware monitors are also intended to connect to an I/O bus to monitor activity.
Reference: [20] <author> A. Mink and R. Carpenter. </author> <title> A VLSI Chip Set for a Multiprocessor Performance Measurement System. </title> <editor> In M. Simmons, R. Koskela, and I. Bucher, editors, </editor> <booktitle> Parallel Computer Systems: Performance Instrumentation and Visualization. </booktitle> <year> 1990. </year>
Reference: [21] <author> A. Mink, R. Carpenter, G. Nacht, and J. Roberts. </author> <title> Multiprocessor Performance-Measurement Instrumentation. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-75, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: In contrast, our hardware monitor supports both such specific studies as well as more general monitoring. Performance monitoring work by Mink et al. shows some similarities in approach <ref> [19, 21, 22] </ref>. Their Mul-tikron and Multikron II hardware monitors are also intended to connect to an I/O bus to monitor activity. <p> The earlier TRAMS system connects to the interconnect of an MIMD machine, and counts events filtered by pattern-matching hardware <ref> [5, 21] </ref>. To our knowledge, however, the TRAMS system's memory design is not intended to provide general multidimensional histograms as in the SHRIMP monitor. The performance monitoring system for Cedar used simple histograms [15], while IBM RP3 used a small set of hardware event counters [4].
Reference: [22] <author> A. Mink and R. J. Carpenter. </author> <title> Operating Principles of Multikron Performance Instrumentation for MIMD Computers. </title> <type> Technical Report NISTIR 4737, </type> <institution> National Institute of Science and Technology, </institution> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: In contrast, our hardware monitor supports both such specific studies as well as more general monitoring. Performance monitoring work by Mink et al. shows some similarities in approach <ref> [19, 21, 22] </ref>. Their Mul-tikron and Multikron II hardware monitors are also intended to connect to an I/O bus to monitor activity.
Reference: [23] <author> J. Rattner. </author> <title> Paragon System. Presentation: </title> <booktitle> DARPA High Performance Software Conf., </booktitle> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: The performance monitoring system for Cedar used simple histograms [15], while IBM RP3 used a small set of hardware event counters [4]. The Intel Paragon includes rudimentary per-node counters <ref> [23] </ref>, but cannot measure message latency. Histogram-based hardware monitors were also used to measure uniprocessor performance in the VAX models 11/780 and 8800 [7, 9]. These monitors offered less flexible histogramming, and could not categorize statistics based on data regions or interrupt the processor based on a user-set threshold.
Reference: [24] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proc. 21st Annual Int'l. Symp. on Computer Architecture, </booktitle> <pages> pages 325-337, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Over the past decade, parallel computers have come into much more widespread use. In particular, a current focus of research in the parallel community has been on "convergence machines" [13], which are created by using high-performance networks and specially-designed network interfaces to interconnect commodity PCs or workstations <ref> [3, 16, 24] </ref>. In convergence machines, the design effort is often focused on supporting fast data This paper appeared in the 1996 ACM SIGMETRICS Symposium on Parallel and Distributed Tools. access and communication amongst an application's cooperating processes.
Reference: [25] <author> J. Roberts, J. Antonishek, and A. Mink. </author> <title> Hybrid Performance Measurement Instrumentation for Loosely-Coupled MIMD Architectures. </title> <booktitle> In Proc. Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> Mar. </month> <year> 1989. </year>
Reference: [26] <author> R. Traylor and D. Dunning. </author> <title> Routing Chip Set for Intel Paragon Parallel Supercomputer. </title> <booktitle> In Proc. Hot Chips '92 Symp., </booktitle> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: SHRIMP nodes are unmodified Pentium PC systems, each configured with disk and standard I/O devices such as tape drives, monitors, keyboards and LAN adaptors. The network is the Intel Paragon mesh routing backplane <ref> [26] </ref>. The connection between a network interface and the routing backplane is via a simple signal-conditioning card and a cable. system. The highlighted components in the figure correspond to the experimental system components being designed and implemented at Princeton.
Reference: [27] <author> S. Woo, M. Ohara, et al. </author> <title> Methodological Considerations and Characterization of the SPLASH-2 Parallel Application Suite. </title> <booktitle> In Proc. 22nd Int'l Symp. on Computer Architecture, </booktitle> <pages> pages 24-37, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: This section presents several such applications. 4.1 Using Joint Distributions to Understand Application Behavior Our first example motivates the use of multidimensional histograms to show application behavior on several axes at once. Figure 4 shows histogram output for FFT, a parallel benchmark from the SPLASH-2 suite <ref> [27] </ref> using 16 processors on 65,536 complex data points.
References-found: 27

