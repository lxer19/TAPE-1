URL: http://ebbets.poly.edu/PDC-lab/research_papers/Hummel-Schmidt-Wein-Uma-heterog-spaa.ps
Refering-URL: http://ebbets.poly.edu/PDC-lab/wein.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: jps@pucs4.poly.edu ruma@photon.poly.edu -wein@mem.poly.edu.  
Title: Load-Sharing in Heterogeneous Systems via Weighted Factoring Regular Paper  
Author: Susan Flynn Hummel Jeanette Schmidt R. N. Uma Joel Wein 
Note: 11201. Research supported by ARPA/USAF under Grant no. F30602-95-1-0008 and the New York State Science and Technology Foundation through its Center for Advanced Technology in Telecommunications. Joel Wein was supported in part by NSF Grant CCR-9211494, and Jeanette Schmidt in part by NSF grant CCR-9305873. hummel@mono.poly.edu (Contact Author)  
Affiliation: Department of Computer Science, Polytechnic University, Brook-lyn, NY,  
Abstract: We consider the problem of scheduling a parallel loop with independent iterations on a network of heterogeneous workstations, and demonstrate the effectiveness of a variant of factoring, a scheduling policy originating in the context of shared address-space homogeneous multiprocessors. In the new scheme, weighted factoring, processors are dynamically assigned decreasing size chunks of iterations in proportion to their processing speeds. Through experiments on a network of SUN Sparc workstations we show that weighted factoring significantly outperforms variants of a work-stealing load-balancing algorithm and on certain applications dramatically outperforms factoring as well. We then study weighted work assignment analytically, giving upper and lower bounds on its performance under the assumption that the processor iteration execution times can be modeled as weighted random variables. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. Banicescu and S. Flynn Hummel, </author> <title> Balancing Processor Loads and Exploiting Data Locality in N-body Simulations, </title> <booktitle> to appear in Proc. of ACM Supercomputing '95, </booktitle> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Another important aspect of achieving high performance on parallel machines is exploiting locality, for example by tiling the iteration space so as to maximize data reuse. Recently, on homogeneous multiprocessors, factoring has been combined with tiling techniques to simultaneously balance processor loads and maximize data reuse <ref> [9, 1] </ref>. Weighted factoring can similarly be combined with tiling (although this is not investigated in the work presented here). The rest of this paper is organized as follows. In Section 2, we introduce the algorithms and discuss relevant prior work. <p> single scheduler processor assign the factoring chunk sizes might be a bottleneck, causing the relative performance of the work stealing algorithms to improve due to their distributed nature; we note, however, that in experiments on up to 64 processors on several homogeneous multiprocessors this proved not to be a problem <ref> [1, 8, 9] </ref>. Nonetheless, we would like to explore factoring schemes that use a hierarchy of scheduler processors or local rules to assign chunk sizes. Finally, we believe that this scheme will adapt well to the setting in which processors enter and leave the pool over time.
Reference: [2] <author> R. D. Blumofe and C. E. Leiserson, </author> <title> Scheduling Mul-tithreaded Computations by Work Stealing, </title> <booktitle> Proc. of Ann. Symp. on Foundations of Computer Science pp. </booktitle> <pages> 356-368, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: The current batch chunk size is dynamically adjusted using information about the execution times of previous batches. 2.2 Load Balancing in Distributed Systems A common scheduling scheme for distributed systems is work-stealing <ref> [3, 11, 23, 15, 19, 2] </ref> wherein work is dynamically migrated from heavily loaded processors to lightly loaded ones. We compare decreasing-chunk schemes to the work stealing algorithm given in [23].
Reference: [3] <author> F. W. Burton and M. R. Sleep, </author> <title> Executing Functional Programs on a Virtual Tree of Processors, </title> <booktitle> Proc. Conf. on Functional Programming Languages and Computer Architecture pp. </booktitle> <pages> 187-194, </pages> <month> Oct. </month> <year> 1981. </year>
Reference-contexts: The current batch chunk size is dynamically adjusted using information about the execution times of previous batches. 2.2 Load Balancing in Distributed Systems A common scheduling scheme for distributed systems is work-stealing <ref> [3, 11, 23, 15, 19, 2] </ref> wherein work is dynamically migrated from heavily loaded processors to lightly loaded ones. We compare decreasing-chunk schemes to the work stealing algorithm given in [23].
Reference: [4] <author> T. Casavant and J. Kuh, </author> <title> A Taxonomy of Scheduling in General-Purpose Distributed Computing Systems, </title> <journal> IEEE Trans. Software Eng. SE-14(2) pp. </journal> <pages> 141-154, </pages> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: It is impossible to survey all of this work in a short paper. We restrict our discussion here to those that have the most relevance to our work, and refer the reader to the surveys <ref> [26, 4, 6, 14, 25] </ref> for a more thorough treatment. 2.1 Loop Scheduling on Multiprocessors Given N iterations of a nested parallel loop and P processors, we wish to schedule the iterations of the loop on the processors, so as to minimize the overall finish time.
Reference: [5] <author> H. A. David, </author> <title> Order Statistics 2nd ed., </title> <publisher> John Wiley & Sons, </publisher> <year> 1981. </year>
Reference-contexts: The execution time of P (parallel) chunks of K iterations that all start at the same time was modeled as the Pth order statistic of P S j 's, which is the maximum of P r.v.s with the same distribution as S j , <ref> [5] </ref>. When a processor finishes its assigned chunk, it receives a new chunk of work. Factoring assigns decreasing chunk sizes to assure that sufficient work is left over to smooth over the variability in execution time.
Reference: [6] <author> D.L. Eager E.D. Lazowska and J. Zahorjan, </author> <title> Adaptive Load Sharing in Homogeneous Distributed Systems, </title> <journal> IEEE Trans. Software Eng. SE-12(5) pp. </journal> <pages> 662-675, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: It is impossible to survey all of this work in a short paper. We restrict our discussion here to those that have the most relevance to our work, and refer the reader to the surveys <ref> [26, 4, 6, 14, 25] </ref> for a more thorough treatment. 2.1 Loop Scheduling on Multiprocessors Given N iterations of a nested parallel loop and P processors, we wish to schedule the iterations of the loop on the processors, so as to minimize the overall finish time.
Reference: [7] <author> L. E. Flynn and S. Flynn Hummel, </author> <title> The Mathematical Foundations of the Factoring Scheduling Method, </title> <institution> IBM Research Report RC18462, </institution> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: In this paper, we discuss the effectiveness of factoring, a decreasing-size chunking parallel loop scheduling technique developed by Flynn and Flynn Hummel <ref> [7] </ref>, as a load balancing technique for heterogeneous systems. Factoring has been shown to be a robust and highly effective technique on homogeneous shared address-space multiprocessors [8]. In addition, it is an extremely simple scheduling rule, incurring very little overhead. <p> To avoid the problem of having to determine the optimal chunk size, and to promote better load balance, several schemes have been introduced in which the chunk size decreases over the course of the algorithm; for example, Guided Self-Scheduling (GSS) [22] and factoring <ref> [7] </ref>. These schemes, described below, allocate large chunks of iterations at the beginning of the computation to reduce scheduling overhead, while dynamically assigning small chunks towards the end to achieve good load balancing. <p> If the initial iterations of a loop are much more time-consuming than the latter iterations, however, GSS may allocate too much work in the early chunks, and the remaining work may not be sufficient to guarantee a good load balance. Like GSS, factoring <ref> [7] </ref> dynamically assigns large chunks before small ones to smooth variances among processors in order to achieve a close-to-optimal finishing time. <p> time of each batch) for the selection of optimal factoring batch sizes, a good rule of thumb is that early batches should contain a total of one-half of the remaining work; experiments have shown that this produces very even finishing times for a variety of applications on a homogeneous multiprocessor <ref> [7] </ref>. An explanation for the effectiveness of this simple rule is that half of the remaining work is enough to smooth over the unevenness of a batch when its chunk finishing times yield a Truncated Normal (bell-shaped) distribution. <p> Unfortunately we can only prove this for P 3 processors, but conjecture it to be true in general. The attractiveness of upper and lower bounding the performance in the heterogeneous system with that of two homogeneous systems, is that for homogeneous systems tight analytical performance bounds have been established, <ref> [16, 7] </ref>. 5.2 Analytical Results The factoring analysis [7] assumed that all processors were equally powerful. The execution times of the parallel-loop iterations were modeled as identical independent random variables (i.i.r.v.s). <p> The attractiveness of upper and lower bounding the performance in the heterogeneous system with that of two homogeneous systems, is that for homogeneous systems tight analytical performance bounds have been established, [16, 7]. 5.2 Analytical Results The factoring analysis <ref> [7] </ref> assumed that all processors were equally powerful. The execution times of the parallel-loop iterations were modeled as identical independent random variables (i.i.r.v.s). That is the execution time of iteration i on processor j was assumed to be a r.v. j i . <p> When a processor finishes its assigned chunk, it receives a new chunk of work. Factoring assigns decreasing chunk sizes to assure that sufficient work is left over to smooth over the variability in execution time. The analysis of factoring in <ref> [7] </ref> made use of a distribution-free bound on the expected value of the Pth order statistic of i.i.r.v.s with mean and variance : + p The term p P=2 bounds the average idle time of a processor after the first iteration due to load imbalance. <p> The Pth order statistics was shown to be a useful and critical estimator for future iterations as well. In particular, it was shown analytically in <ref> [7] </ref> that if a chunk size of K 0 is used in the current iteration, then a chunk size of about K 0 =2 should be used in the next iteration to ensure that reasonable load balance is achieved. This analysis has been validated [8, 7] by experiments. <p> This analysis has been validated <ref> [8, 7] </ref> by experiments. These experiments also show that starting with a (total) chunk size of P K (if the total work consists of 2P K chunks) is a very effective heuristic.
Reference: [8] <author> S. Flynn Hummel, E. Schonberg, and L. E. Flynn, </author> <title> Factoring: A Practical and Robust Method for Scheduling Parallel Loops, </title> <journal> Comm. of the ACM 35(8) pp. </journal> <pages> 90-101, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: In this paper, we discuss the effectiveness of factoring, a decreasing-size chunking parallel loop scheduling technique developed by Flynn and Flynn Hummel [7], as a load balancing technique for heterogeneous systems. Factoring has been shown to be a robust and highly effective technique on homogeneous shared address-space multiprocessors <ref> [8] </ref>. In addition, it is an extremely simple scheduling rule, incurring very little overhead. <p> In addition, factoring balances the load better than GSS when iteration execution times vary widely and unpredictably, and the overhead of factoring is not significantly greater than that of GSS <ref> [8] </ref>. The factoring algorithm schedules independent iterations in batches. In each dynamically created batch, only a fixed portion of the remaining iterations is divided equally into P chunks, where P is the number of processors. <p> This analysis has been validated <ref> [8, 7] </ref> by experiments. These experiments also show that starting with a (total) chunk size of P K (if the total work consists of 2P K chunks) is a very effective heuristic. <p> single scheduler processor assign the factoring chunk sizes might be a bottleneck, causing the relative performance of the work stealing algorithms to improve due to their distributed nature; we note, however, that in experiments on up to 64 processors on several homogeneous multiprocessors this proved not to be a problem <ref> [1, 8, 9] </ref>. Nonetheless, we would like to explore factoring schemes that use a hierarchy of scheduler processors or local rules to assign chunk sizes. Finally, we believe that this scheme will adapt well to the setting in which processors enter and leave the pool over time.
Reference: [9] <author> S. Flynn Hummel, I. Banicescu, C. Wang, and J. Wein, </author> <title> Load Balancing and Data Locality via Fractiling: an Experimental Study, </title> <booktitle> Proc. Third Workshop on Languages, Compilers, and Run-Time Systems for Scalable Computers, </booktitle> <pages> pp. 85-89, </pages> <editor> Boleslaw K. Szymanski and Balaram Sinharoy (Editors), </editor> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1995. </year>
Reference-contexts: Another important aspect of achieving high performance on parallel machines is exploiting locality, for example by tiling the iteration space so as to maximize data reuse. Recently, on homogeneous multiprocessors, factoring has been combined with tiling techniques to simultaneously balance processor loads and maximize data reuse <ref> [9, 1] </ref>. Weighted factoring can similarly be combined with tiling (although this is not investigated in the work presented here). The rest of this paper is organized as follows. In Section 2, we introduce the algorithms and discuss relevant prior work. <p> On twenty-four processors, however, its performance degrades significantly. Upon close examination of the reason for this degradation, we see that usually one processor lags far behind the rest. We attribute this to systemic variance: modern workstations are complex, and OS and network interference can introduce significant variance <ref> [9] </ref>. As the number of processors grows we are more likely to see this, since the probability of a "significant interference event" grows as well. We note that this result, which we find somewhat surprising, occurred repeatedly in the majority of ten runs on twenty-four processors. <p> single scheduler processor assign the factoring chunk sizes might be a bottleneck, causing the relative performance of the work stealing algorithms to improve due to their distributed nature; we note, however, that in experiments on up to 64 processors on several homogeneous multiprocessors this proved not to be a problem <ref> [1, 8, 9] </ref>. Nonetheless, we would like to explore factoring schemes that use a hierarchy of scheduler processors or local rules to assign chunk sizes. Finally, we believe that this scheme will adapt well to the setting in which processors enter and leave the pool over time.
Reference: [10] <author> E. J. Gumbel. </author> <title> The Maxima of the Mean of the Largest Value of the Range. </title> <journal> Ann. Math. Statist. </journal> <pages> 25 pp. 76-84, </pages> <year> 1954. </year>
Reference: [11] <author> R. H. Halstead Jr., </author> <title> Multilisp: A Language for Concurrent Symbolic Computations, </title> <journal> ACM Trans. Prog. Lang. and Systems 7(4) pp. </journal> <pages> 501-538, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: The current batch chunk size is dynamically adjusted using information about the execution times of previous batches. 2.2 Load Balancing in Distributed Systems A common scheduling scheme for distributed systems is work-stealing <ref> [3, 11, 23, 15, 19, 2] </ref> wherein work is dynamically migrated from heavily loaded processors to lightly loaded ones. We compare decreasing-chunk schemes to the work stealing algorithm given in [23].
Reference: [12] <author> H. O. Hartley and H. A. David. </author> <title> Universal Bounds for Mean Range and Extrema Observations. </title> <journal> Ann. Math. Statist. </journal> <pages> 25 pp. 85-99, </pages> <year> 1954. </year>
Reference: [13] <author> J. L. </author> <title> Hennessy Architectural Convergence and Its Implications, </title> <booktitle> Developing a Computer Science Agenda for High-Performance Computing, </booktitle> <pages> pp. 7-8, </pages> <editor> ed. U. Vishkin, </editor> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference: [14] <author> C. Jacqmot E. Milgram W. Joosen and Y. Berbers, </author> <title> UNIX and Load Balancing: a Survey, </title> <booktitle> Proc. UEEG Spring 1989 Conf., </booktitle> <year> 1989. </year>
Reference-contexts: It is impossible to survey all of this work in a short paper. We restrict our discussion here to those that have the most relevance to our work, and refer the reader to the surveys <ref> [26, 4, 6, 14, 25] </ref> for a more thorough treatment. 2.1 Loop Scheduling on Multiprocessors Given N iterations of a nested parallel loop and P processors, we wish to schedule the iterations of the loop on the processors, so as to minimize the overall finish time.
Reference: [15] <author> R. M. Karp and Y. Zhang, </author> <title> Randomized Parallel Algorithms for Backtrack Search and Branch-and-Bound Computations, </title> <journal> Journal of the ACM 40(3) pp. </journal> <pages> 765-789, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The current batch chunk size is dynamically adjusted using information about the execution times of previous batches. 2.2 Load Balancing in Distributed Systems A common scheduling scheme for distributed systems is work-stealing <ref> [3, 11, 23, 15, 19, 2] </ref> wherein work is dynamically migrated from heavily loaded processors to lightly loaded ones. We compare decreasing-chunk schemes to the work stealing algorithm given in [23].
Reference: [16] <author> C. Kruskal and A. Weiss, </author> <title> Allocating Independent Subtasks on Parallel Processors, </title> <journal> IEEE Trans. Software Eng. SE-11(10) pp. </journal> <pages> 1001-1016, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: Self-scheduling achieves almost perfect load balancing because all processors finish within one iteration of each other, but has very high overhead. In a compromise between these two approaches, Kruskal and Weiss introduce a scheme called fixed-size chunking <ref> [16] </ref>, which reduces the overhead by having a processor take a chunk of K iterations at a time instead of the one iteration of self-scheduling. The fixed-size chunking algorithm has a worse load balance than self-scheduling, but significantly less overhead. <p> All numbers are the average of ten runs. The last column represents the improvement of weighted factoring over the best weighted work-stealing algorithm. 5 Analytical Evaluation 5.1 Introduction In this section we study weighted factoring from an analytical perspective. Kruskal & Weiss <ref> [16] </ref>, in analyzing static chunking, made the assumption that the parallel loop iterations were identical independent random variables, and, based on this assumption, derived a formula for the optimal chunk size in terms of the mean and variance of the iteration execution times. <p> Unfortunately we can only prove this for P 3 processors, but conjecture it to be true in general. The attractiveness of upper and lower bounding the performance in the heterogeneous system with that of two homogeneous systems, is that for homogeneous systems tight analytical performance bounds have been established, <ref> [16, 7] </ref>. 5.2 Analytical Results The factoring analysis [7] assumed that all processors were equally powerful. The execution times of the parallel-loop iterations were modeled as identical independent random variables (i.i.r.v.s). <p> An important ingredient of the analysis are the distribution-free upper bound K + p KP=2 for the Pth order statistics of identically distributed random variables with mean K and vari ance K 2 , as well as the bound K+ p (valid when K &gt;> log P ), <ref> [16] </ref> for the Pth order statistics of independent normally distributed r.v.'s with mean K and variance K 2 . As discussed below, with weighted factoring the iteration and chunk execution times are more accurately modeled as nonidentical r.v.s. <p> r.v.'s S i 1 ; : : : S i P with distribu tion F i (x), (and hence expectation K and variance K 2 w i For large K the distributions F i are approximately (trun cated) Normal, so that we can use the following bound es tablished in <ref> [16] </ref> E [maxfS i i Z 1 1 (F i (x)) dx w i 2K log P (1 + o (1)) (5) Substituting back in (4) we get that E [maxfS 1 ; : : : S P g] 1 P P i=1 E [maxfS i 1 ; : : :
Reference: [17] <author> S. Lucco, </author> <title> A Dynamic Scheduling Method for Irreg--ular Parallel Programs, </title> <booktitle> Proc. of Conf. on Programming Language Design and Implementation pp. </booktitle> <pages> 200-210, </pages> <booktitle> ACM Sigplan, </booktitle> <year> 1992. </year>
Reference-contexts: They suggest the use of factoring for loops with independent iterations, and present the results of one experiment in a mildly heterogeneous setting (one slow processor and three fast ones). In <ref> [17] </ref>, a scheme related to factoring is proposed for the dual problem of load balancing iterations with irregular execution times on a homogeneous system.
Reference: [18] <author> MPI Forum, </author> <title> Document for a standard message passing interface, </title> <type> Tech Rep. </type> <institution> CS-93-214, University of Ten-nessee, </institution> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: We implemented distributed versions of the applications with the various load balancing algorithms in C using the Message Passing Interface (MPI) <ref> [18] </ref>. MPI is a widely adopted message passing library whose routines can be called from Fortran, C or C ++ programs. In our application, message passing was used to assign or reassign work to processors. We conducted our experiments on a network of Sun Sparc workstations connected by an Ethernet.
Reference: [19] <author> R. S. Nikhil, </author> <title> A Parallel Shared-Memory C for Distributed-Memory Machines, </title> <booktitle> Proc. Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: The current batch chunk size is dynamically adjusted using information about the execution times of previous batches. 2.2 Load Balancing in Distributed Systems A common scheduling scheme for distributed systems is work-stealing <ref> [3, 11, 23, 15, 19, 2] </ref> wherein work is dynamically migrated from heavily loaded processors to lightly loaded ones. We compare decreasing-chunk schemes to the work stealing algorithm given in [23].
Reference: [20] <author> H. Nishikawa and P. Steenkiste, </author> <title> A General Architecture for Load Balancing in a Distributed-Memory Environment, </title> <booktitle> Proc. 13th Int. Conf. on Distributed Computing, </booktitle> <pages> pp. 47-54, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Nonetheless, we experimentally obtained estimates of the w i and show that these are quite effective in practice. We know of little work that investigates decreasing chunking schemes on heterogeneous systems. One notable reference is the work of <ref> [20] </ref> who studied the simultaneous load balancing of tasks of different execution characteristics on a network of workstations. They proposed the use of a hierarchy of several load balancing algorithms, with different algorithms being used simultaneously for different applications.
Reference: [21] <author> S. J. Olszansky, J. M. Lebak and A. W. Bojanczyk, </author> <title> Parallel Algorithms for Space-Time Adaptive processing, </title> <booktitle> Proc. Int. Parallel Processing Symp. </booktitle> <address> pp.77-83, </address> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Problems of this sort have huge computational requirements, and recently are receiving attention from the perspective of parallel computing <ref> [21] </ref>. The main body of our radar algorithm consists of nested loops that iterate over all sensors and all angle/dopler pairs of the returned signals to generate a visualization of the imaged area.
Reference: [22] <author> C. Polychronopoulos and D. Kuck, </author> <title> Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Computers. </title> <journal> IEEE Trans. on Computers C-36(12) pp. </journal> <pages> 1425-1439, </pages> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: To avoid the problem of having to determine the optimal chunk size, and to promote better load balance, several schemes have been introduced in which the chunk size decreases over the course of the algorithm; for example, Guided Self-Scheduling (GSS) <ref> [22] </ref> and factoring [7]. These schemes, described below, allocate large chunks of iterations at the beginning of the computation to reduce scheduling overhead, while dynamically assigning small chunks towards the end to achieve good load balancing.
Reference: [23] <author> L. Rudolph, M. Slivkin-Allalouf and E. Upfal, </author> <title> A Simple Load Balancing Scheme for Task Allocation in Parallel Machines, </title> <booktitle> Proc. of Sym. on Parallel Algorithms and Architectures pp. </booktitle> <pages> 237-245, </pages> <year> 1991. </year>
Reference-contexts: The current batch chunk size is dynamically adjusted using information about the execution times of previous batches. 2.2 Load Balancing in Distributed Systems A common scheduling scheme for distributed systems is work-stealing <ref> [3, 11, 23, 15, 19, 2] </ref> wherein work is dynamically migrated from heavily loaded processors to lightly loaded ones. We compare decreasing-chunk schemes to the work stealing algorithm given in [23]. <p> We compare decreasing-chunk schemes to the work stealing algorithm given in <ref> [23] </ref>. Although their algorithm was proposed in the context of shared address-space multiprocessors, as there is no single master processor, and no processor needs to have global knowledge of the system load, it is a good candidate for a distributed system.
Reference: [24] <author> Sen, </author> <year> 1970, </year> <editor> in H. A. David, </editor> <title> Order Statistics 2nd ed., </title> <publisher> John Wiley & Sons, </publisher> <year> 1981. </year>
Reference: [25] <author> N. G. Shivaratri, P. Krueger and M. Singhal, </author> <title> Load Distributing for Locally Distributed Systems, </title> <journal> IEEE Computer pp. </journal> <pages> 33-45, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: It is impossible to survey all of this work in a short paper. We restrict our discussion here to those that have the most relevance to our work, and refer the reader to the surveys <ref> [26, 4, 6, 14, 25] </ref> for a more thorough treatment. 2.1 Loop Scheduling on Multiprocessors Given N iterations of a nested parallel loop and P processors, we wish to schedule the iterations of the loop on the processors, so as to minimize the overall finish time.

References-found: 25

