URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-96-1299/CS-TR-96-1299.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-96-1299/
Root-URL: http://www.cs.wisc.edu
Title: Hash Join Processing on Shared Memory Multiprocessors  
Author: Ambuj Shatdal Jeffrey F. Naughton 
Date: February, 1996  
Affiliation: Computer Sciences Department University of Wisconsin-Madison Computer Sciences  
Pubnum: Technical Report 1299  
Abstract: While most scalable database systems are designed for a shared nothing architecture, advances in hardware technology suggest that in the near future shared memory multiprocessors (SMPs) will be capable of handling all but the largest applications. This raises important questions about how scalable database systems should be architected; in particular, do SMPs require specially targeted algorithms, or will algorithms developed for shared nothing hardware suffice? As a first step towards answering the question, with an implementation on an SGI Challenge multiprocessor, we investigate the performance of hash join algorithms on SMPs. We first show that the shared nothing approach, if ported to an SMP by using a transparent message passing library, yields poor performance. However, we show that by using an optimized message passing library and modifying the join code so that it does not reuse message buffers, shared nothing algorithms become viable on an SMP. Next we study the performance of a commonly proposed simple parallel join algorithm designed for shared memory. We show that it performs only marginally better than the optimized shared nothing approach. Finally, we show that further performance improvements are possible over the simple shared memory algorithm by employing optimizations designed to maximize processor cache hits and minimize memory coherence traffic.
Abstract-found: 1
Intro-found: 1
Reference: [AB86] <author> J. Archibald and J.-L. Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <year> 1986. </year>
Reference-contexts: Coherence implies that the data values are guaranteed to be consistent under some coherence semantics, the most common being the sequential semantics i.e. a read returns the value of the most recent preceding write. There are several cache coherence algorithms mentioned in the literature <ref> [AB86] </ref>. For SMPs, which have a shared bus, a common algorithm is the snooping bus write invalidate. In this algorithm, all cache controllers listen on the bus and when a write occurs on a cached address, the cached value is invalidated.
Reference: [BFG + 95] <author> Chaitanya Baru, Gilles Fecteau, Ambuj Goyal, Hui i Hsiao, Anant Jhingran, Sriram Pad-manabhan, and Walter Wilson. </author> <title> An Overview of DB2 Parallel Edition. </title> <booktitle> In Proc. of the 1995 ACM-SIGMOD Conference, </booktitle> <address> San Jose, CA, </address> <year> 1995. </year>
Reference-contexts: Finally, even without the technical 1 arguments for SMPs, the reality is that today most parallel machines sold are SMPs. Today, however, most scalable parallel database systems are designed for the shared nothing hardware paradigm. This includes Informix XPS [Ger95], IBM DB2/PE <ref> [BFG + 95] </ref>, Sybase Navigation Server [Syb], Tandem [Tan87], and Teradata [Ter83].
Reference: [CLR94] <author> Satish Chandra, James R. Larus, and Anne Rogers. </author> <title> Where is time spent in message-passing and shared-memory programs? In Proc. </title> <booktitle> of the 6th ASPLOS Conference, </booktitle> <pages> pages 61-75, </pages> <month> Octo-ber </month> <year> 1994. </year>
Reference-contexts: An access to a shared variable is even more likely to result in a cache miss because of the maintenance of cache coherence. Consequently, cost of data sharing can not be ignored while designing efficient query processing algorithms for the SMPs. <ref> [CLR94] </ref> shows that for some scientific applications, optimized shared memory and message passing programs perform comparably on comparable hardware. The remainder of the paper is organized as follows. In Section 2 we review the basic SMP architecture. Section 3 describes the experimental platform.
Reference: [DGS + 90] <author> D. DeWitt, S. Ghandeharizadeh, D. Schneider, A. Bricker, H.-I Hsiao, and R. Rasmussen. </author> <title> The Gamma database machine project. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1), </volume> <month> March </month> <year> 1990. </year> <month> 16 </month>
Reference-contexts: This is generally much faster than speeds achievable in a traditional MPP interconnection network. Since the hardware is fairly fast, the software cost of message passing becomes important and we find the same in our experiments detailed below. A shared nothing hash join algorithm, as in Gamma <ref> [DGS + 90] </ref>, first repartitions the relations by hashing the tuples on the join attribute, each hash value being assigned a specific node. Each node then joins the partitions of the relations locally as if it were the only node in the system.
Reference: [Ger95] <author> Bob Gerber. </author> <title> Informix Online XPS. </title> <booktitle> In Proc. of the 1995 ACM-SIGMOD Conference, </booktitle> <address> San Jose, CA, </address> <year> 1995. </year>
Reference-contexts: Finally, even without the technical 1 arguments for SMPs, the reality is that today most parallel machines sold are SMPs. Today, however, most scalable parallel database systems are designed for the shared nothing hardware paradigm. This includes Informix XPS <ref> [Ger95] </ref>, IBM DB2/PE [BFG + 95], Sybase Navigation Server [Syb], Tandem [Tan87], and Teradata [Ter83].
Reference: [HS91] <author> W. Hong and M. Stonebraker. </author> <title> Optimization of Parallel Query Execution Plans in XPRS. </title> <booktitle> In Proceedings of the 1st Int'l Conf. on Parallel and Distributed Information Systems, </booktitle> <address> Miami, Florida, </address> <month> December </month> <year> 1991. </year>
Reference-contexts: The access conflicts to the hash table are taken care of by latching. Thereafter, all processors read their partition of the relation S tuples and probe them in the shared hash table. This approach is explicitly used in [LTS90] and is implicit in the XPRS database system <ref> [HS91] </ref>. The Tuple-based Shared Memory algorithm, detailed below, builds the hash table on the tuples themselves. Under the constant time memory access (PRAM) model this algorithm is the most efficient as it does the least amount of "work" i.e. it has least number of memory accesses (and instructions).
Reference: [LTS90] <author> Hongjun Lu, Kian-Lee Tan, and Ming-Chien Shan. </author> <title> Hash-Based Join Algorithms for Multiprocessor Computers with Shared Memory. </title> <booktitle> In Proceedings of the 16th VLDB Conference, </booktitle> <pages> pages 198-209, </pages> <address> Brisbane, Australia, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: Then we studied the performance of the traditional shared memory hash join algorithm which extends from the uniprocessor hash join where all nodes build a shared hash table and then each node probes its tuples in the shared hash table <ref> [LTS90] </ref>. The traditional algorithm 2 and its optimized version performed only marginally better than the shared nothing algorithm with optimized message passing. Next we attempted to optimize the shared memory algorithm in terms of better memory contention and locality by using repartitioning in shared memory, instead of using messages. <p> Finally we exploited the fact that the repartitioning algorithm can be cache optimized as the join processing after repartitioning is local to a processor [SKN94]. The cache optimized algorithms performed 9-90% faster than the traditional shared memory algorithm. In related work, <ref> [LTS90] </ref> did an analytical performance of algorithms in a shared everything environment. However, they considered different basic algorithms comparing hybrid hash, hash loop, etc. The analytical model did not truly reflect an SMP environment. <p> The access conflicts to the hash table are taken care of by latching. Thereafter, all processors read their partition of the relation S tuples and probe them in the shared hash table. This approach is explicitly used in <ref> [LTS90] </ref> and is implicit in the XPRS database system [HS91]. The Tuple-based Shared Memory algorithm, detailed below, builds the hash table on the tuples themselves.
Reference: [Mes94] <author> Message Passing Interface Forum. </author> <title> MPI: A Message Passing Interface Standard. </title> <journal> Int'l Journal of Supercomputing Applications, </journal> <volume> 8(3/4), </volume> <year> 1994. </year>
Reference-contexts: The question as mentioned earlier is: how will the algorithm perform if the SMP provides a message passing library just like a shared nothing architecture? Since no standard implementation of message passing, like MPI <ref> [Mes94] </ref>, was available on the machine, we implemented two different message passing libraries to evaluate the performance of the shared nothing algorithm.
Reference: [SKN94] <author> Ambuj Shatdal, Chander Kant, and Jeffrey F. Naughton. </author> <title> Cache Conscious Algorithms for Relational Query Processing. </title> <booktitle> In Proc. of 20th Int'l Conference on Very Large Data Bases, </booktitle> <pages> pages 510-521, </pages> <address> Santiago, Chile, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: However, these algorithms performed only comparably to the traditional simple approach. Finally we exploited the fact that the repartitioning algorithm can be cache optimized as the join processing after repartitioning is local to a processor <ref> [SKN94] </ref>. The cache optimized algorithms performed 9-90% faster than the traditional shared memory algorithm. In related work, [LTS90] did an analytical performance of algorithms in a shared everything environment. However, they considered different basic algorithms comparing hybrid hash, hash loop, etc. <p> However, they considered different basic algorithms comparing hybrid hash, hash loop, etc. The analytical model did not truly reflect an SMP environment. Furthermore, the considered data domain was too restricted which resulted in misleading conclusions. <ref> [SKN94] </ref> shows that the cache misses on a uniprocessor machine are expensive as the data has to be fetched from (slow) main memory and therefore the algorithms must be designed taking the cache into account. <p> Partitioning the data further into small cache-size partitions is likely to mitigate this shortcoming, as now the entire partitioned hash table is likely to fit in the cache while being accessed <ref> [SKN94] </ref>. Thus, the hash table is likely to be entirely cache resident while being accessed.
Reference: [SN93] <author> Ambuj Shatdal and Jeffrey F. Naughton. </author> <title> Using Shared Virtual Memory for Parallel Join Processing. </title> <booktitle> In Proc. of the 1993 ACM-SIGMOD Conference, </booktitle> <pages> pages 119-128, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: One argument against shared nothing algorithm has been their poor performance when the data is skewed [WDJ91]. However, techniques that have proven effective for shared nothing algorithms, e.g. <ref> [SN93] </ref>, would trivially apply to SMPs. It would be interesting to compare the two approaches for skew handling. Also, cluster of SMPs seems to be gaining popularity for building large scalable database systems, instead of the traditional shared nothing hardware.
Reference: [Syb] <institution> Sybase Inc. </institution> <note> Sybase Navigation Server . URL: http://www.sybase.com/Offerings/Servers/navserver.html. </note>
Reference-contexts: Finally, even without the technical 1 arguments for SMPs, the reality is that today most parallel machines sold are SMPs. Today, however, most scalable parallel database systems are designed for the shared nothing hardware paradigm. This includes Informix XPS [Ger95], IBM DB2/PE [BFG + 95], Sybase Navigation Server <ref> [Syb] </ref>, Tandem [Tan87], and Teradata [Ter83].
Reference: [Tan87] <author> Tandem Database Group. </author> <title> Nonstop SQL, A Distributed, High-Performance, High-Reliability Implementation of SQL. </title> <booktitle> In Workshop on High Performance Transaction Systems, Asilomar, </booktitle> <address> CA, </address> <year> 1987. </year>
Reference-contexts: Today, however, most scalable parallel database systems are designed for the shared nothing hardware paradigm. This includes Informix XPS [Ger95], IBM DB2/PE [BFG + 95], Sybase Navigation Server [Syb], Tandem <ref> [Tan87] </ref>, and Teradata [Ter83]. While one can certainly run a shared nothing system on an SMP (using the shared memory as a fast communication network), SMP's offer alternatives to algorithm design not present in shared nothing machines which include load balancing and easier management (as there are fewer independent nodes).
Reference: [Ter83] <institution> Teradata Corp. Teradata: DBC/1012 Database Computer Concept and Facilities, </institution> <year> 1983. </year>
Reference-contexts: Today, however, most scalable parallel database systems are designed for the shared nothing hardware paradigm. This includes Informix XPS [Ger95], IBM DB2/PE [BFG + 95], Sybase Navigation Server [Syb], Tandem [Tan87], and Teradata <ref> [Ter83] </ref>. While one can certainly run a shared nothing system on an SMP (using the shared memory as a fast communication network), SMP's offer alternatives to algorithm design not present in shared nothing machines which include load balancing and easier management (as there are fewer independent nodes).
Reference: [WDJ91] <author> C. B. Walton, A. G. Dale, and R. M. Jenevein. </author> <title> A taxonomy and performance model of data skew effects in parallel joins. </title> <booktitle> In Proceedings of the 17th VLDB Conference, </booktitle> <pages> pages 537-548, </pages> <address> Barcelona, Spain, </address> <month> September </month> <year> 1991. </year> <month> 17 </month>
Reference-contexts: The performance of the shared memory algorithms can be enhanced by optimizing the algorithms by making them aware of the cache and the SMP architecture. One argument against shared nothing algorithm has been their poor performance when the data is skewed <ref> [WDJ91] </ref>. However, techniques that have proven effective for shared nothing algorithms, e.g. [SN93], would trivially apply to SMPs. It would be interesting to compare the two approaches for skew handling.
References-found: 14

