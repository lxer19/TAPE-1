URL: ftp://ftp.cs.cmu.edu/project/chimera/voyles.gbp_p1.ps.gz
Refering-URL: http://www.cs.cmu.edu/~kem/ri/unused_pages/aml-pubs.html
Root-URL: 
Title: A MULTI-AGENT APPROACH  
Author: RICHARD M. VOYLES PRADEEP K. KHOSLA 
Affiliation: Robotics Ph.D. Program Carnegie Mellon University  Electrical and Computer Engineering Carnegie Mellon University  
Note: 299 GESTURE-BASED PROGRAMMING, PART 1:  
Abstract: Gesture-Based Programming is a paradigm for the evolutionary programming of rapidly deployable manipulation systems by human demonstration. The goal is to provide a more natural environment for the user and to generate more complete and successful programs by focusing on task experts rather than programming experts. What makes it unique from other programming by human demonstration approaches are the same things that make it evolutionary: a compos-able knowledge base of expertise agents and a facility for supervised practice after initial training. Prior knowledge of previously acquired skills (sensorimotor expertise) facilitates the interpretation of gestures during training and then provides closed-loop control of execution during run-time. This paper, part one of two, presents a high-level description of the system as well as descriptions of capabilities weve demonstrated on a PUMA robot and Utah/MIT hand. The companion paper provides a detailed account of one method of acquiring, matching, and even transforming sensorimotor expertise.
Abstract-found: 1
Intro-found: 1
Reference: <author> Brooks, R.A., </author> <year> (1986). </year> <title> A Robust Layered Control System for a Mobile Robot, </title> <journal> IEEE Journal of Robot ics and Automation, v.RA-2, </journal> <volume> n.1, </volume> <month> March, </month> <pages> pp. 14-23. </pages> <address> Gertz, M.W, </address> <year> (1994). </year> <title> A Visual Programming Environment for Real-Time Control Systems, </title> <type> Ph.D. Thesis, </type> <institution> Department of Electrical and Computer Engineering, Carnegie Mellon University. </institution>
Reference-contexts: In Intelligent Engineering Systems Through Artificial Neural Net works, Volume 6; Smart Engineering Systems: Neural Networks, Fuzzy Logic and Evolutionary Programming, ASME Press, 1996 300 Numerous attempts have been made to ease the discomfort of robot programming with varying degrees of success. Behavior-based and multi-agent systems <ref> (Brooks, 1986) </ref> seek to modularize software for easier programming by programming experts. Visual programming environments like Chimera/Onika (Stewart et al, 1993)(Gertz, 1994) and commercial packages such as MatrixX/SystemBuild and LabView capitalize on modular, reconfigurable software blocks by iconifying them within software assembly environments.
Reference: <author> Hartley, J., </author> <year> (1983). </year> <title> Robots at Work, </title> <publisher> IFS Publications, </publisher> <address> Ltd.,Kempston, Bedford, UK, chapters 5, </address> <month> 8. </month>
Reference-contexts: As a result, robotic technologies have had minimal impact on most manufacturing domains (in terms of the number or robots versus number of humans employed), particularly as product life cycles and batch sizes decrease. Ironically, these are the conditions under which robots were originally touted as being most effective <ref> (Hartley, 1983) </ref>. Once implemented, many robotic applications fail to meet expectations or prove insufficiently robust for the desired level of autonomy. A primary reason for this is that programming difficulties maintain a layer of insulation between the systems use and its development.
Reference: <author> Kang, S.B., and K. Ikeuchi, </author> <year> (1996). </year> <title> Toward Automatic Robot Instruction from Perception -- Temporal Segmentation of Tasks from Human Hand Motion, </title> <journal> IEEE Trans. on Robotics and Auto., </journal> <month> Mar. </month>
Reference-contexts: Visual programming does little for the bulk of robotic applications that involve substantive contact with the environment, uncertainty, and complex sensing. A more recent approach to human/robot interaction is the field of learning by observation <ref> (Kang and Ikeuchi, 1996) </ref>(Kuniyoshi et al, 1989). By forcing the robot to observe a human interacting with the world, rather than forcing the human to interact with a textual representation of the robot interacting with the world, a more natural, anthropocentric programming environment results. <p> PROJECT STATUS Learning by Observation GBP is a direct extension of the real-time control work by Voyles for the functional demonstration of Kang and Ikeuchis Learning by Observation system <ref> (Kang and 303 Ikeuchi, 1996) </ref>. The human is instrumented with a sensorized glove but only kinematic information is reported. As the human demonstrates simple tasks, hand and wrist trajectories are recorded and a multi-baseline stereo machine looks for changes in the pose of detected objects.
Reference: <author> Kuniyoshi, T., M. Inaba, and H. Inoue, </author> <year> (1989). </year> <title> Teaching by Showing: Generating Robot Programs by Visual Observation of Human Performance, </title> <booktitle> Proc. of 20th Intnl. Symp. on Ind. Robots, </booktitle> <pages> pp. 119-126. </pages>
Reference: <author> Patrick, J., </author> <year> (1992). </year> <title> Training: Research and Practice, </title> <publisher> Academic Press, </publisher> <address> San Diego, CA. </address>
Reference-contexts: Humans are more effective at 301 teaching by demonstration and practice and, although there are no guarantees here, either, it has been proven empirically to be the most successful technique between humans <ref> (Patrick, 1992) </ref>. But teaching by demonstration is not guaranteed to work and one of the reasons is it assumes a basic set of a priori capabilities. Imagine trying to teach calculus to someone who doesnt know algebra.
Reference: <author> Stewart, D.B., R.A. Volpe, and P.K. Khosla, </author> <year> (1993). </year> <title> Design of Dynamically Reconfigurable Real Time Software Using Port-Based Objects, </title> <institution> CMU Robotics Institute tech. rpt., CMU-RI-TR-93-11. </institution>
Reference-contexts: Behavior-based and multi-agent systems (Brooks, 1986) seek to modularize software for easier programming by programming experts. Visual programming environments like Chimera/Onika <ref> (Stewart et al, 1993) </ref>(Gertz, 1994) and commercial packages such as MatrixX/SystemBuild and LabView capitalize on modular, reconfigurable software blocks by iconifying them within software assembly environments. These visual environments allow programming at a much higher level, hiding many details of the particular implementation, and lessening the burden of programming expertise.
Reference: <author> Todd, D.J., </author> <year> (1986). </year> <title> Fundamentals of Robot Technology, </title> <publisher> John Wiley and Sons, </publisher> <address> chapters 3, </address> <month> 7. </month>
Reference-contexts: Two of the most common industrial applications are spray painting and spot welding. Both of these use natural programming methods such as lead-through teaching <ref> (Todd, 1986) </ref> (i.e. human demonstration) and point-teaching, respectively, that are easily mastered by semi-skilled workers. Lead-through teaching, in particular, allows anyone who can perform a valid task to program the robot to perform the task by demonstrating it.
Reference: <author> Voyles, R.M. and P.K. Khosla, </author> <year> (1995a). </year> <title> Tactile Gestures for Human/Robot Interaction, </title> <booktitle> Proc. of IEEE/RSJ Conf. on Intelligent Robots and Systems, </booktitle> <address> Pittsburgh, PA, </address> <publisher> v. </publisher> <pages> 3, pp. 7-13. </pages>
Reference-contexts: These experiences demonstrated the need for a skill-based approach to programming by human demonstration. They also suggested reconfigurable agents for the implementation of those skills. Gesture Interpretation Networks An example gesture-based interaction system is shown in Figure 3 and described more fully in <ref> (Voyles and Khosla, 1995a) </ref>. This is not a programming environment but an application scenario utilizing tactile gestures. The robot executes one of four polygonal cyclic trajectories: rectangle, triangle, cross, or pick-and-place. Each trajectory has parameterized height, width, and thickness.
Reference: <author> Voyles, R.M. and P.K. Khosla, </author> <year> (1995b). </year> <title> Multi-Agent Gesture Interpretation for Robotic Cable Har nessing, </title> <booktitle> Proc. of IEEE Conf. on Systems, Man, and Cybernetics, Vancouver, B.C., </booktitle> <pages> pp. </pages> <month> 1113-1118. </month> <title> Height Width Thick Triangle RectCross Trackball F/T Sensor Trackball Recognizer Force/Torque Recognizer svar svar Robot Carttraj OspposOsslow Velos Operational Space Controller Agent Inputs Outputs Pick Confusion DataGlove DataGlove Recognizer Execution Agents Pause Contact Gesture Recognition Agents Gesture Interpretation Agents </title>
Reference-contexts: This is an implicit parametrization. In contrast, the gestural characters are explicitly parametrized by either a single integer or oat-ing-point argument. An example can be found in <ref> (Voyles and Khosla, 1995b) </ref>. Gesture recognition is handled by individual gesture recognition agents for each gestural mode. Each recognition agent is made up of two subordinate agents, one general, one domain-specific. The general agent is the gesture agent in Figure 2. <p> The agents individually build a confidence value in their respective hypotheses and vote within agent pools, each pool being winner-take-all. Some of these agents were also applied to a cable harnessing task as described in <ref> (Voyles and Khosla, 1995b) </ref>. CONCLUSIONS We have presented our concept for a gesture-based programming system as well as brief portrayals of work we have completed on individual components of that system.
References-found: 9

