URL: http://www.cs.colostate.edu/~anderson/pubs/thorpems.ps.gz
Refering-URL: http://www.cs.colostate.edu/~anderson/
Root-URL: 
Title: Vehicle Traffic Light Control Using SARSA  
Author: Thomas L. Thorpe 
Date: April 2, 1997  
Abstract: SARSA (Sutton, 1996) is applied to a simulated, traffic-light control problem (Thorpe, 1997) and its performance is compared with several, fixed control strategies. The performance of SARSA with four different representations of the current state of traffic is analyzed using two reinforcement schemes. Training on one intersection is compared to, and is as effective as training on all intersections in the environment. SARSA is shown to be better than fixed-duration light timing and four-way stops for minimizing total traffic travel time, individual vehicle travel times, and vehicle wait times. Comparisons of performance using a constant reinforcement function versus a variable reinforcement function dependent on the number of vehicles at an intersection showed that the variable reinforcement resulted in slightly improved performance for some cases. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, Charles W. </author> <title> (1988) Strategy Learning with Multilayer Connectionist Representations, </title> <institution> GTE Labs TR 87-509.3, </institution> <note> Waltham, </note> <author> MA Crites, Robert H., Barto Andrew G. </author> <title> (1996) Improving elevator performance using reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 8. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press., </publisher> <pages> pp. 1017-1023. </pages>
Reference-contexts: The results are summarized in Section 5, and Section 6 presents conclusions. 3 2. Reinforcement Learning Reinforcement learning is a form of learning typically used for controlling processes. It has been applied to balancing an inverted pole <ref> (Anderson, 1988) </ref>, optimizing elevator performance (Crites and Barto, 1996), determining the actions required to rock an under-powered vehicle out of a valley (Sutton, 1996), playing backgammon (Tesauro, 1995) and many others. The TD-Gammon backgammon program has been so successful that experts are learning new strategies from it.
Reference: <author> Garnaas, Steve, </author> <title> "120th is giving commuters the green light", </title> <address> Denver Post, </address> <month> May 10, </month> <year> 1996, </year> <title> p. 1B and "Caught in Traffic, Timing the thing for Colorado's signal supervisor", </title> <editor> p. 2B. Ho, Fu-Sheng & Ioannou, </editor> <title> Petros (1996) Traffic flow modeling and control using artificial neural networks. </title> <journal> IEEE Control Systems, </journal> <month> Oct </month> <year> 1996, </year> <pages> 16(5) 16-26. </pages>
Reference-contexts: 1. Introduction A variety of traffic control strategies are being studied in real traffic networks and in simulation. The Denver Regional Council of Governments works with the Colorado Department of Transportation and citizens to identify and modify problem intersections <ref> (Garnaas, 1996) </ref>. Computers are used to monitor the traffic flows for critical intersections throughout the Denver region. The computers have the capability to change traffic light timing remotely but are only used to collect data for traffic analysis.
Reference: <author> Oglesby, Clarkson H., </author> <booktitle> (1975) Highway Engineering, 3rd Edition, </booktitle> <address> New York, </address> <publisher> John Wiley and Sons, </publisher> <pages> pp. 332-338. </pages>
Reference-contexts: For most of the tests performed where the SARSA representations performed better than the best fixed-duration strategy, the performance advantage was not that great. The constant intersection spacing of 440 feet may have helped the fixed-duration strategy. It has been stated <ref> (Oglesby 1975) </ref> that efficient synchronized traffic flow between two directions of traffic using fixed-duration timing is only possible if the spacing between intersections is approximately constant. Constant intersection spacing does not occur often in real life.
Reference: <author> Olsson, G. </author> <title> (1996) Fewer traffic jams thanks to computers. </title> <note> At http://www.stockholm.se/bm/projects/- vagtrafiken.html. </note>
Reference-contexts: Recently a major traffic artery was re-timed from 90 seconds in the heavy traffic flow direction to 100 seconds. This resulted in an 87% reduction in times stopped at lights. Stockholm, Sweden, uses remote television cameras to monitor high traffic flow areas <ref> (Olsson, 1996) </ref>. The traffic conditions are directly observed and speed limits and traffic light timing can be slowly adjusted remotely. Vehicles can also be rerouted remotely to reduce congestion.
Reference: <author> Rumelhart, D. E., Hinton, G. E., </author> & <title> Williams (1986), Learning internal representations by error back propogation. </title> <editor> In Rumelhart, D. E., Hinton, & R.Mclelland J. L. </editor> <booktitle> (1986), Parallel Distributed Processing: explorations in the microstructure of cognition, </booktitle> <address> Cambridge, MA: </address> <publisher> Bradford Books, </publisher> <pages> pp. 318-362. </pages>
Reference-contexts: The TD-Gammon backgammon program has been so successful that experts are learning new strategies from it. Reinforcement learning techniques differ from supervised learning, such as error back-propagation in neural networks <ref> (Rumelhart, 1986) </ref>, because neural networks require a teacher to provide answers or desired output values for a set of inputs. The errors or differences between the output of the learning agent and desired values are used to modify the network weights.
Reference: <author> Singh, S.P. and Sutton, </author> <title> R.S. (1996) Reinforcement learning with replacing eligibility traces. </title> <booktitle> Machine Learning </booktitle> 22(1/2/3):123-158. 
Reference-contexts: This allowed the simulations to complete in about 3 weeks time. 1 Acceleration and deceleration were timed for a Honda Civic and a Toyota 4Runner. 10 4. Traffic Light Control with SARSA The SARSA (Sutton, 1996) algorithm was applied to the traffic light control problem using replace traces <ref> (Singh and Sutton, 1996) </ref> and greedy action selection. If the greedy action is not unique, one is chosen randomly. For most representations, the traffic controller is trained using experience at a single intersection with the four lanes of traffic leading into it.
Reference: <author> Sutton, Richard S. </author> <title> (1988) Learning to Predict by the Methods of Temporal Differences, </title> <booktitle> Machine Learning 3(1) </booktitle> <pages> 9-44, </pages> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers Sutton, </publisher> <editor> R.S. </editor> <title> (1996) Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press., </publisher> <pages> pp. 1038-1044. </pages> <editor> Tesauro, </editor> <title> Gerald (1995) Temporal Difference Learning and TD-Gammon, </title> <journal> Communications of the ACM, </journal> <month> March </month> <year> 1995, </year> 38(3):58-68. 
Reference-contexts: Reinforcement learning procedures have been shown to converge for absorbing Markov processes, meaning that the learning agent can see the state of the environment, the next state is only dependent on the current state, and that all sequences eventually terminate <ref> (Sutton, 1988) </ref>. - 0 0 0 0 0 0 0 0 -1 -1 -1 0 -1 -1 -1 -1 -3 -2 -1 0 -5 -4 -3 -2 Initial Values First Update Final Values The mechanics of reinforcement learning can be illustrated with a simple grid problem as shown in the upper
Reference: <author> Thorpe, T. </author> <title> (1997) A physically-realistic simulation of vehicle traffic flow. </title> <type> Technical Report 97-104, </type> <institution> 41 Department of Computer Science, Colorado State University. </institution>
Reference-contexts: The TDerr calculated for the current state-action pair transition is used for all of the updates along the trace for the current step. This helps speed up the learning process considerably. 8 3. Traffic Simulator and Control Strategies The traffic simulator, as described by <ref> (Thorpe, 1997) </ref>, uses one-second, discrete time steps for the traffic-light controller and the simulation of vehicle movement.
References-found: 8

