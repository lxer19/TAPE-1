URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-94-09.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/tr-online/?number+94-09
Root-URL: 
Email: E-mail: [juffi,gerhard]@ai.univie.ac.at  
Phone: 2  
Title: Incremental Reduced Error Pruning  
Author: Johannes Furnkranz and Gerhard Widmer ; 
Keyword: Inductive Logic Programming, Pruning, Noise  
Note: OEFAI-TR-94-09  
Address: Schottengasse 3 A-1010 Vienna Austria  Vienna  
Affiliation: 1 Austrian Research Institute for Artificial Intelligence  Department of Medical Cybernetics and Artificial Intelligence University of  
Abstract: This paper outlines some problems that may occur with Reduced Error Pruning in Inductive Logic Programming, most notably efficiency. Thereafter a new method, Incremental Reduced Error Pruning, is proposed that attempts to address all of these problems. Experiments show that in many noisy domains this method is much more efficient than alternative algorithms, along with a slight gain in accuracy. However, the experiments show as well that the use of this algorithm cannot be recommended for domains with a very specific concept description. 
Abstract-found: 1
Intro-found: 1
Reference: [Ali and Pazzani, 1993] <author> Kamal M. Ali and Michael J. Pazzani. HYDRA: </author> <title> A noise-tolerant relational concept learning algorithm. </title> <booktitle> In Proceedings of the Thirteenth Joint International Conference on Artificial Intelligence, </booktitle> <pages> pages 1064-1071, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: The Votes (VI) set is the Votes data set with the most significant attribute removed. In all of the propositional domains the equality relation was added as background knowledge. The Promoters data also included two background relations specifying that the 4 DNA bases can be split into 2 groups <ref> [Ali and Pazzani, 1993] </ref>, and in the KRKN data the &lt; relation was added for the 6 integer valued attributes.
Reference: [Bratko and Kononenko, 1986] <author> Ivan Bratko and Igor Kononenko. </author> <title> Learning diagnostic rules from incomplete and noisy data. </title> <editor> In B. Phelps, editor, </editor> <booktitle> Interactions in AI and Statistical Methods, </booktitle> <pages> pages 142-153, </pages> <address> London, </address> <year> 1986. </year>
Reference-contexts: Not surprisingly, noise handling methods have also entered the emerging field of Inductive Logic Programming (ILP) [Muggleton, 1992]. Linus [Lavrac and Dzeroski, 1992] relies directly on the noise handling abilities of decision tree learning algorithms like CN2 [Clark and Niblett, 1989, Clark and Boswell, 1991] or ASSISTANT <ref> [Bratko and Kononenko, 1986] </ref>. Others, like mFoil [Dzeroski and Bratko, 1992], have adapted some of these well-known methods from attribute-value learning for the ILP framework. Pruning is a standard way of dealing with noise in decision tree learning (see e.g. [Mingers, 1989] or [Esposito et al., 1993]).
Reference: [Breiman et al., 1984] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks, </publisher> <address> Pacific Grove, CA, </address> <year> 1984. </year>
Reference-contexts: 1 Introduction Being able to deal with noisy data is a must for algorithms that are meant to learn concepts in real-world domains. Significant effort has gone into investigating the effect of noisy data on decision tree learning algorithms (see e.g. <ref> [Quinlan, 1993, Breiman et al., 1984] </ref>). Not surprisingly, noise handling methods have also entered the emerging field of Inductive Logic Programming (ILP) [Muggleton, 1992]. <p> Post-Pruning means that first a concept description is generated that perfectly explains all training instances. This theory will subsequently be generalized by cutting off branches of the decision tree (as in [Quinlan, 1987] or <ref> [Breiman et al., 1984] </ref>). In ILP, pre-pruning has been common in the form of stopping criteria as used in Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992], or Fossil [Furnkranz, 1994a].
Reference: [Brunk and Pazzani, 1991] <author> Clifford A. Brunk and Michael J. Pazzani. </author> <title> An investigation of noise-tolerant relational concept learning algorithms. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 389-393, </pages> <address> Evanston, Illinois, </address> <year> 1991. </year>
Reference-contexts: In ILP, pre-pruning has been common in the form of stopping criteria as used in Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992], or Fossil [Furnkranz, 1994a]. Post-pruning was introduced to ILP with Reduced Error Pruning (REP) <ref> [Brunk and Pazzani, 1991] </ref> based on ideas by [Quinlan, 1987] and [Pagallo and Haussler, 1990]. First the training set is split into two subsets: a growing set and a pruning set . <p> In section 4 we propose Incremental Reduced Error Pruning | a method that integrates pre- and post-pruning | as an alternative solution. Section 5 then reports some experiments with two versions of this algorithm. 1 2 Some Problems with Reduced Error Pruning Reduced Error Pruning (REP) <ref> [Brunk and Pazzani, 1991] </ref> has proven to be quite effective in raising predictive accuracy in noisy domains. <p> Both algorithms, REP and Grow, were implemented as described in [Cohen, 1993] with the exception that delete-last-literal was used as a clause pruning operator (as in <ref> [Brunk and Pazzani, 1991] </ref>) instead of Cohen's operator that deletes a final sequence of literals from a clause. 2 All systems were imple mented in Sicstus PROLOG, run-times were measured on a SUN SPARCstation IPX. 2 Cohen used his delete-final-sequence operator in both REP and Grow, while we have used delete-last-literal <p> The system builds upon ideas introduced by <ref> [Brunk and Pazzani, 1991] </ref> and [Cohen, 1993], but improves upon them in the following ways: Efficiency: [Cohen, 1993] has shown that the complexity of REP is (n 4 ) on random data and has proposed an alternative algorithm | Grow | with time complexity (n 2 log n).
Reference: [Cestnik et al., 1987] <author> Bojan Cestnik, Igor Kononenko, and Ivan Bratko. </author> <title> ASSISTANT 86: A knowledge-elicitation tool for sophisticated users. </title> <editor> In Ivan Bratko and Nada Lavrac, editors, </editor> <booktitle> Progress in Machine Learning, </booktitle> <pages> pages 31-45, </pages> <address> Wilmslow, England, 1987. </address> <publisher> Sigma Press. </publisher>
Reference-contexts: Others, like mFoil [Dzeroski and Bratko, 1992], have adapted some of these well-known methods from attribute-value learning for the ILP framework. Pruning is a standard way of dealing with noise in decision tree learning (see e.g. [Mingers, 1989] or [Esposito et al., 1993]). There are two fundamentally different approaches <ref> [Cestnik et al., 1987] </ref>: Pre-Pruning means that during concept generation some training examples are deliberately ignored, so that the final concept description does not classify all training instances correctly. Post-Pruning means that first a concept description is generated that perfectly explains all training instances.
Reference: [Clark and Boswell, 1991] <author> Peter Clark and Robin Boswell. </author> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proceedings of the 5th European Working Session of Learning, </booktitle> <pages> pages 151-163, </pages> <address> Porto, Portugal, </address> <year> 1991. </year>
Reference-contexts: Not surprisingly, noise handling methods have also entered the emerging field of Inductive Logic Programming (ILP) [Muggleton, 1992]. Linus [Lavrac and Dzeroski, 1992] relies directly on the noise handling abilities of decision tree learning algorithms like CN2 <ref> [Clark and Niblett, 1989, Clark and Boswell, 1991] </ref> or ASSISTANT [Bratko and Kononenko, 1986]. Others, like mFoil [Dzeroski and Bratko, 1992], have adapted some of these well-known methods from attribute-value learning for the ILP framework.
Reference: [Clark and Niblett, 1989] <author> Peter Clark and Tim Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 261-283, </pages> <year> 1989. </year>
Reference-contexts: Not surprisingly, noise handling methods have also entered the emerging field of Inductive Logic Programming (ILP) [Muggleton, 1992]. Linus [Lavrac and Dzeroski, 1992] relies directly on the noise handling abilities of decision tree learning algorithms like CN2 <ref> [Clark and Niblett, 1989, Clark and Boswell, 1991] </ref> or ASSISTANT [Bratko and Kononenko, 1986]. Others, like mFoil [Dzeroski and Bratko, 1992], have adapted some of these well-known methods from attribute-value learning for the ILP framework.
Reference: [Cohen, 1993] <author> William W. Cohen. </author> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 988-994, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: However, this approach has several disadvantages, which we will highlight in section 2. Section 3 briefly presents the approach of <ref> [Cohen, 1993] </ref> designed to solve some of these problems. In section 4 we propose Incremental Reduced Error Pruning | a method that integrates pre- and post-pruning | as an alternative solution. <p> However, this method has several shortcomings, which we will discuss in this section. 2.1 Efficiency In <ref> [Cohen, 1993] </ref> it was shown that the worst-case time complexity of REP is as bad as (n 4 ) on random data (n is the number of examples). The growing of the initial concept, on the other hand, is only (n 2 log n). <p> The growing of the initial concept, on the other hand, is only (n 2 log n). The derivation of these numbers as given in <ref> [Cohen, 1993] </ref> rests on the assumption that for random and thus incompressible data the concept description after the growing phase will contain about 1 rule for each example (n rules altogether), each of them having about log n conditions, because each literal will cover about half of the random instances. <p> Therefore we get a total cost of (n 4 ). A detailed proof can be found in <ref> [Cohen, 1993] </ref>. It has also been pointed out there that this result for random data generalizes to data containing noise, i.e. a constant fraction of random and incompressible data. <p> This method succeeded in improving both run-time and accuracy of REP. 3 Cohen's Grow algorithm In <ref> [Cohen, 1993] </ref> several of the problems of section 2 | in particular efficiency | have been recognized. Cohen has then proposed a pruning algorithm based on the technique used in the Grove learning system [Pagallo and Haussler, 1990]. Like REP, Grow first finds a theory that overfits the data. <p> The generalizations of the clauses are formed by repeatedly deleting a final sequence of conditions from the clause so that the error on the growing set goes up the least. For a detailed description of the Grow algorithm see <ref> [Cohen, 1993] </ref>. <p> This is repeated until all clauses in the final concept description of constant size have been found. Therefore the costs of this algorithm are O (n 2 log n). Again, consult <ref> [Cohen, 1993] </ref> for a detailed proof. * Grow replaces the bottom-up hill-climbing search of REP by a top-down approach (see section 2.4). <p> Both algorithms, REP and Grow, were implemented as described in <ref> [Cohen, 1993] </ref> with the exception that delete-last-literal was used as a clause pruning operator (as in [Brunk and Pazzani, 1991]) instead of Cohen's operator that deletes a final sequence of literals from a clause. 2 All systems were imple mented in Sicstus PROLOG, run-times were measured on a SUN SPARCstation IPX. <p> The system builds upon ideas introduced by [Brunk and Pazzani, 1991] and <ref> [Cohen, 1993] </ref>, but improves upon them in the following ways: Efficiency: [Cohen, 1993] has shown that the complexity of REP is (n 4 ) on random data and has proposed an alternative algorithm | Grow | with time complexity (n 2 log n). <p> The system builds upon ideas introduced by [Brunk and Pazzani, 1991] and <ref> [Cohen, 1993] </ref>, but improves upon them in the following ways: Efficiency: [Cohen, 1993] has shown that the complexity of REP is (n 4 ) on random data and has proposed an alternative algorithm | Grow | with time complexity (n 2 log n). <p> Similar problems may occur with small training sets. In the near future I-REP should be adapted to be capable of dealing with numeric data and multi-valued classes to allow a test in a broader variety of real-world domains as in <ref> [Cohen, 1993] </ref>.
Reference: [Dolsak and Muggleton, 1992] <author> Bojan Dolsak and Stephen Muggleton. </author> <title> The application of Inductive Logic Programming to finite-element mesh design. </title> <editor> In Stephen Mug-gleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 453-472. </pages> <publisher> Academic Press Ltd., </publisher> <address> London, </address> <year> 1992. </year> <month> 13 </month>
Reference-contexts: Tests were performed for most of the datasets in the UCI Machine Learning repository that had only two classes and only symbolic attributes, and also for the KRK [Muggleton et al., 1989] and the Mesh <ref> [Dolsak and Muggleton, 1992] </ref> domains. Table 1 gives an overview of the used databases along with a comparison of the run-times of the different algorithms. In some domains artificial noise was generated by inverting the classification of 10% of the examples.
Reference: [Dzeroski and Bratko, 1992] <author> Saso Dzeroski and Ivan Bratko. </author> <title> Handling noise in Induc--tive Logic Programming. </title> <booktitle> In Proceedings of the International Workshop on Inductive Logic Programming, </booktitle> <address> Tokyo, Japan, </address> <year> 1992. </year>
Reference-contexts: Linus [Lavrac and Dzeroski, 1992] relies directly on the noise handling abilities of decision tree learning algorithms like CN2 [Clark and Niblett, 1989, Clark and Boswell, 1991] or ASSISTANT [Bratko and Kononenko, 1986]. Others, like mFoil <ref> [Dzeroski and Bratko, 1992] </ref>, have adapted some of these well-known methods from attribute-value learning for the ILP framework. Pruning is a standard way of dealing with noise in decision tree learning (see e.g. [Mingers, 1989] or [Esposito et al., 1993]). <p> This theory will subsequently be generalized by cutting off branches of the decision tree (as in [Quinlan, 1987] or [Breiman et al., 1984]). In ILP, pre-pruning has been common in the form of stopping criteria as used in Foil [Quinlan, 1990], mFoil <ref> [Dzeroski and Bratko, 1992] </ref>, or Fossil [Furnkranz, 1994a]. Post-pruning was introduced to ILP with Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] based on ideas by [Quinlan, 1987] and [Pagallo and Haussler, 1990]. First the training set is split into two subsets: a growing set and a pruning set . <p> The Mesh data were tested in 5 runs as described in <ref> [Dzeroski and Bratko, 1992] </ref>, but classification accuracy on negative examples was measured as well. The KRK data were tested on 5 different example set sizes evaluated on 5000 noise-free examples.
Reference: [Esposito et al., 1993] <author> Floriana Esposito, Donato Malerba, and Giovanni Semeraro. </author> <title> Decision tree pruning as a search in the state space. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 165-184, </pages> <address> Vienna, Austria, 1993. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Others, like mFoil [Dzeroski and Bratko, 1992], have adapted some of these well-known methods from attribute-value learning for the ILP framework. Pruning is a standard way of dealing with noise in decision tree learning (see e.g. [Mingers, 1989] or <ref> [Esposito et al., 1993] </ref>). There are two fundamentally different approaches [Cestnik et al., 1987]: Pre-Pruning means that during concept generation some training examples are deliberately ignored, so that the final concept description does not classify all training instances correctly.
Reference: [Furnkranz, 1994a] <author> Johannes Furnkranz. Fossil: </author> <title> A robust relational learner. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, Catania, </booktitle> <address> Italy, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This theory will subsequently be generalized by cutting off branches of the decision tree (as in [Quinlan, 1987] or [Breiman et al., 1984]). In ILP, pre-pruning has been common in the form of stopping criteria as used in Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992], or Fossil <ref> [Furnkranz, 1994a] </ref>. Post-pruning was introduced to ILP with Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] based on ideas by [Quinlan, 1987] and [Pagallo and Haussler, 1990]. First the training set is split into two subsets: a growing set and a pruning set .
Reference: [Furnkranz, 1994b] <author> Johannes Furnkranz. </author> <title> Top-down pruning in relational learning. </title> <type> Technical Report OEFAI-TR-94-03, </type> <institution> Austrian Research Institute for Artificial Intelligence, </institution> <year> 1994. </year>
Reference-contexts: Therefore REP's specific-to-general search can be expected to be slow and imprecise for noisy data, because it has to prune a significant portion of the theory previously generated in the growing phase and is likely to stop at a local maximum during this process. <ref> [Furnkranz, 1994b] </ref> reports experiments with an algorithm that is able to find a starting theory much closer to the final theory than the most specific theory. <p> Consequently Cohen tried to improve Grow by adding two stopping heuristics to the initial stage of overfitting, and thus achieved a further speed-up of the algorithm. Another way of combining pre-pruning and post-pruning methods to get better results can be found in <ref> [Furnkranz, 1994b] </ref>.
Reference: [Holte, 1993] <author> Robert C. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 63-91, </pages> <year> 1993. </year>
Reference-contexts: In the natural domains Mesh, Promoters and Votes I-REP is at least equal to the other algorithms in terms of predictive accuracy, but significantly faster than both of them. This seems to confirm the observation of <ref> [Holte, 1993] </ref> that in most commonly used data sets, simple rules perform reasonably well.
Reference: [Lavrac and Dzeroski, 1992] <author> Nada Lavrac and Saso Dzeroski. </author> <title> Inductive learning of relations from noisy examples. </title> <editor> In Stephen Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 495-516. </pages> <publisher> Academic Press Ltd., </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: Significant effort has gone into investigating the effect of noisy data on decision tree learning algorithms (see e.g. [Quinlan, 1993, Breiman et al., 1984]). Not surprisingly, noise handling methods have also entered the emerging field of Inductive Logic Programming (ILP) [Muggleton, 1992]. Linus <ref> [Lavrac and Dzeroski, 1992] </ref> relies directly on the noise handling abilities of decision tree learning algorithms like CN2 [Clark and Niblett, 1989, Clark and Boswell, 1991] or ASSISTANT [Bratko and Kononenko, 1986].
Reference: [Mingers, 1989] <author> John Mingers. </author> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 227-243, </pages> <year> 1989. </year>
Reference-contexts: Others, like mFoil [Dzeroski and Bratko, 1992], have adapted some of these well-known methods from attribute-value learning for the ILP framework. Pruning is a standard way of dealing with noise in decision tree learning (see e.g. <ref> [Mingers, 1989] </ref> or [Esposito et al., 1993]). There are two fundamentally different approaches [Cestnik et al., 1987]: Pre-Pruning means that during concept generation some training examples are deliberately ignored, so that the final concept description does not classify all training instances correctly.
Reference: [Muggleton et al., 1989] <author> Stephen Muggleton, Michael Bain, Jean Hayes-Michie, and Donald Michie. </author> <title> An experimental comparison of human and machine learning formalisms. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pages 113-118, </pages> <year> 1989. </year>
Reference-contexts: Tests were performed for most of the datasets in the UCI Machine Learning repository that had only two classes and only symbolic attributes, and also for the KRK <ref> [Muggleton et al., 1989] </ref> and the Mesh [Dolsak and Muggleton, 1992] domains. Table 1 gives an overview of the used databases along with a comparison of the run-times of the different algorithms. In some domains artificial noise was generated by inverting the classification of 10% of the examples.
Reference: [Muggleton, 1992] <author> Stephen Muggleton, </author> <title> editor. Inductive Logic Programming. </title> <publisher> Academic Press Ltd., </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: Significant effort has gone into investigating the effect of noisy data on decision tree learning algorithms (see e.g. [Quinlan, 1993, Breiman et al., 1984]). Not surprisingly, noise handling methods have also entered the emerging field of Inductive Logic Programming (ILP) <ref> [Muggleton, 1992] </ref>. Linus [Lavrac and Dzeroski, 1992] relies directly on the noise handling abilities of decision tree learning algorithms like CN2 [Clark and Niblett, 1989, Clark and Boswell, 1991] or ASSISTANT [Bratko and Kononenko, 1986].
Reference: [Pagallo and Haussler, 1990] <author> Giulia Pagallo and David Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 71-99, </pages> <year> 1990. </year>
Reference-contexts: In ILP, pre-pruning has been common in the form of stopping criteria as used in Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992], or Fossil [Furnkranz, 1994a]. Post-pruning was introduced to ILP with Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] based on ideas by [Quinlan, 1987] and <ref> [Pagallo and Haussler, 1990] </ref>. First the training set is split into two subsets: a growing set and a pruning set . A concept description explaining all of the examples in the growing set is generated with a relational learning algorithm. <p> This means that the training set is split into disjoint sets according to the outcome of the test chosen for the top level decision. After this, the algorithm is recursively applied to each of these sets independently. Greedy covering algorithms like Foil follow a separate-and-conquer strategy <ref> [Pagallo and Haussler, 1990] </ref>. This method first learns a rule from the whole training set and subsequently removes all examples that are covered by this rule. Then the algorithm recursively tries to find rules that explain the remaining examples. <p> Cohen has then proposed a pruning algorithm based on the technique used in the Grove learning system <ref> [Pagallo and Haussler, 1990] </ref>. Like REP, Grow first finds a theory that overfits the data.
Reference: [Quinlan, 1987] <author> John Ross Quinlan. </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27 </volume> <pages> 221-234, </pages> <year> 1987. </year>
Reference-contexts: Post-Pruning means that first a concept description is generated that perfectly explains all training instances. This theory will subsequently be generalized by cutting off branches of the decision tree (as in <ref> [Quinlan, 1987] </ref> or [Breiman et al., 1984]). In ILP, pre-pruning has been common in the form of stopping criteria as used in Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992], or Fossil [Furnkranz, 1994a]. <p> In ILP, pre-pruning has been common in the form of stopping criteria as used in Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992], or Fossil [Furnkranz, 1994a]. Post-pruning was introduced to ILP with Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] based on ideas by <ref> [Quinlan, 1987] </ref> and [Pagallo and Haussler, 1990]. First the training set is split into two subsets: a growing set and a pruning set . A concept description explaining all of the examples in the growing set is generated with a relational learning algorithm.
Reference: [Quinlan, 1990] <author> John Ross Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: This theory will subsequently be generalized by cutting off branches of the decision tree (as in [Quinlan, 1987] or [Breiman et al., 1984]). In ILP, pre-pruning has been common in the form of stopping criteria as used in Foil <ref> [Quinlan, 1990] </ref>, mFoil [Dzeroski and Bratko, 1992], or Fossil [Furnkranz, 1994a]. Post-pruning was introduced to ILP with Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] based on ideas by [Quinlan, 1987] and [Pagallo and Haussler, 1990].
Reference: [Quinlan, 1993] <author> John Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Being able to deal with noisy data is a must for algorithms that are meant to learn concepts in real-world domains. Significant effort has gone into investigating the effect of noisy data on decision tree learning algorithms (see e.g. <ref> [Quinlan, 1993, Breiman et al., 1984] </ref>). Not surprisingly, noise handling methods have also entered the emerging field of Inductive Logic Programming (ILP) [Muggleton, 1992].
Reference: [Schaffer, 1993] <author> Cullen Schaffer. </author> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178, </pages> <year> 1993. </year>
Reference-contexts: This is more or less the inversion of REP's delete-clause operator. However, if this accuracy is not measured appropriately, either because there are not enough examples left, or because of a bad split of the examples, I-REP will be prone to overgeneralization. Using the terminology of <ref> [Schaffer, 1993] </ref>, I-REP has a strong Over-fitting Avoidance Bias, which can be detrimental in some domains. 5 Experiments 5.1 Implementations of the Algorithms We have tested two different implementations of I-REP, which differ in the way they prune the clauses (let p (n) be the number of positive (negative) examples covered <p> The reason for this is that I-REP's top-down approach to pruning has an even stronger Overfit-ting Avoidance Bias than Grow, which can be inappropriate in some domains <ref> [Schaffer, 1993] </ref>. I-REP-2 in general seems to be worse than I-REP. Its purity criterion for evaluating a clause that tries to maximize the percentage of the covered positive examples seems to have a preference for more specific clauses than I-REP (which can also be seen from the higher run-times). <p> I-REP's efficiency stems from the tight integration of post-pruning and pre-pruning. Whenever the algorithm learns a clause that is worse than the empty clause, learning stops. However, this may cause the algorithm to over-generalize in domains with a rather specific concept description <ref> [Schaffer, 1993] </ref>. Similar problems may occur with small training sets. In the near future I-REP should be adapted to be capable of dealing with numeric data and multi-valued classes to allow a test in a broader variety of real-world domains as in [Cohen, 1993].
References-found: 23

