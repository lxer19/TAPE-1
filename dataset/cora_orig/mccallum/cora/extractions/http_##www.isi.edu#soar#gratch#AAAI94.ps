URL: http://www.isi.edu/soar/gratch/AAAI94.ps
Refering-URL: http://www.isi.edu/soar/gratch/home.html
Root-URL: 
Email: -gratch, dejong-@cs.uiuc.edu  chien@aig.jpl.nasa.gov  
Title: Improving Learning Performance Through Rational Resource Allocation  
Author: Jonathan Gratch Steve Chien and Gerald DeJong 
Affiliation: *Beckman Institute University of Illinois  Jet Propulsion Laboratory California Institute of Technology  
Date: August, 1994 pp. 576-581  
Address: Seattle, WA,  405 N. Mathews Av., Urbana, IL 61801  4800 Oak Grove Drive, Pasadena, CA 91109-8099  
Note: Appears in the Proceedings of the 12th National Conference on Artificial Intelligence (AAAI94)  
Abstract: This article shows how rational analysis can be used to minimize learning cost for a general class of statistical learning problems. We discuss the factors that influence learning cost and show that the problem of efficient learning can be cast as a resource optimization problem. Solutions found in this way can be significantly more efficient than the best solutions that do not account for these factors. We introduce a heuristic learning algorithm that approximately solves this optimization problem and document its performance improvements on synthetic and real-world problems.
Abstract-found: 1
Intro-found: 1
Reference: [Chien94] <author> S. A. Chien, J. M. Gratch and M. C. Burl, </author> <title> On the Effi cient Allocation of Resources for Hypothesis Evaluation in Machine Learning: A Statistical Approach, </title> <type> Technical Report, </type> <institution> University of Illinois (forthcoming). </institution>
Reference-contexts: See <ref> [Chien94] </ref> for complete discussion of such rational and nonrational algorithms. The probability is computed with equations analogous to Equation 2. Equal Allocation Policy. The non-rational algorithm follows the equal allocation policy.
Reference: [desJardins92] <author> M. E. desJardins, PAGODA: </author> <title> A Model for Auton omous Learning in Probabilistic Domains, </title> <type> Ph.D. Thesis, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: A weaker bias allows higher potential accuracy but requires more data. The selection of an appropriate bias depends on the availability and cost of obtaining training examples as well as usefulness of better prediction (see <ref> [desJardins92] </ref>). The same issue arises in neural networks and in statistics when one must choose a network topology or statistical model that balances the tradeoff between the fit to the data and the number of examples required to reach a given level of predictive accuracy.
Reference: [Doyle90] <author> J. Doyle, </author> <title> Rationality and its Roles in Reasoning (ex tended version), </title> <booktitle> AAAI90, </booktitle> <address> Boston, MA, </address> <year> 1990. </year>
Reference: [Govindarajulu81] <author> Z. Govindarajulu, </author> <title> The Sequential Statistical Analysis, </title> <publisher> American Sciences Press, </publisher> <address> Columbus, OH, </address> <year> 1981. </year>
Reference: [Gratch92] <author> J. Gratch and G. DeJong, COMPOSER: </author> <title> A Probabil istic Solution to the Utility Problem in Speed-up Learning, </title> <booktitle> AAAI92, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992, </year> <pages> pp. 235-240. </pages>
Reference-contexts: We discuss this in the context of parametric hypothesis selection problems, an abstract class of statistical learning problems where a system must select one of a finite set of hypothesized courses of action, where the quality of each hypothesis is described as a function of some unknown parameters (e.g. <ref> [Gratch92, Greiner92, Kaelbling93, Moore94, Musick93] </ref>). A learning system determines and refines estimates of these parameters by paying for training examples. <p> For example, the utility problem in speed-up learning is a selection problem in which a problem solving heuristic is chosen from a set of proposed candidates, where expected utility is defined as the average time to solve a problem <ref> [Gratch92, Greiner92, Minton88] </ref>. The attribute selection problem in classification learning is a problem of selecting one of a set of attributes on which to split, where utility is equated with information gain [Musick93]. In reinforcement learning a system must select an action, where utility is equated with expected reward [Kaelbling93].
Reference: [Gratch93] <author> J. Gratch, S. Chien and G. DeJong, </author> <title> Learning Search Control Knowledge for Deep Space Network Scheduling, </title> <address> ML93, Amherst, MA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: An interesting issue we have not sufficiently explored is possible strategies for setting the initial parameter size. 4.2.2 NASA Scheduling Data. The test of real-world applicability is based on data drawn from a NASA scheduling application detailed in <ref> [Gratch93] </ref>. This data provides a test of the applicability of the techniques. Both algorithms assume estimated utility varies normally from the expected utility. In fact, this common assumption is violated by the data as most of the scheduling heuristics are bi-modally distributed.
Reference: [Greiner92] <author> R. Greiner and I. Jurisica, </author> <title> A Statistical Approach to Solving the EBL Utility Problem, </title> <booktitle> AAAI92, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992, </year> <pages> pp. 241-248. </pages>
Reference-contexts: We discuss this in the context of parametric hypothesis selection problems, an abstract class of statistical learning problems where a system must select one of a finite set of hypothesized courses of action, where the quality of each hypothesis is described as a function of some unknown parameters (e.g. <ref> [Gratch92, Greiner92, Kaelbling93, Moore94, Musick93] </ref>). A learning system determines and refines estimates of these parameters by paying for training examples. <p> For example, the utility problem in speed-up learning is a selection problem in which a problem solving heuristic is chosen from a set of proposed candidates, where expected utility is defined as the average time to solve a problem <ref> [Gratch92, Greiner92, Minton88] </ref>. The attribute selection problem in classification learning is a problem of selecting one of a set of attributes on which to split, where utility is equated with information gain [Musick93]. In reinforcement learning a system must select an action, where utility is equated with expected reward [Kaelbling93].
Reference: [Hogg78] <author> R. V. Hogg and A. T. Craig, </author> <title> Introduction to Mathemati cal Statistics, </title> <publisher> Macmillan Inc., </publisher> <address> London, </address> <year> 1978. </year>
Reference-contexts: To assess these probabilities we must adopt certain statistical assumptions. In this article we adopt the normal parametric model for reasoning about statistical error. This assumes that the difference between the expected utility and estimated utility of a hypothesis can be accurately approximated by a normal distribution (see <ref> [Hogg78] </ref> for an explanation of the robustness of this common assumption which is grounded in the Central Limit Theorem). The expected cost associated with processing data is also assumed to be normally distributed.
Reference: [Howard70] <author> R. A. Howard, </author> <title> Decision Analysis: Perspectives on Inference, Decision, and Experimentation, </title> <booktitle> Proceedings of the IEEE 58, 5 (1970), </booktitle> <pages> pp. 823-834. </pages>
Reference-contexts: Selection problems could be formalized in a bayesian statistical framework as in [Moore94, Rivest88]. This would eliminate the need for an initial sample but require a rigorous encoding of prior knowledge. Related to this, Howard <ref> [Howard70] </ref> has extensively investigated a bayesian framework for assessing learning cost in the case of single hypothesis problems.
Reference: [Kaelbling93] <author> L. P. Kaelbling, </author> <title> Learning in Embedded Systems, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: We discuss this in the context of parametric hypothesis selection problems, an abstract class of statistical learning problems where a system must select one of a finite set of hypothesized courses of action, where the quality of each hypothesis is described as a function of some unknown parameters (e.g. <ref> [Gratch92, Greiner92, Kaelbling93, Moore94, Musick93] </ref>). A learning system determines and refines estimates of these parameters by paying for training examples. <p> The attribute selection problem in classification learning is a problem of selecting one of a set of attributes on which to split, where utility is equated with information gain [Musick93]. In reinforcement learning a system must select an action, where utility is equated with expected reward <ref> [Kaelbling93] </ref>. Several factors affect the cost of identifying a good selection. For example, there may be some cost in obtaining each training example. <p> For example the tradeoff between goal-directed action and exploration behavior has been studied in reinforcement learning <ref> [Kaelbling93] </ref>. Another active area of investigation involves the selection of an inductive bias for classification learning tasks. A weaker bias allows higher potential accuracy but requires more data.
Reference: [Minton88] <author> S. </author> <title> Minton,in Learning Search Control Knowledge: An Explanation-Based Approach, </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: For example, the utility problem in speed-up learning is a selection problem in which a problem solving heuristic is chosen from a set of proposed candidates, where expected utility is defined as the average time to solve a problem <ref> [Gratch92, Greiner92, Minton88] </ref>. The attribute selection problem in classification learning is a problem of selecting one of a set of attributes on which to split, where utility is equated with information gain [Musick93]. In reinforcement learning a system must select an action, where utility is equated with expected reward [Kaelbling93].
Reference: [Moore94] <author> A. W. Moore and M. S. Lee, </author> <title> Efficient Algorithms for Minimizing Cross Validation Error, </title> <address> ML94, New Brunswick, MA, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: We discuss this in the context of parametric hypothesis selection problems, an abstract class of statistical learning problems where a system must select one of a finite set of hypothesized courses of action, where the quality of each hypothesis is described as a function of some unknown parameters (e.g. <ref> [Gratch92, Greiner92, Kaelbling93, Moore94, Musick93] </ref>). A learning system determines and refines estimates of these parameters by paying for training examples. <p> We would like an algorithm to satisfy the requirement with the minimum cost possible. Several of the factors that contribute to the cost are unknown before learning begins. For this reason standard (non-rational) hypothesis selection al-2. We block examples as in <ref> [Moore94] </ref> to further reduce sampling complexity. Blocking forms estimates by averaging the difference in utility between hypotheses on each observed example, which can substantially reduce the variance in the data when hypotheses are related (e.g. when each hypothesis is a variant on a basic search control strategy). <p> These approximations can be almost as effective as the true information in guiding learning behavior. In this section we introduce a rational hypothesis selection algorithm that exploits these approximations to minimize selection cost. This is compared with an efficient non-rational approach similar to Moore and Lee's BRACE algorithm <ref> [Moore94] </ref>. The superiority of the rational approach is documented on artificial and real-world data sets. 4.1 Interval-Based Selection Algorithm We first introduce the basic hypothesis selection approach. Rational and non-rational algorithms derived from this approach differ in how they choose hypotheses to further evaluate. <p> For example, when selecting attributes in a decision tree a multinomial model may be more appropriate. We suspect comparable results will hold for a wide range of statistical models but further analysis is necessary. Selection problems could be formalized in a bayesian statistical framework as in <ref> [Moore94, Rivest88] </ref>. This would eliminate the need for an initial sample but require a rigorous encoding of prior knowledge. Related to this, Howard [Howard70] has extensively investigated a bayesian framework for assessing learning cost in the case of single hypothesis problems.
Reference: [Musick93] <author> R. Musick, J. Catlett and S. Russell, </author> <title> Decision Theo retic Subsampling for Induction on Large Databases, </title> <address> ML93, MA, </address> <month> June </month> <year> 1993, </year> <pages> pp. 212-219. </pages>
Reference-contexts: Somewhat paradoxically, cost is also an issue when there is an overabundance of data. In this case it is expensive to use all of the data and one needs some criteria to decide how much data is enough to achieve a given level of performance <ref> [Musick93] </ref>. Finally, learning may involve ethical issues, as when experiments require giving potentially harmful treatments to human subjects. Under these circumstances it is a moral imperative to utilize as few subjects as possible and to quickly recognize and discard those treatments that worsen the patients condition. <p> We discuss this in the context of parametric hypothesis selection problems, an abstract class of statistical learning problems where a system must select one of a finite set of hypothesized courses of action, where the quality of each hypothesis is described as a function of some unknown parameters (e.g. <ref> [Gratch92, Greiner92, Kaelbling93, Moore94, Musick93] </ref>). A learning system determines and refines estimates of these parameters by paying for training examples. <p> The attribute selection problem in classification learning is a problem of selecting one of a set of attributes on which to split, where utility is equated with information gain <ref> [Musick93] </ref>. In reinforcement learning a system must select an action, where utility is equated with expected reward [Kaelbling93]. Several factors affect the cost of identifying a good selection. For example, there may be some cost in obtaining each training example.
Reference: [Rivest88] <author> R. L. Rivest and R. Sloan, </author> <title> A New Model for Inductive Inference, </title> <booktitle> Second Conference on Theoretical Aspects of Reasoning about Knowledge, </booktitle> <year> 1988. </year>
Reference-contexts: For example, when selecting attributes in a decision tree a multinomial model may be more appropriate. We suspect comparable results will hold for a wide range of statistical models but further analysis is necessary. Selection problems could be formalized in a bayesian statistical framework as in <ref> [Moore94, Rivest88] </ref>. This would eliminate the need for an initial sample but require a rigorous encoding of prior knowledge. Related to this, Howard [Howard70] has extensively investigated a bayesian framework for assessing learning cost in the case of single hypothesis problems.
Reference: [Russell91] <author> S. Russell and E. Wefald, </author> <title> Do the Right Thing: Studies in Limited Rationality, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Finally, these learning issues can be seen as part of the more general area of limited rationality. This is the problem of developing a theory of rational decision making when in the presence of limited reasoning resources <ref> [Russell91, Wel-lman92] </ref>. To summarize, we argue that learning algorithms must assess both the benefits and costs of learning. We provide a theoretical analysis of the factors that contribute to learning cost.
Reference: [Tadepalli92] <author> P. Tadepalli, </author> <title> A theory of unsupervised speedup learning, </title> <booktitle> AAAI92, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992, </year> <pages> pp. 229-234. </pages>
Reference: [Valiant84] <author> L. G. Valiant, </author> <title> A Theory of the Learnable, </title> <booktitle> Commu nications of the ACM 27, </booktitle> <year> (1984), </year> <pages> pp. 1134-1142. </pages>
Reference-contexts: Several alternative requirements have been proposed. In this paper we adopt the probably approximately correct (PAC) requirement favored by computational learning theory <ref> [Valiant84] </ref>. Under this requirement a hypothesis selection algorithm selects a hypothesis that with high probability is close to the best. The expected utility associated with a hypothesis can be estimated by observing its performance over a finite set of training examples.
Reference: [Wellman92] <author> M. P. Wellman and J. Doyle, </author> <title> Modular Utility Re presentation for Decision-Theoretic Planning, </title> <institution> AIPS92, College Park, Maryland, </institution> <month> June </month> <year> 1992, </year> <pages> pp. 236-242. </pages>
References-found: 18

