URL: http://www.cse.ogi.edu/Sparse/paper/autrey.rpe.94.ps
Refering-URL: http://www.cse.ogi.edu/Sparse/sparse.intercprop.html
Root-URL: http://www.cse.ogi.edu
Email: tito@cse.ogi.edu  
Title: Demand-Driven Interprocedural Constant Propagation: Implementation and Evaluation  
Author: Tito Autrey P. O. 
Date: May 6, 1994  
Address: Box 91000, Portland, OR 97291-1000 USA  
Affiliation: Department of Computer Science and Engineering Oregon Graduate Institute of Science Technology  
Abstract: We have developed a hybrid algorithm for interprocedural constant propagation combining the Wegman-Zadeck, Burke and Cytron, and demand-driven approaches. We use a modified Wegman-Zadeck algorithm as the incremental intraprocedural constant propagator in the demand-driven interprocedural framework. Wegman and Zadeck solve the interprocedural constant propagation problem by linking the SSA graphs of a set of procedures into a single large SSA graph and then use their Sparse Conditional Constant (SCC) algorithm on the constructed graph [WZ91]. SCC is an optimistic (assumes values are constant until proven otherwise) worklist algorithm. Burke and Cytron solve the interprocedural constant propagation problem with an algorithm that uses a pessimistic (assumes values are non-constants until proven otherwise) incremental intraprocedural constant propagator and iterates forward and backward over the call graph until no new information is discovered [BC86]. We will show that our hybrid algorithm, Demand-driven With Incremental-intrapro-cedural Modification (DWIM), is fast and finds the same number of constants as the best interprocedural constant propagator in use today. We know of no implementations of either interprocedural Wegman-Zadeck or Burke and Cytron. Over a set of standard benchmarks we find 14% constants interprocedural constants over and above intrapro-cedural constants versus 71.5% by Grove and Torczon [GT93] using Jump Functions [CCKT86]. The discrepancy is due to current limitations in our evaluation environment and our choice of metric. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> References </institution>
Reference: [All72] <author> F. E. Allen. </author> <title> A Catalogue of Optimizing Transformations, </title> <address> pages 1-30. </address> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NH, </address> <year> 1972. </year>
Reference-contexts: This form of dead code is called unreachable code. The other form is code whose results are not used, called unused code <ref> [All72] </ref>. 2 Constant propagation is computed on a data flow graph representation of a procedure, frequently classic use-def and def-use chains [ASU86]. Each variable use or definition is represented by a value from Killdall's three-level constant propagation lattice [Kil73].
Reference: [ASU86] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: This form of dead code is called unreachable code. The other form is code whose results are not used, called unused code [All72]. 2 Constant propagation is computed on a data flow graph representation of a procedure, frequently classic use-def and def-use chains <ref> [ASU86] </ref>. Each variable use or definition is represented by a value from Killdall's three-level constant propagation lattice [Kil73]. The lattice is used to cast constant propagation as a global data flow problem. Top, &gt;, represents an as yet unknown value. Bottom, ?, represents a known non-constant value.
Reference: [BC86] <author> Michael Burke and Ron Cytron. </author> <title> Interprocedural dependence analysis and par-allelization. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 162-175, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: They noted that PNFJFs found no more constants than the PTFJFs. And complete 12 constant propagation found 0.8% more compared to PNFJF with interprocedural M OD. Burke and Cytron note that interprocedural constant propagation can be modeled as incremental intraprocedural constant propagation <ref> [BC86] </ref>. They claim that optimistic algorithms, which assume a value is constant until proven otherwise, such as Wegman-Zadeck, are ill-suited to incremental propagation. This is because all values are initially set to &gt;.
Reference: [CCKT86] <author> David Callahan, Keith D. Cooper, Ken Kennedy, and Linda Torczon. </author> <title> Interproce-dural constant propagation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 152-161, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: The following five analyses were needed to complete the study of interprocedural constant propagation: Call Graph Construction, Alias Analysis, MOD and REF Analysis, and Jump Function identification. The analyses store their results in a procedure summary block which is referenced by later analysis and optimization stages <ref> [CCKT86] </ref>. A brief description of the algorithms we use and 3 If you need the natural graph successor nodes then simply reverse the graph. 4 comments on their implementations are given below. <p> Our implementation for M OD and REF computation is also demand-driven, using the call-graph as the data flow graph. 3.4 Jump Functions Jump functions 5 are a concise way of expressing constant flow through a procedure <ref> [CCKT86] </ref>. The flow of constants in a procedure p to a call site s for each actual argument and global variable (called global argument for this discussion) x, is expressed by a forward jump function (FJF) F x s . <p> Callahan notes that DATA statements and BLOCKDATA routines are a rich source of constants for global variables <ref> [CCKT86] </ref>. Since Nascent doesn't parse either of them DWIM doesn't have access to the constants. No RJFs have been tested, but according to Grove and Torczon RJFs contribute significantly to only one of their 12 benchmarks. <p> Callahan et al. describe a method for efficient interprocedural constant propagation that easily supports separate compilation, but provide no numbers on its effectiveness <ref> [CCKT86] </ref>. They develop the idea of forward and return jump functions. The FJFs summarize the partial computations from procedure invocation to a given call site and the RJFs summarize the side effects (except I/O) of a procedure.
Reference: [CD93] <author> Charles Consel and Olivier Danvy. </author> <title> Tutorial notes on partial evaluation. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1993. </year>
Reference-contexts: It turns out that two passes is sufficient. 17 Constant propagation is one of the primary tools used in partial evaluation <ref> [CD93] </ref>. In partial evaluation a program as specialized with respect to some of its inputs and a new one is generate by a partial evaluation function.
Reference: [CFR + 89] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 25-35, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Modern approaches to constant propagation are performed on sparse graphs. One of the most common sparse graphs is the Static Single Assignment (SSA) form <ref> [CFR + 89] </ref>. The Wegman-Zadeck Sparse Conditional Constant (SCC)-propagator (WZ-SCC) is a worklist algorithm [WZ91]. It has two lists, one of variable uses and the other of edges in the CFG.
Reference: [CK84] <author> Keith D. Cooper and Ken Kennedy. </author> <title> Efficient computation of flow-insensitive interprocedural summary information. </title> <booktitle> In Proceedings of the ACM SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <pages> pages 247-258, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Globals (p))g IM OD (p) = DM OD (p) c2Calledby (p) M OD (c) IREF (p) = DREF (p) S c2Calledby (p) REF (c) M OD (p) = x2IMOD (p) Alias (x) REF (p) = x2IREF (p) Alias (x) MOD and REF analysis computes 8pM OD (p) and REF (p) <ref> [CK84, CK87, CK88] </ref>. They represent the side effects, and read and write sets of variables of a procedure. During parsing we initialize the DM OD (p) and DREF (p) sets for each procedure.
Reference: [CK86] <author> Keith D. Cooper and Ken Kennedy. </author> <title> Fast interprocedural alias analysis. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 29-41, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: There are two types of aliases; Type I are formal-global aliases which are created by passing globals and formals as actuals, and Type II are formal-formal aliases which are created by passing the same local, formal or global in more than one position at a call site <ref> [Coo85, CK88, CK86, 6 MW93] </ref>. The analysis uses two data structures which are built on top of the call graph. The algorithm for computing the formal-global binding graph, fi, is initialized by examining all call sites in all routines.
Reference: [CK87] <author> Keith D. Cooper and Ken Kennedy. </author> <title> Efficient computation of flow-insensitive interprocedural summary infomation (a correction). </title> <type> Technical Report TR87-60, </type> <institution> Rice University, </institution> <year> 1987. </year>
Reference-contexts: Globals (p))g IM OD (p) = DM OD (p) c2Calledby (p) M OD (c) IREF (p) = DREF (p) S c2Calledby (p) REF (c) M OD (p) = x2IMOD (p) Alias (x) REF (p) = x2IREF (p) Alias (x) MOD and REF analysis computes 8pM OD (p) and REF (p) <ref> [CK84, CK87, CK88] </ref>. They represent the side effects, and read and write sets of variables of a procedure. During parsing we initialize the DM OD (p) and DREF (p) sets for each procedure.
Reference: [CK88] <author> Keith D. Cooper and Ken Kennedy. </author> <title> Interprocedural side-effect analysis in linear time. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 57-66, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: There are two types of aliases; Type I are formal-global aliases which are created by passing globals and formals as actuals, and Type II are formal-formal aliases which are created by passing the same local, formal or global in more than one position at a call site <ref> [Coo85, CK88, CK86, 6 MW93] </ref>. The analysis uses two data structures which are built on top of the call graph. The algorithm for computing the formal-global binding graph, fi, is initialized by examining all call sites in all routines. <p> Globals (p))g IM OD (p) = DM OD (p) c2Calledby (p) M OD (c) IREF (p) = DREF (p) S c2Calledby (p) REF (c) M OD (p) = x2IMOD (p) Alias (x) REF (p) = x2IREF (p) Alias (x) MOD and REF analysis computes 8pM OD (p) and REF (p) <ref> [CK84, CK87, CK88] </ref>. They represent the side effects, and read and write sets of variables of a procedure. During parsing we initialize the DM OD (p) and DREF (p) sets for each procedure.
Reference: [Coo85] <author> Keith D. Cooper. </author> <title> Analyzing aliases of reference formal parameters. </title> <booktitle> In Proceedings of the Twelfthh Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 281-290, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: There are two types of aliases; Type I are formal-global aliases which are created by passing globals and formals as actuals, and Type II are formal-formal aliases which are created by passing the same local, formal or global in more than one position at a call site <ref> [Coo85, CK88, CK86, 6 MW93] </ref>. The analysis uses two data structures which are built on top of the call graph. The algorithm for computing the formal-global binding graph, fi, is initialized by examining all call sites in all routines.
Reference: [EB91] <author> Rudolf Eigenmann and William Blume. </author> <title> An effectiveness study of parallelizing compiler techniques. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: Constant propagation makes other analyses simpler and more exact which leads to useful indirect effects. For example, constant loop bounds enhance dependence analysis, enhance parallelization heuristics <ref> [EB91] </ref>, and may remove run-time range checks [Kol94]. It may also make subscript expressions linear which speeds analysis and can enable loop transformations [SLY90]. Constant propagation is generally only performed on integer and logical values.
Reference: [GT93] <author> Dan Grove and Linda Torczon. </author> <title> Interprocedural constant propagation: A study of jump function implementations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 90-99, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: They summarize the effect on the formal of executing the whole procedure. The type of a jump function is an element of the constant propagation lattice. According to Grove and Torczon there are four FJFs of interest <ref> [GT93] </ref>. The literal constant FJF (LCFJF) identifies actual arguments that are literal constants. In Example 1 5 The term is historical and was originated by John Cocke. 8 above, formal A in procedure R has value 1 and no other constants are found. <p> The expression evaluator handles aliasing. They trade completeness for supporting separate compilation and they choose data structures that give them time linear in the size of the call graph. Grove and Torczon evaluate the effectiveness of the various types of JFs <ref> [GT93] </ref>. They only handle integer constants and they don't track them in and out of arrays. First, in a bottom-up walk of the call graph, they build RJFs from an SSA graph, then they destroy the SSA graph.
Reference: [Hal91] <author> Mary Wolcott Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <year> 1991. </year>
Reference-contexts: When one line is changed in a file, a programmer doesn't want all the files to be recompiled just because the interprocedural information may have changed. There are conservative approaches that allow for correct but non-optimal interprocedural information to be used in order to minimize recompilation time <ref> [Hal91] </ref>.
Reference: [HK92] <author> Mary W. Hall and Ken Kennedy. </author> <title> Efficient call graph analysis. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 1(3) </volume> <pages> 227-242, </pages> <month> Sept </month> <year> 1992. </year>
Reference-contexts: Contrast them with procedure variables which can be assigned 4 Nascent is the name of the Fortran compiler being developed by the Sparse group at OGI under the supervision of Michael Wolfe. 5 procedure constants at any point. Hall and Kennedy <ref> [HK92] </ref> describe an algorithm that is complete and fast for languages with procedure formals. It finds the maximal Boundto (pf ) = fc : c is procedure constant bound to pf along some possible execution path g set for all procedure formals.
Reference: [HP90] <author> John Hennessy and David Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Induction variable analysis uses propagation results as input, and folding and propagation use induction expressions as input, so they can all be performed concurrently. Most constants are small integers <ref> [HP90] </ref>. This allows for a more compact encoding of a program by placing constants in an instruction immediate field rather than loading them explicitly from memory. Constant propagation makes other analyses simpler and more exact which leads to useful indirect effects.
Reference: [Kil73] <author> G. Kildall. </author> <title> A unified approach to global program optimization. </title> <booktitle> In Proceedings of the First Annual ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1973. </year>
Reference-contexts: Each variable use or definition is represented by a value from Killdall's three-level constant propagation lattice <ref> [Kil73] </ref>. The lattice is used to cast constant propagation as a global data flow problem. Top, &gt;, represents an as yet unknown value. Bottom, ?, represents a known non-constant value. The constants constitute the middle layer of the lattice, all unordered and comparable only for equality.
Reference: [Kol94] <author> Priyadarshan Kolte. </author> <title> Optimization of array subscript range checks. </title> <type> Technical report, </type> <institution> Oregon Graduate Institute, </institution> <year> 1994. </year>
Reference-contexts: Constant propagation makes other analyses simpler and more exact which leads to useful indirect effects. For example, constant loop bounds enhance dependence analysis, enhance parallelization heuristics [EB91], and may remove run-time range checks <ref> [Kol94] </ref>. It may also make subscript expressions linear which speeds analysis and can enable loop transformations [SLY90]. Constant propagation is generally only performed on integer and logical values. This is because they produce the biggest payoffs in terms of loop bounds, array subscripts and predicate values.
Reference: [KU77] <author> J. B. Kam and J. D. Ullman. </author> <title> Monotone data flow analysis frameworks. </title> <journal> Acta Informatica, </journal> <volume> 7 </volume> <pages> 305-317, </pages> <year> 1977. </year>
Reference-contexts: 3; interproce-dural constant propagation in general and DWIM in particular Section 4; our evaluation experiments Section 5; our results Section 6; related work Section 7; future work Section 8, and we summarize in Section 9. 1 2 Constant Propagation It is well-known that the general constant propagation problem is undecidable <ref> [KU77] </ref>. Solutions to the restricted decidable sub-problem still yield useful results. Constant propagation has several direct effects. It may mark edges as non-executable in the control flow graph (CFG) by static predicate evaluation. Dead code elimination removes non-executable edges and blocks from the CFG of a procedure 1 .
Reference: [LM94] <author> Jon Loeliger and Robert Metzger. </author> <title> Developing an interprocedural optimizing compiler. </title> <journal> SIGPLAN Notices, </journal> <volume> 29(4) </volume> <pages> 41-48, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Folklore has it that flow-sensitive analysis costs more and doesn't provide enough improvement in quality to be worth the cost. An early version of the Convex Application Compiler tried it, and the results confirmed the wisdom of the folklore <ref> [LM94] </ref>. The following five analyses were needed to complete the study of interprocedural constant propagation: Call Graph Construction, Alias Analysis, MOD and REF Analysis, and Jump Function identification. The analyses store their results in a procedure summary block which is referenced by later analysis and optimization stages [CCKT86].
Reference: [MS92] <author> Robert Metzger and Sean Stroud. </author> <title> Interprocedural constant propagation: An empirical study. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 1(2) </volume> <pages> 1-2, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: There is a distinct shortage of papers evaluating the effectiveness or cost of performing interprocedural analysis, optimization and in particular constant propagation. Grove and Torczon is the lone exception. Metzger and Stroud have performed an evaluation of a commercial compiler, but it is not yet published <ref> [MS92] </ref>. We will make comparisons to the Rice group under Ken Kennedy, to Wegman and Zadeck's extension of their intraprocedural SCC algorithm, to Burke and Cytron and to partial evaluation techniques.
Reference: [MW93] <author> Herbert G. Mayer and Michael Wolfe. </author> <title> Interprocedural alias analysis: Implementation and empirical results. </title> <journal> Software- Practice and Experience, </journal> <volume> 23(11) </volume> <pages> 1201-1233, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: These occur when a routine calls itself recursively with the formals used as actuals in the same position. These edges add no information so they may be safely ignored. turns out to be unnecessarily large and complex to compute directly <ref> [MW93] </ref>.
Reference: [SLY90] <author> Zhiyu Shen, Zhiyuan Li, and Pen-Chung Yew. </author> <title> An empirical study of fortran programs for parallelizing compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 356-364, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Constant propagation makes other analyses simpler and more exact which leads to useful indirect effects. For example, constant loop bounds enhance dependence analysis, enhance parallelization heuristics [EB91], and may remove run-time range checks [Kol94]. It may also make subscript expressions linear which speeds analysis and can enable loop transformations <ref> [SLY90] </ref>. Constant propagation is generally only performed on integer and logical values. This is because they produce the biggest payoffs in terms of loop bounds, array subscripts and predicate values.
Reference: [SW94] <author> Eric Stoltz and Michael Wolfe. </author> <title> Constant propagation: A fresh, demand-driven look. </title> <booktitle> In Symposium on Applied Computing. ACM SIGAPP, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: Each use obtains its value from the meet of the reaching defs. In SSA graphs there is only one reaching def, except for at nodes. Stoltz has shown that demand-driven solvers are faster than Wegman-Zadeck <ref> [SW94] </ref>. 2 Demand-driven in this case has nothing to do with lazy evaluation. Constant propagation is inherently eager.
Reference: [Tar72] <author> Robert Tarjan. </author> <title> Depth-first search in linear graph algorithms. </title> <journal> SIAM Journal of Computing, </journal> <volume> 1(2) </volume> <pages> 146-160, </pages> <year> 1972. </year>
Reference-contexts: Uniformly monotone data flow problems can be solved without iteration if the strongly connected components (SCCs) of the data flow graph are identified. Also, the solution is the same at all nodes in a cycle in the graph. Tarjan's fast algorithm for finding SCCs is efficient <ref> [Tar72] </ref>, and well suited to solving uniformly monotone data flow problems. It visits each node and edge of the graph precisely once. The demand-driven style for solving data flow problems bases the solution of the current graph node on the solution of its graph predecessor nodes 3 .
Reference: [WGS93] <author> Michael Wolfe, Michael P. Gerlek, and Eric Stoltz. </author> <title> Uniformly monotonic data flow frameworks. </title> <type> Technical report, </type> <institution> Oregon Graduate Institute, </institution> <year> 1993. </year> <note> Unpublished. </note>
Reference-contexts: Constant propagation is inherently eager. It refers to demanding the solution to a given data flow problem at predecessor nodes before computing your own. 3 3 Interprocedural Analysis -1z A number of interprocedural analyses are uniformly monotone data flow problems, including Alias, and MOD and REF calculations <ref> [WGS93] </ref>. Uniformly monotone data flow problems can be solved without iteration if the strongly connected components (SCCs) of the data flow graph are identified. Also, the solution is the same at all nodes in a cycle in the graph.

References-found: 27

