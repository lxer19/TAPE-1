URL: http://www.erg.sri.com/people/saurav/Papers/compjourn.ps
Refering-URL: http://www.erg.sri.com/people/saurav/paper.html
Root-URL: 
Email: saurav@ece.cmu.edu  
Title: Distributed Pipeline Scheduling: A Framework for Distributed, Heterogeneous Real-Time System Design  
Author: Saurav Chatterjee and Jay Strosnider 
Address: Pittsburgh, PA 15213, USA  
Affiliation: Department of Electrical Computer Engineering Carnegie Mellon University  
Abstract: This research was supported in part by a grant from the Office of Naval Research, in part by a grant from the Naval Research and Development Laboratory, and in part by a grant from Siemens Corporate Research. In The Computer Journal (British Computer Society), Vol. 38, No. 4, 1995. Abstract This paper describes the Distributed Pipeline Scheduling Framework that provides a systematic approach to designing distributed, heterogeneous real-time systems. This paper formalizes Distributed Pipe-lining Scheduling by providing a set of abstractions and transformations to map real-time applications to system resources, to create highly efficient and predictable systems, and to de-compose the very complex multi-resource system timing analysis problem into a set of simpler application stream and single resource schedu-lability problems to ascertain that all real-time application timing requirements are met. Distributed Pipeline Scheduling includes support for distributed, heterogeneous system resources and diverse local scheduling policies, global scheduling policies for efficient resource utilization, flow-control mechanisms for predictable system behavior, and a range of system reconfiguration options to meet application timing requirements. An audio/video example is used in this paper to demonstrate the power and utility of Distributed Pipeline Scheduling. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Bettati and J. Liu, </author> <booktitle> Algorithms for End-to-End Scheduling to Meet Deadlines Proceedings of the 2nd IEEE Conf. on Parallel and Distributed Systems, </booktitle> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: Limitations included a target platform consisting of a single CPU and a disk, each executing the rate-monotonic scheduling paradigm. No end-to-end timing analysis was provided. Natale and Stankovic [16] provided a dual off-line/on-line framework for guaranteeing deadlines for processes communicating via synchronous primitives. Bettati and Liu <ref> [1] </ref> provided a set of heuristics for resource allocation and end-to-end stream analysis for homogeneous multiprocessor systems. Applications were scheduled in a ow-shop/data-ow manner. Flow-shop scheduling has been applied to a homogeneous set of resources, i.e., all CPUs with identical scheduling paradigms.
Reference: [2] <author> S. Chatterjee and J. Strosnider, </author> <title> Distributed Pipeline Scheduling: End-to-End Analysis of Heterogeneous, </title> <booktitle> Multi-Resource Real-Time Systems, In the 15th International Conference on Distributed Computing Systems, </booktitle> <address> Vancouver, Canada, </address> <month> May </month> <year> 1995. </year>
Reference: [3] <author> S. Chatterjee and J. Strosnider, </author> <title> A Generalized Admissions Control Strategy for Heterogeneous, Distributed Multi-media Systems, </title> <type> CMUCSC-95-3 Technical Report, </type> <institution> Carnegie Mellon University, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: The remaining steps are denoted intermediary or synchronization steps. The source step produces the initial data, the intermediary steps filter and process the data, synchronization steps synchronize multiple data streams and the sink consumes the data. This paper does not take into account synchronization steps. Refer to <ref> [3] </ref> on designing real-time systems using synchronization processing steps. Processing step , , executes for the first time after it receives bytes of data; it can execute again after it receives bytes of new data. <p> This work is currently being extended in several fronts. Mapping and optimization techniques are being quantitatively examined. Furthermore, application stream specifications, mapping algorithms and system timing analysis are each being augmented to incorporate stochastic execution times, to allow dynamic application arrivals and departures <ref> [3] </ref>, and to provide probabilistic timing guarantees, respectively.
Reference: [4] <author> S. Chatterjee and J. Strosnider, </author> <title> Designing Distributed Multimedia Systems Configured to Meet Application Latency and Output Jitter Requirements, </title> <type> CMUCSC-95-4 Technical Report, </type> <institution> Carnegie Mellon University, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: In non-ideal environments, however, manipulating the phase delay between the completion of one pipeline stages execution and the commencement of the next pipeline stages execution is one technique to meet application timing requirements <ref> [4] </ref>. This paper assumes the following phase delay between two pipeline stages i and i+1, : where specifies the signed worst-case clock skew between resources r = and p = . <p> In heterogeneous systems described in this paper, where adjacent pipeline stages execute at different rates, multi-rate ow control must be used. Multi-rate ow control is an extension of basic ow control to incorporate different execution rates between pipeline stages. Refer to <ref> [4] </ref> on how to extend the basic ow control mechanisms described in the preceding paragraph into multi-rate ow controllers. This paper assumes that every pair of pipeline stages in the example suite uses Multi-rate Stop & Go Flow Control [4]. Processing steps within a pipeline execute in a data-ow manner. <p> Refer to <ref> [4] </ref> on how to extend the basic ow control mechanisms described in the preceding paragraph into multi-rate ow controllers. This paper assumes that every pair of pipeline stages in the example suite uses Multi-rate Stop & Go Flow Control [4]. Processing steps within a pipeline execute in a data-ow manner. This is indicated in shows the resulting pipelined execution pattern for the audio application. <p> The following three lemmas determine the worst-case end-to-end latency, input and output rates and input and output jitter bounds. These lemmas assume that and stop-and-go ow control between every pair of pipeline stages. Refer to <ref> [4] </ref> for similar bounds for other values of . End-to-end latency for a pipelined application stream k is defined as the worst-case time interval starting when the source pipeline stage becomes eligible for execution until the sink pipeline stage completes execution once. <p> Changing ow control parameters affect latency, input and output rates and jitter only when networks have non-negligible propagation delays and resource clocks are not synchronized. Under these conditions, the phase delay parameter, , can be used to reduce jitter at the expense of latency and vice-versa <ref> [4] </ref>. Changing system configuration parameters can affect end-to-end latency, input and output rates and jitter and resource schedulability. Certain configuration parameters, such as an operating systems timer tick interval, only affect resource schedulability [13].
Reference: [5] <author> S. Chatterjee and J. Strosnider, </author> <title> Distributed Pipeline Scheduling: A Framework for Distributed, Heterogeneous Real-Time System Design, </title> <type> Technical Report, </type> <institution> Carnegie Mellon University, </institution> <month> June </month> <year> 1995. </year> <title> Please email the author at saurav@ece.cmu.edu for a copy. </title>
Reference-contexts: The remainder of this section calculates the worst-case end-to-end latency, I/O rate and jitter bounds and resource schedulability for systems designed using Distributed Pipeline Scheduling techniques. Proofs for the equations can be found in <ref> [5] </ref>. Latency, I/O rate and jitter bounds can each be calculated in O (nS), where n is the number of pipeline stages per distributed pipeline and S is the number of the distributed pipelines per system. <p> 1: Given a set of distributed pipelines, all application timing requirements are met if, , end-to-end latency requirements are met: I/O rate requirements are met: and jitter requirements are met: and all pipeline stage deadlines are met: Proofs for Theorem 1 and Lemmas 1 to 4 can be found in <ref> [5] </ref>. Partially applying Theorem 1 to the example audio/video application suite results in the graphs shown in Figure 11. Graphs for jitter and I/O rates were not shown for space considerations. Figure 11a shows the normalized latency results for the audio/video application suite for both ideal and non-ideal cases.
Reference: [6] <author> S. Daigle and J. Strosnider, </author> <title> Disk Scheduling of Continuous Media Data Streams, </title> <booktitle> SPIE Conference on High-Speed Networking and Multimedia Computing, </booktitle> <year> 1994. </year>
Reference-contexts: The Library provides a set of resource-specific scheduling models describing the timing and concurrency properties of each individual resource available in the Target Platform. The scheduling models will be used in Section 5 for schedulability analysis. Scheduling models for OS/CPUs [13], networks [17], buses [18] and disks <ref> [6] </ref> are available. These models represent a variety of local resource scheduling policies such as rate-monotonic (RMS), earliest-deadline first (EDF), first-in, first-out (FIFO) and round-robin (RR). for an example audio/video application. Buses were excluded from the target platform for simplicity.
Reference: [7] <author> M. Harbour, M. Klein, and J. Lehoczky, </author> <title> Fixed Priority Scheduling of Periodic Tasks with Varying Execution Priority, </title> <booktitle> Proceedings of the 1991 Real-Time Systems Symposium, </booktitle> <month> December </month> <year> 1991. </year>
Reference-contexts: m R), where n is the number of pipeline stages per resource, m is the number of target-specific processing steps per pipeline stage, is the ratio of the periods of the largest pipeline stage to the smallest pipeline stage per resource, and R is the number of resources per system <ref> [7] </ref>. For each pipeline stage i of PAS k, the initial I/O data ratio between pipeline stages i and i+1 is as follows: . The following three lemmas determine the worst-case end-to-end latency, input and output rates and input and output jitter bounds. <p> Comparison to Other Approaches There has been a lot of recent activity in end-to-end design and analysis of real-time systems. A majority of this research has focused on end-to-end analysis over a set of precedence-constrained tasks/ sub-tasks executing on a single resource. Harbour, Klein and Lehoczky <ref> [7] </ref> provided a framework for analyzing an uni-processor resource executing precedence-constrained tasks with varying execution priorities and synchronization requirements. Jeffay [11] introduced a real-time producer/consumer paradigm for expressing task precedence-constraints and for reasoning about timing behavior of programs.
Reference: [8] <author> R. Gerber, S. Hong and M. Saksena, </author> <title> Guaranteeing End-to-End Timing Constraints by Calibrating Intermediate Processes, </title> <booktitle> IEEE Real Time Systems Symposium, </booktitle> <year> 1994. </year>
Reference-contexts: Harbour, Klein and Lehoczky [7] provided a framework for analyzing an uni-processor resource executing precedence-constrained tasks with varying execution priorities and synchronization requirements. Jeffay [11] introduced a real-time producer/consumer paradigm for expressing task precedence-constraints and for reasoning about timing behavior of programs. Gerber, Hong and Saksena <ref> [8] </ref> provided a comprehensive uni-processor design methodology for guaranteeing application end-to-end timing requirements. There have been several papers on multi-resource scheduling. Malcom and Zhao [15], Verma [19], and Sathaye [17] explored end-to-end analysis of multi-hop network resources.
Reference: [9] <author> S. Golestani, </author> <title> Congestion-Free Transmission of Real-Time Traffic in Packet Networks, </title> <booktitle> Proceedings of INFOCOM, </booktitle> <address> San Francisco, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Jitter is defined as the time interval between the completion of two successive instances of a pipeline stage. Furthermore, congestion renders system timing analysis, to be performed in Section 5, intractable <ref> [9] </ref>. To rectify this problem, ow-control regulators may be inserted between every pair of pipeline stages. Flow-controlled service is non-work conserving, i.e., a pipeline stage i can execute for at most time units in any time interval [21]. <p> Flow-controlled service is non-work conserving, i.e., a pipeline stage i can execute for at most time units in any time interval [21]. Various approaches can be used to implement ow-control, including jitter earliest-due-date [20], stop-and-go queueing <ref> [9] </ref>, rate-controlled static priority [22], and hiearchical round robin [12]. Comparisons among these various approaches can be found in [23]. Flow controllers, by guaranteeing that every pipeline stage executes in a strictly periodic manner, eliminate congestion, increase system predictability and reduce jitter [21].
Reference: [10] <author> J. Huang and D. Du, </author> <title> Resource Management for Continuous Multimedia Database Applications, </title> <booktitle> IEEE Real Time Systems Symposium, </booktitle> <year> 1994. </year>
Reference-contexts: Gerber, Hong and Saksena [8] provided a comprehensive uni-processor design methodology for guaranteeing application end-to-end timing requirements. There have been several papers on multi-resource scheduling. Malcom and Zhao [15], Verma [19], and Sathaye [17] explored end-to-end analysis of multi-hop network resources. Huang and Du <ref> [10] </ref> extended the uni-processor real-time consumer/producer paradigm and provided a multi-dimensional bin-packing approach to resource allocation and task scheduling. Limitations included a target platform consisting of a single CPU and a disk, each executing the rate-monotonic scheduling paradigm. No end-to-end timing analysis was provided.
Reference: [11] <author> K. Jeffay, </author> <title> The Real-Time Producer/Consumer Paradigm: A paradigm for the construction of efficient, predictable real-time systems, </title> <booktitle> ACM/SIGAPP Symposium on Applied Computing, </booktitle> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: The first rule eliminates unnecessary buffering and minimizes latency <ref> [11] </ref>. The second rule facilitates the efficient utilization of system resources by enabling applications to execute in a pipelined manner. The third rule is an essential feature for the hierarchical system timing analysis, described in the next section, to work. <p> A majority of this research has focused on end-to-end analysis over a set of precedence-constrained tasks/ sub-tasks executing on a single resource. Harbour, Klein and Lehoczky [7] provided a framework for analyzing an uni-processor resource executing precedence-constrained tasks with varying execution priorities and synchronization requirements. Jeffay <ref> [11] </ref> introduced a real-time producer/consumer paradigm for expressing task precedence-constraints and for reasoning about timing behavior of programs. Gerber, Hong and Saksena [8] provided a comprehensive uni-processor design methodology for guaranteeing application end-to-end timing requirements. There have been several papers on multi-resource scheduling.
Reference: [12] <author> C. Kalmanek, H. Kanakia, and S. Keshav, </author> <title> Rate Controlled Servers for very High-speed Networks, </title> <booktitle> IEEE Global Telecommunications Conference, pp 300.3.1 - 300.3.9, </booktitle> <month> December </month> <year> 1990. </year>
Reference-contexts: Flow-controlled service is non-work conserving, i.e., a pipeline stage i can execute for at most time units in any time interval [21]. Various approaches can be used to implement ow-control, including jitter earliest-due-date [20], stop-and-go queueing [9], rate-controlled static priority [22], and hiearchical round robin <ref> [12] </ref>. Comparisons among these various approaches can be found in [23]. Flow controllers, by guaranteeing that every pipeline stage executes in a strictly periodic manner, eliminate congestion, increase system predictability and reduce jitter [21].
Reference: [13] <author> D. Katcher, H. Arakawa, and J. Strosnider, </author> <title> Engineering and Analysis of Fixed Priority Schedulers, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 19(9), </volume> <month> September </month> <year> 1993. </year>
Reference-contexts: The Library provides a set of resource-specific scheduling models describing the timing and concurrency properties of each individual resource available in the Target Platform. The scheduling models will be used in Section 5 for schedulability analysis. Scheduling models for OS/CPUs <ref> [13] </ref>, networks [17], buses [18] and disks [6] are available. These models represent a variety of local resource scheduling policies such as rate-monotonic (RMS), earliest-deadline first (EDF), first-in, first-out (FIFO) and round-robin (RR). for an example audio/video application. Buses were excluded from the target platform for simplicity. <p> Changing system configuration parameters can affect end-to-end latency, input and output rates and jitter and resource schedulability. Certain configuration parameters, such as an operating systems timer tick interval, only affect resource schedulability <ref> [13] </ref>. Other parameters, such as network packet size, affect the period of a pipeline stage and hence affect latency, I/O rates and jitter and resource schedulability.
Reference: [14] <author> D. Katcher, </author> <title> Engineering and Analysis of Real-Time Operating Systems, </title> <type> Ph.D. Thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1994. </year>
Reference: [15] <author> N. Malcom and W. Zhao, </author> <title> Guaranteeing Synchronous Messages with Arbitrary Deadline Constraints in an FDDI Network, </title> <booktitle> IEEE Conf. on Local Computer Networks, </booktitle> <year> 1993. </year>
Reference-contexts: Jeffay [11] introduced a real-time producer/consumer paradigm for expressing task precedence-constraints and for reasoning about timing behavior of programs. Gerber, Hong and Saksena [8] provided a comprehensive uni-processor design methodology for guaranteeing application end-to-end timing requirements. There have been several papers on multi-resource scheduling. Malcom and Zhao <ref> [15] </ref>, Verma [19], and Sathaye [17] explored end-to-end analysis of multi-hop network resources. Huang and Du [10] extended the uni-processor real-time consumer/producer paradigm and provided a multi-dimensional bin-packing approach to resource allocation and task scheduling.
Reference: [16] <author> M. Natale and J. Stankovic, </author> <title> Dynamic End-to-End Guarantees in Distributed Real Time Systems, </title> <booktitle> IEEE Real Time Systems Symposium, </booktitle> <year> 1994. </year>
Reference-contexts: Limitations included a target platform consisting of a single CPU and a disk, each executing the rate-monotonic scheduling paradigm. No end-to-end timing analysis was provided. Natale and Stankovic <ref> [16] </ref> provided a dual off-line/on-line framework for guaranteeing deadlines for processes communicating via synchronous primitives. Bettati and Liu [1] provided a set of heuristics for resource allocation and end-to-end stream analysis for homogeneous multiprocessor systems. Applications were scheduled in a ow-shop/data-ow manner.
Reference: [17] <author> S. Sathaye, </author> <title> Scheduling Real-Time Traffic in Packet Switched Networks, </title> <type> Ph.D. Thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: The Library provides a set of resource-specific scheduling models describing the timing and concurrency properties of each individual resource available in the Target Platform. The scheduling models will be used in Section 5 for schedulability analysis. Scheduling models for OS/CPUs [13], networks <ref> [17] </ref>, buses [18] and disks [6] are available. These models represent a variety of local resource scheduling policies such as rate-monotonic (RMS), earliest-deadline first (EDF), first-in, first-out (FIFO) and round-robin (RR). for an example audio/video application. Buses were excluded from the target platform for simplicity. <p> Because multiple distributed pipeline stages may execute on a shared resource, resource schedulability analysis can determine if all pipeline deadlines are indeed met. This paper uses the Degree of Schedulable Saturation <ref> [17] </ref>, , as the resource schedulability metric. However, the Framework is able to accommodate any other non-binary resource schedulability metric. <p> Gerber, Hong and Saksena [8] provided a comprehensive uni-processor design methodology for guaranteeing application end-to-end timing requirements. There have been several papers on multi-resource scheduling. Malcom and Zhao [15], Verma [19], and Sathaye <ref> [17] </ref> explored end-to-end analysis of multi-hop network resources. Huang and Du [10] extended the uni-processor real-time consumer/producer paradigm and provided a multi-dimensional bin-packing approach to resource allocation and task scheduling. Limitations included a target platform consisting of a single CPU and a disk, each executing the rate-monotonic scheduling paradigm.
Reference: [18] <author> E. Snow, </author> <title> Engineering FutureBus+ IEEE 896.1 for Real-Time Applications, M.S. </title> <type> Thesis, CMU, </type> <month> December </month> <year> 1992. </year>
Reference-contexts: The Library provides a set of resource-specific scheduling models describing the timing and concurrency properties of each individual resource available in the Target Platform. The scheduling models will be used in Section 5 for schedulability analysis. Scheduling models for OS/CPUs [13], networks [17], buses <ref> [18] </ref> and disks [6] are available. These models represent a variety of local resource scheduling policies such as rate-monotonic (RMS), earliest-deadline first (EDF), first-in, first-out (FIFO) and round-robin (RR). for an example audio/video application. Buses were excluded from the target platform for simplicity.
Reference: [19] <author> D. Verma, </author> <title> Guaranteed Performance Communication in High Speed Networks, </title> <type> Ph.D. Thesis, </type> <institution> University of California at Berkeley, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: Jeffay [11] introduced a real-time producer/consumer paradigm for expressing task precedence-constraints and for reasoning about timing behavior of programs. Gerber, Hong and Saksena [8] provided a comprehensive uni-processor design methodology for guaranteeing application end-to-end timing requirements. There have been several papers on multi-resource scheduling. Malcom and Zhao [15], Verma <ref> [19] </ref>, and Sathaye [17] explored end-to-end analysis of multi-hop network resources. Huang and Du [10] extended the uni-processor real-time consumer/producer paradigm and provided a multi-dimensional bin-packing approach to resource allocation and task scheduling.
Reference: [20] <author> D. Verma, H. Zhang, and D. Ferrari, </author> <title> Guaranteeing Delay Jitter Bounds in Packet-switched Networks, </title> <booktitle> Proceedings of Tricomm 91, </booktitle> <address> pp35-46, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: To rectify this problem, ow-control regulators may be inserted between every pair of pipeline stages. Flow-controlled service is non-work conserving, i.e., a pipeline stage i can execute for at most time units in any time interval [21]. Various approaches can be used to implement ow-control, including jitter earliest-due-date <ref> [20] </ref>, stop-and-go queueing [9], rate-controlled static priority [22], and hiearchical round robin [12]. Comparisons among these various approaches can be found in [23]. Flow controllers, by guaranteeing that every pipeline stage executes in a strictly periodic manner, eliminate congestion, increase system predictability and reduce jitter [21].
Reference: [21] <author> H. Zhang and D. Ferrari, </author> <title> Rate-Controlled Service Disciplines, Journal of High Speed Networks 3(4), </title> <year> 1994. </year>
Reference-contexts: To rectify this problem, ow-control regulators may be inserted between every pair of pipeline stages. Flow-controlled service is non-work conserving, i.e., a pipeline stage i can execute for at most time units in any time interval <ref> [21] </ref>. Various approaches can be used to implement ow-control, including jitter earliest-due-date [20], stop-and-go queueing [9], rate-controlled static priority [22], and hiearchical round robin [12]. Comparisons among these various approaches can be found in [23]. <p> Comparisons among these various approaches can be found in [23]. Flow controllers, by guaranteeing that every pipeline stage executes in a strictly periodic manner, eliminate congestion, increase system predictability and reduce jitter <ref> [21] </ref>. Previous work on ow control has been mostly used in the network domain where all packets are assumed to be of the same size. In heterogeneous systems described in this paper, where adjacent pipeline stages execute at different rates, multi-rate ow control must be used.
Reference: [22] <author> H. Zhang and D. Ferrari, </author> <title> Rate-controlled Static Priority Queueing, </title> <booktitle> Proceedings of IEEE INFOCOM 93, </booktitle> <pages> pp 227-236, </pages> <month> April </month> <year> 1993. </year> <month> - 19 </month> - 
Reference-contexts: Flow-controlled service is non-work conserving, i.e., a pipeline stage i can execute for at most time units in any time interval [21]. Various approaches can be used to implement ow-control, including jitter earliest-due-date [20], stop-and-go queueing [9], rate-controlled static priority <ref> [22] </ref>, and hiearchical round robin [12]. Comparisons among these various approaches can be found in [23]. Flow controllers, by guaranteeing that every pipeline stage executes in a strictly periodic manner, eliminate congestion, increase system predictability and reduce jitter [21].
Reference: [23] <author> H. Zhang and S. Keshav, </author> <title> Comparison of Rate-based Service Disciplines, </title> <booktitle> Proceedings of ACM SIGCOMM 91, </booktitle> <pages> pp 113-122, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Various approaches can be used to implement ow-control, including jitter earliest-due-date [20], stop-and-go queueing [9], rate-controlled static priority [22], and hiearchical round robin [12]. Comparisons among these various approaches can be found in <ref> [23] </ref>. Flow controllers, by guaranteeing that every pipeline stage executes in a strictly periodic manner, eliminate congestion, increase system predictability and reduce jitter [21]. Previous work on ow control has been mostly used in the network domain where all packets are assumed to be of the same size.
References-found: 23

