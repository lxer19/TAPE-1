URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-98-03/MP-TR-98-03.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-98-03/
Root-URL: http://www.cs.wisc.edu
Email: paulb@cs.wisc.edu  olvi@cs.wisc.edu  
Title: Feature Selection via Concave Minimization and Support Vector Machines  
Author: P. S. Bradley O. L. Mangasarian 
Address: Madison, WI 53706  Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin  Computer Sciences Department University of Wisconsin  
Abstract: Computational comparison is made between two feature selection approaches for finding a separating plane that discriminates between two point sets in an n-dimensional feature space that utilizes as few of the n features (dimensions) as possible. In the concave minimization approach [19, 5] a separating plane is generated by minimizing a weighted sum of distances of misclassified points to two parallel planes that bound the sets and which determine the separating plane midway between them. Furthermore, the number of dimensions of the space used to determine the plane is minimized. In the support vector machine approach [27, 7, 1, 10, 24, 28], in addition to minimizing the weighted sum of distances of misclassified points to the bounding planes, we also maximize the distance between the two bounding planes that generate the separating plane. Computational results show that feature suppression is an indirect consequence of the support vector machine approach when an appropriate norm is used. Numerical tests on 6 public data sets show that classifiers trained by the concave minimization approach and those trained by a support vector machine have comparable 10-fold cross-validation correctness. However, in all data sets tested, the classifiers obtained by the concave minimization approach selected fewer problem features than those trained by a support vector machine. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. P. Bennett and J. A. </author> <title> Blue. A support vector machine approach to decision trees. </title> <institution> Department of Mathematical Sciences Math Report No. 97-100, Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1997. </year> <note> http://www.math.rpi.edu/ bennek/. </note>
Reference-contexts: The fi-nal separating plane is taken midway between the two bounding parallel planes. The second approach, that of a support vector machine <ref> [27, 7, 1, 10, 24, 28] </ref>, described in Section 3, constructs two parallel bounding planes in n-dimensional space R n as in the first approach outlined above, but in addition attempts to push these planes as far apart as possible. <p> We have found useful solutions to (8) for the fixed value ff = 5 [5, 4]. Another approach, involving more computation, is to solve (8) for an increasing sequence of ff values. 3 SVM: FEATURE SELECTION VIA SUPPORT VECTOR MACHINES The support vector machine idea <ref> [27, 1, 10, 24, 28] </ref>, although not originally intended as a feature selection tool, does in fact indirectly suppress components of the normal vector w to the separating plane P (1) when an appropriate norm is used for measuring the distance between the two parallel bounding planes for the sets being <p> Usually the support vector machine problem is formulated using the 2-norm in the objective <ref> [27, 1] </ref>. Since the 2-norm is dual to itself, it follows that the distance between the parallel planes defining the separating surface is also measured in the 2-norm when this formulation is used.
Reference: [2] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: Two principal reasons for choosing the 1-norm in (3) are: (1) problem (3) is then reducible to a linear program (4) with many important theoretical properties making it an effective computational tool <ref> [2] </ref>, (2) the 1-norm is less sensitive to outliers such as those occurring when the underlying data distributions have pronounced tails, hence (3) has a similar effect to that of robust regression [13],[11, pp 82-87]. The formulation (3) is equivalent to the following robust linear programming formulation (RLP) proposed in [2] <p> <ref> [2] </ref>, (2) the 1-norm is less sensitive to outliers such as those occurring when the underlying data distributions have pronounced tails, hence (3) has a similar effect to that of robust regression [13],[11, pp 82-87]. The formulation (3) is equivalent to the following robust linear programming formulation (RLP) proposed in [2] and effectively used to solve problems from real world domains [21]: minimize w;fl;y;z m + e T z subject to Aw + efl + e y; y 0; z 0: The linear program (4) or, equivalently, the formulation (3), define a separating plane P that approximately satisfies the conditions (2) <p> In contrast, the formulations (4) and (8) measure average separation error. Minimizing average separation error in (4) ensures that the solution w = 0 occurs iff e T A = k , in which case it is not unique <ref> [2, Theorem 2.5] </ref>. We turn our attention now to computational testing and comparison. 4 COMPUTATIONAL RESULTS 4.1 DATA SETS The Wisconsin Prognostic Breast Cancer Database consists of 198 instances with 35 features represent ing follow-up data for one breast cancer case [23]. We used 2 variants of this data set.
Reference: [3] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: Hence the importance of the feature selection problem in classification [15]. The optimization formulations in Section 2 exploit one realization of the Occam's Razor bias <ref> [3] </ref>: compute a separating plane with a small number of predictive features, discarding irrelevant or redundant features. These formulations can be considered wrapper models as defined in [14]. The first approach [19, 5], described in Section 2, involves the minimization of a concave function on a polyhedral set.

Reference: [5] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Feature selection via mathematical programming. </title> <journal> INFORMS Journal on Computing, </journal> <note> 1998. To appear. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-21.ps.Z. </note>
Reference-contexts: The optimization formulations in Section 2 exploit one realization of the Occam's Razor bias [3]: compute a separating plane with a small number of predictive features, discarding irrelevant or redundant features. These formulations can be considered wrapper models as defined in [14]. The first approach <ref> [19, 5] </ref>, described in Section 2, involves the minimization of a concave function on a polyhedral set. A plane is constructed such that a weighted sum of distances of misclassified points to the plane is minimized and as few dimensions of the original feature space R n are used. <p> plane that attempts to separate R n into two halfspaces such that each open halfspace contains points mostly of A or B. 2 FSV: FEATURE SELECTION VIA CONCAVE MINIMIZATION In this part of the paper we describe a feature selection procedure that has been effective in medical and other applications <ref> [5, 19] </ref>. <p> Thus the objective function of the linear program (4) minimizes the average sum of distances, weighted by kwk 0 , of misclassified points to the bounding planes. The separating plane P (1) is midway between the two bounding planes and parallel to them. Feature selection <ref> [19, 5] </ref> is imposed by attempting to suppress as many components of the normal vector w to the separating plane P that is consistent with obtaining an acceptable separation between the sets A and B. <p> We note that this problem is the minimization of a concave objective function over a polyhedral set. Even though it is difficult to find a global solution to this problem, a fast successive linear approximation (SLA) algorithm <ref> [5, Algorithm 2.1] </ref> terminates finitely (usually in 5 to 7 steps) at a stationary point which satisfies the minimum principle necessary optimality condition for problem (8) [5, Theorem 2.2] and leads to a sparse w with good generalization properties. For convenience we state the SLA algorithm below. <p> Even though it is difficult to find a global solution to this problem, a fast successive linear approximation (SLA) algorithm [5, Algorithm 2.1] terminates finitely (usually in 5 to 7 steps) at a stationary point which satisfies the minimum principle necessary optimality condition for problem (8) <ref> [5, Theorem 2.2] </ref> and leads to a sparse w with good generalization properties. For convenience we state the SLA algorithm below. Algorithm 2.1 Successive Linearization Algorithm (SLA) for FSV (8). Choose 2 [0; 1). <p> The parameter was chosen to "maximize" generalization performance. We have found useful solutions to (8) for the fixed value ff = 5 <ref> [5, 4] </ref>.
Reference: [6] <author> C. J. C. Burges. </author> <title> A tutorial on support vector ma chines. Data Mining and Knowledge Discovery, </title> <type> 2, </type> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: In practice, one usually solves (14) by way of its dual [18]. In this formulation, the data enter only as inner products which are computed in the transformed space via a kernel function K (x; y) = (x) (y) <ref> [6, 27, 28] </ref>. We note that separation errors in (12) - (14) are weighted equally conforming to the SVM formulations in [6, 27]. In contrast, the formulations (4) and (8) measure average separation error. <p> In this formulation, the data enter only as inner products which are computed in the transformed space via a kernel function K (x; y) = (x) (y) [6, 27, 28]. We note that separation errors in (12) - (14) are weighted equally conforming to the SVM formulations in <ref> [6, 27] </ref>. In contrast, the formulations (4) and (8) measure average separation error. Minimizing average separation error in (4) ensures that the solution w = 0 occurs iff e T A = k , in which case it is not unique [2, Theorem 2.5].
Reference: [7] <author> C. Cortes and V. Vapnik. </author> <title> Support vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 273-279, </pages> <year> 1995. </year>
Reference-contexts: The fi-nal separating plane is taken midway between the two bounding parallel planes. The second approach, that of a support vector machine <ref> [27, 7, 1, 10, 24, 28] </ref>, described in Section 3, constructs two parallel bounding planes in n-dimensional space R n as in the first approach outlined above, but in addition attempts to push these planes as far apart as possible.
Reference: [8] <author> CPLEX Optimization Inc., </author> <title> Incline Village, Nevada. Using the CPLEX(TM) Linear Opti mizer and CPLEX(TM) Mixed Integer Optimizer (Version 2.0), </title> <year> 1992. </year>
Reference-contexts: These features are deemed relevant to the prognosis problem. All linear programs formulations were solved using the CPLEX package <ref> [8] </ref> called from within MATLAB [22]. The quadratic programming problem (14) was solved using MATLAB's quadratic optimization solver, which encountered difficulty on conditioning the QP constraint matrix, which may affect the interpretation of the results for this approach.
Reference: [9] <author> T. G. Dietterich. </author> <title> Approximate statistical tests for comparing supervised classification learning algo rithms. </title> <booktitle> Neural Computation, </booktitle> <year> 1997. </year> <note> To appear. http://www.cs.orst.edu/ tgd/cv/pubs.html. </note>
Reference-contexts: t-test to 10-fold cross validation results may indicate a difference in the average test 1 Specifically, this is the p-value of a two-tailed paired t-test testing the hypothesis that the difference in "Test" correctnesses for the FSV and SVM 1-norm classifiers is zero set correctness when one is not present <ref> [9] </ref>. Thus the results of these experiments may be more similar than indicated by the p-values.
Reference: [10] <author> F. Girosi. </author> <title> An equivalence between sparse approximation and support vector machines. </title> <journal> A.I. </journal> <volume> Memo 1606, </volume> <booktitle> Artificial Intelligence Laboratory, </booktitle> <publisher> MIT, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1997. </year> <note> http://www.ai.mit.edu/people/girosi/home-page/svm.html. </note>
Reference-contexts: The fi-nal separating plane is taken midway between the two bounding parallel planes. The second approach, that of a support vector machine <ref> [27, 7, 1, 10, 24, 28] </ref>, described in Section 3, constructs two parallel bounding planes in n-dimensional space R n as in the first approach outlined above, but in addition attempts to push these planes as far apart as possible. <p> We have found useful solutions to (8) for the fixed value ff = 5 [5, 4]. Another approach, involving more computation, is to solve (8) for an increasing sequence of ff values. 3 SVM: FEATURE SELECTION VIA SUPPORT VECTOR MACHINES The support vector machine idea <ref> [27, 1, 10, 24, 28] </ref>, although not originally intended as a feature selection tool, does in fact indirectly suppress components of the normal vector w to the separating plane P (1) when an appropriate norm is used for measuring the distance between the two parallel bounding planes for the sets being
Reference: [11] <author> M. H. Hassoun. </author> <title> Fundamentals of Artificial Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year> <title> Table 3: Average running: Ionosphere data set. Method Time (Seconds) Algorithm 2.1 30.94 k k 1 (13) 3.09 k k 2 RLP (4) 1.28 </title>
Reference: [12] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: Classification performance is determined by the inherent class information available in the features provided. It seems logical to conclude that a large number of features would provide more discriminating ability. But, with a finite training sample, a high-dimensional feature space is almost empty <ref> [12] </ref> and many separators may perform well on the training data, but few may generalize well. Hence the importance of the feature selection problem in classification [15].
Reference: [13] <author> P. J. Huber. </author> <title> Robust Statistics. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1981. </year>
Reference: [14] <author> G. H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The optimization formulations in Section 2 exploit one realization of the Occam's Razor bias [3]: compute a separating plane with a small number of predictive features, discarding irrelevant or redundant features. These formulations can be considered wrapper models as defined in <ref> [14] </ref>. The first approach [19, 5], described in Section 2, involves the minimization of a concave function on a polyhedral set.
Reference: [15] <author> D. Koller and M. Sahami. </author> <title> Toward optimal feature selection. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning-Proceedings of the Thirteenth International Conference (ICML '96)-Bari, </booktitle> <address> Italy July 3-6, </address> <year> 1996, </year> <pages> pages 284-292, </pages> <address> San Francisco, CA, 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: But, with a finite training sample, a high-dimensional feature space is almost empty [12] and many separators may perform well on the training data, but few may generalize well. Hence the importance of the feature selection problem in classification <ref> [15] </ref>. The optimization formulations in Section 2 exploit one realization of the Occam's Razor bias [3]: compute a separating plane with a small number of predictive features, discarding irrelevant or redundant features. These formulations can be considered wrapper models as defined in [14].
Reference: [16] <author> O. L. Mangasarian. </author> <title> Linear and nonlinear separation of patterns by linear programming. </title> <journal> Operations Research, </journal> <volume> 13 </volume> <pages> 444-452, </pages> <year> 1965. </year>
Reference-contexts: to the objective of (11) resulting in the following quadratic program: minimize w;fl;y;z 2 w T w subject to Aw + efl + e y; y 0; z 0: Nonlinear separating surfaces, which are linear in their parameters, can also easily be handled by the formulations (8), (12) and (13) <ref> [16] </ref>. If the data are mapped nonlinearly via : R n ! R ` , a nonlinear separating surface in R n is easily computed as a linear separator in R ` . In practice, one usually solves (14) by way of its dual [18].
Reference: [17] <author> O. L. Mangasarian. </author> <title> Multi-surface method of pattern separation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:801-807, </volume> <year> 1968. </year>
Reference-contexts: the 1-norm and accordingly kwk 0 = kwk 1 in (11) which leads to the following linear programming formulation: minimize w;fl;y;z;s 2 e T s subject to Aw + efl + e y; s w s; (13) We note that the first paper on the multisurface method on pattern separation <ref> [17] </ref> also proposed and implemented, just as does the support vector machine approach, forcing the two parallel planes that bound the sets to be separated to be as far apart as possible. Usually the support vector machine problem is formulated using the 2-norm in the objective [27, 1].
Reference: [18] <author> O. L. Mangasarian. </author> <title> Nonlinear Programming. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1969. </year> <note> Reprint: SIAM Classic in Applied Mathematics 10, 1994, Philadelphia. </note>
Reference-contexts: If the data are mapped nonlinearly via : R n ! R ` , a nonlinear separating surface in R n is easily computed as a linear separator in R ` . In practice, one usually solves (14) by way of its dual <ref> [18] </ref>. In this formulation, the data enter only as inner products which are computed in the transformed space via a kernel function K (x; y) = (x) (y) [6, 27, 28].
Reference: [19] <author> O. L. Mangasarian. </author> <title> Machine learning via polyhedral concave minimization. </title> <editor> In H. Fis-cher, B. Riedmueller, and S. Schae*er, editors, </editor> <booktitle> Applied Mathematics and Parallel Computing - Festschrift for Klaus Ritter, </booktitle> <pages> pages 175-188. </pages> <publisher> Physica-Verlag A Springer-Verlag Company, </publisher> <address> Heidelberg, </address> <year> 1996. </year> <month> ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-20.ps.Z. </month>
Reference-contexts: The optimization formulations in Section 2 exploit one realization of the Occam's Razor bias [3]: compute a separating plane with a small number of predictive features, discarding irrelevant or redundant features. These formulations can be considered wrapper models as defined in [14]. The first approach <ref> [19, 5] </ref>, described in Section 2, involves the minimization of a concave function on a polyhedral set. A plane is constructed such that a weighted sum of distances of misclassified points to the plane is minimized and as few dimensions of the original feature space R n are used. <p> plane that attempts to separate R n into two halfspaces such that each open halfspace contains points mostly of A or B. 2 FSV: FEATURE SELECTION VIA CONCAVE MINIMIZATION In this part of the paper we describe a feature selection procedure that has been effective in medical and other applications <ref> [5, 19] </ref>. <p> Thus the objective function of the linear program (4) minimizes the average sum of distances, weighted by kwk 0 , of misclassified points to the bounding planes. The separating plane P (1) is midway between the two bounding planes and parallel to them. Feature selection <ref> [19, 5] </ref> is imposed by attempting to suppress as many components of the normal vector w to the separating plane P that is consistent with obtaining an acceptable separation between the sets A and B. <p> Because of the discontinuity of the step function term e T v fl , we approximate it by a concave exponential on the nonnegative real line <ref> [19] </ref>.
Reference: [20] <author> O. L. Mangasarian. </author> <title> Arbitrary-norm separating plane. </title> <type> Technical Report 97-07, </type> <institution> Computer Sciences Department, University of Wisconsin, Madi-son, Wisconsin, </institution> <month> May </month> <year> 1997. </year> <note> Operations Research Letters, submitted. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/97-07.ps.Z. </note>
Reference-contexts: 2 R mfin and B 2 R kfin respectively, we wish to discriminate between them by a separating plane: P = fx j x 2 R n ; x T w = flg; (1) with normal w 2 R n and 1-norm distance to the origin of jflj kwk 1 <ref> [20] </ref>. <p> Each positive value of y i determines the distance y i kwk 0 <ref> [20, Theorem 2.2] </ref> between a point A i of A lying on the wrong side of the bounding plane x T w = fl + 1 for A, that is A i lying in the open halfs pace fx fi x T w &lt; fl + 1 g; and the bounding <p> The distance, measured by some norm k k on R n , between these planes is precisely 2 kwk 0 <ref> [20, Theorem 2.2] </ref>. The appended term to the objective function of the RLP (4), 2 , is the reciprocal of this distance, thus driving the distance between these two planes up to obtain better separation.
Reference: [21] <author> O. L. Mangasarian, W. N. Street, and W. H. Wolberg. </author> <title> Breast cancer diagnosis and prognosis via linear programming. </title> <journal> Operations Research, </journal> <volume> 43(4) </volume> <pages> 570-577, </pages> <month> July-August </month> <year> 1995. </year>
Reference-contexts: The formulation (3) is equivalent to the following robust linear programming formulation (RLP) proposed in [2] and effectively used to solve problems from real world domains <ref> [21] </ref>: minimize w;fl;y;z m + e T z subject to Aw + efl + e y; y 0; z 0: The linear program (4) or, equivalently, the formulation (3), define a separating plane P that approximately satisfies the conditions (2) in the following sense.
Reference: [22] <author> MATLAB. </author> <title> User's Guide. The MathWorks, </title> <publisher> Inc., </publisher> <year> 1992. </year>
Reference-contexts: These features are deemed relevant to the prognosis problem. All linear programs formulations were solved using the CPLEX package [8] called from within MATLAB <ref> [22] </ref>. The quadratic programming problem (14) was solved using MATLAB's quadratic optimization solver, which encountered difficulty on conditioning the QP constraint matrix, which may affect the interpretation of the results for this approach.
Reference: [23] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. </title> <type> Technical report, </type> <institution> Department of Information and Computer Science, University of California, Irvine, </institution> <year> 1992. </year> <note> www.ics.uci.edu/ mlearn/MLRepository.html. </note>
Reference-contexts: We turn our attention now to computational testing and comparison. 4 COMPUTATIONAL RESULTS 4.1 DATA SETS The Wisconsin Prognostic Breast Cancer Database consists of 198 instances with 35 features represent ing follow-up data for one breast cancer case <ref> [23] </ref>. We used 2 variants of this data set. <p> Elements of A corresponds to patients with a cancer recurrence in less than 60 months (41 points) and B corresponds to patients which cancer had not recurred in less than 60 months (69 points). The Johns Hopkins University Ionosphere data set consists of 34 continuous features of 351 instances <ref> [23] </ref>. Each instance represents a radar return from the ionosphere. The set A consists of 225 radar returns termed "good" or showing some type of structure in the ionosphere. The set B consists of 126 radar returns termed "bad"; their signals pass through the ionosphere. <p> The set B consists of 126 radar returns termed "bad"; their signals pass through the ionosphere. The Cleveland Heart Disease data set consists of 297 instance with 13 features (see documentation <ref> [23] </ref>). Set A consist of 214 instance. The set B consists of 83 instances. vector machine (13) versus the sparsity-inducing parameter on the WPBC (24 months) data set. Dashed = "tuning" correctness, Solid = test correctness. <p> The set B consists of 83 instances. vector machine (13) versus the sparsity-inducing parameter on the WPBC (24 months) data set. Dashed = "tuning" correctness, Solid = test correctness. The Pima Indians Diabetes data set consists of 768 instances with 8 features plus a class label (see documentation <ref> [23] </ref>). The 500 instances with class label "0" were place in A, the 268 instances with class label "1" were placed in B. The BUPA Liver Disorders data set consists of 345 instances with 6 features plus a selector field used to split the data into 2 sets (see documentation [23]). <p> <ref> [23] </ref>). The 500 instances with class label "0" were place in A, the 268 instances with class label "1" were placed in B. The BUPA Liver Disorders data set consists of 345 instances with 6 features plus a selector field used to split the data into 2 sets (see documentation [23]).
Reference: [24] <author> E. Osuna, R. Freund, and F. Girosi. </author> <title> Training support vector machines: an application to face detection. </title> <booktitle> In IEEE Confer-nece on Computer Vision and Pattern Recognition, </booktitle> <address> Puerto Rico, </address> <month> June </month> <year> 1997, </year> <pages> 130-136, </pages> <year> 1997. </year> <note> http://www.ai.mit.edu/people/girosi/home-page/svm.html. </note>
Reference-contexts: The fi-nal separating plane is taken midway between the two bounding parallel planes. The second approach, that of a support vector machine <ref> [27, 7, 1, 10, 24, 28] </ref>, described in Section 3, constructs two parallel bounding planes in n-dimensional space R n as in the first approach outlined above, but in addition attempts to push these planes as far apart as possible. <p> We have found useful solutions to (8) for the fixed value ff = 5 [5, 4]. Another approach, involving more computation, is to solve (8) for an increasing sequence of ff values. 3 SVM: FEATURE SELECTION VIA SUPPORT VECTOR MACHINES The support vector machine idea <ref> [27, 1, 10, 24, 28] </ref>, although not originally intended as a feature selection tool, does in fact indirectly suppress components of the normal vector w to the separating plane P (1) when an appropriate norm is used for measuring the distance between the two parallel bounding planes for the sets being <p> These points are the only data points that are relevant for determining the optimal separating plane. Their number is usually small and it is proportional to the generalization error of the classifier <ref> [24] </ref>.
Reference: [25] <author> J. W. Shavlik and T. G. Dietterich (editors). </author> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: Choosing = 0 will maximize the training correctness of the resulting classifier, but often this classifier performs poorly on data not in the training set <ref> [25] </ref>.
Reference: [26] <author> M. Stone. </author> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 36 </volume> <pages> 111-147, </pages> <year> 1974. </year>
Reference-contexts: RLP, which underlies the proposed feature selection methods here, has no feature suppression capability built in. We measure generalization ability by 10-fold cross-validation <ref> [26] </ref>. Numerical tests on 6 public data sets show that classifiers trained by the concave minimization approach and those trained by a support vector machine have comparable 10-fold cross-validation correctness. <p> We estimate the generalization ability of a classifier via 10-fold cross-validation <ref> [26] </ref>. We note that the objective function parameter , which can induce sparsity, must be chosen carefully to maximize the generalization ability of the resulting classifier.
Reference: [27] <author> V. N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: The fi-nal separating plane is taken midway between the two bounding parallel planes. The second approach, that of a support vector machine <ref> [27, 7, 1, 10, 24, 28] </ref>, described in Section 3, constructs two parallel bounding planes in n-dimensional space R n as in the first approach outlined above, but in addition attempts to push these planes as far apart as possible. <p> The justification for this, apart from reducing the VC dimension <ref> [27] </ref> which in turn improves generalization, is that for the linearly separable case, the further apart the planes, the smaller the halfspace assigned to each of the two sets, reducing the possibility that new unseen points from the wrong set lie in that halfspace. <p> We have found useful solutions to (8) for the fixed value ff = 5 [5, 4]. Another approach, involving more computation, is to solve (8) for an increasing sequence of ff values. 3 SVM: FEATURE SELECTION VIA SUPPORT VECTOR MACHINES The support vector machine idea <ref> [27, 1, 10, 24, 28] </ref>, although not originally intended as a feature selection tool, does in fact indirectly suppress components of the normal vector w to the separating plane P (1) when an appropriate norm is used for measuring the distance between the two parallel bounding planes for the sets being <p> Usually the support vector machine problem is formulated using the 2-norm in the objective <ref> [27, 1] </ref>. Since the 2-norm is dual to itself, it follows that the distance between the parallel planes defining the separating surface is also measured in the 2-norm when this formulation is used. <p> In practice, one usually solves (14) by way of its dual [18]. In this formulation, the data enter only as inner products which are computed in the transformed space via a kernel function K (x; y) = (x) (y) <ref> [6, 27, 28] </ref>. We note that separation errors in (12) - (14) are weighted equally conforming to the SVM formulations in [6, 27]. In contrast, the formulations (4) and (8) measure average separation error. <p> In this formulation, the data enter only as inner products which are computed in the transformed space via a kernel function K (x; y) = (x) (y) [6, 27, 28]. We note that separation errors in (12) - (14) are weighted equally conforming to the SVM formulations in <ref> [6, 27] </ref>. In contrast, the formulations (4) and (8) measure average separation error. Minimizing average separation error in (4) ensures that the solution w = 0 occurs iff e T A = k , in which case it is not unique [2, Theorem 2.5].
Reference: [28] <author> G. Wahba. </author> <title> Support vector machines, reproducing kernel Hilbert spaces and the randomized gacv. </title> <type> Technical report no. 984, </type> <institution> Department of Statistics, University of Wisconsin, Madison, WI 53706, </institution> <year> 1997. </year> <month> ftp://ftp.stat.wisc.edu/pub/wahba/index.html. </month>
Reference-contexts: The fi-nal separating plane is taken midway between the two bounding parallel planes. The second approach, that of a support vector machine <ref> [27, 7, 1, 10, 24, 28] </ref>, described in Section 3, constructs two parallel bounding planes in n-dimensional space R n as in the first approach outlined above, but in addition attempts to push these planes as far apart as possible. <p> We have found useful solutions to (8) for the fixed value ff = 5 [5, 4]. Another approach, involving more computation, is to solve (8) for an increasing sequence of ff values. 3 SVM: FEATURE SELECTION VIA SUPPORT VECTOR MACHINES The support vector machine idea <ref> [27, 1, 10, 24, 28] </ref>, although not originally intended as a feature selection tool, does in fact indirectly suppress components of the normal vector w to the separating plane P (1) when an appropriate norm is used for measuring the distance between the two parallel bounding planes for the sets being <p> In practice, one usually solves (14) by way of its dual [18]. In this formulation, the data enter only as inner products which are computed in the transformed space via a kernel function K (x; y) = (x) (y) <ref> [6, 27, 28] </ref>. We note that separation errors in (12) - (14) are weighted equally conforming to the SVM formulations in [6, 27]. In contrast, the formulations (4) and (8) measure average separation error.
References-found: 27

