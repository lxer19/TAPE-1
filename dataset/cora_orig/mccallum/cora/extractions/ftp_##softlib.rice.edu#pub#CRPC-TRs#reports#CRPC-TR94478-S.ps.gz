URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94478-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Trust-region interior-point algorithms for minimization problems with simple bounds  
Author: J. E. Dennis Lus N. Vicente 
Keyword: trust-region methods, interior-point algorithms, Dikin-Karmarkar ellipsoid, Coleman and Li scaling, simple bounds.  
Note: AMS subject classification. 49M37, 90C20, 90C30  
Abstract: Two trust-region interior-point algorithms for the solution of minimization problems with simple bounds are presented. The algorithms scale the local model in a way proposed by Coleman and Li [1], but they are new otherwise. The first algorithm is more usual in that the trust region and the local quadratic model are consistently scaled. The second algorithm proposed here uses an unscaled trust region. A first-order convergence result for these algorithms is given and dogleg and conjugate-gradient algorithms to compute trial steps are introduced. Some numerical examples that show the advantages of the the second algorithm are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. F. Coleman and Y. Li. </author> <title> An interior trust region approach for nonlinear minimization subject to bounds. </title> <type> Technical Report TR93-1342, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1993. </year>
Reference-contexts: We assume that f is continuously differentiable in the box B = fx 2 &lt; n : a x bg. Coleman and Li <ref> [1] </ref> give an elegant diagonal scaling for this problem. It has the flavor of the Dikin-Karmarkar scaling, but it has a direct connection to the first-order necessary optimality conditions. A diagonal element corresponding to what appears to be a constraining bound is the same as in the Dikin-Karmarkar scaling. <p> Here we follow the concept of Coleman and Li <ref> [1] </ref> and choose D k as D (x k ). They actually use D k = D (x k ) 1 2 . However, either choice has the effect of only rescaling those components that appear, from the sign of the gradient, to threaten to restrict the step. <p> In other words, we would choose S k = D k , and the shape of the trust region (3) would be ellipsoidal in the original basis. We note that the choice made by Coleman and Li <ref> [1] </ref> is S k = D 2 This substitution of one ellipsoidal constraints for all the bound constraints was a prime motivation for interior-point methods. <p> If x k is near a bound which is not active at x fl , then many iterations may be required to move off that bound. Hence we choose S k to be the identity in the second algorithm. It is important to point out that Coleman and Li <ref> [1] </ref> present a different motivation to their algorithms. They see their algorithms as applying Newton's method to the system of nonlinear (and nondifferentiable equations) D (x)rf (x) = 0. As result, they add to H k a diagonal matrix defined to overcome the nondifferentiability. See their paper for more details. <p> This condition is often called fraction of Cauchy decrease, and in this case is k (0) k (s k ) fi ( k (0) k (c k )) ; (6) where fi is positive and fixed across all the iterations. 4 Coleman and Li <ref> [1] </ref> define the fraction of Cauchy decrease condition in a different way by using k = 1 in (5) although they suggest k 2 (0; 1) in the computation of the trial step s k . <p> The proof follows the same steps as the proof of Theorem 3.5 of Coleman and Li <ref> [1] </ref>. Theorem 4.1 Let f be continuously differentiable and bounded below on L (x 0 ) = fx 2 B : f (x) f (x 0 )g, where fx k g is a sequence generated by the TRIP algorithms. <p> Now we briefly describe how Coleman and Li <ref> [1] </ref> compute the trial steps. They define p k as the solution of the trust-region subproblem minimize k (s) subject to kD 1 k sk ffi k ; and compute the Cauchy point c k for some k 2 (0; 1). They propose two algorithms. <p> In the cases where the initial point given in [2] is not strictly feasible, we scale it back into the interior of B according to the rules used in <ref> [1] </ref>.
Reference: [2] <author> A. R. Conn, N. I. M. Gould, and P. L. Toint. </author> <title> Testing a class of methods for solving mini-mization problems with simple bounds on the variables. </title> <journal> Math. Comp., </journal> <volume> 50 </volume> <pages> 399-340, </pages> <year> 1988. </year>
Reference-contexts: have used ffi 0 = 1, p = 1, k = = 0:99995 for all k, * 0 = 10 10 , * 1 = 10 8 and * = 10 5 . 7 We have tested the algorithms in a set of problems given in Conn, Gould and Toint <ref> [2] </ref>. This set of problems is divided in two groups, labeled by U and C (see Table 1). In problems U, the solution lies in the interior of B and therefore these problems correspond to the situation described in Section 2.1. In the cases where the initial point given in [2] <p> <ref> [2] </ref>. This set of problems is divided in two groups, labeled by U and C (see Table 1). In problems U, the solution lies in the interior of B and therefore these problems correspond to the situation described in Section 2.1. In the cases where the initial point given in [2] is not strictly feasible, we scale it back into the interior of B according to the rules used in [1]. <p> They stop when D 1 k H k D 2 k is positive definite and k (s k ) &lt; 0:5 fl 10 12 . Our stopping criteria is of the type given in <ref> [2] </ref>. The results are given in Table 1. In Table 2 we list the total number of function and gradient evaluations taken by both approaches to solve problems U and C. From this table we observe that the second algorithm (S k = I n ) performed better.
Reference: [3] <author> J. J. </author> <title> More. Recent developments in algorithms and software for trust regions methods. </title> <editor> In A. Bachem, M. Grotschel, and B. Korte, editors, </editor> <booktitle> Mathematical programming. The state of art, </booktitle> <pages> pages 258-287. </pages> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: We do this in the following technical lemma, and it is not a surprise to see that the proof follows the proof given by Powell (see [5, Theorem 4] and <ref> [3, Lemma 4.8] </ref>) for the unconstrained minimization case.

Reference: [5] <author> M. J. D. Powell. </author> <title> Convergence properties of a class of minimization algorithms. </title> <editor> In O. L. Mangasarian, R. R. Meyer, and S. M. Robinson, editors, </editor> <booktitle> Nonlinear Programming 2, </booktitle> <pages> pages 1-27. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: We do this in the following technical lemma, and it is not a surprise to see that the proof follows the proof given by Powell (see <ref> [5, Theorem 4] </ref> and [3, Lemma 4.8]) for the unconstrained minimization case.
Reference: [6] <author> T. Steihaug. </author> <title> The conjugate gradient method and trust regions in large scale optimization. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 </volume> <pages> 626-637, </pages> <year> 1983. </year>
Reference-contexts: The conjugate-gradient algorithm to compute a trial step s k is very similar to the conjugate <p>- gradient algorithm proposed by Steihaug <ref> [6] </ref> and Toint [7] for unconstrained minimization. The only difference is caused by the fact that x k + s k has to be strictly feasible. 6 Conjugate-gradient algorithm for the computation of s k 1.
Reference: [7] <author> Ph. L. Toint. </author> <title> Towards an efficient sparsity exploiting Newton method for minimization. </title> <editor> In I. S. Duff, editor, </editor> <title> Sparse Matrices and Their Uses, </title> <address> pages 57-87. </address> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: The conjugate-gradient algorithm to compute a trial step s k is very similar to the conjugate <p>- gradient algorithm proposed by Steihaug [6] and Toint <ref> [7] </ref> for unconstrained minimization. The only difference is caused by the fact that x k + s k has to be strictly feasible. 6 Conjugate-gradient algorithm for the computation of s k 1.
References-found: 6

