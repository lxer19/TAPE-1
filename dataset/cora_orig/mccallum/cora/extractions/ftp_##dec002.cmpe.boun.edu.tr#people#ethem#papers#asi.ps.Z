URL: ftp://dec002.cmpe.boun.edu.tr/people/ethem/papers/asi.ps.Z
Refering-URL: http://www.cmpe.boun.edu.tr/~ethem/
Root-URL: 
Email: alpaydin@boun.edu.tr  
Title: Multiple Neural Experts for Improved Decision Making  
Author: Ethem Alpaydn 
Address: TR-80815 Istanbul, Turkey  
Affiliation: Dept of Computer Engineering, Bogazi~ci University,  
Abstract: There are many choices that should be made before a neural network can be used in an application. These include the network architecture, learning method, training data, feature representation, initial parameter values, training set order, cost function. A number of alternatives are generated and tested and the one that performs best on a separate test set is adopted. Instead of discarding the rest, we propose here to combine them by taking a vote. First the approach is advocated and the existing literature is surveyed. Then we put the approach into a Bayesian framework where the weights in votes are taken as Bayesian priors computed based on model complexities. Initial results on classification of handwritten numerals are promising. The voting approach can be generalized to regression where the function approximated is continuous as opposed to discrete 0/1 of classification. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Alpaydn, E. </author> <title> (1991) "GAL: Networks that grow when they learn and shrink when they forget," ICSI, </title> <address> TR-91-032, Berkeley, CA. </address>
Reference-contexts: It is for this reason that we do not want to discard other models but combine them suitably so as to improve performance. Examples are "synergy" of clustering multiple networks [8], "portfolios" [10], "multiple" networks <ref> [1] </ref>, "consensus" theory [3], "stacked" generalization [16], "ensembles" [12]. 1.1 Generating a Separate Test Set In this article we assume in several cases that we have a data set C, distinct from T for cross-validating models trained on T . <p> In the case of Pearlmutter and Rosenfeld [11], this parameter is the set of initial random weights in backpropagation. In the case of Hampshire and Waibel ([5], reported in [11]), it is the objective function. In Alpaydn's case <ref> [1] </ref>, this parameter is the order of patterns in the training set. One can also divide the training set into N and train N separate models and take a vote. 3 Weighted Voting When we take a simple majority voting, each network has equal "weight" on the result of voting. <p> This value can be fixed or may change from one input to another. In the case of classification, the "certainty" of a classifier can be computed as the difference of the values of the two highest active units <ref> [1] </ref>. When one has cross-validation data, a separate data set, one can assess the performance of the trained model on this data set and use the success achieved as the measure of "belief" in this model [17].
Reference: 2. <author> Alpaydn, E. </author> <title> (1993) "Multiple networks for function learning," </title> <booktitle> IEEE International Conference on Neural Networks, </booktitle> <address> March, San Francisco, </address> <pages> 9-14. </pages>
Reference-contexts: We know for example that complex models with many free parameters overfit on a small sample. Thus we may like to have the tendency to give them less weight unless they are more successful than simpler models, i.e., their complexity is justified <ref> [2] </ref>. Here we interpret weights as Bayesian priors with which we modulate likelihoods. Regardless of the way we combine the models, there is no reason to have multiple models if they are quite similar. One can only get fault tolerance if the models fail under different circumstances 2 .
Reference: 3. <author> Benediktsson, J.A., Swain, P.H. </author> <title> (1992) "Consensus Theoretic Classification Methods," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 22, </volume> <pages> 688-704. </pages>
Reference-contexts: It is for this reason that we do not want to discard other models but combine them suitably so as to improve performance. Examples are "synergy" of clustering multiple networks [8], "portfolios" [10], "multiple" networks [1], "consensus" theory <ref> [3] </ref>, "stacked" generalization [16], "ensembles" [12]. 1.1 Generating a Separate Test Set In this article we assume in several cases that we have a data set C, distinct from T for cross-validating models trained on T . <p> The disadvantage here is that when there are a number of similar agents whose predictions generally coincide, there is no way of allowing a more expert to overrule them. We would normally like better networks to have more weight. The goal of consensus theory <ref> [3] </ref> is to get a consensus among experts where experts are weighted according to their goodness: 1 This is one of the reasons why democracy is better than monarchy where there is only one decision maker.
Reference: 4. <author> Guyon, I., Poujoud, I., Personnaz, L., Dreyfus, G., Denker, J., le Cun, Y. </author> <title> (1989) "Comparing different neural architectures for classifying handwritten digits," </title> <booktitle> International Joint Conference on Neural Networks, </booktitle> <address> Washington DC. </address>
Reference-contexts: This corresponds to using a hyperplane for combining the models. 6.1 Simulation Results This scheme is applied to the problem of recognition of handwritten digits. The training set and test set each contains 600 digits <ref> [4] </ref>. We have used multiple networks trained with the back-propagation learning rule to train our models. For the same problem, we have used four networks all with one hidden layer with 5, 15, 35, and 75 hidden units. All the networks are trained on the same training set.
Reference: 5. <author> Hampshire, J., Waibel, A. </author> <title> (1989) "A novel objective function for improved phoneme recognition using time delay neural networks," </title> <type> CMU, TR CS-89-118. </type>
Reference: 6. <author> Hansen, L.K., Salamon, P. </author> <title> (1990) "Neural Network Ensembles," </title> <journal> IEEE Pattern Analysis and Machine Intelligence, </journal> <volume> 12, </volume> <pages> 993-1001. </pages>
Reference-contexts: This assumes that the networks have enough variance that they will generally fail under different circumstances. This is demonstrated in the context of neural networks by Hansen and Salomon <ref> [6] </ref> who have shown how local minima differ in the case of a one hidden layer feedforward network.
Reference: 7. <author> Jacobs, R.A., Jordan, M. I., Nowlan, S.J., Hinton, G.E., </author> <title> (1991) "Adaptive Mixtures of Local Experts," </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 79-87. </pages>
Reference-contexts: In such a case an input-dependent assessment of the monitored model is possible. Then we need a competitive strategy by which different models become expert on different parts of the input space to best utilize the computational resources and the voting mechanism works as a "gating" model <ref> [7] </ref>. 5 Stacked Generalization We can treat the voting mechanism as a separate generalizing layer which should be treated using a different learning set. <p> Note that the approach taken here is different from "adaptive mixtures of local experts" <ref> [7] </ref> in that here, participating networks essentially learn the same task but converge to different solutions due to differences in their structure or parameter settings. We can say that the weights denote relative performances of the networks over the whole input space as opposed to some part of it.
Reference: 8. <author> Lincoln, W.P., Skrzypek, J. </author> <title> (1990) "Synergy of Clustering Multiple Back Propagation Networks," </title> <booktitle> in Advances in Neural Information Processing Systems 2, </booktitle> <editor> D. Touretzky (Ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <pages> 650-657. </pages>
Reference-contexts: There may be patterns for which the best one fails but one of the others succeed. It is for this reason that we do not want to discard other models but combine them suitably so as to improve performance. Examples are "synergy" of clustering multiple networks <ref> [8] </ref>, "portfolios" [10], "multiple" networks [1], "consensus" theory [3], "stacked" generalization [16], "ensembles" [12]. 1.1 Generating a Separate Test Set In this article we assume in several cases that we have a data set C, distinct from T for cross-validating models trained on T .
Reference: 9. <author> MacKay, D.J.C. </author> <title> (1992) "Bayesian Interpolation," </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 415-447. </pages>
Reference-contexts: We can say that the weights denote relative performances of the networks over the whole input space as opposed to some part of it. MacKay <ref> [9] </ref> cites Patrick and Wallace's work where, "studying the geometry of ancient stone circles, [they] discuss a practical method of assigning relative prior probabilities to alternative models by evaluating the lengths of the computer programs that decode data previously encoded under each model.
Reference: 10. <author> Mani, G. </author> <title> (1991) "Lowering Variance of Decisions by using Artificial Neural Network Portfolios," </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 484-486. </pages>
Reference-contexts: There may be patterns for which the best one fails but one of the others succeed. It is for this reason that we do not want to discard other models but combine them suitably so as to improve performance. Examples are "synergy" of clustering multiple networks [8], "portfolios" <ref> [10] </ref>, "multiple" networks [1], "consensus" theory [3], "stacked" generalization [16], "ensembles" [12]. 1.1 Generating a Separate Test Set In this article we assume in several cases that we have a data set C, distinct from T for cross-validating models trained on T . <p> When we have one model that has a certain success, we want to add a second model that succeeds best for inputs on which the first one fails; we do not care about the new one's overall performance. Assuming simple voting <ref> [10, 12] </ref>: Var (R) = Var 1 N X F i = N 2 4 i X X Cov (F i ; F j ) 5 (5) 2 This is the reason why democracy is better than oligarchy where there is a group of decision makers with very similar trainings and
Reference: 11. <author> Pearlmutter, B.A., Rosenfeld, R. </author> <title> (1991) "Chaitin-Kolmogorov Complexity and Generalization in Neural Networks," </title> <booktitle> in Advances in Neural Information Processing Systems 3, </booktitle> <editor> R. Lippmann, J. Moody, D. Touretzky (Eds.), </editor> <publisher> Morgan Kauf-mann, </publisher> <pages> 925-931. </pages>
Reference-contexts: The additional bias introduced by choosing one specific value or range of values can effectively be eliminated by training multiple models with alternative values and taking a vote. In the case of Pearlmutter and Rosenfeld <ref> [11] </ref>, this parameter is the set of initial random weights in backpropagation. In the case of Hampshire and Waibel ([5], reported in [11]), it is the objective function. In Alpaydn's case [1], this parameter is the order of patterns in the training set. <p> In the case of Pearlmutter and Rosenfeld <ref> [11] </ref>, this parameter is the set of initial random weights in backpropagation. In the case of Hampshire and Waibel ([5], reported in [11]), it is the objective function. In Alpaydn's case [1], this parameter is the order of patterns in the training set.
Reference: 12. <author> Perrone, </author> <title> M.P. (1993) "Improving Regression Estimation: Averaging Methods for Variance Reduction with Extensions to General Convex Measure Optimization," </title> <type> PhD Thesis, </type> <institution> Department of Physics, Brown University. </institution>
Reference-contexts: It is for this reason that we do not want to discard other models but combine them suitably so as to improve performance. Examples are "synergy" of clustering multiple networks [8], "portfolios" [10], "multiple" networks [1], "consensus" theory [3], "stacked" generalization [16], "ensembles" <ref> [12] </ref>. 1.1 Generating a Separate Test Set In this article we assume in several cases that we have a data set C, distinct from T for cross-validating models trained on T . <p> When we have one model that has a certain success, we want to add a second model that succeeds best for inputs on which the first one fails; we do not care about the new one's overall performance. Assuming simple voting <ref> [10, 12] </ref>: Var (R) = Var 1 N X F i = N 2 4 i X X Cov (F i ; F j ) 5 (5) 2 This is the reason why democracy is better than oligarchy where there is a group of decision makers with very similar trainings and
Reference: 13. <author> Poggio, T., Torre, V., Koch, C. </author> <title> (1985) "Computational vision and regularization theory," </title> <journal> Nature, </journal> <volume> 317, </volume> <pages> 314-319. </pages>
Reference-contexts: This prior may also be interpreted as a regularizer <ref> [13] </ref> or a complexity term preferring less complex models [14]. We can define an error function that takes into account both the success and the model complexities and minimizing this function will give us the desired W values.
Reference: 14. <author> Rissanen, J. </author> <title> (1987) "Stochastic Complexity," </title> <journal> Journal of Royal Statistics Society B, </journal> <volume> 49, </volume> <pages> 223-239, 252-265. </pages>
Reference-contexts: This prior may also be interpreted as a regularizer [13] or a complexity term preferring less complex models <ref> [14] </ref>. We can define an error function that takes into account both the success and the model complexities and minimizing this function will give us the desired W values.
Reference: 15. <author> Beyer, U., Smieja, F. </author> <title> (1993) "Learning from Examples, Agent Teams, and the Concept of Reflection," GMD, </title> <address> TR-93-766, St Augustin, Germany. </address>
Reference-contexts: In the case of reflective agents, each network, together with its output, indicates its "certainty" on its output. In classification, this can simply be the difference of values of the two highest active units. In regression, this requires a separate monitoring network <ref> [15] </ref> and a separate data set on which the monitor is trained. 2. These weights can be thought of as a second learning model combining the output of lower-level models and can thus be trained like any other on a separate data set.
Reference: 16. <author> Wolpert, D.H. </author> <title> (1992) "Stacked Generalization," </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> 241-259. </pages>
Reference-contexts: It is for this reason that we do not want to discard other models but combine them suitably so as to improve performance. Examples are "synergy" of clustering multiple networks [8], "portfolios" [10], "multiple" networks [1], "consensus" theory [3], "stacked" generalization <ref> [16] </ref>, "ensembles" [12]. 1.1 Generating a Separate Test Set In this article we assume in several cases that we have a data set C, distinct from T for cross-validating models trained on T . <p> These weights can be thought of as a second learning model combining the output of lower-level models and can thus be trained like any other on a separate data set. This method, as proposed by Wolpert <ref> [16] </ref>, is called stacked generalization. 3. This certainty may be dependent on the network architecture. We know for example that complex models with many free parameters overfit on a small sample. <p> This is named "stacking" generalizers one on top of another <ref> [16] </ref>. 6 Weights as Bayesian Priors In the case of classification, we have separate R k for each class output and the response of the system is the code of the class, R c , which has the highest R k : k R k (x) = max " i=1 i
Reference: 17. <author> Xu, L., Kryzak, A., Suen, C.Y. </author> <title> (1992) "Methods of Combining Multiple Classifiers and Their Applications to Handwriting Recognition," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 22, </volume> <month> 418-435. </month> <title> This article was processed using the LT E X macro package with LLNCS style </title>
Reference-contexts: When one has cross-validation data, a separate data set, one can assess the performance of the trained model on this data set and use the success achieved as the measure of "belief" in this model <ref> [17] </ref>. It is also possible that this monitoring network is also given a copy of the input. In such a case an input-dependent assessment of the monitored model is possible.
References-found: 17

