URL: http://www.cc.gatech.edu/grads/b/Tucker.Balch/papers/learningroles.ps.Z
Refering-URL: http://www.cs.gatech.edu/aimosaic/robot-lab/publications/learning.html
Root-URL: 
Email: tucker@cc.gatech.edu  
Title: Learning Roles: Behavioral Diversity in Robot Teams  
Author: Tucker Balch 
Address: Atlanta, Georgia 30332-0280  
Affiliation: Mobile Robot Laboratory College of Computing Georgia Institute of Technology  
Abstract: This paper describes research investigating behavioral specialization in learning robot teams. Each agent is provided a common set of skills (motor schema-based behavioral assemblages) from which it builds a task-achieving strategy using reinforcement learning. The agents learn individually to activate particular behavioral assemblages given their current situation and a reward signal. The experiments, conducted in robot soccer simulations, evaluate the agents in terms of performance, policy convergence, and behavioral diversity. The results show that in many cases, robots will automatically diversify by choosing heterogeneous behaviors. The degree of diversification and the performance of the team depend on the reward structure. When the entire team is jointly rewarded or penalized (global reinforcement), teams tend towards heterogeneous behavior. When agents are provided feedback individually (local reinforcement), they converge to identical policies. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Arkin, R., and Balch, T. </author> <year> 1997. </year> <note> Aura: principles and practice in review. Journal of Experimental and Theoretical Artificial Intelligence in press. </note>
Reference-contexts: One way behavioral assemblages may be used in solving complex tasks is to develop an assemblage for each sub-task and to execute the assemblages in an appropriate sequence. The resulting task-solving strategy can be represented as a Finite State Automaton (FSA). The technique is referred to as temporal sequencing <ref> (Arkin & Balch 1997) </ref>. Even though behavior-based approaches are robust for many tasks and environments, they are not necessarily adaptive. We now consider some of the ways learning can be integrated into a behavior-based system.
Reference: <author> Balch, T., and Arkin, R. </author> <year> 1995. </year> <title> Communication in reactive multiagent robotic systems. </title> <booktitle> Autonomous Robots 1(1). </booktitle>
Reference-contexts: Cooperation, robot-robot interference fl 1997 AAAI Workshop on Multiagent Learning. and communication are not considerations for a single robot, but are crucial in multi-robot systems. Fortunately, the additional effort involved in deploying several robots is rewarded by a more robust and efficient solution <ref> (Balch & Arkin 1995) </ref>. When feedback regarding success in a task is available, reinforcement learning can shift the burden of behavior refinement from the designer to the robots operating autonomously in their environment.
Reference: <author> Balch, T. </author> <year> 1997a. </year> <title> Clay: Integrating motor schemas and reinforcement learning. </title> <institution> Coll. of comp. </institution> <type> tech. report, </type> <institution> Ga. Inst. of Tech. </institution>
Reference-contexts: Clay integrates primitive behaviors (motor schemas) using cooperative and competitive coordination operators. Both static and learning operators are available. The system is outlined at a high level here. For more detail the reader is referred to <ref> (Balch 1997a) </ref>. Experiments are conducted by engaging an experimental team against a fixed opponent control team in soccer contests. We begin by describing the control team's behavioral configuration.
Reference: <author> Balch, T. </author> <year> 1997b. </year> <title> Social entropy: a new metric for learning multi-robot teams. </title> <booktitle> In Proc. 10th International FLAIRS Conference (FLAIRS-97). </booktitle>
Reference-contexts: This opens up the possibility that new forms of arbitration and cooperation may be discovered by the robots themselves. Finally, we are interested in measuring the diversity of the resulting society, and uti lize a metric of social entropy for that purpose <ref> (Balch 1997b) </ref>. Robot Soccer Robot soccer is an increasingly popular focus of robotics research (Kitano et al. 1997). It it is an attractive domain for multiagent investigations because a robot team's success against a strong opponent often requires some form of cooperation. <p> In summary the globally-reinforced teams always converged to one "forward," one or two "mid-backs" and one or two "goalies." To help quantify the varying degree of diversity in these teams, Social Entropy <ref> (Balch 1997b) </ref> is used as a measure of behavioral heterogeneity. Social Entropy, inspired by Shannon's Information Entropy (Shannon 1949), evaluates the diversity of a robot society based on the number of behavioral castes it includes and the relative size of each. <p> The maximum entropy for a team of four soccer robots is 2.0. Het (R) = 0 for the homogeneous teams trained using local reinforcement and Het (R) = 1:5 for the heterogeneous teams. For more detail on Social Entropy, the reader is referred to <ref> (Balch 1997b) </ref>. Discussion and Conclusion The results reported above show that in this task local reinforcement provides quicker learning, while global reinforcement leads to better performance and greater diversity. The globally-reinforced teams perform significantly better than the human-designed control team.
Reference: <author> Goldberg, D., and Mataric, M. </author> <year> 1996. </year> <title> Interference as a guide to designing efficient group behavior. </title> <institution> Computer Science Technical Report CS-96-186, Brandeis University. </institution>
Reference-contexts: But teams of mechanically identical robots are especially interesting since they may be homogeneous or heterogeneous depending solely on their behavior. Investigators are just beginning research in this area, but recent work indicates behavioral heterogeneity is advantageous in some tasks <ref> (Goldberg & Mataric 1996) </ref>. Behavior is an extremely flexible dimension of diversity in learning teams since the individuals determine the extent of heterogeneity through their own learned policies. <p> Results indicate that performance is best when the reinforcement function includes all three components. In fact the robots' behavior did not converge otherwise. Goldberg and Mataric have proposed a framework for investigating the relative merits of heterogeneous and homogeneous behavior in foraging tasks <ref> (Goldberg & Mataric 1996) </ref>. Like the research reported in this paper, their work focuses on mechanically identical, but behaviorally different agents. Time, interference and robustness are proposed as metrics for evaluating a foraging robot team, while pack, caste and territorial arbitration are offered as mechanisms for generating efficient behavior.
Reference: <author> Kaelbling, L.; Littman, M.; and Moore, A. </author> <year> 1996. </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research 4 </journal> <pages> 237-285. </pages>
Reference-contexts: When feedback regarding success in a task is available, reinforcement learning can shift the burden of behavior refinement from the designer to the robots operating autonomously in their environment. For some simple tasks, given a sufficiently long trial, agents are even able to develop optimal policies <ref> (Kaelbling, Littman, & Moore 1996) </ref>. Rather than attempting to design an optimal system from the start, the designer imbues his robots with adaptability. The robots strive continuously to improve their performance; finding suitable behaviors automatically as they interact with the environment.
Reference: <author> Kitano, H.; Asada, M.; Kuniyoshi, Y.; Noda, I.; and Os-awa, E. </author> <year> 1997. </year> <title> Robocup: The robot world cup initiative. </title> <booktitle> In Proc. Autonomous Agents 97. ACM. </booktitle> <institution> Marina Del Rey, California. </institution>
Reference-contexts: This research investigates the relationships between reward structure, performance, and behavioral diversity in robot soccer. Soccer is becoming a popular new focus of robotics research <ref> (Kitano et al. 1997) </ref>. Soccer is an interesting task for multiagent research because it is simple and familiar to most people, yet it provides opportunities for diversity in the individual team members. <p> Finally, we are interested in measuring the diversity of the resulting society, and uti lize a metric of social entropy for that purpose (Balch 1997b). Robot Soccer Robot soccer is an increasingly popular focus of robotics research <ref> (Kitano et al. 1997) </ref>. It it is an attractive domain for multiagent investigations because a robot team's success against a strong opponent often requires some form of cooperation.
Reference: <author> Lin, L.-J. </author> <year> 1993. </year> <title> Hierachical learning of robot skills by reinforcement. </title> <booktitle> In International Conference on Neural Networks. </booktitle>
Reference-contexts: The particular task they investigate is for a robot to find, then push a box across a room. Using this approach, their robot, OBELIX was able to learn to perform better than hand-coded behaviors for box-pushing. In research at Carnegie Mellon University <ref> (Lin 1993) </ref>, Lin developed a method for Q-learning to be applied hierarchically, so that complex tasks are learned at several levels. The approach is to decompose the task into sub-tasks. The robot learns at the subtask level first, then at the task level.
Reference: <author> Mahadevan, S., and Connell, J. </author> <year> 1992. </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence 311-365. </journal>
Reference-contexts: He prefers to represent Q (s; a) as a table, Q [s; a], and asserts in (Watkins & Dayan 1992) that the algorithm is not guaranteed to converge otherwise. Mahadevan and Connell <ref> (Mahadevan & Connell 1992) </ref> have applied Q-learning to learn the component behaviors within a pre-defined sequence. The particular task they investigate is for a robot to find, then push a box across a room.
Reference: <author> Mataric, M. </author> <year> 1994. </year> <title> Learning to behave socially. </title> <booktitle> In Proceedings of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats 3. </booktitle>
Reference-contexts: But the focus is not on individual learning agents but rather a learning society. Mataric has investigated learning for multi-robot behavior-based teams in foraging tasks. Her work has focused on developing specialized reinforcement functions for social learning <ref> (Mataric 1994) </ref>. The overall reinforcement, R (t), for each robot is composed of separate components, D; O and V . D indicates progress towards the agent's present goal. O provides a reinforcement if the present action is a repetition of another agent's behavior.
Reference: <author> Parker, L. E. </author> <year> 1994. </year> <title> Heterogeneous Multi-Robot Coopera--tion. </title> <type> Ph.D. Dissertation, </type> <institution> M.I.T. </institution>
Reference-contexts: For these reasons reinforcement learning is becoming pervasive in mobile robot research. This work focuses on behavior that arises from learning in multi-robot societies. Most research in multi-robot groups has centered on homogeneous systems, with work in heterogeneous systems focused primarily on mechanical and sensor differences e.g. <ref> (Parker 1994) </ref>. But teams of mechanically identical robots are especially interesting since they may be homogeneous or heterogeneous depending solely on their behavior. Investigators are just beginning research in this area, but recent work indicates behavioral heterogeneity is advantageous in some tasks (Goldberg & Mataric 1996).
Reference: <author> Shannon, C. E. </author> <year> 1949. </year> <title> The Mathematical Theory of Communication. </title> <publisher> University of Illinois Press. </publisher>
Reference-contexts: In summary the globally-reinforced teams always converged to one "forward," one or two "mid-backs" and one or two "goalies." To help quantify the varying degree of diversity in these teams, Social Entropy (Balch 1997b) is used as a measure of behavioral heterogeneity. Social Entropy, inspired by Shannon's Information Entropy <ref> (Shannon 1949) </ref>, evaluates the diversity of a robot society based on the number of behavioral castes it includes and the relative size of each.
Reference: <author> Watkins, C. J. C. H., and Dayan, P. </author> <year> 1992. </year> <title> Technical note: Q learning. </title> <booktitle> Machine Learning 8 </booktitle> <pages> 279-292. </pages>
Reference-contexts: Where s is the state or situation and a is a possible action. If the function is properly computed, an agent can act optimally simply by looking up the best-valued action for any situation. The problem is to find the Q (s; a) that provides an optimal policy. Watkins <ref> (Watkins & Dayan 1992) </ref> has developed an algorithm for determining Q (s; a) that converges to optimal. He prefers to represent Q (s; a) as a table, Q [s; a], and asserts in (Watkins & Dayan 1992) that the algorithm is not guaranteed to converge otherwise. <p> The problem is to find the Q (s; a) that provides an optimal policy. Watkins <ref> (Watkins & Dayan 1992) </ref> has developed an algorithm for determining Q (s; a) that converges to optimal. He prefers to represent Q (s; a) as a table, Q [s; a], and asserts in (Watkins & Dayan 1992) that the algorithm is not guaranteed to converge otherwise. Mahadevan and Connell (Mahadevan & Connell 1992) have applied Q-learning to learn the component behaviors within a pre-defined sequence. The particular task they investigate is for a robot to find, then push a box across a room. <p> Recall that Clay (the system used for configuring the robots) includes both fixed (non-learning) and learning coordination operators. The control team's configuration uses a fixed selector for coordination. Learning is introduced by replacing the fixed mechanism with a learning selector. A Q-learning <ref> (Watkins & Dayan 1992) </ref> module is embedded in the learning selector. It is acknowledged that other types of reinforcement learning approaches are also appropriate for this system. Q-learning was selected arbitrarily for this initial study. Future investigations may be undertaken to evaluate the impact of learning type on robotic systems.
References-found: 13

