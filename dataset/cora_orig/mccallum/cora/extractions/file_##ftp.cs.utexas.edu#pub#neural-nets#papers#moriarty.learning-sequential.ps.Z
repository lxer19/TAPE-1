URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/moriarty.learning-sequential.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: 
Email: moriarty,risto@cs.utexas.edu  
Title: Learning Sequential Decision Tasks  
Author: David E. Moriarty and Risto Miikkulainen 
Date: January 1995  
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences The University of Texas at Austin  
Pubnum: Technical Report AI95-229  
Abstract: This paper presents a new approach called SANE for learning and performing sequential decision tasks. Compared to problem-general heuristics, SANE forms more effective decision strategies because it learns to utilize domain-specific information. SANE evolves neural networks through genetic algorithms and can learn in a wide range of domains with minimal reinforcement. SANE's evolution algorithm, called symbiotic evolution, is more powerful than standard genetic algorithms because diversity pressures are inherent in the evolution. SANE is shown to be effective in two sequential decision tasks. As a value-ordering method in constraint satisfaction search, SANE required only 1/3 of the backtracks of a problem-general heuristic. As a filter for minimax search, SANE formed a network capable of focusing the search away from misinformation, creating stronger play. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, C. W. </author> <year> (1989). </year> <title> Learning to control an inverted pendulum using neural networks. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 9 </volume> <pages> 31-37. </pages>
Reference: <author> Barto, A. G., Sutton, R. S., and Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, SMC-13:834-846. </journal>
Reference: <author> Barto, A. G., Sutton, R. S., and Watkins, C. J. C. H. </author> <year> (1990). </year> <title> Learning and sequential decision making. </title> <editor> In Gabriel, M., and Moore, J. W., editors, </editor> <booktitle> Learning and Computational Neuroscience. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Belew, R. K., McInerney, J., and Schraudolph, N. N. </author> <year> (1991). </year> <title> Evolving networks: Using genetic algorithm with connectionist learning. </title> <editor> In Farmer, J. D., Langton, C., Rasmussen, S., and Taylor, C., editors, </editor> <booktitle> Artificial Life II. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Grefenstette, J. J., Ramsey, C. L., and Schultz, A. C. </author> <year> (1990). </year> <title> Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 355-381. </pages>
Reference: <author> Hansson, O., and Mayer, A. </author> <year> (1990). </year> <title> Probabilistic heuristic estimates. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 209-220. </pages>
Reference: <author> Haralick, R., and Elliot, G. </author> <year> (1980). </year> <title> Increasing tree search efficiency for constraint satisfaction problems. </title> <journal> Artificial Intelligence, </journal> <volume> 14(3) </volume> <pages> 263-313. </pages> <note> 13 Kale, </note> <author> L. V. </author> <year> (1990). </year> <title> A perfect heuristic for the n non-attacking queens problem. </title> <journal> Information Processing Letters, </journal> 34(4):173-178. 
Reference-contexts: The order in which variables and values are considered determines how soon a solution is found, and therefore, choosing the variable and value bindings wisely can significantly reduce search time. Most CSP applications use the first-fail method <ref> (Haralick and Elliot 1980) </ref> for ordering the variable bindings. At each level of the search, the variable with the smallest number of possible values is chosen for instantiation.
Reference: <author> Korf, R. E. </author> <year> (1988). </year> <title> Search: A survey of recent results. </title> <editor> In Shrobe, H. E., editor, </editor> <booktitle> Exploring Artificial Intelligence. </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Heuristic evaluation functions, therefore, are used to approximate the payoff of a state. However, heuristics create errors that propagate up the search tree, and can greatly diminish the effectiveness of minimax <ref> (Korf 1988) </ref>. Minimax also does not promote risk taking, assuming that the opponent will always make the best move. Often in losing situations the best move may not be towards the highest min/max value, especially if it will still result in a loss.
Reference: <author> Korf, R. E., and Chickering, D. M. </author> <year> (1994). </year> <title> Best-first minimax search: Othello results. </title> <booktitle> In AAAI-94. </booktitle>
Reference: <author> Koza, J. R., and Rice, J. P. </author> <year> (1991). </year> <title> Genetic generalization of both the weights and architecture for a neural network. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> 397-404. </pages> <address> New York, NY: </address> <publisher> IEEE. </publisher>
Reference: <author> Kumar, V. </author> <year> (1992). </year> <title> Algorithms for constraint satisfaction problems: A survey. </title> <journal> AI Magazine, </journal> <volume> 13 </volume> <pages> 32-44. </pages>
Reference-contexts: At each level of the search, the variable with the smallest number of possible values is chosen for instantiation. However, deciding the order in which the values are assigned is much more difficult, partly because good value-ordering heuristics are highly problem specific <ref> (Kumar 1992) </ref>. Learning the domain-specific heuristic information to build an effective value-ordering policy would therefore be a significant demonstration of SANE in an important sequential decision task. 3.1 The Car Sequencing Problem Car sequencing is an instance of the job-shop scheduling problem (Van Hentenryck et al. 1992).
Reference: <author> Lee, K.-F., and Mahajan, S. </author> <year> (1990). </year> <title> The development of a world class Othello program. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 21-36. </pages>
Reference-contexts: In the second task, SANE was implemented to focus a minimax search in the game of Othello. In this task, SANE formed a network to decide which moves from a given board situation are promising enough to evaluate. Using the powerful evaluation function from the Bill program <ref> (Lee and Mahajan 1990) </ref>, SANE was able to generate better play while examining 33% fewer board positions than normal, full-width minimax search. SANE's performance in these domains demonstrates both its effectiveness and applicability to a broad range of tasks. The body of this paper is organized as follows. <p> To create different games, an initial state was selected randomly among the 244 possible board positions after four moves. Both players were allowed to search through the second level and used the evaluation function from the Bill program <ref> (Lee and Mahajan 1990) </ref>, which is composed of large Bayes-optimized lookup tables gathered from expert games. 1 Bill was at one time the world-champion program and is still believed to be one of the best in the world.
Reference: <author> Littman, M. L., and Boyan, J. A. </author> <year> (1993). </year> <title> A distributed reinforcement learning scheme for network routing. </title> <type> Technical Report CMU-CS-93-165, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference: <author> McAllester, D. A. </author> <year> (1988). </year> <title> Conspiracy numbers for min-max search. </title> <journal> Artificial Intelligence, </journal> <volume> 35 </volume> <pages> 287-310. </pages>
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1994a). </year> <title> Efficient reinforcement learning through symbiotic evolution. </title> <type> Technical Report AI94-224, </type> <institution> Department of Computer Sciences, The University of Texas at Austin. </institution>
Reference-contexts: More advanced encodings and evolutionary strategies may enhance both the search efficiency and generalization ability. Extensions to the current implementation will be a subject of future research. An empirical evaluation of SANE was performed in the standard reinforcement learning benchmark of balancing a pole on a cart <ref> (Moriarty and Miikkulainen 1994a) </ref>.
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1994b). </year> <title> Evolutionary neural networks for value ordering in constraint satisfaction problems. </title> <type> Technical Report AI94-218, </type> <institution> Department of Computer Sciences, The University of Texas at Austin. </institution>
Reference-contexts: In our experiments, the first-fail heuristic was used for variable ordering, which results in always assigning each slot in the order they appear on the assembly line <ref> (Moriarty and Miikkulainen 1994b) </ref>. A good strategy for value ordering was left to be developed by SANE. 3.2 Evolving a Value-ordering Neural Network A 2-layer neural network was evolved using SANE to decide which car class to place in the next slot on the assembly line.
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1994c). </year> <title> Evolving neural networks to focus minimax search. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94). </booktitle> <address> Seattle, WA. </address>
Reference-contexts: minimax away from misinformation and towards more effective moves is an important, novel application to game playing and further demonstrates SANE's ability to incorporate domain-specific knowledge (In this case, weaknesses of minimax and the evaluation function) to form an effective decision strategy. 4.1 Creating a Focus Window In earlier work <ref> (Moriarty and Miikkulainen 1994c) </ref>, we showed how standard neuro-evolution methods can evolve a focus network in the game of Othello to decide which moves in a given board situation are to be explored. In this paper, the SANE method was applied to the same task with significantly stronger results. <p> SANE is able to tailor the minimax search to make the best use out of the information the evaluation function provides. SANE can optimize even highly sophisticated evaluation functions, like that of the Bill program. This is a significant improvement over the standard neuro-evolution approach used in <ref> (Moriarty and Miikkulainen 1994c) </ref>, which could improve play with a weak heuristic, but could not extend to Bill's evaluation 11 Level 1 2 3 4 5 6 % of games won by SANE 54 54 62 49 53 51 Avg. states for SANE 198 931 5808 30964 166911 939999 Avg. states
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1995). </year> <title> Discovering complex Othello strategies through evolutionary neural networks. </title> <journal> Connection Science, </journal> <note> 7(3). (To Appear). </note>
Reference-contexts: By not requiring the network to identify which classes are valid, it can more easily learn the value-ordering task. This is analogous to removing the requirement of legal move identification from a move-evaluating network in game playing, which also proved to be a good strategy <ref> (Moriarty and Miikkulainen 1995) </ref>. A simple forward-checking algorithm was also implemented to prune the search space early. For each option station, the total number of cars requiring that option was counted. If the number exceeded the capacity of the option station over all remaining slots, the search path was terminated.
Reference: <author> Van Hentenryck, P., Simonis, H., and Dincbas, M. </author> <year> (1992). </year> <title> Constraint satisfaction using constraint logic programming. </title> <journal> Artificial Intelligence, 58:113. </journal>
Reference-contexts: Learning the domain-specific heuristic information to build an effective value-ordering policy would therefore be a significant demonstration of SANE in an important sequential decision task. 3.1 The Car Sequencing Problem Car sequencing is an instance of the job-shop scheduling problem <ref> (Van Hentenryck et al. 1992) </ref>. <p> If 3 cars require that option, the option station will be overdriven. Different classes of cars require different options. The problem is to find an ordering on the assembly line such that no option station becomes overdriven. Table 2 shows a particular car sequencing problem taken from <ref> (Van Hentenryck et al. 1992) </ref>. The number of classes, number of options, capacities of the option stations, and options required by each class were fixed. The number of cars in each class and total number of cars to schedule were varied in different instances of the problem.
Reference: <author> Whitley, D., Dominic, S., Das, R., and Anderson, C. W. </author> <year> (1993). </year> <title> Genetic reinforcement learning for neurocontrol problems. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 259-284. 14 </pages>
References-found: 20

