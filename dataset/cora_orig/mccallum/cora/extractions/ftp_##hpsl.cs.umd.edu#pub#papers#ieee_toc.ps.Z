URL: ftp://hpsl.cs.umd.edu/pub/papers/ieee_toc.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/papers.brandnew/LocalResources/tech-10-23.htm
Root-URL: 
Title: Distributed Memory Compiler Design for Sparse Problems 4  
Author: Janet Wu Raja Das Joel Saltz Harry Berryman Seema Hiranandani 
Abstract: This paper addresses the issue of compiling concurrent loop nests in the presence of complicated array references and irregularly distributed arrays. Arrays accessed within loops may contain accesses that make it impossible to precisely determine the reference pattern at compile time. This paper proposes a run time support mechanism that is used effectively by a compiler to generate efficient code in these situations. The compiler accepts as input a Fortran 77 program enhanced with specifications for distributing data, and outputs a message passing program that runs on the nodes of a distributed memory machine. The runtime support for the compiler consists of a library of primitives designed to support irregular patterns of distributed array accesses and irregularly distributed array partitions. A variety of performance results on the Intel iPSC/860 are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Andr e, J.-L. Pazat, and H. Thomas, </author> <title> PANDORE: A system to manage data distribution, </title> <booktitle> in International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1990, </year> <pages> pp. 380-388. </pages>
Reference-contexts: In addition, it can be used by programmers in a wide range of applications. By contrast, programming environments such as those described by Baden and Williams are highly customized for use in specific application areas. There are a variety of compilers targeting distributed memory multiprocessors <ref> [44, 8, 33, 31, 1, 39] </ref>. With the exception of the Kali project [22], and the PARTI work described here and in [36, 29, 37], these compilers do not attempt to deal with loops having irregular references efficiently.
Reference: [2] <author> C. Ashcraft, S. C. Eisenstat, and J. W. H. Liu, </author> <title> A fan-in algorithm for distributed sparse numerical factorization, </title> <journal> SISSC, </journal> <volume> 11 (1990), </volume> <pages> pp. 593-599. </pages>
Reference-contexts: of Send Gather- Scheduler Data Receive Exchanger Elements Time (ms) (ratio) (ratio) 100 0.5 1.0 2.1 900 1.8 1.1 1.3 2500 4.3 1.2 1.1 cost of using explicitly coded send/receive pairs to move W words. 7 Relation to Other Work Programs designed to carry out a range of irregular computations, <ref> [2, 26, 4, 43, 13] </ref>, including sparse direct and iterative methods, require many of the optimizations described in this paper. Several researchers have developed programming environments that target particular classes of irregular or adaptive problems.
Reference: [3] <author> S. Baden, </author> <title> Programming abstractions for dynamically partitioning and coordinating localized scientific calculations running on multiprocessors, </title> <journal> SIAM J. Sci. and Stat. Computation., </journal> <month> 12 </month> <year> (1991). </year>
Reference-contexts: Several researchers have developed programming environments that target particular classes of irregular or adaptive problems. Williams [43] describes a programming environment (DIME) for calculations with unstructured triangular meshes using distributed memory machines. Baden <ref> [3] </ref> has developed a programming environment targeting particle computations, which provides facilities that support dynamic load balancing.
Reference: [4] <author> D. Baxter, J. Saltz, M. Schultz, S. Eisentstat, and K. Crowley, </author> <title> An experimental study of methods for parallel preconditioned Krylov methods, </title> <booktitle> in Proceedings of the 1988 Hypercube Multiprocessor Conference, </booktitle> <address> Pasadena CA, </address> <month> January </month> <year> 1988, </year> <pages> pp. 1698-1711. </pages>
Reference-contexts: of Send Gather- Scheduler Data Receive Exchanger Elements Time (ms) (ratio) (ratio) 100 0.5 1.0 2.1 900 1.8 1.1 1.3 2500 4.3 1.2 1.1 cost of using explicitly coded send/receive pairs to move W words. 7 Relation to Other Work Programs designed to carry out a range of irregular computations, <ref> [2, 26, 4, 43, 13] </ref>, including sparse direct and iterative methods, require many of the optimizations described in this paper. Several researchers have developed programming environments that target particular classes of irregular or adaptive problems.
Reference: [5] <author> M. J. Berger and S. H. Bokhari, </author> <title> A partitioning strategy for pdes across multiprocessors, </title> <booktitle> in The Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985. </year>
Reference-contexts: This mesh has 19,155 points. Each mesh point is associated with an (x; y) coordinate in a physical domain. Domain information was used to partition the mesh in three different ways: strips, orthogonal binary dissection algorithm <ref> [5] </ref>, [13], and another mesh partitioning algorithm jagged partitioning [38]. The partitioning of the meshes are done sequentially and mapping arrays are generated for distribution of the data structures. Synthetic Mesh from Templates A finite difference template links K points in a square two dimensional mesh.
Reference: [6] <author> H. Berryman, J. Saltz, and J. Scroggs, </author> <title> Execution time support for adaptive scientific algorithms on distributed memory architectures, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3 (1991), </volume> <pages> pp. 159-178. </pages>
Reference-contexts: In Figure 10 the integer array partition is local to each processor and enumerates a list of indices assigned to the processor. As mentioned earlier, the current implementation partitions only one dimension: the last dimension of the array. PARTI primitives, however, support a broader class of array mappings <ref> [6] </ref>. Thus partition describes the partitioning of the last dimension of the arrays declared in statements S1 and S2. The ARF compiler uses the information in partition to make calls to primitives that initialize the distributed translation tables. <p> These primitives, and hence these optimizations, can be migrated to a variety of compilers targeting distributed memory multiprocessors. It is intended to implement these primitives in the ParaScope parallel programming environment [17]. In addition, PARTI primitives can, and are, being used directly by programmers in applications codes <ref> [6] </ref>, [10]. The applications described in [10] were particularly noteworthy. These applications were explicit and multigrid unstructured Euler solvers which were employed to compute flows over full aircraft configurations. The explicit unstructured Euler solver achieved a computational rate of 1.5 Gflops on 512 processors of the Intel Touchstone Delta.
Reference: [7] <author> M. C. Chen, </author> <title> A design methodology for synthesizing parallel algorithms and architectures, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <year> (1986), </year> <pages> pp. 116-121. </pages>
Reference-contexts: The High Performance Fortran Forum (HPFF), a joint effort between the academic community and industry, has agreed on a preliminary set of data parallel programming language extensions [16], [20]. It has been heavily influenced by experimental languages such as Fortran D [12], Vienna Fortran [45], Crystal <ref> [7] </ref>, [24], [23], [25], Kali [22], DINO [32], and CM Fortran [9].
Reference: [8] <author> A. Cheung and A. P. Reeves, </author> <title> The Paragon multicomputer environment: A first implementation, </title> <type> Tech. Rep. </type> <institution> EE-CEG-89-9, Cornell University Computer Engineering Group, Cornell University School of Electrical Engineering, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: In addition, it can be used by programmers in a wide range of applications. By contrast, programming environments such as those described by Baden and Williams are highly customized for use in specific application areas. There are a variety of compilers targeting distributed memory multiprocessors <ref> [44, 8, 33, 31, 1, 39] </ref>. With the exception of the Kali project [22], and the PARTI work described here and in [36, 29, 37], these compilers do not attempt to deal with loops having irregular references efficiently.
Reference: [9] <author> T. M. Corporation, </author> <title> CM Fortran reference manual, </title> <type> Tech. Rep. version 1.0, </type> <institution> Thinking Machines Corporation, </institution> <month> Feb </month> <year> 1991. </year> <month> 44 </month>
Reference-contexts: It has been heavily influenced by experimental languages such as Fortran D [12], Vienna Fortran [45], Crystal [7], [24], [23], [25], Kali [22], DINO [32], and CM Fortran <ref> [9] </ref>.
Reference: [10] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy, </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives, </title> <booktitle> AIAA-92-0562, in Proceedings of the 30th Aerospace Sciences Meeting, </booktitle> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: These primitives, and hence these optimizations, can be migrated to a variety of compilers targeting distributed memory multiprocessors. It is intended to implement these primitives in the ParaScope parallel programming environment [17]. In addition, PARTI primitives can, and are, being used directly by programmers in applications codes [6], <ref> [10] </ref>. The applications described in [10] were particularly noteworthy. These applications were explicit and multigrid unstructured Euler solvers which were employed to compute flows over full aircraft configurations. The explicit unstructured Euler solver achieved a computational rate of 1.5 Gflops on 512 processors of the Intel Touchstone Delta. <p> It is intended to implement these primitives in the ParaScope parallel programming environment [17]. In addition, PARTI primitives can, and are, being used directly by programmers in applications codes [6], <ref> [10] </ref>. The applications described in [10] were particularly noteworthy. These applications were explicit and multigrid unstructured Euler solvers which were employed to compute flows over full aircraft configurations. The explicit unstructured Euler solver achieved a computational rate of 1.5 Gflops on 512 processors of the Intel Touchstone Delta.
Reference: [11] <author> R. Das, J. Saltz, and R. v. Hanxleden, </author> <title> Slicing analysis and indirect access to distributed arrays, </title> <booktitle> in Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <publisher> Springer-Verlag, </publisher> <month> Aug. </month> <year> 1993, </year> <pages> pp. 152-168. </pages> <note> Also available as University of Maryland Technical Report CS-TR-3076 and UMIACS-TR-93-42. </note>
Reference-contexts: The values of the subscript array col (j) are computed in statement S3. Statement S3 in turn lies within a loop S2 whose upper bound is itself determined by the values taken on by array num. Das et. al. <ref> [11] </ref> describes program slicing techniques that can be used to extend the methods described here to a broader set of constructs. Except for one special case, the ARF compiler is unable to handle loops with loop carried dependencies. The special case involves accumulation type dependencies.
Reference: [12] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu, </author> <title> Fortran D language specification, </title> <type> Tech. Rep. </type> <institution> TR90-141, Dept. of Computer Science, Rice University, </institution> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: 1 Introduction On modern scalable multicomputers it is widely recognized that, in addition to detecting and exploiting available parallelism, reducing communication costs is crucial in achieving good performance. Existing systems such as DINO [34], Fortran D <ref> [12] </ref>, Superb [44], and Id Noveau [31] perform communication optimizations only in the presence of regular array reference patterns within loops, such as message blocking, collective communications utilization, and message coalescing and aggregation. Parallel loop nests, however, often contain array references that cannot be analyzed at compile time. <p> The High Performance Fortran Forum (HPFF), a joint effort between the academic community and industry, has agreed on a preliminary set of data parallel programming language extensions [16], [20]. It has been heavily influenced by experimental languages such as Fortran D <ref> [12] </ref>, Vienna Fortran [45], Crystal [7], [24], [23], [25], Kali [22], DINO [32], and CM Fortran [9].
Reference: [13] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker, </author> <title> Solving Problems on Concurrent Computers, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1988. </year>
Reference-contexts: This mesh has 19,155 points. Each mesh point is associated with an (x; y) coordinate in a physical domain. Domain information was used to partition the mesh in three different ways: strips, orthogonal binary dissection algorithm [5], <ref> [13] </ref>, and another mesh partitioning algorithm jagged partitioning [38]. The partitioning of the meshes are done sequentially and mapping arrays are generated for distribution of the data structures. Synthetic Mesh from Templates A finite difference template links K points in a square two dimensional mesh. <p> of Send Gather- Scheduler Data Receive Exchanger Elements Time (ms) (ratio) (ratio) 100 0.5 1.0 2.1 900 1.8 1.1 1.3 2500 4.3 1.2 1.1 cost of using explicitly coded send/receive pairs to move W words. 7 Relation to Other Work Programs designed to carry out a range of irregular computations, <ref> [2, 26, 4, 43, 13] </ref>, including sparse direct and iterative methods, require many of the optimizations described in this paper. Several researchers have developed programming environments that target particular classes of irregular or adaptive problems.
Reference: [14] <institution> Numerical methods for the computation of inviscid transonic flows with shock waves - a gamm workshop, </institution> <note> in Notes on Numercial Fluid Mechanics, vol. 3. </note>
Reference-contexts: Mesh A: A 21,672 element mesh generated to carry out an aerodynamic simulation involving a multi-element airfoil in a landing configuration [28]. This mesh has 11,143 points. Mesh B: A 37,741 element mesh generated to simulate a 4.2 % circular arc airfoil in a channel <ref> [14] </ref>. This mesh has 19,155 points. Each mesh point is associated with an (x; y) coordinate in a physical domain. Domain information was used to partition the mesh in three different ways: strips, orthogonal binary dissection algorithm [5], [13], and another mesh partitioning algorithm jagged partitioning [38].
Reference: [15] <author> M. Gerndt, </author> <title> Updating distributed variables in local computations, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2 (1990), </volume> <pages> pp. 171-193. </pages>
Reference-contexts: In order to generate efficient code, a compiler needs to have tractable representations of array subscript functions. It also needs tractable representations of how data and computational work are to be partitioned [18], <ref> [15] </ref>, [19], [35]. <p> In addition, it is easy to determine which non-local data must be obtained. For instance, the processor responsible for loop iterations 1 through 250 will need the first two values of y stored on the processor responsible for loop iterations 251 through 500. A variety of researchers [18], <ref> [15] </ref> have implemented techniques to generate optimized calls to message passing routines given compile-time information about array subscript functions, array distribution and the distribution of loop iterations. This paper deals with situations where compile time analysis fails because crucial information is not available until a program executes.
Reference: [16] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran language specification, </title> <booktitle> Scientific Programming, 2 (1993), </booktitle> <pages> pp. 1-170. </pages>
Reference-contexts: Since the development of ARF a significant amount of work has been done in standardizing extensions to the Fortran language. The High Performance Fortran Forum (HPFF), a joint effort between the academic community and industry, has agreed on a preliminary set of data parallel programming language extensions <ref> [16] </ref>, [20]. It has been heavily influenced by experimental languages such as Fortran D [12], Vienna Fortran [45], Crystal [7], [24], [23], [25], Kali [22], DINO [32], and CM Fortran [9].
Reference: [17] <author> S. Hiranandani, K. Kennedy, and C. Tseng, </author> <title> Compiler support for machine-independent parallel programming in Fortran D, in Compilers and Runtime Software for Scalable Multiprocessors, </title> <editor> J. Saltz and P. Mehrotra Editors, </editor> <address> Amsterdam, The Netherlands, </address> <note> To appear 1991, Elsevier. </note>
Reference-contexts: These primitives, and hence these optimizations, can be migrated to a variety of compilers targeting distributed memory multiprocessors. It is intended to implement these primitives in the ParaScope parallel programming environment <ref> [17] </ref>. In addition, PARTI primitives can, and are, being used directly by programmers in applications codes [6], [10]. The applications described in [10] were particularly noteworthy. These applications were explicit and multigrid unstructured Euler solvers which were employed to compute flows over full aircraft configurations.
Reference: [18] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng, </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines, </title> <booktitle> in Proceedings Supercomputing '91, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> Nov. </month> <year> 1991, </year> <pages> pp. 86-100. </pages>
Reference-contexts: In order to generate efficient code, a compiler needs to have tractable representations of array subscript functions. It also needs tractable representations of how data and computational work are to be partitioned <ref> [18] </ref>, [15], [19], [35]. <p> For instance, consider the Fortran D example: S1 REAL x (1000), y (1000) S2 DECOMPOSITION blocks (1000) S3 ALIGN x,y with blocks S4 DISTRIBUTE blocks (BLOCK) S5 DO i=1,750 S7 END DO Assume that each processor is responsible for computing values of data it owns (i.e. the owner computes rule <ref> [18] </ref>). If we have 4 processors, each processor will own contiguous chunks of 250 elements of arrays x and y. Since the subscript function for x (i) is the identity, the owner computes rule implies that each of three processors will execute 250 iterations of loop S5. <p> In addition, it is easy to determine which non-local data must be obtained. For instance, the processor responsible for loop iterations 1 through 250 will need the first two values of y stored on the processor responsible for loop iterations 251 through 500. A variety of researchers <ref> [18] </ref>, [15] have implemented techniques to generate optimized calls to message passing routines given compile-time information about array subscript functions, array distribution and the distribution of loop iterations. This paper deals with situations where compile time analysis fails because crucial information is not available until a program executes.
Reference: [19] <author> C. Koelbel, </author> <title> Compiling Programs for Nonshared Memory Machines, </title> <type> PhD thesis, </type> <institution> Purdue University, West Lafayette, IN, </institution> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: In order to generate efficient code, a compiler needs to have tractable representations of array subscript functions. It also needs tractable representations of how data and computational work are to be partitioned [18], [15], <ref> [19] </ref>, [35].
Reference: [20] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel, </author> <title> The High Performance Fortran Handbook, </title> <booktitle> 1994. </booktitle> <pages> 45 </pages>
Reference-contexts: Since the development of ARF a significant amount of work has been done in standardizing extensions to the Fortran language. The High Performance Fortran Forum (HPFF), a joint effort between the academic community and industry, has agreed on a preliminary set of data parallel programming language extensions [16], <ref> [20] </ref>. It has been heavily influenced by experimental languages such as Fortran D [12], Vienna Fortran [45], Crystal [7], [24], [23], [25], Kali [22], DINO [32], and CM Fortran [9].
Reference: [21] <author> C. Koelbel and P. Mehrotra, </author> <title> Compiling global name-space programs for dis-tributed execution, </title> <type> Report 90-70, </type> <institution> ICASE, </institution> <year> 1990. </year>
Reference-contexts: For instance, ALIGN z (i,j) with map (j) means that the the second dimension of z is aligned with map. In this example it means that we map column j of z to processor map (j). Fortran D also provides a directive, the on clause <ref> [21] </ref>, to specify a processor which will execute each iteration of a loop.
Reference: [22] <author> C. Koelbel, P. Mehrotra, and J. V. Rosendale, </author> <title> Supporting shared data structures on distributed memory architectures, </title> <booktitle> in 2nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, ACM, </booktitle> <month> Mar. </month> <year> 1990, </year> <pages> pp. 177-186. </pages>
Reference-contexts: It has been heavily influenced by experimental languages such as Fortran D [12], Vienna Fortran [45], Crystal [7], [24], [23], [25], Kali <ref> [22] </ref>, DINO [32], and CM Fortran [9]. <p> By contrast, programming environments such as those described by Baden and Williams are highly customized for use in specific application areas. There are a variety of compilers targeting distributed memory multiprocessors [44, 8, 33, 31, 1, 39]. With the exception of the Kali project <ref> [22] </ref>, and the PARTI work described here and in [36, 29, 37], these compilers do not attempt to deal with loops having irregular references efficiently. The work described in this paper is also related to schemes to carry out distributed memory runtime parallelization [29, 27]. <p> These primitives are augmented with a hash table designed to eliminate duplicate data accesses. In addition, the hash table manages copies of off-processor array elements. Other researchers have used different data structures for management of off-processor data copies <ref> [22] </ref>. 8 Conclusion This paper described and experimentally characterized a compiler and runtime support procedures which embody methods that are capable of handling an important class of irregular problems that arise in scientific computing. After examining a number of complete NASA codes, two kernels were extracted to demonstrate the methods.
Reference: [23] <author> J. Li and M. Chen, </author> <title> Generating explicit communication from shared-memory program references, </title> <booktitle> in Proceedings Supercomputing '90, </booktitle> <month> November </month> <year> 1990. </year> <title> [24] , Index domain alignment: Minimizing cost of cross-references between distributed arrays, </title> <booktitle> in Proceedings of the 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <month> October </month> <year> 1990. </year> <title> [25] , Automating the coordination of interprocessor communication., </title> <booktitle> in Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Cambridge Mass, 1991, </address> <publisher> MIT Press. </publisher>
Reference-contexts: The High Performance Fortran Forum (HPFF), a joint effort between the academic community and industry, has agreed on a preliminary set of data parallel programming language extensions [16], [20]. It has been heavily influenced by experimental languages such as Fortran D [12], Vienna Fortran [45], Crystal [7], [24], <ref> [23] </ref>, [25], Kali [22], DINO [32], and CM Fortran [9].
Reference: [26] <author> J. W. Liu, </author> <title> Computational models and task scheduling for parallel sparse Cholesky factorization, </title> <booktitle> Parallel Computing, 3 (1986), </booktitle> <pages> pp. 327-342. </pages>
Reference-contexts: of Send Gather- Scheduler Data Receive Exchanger Elements Time (ms) (ratio) (ratio) 100 0.5 1.0 2.1 900 1.8 1.1 1.3 2500 4.3 1.2 1.1 cost of using explicitly coded send/receive pairs to move W words. 7 Relation to Other Work Programs designed to carry out a range of irregular computations, <ref> [2, 26, 4, 43, 13] </ref>, including sparse direct and iterative methods, require many of the optimizations described in this paper. Several researchers have developed programming environments that target particular classes of irregular or adaptive problems.
Reference: [27] <author> L. C. Lu and M. Chen, </author> <title> Parallelizing loops with indirect array references or pointers, </title> <booktitle> in Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: The work described in this paper is also related to schemes to carry out distributed memory runtime parallelization <ref> [29, 27] </ref>. These schemes are more ambitious than those described in this paper, which include mechanisms to carry out runtime partitioning and parallelization. Chen [27] suggests an optimization similar to one described here. <p> The work described in this paper is also related to schemes to carry out distributed memory runtime parallelization [29, 27]. These schemes are more ambitious than those described in this paper, which include mechanisms to carry out runtime partitioning and parallelization. Chen <ref> [27] </ref> suggests an optimization similar to one described here. She pro 41 posed reducing scheduling overheads by identifying distributed array references for which one can employ identical schedules. At this point only hand coding based timing experiments have been carried out to study the schemes proposed [29, 27]. <p> Chen [27] suggests an optimization similar to one described here. She pro 41 posed reducing scheduling overheads by identifying distributed array references for which one can employ identical schedules. At this point only hand coding based timing experiments have been carried out to study the schemes proposed <ref> [29, 27] </ref>. The prototype compiler described here is able to generate code capable of efficiently handling kernels with parallel loops containing irregular array references. The procedures that carry out runtime optimizations are coupled to a distributed memory compiler via a set of compiler transformations.
Reference: [28] <author> D. J. Mavriplis, </author> <title> Multigrid solution of the two-dimensional Euler equations on unstructured triangular meshes, </title> <journal> AIAA Journal, </journal> <volume> 26 (1988), </volume> <pages> pp. 824-831. </pages>
Reference-contexts: Unstructured Meshes from Aerodynamics: Two unstructured meshes gener ated from aerodynamic simulations were used. Mesh A: A 21,672 element mesh generated to carry out an aerodynamic simulation involving a multi-element airfoil in a landing configuration <ref> [28] </ref>. This mesh has 11,143 points. Mesh B: A 37,741 element mesh generated to simulate a 4.2 % circular arc airfoil in a channel [14]. This mesh has 19,155 points. Each mesh point is associated with an (x; y) coordinate in a physical domain.
Reference: [29] <author> R. Mirchandaney, J. H. Saltz, R. M. Smith, D. M. Nicol, and K. Crowley, </author> <title> Principles of runtime support for parallel processors, </title> <booktitle> in Proceedings of the 1988 ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1988, </year> <pages> pp. 140-152. </pages>
Reference-contexts: There are a variety of compilers targeting distributed memory multiprocessors [44, 8, 33, 31, 1, 39]. With the exception of the Kali project [22], and the PARTI work described here and in <ref> [36, 29, 37] </ref>, these compilers do not attempt to deal with loops having irregular references efficiently. The work described in this paper is also related to schemes to carry out distributed memory runtime parallelization [29, 27]. <p> The work described in this paper is also related to schemes to carry out distributed memory runtime parallelization <ref> [29, 27] </ref>. These schemes are more ambitious than those described in this paper, which include mechanisms to carry out runtime partitioning and parallelization. Chen [27] suggests an optimization similar to one described here. <p> Chen [27] suggests an optimization similar to one described here. She pro 41 posed reducing scheduling overheads by identifying distributed array references for which one can employ identical schedules. At this point only hand coding based timing experiments have been carried out to study the schemes proposed <ref> [29, 27] </ref>. The prototype compiler described here is able to generate code capable of efficiently handling kernels with parallel loops containing irregular array references. The procedures that carry out runtime optimizations are coupled to a distributed memory compiler via a set of compiler transformations. <p> Mechanisms have been developed and demonstrated that support irregularly distributed arrays, making it possible to map data and computational work in an arbitrary manner. Because irregularly distributed arrays can be supported, it was possible to compare the performance effects of different problem mappings. Support for arbitrary distributions was proposed <ref> [29, 37] </ref> but this is the first implementation of a compiler-based distributed translation table mechanism for irregular scientific problems. Many unstructured NASA codes must carry out data accumulations to off-processor memory locations.
Reference: [30] <author> S. Mirchandaney, J. Saltz, P. Mehrotra, and H. Berryman, </author> <title> A scheme for supporting automatic data migration on multicomputers, </title> <booktitle> in Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <address> Charleston S.C., </address> <year> 1990. </year>
Reference-contexts: After examining a variety of sparse and unstructured codes, it was decided not to implement the method described in this section in the ARF compiler. See the analysis in <ref> [30] </ref> for the time and space tradeoffs outlined in this section. 6 Experimental Results This section presents a range of performance data that summarizes the effects of preprocessing on measures of overall efficiency. Also discussed is the performance effects of problem irregularity and partitioning.
Reference: [31] <author> A. Rogers and K. Pingali, </author> <title> Process decomposition through locality of reference, </title> <booktitle> in Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year> <month> 46 </month>
Reference-contexts: 1 Introduction On modern scalable multicomputers it is widely recognized that, in addition to detecting and exploiting available parallelism, reducing communication costs is crucial in achieving good performance. Existing systems such as DINO [34], Fortran D [12], Superb [44], and Id Noveau <ref> [31] </ref> perform communication optimizations only in the presence of regular array reference patterns within loops, such as message blocking, collective communications utilization, and message coalescing and aggregation. Parallel loop nests, however, often contain array references that cannot be analyzed at compile time. Such array references are classified as irregular. <p> In addition, it can be used by programmers in a wide range of applications. By contrast, programming environments such as those described by Baden and Williams are highly customized for use in specific application areas. There are a variety of compilers targeting distributed memory multiprocessors <ref> [44, 8, 33, 31, 1, 39] </ref>. With the exception of the Kali project [22], and the PARTI work described here and in [36, 29, 37], these compilers do not attempt to deal with loops having irregular references efficiently.
Reference: [32] <author> M. Rosing and R. Schnabel, </author> <title> An overview of Dino anew language for numer-ical computation on distributed memory multiprocessors, </title> <type> Tech. Rep. </type> <institution> CU-CS-385-88, University of Colorado, Boulder, </institution> <year> 1988. </year>
Reference-contexts: It has been heavily influenced by experimental languages such as Fortran D [12], Vienna Fortran [45], Crystal [7], [24], [23], [25], Kali [22], DINO <ref> [32] </ref>, and CM Fortran [9].
Reference: [33] <author> M. Rosing, R. Schnabel, and R. Weaver, </author> <title> Expressing complex parallel algorithms in Dino, </title> <booktitle> in Proceedings of the 4th Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <year> 1989, </year> <pages> pp. 553-560. </pages>
Reference-contexts: In addition, it can be used by programmers in a wide range of applications. By contrast, programming environments such as those described by Baden and Williams are highly customized for use in specific application areas. There are a variety of compilers targeting distributed memory multiprocessors <ref> [44, 8, 33, 31, 1, 39] </ref>. With the exception of the Kali project [22], and the PARTI work described here and in [36, 29, 37], these compilers do not attempt to deal with loops having irregular references efficiently.
Reference: [34] <author> M. Rosing, R. Schnabel, and R. Weaver, </author> <title> Massive parallelism and process contraction in Dino, </title> <booktitle> in Proceedings of the Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Chicago, IL, </address> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: 1 Introduction On modern scalable multicomputers it is widely recognized that, in addition to detecting and exploiting available parallelism, reducing communication costs is crucial in achieving good performance. Existing systems such as DINO <ref> [34] </ref>, Fortran D [12], Superb [44], and Id Noveau [31] perform communication optimizations only in the presence of regular array reference patterns within loops, such as message blocking, collective communications utilization, and message coalescing and aggregation.
Reference: [35] <author> M. Rosing, R. B. Schnabel, and R. P. Weaver, </author> <title> The DINO parallel programming language, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 (1991), </volume> <pages> pp. 30-42. </pages>
Reference-contexts: In order to generate efficient code, a compiler needs to have tractable representations of array subscript functions. It also needs tractable representations of how data and computational work are to be partitioned [18], [15], [19], <ref> [35] </ref>.
Reference: [36] <author> J. Saltz and M. Chen, </author> <title> Automated problem mapping: the crystal runtime system, </title> <booktitle> in The Proceedings of the Hypercube Microprocessors Conf., </booktitle> <address> Knoxville, TN, </address> <month> September </month> <year> 1986. </year>
Reference-contexts: There are a variety of compilers targeting distributed memory multiprocessors [44, 8, 33, 31, 1, 39]. With the exception of the Kali project [22], and the PARTI work described here and in <ref> [36, 29, 37] </ref>, these compilers do not attempt to deal with loops having irregular references efficiently. The work described in this paper is also related to schemes to carry out distributed memory runtime parallelization [29, 27].
Reference: [37] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman, </author> <title> Run-time scheduling and execution of loops on message passing machines, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 (1990), </volume> <pages> pp. 303-312. </pages>
Reference-contexts: There are a variety of compilers targeting distributed memory multiprocessors [44, 8, 33, 31, 1, 39]. With the exception of the Kali project [22], and the PARTI work described here and in <ref> [36, 29, 37] </ref>, these compilers do not attempt to deal with loops having irregular references efficiently. The work described in this paper is also related to schemes to carry out distributed memory runtime parallelization [29, 27]. <p> Mechanisms have been developed and demonstrated that support irregularly distributed arrays, making it possible to map data and computational work in an arbitrary manner. Because irregularly distributed arrays can be supported, it was possible to compare the performance effects of different problem mappings. Support for arbitrary distributions was proposed <ref> [29, 37] </ref> but this is the first implementation of a compiler-based distributed translation table mechanism for irregular scientific problems. Many unstructured NASA codes must carry out data accumulations to off-processor memory locations.
Reference: [38] <author> J. Saltz, S. Petiton, H. Berryman, and A. Rifkin, </author> <title> Performance effects of irregular communications patterns on massively parallel multiprocessors, </title> <note> to appear in Journal of Parallel and Distributed Computing, </note> <year> 1991, </year> <type> Report 91-12, </type> <institution> ICASE, </institution> <year> 1991. </year>
Reference-contexts: This mesh has 19,155 points. Each mesh point is associated with an (x; y) coordinate in a physical domain. Domain information was used to partition the mesh in three different ways: strips, orthogonal binary dissection algorithm [5], [13], and another mesh partitioning algorithm jagged partitioning <ref> [38] </ref>. The partitioning of the meshes are done sequentially and mapping arrays are generated for distribution of the data structures. Synthetic Mesh from Templates A finite difference template links K points in a square two dimensional mesh. This connectivity pattern is distorted incrementally.
Reference: [39] <author> P. S. Tseng, </author> <title> A Parallelizing Compiler for Distributed Memory Parallel Computers, </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: In addition, it can be used by programmers in a wide range of applications. By contrast, programming environments such as those described by Baden and Williams are highly customized for use in specific application areas. There are a variety of compilers targeting distributed memory multiprocessors <ref> [44, 8, 33, 31, 1, 39] </ref>. With the exception of the Kali project [22], and the PARTI work described here and in [36, 29, 37], these compilers do not attempt to deal with loops having irregular references efficiently.
Reference: [40] <author> P. Venkatkrishnan, J. Saltz, and D. Mavriplis, </author> <title> Parallel preconditioned iterative methods for the compressible navier stokes equations, </title> <booktitle> in 12th International Conference on Numerical Methods in Fluid Dynamics, </booktitle> <address> Oxford, England, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: Section 5.3.2 presents an example from computational fluid dynamics. 5.3.1 Sparse Block Matrix Vector Multiply This kernel is from an iterative solver produced for a program designed to calculate fluid flow for geometries defined by an unstructured mesh <ref> [40] </ref>. The matrix is assumed to have size 4 by 4 blocks of non-zero entries. Statements S4 and S5 are loops that sweep over the non-zero entries in each block.
Reference: [41] <author> D. L. Whitaker and B. Grossman, </author> <title> Two-dimensional Euler computations on a triangular mesh using an upwind, finite-volume scheme, </title> <booktitle> in Proceedings AIAA 27th Aerospace Sciences Meeting, </booktitle> <address> Reno, Nevada, </address> <month> January </month> <year> 1989. </year> <month> 47 </month>
Reference-contexts: Note that the declarations in S1 and S3 in Figure 10 allow the compiler to determine that accumulations to y are local. 5.3.2 The Fluxroe Kernel This kernel is taken from a program that computes convective fluxes using a method based on Roe's approximate Riemann solver <ref> [41] </ref>, [42]; referred to as Fluxroe kernel in this paper. Fluxroe computes the flux across each edge of an unstructured mesh. Fluxroe accesses elements of array yold, carries out flux calculations and accumulates results to array y.
Reference: [42] <author> D. L. Whitaker, D. C. Slack, and R. W. Walters, </author> <title> Solution algorithms for the two-dimensional Euler equations on unstructured meshes, </title> <booktitle> in Proceedings AIAA 28th Aerospace Sciences Meeting, </booktitle> <address> Reno, Nevada, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: Note that the declarations in S1 and S3 in Figure 10 allow the compiler to determine that accumulations to y are local. 5.3.2 The Fluxroe Kernel This kernel is taken from a program that computes convective fluxes using a method based on Roe's approximate Riemann solver [41], <ref> [42] </ref>; referred to as Fluxroe kernel in this paper. Fluxroe computes the flux across each edge of an unstructured mesh. Fluxroe accesses elements of array yold, carries out flux calculations and accumulates results to array y.
Reference: [43] <author> R. D. Williams and R. Glowinski, </author> <title> Distributed irregular finite elements, </title> <type> Tech. Rep. </type> <institution> C3P 715, Caltech Concurrent Computation Program, </institution> <month> February </month> <year> 1989. </year>
Reference-contexts: of Send Gather- Scheduler Data Receive Exchanger Elements Time (ms) (ratio) (ratio) 100 0.5 1.0 2.1 900 1.8 1.1 1.3 2500 4.3 1.2 1.1 cost of using explicitly coded send/receive pairs to move W words. 7 Relation to Other Work Programs designed to carry out a range of irregular computations, <ref> [2, 26, 4, 43, 13] </ref>, including sparse direct and iterative methods, require many of the optimizations described in this paper. Several researchers have developed programming environments that target particular classes of irregular or adaptive problems. <p> Several researchers have developed programming environments that target particular classes of irregular or adaptive problems. Williams <ref> [43] </ref> describes a programming environment (DIME) for calculations with unstructured triangular meshes using distributed memory machines. Baden [3] has developed a programming environment targeting particle computations, which provides facilities that support dynamic load balancing.
Reference: [44] <author> H. Zima, H.-J. Bast, and M. Gerndt, </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization, </title> <booktitle> Parallel Computing, 6 (1988), </booktitle> <pages> pp. 1-18. </pages>
Reference-contexts: 1 Introduction On modern scalable multicomputers it is widely recognized that, in addition to detecting and exploiting available parallelism, reducing communication costs is crucial in achieving good performance. Existing systems such as DINO [34], Fortran D [12], Superb <ref> [44] </ref>, and Id Noveau [31] perform communication optimizations only in the presence of regular array reference patterns within loops, such as message blocking, collective communications utilization, and message coalescing and aggregation. Parallel loop nests, however, often contain array references that cannot be analyzed at compile time. <p> In addition, it can be used by programmers in a wide range of applications. By contrast, programming environments such as those described by Baden and Williams are highly customized for use in specific application areas. There are a variety of compilers targeting distributed memory multiprocessors <ref> [44, 8, 33, 31, 1, 39] </ref>. With the exception of the Kali project [22], and the PARTI work described here and in [36, 29, 37], these compilers do not attempt to deal with loops having irregular references efficiently.
Reference: [45] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald, </author> <title> Vienna Fortran | a language specification, version 1.1, </title> <type> Interim Report 21, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> Mar. </month> <year> 1992. </year> <month> 48 </month>
Reference-contexts: The High Performance Fortran Forum (HPFF), a joint effort between the academic community and industry, has agreed on a preliminary set of data parallel programming language extensions [16], [20]. It has been heavily influenced by experimental languages such as Fortran D [12], Vienna Fortran <ref> [45] </ref>, Crystal [7], [24], [23], [25], Kali [22], DINO [32], and CM Fortran [9].
References-found: 43

