URL: ftp://www-flash.stanford.edu/pub/flash/ISCA90_rev.ps.Z
Refering-URL: http://www.cs.umd.edu/users/keleher/syllabus.818.html
Root-URL: 
Title: Revision to "Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors"  
Author: Kourosh Gharachorloo, Anoop Gupta, and John Hennessy 
Address: Stanford, CA 94305  
Affiliation: Computer System Laboratory Stanford University  
Abstract: A memory consistency model specifies the set of allowable orderings for memory operations in a shared-memory multiprocessor. In [GLL + 90], we presented the specifications for several memory models based on an abstraction of the system proposed by Dubois et al. [DSB86, SD87]. Our purpose for specifying the processor consistency (PC) and release consistency (RC) models was to capture the behavior of several event ordering optimizations that are exploited in systems such as the Stanford DASH multiprocessor [LLG + 92] and other commercial multiprocessors. Among the optimizations we intended to model was allowing a processor to read the value of its own write before the write is serialized with respect to other writes to the same location. However, we recently realized that due to a subtle interaction with the coherence requirement (i.e., all writes to the same location are to be performed in the same order at all processors), Dubois' "perform with respect to" abstraction is not flexible enough for capturing this optimization. This note presents a simple extension to Dubois' abstraction to alleviate this problem. This modification to the system abstraction affects the semantics for PC and RC (as presented in [GLL + 90]) in a minor manner to now correctly capture the behavior of systems like DASH. Nevertheless, the difference in semantics is not discernable by the majority of programs, and programs that satisfy the properly-labeled programming model (PL [GLL + 90] and PLpc [GAG + 92]) can still be safely executed on such systems. In addition, our previous work on the implementation and performance of various memory models is unaffected by this change.
Abstract-found: 1
Intro-found: 1
Reference: [ABJ + 93] <author> Mustaque Ahamad, Rida Bazzi, Ranjit John, Prince Kohli, and Gil Neiger. </author> <title> The power of processor consistency (extended abstract). </title> <booktitle> In Proceedings of the Fifth Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Our definition of PC is different from the definition provided by Goodman [Goo91]; comparing these two definitions is difficult since the definition in [Goo91] is ambiguous (see <ref> [ABJ + 93] </ref> for an alternative definition of Goodman's PC). To support RCsc, the (C) clause in Condition 2 for RCpc can be replaced by a clause that requires special accesses to be sequentially consistent with respect to one another (see [GLL + 90] for definition of special accesses).
Reference: [AGG + 93] <author> Sarita Adve, Kourosh Gharachorloo, Anoop Gupta, John Hennessy, and Mark Hill. </author> <title> Sufficient system requirements for supporting the PLpc memory model. </title> <type> Technical report, </type> <year> 1993. </year> <note> To appear. </note>
Reference-contexts: Similar proofs are provided by Gibbons et al. [GMG91, GM92] with a correct formalism used for RCsc that captures the optimization we have been discussing. As for PLpc, we show in <ref> [AGG + 93] </ref> that programs that satisfy the PLpc model [GAG + 92] provide sequentially consistent results on systems that obey the PC and RCpc models with the new semantics.
Reference: [Col92] <author> William W. Collier. </author> <title> Reasoning about Parallel Architectures. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1992. </year>
Reference-contexts: Note that the example programs we used in Figures 2 and 3 to illustrate the difference between the old and new semantics are not PL or PLpc programs. 6 system abstractions (e.g., Collier's <ref> [Col92] </ref>, Dubois' [DSB86, SD87]) that are used to specify memory models are unsuitable for properly capturing the behavior of systems that use this optimization. This subtlety has led to problems in formalisms by other researchers as well.
Reference: [DSB86] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: We present our extension to Dubois' abstraction in Section 4. Finally, Section 5 discusses implications of this modification on the semantics of PC and RC and provides some concluding remarks. 2 Dubois' Abstraction and the Conditions for PC and RC This section provides Dubois et al.'s system abstraction <ref> [DSB86, SD87] </ref> and the conditions for PC and RC as they appeared in [GLL + 90]. Definitions 1 and 2 below present Dubois' definitions for the stages that a memory request goes through (these correspond to Definitions 2.1 and 2.2 in [GLL + 90]). <p> Note that the example programs we used in Figures 2 and 3 to illustrate the difference between the old and new semantics are not PL or PLpc programs. 6 system abstractions (e.g., Collier's [Col92], Dubois' <ref> [DSB86, SD87] </ref>) that are used to specify memory models are unsuitable for properly capturing the behavior of systems that use this optimization. This subtlety has led to problems in formalisms by other researchers as well.
Reference: [GAG + 92] <author> Kourosh Gharachorloo, Sarita Adve, Anoop Gupta, John Hennessy, and Mark Hill. </author> <title> Programming for different memory consistency models. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4) </volume> <pages> 399-407, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: For a similar reason, our work on PL [GLL + 90] and PLpc <ref> [GAG + 92] </ref> as higher-level programming models for PC and RCsc/RCpc also remains valid (i.e., PL and PLpc programs still provide sequentially consistent results on such implementations). 7 While the optimization of allowing a processor to read the value of its own write before the write is serialized seems relatively simple, <p> Similar proofs are provided by Gibbons et al. [GMG91, GM92] with a correct formalism used for RCsc that captures the optimization we have been discussing. As for PLpc, we show in [AGG + 93] that programs that satisfy the PLpc model <ref> [GAG + 92] </ref> provide sequentially consistent results on systems that obey the PC and RCpc models with the new semantics.
Reference: [GAG + 93] <author> Kourosh Gharachorloo, Sarita Adve, Anoop Gupta, John Hennessy, and Mark Hill. </author> <title> Specifying system requirements for memory consistency models. </title> <type> Technical report, </type> <year> 1993. </year> <note> To appear. </note>
Reference-contexts: While we have described an extension to Dubois' framework to allow it to capture a desirable optimization, we advocate the use of a more formal and general framework. In <ref> [GAG + 93] </ref>, we present a new framework for specifying conditions for memory models that allows for their precise description.
Reference: [GG91] <author> Kourosh Gharachorloo and Phillip B. Gibbons. </author> <title> Detecting violations of sequential consistency. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991. </year>
Reference: [GGH91a] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 245-257, </pages> <month> April </month> <year> 1991. </year> <title> 8 For example, the outcome (p,q,r,s) = (1,2,0,0) in Figure 2 is allowed by the original TSO model, but not by Kohli et al.'s formalism for TSO. </title> <type> 7 </type>
Reference-contexts: In fact, our 2 Eventually, the writes will get issued, causing updates to be sent to other cache copies and the two writes to location A to be serialized. 4 P2 s = B B = 1 r = A u = A (rel) (acq) (acq) performance results in <ref> [GGH91a] </ref> that show write latency can be fully hidden under PC already assumed this optimization (although our formal definition did not allow it); without it, the processor will inevitably get delayed for part of the write latency. 3 In addition to efficiency reasons, exploiting this optimization can also simplify the design
Reference: [GGH91b] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Two techniques to enhance the performance of memory consistency models. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages I:355-364, </pages> <month> August </month> <year> 1991. </year>
Reference: [GGH92] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Hiding memory latency using dynamic scheduling in shared-memory multiprocessors. </title> <booktitle> In Proceeding of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 22-33, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The definition of "perform" (Definition 1) is modified as follows. 5 3 This optimization is also beneficial in processors that exploit speculative execution <ref> [GGH92] </ref>. Typically, "speculative" writes (e.g., writes depending on an unresolved branch) are buffered until they are no longer considered speculative. The processor can then retract the effect of such writes if the speculation turns out to be incorrect.
Reference: [GHG + 91] <author> Anoop Gupta, John Hennessy, Kourosh Gharachorloo, Todd Mowry, and Wolf-Dietrich Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In Proceeding of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference: [GLL + 90] <author> Kourosh Gharachorloo, Dan Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hen-nessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year> <note> Also available as Stanford University Technical Report CSL-TR-89-405. </note>
Reference-contexts: 1 Organization This note is organized as follows. Section 2 presents Dubois' abstraction and the conditions for processor consistency (PC) and release consistency (RC) as they appeared in <ref> [GLL + 90] </ref>. Section 3 describes the limitation of Dubois' abstraction in capturing the behavior of systems like the DASH multiprocessor [LLG + 92]. We present our extension to Dubois' abstraction in Section 4. <p> Section 5 discusses implications of this modification on the semantics of PC and RC and provides some concluding remarks. 2 Dubois' Abstraction and the Conditions for PC and RC This section provides Dubois et al.'s system abstraction [DSB86, SD87] and the conditions for PC and RC as they appeared in <ref> [GLL + 90] </ref>. Definitions 1 and 2 below present Dubois' definitions for the stages that a memory request goes through (these correspond to Definitions 2.1 and 2.2 in [GLL + 90]). In the following, P i refers to processor i. <p> RC This section provides Dubois et al.'s system abstraction [DSB86, SD87] and the conditions for PC and RC as they appeared in <ref> [GLL + 90] </ref>. Definitions 1 and 2 below present Dubois' definitions for the stages that a memory request goes through (these correspond to Definitions 2.1 and 2.2 in [GLL + 90]). In the following, P i refers to processor i. <p> Given the events defined by this abstraction (e.g., a write performing with respect to a processor), the specification of the memory model is a restriction on the orderings among these events. Before we provide the specific conditions for PC and RC, we describe the implicit assumptions, mentioned in <ref> [GLL + 90] </ref> (page 16), that hold for all the conditions. The first assumption is that uniprocessor control and data dependences are respected. <p> The following two conditions are for PC and RCpc as they appeared in <ref> [GLL + 90] </ref> (Conditions 2.2 and 3.1 in the original paper). Our definition of PC is different from the definition provided by Goodman [Goo91]; comparing these two definitions is difficult since the definition in [Goo91] is ambiguous (see [ABJ + 93] for an alternative definition of Goodman's PC). <p> To support RCsc, the (C) clause in Condition 2 for RCpc can be replaced by a clause that requires special accesses to be sequentially consistent with respect to one another (see <ref> [GLL + 90] </ref> for definition of special accesses). <p> Therefore, the implementation that the PC conditions with Dubois' abstraction capture is a more conservative implementation that delays the read of A until the write is serialized. 3.3 Summary When we originally defined PC in <ref> [GLL + 90] </ref>, our intent was to capture the more aggressive implementation that allows the processor to return the value of its own write before the write is serialized for coherence. <p> The next section discusses the implications of this change in semantics. 5 Discussion and Concluding Remarks Our intent in defining the PC and RCsc/RCpc memory models in <ref> [GLL + 90] </ref> was to capture several optimizations including the specific optimization of allowing a read to return the value of its own processor's write before the write is serialized for coherence purposes. In fact, this optimization was properly captured when RCsc was formalized using a different framework [GMG91]. <p> Our previous work on the implementation and performance of memory models ([GLL + 90, GGH91a, GHG + 91, GG91, GGH91b, GGH92]) is fortunately unaffected by this change simply because we had assumed throughout that our original formalism allows this optimization. For a similar reason, our work on PL <ref> [GLL + 90] </ref> and PLpc [GAG + 92] as higher-level programming models for PC and RCsc/RCpc also remains valid (i.e., PL and PLpc programs still provide sequentially consistent results on such implementations). 7 While the optimization of allowing a processor to read the value of its own write before the write <p> The write's perform event with respect to its own processor corresponds to when the write reaches the coherence serialization point. 7 The proof in <ref> [GLL + 90] </ref>, that shows RCsc provides sequentially consistent results for PL programs, does not require a read to perform after a preceding (in program order) write to the same location as long as the read returns the value of that write or a later write.
Reference: [GM92] <author> Phillip B. Gibbons and Michael Merritt. </author> <title> Specifying nonblocking shared memories. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 158-168, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Similar proofs are provided by Gibbons et al. <ref> [GMG91, GM92] </ref> with a correct formalism used for RCsc that captures the optimization we have been discussing.
Reference: [GMG91] <author> Phillip B. Gibbons, Michael Merritt, and Kourosh Gharachorloo. </author> <title> Proving sequential consistency of high-performance shared memories. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 292-303, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Our modification was influenced by Sindhu et al.'s value axiom in the context of the TSO and PSO models [SFC91] and by the alternative specification of RCsc in <ref> [GMG91] </ref>, both of which correctly capture the mentioned optimization. The conditions for PC and RCsc/RCpc can now be expressed as follows to allow the desired optimization. <p> In fact, this optimization was properly captured when RCsc was formalized using a different framework <ref> [GMG91] </ref>. However, as we showed in this note, our original formalism for expressing these models turns out to be inadequate for capturing this specific optimization due to a subtle interaction among the definitions. We presented a simple extension to the formalism that alleviates the problem. <p> Similar proofs are provided by Gibbons et al. <ref> [GMG91, GM92] </ref> with a correct formalism used for RCsc that captures the optimization we have been discussing.
Reference: [Goo91] <author> James R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report Computer Sciences #1006, </type> <institution> University of Wisconsin, Madison, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: The following two conditions are for PC and RCpc as they appeared in [GLL + 90] (Conditions 2.2 and 3.1 in the original paper). Our definition of PC is different from the definition provided by Goodman <ref> [Goo91] </ref>; comparing these two definitions is difficult since the definition in [Goo91] is ambiguous (see [ABJ + 93] for an alternative definition of Goodman's PC). <p> The following two conditions are for PC and RCpc as they appeared in [GLL + 90] (Conditions 2.2 and 3.1 in the original paper). Our definition of PC is different from the definition provided by Goodman <ref> [Goo91] </ref>; comparing these two definitions is difficult since the definition in [Goo91] is ambiguous (see [ABJ + 93] for an alternative definition of Goodman's PC).
Reference: [KNA93] <author> Prince Kohli, Gil Neiger, and Mustaque Ahamad. </author> <title> A characterization of scalable shared memories. </title> <type> Technical Report GIT-CC-93/04, </type> <institution> Georgia Institute of Technology, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: This subtlety has led to problems in formalisms by other researchers as well. For example, Kohli et al.'s formalism for TSO <ref> [KNA93] </ref> turns out not to be equivalent to the original definition of TSO by Sindhu et al. [SFC91] simply because it fails to model this subtle interaction. 8 Our experience with specifying memory models has made it clear to us that a formal and precise specification methodology is needed.
Reference: [LLG + 90] <author> Dan Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Below, we discuss how conditions for PC (as presented in the previous section) may be satisfied in this implementation. For simplicity, we assume an update-based coherence protocol and a cache line size of 1 word. 1 In DASH, there is a second level write-back cache after the write-buffer <ref> [LLG + 90] </ref>. 2 P1 B=1 B: 0 1 Cache Write Buffer P2 A=2 B: 0 2 Cache Write Buffer Interconnection Network We also assume a simple processor that stalls on a read operation until the return value is back.
Reference: [LLG + 92] <author> Dan Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hen-nessy, Mark Horowitz, and Monica S. Lam. </author> <title> The Stanford Dash multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <year> 1992. </year>
Reference-contexts: 1 Organization This note is organized as follows. Section 2 presents Dubois' abstraction and the conditions for processor consistency (PC) and release consistency (RC) as they appeared in [GLL + 90]. Section 3 describes the limitation of Dubois' abstraction in capturing the behavior of systems like the DASH multiprocessor <ref> [LLG + 92] </ref>. We present our extension to Dubois' abstraction in Section 4.
Reference: [SD87] <author> C. Scheurich and M. Dubois. </author> <title> Correct memory operation of cache-based multiprocessors. </title> <booktitle> In Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 234-243, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: We present our extension to Dubois' abstraction in Section 4. Finally, Section 5 discusses implications of this modification on the semantics of PC and RC and provides some concluding remarks. 2 Dubois' Abstraction and the Conditions for PC and RC This section provides Dubois et al.'s system abstraction <ref> [DSB86, SD87] </ref> and the conditions for PC and RC as they appeared in [GLL + 90]. Definitions 1 and 2 below present Dubois' definitions for the stages that a memory request goes through (these correspond to Definitions 2.1 and 2.2 in [GLL + 90]). <p> Note that the example programs we used in Figures 2 and 3 to illustrate the difference between the old and new semantics are not PL or PLpc programs. 6 system abstractions (e.g., Collier's [Col92], Dubois' <ref> [DSB86, SD87] </ref>) that are used to specify memory models are unsuitable for properly capturing the behavior of systems that use this optimization. This subtlety has led to problems in formalisms by other researchers as well.
Reference: [SFC91] <author> P. S. Sindhu, J.-M. Frailong, and M. Cekleov. </author> <title> Formal specification of memory models. </title> <type> Technical Report (PARC) CSL-91-11, </type> <institution> Xerox Corporation, Palo Alto Research Center, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: provides a simple extension to Dubois' abstraction that alleviates this problem. 4 Extension to Dubois' Abstraction This section presents a simple extension to Dubois' "perform with respect to" framework to make it applicable to specifying the behavior of systems such as DASH and memory models such as TSO and PSO <ref> [SFC91] </ref>. The basic extension involves allowing a read to return the value of its own processor's write before the write is considered performed with respect to its processor. <p> Our modification was influenced by Sindhu et al.'s value axiom in the context of the TSO and PSO models <ref> [SFC91] </ref> and by the alternative specification of RCsc in [GMG91], both of which correctly capture the mentioned optimization. The conditions for PC and RCsc/RCpc can now be expressed as follows to allow the desired optimization. <p> This subtlety has led to problems in formalisms by other researchers as well. For example, Kohli et al.'s formalism for TSO [KNA93] turns out not to be equivalent to the original definition of TSO by Sindhu et al. <ref> [SFC91] </ref> simply because it fails to model this subtle interaction. 8 Our experience with specifying memory models has made it clear to us that a formal and precise specification methodology is needed.
Reference: [SUN91] <institution> The SPARC Architecture Manual. Sun Microsystems Inc., </institution> <month> January </month> <year> 1991. </year> <note> No. 800-199-12, Version 8. 8 </note>
Reference-contexts: This optimization is exploited in DASH and is relatively important from both an efficiency and an ease of design perspective. Several commercial multiprocessors also exploit this optimization. For example, Silicon Graphics and SUN multiprocessors exploit this optimization under models such as TSO and PSO <ref> [SUN91] </ref>. The alternative of delaying a processor's read until its previous write to the same location is serialized for coherence can degrade the performance of the system.
References-found: 21

