URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92202-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Evaluating Parallel Languages for Molecular Dynamics Computations  
Author: Terry W. Clark Reinhard v. Hanxleden Ken Kennedy Charles Koelbel L. Ridgway Scott 
Address: Houston, Houston, TX 77204  Houston, TX 77251  Houston, Houston, TX 77204  
Affiliation: Department of Computer Science, University of  Department of Computer Science, Rice University,  Department of Mathematics, University of  
Abstract: Computational molecular dynamics is an important application requiring large amounts of computing time. Parallel processing offers very high performance potential, but irregular problems like molecular dynamics have proven difficult to map onto parallel machines. In this paper, we describe the practicalities of porting a basic molecular dynamics computation to a distributed-memory machine. In the process, we show how program annotations can aid in parallelizing a moderately complex code. We also argue that algorithm replacement may be necessary in parallelization, a task which cannot be performed automatically. We close with some results from a parallel GROMOS implementation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Bagheri, T. W. Clark, and L. R. Scott. </author> <title> IPfortran (a parallel extension of Fortran) reference manual. </title> <institution> Research Report UH/MD-119, Dept. of Mathematics, University of Houston, </institution> <year> 1991. </year>
Reference-contexts: Both languages provide support for this, in contrast to other languages which required those operations to be programmed explicitly. The languages differ, however, in that one uses a local memory model and the other uses a global memory model. IPfortran utilizes a local memory model <ref> [1] </ref>; the key concept of IPfortran is to provide a better abstraction for interprocessor communication than simple message-passing [15]. IPfortran programs use an SPMD (Single-Program Multiple-Data) style of programming.
Reference: [2] <author> H. Berryman, J. Saltz, and J. Scroggs. </author> <title> Execution time support for adaptive scientific algorithms on distributed memory machines. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(3) </volume> <pages> 159-178, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: However, to allow message blocking, the compiler should still provide buffer space for all elements of F which are accessed nonlocally. The global combination would then be performed via a reduction operation like scatter add <ref> [2] </ref>. A typical implementation of scatter add 4 would again sum all local contributions to a particu-lar array element up before combining them globally with other contributions. However, it would probably not use a dimensional exchange, but instead send point-to-point messages.
Reference: [3] <author> D. K. Bradley. </author> <title> First and second generation hypercube performance. </title> <type> Technical Report UIUCDCS-R-88-1455, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1988. </year>
Reference-contexts: Compiling this navely, without message blocking, would result in roughly P airs ave (short) messages per processor. For systems whose communication time to send m units of data is well modeled by + fim with fi <ref> [3] </ref>, this would increase the communication cost by a factor =fi. This would make it unacceptable to send individual messages for each nonlocal access, instead of combining them at the end of the loop. Another issue besides raw message blocking is how much we can gain by combining non-local reductions.
Reference: [4] <author> T. W. Clark, R. v. Hanxleden, and L. R. Scott. </author> <title> Scalable algorithms for molecular dynamics computations. </title> <type> Technical report, </type> <institution> Dept. of Mathematics, University of Houston, </institution> <note> to appear. </note>
Reference-contexts: This is inherited from sequential GROMOS, another parallel version we are currently developing overcomes this limitation using a hierarchical decomposition <ref> [4] </ref>. Note that this implementation replicates the force array F . A Fortran D version of this kind of algorithm can be written by expanding each array by one dimension (the processor dimension), the introduced index being the processor number, and then distributing that dimension blockwise. <p> The key feature of a Fortran D program is the data distribution, which the compiler uses to generate the low-level communications operations. We plan to continue this work in several areas. In the computational molecular dynamics area, we will continue to design and implement scalable algorithms, producing high-performance codes <ref> [4] </ref>.
Reference: [5] <author> T. W. Clark and J. A. McCammon. </author> <title> Parallelization of a molecular dynamics non-bonded force algorithm for MIMD architectures. </title> <journal> Computers & Chemistry, </journal> <volume> 14(3) </volume> <pages> 219-224, </pages> <year> 1990. </year>
Reference-contexts: Molecular dynamics algorithms commonly iterate 1 over the sequence: 1. Calculate bonded and nonbonded forces on each atom as the analytical gradient of a potential-energy function of the atom positions. The pairwise, nonbonded interactions dominate the computation with O (N 2 ) time complexity <ref> [5] </ref> and therefore are key considerations in both the model and its implementation [6], see Sections 4 and 5. 2. Integrate Newton's equations of motion to determine the new atomic momenta and positions.
Reference: [6] <author> T. W. Clark, J. A. McCammon, and L. R. Scott. </author> <title> Parallel molecular dynamics. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Houston, TX, </address> <month> March </month> <year> 1991. </year>
Reference-contexts: Our examples will illustrate the need for modification of algorithms to achieve scalability. The remainder of this paper is organized as follows. Section 2 describes the GROMOS code [10], a standard molecular dynamics program that we are paral-lelizing <ref> [6] </ref>. Section 3 gives a short description of the parallel languages used, IPfortran and Fortran D. Sections 4, 5, and 6 each describe the parallelization of one phase of GROMOS. <p> Calculate bonded and nonbonded forces on each atom as the analytical gradient of a potential-energy function of the atom positions. The pairwise, nonbonded interactions dominate the computation with O (N 2 ) time complexity [5] and therefore are key considerations in both the model and its implementation <ref> [6] </ref>, see Sections 4 and 5. 2. Integrate Newton's equations of motion to determine the new atomic momenta and positions. By removing uninteresting, high-frequency motions, larger timesteps can be taken resulting in a more efficient computer utilization [19, 20]. <p> Note also that in practice the forces F and the positions X are vectors in IR 3 . In the IPfortran implementation <ref> [6] </ref>, each processor executes the outer loop of the sequential version for a range (f irstI (me) : lastI (me)) of atom indices. (Here, 2 DO I = 1, N DO J = firstJ (I), lastJ (I) force = nbf (X (I) X (JNB (J))) F (I) = F (I) + <p> In IPfortran we use processor specific loop bounds; the Fortran D version distributes JNBL and uses the owner computes rule. The range sizes vary across processors due to the triangular shape of the double loop <ref> [6] </ref>. Each processor will have a local copy of only part of the JNB array.
Reference: [7] <author> G. C. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kre-mer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year> <note> Revised April, </note> <year> 1991. </year>
Reference-contexts: This set of nonlocal access and reduction operations supports programming at a convenient level of abstraction, while still allowing a relatively simple compiler to produce excellent code. Fortran D utilizes a global memory model, providing a modified shared name-space for array elements <ref> [7] </ref>. Arrays are declared to be their full, global size and are aligned with virtual decompositions which are distributed across processors. Statements are executed sequentially (except for the FORALL loop, for which the iterations conceptually execute simultaneously). <p> Instead of replicating the force array F , we should distribute it to distribute the workload. To allow load balancing, we distribute the data irregularly using a mapping array M ap <ref> [7] </ref>, for which firstI (p) I lastI (p) , M ap (I) = p: must hold. We should also rethink how to distribute the neighbor list JNB , which represents the largest data structure of the problem.
Reference: [8] <author> G. C. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Multiprocessors. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: A more sophisticated divide-and-conquer approach works in O (N ), with 2 fi log P messages per processor <ref> [8] </ref>. If we use the latter approach and have a balanced workload, then we have communication cost T comm / N and computation costs T comp / P airs ave . <p> Each processor will have a local copy of only part of the JNB array. However, for performing load balancing we need to know all of INB , which can be collected in O (N ) with O (log P ) communication steps using a dimensional exchange <ref> [8] </ref>, as seen for example in Figure 7. 6 The SHAKE algorithm SHAKE utilizes a typical form of relaxation to solve a system of constraints regarding the distance (or angles) between particular atoms.
Reference: [9] <author> G. Ganti and J. A. McCammon. </author> <title> Transport properties of macromolecules by Brownian dynamics simulation: Vector-ization of Brownian dynamics on the Cyber-205. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 7(4) </volume> <pages> 457-463, </pages> <year> 1986. </year>
Reference-contexts: A simple solution would be to use instead the Jacobi iteration ~ new 0 X a ij ~ old 1 . followed by the assignment ~ new ~ old . This is now perfectly parallelizable, but is (usually) a more slowly convergent algorithm than Gauss-Seidel <ref> [9] </ref>. To overcome this problem, it is common to use a compound algorithm for parallel computation which involves a Jacobi iteration across processors, but a Gauss-Seidel iteration interior to each processor.
Reference: [10] <author> W. F. van Gunsteren and H. J. C. Berendsen. GRO-MOS: </author> <title> GROningen MOlecular Simulation software. </title> <type> Technical report, </type> <institution> Laboratory of Physical Chemistry, University of Groningen, </institution> <address> Nijenborgh, The Netherlands, </address> <year> 1988. </year>
Reference-contexts: We also consider the tradeoffs of paralleliz-ing existing sequential code ("dusty deck") and writing a parallel program from scratch. Our examples will illustrate the need for modification of algorithms to achieve scalability. The remainder of this paper is organized as follows. Section 2 describes the GROMOS code <ref> [10] </ref>, a standard molecular dynamics program that we are paral-lelizing [6]. Section 3 gives a short description of the parallel languages used, IPfortran and Fortran D. Sections 4, 5, and 6 each describe the parallelization of one phase of GROMOS. <p> This usually involves the constraining of some molecular motions, as discussed in Section 6. 3. Save data as appropriate for post analysis. The molecular dynamics program used in this study is from the GROMOS (GROningen MOlecular Simulation) suite designed for the dynamic modeling of biomolecules <ref> [10] </ref>. GROMOS provides programs for the simulation of biological molecules (and arbitrary molecules) using molecular dynamics or stochastic dynamics. In addition, energy minimization and analysis programs are provided.
Reference: [11] <author> R. v. Hanxleden and K. Kennedy. </author> <title> Relaxing SIMD control flow constraints using loop transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: In the computational molecular dynamics area, we will continue to design and implement scalable algorithms, producing high-performance codes [4]. Both the IPfortran and Fortran D implementations will continue to go forward, and the lessons from this study and others like it will affect their development <ref> [11, 13] </ref>. 9 Acknowledgements We thank Professor McCammon for many helpful discussions and the Institute for Molecular Design (IMD) and Intel for providing computing facilities for this work. McCammon and the IMD are supported in part by the National Science Foundation with additional support from Intel.
Reference: [12] <author> R. v. Hanxleden and L. R. Scott. </author> <title> Load balancing on message passing architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 312-324, </pages> <year> 1991. </year>
Reference-contexts: Therefore, the overall computational cost is given by T comp / P p=1 P airs (p) = P p=1 lastI (p) X I=firstI (p) INB (I): This results in a typical load balancing problem <ref> [12] </ref>, where the goal is to lower T comp down to T ideal / P airs ave .
Reference: [13] <institution> Proceedings of the High Performance Fortran Forum, Hous-ton, TX, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: In this kind of situation, where we want to use different algorithms depending on whether we operate within processors or across them, it is useful to have an escape mechanism from the default, global level into the processor level. Here the concept of local blocks is very useful <ref> [13, Thinking Machines proposal] </ref>. The Fortran D code in can realize this concept. <p> In the computational molecular dynamics area, we will continue to design and implement scalable algorithms, producing high-performance codes [4]. Both the IPfortran and Fortran D implementations will continue to go forward, and the lessons from this study and others like it will affect their development <ref> [11, 13] </ref>. 9 Acknowledgements We thank Professor McCammon for many helpful discussions and the Institute for Molecular Design (IMD) and Intel for providing computing facilities for this work. McCammon and the IMD are supported in part by the National Science Foundation with additional support from Intel.
Reference: [14] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Resembling the "vectorizable style" associated with vector architectures [23], an equivalent challenge in the parallel arena is the development of a "machine-independent parallel programming style," a long-term research target of Fortran D <ref> [14] </ref>. * IPfortran improves on the usual message-passing environment by eliminating the need for explicit "send" operations. This simplifies code significantly, allowing the programmer to concentrate on higher-level problems such as load balancing.
Reference: [15] <author> C. A. R. Hoare. </author> <title> Communicating Sequential Processes. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1985. </year>
Reference-contexts: The languages differ, however, in that one uses a local memory model and the other uses a global memory model. IPfortran utilizes a local memory model [1]; the key concept of IPfortran is to provide a better abstraction for interprocessor communication than simple message-passing <ref> [15] </ref>. IPfortran programs use an SPMD (Single-Program Multiple-Data) style of programming. Variables are implicitly local to each processor; thus, X on processor 1 may have a different value from X on processor 2.
Reference: [16] <author> C. Lin and L. Snyder. </author> <title> A comparison of programming models for shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <volume> Vol. 2, </volume> <pages> pages 163-170, </pages> <year> 1991. </year>
Reference-contexts: Exploiting data parallelism on distributed-memory MIMD machines requires careful partitioning of the data and computation for good locality, which can be beneficial for shared-memory machines as well <ref> [16] </ref>. A number of language extensions have been studied which allow the programmer to provide such distribution information. In this work, we examine the facilities available in two such extended languages. We also consider the tradeoffs of paralleliz-ing existing sequential code ("dusty deck") and writing a parallel program from scratch.
Reference: [17] <author> J. A. McCammon. </author> <title> Computer-aided molecular design. </title> <journal> Science, </journal> <volume> 238 </volume> <pages> 486-491, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Sections 4, 5, and 6 each describe the parallelization of one phase of GROMOS. Section 7 gives some performance results, followed by conclusions in Section 8. 2 Molecular dynamics First developed for simulating atomic motion in simple liquids, molecular dynamics is used routinely to simulate biomolecular systems <ref> [17] </ref>. Using the compute-intensive data obtained from a molecular dynamics simulation, various kinetic, thermodynamic, mechanistic, and structural properties can be obtained [18]. In molecular dynamics, the motion of each atom, represented as a point mass, is determined by the forces exerted on it by other atoms.
Reference: [18] <author> J. A. McCammon and Stephen C. Harvey. </author> <title> Dynamics of proteins and nucleic acids. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Using the compute-intensive data obtained from a molecular dynamics simulation, various kinetic, thermodynamic, mechanistic, and structural properties can be obtained <ref> [18] </ref>. In molecular dynamics, the motion of each atom, represented as a point mass, is determined by the forces exerted on it by other atoms. Molecular dynamics algorithms commonly iterate 1 over the sequence: 1.
Reference: [19] <author> F. Muller-Plathe and D. Brown. </author> <title> Multi-colour algorithms in molecular simulation: Vectorisation and parallelisation of internal forces and constraints. </title> <journal> Computer Physics Communications, </journal> <volume> 64 </volume> <pages> 7-14, </pages> <year> 1991. </year>
Reference-contexts: Integrate Newton's equations of motion to determine the new atomic momenta and positions. By removing uninteresting, high-frequency motions, larger timesteps can be taken resulting in a more efficient computer utilization <ref> [19, 20] </ref>. This usually involves the constraining of some molecular motions, as discussed in Section 6. 3. Save data as appropriate for post analysis. The molecular dynamics program used in this study is from the GROMOS (GROningen MOlecular Simulation) suite designed for the dynamic modeling of biomolecules [10].
Reference: [20] <author> J. Rycaert, G. Ciccotti, and H. J. C. Berendsen. </author> <title> Numerical integration of the cartesian equations of motion of a system with constraints: Molecular dynamics of n-Alkanes. </title> <journal> Journal of Computational Physics, </journal> <volume> 23 </volume> <pages> 327-341, </pages> <year> 1977. </year>
Reference-contexts: Integrate Newton's equations of motion to determine the new atomic momenta and positions. By removing uninteresting, high-frequency motions, larger timesteps can be taken resulting in a more efficient computer utilization <ref> [19, 20] </ref>. This usually involves the constraining of some molecular motions, as discussed in Section 6. 3. Save data as appropriate for post analysis. The molecular dynamics program used in this study is from the GROMOS (GROningen MOlecular Simulation) suite designed for the dynamic modeling of biomolecules [10].
Reference: [21] <author> L. R. Scott, J. M. Boyle, and B. Bagheri. </author> <title> Distributed data structures for scientific computation. </title> <editor> In M. T. Heath, editor, </editor> <booktitle> Proceedings of the 3rd Hypercube Multiprocessors Conference, </booktitle> <pages> pages 55-66, </pages> <address> Philadelphia, PA, </address> <year> 1987. </year>
Reference-contexts: molecular dynamics the following observations will apply. * Scalability in molecular dynamics is an achievable goal, but it requires careful algorithm design where the choice of the right algorithm may also depend on the input characteristics. * Both local and global models are feasible interfaces for programming distributed-memory parallel machines <ref> [21] </ref>. In addition, both models can provide the user with a higher-level programming interface than current message-passing languages on distributed-memory machines. * As to be expected, regardless of the implementation model chosen, the key to a good parallel program is the choice of an appropriate algorithm.
Reference: [22] <author> J. Shen and J. A. McCammon. </author> <title> Molecular dynamics simulation of Superoxide interacting with Superoxide Dismutase. </title> <journal> Chemical Physics, </journal> <volume> 158 </volume> <pages> 191-198, </pages> <year> 1991. </year>
Reference-contexts: The calculation on an iPSC/860 uses a model for the enzyme 6 Superoxide Dismutase <ref> [22] </ref>, with a total of 6968 atoms, for 500 timesteps. Both the overall execution times and the breakdowns into the principal sections of the calculation are given. The dominating parts of the sequential algorithm, the nonbonded forces and pair list, have been paral-lelized with nearly perfect speedup.
Reference: [23] <author> M. J. Wolfe. </author> <title> Semi-automatic domain decomposition. </title> <booktitle> In Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year> <month> 8 </month>
Reference-contexts: This implies that programs will have to be rewritten to some extent for parallelization, rather than relying on compiler optimization of "dusty deck" code. Resembling the "vectorizable style" associated with vector architectures <ref> [23] </ref>, an equivalent challenge in the parallel arena is the development of a "machine-independent parallel programming style," a long-term research target of Fortran D [14]. * IPfortran improves on the usual message-passing environment by eliminating the need for explicit "send" operations.
References-found: 23

