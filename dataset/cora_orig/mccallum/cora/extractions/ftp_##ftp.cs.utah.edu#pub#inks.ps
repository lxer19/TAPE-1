URL: ftp://ftp.cs.utah.edu/pub/inks.ps
Refering-URL: http://www.cs.utah.edu/~baford/pub.html
Root-URL: 
Email: E-mail: flepreau,mike,baford,lawg@cs.utah.edu  
Title: In-Kernel Servers on Mach 3.0: Implementation and Performance  
Author: Jay Lepreau Mike Hibler Bryan Ford Jeffrey Law 
Address: Salt Lake City, UT 84112  
Affiliation: Center for Software Science Department of Computer Science University of Utah  
Abstract: The advantages in modularity and power of microkernel-based operating systems such as Mach 3.0 are well known. The existing performance problems of these systems, however, are significant. Much of the performance degradation is due to the cost of maintaining separate protection domains, traversing software layers, and using a semantically rich inter-process communication mechanism. An approach that optimizes the common case is to permit merging of protection domains in performance critical applications, while maintaining protection boundaries for debugging or in situations that demand robustness. In our system, client calls to the server are effectively bound either to a simple system call interface, or to a full RPC mechanism, depending on the server's location. The optimization reduces argument copies, as well as work done in the control path to handle complex and infrequently encountered message types. In this paper we present a general method of doing this for Mach 3.0 and the results of applying it to the Mach microkernel and the OSF/1 single server. We describe the necessary modifications to the kernel, the single server, and the RPC stub generator. Semantic equivalence, backwards compatibility, and common source and binary code are preserved. Performance on micro and macro benchmarks is reported, with RPC performance improving by a factor of three, Unix system calls to the server improving between 20% and a factor of two, and 4-13% performance gain on large benchmarks. A breakdown of the times on the RPC path is also presented. 1 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Mike Accetta, Robert Baron, William Bolosky, David Golub, Richard Rashid, Avadis Tevanian, and Michael Young. </author> <title> Mach: A new kernel foundation for UNIX development. </title> <booktitle> In Proceedings of the Summer 1986 USENIX Conference, </booktitle> <pages> pages 93-112, </pages> <address> Atlanta, GA, June 9-13, 1986. </address> <publisher> Usenix Association. </publisher>
Reference-contexts: A few common I/O operations, such as Unix read, are often implemented with shared memory and can be reasonably fast. Although the Mach 3.0 <ref> [1, 8] </ref> microkernel has shown isolated instances of performance comparable to that of macrokernel systems [5], this has not been observed in general. Many software layers and some hardware boundaries must be traversed even for the most trivial interactions between tasks.
Reference: [2] <author> B. Bershad, R. Draves, and A. Forin. </author> <title> Using microbenchmarks to evaluate system performance. </title> <type> Technical report, </type> <institution> Carnegie Mellon University, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: However, in both in- and out-of-kernel cases we use the same server, so this is not a factor. In general, we do not expect cache effects to be significant and consistent in this experiment. Other studies have shown that random changes can have significant cache effects <ref> [2] </ref>. 5 Limitations A serious limitation that arises from use of this framework is the loss of the protection that separate address spaces provides. Hardware protection is an important mechanism used to provide robustness in the face of unfriendly or malicious programs.
Reference: [3] <author> Brian N. Bershad, Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> Lightweight remote procedure call. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 37-55, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: In the long term, however, we believe that thread-switching is only a temporary way to avoid a problem in Mach. If a general-purpose migrating threads model such as that exploited in LRPC <ref> [3] </ref> is introduced to Mach in the future, this approach to in-kernel server traps will become both faster and cleaner. 3.4.2 Server Task Many of the complexities and shortcomings of the thread migration model could be avoided and the benefits retained by altogether eliminating the notion of a separate server task.
Reference: [4] <author> Bryan Ford, Mike Hibler, and Jay Lepreau. </author> <title> Extending the Mach 3.0 thread model. </title> <type> Technical Report UUCS-93-012, </type> <institution> University of Utah Computer Science Department, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: If effect, the user thread has "migrated" from its task into the server task. This has numerous implications that are briefly discussed later, and in more detail in <ref> [4] </ref>. 3 Implementation To date we have experimented with the OSF/1 server and a number of special purpose micro-servers. <p> In <ref> [4] </ref> we discuss in detail the issues related to this decision, so only a brief summary is presented here. A thread switching model would have resulted in a cleaner, safer 9 implementation and would have fit better with Mach's current thread semantics.
Reference: [5] <author> D. Golub, R. Dean, A. Forin, and R. Rashid. </author> <title> Unix as an application program. </title> <booktitle> In Proceedings of the Summer 1990 USENIX Conference, </booktitle> <pages> pages 87-96, </pages> <address> Anaheim, California, June 11-15, 1990. </address> <publisher> Usenix Association. </publisher>
Reference-contexts: A few common I/O operations, such as Unix read, are often implemented with shared memory and can be reasonably fast. Although the Mach 3.0 [1, 8] microkernel has shown isolated instances of performance comparable to that of macrokernel systems <ref> [5] </ref>, this has not been observed in general. Many software layers and some hardware boundaries must be traversed even for the most trivial interactions between tasks.
Reference: [6] <author> M. Guillemont, J. Lipkis, D. Orr, and M. Rozier. </author> <title> A second-generation micro-kernel based unix; lessons in performance and compatibility. </title> <booktitle> In Proceedings of the Winter 1991 USENIX Conference, </booktitle> <pages> pages 13-22, </pages> <address> Dallas, TX, </address> <month> Winter </month> <year> 1991. </year>
Reference-contexts: Further measurement of the argument type and message frequencies is needed to determine whether it is worthwhile to sort-circuit messages for these arguments. 15 6 Related Work The most similar work to ours is that done in the Chorus <ref> [12, 6] </ref> microkernel/multi-server Unix system. Chorus "supervisor actors" correspond to our in-kernel servers, but there are significant differences. Chorus adopts the "migrating-thread" model, with a single kernel stack which remains with the thread.
Reference: [7] <author> Daniel P. Julin, Jonathan J. Chew, J. Mark Stevenson, Paulo Guedes, Paul Neves, and Paul Roy. </author> <title> Generalized emulation services for Mach 3.0 | overview, experiences and current status. </title> <booktitle> In Proc. of the Second Usenix Mach Symposium, </booktitle> <pages> pages 13-26, </pages> <year> 1991. </year>
Reference-contexts: Further investigation of the "ramp up" and scaling problems will likely reveal bottlenecks. Completing the support for server-server interactions would allow us quantify the improvement in a multi-server environment where the speedup should be substantially greater, due to the increased numbers of inter-domain transfers <ref> [7] </ref>. Optimizing server-kernel interactions should be examined, as it could potentially result in up to two orders of magnitude reduction in overhead (message vs. procedure call). If this optimization were done, note that it generalizes the existing mechanism supporting alternate system call paths to a few Mach kernel calls.
Reference: [8] <institution> Open Systems Foundation and Carnegie Mellon University., </institution> <address> Cambridge MA. </address> <booktitle> MACH 3 Kernel Interface, </booktitle> <year> 1992. </year>
Reference-contexts: A few common I/O operations, such as Unix read, are often implemented with shared memory and can be reasonably fast. Although the Mach 3.0 <ref> [1, 8] </ref> microkernel has shown isolated instances of performance comparable to that of macrokernel systems [5], this has not been observed in general. Many software layers and some hardware boundaries must be traversed even for the most trivial interactions between tasks.
Reference: [9] <author> Douglas B. Orr and Robert W. Mecklenburg. </author> <title> OMOS | an object server for program execution. </title> <booktitle> In Proc. Second International Workshop on Object Orientation in Operating Systems, </booktitle> <pages> pages 200-209, </pages> <address> Paris, France, </address> <month> September </month> <year> 1992. </year> <journal> IEEE Computer Society. </journal>
Reference-contexts: We wanted to make the required system transformations acceptable, by imposing minimal constraints on Mach program structure. We wanted to explore the issues involved, and finally, we wanted a sample system as a target for a more dynamic binding mechanism, based on an object server we have implemented <ref> [9, 10] </ref>. 2.2 Compatibility Constraints To maintain compatibility, we imposed a number of constraints on the design: * The source for in- and out-of-kernel servers must be identical, with little use of conditional compilation. * All changes to the microkernel must be backwards compatible. <p> This would remove it from the realm of special-purpose "hacks" and make it a universally applied optimization. In the longer view, we plan to use the powerful linking capabilities of the OMOS meta-object/object server <ref> [9, 10] </ref> to bind more flexibly and dynamically. OMOS can hook pre-existing programs and modules together in a structured and programmable way. We will move toward making OMOS capable of dynamically binding arbitrary modules with arbitrary interfaces (C-style function calls, C++ method invocations) communicating across arbitrary channels.
Reference: [10] <author> Douglas B. Orr, Robert W. Mecklenburg, Peter J. Hoogenboom, and Jay Lepreau. </author> <title> Dynamic program monitoring and transformation using the OMOS object server. </title> <booktitle> In Proceedings of the 26th Hawaii International Conference on System Sciences, </booktitle> <pages> pages 232-241, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: We wanted to make the required system transformations acceptable, by imposing minimal constraints on Mach program structure. We wanted to explore the issues involved, and finally, we wanted a sample system as a target for a more dynamic binding mechanism, based on an object server we have implemented <ref> [9, 10] </ref>. 2.2 Compatibility Constraints To maintain compatibility, we imposed a number of constraints on the design: * The source for in- and out-of-kernel servers must be identical, with little use of conditional compilation. * All changes to the microkernel must be backwards compatible. <p> This would remove it from the realm of special-purpose "hacks" and make it a universally applied optimization. In the longer view, we plan to use the powerful linking capabilities of the OMOS meta-object/object server <ref> [9, 10] </ref> to bind more flexibly and dynamically. OMOS can hook pre-existing programs and modules together in a structured and programmable way. We will move toward making OMOS capable of dynamically binding arbitrary modules with arbitrary interfaces (C-style function calls, C++ method invocations) communicating across arbitrary channels.
Reference: [11] <author> John Ousterhout. </author> <title> Why aren't operating systems getting faster as fast as hardware? In USENIX Conference Proceedings, </title> <address> pages 247-256, Anaheim, CA, </address> <month> Summer </month> <year> 1990. </year> <booktitle> Usenix. </booktitle>
Reference-contexts: (trap) 41480 17.5% Mach Syscalls (msg) 45654 19.3% Total From Server 87134 36.8% the Mach interface such as device I/O, while trapping syscalls are used for the very common cases like vm_allocate. 4.5 Macro Benchmark Results We also ran somewhat more realistic Unix workloads, in particular the modified Andrew benchmark <ref> [11] </ref> and a Mach kernel build. The more realistic kernel build benchmark showed a 13% speedup under trapping INKS, while the Andrew benchmark yielded 4%. Table 6 shows these timing in seconds, with the ratio to trapping INKS in parentheses.
Reference: [12] <author> M. Rozier, V. Abrossimov, F. Armand, I. Boule, M. Gien, M. Guillemont, F. Herrmann, C. Kaiser, S. Langlois, P. Leonard, and W. Neuhauser. </author> <title> Chorus distributed operating systems. </title> <journal> Computing Systems, </journal> <volume> 1(4) </volume> <pages> 305-367, </pages> <year> 1988. </year>
Reference-contexts: Further measurement of the argument type and message frequencies is needed to determine whether it is worthwhile to sort-circuit messages for these arguments. 15 6 Related Work The most similar work to ours is that done in the Chorus <ref> [12, 6] </ref> microkernel/multi-server Unix system. Chorus "supervisor actors" correspond to our in-kernel servers, but there are significant differences. Chorus adopts the "migrating-thread" model, with a single kernel stack which remains with the thread.
Reference: [13] <author> Bob Wheeler and Brian N. Bershad. </author> <title> Consistency management for virtually indexed caches. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 124-136, </pages> <month> October </month> <year> 1992. </year> <month> 17 </month>
Reference-contexts: This is the port done by Utah, and does not yet include the virtual cache improvements from CMU <ref> [13] </ref>. Little of the port has been optimized, and in particular, the existing implementation of the system call/context switch path is not at all optimal. This likely affects the breakdown of RPC costs, but probably has only a marginal affect on the overall results.
References-found: 13

