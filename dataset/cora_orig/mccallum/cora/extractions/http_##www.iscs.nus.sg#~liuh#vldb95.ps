URL: http://www.iscs.nus.sg/~liuh/vldb95.ps
Refering-URL: http://www.cs.bham.ac.uk/~anp/bibtex/kdd.bib.html
Root-URL: 
Email: fluhj,rudys,liuhg@iscs.nus.sg  
Title: NeuroRule: A Connectionist Approach to Data Mining  
Author: Hongjun Lu Rudy Setiono Huan Liu 
Address: Singapore  
Affiliation: Department of Information Systems and Computer Science National University of  
Abstract: Classification, which involves finding rules that partition a given data set into disjoint groups, is one class of data mining problems. Approaches proposed so far for mining classification rules for large databases are mainly decision tree based symbolic learning methods. The connectionist approach based on neural networks has been thought not well suited for data mining. One of the major reasons cited is that knowledge generated by neural networks is not explicitly represented in the form of rules suitable for verification or interpretation by humans. This paper examines this issue. With our newly developed algorithms, rules which are similar to, or more concise than those generated by the symbolic methods can be extracted from the neural networks. The data mining process using neural networks with the emphasis on rule extraction is described. Experimental results and comparison with previously published works are presented.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, S. Ghosh, T. Imielinski, B. Iyer, and A. Swami. </author> <title> An interval classifier for database mining approaches. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <year> 1992. </year>
Reference-contexts: Proceedings of the 21st VLDB Conference Zurich, Swizerland, 1995 data are a valuable asset of an organization, most organizations may face the problem of data rich but knowledge poor sooner or later. This situation aroused the recent surge of research interests in the area of data mining <ref> [1, 9, 2] </ref>. One of the data mining problems is classification. Data items in databases, such as tuples in relational database systems usually represent real world entities. The values of the attributes of a tuple represent the properties of the entity. <p> The results are often expressed in the form of rules the classification rules. By applying the rules, entities represented by tuples can be easily classified into different classes they belong to. We can restate the problem formally defined by Agrawal et al. <ref> [1] </ref> as follows. Let A be a set of attributes A 1 ; A 2 ; : : : ; A n and dom (A i ) refer to the set of possible values for attribute A i . <p> In our study, we use the hyperbolic tangent function f (x) := ffi (x) = (e x e x )=(e x + e x ) as the activation function for the hidden nodes, which makes the range of activation values of the hidden nodes <ref> [-1, 1] </ref>. <p> The activation function used here is the sigmoid function, (x) = 1=(1 + e x ); which yields activation values of the output nodes in the range <ref> [0, 1] </ref>. <p> of the test data adapted from Agrawal et al.[2] Attribute Description Value salary salary uniformly distributed from 20,000 to 150,000 commission commission if salary 75000 ! commission = 0 else uniformly distributed from 10000 to 75000. age age uniformly distributed from 20 to 80. elevel education level uniformly distributed from <ref> [0; 1; : : :; 4] </ref>. car make of the car uniformly distributed from [1; 2; : : :20]. zipcode zip code of the town uniformly chosen from 9 available zipcodes. hvalue value of the house uniformly distributed from 0.5k10000 to 1.5k1000000 where k 2 f0 : : : 9g depends <p> uniformly distributed from 20,000 to 150,000 commission commission if salary 75000 ! commission = 0 else uniformly distributed from 10000 to 75000. age age uniformly distributed from 20 to 80. elevel education level uniformly distributed from [0; 1; : : :; 4]. car make of the car uniformly distributed from <ref> [1; 2; : : :20] </ref>. zipcode zip code of the town uniformly chosen from 9 available zipcodes. hvalue value of the house uniformly distributed from 0.5k10000 to 1.5k1000000 where k 2 f0 : : : 9g depends on zipcode. hyears years house owned uniformly distributed from [1; 2; : : :; <p> car uniformly distributed from [1; 2; : : :20]. zipcode zip code of the town uniformly chosen from 9 available zipcodes. hvalue value of the house uniformly distributed from 0.5k10000 to 1.5k1000000 where k 2 f0 : : : 9g depends on zipcode. hyears years house owned uniformly distributed from <ref> [1; 2; : : :; 30] </ref>. loan total amount of loan uniformly distributed from 1 to 500000. Neural network pruning algorithm (NP) 1. Let 1 and 2 be positive scalars such that 1 + 2 &lt; 0:5. 2. Pick a fully connected network.
Reference: [2] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 5(6), </volume> <month> December </month> <year> 1993. </year>
Reference-contexts: Proceedings of the 21st VLDB Conference Zurich, Swizerland, 1995 data are a valuable asset of an organization, most organizations may face the problem of data rich but knowledge poor sooner or later. This situation aroused the recent surge of research interests in the area of data mining <ref> [1, 9, 2] </ref>. One of the data mining problems is classification. Data items in databases, such as tuples in relational database systems usually represent real world entities. The values of the attributes of a tuple represent the properties of the entity. <p> It is imperative that during training these weights be prevented from getting too large. At the same time, small weights should be encouraged to decay rapidly to zero. By using penalty function (3), we can achieve both. 2.3 An example We have chosen to use a function described in <ref> [2] </ref> as an example to show how a neural network can be trained and pruned for solving a classification problem. The Page 4 Table 1: Attributes of the test data adapted from Agrawal et al.[2] Attribute Description Value salary salary uniformly distributed from 20,000 to 150,000 commission commission if salary 75000 <p> uniformly distributed from 20,000 to 150,000 commission commission if salary 75000 ! commission = 0 else uniformly distributed from 10000 to 75000. age age uniformly distributed from 20 to 80. elevel education level uniformly distributed from [0; 1; : : :; 4]. car make of the car uniformly distributed from <ref> [1; 2; : : :20] </ref>. zipcode zip code of the town uniformly chosen from 9 available zipcodes. hvalue value of the house uniformly distributed from 0.5k10000 to 1.5k1000000 where k 2 f0 : : : 9g depends on zipcode. hyears years house owned uniformly distributed from [1; 2; : : :; <p> car uniformly distributed from [1; 2; : : :20]. zipcode zip code of the town uniformly chosen from 9 available zipcodes. hvalue value of the house uniformly distributed from 0.5k10000 to 1.5k1000000 where k 2 f0 : : : 9g depends on zipcode. hyears years house owned uniformly distributed from <ref> [1; 2; : : :; 30] </ref>. loan total amount of loan uniformly distributed from 1 to 500000. Neural network pruning algorithm (NP) 1. Let 1 and 2 be positive scalars such that 1 + 2 &lt; 0:5. 2. Pick a fully connected network. <p> Retrain the network. If accuracy of the network falls below an acceptable level, then stop. Other wise, go to Step 3. input tuple consists of nine attributes defined in Ta ble 1. Ten classification problems are given in <ref> [2] </ref>. Limited by space, we will present and discuss a few functions and the experimental results. <p> The training data set consisted of 1000 tuples. The values of the attributes of each tuple were generated randomly according to the distributions given in Table 1. Following Agrawal et al. <ref> [2] </ref>, we also included a perturbation factor as one of the parameters of the random data generator. This perturbation factor was set at 5 percent. For each tuple, a class label was determined according to the rules that define the function above. <p> In this section, we report the experimental results of applying the approach described in the previous sections to the data mining problem defined in <ref> [2] </ref>. As mentioned earlier, the database tuples consisted of nine attributes (See Table 1). Ten classification functions of Agrawal et al. [2] were used to generate classification problems with different complexities. The training set consisted of 1000 tuples and the testing data sets had 1000 tuples. <p> In this section, we report the experimental results of applying the approach described in the previous sections to the data mining problem defined in <ref> [2] </ref>. As mentioned earlier, the database tuples consisted of nine attributes (See Table 1). Ten classification functions of Agrawal et al. [2] were used to generate classification problems with different complexities. The training set consisted of 1000 tuples and the testing data sets had 1000 tuples. Efforts were made Page 8 Rule 1. If (salary &lt; 100000) ^ (commission = 0) ^ (age 40), then Group A. Rule 2. <p> These include functions 1, 2 and 3. One interesting example is Function 2. The detailed process of finding the classification rules is described as an example in Section 2 and 3. The resulting rules are the same as the original functions. As reported by Agrawal et al. <ref> [2] </ref>, ID3 generated a relatively large number of strings for Function 2 when the decision tree is built. We observed similar results when C4.5rules was used (a member of ID3). C4.5rules generated 18 rules. Among the 18 rules, 8 rules define the conditions for Group A. <p> Functions 4 and 5 are another two functions for which ID3 generates a large number of strings. CDP <ref> [2] </ref> also generates a relatively large number of strings than for other functions. The original classification function 4, the rule sets that define Group A tuples extracted using NeuroRule and C4.5, respectively are shown in Figure 7. <p> The proposed approach was applied to a set of classification problems. The results of applying it to a data mining problem defined in <ref> [2] </ref> was discussed in detail. The results indicate that, using the proposed approach, high quality rules can be discovered from the given ten data sets.
Reference: [3] <author> T. Ash. </author> <title> Dynamic node creation in backpropga-tion networks. </title> <journal> Connection Science, </journal> <volume> 1(4) </volume> <pages> 365-375, </pages> <year> 1989. </year>
Reference-contexts: The first approach begins with a minimal network and adds more hidden nodes only when they are needed to improve the learning capability of the network <ref> [3, 11, 19] </ref>. The second approach begins with an oversized network and then prunes redundant hidden nodes and connections between the layers of the network.
Reference: [4] <author> R. Battiti. </author> <title> First- and second-order methods for learning: between steepest descent and newton's method. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 141-166, </pages> <year> 1992. </year>
Reference-contexts: Any unconstrained minimization algorithm can be used for this purpose. In particular, the gradient descent method has been the most widely used in the training algorithm known as the backpropagation algorithm. A number of alternative algorithms for neural network training have been proposed <ref> [4] </ref>. To reduce the network training time, which is very important in the data mining as the data set is usually large, we employed a variant of the quasi Newton algorithm [27], the BFGS method. <p> of the test data adapted from Agrawal et al.[2] Attribute Description Value salary salary uniformly distributed from 20,000 to 150,000 commission commission if salary 75000 ! commission = 0 else uniformly distributed from 10000 to 75000. age age uniformly distributed from 20 to 80. elevel education level uniformly distributed from <ref> [0; 1; : : :; 4] </ref>. car make of the car uniformly distributed from [1; 2; : : :20]. zipcode zip code of the town uniformly chosen from 9 available zipcodes. hvalue value of the house uniformly distributed from 0.5k10000 to 1.5k1000000 where k 2 f0 : : : 9g depends
Reference: [5] <author> N. Cercone and M Tsuchiya. </author> <title> Guest editors, special issue on learning and discovery in databases. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 5(6), </volume> <month> December </month> <year> 1993. </year>
Reference-contexts: The rules that generalize well can be safely applied to the application database with unknown classes to determine each tuple's class. This problem has been widely studied by researchers in the AI field [28]. It is recently reexamined by database researchers in the context of large database systems <ref> [5, 7, 14, 15, 13] </ref>. Two basic approaches to the classification problems studied by AI researchers are the symbolic approach and the connectionist approach. The symbolic approach is based on decision trees and the connectionist approach mainly Page 1 uses neural networks.
Reference: [6] <author> J.E. Dennis Jr. and R.B. Schnabel. </author> <title> Numerical methods for unconstrained optimization and nonlinear equations. </title> <publisher> Prentic Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1983. </year>
Reference-contexts: This algorithm has a superlinear convergence rate, as opposed to the linear rate of the gradient descent method. Details of the BFGS algorithm can be found in <ref> [6, 23] </ref>.
Reference: [7] <author> W. Frawley, G. Piatetsky-Shapiro, and C. Matheus. </author> <title> Knowledge discovery in databases: An overview. </title> <journal> AI Magazine, </journal> <note> Fall 1992. Page 11 </note>
Reference-contexts: The rules that generalize well can be safely applied to the application database with unknown classes to determine each tuple's class. This problem has been widely studied by researchers in the AI field [28]. It is recently reexamined by database researchers in the context of large database systems <ref> [5, 7, 14, 15, 13] </ref>. Two basic approaches to the classification problems studied by AI researchers are the symbolic approach and the connectionist approach. The symbolic approach is based on decision trees and the connectionist approach mainly Page 1 uses neural networks.
Reference: [8] <author> L. Fu. </author> <booktitle> Neural Networks in Computer Intelligence. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1994. </year>
Reference-contexts: The number of links is 17. However, it is still very difficult to articulate the network, i.e., find the explicit relationship between the input tuples and the output tuples. Research work in this area has been reported <ref> [25, 8] </ref>.
Reference: [9] <author> J. Han, Y. Cai, and H. Cercone. </author> <title> Knowledge discovery in databases: An attribute oriented approach. </title> <booktitle> In Proceedings of the VLDB conference, </booktitle> <pages> pages 547-559, </pages> <year> 1992. </year>
Reference-contexts: Proceedings of the 21st VLDB Conference Zurich, Swizerland, 1995 data are a valuable asset of an organization, most organizations may face the problem of data rich but knowledge poor sooner or later. This situation aroused the recent surge of research interests in the area of data mining <ref> [1, 9, 2] </ref>. One of the data mining problems is classification. Data items in databases, such as tuples in relational database systems usually represent real world entities. The values of the attributes of a tuple represent the properties of the entity.
Reference: [10] <author> J. Hertz, A. Krogh, and R.G. Palmer. </author> <title> Introduction to the theory of neural computation. </title> <publisher> Addison-Wesley Pub. Company, </publisher> <year> 1991. </year>
Reference-contexts: Section 4 presents some experimental results obtained and a comparison with previously published results. Finally a conclusion is given in Section 5. 2 Mining classification rules using neu ral networks Artificial neural networks are densely interconnected networks of simple computational elements, neurons. There exist many different network topologies <ref> [10] </ref>. Among them, the multi-layer perceptron is especially useful for implementing a classification function. Figure 1 shows a three layer feedforward network. It consists of an input layer, a hidden layer and an output layer. A node (neuron) in the network has a number of inputs and a single output.
Reference: [11] <author> Y. Hirose, K. Yamashita, and S. Hijiya. </author> <title> Backpropagation algorithm which varies the number of hidden units. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 61-66, </pages> <year> 1991. </year>
Reference-contexts: The first approach begins with a minimal network and adds more hidden nodes only when they are needed to improve the learning capability of the network <ref> [3, 11, 19] </ref>. The second approach begins with an oversized network and then prunes redundant hidden nodes and connections between the layers of the network.
Reference: [12] <author> H. Liu. X2R: </author> <title> A fast rule generator In Proceedings of IEEE International Conference on Systems, </title> <journal> Man and Cybernetics (SMC'95), </journal> <volume> Van-courver, </volume> <year> 1995. </year>
Reference-contexts: A small set of the discrete activation values make it possible to determine both the dependency among the output values and the hidden node values and the dependency among the hidden node activation values and the input values. From the dependencies, rules can be generated <ref> [12] </ref>. Here we show the process of extracting rules from the pruned network in Figure 3 obtained for the classification problem Function 2. The network has three hidden nodes. The activation values of 1000 tuples were discretized. The value of * was set to 0.6.
Reference: [13] <author> C.J. Matheus, P.K. Chan, and G. Piatetsky-Shapiro. </author> <title> Systems for knowledge discovery in databases. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 5(6), </volume> <month> December </month> <year> 1993. </year>
Reference-contexts: The rules that generalize well can be safely applied to the application database with unknown classes to determine each tuple's class. This problem has been widely studied by researchers in the AI field [28]. It is recently reexamined by database researchers in the context of large database systems <ref> [5, 7, 14, 15, 13] </ref>. Two basic approaches to the classification problems studied by AI researchers are the symbolic approach and the connectionist approach. The symbolic approach is based on decision trees and the connectionist approach mainly Page 1 uses neural networks.
Reference: [14] <editor> G. Piatetsky-Shapiro. Editor, </editor> <title> special isssue on knowledge discovery in databases. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 7(7), </volume> <month> September </month> <year> 1992. </year>
Reference-contexts: The rules that generalize well can be safely applied to the application database with unknown classes to determine each tuple's class. This problem has been widely studied by researchers in the AI field [28]. It is recently reexamined by database researchers in the context of large database systems <ref> [5, 7, 14, 15, 13] </ref>. Two basic approaches to the classification problems studied by AI researchers are the symbolic approach and the connectionist approach. The symbolic approach is based on decision trees and the connectionist approach mainly Page 1 uses neural networks.
Reference: [15] <author> G. Piatetsky-Shapiro. </author> <title> Guest editor introduction: Knowledge discovery in databases from research to applications. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 5(1), </volume> <month> January </month> <year> 1995. </year>
Reference-contexts: The rules that generalize well can be safely applied to the application database with unknown classes to determine each tuple's class. This problem has been widely studied by researchers in the AI field [28]. It is recently reexamined by database researchers in the context of large database systems <ref> [5, 7, 14, 15, 13] </ref>. Two basic approaches to the classification problems studied by AI researchers are the symbolic approach and the connectionist approach. The symbolic approach is based on decision trees and the connectionist approach mainly Page 1 uses neural networks.
Reference: [16] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Among 10 functions described, we found that functions 8 and 10 produced highly skewed data that made classification not meaningful. We will only discuss functions other than these two. To assess our approach, we compare the results with that of C4.5, a decision tree-based classifier <ref> [16] </ref>. 4.1 Classification accuracy The following table reports the classification accuracy using both our system and C4.5 for eight functions. Here, classification accuracy is defined as accuracy = no tuples correctly classified total number of tuples (6) Func.
Reference: [17] <author> J.R. Quinlan. </author> <title> Comparing connectionist and symbolic learning methods. </title> <editor> In S.J. Hanson, G.A. Drastall, and R.L. Rivest, editors, </editor> <booktitle> Computational Learning Therory and Natural Learning Systems, </booktitle> <volume> volume 1, </volume> <pages> pages 445-456. </pages> <publisher> A Bradford Book, The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The symbolic approach is based on decision trees and the connectionist approach mainly Page 1 uses neural networks. In general, neural networks give a lower classification error rate than the decision trees but require longer learning time <ref> [17, 24, 18] </ref>. While both approaches have been well received by the AI community, the general impression among the database community is that the connectionist approach is not well suited for data mining. The major criticisms include the following: 1. <p> The contributions of our study include the following: * Different from previous research work that excludes the connectionist approach entirely, we argue that the connectionist approach should have its position in data mining because of its merits such as low classification error rates and robust ness to noise <ref> [17, 18] </ref>. * With our newly developed algorithms, explicit classification rules can be extracted from a neural network. The rules extracted usually have a lower classification error rate than those generated by the decision tree based methods.
Reference: [18] <author> S. Russell and P. </author> <title> Norvig. </title> <journal> Artificial Intelligence: </journal>
Reference-contexts: The symbolic approach is based on decision trees and the connectionist approach mainly Page 1 uses neural networks. In general, neural networks give a lower classification error rate than the decision trees but require longer learning time <ref> [17, 24, 18] </ref>. While both approaches have been well received by the AI community, the general impression among the database community is that the connectionist approach is not well suited for data mining. The major criticisms include the following: 1. <p> The contributions of our study include the following: * Different from previous research work that excludes the connectionist approach entirely, we argue that the connectionist approach should have its position in data mining because of its merits such as low classification error rates and robust ness to noise <ref> [17, 18] </ref>. * With our newly developed algorithms, explicit classification rules can be extracted from a neural network. The rules extracted usually have a lower classification error rate than those generated by the decision tree based methods.
References-found: 18

