URL: ftp://ftp.icsi.berkeley.edu/pub/speech/papers/icassp98-uttcomb.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/~sulin/
Root-URL: http://www.icsi.berkeley.edu
Email: fsulin,bedk,morgan,stevengg@icsi.berkeley.edu  
Title: INCORPORATING INFORMATION FROM SYLLABLE-LENGTH TIME SCALES INTO AUTOMATIC SPEECH RECOGNITION  
Author: Su-Lin Wu, Brian E. D. Kingsbury, Nelson Morgan and Steven Greenberg 
Address: 1947 Center Street, Suite 600, Berkeley, CA 94704, USA  Berkeley, Berkeley, CA 94720, USA  
Affiliation: International Computer Science Institute,  and the University of California at  
Abstract: Including information distributed over intervals of syllabic duration (100-250 ms) may greatly improve the performance of automatic speech recognition (ASR) systems. ASR systems primarily use representations and recognition units covering phonetic durations (40-100 ms). Humans certainly use information at phonetic time scales, but results from psychoacoustics and psycholinguis-tics highlight the crucial role of the syllable, and syllable-length intervals, in speech perception. We compare the performance of three ASR systems: a baseline system that uses phone-scale representations and units, an experimental system that uses a syllable-oriented front-end representation and syllabic units for recognition, and a third system that combines the phone-scale and syllable-scale recognizers by merging and rescoring N -best lists. Using the combined recognition system, we observed an improvement in word error rate for telephone-bandwidth, continuous numbers from 6.8% to 5.5% on a clean test set, and from 27.8% to 19.6% on a reverberant test set, over the baseline phone-based system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Takayuki Arai, Misha Pavel, Hynek Hermansky, and Carlos Avendano. </author> <title> Intelligibility of speech with filtered time trajectories of spectral envelopes. </title> <booktitle> In ICSLP, </booktitle> <volume> volume 4, </volume> <pages> pages 2490-2493, </pages> <year> 1996. </year>
Reference-contexts: Subsequently, work on the prediction of speech intelligibility in reverberant and noisy rooms [12] and over nonlinear communications channels [17] have highlighted the importance of slow modulations. Recent perceptual studies <ref> [5, 1] </ref> show that suppression of modulations in the 2-8 Hz range significantly degrades speech intelligibility. Modulations in this frequency range correspond to the typical durations of single syllables and metrical feet. <p> Next, each amplitude envelope signal is normalized by its average value, computed over an entire utterance, to provide a crude model of the auditory adaptation. The normalized envelope signals are filtered, to model auditory sensitivity to slow modulations, compressed with a cube-root nonlinearity, and normalized to a range of <ref> [1; +1] </ref>. Two different modulation filters are used in parallel: a low-pass filter with a cutoff frequency of 8 Hz and a band-pass filter with cutoff frequencies of 2 Hz and 8 Hz.
Reference: [2] <author> Carlos Avendano, Sangita Tibrewala, and Hynek Hermansky. </author> <title> Multiresolution channel normalization for ASR in reverberant environments. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 3, </volume> <pages> pages 1107-1110, </pages> <address> Rhodes, Greece, </address> <month> September </month> <year> 1997. </year> <pages> ESCA. </pages>
Reference-contexts: The performance improvement achieved for reverberant speech is similar to that obtained by us using a frame-level combination method (unpublished observations), as well as to the results in <ref> [2] </ref>, where a multiresolution channel normalization method that operates over time spans of 10 sec. is used to compensate for reverberation. 5. CONCLUSIONS A syllable-length time scale appears to be a fundamental property of the human speech recognition system.
Reference: [3] <author> Herve Bourlard and Nelson Morgan. </author> <title> Connectionist Speech Recognition: A Hybrid Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference-contexts: Baseline Recognizer Each of the speech recognition systems implemented for these experiments was a hybrid hidden Markov model/multilayer perceptron (HMM/MLP) speech recognizer <ref> [3] </ref> in which the phonetic classification was performed with a single-hidden-layer MLP and speech decoding was performed by NOWAY [16], a start-synchronous stack decoder. The recognizers all used the same backoff bigram grammar, derived from the Numbers training set, for language modeling.
Reference: [4] <author> R. A. Cole, M. Noel, T. Lander, and T. Durham. </author> <title> New telephone speech corpora at CSLU. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 1, </volume> <pages> pages 821-824, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: Syllables constitute a convenient framework for incorporat ing suprasegmental prosodic information into recognition. The implementation of syllable-based recognizers has proven to be challenging, however. 3. EXPERIMENTAL MATERIALS AND RECOGNIZERS 3.1. Speech Material Recognition experiments were performed on a subset of the OGI Numbers corpus <ref> [4] </ref>, a set of continuous, naturally spoken utterances collected from many different speakers over the telephone. The 32-word vocabulary is restricted to numbers, including such confusable sets as four, fourteen, and forty. A sample utterance from the database is eighteen thirty one.
Reference: [5] <author> Rob Drullman, Joost M. Festen, and Reinier Plomp. </author> <title> Effect of temporal envelope smearing on speech reception. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 95(2) </volume> <pages> 1053-1064, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: Subsequently, work on the prediction of speech intelligibility in reverberant and noisy rooms [12] and over nonlinear communications channels [17] have highlighted the importance of slow modulations. Recent perceptual studies <ref> [5, 1] </ref> show that suppression of modulations in the 2-8 Hz range significantly degrades speech intelligibility. Modulations in this frequency range correspond to the typical durations of single syllables and metrical feet.
Reference: [6] <author> Homer Dudley. </author> <title> Remaking speech. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 11(2) </volume> <pages> 169-177, </pages> <month> October </month> <year> 1939. </year>
Reference-contexts: These changes manifest themselves as amplitude modulations at rates of 2-16 Hz in subbands following a critical-band spectral analysis. The first evidence for this relationship between slow modulations and speech intelligibility emerged from the work of Homer Dudley and his colleagues at Bell Labs on the channel vocoder <ref> [6] </ref>. Subsequently, work on the prediction of speech intelligibility in reverberant and noisy rooms [12] and over nonlinear communications channels [17] have highlighted the importance of slow modulations. Recent perceptual studies [5, 1] show that suppression of modulations in the 2-8 Hz range significantly degrades speech intelligibility.
Reference: [7] <author> Stephane Dupont, Herve Bourlard, and Christophe Ris. </author> <title> Using multiple time scales in a multi-stream speech recognition system. </title> <booktitle> In Eurospeech, </booktitle> <pages> pages 3-6, </pages> <address> Rhodes, Greece, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: The syllable was proposed as a unit for automatic speech recognition as early as 1975 [8]. Since then, the syllable has been revisited in a number of speech recognition systems in several languages and in various capacities, most recently in <ref> [19, 7] </ref>. The syllable is an attractive unit for recognition for several reasons: 1. Syllable representations and durations may exhibit greater stability relative to phoneme-based representations and du rations. 2. Syllables appear to offer a natural interface between speech acoustics and lexical access. 3. <p> Merging methods (classifier fusion techniques) at the frame level [14] and at the syllable level <ref> [7] </ref> that blend diverse information sources have been applied to obtain substantial improvements in accuracy. For our experiments, we have focused on merging recognition systems at the whole-utterance level.
Reference: [8] <author> Osamu Fujimura. </author> <title> Syllable as a unit of speech recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-23(1):82-87, </volume> <month> February </month> <year> 1975. </year>
Reference-contexts: These temporal compounds were found to be longer than the typical phoneme length through experiments with loud, clear, repeating vowel acoustic elements catenated together. The syllable was proposed as a unit for automatic speech recognition as early as 1975 <ref> [8] </ref>. Since then, the syllable has been revisited in a number of speech recognition systems in several languages and in various capacities, most recently in [19, 7]. The syllable is an attractive unit for recognition for several reasons: 1.
Reference: [9] <author> Steven Greenberg. </author> <title> On the origins of speech intelligibility in the real world. </title> <booktitle> In Proceedings of the ESCA Workshop on Robust Speech Recognition for Unknown Channels, </booktitle> <pages> pages 23-32. ESCA, </pages> <year> 1997. </year>
Reference-contexts: Measurements of the capacity of the human echoic memory [13, 15] demonstrate that this preperceptual buffer can store roughly 250 ms. In conversational English nearly 80% of syllables have a duration of 250 ms or less <ref> [9] </ref>; thus, the syllable may constitute a natural unit for the segmentation and recognition of the speech signal, as it is the largest unit of speech which can fit into preperceptual storage.
Reference: [10] <author> Steven Greenberg and Brian E. D. Kingsbury. </author> <title> The modulation spectrogram: In pursuit of an invariant representation of speech. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 3, </volume> <pages> pages 1647-1650, </pages> <address> Munich, Germany, </address> <month> April </month> <year> 1997. </year> <note> IEEE. </note>
Reference-contexts: Embedded Viterbi alignment was applied iteratively to optimize the lexicon pronunciations, minimum phone durations, and training labels. 3.3. A Syllable-based Recognizer The syllable-based recognizer used modulation spectrogram features <ref> [10] </ref> for the front-end speech representation. To compute these features, speech sampled at 8 kHz is analyzed into 15 quarter-octave channels using an FIR filterbank.
Reference: [11] <author> Hynek Hermansky and Nelson Morgan. </author> <title> RASTA processing of speech. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 2(4) </volume> <pages> 578-589, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The recognizers all used the same backoff bigram grammar, derived from the Numbers training set, for language modeling. The baseline recognizer used eighth-order log-RASTA-PLP <ref> [11] </ref> features computed over 25-ms windows with a 10-ms window step, supplemented with delta features computed over a 9-frame window. RASTA-PLP features include a filtering operation on critical-band spectral trajectories, using a filter with a 1-12 Hz passband.
Reference: [12] <author> T. Houtgast and H. J. M. Steeneken. </author> <title> The modulation transfer function in room acoustics as a predictor of speech intelligibility. </title> <journal> Acustica, </journal> <volume> 28 </volume> <pages> 66-73, </pages> <year> 1973. </year>
Reference-contexts: The first evidence for this relationship between slow modulations and speech intelligibility emerged from the work of Homer Dudley and his colleagues at Bell Labs on the channel vocoder [6]. Subsequently, work on the prediction of speech intelligibility in reverberant and noisy rooms <ref> [12] </ref> and over nonlinear communications channels [17] have highlighted the importance of slow modulations. Recent perceptual studies [5, 1] show that suppression of modulations in the 2-8 Hz range significantly degrades speech intelligibility. Modulations in this frequency range correspond to the typical durations of single syllables and metrical feet.
Reference: [13] <author> A. W. F. Huggins. </author> <title> Temporally segmented speech. </title> <journal> Perception and Psychophysics, </journal> <volume> 18(2) </volume> <pages> 149-157, </pages> <year> 1975. </year>
Reference-contexts: Modulations in this frequency range correspond to the typical durations of single syllables and metrical feet. A second line of evidence for the role for syllables in speech recognition comes from the study of echoic memory. Measurements of the capacity of the human echoic memory <ref> [13, 15] </ref> demonstrate that this preperceptual buffer can store roughly 250 ms.
Reference: [14] <author> Brian E. D. Kingsbury and Nelson Morgan. </author> <title> Recognizing reverberant speech with RASTA-PLP. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 2, </volume> <pages> pages 1259-1262, </pages> <address> Munich, Germany, </address> <month> April </month> <year> 1997. </year> <note> IEEE. </note>
Reference-contexts: Combining Recognizers Combinations of speech recognition systems can potentially achieve better accuracy than individual recognizers if the recog-nizers being combined tend to make independent errors and the combination method allows correct answers to override incorrect ones. Merging methods (classifier fusion techniques) at the frame level <ref> [14] </ref> and at the syllable level [7] that blend diverse information sources have been applied to obtain substantial improvements in accuracy. For our experiments, we have focused on merging recognition systems at the whole-utterance level.
Reference: [15] <author> Dominic W. Massaro. </author> <title> Preperceptual images, processing time and perceptual units in auditory perception. </title> <journal> Psychological Review, </journal> <volume> 79(2) </volume> <pages> 124-145, </pages> <year> 1972. </year>
Reference-contexts: Modulations in this frequency range correspond to the typical durations of single syllables and metrical feet. A second line of evidence for the role for syllables in speech recognition comes from the study of echoic memory. Measurements of the capacity of the human echoic memory <ref> [13, 15] </ref> demonstrate that this preperceptual buffer can store roughly 250 ms.
Reference: [16] <author> Steve Renals and Mike Hochberg. </author> <title> Efficient evaluation of the LVCSR search space using the NOWAY decoder. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 149-152, </pages> <address> Atlanta, Georgia, </address> <month> May </month> <year> 1996. </year> <note> IEEE. </note>
Reference-contexts: Baseline Recognizer Each of the speech recognition systems implemented for these experiments was a hybrid hidden Markov model/multilayer perceptron (HMM/MLP) speech recognizer [3] in which the phonetic classification was performed with a single-hidden-layer MLP and speech decoding was performed by NOWAY <ref> [16] </ref>, a start-synchronous stack decoder. The recognizers all used the same backoff bigram grammar, derived from the Numbers training set, for language modeling. The baseline recognizer used eighth-order log-RASTA-PLP [11] features computed over 25-ms windows with a 10-ms window step, supplemented with delta features computed over a 9-frame window.
Reference: [17] <author> Herman J. M. Steeneken and Tammo Houtgast. </author> <title> A physical method for measuring speech-transmission quality. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 67(1) </volume> <pages> 318-326, </pages> <month> January </month> <year> 1980. </year>
Reference-contexts: The first evidence for this relationship between slow modulations and speech intelligibility emerged from the work of Homer Dudley and his colleagues at Bell Labs on the channel vocoder [6]. Subsequently, work on the prediction of speech intelligibility in reverberant and noisy rooms [12] and over nonlinear communications channels <ref> [17] </ref> have highlighted the importance of slow modulations. Recent perceptual studies [5, 1] show that suppression of modulations in the 2-8 Hz range significantly degrades speech intelligibility. Modulations in this frequency range correspond to the typical durations of single syllables and metrical feet.
Reference: [18] <author> Richard M. Warren. </author> <title> Perceptual processing of speech and other perceptual patterns: Some similarities and differences. </title> <editor> In Steven Greenberg and William Ainsworth, editors, </editor> <title> Listening to Speech: An Auditory Perspective. </title> <publisher> Oxford University Press, </publisher> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: That the syllable plays an important role in identification can be surmised from <ref> [18] </ref>, in which the author asserts that temporal compounds are formed by acoustic elements and that the human perceptual system can identify these compounds more readily than constituent sounds.
Reference: [19] <author> Su-Lin Wu, Michael L. Shire, Steven Greenberg, and Nel-son Morgan. </author> <title> Integrating syllable boundary information into speech recognition. </title> <booktitle> In ICASSP, volume 1, </booktitle> <address> Munich, Germany, </address> <month> April </month> <year> 1997. </year> <note> IEEE. </note>
Reference-contexts: The syllable was proposed as a unit for automatic speech recognition as early as 1975 [8]. Since then, the syllable has been revisited in a number of speech recognition systems in several languages and in various capacities, most recently in <ref> [19, 7] </ref>. The syllable is an attractive unit for recognition for several reasons: 1. Syllable representations and durations may exhibit greater stability relative to phoneme-based representations and du rations. 2. Syllables appear to offer a natural interface between speech acoustics and lexical access. 3.
References-found: 19

