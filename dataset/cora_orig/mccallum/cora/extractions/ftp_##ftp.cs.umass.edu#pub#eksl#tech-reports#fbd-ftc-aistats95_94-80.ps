URL: ftp://ftp.cs.umass.edu/pub/eksl/tech-reports/fbd-ftc-aistats95_94-80.ps
Refering-URL: http://eksl-www.cs.umass.edu/~stamant/publications.html
Root-URL: 
Title: Two Algorithms for Inducing Structural Equation Models from Data  
Author: Paul R. Cohen, Dawn E. Gregory, Lisa A. Ballesteros and Robert St. Amant 
Address: Box 34610  Amherst, MA 01003-4610  
Affiliation: Computer Science  Experimental Knowledge Systems Laboratory Computer Science Department,  Lederle Graduate Research Center University of Massachusetts  
Note: In Preliminary Papers of the Fifth International Workshop on Artificial Intelligence and Statistics, 1995, pp. 129-139.  This research is supported by ARPA/Rome Laboratory under contract #'s F30602-91-C-0076 and F306023-93 C-0100; and by a NASA GSRP Training Grant, NGT 70358.  
Pubnum: Technical Report 94-80  
Abstract: We present two algorithms for inducing structural equation models from data. Assuming no latent variables, these models have a causal interpretation and their parameters may be estimated by linear multiple regression. Our algorithms are comparable with PC [15] and IC [12, 11], which rely on conditional independence. We present the algorithms and empirical comparisons with PC and IC. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Scott D. Anderson, Adam Carlson, David L. Westbrook, David M. Hart, and Paul R. Cohen. Clip/clasp: </author> <title> Common lisp analytical statistics package/common lisp instrumentation package. </title> <type> Technical Report 93-55, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1993. </year> <note> This document is available under http://eksl-www.cs.umass.edu/publications.html. </note>
Reference-contexts: FBD and FTC Structural equation models can be represented by directed acyclic graphs (DAGs) in which a link x 1 ! x 0 is interpreted to mean x 1 is a predictor of x 0 . fbd and ftc rely on statistical filter 1 fbd and ftc run in clip/clasp <ref> [1] </ref>, a Common Lisp statistical package developed at UMass. For more information on clip/clasp, please contact clasp-support@cs.umass.edu. 1 conditions to remove insignificant links from the model being constructed. The two algorithms use the same filter conditions, differing only in how they are applied.
Reference: [2] <author> Lisa Ballesteros. </author> <title> Regression-based causal induction with latent variable models. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year> <note> This document is available under http://eksl-www.cs.umass.edu/publications.html. 8 </note>
Reference-contexts: In practice, fbd and ftc are more robust than, say, stepwise multiple regression. They often discard predictors that are related to the predictee only through the presence of a latent variable <ref> [2] </ref>. We haven't yet shown analytically why the algorithms have this advantage. <p> Also, we have compared fbd and ftc with other algorithms|pc for fbd [15] and ic for ftc [11]. 2 Finally, to assess how latent variables affected its performance, we compared fbd with stepwise linear regression as implemented in minitab <ref> [2] </ref>. 3.1 Artificial Models and Data We worked with a set of 60 artificially generated data sets: 20 data sets for each of 6, 9, and 12 measured variables. These were generated from the structural equations of 60 randomly selected target models. <p> Selecting variables by their ! scores lessens this problem. In our lab, Ballesteros <ref> [2] </ref> has obtained good results with models Spirtes et al. [15, page 240] show are difficult for ordinary regression. For these models, regression often chooses predictors whose relationships to the dependent variable are mediated by latent variables or common causes (we refer to these variables as proxy variables).
Reference: [3] <author> Jacob Cohen and Patricia Cohen. </author> <title> Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences. </title> <address> Hillsdale, NJ, </address> <year> 1975. </year>
Reference-contexts: These are search algorithms, and they are not guaranteed to find the "best" p|the one that makes R 2 as big as possible, especially when p is a proper subset of P <ref> [10, 3, 7] </ref>. The question arises, what should be done with the variables in T = P p, the ones that aren't selected as predictors of x 0 ? In many data analysis tasks, the variables in T are used to predict the variables in p.
Reference: [4] <author> Paul R. Cohen, Lisa Ballesteros, Dawn Gregory, and Robert St. Amant. </author> <title> Experiments with a regression-based causal induction algorithm. EKSL memo number 94-33, </title> <institution> Dept. of Computer Science, University of Massachusetts/Amherst. </institution> <note> This document is available under http://eksl-www.cs.umass.edu/publications.html, 1994. </note>
Reference-contexts: We tested how well the ! heuristic selects predictors, measured in terms of variance accounted for in the dependent variable <ref> [4] </ref>. <p> Both pc and ic build models from constraints imposed by conditional independencies in the data. In order to provide a comprehensive evaluation of the algorithms, we used a variety of performance measures for each model. The measures we used are shown in table 3. See <ref> [5, 4] </ref> for details of these and other dependent measures. Measure Meaning Dependent R 2 The variance accounted for in x 0 , the sink variable. R 2 The mean of the absolute differences in R 2 between all the predictees in the target model and the model being evaluated.
Reference: [5] <author> Paul R. Cohen, Lisa Ballesteros, Dawn Gregory, and Robert St. Amant. </author> <title> Regression can build predictive causal models. </title> <type> Technical Report 94-15, </type> <institution> Dept. of Computer Science, University of Massachusetts, Amherst, </institution> <year> 1994. </year> <note> This document is available under http://eksl-www.cs.umass.edu/publications.html. </note>
Reference-contexts: If we can assume causal sufficiency [15]|essentially, no latent variables|these models may be interpreted as causal models <ref> [5] </ref>. 1 2. <p> Both pc and ic build models from constraints imposed by conditional independencies in the data. In order to provide a comprehensive evaluation of the algorithms, we used a variety of performance measures for each model. The measures we used are shown in table 3. See <ref> [5, 4] </ref> for details of these and other dependent measures. Measure Meaning Dependent R 2 The variance accounted for in x 0 , the sink variable. R 2 The mean of the absolute differences in R 2 between all the predictees in the target model and the model being evaluated.
Reference: [6] <author> Paul R. Cohen, David M. Hart, Robert St. Amant, Lisa Ballesteros, and Adam Carlson. </author> <title> Path analysis models of an autonomous agent in a complex environment. In Selecting Models from Data: </title> <booktitle> AI and Statistics IV. </booktitle>
Reference-contexts: Empirical Results We have tested fbd and ftc under many conditions: on artificial data (using our own data generator and the TETRAD generator), on published data [15], and on data generated by running an AI planning system called Phoenix <ref> [6] </ref>. We tested how well the ! heuristic selects predictors, measured in terms of variance accounted for in the dependent variable [4].
Reference: [7] <author> N. R. Draper and H. Smith. </author> <title> Applied Regression Analysis. </title> <address> New York, NY, </address> <year> 1966. </year>
Reference-contexts: These are search algorithms, and they are not guaranteed to find the "best" p|the one that makes R 2 as big as possible, especially when p is a proper subset of P <ref> [10, 3, 7] </ref>. The question arises, what should be done with the variables in T = P p, the ones that aren't selected as predictors of x 0 ? In many data analysis tasks, the variables in T are used to predict the variables in p.
Reference: [8] <author> Paul Holland. </author> <title> Statistics and causal inference. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 81 </volume> <pages> 945-960, </pages> <year> 1986. </year>
Reference-contexts: This paper presents two algorithms that build structural equation models. There are clear parallels between building structural equation models and building causal models. Indeed, path analysis refers to the business of interpreting structural equation models as causal models [9, 16]. Path analysis has been heavily criticized (e.g., <ref> [13, 8] </ref>) in part because latent variables can produce large errors in estimated regression coefficients throughout a model [15]. Recent causal induction algorithms rely not on regression coefficients but on conditional independence [11, 15].
Reference: [9] <author> C.C. Li. </author> <title> Path analysis-A primer. </title> <publisher> Boxwood Press, </publisher> <year> 1975. </year>
Reference-contexts: This paper presents two algorithms that build structural equation models. There are clear parallels between building structural equation models and building causal models. Indeed, path analysis refers to the business of interpreting structural equation models as causal models <ref> [9, 16] </ref>. Path analysis has been heavily criticized (e.g., [13, 8]) in part because latent variables can produce large errors in estimated regression coefficients throughout a model [15]. Recent causal induction algorithms rely not on regression coefficients but on conditional independence [11, 15]. <p> This yields a standardized regression coefficient fi ij for each predictor x j . Now, fi ij is partial|it represents the effect of x j on x i when the influences of all the other variables are fixed. Following <ref> [9, 14] </ref> we'll call fi ij an estimate of the direct influence of x j on x i .
Reference: [10] <author> Frederick Mosteller and John W. Tukey. </author> <title> Data Analysis and Regression: A Second Course in Statistics. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1977. </year>
Reference-contexts: These are search algorithms, and they are not guaranteed to find the "best" p|the one that makes R 2 as big as possible, especially when p is a proper subset of P <ref> [10, 3, 7] </ref>. The question arises, what should be done with the variables in T = P p, the ones that aren't selected as predictors of x 0 ? In many data analysis tasks, the variables in T are used to predict the variables in p.
Reference: [11] <author> Judea Pearl and T.S. Verma. </author> <title> A statistical semantics for causation. </title> <journal> Statistics and Computing, </journal> <volume> 2 </volume> <pages> 91-95, </pages> <year> 1991. </year>
Reference-contexts: Path analysis has been heavily criticized (e.g., [13, 8]) in part because latent variables can produce large errors in estimated regression coefficients throughout a model [15]. Recent causal induction algorithms rely not on regression coefficients but on conditional independence <ref> [11, 15] </ref>. These algorithms use covariance information only to infer boolean conditional independence constraints; they do not estimate strengths of causal relationships, and, most importantly from our perspective, they don't use these strengths to guide the search for causal models. <p> We tested how well the ! heuristic selects predictors, measured in terms of variance accounted for in the dependent variable [4]. Also, we have compared fbd and ftc with other algorithms|pc for fbd [15] and ic for ftc <ref> [11] </ref>. 2 Finally, to assess how latent variables affected its performance, we compared fbd with stepwise linear regression as implemented in minitab [2]. 3.1 Artificial Models and Data We worked with a set of 60 artificially generated data sets: 20 data sets for each of 6, 9, and 12 measured variables. <p> Low r 0i , low fi 0i : In this case, ! is small but fbd and ftc will discard x i as a predictor because fi is small. 5 3.3 Comparative Studies fbd and ftc were compared to the pc [15] and ic <ref> [12, 11] </ref> algorithms, respectively. We chose these algorithms for comparison due to their availability and strong theoretical support. Both pc and ic build models from constraints imposed by conditional independencies in the data.
Reference: [12] <author> Judea Pearl and T.S. Verma. </author> <title> A theory of inferred causation. </title> <editor> In J. Allen, R. Fikes, and E. Sandewall, editors, </editor> <booktitle> Principles of Knowledge Representation and Reasoning: Proceedings of the Second International Conference., </booktitle> <pages> pages 441-452. </pages> <publisher> Morgan Kaufman, </publisher> <year> 1991. </year>
Reference-contexts: Low r 0i , low fi 0i : In this case, ! is small but fbd and ftc will discard x i as a predictor because fi is small. 5 3.3 Comparative Studies fbd and ftc were compared to the pc [15] and ic <ref> [12, 11] </ref> algorithms, respectively. We chose these algorithms for comparison due to their availability and strong theoretical support. Both pc and ic build models from constraints imposed by conditional independencies in the data.
Reference: [13] <editor> Juliet Popper Shaffer, editor. </editor> <title> The Role of Models in Nonexperimental Social Science: Two Debates. </title> <journal> American Educational Research Association and American Statistical Association, </journal> <year> 1992. </year>
Reference-contexts: This paper presents two algorithms that build structural equation models. There are clear parallels between building structural equation models and building causal models. Indeed, path analysis refers to the business of interpreting structural equation models as causal models [9, 16]. Path analysis has been heavily criticized (e.g., <ref> [13, 8] </ref>) in part because latent variables can produce large errors in estimated regression coefficients throughout a model [15]. Recent causal induction algorithms rely not on regression coefficients but on conditional independence [11, 15].
Reference: [14] <author> Robert R. Sokal and F. James Rohlf. Biometry: </author> <booktitle> the Principles and Practice of Statistics in Biological Research. W.H. </booktitle> <publisher> Freeman and Co., </publisher> <address> New York, </address> <note> second edition, </note> <year> 1981. </year>
Reference-contexts: This yields a standardized regression coefficient fi ij for each predictor x j . Now, fi ij is partial|it represents the effect of x j on x i when the influences of all the other variables are fixed. Following <ref> [9, 14] </ref> we'll call fi ij an estimate of the direct influence of x j on x i .
Reference: [15] <author> Peter Spirtes, Clark Glymour, and Richard Scheines. </author> <title> Causation, Prediction, and Search. </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Indeed, path analysis refers to the business of interpreting structural equation models as causal models [9, 16]. Path analysis has been heavily criticized (e.g., [13, 8]) in part because latent variables can produce large errors in estimated regression coefficients throughout a model <ref> [15] </ref>. Recent causal induction algorithms rely not on regression coefficients but on conditional independence [11, 15]. <p> Path analysis has been heavily criticized (e.g., [13, 8]) in part because latent variables can produce large errors in estimated regression coefficients throughout a model [15]. Recent causal induction algorithms rely not on regression coefficients but on conditional independence <ref> [11, 15] </ref>. These algorithms use covariance information only to infer boolean conditional independence constraints; they do not estimate strengths of causal relationships, and, most importantly from our perspective, they don't use these strengths to guide the search for causal models. <p> Most of this is attributed to the linear regressions, which have complexity O (n 3 ) in our implementation. 3. Empirical Results We have tested fbd and ftc under many conditions: on artificial data (using our own data generator and the TETRAD generator), on published data <ref> [15] </ref>, and on data generated by running an AI planning system called Phoenix [6]. We tested how well the ! heuristic selects predictors, measured in terms of variance accounted for in the dependent variable [4]. Also, we have compared fbd and ftc with other algorithms|pc for fbd [15] and ic for <p> on published data <ref> [15] </ref>, and on data generated by running an AI planning system called Phoenix [6]. We tested how well the ! heuristic selects predictors, measured in terms of variance accounted for in the dependent variable [4]. Also, we have compared fbd and ftc with other algorithms|pc for fbd [15] and ic for ftc [11]. 2 Finally, to assess how latent variables affected its performance, we compared fbd with stepwise linear regression as implemented in minitab [2]. 3.1 Artificial Models and Data We worked with a set of 60 artificially generated data sets: 20 data sets for each of 6, <p> Low r 0i , low fi 0i : In this case, ! is small but fbd and ftc will discard x i as a predictor because fi is small. 5 3.3 Comparative Studies fbd and ftc were compared to the pc <ref> [15] </ref> and ic [12, 11] algorithms, respectively. We chose these algorithms for comparison due to their availability and strong theoretical support. Both pc and ic build models from constraints imposed by conditional independencies in the data. <p> Wrong Not Reversed The number of links x i ! x j that should not have been drawn in either direction. Table 3: Dependent measures and their meanings. We compared fbd with the pc algorithm <ref> [15] </ref>, because pc can be given exogenous knowledge about causal order, specifically, it can be told which is the sink variable x 0 . 3 ftc was compared to the ic algorithm, since neither uses external knowledge about the data. <p> Selecting variables by their ! scores lessens this problem. In our lab, Ballesteros [2] has obtained good results with models Spirtes et al. <ref> [15, page 240] </ref> show are difficult for ordinary regression. For these models, regression often chooses predictors whose relationships to the dependent variable are mediated by latent variables or common causes (we refer to these variables as proxy variables).
Reference: [16] <author> Sewall Wright. </author> <title> Correlation and causation. </title> <journal> Journal of Agricultural Research, </journal> <volume> 20 </volume> <pages> 557-585, </pages> <year> 1921. </year> <month> 9 </month>
Reference-contexts: This paper presents two algorithms that build structural equation models. There are clear parallels between building structural equation models and building causal models. Indeed, path analysis refers to the business of interpreting structural equation models as causal models <ref> [9, 16] </ref>. Path analysis has been heavily criticized (e.g., [13, 8]) in part because latent variables can produce large errors in estimated regression coefficients throughout a model [15]. Recent causal induction algorithms rely not on regression coefficients but on conditional independence [11, 15].
References-found: 16

