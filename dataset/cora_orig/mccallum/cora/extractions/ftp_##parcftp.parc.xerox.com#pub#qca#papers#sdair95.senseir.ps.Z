URL: ftp://parcftp.parc.xerox.com/pub/qca/papers/sdair95.senseir.ps.Z
Refering-URL: 
Root-URL: 
Email: internet: fschuetze,pederseng@parc.xerox.com  
Title: Information Retrieval Based on Word Senses  
Author: Hinrich Schutze and Jan O. Pedersen 
Web: URL: ftp://parcftp.xerox.com/pub/qca/SenseIR.DAIR95.ps  
Address: 3333 Coyote Hill Palo Alto, CA 94304  
Affiliation: Xerox Palo Alto Research Center  
Abstract: This paper proposes an algorithm for word sense disambiguation based on a vector repre sentation of word similarity derived from lexi cal co-occurrence. It differs from standard ap proaches by allowing for as fine grained dis tinctions as is warranted by the information at hand, rather than supposing a fixed number of senses per word, and by allowing for more than one sense to be assigned to a given word occur rence. The algorithm is applied to the standard vector space information retrieval model and an evalu ation is performed over the Category B TREC-1 corpus (WSJ subcollection). Results show that this sense disambiguation algorithm improves performance by between 7% and 14% on aver age. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. </author> <title> Word-sense disambiguation using statistical methods. </title> <booktitle> In Proceedings of ACL 29, </booktitle> <year> 1991. </year>
Reference-contexts: em ployed to discriminate senses computationally: Kelly and Stone [22] consider hand-constructed disambiguation rules, Lesk [27], Krovetz and Croft [24], and Guthrie et al. [15] use online dic tionaries, Hirst [20] constructs knowledge bases, Cottrell [4] uses syntactic and semantic struc ture encoded in a connectionist net, Brown et al. <ref> [1] </ref> and Church and Gale [3] exploit bilingual corpora, Dagan et al. [7] use a bilingual dictio nary, Hearst [18] and Leacock et al. [26] exploit a hand-labeled training set, and Yarowsky [41] performs a computation based on Roget's the saurus.
Reference: [2] <author> Peter Cheeseman, James Kelly, Matthew Self, John Stutz, Will Taylor, and Don Freeman. </author> <title> AutoClass: A Bayesian clas sification system. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <year> 1988. </year>
Reference-contexts: In summary, the disambiguation algorithm proposed here has three phases. First, a number of context vectors of a target word are computed from a training set. The context vectors are clustered, with each cluster ideally correspond 3 AutoClass <ref> [2] </ref> is used for some of the words in section "Case Studies".
Reference: [3] <author> Kenneth W. Church and William A. Gale. </author> <title> Concordances for parallel text. </title> <booktitle> In Proceed ings of the Seventh Annual Conference of the UW Centre for the New OED and Text Research, </booktitle> <pages> pages 40-62, </pages> <address> England: Oxford, </address> <year> 1991. </year>
Reference-contexts: computationally: Kelly and Stone [22] consider hand-constructed disambiguation rules, Lesk [27], Krovetz and Croft [24], and Guthrie et al. [15] use online dic tionaries, Hirst [20] constructs knowledge bases, Cottrell [4] uses syntactic and semantic struc ture encoded in a connectionist net, Brown et al. [1] and Church and Gale <ref> [3] </ref> exploit bilingual corpora, Dagan et al. [7] use a bilingual dictio nary, Hearst [18] and Leacock et al. [26] exploit a hand-labeled training set, and Yarowsky [41] performs a computation based on Roget's the saurus. McRoy [28] investigates how multiple knowledge sources can be combined for disam biguation.
Reference: [4] <author> Garrison W. Cottrell. </author> <title> A Connectionist Approach to Word Sense Disambiguation. </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: substantial improvement over a baseline system. 2 Related Work Different sources of information have been em ployed to discriminate senses computationally: Kelly and Stone [22] consider hand-constructed disambiguation rules, Lesk [27], Krovetz and Croft [24], and Guthrie et al. [15] use online dic tionaries, Hirst [20] constructs knowledge bases, Cottrell <ref> [4] </ref> uses syntactic and semantic struc ture encoded in a connectionist net, Brown et al. [1] and Church and Gale [3] exploit bilingual corpora, Dagan et al. [7] use a bilingual dictio nary, Hearst [18] and Leacock et al. [26] exploit a hand-labeled training set, and Yarowsky [41] performs a computation
Reference: [5] <author> C. J. Crouch. </author> <title> An approach to the au tomatic construction of global thesauri. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 26(5) </volume> <pages> 629-640, </pages> <year> 1990. </year>
Reference-contexts: However, this matrix must be constructed in a series of steps to keep the computations tractable at each stage. A more detailed discussion of this com putation can be found in [38]. While other au tomatic thesaurus construction algorithms ex ploit similar information <ref> [5, 9, 21] </ref>, they use it 2 A standard ARPA text corpus of roughly 170,000 documents and 500MB of text from the Wall Street Journal and a set of 25 evaluation queries (Topics 51-75). Hinrich Schutze and Jan O.
Reference: [6] <author> Douglas R. Cutting, Jan O. Pedersen, David Karger, and John W. Tukey. Scat ter/gather: </author> <title> A cluster-based approach to browsing large document collections. </title> <booktitle> In Proceedings of SIGIR '92, </booktitle> <pages> pages 318-329, </pages> <year> 1992. </year>
Reference-contexts: For the partitioning we typically employ Buckshot, an efficient approx imation of group agglomerative clustering <ref> [6] </ref> 3 . A vector is included in a region if it is closer to that region's cluster centroid than to any other region's cluster centroid.
Reference: [7] <author> Ido Dagan, Alon Itai, and Ulrike Schwall. </author> <title> Two languages are more informative than one. </title> <booktitle> In Proceedings of ACL 29, </booktitle> <pages> pages 130 137, </pages> <address> Berkeley CA, </address> <year> 1991. </year>
Reference-contexts: disambiguation rules, Lesk [27], Krovetz and Croft [24], and Guthrie et al. [15] use online dic tionaries, Hirst [20] constructs knowledge bases, Cottrell [4] uses syntactic and semantic struc ture encoded in a connectionist net, Brown et al. [1] and Church and Gale [3] exploit bilingual corpora, Dagan et al. <ref> [7] </ref> use a bilingual dictio nary, Hearst [18] and Leacock et al. [26] exploit a hand-labeled training set, and Yarowsky [41] performs a computation based on Roget's the saurus. McRoy [28] investigates how multiple knowledge sources can be combined for disam biguation.
Reference: [8] <author> Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. </author> <title> Indexing by la tent semantic analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(6) </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference-contexts: We address these issues by reducing the di mensionality of the problem to a workable size. The key dimensionality reduction tool is a sin gular value decomposition [14] of the matrix of co-occurrence counts (see <ref> [8] </ref> for a different use of SVD in information retrieval). However, this matrix must be constructed in a series of steps to keep the computations tractable at each stage. A more detailed discussion of this com putation can be found in [38].
Reference: [9] <author> David A. Evans, Kimberly Ginther Webster, Mary Hart, Robert G. Lefferts, and Ira A. Monarch. </author> <title> Automatic index ing using selective nlp and first-order the sauri. </title> <booktitle> In Proceedings of the RIAO, </booktitle> <volume> vol ume 2, </volume> <pages> pages 624-643, </pages> <year> 1991. </year>
Reference-contexts: However, this matrix must be constructed in a series of steps to keep the computations tractable at each stage. A more detailed discussion of this com putation can be found in [38]. While other au tomatic thesaurus construction algorithms ex ploit similar information <ref> [5, 9, 21] </ref>, they use it 2 A standard ARPA text corpus of roughly 170,000 documents and 500MB of text from the Wall Street Journal and a set of 25 evaluation queries (Topics 51-75). Hinrich Schutze and Jan O.
Reference: [10] <author> William Gale, Kenneth Ward Church, and David Yarowsky. </author> <title> Estimating upper and lower bounds on the performance of word sense disambiguation programs. </title> <booktitle> In Pro ceedings of ACL 30, </booktitle> <pages> pages 249-256, </pages> <year> 1992. </year>
Reference-contexts: There is some evidence that many ambigu ous words behave like pseudowords (i.e., have a majority sense) if only coarse sense distinctions are made. For example, looking at a coarse grained notion of ambiguity, Gale et al. <ref> [10] </ref> find that for a random sample of words choosing the most frequent sense results in a disambiguation performance of 92% (averaged over types).
Reference: [11] <author> William A. Gale, Kenneth W. Church, and David Yarowsky. </author> <title> A method for disam biguating word senses in a large corpus. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories, </institution> <address> Murray Hill NJ, </address> <year> 1992. </year>
Reference-contexts: It is known that disam biguation performance decreases when less context is available <ref> [11, 36] </ref>. Further research is necessary to determine how much such a deterioration in dis ambiguation affects performance of the information retrieval model presented here. and combined the ranks for each document. The final ranking was then the ranking of the combined ranks r 0 i .
Reference: [12] <author> Stephen I. Gallant. </author> <title> A practical approach for representing context and for perform ing word sense disambiguation using neural networks. </title> <journal> Neural Computation, </journal> <volume> 3(3):293 309, </volume> <year> 1991. </year>
Reference-contexts: neighbors in the corpus) rather than simple first-order co-occurrence (occurring near to each other) to find synonyms. 4 The Disambiguation Al gorithm An individual occurrence t of a word can be characterized by summing the thesaurus vectors of the words that occur close to it to produce a context vector <ref> [40, 12] </ref>. Since the direction of a word's vector corresponds to its main topic, if several words with the same main topic oc cur close to t, then that topic will dominate in the computation of the context vector.
Reference: [13] <author> Dirk Geeraerts. </author> <title> Vagueness's puzzles, </title> <journal> pol ysemy's vagaries. Cognitive Linguistics, </journal> <volume> 4 </volume> <pages> 223-272, </pages> <year> 1993. </year>
Reference-contexts: that words have a determined number of senses: "Often, the senses as identi fied in the dictionary identify points on a con tinuum of possibilities for how the word is used and dictionary senses might equally have been written which divided up the space differently." In the same vein, Geeraerts <ref> [13] </ref> argues con vincingly that the criteria for distinguishing be tween vagueness and ambiguity are not con sistent, and that this distinction is vague it self. In lexicography, difficulties with sense in dividuation are well known.
Reference: [14] <author> Gene H. Golub and Charles F. van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore and London, </address> <year> 1989. </year>
Reference-contexts: We address these issues by reducing the di mensionality of the problem to a workable size. The key dimensionality reduction tool is a sin gular value decomposition <ref> [14] </ref> of the matrix of co-occurrence counts (see [8] for a different use of SVD in information retrieval). However, this matrix must be constructed in a series of steps to keep the computations tractable at each stage. A more detailed discussion of this com putation can be found in [38].
Reference: [15] <author> Joe A. Guthrie, Louise Guthrie, Yorick Wilks, and Homa Aidinejad. </author> <title> Subject dependent co-occurrence and word sense disambiguation. </title> <booktitle> In Proceedings of ACL 29, </booktitle> <pages> pages 146-152, </pages> <year> 1991. </year>
Reference-contexts: apply it to information retrieval, and present experimental results that show substantial improvement over a baseline system. 2 Related Work Different sources of information have been em ployed to discriminate senses computationally: Kelly and Stone [22] consider hand-constructed disambiguation rules, Lesk [27], Krovetz and Croft [24], and Guthrie et al. <ref> [15] </ref> use online dic tionaries, Hirst [20] constructs knowledge bases, Cottrell [4] uses syntactic and semantic struc ture encoded in a connectionist net, Brown et al. [1] and Church and Gale [3] exploit bilingual corpora, Dagan et al. [7] use a bilingual dictio nary, Hearst [18] and Leacock et al. [26]
Reference: [16] <author> D.K. Harman, </author> <title> editor. The Second Text REtrieval Converence (TREC-2). </title> <type> U.S. </type> <institution> De partment of Commerce, </institution> <address> Washington DC, </address> <year> 1994. </year> <note> NIST Special Publication 500-215. </note>
Reference-contexts: Table 1 reports an experiment in which 20,000 pseudowords with 5 senses each were generated for the Tipster Wall Street Journal subcollection <ref> [16] </ref>. Statistics were gathered for all tokens involved and for 11 word type fre quency ranges: word types with fewer than 25 occurrences, and 10 more frequency ranges, each with roughly the same number of word types.
Reference: [17] <author> Donna Harman. </author> <booktitle> Overview of the first trec conference. In Proceedings of SIGIR '93, </booktitle> <year> 1993. </year>
Reference-contexts: The matrix C has v 2 =2 distinct entries where v is the size of the vocabulary. Although this matrix is sparse, we can expect v to be very large, and hence the overall storage requirement to be unworkable. For example, the category B TREC-1 corpus 2 <ref> [17] </ref>, which is the subject of our experiments, has over 450,000 unique terms. Even if enough memory were found to rep resent C directly, the thesaurus vectors associ ated with each word (columns of C) would be v dimensional.
Reference: [18] <author> Marti A. Hearst. </author> <title> Noun homograph disam biguation using local context in large text corpora. </title> <booktitle> In Seventh Annual Conference of the UW Centre for the New OED and Text Hinrich Schutze and Jan O. Pedersen Research, </booktitle> <pages> pages 1-22, </pages> <address> England: Oxford, </address> <year> 1991. </year>
Reference-contexts: [24], and Guthrie et al. [15] use online dic tionaries, Hirst [20] constructs knowledge bases, Cottrell [4] uses syntactic and semantic struc ture encoded in a connectionist net, Brown et al. [1] and Church and Gale [3] exploit bilingual corpora, Dagan et al. [7] use a bilingual dictio nary, Hearst <ref> [18] </ref> and Leacock et al. [26] exploit a hand-labeled training set, and Yarowsky [41] performs a computation based on Roget's the saurus. McRoy [28] investigates how multiple knowledge sources can be combined for disam biguation.
Reference: [19] <author> Marti A. Hearst and Hinrich Schutze. </author> <title> Cus tomizing a lexicon to better suit a compu tational task. </title> <editor> In Branimir Boguraev and James Pustejovsky, editors, </editor> <title> Corpus Pro cessing for Lexical Acquisition. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: We are not arguing that WordNet should not be used for disambiguation. On the con trary, it is a valuable source of information that may well improve the results presented here if the problems outlined above are overcome. For example, Hearst and Schutze <ref> [19] </ref> consider a possible way to address the lack of specificity. 2.2 Testing disambiguation with pseudowords Sanderson [34] uses pseudowords [36, 42] to test the utility of disambiguation for information re trieval.
Reference: [20] <author> Graeme Hirst. </author> <title> Semantic Interpretation and the Resolution of Ambiguity. </title> <publisher> Cam bridge University Press, </publisher> <address> Cambridge, </address> <year> 1987. </year>
Reference-contexts: present experimental results that show substantial improvement over a baseline system. 2 Related Work Different sources of information have been em ployed to discriminate senses computationally: Kelly and Stone [22] consider hand-constructed disambiguation rules, Lesk [27], Krovetz and Croft [24], and Guthrie et al. [15] use online dic tionaries, Hirst <ref> [20] </ref> constructs knowledge bases, Cottrell [4] uses syntactic and semantic struc ture encoded in a connectionist net, Brown et al. [1] and Church and Gale [3] exploit bilingual corpora, Dagan et al. [7] use a bilingual dictio nary, Hearst [18] and Leacock et al. [26] exploit a hand-labeled training set, and
Reference: [21] <author> Yufeng Jing and W. Bruce Croft. </author> <title> An asso ciation thesaurus for information retrieval. </title> <booktitle> In Proceedings of RIAO, </booktitle> <pages> pages 146-160, </pages> <address> Rockefeller University, New York, </address> <year> 1994. </year>
Reference-contexts: However, this matrix must be constructed in a series of steps to keep the computations tractable at each stage. A more detailed discussion of this com putation can be found in [38]. While other au tomatic thesaurus construction algorithms ex ploit similar information <ref> [5, 9, 21] </ref>, they use it 2 A standard ARPA text corpus of roughly 170,000 documents and 500MB of text from the Wall Street Journal and a set of 25 evaluation queries (Topics 51-75). Hinrich Schutze and Jan O.
Reference: [22] <author> Edward Kelly and Phillip Stone. </author> <title> Com puter Recognition of English Word Senses. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1975. </year>
Reference-contexts: sense disambiguation algorithm based on a vector representation of word similarity derived from lexical co-occurrence, apply it to information retrieval, and present experimental results that show substantial improvement over a baseline system. 2 Related Work Different sources of information have been em ployed to discriminate senses computationally: Kelly and Stone <ref> [22] </ref> consider hand-constructed disambiguation rules, Lesk [27], Krovetz and Croft [24], and Guthrie et al. [15] use online dic tionaries, Hirst [20] constructs knowledge bases, Cottrell [4] uses syntactic and semantic struc ture encoded in a connectionist net, Brown et al. [1] and Church and Gale [3] exploit bilingual corpora, Dagan
Reference: [23] <author> Adam Kilgarriff. </author> <title> Dictionary word sense distinctions: An enquiry into their nature. </title> <journal> Computers and the Humanities, </journal> <volume> 26:365 387, </volume> <year> 1993. </year>
Reference-contexts: The work presented here differs in another respect from previous approaches. In general, two assumptions have been made: * Only one sense of a word is used in each occurrence. * Each word has a fixed number of senses. The first assumption has been recently chal lenged by Kilgarriff <ref> [23] </ref>: "Sometimes two senses of a word are mutually exclusive, but more often they are not : : : ". He finds that a large propor tion of the words he investigates is used with several senses in one context.
Reference: [24] <author> Robert Krovetz and W. Bruce Croft. </author> <title> Word sense disambiguation using machine readable dictionaries. </title> <booktitle> In Proceedings of SI GIR '89, </booktitle> <pages> pages 127-136, </pages> <year> 1989. </year>
Reference-contexts: similarity derived from lexical co-occurrence, apply it to information retrieval, and present experimental results that show substantial improvement over a baseline system. 2 Related Work Different sources of information have been em ployed to discriminate senses computationally: Kelly and Stone [22] consider hand-constructed disambiguation rules, Lesk [27], Krovetz and Croft <ref> [24] </ref>, and Guthrie et al. [15] use online dic tionaries, Hirst [20] constructs knowledge bases, Cottrell [4] uses syntactic and semantic struc ture encoded in a connectionist net, Brown et al. [1] and Church and Gale [3] exploit bilingual corpora, Dagan et al. [7] use a bilingual dictio nary, Hearst [18]
Reference: [25] <author> Rovert Krovetz and W. Bruce Croft. </author> <title> Lex ical ambiguity and information retrieval. </title> <journal> ACM Transactions on Information Sys tems, </journal> <volume> 10(2) </volume> <pages> 115-141, </pages> <year> 1992. </year>
Reference-contexts: One would think therefore that ambiguity would be a problem. However, ex periments have not shown appreciable improve ments in overall system performance through the application of word sense disambiguation algorithms <ref> [25] </ref>. This is partly due to the fact that retrievals typically do not depend on a sin gle term, but rather on a set of related terms expressed in the query.
Reference: [26] <author> Claudia Leacock, Geoffrey Towell, and Ellen Voorhees. </author> <title> Corpus-based statisti cal sense resolution. </title> <booktitle> In Proceedings of the ARPA Workshop on Human Language Technology, </booktitle> <address> Plainsboro NJ, </address> <year> 1993. </year>
Reference-contexts: [15] use online dic tionaries, Hirst [20] constructs knowledge bases, Cottrell [4] uses syntactic and semantic struc ture encoded in a connectionist net, Brown et al. [1] and Church and Gale [3] exploit bilingual corpora, Dagan et al. [7] use a bilingual dictio nary, Hearst [18] and Leacock et al. <ref> [26] </ref> exploit a hand-labeled training set, and Yarowsky [41] performs a computation based on Roget's the saurus. McRoy [28] investigates how multiple knowledge sources can be combined for disam biguation.
Reference: [27] <author> Michael Lesk. </author> <title> Automatic sense disam biguation: How to tell a pine cone from an ice cream cone. </title> <booktitle> In Proceedings of the 1986 SIGDOC Conference, </booktitle> <address> New York, </address> <year> 1986. </year> <note> As sociation for Computing Machinery. </note>
Reference-contexts: vector representation of word similarity derived from lexical co-occurrence, apply it to information retrieval, and present experimental results that show substantial improvement over a baseline system. 2 Related Work Different sources of information have been em ployed to discriminate senses computationally: Kelly and Stone [22] consider hand-constructed disambiguation rules, Lesk <ref> [27] </ref>, Krovetz and Croft [24], and Guthrie et al. [15] use online dic tionaries, Hirst [20] constructs knowledge bases, Cottrell [4] uses syntactic and semantic struc ture encoded in a connectionist net, Brown et al. [1] and Church and Gale [3] exploit bilingual corpora, Dagan et al. [7] use a bilingual
Reference: [28] <author> Susan W. McRoy. </author> <title> Using multiple knowl edge sources for word sense disambigua tion. </title> <journal> Computational Linguistics, </journal> <volume> 18(1):1 30, </volume> <year> 1992. </year>
Reference-contexts: McRoy <ref> [28] </ref> investigates how multiple knowledge sources can be combined for disam biguation. Considerable effort is necessary to construct or obtain some of these information sources, for example hand-constructed disam biguation rules and hand-labeled training sets.
Reference: [29] <author> George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. </author> <title> Introduction to Word Net: An on-line lexical database. </title> <journal> Journal of Lexicography, </journal> <volume> 3(4) </volume> <pages> 235-244, </pages> <year> 1990. </year>
Reference-contexts: In the follow ing we analyze in greater detail two represen tative experiments [39, 34] and suggest possible reasons why they are less successful than the method presented in this paper. 2.1 Using WordNet Voorhees [39] uses the WordNet <ref> [29] </ref> online the saurus to perform disambiguation. Roughly, WordNet is first transformed into a mapping Information Retrieval Based on Word Senses from words to one or more classes (called "hoods").
Reference: [30] <author> Herbert Charles Morton. </author> <title> The story of Webster's third: </title> <editor> Philip Gove's contro versial dictionary. </editor> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1994. </year>
Reference-contexts: In lexicography, difficulties with sense in dividuation are well known. For example, Mor ton quotes Philip Gove, the editor of Webster's third, as saying <ref> [30] </ref>: Rather grotesquely, after centuries of lexicography and language study of one sort or another, it appears that no one has answered the ques tion of how we may know with sharp clarity and definitive exact ness when a word has one meaning alone : : : and when it has
Reference: [31] <author> Yonggang Qiu and H.P. Frei. </author> <title> Concept based query expansion. </title> <booktitle> In Proceedings of SIGIR '93, </booktitle> <year> 1993. </year>
Reference-contexts: Topical or semantic similarity between two words can then be defined as the cosine be tween the corresponding columns of C. The assumption is that words with similar mean ings will occur with similar neighbors if enough text material is available. Qiu and Frei <ref> [31] </ref> use a similar scheme, although the matrix in their case is documents vs. terms. However, simple resource calculations sug gest that this direct approach is not workable. The matrix C has v 2 =2 distinct entries where v is the size of the vocabulary.
Reference: [32] <author> Gerard Salton and Chris Buckley. </author> <title> Im proving retrieval performance by relevance feedback. </title> <journal> Journal of the American So ciety for Information Science, </journal> <volume> 41(4):288 297, </volume> <year> 1990. </year>
Reference-contexts: Since not all words are equally useful for defining topic, the computation of context vec tors weights each word vector according to its discriminating potential as measured by inverse document frequency <ref> [32] </ref>: w i = log ( n i where N is the total number of documents in a text collection (or any other meaningful unit, e.g. paragraphs) and n i is the number of doc uments that the word w i occurs in. <p> In the standard model, documents are ranked according to the number of words they share with the query. In the mod ified model, documents are ranked according to the number of senses (disambiguated words) they share with the query. We used term fre quency/inverse document frequency weighting <ref> [32] </ref> for the senses as well as for the words. We achieved a measurable improvement for sense based retrieval when compared to word-based retrieval: Average precision for 11 points of re call increased by 4% (from 0.299 to 0.311). Two modifications were introduced to fur ther improve these results.
Reference: [33] <author> Gerard Salton and Michael J. McGill. </author> <title> In troduction to modern information retrieval. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: precision (non-interpolated) over all rel docs 0.299 0.321 +7.4 0.342 +14.4 Table 4: Disambiguation markedly improves retrieval performance (3 senses per occur rence). nally, the 25 queries were processed in the same way. 5 A modification of the standard vector sim ilarity model that represents documents and queries as vectors <ref> [33] </ref> was then applied for senses instead of words. In the standard model, documents are ranked according to the number of words they share with the query. In the mod ified model, documents are ranked according to the number of senses (disambiguated words) they share with the query.
Reference: [34] <author> Mark Sanderson. </author> <title> Word sense disambigua tion and information retrieval. </title> <booktitle> In Proceed ings of SIGIR '94, </booktitle> <pages> pages 142-151, </pages> <year> 1994. </year>
Reference-contexts: However, it seems in tuitively clear that a good sense disambiguation algorithm should, at least, not decrease perfor mance and might potentially increase it in some cases (especially for short queries). Recent work <ref> [34] </ref> casts doubt on this intuition. <p> Therefore, we will make fine distinctions wherever there is sufficient information to do so. Various attempts have been made to apply automatic sense disambiguation to information retrieval with indifferent results. In the follow ing we analyze in greater detail two represen tative experiments <ref> [39, 34] </ref> and suggest possible reasons why they are less successful than the method presented in this paper. 2.1 Using WordNet Voorhees [39] uses the WordNet [29] online the saurus to perform disambiguation. <p> On the con trary, it is a valuable source of information that may well improve the results presented here if the problems outlined above are overcome. For example, Hearst and Schutze [19] consider a possible way to address the lack of specificity. 2.2 Testing disambiguation with pseudowords Sanderson <ref> [34] </ref> uses pseudowords [36, 42] to test the utility of disambiguation for information re trieval. A pseudoword is created by assigning two or more types, for example "banana" and "door", to a new type, "banana-door".
Reference: [35] <author> Mark Sanderson, </author> <year> 1995. </year> <title> Electronic mail message to the authors, </title> <month> 26 Jan </month> <year> 1995. </year>
Reference-contexts: a thesaurus since it relates words to other similar words, but it should not be confused with hand-built lexical resources such as WordNet. 1 Mark Sanderson recently applied pseudoword creation to two collections with natural queries (CACM and Cranfield) and found effects "broadly similar" to those of the Reuters experiment <ref> [35] </ref>. If no consistent differences between natural and ar tificial queries are found, one would have to con clude that the different biases of the two disam biguation methods (for vs. against strongly domi nating senses) are responsible for their different im pact on IR performance.
Reference: [36] <author> Hinrich Schutze. </author> <title> Dimensions of mean ing. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 787-796, </pages> <address> Minneapolis MN, </address> <year> 1992. </year>
Reference-contexts: For example, Hearst and Schutze [19] consider a possible way to address the lack of specificity. 2.2 Testing disambiguation with pseudowords Sanderson [34] uses pseudowords <ref> [36, 42] </ref> to test the utility of disambiguation for information re trieval. A pseudoword is created by assigning two or more types, for example "banana" and "door", to a new type, "banana-door". In the subsequent evaluation, the retrieval system has no access to information that would distinguish the two words. <p> It is known that disam biguation performance decreases when less context is available <ref> [11, 36] </ref>. Further research is necessary to determine how much such a deterioration in dis ambiguation affects performance of the information retrieval model presented here. and combined the ranks for each document. The final ranking was then the ranking of the combined ranks r 0 i .
Reference: [37] <author> Hinrich Schutze. </author> <title> Word space. </title> <editor> In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 895-902. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo CA, </address> <year> 1993. </year>
Reference-contexts: Table 3 summarizes the re 4 The thesaurus induction algorithm used for the experiment described in this section differs in de tails from the one presented above, but uses essen tially the same method. See <ref> [37] </ref> for details. sults. On average, the performance of the algo rithm is about 90%.
Reference: [38] <author> Hinrich Schutze and Jan O. Pedersen. </author> <title> A cooccurrence-based thesaurus and two ap plications to information retrieval. </title> <booktitle> In Pro ceedings of RIAO, </booktitle> <pages> pages 266-274, </pages> <address> Rocke feller University, New York, </address> <year> 1994. </year>
Reference-contexts: However, this matrix must be constructed in a series of steps to keep the computations tractable at each stage. A more detailed discussion of this com putation can be found in <ref> [38] </ref>. While other au tomatic thesaurus construction algorithms ex ploit similar information [5, 9, 21], they use it 2 A standard ARPA text corpus of roughly 170,000 documents and 500MB of text from the Wall Street Journal and a set of 25 evaluation queries (Topics 51-75).
Reference: [39] <author> Ellen M. Voorhees. </author> <title> Using wordnet to dis ambiguate word senses for text retrieval. </title> <booktitle> In Proceedings of SIGIR '93, </booktitle> <pages> pages 171-180, </pages> <year> 1993. </year>
Reference-contexts: Therefore, we will make fine distinctions wherever there is sufficient information to do so. Various attempts have been made to apply automatic sense disambiguation to information retrieval with indifferent results. In the follow ing we analyze in greater detail two represen tative experiments <ref> [39, 34] </ref> and suggest possible reasons why they are less successful than the method presented in this paper. 2.1 Using WordNet Voorhees [39] uses the WordNet [29] online the saurus to perform disambiguation. <p> In the follow ing we analyze in greater detail two represen tative experiments [39, 34] and suggest possible reasons why they are less successful than the method presented in this paper. 2.1 Using WordNet Voorhees <ref> [39] </ref> uses the WordNet [29] online the saurus to perform disambiguation. Roughly, WordNet is first transformed into a mapping Information Retrieval Based on Word Senses from words to one or more classes (called "hoods").
Reference: [40] <author> Yorick A. Wilks, Dan C. Fass, Cheng ming Guo, James E. McDonald, Tony Plate, </author> <title> Information Retrieval Based on Word Senses and Brian M. Slator. Providing machine tractable dictionary tools. </title> <journal> Journal of Com puters and Translation, </journal> <volume> 2, </volume> <year> 1990. </year>
Reference-contexts: neighbors in the corpus) rather than simple first-order co-occurrence (occurring near to each other) to find synonyms. 4 The Disambiguation Al gorithm An individual occurrence t of a word can be characterized by summing the thesaurus vectors of the words that occur close to it to produce a context vector <ref> [40, 12] </ref>. Since the direction of a word's vector corresponds to its main topic, if several words with the same main topic oc cur close to t, then that topic will dominate in the computation of the context vector.
Reference: [41] <author> David Yarowsky. </author> <title> Word-sense disambigua tion using statistical models of Roget's cat egories trained on large corpora. </title> <booktitle> In Pro ceedings of Coling-92, </booktitle> <year> 1992. </year>
Reference-contexts: knowledge bases, Cottrell [4] uses syntactic and semantic struc ture encoded in a connectionist net, Brown et al. [1] and Church and Gale [3] exploit bilingual corpora, Dagan et al. [7] use a bilingual dictio nary, Hearst [18] and Leacock et al. [26] exploit a hand-labeled training set, and Yarowsky <ref> [41] </ref> performs a computation based on Roget's the saurus. McRoy [28] investigates how multiple knowledge sources can be combined for disam biguation. Considerable effort is necessary to construct or obtain some of these information sources, for example hand-constructed disam biguation rules and hand-labeled training sets. <p> Of the other words in Table 3 "train" is noteworthy for exhibiting a noun/verb ambigu ity. The algorithm also distinguishes the three senses of "vessel" correctly. Table 3 suggests some similarity with the above-mentioned disambiguation method pro posed by Yarowsky <ref> [41] </ref>. However, his method classifies contexts as belonging to one of a pre defined set of thesaurus classes (taken from Ro get's thesaurus) rather than grouping contexts according to similarity as we do.
Reference: [42] <author> David Yarowsky. </author> <title> One sense per colloca tion. </title> <booktitle> In Proceedings of ARPA Human Lan guage Technology Workshop, </booktitle> <year> 1993. </year> <editor> Hinrich Schutze and Jan O. </editor> <publisher> Pedersen </publisher>
Reference-contexts: For example, Hearst and Schutze [19] consider a possible way to address the lack of specificity. 2.2 Testing disambiguation with pseudowords Sanderson [34] uses pseudowords <ref> [36, 42] </ref> to test the utility of disambiguation for information re trieval. A pseudoword is created by assigning two or more types, for example "banana" and "door", to a new type, "banana-door". In the subsequent evaluation, the retrieval system has no access to information that would distinguish the two words.
References-found: 42

