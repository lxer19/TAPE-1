URL: http://www.eecs.umich.edu/~nelsonr/postscript-docs/acm-mm95.ps
Refering-URL: http://www.eecs.umich.edu/~nelsonr/publications.html
Root-URL: http://www.eecs.umich.edu
Email: email: fnelsonr,aprakashg@eecs.umich.edu  
Title: Dealing with Synchronization and Timing Variability in the Playback of Interactive Session Recordings  
Author: Nelson R. Manohar and Atul Prakash 
Keyword: Media integration and synchronization, collaboration environments, and session capture and replay.  
Address: Ann Arbor, MI 48109-2122 USA.  
Affiliation: Department of Electrical Engineering and Computer Science University of Michigan,  
Abstract: In this paper, we describe scheduling and synchronization support for a novel multimedia document, referred to as a session object. The session object captures a voice-annotated, interactive session with an application | it contains audio and window streams. This paper addresses media scheduling and synchronization issues for the support of faithful replay of session objects when subject to timing variability at the replay workstation. The replay is supported by an adaptive scheduling algorithm. The algorithm preserves relative inter-stream synchronization between window and audio streams. Run-time temporal deformations are applied over the schedule of the window stream. We show that the inter-stream asynchrony floats under statistical control as a function of the scheduling interval. The mechanisms could be generalized to the replay of streams that are subject to timing variability. Our object-oriented toolkit, ReplayKit, enables an application to become replay-aware through access to session capture and replay functionality. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H.M. Abdel-Wahab, S. Guan, and J. Nievergelt. </author> <title> Shared workspaces for group collaboration: An experiment using Internet and Unix inter-process communication. </title> <journal> IEEE Communications Magazine, </journal> <pages> pages 10-16, </pages> <month> Novem-ber </month> <year> 1988. </year>
Reference-contexts: The Replay of Application Workspaces The replay of a user session with an application workspace has collaborative value <ref> [1, 25] </ref>. The following are approaches to replay a user session with an application's workspace. Screen recorders, such as WatchMe (for NeXTs) and QuickTime Conferencing (for the Macs), represent an application-independent approach to session capture and replay. <p> Consequently, the replay of a session reproduces only the external look of the workstation's screen. Since interaction with the application's workspace is not possible, its suitability for asynchronous collaboration work is also reduced. Systems such as SharedX, Xtv <ref> [1] </ref>, and Ceced [10] also represent an application-independent approach that can support session capture and replay. Capture of a session is done by intercepting events sent by an application to a collaboration-aware server [7].
Reference: [2] <author> D.P. Anderson and R. Kuivila. </author> <title> A system for music performance. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 56-82, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: introduce contention to the re-execution overheads, as a result, the fine-grained synchronization of asynchronous media establishes a need for an end-to-end (i.e., fetch, schedule, and execution) solutions to the management of overheads Adaptive Scheduling Strategies for Replay The scheduling of discrete events has also been addressed in computer-based musical systems <ref> [2] </ref>), however, there are two major differences. First, no variability is introduced by the re-execution of MIDI events on the real-time synthesizer. Second, integrated media replay is not supported. Steinmetz [32] provided early illustrations for the need for integration of synchronous and asynchronous media. <p> The schedule compensation adjustment is variable and revised, (upgraded or downgraded, as needed), on every scheduling interval. However, asynchrony floats, under statistical control, during the scheduling interval. Our time deformations are different from temporal transformations, (as in <ref> [2, 15, 23, 30] </ref>).
Reference: [3] <author> R. Baker, A. Downing, K. Finn, E. Rennison, D.D. Kim, and Y.H. Lim. </author> <title> Multimedia processing model for a distributed multimedia I/O system. </title> <booktitle> In Proc. of the 3rd Int'l Workshop on Network and Operating System Support for Digital Audio and Video, </booktitle> <pages> pages 164-175, </pages> <address> La Jolla, CA, USA, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: The Replay of Stored Multimedia Our work relates to research in the replay of stored multimedia. There are two approaches to the replay of stored media: network-based replay and workstation-based replay. However, there are important differences, explained as follows. Research in network-based continuous media players, such as <ref> [3, 23, 30, 31] </ref>, addresses different sources of overheads. The replay of stored media has three basic sources of overhead: (1) fetch, (2) process, and (3) presentation. In network-based replay, the media server implements media access, buffering and synchronization tasks. Variability is primarily attributed to latencies in the network.
Reference: [4] <author> D.C.A. Bulterman and R. van Liere. </author> <title> Multimedia synchronization and UNIX. </title> <booktitle> In Proc. of the 3rd Int'l Workshop on Network and Operating System Support for Digital Audio and Video, </booktitle> <pages> pages 108-119, </pages> <address> La Jolla, CA, USA, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Using thread-based stream handlers, however, reduces our synchronization precision since thread models are subject to interrupts, preemption, and large system call overheads while lack peer-to-peer guaranteed scheduling time. Our synchronization precision was targeted to support about 0:1 to 1s, (e.g., audio-annotated slide shows as quoted from <ref> [4] </ref>). Finer grain synchronization would have imposed demands requiring OS-level support [4] and thus compromising our goal for high level support (R2). Synchronization Operations Under the presence of timing variability, a way to preserve the relative synchronization of streams is needed. <p> Our synchronization precision was targeted to support about 0:1 to 1s, (e.g., audio-annotated slide shows as quoted from <ref> [4] </ref>). Finer grain synchronization would have imposed demands requiring OS-level support [4] and thus compromising our goal for high level support (R2). Synchronization Operations Under the presence of timing variability, a way to preserve the relative synchronization of streams is needed. Synchronization is based on the use of synchronization events, widely accepted in the synchronization literature [32]. <p> Inter-stream Synchronization Mechanism The replay of streams must be kept synchronized. Our synchronization model is based on the notion of a master and multiple slave streams, (as in <ref> [4, 15, 32] </ref>). However, unlike these master/slave models, our approach differs in the following ways. First, synchronization is relative to the progress of the master stream (as opposed to the progress of logical time, as in the Tactus system [11]).
Reference: [5] <author> D.C.A. Bulterman, G. van Rossum, and R. van Liere. </author> <title> A structure for transportable, dynamic multimedia documents. </title> <booktitle> In Proc. of the Summer 1991 USENIX Conference, </booktitle> <pages> pages 137-154, </pages> <address> Nashville, TN, USA., </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Second, integrated media replay is not supported. Steinmetz [32] provided early illustrations for the need for integration of synchronous and asynchronous media. Some issues in specification and presentation of multimedia documents have been addressed in the Firefly system [6] and in Cmif <ref> [5] </ref>. However, the focus of their work is on specification and enforcements of synchronization constraints among high-level parts of a multimedia document. Synchronization at internal points among media segments is not addressed. In our work, the focus is on enforcing fine-grain synchronization at internal points among streams.
Reference: [6] <author> M. Cecelia-Buchanan and P.T. Zellweger. </author> <title> Scheduling multimedia documents using temporal constraints. </title> <booktitle> In Proc. of the 3rd Int'l Workshop on Network and Operating System Support for Digital Audio and Video, </booktitle> <pages> pages 237-249, </pages> <address> La Jolla, CA, USA, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Second, integrated media replay is not supported. Steinmetz [32] provided early illustrations for the need for integration of synchronous and asynchronous media. Some issues in specification and presentation of multimedia documents have been addressed in the Firefly system <ref> [6] </ref> and in Cmif [5]. However, the focus of their work is on specification and enforcements of synchronization constraints among high-level parts of a multimedia document. Synchronization at internal points among media segments is not addressed.
Reference: [7] <author> G. Chung, K. Jeffay, and H. Adbel-Wahab. </author> <title> Accomo-dating latecommers in shared window systems. </title> <journal> IEEE Computer, </journal> <volume> 26(1) </volume> <pages> 72-74, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Systems such as SharedX, Xtv [1], and Ceced [10] also represent an application-independent approach that can support session capture and replay. Capture of a session is done by intercepting events sent by an application to a collaboration-aware server <ref> [7] </ref>. Although this approach, in principle, allows replay of unmodified applications, interacting with the session's application workspace is not possible. Furthermore, currently, these systems do not offer fine-grained audio synchronization support. The ideas presented in this paper can be used to extend these systems to include synchronized audio support.
Reference: [8] <author> R. Clauer, J.D. Kelly, T.J. Rosenberg, C.E.P. Stauning, and et. al. UARC: </author> <title> A Prototype Upper Atmostpheric Research Collaboratory. </title> <journal> EOS Trans. American Geo-phys. Union, </journal> <volume> 267(74), </volume> <year> 1993. </year>
Reference-contexts: The following examples, being pursued as part of our research, illustrate the needs and benefits for session objects in asynchronous collaboration scenarios: UARC: The ReplayKit research was originally motivated by the uarc project, (see Fig. 2), a collaboratory experiment among domain scientists in a wide-area network <ref> [8] </ref>. The domain of research among the scientists is space science. Because domain scientists often work from different time-zones and it is not known a-priori when interesting phenomena will be observed, providing support for session capture, annotation, replay, and exchange should facilitate col laborative work among scientists.
Reference: [9] <author> E. Craighill, M. Fong, K. Skinner, and et. al. SCOOT: </author> <title> An object-oriented toolkit for multimedia collaboration. </title> <booktitle> In Proc. of ACM Multimedia '94, </booktitle> <pages> pages 41-49, </pages> <address> San Francisco, CA, USA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Furthermore, currently, these systems do not offer fine-grained audio synchronization support. The ideas presented in this paper can be used to extend these systems to include synchronized audio support. Toolkits and application-specific prototypes represent application-dependent approaches to session capture and replay. The Scoot toolkit <ref> [9] </ref> proposes two approaches to capture and replay of interactive sessions. The first approach logs periodical application snapshots. This choice results in coarse replay progress feedback and limited synchronization precision. The second approach logs method invocations to log-aware objects.
Reference: [10] <author> E. Craighill, R. Lang, M. Fong, and K. Skinner. CECED: </author> <title> A system for informal multimedia collaboration. </title> <booktitle> In Proc. of ACM Multimedia '93, </booktitle> <pages> pages 436-446, </pages> <address> CA, USA, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Consequently, the replay of a session reproduces only the external look of the workstation's screen. Since interaction with the application's workspace is not possible, its suitability for asynchronous collaboration work is also reduced. Systems such as SharedX, Xtv [1], and Ceced <ref> [10] </ref> also represent an application-independent approach that can support session capture and replay. Capture of a session is done by intercepting events sent by an application to a collaboration-aware server [7]. Although this approach, in principle, allows replay of unmodified applications, interacting with the session's application workspace is not possible.
Reference: [11] <author> R. Dannenberg, T. Neuendorffer, J. M. Newcomer, and D. Rubine. Tactus: </author> <title> Toolkit-level support for synchronized interactive multimedia. </title> <booktitle> In Proc. of the 3rd Int'l Workshop on Network and Operating System Support for Digital Audio and Video, </booktitle> <pages> pages 302-313, </pages> <address> La Jolla, CA, USA, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: However, their synchronization requirements are simplified since only Mouse-Moved events, (to draw and move the pen), are replayed. This removes significant timing variability from the replay of the window stream. A media server approach is taken by the Tactus system <ref> [11, 12] </ref>. Tactus is actually composed of an application-independent media server and an application-dependent toolkit. There are some important differences. First, the Tactus server assumes the use of reliable timing services at the scheduler. Our work, on the other hand, targets timing services as a source of timing variability. <p> However, unlike these master/slave models, our approach differs in the following ways. First, synchronization is relative to the progress of the master stream (as opposed to the progress of logical time, as in the Tactus system <ref> [11] </ref>).
Reference: [12] <author> R.B. Dannenberg, T. Neuendorffer, J.M. Newcomer, D. Rubine, and D.B. Anderson. Tactus: </author> <title> toolkit-level support for synchronized interactive multimedia. </title> <journal> Multimedia Systems, </journal> <volume> 1(1) </volume> <pages> 77-86, </pages> <year> 1993. </year>
Reference-contexts: However, their synchronization requirements are simplified since only Mouse-Moved events, (to draw and move the pen), are replayed. This removes significant timing variability from the replay of the window stream. A media server approach is taken by the Tactus system <ref> [11, 12] </ref>. Tactus is actually composed of an application-independent media server and an application-dependent toolkit. There are some important differences. First, the Tactus server assumes the use of reliable timing services at the scheduler. Our work, on the other hand, targets timing services as a source of timing variability.
Reference: [13] <author> A. Eleftheriadis, S. Pejhan, and D. Anastassiou. </author> <title> Algorithms and Performance Evaluation of the Xphone Multimedia Communication System. </title> <booktitle> Proc. of ACM Multimedia'93, </booktitle> <pages> pages 311-320, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Synchronization at internal points among media segments is not addressed. In our work, the focus is on enforcing fine-grain synchronization at internal points among streams. Research in multimedia network interfaces, such as <ref> [13, 18, 26, 27, 31] </ref>, propose adaptive scheduling protocols to manage bounded network variability. Their focus is on the management of overheads up to the delivery of data to the presentation workstation. These approaches assume negligible overheads on the processing and presentation of streams by the client workstation.
Reference: [14] <author> T. Fisher. </author> <title> Real-time scheduling support in ultrix-4.2 for multimedia communication. </title> <booktitle> In In Proc. of the 3rd Int'l Workshop on Network and Operating System Support for Digital Audio and Video, </booktitle> <pages> pages 321-327, </pages> <address> La Jolla, CA, USA, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Our work can be built of top of these and benefit from this research. Research in the management of scheduling overheads, such as in <ref> [14, 28, 19] </ref> focus on low-level operating system techniques for the support of multimedia (such as bounding of scheduling latencies, use of pre-emptive deadline scheduling, use of correlated task scheduling, etc.). Our work differs from these in several points.
Reference: [15] <author> S. Gibbs. </author> <title> Composite multimedia and active objects. </title> <booktitle> In Proc. of OOPSLA'91, </booktitle> <pages> pages 97-112, </pages> <address> Phoenix, AZ, USA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Inter-stream Synchronization Mechanism The replay of streams must be kept synchronized. Our synchronization model is based on the notion of a master and multiple slave streams, (as in <ref> [4, 15, 32] </ref>). However, unlike these master/slave models, our approach differs in the following ways. First, synchronization is relative to the progress of the master stream (as opposed to the progress of logical time, as in the Tactus system [11]). <p> The schedule compensation adjustment is variable and revised, (upgraded or downgraded, as needed), on every scheduling interval. However, asynchrony floats, under statistical control, during the scheduling interval. Our time deformations are different from temporal transformations, (as in <ref> [2, 15, 23, 30] </ref>).
Reference: [16] <author> S. Gibbs, L. Dami, and D. Tsichritzis. </author> <title> An object-oriented framework for multimedia composition and synchronization. In Multimedia Systems, Interaction and Applications: </title> <booktitle> Proc. of the First Eurographics Workshop, </booktitle> <pages> pages 101-111. </pages> <publisher> Springer-Verlag, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: Table I specifies these protocols in terms of their handling of these asynchrony cases. Protocol P1 and P2 relate to Gibbs' NoSync and InterruptSync synchronization modes between master and slaves found in <ref> [16] </ref>. Protocol P3 is our adaptive scheduling algorithm. Protocol P4 is a two-way, stop and wait, protocol for relative synchronization of discrete streams. Adaptive Scheduling Mechanism The basic idea behind our adaptive mechanism was illustrated in Fig. 3. Next, we analyze this adaptive mechanism.
Reference: [17] <author> T. Imai, K. Yamaguchi, and T. Muranaga. </author> <title> Hypermedia conversation recording to preserve informal artifacts in realtime collaboration. </title> <booktitle> In Proc. of ACM Multimedia '94, </booktitle> <pages> pages 417-424, </pages> <address> San Francisco, CA, USA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The first approach logs periodical application snapshots. This choice results in coarse replay progress feedback and limited synchronization precision. The second approach logs method invocations to log-aware objects. However, synchronization of audio wrt the replay of such an asynchronous log stream is not addressed. Finally, the prototypes found in <ref> [17, 26] </ref> capture and replay both audio and window streams. However, their synchronization requirements are simplified since only Mouse-Moved events, (to draw and move the pen), are replayed. This removes significant timing variability from the replay of the window stream.
Reference: [18] <author> K. Jeffay, D. Stone, and F. Smith. </author> <title> Transport and display mechanisms for multimedia conferencing across packet switched networks. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 26(10) </volume> <pages> 1281-1304, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Synchronization at internal points among media segments is not addressed. In our work, the focus is on enforcing fine-grain synchronization at internal points among streams. Research in multimedia network interfaces, such as <ref> [13, 18, 26, 27, 31] </ref>, propose adaptive scheduling protocols to manage bounded network variability. Their focus is on the management of overheads up to the delivery of data to the presentation workstation. These approaches assume negligible overheads on the processing and presentation of streams by the client workstation.
Reference: [19] <author> K. Jeffay, D.L. Stone, and F. Donelson. </author> <title> Kernel support for live digital audio and video. </title> <journal> Computer Communications, </journal> <volume> 15(6) </volume> <pages> 388-395, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Our work can be built of top of these and benefit from this research. Research in the management of scheduling overheads, such as in <ref> [14, 28, 19] </ref> focus on low-level operating system techniques for the support of multimedia (such as bounding of scheduling latencies, use of pre-emptive deadline scheduling, use of correlated task scheduling, etc.). Our work differs from these in several points.
Reference: [20] <author> H.P. Katseff and B.S. Robinson. </author> <title> Predictive prefetch in the Nemesis multimedia information service. </title> <booktitle> In Proc. of ACM Multimedia '94, </booktitle> <pages> pages 201-210, </pages> <address> San Francisco, CA, USA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: However, both timer inaccuracies and the variability in execution time are more difficult to deal with. Our work extends existing workstation-based replay management of stored media through integration of stateful, asynchronous media that is subject to timing variability. Research in the management of fetch overheads, such as in <ref> [20, 29, 30] </ref> focus on low-level file system extensions (such as predictive prefetching, disk layout, optimal buffering, etc.) for the support of continuous media streams. Our work can be built of top of these and benefit from this research.
Reference: [21] <author> L. Li, A. Karmouch, </author> <title> and N.D. Georganas. Multimedia teleorchestra with independent sources: Part 2 | synchronization algorithms. </title> <journal> Multimedia Systems, </journal> <volume> 1(2) </volume> <pages> 154-165, </pages> <year> 1994. </year>
Reference-contexts: A synchronization event, s i (lhs rhs), preserves relative timing between lhs and rhs streams involved in a synchronization dependency. In this notation, rhs streams synchronize to the lhs stream. In terms of temporal specification notations found in <ref> [21, 22] </ref>, our synchronization specifically preserves the relative synchronization relationship (e i same a j ) between window and audio streams. Figure 4 shows the use of a synchronization event s i (A W ), where A is the audio stream and W is the window stream.
Reference: [22] <author> T. Little and F. Ghafoor. </author> <title> Models for multimedia objects. </title> <journal> IEEE Journal of Selected Areas of Communication, </journal> <volume> 8(3), </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: A synchronization event, s i (lhs rhs), preserves relative timing between lhs and rhs streams involved in a synchronization dependency. In this notation, rhs streams synchronize to the lhs stream. In terms of temporal specification notations found in <ref> [21, 22] </ref>, our synchronization specifically preserves the relative synchronization relationship (e i same a j ) between window and audio streams. Figure 4 shows the use of a synchronization event s i (A W ), where A is the audio stream and W is the window stream.
Reference: [23] <author> P. Lougher and D. Shepherd. </author> <title> The design of storage servers for continuous multimedia. </title> <journal> The Computer Journal, </journal> <volume> 63(1) </volume> <pages> 69-91, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The Replay of Stored Multimedia Our work relates to research in the replay of stored multimedia. There are two approaches to the replay of stored media: network-based replay and workstation-based replay. However, there are important differences, explained as follows. Research in network-based continuous media players, such as <ref> [3, 23, 30, 31] </ref>, addresses different sources of overheads. The replay of stored media has three basic sources of overhead: (1) fetch, (2) process, and (3) presentation. In network-based replay, the media server implements media access, buffering and synchronization tasks. Variability is primarily attributed to latencies in the network. <p> The schedule compensation adjustment is variable and revised, (upgraded or downgraded, as needed), on every scheduling interval. However, asynchrony floats, under statistical control, during the scheduling interval. Our time deformations are different from temporal transformations, (as in <ref> [2, 15, 23, 30] </ref>).
Reference: [24] <author> N.R. Manohar and A. Prakash. </author> <title> Replay by re-execution: a paradigm for asynchronous collaboration via record and replay of interactive multimedia streams. </title> <journal> ACM SIGOIS Bulletin, </journal> <volume> 15(2) </volume> <pages> 32-34, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: However, a synchronous mode of collaboration can often be too imposing on the schedule of the participants. It requires that users be able to find a common time to work together but, in many cases, that is not easy. In <ref> [24, 25] </ref>, we presented the WYSNIWIST (What You See Now, Is What I Saw Then) paradigm for asynchronous collaboration that allows users to record and replay an interactive session with an application. The paradigm introduced an associated data artifact, the session object, used to capture an interactive session.
Reference: [25] <author> N.R. Manohar and A. Prakash. </author> <title> The Session Capture and Replay Paradigm for Asynchronous Collaboration. </title> <booktitle> In Proc. of European Conference on Computer Supported Cooperative Work (ECSCW)'95, </booktitle> <address> Stockholm, Sweden, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: However, a synchronous mode of collaboration can often be too imposing on the schedule of the participants. It requires that users be able to find a common time to work together but, in many cases, that is not easy. In <ref> [24, 25] </ref>, we presented the WYSNIWIST (What You See Now, Is What I Saw Then) paradigm for asynchronous collaboration that allows users to record and replay an interactive session with an application. The paradigm introduced an associated data artifact, the session object, used to capture an interactive session. <p> The Replay of Application Workspaces The replay of a user session with an application workspace has collaborative value <ref> [1, 25] </ref>. The following are approaches to replay a user session with an application's workspace. Screen recorders, such as WatchMe (for NeXTs) and QuickTime Conferencing (for the Macs), represent an application-independent approach to session capture and replay.
Reference: [26] <author> A. Mathur and A. Prakash. </author> <title> Protocols for integrated audio and shared windows in collaborative systems. </title> <booktitle> In Proc. of ACM Multimedia '94, </booktitle> <pages> pages 381-390, </pages> <address> San Fran-cisco, CA, USA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The first approach logs periodical application snapshots. This choice results in coarse replay progress feedback and limited synchronization precision. The second approach logs method invocations to log-aware objects. However, synchronization of audio wrt the replay of such an asynchronous log stream is not addressed. Finally, the prototypes found in <ref> [17, 26] </ref> capture and replay both audio and window streams. However, their synchronization requirements are simplified since only Mouse-Moved events, (to draw and move the pen), are replayed. This removes significant timing variability from the replay of the window stream. <p> Finally, continuous media players assume negligible presentation overheads for the presentation of the streams. Our domain requires fine-grained integration of both continuous and discrete media. The playback integrated media affects our choices for both scheduling and synchronization, as found in <ref> [26] </ref>. The re-execution of fine-grained, discrete, asynchronous media requires additional compensation for timing variability on the re-execution of the stream. <p> Synchronization at internal points among media segments is not addressed. In our work, the focus is on enforcing fine-grain synchronization at internal points among streams. Research in multimedia network interfaces, such as <ref> [13, 18, 26, 27, 31] </ref>, propose adaptive scheduling protocols to manage bounded network variability. Their focus is on the management of overheads up to the delivery of data to the presentation workstation. These approaches assume negligible overheads on the processing and presentation of streams by the client workstation.
Reference: [27] <author> W.A. Montgomery. </author> <title> Techniques for packet voice synchronization. </title> <journal> IEEE Journal on Selected Areas In Communication, </journal> <volume> SAC-1(6):1022-1027, </volume> <month> December </month> <year> 1983. </year>
Reference-contexts: Synchronization at internal points among media segments is not addressed. In our work, the focus is on enforcing fine-grain synchronization at internal points among streams. Research in multimedia network interfaces, such as <ref> [13, 18, 26, 27, 31] </ref>, propose adaptive scheduling protocols to manage bounded network variability. Their focus is on the management of overheads up to the delivery of data to the presentation workstation. These approaches assume negligible overheads on the processing and presentation of streams by the client workstation.
Reference: [28] <author> J. Nakajima, M. Yazaki, and H. Matsumoto. </author> <title> Multimedia/realtime extensions for the Mach operating system. </title> <booktitle> In Proc. of the Summer USENIX Conference, </booktitle> <pages> pages 183-196, </pages> <address> Nashville, TN, USA, </address> <month> Summer </month> <year> 1991. </year> <booktitle> USENIX. </booktitle>
Reference-contexts: Our work can be built of top of these and benefit from this research. Research in the management of scheduling overheads, such as in <ref> [14, 28, 19] </ref> focus on low-level operating system techniques for the support of multimedia (such as bounding of scheduling latencies, use of pre-emptive deadline scheduling, use of correlated task scheduling, etc.). Our work differs from these in several points.
Reference: [29] <author> P. Venkat Rangan and Harrick M. Vin. </author> <title> Designing file systems for digital video and audio. </title> <journal> ACM Transactions of Computer Systems, </journal> <volume> 18(2) </volume> <pages> 197-222, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: However, both timer inaccuracies and the variability in execution time are more difficult to deal with. Our work extends existing workstation-based replay management of stored media through integration of stateful, asynchronous media that is subject to timing variability. Research in the management of fetch overheads, such as in <ref> [20, 29, 30] </ref> focus on low-level file system extensions (such as predictive prefetching, disk layout, optimal buffering, etc.) for the support of continuous media streams. Our work can be built of top of these and benefit from this research.
Reference: [30] <author> L.A. Rowe and B.C. Smith. </author> <title> A Continuous Media Player. </title> <booktitle> In Proc. of the 3rd Int'l Workshop on Network and Operating System Support for Digital Audio and Video, </booktitle> <pages> pages 376-386, </pages> <address> La Jolla, CA, USA, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: The Replay of Stored Multimedia Our work relates to research in the replay of stored multimedia. There are two approaches to the replay of stored media: network-based replay and workstation-based replay. However, there are important differences, explained as follows. Research in network-based continuous media players, such as <ref> [3, 23, 30, 31] </ref>, addresses different sources of overheads. The replay of stored media has three basic sources of overhead: (1) fetch, (2) process, and (3) presentation. In network-based replay, the media server implements media access, buffering and synchronization tasks. Variability is primarily attributed to latencies in the network. <p> However, both timer inaccuracies and the variability in execution time are more difficult to deal with. Our work extends existing workstation-based replay management of stored media through integration of stateful, asynchronous media that is subject to timing variability. Research in the management of fetch overheads, such as in <ref> [20, 29, 30] </ref> focus on low-level file system extensions (such as predictive prefetching, disk layout, optimal buffering, etc.) for the support of continuous media streams. Our work can be built of top of these and benefit from this research. <p> The schedule compensation adjustment is variable and revised, (upgraded or downgraded, as needed), on every scheduling interval. However, asynchrony floats, under statistical control, during the scheduling interval. Our time deformations are different from temporal transformations, (as in <ref> [2, 15, 23, 30] </ref>). <p> The consumer thread gets events from the shared queue and dispatches them to the window system for replay. frames. To reduce overheads in disk I/O access of audio frames, we implemented buffered sampling and prefetching of frames, (as in Cmss <ref> [30] </ref>). We baselined three important parameters that affect overheads f o (A1) during the record and replay of audio frames: (1) the audio frame size, (2) Shared Queue Consumer Thread Wait Get Execute Put Producer Thread Fetch Schedule stream. The producer thread fetches and dispatches events into the event queue.
Reference: [31] <author> D. Rubine, R.B. Dannenberg, and D.B. Anderson. </author> <title> Low-latency interaction through choice-points, buffering and cuts in Tactus. </title> <booktitle> In Proc. of the Int'l Conference on Multimedia Computing and Systems, </booktitle> <pages> pages 224-233, </pages> <address> Los Alamitos, CA, USA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: The Replay of Stored Multimedia Our work relates to research in the replay of stored multimedia. There are two approaches to the replay of stored media: network-based replay and workstation-based replay. However, there are important differences, explained as follows. Research in network-based continuous media players, such as <ref> [3, 23, 30, 31] </ref>, addresses different sources of overheads. The replay of stored media has three basic sources of overhead: (1) fetch, (2) process, and (3) presentation. In network-based replay, the media server implements media access, buffering and synchronization tasks. Variability is primarily attributed to latencies in the network. <p> Synchronization at internal points among media segments is not addressed. In our work, the focus is on enforcing fine-grain synchronization at internal points among streams. Research in multimedia network interfaces, such as <ref> [13, 18, 26, 27, 31] </ref>, propose adaptive scheduling protocols to manage bounded network variability. Their focus is on the management of overheads up to the delivery of data to the presentation workstation. These approaches assume negligible overheads on the processing and presentation of streams by the client workstation.
Reference: [32] <author> R. Steinmetz. </author> <title> Synchronization properties in multimedia systems. </title> <journal> IEEE Journal of Selected Areas of Communication, </journal> <volume> 8(3) </volume> <pages> 401-411, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: First, no variability is introduced by the re-execution of MIDI events on the real-time synthesizer. Second, integrated media replay is not supported. Steinmetz <ref> [32] </ref> provided early illustrations for the need for integration of synchronous and asynchronous media. Some issues in specification and presentation of multimedia documents have been addressed in the Firefly system [6] and in Cmif [5]. <p> Synchronization Operations Under the presence of timing variability, a way to preserve the relative synchronization of streams is needed. Synchronization is based on the use of synchronization events, widely accepted in the synchronization literature <ref> [32] </ref>. A synchronization event, s i (lhs rhs), preserves relative timing between lhs and rhs streams involved in a synchronization dependency. In this notation, rhs streams synchronize to the lhs stream. <p> Inter-stream Synchronization Mechanism The replay of streams must be kept synchronized. Our synchronization model is based on the notion of a master and multiple slave streams, (as in <ref> [4, 15, 32] </ref>). However, unlike these master/slave models, our approach differs in the following ways. First, synchronization is relative to the progress of the master stream (as opposed to the progress of logical time, as in the Tactus system [11]). <p> Variability normally present on the master stream is reduced during replay since the master stream no longer initiates inter-stream synchronization protocols. Overall, we found this scheme to yield better audio continuity than a master-initiated synchronization scheme. Synchronization Protocols A synchronizing operation can be described (as in <ref> [32] </ref>) by: (1) the involved partner (s), (i.e., to which stream to syn chronize). Our prototype implements (A W ). (2) the type of synchronization, (i.e., whether to use a non-blocking, 1-Way or 2-Way blocking protocol). (3) the release mechanism, (whether and how to adapt the scheduling of a stream).
References-found: 32

