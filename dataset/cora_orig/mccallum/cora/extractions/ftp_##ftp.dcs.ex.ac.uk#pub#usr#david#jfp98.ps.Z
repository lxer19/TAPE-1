URL: ftp://ftp.dcs.ex.ac.uk/pub/usr/david/jfp98.ps.Z
Refering-URL: http://www.dcs.ex.ac.uk/~david/research/dynamic.html
Root-URL: http://www.dcs.ex.ac.uk
Title: The Dynamic Compilation of Lazy Functional Programs  
Author: David Wakeling 
Web: (web: http://www.dcs.exeter.ac.uk/~david)  
Address: Exeter, EX4 4PT, UK  
Affiliation: Department of Computer Science, University of Exeter,  
Date: 1 (1): 1-21, January 1993  
Note: J. Functional Programming  c 1993 Cambridge University Press 1  
Abstract: Lazy functional languages seem to be unsuitable for programming embedded computers because their implementations require so much memory for program code and graph. In this paper we describe a new abstract machine for the implementation of lazy functional languages on embedded computers called the X-machine. The X-machine has been designed so that program code can be stored compactly as byte code, yet run quickly using dynamic compilation. Our first results are promising | programs typically require only 33% of the code space of an ordinary implementation, but run at 75% of the speed. Future work needs to concentrate on reducing the size of statically-allocated data and the run-time system, and on developing a more detailed understanding of throw-away compilation.
Abstract-found: 1
Intro-found: 1
Reference: <author> Adl-Tabatabai, A., Langdale, G., Lucco, S., & Wahbe, R. </author> <year> 1996 </year> <month> (May). </month> <title> Efficient and language-independent mobile programs. </title> <booktitle> Pages 127-136 of: Proceedings of the 1996 ACM conference on programming language design and implementation. </booktitle>
Reference: <author> Armstrong, J. L., Virding, R., & Williams, M. </author> <year> (1993). </year> <title> Concurrent programming in erlang. </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: The nearest work that we know of is by Wallace and Runciman in the area of real-time control (Wallace & Runciman, 1995). Ericcson Telecom use a mostly-functional language Erlang <ref> (Armstrong et al., 1993) </ref> for programming telephone switches. Dynamic compilation is an old idea whose popularity waxes and wanes according to the fashion in computer architecture.
Reference: <author> Augustsson, L., & Johnsson, T. </author> <year> (1989). </year> <title> The Chalmers Lazy-ML compiler. </title> <journal> Computer journal, </journal> <volume> 32(2), </volume> <pages> 127-141. </pages>
Reference-contexts: To make programs run fast, many implementations compile the reduction rules into machine code, even though this 2 David Wakeling may produce a lot of code. Suppose, for example, that we are using the popular Chalmers HBC/LML compiler <ref> (Augustsson & Johnsson, 1989) </ref> and an SGI 02 computer with an R5000 processor. At one extreme, consider the 120-line calendar program from (Bird & Wadler, 1988). <p> With this as our goal, we have developed a new abstract machine called the X-machine. It is the result of some experiments with the Chalmers LML/HBC compiler <ref> (Augustsson & Johnsson, 1989) </ref>, and is best described in that context. The Chalmers compiler generates code first for an abstract stack machine called the G-machine, and then for a less-abstract register machine called the M-machine. <p> When the workspace becomes full, it is emptied by throwing the native code for all units away. Thus, the workspace serves as a native code cache with a particularly brutish replacement policy that is cheap to implement. 6 A Test Bed Implementation Modifying the LML/HBC compiler <ref> (Augustsson & Johnsson, 1989) </ref> to implement the ideas above requires little imagination. The static compiler divides the source code into (lambda-lifted) functions and produces X-code for them. The dynamic compiler then produces native code from X-code by simple macro expansion, taking the offsets for jump instructions from the byte code. <p> This work was funded in part by Canon Research Centre Europe, and in part by the University of Exeter Research Fund. Summary of the X-machine Instruction Set make most sense when read in conjunction with the description of the Chalmers HBC/LML compiler <ref> (Augustsson & Johnsson, 1989) </ref>.
Reference: <author> Auslander, J., Philipose, M., Chambers, C., Eggers, S. J., & Bershad, B. N. </author> <year> 1996 </year> <month> (May). </month> <title> Fast, effective dynamic compilation. </title> <booktitle> Pages 149-159 of: Proceedings of the 1996 ACM conference on programming language design and implementation. </booktitle>
Reference-contexts: They reported encouraging results for small programs, but their prototype implementation had no garbage collector, and so they could not say how well it would work for larger, more realistic programs. The paper by Auslander et al. <ref> (Auslander et al., 1996) </ref> deals with the dynamic compilation of general-purpose, imperative, programming languages, such as C. Here, the program must be annotated to indicate regions where dynamic compilation may be worthwhile.
Reference: <author> Bird, R., & Wadler, P. </author> <year> (1988). </year> <title> Introduction to functional programming. </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Suppose, for example, that we are using the popular Chalmers HBC/LML compiler (Augustsson & Johnsson, 1989) and an SGI 02 computer with an R5000 processor. At one extreme, consider the 120-line calendar program from <ref> (Bird & Wadler, 1988) </ref>. This program compiles into 288k bytes of code (of which 34k bytes are for statically-allocated data and 70k bytes are for run-time support); only an additional 30k bytes are needed for the graph. At the other extreme, consider the 25,000-line HBC/LML compiler itself.
Reference: <author> Brown, P. J. </author> <year> (1976). </year> <title> Throw-away compiling. </title> <journal> Software | practice and experience, </journal> <volume> 6, </volume> <pages> 423-434. </pages>
Reference-contexts: This operand representation is the most compact possible for register addressing modes (which we have found are common), and it is also reasonably compact for constants (which we have found are less common). 8 David Wakeling 5.2 Dynamic Compilation X-code is designed to be executed using throw-away compilation <ref> (Brown, 1976) </ref>, one of the oldest forms of dynamic compilation. A throw-away compiler performs both static and dynamic compilation of the program.
Reference: <author> Chambers, C., & Ungar, D. </author> <year> 1989 </year> <month> (June). </month> <title> Customization: Optimizing compiler technology for SELF, a dynamically-typed object-oriented programming language. </title> <booktitle> Pages 146-160 of: Proceedings of the 1989 acm conference on programming language design and implementation. </booktitle>
Reference-contexts: Dynamic compilation has long been used to improve the performance of object-oriented programming languages. Both Deutsch and Schiffman's SmallTalk system (Deutsch & Schiffman, 1984) and Chambers and Ungar's SELF compiler <ref> (Chambers & Ungar, 1989) </ref> generate native code from the byte code of an object method when that method is invoked. The native code is cached and used if the method is invoked again. Should the cache fill up, some native code is flushed, to be regenerated when needed.
Reference: <author> Denning, P. J. </author> <year> (1968). </year> <title> The working set model for program behaviour. </title> <journal> Communications of the ACM, </journal> <volume> 11(5), </volume> <pages> 323-333. </pages>
Reference-contexts: Execution time vs heap size for our compiler. we have so far avoided removing it because the compiler is written in LML and bootstrapping is such an important test. By analogy with Denning's work on virtual memory <ref> (Denning, 1968) </ref>, we suppose that a "working set" of functions exists for most functional programs. We plan to collect data to confirm this supposition, perhaps by separating the storage areas for the code and the graph so that we can study the behaviour of the code alone.
Reference: <author> Deutsch, L. P., & Schiffman, A. M. </author> <year> 1984 </year> <month> (January). </month> <title> Efficient implementation of the Smalltalk-80 system. </title> <booktitle> Pages 297-302 of: Proceedings of the eleventh annual ACM symposium on the principles of programming languages. </booktitle>
Reference-contexts: Here, we concentrate on some of the most recent papers since they provide a good way into the otherwise scattered literature on dynamic compilation. Dynamic compilation has long been used to improve the performance of object-oriented programming languages. Both Deutsch and Schiffman's SmallTalk system <ref> (Deutsch & Schiffman, 1984) </ref> and Chambers and Ungar's SELF compiler (Chambers & Ungar, 1989) generate native code from the byte code of an object method when that method is invoked. The native code is cached and used if the method is invoked again.
Reference: <author> Engler, D. W. </author> <year> 1996 </year> <month> (May). </month> <title> VCODE: A retargetable, extensible, very fast dynamic code generation system. </title> <booktitle> Pages 160-170 of: Proceedings of the 1996 ACM conference on programming language design and implementation. </booktitle>
Reference-contexts: Upto now, portability has not been a concern, but in future as we move towards embedded processors we plan to save work by using something akin to Engler's VCODE <ref> (Engler, 1996) </ref>. 16 David Wakeling 9 Related Work To our knowledge, no one yet programs embedded computers with lazy functional languages. The nearest work that we know of is by Wallace and Runciman in the area of real-time control (Wallace & Runciman, 1995). <p> These translate the byte code for methods into native code as they are called, and then run the native code. SUN claim that this can be upto an order of magnitude faster than interpreting the byte code. Engler's VCODE <ref> (Engler, 1996) </ref> is a general-purpose system for implementing dynamic code generation. The VCODE system consists of a set of C macros and functions for generating machine code. Using them, a VCODE client (for example, a compiler) can construct machine code at run-time assuming the underlying processor is an idealied RISC.
Reference: <author> Ernst, J., Evans, W., Fraser, C. W., Lucco, S., & Proebsting, T. A. </author> <year> 1997 </year> <month> (June). </month> <title> Code compression. </title> <booktitle> Pages 358-365 of: Proceedings of the 1997 ACM conference on programming language design and implementation. </booktitle>
Reference-contexts: This is claimed to be an order of magnitude faster than any other mobile code system. More recent research has concentrated on saving space too by compressing Omniware Virtual Machine code into a byte code that can either be interpreted directly or compiled to native code <ref> (Ernst et al., 1997) </ref>. The Java Virtual Machine (Lindholm & Yellin, 1996) is an abstract stack machine that also runs mobile programs sent across a network. The byte code representation is used to achieve portability and save space.
Reference: <author> Hammond, K., Burn, G. L., & Howe, D. B. </author> <year> (1993). </year> <title> Spiking your caches. </title> <booktitle> Pages 58-68 of: Proceedings of the 1993 Glasgow workshop on functional programming. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The code size. This excludes functions from the standard prelude, which are tedious to pick out, and run-time support code, (where the throw-away compiler | essentially, a large C switch statement | adds 12k bytes to the existing 70k bytes). 2. The execution time. Following Hammond et al. <ref> (Hammond et al., 1993) </ref>, we made measurements with a large number of heap sizes to investigate whether using the heap as a native code cache has any strange effects. All measurements start from a heap size at which the program almost runs out of memory.
Reference: <author> Hennessy, J. L., & Patterson, D. A. </author> <year> (1990). </year> <title> Computer architecture: A quantative approach. Morgan Kaufmann. The Dynamic Compilation of Lazy Functional Programs 21 Hudak, </title> <editor> P., & Jones, M. P. </editor> <year> 1994 </year> <month> (June). </month> <title> Haskell vs. Ada vs. C++ vs Awk vs. </title> . . . . <type> Tech. </type> <institution> rept. Yale University, Department of Computer Science. </institution>
Reference-contexts: Interpretive loops usually contain a small block of code to implement each abstract machine instruction. Unfortunately though, these blocks do not necessarily enjoy good locality of reference. y A machine that uses a small number of bits to encode a program is said to have good instruction density <ref> (Hennessy & Patterson, 1990) </ref> 4 David Wakeling Even older processors now have all these features. An implementation that exploits modern computer architecture better than an interpreter may make it possible to use a processor with a reduced clock rate, or from the previous generation.
Reference: <author> Keppel, D. </author> <year> 1991 </year> <month> (April). </month> <title> A portable interface for on-the-fly instruction space modification. </title> <booktitle> Pages 86-95 of: Proceedings of the 1991 ACM conference on architectural support for programming languages and operating systems. SIGPLAN Notices 26(4) 1991. </booktitle>
Reference-contexts: Happily, many computers provide a cache coherency operation | for example, a system call to flush a range of addresses from the instruction cache | and so this difficulty can be overcome. This issue is explored further in the paper by Kep-pel <ref> (Keppel, 1991) </ref>. 7 Experimental Results To evaluate throw-away compilation, we performed some experiments on an SGI O2 computer. This machine has a 180MHz R5000 processor and 32Mb of main memory. Although the R5000 processor has a 32kb instruction cache and a 32kb data cache, there is no other cache. <p> Dynamic compilation is an old idea whose popularity waxes and wanes according to the fashion in computer architecture. In their 1991 paper, Keppel, Eggers and Henry argued that "new" computer architecture made it worthwhile again <ref> (Keppel et al., 1991) </ref>, and currently (Autumn 1997) there is a considerable amount of research going on this area. Here, we concentrate on some of the most recent papers since they provide a good way into the otherwise scattered literature on dynamic compilation.
Reference: <author> Keppel, D., Eggers, S. J., & Henry, R. R. </author> <year> (1991). </year> <title> A case for runtime code generation. </title> <type> Tech. </type> <institution> rept. 91-11-04. Department of Computer Science and Engineering, University of Washington. </institution>
Reference-contexts: Happily, many computers provide a cache coherency operation | for example, a system call to flush a range of addresses from the instruction cache | and so this difficulty can be overcome. This issue is explored further in the paper by Kep-pel <ref> (Keppel, 1991) </ref>. 7 Experimental Results To evaluate throw-away compilation, we performed some experiments on an SGI O2 computer. This machine has a 180MHz R5000 processor and 32Mb of main memory. Although the R5000 processor has a 32kb instruction cache and a 32kb data cache, there is no other cache. <p> Dynamic compilation is an old idea whose popularity waxes and wanes according to the fashion in computer architecture. In their 1991 paper, Keppel, Eggers and Henry argued that "new" computer architecture made it worthwhile again <ref> (Keppel et al., 1991) </ref>, and currently (Autumn 1997) there is a considerable amount of research going on this area. Here, we concentrate on some of the most recent papers since they provide a good way into the otherwise scattered literature on dynamic compilation.
Reference: <author> Lee, P., & Leone, M. </author> <year> 1996 </year> <month> (May). </month> <title> Optimizing ML with run-time code generation. </title> <booktitle> Pages 137-148 of: Proceedings of the 1996 ACM conference on programming language design and implementation. </booktitle>
Reference-contexts: As part of the Fox Project aimed at improving the design and development of high-performance system software, Lee and Leone have built a run-time code generator for a first-order, purely-functional subset of ML <ref> (Lee & Leone, 1996) </ref>. Here, curried functions take an "early" argument followed by some "late" arguments, the idea being that "early" computations are performed by statically-generated code, and "late" computations by dynamically-generated code. Each curried function is compiled into assembly code, with instructions being annotated as either "early" or "late".
Reference: <author> Leroy, X. </author> <year> 1990 </year> <month> (February). </month> <title> The ZINC experiment: An ecomonical implementation of the ML language. </title> <type> Tech. rept. RT 117. </type> <institution> INRIA. </institution>
Reference-contexts: An instruction is executed by making a call to the opcode. In this case, dispatching on the opcode takes place at compile-time rather than run-time and the interpretive overhead can be halved <ref> (Leroy, 1990) </ref>. <p> Ratio of execution times vs heap size. scribed should not work equally well with a strict functional language such as Standard ML, we need to build an implementation (perhaps based on Leroy's byte code interpreter <ref> (Leroy, 1990) </ref>) to be sure. At the moment, there is no easy way for the programmer to take advantage of profiling information. Although functions compiled with the ordinary compilation and those compiled with the throw-away compilation can be freely mixed, each module must be compiled one way or the other.
Reference: <author> Lindholm, T., & Yellin, F. </author> <year> (1996). </year> <title> The Java virtual machine. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: More recent research has concentrated on saving space too by compressing Omniware Virtual Machine code into a byte code that can either be interpreted directly or compiled to native code (Ernst et al., 1997). The Java Virtual Machine <ref> (Lindholm & Yellin, 1996) </ref> is an abstract stack machine that also runs mobile programs sent across a network. The byte code representation is used to achieve portability and save space.
Reference: <author> Partain, W. </author> <year> (1992). </year> <title> The nofib benchmark suite of Haskell programs. </title> <booktitle> Pages 195-202 of: Proceedings of the 1992 Glasgow workshop on functional programming. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Although the R5000 processor has a 32kb instruction cache and a 32kb data cache, there is no other cache. The machine runs version 6.3 of the IRIX operating system. Six of the larger programs from the "nofib" suite <ref> (Partain, 1992) </ref> were used as benchmarks. These programs were modified in two ways. Firstly, the input was 10 David Wakeling made much larger, usually by repeating it, so that the programs would run for a reasonable length of time.
Reference: <author> Patterson, D. A., & Hennessy, J. L. </author> <year> (1994). </year> <title> Computer organization and design: The hardware software interface. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A certain familiarity with the implementation of functional languages and the basics of computer architecture is assumed throughout this paper. Those without such familiarity may occasionally need to consult the textbooks by Peyton Jones (Peyton Jones, 1987) and Patterson and Hennessy <ref> (Patterson & Hennessy, 1994) </ref>. 2 Interpretation One way to program embedded systems using a functional language is to further cut down an implementation designed for a small personal computer. Such implementations save code space by using either byte code or threaded code interpretation (Rojemo, 1995; Leroy, 1990).
Reference: <editor> Peyton Jones, S. L. </editor> <booktitle> (1987). The implementation of functional programming languages. </booktitle> <publisher> Prentice-Hall. </publisher>
Reference-contexts: A certain familiarity with the implementation of functional languages and the basics of computer architecture is assumed throughout this paper. Those without such familiarity may occasionally need to consult the textbooks by Peyton Jones <ref> (Peyton Jones, 1987) </ref> and Patterson and Hennessy (Patterson & Hennessy, 1994). 2 Interpretation One way to program embedded systems using a functional language is to further cut down an implementation designed for a small personal computer.
Reference: <author> Proebsting, T. A. </author> <year> 1995 </year> <month> (January). </month> <title> Optimizing an ANSI C interpreter with superoperators. </title> <booktitle> Pages 322-332 of: Proceedings of the ACM conference on principles of programming languages. </booktitle>
Reference-contexts: Graph reduction of dup 42. 6 David Wakeling four instructions for dup are of the form CPAIR m; UPDATE n; POP (n 1); RET These could be replaced by CPAIR RET m n This instruction is an example of a superoperator as described in <ref> (Proebsting, 1995) </ref>, although the ones in the Chalmers LML/HBC compiler were derived manually rather than automatically. 4.2 The M-machine The M-machine is an abstract general-purpose register machine.
Reference: <author> Rojemo, N. </author> <year> 1995 </year> <month> (June). </month> <title> Highlights from nhc | a space-efficient Haskell compiler. </title> <booktitle> Pages 282-291 of: Proceedings of the ACM conference on functional programming languages and computer architecture. </booktitle>
Reference-contexts: However, for simple stack operations there is a large interpretive overhead | around 30% of the total execution time can be spent dispatching on the opcode <ref> (Rojemo, 1995) </ref>. 2.2 Threaded Code Interpretation The interpretive overhead can be reduced considerably by using a threaded code interpreter. The opcode byte of each instruction is replaced by the address of the code that implements the instruction. An instruction is executed by making a call to the opcode.
Reference: <author> Turner, D. A. </author> <year> (1979). </year> <title> A new implementation technique for applicative languages. </title> <journal> Software | practice and experience, </journal> <volume> 9(1), </volume> <pages> 31-50. </pages>
Reference-contexts: As a result, programs that generate code dynamically are both portable and efficient (between six and ten instructions per instruction generated). Throw-away compilation was first mentioned in the context of functional programming by Turner <ref> (Turner, 1979) </ref>, and this was where we got the idea. An earlier version of this paper, with the same results for small LML programs, appeared as (Wakeling, 1995). 10 Conclusions In this paper we have described a new abstract machine for the implementation of lazy functional languages on embedded computers.
Reference: <author> Turner, D. A. </author> <year> (1982). </year> <title> Recursion equations as a programming language. Pages 1-28 of: Darlington, </title> <editor> J., Henderson, P., & Turner, D. A. (eds), </editor> <title> Functional programming and its applications. </title> <publisher> Cambridge University Press. </publisher>
Reference: <author> Wakeling, D. </author> <year> (1995). </year> <title> A throw-away compiler for a lazy functional language. Pages 287-300 of: </title> <editor> Takeichi, M., & Ida, T. (eds), </editor> <booktitle> Fuji international workshop on functional and logic programming. </booktitle> <publisher> World Scientific. </publisher>
Reference-contexts: Throw-away compilation was first mentioned in the context of functional programming by Turner (Turner, 1979), and this was where we got the idea. An earlier version of this paper, with the same results for small LML programs, appeared as <ref> (Wakeling, 1995) </ref>. 10 Conclusions In this paper we have described a new abstract machine for the implementation of lazy functional languages on embedded computers. It has been designed so that program code can be stored compactly as byte code, yet run quickly using dy 18 David Wakeling namic compilation.
Reference: <author> Wallace, M., & Runciman, C. </author> <year> 1995 </year> <month> (June). </month> <title> Lambdas in the liftshaft | functional programming and an embedded architecture. </title> <booktitle> Pages 249-258 of: Proceedings of the ACM conference on functional programming langu ages and computer architecture. </booktitle>
Reference-contexts: The nearest work that we know of is by Wallace and Runciman in the area of real-time control <ref> (Wallace & Runciman, 1995) </ref>. Ericcson Telecom use a mostly-functional language Erlang (Armstrong et al., 1993) for programming telephone switches. Dynamic compilation is an old idea whose popularity waxes and wanes according to the fashion in computer architecture.
References-found: 27

