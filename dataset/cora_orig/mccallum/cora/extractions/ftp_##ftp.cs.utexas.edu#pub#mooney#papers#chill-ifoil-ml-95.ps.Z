URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/chill-ifoil-ml-95.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: (zelle,cthomp,mecaliff,mooney)@cs.utexas.edu  
Phone: (512) 471-9589, 471-9558  
Title: on Inductive Logic Programming (ILP-95) Inducing Logic Programs without Explicit Negative Examples  
Author: John M. Zelle Cynthia A. Thompson Mary Elaine Califf Raymond J. Mooney 
Date: February 6, 1995  
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas  
Note: Appears in Proceedings of the Fifth International Workshop  
Abstract: This paper presents a method for learning logic programs without explicit negative examples by exploiting an assumption of output completeness. A mode declaration is supplied for the target predicate and each training input is assumed to be accompanied by all of its legal outputs. Any other outputs generated by an incomplete program implicitly represent negative examples; however, large numbers of ground negative examples never need to be generated. This method has been incorporated into two ILP systems, Chillin and IFoil, both of which use intensional background knowledge. Tests on two natural language acquisition tasks, case-role mapping and past-tense learning, illustrate the advantages of the approach. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bergadano, F., Gunetti, D., & Trinchero, U. </author> <year> (1993). </year> <title> The difficulties of learning logic programs with cut. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 91-107. </pages>
Reference: <author> Cameron-Jones, R. M., & Quinlan, J. R. </author> <year> (1994). </year> <title> Efficient top-down induction of logic programs. </title> <journal> SIGART Bulletin, </journal> <volume> 5 (1), </volume> <pages> 33-42. </pages>
Reference: <author> Fillmore, C. J. </author> <year> (1968). </year> <title> The case for case. In Bach, </title> <editor> E., & Harms, R. T. (Eds.), </editor> <booktitle> Universals in Linguistic Theory. </booktitle> <publisher> Holt, Reinhart and Winston, </publisher> <address> New York. </address>
Reference-contexts: This predicate invention mechanism may be added to Chillin in the future. 4.3 Experiments with Case-role Mapping Case-role mapping is a typical problem in natural language processing. Traditional case theory <ref> (Fillmore, 1968) </ref> decomposes a sentence into a proposition represented by the main verb and various arguments such as agent, patient, and instrument, represented by noun phrases. The basic mapping problem is to decide which sentence constituents fill which roles.
Reference: <author> Kijsirikul, B., Numao, M., & Shimura, M. </author> <year> (1992). </year> <title> Discrimination-based constructive induction of logic programs. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 44-49 San Jose, CA. </address>
Reference-contexts: Unlike Foil, however, Chillin also performs demand-driven predicate invention. The original Chillin algorithm performs invention in a manner analogous to Champ <ref> (Kijsirikul, Numao, & Shimura, 1992) </ref>. The first step is to find a subset of variables appearing in the clause whose instantiations are sufficient to discriminate covered positive examples from covered negatives.
Reference: <editor> Lavrac, N., & Dzeroski, S. (Eds.). </editor> <year> (1994). </year> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood. </publisher>
Reference: <author> Ling, C. X. </author> <year> (1994). </year> <title> Learning the past tense of English verbs: The symbolic pattern associator vs. connectionist models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 209-229. </pages>
Reference: <author> Ling, X., & Marinov, M. </author> <year> (1993). </year> <title> Answering the connectionist challenge: A symbolic model of learning the past tense of English verbs. </title> <journal> Cognition, </journal> <volume> 49 (3), </volume> <pages> 235-290. </pages>
Reference: <author> MacWhinney, B., & Leinbach, J. </author> <year> (1991). </year> <title> Implementations are not conceptualizations: Revising the verb model. </title> <journal> Cognition, </journal> <volume> 40, </volume> <pages> 291-296. </pages>
Reference: <author> McClelland, J. L., & Kawamoto, A. H. </author> <year> (1986). </year> <title> Mechanisms of sentence processing: Assigning roles to constituents of sentences. </title> <editor> In Rumelhart, D. E., & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <note> 14 Mooney, </note> <author> R. J., & Califf, M. E. </author> <year> (1995). </year> <title> Induction of first-order decision lists: Results on learning the past tense of English verbs. </title> <journal> Journal of Artificial Intelligence Research, </journal> ?? <note> submitted. </note>
Reference-contexts: Thus, the desired mode is parse (+,), effectively producing a case-role parser. The experiments were conducted using a set of 1475 sentence/case-structure pairs originally from <ref> (McClelland & Kawamoto, 1986) </ref> The corpus was produced from a set of 19 sentence templates generating sentences such as, the HUMAN ate the FOOD with the UTENSIL, where the capitalized items were replaced by words of the appropriate category.
Reference: <author> Muggleton, S., & Buntine, W. </author> <year> (1988). </year> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 339-352 Ann Arbor, MI. </address>
Reference-contexts: G := Clause in GENS yielding most compaction DEF := (DEF (Clauses subsumed by G)) [ G Until no further compaction 8 Chillin starts with a most specific definition (the set of positive exam-ples) and introduces generalizations which make the definition more compact (as measured by a Cigol-like size metric <ref> (Muggleton & Buntine, 1988) </ref>). The search for more general definitions is carried out in a hill-climbing fashion. At each step, a number of possible generalizations are considered; the one producing the greatest compaction of the theory is implemented, and the process repeats.
Reference: <author> Muggleton, S. H. (Ed.). </author> <year> (1992). </year> <title> Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> Plotkin, G. D. </author> <year> (1970). </year> <title> A note on inductive generalization. </title> <editor> In Meltzer, B., & Michie, D. (Eds.), </editor> <booktitle> Machine Intelligence (Vol. </booktitle> <volume> 5). </volume> <publisher> Elsevier North-Holland, </publisher> <address> New York. </address>
Reference-contexts: The build gen algorithm attempts to construct a clause which empirically subsumes some clauses of DEF without covering any of the negative examples. The first step is to construct the least general generalization (LGG) <ref> (Plotkin, 1970) </ref> of the input clauses. If the LGG does not cover any negative examples, no further refinement is necessary. If the clause is too general, an attempt is made to refine it using a Foil-like mechanism which adds literals derivable either from background or previously invented predicates.
Reference: <author> Quinlan, J. R. </author> <year> (1994). </year> <title> Past tenses of verbs and first-order learning. </title> <editor> In Zhang, C., Debenham, J., & Lukose, D. (Eds.), </editor> <booktitle> Proceedings of the Seventh Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 13-20 Singapore. </address> <publisher> World Scientific. </publisher>
Reference: <author> Quinlan, J. R., & Cameron-Jones, R. M. </author> <year> (1993). </year> <title> FOIL: A midterm report. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pp. 3-20 Vienna. </pages>
Reference: <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 (3), </volume> <pages> 239-266. </pages>
Reference-contexts: It also describes how this approach has been implemented in two ILP systems: Chillin (Zelle & Mooney, 1994), a system that combines top-down and bottom-up methods and includes a method for inventing predicates, and IFoil, a version of Foil <ref> (Quinlan, 1990) </ref> that employs intensional background knowledge. 1 Finally, we present results demonstrating the advantage of this approach using two problems in natural language acquisition.
Reference: <author> Rumelhart, D. E., & McClelland, J. </author> <year> (1986). </year> <title> On learning the past tense of English verbs. </title> <editor> In Rumelhart, D. E., & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 216-271. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Stahl, I., Tausend, B., & Wirth, R. </author> <year> (1993). </year> <title> Two methods for improving inductive logic programming systems. </title> <booktitle> In Machine Learning: ECML-93, </booktitle> <pages> pp. 41-55 Vienna. </pages>
Reference-contexts: Also, they do not provide a way of quantifying implicit negative coverage. Indico <ref> (Stahl, Tausend, & Wirth, 1993) </ref> employs a related method for learning from positive examples only; however, it is only applicable to func 12 tional predicates.
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1993). </year> <title> Learning semantic grammars with constructive inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 817-822 Washing-ton, D.C. </address> <note> 15 Zelle, </note> <author> J. M., & Mooney, R. J. </author> <year> (1994). </year> <title> Combining top-down and bottom-up methods in inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning New Brunswick, </booktitle> <address> NJ. </address> <month> 16 </month>
References-found: 18

