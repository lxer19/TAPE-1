URL: ftp://ftp.sics.se/pub/SICS-reports/Reports/SICS-R--90-17--SE.ps.Z
Refering-URL: http://www.sics.se/libindex.html
Root-URL: 
Title: MOVING THE SHARED MEMORY CLOSER TO THE PROCESSORS DDM  
Author: Erik Hagersten, Anders Landin and Seif Haridi 
Keyword: buses, increasing the bandwidth. Keywords: Multiprocessor, hierarchical architecture, hierarchical buses, mul tilevel cache, shared memory, split-transaction bus, cache coherence.  
Address: Box 1263 164 28 KISTA SWEDEN.  
Affiliation: Swedish Institute of Computer Science;  
Note: The architecture is scalable in that there can be any number of levels in the hierarchy, and that the root bus of the hierarchy can be implemented by several  
Pubnum: SICS Research Report R90:17B  
Email: Email: fhag,landin,seifg@sics.se  
Date: May 1991.  
Abstract: Multiprocessors with shared memory are considered more general and easier to program than message-passing machines. The scalability is, however, in favor of the latter. There are a number of proposals showing how the poor scalability of shared memory multiprocessors can be improved by the introduction of private caches attached to the processors. These caches are kept consistent with each other by cache-coherence protocols. In this paper we introduce a new class of architectures called Cache Only Memory Architectures (COMA). These architectures provide the programming paradigm of the shared-memory architectures, but are believed to be more scalable. COMAs have no physically shared memory; instead, the caches attached to the processors contain all the memory in the system, and their size is therefore large. A datum is allowed to be in any or many of the caches, and will automatically be moved to where it is needed by a cache-coherence protocol, which also ensures that the last copy of a datum is never lost. The location of a datum in the machine is completely decoupled from its address. We also introduce one example of COMA: the Data Diffusion Machine (DDM). The DDM is based on a hierarchical network structure, with processor/memory pairs at its tips. Remote accesses generally cause only a limited amount of traffic over a limited part of the machine. 
Abstract-found: 1
Intro-found: 1
Reference: [AK90] <author> K. A. M. Ali and R. Karlsson. </author> <title> The MUSE OR-parallel Prolog model and its performance. </title> <booktitle> In North American Conference on Logic Programming. </booktitle> <publisher> MIT Press, </publisher> <month> October </month> <year> 1990. </year>
Reference-contexts: The translation to item identifiers (physical addresses) takes place in the modeled MMUs, performing all their necessary memory operations in the DDM. The execution-driven simulator runs at about 50 000 CPU-cycles per second on a SUN SPARC station. In this study, we have used the OR-parallel Prolog system MUSE <ref> [AK90] </ref> optimized for Sequent Symmetry. It can be viewed as a large parallel application written in C, taking Prolog programs as inputs. The execution of parallel Prolog tends to be very irregular compared to traditional computing, having replaced regular structures like arrays with irregular structures like stacks.
Reference: [EK89] <author> S.J. Eggers and R.H. Katz. </author> <title> Evaluating the performance of four snooping cache coherency protocols. </title> <booktitle> In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-15, </pages> <year> 1989. </year> <month> 19 </month>
Reference-contexts: The small item size in combination with the large "cache" size gives it an advantage over the write-broadcast approach, where, on a write, the new value is broadcast to all "caches" with a shared copy of the item <ref> [EK89] </ref>. The protocol also handles the attraction of data (read) and replacement when a set in an attraction memory gets full.
Reference: [GW88] <author> J.R. Goodman and P.J. Woest. </author> <title> The Wisconsin Multicube: a new large--scale cache-coherent multiprocessor. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, Honolulu, Hawaii, </booktitle> <pages> pages 442-431, </pages> <year> 1988. </year>
Reference-contexts: limited to 6 percent in the access time between the processor caches and the attraction memory, and a memory overhead of 7-17 percent for 32-256 processors. 12 RELATED WORK The DDM has many similarities to Wilson's proposal [Wil86] for a hierarchical shared-memory architecture and certain similarities to the Wisconsin Multicube <ref> [GW88] </ref> and the TREEB architecture [VJS88]. However, all of these machines, unlike the DDM, depend on physically shared memory providing a "home" location for data. The Wis-consin Multicube can also be contrasted with the DDM in that certain requests need to be broadcast throughout the entire machine.
Reference: [HHW90] <author> E. Hagersten, S. Haridi, and D.H.D. Warren. </author> <title> The cache-coherence protocol of the data diffusion machine. </title> <editor> In M. Dubois and S. Thakkar, editors, </editor> <title> Cache and Interconnect Architectures in Multiprocessors. </title> <publisher> Kluwer Academic Publisher, Norwell, </publisher> <address> Mass, </address> <year> 1990. </year>
Reference-contexts: A shared item occupies space in many attraction memories, but only one space in the directories above them. Directory replacement is implemented by an extension to the existing protocol, which requires one extra state and two extra transactions <ref> [HHW90] </ref>. 4.5 Other Protocols The described protocol provides a sequentially consistent [LHH91] system to the programmer. While fulfilling the strongest programming model, performance is degraded by waiting for the acknowledge before the write can be performed.
Reference: [HLH91] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> Multiprocessor consistency and synchronization through transient cache states. </title> <editor> In M. Dubois and S. Thakkar, editors, </editor> <title> Scalable Shared-Memory Multiprocessors. </title> <publisher> Kluwer Academic Publisher, Norwell, </publisher> <address> Mass, </address> <year> 1991. </year>
Reference-contexts: A newly written, but not yet acknowledged, item is marked "write pending", while the processor continues its execution. Its new value will not be revealed before the acknowledge is received. This protocol, called fast write <ref> [HLH91] </ref>, results in a better performance, while supporting the looser programming model processor consistency. 3 Other proposals for architectures allowing multiple outstanding writes can only support models of even looser consistency. 5 INCREASING THE BANDWIDTH The system described so far has two apparent bottlenecks: * The size of the directories grows
Reference: [HomBC] <author> Homer. </author> <title> Odyssey. </title> <type> 800 BC. </type>
Reference-contexts: The new states, R and A in the directories, mark the request's path through the hierarchy, shown in Figure 7, like rolling out a red thread when walking 9 in a maze <ref> [HomBC] </ref>. When the request finally reaches an attraction memory with a copy of the item, its data reply simply follows the red thread back to the requesting node, changing all the states along the path to shared (S).
Reference: [LHH91] <author> A. Landin, E. Hagersten, and S. Haridi. </author> <title> Race-free interconnection networks and multiprocessor consistency. </title> <booktitle> In To appear in Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <year> 1991. </year>
Reference-contexts: A shared item occupies space in many attraction memories, but only one space in the directories above them. Directory replacement is implemented by an extension to the existing protocol, which requires one extra state and two extra transactions [HHW90]. 4.5 Other Protocols The described protocol provides a sequentially consistent <ref> [LHH91] </ref> system to the programmer. While fulfilling the strongest programming model, performance is degraded by waiting for the acknowledge before the write can be performed. <p> Transactions on the respective buses can be prevented from overtaking each other, which leaves us with a network that is said to be race-free. This can be achieved by selectively restricting the transactions allowed to be transferred in parallel <ref> [LHH91] </ref>. One example of where the splitting can be used is a ring-based bus time-slotted into different address domains.
Reference: [MBLZ89] <author> H. E. Mizrahi, J-L Baer, D.E. Lazowska, and J. Zahorjan. </author> <title> Introducing memory into the switch elements of multiprocessor interconnection networks. </title> <booktitle> In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 158-176, </pages> <year> 1989. </year>
Reference-contexts: The Wis-consin Multicube can also be contrasted with the DDM in that certain requests need to be broadcast throughout the entire machine. Data moving closer to the processor accessing it can be found in the architecture of Mizrahi <ref> [MBLZ89] </ref>. The memory overhead of that architecture is much bigger than in the DDM, due to a low branch factor and full inclusion.
Reference: [Ste90] <author> P. Stenstrom. </author> <title> A survey of cache coherence for multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6), </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: Their poor scalability can benefit from large caches local to the processors. The contents of the caches are kept coherent by a cache-coherence protocol. Each cache snoops the traffic on the common bus and prevents any inconsistencies from occurring <ref> [Ste90] </ref>. Message-passing architectures also have distributed memory, but lack the common shared address space. Instead, each processor has its own address space. Examples of such architectures are the Hypercube architectures by Intel and architectures based on the Transputer by Inmos. <p> Snooping-cache protocols have a distributed implementation. Each cache is responsible for snooping traffic on the bus and taking necessary actions if an incoherence is about to occur. 3 An example of such a protocol is the write-once protocol introduced by Goodman and reviewed by Stenstrom <ref> [Ste90] </ref>. In that protocol, shown in Figure 2, each cache line can be in one of the four states INVALID, VALID, RESERVED, or DIRTY. Many caches might have the same cache line in the state VALID at the same time, and may read it locally. <p> Snooping caches, as described above, rely on broadcasting and are not suited for general interconnection networks: broadcasting would reduce the bandwidth available to that of a single bus. Instead, directory-based schemes send messages directly between nodes <ref> [Ste90] </ref>. A read request is sent to main memory, without any snooping. The main memory knows if the cache line is cached, in which cache or caches, and whether or not it has been modified.
Reference: [VJS88] <author> M.K. Vernon, R Jog, and G.S. Sohi. </author> <title> Performance analysis of hierarchical cache-consistent multiprocessors. </title> <booktitle> In Conference Proceedings of International Seminar on Performance of Distributed and Parallel Systems, </booktitle> <pages> pages 111 - 126, </pages> <year> 1988. </year>
Reference-contexts: Snooping in the big directories makes the top bus slower rather than faster. A way of taking the load off the higher buses is to have a smaller branch factor at the top of the hierarchy than lower down <ref> [VJS88] </ref>. This solution, however, makes the higher directories bigger rather than smaller. Instead, both problems are solved with the same solution: splitting the higher buses as shown in Figure 9. The directory is split into two directories half the size. The directories deal with different address domains (even and odd). <p> the access time between the processor caches and the attraction memory, and a memory overhead of 7-17 percent for 32-256 processors. 12 RELATED WORK The DDM has many similarities to Wilson's proposal [Wil86] for a hierarchical shared-memory architecture and certain similarities to the Wisconsin Multicube [GW88] and the TREEB architecture <ref> [VJS88] </ref>. However, all of these machines, unlike the DDM, depend on physically shared memory providing a "home" location for data. The Wis-consin Multicube can also be contrasted with the DDM in that certain requests need to be broadcast throughout the entire machine.
Reference: [WH88] <author> D. H. D. Warren and S. Haridi. </author> <title> Data Diffusion Machine-a scalable shared virtual memory multiprocessor. </title> <booktitle> In International Conference on Fifth Generation Computer Systems 1988. </booktitle> <publisher> ICOT, </publisher> <year> 1988. </year>
Reference: [Wil86] <author> A. Wilson. </author> <title> Hierarchical cache/bus architecture for shared memory multiprocessor. </title> <type> Technical report ETR 86-006, </type> <institution> Encore Computer Corporation, </institution> <year> 1986. </year> <month> 20 </month>
Reference-contexts: The overhead of COMA explored in our hardware prototype is limited to 6 percent in the access time between the processor caches and the attraction memory, and a memory overhead of 7-17 percent for 32-256 processors. 12 RELATED WORK The DDM has many similarities to Wilson's proposal <ref> [Wil86] </ref> for a hierarchical shared-memory architecture and certain similarities to the Wisconsin Multicube [GW88] and the TREEB architecture [VJS88]. However, all of these machines, unlike the DDM, depend on physically shared memory providing a "home" location for data.
References-found: 12

