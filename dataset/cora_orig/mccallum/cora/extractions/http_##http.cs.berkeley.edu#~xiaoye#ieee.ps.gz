URL: http://http.cs.berkeley.edu/~xiaoye/ieee.ps.gz
Refering-URL: http://http.cs.berkeley.edu/~xiaoye/
Root-URL: http://www.cs.berkeley.edu
Title: Faster Numerical Algorithms via Exception Handling  
Author: James W. Demmel Xiaoye Li 
Note: This is an expanded version of a paper published in Proceedings of the 11th Symposium on Computer Arithmetic, Windsor, Ontario, June 29-July 2 1993, pp. 234-241, copyright c fl1993, IEEE.  
Date: July 2, 1994  
Abstract: An attractive paradigm for building fast numerical algorithms is the following: (1) try a fast but occasionally unstable algorithm, (2) test the accuracy of the computed answer, and (3) recompute the answer slowly and accurately in the unlikely event it is necessary. This is especially attractive on parallel machines where the fastest algorithms may be less stable than the best serial algorithms. Since unstable algorithms can overflow or cause other exceptions, exception handling is needed to implement this paradigm safely. To implement it efficiently, exception handling cannot be too slow. We y Computer Science Division and Mathematics Department, University of California, Berkeley CA 94720. Email: demmel@cs.berkeley.edu. The author was supported by NSF grant ASC-9005933, DARPA contract DAAL03-91-C-0047 via a subcontract from the University of Tennessee (administered by ARO), and DARPA grant DM28E04120 via a subcontract from Argonne National Laboratory. z Computer Science Division, University of California, Berkeley CA 94720. Email: xiaoye@cs.berkeley.edu. The author was supported by the National Science Foundation under award number ASC-9005933, and by Subcontract ORA4466.02 to the University of Tennessee (Defense Advanced Research Projects Administration contract number DAAL03-91-C-0047). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson. </author> <title> Robust triangular solves for use in condition estimation. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-91-142, </type> <institution> University of Tennessee, Knoxville, </institution> <year> 1991. </year> <note> (LAPACK Working Note #36). 28 </note>
Reference-contexts: The first one is the simpler and faster of the two, and disregards the possibility of over/underflow. The second scales carefully to avoid over/underflow, and is the one currently used in LAPACK for condition estimation and eigenvector computation <ref> [1] </ref>. We will solve Lx = b, where L is a lower triangular n-by-n matrix. We use the notation L (i : j; k : l) to indicate the submatrix of L lying in rows i through j and columns k through 8 l of L. <p> Similarly, let L n (c) be the analogous n-by-n matrix with 0 &lt; c &lt; 1 in the second through n 1-st elements along the main diagonal. This means that (L n (c)) 1 <ref> [1; 0; :::; 0] </ref> T = [1; c 1 ; c 2 ; :::; c 2n ; c 2n ] T . The second algorithm scales carefully to avoid overflow in Algorithm 1. <p> If some L (i; i) = 0 exactly, so 9 that L is singular, the algorithm will set s = 0 and compute a nonzero vector x satisfying Lx = 0 instead. Here is a brief outline of the scaling algorithm; see <ref> [1] </ref> for details. Coarse bounds on the solution size are computed as follows. <p> It is 300 lines long excluding comments. The Fortran implementation of the BLAS routine STRSV, which handles the same input options, is 159 lines long, excluding comments. For more details on SLATRS, see <ref> [1] </ref>. 11 Algorithm 2: Solve a lower triangular system Lx = sb with scale factor 0 s 1.
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK Users' Guide, Release 1.0. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year> <pages> 235 pages. </pages>
Reference-contexts: In this paper we will assume the first response to exceptions is available; this corresponds to the default behavior of IEEE standard floating point arithmetic [3, 4]. Our numerical methods will be drawn from the LAPACK library of numerical linear algebra routines for high performance computers <ref> [2] </ref>. In particular, we will consider condition 2 estimation (error bounding) for linear systems, computing eigenvectors of general complex matrices, the symmetric tridiagonal eigenvalue problem, and the singular value decomposition. <p> This has been done systematically in LAPACK for most of numerical linear algebra, leading to significant speedups on highly pipelined and parallel machines <ref> [2] </ref>. However, the linear systems arising in condition estimation and eigenvector computation are often ill-conditioned, which means that over/underflow is not completely unlikely. <p> Since jjAjj 1 is easy to compute, we focus on estimating jjA 1 jj 1 . The pivot growth may be defined as jjUjj 1 jjAjj 1 (other definitions are possible). This is close to unity except for pathological cases. In the LAPACK library <ref> [2] </ref>, a set of routines have been developed to estimate the reciprocal of the condition number k 1 (A). We estimate the reciprocal of k 1 (A), which we call RCOND, to avoid overflow in k 1 (A). <p> The details of the use of the scale factor s returned by Algorithm 2 are not shown; see routines SGECON and SLACON in LAPACK <ref> [2] </ref>. Our goal is to avoid the slower Algorithm 2 by using exception handling to deal with these ill-conditioned or badly scaled matrices. Our algorithm only calls the BLAS routine STRSV, and has the property that overflow occurs only if the matrix is extremely ill-conditioned. <p> The division involved in the recurrence for t may cause division by zero or overflow. Again, to prevent the occurrence of the exceptions, a more careful scheme was first developed by W. Kahan [16] and later used in LAPACK SSTEBZ routine <ref> [2] </ref>. There, the algorithm first computes a threshold pivmin, which is the smallest number that can divide b 2 i without overflow. Inside the inner loop the divisor t is compared with pivmin and changed to pivmin if it is too close to zero. <p> Phase 2 takes longer than Phase 1 up to n 1200. So in this section we will discuss using exception handling to accelerate Phase 2. Phase 2 is implemented by a slight modification of LAPACK subroutine SBDSQR <ref> [2] </ref>, which we describe below. It suffices to consider one of the main loops in SBDSQR; the others are similar. In addition to 12 multiplies and 4 addition, there are two uses of an operation we will call rot (f; g; cs; sn; r).
Reference: [3] <author> ANSI/IEEE, </author> <title> New York. IEEE Standard for Binary Floating Point Arithmetic, </title> <address> Std 754-1985 edition, </address> <year> 1985. </year>
Reference-contexts: In this paper we will assume the first response to exceptions is available; this corresponds to the default behavior of IEEE standard floating point arithmetic <ref> [3, 4] </ref>. Our numerical methods will be drawn from the LAPACK library of numerical linear algebra routines for high performance computers [2].
Reference: [4] <author> ANSI/IEEE, </author> <title> New York. IEEE Standard for Radix Independent Floating Point Arithmetic, </title> <address> Std 854-1987 edition, </address> <year> 1987. </year>
Reference-contexts: In this paper we will assume the first response to exceptions is available; this corresponds to the default behavior of IEEE standard floating point arithmetic <ref> [3, 4] </ref>. Our numerical methods will be drawn from the LAPACK library of numerical linear algebra routines for high performance computers [2].
Reference: [5] <author> J. Demmel. </author> <title> Underflow and the reliability of numerical software. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 5(4) </volume> <pages> 887-919, </pages> <month> Dec </month> <year> 1984. </year>
Reference-contexts: In complex division, gradual underflow instead of flush to zero can guarantee a more 27 accurate result, see <ref> [5] </ref>. This requires fast arithmetic with denormalized numbers. Floating point parallel prefix is a useful operation for various linear algebra problems. Its robust implementation with the protection against over/underflow requires fine grained detection and handling of exceptions [6].
Reference: [6] <author> J. Demmel. </author> <title> Specifications for robust parallel prefix operations. </title> <type> Technical report, </type> <institution> Thinking Machines Corp., </institution> <year> 1992. </year>
Reference-contexts: The nonsticky exception bits might be used in other applications requiring finer grained exception handling, such as parallel prefix <ref> [6] </ref>. In the algorithms developed in this paper we need only manipulate the trap enable bits (set them to zero to disable software traps) and the sticky bits. Procedure exceptionreset () clears the sticky flags associated with overflow, division by zero and invalid operations, and suppresses the exceptions accordingly. <p> This requires fast arithmetic with denormalized numbers. Floating point parallel prefix is a useful operation for various linear algebra problems. Its robust implementation with the protection against over/underflow requires fine grained detection and handling of exceptions <ref> [6] </ref>. Our final comment concerns the tradeoff between the speed of NaN and infinity arithmetic and the granularity of testing for exceptions. Our current approach uses a very large granularity, since we test for exceptions only after a complete call to STRSV.
Reference: [7] <author> J. Demmel and X. Li. </author> <title> Faster numerical algorithms via exception handling. </title> <editor> In M. J. Irwin E. Swartzlander and G. Jullien, editors, </editor> <booktitle> Proceedings of the 11th Symposium on Computer Arithmetic, </booktitle> <address> Windsor, Ontario, </address> <month> June 29 - July 2 </month> <year> 1993. </year> <note> IEEE Computer Society Press. available as all.ps.Z via anonymous ftp from tr-ftp.cs.berkeley.edu, in directory pub/tech-reports/cs/csd-93-728; software is csd-93-728.shar.Z. </note>
Reference-contexts: Table 4 gives the speeds for both DECstations. The slow DEC 5000 goes 18 3 The test matrices together with the software can be obtained via anonymous ftp <ref> [7] </ref>. 19 Example 1 Example 2 Example 3 "fast" DEC 5000 speedup 2.15 2.32 2.00 "slow" DEC 5000 slowdown 11.67 13.49 9.00 SPARCstation 10 speedup 3.12 3.92 3.24 Table 4: The speeds of some examples with exceptions. Matrix dimensions are 500. to 30 times slower than the fast DEC 5000. <p> This would require re-implementing STRSV. This medium grained approach is less sensitive to the speed of NaN and infinity arithmetic. The effect of granularity on performance is worth exploration. The software described in this report is available from the authors <ref> [7] </ref>. 10 Acknowledgements The authors wish to thank W. Kahan for his detailed criticism and comments. We also wish to thank Inderjit Dhillon for providing us the performance results of the bisection algorithms running on the CM-5.
Reference: [8] <author> I. S. Dhillon and J. W. Demmel. </author> <title> A parallel algorithm for the symmetric tridiagonal eigenproblem and its implementation on the CM-5. </title> <booktitle> In progress, </booktitle> <year> 1993. </year>
Reference: [9] <author> J. Dongarra, J. Du Croz, I. Duff, and S. Hammarling. </author> <title> A set of Level 3 Basic Linear Algebra Subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: What the first two algorithms have in common is the need to solve triangular systems of linear equations which are possibly very ill-conditioned. Triangular system solving is one of the matrix operations found in the Basic Linear Algebra Subroutines, or BLAS <ref> [9, 10, 18] </ref>. The BLAS, which include related operations like dot product, matrix-vector multiplication, and matrix-matrix multiplication, occur frequently in scientific computing. This has led to their standardization and widespread implementation. <p> Algorithm 1: Solve a lower triangular system Lx = b. x (1 : n) = b (1 : n) x (i) = x (i)=L (i; i) endfor This is such a common operation that it has been standardized as subroutine STRSV, one of the BLAS <ref> [9, 10, 18] </ref>. Algorithm 1 can easily overflow even when the matrix L is well-scaled, i.e. all rows and columns are of equal and moderate length.
Reference: [10] <author> J. Dongarra, J. Du Croz, S. Hammarling, and Richard J. Hanson. </author> <title> An Extended Set of FORTRAN Basic Linear Algebra Subroutines. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: What the first two algorithms have in common is the need to solve triangular systems of linear equations which are possibly very ill-conditioned. Triangular system solving is one of the matrix operations found in the Basic Linear Algebra Subroutines, or BLAS <ref> [9, 10, 18] </ref>. The BLAS, which include related operations like dot product, matrix-vector multiplication, and matrix-matrix multiplication, occur frequently in scientific computing. This has led to their standardization and widespread implementation. <p> Algorithm 1: Solve a lower triangular system Lx = b. x (1 : n) = b (1 : n) x (i) = x (i)=L (i; i) endfor This is such a common operation that it has been standardized as subroutine STRSV, one of the BLAS <ref> [9, 10, 18] </ref>. Algorithm 1 can easily overflow even when the matrix L is well-scaled, i.e. all rows and columns are of equal and moderate length.
Reference: [11] <author> Richard L. </author> <title> Sites (editor). Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <year> 1992. </year> <month> 29 </month>
Reference-contexts: We report performance results on a "fast" DECstation 5000 and a "slow" DECstation 5000 (both have a MIPS R3000 chip as CPU [17]), a Sun 4/260 (which has a SPARC chip as CPU [15]), a DEC Alpha <ref> [11] </ref>, a CRAY-C90 and a SPARCstation 10 with a Viking microprocessor. The "slow" DEC 5000 correctly implements IEEE arithmetic, but does arithmetic with NaNs about 80 times slower than normal arithmetic.
Reference: [12] <author> G. Golub and C. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, MD, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: By "right answer" we mean that the algorithm is stable, or that it computes the exact answer for a problem that is a slight perturbation of its input <ref> [12] </ref>; this is all we can reasonably ask of most algorithms. <p> We will measure the error using either the one-norm jjxjj 1 = P n the infinity norm jjxjj 1 = max i jx i j. Then the usual error bound <ref> [12] </ref> is jjx computed x true jj 1 k 1 (A) p (n) * jjx true jj 1 ; (1) where p (n) is a slowly growing function of n (usually about n), * is the machine precision, k 1 (A) is the condition number of A, and is the pivot <p> A is reduced to upper Hessenberg form H, which is zero below the first subdiagonal. The reduction can be written H = Q fl AQ with Q unitary <ref> [12] </ref>. 2. H is reduced to Schur form T . The reduction can be written T = S fl HS, where T is an upper triangular matrix and S is unitary [12]. The eigenvalues are on the diagonal of T . 3. CTREVC computes the eigenvectors of T . <p> The reduction can be written H = Q fl AQ with Q unitary <ref> [12] </ref>. 2. H is reduced to Schur form T . The reduction can be written T = S fl HS, where T is an upper triangular matrix and S is unitary [12]. The eigenvalues are on the diagonal of T . 3. CTREVC computes the eigenvectors of T . Let V be the matrix whose columns are the 20 right eigenvectors of T . <p> It is known that this number equals the number of eigenvalues less than or equal to <ref> [12] </ref>. Suppose we wish to find the eigenvalues in (a; b] using bisection. First, we evaluate count (a) and count (b). The difference of the two count values is the number of eigenvalues in (a; b]. <p> We see the speedups attained by using IEEE arithmetic ranges from 1.18 to 1.47. 7 Singular Value Decomposition In this section we discuss using exception handling to speed up the computation of the singular value decomposition of a matrix <ref> [12] </ref>. This is an important linear algebra computation, with many applications. It consists of two phases. Phase 1, reduction to bidiagonal form (i.e. nonzero on the diagonal and first superdiagonal only), costs O (n 3 ) operations, where n is 25 the matrix dimension.
Reference: [13] <author> W. W. Hager. </author> <title> Condition estimators. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 5 </volume> <pages> 311-316, </pages> <year> 1984. </year>
Reference-contexts: We estimate the reciprocal of k 1 (A), which we call RCOND, to avoid overflow in k 1 (A). The inputs to these routines include the factors L and U from the factorization A = LU and kAk 1 . Higham's modification [14] of Hager's method <ref> [13] </ref> is used to estimate jjA 1 jj 1 . <p> observation that the maximal value of the function f (x) = jjBxjj 1 =jjxjj 1 equals jjBjj 1 and is attained at one of the vectors e j , for j = 1; ; n, where e j is the jth column of the n-by-n identity matrix. 13 Algorithm 3 <ref> [13] </ref>: This algorithm computes a lower bound fl for jjA 1 jj 1 .
Reference: [14] <author> N. J. Higham. </author> <title> Algorithm 674: FORTRAN codes for estimating the one-norm of a real or complex matrix, with applications to condition estimation. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 14 </volume> <pages> 381-396, </pages> <year> 1988. </year>
Reference-contexts: We estimate the reciprocal of k 1 (A), which we call RCOND, to avoid overflow in k 1 (A). The inputs to these routines include the factors L and U from the factorization A = LU and kAk 1 . Higham's modification <ref> [14] </ref> of Hager's method [13] is used to estimate jjA 1 jj 1 .
Reference: [15] <author> SPARC International Inc. </author> <title> The SPARC Architecture Manual: Version 8. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey 07632, </address> <year> 1992. </year>
Reference-contexts: We report performance results on a "fast" DECstation 5000 and a "slow" DECstation 5000 (both have a MIPS R3000 chip as CPU [17]), a Sun 4/260 (which has a SPARC chip as CPU <ref> [15] </ref>), a DEC Alpha [11], a CRAY-C90 and a SPARCstation 10 with a Viking microprocessor. The "slow" DEC 5000 correctly implements IEEE arithmetic, but does arithmetic with NaNs about 80 times slower than normal arithmetic.
Reference: [16] <author> W. Kahan. </author> <title> Accurate eigenvalues of a symmetric tridiagonal matrix. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS41, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> July </month> <year> 1966 </year> <month> (revised June </month> <year> 1968). </year>
Reference-contexts: The division involved in the recurrence for t may cause division by zero or overflow. Again, to prevent the occurrence of the exceptions, a more careful scheme was first developed by W. Kahan <ref> [16] </ref> and later used in LAPACK SSTEBZ routine [2]. There, the algorithm first computes a threshold pivmin, which is the smallest number that can divide b 2 i without overflow.
Reference: [17] <author> Gerry Kane. </author> <title> MIPS Risc Architecture. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ 07632, </address> <year> 1989. </year>
Reference-contexts: We report performance results on a "fast" DECstation 5000 and a "slow" DECstation 5000 (both have a MIPS R3000 chip as CPU <ref> [17] </ref>), a Sun 4/260 (which has a SPARC chip as CPU [15]), a DEC Alpha [11], a CRAY-C90 and a SPARCstation 10 with a Viking microprocessor. The "slow" DEC 5000 correctly implements IEEE arithmetic, but does arithmetic with NaNs about 80 times slower than normal arithmetic.
Reference: [18] <author> C. Lawson, R. Hanson, D. Kincaid, and F. Krogh. </author> <title> Basic Linear Algebra Subprograms for Fortran usage. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 5 </volume> <pages> 308-323, </pages> <year> 1979. </year>
Reference-contexts: What the first two algorithms have in common is the need to solve triangular systems of linear equations which are possibly very ill-conditioned. Triangular system solving is one of the matrix operations found in the Basic Linear Algebra Subroutines, or BLAS <ref> [9, 10, 18] </ref>. The BLAS, which include related operations like dot product, matrix-vector multiplication, and matrix-matrix multiplication, occur frequently in scientific computing. This has led to their standardization and widespread implementation. <p> Algorithm 1: Solve a lower triangular system Lx = b. x (1 : n) = b (1 : n) x (i) = x (i)=L (i; i) endfor This is such a common operation that it has been standardized as subroutine STRSV, one of the BLAS <ref> [9, 10, 18] </ref>. Algorithm 1 can easily overflow even when the matrix L is well-scaled, i.e. all rows and columns are of equal and moderate length.
Reference: [19] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 Technical Summary, October 1991. </title> <address> Cambridge, Massachsetts. </address> <month> 30 </month>
Reference-contexts: We were able to get speedups ranging from 1.14 to 1.24. This is due to the dominant role of the count () function in the bisection algorithm. We also did comparisons on a distributed memory multiprocessor the Thinking Machines CM-5 <ref> [19] </ref>. Our CM-5 configuration contains 64 33-Mhz SPARC 2 processors, interconnected by a fat-tree network. Each processing node has 8 Mbytes of local memory.
References-found: 19

