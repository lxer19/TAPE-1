URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1996/TR50.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Email: Email:  jianpingg@cis.ohio-state.edu  Email: kimsc@metro.telecom.samsung.co.kr,  
Title: Shiv Kalyanaraman, Performance of TCP over ABR on ATM backbone and with various VBR traffic
Author: Shiv Kalyanaraman, Raj Jain, Sonia Fahmy, Rohit Goyal and Jianping Jiang jain, fahmy, goyal, and Seong-Cheol Kim 
Note: fshivkuma,  
Address: Columbus, OH 43210-1277  8-2, Karak-Dong, Songpa-Ku Seoul, Korea 138-160  
Affiliation: The Ohio State University Department of CIS  Principal Engineer, Network Research Group Communication Systems R&D Center Samsung Electronics Co. Ltd. Chung-Ang Newspaper Bldg.  
Abstract: We extend our earlier studies of buffer requirements of TCP over ABR [10, 11, 12] in two directions. First, we study the performance of TCP over ABR in an ATM backbone. On the backbone, the TCP queues are at the edge router and not inside the ATM network. The router requires buffer equal to the sum of the receiver window sizes of the participating TCP connections. Second, we introduce various patterns of VBR background traffic. The VBR background introduces variance in the ABR capacity and the TCP traffic introduces variance in the ABR demand. Some simple switch schemes are unable to keep up with the combined effect of highly varying demands and highly varying ABR capacity. We present our experiences with refining the ERICA+ switch scheme to handle these conditions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ATM Forum, </author> <title> "ATM Traffic Management Specification Version 4.0," </title> <month> April </month> <year> 1996, </year> <note> available as ftp://ftp.atmforum.com/pub/approved-specs/af-tm-0056.000.ps </note>
Reference-contexts: The ABR and UBR services differ in the way they control data traffic. The ABR service requires network switches to constantly monitor their load and feed the information back to the sources, which in turn dynamically adjust their input into the network <ref> [1, 6] </ref>. The UBR service does not provide any standard traffic management mechanism. However, the switches may monitor their queues and simply discard cells or packets of overloading users. It is interesting to see the performance of Internet (TCP) traffic over the ABR and UBR services.
Reference: [2] <author> V. Jacobson, </author> <title> "Congestion Avoidance and Control," </title> <booktitle> Proceedings of the SIGCOMM'88 Symposium, </booktitle> <pages> pp. 314-32, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The introduction of VBR background traffic makes the ABR capacity variable. The subsequent step was to find a better ABR demand model than the infinite traffic model. TCP traffic provides one such model. TCP provides a reliable transfer of data using a window-based flow and error control algorithm <ref> [2] </ref>. When TCP runs over ABR, the TCP window-based control runs on top of the ABR rate-based control. The TCP traffic appears bursty (variable) at the ATM layer [11]. ABR switch schemes typically use the current demand and capacity to calculate feedback to the sources.
Reference: [3] <author> Chien Fang, Arthur Lin, </author> <title> "On TCP Performance of UBR with EPD and UBR-EPD with a Fair Buffer Allocation Scheme," </title> <address> AF-TM 95-1645, </address> <month> 1 , December </month> <year> 1995. </year>
Reference-contexts: It is interesting to see the performance of Internet (TCP) traffic over the ABR and UBR services. In this paper, we concentrate on TCP performance over ABR. TCP performance over UBR has also been studied in the literature <ref> [3, 4, 13, 5] </ref>. One more reason for the study of TCP traffic over ABR is because TCP offers a unique workload for the performance analysis of ABR switch schemes. <p> These effects reduce as the network path gets completely filled by TCP traffic, and the ABR closed loop control becomes effective. The switch scheme then controls the rate of the sources. Several authors have studied the performance of TCP over ABR and UBR services under lossy conditions <ref> [3, 4, 13, 5] </ref>. We show in [9] that, since TCP has a built in congestion avoidance mechanism, it does not lose too many cells under these conditions. The throughput is low because of the time lost during timeout and retransmission.
Reference: [4] <author> Chien Fang, Arthur Lin, </author> <title> "A Simulation Study of ABR Robustness with Binary-Mode Switches: Part II," </title> <address> AF-TM 95-1328R1, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: It is interesting to see the performance of Internet (TCP) traffic over the ABR and UBR services. In this paper, we concentrate on TCP performance over ABR. TCP performance over UBR has also been studied in the literature <ref> [3, 4, 13, 5] </ref>. One more reason for the study of TCP traffic over ABR is because TCP offers a unique workload for the performance analysis of ABR switch schemes. <p> These effects reduce as the network path gets completely filled by TCP traffic, and the ABR closed loop control becomes effective. The switch scheme then controls the rate of the sources. Several authors have studied the performance of TCP over ABR and UBR services under lossy conditions <ref> [3, 4, 13, 5] </ref>. We show in [9] that, since TCP has a built in congestion avoidance mechanism, it does not lose too many cells under these conditions. The throughput is low because of the time lost during timeout and retransmission.
Reference: [5] <author> Rohit Goyal, Raj Jain, Shiv Kalyanaraman, Sonia Fahmy, Seong-Cheol Kim, "UBR+: </author> <title> Improving Performance of TCP over ATM-UBR Service," </title> <note> submitted to ICC'97, </note> <institution> Montreal, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: It is interesting to see the performance of Internet (TCP) traffic over the ABR and UBR services. In this paper, we concentrate on TCP performance over ABR. TCP performance over UBR has also been studied in the literature <ref> [3, 4, 13, 5] </ref>. One more reason for the study of TCP traffic over ABR is because TCP offers a unique workload for the performance analysis of ABR switch schemes. <p> These effects reduce as the network path gets completely filled by TCP traffic, and the ABR closed loop control becomes effective. The switch scheme then controls the rate of the sources. Several authors have studied the performance of TCP over ABR and UBR services under lossy conditions <ref> [3, 4, 13, 5] </ref>. We show in [9] that, since TCP has a built in congestion avoidance mechanism, it does not lose too many cells under these conditions. The throughput is low because of the time lost during timeout and retransmission. <p> The TCP throughput (110.9 Mbps) is the maximum possible given this configuration, scheme and parameters. The total buffering required for N sources is the sum of the N receiver windows. Note that this is the same as the switch buffer requirement for UBR <ref> [5] </ref>.
Reference: [6] <author> Raj Jain, Shiv Kalyanaraman, Rohit Goyal, Sonia Fahmy, </author> <title> "Source Behavior for ATM ABR Traffic Management: An Explanation," </title> <note> To appear in IEEE Communications Magazine, </note> <month> November </month> <year> 1996. </year> <title> 1 Throughout this section, AF-TM refers to ATM Forum Traffic Management sub-working group contributions. Shiv Kalyanaraman, Performance of TCP over ABR on ATM backbone and with various VBR traffic patterns 12 </title>
Reference-contexts: The ABR and UBR services differ in the way they control data traffic. The ABR service requires network switches to constantly monitor their load and feed the information back to the sources, which in turn dynamically adjust their input into the network <ref> [1, 6] </ref>. The UBR service does not provide any standard traffic management mechanism. However, the switches may monitor their queues and simply discard cells or packets of overloading users. It is interesting to see the performance of Internet (TCP) traffic over the ABR and UBR services.
Reference: [7] <author> Raj Jain, Shiv Kalyanaraman, Rohit Goyal, Sonia Fahmy, and Ram Viswanathan, </author> <title> "ERICA Switch Algorithm: </title>
Reference-contexts: The introduction of VBR traffic makes the ABR capacity variable resulting in more more variance in measurement. We examine the effect of using different VBR background patterns, the feedback delay and the switch scheme used. We present our experiences with refining the ERICA and ERICA+ switch schemes <ref> [7] </ref> to handle these conditions. 3 TCP Options And ERICA Parameters We use a TCP maximum segment size (MSS) of 512 bytes. The MTU size used by IP is generally 9180 bytes and so there is no segmentation caused by IP. <p> In our simulations, we have not used the "fast retransmit and recovery" algorithms. The zero-loss buffer requirement is valid for fast retransmit and recovery too, since these algorithms are not exercised when there is zero-loss. The ERICA algorithm <ref> [7] </ref> uses two key parameters: target utilization and averaging interval length. The algorithm measures the load and number of active sources over successive averaging intervals and tries to achieve a link utilization equal to the target. <p> These values are compared to the maximum receiver window size (indicated as "Win" in the table) which is 1024 kB = 24576 cells. The switch has infinite buffers and uses a modified version of the ERICA algorithm <ref> [7] </ref> including the averaging feature for the number of sources and an averaging interval of (5 ms, 500 cells) as described in Section 5.2.4. The maximum source queue values (third column) are tabulated for every VC, while the maximum switch queue values (fourth column) are for all the VCs together. <p> All link lengths are 1000km. The round trip time is 30 ms and the feedback delay is 10 ms. We use the ERICA+ algorithm <ref> [7] </ref> in our results. The ERICA+ algorithm maximizes the efficiency by allowing 100% utilization in the steady state. It also tries to achieve a target queueing delay. <p> The switch will then allocate a high rate in the feedback it writes to the BRM cell. We have designed two averaging schemes for the overload described in reference <ref> [7] </ref>. Both schemes use an averaging parameter "ff z ". The first scheme is similar to the exponential averaging technique for a random variable. However, it differs because it resets the averaging mechanism whenever the instantaneous value of overload is measured to be zero or infinity.
References-found: 7

