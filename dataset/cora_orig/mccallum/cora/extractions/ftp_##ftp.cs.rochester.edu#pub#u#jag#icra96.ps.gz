URL: ftp://ftp.cs.rochester.edu/pub/u/jag/icra96.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/fuentes/fuentespub.html
Root-URL: 
Email: fjag,fuentes,nelsong@cs.rochester.edu  
Title: Experimental Evaluation of Uncalibrated Visual Servoing for Precision Manipulation  
Author: Martin Jagersand, Olac Fuentes, Randal Nelson 
Web: http://www.cs.rochester.edu/u/fjag,fuentes,nelsong/  
Address: Rochester, Rochester, NY 14627  
Affiliation: Department of Computer Science, University of  
Abstract: In this paper we present an extensive experimental evaluation of adaptive and non-adaptive visual servo-ing in 3, 6 and 12 Degrees of Freedom, comparing it to traditional joint feedback control. While the purpose of experiments in most other work have been to show that the particular algorithm presented indeed also works in practice, we do not focus on the algorithm, but rather on properties important to visual servoing in general. Our main results are: Positioning of a 6 axis PUMA 762 arm is up to 5 times more precise under visual control, than under joint control. Positioning of a Utah/MIT dextrous hand is better under visual control, than under joint control alone. We also found that a trust region based adaptive visual feedback controller is very robust. For m tracked visual features the algorithm can successfully estimate online the m fi 3, m 3 image Jacobian without any prior information, while carrying out a 3 DOF manipulation task. For 6 and higher DOF manipulation, a reasonable initial estimate of J is beneficial. We describe how to bootstrap a high DOF manipulation from a 3 DOF one. We also verified that redundant visual information is valuable. Errors due to imprecise tracking and goal specification were reduced as the number of visual features, m, was increased. Furthermore highly redundant systems allow us to detect outliers in the feature vector, and deal with partial occlusion. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Weiss L. E. Sanderson A. C. Neumann C. P. </author> <title> "Dynamic Sensor-Based Control of Robots with Visual Feedback" J. of Robotics and Aut. </title> <publisher> v. </publisher> <month> RA-3 </month> <year> 1987 </year>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In [13] we show how to derive the robust visual servoing controllers used in this paper. <p> The changes in visual appearance are recorded in a perception or feature vector y = (y 1 : : : y m ) T . Visual features can be drawn from 2 Vectors written bold, scalars plain and matrices capitalized. 1 a large class of visual measurements <ref> [1, 9] </ref>, but we have found that the ones which can be represented as points or point vectors in camera space are suitable, since they yield smooth transfer functions f [10]. We track features such as boundary discontinuities (lines,corners) and surface markings. <p> The disturbed Jacobian is defined as J dist = a fl ^ J + (1 a) fl J random where a 2 <ref> [0; 1] </ref> indicates the model accuracy and J random is a random m fi n matrix with elements drawn from a uniform distribution on the interval [j2 ^ J i;j j; j2 ^ J i;j j]. This restricts the gain, but not the direction (angular transformation) of the random Jacobian.
Reference: [2] <author> Feddema J. T. Lee G. C. S. </author> <title> "Adaptive Image Feature Prediction and Control for Visual Tracking with a Hand-Eye Coordinated Camera" IEEE Tr. </title> <journal> on Systems, Man and Cyber., </journal> <volume> v 20, no 5 1990 </volume>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In [13] we show how to derive the robust visual servoing controllers used in this paper. <p> x k ), valid around the current system configuration x k , and described by the "image"[15] or visual-motor Jacobian defined as (J j;i )(x k ) = @x i The image Jacobian not only relates visual changes to motor changes, as has been previously exploited in visual feedback control <ref> [2] </ref> but highly constrains the possible visual changes to the set of possible solutions y k+1 of y k+1 = J x + y k . <p> Partial modeling of the viewing geometry using an AR-MAX model and estimating only one or a few parameters (e.g. depth) has also been tried <ref> [2, 6] </ref>. However this restricts the camera-robot configurations and environments to structured, easy to model settings. Estimation of the full Jacobian was solved mathematically by Broyden in the 60's [21], and later rediscovered in robotics by [9, 10, 16].
Reference: [3] <author> Conkie A. Chongstitvatana P. </author> <title> "An Uncalibrated Stereo Visual Servo System" DAITR#475, </title> <month> Edin-burgh </month> <year> 1990 </year>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In [13] we show how to derive the robust visual servoing controllers used in this paper. <p> More details can be found in [9]. An estimate to the image Jacobian can be obtained by physically executing a set of calibration movements <ref> [3, 8, 19] </ref> along the basis direction of motor space e i as ^ J (x; d) = (f (x+d 1 e 1 )f (x); : : : ; f (x+d n e n )f (x))D 1 , where D = diag (d); d 2 &lt; n . <p> We need a control system capable of turning these goal perceptions into motor actions x. A simple control law, occuring in some form in most visual servoing research (e.g. <ref> [3, 19, 16] </ref>) is y fl y = KJ _ x where K is a gain matrix.
Reference: [4] <author> Curwen R. Blake A. </author> <title> "Dynamic Contours: Real time active splines" Active VisionBlake, </title> <publisher> Yuille MIT Press 1992. </publisher>
Reference-contexts: The object of the system is to bend or fold the beam into a specified shape. The manipulation is specified by showing the system a sequence of images of the desired manipulation. The Oxford snake trackers <ref> [4] </ref> we usually use rely on an affine constraint. We could not get them to track the foam edges through nonrigid deformations. Instead a point representation of the foam outline is tracked using special purpose trackers on the markers seen in fig. 6 4 .
Reference: [5] <author> Wijesoma S. W. Wolfe D. F. H. Richards R. J. </author> <title> "Eye-to-Hand Coordination for vision guided Robot Control Applications" Int. </title> <journal> J. of Robotics Research, </journal> <volume> v 12 No 1 1993 </volume>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In [13] we show how to derive the robust visual servoing controllers used in this paper. <p> The closest to a general evaluation we have found is: Wi-jesoma et al. <ref> [5] </ref> showed for a 2 DOF implementation, the advantage of visual feedback over open loop control when model errors are large.
Reference: [6] <author> Papanikolopoulos N. P. Khosla P. K. </author> <title> "Adaptive Robotic Visual Tracking: </title> <journal> Theory and Experiments" IEEE Tr. on Aut. </journal> <note> Control Vol 38 no 3 1993 </note>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In [13] we show how to derive the robust visual servoing controllers used in this paper. <p> Partial modeling of the viewing geometry using an AR-MAX model and estimating only one or a few parameters (e.g. depth) has also been tried <ref> [2, 6] </ref>. However this restricts the camera-robot configurations and environments to structured, easy to model settings. Estimation of the full Jacobian was solved mathematically by Broyden in the 60's [21], and later rediscovered in robotics by [9, 10, 16].
Reference: [7] <author> Harris M. </author> <title> "Vision Guided Part Alignment with Degraded Data" DAI TR #615, </title> <address> Edinburgh 1993 </address>
Reference-contexts: Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] 1 . Visual models suitable for specifying simple visual alignments have also been studied <ref> [19, 8, 7] </ref>. In [13] we show how to derive the robust visual servoing controllers used in this paper. In [12, 10, 9] we have shown how different visual servoing behaviors can be combined to solve complex real-world tasks in unstructured environments.
Reference: [8] <author> Hollinghurst N. Cipolla R. </author> <title> "Uncalibrated Stereo Hand-Eye Coordination" Brit. </title> <booktitle> Machine Vision Conf 1993 </booktitle>
Reference-contexts: Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] 1 . Visual models suitable for specifying simple visual alignments have also been studied <ref> [19, 8, 7] </ref>. In [13] we show how to derive the robust visual servoing controllers used in this paper. In [12, 10, 9] we have shown how different visual servoing behaviors can be combined to solve complex real-world tasks in unstructured environments. <p> More details can be found in [9]. An estimate to the image Jacobian can be obtained by physically executing a set of calibration movements <ref> [3, 8, 19] </ref> along the basis direction of motor space e i as ^ J (x; d) = (f (x+d 1 e 1 )f (x); : : : ; f (x+d n e n )f (x))D 1 , where D = diag (d); d 2 &lt; n .
Reference: [9] <author> Jagersand M. Nelson R. </author> <title> Adaptive Differential Visual Feedback for uncalibrated hand-eye coordination and motor control TR# 579, </title> <type> U. </type> <institution> of Rochester 1994. </institution>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In [13] we show how to derive the robust visual servoing controllers used in this paper. <p> Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In [13] we show how to derive the robust visual servoing controllers used in this paper. In <ref> [12, 10, 9] </ref> we have shown how different visual servoing behaviors can be combined to solve complex real-world tasks in unstructured environments. Surprisingly, a thorough experimental evaluation of the accuracy related properties of visual feedback control, to our knowledge, has never been performed. <p> The changes in visual appearance are recorded in a perception or feature vector y = (y 1 : : : y m ) T . Visual features can be drawn from 2 Vectors written bold, scalars plain and matrices capitalized. 1 a large class of visual measurements <ref> [1, 9] </ref>, but we have found that the ones which can be represented as points or point vectors in camera space are suitable, since they yield smooth transfer functions f [10]. We track features such as boundary discontinuities (lines,corners) and surface markings. <p> More details can be found in <ref> [9] </ref>. <p> However this restricts the camera-robot configurations and environments to structured, easy to model settings. Estimation of the full Jacobian was solved mathematically by Broyden in the 60's [21], and later rediscovered in robotics by <ref> [9, 10, 16] </ref>. <p> Joint feedback alignment feedb. feedback Bill 0.13 0.079 0.64 0.2 Hillary 0.059 0.039 0.080 0.2 Table 1: Measured and specified repeatability of our Unimation PUMA 762 and 761 robots under visual and joint feedback control. of the experimental conditions, and the experiment results can be found in our technical report <ref> [9] </ref>. 4.1 Repeatability We tested repeatability under closed loop visual control and compared the results to traditional joint control, both with our own experiments and with published figures. Repeatability is the ability of the robot to reachieve a previously attained pose.
Reference: [10] <author> Jagersand M. </author> <title> "Perception level control for uncalibrated hand-eye coordination and motor actions" Thesis proposal, </title> <institution> University of Rochester, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In [13] we show how to derive the robust visual servoing controllers used in this paper. <p> Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In [13] we show how to derive the robust visual servoing controllers used in this paper. In <ref> [12, 10, 9] </ref> we have shown how different visual servoing behaviors can be combined to solve complex real-world tasks in unstructured environments. Surprisingly, a thorough experimental evaluation of the accuracy related properties of visual feedback control, to our knowledge, has never been performed. <p> Experimental results in most papers fl Support was provided by Sverige-Amerika Stiftelsen, the Ful-bright Commission and ONR grant N00014-93-I-0221. 1 For a review of this work we direct the reader to <ref> [10] </ref> or [15]. have been limited to a few runs, only to validate that a particular algorithm indeed works in practice. <p> can be drawn from 2 Vectors written bold, scalars plain and matrices capitalized. 1 a large class of visual measurements [1, 9], but we have found that the ones which can be represented as points or point vectors in camera space are suitable, since they yield smooth transfer functions f <ref> [10] </ref>. We track features such as boundary discontinuities (lines,corners) and surface markings. Redundant visual perceptions (m n) are desirable as they are used to constrain the raw visual sensory information. <p> However this restricts the camera-robot configurations and environments to structured, easy to model settings. Estimation of the full Jacobian was solved mathematically by Broyden in the 60's [21], and later rediscovered in robotics by <ref> [9, 10, 16] </ref>. <p> By taking out the two problematic DOF's we get the low condition numbers of the 3 and 4 DOF manipulations. 5 Discussion The estimation and control algorithms we developed have strong theoretical properties (see <ref> [10, 13] </ref>). It is still very important to experimentally evaluate how they perform in real environments, with real process disturbances/noise and manipulators with real mechanically caused errors and finite precision.
Reference: [11] <author> Kutulakos K. Jagersand M. </author> <title> "Exploring objects by purposive viewpoint control and invariant-based hand-eye coordination" Workshop on vision for robots In conjunction with IROS 1995. </title>
Reference-contexts: Eye-in-hand type manipulation for instance does not work well without online Jacobian estimation [12]. The Utah/MIT hand control we presented, as well as control of large, complex object rotations we have shown earlier <ref> [11] </ref>, needed the trust region controller, and we could not make the 12 DOF nonrigid control in section 4.4 work without the way points, and the homotopy method. In addition the visual space way point generation is the natural way of doing trajectory planning in a hand-eye system [12].
Reference: [12] <author> Jagersand M. Nelson R. </author> <title> "Visual Space Task Specification, </title> <booktitle> Planning and Control" In Proc on IEEE Symp. on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In [13] we show how to derive the robust visual servoing controllers used in this paper. <p> Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In [13] we show how to derive the robust visual servoing controllers used in this paper. In <ref> [12, 10, 9] </ref> we have shown how different visual servoing behaviors can be combined to solve complex real-world tasks in unstructured environments. Surprisingly, a thorough experimental evaluation of the accuracy related properties of visual feedback control, to our knowledge, has never been performed. <p> The bootstrapping works because the visual feature motions are correlated. See our visual space paper <ref> [12] </ref> for details. 4.3 Effects of over-determined input spaces In this experiment we studied how accuracy is affected by adding redundant visual information, making the system more or less over determined. We found two main advantages. <p> While non-adaptive visual servoing methods have been shown to converge in simple settings (eg. stationary cameras, world coordinate robot control)[3, 8], we have found that in more complex cases the adaptiveness is crucial. Eye-in-hand type manipulation for instance does not work well without online Jacobian estimation <ref> [12] </ref>. The Utah/MIT hand control we presented, as well as control of large, complex object rotations we have shown earlier [11], needed the trust region controller, and we could not make the 12 DOF nonrigid control in section 4.4 work without the way points, and the homotopy method. <p> In addition the visual space way point generation is the natural way of doing trajectory planning in a hand-eye system <ref> [12] </ref>. We believe that the results we show are of some general value in that the repeatability for a visual feedback method is insensitive to the exact controller parameters.
Reference: [13] <institution> Jagersand "Visual Servoing using Trust Region Methods and Estimation of the Full Coupled Visual-Motor Jacobian" Submitted IASTED Applications of Robotics and Control, </institution> <year> 1996. </year>
Reference-contexts: Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In <ref> [13] </ref> we show how to derive the robust visual servoing controllers used in this paper. In [12, 10, 9] we have shown how different visual servoing behaviors can be combined to solve complex real-world tasks in unstructured environments. <p> Intuitively both these techniques aid to synchronize actions with model acquisition, so that the actions never run too far ahead before the local model has been adapted to the new environment. For details and theoretical properties of these two methods see our control theory paper <ref> [13] </ref>. 3.2 Control of the UTAH/MIT hand Fine manipulating an object held in the Utah/MIT hand, using finger movements, is much more difficult than fine manipulation with robot arm, where the object is rigidly attached to the end effector. <p> By taking out the two problematic DOF's we get the low condition numbers of the 3 and 4 DOF manipulations. 5 Discussion The estimation and control algorithms we developed have strong theoretical properties (see <ref> [10, 13] </ref>). It is still very important to experimentally evaluate how they perform in real environments, with real process disturbances/noise and manipulators with real mechanically caused errors and finite precision.
Reference: [14] <author> W. Z. Chen U. A. Korde S. B. </author> <title> Skaar "Position Control Experiments Using Vision" Int. </title> <journal> Journal of Robotics Research, </journal> <volume> v13 No 3 p199-208, </volume> <year> 1994 </year>
Reference-contexts: The closest to a general evaluation we have found is: Wi-jesoma et al. [5] showed for a 2 DOF implementation, the advantage of visual feedback over open loop control when model errors are large. Chen et al. in <ref> [14] </ref> show that two heavy industrial robot arms can perform a peg-in-hole parts mating task with a clearance of 0.7mm between the peg and the hole. In this paper we present an extensive evaluation of visual servoing, based on numerous real positioning experiments, giving the following four main results: 1.
Reference: [15] <author> Corke P. I. </author> <title> High-Performance Visual Closed-Loop Robot Control PhD thesis U of Melbourne 1994. </title>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In [13] we show how to derive the robust visual servoing controllers used in this paper. <p> Experimental results in most papers fl Support was provided by Sverige-Amerika Stiftelsen, the Ful-bright Commission and ONR grant N00014-93-I-0221. 1 For a review of this work we direct the reader to [10] or <ref> [15] </ref>. have been limited to a few runs, only to validate that a particular algorithm indeed works in practice.
Reference: [16] <author> Hosoda K. Asada M. </author> <title> "Versatile Visual Servoing without Knowledge of True Jacobian" Proc. </title> <booktitle> IROS 1994. </booktitle>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In [13] we show how to derive the robust visual servoing controllers used in this paper. <p> However this restricts the camera-robot configurations and environments to structured, easy to model settings. Estimation of the full Jacobian was solved mathematically by Broyden in the 60's [21], and later rediscovered in robotics by <ref> [9, 10, 16] </ref>. <p> We need a control system capable of turning these goal perceptions into motor actions x. A simple control law, occuring in some form in most visual servoing research (e.g. <ref> [3, 19, 16] </ref>) is y fl y = KJ _ x where K is a gain matrix. <p> The convergence results in section 4.2 should be similar among non-adaptive methods, and also valuable for adaptive methods using other Jaco-bian estimation schemes (eg. <ref> [16] </ref>), or (partially) model based schemas (eg.[2]), so long the estimate accuracy is similar.
Reference: [17] <author> Fuentes O. Nelson R. </author> " <title> Experiments on Dextrous Manipulation without Prior Object Models". </title> <note> Submitted ICRA 1996. </note>
Reference-contexts: We found significant precision improvements with the Cartesian controller <ref> [17] </ref>. 4 Experiments with visual servoing In this section we present the main contribution of this paper: a thorough experimental evaluation of visual servoing on several real manipulators (PUMA 761 and 762 6 DOF robot arms, and Utah/MIT 16 DOF dextrous hand).
Reference: [18] <author> B. H. Yoshimi P. K. Allen "Active, Uncalibrated Visual Servoing" ICRA, </author> <year> 1995. </year>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [19, 8, 7]. In [13] we show how to derive the robust visual servoing controllers used in this paper.
Reference: [19] <author> Hager G. </author> <title> "Calibration-Free Visual Control Using Projective Invariance" In Proc. </title> <booktitle> of 5:th ICCV 1995. </booktitle>
Reference-contexts: Recently, "uncalibrated" visual servoing control has been demonstrated in a variety of settings e.g. [1, 3, 2, 5, 6, 9, 10, 12, 15, 16, 18] 1 . Visual models suitable for specifying simple visual alignments have also been studied <ref> [19, 8, 7] </ref>. In [13] we show how to derive the robust visual servoing controllers used in this paper. In [12, 10, 9] we have shown how different visual servoing behaviors can be combined to solve complex real-world tasks in unstructured environments. <p> More details can be found in [9]. An estimate to the image Jacobian can be obtained by physically executing a set of calibration movements <ref> [3, 8, 19] </ref> along the basis direction of motor space e i as ^ J (x; d) = (f (x+d 1 e 1 )f (x); : : : ; f (x+d n e n )f (x))D 1 , where D = diag (d); d 2 &lt; n . <p> We need a control system capable of turning these goal perceptions into motor actions x. A simple control law, occuring in some form in most visual servoing research (e.g. <ref> [3, 19, 16] </ref>) is y fl y = KJ _ x where K is a gain matrix.
Reference: [20] <author> Andersen J. N. </author> <title> "Specifications" In Int. Enc. of Robotics, </title> <editor> Ed: Dorf R. C., </editor> <publisher> Wiley 1988. </publisher>
Reference-contexts: Repeatability is the ability of the robot to reachieve a previously attained pose. Accuracy measures the ability to achieve any prespecified pose <ref> [20] </ref>. Visual feedback control overcomes those problems in traditional inverse kinematics approaches where positioning accuracy is limited by the accuracy of the kinematic model. Positioning performance is improved also by reducing the effect of some non-kinematic disturbances, such as gear backlash and manipulator flexibility. We did not explicitly measure accuracy.
Reference: [21] <author> Broyden C. G. </author> <title> Mathematics of Computation, v 19 p 577-593, </title> <year> 1965. </year>
Reference-contexts: However this restricts the camera-robot configurations and environments to structured, easy to model settings. Estimation of the full Jacobian was solved mathematically by Broyden in the 60's <ref> [21] </ref>, and later rediscovered in robotics by [9, 10, 16].
Reference: [22] <author> Garcia, </author> <title> Zangwill Pathways to solutions, fixed points, and equilibria, </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: We adopt a trust region method [25] similar to the well known Marquart step length (ff) adaption schema to solve the first problem. For the second we use a technique known in numerical analysis as "inbaddning"[24] or homotopy methods <ref> [22] </ref> for, which involves the generation of intermediate goals or "way points" along the way to the main goal y fl , transforming a globally non convex problem into a set of locally convex subprob-lems.
Reference: [23] <author> Fletcher R. </author> <title> Practical Methods of Optimization Chichester, second ed. </title> <year> 1987 </year>
Reference: [24] <author> Gustafsson I. </author> <note> Tillampad Optimeringslara Komp., </note> <institution> Inst. for Inf. Beh., </institution> <note> Chalmers 1991. </note>
Reference: [25] <author> Dahlquist G. Bjorck A. </author> <title> Numerical Methods Second Ed, </title> <publisher> Prentice Hall, </publisher> <year> 1991, </year> <type> preprint. </type>
Reference-contexts: Dynamic stability of the robot at this low sampling frequency is achieved by a secondary set of high bandwidth joint feedback controllers. This popular controller however has two major defi ciencies. First, even for a convex problem (f T f convex) it is not guaranteed to be convergent <ref> [25] </ref>, and second in the case of a non convex problem it often does not converge at all [25]. Previous work has overcome this problem by making only single, small distance moves within a relatively smooth and well scaled region of f. <p> This popular controller however has two major defi ciencies. First, even for a convex problem (f T f convex) it is not guaranteed to be convergent <ref> [25] </ref>, and second in the case of a non convex problem it often does not converge at all [25]. Previous work has overcome this problem by making only single, small distance moves within a relatively smooth and well scaled region of f. To solve whole, real tasks this is not a viable solution. We adopt a trust region method [25] similar to the well known Marquart step length (ff) <p> convex problem it often does not converge at all <ref> [25] </ref>. Previous work has overcome this problem by making only single, small distance moves within a relatively smooth and well scaled region of f. To solve whole, real tasks this is not a viable solution. We adopt a trust region method [25] similar to the well known Marquart step length (ff) adaption schema to solve the first problem.
References-found: 25

