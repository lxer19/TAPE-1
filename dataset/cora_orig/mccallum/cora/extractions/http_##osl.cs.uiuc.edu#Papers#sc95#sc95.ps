URL: http://osl.cs.uiuc.edu/Papers/sc95/sc95.ps
Refering-URL: http://osl.cs.uiuc.edu/Papers/Parallel.html
Root-URL: http://www.cs.uiuc.edu
Email: Email: fwooyoung j aghag@cs.uiuc.edu  
Phone: Phone: (217)244-3087 Fax: (217)333-3501  
Title: Efficient Support of Location Transparency in Concurrent Object-Oriented Programming Languages  
Author: WooYoung Kim and Gul Agha 
Address: Urbana, IL 61801, USA  
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign  
Abstract: We describe the design of a runtime system for a fine-grained concurrent object-oriented (actor) language and its performance. The runtime system provides considerable flexibility to users; specifically, it supports location transparency, actor creation and dynamic placement, and migration. The runtime system includes an efficient distributed name server, a latency hiding scheme for remote actor creation, and a compiler-controlled intra-node scheduling mechanism for local messages and dynamic load balancing. Our preliminary evaluation results suggest that the efficiency that is lost by the greater flexibility of actors can be restored by an efficient runtime system which provides an open interface that can be used by a compiler to allow optimizations. On several standard algorithms, the performance results for our system are comparable to efficient C implementations. Key Words: Concurrent Object-Oriented Programming, Actors, Location Transparency, Mi gration
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: We also discuss language abstractions relevant to the rest of the paper. For a more detailed description of the language constructs, see [3]. 2.1 The Actor Model Actors are independent, concurrent objects which interact through asynchronous communication <ref> [1] </ref>. In response to a message, an actor may: (i) send a message asynchronously to the specified actor, (ii) create an actor with the specified behavior, or (iii) become a new behavior and be ready to process the next message.
Reference: [2] <author> G. Agha, S. Frtlund, W. Kim, R. Panwar, A. Patterson, and D. Sturman. </author> <title> Abstraction and Modularity Mechanisms for Concurrent Computing. </title> <journal> IEEE Parallel and Distributed Technology: Systems and Applications, </journal> <volume> 1(2) </volume> <pages> 3-14, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Hal supports the modular specification of local synchronization constraints as disabling conditions; this allows the programmer to express the minimal synchronization specification that is required for correct program execution [12]. Hal communication abstractions can be used to efficiently implement a variety of algorithms <ref> [2, 3] </ref>. 1 Columns Seq, BP and CP were implemented using a broadcast mechanism based on a hypercube-like minimum spanning tree communication structure, built on top of the CM-5 Active Message (CMAM) layer [35], except for column Bcast which was implemented using vendor-provided CMMD broadcast primitive [32]. 3 3 The Runtime
Reference: [3] <author> G. Agha, W. Kim, and R. Panwar. </author> <title> Actor Languages for Specification of Parallel Computations. </title> <editor> In G. E. Blelloch, K. Mani Chandy, and S. Jagannathan, editors, </editor> <booktitle> DIMACS. Series in Discrete Mathematics and Theoretical Computer Science. </booktitle> <volume> vol 18. </volume> <booktitle> Specification of Parallel Algorithms, </booktitle> <pages> pages 239-258. </pages> <publisher> American Mathematical Society, </publisher> <year> 1994. </year> <booktitle> Proceedings of DIMACS '94 Workshop. </booktitle>
Reference-contexts: 1 Introduction We argue that compromising flexibility is not a necessary condition to obtain good execution performance. Specifically, we describe the design of an efficient runtime system for an actor-based language, Hal <ref> [15, 3] </ref>, which supports location transparency, placement specification for dynamically created objects, and migration. We have argued that such flexibility is essential for scalable execution of dynamic, irregular applications over sparse data structures [28]. We also provide preliminary performance results showing that the runtime system incurs a tolerable overhead. <p> The new techniques implemented in the runtime system are described in detail in x 4 through x 6. Preliminary evaluation results are reported in x 7. We briefly describe related research in x 8. 2 The Language and Its Computational Model Hal <ref> [15, 3] </ref> is an untyped but statically type-checked, actor-based language. Types are implicit in programs. The compiler infers types for each expression in a program using a constraint-based type inference algorithm [27]. It generates C code as its output. <p> It generates C code as its output. In this section, we briefly review the Actor model of computation on which Hal is based. We also discuss language abstractions relevant to the rest of the paper. For a more detailed description of the language constructs, see <ref> [3] </ref>. 2.1 The Actor Model Actors are independent, concurrent objects which interact through asynchronous communication [1]. <p> In this section we describe some simple extensions to the Actor model as well as other abstractions in Hal <ref> [3] </ref>. The new primitive operator is used to specify a single actor creation: it creates an actor of the specified behavior (cf., class) and returns a unique mail address. <p> It returns a unique identifier which may be subsequently used to refer to the group. Hal also extends the asynchronous communication in the Actor model with two communication abstractions: call/return [26, 37] and broadcast <ref> [3] </ref>. Two operators, request and reply, implement the call/return communication abstraction; request sends a message to the server and blocks the sender while it is waiting for a reply and reply sends a result back to the sender of the request. <p> Hal supports the modular specification of local synchronization constraints as disabling conditions; this allows the programmer to express the minimal synchronization specification that is required for correct program execution [12]. Hal communication abstractions can be used to efficiently implement a variety of algorithms <ref> [2, 3] </ref>. 1 Columns Seq, BP and CP were implemented using a broadcast mechanism based on a hypercube-like minimum spanning tree communication structure, built on top of the CM-5 Active Message (CMAM) layer [35], except for column Bcast which was implemented using vendor-provided CMMD broadcast primitive [32]. 3 3 The Runtime <p> If it does, it dispatches the pending messages one by one before it schedules the next actor. 6.2 Support for Call/Return Communication Abstraction The Hal compiler transforms a request send to an asynchronous send and separates out its continuation through dependence analysis <ref> [21, 3, 22] </ref>. Message sends which have no dependence among them are grouped together to share the same continuation. Such continuation have a deterministic behavior: as soon as all the expected replies are received, they execute the computation specified at their creation time and never receive further messages.
Reference: [4] <author> T.E. Anderson, D.E. Culler, D.A. Patterson, </author> <title> and the NOW team. A Case for NOW (Networks of Workstations). </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 54-64, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: We are investigating the possibility of using a real-time garbage collector, such as the one described in [33]. Recently, networks of workstations with fast interconnect network have drawn more and more attention as the potential work force for high performance concurrent computing <ref> [4] </ref>. Our runtime system allows dynamic loading and linking of application programs and can support the concurrent execution of multiple programs from different users. We are investigating ways to reconcile such hardware platforms and our runtime system in a multi-user environment.
Reference: [5] <author> Arvind and Robert A. </author> <title> Iannucci. Two Fundamental Issues in Multiprocessing. </title> <booktitle> In 4th International DFVLR Seminar on Foundations of Engineering Sciences, </booktitle> <pages> pages 61-88, </pages> <year> 1987. </year> <note> LNCS 295. </note>
Reference-contexts: The runtime system implements dynamic load balancing using location transparency and migration. * latency hiding in remote creation: A common way to reduce the inefficiency caused by the unpredictable remote creation time in fine-grained multicomputers is split-phase allocation <ref> [5] </ref>. An object is context-switched to another when it requests a remote creation. However, split-phase allocation is not desirable in stock-hardware multicomputers because of their high context switching cost.
Reference: [6] <author> R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall, A. Shaw, and Y. Zhou. Cilk: </author> <title> An Efficient Multithreaded Runtime System. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming PPOPP, </booktitle> <year> 1994. </year>
Reference-contexts: Table 4 shows the execution results when using a dynamic load balancing scheme. Since Fibonacci actors are purely functional, actor creations were optimized away. Receiver-initiated random polling scheme [25] is used for dynamic load balancing. As a point of comparison, executing the Fibonacci of 33 using the Cilk system <ref> [6] </ref> takes 73.16 seconds on the same Sparc processor and an optimized C version completes in 8.49 seconds. 7.3 Systolic Matrix Multiplication The systolic matrix multiplication algorithm involves first skewing the blocks within a square processor grid, and then, cyclicly shifting the blocks at each step.
Reference: [7] <author> R. Chandra, A. Gupta, and J. L. Hennessy. </author> <title> COOL: An Object-Based Language for Parallel Programming. </title> <journal> IEEE Computer, </journal> <volume> 27(8) </volume> <pages> 13-26, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Our work relies on software technologies without any special hardware support; it is more closely related to the software-based implementations [29, 18]. Our implementation supports fine-grain object-level concurrency and thus differs from research in coarse-grained COOP languages such as <ref> [7, 11, 13, 16] </ref>. However, the techniques we use in the implementation of a name server and remote object creation 11 P 2 4 8 16 256 1.06 0.31 0.12 0.05 1024 72.78 12.51 4.94 1.46 Table 5: Execution times of systolic matrix multiplication (unit: seconds).
Reference: [8] <author> A. Chien, V. Karamcheti, and J. Plevyak. </author> <title> The Concert System Compiler and Runtime Support for Efficient Fine-Grained Concurrent Object-Oriented Programs. </title> <type> Technical Report 13 UIUCDCS-R-93-1815, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: On the other hand, our runtime system guarantees location transparency through the indirection provided by locality descriptors. Furthermore, the use of locality descriptor allows us to use aliases to mask remote creation latency. Our work and the Concert system <ref> [8, 18] </ref> is similar in that the runtime system provides the compiler with a flexible interface. The two runtime systems make cost distinctions explicit for runtime operations to enable a variety of optimizations by the compiler.
Reference: [9] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 262-273, </pages> <year> 1993. </year>
Reference-contexts: No global synchronization is used in the implementation. Instead, per actor basis local synchronization is used to enforce the necessary synchronization. The local block matrix multiplication is implemented using the assembly routine used in <ref> [9] </ref>. The execution times are shown in Table 5. The results are comparable to the results given in [9]. <p> Instead, per actor basis local synchronization is used to enforce the necessary synchronization. The local block matrix multiplication is implemented using the assembly routine used in <ref> [9] </ref>. The execution times are shown in Table 5. The results are comparable to the results given in [9]. For example, the performance peaks at 434 MFlops for 1024 by 1024 matrix on 64 node partition of the CM-5. 8 Related Work Many efficient software-based and hardware-based implementations for fine-grained concurrent object-oriented programming languages have been reported in literature [14, 36, 29, 18].
Reference: [10] <author> D.E. Culler, A. Sah, K. E. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine. </title> <booktitle> In Proceedings of ASPLOS, </booktitle> <pages> pages 166-175, </pages> <year> 1991. </year>
Reference-contexts: However, split-phase allocation is not desirable in stock-hardware multicomputers because of their high context switching cost. We propose an efficient scheme which masks latency in remote object creation without context switching. * compiler-controlled intra-node scheduling: The runtime system employs compiler-controlled stack-based scheduling for local messages inspired by <ref> [10, 29] </ref>. Because our runtime system provides a flexible interface to the compiler, we were able to implement an efficient stack-based scheduling mechanism [22]. We have implemented our runtime system on the CM-5 [32, 31]. <p> However, it exposes part of its scheduling mechanism to the compiler so that the compiler can exploit frequently occurring special cases <ref> [10, 22] </ref>. 6.1 Support for Local Synchronization Constraints The enforcement of local synchronization constraints is implemented using an auxiliary mail queue (i.e., pending queue). When an actor dispatches a message, it examines whether the corresponding method is enabled. <p> The broadcast primitive is implemented on top of the CMAM layer using a hypercube-like minimum spanning tree communication structure. By distinguishing broadcast messages and exposing the implementation of groups to the compiler, broadcast messages are scheduled in a manner similar to the quasi-dynamic scheduling in Threaded Abstract Machine (TAM) <ref> [10] </ref>.
Reference: [11] <author> F. Bodin and P. Beckman and D. Gannon and S. Yang and S. Kesavan and A. Malony and B. Mohr. </author> <title> Implementing a Parallel C++ Runtime System for Scalable Parallel Systems. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 588-597, </pages> <year> 1993. </year>
Reference-contexts: Our work relies on software technologies without any special hardware support; it is more closely related to the software-based implementations [29, 18]. Our implementation supports fine-grain object-level concurrency and thus differs from research in coarse-grained COOP languages such as <ref> [7, 11, 13, 16] </ref>. However, the techniques we use in the implementation of a name server and remote object creation 11 P 2 4 8 16 256 1.06 0.31 0.12 0.05 1024 72.78 12.51 4.94 1.46 Table 5: Execution times of systolic matrix multiplication (unit: seconds).
Reference: [12] <author> S. Frtlund. </author> <title> Inheritance of Synchronization Constraints in Concurrent Object-Oriented Programming Languages. </title> <editor> In O. Lehrmann Madsen, editor, </editor> <booktitle> ECOOP'92 European Conference on Object-Oriented Programming, </booktitle> <pages> pages 185-196. </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1992. </year> <note> Lecture Notes in Computer Science 615. </note>
Reference-contexts: We also provide preliminary performance results showing that the runtime system incurs a tolerable overhead. The distinguishing features of our runtime system are: * support for local synchronization constraints: Synchronization constraints specify the subset of possible states of an object under which the object's methods may be invoked <ref> [12] </ref>. Synchronization constraints are local if they are specified on a per-object basis. <p> Hal supports the modular specification of local synchronization constraints as disabling conditions; this allows the programmer to express the minimal synchronization specification that is required for correct program execution <ref> [12] </ref>.
Reference: [13] <author> A. Grimshaw, W. T. Strayer, and P. Narayan. </author> <title> Dynamic Object-Oriented Parallel Processing. </title> <journal> IEEE Parallel and Distributed Technology: Systems and Applications, </journal> <volume> 1(2) </volume> <pages> 33-47, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Our work relies on software technologies without any special hardware support; it is more closely related to the software-based implementations [29, 18]. Our implementation supports fine-grain object-level concurrency and thus differs from research in coarse-grained COOP languages such as <ref> [7, 11, 13, 16] </ref>. However, the techniques we use in the implementation of a name server and remote object creation 11 P 2 4 8 16 256 1.06 0.31 0.12 0.05 1024 72.78 12.51 4.94 1.46 Table 5: Execution times of systolic matrix multiplication (unit: seconds).
Reference: [14] <author> W. Horwat. </author> <title> Concurrent Smalltalk on the Message Driven Processor. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: For example, the performance peaks at 434 MFlops for 1024 by 1024 matrix on 64 node partition of the CM-5. 8 Related Work Many efficient software-based and hardware-based implementations for fine-grained concurrent object-oriented programming languages have been reported in literature <ref> [14, 36, 29, 18] </ref>. Our work relies on software technologies without any special hardware support; it is more closely related to the software-based implementations [29, 18]. Our implementation supports fine-grain object-level concurrency and thus differs from research in coarse-grained COOP languages such as [7, 11, 13, 16].
Reference: [15] <author> C. Houck and G. Agha. HAL: </author> <title> A High-level Actor Language and Its Distributed Implementation. </title> <booktitle> In Proceedings of th 21st International Conference on Parallel Processing (ICPP '92), </booktitle> <volume> volume II, </volume> <pages> pages 158-165, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: 1 Introduction We argue that compromising flexibility is not a necessary condition to obtain good execution performance. Specifically, we describe the design of an efficient runtime system for an actor-based language, Hal <ref> [15, 3] </ref>, which supports location transparency, placement specification for dynamically created objects, and migration. We have argued that such flexibility is essential for scalable execution of dynamic, irregular applications over sparse data structures [28]. We also provide preliminary performance results showing that the runtime system incurs a tolerable overhead. <p> The new techniques implemented in the runtime system are described in detail in x 4 through x 6. Preliminary evaluation results are reported in x 7. We briefly describe related research in x 8. 2 The Language and Its Computational Model Hal <ref> [15, 3] </ref> is an untyped but statically type-checked, actor-based language. Types are implicit in programs. The compiler infers types for each expression in a program using a constraint-based type inference algorithm [27]. It generates C code as its output.
Reference: [16] <author> L. V. Kale and S. Krishnan. CHARM++: </author> <title> A Portable Concurrent Object Oriented System Based On C++. </title> <editor> In Andreas Paepcke, editor, </editor> <booktitle> Proceedings of OOPSLA 93'. </booktitle> <publisher> ACM Press, </publisher> <month> October </month> <year> 1993. </year> <journal> ACM SIGPLAN Notices 28(10). </journal>
Reference-contexts: Our work relies on software technologies without any special hardware support; it is more closely related to the software-based implementations [29, 18]. Our implementation supports fine-grain object-level concurrency and thus differs from research in coarse-grained COOP languages such as <ref> [7, 11, 13, 16] </ref>. However, the techniques we use in the implementation of a name server and remote object creation 11 P 2 4 8 16 256 1.06 0.31 0.12 0.05 1024 72.78 12.51 4.94 1.46 Table 5: Execution times of systolic matrix multiplication (unit: seconds).
Reference: [17] <author> V. </author> <title> Karamcheti. </title> <type> Private Communication, </type> <year> 1994. </year>
Reference-contexts: The difference between the two systems lies in the extent of location transparency they support. Aggregates in the Concert system are located at the same address offset on each node <ref> [17] </ref>. This location dependence limits the aggregates's mobility, making it difficult to balance load in dynamic irregular computations. Objects other than aggregates are allocated in a global space and subject to global name translation.
Reference: [18] <author> V. Karamcheti and A. A. Chien. </author> <title> Concert Efficient Runtime Support for Concurrent Object-Oriented Programming Languages on Stock Hardware. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: The runtime system exposes part of its scheduling mechanism to the compiler so that the compiler can exploit frequently occurring special cases. The runtime system is also designed to optimally utilize the information available at compile-time. Our performance results for primitive operations are comparable to those of other implementations <ref> [29, 18] </ref>. This suggests that the overhead caused by adding more flexibility can be kept tolerable by proper system design and the close interaction between a compiler and its runtime system. <p> the join continuation; it is used to notify the actor of the completion of continuation if necessary. 6.3 Compiler-Controlled Intra-Node Scheduling The large performance difference between the generic message send mechanism and function invocation justifies the use of runtime locality check to enable static method dispatch for scheduling local messages <ref> [29, 18, 22] </ref>. The difficulty in using static method dispatch lies in the fact that the method to be invoked may not be determined statically because of late binding and type-dependent method dispatch in object-oriented languages. <p> The locality check is done using only locally available information and completes within 1 sec for the locally created actors. The performance of the runtime primitives is comparable to that in other systems <ref> [29, 18] </ref> (Table 3). The overall performance of a runtime system on stock-hardware distributed memory multicom-puters depends on the efficiency of its communication module. <p> For example, the performance peaks at 434 MFlops for 1024 by 1024 matrix on 64 node partition of the CM-5. 8 Related Work Many efficient software-based and hardware-based implementations for fine-grained concurrent object-oriented programming languages have been reported in literature <ref> [14, 36, 29, 18] </ref>. Our work relies on software technologies without any special hardware support; it is more closely related to the software-based implementations [29, 18]. Our implementation supports fine-grain object-level concurrency and thus differs from research in coarse-grained COOP languages such as [7, 11, 13, 16]. <p> Our work relies on software technologies without any special hardware support; it is more closely related to the software-based implementations <ref> [29, 18] </ref>. Our implementation supports fine-grain object-level concurrency and thus differs from research in coarse-grained COOP languages such as [7, 11, 13, 16]. <p> On the other hand, our runtime system guarantees location transparency through the indirection provided by locality descriptors. Furthermore, the use of locality descriptor allows us to use aliases to mask remote creation latency. Our work and the Concert system <ref> [8, 18] </ref> is similar in that the runtime system provides the compiler with a flexible interface. The two runtime systems make cost distinctions explicit for runtime operations to enable a variety of optimizations by the compiler.
Reference: [19] <author> V. Karamcheti and A.A. Chien. </author> <title> A Comparison of Architectural Support for Messaging on the TMC CM-5 and the Cray T3D. </title> <booktitle> In Proceedings of International Symposium of Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: The communication module is built on top of CMAM [35], a messaging layer which provides a transparent view of the underlying architecture; thus, porting the kernel to other platforms is 4 straight-forward as long as a well-defined messaging layer is supported (for example, <ref> [34, 19, 20] </ref>). Messages in Hal have some unique properties. In particular, all actor messages have a destination mail address and a method selector. Many of them may also contain a continuation address.
Reference: [20] <author> K.E. Schauser and C.J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In Proceedings of IPPS '95, </booktitle> <year> 1995. </year>
Reference-contexts: The communication module is built on top of CMAM [35], a messaging layer which provides a transparent view of the underlying architecture; thus, porting the kernel to other platforms is 4 straight-forward as long as a well-defined messaging layer is supported (for example, <ref> [34, 19, 20] </ref>). Messages in Hal have some unique properties. In particular, all actor messages have a destination mail address and a method selector. Many of them may also contain a continuation address.
Reference: [21] <author> W. Kim and G. Agha. </author> <title> Compilation of a Highly Parallel Actor-Based Language. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Proceedings of the Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 1-15. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year> <note> LNCS 757. </note>
Reference-contexts: If it does, it dispatches the pending messages one by one before it schedules the next actor. 6.2 Support for Call/Return Communication Abstraction The Hal compiler transforms a request send to an asynchronous send and separates out its continuation through dependence analysis <ref> [21, 3, 22] </ref>. Message sends which have no dependence among them are grouped together to share the same continuation. Such continuation have a deterministic behavior: as soon as all the expected replies are received, they execute the computation specified at their creation time and never receive further messages.
Reference: [22] <author> W. Kim and G. Agha. </author> <title> A Scalable Implementation of Communication Abstractions in Actor Programming. </title> <note> in preparation, 1995. 14 </note>
Reference-contexts: Because our runtime system provides a flexible interface to the compiler, we were able to implement an efficient stack-based scheduling mechanism <ref> [22] </ref>. We have implemented our runtime system on the CM-5 [32, 31]. Primitive operations, such as object creation and message send, have been carefully designed and optimized while maintaining the semantics of the language. <p> However, it exposes part of its scheduling mechanism to the compiler so that the compiler can exploit frequently occurring special cases <ref> [10, 22] </ref>. 6.1 Support for Local Synchronization Constraints The enforcement of local synchronization constraints is implemented using an auxiliary mail queue (i.e., pending queue). When an actor dispatches a message, it examines whether the corresponding method is enabled. <p> If it does, it dispatches the pending messages one by one before it schedules the next actor. 6.2 Support for Call/Return Communication Abstraction The Hal compiler transforms a request send to an asynchronous send and separates out its continuation through dependence analysis <ref> [21, 3, 22] </ref>. Message sends which have no dependence among them are grouped together to share the same continuation. Such continuation have a deterministic behavior: as soon as all the expected replies are received, they execute the computation specified at their creation time and never receive further messages. <p> the join continuation; it is used to notify the actor of the completion of continuation if necessary. 6.3 Compiler-Controlled Intra-Node Scheduling The large performance difference between the generic message send mechanism and function invocation justifies the use of runtime locality check to enable static method dispatch for scheduling local messages <ref> [29, 18, 22] </ref>. The difficulty in using static method dispatch lies in the fact that the method to be invoked may not be determined statically because of late binding and type-dependent method dispatch in object-oriented languages. <p> The compiler uses a constraint-based type inference [27] to determine the recipient's type (i.e. class) for each message send. If the compiler can infer the unique type for the recipient it generates code for static method dispatch with locality check as well as code for the message send <ref> [22] </ref>. The runtime system provides the compiler with the locality check routine which is part of the generic message send mechanism. The routine additionally checks if the recipient actor is in a state in which it is enabled to process the message.
Reference: [23] <author> E.J. Koldinger, J.S. Chase, and S.J. Eggers. </author> <title> Architectural Support for Single Address Space Operating Systems. </title> <booktitle> In ASPLOS V '92, </booktitle> <pages> pages 175-186, </pages> <year> 1992. </year>
Reference-contexts: In order to efficiently support sharing, we have designed the kernel to execute all computations on a single address space <ref> [23] </ref>. The compiler generates executables with load information. Upon execution, the executable is dynamically loaded and integrated into each kernel. The kernel does not discriminate between actors created by different programs. Users are provided with a simple command interpreter which communicates with the front-end to load the executables.
Reference: [24] <author> V. Kumar, A. Grama, A. Gupta, and G. Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <publisher> Benjamin/Cummings Publishing Company, Inc., </publisher> <year> 1994. </year>
Reference-contexts: The overall performance of a runtime system on stock-hardware distributed memory multicom-puters depends on the efficiency of its communication module. In order to assess the efficiency of the communication module we have implemented two benchmark programs, Fibonacci number generator and a systolic dense matrix multiplication algorithm <ref> [24] </ref> and measured the execution time on different CM-5 partitions. 7.2 Fibonacci Number Generator 10 Local Remote invocation invocation sec cycle sec cycle ABCL/AP1000 (25 MHz) 2.3 58 8.9 223 Concert (33 MHz) 3.7 fl 122 7.67-16.3 253-538 Hal (33 MHz) 1.45 y 48 9.91 327 Table 3: The comparison of
Reference: [25] <author> V. Kumar, A. Y. Grama, and V. N. Rao. </author> <title> Scalable Load Balancing Techniques for Parallel Computers. </title> <type> Technical Report 91-55, </type> <institution> CS Dept., University of Minnesota, </institution> <year> 1991. </year> <note> available via ftp ftp.cs.umn.edu:/users/kumar/lb_MIMD.ps.Z. </note>
Reference-contexts: Moreover, its computation tree has a great deal of load imbalance. Table 4 shows the execution results when using a dynamic load balancing scheme. Since Fibonacci actors are purely functional, actor creations were optimized away. Receiver-initiated random polling scheme <ref> [25] </ref> is used for dynamic load balancing.
Reference: [26] <author> C. Manning. ACORE: </author> <title> The Design of a Core Actor Language and its Compiler. </title> <type> Master's thesis, </type> <institution> MIT, Artificial Intelligence Laboratory, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: It returns a unique identifier which may be subsequently used to refer to the group. Hal also extends the asynchronous communication in the Actor model with two communication abstractions: call/return <ref> [26, 37] </ref> and broadcast [3]. Two operators, request and reply, implement the call/return communication abstraction; request sends a message to the server and blocks the sender while it is waiting for a reply and reply sends a result back to the sender of the request.
Reference: [27] <author> N. Oxhoj, J. Palsberg, and M. I. Schwartzbach. </author> <title> Making Type Inference Practical. </title> <booktitle> In Proc. ECOOP'92, </booktitle> <pages> pages 329-349. </pages> <publisher> Springer-Verlag (LNCS 615), </publisher> <year> 1992. </year>
Reference-contexts: We briefly describe related research in x 8. 2 The Language and Its Computational Model Hal [15, 3] is an untyped but statically type-checked, actor-based language. Types are implicit in programs. The compiler infers types for each expression in a program using a constraint-based type inference algorithm <ref> [27] </ref>. It generates C code as its output. In this section, we briefly review the Actor model of computation on which Hal is based. We also discuss language abstractions relevant to the rest of the paper. <p> The difficulty in using static method dispatch lies in the fact that the method to be invoked may not be determined statically because of late binding and type-dependent method dispatch in object-oriented languages. The compiler uses a constraint-based type inference <ref> [27] </ref> to determine the recipient's type (i.e. class) for each message send. If the compiler can infer the unique type for the recipient it generates code for static method dispatch with locality check as well as code for the message send [22]. <p> The compiler uses flow analysis 9 Local Remote Local Send/ Remote Send/ Locality Creation Creation Dispatch Dispatch Check time 8.04 5.83 (20.83) 0.45 - 5.67 9.91 1.00 Table 2: Execution time of runtime primitives (unit: sec) information <ref> [27] </ref> to provide information for the runtime system to collectively schedule a broadcast message. 6.5 Minimal Flow Control Since Active Messages are not buffered, a three-phase protocol is required to send bulk data from one node to another [35].
Reference: [28] <author> R. Panwar and G. Agha. </author> <title> A Methodology for Programming Scalable Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 479-487, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Specifically, we describe the design of an efficient runtime system for an actor-based language, Hal [15, 3], which supports location transparency, placement specification for dynamically created objects, and migration. We have argued that such flexibility is essential for scalable execution of dynamic, irregular applications over sparse data structures <ref> [28] </ref>. We also provide preliminary performance results showing that the runtime system incurs a tolerable overhead. The distinguishing features of our runtime system are: * support for local synchronization constraints: Synchronization constraints specify the subset of possible states of an object under which the object's methods may be invoked [12].
Reference: [29] <author> K. Taura, S. Matsuoka, and A. Yonezawa. </author> <title> An Efficient Implementation Scheme of Concurrent Object-Oriented Languages on Stock Multicomputers. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming PPOPP, </booktitle> <pages> pages 218-228, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: However, split-phase allocation is not desirable in stock-hardware multicomputers because of their high context switching cost. We propose an efficient scheme which masks latency in remote object creation without context switching. * compiler-controlled intra-node scheduling: The runtime system employs compiler-controlled stack-based scheduling for local messages inspired by <ref> [10, 29] </ref>. Because our runtime system provides a flexible interface to the compiler, we were able to implement an efficient stack-based scheduling mechanism [22]. We have implemented our runtime system on the CM-5 [32, 31]. <p> The runtime system exposes part of its scheduling mechanism to the compiler so that the compiler can exploit frequently occurring special cases. The runtime system is also designed to optimally utilize the information available at compile-time. Our performance results for primitive operations are comparable to those of other implementations <ref> [29, 18] </ref>. This suggests that the overhead caused by adding more flexibility can be kept tolerable by proper system design and the close interaction between a compiler and its runtime system. <p> the join continuation; it is used to notify the actor of the completion of continuation if necessary. 6.3 Compiler-Controlled Intra-Node Scheduling The large performance difference between the generic message send mechanism and function invocation justifies the use of runtime locality check to enable static method dispatch for scheduling local messages <ref> [29, 18, 22] </ref>. The difficulty in using static method dispatch lies in the fact that the method to be invoked may not be determined statically because of late binding and type-dependent method dispatch in object-oriented languages. <p> The locality check is done using only locally available information and completes within 1 sec for the locally created actors. The performance of the runtime primitives is comparable to that in other systems <ref> [29, 18] </ref> (Table 3). The overall performance of a runtime system on stock-hardware distributed memory multicom-puters depends on the efficiency of its communication module. <p> For example, the performance peaks at 434 MFlops for 1024 by 1024 matrix on 64 node partition of the CM-5. 8 Related Work Many efficient software-based and hardware-based implementations for fine-grained concurrent object-oriented programming languages have been reported in literature <ref> [14, 36, 29, 18] </ref>. Our work relies on software technologies without any special hardware support; it is more closely related to the software-based implementations [29, 18]. Our implementation supports fine-grain object-level concurrency and thus differs from research in coarse-grained COOP languages such as [7, 11, 13, 16]. <p> Our work relies on software technologies without any special hardware support; it is more closely related to the software-based implementations <ref> [29, 18] </ref>. Our implementation supports fine-grain object-level concurrency and thus differs from research in coarse-grained COOP languages such as [7, 11, 13, 16]. <p> All results were obtained by executing the program with M fi M matrix on P fi P processor array. may also be used in the implementation of coarse-grained COOP languages. The implementation of ABCL/onAP1000 <ref> [29, 30] </ref> uses an encapsulated runtime system. For example, the runtime system determines whether to use the stack-based or the queue-based scheduling mechanism for local messages. By contrast, our runtime system and the compiler interact more closely to generate executables which choose the mechanism to use to send local messages.
Reference: [30] <author> K. Taura, S. Matsuoka, and A. Yonezawa. ABCL/f: </author> <title> A Future-Based Polymorphic Typed Concurrent Object-Oriented Language Its Design and Implementation. </title> <editor> In G. E. Blelloch, K. Mani Chandy, and S. Jagannathan, editors, </editor> <booktitle> DIMACS. Series in Discrete Mathematics and Theoretical Computer Science. </booktitle> <volume> vol 18. </volume> <booktitle> Specification of Parallel Algorithms, </booktitle> <pages> pages 275-291. </pages> <publisher> American Mathematical Society, </publisher> <year> 1994. </year> <booktitle> Proceedings of DIMACS '94 Workshop. </booktitle>
Reference-contexts: All results were obtained by executing the program with M fi M matrix on P fi P processor array. may also be used in the implementation of coarse-grained COOP languages. The implementation of ABCL/onAP1000 <ref> [29, 30] </ref> uses an encapsulated runtime system. For example, the runtime system determines whether to use the stack-based or the queue-based scheduling mechanism for local messages. By contrast, our runtime system and the compiler interact more closely to generate executables which choose the mechanism to use to send local messages.
Reference: [31] <institution> Thinking Machine Corporation. </institution> <note> Connection Machine CM-5 Technical Summary, revised edition edition, </note> <month> November </month> <year> 1992. </year>
Reference-contexts: Because our runtime system provides a flexible interface to the compiler, we were able to implement an efficient stack-based scheduling mechanism [22]. We have implemented our runtime system on the CM-5 <ref> [32, 31] </ref>. Primitive operations, such as object creation and message send, have been carefully designed and optimized while maintaining the semantics of the language. The runtime system exposes part of its scheduling mechanism to the compiler so that the compiler can exploit frequently occurring special cases. <p> The machine may be configured in different partitions; a partition has a control processor called partition manager and a set of processors called processing elements. Each processing element contains a 33 MHz Sparc processor and a network interface chip which supports all accesses to the interconnection network <ref> [31] </ref>. system consists of a front-end which runs on the partition manager and a set of runtime kernels which run on the processing elements (i.e., nodes). Each kernel contains a system (meta-level) actor named node manager. Kernels are implemented as ordinary UNIX processes.
Reference: [32] <institution> Thinking Machine Corporation. </institution> <note> CMMD Reference Manual Version 3.0, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: Because our runtime system provides a flexible interface to the compiler, we were able to implement an efficient stack-based scheduling mechanism [22]. We have implemented our runtime system on the CM-5 <ref> [32, 31] </ref>. Primitive operations, such as object creation and message send, have been carefully designed and optimized while maintaining the semantics of the language. The runtime system exposes part of its scheduling mechanism to the compiler so that the compiler can exploit frequently occurring special cases. <p> a variety of algorithms [2, 3]. 1 Columns Seq, BP and CP were implemented using a broadcast mechanism based on a hypercube-like minimum spanning tree communication structure, built on top of the CM-5 Active Message (CMAM) layer [35], except for column Bcast which was implemented using vendor-provided CMMD broadcast primitive <ref> [32] </ref>. 3 3 The Runtime System Architecture The runtime system is currently running on the CM-5 and on networks of workstations. This paper describes the implementation of the runtime system on the CM-5. The CM-5 machine is a distributed memory multicomputer which can be scaled up to 16K processors.
Reference: [33] <author> N. Venkatasubramaniam, G. Agha, and C. Talcott. </author> <title> Scalable Distributed Garbage Collection for Systems of Active Objects. </title> <booktitle> In Proceedings International Workshop on Memory Management, </booktitle> <pages> pages 441-451, </pages> <address> St. Malo, France, </address> <month> September </month> <year> 1992. </year> <note> ACM SIGPLAN and INRIA, Springer-Verlag. Lecture Notes in Computer Science. </note>
Reference-contexts: The use of locality descriptors to support location transparency has the advantage of supporting 12 an efficient garbage collection scheme. We are investigating the possibility of using a real-time garbage collector, such as the one described in <ref> [33] </ref>. Recently, networks of workstations with fast interconnect network have drawn more and more attention as the potential work force for high performance concurrent computing [4]. Our runtime system allows dynamic loading and linking of application programs and can support the concurrent execution of multiple programs from different users.
Reference: [34] <author> T. von Eicken, A. Basu, and V. </author> <title> Buch. Low-Latency Communication over ATM Networks Using Active Messages. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 46-53, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The communication module is built on top of CMAM [35], a messaging layer which provides a transparent view of the underlying architecture; thus, porting the kernel to other platforms is 4 straight-forward as long as a well-defined messaging layer is supported (for example, <ref> [34, 19, 20] </ref>). Messages in Hal have some unique properties. In particular, all actor messages have a destination mail address and a method selector. Many of them may also contain a continuation address.
Reference: [35] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of International Symposium of Computer Architectures, </booktitle> <pages> pages 256-266, </pages> <year> 1992. </year>
Reference-contexts: Hal communication abstractions can be used to efficiently implement a variety of algorithms [2, 3]. 1 Columns Seq, BP and CP were implemented using a broadcast mechanism based on a hypercube-like minimum spanning tree communication structure, built on top of the CM-5 Active Message (CMAM) layer <ref> [35] </ref>, except for column Bcast which was implemented using vendor-provided CMMD broadcast primitive [32]. 3 3 The Runtime System Architecture The runtime system is currently running on the CM-5 and on networks of workstations. This paper describes the implementation of the runtime system on the CM-5. <p> Communication module and program load module together constitute the kernel's interface to the underlying machine and represent the kernel's machine-dependent part. In between these two layers are node manager, dispatcher and name server. The communication module is built on top of CMAM <ref> [35] </ref>, a messaging layer which provides a transparent view of the underlying architecture; thus, porting the kernel to other platforms is 4 straight-forward as long as a well-defined messaging layer is supported (for example, [34, 19, 20]). Messages in Hal have some unique properties. <p> 9.91 1.00 Table 2: Execution time of runtime primitives (unit: sec) information [27] to provide information for the runtime system to collectively schedule a broadcast message. 6.5 Minimal Flow Control Since Active Messages are not buffered, a three-phase protocol is required to send bulk data from one node to another <ref> [35] </ref>. We have learned from the implementation of Cholesky Decomposition algorithm (x 2.2) that without flow control, software pipelining may not be correctly implemented because of the three-phase protocol. The runtime system supports minimal flow control for sending messages of large size to guarantee the correct implementation of software pipelining.
Reference: [36] <author> M. Yasugi, S. Matsuoka, and A. Yonezawa. ABCL/onEM-4: </author> <title> A New Software/Hardware Architecture for Object-Oriented Concurrent Computing on an Extended Dataflow Supercomputer. </title> <booktitle> In ICS '92, </booktitle> <pages> pages 93-103, </pages> <year> 1992. </year>
Reference-contexts: For example, the performance peaks at 434 MFlops for 1024 by 1024 matrix on 64 node partition of the CM-5. 8 Related Work Many efficient software-based and hardware-based implementations for fine-grained concurrent object-oriented programming languages have been reported in literature <ref> [14, 36, 29, 18] </ref>. Our work relies on software technologies without any special hardware support; it is more closely related to the software-based implementations [29, 18]. Our implementation supports fine-grain object-level concurrency and thus differs from research in coarse-grained COOP languages such as [7, 11, 13, 16].
Reference: [37] <author> A. Yonezawa, </author> <title> editor. ABCL An Object-Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year> <month> 15 </month>
Reference-contexts: It returns a unique identifier which may be subsequently used to refer to the group. Hal also extends the asynchronous communication in the Actor model with two communication abstractions: call/return <ref> [26, 37] </ref> and broadcast [3]. Two operators, request and reply, implement the call/return communication abstraction; request sends a message to the server and blocks the sender while it is waiting for a reply and reply sends a result back to the sender of the request.
References-found: 37

