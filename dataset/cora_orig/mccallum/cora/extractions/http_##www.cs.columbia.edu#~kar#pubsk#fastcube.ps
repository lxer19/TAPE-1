URL: http://www.cs.columbia.edu/~kar/pubsk/fastcube.ps
Refering-URL: http://www.cs.columbia.edu/~kar/pubsk/pubsk.html
Root-URL: http://www.cs.columbia.edu
Email: kar@cs.columbia.edu  divesh@research.att.com  
Title: Fast Computation of Sparse Datacubes  
Author: Kenneth A. Ross Divesh Srivastava 
Note: The research of Kenneth A. Ross was supported by a grant from the AT&T Foundation, by a David and Lucile Packard Foundation Fellowship in Science and Engineering, by a Sloan Foundation Fellowship, by an NSF Young Investigator Award, and by NSF CISE award CDA-9625374. Proceedings of the 23rd VLDB Conference Athens, Greece, 1997  
Address: Labs-Research  
Affiliation: Columbia University  AT&T  
Abstract: Datacube queries compute aggregates over database relations at a variety of granularities, and they constitute an important class of decision support queries. Real-world data is frequently sparse, and hence efficiently computing datacubes over large sparse relations is important. We show that current techniques for computing datacubes over sparse relations do not scale well with the number of CUBE BY attributes, especially when the relation is much larger than main memory. We propose a novel algorithm for the fast computation of datacubes over sparse relations, and demonstrate the efficiency of our algorithm using synthetic, benchmark and real-world data sets. When the relation fits in memory, our technique performs multiple in-memory sorts, and does not incur any I/O beyond the input of the relation and the output of the datacube itself. When the relation does not fit in memory, a divide-and-conquer strategy divides the problem of computing the datacube into several simpler computations of sub-datacubes. Often, all but one of the sub-datacubes can be computed in memory and our in-memory solution applies. In that case, the total I/O overhead is linear in the number of CUBE BY attributes. We demonstrate with an implementation that the CPU cost of our algorithm is dominated by the I/O cost for sparse relations. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. 
Abstract-found: 1
Intro-found: 1
Reference: [AAD + 96] <author> S. Agarwal, R. Agrawal, P. M. Deshpande, A. Gupta, J. F. Naughton, R. Ramakrishnan, and S. Sarawagi. </author> <title> On the computation of multidimensional aggregates. </title> <booktitle> In Proceedings of the International Conference on Very Large Databases, </booktitle> <pages> pages 506-521, </pages> <year> 1996. </year>
Reference-contexts: Each of these 2 k granularities is referred to as a cuboid following <ref> [DANR96, AAD + 96] </ref>, and we use the notation Q ( ~ B i ) to denote the cuboid at granularity ~ B i . <p> The various existing techniques for computing dat-acubes, outlined below, attempt to make use of the relationships between the cuboids to compute the datacube Q more efficiently than the naive approach of independently computing each of the 2 k cuboids. 2.1 PIPESORT The PIPESORT algorithm proposed by Sarawagi et al. <ref> [SAG96, AAD + 96] </ref> tries to optimize the overall cost of the computation of datacube Q using cost estimates of various ways to compute each cuboid Q ( ~ B j ) to determine which cuboid will be used to actually compute the tuples of Q ( ~ B j ). <p> It thus incurs an I/O overhead of only about 25%! (We shall address the CPU cost separately.) 2 2.2 OVERLAP The OVERLAP algorithm proposed by Deshpande et al. <ref> [DANR96, AAD + 96] </ref> tries to minimize the number of disk accesses by overlapping the computation of the cuboids, by making use of partially matching sort orders to reduce the number of sorting steps performed. <p> In the graphs below we assume a disk transfer rate of 1.5 MB/sec, as in <ref> [SAG96, AAD + 96] </ref>. Sorting was performed in-place (on pointers to tuples) using quicksort [Hoa62].
Reference: [DANR96] <author> P. M. Deshpande, S. Agarwal, J. F. Naughton, and R. Ramakrishnan. </author> <title> Computation of multidimensional aggregates. </title> <type> Technical Report 1314, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1996. </year>
Reference-contexts: Each of these 2 k granularities is referred to as a cuboid following <ref> [DANR96, AAD + 96] </ref>, and we use the notation Q ( ~ B i ) to denote the cuboid at granularity ~ B i . <p> It thus incurs an I/O overhead of only about 25%! (We shall address the CPU cost separately.) 2 2.2 OVERLAP The OVERLAP algorithm proposed by Deshpande et al. <ref> [DANR96, AAD + 96] </ref> tries to minimize the number of disk accesses by overlapping the computation of the cuboids, by making use of partially matching sort orders to reduce the number of sorting steps performed.
Reference: [GBLP96] <author> J. Gray, A. Bosworth, A. Layman, and H. Pi-rahesh. </author> <title> Datacube : A relational aggregation operator generalizing group-by, </title> <booktitle> cross-tab, and sub-totals. In Proceedings of the IEEE International Conference on Data Engineering, </booktitle> <pages> pages 152-159, </pages> <year> 1996. </year>
Reference-contexts: is the first work specifically addressing datacube computations for the important class of sparse database relations. 2 Motivation: Existing Techniques We present the ideas that underlie various existing techniques for computing datacubes using the datacube query Q in Figure 1, expressed in the generalized SQL syntax of Gray et al. <ref> [GBLP96] </ref>. Here A and the B i 's are attributes of the relation R, and G is an aggregate function. In this paper, we consider only distributive aggregate functions [GBLP96]. <p> for computing datacubes using the datacube query Q in Figure 1, expressed in the generalized SQL syntax of Gray et al. <ref> [GBLP96] </ref>. Here A and the B i 's are attributes of the relation R, and G is an aggregate function. In this paper, we consider only distributive aggregate functions [GBLP96]. <p> This analysis gives OVERLAP an I/O overhead of at least 45% over the size of the datacube result! 2 2.3 Array-Based Algorithms The array-based algorithm proposed by Gray et al. <ref> [GBLP96] </ref> is essentially a main memory algorithm, where all the tuples of the datacube are kept in memory as a k-dimensional array, where k is the number of CUBE BY attributes.
Reference: [Hoa62] <author> C. A. R. Hoare. </author> <title> Quicksort. </title> <journal> Computer Journal, </journal> <volume> 5(1) </volume> <pages> 10-15, </pages> <year> 1962. </year>
Reference-contexts: In the graphs below we assume a disk transfer rate of 1.5 MB/sec, as in [SAG96, AAD + 96]. Sorting was performed in-place (on pointers to tuples) using quicksort <ref> [Hoa62] </ref>.
Reference: [HRU96] <author> V. Harinarayan, A. Rajaraman, and J. D. Ull-man. </author> <title> Implementing data cubes efficiently. </title> <booktitle> In Proceedings of the ACM SIGMOD Conference on Management of Data, </booktitle> <pages> pages 205-216, </pages> <year> 1996. </year>
Reference-contexts: The computation of the various cuboids are not independent of each other, but are closely related in that some of them can be computed using others. These relationships are captured in terms of the search lattice for the datacube <ref> [HRU96] </ref>.
Reference: [HWL94] <author> C. J. Hahn, S. G. Warren, and J. </author> <title> London. Edited synoptic cloud reports from ships and land stations over the globe, </title> <type> 1982-1991. </type> <note> Available from http://cdiac.esd.ornl.gov/cdiac/ndps/ndp026b.html, 1994. </note>
Reference-contexts: Better would be to give unique integer identifiers to the values of each type, perform the datacube over the identifiers, and then reconstitute the values at the end. sized fragment would have CPU characteristics similar to Example 6.3 In this example we use some real-world data on cloud coverage <ref> [HWL94] </ref>.
Reference: [SAG96] <author> S. Sarawagi, R. Agrawal, and A. Gupta. </author> <title> On computing the data cube. </title> <type> Technical Report RJ10026, </type> <institution> IBM Almaden Research Center, </institution> <address> San Jose, CA, </address> <year> 1996. </year>
Reference-contexts: The various existing techniques for computing dat-acubes, outlined below, attempt to make use of the relationships between the cuboids to compute the datacube Q more efficiently than the naive approach of independently computing each of the 2 k cuboids. 2.1 PIPESORT The PIPESORT algorithm proposed by Sarawagi et al. <ref> [SAG96, AAD + 96] </ref> tries to optimize the overall cost of the computation of datacube Q using cost estimates of various ways to compute each cuboid Q ( ~ B j ) to determine which cuboid will be used to actually compute the tuples of Q ( ~ B j ). <p> In the graphs below we assume a disk transfer rate of 1.5 MB/sec, as in <ref> [SAG96, AAD + 96] </ref>. Sorting was performed in-place (on pointers to tuples) using quicksort [Hoa62].
Reference: [Tra95] <institution> Transaction Processing Performance Council (TPC), </institution> <address> 777 N. First Street, Suite 600, San Jose, CA 95112, USA. </address> <booktitle> TPC Benchmark D (Decision Support), </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: The benefit increases with the number of CUBE BY attributes as the number of common attributes between consecutive sort orders increases. 2 Example 6.2 In this example we consider data based on the TPC-D benchmark <ref> [Tra95] </ref>. The benchmark considers data proportional to a scale factor sf . We vary sf from 0:0001 to 0:1 which corresponds to a LINEITEM table of size 600 to 600; 000 tuples. <p> The respective cardinalities of these attributes are: sf fl 1500000, sf fl 200000, sf fl 10000, 5110, 5110, 5110, 7, 2, and 2. See <ref> [Tra95] </ref> for a description of the schema for the LINEITEM table; for example, the value 5110 corresponds to the number of days in a 14-year period. Data is chosen uniformly from each range independently. CUBE BY attributes.
Reference: [ZDN97] <author> Y. Zhao, P. M. Deshpande, and J. F. Naughton. </author> <title> An array-based algorithm for simultaneous multidimensional aggregates. </title> <booktitle> In Proceedings of the ACM SIGMOD Conference on Management of Data, </booktitle> <pages> pages 159-170, </pages> <year> 1997. </year>
Reference-contexts: The data structures needed by Gray et al.'s algorithm will often not fit into memory for sparse relations, even when R does. In this case, the algorithm does not apply. The array-based algorithm proposed by Zhao et al. <ref> [ZDN97] </ref> overcomes some of the limitations of Gray et al.'s algorithm. The data is partitioned and processed in an order that requires only fragments of the array to be present in memory at any one time. Data compression is also used to speed up the I/O. The algorithm of [ZDN97] performs <p> al. <ref> [ZDN97] </ref> overcomes some of the limitations of Gray et al.'s algorithm. The data is partitioned and processed in an order that requires only fragments of the array to be present in memory at any one time. Data compression is also used to speed up the I/O. The algorithm of [ZDN97] performs particularly well because the array representation allows direct access to the needed cells. In the present paper we consider real-world data sets where the data is orders of magnitude more sparse than the synthetic data sets considered in [ZDN97]. For extremely sparse data, the array representation of [ZDN97] cannot <p> The algorithm of <ref> [ZDN97] </ref> performs particularly well because the array representation allows direct access to the needed cells. In the present paper we consider real-world data sets where the data is orders of magnitude more sparse than the synthetic data sets considered in [ZDN97]. For extremely sparse data, the array representation of [ZDN97] cannot fit into memory, and so a more costly data structure would be necessary. 3 Algorithm Partitioned-Cube Our solution for the fast computation of the datacube is based on two fundamental ideas that have been successfully used for performing complex operations <p> of <ref> [ZDN97] </ref> performs particularly well because the array representation allows direct access to the needed cells. In the present paper we consider real-world data sets where the data is orders of magnitude more sparse than the synthetic data sets considered in [ZDN97]. For extremely sparse data, the array representation of [ZDN97] cannot fit into memory, and so a more costly data structure would be necessary. 3 Algorithm Partitioned-Cube Our solution for the fast computation of the datacube is based on two fundamental ideas that have been successfully used for performing complex operations (such as sorting and joins) over very large relations:
References-found: 9

