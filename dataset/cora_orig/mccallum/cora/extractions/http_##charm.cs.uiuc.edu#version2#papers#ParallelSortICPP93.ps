URL: http://charm.cs.uiuc.edu/version2/papers/ParallelSortICPP93.ps
Refering-URL: http://charm.cs.uiuc.edu/version2/papers/ParallelSortICPP93.html
Root-URL: http://www.cs.uiuc.edu
Email: E-mail: kale@cs.uiuc.edu E-mail: sanjeev@cs.uiuc.edu  
Title: A COMPARISON BASED PARALLEL SORTING ALGORITHM  
Author: Laxmikant V. Kale Sanjeev Krishnan 
Address: Urbana, IL 61801 Urbana, IL 61801  
Affiliation: Department of Computer Science Department of Computer Science University of Illinois University of Illinois  
Abstract: We present a fast comparison based parallel sorting algorithm that can handle arbitrary key types. Data movement is the major portion of sorting time for most algorithms in the literature. Our algorithm is parameterized so that it can be tuned to control data movement time, especially for large data sets. Parallel histograms are used to partition the key set exactly. The algorithm is architecture independent, and has been implemented in the CHARM portable parallel programming system, allowing it to be efficiently run on virtually any MIMD computer. Performance results for sorting different data sets are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Abali, F. Ozguner, and A. Bataineh. </author> <title> Load balanced sort on hypercube multiprocessors. </title> <booktitle> In Proc. Fifth Distributed Memory Computing Conference, </booktitle> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: The data within each processor must be in sorted order. 2.1 Overview of algorithm The basic structure of the algorithm is similar to sample sort [4, 5, 6], load balanced sort <ref> [1] </ref>, hyperquicksort [10] and binsort [11], in that in each phase, it finds k 1 "splitter" keys that partition the linear order of keys into k equal partitions. These keys are found by an initial local sort on each processor followed by repeated iterations of global histogramming (Section 2.2). <p> In our algorithm, we can set k equal to p, so that only one move of data is required. Even if we set k = p so that two moves are required, the second move involves only p p processors, hence will have less cost. Load balanced sort <ref> [1] </ref> has the same high level steps as our algorithm. However, our data partitioning step avoids the transpose operation, and moreover, can use more probes than partitions, for faster convergence.
Reference: [2] <author> M. Baber. </author> <title> An implementation of the radix sorting algorithm on the Touchstone Delta Prototype. </title> <booktitle> In Proc. Sixth Distributed Memory Computing Conference, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: The timings in all tables do not include startup, data generation and correctness checking times. From the table we can see good speedups as the number of processors increases. The results for the iPSC/860 compare well with other results reported in the literature <ref> [3, 2] </ref>. Considering the fact that parallel sorting is inherently a communication intensive application, these results demonstrate that our algorithm successfully reduces communication. Table 1: Histogram Sort Basic Timings on the nCUBE/2 and iPSC/860. The keys are integers obtained by averaging 4 sets of random integers. <p> Local Hist Move Total Partitions, Sort Time Phases (%) (%) (%) (s) 128, 4, 4 43.8 2.2 12.6 7.27 128, 16, 2 49.5 2.7 8.1 6.44 5 Previous work The fast parallel sorting algorithms reported in the literature have been mostly non comparison-based ones, such as those based on radix-sort <ref> [2, 9] </ref>. Even with lexicographically-ordered data (such as names), radix sort based methods are inefficient or impossible to use, if the length of the keys is large and variable (and possibly unbounded). In addition, for non lexicographically-ordered data, such methods cannot be used at all.
Reference: [3] <author> D. Bailey, E. Barszcz, L. Dagum, and H. Simon. </author> <title> NAS parallel benchmark results. </title> <booktitle> In Proc. Supercomputing, </booktitle> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: The data set consists of 2 23 integers formed by averaging four sets of random numbers as in the NAS Integer Sort Benchmark <ref> [3] </ref>. All random numbers for these measurements were generated using the C library function lrand48. <p> The timings in all tables do not include startup, data generation and correctness checking times. From the table we can see good speedups as the number of processors increases. The results for the iPSC/860 compare well with other results reported in the literature <ref> [3, 2] </ref>. Considering the fact that parallel sorting is inherently a communication intensive application, these results demonstrate that our algorithm successfully reduces communication. Table 1: Histogram Sort Basic Timings on the nCUBE/2 and iPSC/860. The keys are integers obtained by averaging 4 sets of random integers.
Reference: [4] <author> G. Blelloch et al. </author> <title> A comparison of sorting algorithms for the Connection Machine CM-2. </title> <booktitle> In Proc. Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991. </year>
Reference-contexts: In other words, processor 0 must have the smallest n=p keys, processor 1 must have the next n=p keys, and so on. The data within each processor must be in sorted order. 2.1 Overview of algorithm The basic structure of the algorithm is similar to sample sort <ref> [4, 5, 6] </ref>, load balanced sort [1], hyperquicksort [10] and binsort [11], in that in each phase, it finds k 1 "splitter" keys that partition the linear order of keys into k equal partitions. <p> Finally, our algorithm does not depend on the topology of the underlying machine. Our algorithm has the same high level steps as sample sort <ref> [4, 5, 6] </ref>, with some important differences : * Data Partitioning in our algorithm is exact; the time (number of histogram iterations) taken depends on the input distribution : for uniform distributions, one or two iterations are enough, hence an almost exact partition can be found quickly. <p> Sample sort does not optimize the uniform case. * Sample sort is not scalable, because the size of the messages that need to be communicated is samplesize fl O (p), which is usually 64 fl p keys that could be large <ref> [4] </ref>. In our algorithm, all messages going up and down the spanning tree have size O (k), which in the worst case is 5flk integers, and the height of the spanning tree is O (log (p)). No keys are moved except in the actual data movement step.
Reference: [5] <author> W. Fraser and A. </author> <title> McKellar. Samplesort : A sampling approach to minimal storage tree sorting. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 17(3), </volume> <month> July </month> <year> 1970. </year>
Reference-contexts: In other words, processor 0 must have the smallest n=p keys, processor 1 must have the next n=p keys, and so on. The data within each processor must be in sorted order. 2.1 Overview of algorithm The basic structure of the algorithm is similar to sample sort <ref> [4, 5, 6] </ref>, load balanced sort [1], hyperquicksort [10] and binsort [11], in that in each phase, it finds k 1 "splitter" keys that partition the linear order of keys into k equal partitions. <p> Finally, our algorithm does not depend on the topology of the underlying machine. Our algorithm has the same high level steps as sample sort <ref> [4, 5, 6] </ref>, with some important differences : * Data Partitioning in our algorithm is exact; the time (number of histogram iterations) taken depends on the input distribution : for uniform distributions, one or two iterations are enough, hence an almost exact partition can be found quickly.
Reference: [6] <author> J. Huang and Y. Chow. </author> <title> Parallel sorting and data partitioning by sampling. </title> <booktitle> In Proc. Seventh International Computer Software and Applications Conference, </booktitle> <month> Nov. </month> <year> 1983. </year>
Reference-contexts: In other words, processor 0 must have the smallest n=p keys, processor 1 must have the next n=p keys, and so on. The data within each processor must be in sorted order. 2.1 Overview of algorithm The basic structure of the algorithm is similar to sample sort <ref> [4, 5, 6] </ref>, load balanced sort [1], hyperquicksort [10] and binsort [11], in that in each phase, it finds k 1 "splitter" keys that partition the linear order of keys into k equal partitions. <p> Finally, our algorithm does not depend on the topology of the underlying machine. Our algorithm has the same high level steps as sample sort <ref> [4, 5, 6] </ref>, with some important differences : * Data Partitioning in our algorithm is exact; the time (number of histogram iterations) taken depends on the input distribution : for uniform distributions, one or two iterations are enough, hence an almost exact partition can be found quickly.
Reference: [7] <author> L. Kale. </author> <title> The Chare Kernel parallel programming language and system. </title> <booktitle> In Proc. International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: Using the above result, the total time taken by the algorithm is : local sort time + dlog k (p)efO (log (n))(time per histogram) + data movement and merging time g 4 Implementation and results We have implemented our algorithm in the CHARM portable parallel programming system <ref> [7] </ref>. CHARM supports C with a few extensions for creation of tasks and message passing. CHARM has a message driven model of execution, allowing overlap of computation and communication.
Reference: [8] <author> C. Shannon and W. Weaver. </author> <title> The Mathematical Theory of Communication. </title> <publisher> University of Illinois Press, </publisher> <address> Urbana, </address> <year> 1949. </year>
Reference-contexts: In general, uniform, random distributions are easier to sort as compared to non-uniform distri butions having significant amounts of data concentrated in small value ranges. Entropy <ref> [9, 8] </ref> has been suggested as a metric of distribution. Informally, the entropy of a key set corresponds to the number of "unique" bits in the key.
Reference: [9] <author> K. Thearling and S. Smith. </author> <title> An improved supercomputer sorting benchmark. </title> <booktitle> In Proc. Supercomputing, </booktitle> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: In general, uniform, random distributions are easier to sort as compared to non-uniform distri butions having significant amounts of data concentrated in small value ranges. Entropy <ref> [9, 8] </ref> has been suggested as a metric of distribution. Informally, the entropy of a key set corresponds to the number of "unique" bits in the key. <p> Distribution D1 is a uniform random distribution. Distribution D2 was generated by averaging four sets of random numbers. Distribution D3 has entropy 25.95 and was generated by performing a bitwise AND operation on two sets of random integers <ref> [9] </ref>. Distribution D4 has only 4 distinct values for the most significant 16 bits, while the least significant 16 bits have uniformly distributed random values. Distribution D4 has entropy 10.78 and was generated by doing an AND on four sets of random integers. <p> Local Hist Move Total Partitions, Sort Time Phases (%) (%) (%) (s) 128, 4, 4 43.8 2.2 12.6 7.27 128, 16, 2 49.5 2.7 8.1 6.44 5 Previous work The fast parallel sorting algorithms reported in the literature have been mostly non comparison-based ones, such as those based on radix-sort <ref> [2, 9] </ref>. Even with lexicographically-ordered data (such as names), radix sort based methods are inefficient or impossible to use, if the length of the keys is large and variable (and possibly unbounded). In addition, for non lexicographically-ordered data, such methods cannot be used at all.
Reference: [10] <author> B. Wagar. Hyperquicksort: </author> <title> A fast sorting algorithm for hypercubes. </title> <booktitle> In Proc. Second Conference on Hypercube Multiprocessors, </booktitle> <month> Sept. </month> <year> 1986. </year>
Reference-contexts: The data within each processor must be in sorted order. 2.1 Overview of algorithm The basic structure of the algorithm is similar to sample sort [4, 5, 6], load balanced sort [1], hyperquicksort <ref> [10] </ref> and binsort [11], in that in each phase, it finds k 1 "splitter" keys that partition the linear order of keys into k equal partitions. These keys are found by an initial local sort on each processor followed by repeated iterations of global histogramming (Section 2.2).
Reference: [11] <author> Y. Won and S. Sahni. </author> <title> A balanced bin sort for hypercube multicomputers. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 435-448, </pages> <year> 1988. </year>
Reference-contexts: The data within each processor must be in sorted order. 2.1 Overview of algorithm The basic structure of the algorithm is similar to sample sort [4, 5, 6], load balanced sort [1], hyperquicksort [10] and binsort <ref> [11] </ref>, in that in each phase, it finds k 1 "splitter" keys that partition the linear order of keys into k equal partitions. These keys are found by an initial local sort on each processor followed by repeated iterations of global histogramming (Section 2.2).
References-found: 11

