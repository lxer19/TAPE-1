URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/identification.ps.Z
Refering-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Root-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Email: E-mail: mschmitt@igi.tu-graz.ac.at  
Title: Identification Criteria and Lower Bounds for Perceptron-like Learning Rules  
Author: Michael Schmitt 
Address: Klosterwiesgasse 32/2 A-8010 Graz, Austria  
Affiliation: Institute for Theoretical Computer Science Technische Universitat Graz  
Abstract: Perceptron-like learning rules are known to require exponentially many correction steps in order to identify Boolean threshold functions exactly. We introduce criteria that are weaker than exact identification and investigate whether learning becomes significantly faster if exact identification is replaced by one of these criteria: PAC identification, order identification, and sign identification. PAC identification is based on the learning paradigm introduced by Valiant and known to be easier than exact identification. Order identification uses the fact that each threshold function induces an ordering relation on the input variables which can be represented by weights of linear size. Sign identification is based on a property of threshold functions known as unateness and requires only weights of constant size. We show that Perceptron-like learning rules cannot satisfy these criteria when the number of correction steps is to be bounded by a polynomial. We also present an exponential lower bound for order identification with the learning rules introduced by Littlestone. Our results show that efficiency imposes severe restrictions on what can be learned with local learning rules. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D. </author> <year> 1988. </year> <title> Queries and concept learning. </title> <booktitle> Machine Learning 2, </booktitle> <pages> 319-342. </pages>
Reference: <author> Anthony, M., Brightwell, G., and Shawe-Taylor, J. </author> <year> 1995. </year> <title> On specifying Boolean functions by labelled examples. </title> <booktitle> Discrete Applied Mathematics 61, </booktitle> <pages> 1-25. </pages>
Reference-contexts: It has been shown that any nested Boolean function f on n inputs has specification number n + 1 <ref> (Anthony et al. 1995) </ref> which means that there exists a set A f0; 1g n of cardinality n + 1 such that any Boolean function g that agrees with f on A is identical with f .
Reference: <author> Anthony, M., and Shawe-Taylor, J. </author> <year> 1993. </year> <title> Using the Perceptron algorithm to find consistent hypotheses. </title> <booktitle> Combinatorics, Probability and Computing 2, </booktitle> <pages> 385-387. </pages>
Reference: <author> Bartlett, P. L., and Williamson, R. C. </author> <year> 1991. </year> <title> Investigating the distribution assumptions in the pac learning model. </title> <editor> In Valiant, L. G., and Warmuth, M. K., editors, </editor> <booktitle> Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pp. 24-32. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. </author> <year> 1989. </year> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery 36, </journal> <pages> 929-965. </pages>
Reference-contexts: On the other hand, it is known that Boolean threshold functions are polynomially PAC learnable by methods of linear programming <ref> (Blumer et al. 1989) </ref>. Therefore, it is worth considering if the algorithms for linear programming, which are quite complicated, can be replaced by the much simpler Perceptron-like rules such that PAC identification can still be achieved in polynomial time. <p> In the definition we use the polynomial p to bound the number of examples and the polynomial q to bound the number of computation steps. For Boolean threshold functions it is known that a polynomial number of examples is sufficient for PAC learning <ref> (Blumer et al. 1989) </ref>. Thus the existence of p is guaranteed. Consequently, in order to show that PAC identification cannot be achieved by a given algorithm, one has to prove that the number of computation steps cannot be bounded by a polynomial.
Reference: <author> Hampson, S.E., and Volper, D. J. </author> <year> 1986. </year> <title> Linear function neurons: Structure and training. </title> <booktitle> Biological Cybernetics 53, </booktitle> <pages> 203-217. </pages>
Reference-contexts: By a more involved proof, Hastad (1994) has defined a function that requires weights at least as large as 2 (n log n) (see also Parberry 1994). Further, 3 it has been shown that a Boolean threshold function has weight complexity at least 1:4 n in the average <ref> (Hampson and Volper 1986) </ref>. To simplify notation we shall assume from now on that all threshold functions have threshold 0. This can be done without loss of generality by introducing a weight w n+1 with value t and constant input 1. <p> However, as we have already mentioned in Section 2, the average weight complexity of a Boolean threshold function is at least 1:4 n <ref> (Hampson and Volper 1986) </ref>. Thus, the polynomial bound on the weights involves a considerable loss of the computational power of single neurons. The lower bound for PAC identification is based on the distribution independence assumption.
Reference: <author> Hastad, J. </author> <year> 1994. </year> <title> On the size of weights for threshold gates. </title> <journal> SIAM Journal on Discrete Mathematics 7, </journal> <pages> 484-492. </pages>
Reference: <author> Kivinen, J., and Warmuth, M. K. </author> <year> 1995. </year> <title> The Perceptron algorithm vs. Winnow: linear vs. logarithmic mistake bounds when few input variables are relevant. </title> <booktitle> In Proceedings of the Eighth Annual Conference on Computational Learning Theory, </booktitle> <pages> pp. 289-296. </pages> <publisher> ACM Press, </publisher> <address> New York. </address>
Reference: <author> Lewis II, P. M. </author> <year> 1966. </year> <title> A lower bound on the number of corrections required for convergence of the single threshold gate adaptive procedure. </title> <journal> IEEE Transactions on Electronic Computers 15, </journal> <pages> 933-935. </pages>
Reference: <author> Littlestone, N. </author> <year> 1988. </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <booktitle> Machine Learning 2, </booktitle> <pages> 285-318. </pages>
Reference: <author> Maass, W. </author> <year> 1994. </year> <title> Perspectives of current research about the complexity of learning on neural nets. </title> <editor> In Roychowdhury, V., Siu, K.-Y., and Orlitsky, A., editors, </editor> <booktitle> Theoretical Advances in Neural Computation and Learning, </booktitle> <pages> pp. 295-336. </pages> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA. </address>
Reference-contexts: A further question concerns the separability of the identification criteria. All criteria considered here are met by polynomial-time algorithms for linear programming, some of which can even be implemented as on-line learning rules <ref> (Maass and Turan 1994) </ref>. Of course, these algorithms achieve exact identification which is the strongest criterion one can imagine.
Reference: <author> Maass, W., and Turan, G. </author> <year> 1994. </year> <title> How fast can a threshold gate learn? In Hanson, </title> <editor> S. J., Drastal, G., and Rivest, R., editors, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems: Constraints and Prospects, </booktitle> <pages> pp. 381-414. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <note> 13 McCulloch, </note> <author> W. S., and Pitts, W. </author> <year> 1943. </year> <title> A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics 5, </journal> <pages> 115-133. </pages>
Reference-contexts: A further question concerns the separability of the identification criteria. All criteria considered here are met by polynomial-time algorithms for linear programming, some of which can even be implemented as on-line learning rules <ref> (Maass and Turan 1994) </ref>. Of course, these algorithms achieve exact identification which is the strongest criterion one can imagine.
Reference: <author> McNaughton, R. </author> <year> 1961. </year> <title> Unate truth functions. </title> <journal> IRE Transactions on Electronic Computers 10, </journal> <pages> 1-6. </pages>
Reference: <author> Minsky, M. L., and Papert, S. A. </author> <year> 1988. </year> <title> Perceptrons: An Introduction to Computational Geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <note> expanded edition. </note>
Reference: <author> Muroga, S. </author> <year> 1971. </year> <title> Threshold Logic and Its Applications. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: For any threshold function the relation defined above provides a total ordering of the variables. This is stated in the following lemma by Muroga (1971). It also estab lishes a relationship between the ordering of variables and the ordering of weights. We quote it without proof. Lemma 1 <ref> (Muroga 1971, Theorem 5.1.4) </ref> Let (w 1 ; : : : ; w n ; t) be a weight vector for the threshold function f : f0; 1g n ! f0; 1g. (i) The variables of f are totally ordered with respect to f . f O x j , then
Reference: <author> Nagy, G. </author> <year> 1991. </year> <title> Neural networks|then and now. </title> <journal> IEEE Transactions on Neural Networks 2, </journal> <pages> 316-318. </pages>
Reference: <author> Novikoff, A. </author> <year> 1962. </year> <title> On convergence proofs for Perceptrons. </title> <booktitle> In Symposium on Mathematical Theory of Automata, </booktitle> <pages> pp. 615-622. </pages> <institution> Polytechnic Institute of Brooklyn. </institution>
Reference: <author> Palm, G. </author> <year> 1991. </year> <title> Memory capacities of local rules for synaptic modification: A comparative review. </title> <booktitle> Concepts in Neuroscience 2, </booktitle> <pages> 97-128. </pages>
Reference: <author> Parberry, I. </author> <year> 1994. </year> <title> Circuit Complexity and Neural Networks. </title> <booktitle> Foundations of Computing Series, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Several functions of weight complexity at least 2 (n) have been constructed (Muroga 1971; Minsky and Papert 1988; Parberry 1994). By a more involved proof, Hastad (1994) has defined a function that requires weights at least as large as 2 (n log n) <ref> (see also Parberry 1994) </ref>. Further, 3 it has been shown that a Boolean threshold function has weight complexity at least 1:4 n in the average (Hampson and Volper 1986). To simplify notation we shall assume from now on that all threshold functions have threshold 0. <p> the i-th Fibonacci number a i = a i1 + a i2 ; a 1 = a 2 = 1: (1) According to Hampson and Volper (1986) this function has weight complexity at least (1:6 n ), more precisely (OE n ) where OE is the golden ratio OE = <ref> ( p also Parberry 1994) </ref>. As the function can be written in the form ( (((x 1 ^ x 2 ) _ x 3 ) ^ x 4 ) ) _ x n it belongs to the class of so-called nested functions defined by Anthony et al. (1995). <p> The nested function f we have used is different from that in Anthony and Shawe-Taylor (1993). They show that the largest weight of their function has to be at least ( p 3) n=21 , whereas for f the slightly better bound a n is known <ref> (Parberry 1994) </ref>. 4 Order Identification The result in the previous section shows that not only exact but also PAC identification requires to generate large weights. Therefore, to be able to learn with Perceptron-like rules in polynomial time one has to look for identification criteria that avoid this obstacle.
Reference: <author> Paull, M. C., and McCluskey, Jr., E. J. </author> <year> 1960. </year> <title> Boolean functions realizable with single threshold devices. </title> <booktitle> Proceedings of the IRE 48, </booktitle> <pages> 1335-1337. </pages>
Reference-contexts: For non-degenerate threshold functions they have also established a relationship between the property of being positive, resp. negative, in x i and the sign of weight w i . We quote the result, which can also be found in McNaughton (1961), without proof. Lemma 2 <ref> (Paull and McCluskey, Jr. 1960, Theorem 1) </ref> Every Boolean threshold function is unate. Let f : f0; 1g n ! f0; 1g be non-degenerate and (w 1 ; : : : ; w n+1 ) a weight vector representing f .
Reference: <author> Pitt, L., and Valiant, L. G. </author> <year> 1988. </year> <title> Computational limitations on learning from examples. </title> <journal> Journal of the Association for Computing Machinery 35, </journal> <pages> 965-984. </pages>
Reference: <author> Rosenblatt, F. </author> <year> 1958. </year> <title> The Perceptron: A probabilistic model for information storage and organization in the brain. </title> <journal> Psychological Review 65, </journal> <pages> 386-408. </pages>
Reference: <author> Rosenblatt, F. </author> <year> 1962. </year> <title> Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. </title> <publisher> Spartan Books, </publisher> <address> Washington, DC. </address>
Reference: <author> Schmitt, M., </author> <year> 1994. </year> <title> On the size of weights for McCulloch-Pitts neurons. </title> <editor> In Ca-ianiello, E. R., editor, </editor> <booktitle> Proceedings of the Sixth Italian Workshop on Neural Nets WIRN VIETRI-93, </booktitle> <pages> pp. 241-246. </pages> <publisher> World Scientific, Singapore. </publisher>
Reference-contexts: The weight complexity of Boolean threshold functions where the truth values are represented as f0; 1g is known to be at most 2 (n+1) (n + 1) (n+3)=2 + 1=2 <ref> (Schmitt 1994) </ref>. For bipolar inputs f1; 1g the more succinct bound 2 n (n + 1) (n+1)=2 has been shown (see also Hastad 1994; Parberry 1994). Several functions of weight complexity at least 2 (n) have been constructed (Muroga 1971; Minsky and Papert 1988; Parberry 1994).
Reference: <author> Valiant, L. G. </author> <year> 1984. </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM 27, </journal> <pages> 1134-1142. </pages>
References-found: 25

