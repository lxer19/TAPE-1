URL: ftp://ftp.cs.brown.edu/pub/techreports/92/cs92-48.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-92-48.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [AAC] <author> Alok Aggarwal, Bowen Alpern, Ashok K. Chandra, and Marc Snir, </author> <title> "A Model for Hierarchical Memory," </title> <booktitle> Proceedings of 19th Annual ACM Symposium on Theory of Computing (May 1987), </booktitle> <pages> 305-314. </pages>
Reference-contexts: Aggarwal et al. advanced the first model of memory hierarchies, HMM, on which they considered special-purpose algorithms, including matrix multiplication, FFT, and sorting <ref> [AAC] </ref>. Aggarwal et al. also proposed a follow-up model called BT in which they added blocking considerations to the HMM memory model [ACSa]. Alpern et al. gave a somewhat more constrained model of memory hierarchies, the UMH model, with interesting blocking issues and parallel transfer between levels [ACF]. <p> The more general model, in which the internal processing is done on a CRCW PRAM with P processors, is shown in 4.1.2 Parallel multilevel hierarchies The simplest multilevel hierarchy model is the Hierarchical Memory Model (HMM) proposed by Aggarwal et al. <ref> [AAC] </ref>, depicted in Figure 4.2a. In the HMM f (x) model, access to memory location x takes f (x) time. Figure 4.2a suggests the HMM dlog xe model, where each layer in the hierarchy has a size that is some constant multiple of the size of the previous layer. <p> Several interesting and elegant hierarchical memory models have been proposed recently to model the many levels of memory typically found in large-scale computer systems. The HMM model of Aggarwal, Alpern, Chandra, and Snir <ref> [AAC] </ref> views the entire memory hierarchy as a linear address space and allows access to individual location x in time f (x). <p> Optimal sorting algorithms for each of these models have been developed <ref> [AAC, ACSa, LuP] </ref>, and optimal algorithms for the parallelizations of the HMM and BT models was given in Chapter 4. <p> Vitter and Shriver introduced optimal randomized sorting algorithms for P-HMM and P-BT [ViSa]. The algorithms were based on their randomized two-level partitioning technique applied to the optimal single-hierarchy algorithms for HMM and BT developed in <ref> [AAC, ACSa] </ref>. In Chapter 4, we gave optimal deterministic sorting algorithms for P-HMM and P-BT. <p> Theorems 14 and 15); accordingly the upper and lower bounds combine in an interesting way several techniques from <ref> [AAC, ACSa, AgV, ViSa] </ref>. Theorem 22 The running times mentioned in Theorem 21 are matching upper and lower bounds for sorting in P-RUMH. The algorithms for nonconstant H for the first two bandwidth cases are based on Balance Sort.
Reference: [ACSa] <author> Alok Aggarwal, Ashok K. Chandra, and Marc Snir, </author> <title> "Hierarchical Memory with Block Transfer," </title> <booktitle> Proceedings of 28th Annual IEEE Symposium on Foundations of Computer Science (October 1987), </booktitle> <pages> 204-216. </pages>
Reference-contexts: Aggarwal et al. advanced the first model of memory hierarchies, HMM, on which they considered special-purpose algorithms, including matrix multiplication, FFT, and sorting [AAC]. Aggarwal et al. also proposed a follow-up model called BT in which they added blocking considerations to the HMM memory model <ref> [ACSa] </ref>. Alpern et al. gave a somewhat more constrained model of memory hierarchies, the UMH model, with interesting blocking issues and parallel transfer between levels [ACF]. <p> Our model is pessimistic in that f (x) could grow much slower or not at all. An elaboration of HMM is the Block Transfer (BT) model of Aggarwal et al. <ref> [ACSa] </ref>, depicted schematically in Figure 4.2b. Like HMM, it has a cost function f (x), but additionally it simulates the effect of block transfer by allowing the ` + 1 locations x `; . . . ; x to be accessed for cost f (x) + `. <p> The only difference is that in Algorithm 10, we need to add another line right after step (6) to reposition all the buckets into consecutive locations on each virtual memory hierarchy. This repositioning is done on a virtual-hierarchy-by-virtual-hierarchy basis, using the generalized matrix transposition algorithm given in <ref> [ACSa] </ref>. We concentrate in this section on the cost function f (x) = x ff , where 0 &lt; ff &lt; 1. Deterministic algorithms for f (x) = x ff with ff 1 have been reported previously [ViSa]. <p> We need to make one more change to the algorithm for BT hierarchies, but one that is hard to write explicitly. Aggarwal et al. gave an algorithm called the "touch" algorithm <ref> [ACSa] </ref>. This algorithm takes an array of n consecutive records stored at the lowest possible level and passes them through the base memory level in order, using time O (n log log n) for 0 &lt; ff &lt; 1. <p> Accordingly, data are usually transferred in large units of blocks. The BT model of Aggarwal, Chandra, and Snir <ref> [ACSa] </ref> represents a notion of block transfer applied to HMM; in the BT model, access to the t + 1 records at locations x t, x t + 1, . . . , x takes time f (x) + t. <p> Optimal sorting algorithms for each of these models have been developed <ref> [AAC, ACSa, LuP] </ref>, and optimal algorithms for the parallelizations of the HMM and BT models was given in Chapter 4. <p> Vitter and Shriver introduced optimal randomized sorting algorithms for P-HMM and P-BT [ViSa]. The algorithms were based on their randomized two-level partitioning technique applied to the optimal single-hierarchy algorithms for HMM and BT developed in <ref> [AAC, ACSa] </ref>. In Chapter 4, we gave optimal deterministic sorting algorithms for P-HMM and P-BT. <p> Theorems 14 and 15); accordingly the upper and lower bounds combine in an interesting way several techniques from <ref> [AAC, ACSa, AgV, ViSa] </ref>. Theorem 22 The running times mentioned in Theorem 21 are matching upper and lower bounds for sorting in P-RUMH. The algorithms for nonconstant H for the first two bandwidth cases are based on Balance Sort.
Reference: [AgP] <author> Alok Aggarwal and James Park, </author> <title> "Notes on Searching in Multidimensional Monotone Arrays," </title> <booktitle> Proceedings of 29th Annual IEEE Symposium on Foundations of Computer Science (October 1988), </booktitle> <pages> 497-512. </pages>
Reference-contexts: There are many applications for using blocks efficiently in external graph searching. These include A.I. searching in constraint networks, robot motion planning, the simulation of large Deterministic Finite Automata (DFA), browsing in hypertext applications, accesses in object-oriented databases, and some matrix algorithms such as searching in monotone arrays <ref> [AgP] </ref>. Sometimes the graphs are well-structured, as in the case of matrix searches and robot motion planning in a space discretized in a grid (which gives rise to grid graphs). At other times the graphs are unstructured, as in A.I. searching and DFA simulation.
Reference: [AgV] <author> Alok Aggarwal and Jeffrey Scott Vitter, </author> <title> "The Input/Output Complexity of Sorting and Related Problems," </title> <journal> Communications of the ACM (September 1988), </journal> <pages> 1116-1127. </pages>
Reference-contexts: Savage and Vitter examined the effects of block size on a number of problems, including FFT and matrix multiplication [SaV]. Aggarwal and Vitter introduced a parallel disk model and gave algorithms for matrix multiplication, permuting, FFT, and sorting in their model <ref> [AgV] </ref>. Vitter and Shriver gave a more realistic parallel disk model and gave optimal algorithms for the same suite of problems as considered by Aggarwal and Vitter [ViSa]. Ullman and Yannakakis investigated the input/output complexity of the transitive closure problem [UlY]. <p> An increasingly popular way to get further speedup is to use many disk drives working in parallel [GHK, GiS, Jil, Mag, PGK, Uni]. Initial work in the use of parallel block transfer for sorting was done by Aggarwal and Vitter <ref> [AgV] </ref>. In their model, they considered the parameters N = # records in the file M = # records that can fit in internal memory B = # records per block D = # blocks transferred per I=O where M &lt; N , and 1 DB M=2. <p> Thus, D blocks can be transferred per I/O, as in the <ref> [AgV] </ref> model, but only if no two blocks access the same disk. This assumption is very reasonable in light of the way real systems are constructed. The measure of performance is the number of parallel I/Os required; internal computation time is ignored. <p> The bottleneck can be expected to be the I/O. Vitter and Shriver presented a randomized version of distribution sort using two complementary partitioning techniques. Their algorithm meets the I/O bound (2.1) for the more lenient model of <ref> [AgV] </ref>, and thus the algorithm is optimal. They posed as an open problem the question of whether there is an optimal algorithm that is deterministic. They also showed that matrix transposition can be done optimally in the D-disk model, achieving the bound (2.2). <p> In the area of parallel computing, Leighton introduced the columnsort algorithm as an optimal sorting algorithm on linear-sized sorting networks [Lei]. It was pointed out in <ref> [AgV] </ref> that columnsort can be used for optimal sorting with respect to I/O if N and B are not too large. Since columnsort is a (non-adaptive) sorting network algorithm, its I/O schedule can be pre-specified independently of the data to take full advantage of parallel block transfer. <p> To provide for the data structures, we need to strengthen the condition imposed by <ref> [AgV] </ref> so that DB 1 2 bM M 1=2+fi c, for Theorem 4 For 0 &lt; fi &lt; 1=2, the amount of primary memory space needed for the data structures of Greed Sort is O (M 1=2+fi ). <p> Since we started with N=M runs and at each pass merged together p M=B=2 of them, the total I/O bound is O N M=B=2 M = O N log N log M ! which is optimal, by the lower bound of <ref> [AgV] </ref>. 2 2.3 Conclusions Greed sort is an optimal deterministic algorithm for external sorting applications that is easy to implement in practice. <p> Theorem 11 The algorithm given is optimal for sorting in the parallel disk model. Proof : The preceding analysis shows that the algorithm achieves the same time bound as was shown in <ref> [AgV] </ref> to be a lower bound for sorting with parallel block transfer. 2 3.2.3 Memory usage of the sorting algorithm One aspect of the algorithm that we still need to show is that we do not exceed the memory requirements of the computer. <p> Thus, we get the recurrence T (N ) = ST N 0 if N M which has solution T (N ) = O N log S M = O N log (N=B) This is the same bound as was shown to be optimal for the parallel disk model <ref> [AgV] </ref>. We devote the remainder of this section to showing that the algorithm operates with optimal internal processing time as long as the number of processors P M log minfM=B; log M g= log M . The optimal internal processing time (assuming comparison-based sorting) is ((N=P ) log N ). <p> Theorems 14 and 15); accordingly the upper and lower bounds combine in an interesting way several techniques from <ref> [AAC, ACSa, AgV, ViSa] </ref>. Theorem 22 The running times mentioned in Theorem 21 are matching upper and lower bounds for sorting in P-RUMH. The algorithms for nonconstant H for the first two bandwidth cases are based on Balance Sort. <p> The sequential time can be at most H times the P-SUMH running time. We superimpose on the P-SUMH model a sequence of one disk, two-level memories of the type studied in <ref> [AgV, ViSa] </ref>, in the following way: For 1 ` 1 2 log (N=ffH), the `th two-level memory has one disk, internal memory size M ` = Hff ( 2 (`+1) 1)=( 2 1), and block size B ` = ` . <p> The minimum number of I/Os required for sorting in the `th two-level memory is N log (N=B ` ) B ` ; as shown in <ref> [AgV] </ref>. Each such I/O contributes C i to the sequential time in the P-SUMH model, since in the P-SUMH model only one level can be active at a time in each hierarchy.
Reference: [Akl] <author> S. G. Akl, </author> <title> Parallel Sorting Algorithms, </title> <booktitle> Notes and Reports in Computer Science and Applied Mathematics #12, </booktitle> <publisher> Academic Press, Inc., </publisher> <address> Orlando, </address> <year> 1985. </year>
Reference-contexts: The fastest oblivious algorithm we have found for sorting in UMH 1=(`+1) is based on a simple schedule of Batcher's bitonic sort <ref> [Akl] </ref> where each of the log 2 N parallel time steps is implemented in O (N log N ) time for an overall running time of O (N log 3 N ).
Reference: [AlR] <author> Romas Aleliunas and Arnold L. Rosenberg, </author> <title> "On Embedding Rectangular Grids in Square Grids," </title> <journal> IEEE Transactions on Computers C-31 (1982), </journal> <pages> 907-913. </pages>
Reference-contexts: Alternatively, all the work done in the database community on B-trees could be viewed as a solution to our problem for complete trees with s = 1. There is considerable previous work on embedding of one type of data structure into another <ref> [AlR, CRS, DEL, Rosa, Rosb, Rosc, RoS] </ref>. This work is relevant because if we view the data structures as graphs, the goal of the embedding is to maintain locality in the original graph when accesses are done according to the target graph.
Reference: [ACF] <author> Bowen Alpern, Larry Carter, and Ephraim Feig, </author> <title> "Uniform Memory Hierarchies," </title> <booktitle> Proceedings of the 31st Annual IEEE Symposium on Foundations of Computer Science (October 1990), </booktitle> <pages> 600-608. </pages>
Reference-contexts: Aggarwal et al. also proposed a follow-up model called BT in which they added blocking considerations to the HMM memory model [ACSa]. Alpern et al. gave a somewhat more constrained model of memory hierarchies, the UMH model, with interesting blocking issues and parallel transfer between levels <ref> [ACF] </ref>. Luccio considered 5 a model called LPM based on pipelined access to memory that contains many of the same considerations as are present in memory hierarchies [LuP]. <p> A possibly more realistic memory hierarchy is the Uniform Memory Hierarchy (UMH) of Alpern et al. <ref> [ACF] </ref>, depicted in Figure 4.2c. This model is described in more detail in Chapter 5. As with two-level hierarchies, multilevel hierarchies can be parallelized. The paral-lelization is done in the same way as for the two-level memory hierarchies, as shown in PRAM with H memory locations. <p> Optimal sorting algorithms for each of these models have been developed [AAC, ACSa, LuP], and optimal algorithms for the parallelizations of the HMM and BT models was given in Chapter 4. In this chapter, we concentrate on a newer hierarchical memory model introduced by Alpern, Carter, and Feig <ref> [ACF, ACSb] </ref>, called the Uniform Memory Hierarchy (UMH), which offers an alternative model of blocked multilevel memories. <p> In Chapter 4, we gave optimal deterministic sorting algorithms for P-HMM and P-BT. We can consider parallel UMH hierarchies (analogous to P-HMM and P-BT), and we call the resulting model P-UMH. (This is fundamentally different from the parallel type of UMH called UPHM mentioned in <ref> [ACF] </ref>.) Each level ` in each of the H hierarchies of the P-UMH holds ff 2` records, for a total of ffH 2` records on level `. <p> For the special case of constant bandwidth, we present a parsimonious algorithm. Since optimal sorting seems to require nonoblivious UMH programs, the oblivious UMH model of <ref> [ACF] </ref> must be modified in a reasonable way. In Theorem 20, we assume that the `th level of the hierarchy can initiate a transfer from the (` + 1)st level without involving the CPU when one of its blocks becomes empty. <p> An earlier version of <ref> [ACF] </ref> introduced a sequential UMH model, appropriately called SUMH, that allowed at most one bus to be active at a time. However, the SUMH restriction can be regarded as too severe, since it forfeits much power of the UMH model.
Reference: [ACSb] <author> Bowen Alpern, Larry Carter, and Ted Selker, </author> <title> "Visualizing Computer Memory Architectures," </title> <booktitle> Proceedings of the First Annual IEEE Conference on Visualization (October 1990), </booktitle> <pages> 107-113. </pages>
Reference-contexts: Optimal sorting algorithms for each of these models have been developed [AAC, ACSa, LuP], and optimal algorithms for the parallelizations of the HMM and BT models was given in Chapter 4. In this chapter, we concentrate on a newer hierarchical memory model introduced by Alpern, Carter, and Feig <ref> [ACF, ACSb] </ref>, called the Uniform Memory Hierarchy (UMH), which offers an alternative model of blocked multilevel memories.
Reference: [Amd] <author> G. </author> <title> Amdahl, "Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities," </title> <booktitle> Proceedings AFIPS 1967 Spring Joint Computer Conference (April 1967), </booktitle> <pages> 483-485. </pages>
Reference-contexts: The recent trend toward parallel computers aggravates the I/O bottleneck further, since improving the performance of one part of a subsystem while leaving other parts unchanged does not take full advantage of those improvements <ref> [Amd] </ref>. It seems that the only way around the I/O bottleneck is to use multiple disks in parallel. This approach is taken, for example, in the TWA reservation system [GiS]. So the second elaboration is being able to handle disk parallelism.
Reference: [Bai] <author> David H. Bailey, </author> <title> "FFTs in External or Hierarchical Memory," </title> <editor> J. </editor> <booktitle> Supercomputing 4 (1990), </booktitle> <pages> 23-35. </pages>
Reference-contexts: Bailey investigated the performance of FFT in external or hierarchical memory, implementing a version of FFT that requires as few as two passes through the data to compute a 2 n -point FFT <ref> [Bai] </ref>. In this thesis, we will occasionally need to consider processor parallelism as well as memory parallelism.
Reference: [Ber] <author> Claude Berge, </author> <title> Graphs and Hypergraphs (second edition), </title> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, </address> <year> 1976. </year>
Reference-contexts: The following lemma gives such a bound with some restrictions on the blocking. We later generalize the lemma to remove any such restrictions. Before presenting the lemma, however, we want to make some observations about hypergraphs <ref> [Ber] </ref>. First, a hypergraph is a pair G = (V; H), where V is a set of vertices and H is a set of subsets of V called edges. If each set H 2 H has cardinality 2, then G is an ordinary graph.
Reference: [BKT] <author> P. Berman, H. J. Karloff, and G. Tardos, </author> <title> "A Competitive 3-Server Algorithm," </title> <booktitle> Proceedings of the First Annual IEEE Symposium on Discrete Algorithms (Jan-uary 1990), </booktitle> <pages> 280-290. 155 </pages>
Reference-contexts: The decision of which block to remove is the source of a number of different algorithms, and this topic has been widely studied <ref> [BKT, CKP, FKL, McS, RaS, SlT] </ref>. Other expedients such as prefetching and clustering [PeS] have been proposed to improve performance.
Reference: [BDH] <author> Dina Bitton, David J. DeWitt, David K. Hsiao, and Jaishankar Menon, </author> <title> "A Taxonomy of Parallel Sorting," </title> <journal> Computing Surveys 16 (September 1984), </journal> <pages> 287-318. </pages>
Reference-contexts: Hong and Kung gave the first general techniques for proving lower bounds on I/O complexity, considering a number of different problems [HoK]. Bitton et al. consider a special case of parallel disk sorting where the processors comprise a tree, with a disk attached to each of the leaf nodes <ref> [BDH, Fri] </ref>. Savage and Vitter examined the effects of block size on a number of problems, including FFT and matrix multiplication [SaV]. Aggarwal and Vitter introduced a parallel disk model and gave algorithms for matrix multiplication, permuting, FFT, and sorting in their model [AgV].
Reference: [BFP] <author> Manuel Blum, Robert W. Floyd, Vaughan Pratt, Ronald L. Rivest, and Robert E. Tarjan, </author> <title> "Time Bounds for Selection," </title> <editor> J. </editor> <booktitle> Computer and System Sciences 7 (1973), </booktitle> <pages> 448-461. </pages>
Reference-contexts: t := djS 0 j=M e Read M elements in parallel (dM=DBe tracks; the last may be partial) Sort internally R i := the median element if t = 1 return (kth element) s := the median of R 1 ; . . . ; R t using algorithm from <ref> [BFP] </ref> Partition into two sets S 1 and S 2 such that S 1 &lt; s and S 2 s k 1 := jS 1 j if k k 1 return (Select (S 1 ; k)) else return (Select (S 2 ; k k 1 )) parallel I/O's, as long as <p> t := djS 0 j=M e Read M elements in parallel (dM=DBe tracks; the last may be partial) Sort internally R i := the median element if t = 1 return (kth element) s := the median of R 1 ; . . . ; R t using algorithm from <ref> [BFP] </ref> Partition into two sets S 1 and S 2 such that S 1 &lt; s and S 2 s k 1 := jS 1 j if k k 1 Select (S 1 ; k) else Select (S 2 ; k k 1 ) Algorithm 19 [ComputePartitionElements (S)] for each memoryload
Reference: [BIR] <author> Allan Borodin, Sandy Irani, Prabhakar Raghavan, and Baruch Schieber, </author> <title> "Competitive Paging with Locality of Reference," </title> <booktitle> Proc. of the 23rd ACM Symposium on Theory of Computing (May 1991), </booktitle> <pages> 249-259. </pages>
Reference-contexts: We know of no previous work for this problem, but there is considerable related work. Borodin et al. considered a similar problem where they were traversing an access graph, but they considered only block size B = 1 and worked on developing an efficient paging algorithm <ref> [BIR] </ref>. Alternatively, all the work done in the database community on B-trees could be viewed as a solution to our problem for complete trees with s = 1. There is considerable previous work on embedding of one type of data structure into another [AlR, CRS, DEL, Rosa, Rosb, Rosc, RoS].
Reference: [Car] <author> David A. Carlson, </author> <title> "Using Local Memory to Boost the Performance of FFT Algorithms on the CRAY-2 Supercomputer," </title> <editor> J. </editor> <booktitle> Supercomputing 4 (1990), </booktitle> <pages> 345-356. </pages>
Reference-contexts: Ullman and Yannakakis investigated the input/output complexity of the transitive closure problem [UlY]. Carlson showed how to use local memory judiciously in order to compute the FFT efficiently on a CRAY-2 computer <ref> [Car] </ref>. Cormen gave fast algorithms for permuting in the Vitter-Shriver parallel disk model [Cor]. It is sometimes unrealistic to deal with a computer's memory as comprising only two layers.
Reference: [CGK] <author> Peter Chen, Garth Gibson, Randy H. Katz, David A. Patterson, and Martin Schulze, </author> <title> "Two Papers on RAIDs," </title> <editor> U. C. </editor> <address> Berkeley, UCB/CSD 88/479, </address> <month> Decem-ber </month> <year> 1988. </year>
Reference-contexts: A second elaboration to the two-level memory hierarchy is motivated because information processing is currently facing what several researchers have called an input/output (I/O) crisis <ref> [CGK, Che, PGK] </ref>. CPU speeds have been increasing approximately exponentially over the last decade, as have memory capacities [PGK]. Unfortunately, disk speeds have seen little improvement in rotational latency and only a halving of seek times [CGK]. <p> CPU speeds have been increasing approximately exponentially over the last decade, as have memory capacities [PGK]. Unfortunately, disk speeds have seen little improvement in rotational latency and only a halving of seek times <ref> [CGK] </ref>. Even if we consider the improvements in disk technology to be exponential, it is of a lower order than that seen by processors.
Reference: [Che] <author> Peter M. Chen, </author> <title> "An Evaluation of Redundant Arrays of Disks using an Amdahl 5890," </title> <editor> U. C. </editor> <address> Berkeley, UCB/CSD 89/506, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: A second elaboration to the two-level memory hierarchy is motivated because information processing is currently facing what several researchers have called an input/output (I/O) crisis <ref> [CGK, Che, PGK] </ref>. CPU speeds have been increasing approximately exponentially over the last decade, as have memory capacities [PGK]. Unfortunately, disk speeds have seen little improvement in rotational latency and only a halving of seek times [CGK].
Reference: [CKP] <author> M. Chrobak, H. Karloff, T. Payne, and S. Vishwanathan, </author> <title> "New Results on Server Problems," </title> <booktitle> Proceedings of the First Annual IEEE Symposium on Discrete Algorithms (January 1990), </booktitle> <pages> 291-300. </pages>
Reference-contexts: The decision of which block to remove is the source of a number of different algorithms, and this topic has been widely studied <ref> [BKT, CKP, FKL, McS, RaS, SlT] </ref>. Other expedients such as prefetching and clustering [PeS] have been proposed to improve performance.
Reference: [CRS] <author> F.R.K. Chung, A. L. Rosenberg, and Lawrence Snyder, </author> <title> "Perfect Storage Representations for Families of Data Structures," </title> <booktitle> SIAM J. Algorithms and Discrete Methods 4 (1983), </booktitle> <pages> 548-565. </pages>
Reference-contexts: Alternatively, all the work done in the database community on B-trees could be viewed as a solution to our problem for complete trees with s = 1. There is considerable previous work on embedding of one type of data structure into another <ref> [AlR, CRS, DEL, Rosa, Rosb, Rosc, RoS] </ref>. This work is relevant because if we view the data structures as graphs, the goal of the embedding is to maintain locality in the original graph when accesses are done according to the target graph.
Reference: [Col] <author> Richard Cole, </author> <title> "Parallel Merge Sort," </title> <note> SIAM J. Computing 17 (August 1988), 770-785. </note>
Reference-contexts: Proof : To group the records within each bucket together, we sort the records in internal memory. If log M = O (log M=B) then we can sort the M records using Cole's parallel merge sort algorithm <ref> [Col] </ref> in time (M=P ) log M = O ((M=P ) log (M=B)) for any P M . Otherwise, instead of sorting using the keys contained in the records, we sort using the bucket number as the key. We will start by considering only P M= log M .
Reference: [Coo] <author> Stephen A. Cook, </author> <title> "An Observation on Time-Storage Trade Off," </title> <booktitle> Proceedings of the 5th Annual ACM Symposium on Theory of Computation (May 1973), </booktitle> <pages> 29-33. </pages>
Reference-contexts: Appendix A proves a closed form for the optimal number of generations to compute in the multigeneration sweep architecture before viewing the results. Our conclusions are given in Section 6.3. 6.1 Lower Bounds on I/O Overhead Graph pebbling is a powerful technique for proving computational lower bounds <ref> [Coo, HoK, KSS, SaV] </ref>. For nine-cell lattice computations, a general result in Kugelmass et al. can be specialized to 8 ff assuming that each of n 2 processors (e.g., an n fi n array) has ff bits of local storage.
Reference: [Cor] <author> Thomas H. Cormen, </author> <title> "Fast Permuting on Disk Arrays," </title> <booktitle> Proc. Brown/MIT Conference on Advanced Research in VLSI and Parallel Systems (March 1992), </booktitle> <pages> 58-76. </pages>
Reference-contexts: Ullman and Yannakakis investigated the input/output complexity of the transitive closure problem [UlY]. Carlson showed how to use local memory judiciously in order to compute the FFT efficiently on a CRAY-2 computer [Car]. Cormen gave fast algorithms for permuting in the Vitter-Shriver parallel disk model <ref> [Cor] </ref>. It is sometimes unrealistic to deal with a computer's memory as comprising only two layers. Modern computer systems often have many levels of hierarchy such as CPU registers, fast CPU cache, main memory, hardware cache, local fast disk, slower network file servers, and finally archival storage.
Reference: [Cyp] <author> Robert Cypher, </author> <title> "Theoretical Aspects of VLSI Pin Limitations," </title> <institution> IBM Almaden Research Center, </institution> <note> Research Report RJ 7115 (67362), </note> <month> November </month> <year> 1989. </year>
Reference-contexts: However, an area that is not studied as diligently is the I/O needs of VLSI chips. In particular, it is possible for the I/O to become the bottleneck in the use of special-purpose processors. Cypher has examined some of the theoretical aspects of VLSI pin limitations <ref> [Cyp] </ref>. Even in general-purpose computer systems, any algorithm is going to require I/O when the size of the problem gets to be too large for internal memory. <p> The adjustments necessary to obtain optimal performance are the subject of the next chapter. An interesting sorting algorithm in the context of fixed interconnection networks was proposed recently by Cypher and Plaxton <ref> [CyP] </ref>. The algorithm runs in O (log n (log log n) 2 ) time on an n-processor hypercube, shu*e-exchange, or cube-connected cycles network.
Reference: [CyP] <author> Robert Cypher and C. Greg Plaxton, </author> <title> "Deterministic Sorting in Nearly Logarithmic Time on the Hypercube and Related Computers," </title> <note> Journal of Computer and System Sciences (to appear), also appears in Proceedings of the 22nd Annual ACM Symposium on Theory of Computing, </note> <month> (May </month> <year> 1990), </year> <pages> 193-203. </pages>
Reference-contexts: However, an area that is not studied as diligently is the I/O needs of VLSI chips. In particular, it is possible for the I/O to become the bottleneck in the use of special-purpose processors. Cypher has examined some of the theoretical aspects of VLSI pin limitations <ref> [Cyp] </ref>. Even in general-purpose computer systems, any algorithm is going to require I/O when the size of the problem gets to be too large for internal memory. <p> The adjustments necessary to obtain optimal performance are the subject of the next chapter. An interesting sorting algorithm in the context of fixed interconnection networks was proposed recently by Cypher and Plaxton <ref> [CyP] </ref>. The algorithm runs in O (log n (log log n) 2 ) time on an n-processor hypercube, shu*e-exchange, or cube-connected cycles network.
Reference: [DEL] <author> Richard A. DeMillo, Stanley C. Eisenstat, and Richard J. Lipton, </author> <title> "Preserving Average Proximity in Arrays," </title> <booktitle> Communication of the ACM 21 (1978), </booktitle> <pages> 228-231. </pages>
Reference-contexts: Alternatively, all the work done in the database community on B-trees could be viewed as a solution to our problem for complete trees with s = 1. There is considerable previous work on embedding of one type of data structure into another <ref> [AlR, CRS, DEL, Rosa, Rosb, Rosc, RoS] </ref>. This work is relevant because if we view the data structures as graphs, the goal of the embedding is to maintain locality in the original graph when accesses are done according to the target graph. <p> Rosenberg showed that there is no linear mapping scheme that preserves proximity globally in arrays that are extendible in more than one dimension. DeMillo et al. considered the same problem and showed that proximity could be maintained if the storage mechanism was a binary tree <ref> [DEL] </ref>. Both concluded that the standard row-major (or column-major) way of storing arrays was asymptotically optimal for preserving locality in arrays. We find, however, that our earlier hypothesis concerning blocking does not hold even for finite arrays, as long as the array structure is much larger than the memory size.
Reference: [Dew] <author> A. K. Dewdney, </author> <title> "Computer Recreations," </title> <booktitle> Scientific American 252 (May 1985), </booktitle> <pages> 18-30. </pages>
Reference-contexts: Despite the apparent simplicity in having purely local rules govern the time evolution of the system, Life exhibits complex behavior and in fact possesses the same computational power as a Turing machine <ref> [Dew] </ref>. It also admits of a universal constructor to allow self-replicating structures [Pou]. Lattice computations have important applications in physical simulations. Examples of simulations that use such automata are two-dimensional lattice gas computations [KSS], diffusion-limited aggregation [MaT], two-dimensional diffusion, fluid dynamics, spin glasses, and ballistics [ToM].
Reference: [Eve] <author> Shimon Even, </author> <title> "Parallelism in Tape-Sorting," </title> <booktitle> Communications of the ACM 17 (April 1974), </booktitle> <pages> 202-204. </pages>
Reference-contexts: Knuth has 132 pages devoted to the I/O efficiency of sorting, all but 17 of which deal with the problems of using tapes [Knu]. Even considered the problem of sorting using parallel tapes with parallel processors <ref> [Eve] </ref>, but his algorithms are hampered a bit by the fact that he assumes that each of the P processors has exactly 4 tape drives attached to it. Hong and Kung gave the first general techniques for proving lower bounds on I/O complexity, considering a number of different problems [HoK].
Reference: [FKL] <author> Amos Fiat, Richard M. Karp, Michael Luby, Lyle A. McGeoch, Daniel D. Sleator, and Neal E. Young, </author> <title> "Competitive Paging Algorithms," </title> <institution> Carnegie-Mellon University, CMU-CS-88-196, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: The decision of which block to remove is the source of a number of different algorithms, and this topic has been widely studied <ref> [BKT, CKP, FKL, McS, RaS, SlT] </ref>. Other expedients such as prefetching and clustering [PeS] have been proposed to improve performance.
Reference: [Flo] <author> Robert W. Floyd, </author> <title> "Permuting Information in Idealized Two-Level Storage," in Complexity of Computer Computations, </title> <editor> R. Miller and J. Thatcher, ed., </editor> <publisher> Plenum, </publisher> <year> 1972, </year> <pages> 105-109. </pages>
Reference-contexts: Floyd considered an idealized two-level storage where the slow memory was separated into pages of size p, and looked at the problem of permuting <ref> [Flo] </ref>. In the two-level memory hierarchy, the data elements reside initially in the external memory and the goal of the computation is to write the answer to the problem back into external memory. <p> This thesis focuses specifically on developing special-purpose algorithms that are optimal for the problem and architecture. There is a long history of developing special-purpose algorithms with a view towards I/O efficiency. As mentioned above, Floyd considered an algorithm for permuting on two-level memory hierarchies <ref> [Flo] </ref>. Knuth has 132 pages devoted to the I/O efficiency of sorting, all but 17 of which deal with the problems of using tapes [Knu]. <p> In each I/O, D blocks of B records can be transferred simultaneously, as illustrated in Figure 2.1. This model generalized the initial work on I/O of Floyd <ref> [Flo] </ref> and Hong and Kung [HoK].
Reference: [FrW] <author> Michael L. Fredman and Dan E. Willard, </author> <title> "BLASTING Through the Information Theoretic Barrier with FUSION TREES," </title> <booktitle> Proc. of the 22nd Annual ACM Symposium on the Theory of Computing (May 1990), </booktitle> <pages> 1-7. </pages>
Reference-contexts: bandwidth function b (`) = 1=(` + 1) using time O (N log N )? Alternatively, is there a way of proving a tight lower bound? Conceivably an O (N log N ) algorithm could be developed using the Fusion Tree techniques to remove the extra log log N factor <ref> [FrW] </ref>. 2. Is it possible to prove that the simple balancing algorithm is optimal? 3. Are there deterministic algorithms for parallel memory hierarchies that can work with weaker interconnections between the processors than a CRCW-PRAM? 4.
Reference: [Fri] <author> D. Bitton Friedland, </author> <title> "Design, Analysis, and Implementation of Parallel External Sorting Algorithms," </title> <type> U. </type> <institution> Wisconsin | Madison, </institution> <type> Ph.D. Dissertation, </type> <year> 1981. </year>
Reference-contexts: Hong and Kung gave the first general techniques for proving lower bounds on I/O complexity, considering a number of different problems [HoK]. Bitton et al. consider a special case of parallel disk sorting where the processors comprise a tree, with a disk attached to each of the leaf nodes <ref> [BDH, Fri] </ref>. Savage and Vitter examined the effects of block size on a number of problems, including FFT and matrix multiplication [SaV]. Aggarwal and Vitter introduced a parallel disk model and gave algorithms for matrix multiplication, permuting, FFT, and sorting in their model [AgV].
Reference: [GaJ] <author> Michael R. Garey and David S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness, </title> <editor> W. H. </editor> <publisher> Freeman & Co., </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: But then the edge (v; u) has no representative in V 0 , so V 0 was not a vertex cover. 2 VERTEX COVER is an NP-complete problem <ref> [GaJ] </ref>, but there is a well-known polynomial algorithm that approximates the optimal vertex cover within a factor of 2. Take any maximal matching of G.
Reference: [GHK] <author> Garth Gibson, Lisa Hellerstein, Richard M. Karp, Randy H. Katz, and David A. Patterson, </author> <title> "Coding Techniques for Handling Failures in Large Disk Arrays," </title> <editor> U. C. </editor> <address> Berkeley, UCB/CSD 88/477, </address> <month> December </month> <year> 1988. </year>
Reference-contexts: This approach takes advantage of the fact that the seek time is usually much longer than the time needed for transferring a record of data once the disk read/write head is positioned. An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Initial work in the use of parallel block transfer for sorting was done by Aggarwal and Vitter [AgV].
Reference: [GiS] <author> David Gifford and Alfred Spector, </author> <title> "The TWA Reservation System," </title> <booktitle> Communications of the ACM 27 (July 1984), </booktitle> <pages> 650-665. </pages>
Reference-contexts: It seems that the only way around the I/O bottleneck is to use multiple disks in parallel. This approach is taken, for example, in the TWA reservation system <ref> [GiS] </ref>. So the second elaboration is being able to handle disk parallelism. There are two traditional ways of tackling the I/O bottleneck: general-purpose algorithms and special-purpose ones. With general-purpose algorithms, the operating system takes care of handling the I/Os efficiency. <p> This approach takes advantage of the fact that the seek time is usually much longer than the time needed for transferring a record of data once the disk read/write head is positioned. An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Initial work in the use of parallel block transfer for sorting was done by Aggarwal and Vitter [AgV].
Reference: [GoS] <author> Mark Goldberg and Thomas Spencer, </author> <title> "Constructing a Maximal Independent Set in Parallel," </title> <journal> SIAM J. Discrete Math 2, </journal> <pages> 322-328. </pages>
Reference-contexts: even the fastest known deterministic parallel algorithm for maximal matching with n items is O (log 2 n) with a quartic number of processors for dense graphs (which we have) [Luba], or O (log 3 n) with a quadratic number of processors [IsS] or n 2 = log n processors <ref> [GoS] </ref>. Since we have n = H 0 , this means that the fastest known algorithm is fi (log 2 H), and still doesn't work since we have only H = fi ((H 0 ) 2 log 2 H 0 ) processors.
Reference: [GKP] <author> Ronald L. Graham, Donald E. Knuth, and Oren Patashnik, </author> <title> in Concrete Mathematics, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989, </year> <note> Chapter 4. </note>
Reference-contexts: all multiples of any irrational number fi is dense on the unit interval (0; 1), which is to say that given any 0 &lt; fl &lt; 1 and * &gt; 0, A.1 The Stern-Brocot Tree The proofs of the theorem depend heavily on the properties of the Stern-Brocot (S-B) tree <ref> [GKP] </ref>. This section explains how to derive the tree and gives the properties it has that are important to the proof.
Reference: [HoK] <author> Jia-Wei Hong and H. T. Kung, </author> <title> "I/O Complexity: The Red-Blue Pebble Game," </title> <booktitle> Proc. of the 13th Annual ACM Symposium on the Theory of Computing (May 1981), </booktitle> <pages> 326-333. </pages>
Reference-contexts: Hong and Kung gave the first general techniques for proving lower bounds on I/O complexity, considering a number of different problems <ref> [HoK] </ref>. Bitton et al. consider a special case of parallel disk sorting where the processors comprise a tree, with a disk attached to each of the leaf nodes [BDH, Fri]. Savage and Vitter examined the effects of block size on a number of problems, including FFT and matrix multiplication [SaV]. <p> In each I/O, D blocks of B records can be transferred simultaneously, as illustrated in Figure 2.1. This model generalized the initial work on I/O of Floyd [Flo] and Hong and Kung <ref> [HoK] </ref>. <p> Appendix A proves a closed form for the optimal number of generations to compute in the multigeneration sweep architecture before viewing the results. Our conclusions are given in Section 6.3. 6.1 Lower Bounds on I/O Overhead Graph pebbling is a powerful technique for proving computational lower bounds <ref> [Coo, HoK, KSS, SaV] </ref>. For nine-cell lattice computations, a general result in Kugelmass et al. can be specialized to 8 ff assuming that each of n 2 processors (e.g., an n fi n array) has ff bits of local storage.
Reference: [Ihl] <author> Edmund Ihler, </author> <title> "Bounds on the Quality of Approximate Solutions to the Group Steiner Problem," </title> <booktitle> in Graph-Theoretic Concepts in Computer Science, Proceedings of the 16th International Workshop WG '90 , G. </booktitle> <editor> Goos and J. Hartmanis, eds., </editor> <booktitle> Lecture Notes in Computer Science #484, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1991, </year> <pages> 109-118. </pages>
Reference-contexts: Ihler has shown the problem to be NP-hard, even if the graph is a tree, and gave a trivial algorithm to approximate the optimal tree within a factor of g 1, where g is the number of groups <ref> [Ihl] </ref>. The problem with applying Lemma 47 to blockings where there are not dN=Be strongly stable vertices is that we are not guaranteed that each of the "must-visit" vertices will cause a page fault.
Reference: [IsS] <author> Amos Israeli and Y. Shiloach, </author> <title> "An Improved Parallel Algorithm for Maximal Matching," </title> <booktitle> Information Processing Letters 22 (1986), </booktitle> <pages> 57-60. </pages>
Reference-contexts: Likewise, it is not hard to show that any maximal matching in the Rebalance routine is also a maximum matching. Thus, we can use maximal matching in both the Balance and Rebalance routines. Unfortunately, the best known deterministic parallel time for maximal matching is O (log 3 P ) <ref> [IsS] </ref>, which is not good enough to get optimal performance on the parallel memory hierarchies with effective logarithmic cost functions. The adjustments necessary to obtain optimal performance are the subject of the next chapter. <p> Unfortunately, even the fastest known deterministic parallel algorithm for maximal matching with n items is O (log 2 n) with a quartic number of processors for dense graphs (which we have) [Luba], or O (log 3 n) with a quadratic number of processors <ref> [IsS] </ref> or n 2 = log n processors [GoS]. Since we have n = H 0 , this means that the fastest known algorithm is fi (log 2 H), and still doesn't work since we have only H = fi ((H 0 ) 2 log 2 H 0 ) processors.
Reference: [Jil] <author> W. Jilke, </author> <title> "Disk Array Mass Storage Systems: The New Opportunity," </title> <publisher> Amperif Corporation, </publisher> <month> September </month> <year> 1986. </year>
Reference-contexts: This approach takes advantage of the fact that the seek time is usually much longer than the time needed for transferring a record of data once the disk read/write head is positioned. An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Initial work in the use of parallel block transfer for sorting was done by Aggarwal and Vitter [AgV].
Reference: [Knu] <author> Donald E. Knuth, </author> <title> in The Art of Computer Programming, Volume 3: Sorting and Searching, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: As mentioned above, Floyd considered an algorithm for permuting on two-level memory hierarchies [Flo]. Knuth has 132 pages devoted to the I/O efficiency of sorting, all but 17 of which deal with the problems of using tapes <ref> [Knu] </ref>. Even considered the problem of sorting using parallel tapes with parallel processors [Eve], but his algorithms are hampered a bit by the fact that he assumes that each of the P processors has exactly 4 tape drives attached to it. <p> Our fundamental conclusion is that a certain amount of redundancy of information is required in order to permit efficient searching of graphs. 7 Chapter 2 Sorting on Parallel Disks: Greed Sort 2.1 Introduction Sorting consumes roughly 20 percent of computing resources in large-scale installations <ref> [Knu, LiV] </ref>. Of particular importance is external sorting, in which the records to be sorted are too numerous to fit in the processor's main memory and instead are stored in secondary storage, typically made up of one or more magnetic disks. <p> An interesting extension to this work currently being investigated is the I/O overhead associated with simulating neural net computations. 86 Chapter 7 Blocking for External Graph Searching 7.1 Introduction Searching is a fundamental topic in computer science <ref> [Knu] </ref>. In external searching, the records to be searched cannot fit simultaneously in the internal memory. In this chapter, we consider searching in graphs.
Reference: [KSS] <author> Steven D. Kugelmass, Richard Squier, and Kenneth Steiglitz, </author> <title> "Performance of VLSI Engines for Lattice Computations," </title> <booktitle> Complex Systems 1 (October 1987), </booktitle> <pages> 939-965. </pages>
Reference-contexts: It also admits of a universal constructor to allow self-replicating structures [Pou]. Lattice computations have important applications in physical simulations. Examples of simulations that use such automata are two-dimensional lattice gas computations <ref> [KSS] </ref>, diffusion-limited aggregation [MaT], two-dimensional diffusion, fluid dynamics, spin glasses, and ballistics [ToM]. A VLSI circuit to solve the Poisson equation has been implemented using lattice computation techniques [Man]. <p> Highly local data movement coupled with a tremendous potential for parallelism would seem to make these problems an ideal match for VLSI. Kugelmass et al. showed, however, that VLSI-based machines performing such computations are severely constrained by input/output (I/O) requirements <ref> [KSS] </ref>. Using a simpler new argument, we improve by a constant factor the theoretical lower bound on I/O that can be derived from their work. We then present and analyze four VLSI architectures within this framework. <p> Appendix A proves a closed form for the optimal number of generations to compute in the multigeneration sweep architecture before viewing the results. Our conclusions are given in Section 6.3. 6.1 Lower Bounds on I/O Overhead Graph pebbling is a powerful technique for proving computational lower bounds <ref> [Coo, HoK, KSS, SaV] </ref>. For nine-cell lattice computations, a general result in Kugelmass et al. can be specialized to 8 ff assuming that each of n 2 processors (e.g., an n fi n array) has ff bits of local storage.
Reference: [Lei] <author> Tom Leighton, </author> <title> "Tight Bounds on the Complexity of Parallel Sorting," </title> <journal> IEEE Transactions on Computers C-34 (April 1985), </journal> <pages> 344-354, </pages> <booktitle> also appears in Proceedings of the 16th Annual ACM Symposium on Theory of Computing, </booktitle> <month> (April </month> <year> 1983), </year> <pages> 71-80. </pages>
Reference-contexts: Merge sort combined with disk striping is deterministic, but the number of I/Os used can be much larger than optimal, by a multiplicative factor of log (M=B). In the area of parallel computing, Leighton introduced the columnsort algorithm as an optimal sorting algorithm on linear-sized sorting networks <ref> [Lei] </ref>. It was pointed out in [AgV] that columnsort can be used for optimal sorting with respect to I/O if N and B are not too large. <p> By an appropriate use of clustering throughout the course of the algorithm, this approximately merged list can be completely merged by a single pass consisting of several applications of the columnsort algorithm of Leighton <ref> [Lei] </ref> to subfiles of size D p M B. Then the next merge begins. Columnsort is easiest to visualize as sorting into column-major order a matrix with r rows and c columns, with the requirement that c divides r and r &gt; 2 (c 1) 2 . <p> This problem would have had to have been solved before Greed Sort could have had any applicability to these parallel memory hierarchies. 2. The hidden constants in the big-oh notation are small. The problem with the Greed Sort algorithm is that it uses Columnsort <ref> [Lei] </ref> as a subroutine, which introduces at least an additional factor of 4 into the constant of proportionality. 3. The algorithm can operate using only striped write operations. <p> The parallel disk version of the algorithm also improves upon the algorithms of the previous chapters in the following ways: 61 1. The hidden constants in the big-O notation are small. The problem with the Greed Sort algorithm is that it uses Columnsort <ref> [Lei] </ref> as a subroutine, which introduces at least an additional factor of 4 into the constant of proportionality. 2. The algorithm can operate using only striped write operations. <p> It is also possible to schedule a recursive version of Columnsort <ref> [Lei] </ref> on UMH 1=(`+1) in a manner that is efficient with respective to the RAM algorithm, but this observation is not very useful since both algorithms have running time that is O (N log c N ), where c 3:4. 5.2.1 Parsimonious sorting in UMH 1 Theorem 20 A variant of
Reference: [LiV] <author> Eugene E. Lindstrom and Jeffrey Scott Vitter, </author> <title> "The Design and Analysis of BucketSort for Bubble Memory Secondary Storage," </title> <journal> IEEE Transactions on Computers C-34 (March 1985), </journal> <pages> 218-233. </pages>
Reference-contexts: Our fundamental conclusion is that a certain amount of redundancy of information is required in order to permit efficient searching of graphs. 7 Chapter 2 Sorting on Parallel Disks: Greed Sort 2.1 Introduction Sorting consumes roughly 20 percent of computing resources in large-scale installations <ref> [Knu, LiV] </ref>. Of particular importance is external sorting, in which the records to be sorted are too numerous to fit in the processor's main memory and instead are stored in secondary storage, typically made up of one or more magnetic disks.
Reference: [LED] <author> R. J. Lipton, S. C. Eisenstat, and R. A. DeMillo, </author> <title> "Space and Time Hierarchies for Classes of Control Structures and Data Structures," </title> <editor> J. </editor> <booktitle> Association for Computing Machinery 23 (1976), </booktitle> <pages> 720-732. </pages>
Reference-contexts: taken by Lipton et al., who consider hierarchies of embeddings of graphs, where they do allow a given vertex to be replicated up to S times in going from one level of the hierarchy to the next, as well as a factor of T expansion in the distance between vertices <ref> [LED] </ref>. Their paper, however, focuses on lower bounds for general graphs, and does not deal with the issue of how to find optimal embeddings. It is also not clear what the relevance of any optimal embeddings is to blocking.
Reference: [Luba] <author> Michael Luby, </author> <title> "A Simple Parallel Algorithm for the Maximal Independent Set Problem," </title> <journal> SIAM J. Computing 15 (1986), </journal> <pages> 1036-1053. </pages>
Reference-contexts: Unfortunately, even the fastest known deterministic parallel algorithm for maximal matching with n items is O (log 2 n) with a quartic number of processors for dense graphs (which we have) <ref> [Luba] </ref>, or O (log 3 n) with a quadratic number of processors [IsS] or n 2 = log n processors [GoS]. <p> But we know that k bH 0 =3c, so that the constant is at least 1=2. Hence, in expected O (log H 0 ) time, we can match all the vertices on the left. 2 This algorithm can be derandomized in an efficient way using the techniques of Luby <ref> [Luba, Lubb] </ref>. First, notice that we have H processors available instead of only H 0 . When H 0 = p H= log H, that means that H = fi ((H 0 ) 2 log 2 H 0 ).
Reference: [Lubb] <author> Michael G. Luby, </author> <title> "Removing Randomness in a Parallel Computation Without a Processor Penalty," </title> <booktitle> International Computer Science Institute, </booktitle> <address> TR-89-044, </address> <month> July </month> <year> 1989, </year> <booktitle> also appears in Proceedings of the 29th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <month> (October </month> <year> 1988), </year> <pages> 162-173. </pages>
Reference-contexts: But we know that k bH 0 =3c, so that the constant is at least 1=2. Hence, in expected O (log H 0 ) time, we can match all the vertices on the left. 2 This algorithm can be derandomized in an efficient way using the techniques of Luby <ref> [Luba, Lubb] </ref>. First, notice that we have H processors available instead of only H 0 . When H 0 = p H= log H, that means that H = fi ((H 0 ) 2 log 2 H 0 ).
Reference: [LuP] <author> F. Luccio and L. Pagli, </author> <title> "Sequential Computation Based on Pipelined Access to Memory," </title> <booktitle> Proceedings of the 27th Annual Allerton Conference on Communication, Control, and Computing (September 1989), </booktitle> <pages> 702-711. </pages>
Reference-contexts: Luccio considered 5 a model called LPM based on pipelined access to memory that contains many of the same considerations as are present in memory hierarchies <ref> [LuP] </ref>. Bailey investigated the performance of FFT in external or hierarchical memory, implementing a version of FFT that requires as few as two passes through the data to compute a 2 n -point FFT [Bai]. <p> A model similar to the BT model that 1 We use the notation log x, where x 1, to denote the quantity maxf1; log 2 xg. 63 allows pipelined access to memory in O (log n) time was developed independently by Luccio and Pagli <ref> [LuP] </ref>. Optimal sorting algorithms for each of these models have been developed [AAC, ACSa, LuP], and optimal algorithms for the parallelizations of the HMM and BT models was given in Chapter 4. <p> Optimal sorting algorithms for each of these models have been developed <ref> [AAC, ACSa, LuP] </ref>, and optimal algorithms for the parallelizations of the HMM and BT models was given in Chapter 4.
Reference: [Mag] <author> Ninamary Buba Maginnis, </author> <title> "Store More, Spend Less: Mid-Range Options Around," </title> <type> Computerworld (November 16, </type> <year> 1987), </year> <pages> 71-82. </pages>
Reference-contexts: This approach takes advantage of the fact that the seek time is usually much longer than the time needed for transferring a record of data once the disk read/write head is positioned. An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Initial work in the use of parallel block transfer for sorting was done by Aggarwal and Vitter [AgV].
Reference: [Man] <author> Swaminathan Manohar, </author> <title> "Supercomputing with VLSI," </title> <institution> Brown Univ., </institution> <type> Ph.D. Thesis, </type> <year> 1989. </year>
Reference-contexts: The advances in Very Large-Scale Integrated circuit (VLSI) technology has resulted in an abundance of special-purpose processors. VLSI has obvious uses for things like serial communications and graphics. People are also implementing chips for such things as solving the Poisson equation in two dimensions <ref> [Man] </ref> and neural network computations [PLK, TML, WHA]. The advantage of using VLSI is that it is possible to create very computation-efficient special-purpose processors. However, an area that is not studied as diligently is the I/O needs of VLSI chips. <p> Lattice computations have important applications in physical simulations. Examples of simulations that use such automata are two-dimensional lattice gas computations [KSS], diffusion-limited aggregation [MaT], two-dimensional diffusion, fluid dynamics, spin glasses, and ballistics [ToM]. A VLSI circuit to solve the Poisson equation has been implemented using lattice computation techniques <ref> [Man] </ref>. Highly local data movement coupled with a tremendous potential for parallelism would seem to make these problems an ideal match for VLSI. Kugelmass et al. showed, however, that VLSI-based machines performing such computations are severely constrained by input/output (I/O) requirements [KSS].
Reference: [MaT] <author> Norman Margolus and Tommaso Toffoli, </author> <title> "Cellular Automata Machines," </title> <booktitle> Complex Systems 1 (October 1987), </booktitle> <pages> 967-993. </pages>
Reference-contexts: It also admits of a universal constructor to allow self-replicating structures [Pou]. Lattice computations have important applications in physical simulations. Examples of simulations that use such automata are two-dimensional lattice gas computations [KSS], diffusion-limited aggregation <ref> [MaT] </ref>, two-dimensional diffusion, fluid dynamics, spin glasses, and ballistics [ToM]. A VLSI circuit to solve the Poisson equation has been implemented using lattice computation techniques [Man]. Highly local data movement coupled with a tremendous potential for parallelism would seem to make these problems an ideal match for VLSI. <p> We modify the algorithm slightly so that the processor array computes only (n k)=2 + 1 generations in step 4. Toffoli and Margolus mention this technique under the name "scooping", but do not analyze it from the standpoint of I/O efficiency <ref> [MaT, ToM] </ref>.
Reference: [MGS] <author> R. L. Mattson, J. Gecsei, D. R. Slutz, and I. L. Traiger, </author> <title> "Evaluation Techniques for Storage Hierarchies," </title> <journal> IBM Systems Journal 9 (1970), </journal> <pages> 78-117. </pages>
Reference-contexts: A number of authors have investigated memory hierarchies. Mattson et al. gave evaluation techniques for paging algorithms in linear memory hierarchies, proposing a class of paging algorithms called "stack algorithms" from which they derive an optimal page replacement algorithm <ref> [MGS] </ref>. Silberman proposed an interesting memory hierarchy in which the CPU is directly connected to the lowest k levels of the hierarchy, so that a page "miss" only occurs if the needed page is on level k + 1 or greater [Sil].
Reference: [McS] <author> Lyle A. McGeoch and Daniel D. Sleator, </author> <title> "A Strongly Competitive Randomized Paging Algorithm," </title> <institution> Carnegie-Mellon University, CMU-CS-89-122, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: The decision of which block to remove is the source of a number of different algorithms, and this topic has been widely studied <ref> [BKT, CKP, FKL, McS, RaS, SlT] </ref>. Other expedients such as prefetching and clustering [PeS] have been proposed to improve performance.
Reference: [NoVa] <author> Mark H. Nodine and Jeffrey Scott Vitter, </author> <title> "Greed Sort: An Optimal External Sorting Algorithm for Multiple Disks," </title> <institution> Brown University, CS-91-20, </institution> <month> August </month> <year> 1991, </year> <title> also appears in shortened form in "Large-Scale Sorting in Parallel Memories," </title> <booktitle> Proc. 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Hilton Head, SC (July 1991), </address> <pages> 29-39. </pages>
Reference-contexts: In particular we get deterministic (as well as more practical) versions of the optimal randomized algorithms based on [ViSa]and [ViN]. We also improve upon the deterministic Greed Sort algorithm in <ref> [NoVa] </ref>, which was optimal only for the parallel disk models and could not be used optimally for hierarchical memories.
Reference: [NoVb] <author> Mark H. Nodine and Jeffrey Scott Vitter, </author> <title> "Optimal Deterministic Sorting on Parallel Memory Hierarchies," </title> <institution> Department of Computer Science, Brown University, CS-92-38, </institution> <month> August </month> <year> 1992. </year>
Reference: [NoVc] <author> Mark H. Nodine and Jeffrey Scott Vitter, </author> <title> "Optimal Deterministic Sorting on Parallel Disks," </title> <institution> Department of Computer Science, Brown University, CS-92-08, </institution> <month> August </month> <year> 1992. </year> <month> 158 </month>
Reference: [PGK] <author> David A. Patterson, Garth Gibson, and Randy H. Katz, </author> <title> "A Case for Redundant Arrays of Inexpensive Disks (RAID)," </title> <booktitle> Proceedings ACM SIGMOD Conference (June 1988), </booktitle> <pages> 109-116. </pages>
Reference-contexts: A second elaboration to the two-level memory hierarchy is motivated because information processing is currently facing what several researchers have called an input/output (I/O) crisis <ref> [CGK, Che, PGK] </ref>. CPU speeds have been increasing approximately exponentially over the last decade, as have memory capacities [PGK]. Unfortunately, disk speeds have seen little improvement in rotational latency and only a halving of seek times [CGK]. <p> A second elaboration to the two-level memory hierarchy is motivated because information processing is currently facing what several researchers have called an input/output (I/O) crisis [CGK, Che, PGK]. CPU speeds have been increasing approximately exponentially over the last decade, as have memory capacities <ref> [PGK] </ref>. Unfortunately, disk speeds have seen little improvement in rotational latency and only a halving of seek times [CGK]. Even if we consider the improvements in disk technology to be exponential, it is of a lower order than that seen by processors. <p> This approach takes advantage of the fact that the seek time is usually much longer than the time needed for transferring a record of data once the disk read/write head is positioned. An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Initial work in the use of parallel block transfer for sorting was done by Aggarwal and Vitter [AgV].
Reference: [PeS] <author> James L. Peterson and Abraham Silberschatz, </author> <title> in Operating System Concepts, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: The decision of which block to remove is the source of a number of different algorithms, and this topic has been widely studied [BKT, CKP, FKL, McS, RaS, SlT]. Other expedients such as prefetching and clustering <ref> [PeS] </ref> have been proposed to improve performance. The difficulty with all of these schemes is that the operating system must make some assumptions about the ways that data will be accessed (such as "locality of reference"), and those assumptions may not always hold.
Reference: [Pou] <author> William Poundstone, </author> <title> The Recursive Universe: Cosmic Complexity and the Limits of Scientific Knowledge, Contemporary Books, </title> <address> Chicago, IL, </address> <year> 1985. </year>
Reference-contexts: Despite the apparent simplicity in having purely local rules govern the time evolution of the system, Life exhibits complex behavior and in fact possesses the same computational power as a Turing machine [Dew]. It also admits of a universal constructor to allow self-replicating structures <ref> [Pou] </ref>. Lattice computations have important applications in physical simulations. Examples of simulations that use such automata are two-dimensional lattice gas computations [KSS], diffusion-limited aggregation [MaT], two-dimensional diffusion, fluid dynamics, spin glasses, and ballistics [ToM]. A VLSI circuit to solve the Poisson equation has been implemented using lattice computation techniques [Man].
Reference: [PLK] <author> K. Wojtek Przytula, Wei-Ming Lin, and V.K. Prassana Kumar, </author> <title> "Partitioned Implementation of Neural Networks in Mesh Connected Array Processors," </title> <booktitle> VLSI Signal Processing IV: Proceedings of IEEE VLSI For Signal Processing Workshop (November 1990), </booktitle> <pages> 106-115. </pages>
Reference-contexts: The advances in Very Large-Scale Integrated circuit (VLSI) technology has resulted in an abundance of special-purpose processors. VLSI has obvious uses for things like serial communications and graphics. People are also implementing chips for such things as solving the Poisson equation in two dimensions [Man] and neural network computations <ref> [PLK, TML, WHA] </ref>. The advantage of using VLSI is that it is possible to create very computation-efficient special-purpose processors. However, an area that is not studied as diligently is the I/O needs of VLSI chips.
Reference: [RaS] <author> Prabhakar Raghavan and Marc Snir, </author> <title> "Memory Versus Randomization in Online Algorithms," </title> <booktitle> in Proc. of the 16th International Colloquium on Automata, Languages, and Programming, </booktitle> <editor> G. Goos and J. Hartmanis, eds., </editor> <booktitle> Lecture Notes in Computer Science #372, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> July </month> <year> 1989, </year> <pages> 687-703. </pages>
Reference-contexts: The decision of which block to remove is the source of a number of different algorithms, and this topic has been widely studied <ref> [BKT, CKP, FKL, McS, RaS, SlT] </ref>. Other expedients such as prefetching and clustering [PeS] have been proposed to improve performance.
Reference: [RaR] <author> Sanguthevar Rajasekaran and John H. Reif, </author> <title> "Optimal and Sublogarithmic Time Randomized Parallel Sorting Algorithms," </title> <journal> SIAM J. Computing 18 (1989), </journal> <pages> 594-607. </pages>
Reference-contexts: Otherwise, instead of sorting using the keys contained in the records, we sort using the bucket number as the key. We will start by considering only P M= log M . We will make use of a deterministic algorithm by Rajasekaran and Reif that appears as Lemma 3.1 in <ref> [RaR] </ref>. Their algorithm sorts n elements in the range 1; . . . ; log n in time O (log n) using n= log n processors. We need to consider two cases: Case 1: M=B log M . In this case, we let n = M .
Reference: [ReW] <author> Gabriele Reich and Peter Widmayer, </author> <title> "Beyond Steiner's Problem: A VLSI Oriented Generalization," </title> <booktitle> in Graph-Theoretic Comcepts in Computer Science: Proceedings of the 15th International Workshop WG '89 , G. </booktitle> <editor> Goos and J. Hart-manis, eds., </editor> <booktitle> Lecture Notes in Computer Science #411, </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990, </year> <pages> 196-210. </pages>
Reference-contexts: Reich and Widmayer gave an approximation algorithm for the minimum group Steiner tree problem, but did not give any guarantees on how much worse than optimal their results could be <ref> [ReW] </ref>. Ihler has shown the problem to be NP-hard, even if the graph is a tree, and gave a trivial algorithm to approximate the optimal tree within a factor of g 1, where g is the number of groups [Ihl].
Reference: [Rosa] <author> Arnold L. Rosenberg, </author> <title> "Preserving Proximity in Arrays," </title> <journal> SIAM J. Comput. </journal> <volume> 4 (1975), </volume> <pages> 443-460. </pages>
Reference-contexts: Alternatively, all the work done in the database community on B-trees could be viewed as a solution to our problem for complete trees with s = 1. There is considerable previous work on embedding of one type of data structure into another <ref> [AlR, CRS, DEL, Rosa, Rosb, Rosc, RoS] </ref>. This work is relevant because if we view the data structures as graphs, the goal of the embedding is to maintain locality in the original graph when accesses are done according to the target graph. <p> Of course, this is only a heuristic, and local references in the target graph do not always get translated into local references in the host. For example, Rosenberg considered the problem of preserving proximity in arrays <ref> [Rosa] </ref> when mapping onto a linear access structure. A grid graph describes the proximity properties of arrays, so the embedding problem here is to map a grid graph into a semi-infinite number line. <p> The problem of computing how large the neighborhoods of radius r in d-dimensional grid graphs was considered by Rosenberg <ref> [Rosa] </ref>, where he derived the answer O (r d ). In the following derivation, we find the exact coefficient of the r d term.
Reference: [Rosb] <author> Arnold L. Rosenberg, </author> <title> "Data Encodings and Their Costs," </title> <journal> Acta Informatica 9 (1978), </journal> <pages> 273-292. </pages>
Reference-contexts: Alternatively, all the work done in the database community on B-trees could be viewed as a solution to our problem for complete trees with s = 1. There is considerable previous work on embedding of one type of data structure into another <ref> [AlR, CRS, DEL, Rosa, Rosb, Rosc, RoS] </ref>. This work is relevant because if we view the data structures as graphs, the goal of the embedding is to maintain locality in the original graph when accesses are done according to the target graph.
Reference: [Rosc] <author> Arnold L. Rosenberg, </author> <title> "Encoding Data Structures in Trees," </title> <editor> J. </editor> <booktitle> Association for Computing Machinery 26 (1979), </booktitle> <pages> 668-689. </pages>
Reference-contexts: Alternatively, all the work done in the database community on B-trees could be viewed as a solution to our problem for complete trees with s = 1. There is considerable previous work on embedding of one type of data structure into another <ref> [AlR, CRS, DEL, Rosa, Rosb, Rosc, RoS] </ref>. This work is relevant because if we view the data structures as graphs, the goal of the embedding is to maintain locality in the original graph when accesses are done according to the target graph.
Reference: [RoS] <author> Arnold L. Rosenberg and Lawrence Snyder, </author> <title> "Bounds on the Costs of Data Encodings," Math. </title> <booktitle> Systems Theory 12 (1978), </booktitle> <pages> 9-39. </pages>
Reference-contexts: Alternatively, all the work done in the database community on B-trees could be viewed as a solution to our problem for complete trees with s = 1. There is considerable previous work on embedding of one type of data structure into another <ref> [AlR, CRS, DEL, Rosa, Rosb, Rosc, RoS] </ref>. This work is relevant because if we view the data structures as graphs, the goal of the embedding is to maintain locality in the original graph when accesses are done according to the target graph.
Reference: [SaV] <author> John Savage and Jeffrey Scott Vitter, </author> <title> "Parallelism in Space-Time Tradeoffs," </title> <booktitle> in Advances in Computing Research, </booktitle> <volume> Volume 4 , F. </volume> <editor> P. Preparata, ed., </editor> <publisher> JAI Press, </publisher> <year> 1987, </year> <pages> 117-146, </pages> <note> Also appears in Proceedings of the International Workshop on Parallel Computing and VLSI, Amalfi, Italy (May 1984), </note> <editor> P. Bertolazzi and F. Luccio, ed., </editor> <publisher> Elsevier Science Press, </publisher> <year> 1985, </year> <pages> 49-58. </pages>
Reference-contexts: Bitton et al. consider a special case of parallel disk sorting where the processors comprise a tree, with a disk attached to each of the leaf nodes [BDH, Fri]. Savage and Vitter examined the effects of block size on a number of problems, including FFT and matrix multiplication <ref> [SaV] </ref>. Aggarwal and Vitter introduced a parallel disk model and gave algorithms for matrix multiplication, permuting, FFT, and sorting in their model [AgV]. Vitter and Shriver gave a more realistic parallel disk model and gave optimal algorithms for the same suite of problems as considered by Aggarwal and Vitter [ViSa]. <p> Appendix A proves a closed form for the optimal number of generations to compute in the multigeneration sweep architecture before viewing the results. Our conclusions are given in Section 6.3. 6.1 Lower Bounds on I/O Overhead Graph pebbling is a powerful technique for proving computational lower bounds <ref> [Coo, HoK, KSS, SaV] </ref>. For nine-cell lattice computations, a general result in Kugelmass et al. can be specialized to 8 ff assuming that each of n 2 processors (e.g., an n fi n array) has ff bits of local storage.
Reference: [Sil] <author> Gabriel M. Silberman, </author> <title> "Delayed-Staging Hierarchy Optimization," </title> <journal> IEEE Transactions on Computers C-32 (November 1983), </journal> <pages> 1029-1037. </pages>
Reference-contexts: Silberman proposed an interesting memory hierarchy in which the CPU is directly connected to the lowest k levels of the hierarchy, so that a page "miss" only occurs if the needed page is on level k + 1 or greater <ref> [Sil] </ref>. Aggarwal et al. advanced the first model of memory hierarchies, HMM, on which they considered special-purpose algorithms, including matrix multiplication, FFT, and sorting [AAC]. Aggarwal et al. also proposed a follow-up model called BT in which they added blocking considerations to the HMM memory model [ACSa].
Reference: [SlT] <author> Daniel D. Sleator and Robert E. Tarjan, </author> <title> "Amortized Efficiency of List Update and Paging Rules," </title> <booktitle> Communications of the ACM 28 (February 1985), </booktitle> <pages> 202-208. 159 </pages>
Reference-contexts: The decision of which block to remove is the source of a number of different algorithms, and this topic has been widely studied <ref> [BKT, CKP, FKL, McS, RaS, SlT] </ref>. Other expedients such as prefetching and clustering [PeS] have been proposed to improve performance.
Reference: [TML] <author> A. P. Thakoor, A. Moopenn, John Lambe, and S. K. Khanna, </author> <title> "Electronic Hardware Implementations of Neural Networks," </title> <journal> Applied Optics 26, </journal> <pages> 5085-5092. </pages>
Reference-contexts: The advances in Very Large-Scale Integrated circuit (VLSI) technology has resulted in an abundance of special-purpose processors. VLSI has obvious uses for things like serial communications and graphics. People are also implementing chips for such things as solving the Poisson equation in two dimensions [Man] and neural network computations <ref> [PLK, TML, WHA] </ref>. The advantage of using VLSI is that it is possible to create very computation-efficient special-purpose processors. However, an area that is not studied as diligently is the I/O needs of VLSI chips.
Reference: [ToM] <author> Tomasso Toffoli and Norman Margolus, </author> <title> Cellular Automata Machines: A New Environment for Modeling, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: It also admits of a universal constructor to allow self-replicating structures [Pou]. Lattice computations have important applications in physical simulations. Examples of simulations that use such automata are two-dimensional lattice gas computations [KSS], diffusion-limited aggregation [MaT], two-dimensional diffusion, fluid dynamics, spin glasses, and ballistics <ref> [ToM] </ref>. A VLSI circuit to solve the Poisson equation has been implemented using lattice computation techniques [Man]. Highly local data movement coupled with a tremendous potential for parallelism would seem to make these problems an ideal match for VLSI. <p> In practice, of course, this scheme is not useful because it ignores problems larger than n fi n. 6.2.2 Array sweep architecture In this architecture, described by Toffoli <ref> [ToM] </ref>, the complete lattice computation is stored in an m fi m grid of memory cells. The processor array repetitively loads n fi n 82 subproblems, updates states by a single generation, and writes results back to their original off-chip locations. <p> We modify the algorithm slightly so that the processor array computes only (n k)=2 + 1 generations in step 4. Toffoli and Margolus mention this technique under the name "scooping", but do not analyze it from the standpoint of I/O efficiency <ref> [MaT, ToM] </ref>.
Reference: [Ull] <author> Jeffrey D. Ullman, </author> <title> in Principles of Database and Knowledge-Base Systems, </title> <publisher> Computer Science Press, </publisher> <address> Rockville, MD, </address> <year> 1988. </year>
Reference-contexts: This is a stronger assumption than that used, for example, by external B-trees, where a record of data may be pointed to from several places, although the data (exclusive of the key being indexed) exist in only one block <ref> [Ull] </ref>. We allow actual replication of the data and define the storage blow-up s to be the ratio between the actual number of blocks used to represent the data and the minimum number that could be used.
Reference: [UlY] <author> Jeffrey D. Ullman and Mihalis Yannakakis, </author> <title> "The Input/Output Complexity of Transitive Closure," </title> <journal> Annals of Mathematics and Artificial Intelligence (1991), </journal> <pages> 331-360. </pages>
Reference-contexts: Vitter and Shriver gave a more realistic parallel disk model and gave optimal algorithms for the same suite of problems as considered by Aggarwal and Vitter [ViSa]. Ullman and Yannakakis investigated the input/output complexity of the transitive closure problem <ref> [UlY] </ref>. Carlson showed how to use local memory judiciously in order to compute the FFT efficiently on a CRAY-2 computer [Car]. Cormen gave fast algorithms for permuting in the Vitter-Shriver parallel disk model [Cor]. It is sometimes unrealistic to deal with a computer's memory as comprising only two layers.
Reference: [Uni] <institution> University of California at Berkeley, "Massive Information Storage, Management, and Use (NSF Institutional Infrastructure Proposal)," </institution> <note> Technical Report No. UCB/CSD 89/493, </note> <month> January </month> <year> 1989. </year>
Reference-contexts: This approach takes advantage of the fact that the seek time is usually much longer than the time needed for transferring a record of data once the disk read/write head is positioned. An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Initial work in the use of parallel block transfer for sorting was done by Aggarwal and Vitter [AgV].
Reference: [ViN] <author> Jeffrey Scott Vitter and Mark H. Nodine, </author> <title> "Large-Scale Sorting in Uniform Memory Hierarchies," Journal of Parallel and Distributed Computing (January 1993), also appears in shortened form in "Large-Scale Sorting in Parallel Memories," </title> <booktitle> Proc. 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Hilton Head, SC (July 1991), </address> <pages> 29-39. </pages>
Reference-contexts: The modified Balance Sort algorithm we describe in the next section gives us optimal deterministic algorithms for all the models we consider. In particular we get deterministic (as well as more practical) versions of the optimal randomized algorithms based on [ViSa]and <ref> [ViN] </ref>. We also improve upon the deterministic Greed Sort algorithm in [NoVa], which was optimal only for the parallel disk models and could not be used optimally for hierarchical memories.
Reference: [ViSa] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver, </author> <title> "Optimal Disk I/O with Parallel Block Transfer," </title> <booktitle> Proceedings of the 22nd Annual ACM Symposium on Theory of Computing (May 1990), </booktitle> <pages> 159-169. </pages>
Reference-contexts: Aggarwal and Vitter introduced a parallel disk model and gave algorithms for matrix multiplication, permuting, FFT, and sorting in their model [AgV]. Vitter and Shriver gave a more realistic parallel disk model and gave optimal algorithms for the same suite of problems as considered by Aggarwal and Vitter <ref> [ViSa] </ref>. Ullman and Yannakakis investigated the input/output complexity of the transitive closure problem [UlY]. Carlson showed how to use local memory judiciously in order to compute the FFT efficiently on a CRAY-2 computer [Car]. Cormen gave fast algorithms for permuting in the Vitter-Shriver parallel disk model [Cor]. <p> they showed that the number of I/Os required to transpose a p fi q matrix is fi N log minfM; minfp; qg; N=Bg log (M=B) : (2:2) Vitter and Shriver considered a more realistic D-disk model , in which the secondary storage is partitioned into D physically distinct disk drives <ref> [ViSa, ViSb] </ref>, as in in this definition, as long as each could operate independently of the other heads on the drive.) In each I/O operation, each of the D disks can simultaneously transfer one block of B records. <p> Since columnsort is a (non-adaptive) sorting network algorithm, its I/O schedule can be pre-specified independently of the data to take full advantage of parallel block transfer. In this chapter, we answer the open question posed in <ref> [ViSa] </ref> and present an optimal deterministic sorting algorithm. The algorithm, which we call Greed Sort, is an interesting variant of merge sort. <p> Although the array E = fe j g is listed as being returned from the routine, it is too large to fit into the base memory level, and is therefore implemented as a "global" variable. This algorithm is essentially identical to that given in <ref> [ViSa] </ref>. <p> Theorem 19 The algorithm given above is optimal for all well-behaved cost functions f (x). Proof : This proof is essentially that of the corresponding theorem in <ref> [ViSa] </ref> for their randomized algorithm. Let T M;H (N ) denote the average number of I/O steps the sorting algorithm does with an internal memory of size M , where we allow each hierarchy to move simultaneously, in a single I/O step, a record between internal memory and external memory. <p> We concentrate in this section on the cost function f (x) = x ff , where 0 &lt; ff &lt; 1. Deterministic algorithms for f (x) = x ff with ff 1 have been reported previously <ref> [ViSa] </ref>. <p> We will likewise use partial striping to group D=D 0 disks together to make D 0 virtual disks, with D 0 = p D= log D. We also use a different method for computing the partitioning elements, described in <ref> [ViSa] </ref>. Finally, we will select S = M fl The procedure for finding the partitioning elements uses as a subroutine Algorithm 18, which computes the kth smallest of n elements in O (n=DB) I/Os. <p> Since there are overall dN=M e memoryloads, we find that the number of I/Os done per call to Balance Disks is O (N=DB). Vitter and Shriver showed that the partition elements can also be computed using O (N=DB) I/Os <ref> [ViSa] </ref>. Thus, we get the recurrence T (N ) = ST N 0 if N M which has solution T (N ) = O N log S M = O N log (N=B) This is the same bound as was shown to be optimal for the parallel disk model [AgV]. <p> This algorithm improves upon the randomized algorithms of Vitter and Shriver <ref> [ViSa] </ref>. The algorithm applies to P-HMM, P-BT, the parallel UMH hierarchies in Chapter 5, as well as the parallel disk model. The parallel disk version of the algorithm also improves upon the algorithms of the previous chapters in the following ways: 61 1. <p> In this chapter we will assume that the "network" connecting the processors is a CRCW-PRAM, as in Chapter 4. Vitter and Shriver introduced optimal randomized sorting algorithms for P-HMM and P-BT <ref> [ViSa] </ref>. The algorithms were based on their randomized two-level partitioning technique applied to the optimal single-hierarchy algorithms for HMM and BT developed in [AAC, ACSa]. In Chapter 4, we gave optimal deterministic sorting algorithms for P-HMM and P-BT. <p> Theorems 14 and 15); accordingly the upper and lower bounds combine in an interesting way several techniques from <ref> [AAC, ACSa, AgV, ViSa] </ref>. Theorem 22 The running times mentioned in Theorem 21 are matching upper and lower bounds for sorting in P-RUMH. The algorithms for nonconstant H for the first two bandwidth cases are based on Balance Sort. <p> Hence, the lower bound for P-HMM for f (x) = log x given in <ref> [ViSa] </ref> also holds for P-RUMH 1=(`+1) . 2 Theorem 23 The following bounds are matching upper and lower bounds for sorting in P-SUMH. <p> Proof : We prove the lower bounds using an approach similar to that of <ref> [ViSa] </ref>. Let us define the "sequential time" of a P-SUMH algorithm to be the sum of its time costs for each of the H hierarchies. The sequential time can be at most H times the P-SUMH running time. <p> The sequential time can be at most H times the P-SUMH running time. We superimpose on the P-SUMH model a sequence of one disk, two-level memories of the type studied in <ref> [AgV, ViSa] </ref>, in the following way: For 1 ` 1 2 log (N=ffH), the `th two-level memory has one disk, internal memory size M ` = Hff ( 2 (`+1) 1)=( 2 1), and block size B ` = ` .
Reference: [ViSb] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver, </author> <title> "Algorithms for Parallel Memory I: Two-Level Memories," </title> <institution> Brown University, CS-90-21, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: they showed that the number of I/Os required to transpose a p fi q matrix is fi N log minfM; minfp; qg; N=Bg log (M=B) : (2:2) Vitter and Shriver considered a more realistic D-disk model , in which the secondary storage is partitioned into D physically distinct disk drives <ref> [ViSa, ViSb] </ref>, as in in this definition, as long as each could operate independently of the other heads on the drive.) In each I/O operation, each of the D disks can simultaneously transfer one block of B records. <p> Steps 1, 3, 5, 6, 7, and 8 can be done easily with O (N=DB) I/Os. The transpose like operation in Steps 2 and 4 can be done with O (N=DB) I/Os by a variant of the p fi q matrix transpose algorithm of <ref> [ViSb] </ref>, for p = M and q = N=M D p which we omit for brevity. <p> The next few subsections establish the proof of this theorem. 3.2.1 Finding the partition elements The following procedure for finding the partition elements was presented in <ref> [ViSb] </ref>, and is reproduced here for convenience in Algorithm 8. The algorithm is deterministic and guarantees to find S 1 partition elements such that, for any bucket b, the number of elements in that bucket N b obeys the constraint N N b 2S The algorithm was analyzed in [ViSb] and <p> in <ref> [ViSb] </ref>, and is reproduced here for convenience in Algorithm 8. The algorithm is deterministic and guarantees to find S 1 partition elements such that, for any bucket b, the number of elements in that bucket N b obeys the constraint N N b 2S The algorithm was analyzed in [ViSb] and shown to take O (N=DB) parallel I/O operations which, as we will see in the analysis, is sufficient for achieving optimal performance. The procedure for finding the partitioning elements uses as a subroutine Algorithm 7, which computes the kth smallest of n elements in O (n=DB) I/Os. <p> From Theorem 8, the bucket will take no more than 3N=DB reads and writes. Step (3), as mentioned, has been shown by <ref> [ViSb] </ref> to require only O (N=DB) I/Os, regardless of whether S = 2 p M=B or S = 2N=M . Step (4), as shown in Theorem 9, requires no more than 6N=DB I/Os. Step (5) requires writing a set of cardinality DS, which can be done in S=B parallel writes.
Reference: [ViSc] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver, </author> <title> "Algorithms for Parallel Memory II: Hierarchical Multilevel Memories," </title> <institution> Brown University, CS-90-22, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: When N &lt; H 2 , the values of S and G are the same as in the P-HMM case, so the same proof holds as before. 2 As with the P-HMM algorithm, there are at most two additional levels of recursion after N H 2 . <ref> [ViSc] </ref> showed that two lists can be merged in P-BT using the same amount of time as that needed to touch all the elements. We need to make one more change to the algorithm for BT hierarchies, but one that is hard to write explicitly. <p> The b (`) = c` case additionally requires the use of the conventional N log N serial bound for sorting. The upper bounds for the first two cases b (`) = 1 and b (`) = 1=(` + 1) are achieved by simulating the optimal P-HMM algorithm of <ref> [ViSc] </ref>, for access cost functions f (x) = log x and f (x) = log 2 x, respectively. In each case, we alter the P-HMM algorithm in the same way as that for P-BT by reblocking the buckets into consecutive locations after the call to Balance.
Reference: [WHA] <author> Mark Walker, Paul Hasler, and Lex Akers, </author> <title> "A CMOS Neural Network for Pattern Association," </title> <booktitle> IEEE Micro 9 (October 1989), </booktitle> <pages> 68-74. 160 </pages>
Reference-contexts: The advances in Very Large-Scale Integrated circuit (VLSI) technology has resulted in an abundance of special-purpose processors. VLSI has obvious uses for things like serial communications and graphics. People are also implementing chips for such things as solving the Poisson equation in two dimensions [Man] and neural network computations <ref> [PLK, TML, WHA] </ref>. The advantage of using VLSI is that it is possible to create very computation-efficient special-purpose processors. However, an area that is not studied as diligently is the I/O needs of VLSI chips.
References-found: 81

