URL: ftp://ei.dtu.dk/dist/1995/with.nips95.ps.Z
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Email: emails: with,lkhansen,jlarsen@ei.dtu.dk  
Title: Pruning with generalization based weight saliencies: flOBD, flOBS  
Author: Morten With Pedersen Lars Kai Hansen Jan Larsen 
Address: Denmark B349 DK-2800 Lyngby, DENMARK  
Affiliation: connect, Electronics Institute Technical University of  
Abstract: The purpose of most architecture optimization schemes is to improve generalization. In this presentation we suggest to estimate the weight saliency as the associated change in generalization error if the weight is pruned. We detail the implementation of both an O(N )-storage scheme extending OBD, as well as an O(N 2 ) scheme extending OBS. We illustrate the viability of the approach on pre diction of a chaotic time series.
Abstract-found: 1
Intro-found: 1
Reference: <author> H. Akaike: </author> <title> Fitting Autoregressive Models for Prediction. </title> <journal> Ann. Inst. Stat. Mat. </journal> <volume> 21, </volume> <pages> 243-247, </pages> <year> (1969). </year>
Reference-contexts: The most obvious candidate estimate is a test error estimated on a validation set. Validation sets, unfortunately, are notoriously very noisy (see, e.g., the discussion in Weigend et al., 1990). Hence, an attractive alternative is to estimate the test error by statistical means, e.g., Akaike's FPE <ref> (Akaike, 69) </ref>. For regression type problems such a pruning stop criterion was suggested in (Svarer et al., 93). <p> However, why not let the saliency itself reflect the possible improvement in test error? This is the idea that we explore in this contribution. 2 GENERALIZATION IN REGULARIZED NEURAL NETWORKS The basic asymptotic estimate of the generalization error was derived by Akaike <ref> (Akaike, 1969) </ref>; the so-called Final Prediction Error (FPE). The use of FPE-theory for neural net learning has been pioneered by Moody (see e.g. (Moody, 91)), who derived estimators for the average generalization error in regularized networks.
Reference: <author> Y. Le Cun, J.S. Denker, and S.A. Solla: </author> <title> Optimal Brain Damage. </title> <booktitle> In Advances in Neural Information Processing Systems 2, </booktitle> <publisher> Morgan Kaufman, </publisher> <pages> 598-605, </pages> <year> (1990). </year>
Reference-contexts: By careful fine tuning of the network architecture we may improve generalization, decrease the amount of computation, and facilitate interpretation. The two most widely used schemes for pruning of feed-forward nets are: Optimal Brain Damage (OBD) due to <ref> (LeCun et al., 90) </ref> and the Optimal Brain Surgeon (OBS) (Hassibi et al., 93). Both schemes are based on weight ranking according to saliency defined as the change in training error when the particular weight is pruned. <p> The scheme, being based on the diagonal approximation for the Hessian, requires storage of a number of variables scaling linearly with the number of parameters N . As in <ref> (Le Cun et al., 90) </ref> we approximate the second derivative matrix by the positive semi-definite expression: @ 2 E train @u 2 p k=1 @F u (x (k)) 2 In the diagonal approximation we find N eff = j=1 j 2 where j @ 2 E train ffi j . <p> Further, ff j =p are the weight decay parameters (diagonal elements of the regularization matrix R). The OBD method proposed by <ref> (Le Cun et al., 90) </ref> was successfully applied to reduce large networks for recognition of handwritten digits. The basic idea is to estimate the increase in the training error when deleting weights.
Reference: <author> L.K. Hansen and M. </author> <title> With Petersen: Controlled Growth of Cascade Correlation Nets. </title> <booktitle> Proceedings of ICANN'94 International Conference on Neural Networks, </booktitle> <address> Sorrento, Italy, </address> <year> 1994. </year> <editor> Eds. M. Marinaro and P.G. </editor> <booktitle> Morasso, </booktitle> <pages> 797-800, </pages> <year> (1994). </year>
Reference-contexts: This averaged generalization error is estimated by b E test = N eff ; (4) with the effective number of parameters being N eff = tr (HJ 1 HJ 1 ) <ref> (Larsen and Hansen, 94) </ref>. The Hessian, H, is the second derivative matrix of the training error with respect to the weights and thresholds, while J is the regularized Hessian: J = H + R. <p> When eliminating the l'th weight retraining is determined by ffiu l = u l J 1 e l (11) where e l is the l'th unit vector. We need to modify the OBS saliencies when working from a weight decay regularized cost function. The modified saliencies were given in <ref> (Hansen and With, 94) </ref> 2 ffiE train;l = 1 u 2 (J 1 ) ll ff l J 1 u) 2 l (J 2 ) ll Whether using the generalization based flOBS or standard OBS, we want to point to an important aspect of OBS that seems not to be generally <p> 1 u) 2 l (J 2 ) ll Whether using the generalization based flOBS or standard OBS, we want to point to an important aspect of OBS that seems not to be generally appreciated, namely the 2 The expression is for the case of all weight decays being equal, see <ref> (Hansen and With, 94) </ref> for the general expression. problem of "nuisance" parameters (White, 89), (Larsen, 93). When eliminating an output weight u o , all the weights to the corresponding hidden unit are in effect also pruned away.
Reference: <author> B. Hassibi, D. G. Stork, and G. J. Wolff: </author> <title> Optimal Brain Surgeon and General Network Pruning, </title> <booktitle> in Proceedings of the 1993 IEEE International Conference on Neural Networks, </booktitle> <address> San Francisco (Eds. </address> <publisher> E.H. </publisher> <editor> Ruspini et al. </editor> ) <month> 293-299, </month> <year> (1993). </year>
Reference-contexts: By careful fine tuning of the network architecture we may improve generalization, decrease the amount of computation, and facilitate interpretation. The two most widely used schemes for pruning of feed-forward nets are: Optimal Brain Damage (OBD) due to (LeCun et al., 90) and the Optimal Brain Surgeon (OBS) <ref> (Hassibi et al., 93) </ref>. Both schemes are based on weight ranking according to saliency defined as the change in training error when the particular weight is pruned. <p> Using a standard lemma for partitioned matrices, we obtain (J 1 ) 1 = (J 1 ) 1 (J 1 ) 2 [(J 1 ) 4 ] 1 (J 1 ) 3 (14) which only calls for inversion of the (small) submatrix (J 1 ) 4 . In <ref> (Hassibi et al., 93) </ref> it was argued that one might save on computation by using an iterative scheme for calculation of the inverse Hessian J 1 .
Reference: <author> J. Larsen: </author> <title> Design of Neural Network Filters. </title> <type> Ph.D. Thesis, </type> <institution> Electronics Institute, Technical University of Denmark, </institution> <year> (1993). </year>
Reference-contexts: or standard OBS, we want to point to an important aspect of OBS that seems not to be generally appreciated, namely the 2 The expression is for the case of all weight decays being equal, see (Hansen and With, 94) for the general expression. problem of "nuisance" parameters (White, 89), <ref> (Larsen, 93) </ref>. When eliminating an output weight u o , all the weights to the corresponding hidden unit are in effect also pruned away. Such a situation is well-known in the statistics literature on model selection where such "ghost" input weights are known as nuisance parameters.
Reference: <author> J. Larsen and L.K. Hansen: </author> <title> Generalization Performance of Regularized Neural Network Models. </title> <booktitle> "Neural Networks for Signal Processing IV" Proceedings of the IEEE Workshop, </booktitle> <editor> Eds. J. Vlontzos et al., </editor> <publisher> IEEE Service Center, </publisher> <address> Piscataway NJ, 42-51, </address> <year> (1994). </year>
Reference-contexts: This averaged generalization error is estimated by b E test = N eff ; (4) with the effective number of parameters being N eff = tr (HJ 1 HJ 1 ) <ref> (Larsen and Hansen, 94) </ref>. The Hessian, H, is the second derivative matrix of the training error with respect to the weights and thresholds, while J is the regularized Hessian: J = H + R.
Reference: <author> J.E. Moody: </author> <title> Note on Generalization, Regularization and Architecture Selection in Nonlinear Systems. </title> <booktitle> In Neural Networks For Signal Processing; Proceedings of the 1991 IEEE-SP Workshop, </booktitle> <editor> (Eds. B.H. Juang, S.Y. Kung, and C. Kamm), </editor> <publisher> IEEE Service Center, </publisher> <pages> 1-10, </pages> <year> (1991). </year>
Reference-contexts: The use of FPE-theory for neural net learning has been pioneered by Moody (see e.g. <ref> (Moody, 91) </ref>), who derived estimators for the average generalization error in regularized networks. Our network is a feed-forward architecture with n I input units, n H hidden sigmoid units and a single linear output unit, appropriate for scalar function approximation.
Reference: <author> C. Svarer, L.K. Hansen, and J. Larsen: </author> <title> On Design and Evaluation of Tapped Delay Line Networks, </title> <booktitle> In Proceedings of the 1993 IEEE International Conference on Neural Networks, San Francisco, </booktitle> <editor> (Eds. E.H. Ruspini et al. </editor> ) <month> 46-51, </month> <year> (1993). </year>
Reference-contexts: Validation sets, unfortunately, are notoriously very noisy (see, e.g., the discussion in Weigend et al., 1990). Hence, an attractive alternative is to estimate the test error by statistical means, e.g., Akaike's FPE (Akaike, 69). For regression type problems such a pruning stop criterion was suggested in <ref> (Svarer et al., 93) </ref>. <p> To emphasize that we use the generalization error for ranking of weights we use the prefix fl: flOBD and flOBS. 3 flOBD: AN O (N ) IMPLEMENTATION Our O (N ) simulator is based on batch mode, second order pseudo-Gauss Newton optimization which is described in <ref> (Svarer et al., 93) </ref>. The scheme, being based on the diagonal approximation for the Hessian, requires storage of a number of variables scaling linearly with the number of parameters N .
Reference: <author> A.S. Weigend, B.A. Huberman, and D.E. Rumelhart: </author> <title> Prediction the future: A Connectionist Approach. </title> <booktitle> Int. J. of Neural Systems 3, </booktitle> <pages> 193-209, </pages> <year> (1990). </year>
Reference-contexts: However, in both cases one clearly needs a stop criterion. As both schemes aim at minimal generalization error an estimator for this quantity is needed. The most obvious candidate estimate is a test error estimated on a validation set. Validation sets, unfortunately, are notoriously very noisy <ref> (see, e.g., the discussion in Weigend et al., 1990) </ref>. Hence, an attractive alternative is to estimate the test error by statistical means, e.g., Akaike's FPE (Akaike, 69). For regression type problems such a pruning stop criterion was suggested in (Svarer et al., 93).
Reference: <author> H. White: </author> <title> Learning in Artificial Neural Networks: A Statistical Perspective. </title> <booktitle> Neural Computation 1, </booktitle> <pages> 425-464, </pages> <year> (1989). </year>
Reference-contexts: based flOBS or standard OBS, we want to point to an important aspect of OBS that seems not to be generally appreciated, namely the 2 The expression is for the case of all weight decays being equal, see (Hansen and With, 94) for the general expression. problem of "nuisance" parameters <ref> (White, 89) </ref>, (Larsen, 93). When eliminating an output weight u o , all the weights to the corresponding hidden unit are in effect also pruned away. Such a situation is well-known in the statistics literature on model selection where such "ghost" input weights are known as nuisance parameters.
References-found: 10

