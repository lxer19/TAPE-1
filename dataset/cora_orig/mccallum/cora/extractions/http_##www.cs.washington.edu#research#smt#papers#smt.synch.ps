URL: http://www.cs.washington.edu/research/smt/papers/smt.synch.ps
Refering-URL: http://www.cs.washington.edu/research/smt/
Root-URL: 
Email: tullsen@cs.ucsd.edu fjlo,eggers,levyg@cs.washington.edu  
Title: Supporting Fine-Grained Synchronization on a Simultaneous Multithreading Processor  
Author: Dean M. Tullsen Jack L. Lo, Susan J. Eggers, Henry M. Levy 
Note: June 1998  
Address: 9500 Gilman Drive Box 352350 La Jolla, CA 92093-0114 Seattle, WA 98195-2350  
Affiliation: Dept. of Computer Science and Engineering Dept. of Computer Science and Engineering University of California, San Diego University of Washington  
Abstract: UCSD CSE Technical Report CS98-587 UW CSE Technical Report UW-CSE-98-06-02 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, J. Kubiatowicz, D. Kranz, B-H. Lim, D. Yeung, G. D'Souza, and M. Parkin. Sparcle: </author> <title> An evolutionary processor design for large-scale multiprocessors. </title> <booktitle> IEEE Micro, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Otherwise, software retries the critical section. Lock-free synchronization is effective when it succeeds, because the synchronization is embodied in the memory access itself | the only extra overhead is the check for failure. The multithreaded processors on the Tera [2] and Alewife <ref> [1] </ref> machines rely on full/empty (F/E) bits associated with each memory block. F/E bits allow memory access and lock acquisition with a single instruction, such as read if full/set empty, where the full/empty bit acts as the lock, and the data is returned only if the lock succeeds.
Reference: [2] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Otherwise, software retries the critical section. Lock-free synchronization is effective when it succeeds, because the synchronization is embodied in the memory access itself | the only extra overhead is the check for failure. The multithreaded processors on the Tera <ref> [2] </ref> and Alewife [1] machines rely on full/empty (F/E) bits associated with each memory block. <p> These studies showed that an SMT processor achieves 20 significant speedups on code that is parallelized for multiprocessors, but that assumptions about certain compiler transformations appropriate for a multiprocessor are not necessarily appropriate for SMT. The Tera MTA architecture <ref> [2] </ref> supports fine-grain parallelism via multithreading and full-empty memory synchronization. Despite having a higher-latency synchronization mechanism than what we propose, the Tera processor and compilation system exploit fine-grain synchronization among cooperating threads, confirming our thesis that a different style of code generation is required for multithreaded processors.
Reference: [3] <author> T.E. Anderson. </author> <title> The performance of spin lock alternatives for shared memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 1-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Most common are Spin Locks, such as test-and-set. While logically test-and-set modifies memory, optimizations typically allow the spinning to take place in the local cache to reduce bus traffic <ref> [3, 14] </ref>. More recently, Lock-Free synchronization has been widely studied [9, 11] and is included in modern instruction sets, e.g., the DEC Alpha's load-locked (ldl l) and store-conditional (stl c) [17] (collectively, LL-SC). <p> High performance implies both high throughput and low latency. 2. Resource-conservative. Most parallel machines have spin locks of some type, which require repeated reads or writes. Lock-free synchronization also includes a potential repeated retry of a lock. On SMPs, Anderson, et al. <ref> [3] </ref> favor queueing locks and test-and-test-and-set over test-and-set locks; in that situation, it is acceptable for a blocked process to waste processor-private resources (e.g., local cpu, local cache), but not shared resources (e.g., bus and memory bandwidth).
Reference: [4] <author> T.E. Anderson, E. Lazowska, and H.M. Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12), </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: 1 Introduction The performance and style of a multiprocessor's synchronization mechanisms determine to a large extent the granularity of parallelism that can be exploited on that machine. For this reason, substantial effort has gone into studying synchronization techniques for shared-memory multiprocessors <ref> [14, 4] </ref>, with the objective of supporting fine-grained parallelism. Despite this work, synchronization and communication are still relatively costly on contemporary multiprocessors; this high cost is inherent in the hardware levels at which inter-thread synchronization and communication must occur.
Reference: [5] <author> J.P. Bradford and S. Abraham. </author> <title> Efficient synchronizatin for multithreaded processors. In Proceedings of the Workshop on Multithreaded Execution Architecture and Compilation in conjunction with HPCA-4, </title> <month> January </month> <year> 1998. </year>
Reference-contexts: It takes responsibility for retrying the lock so that software does not have to loop. They do not otherwise block the thread, nor do they associate releases with locks in their structure. Bradford and Abraham <ref> [5] </ref> propose hardware-implemented semaphores which block a thread waiting for a semaphore.
Reference: [6] <author> S.J. Eggers, J.S. Emer, H.M. Levy, J.L. Lo, </author> <title> R.L. Stamm, and D.M. Tullsen. Simultaneous multithreading: A foundation for next-generation processors. </title> <booktitle> IEEE Micro, </booktitle> <month> September </month> <year> 1997. </year>
Reference-contexts: Multithreaded processors, such as SMT, provide an opportunity to greatly decrease synchronization cost, because the communicating threads are internal to a single processor. While previous work has shown the benefits of SMT on parallel workloads <ref> [6, 13] </ref>, those studies relied on traditional synchronization mechanisms, ignoring the potential advantages (and problems) of synchronizing in an SMT CPU. <p> Other work which has impacted or is related to this study follows. There have been several papers that describe the design of simultaneous multithreading processors [18] and analyze their performance on parallel <ref> [12, 6, 13] </ref> and multiprogrammed [18, 6] workloads. The first group [12, 6, 13], which examined simultaneous multithreading with a more traditional multiprocessor workload, and in light of more traditional parallel compiler transformations is the most relevant to this work. <p> Other work which has impacted or is related to this study follows. There have been several papers that describe the design of simultaneous multithreading processors [18] and analyze their performance on parallel [12, 6, 13] and multiprogrammed <ref> [18, 6] </ref> workloads. The first group [12, 6, 13], which examined simultaneous multithreading with a more traditional multiprocessor workload, and in light of more traditional parallel compiler transformations is the most relevant to this work. <p> Other work which has impacted or is related to this study follows. There have been several papers that describe the design of simultaneous multithreading processors [18] and analyze their performance on parallel <ref> [12, 6, 13] </ref> and multiprogrammed [18, 6] workloads. The first group [12, 6, 13], which examined simultaneous multithreading with a more traditional multiprocessor workload, and in light of more traditional parallel compiler transformations is the most relevant to this work.
Reference: [7] <author> M.W. Hall et al. </author> <title> Maximizing multiprocessor performance with the SUIF compiler. </title> <journal> IEEE Computer, </journal> <volume> 29(12), </volume> <month> December </month> <year> 1996. </year>
Reference-contexts: For the unordered variables, they were each updated atomically using LL-SC. The overall performance was poor due to spinning for contested locks. 17 executing with 8 threads. 6.2 Livermore Loops Livermore loops 6, 13, and 14 all have cross-iteration dependencies, and consequently the SUIF compiler <ref> [7] </ref> chose not to parallelize them. These loops have a reasonable amount of code that is independent, however, and should be amenable to parallelization, if fine-grained parallelism (and synchronization) were available. Loop 6 is a doubly-nested loop that reads a triangular region of a matrix.
Reference: [8] <author> M. Fillo, S.W. Keckler, W.J. Dally, N.P. Carter, A. Chang, Y. Gurevich, and W.S. Lee. </author> <title> The M-Machine multicomputer. </title> <booktitle> In 28th Annual International Symposium on Microarchitecture, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: If it fails, the access stalls at the memory or is returned to the CPU to be retried. Another multithreaded processor, the M-Machine <ref> [8, 10] </ref>, attaches full/empty bits to registers. In the M-Machine, multiple threads execute on each of several clusters. Each cluster has a unique register file with full/empty bits attached to all registers.
Reference: [9] <author> M. Herlihy. </author> <title> A methodology for implementing highly concurrent data objects. </title> <booktitle> In 2nd ACM Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 197-206, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Most common are Spin Locks, such as test-and-set. While logically test-and-set modifies memory, optimizations typically allow the spinning to take place in the local cache to reduce bus traffic [3, 14]. More recently, Lock-Free synchronization has been widely studied <ref> [9, 11] </ref> and is included in modern instruction sets, e.g., the DEC Alpha's load-locked (ldl l) and store-conditional (stl c) [17] (collectively, LL-SC).
Reference: [10] <author> S.W. Keckler, W.J. Dally, D. Maskit, N.P. Carter, A. Chang, and W.S. Lee. </author> <title> Exploiting fine-grain thread level parallelism on the MIT multi-alu processor. </title> <booktitle> In 25th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: If it fails, the access stalls at the memory or is returned to the CPU to be retried. Another multithreaded processor, the M-Machine <ref> [8, 10] </ref>, attaches full/empty bits to registers. In the M-Machine, multiple threads execute on each of several clusters. Each cluster has a unique register file with full/empty bits attached to all registers. <p> A read of that register by the local thread will not succeed until the register has been filled. Keckler et al. <ref> [10] </ref> provide a good description of these mechanisms in a study with similar goals to ours. <p> Despite having a higher-latency synchronization mechanism than what we propose, the Tera processor and compilation system exploit fine-grain synchronization among cooperating threads, confirming our thesis that a different style of code generation is required for multithreaded processors. Keckler, et al., <ref> [10] </ref> describe mechanisms for efficient synchronization, communication, and thread creation in the Multi-ALU Processor (MAP). There are several key differences between their mechanisms and ours, and between the target applications of the two studies. These result from the two different machine models, as discussed in Section 2.1.
Reference: [11] <author> L. Lamport. </author> <title> Concurrent reading and writing. </title> <journal> Communications of the ACM, </journal> <volume> 11(1) </volume> <pages> 806-811, </pages> <month> November </month> <year> 1977. </year>
Reference-contexts: Most common are Spin Locks, such as test-and-set. While logically test-and-set modifies memory, optimizations typically allow the spinning to take place in the local cache to reduce bus traffic [3, 14]. More recently, Lock-Free synchronization has been widely studied <ref> [9, 11] </ref> and is included in modern instruction sets, e.g., the DEC Alpha's load-locked (ldl l) and store-conditional (stl c) [17] (collectively, LL-SC).
Reference: [12] <author> J.L. Lo, S.J. Eggers, J.S. Emer, H.M. Levy, S.S. Parekh, </author> <title> R.L. Stamm, and D.M. Tullsen. Converting thread-level parallelism into instruction-level parallelism via simultaneous multithreading. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 322-354, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: Other work which has impacted or is related to this study follows. There have been several papers that describe the design of simultaneous multithreading processors [18] and analyze their performance on parallel <ref> [12, 6, 13] </ref> and multiprogrammed [18, 6] workloads. The first group [12, 6, 13], which examined simultaneous multithreading with a more traditional multiprocessor workload, and in light of more traditional parallel compiler transformations is the most relevant to this work. <p> Other work which has impacted or is related to this study follows. There have been several papers that describe the design of simultaneous multithreading processors [18] and analyze their performance on parallel <ref> [12, 6, 13] </ref> and multiprogrammed [18, 6] workloads. The first group [12, 6, 13], which examined simultaneous multithreading with a more traditional multiprocessor workload, and in light of more traditional parallel compiler transformations is the most relevant to this work.
Reference: [13] <author> J.L. Lo, S.J. Eggers, H.M. Levy, S.S. Parekh, </author> <title> and D.M. Tullsen. Tuning compiler optimizations for simultaneous multi-threading. </title> <booktitle> In 30th Annual International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1997. </year>
Reference-contexts: Multithreaded processors, such as SMT, provide an opportunity to greatly decrease synchronization cost, because the communicating threads are internal to a single processor. While previous work has shown the benefits of SMT on parallel workloads <ref> [6, 13] </ref>, those studies relied on traditional synchronization mechanisms, ignoring the potential advantages (and problems) of synchronizing in an SMT CPU. <p> Other work which has impacted or is related to this study follows. There have been several papers that describe the design of simultaneous multithreading processors [18] and analyze their performance on parallel <ref> [12, 6, 13] </ref> and multiprogrammed [18, 6] workloads. The first group [12, 6, 13], which examined simultaneous multithreading with a more traditional multiprocessor workload, and in light of more traditional parallel compiler transformations is the most relevant to this work. <p> Other work which has impacted or is related to this study follows. There have been several papers that describe the design of simultaneous multithreading processors [18] and analyze their performance on parallel <ref> [12, 6, 13] </ref> and multiprogrammed [18, 6] workloads. The first group [12, 6, 13], which examined simultaneous multithreading with a more traditional multiprocessor workload, and in light of more traditional parallel compiler transformations is the most relevant to this work.
Reference: [14] <author> J.M. Mellor-Crummey and M.L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1), </volume> <month> February </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The performance and style of a multiprocessor's synchronization mechanisms determine to a large extent the granularity of parallelism that can be exploited on that machine. For this reason, substantial effort has gone into studying synchronization techniques for shared-memory multiprocessors <ref> [14, 4] </ref>, with the objective of supporting fine-grained parallelism. Despite this work, synchronization and communication are still relatively costly on contemporary multiprocessors; this high cost is inherent in the hardware levels at which inter-thread synchronization and communication must occur. <p> Most common are Spin Locks, such as test-and-set. While logically test-and-set modifies memory, optimizations typically allow the spinning to take place in the local cache to reduce bus traffic <ref> [3, 14] </ref>. More recently, Lock-Free synchronization has been widely studied [9, 11] and is included in modern instruction sets, e.g., the DEC Alpha's load-locked (ldl l) and store-conditional (stl c) [17] (collectively, LL-SC).
Reference: [15] <author> B.A. Nayfeh, L. Hammond, and K. Olukotun. </author> <title> Evaluation of design alternatives for a multiprocessor microprocessor. </title> <booktitle> In 23nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 67-77, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: While the single-chip multiprocessor (SMP-L2) is only half as efficient as SMT-block, it is much closer to SMT-block than to the memory-based multiprocessor (SMP-MEM). Therefore, many of our arguments about new opportunities for parallelism extend to the single-chip multiprocessor model <ref> [15] </ref> as well. for this benchmark the primary factor is not resource waste due to spinning, but the latency of the synchronization operation. This is made clear by examining the critical path through successive iterations of the for loop.
Reference: [16] <author> V.S. Pai, P. Ranganathan, S.V. Adve, and T. Harton. </author> <title> An evaluation of memory consistency models for shared-memory systems with ilp processors. </title> <booktitle> In Seventh International Conference on Architectural Support for Programming Languages and Operating Systems. ACM, </booktitle> <year> 1996. </year>
Reference-contexts: These result from the two different machine models, as discussed in Section 2.1. Their mechanism does not scale in our environment, and our acquire-release is not possible in theirs. Our focus is on uniprocessor speedup on traditionally serial applications. Pai, et al. <ref> [16] </ref> describe a synchronization buffer for multiprocessors of single-threaded processors. The synchronization buffer is an off-chip structure which holds lock addresses from executing lock instructions. It takes responsibility for retrying the lock so that software does not have to loop.
Reference: [17] <author> R.L. </author> <title> Sites and R.T. Witek. Alpha AXP Architecture Reference Manual, Second Edition. </title> <publisher> Digital Press, </publisher> <year> 1995. </year>
Reference-contexts: More recently, Lock-Free synchronization has been widely studied [9, 11] and is included in modern instruction sets, e.g., the DEC Alpha's load-locked (ldl l) and store-conditional (stl c) <ref> [17] </ref> (collectively, LL-SC). Rather than achieve mutual exclusion by preventing multiple threads from entering the critical section, lock-free synchronization prevents more than one thread from successfully writing data and exiting the critical section. <p> This scheme uses the lock-free synchronization currently supported by the Alpha. To implement the ordered access in the benchmark, the acquire primitive is implemented with load locked and store conditional, as given in <ref> [17] </ref>, and the release is a store instruction. * SMP-*. These each use the same primitives as SMT-block, but force the synchronization (and data sharing) to occur at different levels in the memory hierarchy (i.e., the L2 cache, the L3 cache, or memory).
Reference: [18] <author> D.M. Tullsen, S.J. Eggers, J.S. Emer, H.M. Levy, J.L. Lo, and R.L. Stamm. </author> <title> Exploiting choice: Instruction fetch and issue on an implementable simultaneous multithreading processor. </title> <booktitle> In 23nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 191-202, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: As a result, compilers and programmers must decompose parallel applications in a relatively coarse-grained way in order to reduce synchronization overhead. This paper examines fine-grained synchronization on a simultaneous multithreaded (SMT) processor | a processor in which the CPU can issue instructions from multiple threads in a single cycle <ref> [19, 18] </ref>. Multithreaded processors, such as SMT, provide an opportunity to greatly decrease synchronization cost, because the communicating threads are internal to a single processor. <p> Table 1 provides more details of the processor model; Table 2 contains memory system parameters. We simulate the ICOUNT.2.8 instruction fetch scheme from <ref> [18] </ref>, which fetches up to eight instructions from up to two threads each cycle. ICOUNT.2.8 is the most aggressive fetch scheme proposed for SMT, and is much more tolerant of synchronization mechanisms that are not resource conservative than less aggressive schemes. <p> Other work which has impacted or is related to this study follows. There have been several papers that describe the design of simultaneous multithreading processors <ref> [18] </ref> and analyze their performance on parallel [12, 6, 13] and multiprogrammed [18, 6] workloads. The first group [12, 6, 13], which examined simultaneous multithreading with a more traditional multiprocessor workload, and in light of more traditional parallel compiler transformations is the most relevant to this work. <p> Other work which has impacted or is related to this study follows. There have been several papers that describe the design of simultaneous multithreading processors [18] and analyze their performance on parallel [12, 6, 13] and multiprogrammed <ref> [18, 6] </ref> workloads. The first group [12, 6, 13], which examined simultaneous multithreading with a more traditional multiprocessor workload, and in light of more traditional parallel compiler transformations is the most relevant to this work.
Reference: [19] <author> D.M. Tullsen, S.J. Eggers, and H.M. Levy. </author> <title> Simultaneous multithreading: Maximizing on-chip parallelism. </title> <booktitle> In 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392-403, </pages> <month> June </month> <year> 1995. </year> <month> 22 </month>
Reference-contexts: As a result, compilers and programmers must decompose parallel applications in a relatively coarse-grained way in order to reduce synchronization overhead. This paper examines fine-grained synchronization on a simultaneous multithreaded (SMT) processor | a processor in which the CPU can issue instructions from multiple threads in a single cycle <ref> [19, 18] </ref>. Multithreaded processors, such as SMT, provide an opportunity to greatly decrease synchronization cost, because the communicating threads are internal to a single processor.
References-found: 19

