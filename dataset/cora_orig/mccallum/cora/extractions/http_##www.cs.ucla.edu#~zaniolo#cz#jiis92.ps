URL: http://www.cs.ucla.edu/~zaniolo/cz/jiis92.ps
Refering-URL: http://www.cs.ucla.edu/~zaniolo/cz/ZanioloPapers.html
Root-URL: http://www.cs.ucla.edu
Email: zaniolo@cs.ucla.edu  
Title: Intelligent Databases: Old Challenges and New Opportunities  
Author: Carlo Zaniolo 
Address: Los Angeles, CA 90024  
Affiliation: Computer Science Department, University of California  
Abstract: The evolution of existing information systems and a new wave of data-intensive applications are creating a strong demand for database-centered programming environments much more sophisticated and intelligent than those supported by current database systems. In this paper, we describe the contributions that deductive databases offer to the evolution of databases and information systems to statisfy said demands. In addition to all database essentials, deductive databases support rule-based logic-oriented languages that allow terse formulations of complete applications, along with reasoning and queries. Thus, they support a rule-based interface that eliminates the impedance mismatch problem (between programming language and query sublanguage) and elevates the design and development of database applications to the level of declarative, knowledge-based specifications. In this paper, we review the evolution of the enabling technology and architectures of deductive database prototypes; then we focus on their applications, as seen by the author through his experience with the LDL/ LDL++ project. In particular, the paper describes the languages and the (bottom-up) execution technolgy used by the first generation of deductive database prototypes. Then the paper discusses how the experience with a first-generation system (LDL) guided the design and implementation of a second-generation prototype (LDL++). 
Abstract-found: 1
Intro-found: 1
Reference: [Ack91] <author> Ackley, D., "Process-Object-State: </author> <title> an integrated modeling method," A Framework of Information Systems Architecture, </title> <booktitle> Conference, </booktitle> <address> March 25-27, Virginia, </address> <note> 1991 (reprints available from author: 210 Almeira Ave, Freemont, CA 94539, 415 656-1665) </note>
Reference-contexts: The advantages offered by a deductive database environment in this domain were confirmed during the one-year field study described in [Aeta], which summarizes the experience of using LDL in conjunction with a structured-design methodology called POS (Process, Object and State) <ref> [Ack91, Tryon] </ref>. A key idea of the POS methodology is that of using the ER framework for modeling both dynamic and static aspects of the enterprise [Ack91]. <p> A key idea of the POS methodology is that of using the ER framework for modeling both dynamic and static aspects of the enterprise <ref> [Ack91] </ref>. By using the notions of aggregation and abstraction within the ER framework, to capture what has traditionally been thought of as derived data, the ER model can specify most of the processing associated with a specific problem domain.
Reference: [Aeta] <author> Ackley, D., et al. </author> <title> "System Analysis for Deductive Database Environments: an Enhanced role for Aggregate Entities," Procs. </title> <booktitle> 9th Int. Conference on Entity-Relationship Approach, </booktitle> <address> Lausanne, CH, </address> <month> Oct. </month> <pages> 8-10, </pages> <year> 1990. </year>
Reference-contexts: The advantages offered by a deductive database environment in this domain were confirmed during the one-year field study described in <ref> [Aeta] </ref>, which summarizes the experience of using LDL in conjunction with a structured-design methodology called POS (Process, Object and State) [Ack91, Tryon]. A key idea of the POS methodology is that of using the ER framework for modeling both dynamic and static aspects of the enterprise [Ack91]. <p> laws, such as those pertaining to fuel consumption and transfer of ownership. 9 In an informal study, also including a comparison with alternative prototyping frameworks, LDL proved very effective in this role and preferable to other approaches in terms of naturalness of coding, terseness, and readability of the resulting programs <ref> [Aeta, Tryon] </ref>. To a large extend, this is the result of the inherent ability of logic-based rule system to express and enforce business rules.
Reference: [Ceta] <editor> Chimenti, D. et al., </editor> <title> "The LDL System Prototype," </title> <journal> IEEE Journal on Data and Knowledge Engineering, </journal> <volume> vol. 2, no. 1, </volume> <pages> pp. 76-90, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: In this paper, we will focus primarily on the LDL project <ref> [Ceta] </ref>, due to the level of maturity of the LDL/LDL++ prototypes and the author's familiarity with them. Valuable knowledge was acquired on technology development and technology transfer during the eight year life of the project, and can be summarized as follows: 1. <p> A global analysis is performed at compile time to evaluate the bound arguments and free arguments of predicates, on suitable representations such as the Rule/Goal graph [Ullm] or the predicate connection graph <ref> [Ceta] </ref>. For a general idea of this global analysis is performed, consider the following example: usanc (X, Y) &lt;- anc (X,Y), born (Y, usa). anc (X,Z) &lt;- parent (X,Z). anc (X,Z) &lt;- parent (X,Y), anc (Y,Z). <p> The current LDL system is based on a uniform interface supporting get-next commands on databases residing in main memory or secondary store. The single-tuple interface supplies various opportunities for intelligent backtracking and existential variables optimization, exploited by the compiler to obtain good performance from the object code <ref> [Ceta] </ref>. The intermediate object code is actually C, to support portability and a open architecture whereby external procedures can be incorporated into LDL and treated as database predicates. Other experimental systems differ in several ways from the architecture of Figure 3.
Reference: [CoSh] <author> Connell, J.L. and Shafer, </author> <title> L.B., "Structured Rapid Prototyping", </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference: [CGT] <author> Ceri, S., G. Gottlob and L. Tanca, </author> <title> "Logic Programming and Deductive Databases," </title> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: This similarities, and the realization that Prolog represents a query language much more powerful than SQL [Zan1], enticed several researchers into building intelligent database systems by coupling Prolog with relational DBMS or by extending Prolog with database capabilities <ref> [CGT] </ref>. While these experiments have been successful in producing powerful systems, they have also revealed several problems that stand in the way of a complete integration.
Reference: [Dozi] <author> Dozier, J., </author> <title> "Access to Data In NASA's Earth Observation System (Abstract)," </title> <booktitle> Proc. ACM-SIGMOD Conference on Management of Data, </booktitle> <year> 1992. </year> <title> [DM89] "The Rapid Prototyping Conundrum", </title> <journal> DATAMATION, </journal> <month> June </month> <year> 1989. </year>
Reference-contexts: Key to the success of these initiatives is the sharing of data and knowledge obtained by several teams of cooperating researchers [PeMa, Eric]. Databases are needed for sharing, protecting and organizing the project's scientific information, which frequently amounts to gigabytes or terabytes <ref> [PeMa, Eric, Dozi] </ref>. Deductive databases have much to offer for these applications, where the limitations of current database technology are obvious [SiSU]. In particular, considerable experience was gained with using LDL in solving various microbiology problems relating to the Human Genome initiative [Eric].
Reference: [Eric] <author> Erickson, D., </author> <title> "Hacking the Genome," </title> <publisher> Scientific American, </publisher> <month> April 92. </month>
Reference-contexts: Key to the success of these initiatives is the sharing of data and knowledge obtained by several teams of cooperating researchers <ref> [PeMa, Eric] </ref>. Databases are needed for sharing, protecting and organizing the project's scientific information, which frequently amounts to gigabytes or terabytes [PeMa, Eric, Dozi]. Deductive databases have much to offer for these applications, where the limitations of current database technology are obvious [SiSU]. <p> Key to the success of these initiatives is the sharing of data and knowledge obtained by several teams of cooperating researchers [PeMa, Eric]. Databases are needed for sharing, protecting and organizing the project's scientific information, which frequently amounts to gigabytes or terabytes <ref> [PeMa, Eric, Dozi] </ref>. Deductive databases have much to offer for these applications, where the limitations of current database technology are obvious [SiSU]. In particular, considerable experience was gained with using LDL in solving various microbiology problems relating to the Human Genome initiative [Eric]. <p> Deductive databases have much to offer for these applications, where the limitations of current database technology are obvious [SiSU]. In particular, considerable experience was gained with using LDL in solving various microbiology problems relating to the Human Genome initiative <ref> [Eric] </ref>. In addition to being used in developing several molecular biology applications, LDL's rules were used for modeling and supporting scientific taxonomies and concepts|thus filling the gap between high-level models and the low-level experimental data [Ts1].
Reference: [For82] <author> Forgy, C. L., </author> <title> Rete: a Fast Algorithm for the Many Pattern/Many Object Pattern Match Problem, </title> <booktitle> Artificial Intelligence 19 (1), </booktitle> <pages> pp. 17-37, </pages> <year> 1982. </year>
Reference-contexts: There are significant differences even with respect to Horn Clauses, as illustrated by the fact that in deductive databases programs are less dependent on a particular execution model, such as forward-chaining or backward-chaining. A Prolog programmer can only write rules that work with backward chaining; an OPS5 programmer <ref> [For82] </ref> can only write rules that work in a forward chaining mode. By contrast, systems such as LDL [NaTs] and Nail!~citeMeta, select the proper inference mode automatically, enabling the user to focus on the logical correctness of the rules rather than on the underlying execution strategy.
Reference: [Gane] <author> Gane, C. </author> <title> "Rapid System Development," </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference: [Gell] <author> Gellene, D., </author> <title> "Airlines Discourage Bargain Hunts| Travel Agencies Face Hefty Fees to Use New Computer Programs," </title> <address> Los Angeles Times, </address> <month> September 10, </month> <year> 1991, </year> <pages> pp. </pages> <institution> D1, D7 (Business Section). </institution> <month> 14 </month>
Reference-contexts: otherwise assists dozens of flight analysts and operational researchers." A cutting-edge information technology such as American Airlines' Sabre is not without risks: travel agents have started using a package developed by Associated Travel Management to "scan through a reservation system and snare customer-pleasing bargains that an agent might never spot" <ref> [Gell] </ref>. <p> Of course, we have not yet seen the last chapter of this story, inasmuch as Associated Travel Management is currently developing a `stealth' version of the program, where the number of hits is reduced through intelligent browsing and memorization,... <ref> [Gell] </ref>. The airlines' tale of measures and countermeasures is symptomatic of a corporate world where the information system becomes an enterprise's most critical tool for surviving and succeeding in business.
Reference: [GeLi] <author> M. Gelfond and V. Lifschitz. </author> <title> The stable model semantics of logic programming. </title> <booktitle> In Proc. Fifth Int. Conference on Logic Programming, </booktitle> <pages> pp. 1070-1080, </pages> <year> 1988. </year>
Reference-contexts: The problem of going beyond stratification represents a topic of much current research, with contributions and ideas coming from the areas of AI, non-monotonic logic and deductive databases. Among the most important contributions, there is the introduction of the concepts of well-founded models [GeRS] and stable models <ref> [GeLi] </ref>, which provide a declarative semantics for most programs of practical interest, including programs pertaining to various aspects of bill-of-materials applications. Formal declarative semantics, however, only represents one of the requirements that must be satisfied before non-monotonic programs can be allowed in the recursive rules of the language.
Reference: [GGZ] <author> Ganguly, S., S. Greco and C. Zaniolo, </author> <title> "Minimum and Maximum Predicates in Logic Programming," </title> <booktitle> Proc. 10th, ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, </booktitle> <pages> pp. 154-164, </pages> <year> 1991. </year>
Reference-contexts: This decision was taken to facilitate the writing of simpler and more efficient programs for critical algorithms for graph minimization problems <ref> [GGZ] </ref>, or Bill-of-Materials applications. While a complete discussion of this complex topic is beyond the scope of this paper, the following discussion on the evolution of non-deterministic choice construct, will give the reader a concrete idea of the research problems faced and solution approach taken.
Reference: [GPSZ] <author> Giannotti, F. Pedreschi, D., Sacca, D., and C. Zaniolo. </author> <title> Nondeterminism in deductive databases. </title> <booktitle> In Proc. 2nd Int. Conf. on Deductive and Object-Oriented Databases, </booktitle> <year> 1991. </year>
Reference-contexts: These problems were avoided by using instead a semantics based on the use of negation and stable models <ref> [GPSZ] </ref>.
Reference: [Hopp] <author> Hopper, D.E., </author> <title> "Rattling SABRE|New Ways to Compete on Information," Harvard Business Review, </title> <booktitle> May-June 1990, </booktitle> <pages> pp. 118-125. </pages>
Reference-contexts: The urgency of these changes is illustrated by what is happening in the more competitive industries. Consider, for instance, the fiercely competitive airline industry and their aggressive approach to `yield management' using their reservation systems, such as Sabre (American Airlines) and Apollo (United Airlines). Quoting <ref> [Hopp] </ref>, yield management is the "process of establishing different prices for seats and allocating seats to maximize revenues. <p> This usage of databases|in the past primarily associated with the intelligence community| is now becoming pervasive in medicine and science. Data Dredging is also becoming common practice in such business applications as selective marketing and yield-management <ref> [Hopp] </ref>. 8 The source of the data is typically a large volume of low-level records, collected from measurements and monitoring of empirical processes, intelligence operations and businesses. The problem is how to use this data to verify certain conjectures and to help refine or formulate hypotheses.
Reference: [KiMS] <author> Kiernan, G., C. de Maindreville, and E. </author> <title> Simon "Making Deductive Database a Practical Technology: a step forward," </title> <booktitle> Proc. 1990 ACM-SIGMOD Conference on Management of Data, </booktitle> <pages> pp. 237-246. </pages>
Reference-contexts: algorithms, which are not well-suited for an implementation based on secondary store. 3 Because of the problems just mentioned, many of the deductive database projects, started in the mid 80's, such as LDL, Nail! and Lola, chose to support Horn clauses and logic rules using extensions of relational database technology <ref> [Meta, Seta, KiMS] </ref>. These projects thus abandoned SLD-resolution in favor of a bottom-up (fixpoint-based) approach [Meta, Seta, KiMS]. Deductive Databases represent the first major research trend of the 80s in the area of databases and programming languages. <p> the problems just mentioned, many of the deductive database projects, started in the mid 80's, such as LDL, Nail! and Lola, chose to support Horn clauses and logic rules using extensions of relational database technology <ref> [Meta, Seta, KiMS] </ref>. These projects thus abandoned SLD-resolution in favor of a bottom-up (fixpoint-based) approach [Meta, Seta, KiMS]. Deductive Databases represent the first major research trend of the 80s in the area of databases and programming languages.
Reference: [KiWu] <author> Kifer, M. and J. Wu, </author> <title> "A Logic for Object-Oriented Programming (Maier's O-Logic Revisite), </title> <booktitle> Proc. 8th, ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Management (PODS), </booktitle> <year> 1989. </year>
Reference-contexts: However, a deductive approach does not preclude object orientation, and several research efforts are currently devoted at combining these two paradigms <ref> [KiWu, Zan2] </ref> 3 Deductive Databases After generating significant theoretical interest in the the 70's deductive databases experienced a remarkable technological growth in the second half of 80's, when several research projects were started using a bottom-up approach [Meta, Seta].
Reference: [KrNa] <author> R. Krishnamurthy and S. Naqvi. </author> <title> "Non-deterministic choice in Datalog," </title> <booktitle> In Proceedings of the 3rd International Conference on Data and Knowledge Bases, </booktitle> <year> 1988. </year>
Reference-contexts: While a complete discussion of this complex topic is beyond the scope of this paper, the following discussion on the evolution of non-deterministic choice construct, will give the reader a concrete idea of the research problems faced and solution approach taken. The idea of choice was introduced in <ref> [KrNa] </ref> to express non-determinism in a declarative fashion. Thus, a construct such as choice (X, Y) is used to denote that the functional dependency X ! Y must hold in the model defining the meaning of this program. <p> In LDL, this very powerful construct was disallowed in recursion, inasmuch as the functional-dependency based semantics proposed in <ref> [KrNa] </ref> suffers from technical problems such as a lack of justifiability property and its unsuitability to efficient implementation due to its static nature [SaZa]. These problems were avoided by using instead a semantics based on the use of negation and stable models [GPSZ].
Reference: [KNZ] <author> Krishnamurthy, R., S. Naqvi and C. Zaniolo, </author> <title> "Database Transactions in LDL", </title> <booktitle> Proc. Logic Programming North American Conference, </booktitle> <pages> pp. 795-830, </pages> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Furthermore, none of the nine different semantics for updates in Prolog proposed so far [Moss] are compatible with that of the relational data model. Indeed, the snapshot-based semantics of relational databases is incompatible with Prolog's execution model, which is instead oriented toward pipelined execution <ref> [KNZ] </ref>. Supporting the notion of transactions, which is totally alien to Prolog, compounds these problems. * The efficiency of Prolog's execution model is predicated upon the use of main memory. <p> In LDL, the database is viewed as time varying, with the notions of recovery and database transactions deeply engrained in the semantics of the language <ref> [KNZ] </ref>; the rule set and the database schema that define the program are instead time-invariant. Rules contain three kinds of constructs: 4 1. Horn-clause based constructs 2. Non-monotonic logic-based constructs (such as negation, aggregate operators and choice) 3. Imperative constructs (such as, updates and I/O).
Reference: [Meta] <author> Morris, K. et al. </author> <title> "YAWN! (Yet Another Window on NAIL!), </title> <journal> Data Engineering, Vol.10, </journal> <volume> No. 4, </volume> <pages> pp. 28-44, </pages> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: algorithms, which are not well-suited for an implementation based on secondary store. 3 Because of the problems just mentioned, many of the deductive database projects, started in the mid 80's, such as LDL, Nail! and Lola, chose to support Horn clauses and logic rules using extensions of relational database technology <ref> [Meta, Seta, KiMS] </ref>. These projects thus abandoned SLD-resolution in favor of a bottom-up (fixpoint-based) approach [Meta, Seta, KiMS]. Deductive Databases represent the first major research trend of the 80s in the area of databases and programming languages. <p> the problems just mentioned, many of the deductive database projects, started in the mid 80's, such as LDL, Nail! and Lola, chose to support Horn clauses and logic rules using extensions of relational database technology <ref> [Meta, Seta, KiMS] </ref>. These projects thus abandoned SLD-resolution in favor of a bottom-up (fixpoint-based) approach [Meta, Seta, KiMS]. Deductive Databases represent the first major research trend of the 80s in the area of databases and programming languages. <p> orientation, and several research efforts are currently devoted at combining these two paradigms [KiWu, Zan2] 3 Deductive Databases After generating significant theoretical interest in the the 70's deductive databases experienced a remarkable technological growth in the second half of 80's, when several research projects were started using a bottom-up approach <ref> [Meta, Seta] </ref>. In this paper, we will focus primarily on the LDL project [Ceta], due to the level of maturity of the LDL/LDL++ prototypes and the author's familiarity with them. <p> Other experimental systems differ in several ways from the architecture of Figure 3. For instance, Nail! uses a relational algebra-based intermediate code, and employs capture rules (rather than cost-prediction based optimization, to drive the selection of a proper execution strategy <ref> [Meta] </ref>).
Reference: [Moss] <author> Moss, C., </author> <title> "Cut and Paste|defining the Impure Primitives of Prolog", </title> <booktitle> Proc. Third Int. Conference on Logic Programming, </booktitle> <address> London, </address> <month> July </month> <year> 1986, </year> <pages> pp. 686-694. </pages>
Reference-contexts: For instance, in the style of many AI systems, Prolog update constructs (i.e., assert and retract) are powerful but unruly, inasmuch as they can modify both the data and the program. Furthermore, none of the nine different semantics for updates in Prolog proposed so far <ref> [Moss] </ref> are compatible with that of the relational data model. Indeed, the snapshot-based semantics of relational databases is incompatible with Prolog's execution model, which is instead oriented toward pipelined execution [KNZ].
Reference: [NaTs] <author> S. A. Naqvi, S. </author> <title> Tsur "A Logical Language for Data and Knowledge Bases", </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <year> 1989. </year>
Reference-contexts: A Prolog programmer can only write rules that work with backward chaining; an OPS5 programmer [For82] can only write rules that work in a forward chaining mode. By contrast, systems such as LDL <ref> [NaTs] </ref> and Nail!~citeMeta, select the proper inference mode automatically, enabling the user to focus on the logical correctness of the rules rather than on the underlying execution strategy. This point is better illustrated by an example. A methane molecule consists of a carbon atom linked with four hydrogen atoms. <p> The first two examples can be supported only through a forward chaining computation, which, in turn, translates naturally into the least-fixpoint computation that defines the model-theoretic based semantics of recursive Horn clause programs <ref> [NaTs] </ref>. The least fixpoint computation amounts to an iterative procedure, where partial results are added to a relation until a steady state is reached. <p> The declarative semantics and programming paradigm of deductive data bases extend beyond Horn clause programming, to include non-monotonic logic-based constructs, such as negation, sets and choice operators. Thus, deductive databases currently support stratified negation <ref> [NaTs, Ullm] </ref>, which is more powerful than negation-by-failure provided by Prolog. LDL also supports stratified aggregates.
Reference: [PeMa] <author> Pechura, M. and Martin J. </author> <title> (eds),"Mapping the Brain and its Functions," </title> <publisher> National Academy Press, </publisher> <address> D.C., </address> <year> 1991. </year>
Reference-contexts: Key to the success of these initiatives is the sharing of data and knowledge obtained by several teams of cooperating researchers <ref> [PeMa, Eric] </ref>. Databases are needed for sharing, protecting and organizing the project's scientific information, which frequently amounts to gigabytes or terabytes [PeMa, Eric, Dozi]. Deductive databases have much to offer for these applications, where the limitations of current database technology are obvious [SiSU]. <p> Key to the success of these initiatives is the sharing of data and knowledge obtained by several teams of cooperating researchers [PeMa, Eric]. Databases are needed for sharing, protecting and organizing the project's scientific information, which frequently amounts to gigabytes or terabytes <ref> [PeMa, Eric, Dozi] </ref>. Deductive databases have much to offer for these applications, where the limitations of current database technology are obvious [SiSU]. In particular, considerable experience was gained with using LDL in solving various microbiology problems relating to the Human Genome initiative [Eric].
Reference: [PDR] <author> Phipps, G., M.A., Derr and K. A. Ross, </author> <title> "Glue-Nail: a Deductive Database System," </title> <booktitle> Proc. 1991 ACM-SIGMOD Conference on Management of Data, </booktitle> <pages> pp. </pages> <month> 308-317 </month> <year> (1991). </year>
Reference-contexts: Yet, this experience has also revealed the need for substantial improvements and extensions, which has motivated the design and development of a second generation of deductive database systems, such as the Glue-Nail! system, which supports a a procedural shell called Glue that is closely integrated with declarative rules <ref> [PDR] </ref>, and the LDL++ system. (Also a system such as CORAL [RaSS], that builds on the experiences with the first generation can be classified as a second-generation prototype.) The prototypes of the second generation offer significant improvements and, in particular they (i) correct the limitations of their predecessors, (ii) reinforce their <p> Furthermore, since compiled LDL++ modules become C++ classes, they can be freely used in any C++ program. 5.3 Advances in the Enabling Technology Deductive database technology is fast-maturing and important advances have been included in LDL++, such as meta-level predicates with first order semantics, as <ref> [PDR] </ref>. A major extension featured by LDL++ is its support for certain classes of non-stratified programs with negation and aggregates in recursion. The problem of going beyond stratification represents a topic of much current research, with contributions and ideas coming from the areas of AI, non-monotonic logic and deductive databases.
Reference: [RaSS] <author> Ramakrishan, R., Srivastava, D. and Sudarshan, S., </author> <title> "CORAL: A Deductive Database Programming Language," </title> <booktitle> Proc. VLDB`92 Int. Conf, </booktitle> <year> 1992. </year>
Reference-contexts: The CORAL system also supports more general binding patterns, whereby certain variables of a predicate can remain unbound after the execution of a goal <ref> [RaSS] </ref>. 4 Applications A most encouraging aspects of the LDL experience is represented by the range and significance of application domains for which it was found that deductive databases offer unique advantages. <p> substantial improvements and extensions, which has motivated the design and development of a second generation of deductive database systems, such as the Glue-Nail! system, which supports a a procedural shell called Glue that is closely integrated with declarative rules [PDR], and the LDL++ system. (Also a system such as CORAL <ref> [RaSS] </ref>, that builds on the experiences with the first generation can be classified as a second-generation prototype.) The prototypes of the second generation offer significant improvements and, in particular they (i) correct the limitations of their predecessors, (ii) reinforce their strengths and (iii) include the more recent advances in the enabling
Reference: [RoSh] <author> Rowe, L. and Shoens K., </author> <title> "Data Abstraction, Views and Updates in RIGEL," </title> <booktitle> Proc. ACM-SIGMOD Int. Conference on Management of Data, </booktitle> <address> Boston, MA, </address> <year> 1979. </year>
Reference-contexts: Among the early attempts by database researchers, we find RIGEL <ref> [RoSh] </ref> and Pascal-R [Schm], which focused on a better fusion 2 of procedural languages and relational systems, and thus inherited the limitations of the relational data model.
Reference: [SaZa] <author> Sacca, D., and Zaniolo, C., </author> <title> "Stable models and non determinism in logic programs with negation", </title> <booktitle> Proc. 9th, ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, </booktitle> <pages> pp. 205-218, </pages> <year> 1990. </year>
Reference-contexts: In LDL, this very powerful construct was disallowed in recursion, inasmuch as the functional-dependency based semantics proposed in [KrNa] suffers from technical problems such as a lack of justifiability property and its unsuitability to efficient implementation due to its static nature <ref> [SaZa] </ref>. These problems were avoided by using instead a semantics based on the use of negation and stable models [GPSZ].
Reference: [Schm] <author> Schmidt, J. </author> <title> "Some High Level Language Constructs for Data Type Relation," </title> <journal> ACM TODS, </journal> <volume> Vol. 2, No. 3, </volume> <month> Sept </month> <year> 1977. </year>
Reference-contexts: Among the early attempts by database researchers, we find RIGEL [RoSh] and Pascal-R <ref> [Schm] </ref>, which focused on a better fusion 2 of procedural languages and relational systems, and thus inherited the limitations of the relational data model.
Reference: [Seta] <author> Schmidt, H. </author> <title> et al "Combining Deduction by Certainty with the Power of Magic" Proc. </title> <booktitle> 1st Int. Conf. on Deductive and O-O Databases, </booktitle> <month> Dec. </month> <pages> 4-6, </pages> <address> 1989, Kyoto, Japan. </address> <month> 15 </month>
Reference-contexts: algorithms, which are not well-suited for an implementation based on secondary store. 3 Because of the problems just mentioned, many of the deductive database projects, started in the mid 80's, such as LDL, Nail! and Lola, chose to support Horn clauses and logic rules using extensions of relational database technology <ref> [Meta, Seta, KiMS] </ref>. These projects thus abandoned SLD-resolution in favor of a bottom-up (fixpoint-based) approach [Meta, Seta, KiMS]. Deductive Databases represent the first major research trend of the 80s in the area of databases and programming languages. <p> the problems just mentioned, many of the deductive database projects, started in the mid 80's, such as LDL, Nail! and Lola, chose to support Horn clauses and logic rules using extensions of relational database technology <ref> [Meta, Seta, KiMS] </ref>. These projects thus abandoned SLD-resolution in favor of a bottom-up (fixpoint-based) approach [Meta, Seta, KiMS]. Deductive Databases represent the first major research trend of the 80s in the area of databases and programming languages. <p> orientation, and several research efforts are currently devoted at combining these two paradigms [KiWu, Zan2] 3 Deductive Databases After generating significant theoretical interest in the the 70's deductive databases experienced a remarkable technological growth in the second half of 80's, when several research projects were started using a bottom-up approach <ref> [Meta, Seta] </ref>. In this paper, we will focus primarily on the LDL project [Ceta], due to the level of maturity of the LDL/LDL++ prototypes and the author's familiarity with them.
Reference: [SiSU] <author> Silverschatz, A., M. Stonebraker and J. Ullman (eds), </author> <title> `Database Systems: Achievements and Op--portunities. The Lagunita report of the NSF inv. </title> <booktitle> workshop on the future of database systems research, </booktitle> <address> held in Palo Alto, Ca, </address> <month> Feb 22-23, </month> <booktitle> 1990" SIGMOD RECORD, </booktitle> <volume> Vol. 19, No. 4, </volume> <month> Dec </month> <year> 1990. </year> <journal> [SRH] "The Implementation of POSTGERS" IEEE Journal on Data and Knowledge Engineering, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 125-143, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: 1 Introduction The ever-increasing complexity of information systems and a new wave of data-intensive applications demand computing environments that are much more advanced than those supported by current Data Base Management Systems (DBMS) <ref> [SiSU] </ref>. Two serious manifestations of the inadequacy of current commercial technology are as follows: * The impedance mismatch problem. <p> procedural language and the embedded query language complicate the task of designing, writing and maintaining information systems, and also reduces the run-time performance of applications. fl Paper appeared in the Journal of Intellingent Information Systems, 1, 271-292, Kluwer Academic, December, 1992. 1 * The new wave of database applications problem <ref> [SiSU] </ref>. In the past, the problems of SQL were circum-scribed within specialized domains, such as the Bill-of-Materials applications that require the computation of transitive closures. <p> Databases are needed for sharing, protecting and organizing the project's scientific information, which frequently amounts to gigabytes or terabytes [PeMa, Eric, Dozi]. Deductive databases have much to offer for these applications, where the limitations of current database technology are obvious <ref> [SiSU] </ref>. In particular, considerable experience was gained with using LDL in solving various microbiology problems relating to the Human Genome initiative [Eric].
Reference: [Tryon] <author> Tryon, D. </author> <title> "Deductive Computing: </title> <booktitle> Living in the Future," Proc. of the Monterey Software Conference, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: The advantages offered by a deductive database environment in this domain were confirmed during the one-year field study described in [Aeta], which summarizes the experience of using LDL in conjunction with a structured-design methodology called POS (Process, Object and State) <ref> [Ack91, Tryon] </ref>. A key idea of the POS methodology is that of using the ER framework for modeling both dynamic and static aspects of the enterprise [Ack91]. <p> laws, such as those pertaining to fuel consumption and transfer of ownership. 9 In an informal study, also including a comparison with alternative prototyping frameworks, LDL proved very effective in this role and preferable to other approaches in terms of naturalness of coding, terseness, and readability of the resulting programs <ref> [Aeta, Tryon] </ref>. To a large extend, this is the result of the inherent ability of logic-based rule system to express and enforce business rules. <p> Dynamic relationships dealing with validated events can be handled in a similar way. Such a declarative description of the business entities and activities is the key for successful enterprise modeling, rapid prototyping of new applications and the integration of various information systems via an enterprise schema <ref> [Tryon] </ref>, which is discussed next. 4.4 Enterprise Integration For historical reasons, current operations in most companys are supported by patchwork collections of information systems and database systems, whose cooperation is impeded by the following discrepancies: * Different representations for the same logical information, * Different data models and DBMS, * Different
Reference: [Ts1] <author> Tsur S., </author> <title> `Deductive Databases in Action,' </title> <booktitle> Proc. 10th, ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, </booktitle> <pages> pp. 205-218, </pages> <year> 1990. </year>
Reference-contexts: We now summarize some of the most significant applications, which are discussed in more details in <ref> [Ts1, Ts2] </ref>. 4.1 Scientific Databases A major trend in modern science is toward large cooperative efforts in areas such as molecular biology, earth science or neural science. Key to the success of these initiatives is the sharing of data and knowledge obtained by several teams of cooperating researchers [PeMa, Eric]. <p> In addition to being used in developing several molecular biology applications, LDL's rules were used for modeling and supporting scientific taxonomies and concepts|thus filling the gap between high-level models and the low-level experimental data <ref> [Ts1] </ref>. Preliminary investigations on applying the LDL technology to support geophysical information systems have also produced encouraging results, but brought to focus the need for supporting Abstract Data Types (ADTs) to provide specialized mechanism for spatial and temporal reasoning in rule-based systems.
Reference: [Ts2] <author> Tsur S., </author> <title> "Data Dredging," </title> <journal> Data Engineering, </journal> <volume> Vol. 13, No. 4, </volume> <publisher> IEEE Computer Society, </publisher> <month> Dec. 90. </month>
Reference-contexts: We now summarize some of the most significant applications, which are discussed in more details in <ref> [Ts1, Ts2] </ref>. 4.1 Scientific Databases A major trend in modern science is toward large cooperative efforts in areas such as molecular biology, earth science or neural science. Key to the success of these initiatives is the sharing of data and knowledge obtained by several teams of cooperating researchers [PeMa, Eric]. <p> These topics will be further discussed in the next section. 4.2 Data Dredging and Knowledge Discovery These terms denote the emerging computational paradigm which supports "knowledge extraction" from, and the "discovery process" on the ever-growing repository of stored data <ref> [Ts2] </ref>. This usage of databases|in the past primarily associated with the intelligence community| is now becoming pervasive in medicine and science.
Reference: [Ullm] <author> Ullman, J.D., </author> <title> "Database and Knowledge-Based Systems, Vols. I and II, </title> <publisher> Computer Science Press, </publisher> <address> Rockville, Md., </address> <year> 1989. </year>
Reference-contexts: Horn clauses are near relatives of database query languages (as per the textbook exercises of mapping a non-recursive Datalog program into domain calculus, tuple calculus, and relational query languages such as SQL <ref> [Ullm] </ref>). This similarities, and the realization that Prolog represents a query language much more powerful than SQL [Zan1], enticed several researchers into building intelligent database systems by coupling Prolog with relational DBMS or by extending Prolog with database capabilities [CGT]. <p> Now, deductive databases solve this problem equally well, by using techniques such as the Magic Set Method, or the Counting Method that simulate backward chaining through a pair of coupled fixpoint computations <ref> [Ullm] </ref>. Since fixpoint computations check newly generated values against the set of previous values, cycles are handled automatically. This is a most useful feature since cyclic graphs are often stored in the database; furthermore, derived relations can also be cyclic. <p> The declarative semantics and programming paradigm of deductive data bases extend beyond Horn clause programming, to include non-monotonic logic-based constructs, such as negation, sets and choice operators. Thus, deductive databases currently support stratified negation <ref> [NaTs, Ullm] </ref>, which is more powerful than negation-by-failure provided by Prolog. LDL also supports stratified aggregates. <p> A global analysis is performed at compile time to evaluate the bound arguments and free arguments of predicates, on suitable representations such as the Rule/Goal graph <ref> [Ullm] </ref> or the predicate connection graph [Ceta]. For a general idea of this global analysis is performed, consider the following example: usanc (X, Y) &lt;- anc (X,Y), born (Y, usa). anc (X,Z) &lt;- parent (X,Z). anc (X,Z) &lt;- parent (X,Y), anc (Y,Z). <p> unifying the query goal with the head of the usanc rule, we obtain the adorned rule: usanc bf &lt; anc bf ; born bb : This adornment assumes that the first argument of born is bound by the second argument of anc, according to a sideway information passing principle (SIP) <ref> [Ullm] </ref>. The next question to arise is whether the recursive goal anc bf is supportable. <p> The recursive predicate anc can also be solved efficiently: in fact, a further analysis indicates that the recursive rule is left-linear <ref> [Ullm] </ref> and that the given adornment can, after some rewriting of the rules, be supported by a single-fixpoint computation [Ullm]. When the recursive predicate cannot be supported through a single fixpoint, other methods are used, including the counting method, and the very general magic set method [Ullm]. 7 form is given <p> The recursive predicate anc can also be solved efficiently: in fact, a further analysis indicates that the recursive rule is left-linear <ref> [Ullm] </ref> and that the given adornment can, after some rewriting of the rules, be supported by a single-fixpoint computation [Ullm]. When the recursive predicate cannot be supported through a single fixpoint, other methods are used, including the counting method, and the very general magic set method [Ullm]. 7 form is given (a query form is a query template with an indication of bound/free arguments) is to propagate constants into recursive <p> recursive rule is left-linear <ref> [Ullm] </ref> and that the given adornment can, after some rewriting of the rules, be supported by a single-fixpoint computation [Ullm]. When the recursive predicate cannot be supported through a single fixpoint, other methods are used, including the counting method, and the very general magic set method [Ullm]. 7 form is given (a query form is a query template with an indication of bound/free arguments) is to propagate constants into recursive rules and to extract the subset of rules relevant to this particular query.
Reference: [UlZa] <author> Ullman, J. and C. Zaniolo, </author> <title> "Deductive Databases, Achievements and Future Directions," </title> <journal> SIGMOD Record, pp. </journal> <volume> 77-83, Vol. 19, No. 4, </volume> <publisher> ACM Press, </publisher> <month> Dec. </month> <year> 1990. </year>
Reference: [GeRS] <author> A. Van Gelder, K.A. Ross, and J.S. Schlipf. </author> <title> The well-founded semantics for general logic programs. </title> <journal> Journal of ACM, </journal> <volume> 38(3) </volume> <pages> 620-650, </pages> <year> 1991. </year>
Reference-contexts: The problem of going beyond stratification represents a topic of much current research, with contributions and ideas coming from the areas of AI, non-monotonic logic and deductive databases. Among the most important contributions, there is the introduction of the concepts of well-founded models <ref> [GeRS] </ref> and stable models [GeLi], which provide a declarative semantics for most programs of practical interest, including programs pertaining to various aspects of bill-of-materials applications.
Reference: [WAM] <author> Warren, D.H.D., </author> <title> "An Abstract Prolog Instruction Set," </title> <type> Tech. Note 309, </type> <institution> AI Center, Computer Science and Technology Div., SRI, </institution> <year> 1983. </year>
Reference-contexts: Supporting the notion of transactions, which is totally alien to Prolog, compounds these problems. * The efficiency of Prolog's execution model is predicated upon the use of main memory. Indeed, all current Prolog implementations <ref> [WAM] </ref> rely on pointers, stacks and full unification algorithms, which are not well-suited for an implementation based on secondary store. 3 Because of the problems just mentioned, many of the deductive database projects, started in the mid 80's, such as LDL, Nail! and Lola, chose to support Horn clauses and logic <p> A rule rewriting approach is also used to support the idempotence and commutativity properties of set terms. Since recursion is implemented by fixpoint iterations, and only matching is needed at execution time, the abstract target machine and code can be greatly simplified, with respect to that of Prolog <ref> [WAM] </ref>; thus, it can also be based on simple extensions of relational algebra. For instance, the first (limited) LDL prototype generated code for an intermediate relational-algebra language for a parallel database machine.
Reference: [Wahl] <author> Wahl, D., </author> <title> "Bill of Materials in Relational Databases-an analysis of current research and its applications to manufacturing databases," </title> <institution> Digital Equipment Corp. </institution> <note> Internal Report, Febr., 91. </note>
Reference-contexts: Inventory control and Bill-of-Materials applications were among the first to be generated on the LDL system, with independent studies confirming the potential of the technology in this application domain <ref> [Wahl] </ref>. However, several of the applications developed in this domain had to use lists or imperative programs, since LDL does not allow aggregates or negation in recursive definitions|a limitation known as stratification.
Reference: [Zan1] <author> Zaniolo, C., </author> <title> "Prolog, a Database Query Language for All Seasons," </title> <booktitle> Proc. 1st Int. Workshop on Expert Database Systems, </booktitle> <address> Kiawah Island, SC, </address> , <month> Oct. </month> <year> 1984, </year> <month> (Benjain/Cumming) </month>
Reference-contexts: Horn clauses are near relatives of database query languages (as per the textbook exercises of mapping a non-recursive Datalog program into domain calculus, tuple calculus, and relational query languages such as SQL [Ullm]). This similarities, and the realization that Prolog represents a query language much more powerful than SQL <ref> [Zan1] </ref>, enticed several researchers into building intelligent database systems by coupling Prolog with relational DBMS or by extending Prolog with database capabilities [CGT]. While these experiments have been successful in producing powerful systems, they have also revealed several problems that stand in the way of a complete integration.
Reference: [Zan2] <author> Zaniolo, C. </author> <title> "Object Identity and Inheritance in Deductive Databases: an Evolutionary Approach," </title> <booktitle> Proc. 1st Int. Conf. on Deductive and O-O Databases, </booktitle> <month> Dec. </month> <pages> 4-6, </pages> <address> 1989, Kyoto, Japan. </address> <month> 16 </month>
Reference-contexts: However, a deductive approach does not preclude object orientation, and several research efforts are currently devoted at combining these two paradigms <ref> [KiWu, Zan2] </ref> 3 Deductive Databases After generating significant theoretical interest in the the 70's deductive databases experienced a remarkable technological growth in the second half of 80's, when several research projects were started using a bottom-up approach [Meta, Seta].
References-found: 39

