URL: http://www.neci.nj.nec.com/homepages/pny/papers/vptree/vptree.ps
Refering-URL: http://www.neci.nj.nec.com/homepages/pny/papers/vptree/main.html
Root-URL: 
Title: Data Structures and Algorithms for Nearest Neighbor Search in General Metric Spaces which the distribution
Author: Peter N. Yianilos 
Keyword: Metric Space, Nearest Neighbor, Computational Geometry, Associative Memory, Randomized Methods, Pattern Recognition, Clustering.  
Note: Also relevant are high-dimensional Euclidian settings in  
Abstract: We consider the computational problem of finding nearest neighbors in general metric spaces. Of particular interest are spaces that may not be conveniently embedded or approximated in Euclidian space, or where the dimensionality of a Euclidian representation is very high. The theoretical basis for this approach is developed and the results of several experiments are reported. In Euclidian cases, kd-tree performance is compared. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. L. Kelly, </author> <title> General Topology. </title> <address> New York: D. </address> <publisher> Van Nos-trand, </publisher> <year> 1955. </year>
Reference-contexts: It is also highly intuitive notion that seems to correspond in some way with the kind of inexact associative recall that is clearly a component of biological intelligence. One useful abstraction of nearness is provided by the classical notion of a mathematical metric space <ref> [1] </ref>. Eu-clidian n-space is but one example of a metric space. The Nearest Neighbor field includes the study of decision making and learning based on neighborhoods, the underlying metrics and spaces, and the matter of computing/searching for the neighborhood about a point. See [2] for a recent survey. <p> Also observe that while computing NN (q), one may by reduce t as ever nearer neighbors are encountered. We write NNj t (q; S D ) to denote this important notion of t restricted search. Next we will assume that the range of the space's distance distance function is <ref> [0; 1] </ref>. Since any metric's range may be compressed to this interval without affecting the nearest-neighbor relation 2 , this restriction may be made without loss of generality 3 . 2.2 Vantage Points. <p> Here, full-space search is unavoidable. However, to the extent that information is present, we will see that nearest neighbor search may benefit. We begin by formalizing this notion of perspective and defining a related pseudo-distance function: Definition 1 Let (S; d) be a <ref> [0; 1] </ref> bounded metric space. Given a distinguished element p 2 S, we make the following definitions for all a; b 2 S: 1. p : S ! [0; 1] is given by: p (a) = d (a; p). 2. d p : S fi S ! [0; 1] is given <p> We begin by formalizing this notion of perspective and defining a related pseudo-distance function: Definition 1 Let (S; d) be a <ref> [0; 1] </ref> bounded metric space. Given a distinguished element p 2 S, we make the following definitions for all a; b 2 S: 1. p : S ! [0; 1] is given by: p (a) = d (a; p). 2. d p : S fi S ! [0; 1] is given by: d p (a; b) = j p (a) p (b)j = jd (a; p) d (b; p)j. <p> d) be a <ref> [0; 1] </ref> bounded metric space. Given a distinguished element p 2 S, we make the following definitions for all a; b 2 S: 1. p : S ! [0; 1] is given by: p (a) = d (a; p). 2. d p : S fi S ! [0; 1] is given by: d p (a; b) = j p (a) p (b)j = jd (a; p) d (b; p)j. Function p is best thought of as a projection of S into [0; 1], from the perspective of p. <p> given by: p (a) = d (a; p). 2. d p : S fi S ! <ref> [0; 1] </ref> is given by: d p (a; b) = j p (a) p (b)j = jd (a; p) d (b; p)j. Function p is best thought of as a projection of S into [0; 1], from the perspective of p. I.e. it is S as seen by p, via d. Function d p is not in general a metric since if a and b are distinct but equidistant from p, d p (a; b) = 0. <p> For some p 2 S D consider now the image p (S D ) of S D in <ref> [0; 1] </ref>. Next denote by the median of p (S D ) thus dividing [0; 1] into [0; ) and [; 1]. The first of these intervals contains points from the inside the sphere S (p; ), while the second consists of points from outside and on the surface. <p> For some p 2 S D consider now the image p (S D ) of S D in <ref> [0; 1] </ref>. Next denote by the median of p (S D ) thus dividing [0; 1] into [0; ) and [; 1]. The first of these intervals contains points from the inside the sphere S (p; ), while the second consists of points from outside and on the surface. <p> For some p 2 S D consider now the image p (S D ) of S D in [0; 1]. Next denote by the median of p (S D ) thus dividing [0; 1] into <ref> [0; ) and [; 1] </ref>. The first of these intervals contains points from the inside the sphere S (p; ), while the second consists of points from outside and on the surface. <p> For some p 2 S D consider now the image p (S D ) of S D in [0; 1]. Next denote by the median of p (S D ) thus dividing [0; 1] into [0; ) and <ref> [; 1] </ref>. The first of these intervals contains points from the inside the sphere S (p; ), while the second consists of points from outside and on the surface. <p> Now N L = jS p L j counts the points of S D mapped left to [0; ), while N R = jS p R j counts those sent right to <ref> [; 1] </ref>. We can say little in general about the comparative sizes of N L and N R since nothing has been assumed about the nature of the metric space. <p> Thus, given some such point or interval X, we may for the sake of notational simplicity refer to P (X) which is understood to mean P ( 1 p (X)). So if x 2 <ref> [0; 1] </ref>, then P (x) is the probability of the surface of sphere S (p; x). Now in the case of the discrete metric, there is about every point a spherical surface of probability one. <p> The probability of a countable intersection of nested sets, is just the limit of the individual probabilities. This and ZPS then imply that about any x 2 <ref> [0; 1] </ref>, there exists an interval of arbitrarily small probability. Nothing more than this basic observation is necessary to establish the two simple theorems that follow. It is however worth remarking that uniform convergence to zero probability can be established. <p> Nothing more than this basic observation is necessary to establish the two simple theorems that follow. It is however worth remarking that uniform convergence to zero probability can be established. I.e., given * &gt; 0, there exists N sufficiently large, so that every interval of the canonical <ref> [0; 1] </ref> N -partitioning, has probability less than *. We now make more rigorous our earlier comments re garding procedures for recursive space bisection. Theorem 1 Let (S; d; P ), be a [0; 1] bounded metric space with the ZPS property under probability measure P . <p> I.e., given * &gt; 0, there exists N sufficiently large, so that every interval of the canonical <ref> [0; 1] </ref> N -partitioning, has probability less than *. We now make more rigorous our earlier comments re garding procedures for recursive space bisection. Theorem 1 Let (S; d; P ), be a [0; 1] bounded metric space with the ZPS property under probability measure P . <p> This is analogous to vector quantization in Eu-clidian space. Thinking of the root-leaf path as a binary code, we might also interpret it as metric space hashing. Theorem 2 Let (S; d; P ), be a <ref> [0; 1] </ref> bounded metric space with the ZPS property under probability measure P . <p> The expected size of each list is clearly one. So the final expected number of metric evaluations remains M dlog 2 (n) + 1e. 2 These theorems describe binary constructions to motivate the algorithms and experiments which follow; but higher degree trees may be built by further partitioning <ref> [0; 1] </ref>. In the extreme case the root has degree n and only three metric evaluations are expected. As a practical matter however, the t values necessary to approach even log 2 (n) search are so small as to be of little practical value.
Reference: [2] <author> B. V. Dasarathy, ed., </author> <title> Nearest neighbor pattern classification techniques. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Eu-clidian n-space is but one example of a metric space. The Nearest Neighbor field includes the study of decision making and learning based on neighborhoods, the underlying metrics and spaces, and the matter of computing/searching for the neighborhood about a point. See <ref> [2] </ref> for a recent survey. Our focus is on search, and, for simplicity, on locating any single nearest neighbor.
Reference: [3] <author> P. N. Yianilos, </author> <title> "A dedicated comparator matches symbol strings fast and intelligently," </title> <journal> Electronics Magazine, </journal> <month> December </month> <year> 1983. </year>
Reference-contexts: We introduce the vp-tree (vantage point tree) in several forms as one solution. This data structure and the algorithms to build and search it were first discovered and implemented by the author during 1986-87 1 in conjunction with the development of improved retrieval techniques relating to the PF474 device <ref> [3] </ref>. Motivation was provided by the fact that this chip's notion of string distance is non-Euclidian. Here elements of the metric space are strings, E.g., a database of city names and associated postal codes. This early work was described in [4].
Reference: [4] <author> P. N. Yianilos, </author> <title> "New methods for neighborhood searches in metric spaces." </title> <type> Invited Talk: </type> <institution> The Institute for Defense Analyses, Princeton, NJ, </institution> <month> July 23 </month> <year> 1990. </year>
Reference-contexts: Motivation was provided by the fact that this chip's notion of string distance is non-Euclidian. Here elements of the metric space are strings, E.g., a database of city names and associated postal codes. This early work was described in <ref> [4] </ref>. Independently, Uhlmann has reported the same basic structure [5, 6] calling it a metric tree. There is a sizable Nearest Neighbor Search literature, and the vp-tree should properly be viewed as related to and descended from many earlier contributions which we now proceed to summarize.
Reference: [5] <author> J. K. Uhlmann, </author> <title> "Satisfying general proximity/similarity queries with metric trees," </title> <journal> Information Processing Letters, </journal> <month> November </month> <year> 1991. </year>
Reference-contexts: Motivation was provided by the fact that this chip's notion of string distance is non-Euclidian. Here elements of the metric space are strings, E.g., a database of city names and associated postal codes. This early work was described in [4]. Independently, Uhlmann has reported the same basic structure <ref> [5, 6] </ref> calling it a metric tree. There is a sizable Nearest Neighbor Search literature, and the vp-tree should properly be viewed as related to and descended from many earlier contributions which we now proceed to summarize. Burkhard and Keller in [7] present three file structures for nearest neighbor retrieval.
Reference: [6] <author> J. K. Uhlmann, </author> <title> "Metric trees," </title> <journal> Applied Mathematics Letters, </journal> <volume> vol. 4, no. 5, </volume> <year> 1991. </year>
Reference-contexts: Motivation was provided by the fact that this chip's notion of string distance is non-Euclidian. Here elements of the metric space are strings, E.g., a database of city names and associated postal codes. This early work was described in [4]. Independently, Uhlmann has reported the same basic structure <ref> [5, 6] </ref> calling it a metric tree. There is a sizable Nearest Neighbor Search literature, and the vp-tree should properly be viewed as related to and descended from many earlier contributions which we now proceed to summarize. Burkhard and Keller in [7] present three file structures for nearest neighbor retrieval.
Reference: [7] <author> W. A. Burkhard and R. M. Keller, </author> <title> "Some approaches to best-match file searching," </title> <journal> Communications of the ACM, </journal> <volume> vol. 16, </volume> <month> April </month> <year> 1973. </year>
Reference-contexts: Independently, Uhlmann has reported the same basic structure [5, 6] calling it a metric tree. There is a sizable Nearest Neighbor Search literature, and the vp-tree should properly be viewed as related to and descended from many earlier contributions which we now proceed to summarize. Burkhard and Keller in <ref> [7] </ref> present three file structures for nearest neighbor retrieval. All three involve picking distinguished elements, and structuring according to distance from these members. Their techniques are coordinate free. The data structures amount to multi-way trees corresponding to integral valued metrics only. <p> Collections of graphs are considered in [11] as an abstract metric space with a metric assuming discrete values only. This work is thus highly related to the constructions of <ref> [7] </ref>. In their concluding remarks the authors correctly anticipate generalization to more continuous settings such as R n . The kd-tree of Friedman and Bentley [12, 13, 14, 15] has emerged as a useful tool in Euclidian spaces of moderate dimension. <p> The ZPS distribution restriction is key to achieving them; and our overall outlook in which finite cases are imagined to be drawn from a larger more continuous space, distinguishes in part this work from the discrete distance setting of <ref> [7, 11] </ref>. 2.6 Set Perspectives. We have seen that a single distinguished element p, induces a pseudo-metric d p , which is always dominated by d.
Reference: [8] <author> K. Fukunaga, </author> <title> "A branch and bound algorithm for computing k-nearest neighbors," </title> <journal> IEEE Transactions on Computers, </journal> <month> July </month> <year> 1975. </year>
Reference-contexts: Burkhard and Keller in [7] present three file structures for nearest neighbor retrieval. All three involve picking distinguished elements, and structuring according to distance from these members. Their techniques are coordinate free. The data structures amount to multi-way trees corresponding to integral valued metrics only. Fukunaga in <ref> [8, 9] </ref> exploits the triangle inequality to reduce distance computations searching a hierarchical decomposition of Euclidian Space. His methods are restricted to a Euclidian setting only by his use of a computed mean point for each subset. However this is not an essential component of his approach at least conceptually. <p> This contrasts with the coordinate aligned hyperplanar cuts of the kd-tree (See Figures 1 & 2), and the use of computed Eu-clidian cluster centroids in <ref> [8] </ref>. Randomized algorithms for vp-tree construction execute in O (n log (n)) time and the resulting tree occupies linear space. For completeness, early work dealing with two special cases should be mentioned. <p> By this point however, neither perform very well visiting roughly 25% of the nodes. So despite their ignorance of the coordinate structure of the space, and randomized construction, vp-trees compare well in this setting with the kd-tree. Comparisons with the experiments of <ref> [8] </ref> are more difficult because leaf level buckets are employed. Nevertheless, based purely on metric evaluations, the kd-tree and vp/vp s -tree methods produce superior results. 4.2 Random Linear Embedding.
Reference: [9] <author> K. Fukunaga, </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, Inc., </publisher> <editor> second ed., </editor> <year> 1990. </year>
Reference-contexts: Burkhard and Keller in [7] present three file structures for nearest neighbor retrieval. All three involve picking distinguished elements, and structuring according to distance from these members. Their techniques are coordinate free. The data structures amount to multi-way trees corresponding to integral valued metrics only. Fukunaga in <ref> [8, 9] </ref> exploits the triangle inequality to reduce distance computations searching a hierarchical decomposition of Euclidian Space. His methods are restricted to a Euclidian setting only by his use of a computed mean point for each subset. However this is not an essential component of his approach at least conceptually.
Reference: [10] <author> G. Salton and A. Wong, </author> <title> "Generation and search of clustered files," </title> <journal> ACM Transactions on Database Systems, </journal> <month> December </month> <year> 1978. </year>
Reference-contexts: His methods are restricted to a Euclidian setting only by his use of a computed mean point for each subset. However this is not an essential component of his approach at least conceptually. He recursively employs standard clustering <ref> [10] </ref> techniques to effect the decomposition and then branch-and-bound searches the resulting data structure. Dur 1 First coded and tested in July of 1987 during a Proximity Technology Inc. company workshop in Sunnyvale California. Participants in addition to the author were Samuel Buss (then at U.C.
Reference: [11] <author> C. D. Feustel and L. G. Shapiro, </author> <title> "The nearest neighbor problem in an abstract metric space," </title> <journal> Pattern Recognition Letters, </journal> <month> December </month> <year> 1982. </year>
Reference-contexts: A key point apparently overlooked, is that when the query is well inside of a cluster, the exterior need not be searched. Collections of graphs are considered in <ref> [11] </ref> as an abstract metric space with a metric assuming discrete values only. This work is thus highly related to the constructions of [7]. In their concluding remarks the authors correctly anticipate generalization to more continuous settings such as R n . <p> The ZPS distribution restriction is key to achieving them; and our overall outlook in which finite cases are imagined to be drawn from a larger more continuous space, distinguishes in part this work from the discrete distance setting of <ref> [7, 11] </ref>. 2.6 Set Perspectives. We have seen that a single distinguished element p, induces a pseudo-metric d p , which is always dominated by d.
Reference: [12] <author> J. H. Friedman, F. Baskett, and L. J. Shustek, </author> <title> "An algorithm for finding nearest neighbors," </title> <journal> IEEE Transactions on Computers, </journal> <month> October </month> <year> 1975. </year>
Reference-contexts: This work is thus highly related to the constructions of [7]. In their concluding remarks the authors correctly anticipate generalization to more continuous settings such as R n . The kd-tree of Friedman and Bentley <ref> [12, 13, 14, 15] </ref> has emerged as a useful tool in Euclidian spaces of moderate dimension. Improvements relating to high-dimensional settings, distribution adaptation, and incremental searches, are described in [16], [17], and [18] respectively. A kd-tree is built by recursively bisecting the database using single coordinate position cuts.
Reference: [13] <author> J. H. Friedman, J. L. Bentley, and R. A. Finkel, </author> <title> "An algorithm for finding best matches in logarithmic expected time," </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> vol. 3, </volume> <month> September </month> <year> 1977. </year>
Reference-contexts: This work is thus highly related to the constructions of [7]. In their concluding remarks the authors correctly anticipate generalization to more continuous settings such as R n . The kd-tree of Friedman and Bentley <ref> [12, 13, 14, 15] </ref> has emerged as a useful tool in Euclidian spaces of moderate dimension. Improvements relating to high-dimensional settings, distribution adaptation, and incremental searches, are described in [16], [17], and [18] respectively. A kd-tree is built by recursively bisecting the database using single coordinate position cuts. <p> A modular software system was built with plug-in metric spaces and search routines. Two basic metric space settings were implemented: hypercubes, and image fragments. Both support various metrics including the standard Minkowski family. 4.1 Hypercubes. We very nearly reproduce certain of the experimental settings from <ref> [13] </ref> and use them to begin our process of evaluation. Hypercubes of increasing dimension are first considered, each containing a database consisting of 8,192 coordinate-wise normally distributed pseudorandom points.
Reference: [14] <author> J. L. Bentley and J. H. Friedman, </author> <title> "Data structures for range searching," </title> <journal> Computing Surveys, </journal> <month> December </month> <year> 1979. </year>
Reference-contexts: This work is thus highly related to the constructions of [7]. In their concluding remarks the authors correctly anticipate generalization to more continuous settings such as R n . The kd-tree of Friedman and Bentley <ref> [12, 13, 14, 15] </ref> has emerged as a useful tool in Euclidian spaces of moderate dimension. Improvements relating to high-dimensional settings, distribution adaptation, and incremental searches, are described in [16], [17], and [18] respectively. A kd-tree is built by recursively bisecting the database using single coordinate position cuts.
Reference: [15] <author> J. L. Bentley, </author> <title> "Multidimensional divide-and-conquer," </title> <journal> Communications of the ACM, </journal> <volume> vol. 23, </volume> <month> April </month> <year> 1980. </year>
Reference-contexts: This work is thus highly related to the constructions of [7]. In their concluding remarks the authors correctly anticipate generalization to more continuous settings such as R n . The kd-tree of Friedman and Bentley <ref> [12, 13, 14, 15] </ref> has emerged as a useful tool in Euclidian spaces of moderate dimension. Improvements relating to high-dimensional settings, distribution adaptation, and incremental searches, are described in [16], [17], and [18] respectively. A kd-tree is built by recursively bisecting the database using single coordinate position cuts.
Reference: [16] <author> C. M. Eastman and S. F. Weiss, </author> <title> "Tree structures for high dimensionality nearest neighbor searching," </title> <journal> Information Systems, </journal> <volume> vol. 7, no. 2, </volume> <year> 1982. </year>
Reference-contexts: The kd-tree of Friedman and Bentley [12, 13, 14, 15] has emerged as a useful tool in Euclidian spaces of moderate dimension. Improvements relating to high-dimensional settings, distribution adaptation, and incremental searches, are described in <ref> [16] </ref>, [17], and [18] respectively. A kd-tree is built by recursively bisecting the database using single coordinate position cuts. For a given coordinate, the database is cut at the median of the distribution generated by projection onto that coordinate.
Reference: [17] <author> B. S. Kim and S. B. Park, </author> <title> "A fast nearest neighbor finding algorithm based on the ordered partition," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 8, </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: The kd-tree of Friedman and Bentley [12, 13, 14, 15] has emerged as a useful tool in Euclidian spaces of moderate dimension. Improvements relating to high-dimensional settings, distribution adaptation, and incremental searches, are described in [16], <ref> [17] </ref>, and [18] respectively. A kd-tree is built by recursively bisecting the database using single coordinate position cuts. For a given coordinate, the database is cut at the median of the distribution generated by projection onto that coordinate.
Reference: [18] <author> A. J. Broder, </author> <title> "Strategies for efficient incremental nearest neighbor search," </title> <journal> Pattern Recognition, </journal> <volume> vol. 23, no. 1/2, </volume> <year> 1990. </year>
Reference-contexts: The kd-tree of Friedman and Bentley [12, 13, 14, 15] has emerged as a useful tool in Euclidian spaces of moderate dimension. Improvements relating to high-dimensional settings, distribution adaptation, and incremental searches, are described in [16], [17], and <ref> [18] </ref> respectively. A kd-tree is built by recursively bisecting the database using single coordinate position cuts. For a given coordinate, the database is cut at the median of the distribution generated by projection onto that coordinate.
Reference: [19] <author> R. L. Rivest, </author> <title> "On the optimality of Elias's algorithm for performing best-match searches," </title> <booktitle> Information Processing 74, </booktitle> <year> 1974. </year>
Reference-contexts: Randomized algorithms for vp-tree construction execute in O (n log (n)) time and the resulting tree occupies linear space. For completeness, early work dealing with two special cases should be mentioned. Retrieval of similar binary keys is considered by Rivest in <ref> [19] </ref> and the L 1 (max) metric setting is the focus of [20].
Reference: [20] <author> T. P. Yunck, </author> <title> "A technique to identify nearest neighbors," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <month> October </month> <year> 1976. </year>
Reference-contexts: For completeness, early work dealing with two special cases should be mentioned. Retrieval of similar binary keys is considered by Rivest in [19] and the L 1 (max) metric setting is the focus of <ref> [20] </ref>. More recently, the Voronoi digram [21] has provided a useful tool in low- dimensional Euclidian settings and the overall field and outlook of Computational Geometry has yielded many interesting results such as those of [22, 23, 24, 25] and earlier [26].
Reference: [21] <author> F. Aurenhammer, </author> <title> "Voronoi diagrams a survey of a fundemental geometric data structure," </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 23, </volume> <month> September </month> <year> 1991. </year>
Reference-contexts: For completeness, early work dealing with two special cases should be mentioned. Retrieval of similar binary keys is considered by Rivest in [19] and the L 1 (max) metric setting is the focus of [20]. More recently, the Voronoi digram <ref> [21] </ref> has provided a useful tool in low- dimensional Euclidian settings and the overall field and outlook of Computational Geometry has yielded many interesting results such as those of [22, 23, 24, 25] and earlier [26].
Reference: [22] <author> P. M. Vaidya, </author> <title> "An O(n log n) algorithm for the all-nearest-neighbor problem," </title> <journal> Discrete & Computational Geometry, </journal> <volume> vol. 4, no. 2, </volume> <pages> pp. 101-115, </pages> <year> 1989. </year>
Reference-contexts: More recently, the Voronoi digram [21] has provided a useful tool in low- dimensional Euclidian settings and the overall field and outlook of Computational Geometry has yielded many interesting results such as those of <ref> [22, 23, 24, 25] </ref> and earlier [26]. Unfortunately neither the kd-tree, or the constructions of computational geometry, seem to provide a practical solution for high dimensions. As dimension increases, the kd-tree soon visits nearly every database element while other constructions rapidly consume storage space.
Reference: [23] <author> K. L. Clarkson, </author> <title> "A randomized algorithm for closest-point queries," </title> <journal> SIAM Journal on Computing, </journal> <volume> vol. 17, </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: More recently, the Voronoi digram [21] has provided a useful tool in low- dimensional Euclidian settings and the overall field and outlook of Computational Geometry has yielded many interesting results such as those of <ref> [22, 23, 24, 25] </ref> and earlier [26]. Unfortunately neither the kd-tree, or the constructions of computational geometry, seem to provide a practical solution for high dimensions. As dimension increases, the kd-tree soon visits nearly every database element while other constructions rapidly consume storage space.
Reference: [24] <author> K. L. Clarkson, </author> <title> "New applications of random sampling in computational geometry," </title> <journal> Discrete & Computational Geometry, </journal> <volume> vol. 2, </volume> <pages> pp. 195-222, </pages> <year> 1987. </year>
Reference-contexts: More recently, the Voronoi digram [21] has provided a useful tool in low- dimensional Euclidian settings and the overall field and outlook of Computational Geometry has yielded many interesting results such as those of <ref> [22, 23, 24, 25] </ref> and earlier [26]. Unfortunately neither the kd-tree, or the constructions of computational geometry, seem to provide a practical solution for high dimensions. As dimension increases, the kd-tree soon visits nearly every database element while other constructions rapidly consume storage space.
Reference: [25] <author> A. M. Frieze, G. L. Miller, and S.-H. Teng, </author> <title> "Separator based parallel divide and conquer in computational geometry," </title> <booktitle> SPAA 92, </booktitle> <year> 1992. </year>
Reference-contexts: More recently, the Voronoi digram [21] has provided a useful tool in low- dimensional Euclidian settings and the overall field and outlook of Computational Geometry has yielded many interesting results such as those of <ref> [22, 23, 24, 25] </ref> and earlier [26]. Unfortunately neither the kd-tree, or the constructions of computational geometry, seem to provide a practical solution for high dimensions. As dimension increases, the kd-tree soon visits nearly every database element while other constructions rapidly consume storage space.
Reference: [26] <author> D. Dobkin and R. J. Lipton, </author> <title> "Multidimensional searching problems," </title> <journal> SIAM Journal on Computing, </journal> <volume> vol. 5, </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: More recently, the Voronoi digram [21] has provided a useful tool in low- dimensional Euclidian settings and the overall field and outlook of Computational Geometry has yielded many interesting results such as those of [22, 23, 24, 25] and earlier <ref> [26] </ref>. Unfortunately neither the kd-tree, or the constructions of computational geometry, seem to provide a practical solution for high dimensions. As dimension increases, the kd-tree soon visits nearly every database element while other constructions rapidly consume storage space.
Reference: [27] <author> P. N. Yianilos, </author> <title> "Normalized forms for two common met-rics," </title> <type> tech. rep., </type> <institution> The NEC Research Institute, Princeton, </institution> <address> New Jersey, </address> <month> December </month> <year> 1991. </year>
Reference-contexts: It measures the angle that two points form with the origin. and, 2) the ordinary Euclidian vector distance d (X; Y ) normalized by (kXk + kY k). That the result is a metric follows from a tedious argument provided in <ref> [27] </ref>. In both cases the vp-tree is applied to randomly constructed databases and exhibits search performance that corresponds well qualitatively to a standard Euclidian setting. 13 Our methods have been presented for metric spaces only but may be generalized to pseudo-metric spaces. 5 Concluding Remarks.
References-found: 27

