URL: ftp://ftp.cs.arizona.edu/xkernel/Papers/fbuf.ps
Refering-URL: http://www.cs.rice.edu/~druschel/
Root-URL: 
Title: Fbufs: A High-Bandwidth Cross-Domain Transfer Facility  
Author: Peter Druschel and Larry L. Peterson 
Address: Tucson, AZ 85721  
Affiliation: Department of Computer Science University of Arizona  
Abstract: We have designed and implemented a new operating system facility for I/O buffer management and data transfer across protection domain boundaries on shared memory machines. This facility, called fast buffers (fbufs), combines virtual page remapping with shared virtual memory, and exploits locality in I/O traffic to achieve high throughput without compromising protection, security, or modularity. Its goal is to help deliver the high bandwidth afforded by emerging high-speed networks to user-level processes, both in monolithic and microkernel-based operating systems. This paper outlines the requirements for a cross-domain transfer facility, describes the design of the fbuf mechanism that meets these requirements, and experimentally quantifies the impact of fbufs on network performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Accetta, R. Baron, D. Golub, R. Rashid, A. Teva-nian, and M. Young. </author> <title> Mach: A New Kernel Foundation for UNIX Development. </title> <booktitle> In Proceedings of the Summer 1986 USENIX Technical Conference and Exhibition, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: This subsection reviews literature that addresses this problem. 2.2.1 Page Remapping Several operating systems provide various forms of virtual memory (VM) support for transferring data from one domain to another. For example, the V kernel and DASH [4, 19] support page remapping, while Accent and Mach support copy-on-write (COW) <ref> [8, 1] </ref>. Page remapping has move rather than copy semantics, which limits its utility to situations where the sender needs no further access to the transferred data. <p> To prevent this, the kernel limits the number of chunks that can be allocated to any data path-specific fbuf allocator. 4 Performance This section reports on several experiments designed to evaluate the performance of fbufs. The software platform used in these experiments consists of CMU's Mach 3.0 microkernel (MK74) <ref> [1] </ref>, augmented with a network subsystem based on the University of Arizona's x-kernel (Version 3.2) [10].
Reference: [2] <author> B. Bershad, T. Anderson, E. Lazowska, and H. Levy. </author> <title> Lightweight remote procedure call. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 37-55, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Optimizing operations that cross protection domain boundaries has received a great deal of attention recently <ref> [2, 3] </ref>. This is because an efficient cross-domain invocation facility enables a more modular operating system design. <p> Since all domains have read and write access permissions to the entire pool, protection and security are compromised. Data is copied between the shared buffer pool and an application's private memory. As another example, LRPC <ref> [2] </ref> uses argument stacks that are pairwise shared between communicating protection domains. Arguments must generally be copied into and out of the argument stack. Both techniques reduce the number of copies required, rather than eliminating copying.
Reference: [3] <author> B. N. Bershad, T. E. Anderson, E. D. Lazowska, and H. M. Levy. </author> <title> User-level interprocess communication for shared memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(2) </volume> <pages> 175-198, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Optimizing operations that cross protection domain boundaries has received a great deal of attention recently <ref> [2, 3] </ref>. This is because an efficient cross-domain invocation facility enables a more modular operating system design. <p> Moreover, in the common case, no kernel involvement is required during cross-domain data transfer. Our facility is therefore well suited for use with user-level IPC facilities such as URPC <ref> [3] </ref>, and other highly optimized IPC mechanisms such as MMS [9]. 3.3 Implementation Issues A two-level allocation scheme with per-domain allo-cators ensures that most fbuf allocations can be satisfied without kernel involvement. A range of virtual addresses, the fbuf region, is reserved in each protection domain, including the kernel.
Reference: [4] <author> D. R. Cheriton. </author> <title> The V distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> Mar. </month> <year> 1988. </year>
Reference-contexts: This subsection reviews literature that addresses this problem. 2.2.1 Page Remapping Several operating systems provide various forms of virtual memory (VM) support for transferring data from one domain to another. For example, the V kernel and DASH <ref> [4, 19] </ref> support page remapping, while Accent and Mach support copy-on-write (COW) [8, 1]. Page remapping has move rather than copy semantics, which limits its utility to situations where the sender needs no further access to the transferred data.
Reference: [5] <author> C. Dalton, G. Watson, D. Banks, C. Calamvokis, A. Edwards, and J. Lumley. </author> <title> Afterburner. </title> <journal> IEEE Network, </journal> <volume> 7(4) </volume> <pages> 36-43, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: All forms of shared memory may compromise protection between the sharing domains. Several recent systems attempt to avoid data copying by transferring data directly between UNIX application buffers and network interface <ref> [5, 17] </ref>. This approach works when data is accesses only in a single application domain. A substantial amount of memory may be required in the network adapter when interfacing to high-bandwidth, high-latency networks. Moreover, this memory is a limited resource dedicated to network buffering. <p> While most low-bandwidth (Ethernet) network adapters do not have this capability, network adapters for high-speed networks are still the subject of research. Two prototypes of such interfaces we are familiar with (the Osiris board, and the HP Afterburner board <ref> [5] </ref>) do have adequate support. 5.3 Architectural Considerations As observed in Section 5, the performance of cached fbufs for large messages is limited by TLB miss handling overhead. 7 In many modern architectures (including the MIPS), TLB entries are tagged with a domain identifier.
Reference: [6] <author> B. S. Davie. </author> <title> A host-network interface architecture for ATM. </title> <booktitle> In Proceedings of the SIGCOMM '91 Conference, </booktitle> <pages> pages 307-315, </pages> <address> Zuerich, Switzerland, </address> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: The hardware platform consists of a pair of DecStation 5000/200 workstations (25MHz MIPS R3000), each of which was attached to a prototype ATM network interface board, called Osiris, designed by Bell-core for the Aurora Gigabit testbed <ref> [6] </ref>. The Osiris boards were connected by a null modem, and support a link speed of 622Mbps. The x-kernel based network subsystem consists of a protocol graph that can span multiple protection domains, including the Mach microkernel.
Reference: [7] <author> P. Druschel, M. B. Abbott, M. A. Pagels, and L. L. Peterson. </author> <title> Network subsystem design. </title> <journal> IEEE Network, </journal> <volume> 7(4) </volume> <pages> 8-17, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: This task is made difficult by the limitations of the memory architecture, most notably the CPU/memory bandwidth. As network bandwidth approaches memory bandwidth, copying data from one domain to another simply cannot keep up with improved network performance <ref> [15, 7] </ref>. This paper introduces a high-bandwidth cross-domain transfer and buffer management facility, called fast buffers (fbufs), and shows how it can be optimized to support data that originates and/or terminates at an I/O device, potentially traversing multiple protection domains. <p> This is because the UNIX interface has copy semantics, and it allows the application to specify an unaligned buffer address anywhere in the its address space. We therefore propose the addition of an interface for high-bandwidth I/O that uses immutable buffer aggregates <ref> [7] </ref>. New high-bandwidth applications can use this interface; existing applications can continue to use the old interface, which requires copying. The use of such an interface requires applications to use an abstract data type that encapsulates buffer aggregates.
Reference: [8] <author> R. Fitzgerald and R. F. Rashid. </author> <title> The integration of virtual memory management and interprocess communication in Accent. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(2) </volume> <pages> 147-177, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: This subsection reviews literature that addresses this problem. 2.2.1 Page Remapping Several operating systems provide various forms of virtual memory (VM) support for transferring data from one domain to another. For example, the V kernel and DASH [4, 19] support page remapping, while Accent and Mach support copy-on-write (COW) <ref> [8, 1] </ref>. Page remapping has move rather than copy semantics, which limits its utility to situations where the sender needs no further access to the transferred data.
Reference: [9] <author> R. Govindan and D. P. Anderson. </author> <title> Scheduling and IPC mechanisms for continuous media. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 68-80. </pages> <institution> Association for Computing Machinery SIGOPS, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: Moreover, in the common case, no kernel involvement is required during cross-domain data transfer. Our facility is therefore well suited for use with user-level IPC facilities such as URPC [3], and other highly optimized IPC mechanisms such as MMS <ref> [9] </ref>. 3.3 Implementation Issues A two-level allocation scheme with per-domain allo-cators ensures that most fbuf allocations can be satisfied without kernel involvement. A range of virtual addresses, the fbuf region, is reserved in each protection domain, including the kernel.
Reference: [10] <author> N. C. Hutchinson and L. L. Peterson. </author> <title> The x-Kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Such abstractions typically provide operations to logically join one or more buffers into an aggregate, split an aggregate into separate buffers, clip data from one end of an aggregate, and so on. Examples of such aggregation abstractions include x-kernel messages <ref> [10] </ref>, and BSD Unix mbufs [12]. For the purpose of the following discussion, we refer to such an abstraction as an aggregate object, and we use the x-kernel's directed acyclic graph (DAG) representation depicted in Figure 2. <p> The software platform used in these experiments consists of CMU's Mach 3.0 microkernel (MK74) [1], augmented with a network subsystem based on the University of Arizona's x-kernel (Version 3.2) <ref> [10] </ref>. The hardware platform consists of a pair of DecStation 5000/200 workstations (25MHz MIPS R3000), each of which was attached to a prototype ATM network interface board, called Osiris, designed by Bell-core for the Aurora Gigabit testbed [6].
Reference: [11] <author> D. B. Johnson and W. Zwaenepoel. </author> <title> The Peregrine high-performance RPC system. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 23(2) </volume> <pages> 201-221, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Of the 22secs required to remap another page, we found that the CPU was stalled waiting for cache fills approximately half of the time. The operation is likely to become more memory bound as the gap between CPU and memory speeds widens. Second, the Peregrine RPC system <ref> [11] </ref> reduces RPC latency by remapping a single kernel page containing the request packet into the server's address space, to serve as the server thread's runtime stack. The authors report a cost of only 4secs for this operation on a Sun 3/60.
Reference: [12] <author> S. J. Leffler, M. K. McKusick, M. J. Karels, and J. S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: Such abstractions typically provide operations to logically join one or more buffers into an aggregate, split an aggregate into separate buffers, clip data from one end of an aggregate, and so on. Examples of such aggregation abstractions include x-kernel messages [10], and BSD Unix mbufs <ref> [12] </ref>. For the purpose of the following discussion, we refer to such an abstraction as an aggregate object, and we use the x-kernel's directed acyclic graph (DAG) representation depicted in Figure 2.
Reference: [13] <author> C. Maeda and B. Bershad. </author> <title> Protocol service decomposition for high-performance networking. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating System Principles, </booktitle> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: For example, recent work suggests that it is possible to implement the TCP/IP protocol suite using application libraries, thus requiring only a single user/kernel crossing in the common case <ref> [13, 18] </ref>. There are three responses to this question. First, server-based systems have undeniable advantages; it is a general technique that makes it possible to transparently add new services and entire OS personalities without requiring modification/rebuilding of the kernel and applications.
Reference: [14] <author> J. C. Mogul. </author> <title> Network locality at the scale of processes. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(2) </volume> <pages> 81-109, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Locality in network communication <ref> [14] </ref> implies that if there is traffic on a particular I/O data path, then more traffic can be expected on the same path in the near future.
Reference: [15] <author> J. K. Ousterhout. </author> <booktitle> Why Aren't Operating Systems Getting Faster As Fast as Hardware? In Usenix 1990 Summer Conference, </booktitle> <pages> pages 247-256, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This task is made difficult by the limitations of the memory architecture, most notably the CPU/memory bandwidth. As network bandwidth approaches memory bandwidth, copying data from one domain to another simply cannot keep up with improved network performance <ref> [15, 7] </ref>. This paper introduces a high-bandwidth cross-domain transfer and buffer management facility, called fast buffers (fbufs), and shows how it can be optimized to support data that originates and/or terminates at an I/O device, potentially traversing multiple protection domains.
Reference: [16] <author> M. D. Schroeder and M. Burrows. </author> <title> Performance of Firefly RPC. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 1-17, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: For example, the DEC Firefly RPC facility uses a pool of buffers that is globally and permanently shared among all domains <ref> [16] </ref>. Since all domains have read and write access permissions to the entire pool, protection and security are compromised. Data is copied between the shared buffer pool and an application's private memory. As another example, LRPC [2] uses argument stacks that are pairwise shared between communicating protection domains.
Reference: [17] <author> J. M. Smith and C. B. S. Traw. </author> <title> Giving applications access to Gb/s networking. </title> <journal> IEEE Network, </journal> <volume> 7(4) </volume> <pages> 44-52, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: All forms of shared memory may compromise protection between the sharing domains. Several recent systems attempt to avoid data copying by transferring data directly between UNIX application buffers and network interface <ref> [5, 17] </ref>. This approach works when data is accesses only in a single application domain. A substantial amount of memory may be required in the network adapter when interfacing to high-bandwidth, high-latency networks. Moreover, this memory is a limited resource dedicated to network buffering.
Reference: [18] <author> C. Thekkath, T. Nguyen, E. Moy, and E. Lazowska. </author> <title> Implementing network protocols at user level. </title> <booktitle> In Proceedings of the SIGCOMM '93 Symposium, </booktitle> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: For example, recent work suggests that it is possible to implement the TCP/IP protocol suite using application libraries, thus requiring only a single user/kernel crossing in the common case <ref> [13, 18] </ref>. There are three responses to this question. First, server-based systems have undeniable advantages; it is a general technique that makes it possible to transparently add new services and entire OS personalities without requiring modification/rebuilding of the kernel and applications.
Reference: [19] <author> S.-Y. Tzou and D. P. Anderson. </author> <title> The performance of message-passing using restricted virtual memory remapping. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 21 </volume> <pages> 251-267, </pages> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: This subsection reviews literature that addresses this problem. 2.2.1 Page Remapping Several operating systems provide various forms of virtual memory (VM) support for transferring data from one domain to another. For example, the V kernel and DASH <ref> [4, 19] </ref> support page remapping, while Accent and Mach support copy-on-write (COW) [8, 1]. Page remapping has move rather than copy semantics, which limits its utility to situations where the sender needs no further access to the transferred data. <p> We consider two of the more highly tuned implementations in more detail. First, Tzou and Anderson evaluate the remap facility in the DASH operating system <ref> [19] </ref>. The paper reports an incremental overhead of 208secs/page on a Sun 3/50. However, because it measures a ping-pong test casethe same page is remapped back and forth between a pair of processesit does not include the cost of allocating and deallocating pages.
References-found: 19

