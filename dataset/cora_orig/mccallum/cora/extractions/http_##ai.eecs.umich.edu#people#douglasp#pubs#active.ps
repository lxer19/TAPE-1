URL: http://ai.eecs.umich.edu/people/douglasp/pubs/active.ps
Refering-URL: http://ai.eecs.umich.edu/people/douglasp/pubs/active.html
Root-URL: 
Email: dpearson@umich.edu  
Title: Active Learning in Correcting Domain Theories: Help or Hindrance?  
Author: Douglas J. Pearson 
Address: 1101 Beal Ave. Ann Arbor, MI 48109  
Affiliation: Artificial Intelligence Laboratory The University of Michigan  
Abstract: In active learning, the learner uses its current knowledge to guide the choice of future training instances. This dependency between the agent's knowledge and its subsequent training set provides the active learner with a possibility for improved learning and at the same time a potential pitfall that can hinder learning. The opportunity for improved learning comes from choosing closely related instances, in such a way that they help the learner identify why one instance is positive and another is negative. The potential problem is that incorrect early learning can inhibit the learner from seeing instances that will lead it to correctly learn the target concept. These two problems are explored within the task of learning domain knowledge. I propose a method for avoiding the problem and taking advantage of the opportunity, with experimental evidence to verify that this improves the performance of a particular active learner, IMPROV. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Holland, J. H. </author> <year> 1986. </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In Michalski, R. S.; Carbonell, J. G.; and Mitchell, T. M., eds., </editor> <booktitle> Machine Learning: An artificial intelligence approach, Volume II. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These systems typically learn in a purely passive manner. A set of training instances are selected in advance and are unrelated to the state of the agent's knowledge during training. The second class of domain theory learning systems are reinforcement learners (Q-learning (Watkins & Dayan 1992), Classifiers <ref> (Holland 1986) </ref>, Backpropogation (Rumelhart, Hinton, & Williams 1986)). These systems use a general purpose induction algorithm applied to a procedural domain theory. They do not reason explicitly about the correctness of the domain knowledge, rather they focus on the task performance and reward. Reinforcement learners are often used actively.
Reference: <author> Murphy, P. M., and Pazzani, M. J. </author> <year> 1994. </year> <title> Revision of production system rule-bases. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> 199-207. </pages>
Reference-contexts: Agents that learn about the effect of executing different sequences of actions, or plans, are learning or revising their domain knowledge. Existing approaches to learning domain knowledge can be divided into two main classes. First, theory revision systems (EITHER (Ourston & Mooney 1990), OCCAM (Pazzani 1988; 1991), CLIPS-R <ref> (Murphy & Pazzani 1994) </ref>) that reason deliberately about the correctness of the agent's domain knowledge. These systems typically learn in a purely passive manner. A set of training instances are selected in advance and are unrelated to the state of the agent's knowledge during training.
Reference: <author> Ourston, D., and Mooney, R. J. </author> <year> 1990. </year> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 815-820. </pages>
Reference-contexts: Agents that learn about the effect of executing different sequences of actions, or plans, are learning or revising their domain knowledge. Existing approaches to learning domain knowledge can be divided into two main classes. First, theory revision systems (EITHER <ref> (Ourston & Mooney 1990) </ref>, OCCAM (Pazzani 1988; 1991), CLIPS-R (Murphy & Pazzani 1994)) that reason deliberately about the correctness of the agent's domain knowledge. These systems typically learn in a purely passive manner.
Reference: <author> Pazzani, M. J. </author> <year> 1988. </year> <title> Integrated learning with incorrect and incomplete theories. </title> <booktitle> In Proceedings of the International Machine Learning Conference, </booktitle> <pages> 291-297. </pages>
Reference: <author> Pazzani, M. </author> <year> 1991. </year> <title> Learning to predict and explain: An integration of similarity-based, theory driven, and explanation-based learning. </title> <journal> Journal of the Learning Sciences 1(2) </journal> <pages> 153-199. </pages>
Reference: <author> Pearson, D. J., and Laird, J. E. </author> <year> 1995. </year> <title> Toward incremental knowledge correction for agents in complex environments. </title> <editor> In Muggleton, S.; Michie, D.; and Fu-rukawa, K., eds., </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 15. </volume> <publisher> Oxford University Press. </publisher>
Reference-contexts: This dependency of training instances on the agent's knowledge is at the heart of the problem and opportunity that are available for an active learner, as will be discussed in the rest of this paper. The research reported here focuses on a hybrid system, IMPROV <ref> (Pearson & Laird 1995) </ref>, (see Figure 1), that attempts to draw on the strengths of both passive theory revision systems and active reinforcement learners. An IMPROV agent learns actively from its experiences of executing plans in a domain. <p> The experiment was run on a more complex version of the driving task described earlier and is described in more detail in <ref> (Pearson & Laird 1995) </ref>. The graph in Figure 4 show the cumulative number of errors made by each system during a series of different performance trials. Each trial consisted of the agent attempting to cross an intersection.
Reference: <author> Rumelhart, D. E.; Hinton, G. E.; and Williams, R. J. </author> <year> 1986. </year> <title> Learning internal representations by error pro-pogation. </title> <booktitle> In Parallel Distributed Processing, volume 1. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: A set of training instances are selected in advance and are unrelated to the state of the agent's knowledge during training. The second class of domain theory learning systems are reinforcement learners (Q-learning (Watkins & Dayan 1992), Classifiers (Holland 1986), Backpropogation <ref> (Rumelhart, Hinton, & Williams 1986) </ref>). These systems use a general purpose induction algorithm applied to a procedural domain theory. They do not reason explicitly about the correctness of the domain knowledge, rather they focus on the task performance and reward. Reinforcement learners are often used actively.
Reference: <author> VanLehn, K. </author> <year> 1987. </year> <title> Learning one subprocedure per lesson. </title> <booktitle> Artificial Intelligence 31(1) </booktitle> <pages> 1-40. </pages>
Reference-contexts: One such benefit, is that the agent can select instances which are closely related during training. The correct choice of training instances can make learning easier <ref> (VanLehn 1987) </ref>. To return to the driving example, let's assume the agent fails to cross an intersection when the light is red and it's driving at 30mph.
Reference: <author> Watkins, C. J. C. H., and Dayan, P. </author> <year> 1992. </year> <title> Technical note: </title> <booktitle> Q-learning. Machine Learning 8 </booktitle> <pages> 279-292. </pages>
Reference-contexts: These systems typically learn in a purely passive manner. A set of training instances are selected in advance and are unrelated to the state of the agent's knowledge during training. The second class of domain theory learning systems are reinforcement learners (Q-learning <ref> (Watkins & Dayan 1992) </ref>, Classifiers (Holland 1986), Backpropogation (Rumelhart, Hinton, & Williams 1986)). These systems use a general purpose induction algorithm applied to a procedural domain theory. They do not reason explicitly about the correctness of the domain knowledge, rather they focus on the task performance and reward.
References-found: 9

