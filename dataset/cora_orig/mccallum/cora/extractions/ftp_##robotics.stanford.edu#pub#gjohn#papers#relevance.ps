URL: ftp://robotics.stanford.edu/pub/gjohn/papers/relevance.ps
Refering-URL: http://www.robotics.stanford.edu/~gjohn/pubs.html
Root-URL: http://www.robotics.stanford.edu
Email: gjohn@CS.Stanford.EDU  ronnyk@CS.Stanford.EDU  kpfleger@CS.Stanford.EDU  
Title: Irrelevant Features and the Subset Selection Problem  
Author: William W. Cohen Haym Hirsh, George H. John Ron Kohavi Karl Pfleger 
Address: Stanford, CA 94305  Stanford, CA 94305  Stanford, CA 94305  
Affiliation: Computer Science Dept. Stanford University  Computer Science Dept. Stanford University  Computer Science Dept. Stanford University  
Note: Published in 1994,  eds., Machine Learning: Proceedings of the Eleventh International Conference, 121-129, Morgan Kaufmann Publishers, San Francisco, CA.  
Abstract: We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show that the definitions used in the machine learning literature do not adequately partition the features into useful categories of relevance. We present definitions for irrelevance and for two degrees of relevance. These definitions improve our understanding of the behavior of previous subset selection algorithms, and help define the subset of features that should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation that is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on artificial and real datasets.
Abstract-found: 1
Intro-found: 1
Reference: <author> Almuallim, H., and Dietterich, T. G. </author> <year> 1991. </year> <title> Learning with many irrelevant features. </title> <booktitle> In Ninth National Conference on Artificial Intelligence, </booktitle> <pages> 547-552. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: Irrelevant features can never contribute to prediction accuracy, by definition. In Example 1, feature X 1 is strongly relevant; features X 2 and X 4 are weakly relevant; and X 3 and X 5 are irrelevant. Figure 2 shows our view of relevance. Algorithms such as FOCUS <ref> (Almuallim & Dietterich 1991) </ref> (see Section 3.1) find a minimal set of features that are sufficient to determine the concept. <p> The FOCUS algorithm <ref> (Almuallim & Dietterich 1991) </ref>, originally defined for noise-free Boolean domains, exhaustively examines all subsets of features, selecting the minimal subset of features that is sufficient to determine the label. This is referred to as the MIN-FEATURES bias.
Reference: <author> Ben-Bassat, M. </author> <year> 1982. </year> <title> Use of distance measures, information measures and error bounds in feature evaluation. </title> <editor> In Krishnaiah, P. R., and Kanal, L. N., eds., </editor> <booktitle> Handbook of Statistics, </booktitle> <volume> volume 2. </volume> <publisher> North-Holland Publishing Company. </publisher> <pages> 773-791. </pages>
Reference: <author> Blum, A. L., and Rivest, R. L. </author> <year> 1992. </year> <title> Training a 3-node neural network is NP-complete. </title> <booktitle> Neural Networks 5 </booktitle> <pages> 117-127. </pages>
Reference: <author> Blumer, A.; Ehrenfeucht, A.; Haussler, D.; and War-muth, M. K. </author> <year> 1987. </year> <title> Occam's razor. </title> <journal> Information Processing Letters 24 </journal> <pages> 377-380. </pages>
Reference-contexts: 1 INTRODUCTION In supervised learning, one is given a training set containing labelled instances. The instances are typically specified by assigning values to a set of features, and the task is to induce a hypothesis that accurately predicts the label of novel instances. Following Occam's razor <ref> (Blumer et al. 1987) </ref>, minimum description length (Rissanen 1986), and minimum message length (Wallace & Freeman 1987), one usually attempts to find structures that correctly classify a large subset of the training set, and yet are not so complex that they begin to overfit the data.
Reference: <author> Boyce, D.; Farhi, A.; and Weischedel, R. </author> <year> 1974. </year> <title> Optimal Subset Selection. </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: The feature named "irrelevant" is uniformly random, and the feature "correlated" matches the class label 75% of the time. The left subtree is the correct decision tree, which is correctly induced if the "correlated" feature is removed from the data. C4.5 (Quinlan 1992) and CART <ref> (Breiman et al. 1984) </ref> induce similar trees with the "correlated" feature at the root. Such a split causes all these induction algorithms to generate trees that are less accurate than if this feature is completely removed.
Reference: <author> Cardie, C. </author> <year> 1993. </year> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 25-32. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Caruana, R., and Freitag, D. </author> <year> 1994. </year> <title> Greedy attribute selection. </title> <editor> In Cohen, W. W., and Hirsh, H., eds., </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cohen, W. W. </author> <year> 1993. </year> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 988-994. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Devijver, P. A., and Kittler, J. </author> <year> 1982. </year> <title> Pattern Recognition: A Statistical Approach. </title> <booktitle> Prentice-Hall International. </booktitle>
Reference: <author> Draper, N. R., and Smith, H. </author> <year> 1981. </year> <title> Applied Regression Analysis. </title> <publisher> John Wiley & Sons, 2nd edition. </publisher>
Reference: <author> Gennari, J. H.; Langley, P.; and Fisher, D. </author> <year> 1989. </year> <title> Models of incremental concept formation. </title> <booktitle> Artificial Intelligence 40 </booktitle> <pages> 11-61. </pages>
Reference: <author> Hancock, T. R. </author> <year> 1989. </year> <title> On the difficulty of finding small consistent decision trees. </title> <type> Unpublished Manuscript, </type> <institution> Harvard University. </institution>
Reference: <author> Holte, R. C. </author> <year> 1993. </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning 11 </booktitle> <pages> 63-90. </pages>
Reference-contexts: One possible reason for the lack of significant improvement of prediction accuracy over C4.5 is that C4.5 does quite well on most of the datasets tested here, leaving little room for improvement. This seems to be in line with with Holte's claims <ref> (Holte 1993) </ref>. Harder datasets might show more significant improvement. Indeed the wrapper model produced the most significant improvement for the two datasets (parity5+5 and CorrAL) on which C4.5 performed the worst.
Reference: <author> Kira, K., and Rendell, L. A. </author> <year> 1992a. </year> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 129-134. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: The Relief algorithm does not attempt to determine useful subsets of the weakly relevant features: Relief does not help with redundant features. If most of the given features are relevant to the concept, it would select most of them even though only a fraction are necessary for concept description <ref> (Kira & Rendell 1992a, page 133) </ref>. In real domains, many features have high correlations, and thus many are (weakly) relevant, and will not be removed by Relief 3 . Cardie (1993) uses subset selection to remove irrelevant features from a dataset to be used with the nearest-neighbor algorithm.
Reference: <author> Kira, K., and Rendell, L. A. </author> <year> 1992b. </year> <title> A practical approach to feature selection. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kononenko, I. </author> <year> 1994. </year> <title> Estimating attributes: Analysis and extensions of Relief. </title> <booktitle> In Proceedings of the European Conference on Machine Learning. </booktitle>
Reference: <author> Langley, P., and Sage, S. </author> <year> 1994. </year> <title> Oblivious decision trees and abstract cases. </title> <booktitle> In Working Notes of the AAAI94 Workshop on Case-Based Reasoning. </booktitle> <publisher> In press. </publisher>
Reference: <author> Levy, A. Y. </author> <year> 1993. </year> <title> Irrelevance Reasoning in Knowledge Based Systems. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University. </institution>
Reference: <author> Littlestone, N. </author> <year> 1988. </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <booktitle> Machine Learning 2 </booktitle> <pages> 285-318. </pages>
Reference: <author> Mallows, C. L. </author> <year> 1973. </year> <title> Some comments on c p . Tech-nometrics 15 </title> <type> 661-675. </type>
Reference-contexts: Many measures have been suggested to evaluate the subset selection (as opposed to cross validation), such as adjusted mean squared error, adjusted multiple correlation coefficient, and the C p statistic <ref> (Mallows 1973) </ref>. In Mucciardi & Gose (1971), seven different techniques for subset selection were empirically compared for a nine-class electrocardiographic problem. The search for the best subset can be improved by making assumptions on the evaluation function.
Reference: <author> Marill, T., and Green, D. M. </author> <year> 1963. </year> <title> On the effectiveness of receptors in recognition systems. </title> <journal> IEEE Transactions on Information Theory 9 </journal> <pages> 11-17. </pages>
Reference: <author> Miller, A. J. </author> <year> 1990. </year> <title> Subset Selection in Regression. </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Modrzejewski, M. </author> <year> 1993. </year> <title> Feature selection using rough sets theory. </title> <editor> In Brazdil, P. B., ed., </editor> <booktitle> Proceedings of the European Conference on Machine Learning, </booktitle> <pages> 213-226. </pages>
Reference: <author> Moore, A. W., and Lee, M. S. </author> <year> 1994. </year> <title> Efficient algorithms for minimizing cross validation error. </title> <editor> In Co-hen, W. W., and Hirsh, H., eds., </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moret, B. M. E. </author> <year> 1982. </year> <title> Decision trees and diagrams. </title> <journal> ACM Computing Surveys 14(4) </journal> <pages> 593-623. </pages>
Reference: <author> Mucciardi, A. N., and Gose, E. E. </author> <year> 1971. </year> <title> A comparison of seven techniques for choosing subsets of pattern recognition properties. </title> <journal> IEEE Transactions on Computers C-20(9):1023-1031. </journal>
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1994. </year> <title> UCI repository of machine learning databases. For information contact ml-repository@ics.uci.edu. </title>
Reference-contexts: Since we are no longer worried by high variance, we call this deterministic variant RelieveD. In our experiments, features with relevancy rankings below 0 were removed. The real-world datasets were taken from the UC-Irvine repository <ref> (Murphy & Aha 1994) </ref> and from Quinlan (1992) . Figures 5 and 6 summarize our results. We give details for those datasets that had the largest differences either in accuracy or tree size.
Reference: <author> Narendra, M. P., and Fukunaga, K. </author> <year> 1977. </year> <title> A branch and bound algorithm for feature subset selection. </title> <journal> IEEE Transactions on Computers C-26(9):917-922. </journal>
Reference: <author> Neter, J.; Wasserman, W.; and Kutner, M. H. </author> <year> 1990. </year> <title> Applied Linear Statistical Models. </title> <type> Irwin: Homewood, </type> <institution> IL, </institution> <note> 3rd edition. </note>
Reference: <author> Pagallo, G., and Haussler, D. </author> <year> 1990. </year> <title> Boolean feature discovery in empirical learning. </title> <booktitle> Machine Learning 5 </booktitle> <pages> 71-99. </pages>
Reference-contexts: Feature subset selection is an important problem that has many ramifications. Our introductory example (Figure 1) shows that common algorithms such as ID3, C4.5, and CART, fail to ignore features which, if ignored, would improve accuracy. Feature subset selection is also useful for constructive induction <ref> (Pagallo & Haussler 1990) </ref> where features can be constructed and tested using the wrapper model to determine if they improve performance. Finally, in real world applications, features may have an associated cost (i.e., when the value of a feature is determined by an expensive test).
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 81-106. </pages> <note> Reprinted in Shavlik and Dietterich (eds.) Readings in Machine Learning. </note>
Reference-contexts: This heuristic search may lead to induced concepts which depend on irrelevant features, or in some cases even relevant features that hurt the overall accuracy. Figure 1 shows such a choice of a non-optimal split at the root made by ID3 <ref> (Quinlan 1986) </ref>. The Boolean target concept is (A0 ^ A1) _ (B0 ^ B1). The feature named "irrelevant" is uniformly random, and the feature "correlated" matches the class label 75% of the time.
Reference: <author> Quinlan, J. R. </author> <year> 1992. </year> <title> C4.5: Programs for Machine Learning. </title> <address> Los Altos, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The feature named "irrelevant" is uniformly random, and the feature "correlated" matches the class label 75% of the time. The left subtree is the correct decision tree, which is correctly induced if the "correlated" feature is removed from the data. C4.5 <ref> (Quinlan 1992) </ref> and CART (Breiman et al. 1984) induce similar trees with the "correlated" feature at the root. Such a split causes all these induction algorithms to generate trees that are less accurate than if this feature is completely removed. <p> By pruning after feature subset selection, pruning may be much faster. 4 EXPERIMENTAL RESULTS In order to evaluate the feature subset selection using the wrapper model we propose, we ran experiments on nine datasets. The C4.5 program is the program that comes with Quinlan's book <ref> (Quinlan 1992) </ref>; the ID3 results were obtained by running C4.5 and using the unpruned trees. On the artificial datasets, we used the "-s -m1" C4.5 flags, which indicate that subset splits may be used and that splitting should continue until purity.
Reference: <author> Rissanen, J. </author> <year> 1986. </year> <title> Stochastic complexity and modeling. </title> <journal> Ann. </journal> <volume> Statist 14 </volume> <pages> 1080-1100. </pages>
Reference-contexts: The instances are typically specified by assigning values to a set of features, and the task is to induce a hypothesis that accurately predicts the label of novel instances. Following Occam's razor (Blumer et al. 1987), minimum description length <ref> (Rissanen 1986) </ref>, and minimum message length (Wallace & Freeman 1987), one usually attempts to find structures that correctly classify a large subset of the training set, and yet are not so complex that they begin to overfit the data.
Reference: <author> Russel, S. J. </author> <year> 1986. </year> <title> Preliminary steps toward the automation of induction. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 477-484. </pages>
Reference: <author> Russel, S. J. </author> <year> 1989. </year> <title> The Use of Knowledge in Analogy and Induction. </title> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schlimmer, J. C. </author> <year> 1993. </year> <title> Efficiently inducing determinations: A complete and systematic search algorithm that uses optimal pruning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 284-290. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Siedlecki, W., and Sklansky, J. </author> <year> 1988. </year> <title> On automatic feature selection. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence 2(2) </journal> <pages> 197-220. </pages>
Reference-contexts: Kittler generalized the different variants including forward methods, stepwise methods, and "plus `-take away r." Branch and bound algorithms were introduced by Narendra & Fukunaga (1977). Finally, more recent papers attempt to use AI techniques, such as beam search and bidirectional search <ref> (Siedlecki & Sklansky 1988) </ref>, best first search (Xu, Yan, & Chang 1989), and genetic algorithms (Vafai & De Jong 1992).
Reference: <author> Skalak, D. B. </author> <year> 1994. </year> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <editor> In Cohen, W. W., and Hirsh, H., eds., </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Thrun, S. B., et al. </author> <year> 1991. </year> <title> The monk's problems: A performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Vafai, H., and De Jong, K. </author> <year> 1992. </year> <title> Genetic algorithms as a tool for feature selection in machine learning. </title> <booktitle> In Fourth International Conference on Tools with Artificial Intelligence, </booktitle> <pages> 200-203. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Finally, more recent papers attempt to use AI techniques, such as beam search and bidirectional search (Siedlecki & Sklansky 1988), best first search (Xu, Yan, & Chang 1989), and genetic algorithms <ref> (Vafai & De Jong 1992) </ref>. Many measures have been suggested to evaluate the subset selection (as opposed to cross validation), such as adjusted mean squared error, adjusted multiple correlation coefficient, and the C p statistic (Mallows 1973).
Reference: <author> Wallace, C., and Freeman, P. </author> <year> 1987. </year> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society (B) 49 </journal> <pages> 240-265. </pages>
Reference-contexts: The instances are typically specified by assigning values to a set of features, and the task is to induce a hypothesis that accurately predicts the label of novel instances. Following Occam's razor (Blumer et al. 1987), minimum description length (Rissanen 1986), and minimum message length <ref> (Wallace & Freeman 1987) </ref>, one usually attempts to find structures that correctly classify a large subset of the training set, and yet are not so complex that they begin to overfit the data. Ideally, the induction algorithm should use only the subset of features that leads to the best performance.
Reference: <author> Weiss, S. M., and Kulikowski, C. A. </author> <year> 1991. </year> <title> Computer Systems that Learn. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Xu, L.; Yan, P.; and Chang, T. </author> <year> 1989. </year> <title> Best first strategy for feature selection. </title> <booktitle> In Ninth International Conference on Pattern Recognition, </booktitle> <pages> 706-708. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Kittler generalized the different variants including forward methods, stepwise methods, and "plus `-take away r." Branch and bound algorithms were introduced by Narendra & Fukunaga (1977). Finally, more recent papers attempt to use AI techniques, such as beam search and bidirectional search (Siedlecki & Sklansky 1988), best first search <ref> (Xu, Yan, & Chang 1989) </ref>, and genetic algorithms (Vafai & De Jong 1992). Many measures have been suggested to evaluate the subset selection (as opposed to cross validation), such as adjusted mean squared error, adjusted multiple correlation coefficient, and the C p statistic (Mallows 1973).
References-found: 44

