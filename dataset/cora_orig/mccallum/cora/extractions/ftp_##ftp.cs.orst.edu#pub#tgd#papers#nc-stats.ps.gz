URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/nc-stats.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Email: tgd@cs.orst.edu  
Title: Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms  
Author: Thomas G. Dietterich 
Date: December 30, 1997  
Address: Corvallis, OR 97331  
Affiliation: Department of Computer Science Oregon State University  
Abstract: This paper reviews five approximate statistical tests for determining whether one learning algorithm out-performs another on a particular learning task. These tests are compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely-used statistical tests are shown to have high probability of Type I error in certain situations and should never be used. These tests are (a) a test for the difference of two proportions and (b) a paired-differences t test based on taking several random train/test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of Type I error. A fourth test, McNemar's test, is shown to have low Type I error. The fifth test is a new test, 5x2cv, based on 5 iterations of 2-fold cross-validation. Experiments show that this test also has acceptable Type I error. The paper also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 5x2cv test is shown to be slightly more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, McNemar's test is the only test with acceptable Type I error. For algorithms that can be executed ten times, the 5x2cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: Breiman (1994, 1996) has called this behavior "instability", and he has shown that this is a serious problem for the decision tree algorithms, such as CART <ref> (Breiman, Friedman, Olshen, & Stone, 1984) </ref>. A third source of variance can be internal randomness in the learning algorithm. Consider, for example, the widely-used backpropagation algorithm for training feed-forward neural networks. This algorithm is usually initialized with a set of random weights which it then improves.
Reference: <author> Breiman, L. </author> <year> (1994). </year> <title> Heuristics of instability and stabilization in model selection. </title> <type> Tech. rep. 416, </type> <institution> Department of Statistics, University of California, Berkeley, CA. </institution> <note> 23 Breiman, </note> <author> L. </author> <year> (1996). </year> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24 (2), </volume> <pages> 123-140. </pages>
Reference: <author> Dasarathy, B. V. (Ed.). </author> <year> (1991). </year> <title> Nearest neighbor (NN) norms: NN pattern classification techniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA. </address>
Reference-contexts: We also needed the learning algorithms to be very efficient, so that the experiments could be replicated many times. To achieve this, we chose C4.5 Release 1 (Quinlan, 1993) and the first nearest-neighbor (NN) algorithm <ref> (Dasarathy, 1991) </ref>. We then selected three difficult problems: the EXP6 problem developed by Kong (1995), the Letter Recognition data set (Frey & Slate, 1991), and the Pima Indians Diabetes Task (Merz & Murphy, 1996). Of course, C4.5 and NN do not have the same performance on these data sets.
Reference: <author> Dietterich, T. G., Hild, H., & Bakiri, G. </author> <year> (1995). </year> <title> A comparison of ID3 and backpropagation for English text-to-speech mapping. </title> <journal> Machine Learning, </journal> <volume> 18, </volume> <pages> 51-80. </pages>
Reference-contexts: We can reject the null hypothesis if jzj &gt; Z 0:975 = 1:96 (for a 2-sided test with probability of incorrectly rejecting the null hypothesis of 0.05). This test has been used by many researchers including the author <ref> (Dietterich, Hild, & Bakiri, 1995) </ref>. However, there are several problems with this test. First, because p A and p B are each measured on the same test set T , they are not independent.
Reference: <author> Efron, B., & Tibshirani, R. J. </author> <year> (1993). </year> <title> An Introduction to the Bootstrap. </title> <publisher> Chapman and Hall, </publisher> <address> New York, NY. </address>
Reference: <author> Everitt, B. S. </author> <year> (1977). </year> <title> The analysis of contingency tables. </title> <publisher> Chapman and Hall, London. </publisher>
Reference-contexts: We begin with simple holdout tests and then consider tests based on resampling from the available data. 3.1 McNemar's test To apply McNemar's test <ref> (Everitt, 1977) </ref>, we divide our available sample of data S, into a training set R and a test set T . We train both algorithms A and B on the training set yielding classifiers ^ f A and ^ f B .
Reference: <author> Frey, P. W., & Slate, D. J. </author> <year> (1991). </year> <title> Letter recognition using Holland-style adaptive classifiers. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 161-182. </pages>
Reference-contexts: To achieve this, we chose C4.5 Release 1 (Quinlan, 1993) and the first nearest-neighbor (NN) algorithm (Dasarathy, 1991). We then selected three difficult problems: the EXP6 problem developed by Kong (1995), the Letter Recognition data set <ref> (Frey & Slate, 1991) </ref>, and the Pima Indians Diabetes Task (Merz & Murphy, 1996). Of course, C4.5 and NN do not have the same performance on these data sets. In EXP6 and Letter Recognition, NN performs much better than C4.5; the reverse is true in the Pima data set.
Reference: <author> Haussler, D., Kearns, M., Seung, H. S., & Tishby, N. </author> <year> (1994). </year> <title> Rigorous learning curve bounds from statistical mechanics. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pp. 76-87. </pages> <publisher> ACM Press, </publisher> <address> New York, NY. </address>
Reference-contexts: This assumption can be checked experimentally (by performing additional cross-validation studies) with even smaller training sets, but it cannot be checked directly for training sets of the size of S. Results on the shape of learning curves show that in some cases this smoothness assumption will be violated <ref> (Haussler, Kearns, Seung, & Tishby, 1994) </ref>. Nonetheless, it is observed to hold experimentally in most applications.
Reference: <author> Hinton, G. E., Neal, R. M., Tibshirani, R., </author> & <title> DELVE team members (1995). Assessing learning procedures using DELVE. </title> <type> Tech. rep., </type> <institution> University of Toronto, Department of Computer Science, </institution> <note> http://www.cs.utoronto.ca/neuron/delve/delve.html. </note>
Reference: <author> Kohavi, R. </author> <year> (1995). </year> <title> Wrappers for performance enhancement and oblivious decision graphs. </title> <type> Ph.D. thesis, </type> <institution> Stanford University. </institution>
Reference: <author> Kolen, J. F., & Pollack, J. B. </author> <year> (1991). </year> <title> Back propagation is sensitive to initial conditions. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 3, </volume> <pages> pp. </pages> <address> 860-867 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Consider, for example, the widely-used backpropagation algorithm for training feed-forward neural networks. This algorithm is usually initialized with a set of random weights which it then improves. The resulting learned network depends critically on the random starting state <ref> (Kolen & Pollack, 1991) </ref>. In this case, even if the training data are not changed, the algorithm is likely to produce a different hypothesis if it is executed again from a different random starting state.
Reference: <author> Kong, E. B., & Dietterich, T. G. </author> <year> (1995). </year> <title> Error-correcting output coding corrects bias and variance. </title>
Reference: <editor> In Prieditis, A., & Russell, S. (Eds.), </editor> <booktitle> The Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 313-321 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lindman, H. R. </author> <year> (1992). </year> <title> Analysis of Variance in Experimental Design. </title> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference-contexts: Each algorithm is trained on each training set, and all resulting classifiers are tested on the test set. An analysis of variance can then be performed that includes terms for the choice of learning algorithm, the choice of the training set, and each individual test example. The Quasi-F test <ref> (Lindman, 1992) </ref> is applied to determine whether the effect due to the choice of learning algorithms is significantly non-zero.
Reference: <author> Merz, C. J., & Murphy, P. M. </author> <year> (1996). </year> <note> UCI repository of machine learning databases. http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: To achieve this, we chose C4.5 Release 1 (Quinlan, 1993) and the first nearest-neighbor (NN) algorithm (Dasarathy, 1991). We then selected three difficult problems: the EXP6 problem developed by Kong (1995), the Letter Recognition data set (Frey & Slate, 1991), and the Pima Indians Diabetes Task <ref> (Merz & Murphy, 1996) </ref>. Of course, C4.5 and NN do not have the same performance on these data sets. In EXP6 and Letter Recognition, NN performs much better than C4.5; the reverse is true in the Pima data set.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Empirical Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: If * A (x) = 0, then x is always correctly classified by classifiers produced by A. If * A (x) = 1, then x is always misclassified. the C4.5 decision tree algorithm <ref> (Quinlan, 1993) </ref> trained on randomly-drawn training sets of 100 examples. The points were sorted by their * values. Given these * values, we could simulate the behavior of C4.5 on a randomly drawn test set of points by taking each point x and misclassifying it with probability *(x). <p> We also needed the learning algorithms to be very efficient, so that the experiments could be replicated many times. To achieve this, we chose C4.5 Release 1 <ref> (Quinlan, 1993) </ref> and the first nearest-neighbor (NN) algorithm (Dasarathy, 1991). We then selected three difficult problems: the EXP6 problem developed by Kong (1995), the Letter Recognition data set (Frey & Slate, 1991), and the Pima Indians Diabetes Task (Merz & Murphy, 1996).
Reference: <author> Rasmussen, C. E. </author> <year> (1996). </year> <title> Evaluation of Gaussian processes and other methods for non-linear regression. </title> <type> Ph.D. thesis, </type> <institution> University of Toronto, Department of Computer Science, Toronto, Canada. </institution>
Reference: <author> Snedecor, G. W., & Cochran, W. G. </author> <year> (1989). </year> <title> Statistical Methods. </title> <institution> Iowa State University Press, Ames, IA. </institution> <note> Eighth Edition. 24 </note>
Reference-contexts: observed on training sets of size jRj will still hold for training sets of size jSj. 3.2 A test for the difference of two proportions A second simple statistical test is based on measuring the difference between the error rate of algorithm A and the error rate of algorithm B <ref> (Snedecor & Cochran, 1989) </ref>. Specifically, let p A = (n 00 + n 01 )=n be the proportion of test examples incorrectly classified by algorithm A and let p B = (n 00 + n 10 )=n be the proportion of test examples incorrectly classified by algorithm B. <p> The lack of independence of p A and p B can be corrected by changing the estimate of the standard error to be se 0 = n 01 + n 10 This estimate focuses on the probability of disagreement of the two algorithms <ref> (Snedecor & Cochran, 1989) </ref>. The resulting z statistic can be written as z 0 = p ; which we can recognize as the square root of the 2 statistic in McNemar's test.
References-found: 18

