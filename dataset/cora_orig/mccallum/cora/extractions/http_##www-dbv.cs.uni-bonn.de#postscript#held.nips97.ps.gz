URL: http://www-dbv.cs.uni-bonn.de/postscript/held.nips97.ps.gz
Refering-URL: http://www-dbv.cs.uni-bonn.de/abstracts/held.nips97.html
Root-URL: http://cs.uni-bonn.de
Email: email: fheld, jbg.cs.uni-bonn.de  
Title: Unsupervised On-Line Learning of Decision Trees for Hierarchical Data Analysis  
Author: Marcus Held and Joachim M. Buhmann Rheinische Friedrich-Wilhelms-Universitat 
Web: WWW: http://www-dbv.cs.uni-bonn.de  
Address: D-53117 Bonn, Germany  
Affiliation: Institut fur Informatik III, Romerstrae 164  
Abstract: An adaptive on-line algorithm is proposed to estimate hierarchical data structures for non-stationary data sources. The approach is based on the principle of minimum cross entropy to derive a decision tree for data clustering and it employs a metalearning idea (learning to learn) to adapt to changes in data characteristics. Its efficiency is demonstrated by grouping non-stationary artifical data and by hierarchical segmentation of LANDSAT images. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. M. Buhmann and H. Kuhnel. </author> <title> Vector quantization with complexity costs. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(4) </volume> <pages> 1133-1145, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: random variables yielding a fuzzy centroid rule y ff = i=1 X N hM iff i; (1) where the expected assignments hM iff i are given by Gibbs distributions hM iff i = P K : (2) For a more detailed discussion of the DA approach to data clustering cf. <ref> [1, 3, 5] </ref>. In addition to assigning data to clusters (1,2), hierarchical clustering provides the partitioning of data space with a tree structure. Each data sample x is sequentially assigned to a nested structure of partitions which hierarchically cover the data space IR d . <p> For this setting there exists the need of on-line learning rules for the prototypes at the leaves, the test vectors at the inner nodes and the parameters fl and fi. Unbalanced trees also require rules for splitting and merging nodes. Following Buhmann and Kuhnel <ref> [1] </ref> we use an expansion of order O (1=n) of (1) to estimate the prototypes for the N th datapoint y N ff + j ff Nff i ff M x N y N1 where p N ff p N1 Nff i p N1 denotes the probability of the occurence of
Reference: [2] <author> T.M. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley & Sons, </publisher> <year> 1991. </year>
Reference-contexts: To facilitate this minimization a deterministic annealing approach was proposed in [5] which maps the discrete optimization problem, i.e. how to determine the data assignments, via the Maximum Entropy Principle <ref> [2] </ref> to a continuous parameter es in Advances in Neural Information Processing Systems (NIPS), 97 (to appear) 2 timation problem. Deterministic annealing introduces a Lagrange multiplier fi to control the approximation of H (M; Y) in a probabilistic sense.
Reference: [3] <author> D. Miller and K. Rose. </author> <title> Hierarchical unsupervised learning with growing via phase transitions. </title> <journal> Neural Computation, </journal> <volume> 8 </volume> <pages> 425-450, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: random variables yielding a fuzzy centroid rule y ff = i=1 X N hM iff i; (1) where the expected assignments hM iff i are given by Gibbs distributions hM iff i = P K : (2) For a more detailed discussion of the DA approach to data clustering cf. <ref> [1, 3, 5] </ref>. In addition to assigning data to clusters (1,2), hierarchical clustering provides the partitioning of data space with a tree structure. Each data sample x is sequentially assigned to a nested structure of partitions which hierarchically cover the data space IR d . <p> This problem is solvable by a two-stage approach, which on the one hand minimizes the distortion costs at the leaves given the tree structure and on the other hand optimizes the tree structure given the leaf induced partition of IR d . This approach, due to Miller & Rose <ref> [3] </ref>, is summarized in section 2. The extensions for adaptive on line learning and experimental results are described in sections 3 and 4, respectively. data space (positions of the letters also indicate the positions of the prototypes). <p> Based on the maximum entropy principle, the probability H i;j that data point x i reaches inner node s j is recursively defined by (see <ref> [3] </ref>): H i;root := 1; H i;j = H i;parent (j) i;j ; i;j = exp (flD (x i ; s j )) k2siblings (j) exp (flD (x i ; s k )) in Advances in Neural Information Processing Systems (NIPS), 97 (to appear) 3 where the Lagrange multiplier fl controls <p> After convergence one increases fi and optimizes the hierarchy and the prototypes at the leaves again. The increment of fi leads to phase transitions where test vectors separate from each other and the formerly completely degenerated tree evolves its structure. For a detailed description of this algorithm see <ref> [3] </ref>. in Advances in Neural Information Processing Systems (NIPS), 97 (to appear) 4 3 On-Line Learning of Decision Trees Learning of decision trees is refined in this paper to deal with unbalanced trees and on-line learning of trees. <p> The adaptation of fi has to observe the necessary condition for a phase transition fi &gt; fi crit j 1=2ffi max , ffi max being the largest eigenvalue of the covariance matrix <ref> [3] </ref> ff = i=1 t M X hM iff i: (16) Rules for splitting and merging nodes of the tree are introduced to deal with unbalanced trees and non-stationary data. Simple rules measure the distortion costs at the prototypes of the leaves.
Reference: [4] <author> N. Murata, K.-R. Muller, A. Ziehe, and S. Amari. </author> <title> Adaptive on-line learning in changing environments. </title> <editor> In M.C. Mozer, M.I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> number 9, </volume> <pages> pages 599-605. </pages> <publisher> MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: the gradients (8) and (9) s N j + j j fl N1 1 x N ; s N1 fl N := fl N1 j fl s j 2S j : (12) For an appropriate choice of the learning rates j, the learning to learn approach of Murata et al. <ref> [4] </ref> suggests the learning algorithm w N = w N1 j N1 f x N ; w N1 The flow f in parameter space determines the change of w N1 given a new datapoint x N .
Reference: [5] <author> K. Rose, E. Gurewitz, and G.C. Fox. </author> <title> A deterministic annealing approach to clustering. </title> <journal> Pattern Recognition Letters, </journal> <volume> 11(9) </volume> <pages> 589-594, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: For reasons of simplicity we restrict the presentation to the sum-of-squared-error criterion D (x; y) = kx yk 2 in this paper. To facilitate this minimization a deterministic annealing approach was proposed in <ref> [5] </ref> which maps the discrete optimization problem, i.e. how to determine the data assignments, via the Maximum Entropy Principle [2] to a continuous parameter es in Advances in Neural Information Processing Systems (NIPS), 97 (to appear) 2 timation problem. <p> random variables yielding a fuzzy centroid rule y ff = i=1 X N hM iff i; (1) where the expected assignments hM iff i are given by Gibbs distributions hM iff i = P K : (2) For a more detailed discussion of the DA approach to data clustering cf. <ref> [1, 3, 5] </ref>. In addition to assigning data to clusters (1,2), hierarchical clustering provides the partitioning of data space with a tree structure. Each data sample x is sequentially assigned to a nested structure of partitions which hierarchically cover the data space IR d .
References-found: 5

