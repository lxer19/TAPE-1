URL: http://www.neci.nj.nec.com/homepages/giles/papers/IEEE.TNN.1st-vs-2nd.order.rnn.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/html/CLG_pub.html
Root-URL: 
Title: First-Order vs. Second-Order Single Layer Recurrent Neural Networks  
Author: Mark W. Goudreau C. Lee Giles Srimat T. Chakradhar D. Chen 
Abstract: We examine the representational capabilities of first-order and second-order Single Layer Recurrent Neural Networks (SLRNNs) with hard-limiting neurons. We show that a second-order SLRNN is strictly more powerful than a first-order SLRNN. However, if the first-order SLRNN is augmented with output layers of feedforward neurons, it can implement any finite-state recognizer, but only if state-splitting is employed. When a state is split, it is divided into two equivalent states. The judicious use of state-splitting allows for efficient implementation of finite-state recognizers using augmented first-order SLRNNs.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon, A. Dewdney, and T. Ott, </author> <title> "Efficient simulation of finite automata by neural nets," </title> <journal> 7 Journal of the Association for Computing Machinery, </journal> <volume> vol. 38, no. 2, </volume> <pages> pp. 495-514, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recurrent Neural Networks (RNNs) have been used for a variety of problems, including grammatical inference [4, 5], and the implementation of finite automata <ref> [1, 7] </ref>. There has also been interest in the representational abilities of neural networks [9, 10]. Ideally, we would like to find an RNN architecture that has good representational capabilities and can be efficiently constructed as a VLSI circuit [2]. We show that certain simple RNNs have limited representational capabilities. <p> States C and D of Figure 2 (b) are equivalent to state A in is called state-splitting since state A "splits" into states C and D. Let M = 2 and N = 2. Now let S 0 = <ref> [0; 1] </ref> T (for state C), S 1 = [1; 0] T (for state D), and S 2 = [1; 1] T (for state E). S 0 is the initial state vector. The input vectors are I 0 = [1; 0] T and I 1 = [1; 1] T . <p> States C and D of Figure 2 (b) are equivalent to state A in is called state-splitting since state A "splits" into states C and D. Let M = 2 and N = 2. Now let S 0 = [0; 1] T (for state C), S 1 = <ref> [1; 0] </ref> T (for state D), and S 2 = [1; 1] T (for state E). S 0 is the initial state vector. The input vectors are I 0 = [1; 0] T and I 1 = [1; 1] T . <p> Let M = 2 and N = 2. Now let S 0 = [0; 1] T (for state C), S 1 = [1; 0] T (for state D), and S 2 = <ref> [1; 1] </ref> T (for state E). S 0 is the initial state vector. The input vectors are I 0 = [1; 0] T and I 1 = [1; 1] T . <p> Now let S 0 = [0; 1] T (for state C), S 1 = <ref> [1; 0] </ref> T (for state D), and S 2 = [1; 1] T (for state E). S 0 is the initial state vector. The input vectors are I 0 = [1; 0] T and I 1 = [1; 1] T . <p> let S 0 = [0; 1] T (for state C), S 1 = [1; 0] T (for state D), and S 2 = <ref> [1; 1] </ref> T (for state E). S 0 is the initial state vector. The input vectors are I 0 = [1; 0] T and I 1 = [1; 1] T .
Reference: [2] <author> L. Atlas and Y. Suzuki, </author> <title> "Digital systems for artificial neural networks," </title> <journal> IEEE Circuits and Devices Magazine, </journal> <volume> vol. 5, </volume> <pages> pp. 20-24, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: There has also been interest in the representational abilities of neural networks [9, 10]. Ideally, we would like to find an RNN architecture that has good representational capabilities and can be efficiently constructed as a VLSI circuit <ref> [2] </ref>. We show that certain simple RNNs have limited representational capabilities. This is a first step in the design of more sophisticated architectures. Several RNN architectures have been proposed in the literature [3, 8, 11].
Reference: [3] <author> J. Elman, </author> <title> "Finding structure in time," </title> <journal> Cognitive Science, </journal> <volume> vol. 14, </volume> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference-contexts: We show that certain simple RNNs have limited representational capabilities. This is a first step in the design of more sophisticated architectures. Several RNN architectures have been proposed in the literature <ref> [3, 8, 11] </ref>. In this work we first consider the Single Layer Recurrent Neural Network (SLRNN) shown in Figure 1. The SLRNN has M inputs, that are labelled x 1 ; x 2 ; : : : ; x M .
Reference: [4] <author> S. Fahlman, </author> <title> "The recurrent cascade-correlation architecture," </title> <booktitle> in Advances in Neural Information Processing Systems 3 (R. </booktitle> <editor> Lippmann, J. Moody, and D. Touretzky, eds.), </editor> <address> (San Mateo, CA), </address> <pages> pp. 190-196, </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction Recurrent Neural Networks (RNNs) have been used for a variety of problems, including grammatical inference <ref> [4, 5] </ref>, and the implementation of finite automata [1, 7]. There has also been interest in the representational abilities of neural networks [9, 10]. Ideally, we would like to find an RNN architecture that has good representational capabilities and can be efficiently constructed as a VLSI circuit [2].
Reference: [5] <author> C. Giles, C. Miller, D. Chen, H. Chen, G. Sun, and Y. Lee, </author> <title> "Learning and extracting finite state automata with second-order recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <pages> pp. 393-405, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Recurrent Neural Networks (RNNs) have been used for a variety of problems, including grammatical inference <ref> [4, 5] </ref>, and the implementation of finite automata [1, 7]. There has also been interest in the representational abilities of neural networks [9, 10]. Ideally, we would like to find an RNN architecture that has good representational capabilities and can be efficiently constructed as a VLSI circuit [2].
Reference: [6] <author> Z. Kohavi, </author> <title> Switching and Finite Automata Theory. </title> <address> New York, NY: </address> <publisher> McGraw-Hill, Inc., </publisher> <editor> second ed., </editor> <year> 1978. </year>
Reference-contexts: We represent the output vector by O t = [y t 1 ; y t K ] T . SLRNN architectures are similar to Mealy machines, which can be used to implement arbitrary finite-state machines <ref> [6] </ref>.
Reference: [7] <author> M. Minsky, </author> <title> Computation: Finite and Infinite Machines, </title> <journal> ch. </journal> <volume> 3, </volume> <pages> pp. 32-66. </pages> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, Inc., </publisher> <year> 1967. </year>
Reference-contexts: 1 Introduction Recurrent Neural Networks (RNNs) have been used for a variety of problems, including grammatical inference [4, 5], and the implementation of finite automata <ref> [1, 7] </ref>. There has also been interest in the representational abilities of neural networks [9, 10]. Ideally, we would like to find an RNN architecture that has good representational capabilities and can be efficiently constructed as a VLSI circuit [2]. We show that certain simple RNNs have limited representational capabilities. <p> Therefore, second-order SLRNNs are strictly more powerful than first-order SLRNNs. Furthermore, we show that second-order SLRNNs can simulate any finite-state recog-nizer and hence, they have the same representational abilities as Melay machines. If the first-order SLRNN is augmented by a feedforward network of neurons (see Figure 1), Minsky <ref> [7] </ref> shows that the augmented first-order SLRNN can simulate any finite-state recognizer. Therefore, the augmented first-order SLRNN, the second-order SLRNN, and the Mealy machine have identical representational abilities. We also investigate the use of augmented first-order SLRNNs to implement finite-state recogniz-ers. <p> Therefore, we will ignore the output mapping and concern ourselves solely with the state transition problem. Minsky has already shown that an augmented first-order SLRNN can implement any finite-state recognizer <ref> [7] </ref>. However, his approach is likely to lead to implementations that have many more neurons than are necessary. If there are n states and m inputs to a finite-state recognizer, Minsky's approach requires nm neurons in the bottom (feedback) layer. <p> SLRNN with the following weights recognizes odd parity strings: w 11 = 1; w 12 = 1; w 13 = 3 w 21 = 1; w 22 = 1; w 23 = 5 (11) This implementation of odd parity requires only two state neurons, while Minsky's implementation would require four <ref> [7] </ref>. 4 Conclusion We have shown that a second-order SLRNN can implement any finite-state recognizer, while a first-order SLRNN can not. This is an example of the improved representational ability achieved by utilizing a second-order network.
Reference: [8] <author> J. Pollack, </author> <title> "The induction of dynamical recognizers," </title> <journal> Machine Learning, </journal> <volume> vol. 7, </volume> <editor> p. </editor> <volume> 227, </volume> <year> 1991. </year>
Reference-contexts: We show that certain simple RNNs have limited representational capabilities. This is a first step in the design of more sophisticated architectures. Several RNN architectures have been proposed in the literature <ref> [3, 8, 11] </ref>. In this work we first consider the Single Layer Recurrent Neural Network (SLRNN) shown in Figure 1. The SLRNN has M inputs, that are labelled x 1 ; x 2 ; : : : ; x M .
Reference: [9] <author> D. Seidl and R. Lorenz, </author> <title> "A structure by which a recurrent neural network can approximate a nonlinear dynamic system," </title> <booktitle> in Proceedings of the International Joint Conference on Neural Networks 1991, </booktitle> <volume> vol. II, </volume> <pages> pp. 709-714, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recurrent Neural Networks (RNNs) have been used for a variety of problems, including grammatical inference [4, 5], and the implementation of finite automata [1, 7]. There has also been interest in the representational abilities of neural networks <ref> [9, 10] </ref>. Ideally, we would like to find an RNN architecture that has good representational capabilities and can be efficiently constructed as a VLSI circuit [2]. We show that certain simple RNNs have limited representational capabilities. This is a first step in the design of more sophisticated architectures.
Reference: [10] <author> H. Siegelmann and E. Sontag, </author> <title> "On the computational power of neural nets," </title> <booktitle> in Proceedings of the Fifth ACM Workshop on Computational Learning Theory, </booktitle> <address> (Pittsburgh, PA), </address> <month> July </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Recurrent Neural Networks (RNNs) have been used for a variety of problems, including grammatical inference [4, 5], and the implementation of finite automata [1, 7]. There has also been interest in the representational abilities of neural networks <ref> [9, 10] </ref>. Ideally, we would like to find an RNN architecture that has good representational capabilities and can be efficiently constructed as a VLSI circuit [2]. We show that certain simple RNNs have limited representational capabilities. This is a first step in the design of more sophisticated architectures.
Reference: [11] <author> R. Watrous and G. Kuhn, </author> <title> "Induction of finite state languages using second-order recurrent networks," </title> <booktitle> in Advances in Neural Information Processing Systems 4 (J. </booktitle> <editor> Moody, S. Hanson, and R. Lippmann, eds.), </editor> <address> (San Mateo, CA), </address> <pages> pp. 309-316, </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year> <month> 8 </month>
Reference-contexts: We show that certain simple RNNs have limited representational capabilities. This is a first step in the design of more sophisticated architectures. Several RNN architectures have been proposed in the literature <ref> [3, 8, 11] </ref>. In this work we first consider the Single Layer Recurrent Neural Network (SLRNN) shown in Figure 1. The SLRNN has M inputs, that are labelled x 1 ; x 2 ; : : : ; x M .
References-found: 11

