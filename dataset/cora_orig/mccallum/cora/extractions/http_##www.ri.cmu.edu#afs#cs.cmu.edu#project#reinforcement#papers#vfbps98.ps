URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/reinforcement/papers/vfbps98.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs/user/awm/web/papers.html
Root-URL: 
Email: fschneide,jab,awmg@cs.cmu.edu  
Title: Value Function Based Production Scheduling  
Author: Jeff G. Schneider and Justin A. Boyan and Andrew W. Moore 
Keyword: Reinforcement learning, Applications, Production scheduling, Optimization, Memory-based learning.  
Address: Pittsburgh, PA 15213  
Affiliation: Robotics Institute and Computer Science Department Carnegie Mellon University  
Abstract: Production scheduling, the problem of sequentially configuring a factory to meet forecasted demands, is a critical problem throughout the manufacturing industry. The requirement of maintaining product inventories in the face of unpredictable demand and stochastic factory output makes standard scheduling models, such as job-shop, inadequate. Currently applied algorithms, such as simulated annealing and constraint propagation, must employ ad-hoc methods such as frequent replanning to cope with uncertainty. In this paper, we describe a Markov Decision Process (MDP) formulation of production scheduling which captures stochasticity in both production and demands. The solution to this MDP is a value function which can be used to generate optimal scheduling decisions online. A simple example illustrates the theoretical superiority of this approach over replanning-based methods. We then describe an industrial application and two reinforcement learning methods for generating an approximate value function on this domain. Our results demonstrate that in both deterministic and noisy scenarios, value function approx imation is an effective technique. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Atkeson, S. Schaal, and A. Moore. </author> <title> Locally weighted learning. </title> <journal> AI Review, </journal> <year> 1995. </year>
Reference-contexts: At each step of each trajectory, a one-step backup operation (Eq. 1) is performed and the function approximator is updated. In Memory-Based RTDP, the value function is represented by a nonpara-metric memory-based function approximator <ref> [5, 7, 1] </ref>. Memory-based learning simply accumulates training data points, rather than running a training algorithm on them. Then whenever a query is made, the approximator's output is computed by an average or regression over nearby points in memory.
Reference: [2] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <journal> AI Journal, </journal> <year> 1995. </year>
Reference-contexts: The two methods we tested are Memory-Based RTDP and ROUT. 3.3.1 Memory-Based RTDP Memory-Based RTDP is a reinforcement learning approach that is closely related to RTDP (Real-Time Dynamic Programming) <ref> [2] </ref> and to Tesauro's 9 application of TD (0) to the game of backgammon [10, 11]. It is also similar to the instance based approach to representing value functions is done in [9].
Reference: [3] <author> R. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <year> 1957. </year> <month> 15 </month>
Reference-contexts: Dynamic programming methods tabulate this optimal cumulative reward in the value function V fl (x), which is the unique solution to the Bellman equations <ref> [3] </ref>: V fl (x) = max 0 X P (x 0 jx; a)V fl (x 0 ) A (1) Once V fl is computed, the optimal policy fl is immediately obtained by choosing any action which instantiates the max in Eq. 1.
Reference: [4] <author> J. A. Boyan and A. W. Moore. </author> <title> Learning evaluation functions for large acyclic domains. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: RTDP's early trajectories are sure to be inaccurate samples of V fl , we did not find it necessary to include an explicit "forgetting" mechanism in the learning. 3.3.2 ROUT ROUT is an active learning algorithm for value function approximation that is specifically designed for the subclass of acyclic MDPs <ref> [4] </ref>. (The scheduling MDP is certainly acyclic, since the time counter t is part of its state representation.) Using simulations of the process, ROUT repeatedly identifies a state x at which (1) the function approximator is currently in error, and (2) an accurate sample of V fl can be obtained from <p> Unlike Memory-Based RTDP and most other reinforcement learning methods, ROUT explicitly tries to prevent the function approximator from seeing any inaccurate samples of V fl . Details of how ROUT identifies such states automatically are given in <ref> [4] </ref>. One by one, these useful states are accumulated into a training set of accurate samples of V fl (x). The training set grows backwards from the terminal states.
Reference: [5] <author> W. Cleveland and S. Delvin. </author> <title> Locally weighted regression: An approach to regression analysis by local fitting. </title> <journal> Journal of the American Statistical Association, </journal> <pages> pages 596-610, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: At each step of each trajectory, a one-step backup operation (Eq. 1) is performed and the function approximator is updated. In Memory-Based RTDP, the value function is represented by a nonpara-metric memory-based function approximator <ref> [5, 7, 1] </ref>. Memory-based learning simply accumulates training data points, rather than running a training algorithm on them. Then whenever a query is made, the approximator's output is computed by an average or regression over nearby points in memory.
Reference: [6] <author> S. Mahadevan, N. Marchalleck, T. Das, and A. Gosavi. </author> <title> Self-Improving Factory Simulation using Continuous-Time Average-Reward Reinforcement Learning. </title> <booktitle> In Proceedings of the 14th International Conference on Machine Learning (IMLC '97), </booktitle> <address> Nashville, TN. </address> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1997. </year>
Reference-contexts: To our knowledge, this work represents the first application of reinforcement learning to the production scheduling task. The scheduling of machine maintenance is discussed in <ref> [6] </ref>. There, reinforcement learning and queueing theory techniques are used to make binary decisions about taking machines down for maintenance. A reinforcement learning approach to the Space Shuttle scheduling problem is described by [13]. In that framework, states are complete schedules and actions are modification operators applied to the schedules.
Reference: [7] <author> A. Moore, C. Atkeson, and S. Schaal. </author> <title> Locally weighted learning for control. </title> <journal> AI Review, </journal> <year> 1995. </year>
Reference-contexts: At each step of each trajectory, a one-step backup operation (Eq. 1) is performed and the function approximator is updated. In Memory-Based RTDP, the value function is represented by a nonpara-metric memory-based function approximator <ref> [5, 7, 1] </ref>. Memory-based learning simply accumulates training data points, rather than running a training algorithm on them. Then whenever a query is made, the approximator's output is computed by an average or regression over nearby points in memory.
Reference: [8] <author> E. Ochotta. </author> <title> Synthesis of High-Performance Analog Cells in AS-TRX/OBLX. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University Department of Electrical and Computer Engineering, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: It usually incurs no heuristic penalties in its schedules, so that figure is a profit in real dollars. Simulated annealing, greedy exploration, and Memory-Based RTDP are run as described in the previous sections. The simulated annealing runs made use of the successful "modified Lam" adaptive annealing schedule <ref> [8] </ref>. The poor result from greedy shows that generating trajectories based solely on the one-step cost of configurations is not an effective way to search even when compared to a random search method such as simulated annealing.
Reference: [9] <author> Jing Peng. </author> <title> Efficient Dynamic Programming-based Learning for Control. </title> <type> PhD. Thesis, </type> <institution> Northeastern University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: It is also similar to the instance based approach to representing value functions is done in <ref> [9] </ref>. Trajectories through the MDP model are generated repeatedly, using the current approximation of the value function to guide Boltzmann-style exploration. At each step of each trajectory, a one-step backup operation (Eq. 1) is performed and the function approximator is updated.
Reference: [10] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <year> 1988. </year>
Reference-contexts: The two methods we tested are Memory-Based RTDP and ROUT. 3.3.1 Memory-Based RTDP Memory-Based RTDP is a reinforcement learning approach that is closely related to RTDP (Real-Time Dynamic Programming) [2] and to Tesauro's 9 application of TD (0) to the game of backgammon <ref> [10, 11] </ref>. It is also similar to the instance based approach to representing value functions is done in [9]. Trajectories through the MDP model are generated repeatedly, using the current approximation of the value function to guide Boltzmann-style exploration.
Reference: [11] <author> G. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8(3/4), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: The two methods we tested are Memory-Based RTDP and ROUT. 3.3.1 Memory-Based RTDP Memory-Based RTDP is a reinforcement learning approach that is closely related to RTDP (Real-Time Dynamic Programming) [2] and to Tesauro's 9 application of TD (0) to the game of backgammon <ref> [10, 11] </ref>. It is also similar to the instance based approach to representing value functions is done in [9]. Trajectories through the MDP model are generated repeatedly, using the current approximation of the value function to guide Boltzmann-style exploration.
Reference: [12] <author> G. Tesauro and G. R. Galperin. </author> <title> On-line policy improvement using Monte-Carlo search. </title> <editor> In M. Mozer, M. Jordan, and T. Petsche, editors, NIPS-9, </editor> <year> 1997. </year>
Reference-contexts: Further empirical work is required to answer that question. As the size of the scheduling problem increases, it becomes increasingly expensive to compute the value function accurately. However, even an inexact value function can be useful as the basis for a quasi-greedy search or "rollout" search performed online <ref> [12] </ref>. We intend to test such methods in future work on larger scheduling problems.
Reference: [13] <author> W. Zhang and T. G. Dietterich. </author> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In Proceedings of IJCAI-95, </booktitle> <pages> pages 1114-1120, </pages> <year> 1995. </year>
Reference-contexts: The scheduling of machine maintenance is discussed in [6]. There, reinforcement learning and queueing theory techniques are used to make binary decisions about taking machines down for maintenance. A reinforcement learning approach to the Space Shuttle scheduling problem is described by <ref> [13] </ref>. In that framework, states are complete schedules and actions are modification operators applied to the schedules. Their feature representation introduces noise, but the underlying problem is deterministic. Our work to date covers stochasticity only in production.
Reference: [14] <author> M. Zweben and M. Fox. </author> <title> Intelligent Scheduling. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: This model cannot readily be adapted to handle production rate interdependencies among machines, the desire to keep inventory levels above zero at all times (rather than just completing jobs by their deadlines), and stochasticity of demand forecasts and production. Constraint propagation methods (e.g. <ref> [14] </ref>) are commonly used to solve industrial problems. They operate by efficiently managing constraints on production deadlines and machine capabilities. Solution methods tend to search by iteratively fixing violated constraints, and use heuristics to guide the fixes.
References-found: 14

