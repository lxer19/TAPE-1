URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/rtls93.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/
Root-URL: http://www.cs.umn.edu
Title: Recursive Total Least Squares: An Alternative to the Discrete Kalman Filter  
Author: Daniel L. Boley and Karen T. Sutherland 
Address: Minneapolis, MN 55455  
Affiliation: Computer Science Department University of Minnesota  
Abstract: The discrete Kalman filter, which is becoming a common tool for reducing uncertainty in robot navigation, suffers from some basic limitations when used for such applications. In this paper, we describe a recursive total least squares estimator (RTLS) as an alternative to the Kalman filter, and compare their performances in three sets of experiments involving problems in robot navigation. In all cases, the RTLS filter converged faster and to more accuracy than the Kalman filter. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen, </author> <title> LAPACK User's Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year> <month> 17 </month>
Reference-contexts: In our implementation we chose the one of Van Loan [26], which is based on the Euclidean norm and usually yields good accuracy especially for well separated singular values. But one could also use the ones in LINPACK [6], LAPACK <ref> [1] </ref>, or any in Higham's excellent survey [13]. * Deflate One: Deflate the ULV Decomposition by one (i.e., apply transformation and decrement the rank index by one so that the smallest singular value in the leading r fi r part of L is "moved" to the trailing rows).
Reference: [2] <author> N. Ayache and O. D. Faugeras, </author> <title> Maintaining representations of the environment of a mobile robot, </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 5 (1989), </volume> <pages> pp. 804-819. </pages>
Reference-contexts: Also, due to the fact that functions are frequently non-linear, the extended Kalman filter (EKF) is used <ref> [2, 15] </ref>. The EKF formalism linearizes the function by taking a first order Taylor expansion around the current estimate of the state vector [9]. <p> Ten trials were run with each algorithm with lines of different slopes. 0, 1 and 5. In our second set of experiments, we simulated a simple robot navigation problem typical of that faced by an actual mobile robot <ref> [2, 15, 16, 17] </ref>. The robot has identified a single landmark in a two-dimensional environment and knows landmark location on a map. It does not know its own position. It moves in a straight line and with a known uniform velocity.
Reference: [3] <author> N. K. Bose, H. C. Kim, and H. M. Valenzuela, </author> <title> Recursive implementation of total least squares algorithm for image reconstruction from noisy, </title> <booktitle> undersampled multiframes, in Proceedings of 1993 International Conference on Acoustics, Speech and Signal Processing, IEEE, </booktitle> <month> May </month> <year> 1993, </year> <pages> pp. </pages> <month> V-269-V-272. </month>
Reference: [4] <author> C. E. Davila, </author> <title> Recursive total least squares algorithms for adaptive filtering, </title> <booktitle> in Proceedings of 1991 International Conference on Acoustics, Speech and Signal Processing, IEEE, </booktitle> <month> May </month> <year> 1991, </year> <pages> pp. 1853-1856. </pages>
Reference-contexts: The most common algorithms to compute the TLS solution are based on the Singular Value Decomposition (SVD), a non-recursive matrix decomposition which is computationally expensive to update. Recently, some recursive TLS filters have been developed for applications in signal processing <ref> [4, 5, 7, 27] </ref>. Davila [4] used a Kalman filter to obtain a fast update for the eigenvector corresponding to the smallest eigenvalue of the covariance matrix. This eigenvector was then used to solve a symmetric TLS problem for the filter. <p> The most common algorithms to compute the TLS solution are based on the Singular Value Decomposition (SVD), a non-recursive matrix decomposition which is computationally expensive to update. Recently, some recursive TLS filters have been developed for applications in signal processing [4, 5, 7, 27]. Davila <ref> [4] </ref> used a Kalman filter to obtain a fast update for the eigenvector corresponding to the smallest eigenvalue of the covariance matrix. This eigenvector was then used to solve a symmetric TLS problem for the filter.
Reference: [5] <author> R. D. DeGroat, </author> <title> Noniterative subspace tracking, </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 40 (1992), </volume> <pages> pp. 571-577. </pages>
Reference-contexts: The most common algorithms to compute the TLS solution are based on the Singular Value Decomposition (SVD), a non-recursive matrix decomposition which is computationally expensive to update. Recently, some recursive TLS filters have been developed for applications in signal processing <ref> [4, 5, 7, 27] </ref>. Davila [4] used a Kalman filter to obtain a fast update for the eigenvector corresponding to the smallest eigenvalue of the covariance matrix. This eigenvector was then used to solve a symmetric TLS problem for the filter. <p> In <ref> [5, 7] </ref>, DeGroat and Dowling used this approach combined with the averaging technique used in [27], again assuming that the singular values could be grouped into two clusters.
Reference: [6] <author> J. J. Dongarra, J. R. Bunch, C. B. Moler, and G. W. Stewart, </author> <title> LINPACK User's Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: In our implementation we chose the one of Van Loan [26], which is based on the Euclidean norm and usually yields good accuracy especially for well separated singular values. But one could also use the ones in LINPACK <ref> [6] </ref>, LAPACK [1], or any in Higham's excellent survey [13]. * Deflate One: Deflate the ULV Decomposition by one (i.e., apply transformation and decrement the rank index by one so that the smallest singular value in the leading r fi r part of L is "moved" to the trailing rows).
Reference: [7] <author> E. M. Dowling and R. D. DeGroat, </author> <title> Recursive total least squares adaptive filtering, </title> <booktitle> in SPIE Proceedings on Adaptive Signal Processing, </booktitle> <volume> vol. 1565, </volume> <booktitle> SPIE, </booktitle> <month> July </month> <year> 1991, </year> <pages> pp. 35-46. </pages>
Reference-contexts: The most common algorithms to compute the TLS solution are based on the Singular Value Decomposition (SVD), a non-recursive matrix decomposition which is computationally expensive to update. Recently, some recursive TLS filters have been developed for applications in signal processing <ref> [4, 5, 7, 27] </ref>. Davila [4] used a Kalman filter to obtain a fast update for the eigenvector corresponding to the smallest eigenvalue of the covariance matrix. This eigenvector was then used to solve a symmetric TLS problem for the filter. <p> In <ref> [5, 7] </ref>, DeGroat and Dowling used this approach combined with the averaging technique used in [27], again assuming that the singular values could be grouped into two clusters.
Reference: [8] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley and Sons, Inc., </publisher> <editor> 1st ed., </editor> <year> 1973. </year>
Reference-contexts: (a) In an LS solution, the sum of the squared vertical distances to the line of best fit is minimized. (b) In a TLS solution, the sum of the squared perpendicular distances to the line of best fit is minimized. perpendicular distances from the points to the lines are minimized <ref> [8] </ref> (Fig. 1). This second method is known in the statistical literature as orthogonal regression and in numerical analysis as total least squares (TLS) [24].
Reference: [9] <author> A. Gelb, </author> <title> Applied Optimal Estimation, </title> <editor> The M. I. T. Press, 1st ed., </editor> <year> 1974. </year>
Reference-contexts: The Kalman filter equations and a schematic diagram of the filter are in Appendix I. A complete description of the filter can be found in <ref> [9] </ref>. It requires an initial estimate of the solution and assumes that noise is weighted white gaussian. The discrete Kalman filter is guaranteed to be optimal in that it is guaranteed to find the best solution in the least squares sense. <p> Also, due to the fact that functions are frequently non-linear, the extended Kalman filter (EKF) is used [2, 15]. The EKF formalism linearizes the function by taking a first order Taylor expansion around the current estimate of the state vector <ref> [9] </ref>.
Reference: [10] <author> G. H. Golub and C. F. V. Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins, </publisher> <editor> 2nd ed., </editor> <year> 1989. </year>
Reference-contexts: The gathering and processing of each image is a time consuming process so a successful method must make do with relatively few readings. * An underlying assumption in least squares estimation is that the entries in the data matrix are error-free <ref> [10] </ref>. In many actual applications, the errors in the data matrix can be at least as great as the measurement errors. In such cases, the Kalman filter can give poor results. <p> The TLS approach has received a lot of attention in the numerical analysis literature, partly because it arises in so many applications (see e.g. <ref> [10, 24] </ref>). The most common algorithms to compute the TLS solution are based on the Singular Value Decomposition (SVD), a non-recursive matrix decomposition which is computationally expensive to update. Recently, some recursive TLS filters have been developed for applications in signal processing [4, 5, 7, 27]. <p> However, if the eigenproblem for the covariance matrix is replaced by the singular value problem on the signals, the condition 4 numbers involved in the least squares solution can be reduced to their square roots <ref> [10, 12] </ref>, potentially doubling the number of digits of accuracy in the computed solutions. In [5, 7], DeGroat and Dowling used this approach combined with the averaging technique used in [27], again assuming that the singular values could be grouped into two clusters. <p> Each basic procedure costs O (p 2 ) operations. The basic procedures consist of a series of simple annihilation operations. Each annihilation operation is accomplished with a sequence of plane (Givens) rotations <ref> [10] </ref>, which are orthogonal matrices of the form Q = B @ 0 c s 0 0 0 0 I C A where c 2 + s 2 = 1 and the I's represent identity matrices of appropriate dimensions. <p> No determination is made if the rank has really increased by 1; this is done elsewhere. The process begins by applying p Givens rotations <ref> [10] </ref> from the right to rotate all the nonzeroes in the new row all the way to the left. Then p rotations are applied from the left to restore L to lower triangular. <p> After trying several larger values of j, and comparing with the mixed LS-TLS method <ref> [10, 24] </ref>, a nonrecursive algorithm capable of treating certain columns as exact, we found it sufficed to use j = 100 for all experiments. The RTLS filter performs better as the lines become more vertical.
Reference: [11] <author> G. Hager and M. Mintz, </author> <title> Computational methods for task-directed sensor data fusion and sensor planning, </title> <journal> The International Journal of Robotics Research, </journal> <volume> 10 (1991), </volume> <pages> pp. 285-313. </pages>
Reference-contexts: As the interesting recent work by Mintz et al <ref> [11, 18] </ref> in robust estimation and modeling of sensor noise has demonstrated, the criterion of optimality depends critically on the specific model being used. Given two methods, the first may produce optimality in one sense but not do as well as the second in another sense.
Reference: [12] <author> S. Haykin, </author> <title> Adaptive Filter Theory, </title> <publisher> Prentice Hall, </publisher> <editor> 2nd ed., </editor> <year> 1991. </year>
Reference-contexts: However, if the eigenproblem for the covariance matrix is replaced by the singular value problem on the signals, the condition 4 numbers involved in the least squares solution can be reduced to their square roots <ref> [10, 12] </ref>, potentially doubling the number of digits of accuracy in the computed solutions. In [5, 7], DeGroat and Dowling used this approach combined with the averaging technique used in [27], again assuming that the singular values could be grouped into two clusters. <p> a ULV Decomposition of the matrix (A; b) and an approximate TLS solution to Ax b, our task is to find a TLS solution b x to the augmented system b A b x b b, where b A = A fi ; and is an optional exponential forgetting factor <ref> [12] </ref>. The algorithm presented here is derived from the SVD-based TLS algorithm (Algorithm 3.1 of [24]), but simplified for a single right hand side for the sake of clarity. Multiple right 9 hand sides can be handled in a similar fashion.
Reference: [13] <author> N. J. Higham, </author> <title> A survey of condition number estimators for triangular matrices, </title> <journal> SIAM Rev., </journal> <volume> 29 (1987), </volume> <pages> pp. 575-596. </pages>
Reference-contexts: There have been many condition number estimators proposed in the literature, differing on their accuracy and on which matrix norm they are based, and a large body of computational experience exists <ref> [13] </ref>. In our implementation we chose the one of Van Loan [26], which is based on the Euclidean norm and usually yields good accuracy especially for well separated singular values. But one could also use the ones in LINPACK [6], LAPACK [1], or any in Higham's excellent survey [13]. * Deflate <p> experience exists <ref> [13] </ref>. In our implementation we chose the one of Van Loan [26], which is based on the Euclidean norm and usually yields good accuracy especially for well separated singular values. But one could also use the ones in LINPACK [6], LAPACK [1], or any in Higham's excellent survey [13]. * Deflate One: Deflate the ULV Decomposition by one (i.e., apply transformation and decrement the rank index by one so that the smallest singular value in the leading r fi r part of L is "moved" to the trailing rows).
Reference: [14] <author> R. E. </author> <title> Kalman, A new approach to linear filtering and prediction problems, </title> <journal> Journal of Basic Engineering, </journal> <year> (1960), </year> <pages> pp. 35-45. </pages>
Reference-contexts: 1 Introduction The discrete Kalman filter <ref> [14] </ref>, commonly used for prediction and detection of signals in communication and control problems, has more recently become a popular method of reducing uncertainty in robot navigation. One of the main advantages of using the filter is that it is recursive, eliminating the necessity for storing large amounts of data. <p> There are two basic problems which can occur when using either the Kalman or extended Kalman filter in robot navigation applications: * The filter was developed for applications such as those in signal processing in which many measurements are taken <ref> [14] </ref>. Sensing in robot navigation is often done using camera images.
Reference: [15] <author> A. Kosaka and A. C. Kak, </author> <title> Fast vision-guided mobile robot navigation using model- based reasoning and prediction of uncertainties, CVGIP: Image Understanding, </title> <booktitle> 56 (1992), </booktitle> <pages> pp. 271-329. </pages>
Reference-contexts: Also, due to the fact that functions are frequently non-linear, the extended Kalman filter (EKF) is used <ref> [2, 15] </ref>. The EKF formalism linearizes the function by taking a first order Taylor expansion around the current estimate of the state vector [9]. <p> Ten trials were run with each algorithm with lines of different slopes. 0, 1 and 5. In our second set of experiments, we simulated a simple robot navigation problem typical of that faced by an actual mobile robot <ref> [2, 15, 16, 17] </ref>. The robot has identified a single landmark in a two-dimensional environment and knows landmark location on a map. It does not know its own position. It moves in a straight line and with a known uniform velocity.
Reference: [16] <author> D. J. Kriegman, E. Trendl, and T. O. Binford, </author> <title> Stereo vision and navigation in buildings for mobile robots, </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 5 (1989), </volume> <pages> pp. 792-803. </pages>
Reference-contexts: Ten trials were run with each algorithm with lines of different slopes. 0, 1 and 5. In our second set of experiments, we simulated a simple robot navigation problem typical of that faced by an actual mobile robot <ref> [2, 15, 16, 17] </ref>. The robot has identified a single landmark in a two-dimensional environment and knows landmark location on a map. It does not know its own position. It moves in a straight line and with a known uniform velocity.
Reference: [17] <author> L. Matthies and S. A. Shafer, </author> <title> Error modeling in stereo navigation, </title> <journal> IEEE Journal of Robotics and Automation, </journal> <month> RA-3 </month> <year> (1987), </year> <pages> pp. 239-248. </pages>
Reference-contexts: Ten trials were run with each algorithm with lines of different slopes. 0, 1 and 5. In our second set of experiments, we simulated a simple robot navigation problem typical of that faced by an actual mobile robot <ref> [2, 15, 16, 17] </ref>. The robot has identified a single landmark in a two-dimensional environment and knows landmark location on a map. It does not know its own position. It moves in a straight line and with a known uniform velocity.
Reference: [18] <author> R. McKendall and M. Mintz, </author> <title> Sensor-fusion with statistical decision theory: A prospectus of research in the grasp lab, </title> <type> Tech. Rep. </type> <institution> MS-CIS-90-68, University of Pennsylvania, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: As the interesting recent work by Mintz et al <ref> [11, 18] </ref> in robust estimation and modeling of sensor noise has demonstrated, the criterion of optimality depends critically on the specific model being used. Given two methods, the first may produce optimality in one sense but not do as well as the second in another sense.
Reference: [19] <author> R. C. Smith and P. Cheeseman, </author> <title> On the representation and estimation of spatial uncertainty, </title> <journal> The International Journal of Robotics Research, </journal> <volume> 5 (1986), </volume> <pages> pp. 56-68. </pages>
Reference-contexts: The discrete Kalman filter is guaranteed to be optimal in that it is guaranteed to find the best solution in the least squares sense. Although originally designed as an estimator for dynamical systems, the filter is used in many applications as a static state estimator <ref> [19] </ref>.
Reference: [20] <author> H. W. Sorenson, </author> <title> Least-squares estimation: from gauss to kalman, </title> <journal> IEEE Spectrum, </journal> <year> (1970), </year> <pages> pp. 63-68. </pages>
Reference-contexts: In such cases, the Kalman filter can give poor results. Two additional problems occur when using the EKF: * The linearization process itself has the potential to introduce significant error into the problem. * The EKF is not guaranteed to be optimal or to even converge <ref> [20] </ref>. It can easily fall into a local minimum when an initial estimate of the solution is poor, often the type of situation faced by robot navigators.
Reference: [21] <author> G. W. Stewart, </author> <title> Updating a rank-revealing ULV decomposition, </title> <type> Technical Report CS-TR 2627, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1991. </year> <title> [22] , An updating algorithm for subspace tracking, </title> <journal> IEEE Trans. Signal Proc., </journal> <volume> 40 (1992), </volume> <pages> pp. 1535-1541. </pages>
Reference-contexts: In all the experiments we performed, the RTLS filter converged faster and to greater accuracy than did the Kalman filter. 2 The ULV Decomposition In this section, we describe the ULV Decomposition, first introduced by Stewart <ref> [21, 22] </ref>. This is a method which reveals the noise subspace (i.e., the subspace corresponding to the smaller singular values), and which is easily updated when new data arrives without making any a priori assumptions about the overall distribution of the singular values.
Reference: [23] <author> K. T. Sutherland and W. B. Thompson, </author> <title> Inexact navigation, </title> <booktitle> in Proceedings 1993 International Conference on Robotics and Automation, IEEE, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: It can easily fall into a local minimum when an initial estimate of the solution is poor, often the type of situation faced by robot navigators. Our work in outdoor navigation <ref> [23] </ref>, where measurements are expensive to obtain and have very significant error inherent to the system, motivated us to look for another filtering method, preferably one which would not require numerous measurements to converge and was not dependent on an error-free data matrix.
Reference: [24] <author> S. Van Huffel and J. Vandewalle, </author> <title> The Total Least Squares Problem Computational Aspects and Analysis, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: This second method is known in the statistical literature as orthogonal regression and in numerical analysis as total least squares (TLS) <ref> [24] </ref>. The TLS problem, in its simplest form, is to find a matrix E and vector f that minimizes k (E; f )k 2 such that (A + E)x = b + f for some vector x. <p> If v p is too small or zero, then the TLS solution may be too big or nonexistent, in which case an approximate solution of reasonable size can be obtained by using the next smallest singular values (s) <ref> [24] </ref>. The TLS approach has received a lot of attention in the numerical analysis literature, partly because it arises in so many applications (see e.g. [10, 24]). <p> The TLS approach has received a lot of attention in the numerical analysis literature, partly because it arises in so many applications (see e.g. <ref> [10, 24] </ref>). The most common algorithms to compute the TLS solution are based on the Singular Value Decomposition (SVD), a non-recursive matrix decomposition which is computationally expensive to update. Recently, some recursive TLS filters have been developed for applications in signal processing [4, 5, 7, 27]. <p> The algorithm presented here is derived from the SVD-based TLS algorithm (Algorithm 3.1 of <ref> [24] </ref>), but simplified for a single right hand side for the sake of clarity. Multiple right 9 hand sides can be handled in a similar fashion. The main computation cost of that algorithm occurs in the computation of the SVD. That cost is O (p 3 ) for each update. <p> The method depends on the use of the primitive operations Update and Deflate To Gap in a way very analogous to the use of the SVD in the standard algorithm in <ref> [24] </ref>. For our application of robot navigation, it sufficed to set both zero tolerances to zero and the Spread to 1.5. 4 Experimental Results In our first set of experiments, we compared performance of the Kalman, EKF, and RTLS using a very simple line fitting problem. <p> After trying several larger values of j, and comparing with the mixed LS-TLS method <ref> [10, 24] </ref>, a nonrecursive algorithm capable of treating certain columns as exact, we found it sufficed to use j = 100 for all experiments. The RTLS filter performs better as the lines become more vertical.
Reference: [25] <author> S. Van Huffel and H. Zha, </author> <title> An efficient total least squares algorithm based on a rank-revealing two-sided orthogonal decomposition, Numerical Algorithms, </title> <booktitle> 4 (1993), </booktitle> <pages> pp. 101-133. </pages>
Reference-contexts: The adaptation of the ULV to the TLS problem has also been analyzed independently in great detail in <ref> [25] </ref>, though the recursive updating process was not discussed at length. For our specific purposes, let A be an n fi (p 1) matrix and b be an n-vector, where p is fixed and n is growing as new measurements arrive.
Reference: [26] <author> C. F. Van Loan, </author> <title> On estimating the condition of eigenvalues and eigenvectors, </title> <journal> Lin. Alg. & Appl., </journal> <volume> 88/89 (1987), </volume> <pages> pp. 715-732. </pages>
Reference-contexts: There have been many condition number estimators proposed in the literature, differing on their accuracy and on which matrix norm they are based, and a large body of computational experience exists [13]. In our implementation we chose the one of Van Loan <ref> [26] </ref>, which is based on the Euclidean norm and usually yields good accuracy especially for well separated singular values.
Reference: [27] <author> K.-B. Yu, </author> <title> Recursive updating the eigenvalue decomposition of a covariance matrix, </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 39 (1991), </volume> <pages> pp. 1136-1145. </pages>
Reference-contexts: The most common algorithms to compute the TLS solution are based on the Singular Value Decomposition (SVD), a non-recursive matrix decomposition which is computationally expensive to update. Recently, some recursive TLS filters have been developed for applications in signal processing <ref> [4, 5, 7, 27] </ref>. Davila [4] used a Kalman filter to obtain a fast update for the eigenvector corresponding to the smallest eigenvalue of the covariance matrix. This eigenvector was then used to solve a symmetric TLS problem for the filter. <p> It was not explained how the algorithm might be modified for the case where the smallest eigenvalue is multiple (i.e., corresponding to a noise subspace of dimension higher than one), or variable (i.e., of unknown multiplicity). In <ref> [27] </ref>, Yu described a method for the fast update of an approximate eigendecomposition of a covariance matrix. <p> In [5, 7], DeGroat and Dowling used this approach combined with the averaging technique used in <ref> [27] </ref>, again assuming that the singular values could be grouped into two clusters. Recently, Bose et al.[3] applied Davila's algorithm to reconstruct images from noisy, undersampled frames after converting complex-valued image data into equivalent real data.
References-found: 26

