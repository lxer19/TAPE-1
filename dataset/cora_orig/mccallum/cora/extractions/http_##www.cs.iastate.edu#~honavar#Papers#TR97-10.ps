URL: http://www.cs.iastate.edu/~honavar/Papers/TR97-10.ps
Refering-URL: http://www.cs.iastate.edu/~cs572/weekly.html
Root-URL: 
Title: Power System Security Margin Prediction Using Radial Basis Function Networks  
Author: TR #- Guozhong Zhou, James D. McCalley and Vasant Honavar 
Keyword: I.2.6 [Artificial Intelligence] Learning connectionism and neural nets Keywords: power systems, security margin feature subset selection, genetic algorithms, radial basis function networks  
Address: 226 Atanasoff Hall  Ames, Iowa 50011-1040, USA  
Affiliation: Artificial Intelligence Research Group Department of Computer Science  Iowa Sate University  
Note: ACM Computing Classification System Categories (1991):  
Date: June 27, 1997  
Abstract: Dr. McCalley's research is partially supported through grants from National Science Foundation and Pacific Gas and Electric Company. Dr. Honavar's research is partially supported through grants from National Science Foundation and the John Deere Foundation. This paper will appear in: Proceedings of the 29th Annual North American Power Symposium, Oct. 13-14. 1997, Laramie, Wyoming. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Haykin, </author> <title> Neural Networks, </title> <address> New York: </address> <publisher> Macmillan College Publishing Company, </publisher> <year> 1994. </year>
Reference-contexts: Park and Sandberg [7] have shown that RBF networks with Gaussian basis functions are universal function approxi-mators. Girosi and Poggio [8] have shown pointwise convergence property of a class of RBF networks. 7 3 Radial Basis Function Network Training Strategies There are different training strategies for RBF networks <ref> [1] </ref>. Since the linear weights associated with the output node tend to evolve more slowly than the nonlinear activation functions of the hidden nodes, it is reasonable to train the network layer by layer to get fast training speed. <p> A natural candidate for such a process is error correction iteration algorithm using gradient-descent technique. The squared error for the output can be written as E = 2 k=1 The update equations for the linear weights, centers, and widths are given as follows <ref> [1] </ref> w i (t + 1) = w i (t) + 1 k=1 kx k c i k 2 i (6) 2 i k=1 2 2 (x kj c ij (t)) (7) i (t + 1) = 2 N X (d k y k )w i z ik kx k c
Reference: [2] <author> D. S. Broomhead, D. Lowe, </author> <title> "Multivariable functional interpolation and adaptive networks," </title> <journal> Complex Systems, </journal> <volume> vol. 2, </volume> <year> 1988, </year> <pages> pp. 321-355. </pages>
Reference-contexts: Generalization is then equivalent to using this multidimensional surface to interpolate the test data. Such a viewpoint is indeed the motivation behind the method of radial basis functions (RBFs). Broomhead and Lowe <ref> [2] </ref> first explored the use of RBFs in neural networks. Moody and Darken [3], Re-nals and Rohwer [4], and Poggio and Girosi [5] among others made major contributions to the theory, design, and application of RBF networks. The basic architecture of a RBF network is shown in Fig. 3. <p> Usually the number of training patterns is much larger than the number of selected centers, so the resulting linear equations are overdetermined. A straightforward procedure for solving such equations is to use the pseudoinverse method <ref> [2] </ref> to obtain a solution with the minimum least square error. The linear weights can also be solved by iteration using gradient-descent technique. This approach is much faster than the backpropagation (BP) algorithm for MLPs because it adjusts only the weights between the hidden and output layer.
Reference: [3] <author> J. Moody, C. J. Darken, </author> <title> "Fast learning in networks of locally tuned processing units," </title> <journal> Neural Computation, </journal> <volume> vol. 1, </volume> <year> 1989, </year> <pages> pp. 281-294. </pages>
Reference-contexts: Generalization is then equivalent to using this multidimensional surface to interpolate the test data. Such a viewpoint is indeed the motivation behind the method of radial basis functions (RBFs). Broomhead and Lowe [2] first explored the use of RBFs in neural networks. Moody and Darken <ref> [3] </ref>, Re-nals and Rohwer [4], and Poggio and Girosi [5] among others made major contributions to the theory, design, and application of RBF networks. The basic architecture of a RBF network is shown in Fig. 3. It includes three entirely different layers. <p> For example, we may use k-means clustering algorithm <ref> [3] </ref>, hierarchical cluster analysis, or self-organizing map. After we determine the centers, we may obtain the linear weights either by directly solving the linear equations or by iteration. 3.3 Supervised selection of centers This is the most flexible but most time-consuming training approach among the three strategies.
Reference: [4] <author> S. Renals, </author> <title> "Radial basis function network for speech pattern classification," </title> <journal> Electronics Letters, </journal> <volume> vol. 25, </volume> <year> 1989, </year> <pages> pp. 437-439. </pages>
Reference-contexts: Generalization is then equivalent to using this multidimensional surface to interpolate the test data. Such a viewpoint is indeed the motivation behind the method of radial basis functions (RBFs). Broomhead and Lowe [2] first explored the use of RBFs in neural networks. Moody and Darken [3], Re-nals and Rohwer <ref> [4] </ref>, and Poggio and Girosi [5] among others made major contributions to the theory, design, and application of RBF networks. The basic architecture of a RBF network is shown in Fig. 3. It includes three entirely different layers.
Reference: [5] <author> T. Poggio and F. Girosi, </author> <title> "Networks for approximation and learning," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 78, </volume> <year> 1990, </year> <pages> pp. 1481-1497. </pages>
Reference-contexts: Such a viewpoint is indeed the motivation behind the method of radial basis functions (RBFs). Broomhead and Lowe [2] first explored the use of RBFs in neural networks. Moody and Darken [3], Re-nals and Rohwer [4], and Poggio and Girosi <ref> [5] </ref> among others made major contributions to the theory, design, and application of RBF networks. The basic architecture of a RBF network is shown in Fig. 3. It includes three entirely different layers.
Reference: [6] <author> D. Lowe, </author> <title> "Adaptive radial basis function nonlinearities, and the problem of generalization," </title> <booktitle> 1st IEE International Conference on Artificial Neural Networks, </booktitle> <address> London, UK, </address> <year> 1989, </year> <pages> pp. 171-175. 17 </pages>
Reference-contexts: This is reasonable provided that the training data are well representative of the problem <ref> [6] </ref>. The widths for all RBFs are also fixed and are the same. This width can be taken as the standard deviation of the Gaussian function, expressed as d 2M where d is the maximum distance between the selected centers.
Reference: [7] <author> J. Park and I. W. Sandberg, </author> <title> "Universal approximation using radial basis function networks," </title> <journal> Neural Computation, </journal> <volume> vol. 3, </volume> <year> 1991, </year> <pages> pp. 246-257. </pages>
Reference-contexts: Park and Sandberg <ref> [7] </ref> have shown that RBF networks with Gaussian basis functions are universal function approxi-mators. Girosi and Poggio [8] have shown pointwise convergence property of a class of RBF networks. 7 3 Radial Basis Function Network Training Strategies There are different training strategies for RBF networks [1].
Reference: [8] <author> F. Girosi and T. Poggio, </author> <title> "Networks and the best approximation property," </title> <journal> Biological Cybernetics, </journal> <volume> vol. 63, </volume> <year> 1990, </year> <pages> pp. 169-176. </pages>
Reference-contexts: Park and Sandberg [7] have shown that RBF networks with Gaussian basis functions are universal function approxi-mators. Girosi and Poggio <ref> [8] </ref> have shown pointwise convergence property of a class of RBF networks. 7 3 Radial Basis Function Network Training Strategies There are different training strategies for RBF networks [1].
Reference: [9] <author> M. T. Musavi, W. Ahmed, K. H. Chan, K. B. Faris, and D. M. Hummels, </author> <title> "On the training of radial basis function classifiers," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 5, </volume> <year> 1992, </year> <pages> pp. 595-603. </pages>
Reference: [10] <author> N. R. Draper and H. Smith, </author> <title> Applied Regression Analysis, 2nd ed., </title> <address> New York: </address> <publisher> John Wiley & Sons, </publisher> <year> 1981. </year>
Reference: [11] <author> A. J. Miller, </author> <title> Subset Selection in Regression, </title> <address> New York: </address> <publisher> Chapman and Hall, </publisher> <year> 1990. </year>
Reference: [12] <author> J. Neter, W. Wasserman, and M. H. Kutner, </author> <title> Applied Linear Statistical Models: Regression, Analysis of Variance, and Experimental Designs, 3rd ed., </title> <type> Homewood, </type> <institution> IL: Irwin, </institution> <year> 1990. </year>
Reference: [13] <author> L. Chambers, ed., </author> <title> Practical Handbook of Genetic Algorithms: Applications, Volume I, </title> <address> Boca Raton, FL: </address> <publisher> CRC Press, </publisher> <year> 1995. </year>
Reference: [14] <author> P. A. Devijver and J. Kittler, </author> <title> Pattern Recognition: A Statistical Approach, </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference: [15] <author> H. Vafaie and K. D. Jong, </author> <title> "Genetic algorithms as a tool for feature selection in machine learning," </title> <booktitle> Fourth International Conference on Tools with Artificial Intelligence, </booktitle> <address> Arlington, VA, </address> <month> November </month> <year> 1992, </year> <pages> pp. 200-203. </pages>
Reference: [16] <author> J. Yang and V. Honavar, </author> <title> "Feature subset selection using a genetic algorithm," </title> <booktitle> to appear in Proceedings of International Conference on Genetic Programming, </booktitle> <address> Stanford, CA, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: Many of these methods are based only on the ability to predict the output. There are no efficient way to account for cardinality constraints on the feature set although there are several techniques that have attempted to find minimum feature subsets (see <ref> [16] </ref> for a discussion). Furthermore, there is no capability to preselect some parameters into the solution. Some of the models select the feature sets that satisfy the sufficiency condition, but no mechanism has been proposed to ensure that the chosen subset will satisfy the cardinality and the controllability conditions.
Reference: [17] <author> V. Van Acker, S. Wang, J. D. McCalley, G. Zhou, and M. Mitchell, </author> <title> "Data generation using automated security assessment for neural network training," </title> <booktitle> to appear in Proceedings of the 29th North American Power Symposium, </booktitle> <institution> Laramie, WY, </institution> <month> October </month> <year> 1997. </year>
Reference-contexts: If the data do not accurately reflect the actual behaviors of the power system operations, we can not expect the neural network to predict the margin correctly. Automatic security assessment software (ASAS) <ref> [17, 18] </ref> is used to generate the data for the use of the RBF network. The resulting data set consists of 1005 patterns, with each pattern corresponding to a simulation of tie line 3 outage (see Fig. 2) under various operating conditions.

References-found: 17

