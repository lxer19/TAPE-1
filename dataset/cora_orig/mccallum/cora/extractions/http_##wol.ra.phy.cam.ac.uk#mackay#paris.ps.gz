URL: http://wol.ra.phy.cam.ac.uk/mackay/paris.ps.gz
Refering-URL: http://131.111.48.24/mackay/README.html
Root-URL: 
Email: mackay@mrao.cam.ac.uk  
Title: Probabilistic Networks: New Models and New Methods  
Author: David J.C. MacKay 
Note: To appear in Proceedings of ICANN'95  
Address: Madingley Road, Cambridge CB3 0HE United Kingdom  
Affiliation: Cavendish Laboratory  
Abstract: In this paper I describe the implementation of a probabilistic regression model in BUGS. BUGS is a program that carries out Bayesian inference on statistical problems using a simulation technique known as Gibbs sampling. It is possible to implement surprisingly complex regression models in this environment. I demonstrate the simultaneous inference of an interpolant and an input-dependent noise level. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Freedman, W. L., Madore, B. F., Mould, J. R., Hill, R., and others. </author> <title> (1994) Distance to the Virgo cluster galaxy M100 from Hubble space telescope observations of cepheids. </title> <booktitle> Nature 371: </booktitle> <pages> 757-762. </pages>
Reference-contexts: We have fifty labelled points from each population. I make the assumption that we are particularly interested in inferring the offset between the populations. This task is motivated by the problem of inferring the difference between the distances to two populations of Cepheid stars <ref> (Freedman et al. 1994) </ref>. The underlying function is described by a linear combination of basis functions y (x; w) = P h=1 h (x). The basis functions are Legendre polynomials. In neural network terms, this is a two-layer network with a fixed non-linear hidden layer and adaptive linear output connections.
Reference: <author> Hinton, G. E., and van Camp, D., </author> <title> (1993) Keeping neural networks simple by minimizing the description length of the weights. </title> <booktitle> In: Proceedings of COLT-93. </booktitle>
Reference: <author> MacKay, D. J. C., </author> <title> (1991) Bayesian Methods for Adaptive Models. </title> <institution> California Institute of Technology dissertation. </institution>
Reference: <author> MacKay, D. J. C. </author> <title> (1992a) Bayesian interpolation. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 415-447. </pages>
Reference-contexts: In the Bayesian interpretation (reviewed in <ref> (MacKay 1992a) </ref>), this optimization maps onto a probabilistic model H where the function fiE D is the log of a likelihood function, P (ft n gjw; fi; H) = exp (fiE D )=Z D (fi), and ffE W is the log of a prior distribution on the parameters, P (wjff; H) <p> An important problem in regression modelling is to control the relative strength of these two hyperparameters. If ff=fi is too large then the model cannot fit the data well, but if the ratio is too small then overfitting occurs. As reviewed in <ref> (MacKay 1992a) </ref>, we can extend this probabilistic model by including a prior distribution over the hyperparameters P (ff; fijH) that expresses our lack of knowledge about the noise variance and the variance of the parameters w, and then we can use Bayesian methods to infer the most plausible values for the <p> (MacKay 1994; MacKay and Takeuchi 1995). (2) One obtains quantified error bars on model parameters and predictions. (3) One obtains a measure of the effective number of well-determined parameters in a model. (4) Objective model comparison is possible, comparing regression models with different basis functions, for example, or different regularizers <ref> (MacKay 1992a) </ref>. (5) Generalizations to better probabilistic models are easy to formulate. Modelling an input-dependent noise level One improvement that we might wish to make is a modification of the noise model, which, in the above model, assumes that all the residuals are independent and Gaussian with identical variance.
Reference: <author> MacKay, D. J. C. </author> <title> (1992b) A practical Bayesian framework for backpropagation networks. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 448-472. </pages>
Reference-contexts: There are three principal approaches to implementing Bayesian inference for this sort of problem: approximations based on Gaussians, as for example in <ref> (MacKay 1992b) </ref>; an `ensemble learning' approach, in which an approximating distribution is optimized by variational free energy minimization, introduced by Hinton and van Camp (1993); and Markov chain Monte Carlo techniques, as reviewed and developed by Neal (1993).
Reference: <author> MacKay, D. J. C. </author> <title> (1994) Bayesian non-linear modelling for the prediction competition. </title> <journal> In ASHRAE Transactions, </journal> <volume> V.100, </volume> <pages> Pt.2 , pp. 1053-1062, </pages> <address> Atlanta Georgia. ASHRAE. </address>
Reference: <author> MacKay, D. J. C., and Takeuchi, R. </author> <title> (1995) Interpolation models with multiple hyperpa-rameters. In Maximum Entropy and Bayesian Methods, Cambridge 1994 , ed. </title> <editor> by J. Skilling and S. Sibisi, </editor> <publisher> Dordrecht. Kluwer. </publisher>
Reference: <author> Neal, R. M. </author> <title> (1993) Probabilistic inference using Markov chain Monte Carlo methods. </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Dept. of Computer Science, University of Toronto. </institution>
Reference: <author> Thomas, A., Spiegelhalter, D. J., and Gilks, W. R. </author> <title> (1992) BUGS: A program to perform Bayesian inference using Gibbs sampling. In Bayesian Statistics 4 , ed. </title> <editor> by J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. </editor> <volume> Smith, </volume> <pages> pp. 837-842. </pages> <publisher> Oxford: Clarendon Press. </publisher>
Reference-contexts: However it is a relatively parameter-free method and so is attractive as an implementation strategy. 3 BUGS A new tool, BUGS, makes simple the implementation of complex Bayesian models by Gibbs sampling. BUGS <ref> (Thomas et al. 1992) </ref> is copyright by the MRC Biostatistics Unit, Robinson Way, Cambridge CB2 2SR, and is available by ftp from ftp.mrc-bsu.cam.ac.uk.
References-found: 9

