URL: ftp://ftp.cs.indiana.edu/pub/gasser/yom.ps
Refering-URL: http://www.cs.indiana.edu/ai/Gasser/Morphophon/home.html
Root-URL: http://www.cs.indiana.edu
Email: gasser@indiana.edu  
Title: Transfer in a Connectionist Model of the Acquisition of Morphology  
Author: Michael Gasser 
Note: I thank Harald Baayen and Robert Schreuder for helpful comments on an earlier draft.  
Affiliation: Computer Science and Linguistics Departments, Cognitive Science Program Indiana University  
Abstract: The morphological systems of natural languages are replete with examples of the same devices used for multiple purposes: (1) the same type of morphological process (for example, suffixation for both noun case and verb tense) and (2) identical morphemes (for example, the same suffix for English noun plural and possessive). These sorts of similarity would be expected to convey advantages on language learners in the form of transfer from one morphological category to another. Connectionist models of morphology acquisition have been faulted for their supposed inability to represent phonological similarity across morphological categories and hence to facilitate transfer. This paper describes a connectionist model of the acquisition of morphology which is shown to exhibit transfer of this type. The model treats the morphology acquisition problem as one of learning to map forms onto meanings and vice versa. As the network learns these mappings, it makes phonological generalizations which are embedded in connection weights. Since these weights are shared by different morphological categories, transfer is enabled. In a set of simulations with artificial stimuli, networks were trained first on one morphological task (e.g., tense) and then on a second (e.g., number). It is shown that in the context of suffixation, prefixation, and template rules, the second task is facilitated when the second category either makes use of the same forms or the same general process type (e.g., prefixation) as the first. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Amit, D. J. </author> <year> (1989). </year> <title> Modeling Brain Function: The World of Attractor Neural Networks. </title> <publisher> Cambridge University Press, Combridge. </publisher>
Reference-contexts: Each pattern of activation is a point in a multi-dimensional space, one dimension for each unit in the network or subnetwork under consideration. In networks whose activation patterns evolve over time, such as attractor net-works <ref> (Amit, 1989) </ref> or recurrent sequential networks like MCNAM, one can examine the temporal behavior of the system, looking for attractors, regions in the representational space which the network tends to fall into in response to classes of inputs. Consider the behavior of the perception component.
Reference: <author> Cottrell, G. W. & Plunkett, K. </author> <year> (1991). </year> <title> Learning the past tense in a recurrent network: acquiring the mapping from meaning to sounds. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <volume> 13, </volume> <pages> 328-333. </pages>
Reference: <author> Cutler, A. </author> <year> (1990). </year> <title> Exploiting prosodic probabilities in speech segmenta-tion. </title> <editor> In Altmann, G. T. M. (Ed.), </editor> <booktitle> Cognitive Models of Speech Processing: Psycholinguistic and Computational Perspectives, </booktitle> <pages> pp. 105-121. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: is obviously not always the case, word boundaries should be relatively salient in child-directed speech, and by the time they are learning morphology, children seem to have already learned a great deal about the specific prosodic structure of their language, which in turn may provide strong clues about word boundaries <ref> (Cutler, 1990) </ref>. 5 Reduplication and metathesis are not accommodated by the simple segment-based model; these would require a hierarchical version of the network which has not yet been implemented (Gasser, 1994a). 6 Somewhat more realistically, it would also be possible to train perception and production simultaneously.
Reference: <author> Daugherty, K. & Seidenberg, M. </author> <year> (1992). </year> <title> Rules or connections? the past tense revisited. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <volume> 14, </volume> <pages> 259-264. </pages>
Reference: <author> Elman, J. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14, </volume> <pages> 179-211. </pages>
Reference: <author> Gasser, M. </author> <year> (1992). </year> <title> Learning distributed syllable representations. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <volume> 14, </volume> <pages> 396-401. </pages>
Reference-contexts: In the perception module, phonology is learned as a side-effect as the system is trained to recognize words. Phonological knowledge takes the form of the weights on the connections from the input (phonetic) layer of processing units to the recurrent hidden layer of units. I have shown elsewhere <ref> (Gasser, 1992) </ref> that the patterns of activation appearing on this hidden layer embody generalizations about the phonological structure found in the input forms and can provide a basis for learning in the production module of the system.
Reference: <author> Gasser, M. </author> <year> (1994a). </year> <title> Acquiring receptive morphology: a connectionist model. </title> <booktitle> Annual Meeting of the Association for Computational Linguistics, </booktitle> <volume> 32, </volume> <pages> 279-286. </pages>
Reference-contexts: For each morpheme, the network's response is taken to be the morpheme which its output is closest to. Performance is evaluated separately for each morphological category, that is, for the root and each inflection in a word. I have demonstrated elsewhere <ref> (Gasser, 1994a) </ref> that the perception component has the capacity to learn prefixation, suffixation, circumfixation, infixation, deletion, mutation, and template rules. 5 I have also shown that performance is always superior with a version of the model in which root and inflection recognition are handled by separate hidden-layer modules (Gasser, 1994b). <p> a great deal about the specific prosodic structure of their language, which in turn may provide strong clues about word boundaries (Cutler, 1990). 5 Reduplication and metathesis are not accommodated by the simple segment-based model; these would require a hierarchical version of the network which has not yet been implemented <ref> (Gasser, 1994a) </ref>. 6 Somewhat more realistically, it would also be possible to train perception and production simultaneously. In this case phone production would be based on changing syllable representations as the weights from the phone to hidden layers of the perception component are modified.
Reference: <author> Gasser, M. </author> <year> (1994b). </year> <title> Modularity in a connectionist model of morphology acquisition. </title> <booktitle> Proceedings of the International Conference on Computational Linguistics, </booktitle> <volume> 15, </volume> <pages> 214-220. </pages>
Reference-contexts: demonstrated elsewhere (Gasser, 1994a) that the perception component has the capacity to learn prefixation, suffixation, circumfixation, infixation, deletion, mutation, and template rules. 5 I have also shown that performance is always superior with a version of the model in which root and inflection recognition are handled by separate hidden-layer modules <ref> (Gasser, 1994b) </ref>. In the modular version, shown in Figure 2 the input (phone) layer is connected to both hidden-layer groups of units.
Reference: <author> Hare, M. & Elman, J. L. </author> <year> (1995). </year> <title> Learning and morphological change. </title> <journal> Cognition, </journal> <volume> 56, </volume> <pages> 61-98. </pages>
Reference: <author> MacWhinney, B. & Leinbach, J. </author> <year> (1991). </year> <title> Implementations are not conceptualization: revising the verb learning model. </title> <journal> Cognition, </journal> <volume> 40, </volume> <pages> 121-157. </pages>
Reference: <author> Pinker, S. & Prince, A. </author> <year> (1988). </year> <title> On language and connectionism: analysis of a parallel distributed processing model of language acquisition. </title> <journal> Cognition, </journal> <volume> 28, </volume> <pages> 73-193. </pages>
Reference-contexts: the Rumelhart and McClelland model of the acquisition of the English past tense (Rumelhart & McClel--land, 1986), Pinker and Prince fault the model on these grounds, for what they call "morphological localism": the English past tense forms are learned in a network which is dedicated to this morphological task alone <ref> (Pinker & Prince, 1988) </ref>. In the Rumelhart and McClelland model, as in most of the succeeding connectionist models of morphology acquisition (Daugherty & Seidenberg, 1992; MacWhinney & Leinbach, 1991; Plunkett & Marchman, 1991), morphology learning consists in learning to map a stem onto an affixed form.
Reference: <author> Plunkett, K. & Marchman, V. </author> <year> (1991). </year> <title> U-shaped learning and frequency effects in a multi-layered perceptron: implications for child language acquisition. </title> <journal> Cognition, </journal> <volume> 38, </volume> <pages> 1-60. </pages>
Reference: <author> Rumelhart, D. E. & McClelland, J. L. </author> <year> (1986). </year> <title> On learning the past tense of English verbs. </title> <editor> In McClelland, J. L. & Rumelhart, D. E. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Volume 2, </volume> <pages> pp. 216-271. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: If there is complete modularity between the parts of the system dedicated to the different tasks, then no generalization is possible. In their critique of the Rumelhart and McClelland model of the acquisition of the English past tense <ref> (Rumelhart & McClel--land, 1986) </ref>, Pinker and Prince fault the model on these grounds, for what they call "morphological localism": the English past tense forms are learned in a network which is dedicated to this morphological task alone (Pinker & Prince, 1988). <p> For each input phone the hidden layer and output layer of units are activated in turn. The network's output is compared to the target pattern, an error is calculated, and the network's weights are adjusted accordingly with the familiar back-propagation learning algorithm <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>. For purposes of evaluating the performance of the perception module of the network, the output of the module is examined following the presentation of the word-final boundary pattern.
Reference: <author> Rumelhart, D. E., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Volume 1, </volume> <pages> pp. 318-364. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: If there is complete modularity between the parts of the system dedicated to the different tasks, then no generalization is possible. In their critique of the Rumelhart and McClelland model of the acquisition of the English past tense <ref> (Rumelhart & McClel--land, 1986) </ref>, Pinker and Prince fault the model on these grounds, for what they call "morphological localism": the English past tense forms are learned in a network which is dedicated to this morphological task alone (Pinker & Prince, 1988). <p> For each input phone the hidden layer and output layer of units are activated in turn. The network's output is compared to the target pattern, an error is calculated, and the network's weights are adjusted accordingly with the familiar back-propagation learning algorithm <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>. For purposes of evaluating the performance of the perception module of the network, the output of the module is examined following the presentation of the word-final boundary pattern.
Reference: <author> Schreuder, R. & Baayen, R. H. </author> <year> (1995). </year> <title> Modeling morphological processing. </title>
Reference-contexts: While the focus here has obviously been on the formal end of the acquisition of morphology, it is clear that factors such as phonological, semantic, and conceptual transparency interact in acquisition <ref> (Schreuder & Baayen, 1995) </ref>, and in future versions of the model it may be impossible to ignore the acquisition of semantics itself. Another potential concern is the size of the lexicon used in the simulations.
Reference: <editor> In Feldman, L. B. (Ed.), </editor> <booktitle> Morphological Aspects of Language Processing, </booktitle> <pages> pp. 131-154. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
References-found: 16

