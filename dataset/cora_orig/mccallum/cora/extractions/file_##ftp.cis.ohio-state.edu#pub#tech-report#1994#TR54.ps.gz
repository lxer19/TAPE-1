URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1994/TR54.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Email: email: framach,singhalg@cis.ohio-state.edu  
Title: On the Synchronization Mechanisms in Distributed Shared Memory Systems  
Author: Mahendra Ramachandran and Mukesh Singhal 
Keyword: Key phrases: Distributed Shared Memory, Distributed Memory Architectures, Operating Systems, Synchronization.  
Address: Columbus, Ohio 43210-1277  
Affiliation: Department of Computer and Information Science The Ohio State University,  
Abstract: Distributed Shared Memory (DSM) is the implementation of the shared memory programming paradigm on a distributed memory (or multicomputer) system. Programming multicomputer systems using Distributed Shared Memory as the programming model is appealing because it combines the performance advantage of distributed memory systems and the ease of programming of shared memory systems. In DSM systems, cooperating tasks communicate with one another through shared variables. Thus, DSM systems must provide synchronization mechanisms to Coordinate concurrent access to these shared variables. In this paper we describe and classify the synchronization mechanisms supported by several Distributed Shared Memory systems. We classify these systems according to whether they are hardware or software based, whether the mechanism is integrated into the system or not and whether implementation of the synchronization mechanism is centralized or distributed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Bennett, S. Dwarkadas, J. Greenwood, and E. Speight. </author> <title> "Willow: </title> <booktitle> A Scalable Shared Memory Multiprocessor ". In Proceedings of Supercomputing '92, </booktitle> <pages> pages 336-345, </pages> <month> Nov </month> <year> 1992. </year>
Reference-contexts: Hardware versus Software A DSM system can be implemented either in hardware or software. Early DSMs were software based and were implemented on multicomputers whose architecture provided no support for a shared memory environment [19, 23, 27, 3]. Increasingly, support for DSM is being provided in hardware for efficiency <ref> [2, 11, 12, 1, 17] </ref>. This has enabled the development of distributed memory computers with hardware support for shared memory. Software implemented systems are designed to provide the DSM on existing distributed memory systems. <p> A variety of architectures exist. In some systems, local memory of a processor is used as its cache and the local memories of the rest of the system are viewed as the global memory for that processor <ref> [11, 1, 6] </ref>. Other systems support a hierarchical view of memory similar to the cache-main-secondary memory hierarchy in traditional operating systems. <p> This helps 16 reduce the effect of the synchronization delay on the performance of the program. Since the synchronization operations are performed at the node where the synchronization variable is located, this mechanism uses a static distributed scheme. Willow The architecture of Willow is tree-based hierarchical multiprocessor <ref> [1] </ref>. All intermediate nodes are memory modules and each leaf node of the tree is a processor module. Both types of modules (memory and processor) have caches associated with them. The connection between a node and its children is through a shared bus.
Reference: [2] <author> R. Bisiani and M. Ravishankar. </author> <title> "Plus: A Distributed Shared-Memory System". </title> <booktitle> In International Symp. on Computer Architecture, </booktitle> <pages> pages 115-124, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Hardware versus Software A DSM system can be implemented either in hardware or software. Early DSMs were software based and were implemented on multicomputers whose architecture provided no support for a shared memory environment [19, 23, 27, 3]. Increasingly, support for DSM is being provided in hardware for efficiency <ref> [2, 11, 12, 1, 17] </ref>. This has enabled the development of distributed memory computers with hardware support for shared memory. Software implemented systems are designed to provide the DSM on existing distributed memory systems. <p> We next describe a few representative systems. Plus The Plus system, a hardware implementation of a DSM, is composed of a group of Plus nodes, connected via a "fast interconnection network" <ref> [2] </ref>. Each Plus node consists of a processor with its cache, memory (for local and replicated global data), and a Memory Coherence Manager (MCM) connected by a bus. Shared data in Plus are grouped into pages, which are replicated on the nodes where they are required.
Reference: [3] <author> J. Carter, J. Bennett, and W. Zwaenepoel. </author> <title> "Implementation and Performance of Munin". </title> <booktitle> In ACM Symp. on Operating System Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: We next discuss these criteria in detail. Hardware versus Software A DSM system can be implemented either in hardware or software. Early DSMs were software based and were implemented on multicomputers whose architecture provided no support for a shared memory environment <ref> [19, 23, 27, 3] </ref>. Increasingly, support for DSM is being provided in hardware for efficiency [2, 11, 12, 1, 17]. This has enabled the development of distributed memory computers with hardware support for shared memory. Software implemented systems are designed to provide the DSM on existing distributed memory systems. <p> In order to do so, Munin defines nine shared-data types, each with it own coherence protocol <ref> [3] </ref>. Munin supports an integrated synchronization mechanism; one of the shared-data types is the synchronization type. This data type is implemented as a distributed lock. Aside from locks, centralized implementations of barriers and conditional variables are also provided in Munin [4].
Reference: [4] <author> John Carter. </author> <title> "Efficient Distributed Shared Memory Based On Multi-Protocol Release Consistency". </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> Sep </month> <year> 1993. </year> <month> 20 </month>
Reference-contexts: Munin and TreadMarks The Munin system, developed at Rice University, has been implemented on a network of workstations interconnected via Ethernet <ref> [4] </ref>. The system consists of a runtime system which interacts with a modified version of the V kernel operating system [5]. <p> Munin supports an integrated synchronization mechanism; one of the shared-data types is the synchronization type. This data type is implemented as a distributed lock. Aside from locks, centralized implementations of barriers and conditional variables are also provided in Munin <ref> [4] </ref>. A condition variable makes it possible to implement synchronization based on Monitors [18]. Processes block by executing a wait primitive on Condition variable. 12 When a process executes the signal primitive, one of the blocked processes is unblocked and allowed to proceed.
Reference: [5] <author> David R. Cheriton and Willy Zwaenepoel. </author> <title> Distributed Process Groups in the V Kernel. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(2) </volume> <pages> 77-107, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: Munin and TreadMarks The Munin system, developed at Rice University, has been implemented on a network of workstations interconnected via Ethernet [4]. The system consists of a runtime system which interacts with a modified version of the V kernel operating system <ref> [5] </ref>. Unlike IVY and Clouds, Munin's design philosophy is that different shared data require varying degrees of consistency and the performance can be improved by using a spectrum of consistency maintenance protocols for these shared variables.
Reference: [6] <institution> Convex Pres, Richardson, TX. "Convex Exemplar Architecture", </institution> <month> Nov </month> <year> 1993. </year>
Reference-contexts: A variety of architectures exist. In some systems, local memory of a processor is used as its cache and the local memories of the rest of the system are viewed as the global memory for that processor <ref> [11, 1, 6] </ref>. Other systems support a hierarchical view of memory similar to the cache-main-secondary memory hierarchy in traditional operating systems. <p> This scheme is a distributed one which has been integrated into the hardware of the system. 17 Convex Exemplar The Exemplar architecture, developed by Convex Corp. <ref> [6] </ref>, is a hierarchical distributed memory architecture. It is composed of clusters called hypernodes. Each hypernode has eight processors and shared memory connected via a cross-bar. Each processor has a private portion of that shared memory for its own exclusive use as well.
Reference: [7] <author> A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> "Software Versus Hardware Shared-memory Implementation: A Case Study". </title> <booktitle> In 21st Annual Int. Symp. on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: In this manner, a distributed queue is created on the network with every waiting processor queuing the processor that is to receive the lock after it. TreadMarks, also developed at Rice University, supports locks and barriers for synchronization in a manner similar to Munin <ref> [7] </ref>. Unlike Munin, however, this system uses a single memory consistency model called lazy release consistency [7], which allows differing the propagation of data updates (or invalidations) until processes synchronize. In order to facilitate this delayed propagation, the lock request must be timestamped by the sending processor. <p> TreadMarks, also developed at Rice University, supports locks and barriers for synchronization in a manner similar to Munin <ref> [7] </ref>. Unlike Munin, however, this system uses a single memory consistency model called lazy release consistency [7], which allows differing the propagation of data updates (or invalidations) until processes synchronize. In order to facilitate this delayed propagation, the lock request must be timestamped by the sending processor.
Reference: [8] <author> G. Delp, D. Farber, R. Minnich, J. Smith, and I. Tam. </author> <title> "Memory as a Network Abstraction". </title> <editor> In T. Cassavant and M. Singhal, editors, </editor> <booktitle> Readings in Distributed Computing Systems, </booktitle> <pages> pages 409-423. </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference-contexts: For instance, MemNet was composed of processors connected to a token-ring by an interface device called a MemNet device <ref> [8] </ref>. This device was responsible for performing remote access requests on behalf of its processor as well as servicing the remote requests of other processors in the system. More recent implementations encompass processor networks where the nodes themselves are clusters or multiprocessors.
Reference: [9] <author> E. W. Dijkstra. </author> <title> "Cooperating Sequential Processes". </title> <editor> In F. Genuys, editor, </editor> <booktitle> Programming Languages, </booktitle> <pages> pages 43-112. </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1968. </year>
Reference-contexts: We then present a taxonomy of synchronization mechanisms in the DSM systems. 3 2.1 Basic Synchronization Mechanisms Semaphores The concept of semaphores was proposed by Dijkstra to solve a variety of synchronization problems, namely, mutual exclusion, K-ary mutual exclusion, and process synchronization in the uniprocessor system <ref> [9] </ref>. A semaphore is a synchronization variable which is initialized to one for a binary semaphore or greater than one for a resource counting semaphore.
Reference: [10] <author> M Dubois, C Scheurich, and F. Briggs. </author> <title> "Synchronization, Coherence and Event Ordering in Multiprocessors". </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 9-21, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: While this replication allows concurrent local access to the pages by the processes, the copies become outdated when one of the pages is modified by its processor; this problem is similar to the cache-coherence problem in multiprocessors <ref> [10, 29] </ref>. Maintaining the coherence of shared data in DSM systems has drawn a lot of attention [24].
Reference: [11] <author> D. Lenoski et. al. </author> <title> "The Stanford Dash Multiprocessor". </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Hardware versus Software A DSM system can be implemented either in hardware or software. Early DSMs were software based and were implemented on multicomputers whose architecture provided no support for a shared memory environment [19, 23, 27, 3]. Increasingly, support for DSM is being provided in hardware for efficiency <ref> [2, 11, 12, 1, 17] </ref>. This has enabled the development of distributed memory computers with hardware support for shared memory. Software implemented systems are designed to provide the DSM on existing distributed memory systems. <p> A variety of architectures exist. In some systems, local memory of a processor is used as its cache and the local memories of the rest of the system are viewed as the global memory for that processor <ref> [11, 1, 6] </ref>. Other systems support a hierarchical view of memory similar to the cache-main-secondary memory hierarchy in traditional operating systems. <p> More recent implementations encompass processor networks where the nodes themselves are clusters or multiprocessors. The DASH system, for instance, is a two dimensional mesh where the nodes are Silicon Graphics 4D/240 bus-based shared memory multiprocessors <ref> [11] </ref>. The network consists of a pair of meshes, one for sending remote requests and the other to receive replies. DASH uses a directory-based scheme to maintain the coherence of shared data.
Reference: [12] <author> J. Kuskin et. al. </author> <title> "The Stanford FLASH Multiprocessor". </title> <booktitle> In International Symp. on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Hardware versus Software A DSM system can be implemented either in hardware or software. Early DSMs were software based and were implemented on multicomputers whose architecture provided no support for a shared memory environment [19, 23, 27, 3]. Increasingly, support for DSM is being provided in hardware for efficiency <ref> [2, 11, 12, 1, 17] </ref>. This has enabled the development of distributed memory computers with hardware support for shared memory. Software implemented systems are designed to provide the DSM on existing distributed memory systems. <p> Thus, Dash supports an integrated, dynamic distributed mechanism as well as a non-integrated, static distributed mechanism. Some systems implement the synchronization mechanism in software even though the data consistency is maintained in hardware. For instance, the FLASH system, which is designed to test a variety of coherence maintenance protocols <ref> [12] </ref>, implements 15 synchronization primitives at the operating system level. We next describe a few representative systems. Plus The Plus system, a hardware implementation of a DSM, is composed of a group of Plus nodes, connected via a "fast interconnection network" [2].
Reference: [13] <author> B. Fleisch. </author> <title> "Distributed System V IPC in Locus: A design and Implementation Retrospective". </title> <booktitle> In Sigcomm '86 Symposium, </booktitle> <pages> pages 386-396, </pages> <year> 1986. </year>
Reference-contexts: When a page is cached on a processor, it is not swapped out of the processor until time units have expired. Synchronization in Mirage is provided as semaphores, implemented in the Locus kernel <ref> [13] </ref>. This mechanism, is integrated into the Locus kernel. The Locus system was designed to provide reliability in a distributed system. The Locus semaphore mechanism is a fault-tolerant to processor failures. The semaphores are grouped into semaphore sets.
Reference: [14] <author> B. Fleisch, R. Hyde, and N. Juul. </author> <title> "Mirage+: A Kernel Implementation of Distributed Shared Memory on a Network of Personal Computers". </title> <journal> Software Practice and Experience, </journal> <note> 1994. To appear. </note>
Reference-contexts: Some systems have been built on mini-computers connected by Ethernet; Mirage, for instance, is built on a network of Vax 11/750 computers [15]. Many others are built on a network of workstations or even personal computers, e.g., Mether and Mirage+ <ref> [23, 14] </ref>. Since most software implementations are academic research projects, the choice of the platforms have been dictated by the existing infrastructure. Nevertheless, the performance results reported give us insight on the viability of DSM systems on such platforms. <p> Mirage The Mirage system is implemented in the kernel of a modified Locus operating system on a group of Vax 11/750 computers connected by Ethernet [15]. More recently, a 13 modified version of the system, Mirage+, has been ported to a network of personal computers running AIX <ref> [14] </ref>. Mirage and Mirage+ support paged-segmentation where the unit of data migration or distribution is the page. The coherence protocol used is similar to IVY with one important distinction, the use of a variable parameter to reduce the effects of thrashing.
Reference: [15] <author> B. Fleisch and G. Popek. </author> <title> "Mirage: A Coherent Distributed Shared Memory Design". </title> <booktitle> In Proceedings of the eleventh ACM Symp. on Operating System Principles, </booktitle> <pages> pages 211-223, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Some systems have been built on mini-computers connected by Ethernet; Mirage, for instance, is built on a network of Vax 11/750 computers <ref> [15] </ref>. Many others are built on a network of workstations or even personal computers, e.g., Mether and Mirage+ [23, 14]. Since most software implementations are academic research projects, the choice of the platforms have been dictated by the existing infrastructure. <p> This synchronization mechanism is unusual in that the coherence maintenance protocol is combined into the synchronization mechanism and not vice-versa. Mirage The Mirage system is implemented in the kernel of a modified Locus operating system on a group of Vax 11/750 computers connected by Ethernet <ref> [15] </ref>. More recently, a 13 modified version of the system, Mirage+, has been ported to a network of personal computers running AIX [14]. Mirage and Mirage+ support paged-segmentation where the unit of data migration or distribution is the page.
Reference: [16] <author> Robert Fowler. </author> <title> "Decentralized Object Finding Using Forwarding Addresses". </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1985. </year>
Reference-contexts: This is because the static distributed manager scheme is still a centralized scheme. In distributed schemes, the idea of a manager is no longer applicable; instead the node that currently holds the synchronization variable is considered the holder or owner <ref> [16] </ref>. When the variable is migrated to another node, the ownership of the synchronization variable is transferred to that site as well. Since the ownership of the resource changes dynamically, a requesting site may not know the identity of the current owner of the resource. <p> These techniques have been used in locating other resources (including DSM pages) in distributed systems and are not unique to synchronization variables. Fowler presents a comprehensive exposition on 8 locating objects in a distributed environment <ref> [16] </ref>. Integrated versus Non-integrated Synchronization Mechanisms A DSM system supports an integrated synchronization mechanism if the programming model (or the communication model supported by it) defines the synchronization available for user processes.
Reference: [17] <author> G. Hermannsson and L. Wittie. </author> <title> "Optimistic Synchronization in Distributed Shared Memory". </title> <booktitle> In International Conference on Distributed Computing Systems, </booktitle> <year> 1994. </year>
Reference-contexts: Hardware versus Software A DSM system can be implemented either in hardware or software. Early DSMs were software based and were implemented on multicomputers whose architecture provided no support for a shared memory environment [19, 23, 27, 3]. Increasingly, support for DSM is being provided in hardware for efficiency <ref> [2, 11, 12, 1, 17] </ref>. This has enabled the development of distributed memory computers with hardware support for shared memory. Software implemented systems are designed to provide the DSM on existing distributed memory systems.
Reference: [18] <author> C. A. R. Hoare. </author> <title> "Monitors: An Operating System Structuring Concept". </title> <journal> Communications of the ACM, </journal> <volume> 17(10) </volume> <pages> 549-557, </pages> <month> October </month> <year> 1974. </year>
Reference-contexts: - 1; de-queue a process; else else block the process on S := S + 1; the semaphore queue; Eventcounts The eventcount synchronization mechanism was proposed by Reed and Kanodia [28] as an alternative to the mutual exclusion type of synchronization provided by mechanisms such as binary semaphores and monitors <ref> [18] </ref>. An eventcount is a monotonically increasing integer variable. Whenever a process needs to signal the occurrence of an event to the other processes in the system, it does so by incrementing the eventcount variable. To increment the eventcount variable, a process executes the advance (eventcount) primitive. <p> This data type is implemented as a distributed lock. Aside from locks, centralized implementations of barriers and conditional variables are also provided in Munin [4]. A condition variable makes it possible to implement synchronization based on Monitors <ref> [18] </ref>. Processes block by executing a wait primitive on Condition variable. 12 When a process executes the signal primitive, one of the blocked processes is unblocked and allowed to proceed. The definition of the signal primitive in Munin is different; all the blocked processes are unblocked together.
Reference: [19] <author> K. Li and P. Hudak. </author> <title> "Memory Coherence in Shared Virtual Memory Systems". </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: These pages are then cached onto processors on demand similar to the manner in which pages are brought into main memory in traditional virtual memory systems <ref> [19] </ref>. However, unlike in virtual memory, the pages in DSM systems are resources that are competed for by concurrent processes executing on different processors. Program execution efficiency is improved by replicating the shared pages and placing copies of the page on the processors. <p> We next discuss these criteria in detail. Hardware versus Software A DSM system can be implemented either in hardware or software. Early DSMs were software based and were implemented on multicomputers whose architecture provided no support for a shared memory environment <ref> [19, 23, 27, 3] </ref>. Increasingly, support for DSM is being provided in hardware for efficiency [2, 11, 12, 1, 17]. This has enabled the development of distributed memory computers with hardware support for shared memory. Software implemented systems are designed to provide the DSM on existing distributed memory systems. <p> Software implemented systems are designed to provide the DSM on existing distributed memory systems. These systems were implemented either as an entire operating system or as a library of user callable routines and a runtime system which ran on top of the operating system <ref> [27, 19] </ref>. The libraries provide the primitives supported by the DSM and the runtime system implements the mechanisms that access the shared data and perform synchronization. While such systems serve as valuable testbeds to validate a particular mechanism, the overhead incurred is high. <p> IVY supports shared pages which are replicated on the nodes of the system to reduce latency. Strict consistency is used to maintain the coherence of the replicated pages <ref> [19] </ref>. In the IVY system, eventcounts are used for synchronization. The choice was motivated by the fact that the Aegis operating system, on top of which IVY was implemented, 10 already provided eventcounts. In an earlier version of IVY, IVY I, eventcounts were im-plemented using remote procedure calls.
Reference: [20] <author> K. Li and R. Schaefer. </author> <title> "A Hypercube Shared Virtual Memory System". </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages I125-I132, </pages> <year> 1989. </year>
Reference-contexts: Nevertheless, the performance results reported give us insight on the viability of DSM systems on such platforms. A few systems are built on existing multicomputers such as the Intel iPSC/2 hypercube <ref> [20] </ref>. IVY The IVY system, one of the early DSM systems, is implemented on a network of Apollo workstations connected via Ethernet [21]. IVY supports shared pages which are replicated on the nodes of the system to reduce latency. <p> If a process is aborted or the site where it resides crashes, the manager uses the undo log to clean up any half-finished semaphore operations. Shiva Shiva, a DSM with a coherence protocol similar to IVY, has been developed on the Intel iPSC/2 hypercube multicomputer <ref> [20] </ref>. The prototype was implemented on NX/2, the minimal operating system executed on each node of the iPSC/2. The system provides binary semaphores. These semaphores are accessed by using send/receive primitives to the central semaphore controller. <p> The system provides binary semaphores. These semaphores are accessed by using send/receive primitives to the central semaphore controller. It is also possible to implement the mechanism using the dynamic distributed technique to cache the semaphore on the processor performing P and V operations <ref> [20] </ref>. Since only the semaphore being accessed is cached, this implementation avoids the false sharing problem inherent in 14 mechanisms which cache a group of semaphores.
Reference: [21] <author> Kai Li. </author> <title> "Shared Virtual Memory on Loosely Coupled Multiprocessors". </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> September </month> <year> 1986. </year> <month> 21 </month>
Reference-contexts: A few systems are built on existing multicomputers such as the Intel iPSC/2 hypercube [20]. IVY The IVY system, one of the early DSM systems, is implemented on a network of Apollo workstations connected via Ethernet <ref> [21] </ref>. IVY supports shared pages which are replicated on the nodes of the system to reduce latency. Strict consistency is used to maintain the coherence of the replicated pages [19]. In the IVY system, eventcounts are used for synchronization. <p> All eventcount variables were placed in the local (non-shared) memory of a processor and operations on eventcount variables were sent as remote calls to this processor. A variation to this scheme is a static distributed technique <ref> [21] </ref>. The RPC based implementation is essentially a centralized scheme which is not integrated with the programming model supported by IVY. IVY II supports an integrated synchronization mechanism where the eventcount variables were located in the shared pages of the shared virtual memory space. <p> It is a dynamic distributed scheme; the pages containing the eventcount variables migrate among the processors where they are used. To ensure the atomicity of the eventcount operations, the shared page containing the eventcount variables is kept on a processor by locking it (referred to as "wiring" the page <ref> [21] </ref>) during the operation. Li claims that this mechanism not only has cleaner semantics than the RPC version, but also is more efficient than the RPC implementation of IVY I when there are multiple processes per processor.
Reference: [22] <author> V. Lo. </author> <title> "Operating Systems Enhancements for Distributed Shared Memory". </title> <editor> In M. Yovits, editor, </editor> <booktitle> Advances in Computers, </booktitle> <pages> pages 191-237. </pages> <publisher> Academic Press, </publisher> <year> 1994. </year>
Reference-contexts: While such systems serve as valuable testbeds to validate a particular mechanism, the overhead incurred is high. This overhead can be reduced by incorporating these mechanisms in the kernel <ref> [22] </ref>. The architecture on which software based DSMs are implemented typically provide rudimentary synchronization (e.g., lock-unlock, semaphores or just a hardware Test&Set) on a per-node basis.
Reference: [23] <author> R. Minnich and D. Farber. </author> <title> "The Mether System: </title> <booktitle> Distributed Shared Memory for SunOS 4.0 ". In Proceedings of the Summer 1989 Usenix Conference, </booktitle> <pages> pages 51-60, </pages> <month> Summer </month> <year> 1989. </year>
Reference-contexts: We next discuss these criteria in detail. Hardware versus Software A DSM system can be implemented either in hardware or software. Early DSMs were software based and were implemented on multicomputers whose architecture provided no support for a shared memory environment <ref> [19, 23, 27, 3] </ref>. Increasingly, support for DSM is being provided in hardware for efficiency [2, 11, 12, 1, 17]. This has enabled the development of distributed memory computers with hardware support for shared memory. Software implemented systems are designed to provide the DSM on existing distributed memory systems. <p> Some systems have been built on mini-computers connected by Ethernet; Mirage, for instance, is built on a network of Vax 11/750 computers [15]. Many others are built on a network of workstations or even personal computers, e.g., Mether and Mirage+ <ref> [23, 14] </ref>. Since most software implementations are academic research projects, the choice of the platforms have been dictated by the existing infrastructure. Nevertheless, the performance results reported give us insight on the viability of DSM systems on such platforms.
Reference: [24] <author> B. Nitzberg and V. Lo. </author> <title> "Distributed Shared Memory: A Survey of Issues and Algorithms". </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Maintaining the coherence of shared data in DSM systems has drawn a lot of attention <ref> [24] </ref>.
Reference: [25] <author> M. Ramachandran and M. Singhal. </author> <title> "Decentralized Semaphore Support in a Virtual Shared Memory System". </title> <journal> Journal of Supercomputing, </journal> <note> 1994. To appear. </note>
Reference-contexts: Unlike in distributed schemes, there is no need to locate the owner of the synchronization variable. However, a centralized scheme will result in a bottleneck at the manager node when the access frequency of synchronization variables is high <ref> [25] </ref>. It also makes the manager the single point of failure in the system. It is possible to improve on the centralized scheme by using more than one manager in the system.
Reference: [26] <author> U. Ramachandran, M. Ahamad, and Y. Khalidi. </author> <title> "Coherence of Distributed Shared Memory Unifying Synchronization and Data Transfer". </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 160-169, </pages> <month> aug </month> <year> 1989. </year>
Reference-contexts: However, if different eventcount variables on the same shared page are frequently accessed on different processors, thrashing of the page may result due to false sharing. Clouds Clouds is a DSM system developed at Georgia Tech on a network of workstation connected via Ethernet <ref> [26] </ref>. It supports an object-based model of programming and allows for the migration of objects and other shared data to the processors where they are needed. The object model of computation integrates the synchronization mechanism by providing atomicity in the form of remote procedure calls.
Reference: [27] <author> U. Ramachandran and Y. Khalidi. </author> <title> "An Implementation of Distributed Shared Memory". </title> <journal> Software Practice and Experience, </journal> <volume> 21(5) </volume> <pages> 443-464, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: We next discuss these criteria in detail. Hardware versus Software A DSM system can be implemented either in hardware or software. Early DSMs were software based and were implemented on multicomputers whose architecture provided no support for a shared memory environment <ref> [19, 23, 27, 3] </ref>. Increasingly, support for DSM is being provided in hardware for efficiency [2, 11, 12, 1, 17]. This has enabled the development of distributed memory computers with hardware support for shared memory. Software implemented systems are designed to provide the DSM on existing distributed memory systems. <p> Software implemented systems are designed to provide the DSM on existing distributed memory systems. These systems were implemented either as an entire operating system or as a library of user callable routines and a runtime system which ran on top of the operating system <ref> [27, 19] </ref>. The libraries provide the primitives supported by the DSM and the runtime system implements the mechanisms that access the shared data and perform synchronization. While such systems serve as valuable testbeds to validate a particular mechanism, the overhead incurred is high. <p> Clouds ensures strict consistency of these segments by combining the locking and unlocking the data segment with the accessing and releasing operations, respectively. In order to support inter-process synchronization that is independent of the RPC mechanism, semaphores are supported in the kernel of the Clouds system <ref> [27] </ref>. The kernel, known as Ra, groups the semaphores into semaphore segments which reside on a processor that created them. The P and V operations are performed on that processor in a centralized manner. The semaphore segment also provides linked-lists to queue blocked P requests. An alternative, proposed in [27], is <p> system <ref> [27] </ref>. The kernel, known as Ra, groups the semaphores into semaphore segments which reside on a processor that created them. The P and V operations are performed on that processor in a centralized manner. The semaphore segment also provides linked-lists to queue blocked P requests. An alternative, proposed in [27], is to move the entire segment of semaphores to the requesting site. This alternative would improve the performance for semaphore access if multiple processes reside on a processor; however, false sharing of the semaphore segment by processes on different processors can increase overheads.
Reference: [28] <author> D. Reed and R. Kanodia. </author> <title> "Synchronization with Eventcounts and Sequencers". </title> <journal> Communications of the ACM, </journal> <month> Feb </month> <year> 1979. </year>
Reference-contexts: as follows: P (S): if (S 1) then V (S): if (semaphore queue is not empty) then S := S - 1; de-queue a process; else else block the process on S := S + 1; the semaphore queue; Eventcounts The eventcount synchronization mechanism was proposed by Reed and Kanodia <ref> [28] </ref> as an alternative to the mutual exclusion type of synchronization provided by mechanisms such as binary semaphores and monitors [18]. An eventcount is a monotonically increasing integer variable.
Reference: [29] <author> Per Stenstrom. </author> <title> "A Surevey of Cache Coherence Schemes for Multiprocessors". </title> <journal> IEEE Computers, </journal> <volume> 23(6) </volume> <pages> 12-24, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: While this replication allows concurrent local access to the pages by the processes, the copies become outdated when one of the pages is modified by its processor; this problem is similar to the cache-coherence problem in multiprocessors <ref> [10, 29] </ref>. Maintaining the coherence of shared data in DSM systems has drawn a lot of attention [24].
Reference: [30] <author> P. Tang and P.-C. Yew. </author> <title> "Processor Self-scheduling for Multiple-nested Parallel Loops". </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 528-535, </pages> <year> 1986. </year> <month> 22 </month>
Reference-contexts: This is done by performing a release operation. When a lock is released, any process waiting to acquire the lock is granted. Barriers A barrier is a synchronization mechanism that allows multiple processes to synchronize together at certain points during program execution <ref> [30] </ref>. This allows multiple processes to complete a phase of computation, synchronize to ensure that all other processes have also completed the phase, and then proceed to the next phase of computation. When each process arrives at a barrier, it blocks until all other processes have arrived at the barrier.
References-found: 30

