URL: http://www.cs.jhu.edu/~sheppard/cs.605.754/papers/paper13b.ps.gz
Refering-URL: http://www.cs.jhu.edu/~sheppard/cs.605.754/sched.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: On the Convergence of Stochastic Iterative Dynamic Programming Algorithms  
Author: Tommi Jaakkola, Michael I. Jordan and Satinder P. Singh 
Note: Copyright c Massachusetts Institute of Technology, 1993  
Date: 1441 August 6, 1993  84  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES  
Pubnum: A.I. Memo No.  C.B.C.L. Memo No.  
Abstract: Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments. These algorithms, including the TD() algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP). In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem. The theorem establishes a general class of convergent algorithms to which both TD() and Q-learning belong. This report describes research done at the Dept. of Brain and Cognitive Sciences, the Center for Biological and Computational Learning, and the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for CBCL is provided in part by a grant from the NSF (ASC-9217041). Support for the laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Dept. of Defense. The authors were supported by a grant from the McDonnell-Pew Foundation, by a grant from ATR Human Information Processing Research Laboratories, by a grant from Siemens Corporation, by by grant IRI-9013991 from the National Science Foundation, by grant N00014-90-J-1942 from the Office of Naval Research, and by NSF grant ECS-9216531 to support an Initiative in Intelligent Control at MIT. Michael I. Jordan is a NSF Presidential Young Investigator. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aoki, M. </author> <year> (1967). </year> <title> Optimization of Stochastic Systems. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <year> (1993). </year> <title> Learning to act using real-time dynamic programming. </title> <note> Submitted to: AI Journal. </note>
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C.J.C.H. </author> <year> (1990). </year> <title> Sequential decision problems and neural networks. </title> <editor> In D. Touretzky (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 2, </volume> <pages> pp. 686-693. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <address> Engle-wood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Bertsekas, D. P., & Tsitsiklis, J. N. </author> <year> (1989). </year> <title> Parallel and Distributed Computation: Numerical Methods. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: When the future costs are not discounted (fl = 1) but the chain is absorbing and all policies lead to the terminal state w.p.1 there still exists a weighted maximum norm with respect to which T is a contraction mapping <ref> (see e.g. Bertsekas & Tsitsiklis, 1989) </ref>. The variance of F t (s; u) given the past is within the bounds of theorem 1 as it depends on Q t (s; u) at most linearly and the variance of c s (u) is bounded.
Reference: <author> Dayan, P. </author> <year> (1992). </year> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8, </booktitle> <pages> 341-362. </pages>
Reference-contexts: We note that the above derivation of the TD () algorithm corresponds to the specific choice of a linear representation for the predictors V t (i) <ref> (see, e.g., Dayan, 1992) </ref>. Learning rules for other representations can be obtained using gradient descent but these are not considered here. In practice TD () is usually applied to an absorbing chain thus allowing the use of either the batch or the on-line version but the latter is usually preferred.
Reference: <author> Dayan, P., & Sejnowski, T. J. </author> <year> (1993). </year> <title> TD() converges with probability 1. </title> <booktitle> CNL, The Salk Institute, </booktitle> <address> San Diego, CA. </address>
Reference: <author> Dvoretzky, A. </author> <year> (1956). </year> <title> On stochastic approximation. </title> <booktitle> Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability. </booktitle> <publisher> University of California Press. </publisher>
Reference-contexts: Proof. Except for the appearance of fi n (x) this is a standard result. With the above definitions convergence follows directly from Dvoretzky's extended theorem <ref> (Dvoretzky, 1956) </ref>.
Reference: <author> Robbins, H., & Monro, S. </author> <year> (1951). </year> <title> A stochastic approximation model. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22, </volume> <pages> 400-407. </pages>
Reference: <author> Ross, S. M. </author> <year> (1970). </year> <title> Applied Probability Models with Optimization Applications. </title> <address> San Francisco: </address> <publisher> Holden-Day. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: Note that the proof covers both the on-line and batch versions. 2 The TD () algorithm The TD () <ref> (Sutton, 1988) </ref> is also a DP-based learning algorithm that is naturally defined in a Markovian environment. Unlike Q-learning, however, TD does not involve decision-making tasks but rather predictions about the future costs of an evolving system.
Reference: <author> Watkins, C.J.C.H. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> PhD Thesis, </type> <institution> University of Cam-bridge, </institution> <address> England. </address> <note> 12 Watkins, </note> <author> C.J.C.H, & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 279-292. </pages>
Reference-contexts: i t + flc i t+1 + fl 2 c i t+2 + : : : + fl n1 c i t+n1 + fl n V t (i t+n ) (13) The expected value of this can be shown to be a strictly better estimate than the current estimate is <ref> (Watkins, 1989) </ref>. In the undiscounted case this holds only when n is larger than some chain-dependent constant. <p> Furthermore, we have introduced a new parameter which affects the trade-off between the bias and variance of the estimate <ref> (Watkins, 1989) </ref>. An increase in puts more weight on less biased estimates with higher variances and thus the bias in V t decreases at the expense of a higher variance. The mathematical convenience of using the geometric average can be seen as follows.
Reference: <author> Werbos, P. </author> <year> (1992). </year> <title> Approximate dynamic programming for real-time control and neural modeling. </title> <editor> In D. A. White and D. A. Sofge, (Eds.), </editor> <booktitle> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, </booktitle> <pages> pp. 493-525. </pages> <address> New York: </address> <publisher> Van Nostrand Reinhold. </publisher> <pages> 13 </pages>
References-found: 13

