URL: http://omega.albany.edu:8008/entpriors.ps
Refering-URL: http://omega.albany.edu:8008/
Root-URL: 
Email: E-Mail: carlos@omega.albany.edu  
Title: Entropic Priors  
Author: Carlos C. Rodrguez 
Date: October 4, 1991  
Affiliation: Department of Mathematics and Statistics State University of New York at Albany  
Abstract: Entropic priors assign probabilities by combining in an inseparable way the information theoretic concept of entropy with the underlying Riemannian geometry of the hypothesis space. These priors form the cornerstone of a developing new and more objective Bayesian theory of inference. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Shun-ichi Amari. </author> <title> Differential-Geometrical Methods in Statistics, </title> <booktitle> volume 28 of Lecture Notes in Statistics. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1985. </year> <title> 4 Regular models are smooth Riemannian manifolds relative to the Haussdorf topology of the Hellinger distance. 5 I recently learned about this from an unpublishedmanuscript by Michael Hardy a graduate student in statistics at the university of Minnesota 21 </title>
Reference-contexts: With this imagery in mind, and the paraphernalia of modern geometry, the theory of inference is rapidly developing into a new science that Amari has baptized as information geometry <ref> [1, p. 7] </ref>. 3 I have implemented the algorithm in Quick-C and it takes a few minutes runnig on a 386/33Mhz to obtain solutions comparable to those in [3]. 16 4.1 Robustness and the Lie Derivative This section contributes to the ongoing development of information geometry by showing how the geometric
Reference: [2] <author> G. Larry Bretthorst. </author> <title> Bayesian Spectrum Analysis and Parameter Estima--tion, </title> <booktitle> volume 48 of Lecture Notes in Statistics. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: It is, therefore, straight forward to re-write the analysis in <ref> [2] </ref> using equation (1) as the prior. For ff = 0 (total ignorance case) we obtain the posterior distribution for the frequencies given by: P r (d!jx; ff = 0) / jg (h; !)j 1 N d 2 2 which is different from the result obtained in [2]. <p> the analysis in <ref> [2] </ref> using equation (1) as the prior. For ff = 0 (total ignorance case) we obtain the posterior distribution for the frequencies given by: P r (d!jx; ff = 0) / jg (h; !)j 1 N d 2 2 which is different from the result obtained in [2]. However, when N is large, the generalized periodogram h 2 peacks about the "true" ! but from equation (41) 12 the term p g (h; !) increases only polynomially in h which is negligible compared to the other term that increases exponentially in h.
Reference: [3] <author> P. Carnevalli, L. Colleti, and S. Patarnello. </author> <title> Image processing by simulated annealing. </title> <journal> IBM journal of Research and Devolopment, </journal> <volume> 29 </volume> <pages> 569-579, </pages> <year> 1985. </year>
Reference-contexts: However, an stochastic optimization algorithm (like simulated annealing) rapidly produces high quality solutions 3 (see <ref> [3] </ref>). It seems profitable to exploit the use of entropic priors in image processing problems of this kind. 4 Towards a Bayesian Theory of Information Geometry The main advantage of having a geometric theory of inference is the enhancement on imagination that it produces. <p> paraphernalia of modern geometry, the theory of inference is rapidly developing into a new science that Amari has baptized as information geometry [1, p. 7]. 3 I have implemented the algorithm in Quick-C and it takes a few minutes runnig on a 386/33Mhz to obtain solutions comparable to those in <ref> [3] </ref>. 16 4.1 Robustness and the Lie Derivative This section contributes to the ongoing development of information geometry by showing how the geometric concept of the Lie derivative is naturally related to the statistical concept of robustness. Statistical robustness is a desirable property of an inferential procedure.
Reference: [4] <author> B.A. Dubrovin, </author> <title> A.T. Fomenko, and S.P. Novikov. Modern Geometry-Methods and Applications, Part-I, volume GTM 93 of Graduate Texts in Mathematics. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: It is therefore not surprising, that ideas developed in the physical theories of elasticity and the mechanics of continuous media, are found useful to quantify statistical robustness. One aspect of the intrinsic robustness of a hypothesis space can be quantified by the strain tensor (see for example <ref> [4, pages 155 and 205-211] </ref>). Recall that the strain tensor is a covariant tensor of order two (i.e. a bilinear form on vectors) that quantifies how the metric of the space changes under deformations defined by a vector field.
Reference: [5] <author> C.T. Herman. </author> <title> Image reconstruction from projections. The fundamentals of Computarized Tomography. </title> <publisher> Academic Press, </publisher> <year> 1980. </year>
Reference-contexts: The (approximate) linear functional relation between the input (object) and the output (data) is fixed by the physics of the particular situation, but the basic idea common to most methods is very simple (see <ref> [5] </ref>). The object f to be reconstucted, is hit with some kind of radiation, and a characteristic g, of the scattered output field is recorded. Typical examples include: radio frequency in NMR, x-rays in computed tomography, or sound waves in ultrasound imaging.
Reference: [6] <author> I.A. Ibragimov and R.Z. Has'minskii. </author> <title> Statistical Estimation, </title> <booktitle> volume 16 of Applications of Mathematics. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1981. </year>
Reference-contexts: Thus, in dimensions grater than two, x is inadmissible (see for example <ref> [6, pages 25-27] </ref>). These are all classic results of the theory of estimation produced effortlessly from equation (1). Stein's estimator exemplifies the general (Entropic) Empirical Bayes approach: first, reduce the problem of estimation of the (multi) parameter to the estimation of the positive scalar parameter ff, as in (20).
Reference: [7] <author> Harold Jeffreys. </author> <title> Theory of Probability. </title> <publisher> Oxford University Press, </publisher> <year> 1939. </year>
Reference-contexts: Due to the lack of general methods for transforming prior information into assigments of prior probabilities, there is greater agreement among statisticians about models than about priors. The maximum entropy formalism (see Jaynes [12]) and the total ignorance priors obtained from invariance (see Jeffreys <ref> [7] </ref>, and Jaynes [12]) are two isolated exceptions. It is remarkable that entropic priors include both methods as extreme special cases. An overview of the main quatities that define formula (1) is presented below.
Reference: [8] <author> S. Kullback. </author> <title> Information Theory and Statistics. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: When both P and Q are probability measures absolutely continuous with respect to each other, the above integral always exists even though it may be infinite (see Kullback <ref> [8] </ref> p. 5). Moreover, in this case, a simple application of Jensen's inequality shows that I (P : Q) is non negative and that it is zero only when P Q.
Reference: [9] <author> T.L. Lai and D. Siegmund, </author> <title> editors. Herbert Robbins Selected Papers. </title> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: Efron and Morris were the first to provide an empirical Bayes context to Stein-type estimators, but their priors and unbiased method of estimation were chosen in an ad-hoc manner explicitly designed to obtain the result (see Robbins collection of selected papers <ref> [9] </ref> and list of references on pages 1-5).The sim plest non-trivial application of entropic priors generates the celebrated Stein's 6 shrinking phenomenon without the ad-hockeries and at the same time provides a rationale for it.
Reference: [10] <author> Carlos C. Rodrguez. </author> <title> The metrics induced by the kullback number. </title> <editor> In John Skilling, editor, </editor> <title> Maximum Entropy and Bayesian Methods. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: Notice that the quadratic form must be positive definite because u (*) has a global minimum at * = 0. The matrix of this quadratic form is known as the Fisher information matrix and the induced Riemannian metric is known as the information metric 1 (see [8],[1] or <ref> [10] </ref>).
Reference: [11] <author> Carlos C. Rodrguez. </author> <title> Objective bayesianism and geometry. </title> <editor> In Paul F. Fougere, editor, </editor> <title> Maximum Entropy and Bayesian Methods. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1990. </year>
Reference: [12] <editor> R.D. Rosenkrantz, editor. E.T. </editor> <title> Jaynes: Papers on Probability, </title> <journal> Statistics and Statistical Physics, </journal> <volume> volume 158. </volume> <publisher> Synthese Library, </publisher> <year> 1983. </year>
Reference-contexts: Although they solve only half of the problem, they solve the important half. Due to the lack of general methods for transforming prior information into assigments of prior probabilities, there is greater agreement among statisticians about models than about priors. The maximum entropy formalism (see Jaynes <ref> [12] </ref>) and the total ignorance priors obtained from invariance (see Jeffreys [7], and Jaynes [12]) are two isolated exceptions. It is remarkable that entropic priors include both methods as extreme special cases. An overview of the main quatities that define formula (1) is presented below. <p> Due to the lack of general methods for transforming prior information into assigments of prior probabilities, there is greater agreement among statisticians about models than about priors. The maximum entropy formalism (see Jaynes <ref> [12] </ref>) and the total ignorance priors obtained from invariance (see Jeffreys [7], and Jaynes [12]) are two isolated exceptions. It is remarkable that entropic priors include both methods as extreme special cases. An overview of the main quatities that define formula (1) is presented below.
Reference: [13] <author> J. Shore and R. Johnson. </author> <title> Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> IT-26:26-37, </volume> <year> 1980. </year>
Reference: [14] <author> J. </author> <title> Skilling and S.F. Gull. Algorithms and applications. </title> <editor> In C. Ray Smith and W. T. Grandy, Jr., editors, </editor> <title> Maximum-Entropy and Bayesian Methods in Inverse Problems. </title> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1985. </year>
Reference-contexts: It is interesting to notice that the last term in (50) i.e. the invariant measure on F was discovered experimentally by testing different functions of f to penalize values close to zero (Skilling personnal communication, see also <ref> [14, p. 99] </ref>). 3.3.2 An Example with Binary Images To illustrate the use of the main formula (1) when specific prior information is available, consider the following simple problem: On a square of M pixels (e.g. the terminal screen) a few rectangles of random lengths and widths are allocated at random
Reference: [15] <author> John Skilling. </author> <title> The axioms of maximum entropy. </title> <editor> In G. J. Erickson and C. R. Smith, editors, </editor> <title> Maximum Entropy and Bayesian Methods. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1987. </year>
Reference: [16] <author> John Skilling. </author> <title> Classical Max Ent data analysis. </title> <editor> In John Skilling, editor, </editor> <title> Maximum Entropy and Bayesian Methods. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: The MAP defined by the solution of (50) is what is computed by the celebrated MEMSYSs algorithm of Skilling, Gull and their co-workers (see <ref> [16] </ref>).
References-found: 16

