URL: http://www.cse.ucsc.edu/~manfred/pubs/gmix.ps
Refering-URL: http://www.cse.ucsc.edu/~manfred/pubs.html
Root-URL: http://www.cse.ucsc.edu
Email: singer@research.att.com  manfred@cse.ucsc.edu  
Title: A New Parameter Estimation Method for Gaussian Mixtures  
Author: Yoram Singer Manfred K. Warmuth 
Keyword: Mixture of Gaussians, On-line learning, EM, Convergence rate, Digit recognition  
Date: March 16, 1998  
Address: 180 Park Avenue Florham Park, NJ 07932  Santa Cruz, CA 95064  
Affiliation: AT&T Labs  Computer and Information Sciences University of California, Santa Cruz  
Abstract: We describe a new iterative method for parameter estimation of Gaussian mixtures. The new method is based on a framework developed by Kivinen and Warmuth for supervised online learning. In contrast to gradient descent and EM, which estimate the mixture's covariance matrices, the proposed method estimates the inverses of the covariance matrices. Furthermore, the new parameter estimation procedure can be applied in both on-line and batch settings. We show experimentally that it is typically faster than EM, and usually requires about half as many iterations as EM. We also describe experiments with digit recognition that demonstrate the merits of the on-line version when the source generating the data is non-stationary. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Bauer, D. Koller, and Y. Singer. </author> <title> Update rules for parameter estimation in Bayesian networks. </title> <booktitle> In Proceedings of the 13th Annual Conference on Uncertainty in AI, </booktitle> <pages> pages 3-13, </pages> <year> 1997. </year>
Reference-contexts: The new parameter estimation method is based on a framework developed by Kivinen and Warmuth [15] for supervised on-line learning. This framework was successfully used in a large number of supervised and un-supervised problems (see for instance <ref> [12, 11, 16, 1] </ref>). Our goal is to find a local minimum of a loss function which, in our case, is the negative log likelihood induced by a mixture of Gaussians. <p> We would like to note that this behavior is by no means esoteric most of our experiments with high dimensional data yielded similar results. These results agree with infinitesimal analysis described briefly in the previous sections and with similar analysis used for other settings <ref> [19, 20, 14, 24, 1] </ref>). We found a different behavior in low dimensional settings.
Reference: [2] <author> Christopher M. Bishop. </author> <title> Neural Networks and Pattern Recognition. </title> <publisher> Oxford University Press, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction Mixture models, in particular mixtures of Gaussians, have been a popular tool for density estimation, clustering, and un-supervised learning with a wide range of applications in statistics, machine learning, and data mining (see for instance <ref> [7, 2] </ref> and the references therein). Mixture models are one of the most useful tools for handling incomplete data, in particular hidden variables. For Gaussian mixtures the hidden variables indicate for each data point the index of the Gaussian that generated it.
Reference: [3] <author> J. Bromley and E. Sackinger. </author> <title> Neural-network and k-nearest neighbor classifiers. </title> <type> Technical Report Technical report 11359-010819-16TM, </type> <institution> AT&T, </institution> <year> 1991. </year>
Reference-contexts: We also would like to note that the best error rate achieved using a mixture model in an on-line mode is 1:8%, which is lower than the error rates of previously studied classifiers. Furthermore, it is actually better than human performance (in the sense defined in <ref> [3] </ref>). These results provide some empirical evidence that the JE update, when used in the on-line mode, is able to track and efficiently approximate the distribution of a time varying source.
Reference: [4] <author> Corinna Cortes and Vladimir Vapnik. </author> <title> Support-vector networks. </title> <journal> Machine Learning, </journal> <volume> 20(3) </volume> <pages> 273-297, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: Experiments with US Postal Service data set. The US Postal Service (USPS) data set is a collection of digits collected from actual handwritten mailings. The problem of automatic digit recognition using this data set was by several researchers (see <ref> [4] </ref> and the references therein). This data set contains 7; 300 training digits and 2; 000 digits for testing. Each digit is represented by a 16 fi 16 pixel image that can take 256 different values. <p> This data set contains 7; 300 training digits and 2; 000 digits for testing. Each digit is represented by a 16 fi 16 pixel image that can take 256 different values. Error rates of various classifiers for this data set are given in Table 2 (taken from <ref> [4] </ref>). In our experiments with this data set we used the same pre-processing that was used by Cortes and Vapnik in [4]. <p> Error rates of various classifiers for this data set are given in Table 2 (taken from <ref> [4] </ref>). In our experiments with this data set we used the same pre-processing that was used by Cortes and Vapnik in [4]. This data set is rather small and the error rates of the various learning algorithms tested on this data set are typically high compared to other digit data sets.
Reference: [5] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: Thus, the loss function V t ( e Q) may have multiple minima, making the problem of finding arg min e Q V t ( e Q) difficult. 3 In order to sidestep this problem we use the log-sum inequality <ref> [5] </ref> to obtain an upper bound for the distance function D ( e Q; Q). We denote this upper bound as denoted b D ( e Q; Q).
Reference: [6] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B39:1-38, </volume> <year> 1977. </year>
Reference-contexts: For brevity we term the new iterative parameter estimation method the joint-entropy (JE) update. The JE update shares a common characteristic with the Expectation Maximization <ref> [6, 20] </ref> algorithm as it first calculates the same expectations. However, it replaces the maximization step with a different update of the parameters. For instance, it updates the inverse of the covariance matrix of each Gaussian in the mixture, rather than the covariance matrices themselves. <p> For density estimation the natural loss function is the negative log-likelihood of the data loss (SjQ) = 1 ln P (SjQ) = jSj x2S The best parameters which minimize the above loss cannot be found analytically. The common approach is to use iterative methods such as EM <ref> [6, 20] </ref> to find a local minimizer of the loss. In an iterative parameter estimation framework we are given the old set of parameters Q t and we need to find a set of new parameters Q t+1 that induce smaller loss. <p> A comparison of EM and the JE update is given in Table 5. The JE update and EM differ in several aspects. First, EM uses a simple update for the mixture weights w. Second, EM uses the expectations (with respect to the current parameters) of the sufficient statistics <ref> [6] </ref> for i and C i to find new sets of mean vectors and covariance matrices. The JE uses a (slightly different) weighted average of the observation and, in addition, it adds the old parameters. <p> Furthermore, like EM, the JE update first estimates the expectations P (xjQ i ) and then updates the parameters. Hence, it can be viewed as a generalized EM (GEM) algorithm and thus guaranteed to converge under several conditions <ref> [6, 18] </ref>. Using standard infinitesimal analysis (see for instance [19, 20]) it is possible to show (though extremely tedious) that the JE update is a contraction mapping. <p> This behavior underscores the advantages of both methods. EM uses a fixed learning rate and is guaranteed to converge to a local maximum of the likelihood, under conditions that typically hold for mixture of Gaussians <ref> [6, 23] </ref>. the JE update, on the other hand, encompasses a learning rate and in many settings it converges much faster than EM. However, the superior performance in high dimensional cases demands its price in low dimensional dense cases.
Reference: [7] <author> Richard O. Duda and Peter E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: 1 Introduction Mixture models, in particular mixtures of Gaussians, have been a popular tool for density estimation, clustering, and un-supervised learning with a wide range of applications in statistics, machine learning, and data mining (see for instance <ref> [7, 2] </ref> and the references therein). Mixture models are one of the most useful tools for handling incomplete data, in particular hidden variables. For Gaussian mixtures the hidden variables indicate for each data point the index of the Gaussian that generated it.
Reference: [8] <author> A. Dvoretzky. </author> <title> On stochastic approximation. </title> <booktitle> In Proceedings of the Third Berkley Symposium on Mathematical Statistics and Probability. </booktitle> <institution> University of California Press, </institution> <year> 1956. </year>
Reference-contexts: However, this learning rate cannot be determined in advance since it depends on the eigen values of the likelihood's Hessian evaluated at the local maximum. It is also possible to show that the on-line version of the JE update is a stochastic approximation algorithm <ref> [8, 21] </ref>. In short, the on-line update is guaranteed to converge with probability one to a local maximizer of the likelihood if P 1 P 1 t &lt; 1, for instance, if we set t = 1=t.
Reference: [9] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins Univ Press, </publisher> <year> 1996. </year>
Reference-contexts: Thus, it avoids expensive matrix inversion operation (although we still need to find the determinants jC i j) and is potentially be more stable numerically in cases where the covariance matrices have small condition number <ref> [9] </ref>. To obtain an on-line procedure we need to update the parameters after each new observation at a time.
Reference: [10] <author> David P. Helmbold and Duncan Herrin. </author> <title> A new gradient-assent method for learning mixture distribution. </title> <type> Technical report, </type> <institution> University of California Santa Cruz, Computer Research Laboratory, </institution> <year> 1998. </year>
Reference-contexts: Finally, we would like to note that, using the same framework, other parameter estimation methods can be derived for Gaussian mixtures. For instance, in parallel work David Helmbold and Duncan Herring <ref> [10] </ref> avoided the non-convexity of the distance (5) by using separate distances in the derivations of the updates for each of the three parameter classes.
Reference: [11] <author> David P. Helmbold, Jyrki Kivinen, and Manfred K. Warmuth. </author> <title> Worst-case loss bounds for sigmoided neurons. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 309-315, </pages> <year> 1995. </year>
Reference-contexts: The new parameter estimation method is based on a framework developed by Kivinen and Warmuth [15] for supervised on-line learning. This framework was successfully used in a large number of supervised and un-supervised problems (see for instance <ref> [12, 11, 16, 1] </ref>). Our goal is to find a local minimum of a loss function which, in our case, is the negative log likelihood induced by a mixture of Gaussians.
Reference: [12] <author> David P. Helmbold, Robert E. Schapire, Yoram Singer, and Manfred K. Warmuth. </author> <title> A comparison of new and old algorithms for a mixture estimation problem. </title> <booktitle> In Proceedings of the Eighth Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 69-78, </pages> <year> 1995. </year>
Reference-contexts: The new parameter estimation method is based on a framework developed by Kivinen and Warmuth [15] for supervised on-line learning. This framework was successfully used in a large number of supervised and un-supervised problems (see for instance <ref> [12, 11, 16, 1] </ref>). Our goal is to find a local minimum of a loss function which, in our case, is the negative log likelihood induced by a mixture of Gaussians. <p> This approximation degrades the further the new parameter values are from the old ones, which further motivates the use of the distance function D ( e Q; Q t ) (see also the discussion in <ref> [12] </ref>).
Reference: [13] <author> A. Jagota and M.K. Warmuth. </author> <title> Continuous and discrete time nonlinear gradient descent: relative loss bounds and convergenc. </title> <booktitle> In International Symposium on Artificial Intelligence and Mathematic, </booktitle> <year> 1998. </year>
Reference-contexts: We use the above method Eq. (3) to derive the updates of this paper. The question is what distances should be used. In general, a distance function must have the following minimal properties <ref> [13] </ref>: D ( e Q; Q) 0 for all e Q; Q and D ( e Q; Q) = 0 when e Q = Q. (Note that we do not require the triangular inequality.) Our model of density estimation is characterized by some density that is parameterized by Q. <p> First, it is straightforward to show for the batch setting that there is always a learning rate such that the likelihood increases after each parameter update <ref> [13] </ref>. Furthermore, like EM, the JE update first estimates the expectations P (xjQ i ) and then updates the parameters. Hence, it can be viewed as a generalized EM (GEM) algorithm and thus guaranteed to converge under several conditions [6, 18].
Reference: [14] <author> M.I. Jordan and L. Xu. </author> <title> Convergence results for the EM approach to mixtures of experts architectures. </title> <booktitle> Neural Networks, </booktitle> <volume> 8 </volume> <pages> 1409-1431, </pages> <year> 1995. </year>
Reference-contexts: We would like to note that this behavior is by no means esoteric most of our experiments with high dimensional data yielded similar results. These results agree with infinitesimal analysis described briefly in the previous sections and with similar analysis used for other settings <ref> [19, 20, 14, 24, 1] </ref>). We found a different behavior in low dimensional settings.
Reference: [15] <author> Jyrki Kivinen and Manfred K. Warmuth. </author> <title> Additive versus exponentiated gradient updates for linear prediction. </title> <journal> Information and Computation, </journal> <volume> 132(1) </volume> <pages> 1-64, </pages> <month> January </month> <year> 1997. </year>
Reference-contexts: The common technique used for estimating the parameters of a stochastic source with hidden variables is the EM algorithm. In this paper we describe a new technique for estimating the parameters of Gaussian mixtures. The new parameter estimation method is based on a framework developed by Kivinen and Warmuth <ref> [15] </ref> for supervised on-line learning. This framework was successfully used in a large number of supervised and un-supervised problems (see for instance [12, 11, 16, 1]). <p> Then, we denote by dB=dx the matrix of derivatives of the elements in B with respect to x, namely, the ij element of dB=dx is db ij (x)=dx. 3 The framework for deriving updates Kivinen and Warmuth <ref> [15] </ref> introduced a general framework for deriving on-line parameter updates. In this section we describe how to apply their framework for the problem of parameter estimation of Gaussian mixtures in a batch setting. We later discuss how a simple modification gives the on-line updates. <p> In an iterative parameter estimation framework we are given the old set of parameters Q t and we need to find a set of new parameters Q t+1 that induce smaller loss. The framework introduced by Kivinen and Warmuth <ref> [15] </ref> deviates from the common approaches as it also requires to the new parameter vector to stay close to the old set of parameters which incorporates all that was learned in the previous iterations.
Reference: [16] <author> Jyrki Kivinen and Manfred K. Warmuth. </author> <title> Relative loss bounds for multidimensional regression problems. </title> <booktitle> In Advances in Neural Information Processing Systems 10, </booktitle> <year> 1997. </year>
Reference-contexts: The new parameter estimation method is based on a framework developed by Kivinen and Warmuth [15] for supervised on-line learning. This framework was successfully used in a large number of supervised and un-supervised problems (see for instance <ref> [12, 11, 16, 1] </ref>). Our goal is to find a local minimum of a loss function which, in our case, is the negative log likelihood induced by a mixture of Gaussians.
Reference: [17] <author> Y. LeCun, L. D. Jackel, L. Bottou, A. Brunot, C. Cortes, J. S. Denker, H. Drucker, I. Guyon, U. A. Muller, E. Sackinger, P. Simard, and V Vapnik. </author> <title> Comparison of learning algorithms for handwritten digit recognition. </title> <booktitle> In International Conference on Artificial Neural Networks, </booktitle> <pages> pages 53-60. </pages> <publisher> World Scientific, </publisher> <year> 1995. </year>
Reference-contexts: One reason for the relatively poor performance of all classifiers is due to a significant disparity in the shape of the digits constituting the training and test sets <ref> [17] </ref>. While the training set was cleaned by removing digits that were chopped by a segmentation algorithm, the test set was kept untouched. Thus, there are shapes that occur rather frequently in the test data but do not resemble any of the images in the training set.
Reference: [18] <author> X. L. Meng and D. B. Rubin. </author> <title> Recent extensions of the EM algorithm (with discussion). </title> <editor> In J.M. Bernardo, J.O. Berger, A.P. Dawid, and A.F.M. Smith, editors, </editor> <booktitle> Bayesian Statistics, 4. </booktitle> <address> Oxfod: </address> <publisher> Clarendon Press, </publisher> <year> 1992. </year>
Reference-contexts: Furthermore, like EM, the JE update first estimates the expectations P (xjQ i ) and then updates the parameters. Hence, it can be viewed as a generalized EM (GEM) algorithm and thus guaranteed to converge under several conditions <ref> [6, 18] </ref>. Using standard infinitesimal analysis (see for instance [19, 20]) it is possible to show (though extremely tedious) that the JE update is a contraction mapping.
Reference: [19] <author> B.C. Peters and H.F. Walker. </author> <title> An iterative procedure for obtaining maximum-likelihood estimates of the parameters for a mixture of normal distributions. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 35 </volume> <pages> 362-378, </pages> <year> 1978. </year>
Reference-contexts: Furthermore, like EM, the JE update first estimates the expectations P (xjQ i ) and then updates the parameters. Hence, it can be viewed as a generalized EM (GEM) algorithm and thus guaranteed to converge under several conditions [6, 18]. Using standard infinitesimal analysis (see for instance <ref> [19, 20] </ref>) it is possible to show (though extremely tedious) that the JE update is a contraction mapping. <p> We would like to note that this behavior is by no means esoteric most of our experiments with high dimensional data yielded similar results. These results agree with infinitesimal analysis described briefly in the previous sections and with similar analysis used for other settings <ref> [19, 20, 14, 24, 1] </ref>). We found a different behavior in low dimensional settings.
Reference: [20] <author> Richard A. Redner and Homer F. Walker. </author> <title> Mixture densities, maximum likelihood and the EM algorithm. </title> <journal> SIAM Review, </journal> <volume> 26(2), </volume> <year> 1984. </year>
Reference-contexts: For brevity we term the new iterative parameter estimation method the joint-entropy (JE) update. The JE update shares a common characteristic with the Expectation Maximization <ref> [6, 20] </ref> algorithm as it first calculates the same expectations. However, it replaces the maximization step with a different update of the parameters. For instance, it updates the inverse of the covariance matrix of each Gaussian in the mixture, rather than the covariance matrices themselves. <p> For density estimation the natural loss function is the negative log-likelihood of the data loss (SjQ) = 1 ln P (SjQ) = jSj x2S The best parameters which minimize the above loss cannot be found analytically. The common approach is to use iterative methods such as EM <ref> [6, 20] </ref> to find a local minimizer of the loss. In an iterative parameter estimation framework we are given the old set of parameters Q t and we need to find a set of new parameters Q t+1 that induce smaller loss. <p> Furthermore, like EM, the JE update first estimates the expectations P (xjQ i ) and then updates the parameters. Hence, it can be viewed as a generalized EM (GEM) algorithm and thus guaranteed to converge under several conditions [6, 18]. Using standard infinitesimal analysis (see for instance <ref> [19, 20] </ref>) it is possible to show (though extremely tedious) that the JE update is a contraction mapping. <p> We would like to note that this behavior is by no means esoteric most of our experiments with high dimensional data yielded similar results. These results agree with infinitesimal analysis described briefly in the previous sections and with similar analysis used for other settings <ref> [19, 20, 14, 24, 1] </ref>). We found a different behavior in low dimensional settings.
Reference: [21] <author> H. Robbins and S. Monro. </author> <title> A stochastic approximation model. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22, </volume> <year> 1951. </year>
Reference-contexts: However, this learning rate cannot be determined in advance since it depends on the eigen values of the likelihood's Hessian evaluated at the local maximum. It is also possible to show that the on-line version of the JE update is a stochastic approximation algorithm <ref> [8, 21] </ref>. In short, the on-line update is guaranteed to converge with probability one to a local maximizer of the likelihood if P 1 P 1 t &lt; 1, for instance, if we set t = 1=t.
Reference: [22] <author> Y. Singer and M.K. Warmuth. </author> <title> Training algorithms for hidden Markov models using entropy based distance functions. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <year> 1996. </year>
Reference-contexts: The same framework used in this paper can be also used for deriving parameter estimation procedures for temporal models such as hidden Markov models <ref> [22] </ref>. 2 Notation and preliminaries Let S be a sequence of training examples hx 1 ; x 2 ; : : : ; x N i where each x i is a d-dimensional vector in R d . To model the distribution of the examples we use m d-dimensional Gaussians.
Reference: [23] <author> C.F.J. Wu. </author> <title> On the convergence properties of the EM algorithm. </title> <journal> Annals of Statistics, </journal> <volume> 11 </volume> <pages> 95-103, </pages> <year> 1983. </year>
Reference-contexts: This behavior underscores the advantages of both methods. EM uses a fixed learning rate and is guaranteed to converge to a local maximum of the likelihood, under conditions that typically hold for mixture of Gaussians <ref> [6, 23] </ref>. the JE update, on the other hand, encompasses a learning rate and in many settings it converges much faster than EM. However, the superior performance in high dimensional cases demands its price in low dimensional dense cases.
Reference: [24] <author> L. Xu and M.I. Jordan. </author> <title> On convergence properties of the EM algorithm for Gaussian mixtures. </title> <journal> Neural Computation, </journal> <volume> 8 </volume> <pages> 129-151, </pages> <year> 1996. </year>
Reference-contexts: We would like to note that this behavior is by no means esoteric most of our experiments with high dimensional data yielded similar results. These results agree with infinitesimal analysis described briefly in the previous sections and with similar analysis used for other settings <ref> [19, 20, 14, 24, 1] </ref>). We found a different behavior in low dimensional settings.
References-found: 24

