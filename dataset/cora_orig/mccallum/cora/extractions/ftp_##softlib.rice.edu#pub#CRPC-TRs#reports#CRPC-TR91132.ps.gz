URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR91132.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Phone: 77251-1892  
Title: Compiler Support for Machine-Independent Parallel Programming in Fortran D  In Compilers and Runtime Software for Scalable  
Author: Seema Hiranandani Ken Kennedy Chau-Wen Tseng Multiprocessors, (J. Saltz and P. Mehrotra, eds.), 
Note: Center for Research on Parallel Computation  North-Holland, Amsterdam, The Netherlands, 1992.  
Date: January 1991  
Address: 91132  P.O. Box 1892 Houston, TX  
Affiliation: CRPC-TR  Rice University  
Abstract-found: 0
Intro-found: 1
Reference: [ACK87] <author> J. R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Proceedings of the Fourteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Munich, Germany, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: ffl after distribution & interchange flg do i = 1, n perform send (X (i-1,1:n)) perform recv (X (i-1,1:n)) do j = 1, n enddo enddo do j = 1, n perform send (Y (1:n,j-1)) perform recv (Y (1:n,j-1)) do i = 1, n enddo enddo 4.2.4 Align Loop alignment <ref> [ACK87] </ref> can improve guard introduc tion. ffl statements require masks flg do i = 1, n Y (i+1) = i enddo ffl alignment eliminates masks in loop flg X (1) = i X (i) = i enddo Y (n+1) = n 4.3 Communications Optimization A major goal of the Fortran D
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: This can be accomplished by calculating Send and Receive iteration sets. For simple loop nests which do not contain loop-carried (inter-iteration) true dependences <ref> [AK87] </ref>, These itera tion sets may also be used to generate In and Out array index sets that combine messages to a single processor into one message.
Reference: [AKLS88] <author> E. Albert, K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Compiling Fortran 8x array features for the Connection Machine computer system. </title> <booktitle> In Symposium on Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: The Fortran D compiler will apply many of the same transformations. booster [PvGS90] provides user-specified distribution functions defined as program views, but does not generate or optimize communications. 6.3 SIMD Compilation Systems 6.3.1 CM Fortran CM Fortran <ref> [AKLS88, TMC89] </ref> is a version of Fortran 77 extended with vector notation, alignment, and data layout specifications. Programmers must explicitly specify data-parallelism in cm fortran programs by marking certain array dimensions as parallel.
Reference: [APT90] <author> F. Andre, J. Pazat, and H. Thomas. </author> <title> Pandore: A system to manage data distribution. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: When programming for distributed-memory machines, users interactively select BLOCK or CYCLIC distributions for selected array dimensions. Code spreading is applied interactively to loops to introduce parallelism. Alignment is not provided. Mimdizer automatically generates communications corresponding to nonlocal memory accesses at the end of the parallelization session. 6.4.6 Pandore Pandore <ref> [APT90] </ref> is a compiler for distributed-memory machines that takes as input C programs extended with BLOCK, CYCLIC, and overlapping data distributions. Distributed arrays are mapped by the compiler onto a user-declared virtual distributed machine that may be configured as a vector, ring, grid, or torus.
Reference: [Bal91] <author> V. Balasundaram. </author> <title> Translating control parallelism to data parallelism. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Houston, TX, </address> <month> March </month> <year> 1991. </year>
Reference-contexts: The amount of data communicated may be reduced by half if the computation is first performed by the processor owning B and C, then sent to the processor owning A. This optimization is a simple application of the "owner stores" rule proposed by Balasundaram <ref> [Bal91] </ref>. In particular, it may be desirable for the Fortran D compiler to partition loops amongst processors so that each loop iteration is executed on a single processor, such as in Kali and Arf [KMV90, WSBH91].
Reference: [BFKK90] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kre-mer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: To handle loop-carried dependences, In and Out index sets need to be constructed at each loop level. Dependence information may be used to calculate the appropriate loop level for each message, using the algorithms described by Balasundaram et al. and Gerndt <ref> [BFKK90, Ger90] </ref>. <p> Unfortunately, this approach generates large numbers of small messages that may prove inefficient because of high communications overhead and latency. Algorithms developed by Balasundaram et al. and Gerndt employ data dependence information to insert communications at the outermost loop allowable, without violating dependences <ref> [BFKK90, Ger90] </ref>. <p> The Fortran D compiler will use the algorithm comm from <ref> [BFKK90] </ref> for determining the loop level for inserting communications. Message vectorization is a special case of prefetch-ing data; i.e., fetching nonlocal data before it is used in a computation. More general data prefetching optimizations are possible. <p> Another part of the Fortran D system is a static performance estimator that will take as input a Fortran D or message-passing Fortran program and predicts its performance on the target machine <ref> [BFKK90, BFKK91] </ref>. The performance estimation is based on training sets, programs containing kernel computation and communications that can be executed on the target machine. The automatic data partitioner is the final component of the Fortran D programming system.
Reference: [BFKK91] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kre-mer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Another part of the Fortran D system is a static performance estimator that will take as input a Fortran D or message-passing Fortran program and predicts its performance on the target machine <ref> [BFKK90, BFKK91] </ref>. The performance estimation is based on training sets, programs containing kernel computation and communications that can be executed on the target machine. The automatic data partitioner is the final component of the Fortran D programming system.
Reference: [BK89] <author> V. Balasundaram and K. Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the SIG-PLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: The function get value () is used to fetch nonlocal data from their designated storage locations. 3.4 Regular Sections For the sake of efficiency, when generating communications the Fortran D compiler constructs approximations of the image for each distributed array using regular section or data access descriptors <ref> [CK87, BK89, HK90] </ref>. A regular section descriptor (RSD) is a compact representation of rectangular or right-triangular array sections and their higher dimension analogs. They may also possess some constant step. The union and intersection of RSDs can be calculated inexpensively, making them highly useful for the Fortran D compiler.
Reference: [CCH + 88] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: The structure of the programming system is shown in Figure 12. It is being developed in the context of the ParaScope parallel programming environment <ref> [CCH + 88] </ref>, and will take advantage of its analysis and transformation abilities [CKT86, KMT91]. Another part of the Fortran D system is a static performance estimator that will take as input a Fortran D or message-passing Fortran program and predicts its performance on the target machine [BFKK90, BFKK91].
Reference: [CCL89] <author> M. Chen, Y. Choo, and J. Li. </author> <title> Theory and pragmatics of compiling efficient parallel code. </title> <type> Technical Report YALEU/DCS/TR-760, </type> <institution> Dept. of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: Rather than generating a large number of individual send and receive communication primitives, the compiler can instead take advantage of efficient collective communications libraries such as Express [EXP89], Crystal router [FJL + 86], Crystal communications <ref> [CCL89] </ref>, and Parti [SBW90]. The compiler will exploit these routines to reduce the cost of communications. <p> No alignment and or distribution specifications are provided. It is not clear how Spot will support computation patterns that cannot be described by stencils, or those involving multiple arrays. 6.4 MIMD Compilation Systems 6.4.1 Crystal Crystal <ref> [CCL89, LC90b, LC90a] </ref> is a high-level functional language. The Crystal compiler targets distributed-memory machines, performing both automatic data decomposition and communications generation. Programs are first separated into phases. Each phase has a different computation structure, represented by an index domain.
Reference: [CG89] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: However, their suitability for scientific programming is unclear, and they also lack language or compiler support. High-level parallel languages such as Linda <ref> [CG89] </ref>, Strand [FT90, FO90], and Delirium [LS91] are valuable when used to coordinate coarse-grained functional parallelism. However, they tend to be inefficient for capturing fine-grain data parallelism of the type described by Hillis and Steele [HS86], because they lack both language and compiler support to assist in efficient data placement.
Reference: [CK87] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interproce-dural side effects in a parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: The function get value () is used to fetch nonlocal data from their designated storage locations. 3.4 Regular Sections For the sake of efficiency, when generating communications the Fortran D compiler constructs approximations of the image for each distributed array using regular section or data access descriptors <ref> [CK87, BK89, HK90] </ref>. A regular section descriptor (RSD) is a compact representation of rectangular or right-triangular array sections and their higher dimension analogs. They may also possess some constant step. The union and intersection of RSDs can be calculated inexpensively, making them highly useful for the Fortran D compiler.
Reference: [CK88] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The Fortran D compiler is designed to exploit large-scale data parallelism. Our philosophy is to use the owner computes rule, where every processor only performs computation for data it owns <ref> [ZBG88, CK88, RP89] </ref>. However, the owner compute rule is relaxed depending on the structure of the computation. However, in this paper we concentrate on deriving a functional decomposition and communication generation by applying the owner computes rule. <p> These researchers do not discuss generating communication for these complex distributions. They also make simplifying assumptions about the effects of the underlying processor topology, and do not consider collective communications. Wolfe [Wol89, Wol90] describes transformations such as loop rotation for distributed-memory programs with simple BLOCK distributions. Callahan and Kennedy <ref> [CK88] </ref> propose methods for compiling programs with user-specified data distribution functions and using compiler inserted load and store commands to support nonlocal memory accesses. They also demonstrate how such programs can be optimized using transformations such as loop distribution, loop peeling, etc.
Reference: [CKK89] <author> D. Callahan, K. Kennedy, and U. Kremer. </author> <title> A dynamic study of vectorization in PFC. </title> <type> Technical Report TR89-97, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: A major component of the success of vector supercomputers is the ability to write machine-independent vector programs in a subdialect of Fortran. Advances in compiler technology, especially automatic vectorization, have made it possible for the scientist to structure Fortran loops according the rules of "vectorizable style" <ref> [Wol89, CKK89] </ref>, which are well understood, and expect the resulting program to be compiled to efficient code on any vector machine. Compare this with the current situation for parallel machines.
Reference: [CKT86] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact 21 of interprocedural analysis and optimization in the IR n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: Since Fortran D semantics limit the scope of all decompositions to the procedure in which they are declared, this can be solved as a forward interprocedural dataflow problem, using the techniques developed for the ParaScope environment <ref> [CKT86] </ref>. Unfortunately, the same semantics also require the compiler to insert calls to run-time data decomposition routines to restore the original data decomposition upon procedure return. Since dynamic data decomposition is an expensive operation, these calls should be eliminated where possible. <p> The structure of the programming system is shown in Figure 12. It is being developed in the context of the ParaScope parallel programming environment [CCH + 88], and will take advantage of its analysis and transformation abilities <ref> [CKT86, KMT91] </ref>. Another part of the Fortran D system is a static performance estimator that will take as input a Fortran D or message-passing Fortran program and predicts its performance on the target machine [BFKK90, BFKK91].
Reference: [CR89] <author> A. Cheung and A. Reeves. </author> <title> The Paragon multicom-puter environment: A first implementation. </title> <type> Technical Report EE-CEG-89-9, </type> <institution> Cornell University Computer Engineering Group, </institution> <address> Ithaca, NY, </address> <month> July </month> <year> 1989. </year>
Reference-contexts: Special Dino language constructs are provided for reductions. Dino programs are deterministic unless special asynchronous distributed arrays are used. As with CM Fortran, Dino programs generate multiple processes per physical processor when large numbers of virtual processors are declared in the environment. 6.3.4 Paragon Paragon <ref> [CR89, Ree90] </ref> is a programming environment targeted at supporting SIMD programs on MIMD distributed-memory machines. It provides both language extensions and run-time support for task management and load balancing. Data distribution in Paragon may either be performed by the user or the system.
Reference: [EXP89] <institution> Parasoft Corporation. </institution> <note> Express User's Manual, </note> <year> 1989. </year>
Reference-contexts: Rather than generating a large number of individual send and receive communication primitives, the compiler can instead take advantage of efficient collective communications libraries such as Express <ref> [EXP89] </ref>, Crystal router [FJL + 86], Crystal communications [CCL89], and Parti [SBW90]. The compiler will exploit these routines to reduce the cost of communications. <p> It utilizes collective communication primitives from the Express run-time system for distributed-memory machines <ref> [EXP89] </ref>. Aspar performs simple dependence analysis using A-lists to detect parallelizable loops. The structure of the loop computation may be recognized as a reduction operation, in which case the loop is paral-lelized by replacing the reduction with the appropriate Express combine operation.
Reference: [FHK + 90] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: MAP may be either distributed or replicated; distributed MAP arrays will consume less memory but may require extra communication to determine location. Fortran D also supports dynamic data decomposition, i.e., changing the decomposition at any point in the program. The complete Fortran D language is described in detail elsewhere <ref> [FHK + 90] </ref>. 2.3 Distribution Functions Distribution functions specify the mapping of an array or The ALIGN and DISTRIBUTE statements in Fortran D specify how distributed arrays are mapped to the physical machine.
Reference: [FJL + 86] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1986. </year>
Reference-contexts: Rather than generating a large number of individual send and receive communication primitives, the compiler can instead take advantage of efficient collective communications libraries such as Express [EXP89], Crystal router <ref> [FJL + 86] </ref>, Crystal communications [CCL89], and Parti [SBW90]. The compiler will exploit these routines to reduce the cost of communications.
Reference: [FO90] <author> I. Foster and R. Overbeek. </author> <title> Bilingual parallel programming. </title> <booktitle> In Proceedings of the Third Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: However, their suitability for scientific programming is unclear, and they also lack language or compiler support. High-level parallel languages such as Linda [CG89], Strand <ref> [FT90, FO90] </ref>, and Delirium [LS91] are valuable when used to coordinate coarse-grained functional parallelism. However, they tend to be inefficient for capturing fine-grain data parallelism of the type described by Hillis and Steele [HS86], because they lack both language and compiler support to assist in efficient data placement.
Reference: [FT90] <author> I. Foster and S. Taylor. Strand: </author> <title> New Concepts in Parallel Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: However, their suitability for scientific programming is unclear, and they also lack language or compiler support. High-level parallel languages such as Linda [CG89], Strand <ref> [FT90, FO90] </ref>, and Delirium [LS91] are valuable when used to coordinate coarse-grained functional parallelism. However, they tend to be inefficient for capturing fine-grain data parallelism of the type described by Hillis and Steele [HS86], because they lack both language and compiler support to assist in efficient data placement.
Reference: [GB90] <author> M. Gupta and P. Banerjee. </author> <title> Automatic data partitioning on distributed memory multiprocessors. </title> <type> Technical Report CRHC-90-14, </type> <institution> Center for Reliable and High-Performance Computing, Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: In the following sections, we look at related systems and their contributions to our Fortran D compilation strategy. First we look at some compilation techniques, then we examine existing compilation systems. Gupta and Banerjee <ref> [GB90] </ref> propose a constraint-based approach to automatically calculate suitable data decompositions. They use simple alignments and distributions similar to those in Fortran D. Prins [Pri90] utilizes shape refinement in conjunction with linear transformations to specify data layouts and guide resulting data motion.
Reference: [Ger90] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency|Practice & Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: To handle loop-carried dependences, In and Out index sets need to be constructed at each loop level. Dependence information may be used to calculate the appropriate loop level for each message, using the algorithms described by Balasundaram et al. and Gerndt <ref> [BFKK90, Ger90] </ref>. <p> Unfortunately, this approach generates large numbers of small messages that may prove inefficient because of high communications overhead and latency. Algorithms developed by Balasundaram et al. and Gerndt employ data dependence information to insert communications at the outermost loop allowable, without violating dependences <ref> [BFKK90, Ger90] </ref>. <p> There are several different storage types, described below: 15 * Overlaps are expansion of local array sections to ac-commodate neighboring nonlocal elements <ref> [Ger90] </ref>. Overlaps are useful for regular computations because they allow the generation of clear and readable code. However, for certain computations storage may be wasted because all array elements between the local section and the one accessed must also be part of the overlap. <p> Guard elimination is described as compile-time resolution; it is performed by calculating the set of evaluators and participants for each statement. Message presending and blocking optimizations are performed using vectorization transformations such as loop fusion and strip mining. Global accumulates are also supported. 6.4.3 SUPERB Superb <ref> [ZBG88, Ger90] </ref> is a semi-automatic paralleliza-tion tool that supports arbitrary user-specified contigu 19 ous rectangular distributions. It performs dependence analysis to guide interactive program transformations in a manner similar to the ParaScope Editor [KMT91]. Superb originated the overlap concept as a means to both specify and store nonlocal data accesses.
Reference: [HA90] <author> D. Hudak and S. Abraham. </author> <title> Compiler techniques for data partitioning of sequentially iterated parallel loops. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Some researchers concentrate on computations within loops that only involve a single array. Ramanujam and Sadayappan [RS89] examine both the data and iteration space to derive a combined task and data partition of the loop nest. Hudak and Abraham <ref> [HA90] </ref> find a stencil-based approach useful for analyzing communications and deriving efficient rectangular or hexagonal data distributions. These researchers do not discuss generating communication for these complex distributions. They also make simplifying assumptions about the effects of the underlying processor topology, and do not consider collective communications.
Reference: [HK90] <author> P. Havlak and K. Kennedy. </author> <title> Experience with interpro-cedural analysis of array side effects. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: The function get value () is used to fetch nonlocal data from their designated storage locations. 3.4 Regular Sections For the sake of efficiency, when generating communications the Fortran D compiler constructs approximations of the image for each distributed array using regular section or data access descriptors <ref> [CK87, BK89, HK90] </ref>. A regular section descriptor (RSD) is a compact representation of rectangular or right-triangular array sections and their higher dimension analogs. They may also possess some constant step. The union and intersection of RSDs can be calculated inexpensively, making them highly useful for the Fortran D compiler. <p> They may also possess some constant step. The union and intersection of RSDs can be calculated inexpensively, making them highly useful for the Fortran D compiler. RSDs have also proven to be quite precise in practice, due to the regular computation patterns exhibited by scientific programs <ref> [HK90] </ref>. Figure 11 shows some examples of regular section descriptors. 4 Compilation of Whole Programs We have shown how the Fortran D compiler introduces guards and generates communications for a simple loop nest.
Reference: [HKK + 91] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <type> Technical Report TR91-154, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: data (b) Maintain overlap areas, temporaries, and hash tables 16 Estimator Performance Static Partitioner Data Automatic Sets Training EnvironmentFortran 77User Fortran D Fortran D Compiler Message Passing Fortran 4.7 Fortran D Programming Environment The compiler is a key element of the Fortran D programming system being developed at Rice University <ref> [HKK + 91] </ref>. The structure of the programming system is shown in Figure 12. It is being developed in the context of the ParaScope parallel programming environment [CCH + 88], and will take advantage of its analysis and transformation abilities [CKT86, KMT91].
Reference: [HS86] <author> W. Hillis and G. Steele, Jr. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <year> 1986. </year>
Reference-contexts: High-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] are valuable when used to coordinate coarse-grained functional parallelism. However, they tend to be inefficient for capturing fine-grain data parallelism of the type described by Hillis and Steele <ref> [HS86] </ref>, because they lack both language and compiler support to assist in efficient data placement. Parallelism must also be explicitly specified when using these languages.
Reference: [IFKF90] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An automatic and symbolic parallelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Some interproce-dural analysis is supported. The Fortran D compiler uses overlaps for storing certain classes of nonlocal data. Major differences between Superb and the Fortran D compiler include support for data alignment, automatic compilation, collective communications, dynamic data decomposition, and storage choices for nonlocal values. 6.4.4 ASPAR, Express Aspar <ref> [IFKF90] </ref> is a compiler that performs automatic data decomposition and communications generation for loops containing a single distributed array. It utilizes collective communication primitives from the Express run-time system for distributed-memory machines [EXP89]. Aspar performs simple dependence analysis using A-lists to detect parallelizable loops.
Reference: [KLS88] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Massively parallel data optimization. </title> <booktitle> In Frontiers88: The 2nd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> Fairfax, VA, </address> <month> October </month> <year> 1988. </year>
Reference-contexts: This greatly simplifies the data distribution and communication generation responsibilities of the compiler, and has freed researchers to concentrate on techniques to automatically derive both static and dynamic data alignments <ref> [KLS88, TMC89, KLS90, KN90] </ref>. More recently, researchers have also begun to study strip mining and other techniques to avoid the inefficiencies of using virtual processors [Wei91]. 6.3.2 C fl C fl [RS87] is an extension of C similar to C++ that supports SIMD data-parallel programs.
Reference: [KLS90] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <year> 1990. </year>
Reference-contexts: Relying on a single static data decomposition for these programs will result in excessive data movement <ref> [KLS90, KN90] </ref>. To overcome this problem, Fortran D permits data decomposition specifications to be inserted at any point in a program, providing dynamic data decompositions. However, to support a modular programming style, we restrict the scope of dynamically declared data decompositions to that of the current procedure. <p> This greatly simplifies the data distribution and communication generation responsibilities of the compiler, and has freed researchers to concentrate on techniques to automatically derive both static and dynamic data alignments <ref> [KLS88, TMC89, KLS90, KN90] </ref>. More recently, researchers have also begun to study strip mining and other techniques to avoid the inefficiencies of using virtual processors [Wei91]. 6.3.2 C fl C fl [RS87] is an extension of C similar to C++ that supports SIMD data-parallel programs.
Reference: [KMSB90] <author> C. Koelbel, P. Mehrotra, J. Saltz, and S. Berryman. </author> <title> Parallel loops on distributed machines. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: However, an inspector <ref> [SMC89, KMSB90] </ref> may be constructed to preprocess the loop body at run-time to determine what nonlocal data will be accessed. This in effect calculates the In index set for each processor. A global transpose operation between processors can then be used to calculate the Out index sets as well. <p> Upon receipt, they become Out index sets for the receiving processor. 3.3.3 Resulting Program Once the Send and Receive sets have been calculated, the example loop nest is transformed into the loops pictured in Figure 10 <ref> [KMSB90] </ref>. In the send loop, every processor sends data they own to processors that need the data. The Out index set for rhs of the statement in the example loop has already been calculated. <p> Parti has also motivated the development of arf, a compiler designed to interface Fortran application programs with Parti run-time routines. Arf supports BLOCK, CYCLIC, and user-defined irregular distributions, and generates inspector and executor loops for run-time preprocessing <ref> [KMSB90, WSBH91] </ref>. The goal of Arf is to demonstrate that inspector/executors can be automatically generated by the compiler. It does not currently generate messages at compile-time for regular computations. 6.4.10 Kali Kali [KMV90, MV90] is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines. <p> This is accomplished by specifying an on clause for each parallel loop. Communication is then generated automatically based on the on clause and data distributions. An inspector/executor strategy is used for run-time preprocessing of communication for irregularly distributed arrays <ref> [KMSB90] </ref>. Major differences between Kali and the Fortran D compiler include mandatory on clauses for parallel loops, support for alignment, collective communications, and dynamic decomposition. 7 Conclusions and Future Work An efficient yet usable machine-independent parallel programming model is needed to make large-scale parallel machines useful for scientific programmers.
Reference: [KMT91] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: The structure of the programming system is shown in Figure 12. It is being developed in the context of the ParaScope parallel programming environment [CCH + 88], and will take advantage of its analysis and transformation abilities <ref> [CKT86, KMT91] </ref>. Another part of the Fortran D system is a static performance estimator that will take as input a Fortran D or message-passing Fortran program and predicts its performance on the target machine [BFKK90, BFKK91]. <p> Global accumulates are also supported. 6.4.3 SUPERB Superb [ZBG88, Ger90] is a semi-automatic paralleliza-tion tool that supports arbitrary user-specified contigu 19 ous rectangular distributions. It performs dependence analysis to guide interactive program transformations in a manner similar to the ParaScope Editor <ref> [KMT91] </ref>. Superb originated the overlap concept as a means to both specify and store nonlocal data accesses. Once program analysis and transformation is complete, communication is automatically generated and blocked utilizing data dependence information. Some interproce-dural analysis is supported.
Reference: [KMV90] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory machines. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: A global transpose operation between processors can then be used to calculate the Out index sets as well. An inspector is the most general way to generate In and Out sets for loops without loop-carried dependences. Despite the expense of additional communications, experimental evidence from several systems <ref> [KMV90, WSBH91] </ref> proves that it can improve performance by blocking together communications to access nonlocal data outside of the loop nest. In addition it also allows multiple messages to the same processor to be blocked together. The Fortran D compiler plans to automatically generate inspectors where needed for irregular computations. <p> The Kali compiler, developed by Koelbel et al., utilizes this strategy for individual parallel loops by computing loop iterations that access only local data while waiting for data from other processors <ref> [KMV90] </ref>. 4.3.2 Utilize Collective Communications Li and Chen showed that the compiler can take advantage of the highly regular communication patterns displayed by many computations [LC90a]. <p> This optimization is a simple application of the "owner stores" rule proposed by Balasundaram [Bal91]. In particular, it may be desirable for the Fortran D compiler to partition loops amongst processors so that each loop iteration is executed on a single processor, such as in Kali and Arf <ref> [KMV90, WSBH91] </ref>. This technique may improve communication and provides greater control over load balance, especially for irregular computations. It also eliminates the need for individual statement masks and simplifies handling of control flow within the loop body. <p> Arf supports BLOCK, CYCLIC, and user-defined irregular distributions, and generates inspector and executor loops for run-time preprocessing [KMSB90, WSBH91]. The goal of Arf is to demonstrate that inspector/executors can be automatically generated by the compiler. It does not currently generate messages at compile-time for regular computations. 6.4.10 Kali Kali <ref> [KMV90, MV90] </ref> is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines. Programs writ 20 ten for Kali must specify a virtual processor array and assign distributed arrays to BLOCK, CYCLIC, or user-specified distributions.
Reference: [KN90] <author> K. Knobe and V. Natarajan. </author> <title> Data optimization: Minimizing residual interprocessor data motion on SIMD machines. </title> <booktitle> In Frontiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: Relying on a single static data decomposition for these programs will result in excessive data movement <ref> [KLS90, KN90] </ref>. To overcome this problem, Fortran D permits data decomposition specifications to be inserted at any point in a program, providing dynamic data decompositions. However, to support a modular programming style, we restrict the scope of dynamically declared data decompositions to that of the current procedure. <p> This greatly simplifies the data distribution and communication generation responsibilities of the compiler, and has freed researchers to concentrate on techniques to automatically derive both static and dynamic data alignments <ref> [KLS88, TMC89, KLS90, KN90] </ref>. More recently, researchers have also begun to study strip mining and other techniques to avoid the inefficiencies of using virtual processors [Wei91]. 6.3.2 C fl C fl [RS87] is an extension of C similar to C++ that supports SIMD data-parallel programs.
Reference: [LC90a] <author> J. Li and M. Chen. </author> <title> Generating explicit communication from shared-memory program references. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: et al., utilizes this strategy for individual parallel loops by computing loop iterations that access only local data while waiting for data from other processors [KMV90]. 4.3.2 Utilize Collective Communications Li and Chen showed that the compiler can take advantage of the highly regular communication patterns displayed by many computations <ref> [LC90a] </ref>. Rather than generating a large number of individual send and receive communication primitives, the compiler can instead take advantage of efficient collective communications libraries such as Express [EXP89], Crystal router [FJL + 86], Crystal communications [CCL89], and Parti [SBW90]. <p> No alignment and or distribution specifications are provided. It is not clear how Spot will support computation patterns that cannot be described by stencils, or those involving multiple arrays. 6.4 MIMD Compilation Systems 6.4.1 Crystal Crystal <ref> [CCL89, LC90b, LC90a] </ref> is a high-level functional language. The Crystal compiler targets distributed-memory machines, performing both automatic data decomposition and communications generation. Programs are first separated into phases. Each phase has a different computation structure, represented by an index domain.
Reference: [LC90b] <author> J. Li and M. Chen. </author> <title> Index domain alignment: Minimizing cost of cross-referencing between distributed arrays. </title> <booktitle> In Frontiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: No alignment and or distribution specifications are provided. It is not clear how Spot will support computation patterns that cannot be described by stencils, or those involving multiple arrays. 6.4 MIMD Compilation Systems 6.4.1 Crystal Crystal <ref> [CCL89, LC90b, LC90a] </ref> is a high-level functional language. The Crystal compiler targets distributed-memory machines, performing both automatic data decomposition and communications generation. Programs are first separated into phases. Each phase has a different computation structure, represented by an index domain.
Reference: [Lea90] <author> B. Leasure, </author> <title> editor. PCF Fortran: Language Definition, version 3.1. </title> <booktitle> The Parallel Computing Forum, </booktitle> <address> Champaign, IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: the sequential machine, Fortran 90 for the SIMD parallel machine (e.g., the TMC Connection Machine), message-passing Fortran 1 Fortran 90 SIMD message passing Fortran memory distributed MIMD Fortran 77 sequential PCF Fortran MIMD shared memory for the MIMD distributed-memory machine (e.g., the Intel iPSC/860) and Parallel Computer Forum (PCF) Fortran <ref> [Lea90] </ref> for the MIMD shared-memory machine (e.g., the BBN TC2000 Butterfly). Each of these languages seems to be a plausible candidate for use as a machine-independent parallel programming model. Research on automatic parallelization has already shown that Fortran is unsuitable for general parallel programming.
Reference: [LS91] <author> S. Lucco and O. Sharp. </author> <title> Parallel programming with coordination structures. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Orlando, FL, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: However, their suitability for scientific programming is unclear, and they also lack language or compiler support. High-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium <ref> [LS91] </ref> are valuable when used to coordinate coarse-grained functional parallelism. However, they tend to be inefficient for capturing fine-grain data parallelism of the type described by Hillis and Steele [HS86], because they lack both language and compiler support to assist in efficient data placement.
Reference: [MSMB90] <author> S. Mirchandaney, J. Saltz, P. Mehrotra, and H. Berry-man. </author> <title> A scheme for supporting automatic data migration on multicomputers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: They may be reused after the loop, or even within the loop. * Hash tables are mainly used to store nonlocal data for irregular distributions. They provide a nonlocal value cache that allows quick lookup for nonlocal values <ref> [MSMB90] </ref>. 4.4.2 Maintain Overlap Areas, Temporaries, and Hash Tables Once the type of storage is chosen, the compiler needs to perform analysis to determine the total amount of storage needed as overlaps, persistent buffers, or temporary buffers. <p> Parti is first to propose and implement user-defined irregular distributions [MSS + 88] and a hashed cache for nonlocal values <ref> [MSMB90] </ref>. Parti has also motivated the development of arf, a compiler designed to interface Fortran application programs with Parti run-time routines. Arf supports BLOCK, CYCLIC, and user-defined irregular distributions, and generates inspector and executor loops for run-time preprocessing [KMSB90, WSBH91].
Reference: [MSS + 88] <author> R. Mirchandaney, J. Saltz, R. Smith, D. Nicol, and K. Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Work is in progress to automatically generate Oxygen directives for functional and data decomposition. 6.4.9 PARTI, ARF Parti [SBW90] is a set of run-time library routines that support irregular computations on MIMD distributed-memory machines. Parti is first to propose and implement user-defined irregular distributions <ref> [MSS + 88] </ref> and a hashed cache for nonlocal values [MSMB90]. Parti has also motivated the development of arf, a compiler designed to interface Fortran application programs with Parti run-time routines. Arf supports BLOCK, CYCLIC, and user-defined irregular distributions, and generates inspector and executor loops for run-time preprocessing [KMSB90, WSBH91].
Reference: [MV90] <author> P. Mehrotra and J. Van Rosendale. </author> <title> Programming distributed memory architectures using Kali. </title> <type> ICASE Report 90-69, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: The distribution functions for decomposition A and array X are then computed through run-time preprocessing techniques <ref> [SBW90, MV90] </ref>. 3 Basic Compilation Strategy In this section we provide a formal description of the general Fortran D compiler strategy. The basic approach is to convert Fortran D programs into single-program, multiple-data (SPMD) node programs with explicit message-passing. <p> Arf supports BLOCK, CYCLIC, and user-defined irregular distributions, and generates inspector and executor loops for run-time preprocessing [KMSB90, WSBH91]. The goal of Arf is to demonstrate that inspector/executors can be automatically generated by the compiler. It does not currently generate messages at compile-time for regular computations. 6.4.10 Kali Kali <ref> [KMV90, MV90] </ref> is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines. Programs writ 20 ten for Kali must specify a virtual processor array and assign distributed arrays to BLOCK, CYCLIC, or user-specified distributions.
Reference: [PB90] <author> C. Pancake and D. Bergmark. </author> <title> Do parallel languages respond to the needs of scientific programmers? IEEE Computer, </title> <booktitle> 23(12) </booktitle> <pages> 13-23, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: We believe that specifying the data decomposition is the most important intellectual step in developing large data-parallel scientific programs. Most parallel programming languages, however, are inadequate in this regard because they only provide constructs to express functional parallelism <ref> [PB90] </ref>. Hence, we designed a language that extends Fortran by introducing constructs that specify data decompositions. We call the extended language Fortran D, for obvious reasons.
Reference: [Pri90] <author> J. Prins. </author> <title> A framework for efficient execution of array-based languages on SIMD computers. </title> <booktitle> In Frontiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: First we look at some compilation techniques, then we examine existing compilation systems. Gupta and Banerjee [GB90] propose a constraint-based approach to automatically calculate suitable data decompositions. They use simple alignments and distributions similar to those in Fortran D. Prins <ref> [Pri90] </ref> utilizes shape refinement in conjunction with linear transformations to specify data layouts and guide resulting data motion. These researchers do not discuss providing compiler support to generate communications. Some researchers concentrate on computations within loops that only involve a single array.
Reference: [PvGS90] <author> E. Paalvast, A. van Gemund, and H. Sips. </author> <title> A method for parallel program generation with an application to the Booster language. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: They also demonstrate how such programs can be optimized using transformations such as loop distribution, loop peeling, etc. The Fortran D compiler will apply many of the same transformations. booster <ref> [PvGS90] </ref> provides user-specified distribution functions defined as program views, but does not generate or optimize communications. 6.3 SIMD Compilation Systems 6.3.1 CM Fortran CM Fortran [AKLS88, TMC89] is a version of Fortran 77 extended with vector notation, alignment, and data layout specifications.
Reference: [QH90] <author> M. Quinn and P. Hatcher. </author> <title> Data parallel programming on multicomputers. </title> <journal> IEEE Software, </journal> <month> September </month> <year> 1990. </year>
Reference-contexts: Communications are automatically generated by the compiler. As with CM Fortran, virtual processors are generated for each element of a domain and mapped to each physical processor. Researchers have also examined synchronization problems when translating SIMD programs into equivalent SPMD programs, as well as several communication optimizations <ref> [QH90] </ref>. 6.3.3 DINO Dino [RSW89, RSW90, RW90] is an extended version of C supporting general-purpose distributed computation. Dino supports BLOCK, CYCLIC, and special stencil-based data distributions with overlaps, but provides no alignment specifications. A Dino program contains a virtual parallel machine declared to be an environment.
Reference: [RA90] <author> R. Ruhl and M. Annaratone. </author> <title> Parallelization of fortran code on distributed-memory parallel processors. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Nether-lands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The programmer utilizes darray declarations to mark parallel arrays. The Al compiler then applies data relations to automatically align and distribute each darray, detect parallelism, and generate communication. Only one dimension of each darray may be distributed, and computations must be linearly related. 6.4.8 Oxygen Oxygen <ref> [RA90] </ref> is a compiler for the K2 distributed-memory machine. Unlike systems discussed previously, Oxygen follows a functional rather than data decomposition strategy. Task-level parallelism is specified by labeling each parallel block of code with a p block directive.
Reference: [Ree90] <author> A. Reeves. </author> <title> The Paragon programming paradigm and distributed memory compilers. </title> <type> Technical Report EE-CEG-90-7, </type> <institution> Cornell University Computer Engineering Group, </institution> <address> Ithaca, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Special Dino language constructs are provided for reductions. Dino programs are deterministic unless special asynchronous distributed arrays are used. As with CM Fortran, Dino programs generate multiple processes per physical processor when large numbers of virtual processors are declared in the environment. 6.3.4 Paragon Paragon <ref> [CR89, Ree90] </ref> is a programming environment targeted at supporting SIMD programs on MIMD distributed-memory machines. It provides both language extensions and run-time support for task management and load balancing. Data distribution in Paragon may either be performed by the user or the system.
Reference: [RP89] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: The Fortran D compiler is designed to exploit large-scale data parallelism. Our philosophy is to use the owner computes rule, where every processor only performs computation for data it owns <ref> [ZBG88, CK88, RP89] </ref>. However, the owner compute rule is relaxed depending on the structure of the computation. However, in this paper we concentrate on deriving a functional decomposition and communication generation by applying the owner computes rule. <p> It is currently unclear whether the Crystal language can express all scientific computations. Work in progress to adapt the Crystal compiler for scientific Fortran codes will help answer this question. 6.4.2 Id Nouveau id nouveau <ref> [RP89] </ref> is a functional language extended with single assignment arrays called I-structures. User-specified BLOCK distributions are provided. The basic run-time resolution algorithm is similar to the guard and message introduction phases of the Fortran D compiler, but without any attempt to eliminate redundant guards.
Reference: [RS87] <author> J. Rose and G. Steele, Jr. </author> <title> C fl : An extended C language for data parallel programming. </title> <editor> In L. Kartashev and S. Kartashev, editors, </editor> <booktitle> Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> Santa Clara, CA, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: More recently, researchers have also begun to study strip mining and other techniques to avoid the inefficiencies of using virtual processors [Wei91]. 6.3.2 C fl C fl <ref> [RS87] </ref> is an extension of C similar to C++ that supports SIMD data-parallel programs. C fl labels data as mono (local) or poly (distributed). There are no 18 alignment or distribution specifications; the compiler automatically chooses the data decomposition.
Reference: [RS89] <author> J. Ramanujam and P. Sadayappan. </author> <title> A methodology for parallelizing programs for multicomputers and complex memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Prins [Pri90] utilizes shape refinement in conjunction with linear transformations to specify data layouts and guide resulting data motion. These researchers do not discuss providing compiler support to generate communications. Some researchers concentrate on computations within loops that only involve a single array. Ramanujam and Sadayappan <ref> [RS89] </ref> examine both the data and iteration space to derive a combined task and data partition of the loop nest. Hudak and Abraham [HA90] find a stencil-based approach useful for analyzing communications and deriving efficient rectangular or hexagonal data distributions.
Reference: [RSW89] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> Expressing complex parallel algorithms in DINO. </title> <booktitle> In Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: As with CM Fortran, virtual processors are generated for each element of a domain and mapped to each physical processor. Researchers have also examined synchronization problems when translating SIMD programs into equivalent SPMD programs, as well as several communication optimizations [QH90]. 6.3.3 DINO Dino <ref> [RSW89, RSW90, RW90] </ref> is an extended version of C supporting general-purpose distributed computation. Dino supports BLOCK, CYCLIC, and special stencil-based data distributions with overlaps, but provides no alignment specifications. A Dino program contains a virtual parallel machine declared to be an environment. Parallelism is explicitly specified by composite functions.
Reference: [RSW90] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The DINO parallel programming language. </title> <type> Technical Report CU-CS-457-90, </type> <institution> Dept. of Computer Science, University of Colorado, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: As with CM Fortran, virtual processors are generated for each element of a domain and mapped to each physical processor. Researchers have also examined synchronization problems when translating SIMD programs into equivalent SPMD programs, as well as several communication optimizations [QH90]. 6.3.3 DINO Dino <ref> [RSW89, RSW90, RW90] </ref> is an extended version of C supporting general-purpose distributed computation. Dino supports BLOCK, CYCLIC, and special stencil-based data distributions with overlaps, but provides no alignment specifications. A Dino program contains a virtual parallel machine declared to be an environment. Parallelism is explicitly specified by composite functions.
Reference: [RW90] <author> M. Rosing and R. Weaver. </author> <title> Mapping data to processors in distributed memory computations. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: As with CM Fortran, virtual processors are generated for each element of a domain and mapped to each physical processor. Researchers have also examined synchronization problems when translating SIMD programs into equivalent SPMD programs, as well as several communication optimizations [QH90]. 6.3.3 DINO Dino <ref> [RSW89, RSW90, RW90] </ref> is an extended version of C supporting general-purpose distributed computation. Dino supports BLOCK, CYCLIC, and special stencil-based data distributions with overlaps, but provides no alignment specifications. A Dino program contains a virtual parallel machine declared to be an environment. Parallelism is explicitly specified by composite functions.
Reference: [SBW90] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Multiprocessors and runtime compilation. </title> <type> ICASE Report 90-59, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: The distribution functions for decomposition A and array X are then computed through run-time preprocessing techniques <ref> [SBW90, MV90] </ref>. 3 Basic Compilation Strategy In this section we provide a formal description of the general Fortran D compiler strategy. The basic approach is to convert Fortran D programs into single-program, multiple-data (SPMD) node programs with explicit message-passing. <p> Rather than generating a large number of individual send and receive communication primitives, the compiler can instead take advantage of efficient collective communications libraries such as Express [EXP89], Crystal router [FJL + 86], Crystal communications [CCL89], and Parti <ref> [SBW90] </ref>. The compiler will exploit these routines to reduce the cost of communications. <p> The Oxygen compiler then converts Fortran code with user directives into C++ node programs with communications. Messages are inserted at points in the program called checkpoints to enforce coarse-grain synchronization. Work is in progress to automatically generate Oxygen directives for functional and data decomposition. 6.4.9 PARTI, ARF Parti <ref> [SBW90] </ref> is a set of run-time library routines that support irregular computations on MIMD distributed-memory machines. Parti is first to propose and implement user-defined irregular distributions [MSS + 88] and a hashed cache for nonlocal values [MSMB90].
Reference: [Ski90] <author> D. Skillicorn. </author> <title> Architecture-independent parallel computation. </title> <journal> IEEE Computer, </journal> <volume> 23(12), </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: Some researchers have proposed 17 Target Machine Best Message Passing Fortran 77 Node Compiler Nearby Fortran D Fortran D Compiler elegant architecture-independent programming models such as Bird-Meertens formalism <ref> [Ski90] </ref> and the Bulk-Synchronous bridging model [Val90]. However, their suitability for scientific programming is unclear, and they also lack language or compiler support. High-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] are valuable when used to coordinate coarse-grained functional parallelism.
Reference: [SMC89] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Runtime parallelization and scheduling of loops. </title> <booktitle> In Proceedings of the 1st Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Santa Fe, NM, </address> <year> 1989. </year>
Reference-contexts: However, an inspector <ref> [SMC89, KMSB90] </ref> may be constructed to preprocess the loop body at run-time to determine what nonlocal data will be accessed. This in effect calculates the In index set for each processor. A global transpose operation between processors can then be used to calculate the Out index sets as well.
Reference: [Soc90] <author> D. Socha. </author> <title> Compiling single-point iterative programs for distributed memory computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Redistribution and replication of arrays and subarrays, as well as permutation and reduction mechanism are supported. Irregular distributions and run-time preprocessing support is being planned. Paragon does not perform analysis or transformations to detect or enhance parallelism. 6.3.5 SPOT Spot <ref> [SS90, Soc90] </ref> is a point-based SIMD data-parallel programming language. Distributed arrays are defined as regions. Computations are specified from the point of view of a single element in the region, called a point. Locations relative to a given point are assigned symbolic names by neighbor declarations.
Reference: [SS90] <author> L. Snyder and D. Socha. </author> <title> An algorithm producing balanced partitionings of data arrays. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Redistribution and replication of arrays and subarrays, as well as permutation and reduction mechanism are supported. Irregular distributions and run-time preprocessing support is being planned. Paragon does not perform analysis or transformations to detect or enhance parallelism. 6.3.5 SPOT Spot <ref> [SS90, Soc90] </ref> is a point-based SIMD data-parallel programming language. Distributed arrays are defined as regions. Computations are specified from the point of view of a single element in the region, called a point. Locations relative to a given point are assigned symbolic names by neighbor declarations.
Reference: [SWW91] <author> R. Sawdayi, G. Wagenbreth, and J. Williamson. MIMDizer: </author> <title> Functional and data decomposition; creating parallel programs from scratch, transforming existing Fortran programs to parallel. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Compilers and Runtime Software for Scalable Multiprocessors. </title> <publisher> Elsevier, </publisher> <address> Amster-dam, The Netherlands, </address> <note> to appear 1991. </note>
Reference-contexts: If the loop performs regular computations on a distributed array, a micro-stencil is derived and used to generate a macro-stencil to identify communication requirements. Communications utilizing Express primitives are then automatically generated. Aspar automatically selects BLOCK distributions; no alignment or distribution specifications are provided. 6.4.5 MIMDizer Mimdizer <ref> [SWW91] </ref> is an interactive parallelization system for MIMD shared and distributed-memory machines. Based on Forge, it performs dataflow and dependence analyses and also supports loop-level transformations. Associated tools also graphically display call graph, control flow, dependence, and profiling information.
Reference: [TMC89] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM Fortran Reference Manual, version 5.2-0.6 edition, </note> <month> September </month> <year> 1989. </year>
Reference-contexts: The Fortran D compiler will apply many of the same transformations. booster [PvGS90] provides user-specified distribution functions defined as program views, but does not generate or optimize communications. 6.3 SIMD Compilation Systems 6.3.1 CM Fortran CM Fortran <ref> [AKLS88, TMC89] </ref> is a version of Fortran 77 extended with vector notation, alignment, and data layout specifications. Programmers must explicitly specify data-parallelism in cm fortran programs by marking certain array dimensions as parallel. <p> This greatly simplifies the data distribution and communication generation responsibilities of the compiler, and has freed researchers to concentrate on techniques to automatically derive both static and dynamic data alignments <ref> [KLS88, TMC89, KLS90, KN90] </ref>. More recently, researchers have also begun to study strip mining and other techniques to avoid the inefficiencies of using virtual processors [Wei91]. 6.3.2 C fl C fl [RS87] is an extension of C similar to C++ that supports SIMD data-parallel programs.
Reference: [Tse90] <author> P. S. Tseng. </author> <title> A parallelizing compiler for distributed memory parallel computers. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The compiler then outputs code in the vdm l intermediate language. Calls to the Pandore communication library to access nonlocal data is also automatically generated by the compiler. Guard introduction and communications optimization techniques are under development. 6.4.7 AL Al <ref> [Tse90] </ref> is a language designed for the Warp distributed-memory systolic processor. The programmer utilizes darray declarations to mark parallel arrays. The Al compiler then applies data relations to automatically align and distribute each darray, detect parallelism, and generate communication.
Reference: [Val90] <author> L. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Some researchers have proposed 17 Target Machine Best Message Passing Fortran 77 Node Compiler Nearby Fortran D Fortran D Compiler elegant architecture-independent programming models such as Bird-Meertens formalism [Ski90] and the Bulk-Synchronous bridging model <ref> [Val90] </ref>. However, their suitability for scientific programming is unclear, and they also lack language or compiler support. High-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] are valuable when used to coordinate coarse-grained functional parallelism.
Reference: [Wei91] <author> M. Weiss. </author> <title> Strip mining on SIMD architectures. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: More recently, researchers have also begun to study strip mining and other techniques to avoid the inefficiencies of using virtual processors <ref> [Wei91] </ref>. 6.3.2 C fl C fl [RS87] is an extension of C similar to C++ that supports SIMD data-parallel programs. C fl labels data as mono (local) or poly (distributed). There are no 18 alignment or distribution specifications; the compiler automatically chooses the data decomposition.
Reference: [Wol89] <author> M. J. Wolfe. </author> <title> Semi-automatic domain decomposition. </title> <booktitle> In Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: A major component of the success of vector supercomputers is the ability to write machine-independent vector programs in a subdialect of Fortran. Advances in compiler technology, especially automatic vectorization, have made it possible for the scientist to structure Fortran loops according the rules of "vectorizable style" <ref> [Wol89, CKK89] </ref>, which are well understood, and expect the resulting program to be compiled to efficient code on any vector machine. Compare this with the current situation for parallel machines. <p> These researchers do not discuss generating communication for these complex distributions. They also make simplifying assumptions about the effects of the underlying processor topology, and do not consider collective communications. Wolfe <ref> [Wol89, Wol90] </ref> describes transformations such as loop rotation for distributed-memory programs with simple BLOCK distributions. Callahan and Kennedy [CK88] propose methods for compiling programs with user-specified data distribution functions and using compiler inserted load and store commands to support nonlocal memory accesses.
Reference: [Wol90] <author> M. J. Wolfe. </author> <title> Loop rotation. </title> <editor> In D. Gelernter, A. Nico-lau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: These researchers do not discuss generating communication for these complex distributions. They also make simplifying assumptions about the effects of the underlying processor topology, and do not consider collective communications. Wolfe <ref> [Wol89, Wol90] </ref> describes transformations such as loop rotation for distributed-memory programs with simple BLOCK distributions. Callahan and Kennedy [CK88] propose methods for compiling programs with user-specified data distribution functions and using compiler inserted load and store commands to support nonlocal memory accesses.
Reference: [WSBH91] <author> J. Wu, J. Saltz, H. Berryman, and S. Hiranandani. </author> <title> Distributed memory compiler design for sparse problems. </title> <type> ICASE Report 91-13, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: A global transpose operation between processors can then be used to calculate the Out index sets as well. An inspector is the most general way to generate In and Out sets for loops without loop-carried dependences. Despite the expense of additional communications, experimental evidence from several systems <ref> [KMV90, WSBH91] </ref> proves that it can improve performance by blocking together communications to access nonlocal data outside of the loop nest. In addition it also allows multiple messages to the same processor to be blocked together. The Fortran D compiler plans to automatically generate inspectors where needed for irregular computations. <p> This optimization is a simple application of the "owner stores" rule proposed by Balasundaram [Bal91]. In particular, it may be desirable for the Fortran D compiler to partition loops amongst processors so that each loop iteration is executed on a single processor, such as in Kali and Arf <ref> [KMV90, WSBH91] </ref>. This technique may improve communication and provides greater control over load balance, especially for irregular computations. It also eliminates the need for individual statement masks and simplifies handling of control flow within the loop body. <p> Parti has also motivated the development of arf, a compiler designed to interface Fortran application programs with Parti run-time routines. Arf supports BLOCK, CYCLIC, and user-defined irregular distributions, and generates inspector and executor loops for run-time preprocessing <ref> [KMSB90, WSBH91] </ref>. The goal of Arf is to demonstrate that inspector/executors can be automatically generated by the compiler. It does not currently generate messages at compile-time for regular computations. 6.4.10 Kali Kali [KMV90, MV90] is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines.
Reference: [ZBG88] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 23 </month>
Reference-contexts: The Fortran D compiler is designed to exploit large-scale data parallelism. Our philosophy is to use the owner computes rule, where every processor only performs computation for data it owns <ref> [ZBG88, CK88, RP89] </ref>. However, the owner compute rule is relaxed depending on the structure of the computation. However, in this paper we concentrate on deriving a functional decomposition and communication generation by applying the owner computes rule. <p> Guard elimination is described as compile-time resolution; it is performed by calculating the set of evaluators and participants for each statement. Message presending and blocking optimizations are performed using vectorization transformations such as loop fusion and strip mining. Global accumulates are also supported. 6.4.3 SUPERB Superb <ref> [ZBG88, Ger90] </ref> is a semi-automatic paralleliza-tion tool that supports arbitrary user-specified contigu 19 ous rectangular distributions. It performs dependence analysis to guide interactive program transformations in a manner similar to the ParaScope Editor [KMT91]. Superb originated the overlap concept as a means to both specify and store nonlocal data accesses.
References-found: 67

