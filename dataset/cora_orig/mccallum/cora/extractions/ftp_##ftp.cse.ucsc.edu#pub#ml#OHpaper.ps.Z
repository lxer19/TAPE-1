URL: ftp://ftp.cse.ucsc.edu/pub/ml/OHpaper.ps.Z
Refering-URL: http://www.cse.ucsc.edu/~haussler/pubs.html
Root-URL: http://www.cse.ucsc.edu
Title: Mutual Information, Metric Entropy, and Risk in Estimation of Probability Distributions New bounds on this
Author: David Haussler Manfred Opper 
Note: Supported by NSF grant IRI-9123692. Email addresses: haussler@cse.ucsc.edu Supported by Heisenberg fellowship of DFG. Email addresses: opper@physik.uni-wuerzburg.de  
Address: Santa Cruz, CA 96064  
Date: December 29, 1996  
Affiliation: UC Santa Cruz  Universitat Wurzburg  UC  
Abstract: University of California Technical Report UCSC-CRL-96-27 Baskin Center for Computer Science and Computer Engineering This is a revision of the Dec. 27, 1995 version of this paper. An abbreviated version of this paper is to appear in Annals of Statistics. Abstract Assume fP : 2 fig is a set of probability distributions with a common dominating measure on a complete separable metric space Y . A state fl 2 fi is chosen by Nature. A statistician gets n independent observations Y 1 ; : : : ; Y n from Y distributed according to P fl . For each time t between 1 and n, based on the observations Y 1 ; : : : ; Y t1 , the statistician produces an estimated distribution ^ P t for P fl , and suffers a loss L(P fl ; ^ P t ). The cumulative risk for the statistician is the average total loss up to time n. Of special interest in information theory, data compression, mathematical finance, computational learning theory and statistical mechanics is the special case when the loss L(P fl ; ^ P t ) is the relative entropy between the true distribution P fl and the estimated distribution ^ P t . Here the cumulative Bayes risk from time 1 to n is the mutual information between the random parameter fi fl and the observations Y 1 ; : : : ; Y n . 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Barron and L. Gyorfi and E. van der Meulen. </author> <title> Distribution estimation consistent in total variation and in two types of information divergence. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38 </volume> <pages> 1437-1454, </pages> <year> 1992. </year>
Reference-contexts: All logarithms are natural logarithms unless otherwise specified. We assume throughout that 0 log 0 = 0 log x 0 = 0, where x is any nonnegative finite number. We will also employ functions taking values in the extended reals <ref> [1; +1] </ref>, and and in particular use the extended log function obtained by defining log 0 = 1 and log 1 = 1. Expectations over extended real-valued functions are defined whenever they do not take both the value +1 with positive probability and the value 1 with positive probability. <p> This allows the statistician, if necessary, to use predictive distributions that are not in the set fP : 2 fig. Let L be a function that maps from pairs of distributions on Y into <ref> [0; 1] </ref>. We call L the loss function. <p> Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. [6, 10, 52, 59, 55]. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in <ref> [1, 41] </ref>. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. [24, 38]). <p> It can be shown that for any fi and , f fi; () is a nondecreasing function on <ref> [0; 1) taking values in [0; 1] </ref>, and if f fi; () is finite for any &gt; 0, then lim f fi; () = f ; (0): To verify this last property, note that lim !0 R Bayes 1;; 1+ = 1. <p> It can be shown that for any fi and , f fi; () is a nondecreasing function on [0; 1) taking values in <ref> [0; 1] </ref>, and if f fi; () is finite for any &gt; 0, then lim f fi; () = f ; (0): To verify this last property, note that lim !0 R Bayes 1;; 1+ = 1. <p> and use the fact that for Gaussian processes and c &gt; 0 fi Z d ( ~ )e c R 1 2 1 X " c k # 21 Here k , k = 1; 2; : : : ; 1 are the eigenvalues of the process on the interval <ref> [0; 1] </ref>. <p> Let us assume that the statistician observes a set of n observations y 1 ; : : : ; y n which are drawn independently from a density dP (y), 2 fi on the interval <ref> [0; 1] </ref>. <p> Lipschitz class F p;ff (C; L) of densities satisfying sup y2 [0;1] jdP (y)j C and having derivatives dP (k) (y) of order k p with the Lipschitz condition on the p- th derivative jdP (y) dP (y 0 )j Ljy y 0 j ff for y; y 0 2 <ref> [0; 1] </ref>. Since the functions in F p;ff (C 1 ; L) are uniformly bounded, they have an integrable envelope function, and hence R minimax 1; 1+ &lt; 1 for all &gt; 0.
Reference: [2] <author> S. Amari. </author> <title> Differential geometry of curved exponential families-curvatures and information loss. </title> <journal> Annals of Statistics, </journal> <volume> 10 </volume> <pages> 357-385, </pages> <year> 1982. </year>
Reference-contexts: Amari has developed an extensive theory that relates the risk when fl is the true state of Nature to certain differential-geometric properties of the parameter space fi in the neighborhood of fl involving Fisher information and related quantities <ref> [2, 3] </ref> (see also [60, 40]). Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. [6, 10, 52, 59, 55].
Reference: [3] <author> S. Amari and N. Murata. </author> <title> Statistical theory of learning curves under entropic loss. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 140-153, </pages> <year> 1993. </year>
Reference-contexts: Amari has developed an extensive theory that relates the risk when fl is the true state of Nature to certain differential-geometric properties of the parameter space fi in the neighborhood of fl involving Fisher information and related quantities <ref> [2, 3] </ref> (see also [60, 40]). Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. [6, 10, 52, 59, 55].
Reference: [4] <author> A. Barron. </author> <title> The strong ergodic theorem for densities: generalized Shannon-McMillan-Breiman theorem. </title> <journal> The Annals of Probability, </journal> <volume> 13 </volume> <pages> 1292-1303, </pages> <year> 1985. </year>
Reference-contexts: More general results, including the above corollary, follow from results in Pinsker's book [47] (see also <ref> [4] </ref>). Applying Theorem (1) and taking the supremum over in Corollary (2), it follows that if fi is finite then for all n, R minimax n log jfij and lim n!1 R minimax n = log jfij.
Reference: [5] <author> A. Barron. In T. M. Cover and B. Gopinath, </author> <title> editors, Open Problems in Communication and Computation, chapter 3.20. Are Bayes rules consistent in information?, </title> <address> pages 85-91. </address> <year> 1987. </year>
Reference-contexts: Since maximin minimax always, we have sup R Bayes 1;; 1+ R minimax 1; 1+ , and from this we obtain the result stated in the Theorem. 2 The method used in obtaining the upper bound in the above result is a familiar one (see e.g. <ref> [5, 34] </ref>). <p> While it is easily verified that R Bayes n; = t=1 r Bayes t; , the exact relationship between the instantaneous and cumulative minimax risks is less clear. However, Barron et al. <ref> [5, 18, 10] </ref> have shown the following. <p> While it is easily verified that R Bayes n; = t=1 r Bayes t; , the exact relationship between the instantaneous and cumulative minimax risks is less clear. However, Barron et al. [5, 18, 10] have shown the following. Lemma 9 <ref> [5] </ref> n X r minimax t R minimax n nr minimax n Proof: For the first inequality, simply note that n X r minimax t = t=1 ^ P fl 2fi Y t1 fl D KL (P fl jj ^ P t ) ^ P t=1 fl 2fi Y t1 fl
Reference: [6] <author> A. Barron. </author> <title> The exponential convergence of posterior probabilities with implications for Bayes estimators of density functions. </title> <type> Technical Report 7, </type> <institution> Dept. of Statistics, U. Ill. Urbana-Champaign, </institution> <year> 1987. </year>
Reference-contexts: Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. <ref> [6, 10, 52, 59, 55] </ref>. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 41]. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. [24, 38]). <p> fi d ( ~ )e n (1ff)I ff (P fljjP ~ ) fl R n;P Bayes ( fl ) = D KL (P n log fi d ( ~ )e nI 1 (P fljjQ ~ ) + fl: The upper bound of part (1.) is similar to results given in <ref> [6] </ref>, and is mentioned there for the case P = Q. To the best of our knowledge, the lower bound, and the results in part (2.), are new. The proof is given in a series of lemmas and calculations.
Reference: [7] <author> A. Barron, B. Clarke, and D. Haussler. </author> <title> Information bounds for the risk of bayesian predictions and the redundancy of universal codes. </title> <booktitle> In Proc. International Symposium on Information Theory. </booktitle>
Reference-contexts: These results were extended to the Bayes and minimax risk in [19] (see also <ref> [7] </ref>). Related lower bounds, which are often quoted, were obtained by Rissanen [51], based on certain asymptotic normality assumptions. Further extensions of this work are given by 7 Yamanishi [56, 58, 57].
Reference: [8] <author> A. Barron, B. Clarke, and D. Haussler. </author> <title> Information bounds for the risk of bayesian predictions and the redundancy of universal codes. </title> <booktitle> In Proc. International Symposium on Information Theory, </booktitle> <month> Jan. </month> <year> 1993. </year> <month> 43 </month>
Reference-contexts: This provides a beautiful connection between information theory and statistics. This connection also extends to other fields, as is discussed in <ref> [18, 8] </ref>. In data compression, the cumulative relative entropy risk is the redundancy, which is the expected excess code length for the best adaptive coding method, as compared to the best coding method that has prior knowledge of the true distribution [18, 41, 44].
Reference: [9] <author> A. Barron and T. </author> <title> Cover. A bound on the financial value of information. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> 34 </volume> <pages> 1097-1100, </pages> <year> 1988. </year>
Reference-contexts: The minimax risk is called "information" channel capacity [21], p. 184. In mathematical finance and gambling theory, the cumulative relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution <ref> [9, 18] </ref>. In computational learning theory, 2 this risk is the average additional loss suffered by an adaptive algorithm that predicts each observation before it arrives, based on the previous observations, as compared to an algorithm that makes predictions knowing the true distribution [34, 35].
Reference: [10] <author> A. Barron and Y. Yang. </author> <title> Information theoretic lower bounds on convergence rates of nonparametric estimators, 1995. </title> <type> unpublished manuscript. </type>
Reference-contexts: Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. <ref> [6, 10, 52, 59, 55] </ref>. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 41]. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. [24, 38]). <p> While this work is too extensive to summarize here, we do note that some authors have also taken the general approach that we take here in using notions of metric entropy (defined below), and specifically using the Hellinger distance in obtaining these bounds (e.g. <ref> [42, 11, 12, 32, 54, 13, 10] </ref>). The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [55] (see Corollary 1, p. 360) and Barron and Yang [10]. <p> The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [55] (see Corollary 1, p. 360) and Barron and Yang <ref> [10] </ref>. This work is somewhat complementary to ours, in that it treats instantaneous risk, whereas we focus on cumulative risk. <p> Different assumptions, and different methods (using Fano's inequality) are used to obtain related general results in <ref> [10] </ref>. In this paper we describe a new approach, employing the Hellinger metric and certain Laplace integrals, to bounding both the Bayes and minimax risks for the cumulative relative entropy loss, and the instantaneous minimax risk for all three losses mentioned above. <p> The method for obtaining the lower bound by choosing a discrete prior on a well-separated set of is also similar in many respects to standard lower bound methods, such as those that use Fano's inequality or Assouad's lemma (see e.g. <ref> [12, 10, 59] </ref>), but the 27 method is particularly clean in the present framework, giving a fairly good match to the upper bound. <p> One way to do this is to use Fano's inequality, as described in <ref> [10] </ref>. Here we look at a simple alternate approach. Recall that the instantaneous minimax risk at time t is denoted r minimax t;L = inf sup Z dP t1 for loss function L. <p> While it is easily verified that R Bayes n; = t=1 r Bayes t; , the exact relationship between the instantaneous and cumulative minimax risks is less clear. However, Barron et al. <ref> [5, 18, 10] </ref> have shown the following. <p> Furthermore, since the functions in F p;ff (C 1 ; L) are uniformly bounded, all L q distances (q 1) are equivalent. As shown by Barron and Yang <ref> [10] </ref>, a further restriction to uniformly lower bounded densities also insures that the condition lim *!0 b (*) &lt; 1 holds, and makes the Hellinger distance equivalent to the L q distances, without changing the metric entropy asymptotically. <p> Sharper results are known (see e.g. [12, 32]). Since the metric entropies are known for many interesting classes of functions, many more examples of this type are possible. Many such examples are given by Birge [11, 12] and Barron and Yang <ref> [10] </ref>. 10 Discussion We have shown that under relatively weak assumptions, (in particular, whenever there exists a distribution U and a &gt; 0 such that the (1 + )-affinity between P and U is uniformly bounded for all 2 fi) one can obtain explicit bounds on the mutual information I (fi
Reference: [11] <author> L. Birge. </author> <title> Approximation dans les espaces metriques et theorie de l'estimation. </title> <journal> Zeitschrift fuer Wahrscheinlichkeitstheorie und Verwandte Gebiete, </journal> <volume> 65 </volume> <pages> 181-237, </pages> <year> 1983. </year>
Reference-contexts: These results are then used further to obtain similar bounds for the risk under other loss functions, including the Hellinger and L 1 distance. Here the results are not as sharp as one can obtain by other methods, such as those of Le Cam [15, 42], Birge <ref> [11, 12] </ref>, Hasminskii and Ibragimov [32], and Wong and Shen [55], but these applications nevertheless illustrate the general utility of the method. Finally, we discuss some possible further work in section 10. 2 Basic definitions, notation and assumptions The following notation and assumptions will be used throughout the paper. <p> While this work is too extensive to summarize here, we do note that some authors have also taken the general approach that we take here in using notions of metric entropy (defined below), and specifically using the Hellinger distance in obtaining these bounds (e.g. <ref> [42, 11, 12, 32, 54, 13, 10] </ref>). The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [55] (see Corollary 1, p. 360) and Barron and Yang [10]. <p> This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including Le Cam [42], Birge <ref> [11, 12] </ref>, Hasminskii and Ibragimov [32], and van de Geer [54]. 9 4.1 Basic bounds Our main theorem gives bounds on I (fi fl ; Y n ) and D KL (P n fl jjM n; ) in terms of the logarithms of two Laplace transforms of the I divergence, one <p> Since I ff (P jjQ) D ff (P; Q), the lower bounds follow directly from the lower bounds of Theorem 2. For the upper bounds, we will need the following lemma, which is a simple extension of Lemma 4.4 of <ref> [11] </ref>. Lemma 4 For any distributions P and Q on Y and any 0 &lt; ff &lt; 1, D KL (P jjQ) sup b ff dQ (y) !! Proof. <p> This general approach was developed by Le Cam and Birge <ref> [42, 11, 12] </ref>. We illustrate it by applying the above lemma to all of the standard cases for the asymptotic growth rate of the metric entropy K * (fi; h). Theorem 4 Assume there exists &gt; 0 such that R minimax 1; 1+ &lt; 1. 5 1. <p> Sharper results are known (see e.g. [12, 32]). Since the metric entropies are known for many interesting classes of functions, many more examples of this type are possible. Many such examples are given by Birge <ref> [11, 12] </ref> and Barron and Yang [10]. 10 Discussion We have shown that under relatively weak assumptions, (in particular, whenever there exists a distribution U and a &gt; 0 such that the (1 + )-affinity between P and U is uniformly bounded for all 2 fi) one can obtain explicit bounds
Reference: [12] <author> L. Birge. </author> <title> On estimating a density using Hellinger distance and some other strange facts. Probability theory and related fields, </title> <booktitle> 71 </booktitle> <pages> 271-291, </pages> <year> 1986. </year>
Reference-contexts: These results are then used further to obtain similar bounds for the risk under other loss functions, including the Hellinger and L 1 distance. Here the results are not as sharp as one can obtain by other methods, such as those of Le Cam [15, 42], Birge <ref> [11, 12] </ref>, Hasminskii and Ibragimov [32], and Wong and Shen [55], but these applications nevertheless illustrate the general utility of the method. Finally, we discuss some possible further work in section 10. 2 Basic definitions, notation and assumptions The following notation and assumptions will be used throughout the paper. <p> While this work is too extensive to summarize here, we do note that some authors have also taken the general approach that we take here in using notions of metric entropy (defined below), and specifically using the Hellinger distance in obtaining these bounds (e.g. <ref> [42, 11, 12, 32, 54, 13, 10] </ref>). The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [55] (see Corollary 1, p. 360) and Barron and Yang [10]. <p> This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including Le Cam [42], Birge <ref> [11, 12] </ref>, Hasminskii and Ibragimov [32], and van de Geer [54]. 9 4.1 Basic bounds Our main theorem gives bounds on I (fi fl ; Y n ) and D KL (P n fl jjM n; ) in terms of the logarithms of two Laplace transforms of the I divergence, one <p> The method for obtaining the lower bound by choosing a discrete prior on a well-separated set of is also similar in many respects to standard lower bound methods, such as those that use Fano's inequality or Assouad's lemma (see e.g. <ref> [12, 10, 59] </ref>), but the 27 method is particularly clean in the present framework, giving a fairly good match to the upper bound. <p> This general approach was developed by Le Cam and Birge <ref> [42, 11, 12] </ref>. We illustrate it by applying the above lemma to all of the standard cases for the asymptotic growth rate of the metric entropy K * (fi; h). Theorem 4 Assume there exists &gt; 0 such that R minimax 1; 1+ &lt; 1. 5 1. <p> Sharper results are known (see e.g. <ref> [12, 32] </ref>). Since the metric entropies are known for many interesting classes of functions, many more examples of this type are possible. <p> Sharper results are known (see e.g. [12, 32]). Since the metric entropies are known for many interesting classes of functions, many more examples of this type are possible. Many such examples are given by Birge <ref> [11, 12] </ref> and Barron and Yang [10]. 10 Discussion We have shown that under relatively weak assumptions, (in particular, whenever there exists a distribution U and a &gt; 0 such that the (1 + )-affinity between P and U is uniformly bounded for all 2 fi) one can obtain explicit bounds
Reference: [13] <author> L. Birge and P. Massart. </author> <title> Rates of convergence for minimum contrast estimators. Probability Theory and Related Fields, </title> <booktitle> 97 </booktitle> <pages> 113-150, </pages> <year> 1993. </year>
Reference-contexts: While this work is too extensive to summarize here, we do note that some authors have also taken the general approach that we take here in using notions of metric entropy (defined below), and specifically using the Hellinger distance in obtaining these bounds (e.g. <ref> [42, 11, 12, 32, 54, 13, 10] </ref>). The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [55] (see Corollary 1, p. 360) and Barron and Yang [10]. <p> These are the the packing and covering numbers, and the associated metric entropy, introduced by Kolmogorov and Tikhomirov in [39] and commonly used in the theory of empirical processes (see e.g. <ref> [26, 48, 31, 13] </ref>). For the following definitions, let (S; ) be any complete separable metric space. Definition 1 (Metric entropy, also called Kolmogorov *-entropy [39]) A partition of S is a collection f i g of Borel subsets of S that are pairwise disjoint and whose union is S.
Reference: [14] <author> L. L. </author> <title> Cam. An extension of Wald's theory of statistical decision functions. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 26 </volume> <pages> 69-81, </pages> <year> 1955. </year>
Reference-contexts: This result can be obtained with limited effort from the general results in an early paper of Le Cam <ref> [14] </ref>. Special cases of the result were derived by Gallager [29] and Davisson and Leon-Garcia [23], and the general result is given in [33].
Reference: [15] <author> L. L. </author> <title> Cam. Convergence of estimates under dimensionality restrictions. </title> <journal> Annals of Statistics, </journal> <volume> 1 </volume> <pages> 38-53, </pages> <year> 1973. </year>
Reference-contexts: These results are then used further to obtain similar bounds for the risk under other loss functions, including the Hellinger and L 1 distance. Here the results are not as sharp as one can obtain by other methods, such as those of Le Cam <ref> [15, 42] </ref>, Birge [11, 12], Hasminskii and Ibragimov [32], and Wong and Shen [55], but these applications nevertheless illustrate the general utility of the method.
Reference: [16] <author> R. H. Cameron and W. T. Martin. </author> <title> Transformation of wiener integrals under translations. </title> <journal> Ann. Math., </journal> <volume> 45 </volume> <pages> 386-396, </pages> <year> 1944. </year>
Reference-contexts: In this case, it is easy to calculate the I-divergences explicitly for all ff. Let P be the measure corresponding to the random process Y (x) and let the dominating measure - be the Wiener measure. Then, from the Cameron-Martin formula <ref> [16] </ref>, the Radon-Nikodym derivative is found to be dP = exp [ 0 1 Z 1 2 (x)dx ]: (10) Inserting this into the definition of the I-divergences, we obtain I ff (P fl jjP ) = 2 2 0 2 For the case where the prior over the space of
Reference: [17] <author> B. Clarke. </author> <title> Asymptotic cumulative risk and Bayes risk under entropy loss with applications. </title> <type> PhD thesis, </type> <institution> Dept. of Statistics, University of Ill., </institution> <year> 1989. </year>
Reference-contexts: In this case they were even able to estimate the lower order additive terms in this approximation, which involve the Fisher information and the entropy of the prior. Further related results were given by Efroimovich [27] and Clarke <ref> [17] </ref>.
Reference: [18] <author> B. Clarke and A. Barron. </author> <title> Information-theoretic asymptotics of Bayes methods. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 36(3) </volume> <pages> 453-471, </pages> <year> 1990. </year>
Reference-contexts: For cumulative relative entropy loss, the Bayes risk has a fundamental information theoretic interpretation: it is the mutual information between a random variable representing the choice of the parameter fl of the true distribution, and the random variable given by the n observations <ref> [37, 27, 18] </ref>. This provides a beautiful connection between information theory and statistics. This connection also extends to other fields, as is discussed in [18, 8]. <p> This provides a beautiful connection between information theory and statistics. This connection also extends to other fields, as is discussed in <ref> [18, 8] </ref>. In data compression, the cumulative relative entropy risk is the redundancy, which is the expected excess code length for the best adaptive coding method, as compared to the best coding method that has prior knowledge of the true distribution [18, 41, 44]. <p> In data compression, the cumulative relative entropy risk is the redundancy, which is the expected excess code length for the best adaptive coding method, as compared to the best coding method that has prior knowledge of the true distribution <ref> [18, 41, 44] </ref>. The minimax risk is called "information" channel capacity [21], p. 184. In mathematical finance and gambling theory, the cumulative relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution [9, 18]. <p> The minimax risk is called "information" channel capacity [21], p. 184. In mathematical finance and gambling theory, the cumulative relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution <ref> [9, 18] </ref>. In computational learning theory, 2 this risk is the average additional loss suffered by an adaptive algorithm that predicts each observation before it arrives, based on the previous observations, as compared to an algorithm that makes predictions knowing the true distribution [34, 35]. <p> The bounds are also fairly tight. However, in smooth parametric cases, our general bounds are too crude to give the precise estimates of the low order additive constants that were obtained by Clarke and Barron <ref> [18, 19] </ref>. The paper is organized as follows. In sections 2 and 3 we give precise definitions of the risks that we evaluate, and discuss the conditions required for our bounds to hold. Here we also compare our bounds to those obtained previously by other authors. <p> Further related results were given by Efroimovich [27] and Clarke [17]. Clarke and Barron gave a detailed analysis, with applications, of the risk of the Bayes strategy as a function of the true state of Nature <ref> [18] </ref>, discussing the relation of the Bayes risk to the notion of redundancy in information theory, and giving applications to hypothesis testing and portfolio selection theory. These results were extended to the Bayes and minimax risk in [19] (see also [7]). <p> In this case, for large n, both bounds differ by a constant approximately equal to D log 4 2 for small fl. In this classical case, Clarke and Barron <ref> [18] </ref> have determined the exact answer to within o (1), and it is R n;P Bayes ( fl ) = D log 2 1 log detJ ( fl ) D + o (1): Thus our simpler methods do not give the best known additive constants in the bounds for this classical <p> As pointed out by Clarke and Barron <ref> [18] </ref>, the scaling ~ D 2 log n of the Bayes risk for the smooth parametric families is strongly related to the asymptotic normality of the properly normalized posterior distribution. <p> While it is easily verified that R Bayes n; = t=1 r Bayes t; , the exact relationship between the instantaneous and cumulative minimax risks is less clear. However, Barron et al. <ref> [5, 18, 10] </ref> have shown the following.
Reference: [19] <author> B. Clarke and A. Barron. </author> <title> Jefferys' prior is asymptotically least favorable under entropy risk. </title> <journal> J. Statistical Planning and Inference, </journal> <volume> 41 </volume> <pages> 37-60, </pages> <year> 1994. </year>
Reference-contexts: The bounds are also fairly tight. However, in smooth parametric cases, our general bounds are too crude to give the precise estimates of the low order additive constants that were obtained by Clarke and Barron <ref> [18, 19] </ref>. The paper is organized as follows. In sections 2 and 3 we give precise definitions of the risks that we evaluate, and discuss the conditions required for our bounds to hold. Here we also compare our bounds to those obtained previously by other authors. <p> These results were extended to the Bayes and minimax risk in <ref> [19] </ref> (see also [7]). Related lower bounds, which are often quoted, were obtained by Rissanen [51], based on certain asymptotic normality assumptions. Further extensions of this work are given by 7 Yamanishi [56, 58, 57]. <p> 2 with ff = 1=2 and Fatou's lemma lim inf I (fi fl ; Y n ) lim inf i X ( j )e 2 D 2 i n!1 X ( j )e 2 D 2 = i = H (fi fl ): This result generalizes the similar result in <ref> [19] </ref> (Corollary 1) by removing the additional conditions assumed there. More general results, including the above corollary, follow from results in Pinsker's book [47] (see also [4]).
Reference: [20] <author> G. F. Clements. </author> <title> Entropy of several sets of real-valued functions. </title> <journal> Pacific J. Math., </journal> <volume> 13 </volume> <pages> 1085-1095, </pages> <year> 1963. </year>
Reference-contexts: As shown by Barron and Yang [10], a further restriction to uniformly lower bounded densities also insures that the condition lim *!0 b (*) &lt; 1 holds, and makes the Hellinger distance equivalent to the L q distances, without changing the metric entropy asymptotically. By a result of Clements <ref> [20] </ref>, the metric entropy of fi under L 1 distance is given by K * (fi; L 1 ) * p+ff : Hence K * (fi; h) * p+ff : Thus from Theorem 5 we get R minimax n n 2 (p+ff)+1 , and from Theorem 7 we get that the
Reference: [21] <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: The minimax risk is called "information" channel capacity <ref> [21] </ref>, p. 184. In mathematical finance and gambling theory, the cumulative relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution [9, 18]. <p> Then R n; ^ P ( fl ) = t=1 Y t1 fl (y t1 ) Y dP fl (y t ) = D KL (P n by the chain rule for relative entropy (see e.g. <ref> [21] </ref>, p. 23). Of course the statistician seeks a strategy that minimizes risk. One approach is to assume that Nature is a strategic adversary, and hence selects the worst case fl for any particular strategy of the statistician. <p> log d ^ P It follows that the (cumulative) Bayes risk for relative entropy loss is given by R Bayes n; = fi fl jjM n; ) = I (fi fl ; Y n ); the mutual information between the parameter fi fl and the observations Y n . (See <ref> [21] </ref>, p. 18, for general definition and discussion of the mutual information.) It also turns out that for relative entropy loss, there is a simple, universal relationship between the Bayes risk R Bayes n; and the minimax risk R minimax n . <p> The last equality follows from the fact that the KL divergence is additive over the product of independent distributions (see e.g. <ref> [21] </ref>, p. 23). <p> Note that this quantity is nonnegative. When H (fi) is finite it is easily verified that I (fi fl ; Y n ) = H (fi fl ) H (fi fl jY n ) (see e.g. <ref> [21] </ref>, p. 20), and thus lim sup n!1 I (fi fl ; Y n ) H (fi fl ) in this case as well. <p> Here the inequality follows from Jensen's inequality and the next equality from the chain rule for relative entropy (see e.g. <ref> [21] </ref>, p. 23).
Reference: [22] <author> I. Csiszar. </author> <title> On the topological properties of f -divergences. </title> <journal> Studia Scientiarium Math. Hungarica, </journal> <volume> 2 </volume> <pages> 329-339, </pages> <year> 1967. </year>
Reference-contexts: &gt; 1, defined by L (P; Q) = ff (P; Q) = (dP ) ff (dQ) 1ff : All of these loss functions are in the family of functions investigated by Csiszar, known as f -divergences, and all f -divergences are easily seen to be independent of the dominating measure <ref> [22] </ref>. The bulk of the paper is devoted to the relative entropy loss, so this loss is assumed unless otherwise specified.
Reference: [23] <author> L. Davisson and A. Leon-Garcia. </author> <title> A source matching approach to finding minimax codes. </title> <journal> IEEE transactions on information theory, </journal> <volume> IT-26:166-174, </volume> <year> 1980. </year>
Reference-contexts: This result can be obtained with limited effort from the general results in an early paper of Le Cam [14]. Special cases of the result were derived by Gallager [29] and Davisson and Leon-Garcia <ref> [23] </ref>, and the general result is given in [33]. Theorem 1 [33] R minimax n = sup R Bayes n; ; where the supremum is taken over all (Borel) probability measures on the parameter space fi.
Reference: [24] <author> L. Devroye and L. Gyorfi. </author> <title> Nonparametric density estimation, the L 1 view. </title> <publisher> Wiley, </publisher> <year> 1985. </year>
Reference-contexts: Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 41]. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. <ref> [24, 38] </ref>).
Reference: [25] <author> P. Diaconis and D. Freedman. </author> <title> On the consistency of Bayes estimates. </title> <journal> Ann. Statist., </journal> <volume> 14 </volume> <pages> 1-26, </pages> <year> 1986. </year>
Reference-contexts: All prior distributions on fi used in this paper are assumed to be Borel distributions of this type, and suprema over priors are also assumed to be only with respect to Borel distributions of this type. Further discussion of these issues can be found in the appendix of <ref> [25] </ref>. Finally, for integer or real-valued functions f and g, we say f ~ g if lim n!1 f (n) f g if lim inf n!1 g (n) &gt; 0 and lim sup n!1 g (n) &lt; 1. All logarithms are natural logarithms unless otherwise specified.
Reference: [26] <author> R. M. Dudley. </author> <title> A course on empirical processes. </title> <booktitle> Lecture Notes in Mathematics, </booktitle> <address> 1097:2--142, </address> <year> 1984. </year>
Reference-contexts: These are the the packing and covering numbers, and the associated metric entropy, introduced by Kolmogorov and Tikhomirov in [39] and commonly used in the theory of empirical processes (see e.g. <ref> [26, 48, 31, 13] </ref>). For the following definitions, let (S; ) be any complete separable metric space. Definition 1 (Metric entropy, also called Kolmogorov *-entropy [39]) A partition of S is a collection f i g of Borel subsets of S that are pairwise disjoint and whose union is S.
Reference: [27] <author> S. Y. Efroimovich. </author> <title> Information contained in a sequence of observations. Problems in Information Transmission, </title> <booktitle> 15 </booktitle> <pages> 178-189, </pages> <year> 1980. </year>
Reference-contexts: For cumulative relative entropy loss, the Bayes risk has a fundamental information theoretic interpretation: it is the mutual information between a random variable representing the choice of the parameter fl of the true distribution, and the random variable given by the n observations <ref> [37, 27, 18] </ref>. This provides a beautiful connection between information theory and statistics. This connection also extends to other fields, as is discussed in [18, 8]. <p> In this case they were even able to estimate the lower order additive terms in this approximation, which involve the Fisher information and the entropy of the prior. Further related results were given by Efroimovich <ref> [27] </ref> and Clarke [17].
Reference: [28] <author> M. Feder, Y. Freund, and Y. Mansour. </author> <title> Optimal universal learning and prediction of probabilistic concepts. </title> <booktitle> In Proc. of IEEE Information Theory Conference, </booktitle> <pages> page 233. </pages> <publisher> IEEE, </publisher> <year> 1995. </year>
Reference-contexts: Some initial results along these lines can be found in [46, 36] (see also <ref> [28, 43] </ref>). There are also several other directions for further research one might pursue.
Reference: [29] <author> R. Gallager. </author> <title> Source coding with side information and universal coding. </title> <type> Technical Report LIDS-P-937, </type> <institution> MIT Laboratory for Information and Decision Systems, </institution> <year> 1979. </year>
Reference-contexts: This result can be obtained with limited effort from the general results in an early paper of Le Cam [14]. Special cases of the result were derived by Gallager <ref> [29] </ref> and Davisson and Leon-Garcia [23], and the general result is given in [33]. Theorem 1 [33] R minimax n = sup R Bayes n; ; where the supremum is taken over all (Borel) probability measures on the parameter space fi.
Reference: [30] <author> J. Ghosh, S. Ghosal, and T. Samanta. </author> <title> Statistical decision theory and related topics v. </title> <editor> In S. Gupta and J. O. Berger, editors, </editor> <title> Stability and Convergence of the Posterior in Non-Regular Problems. </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: It is interesting to look at nonregular families of probabilities, for which the posterior fails to converge to a nontrivial limit. (For conditions that are necessary for convergence, see <ref> [30] </ref>). As an example for such nonsmooth densities, we study the following simple family on R dP (y) = e (y) I fy&gt;g ; 2 R: (8) Obviously, D KL (P fl jjP ) = 1, whenever &gt; fl and the Fisher information does not exist for any .
Reference: [31] <author> E. Gine and J. Zinn. </author> <title> Some limit theorems for empirical processes. </title> <journal> Annals of Probability, </journal> <volume> 12 </volume> <pages> 929-989, </pages> <year> 1984. </year>
Reference-contexts: These are the the packing and covering numbers, and the associated metric entropy, introduced by Kolmogorov and Tikhomirov in [39] and commonly used in the theory of empirical processes (see e.g. <ref> [26, 48, 31, 13] </ref>). For the following definitions, let (S; ) be any complete separable metric space. Definition 1 (Metric entropy, also called Kolmogorov *-entropy [39]) A partition of S is a collection f i g of Borel subsets of S that are pairwise disjoint and whose union is S.
Reference: [32] <author> R. Hasminskii and I. Ibragimov. </author> <title> On density estimation in the view of Kolmogorov's ideas in approximation theory. </title> <journal> Annals of statistics, </journal> <volume> 18 </volume> <pages> 999-1010, </pages> <year> 1990. </year>
Reference-contexts: Here the results are not as sharp as one can obtain by other methods, such as those of Le Cam [15, 42], Birge [11, 12], Hasminskii and Ibragimov <ref> [32] </ref>, and Wong and Shen [55], but these applications nevertheless illustrate the general utility of the method. Finally, we discuss some possible further work in section 10. 2 Basic definitions, notation and assumptions The following notation and assumptions will be used throughout the paper. <p> While this work is too extensive to summarize here, we do note that some authors have also taken the general approach that we take here in using notions of metric entropy (defined below), and specifically using the Hellinger distance in obtaining these bounds (e.g. <ref> [42, 11, 12, 32, 54, 13, 10] </ref>). The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [55] (see Corollary 1, p. 360) and Barron and Yang [10]. <p> This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including Le Cam [42], Birge [11, 12], Hasminskii and Ibragimov <ref> [32] </ref>, and van de Geer [54]. 9 4.1 Basic bounds Our main theorem gives bounds on I (fi fl ; Y n ) and D KL (P n fl jjM n; ) in terms of the logarithms of two Laplace transforms of the I divergence, one at the value ff = <p> Sharper results are known (see e.g. <ref> [12, 32] </ref>). Since the metric entropies are known for many interesting classes of functions, many more examples of this type are possible.
Reference: [33] <author> D. Haussler. </author> <title> A general minimax result for relative entropy. </title> <journal> IEEE Trans. Info Th., </journal> <note> 1997. to appear. </note>
Reference-contexts: This result can be obtained with limited effort from the general results in an early paper of Le Cam [14]. Special cases of the result were derived by Gallager [29] and Davisson and Leon-Garcia [23], and the general result is given in <ref> [33] </ref>. Theorem 1 [33] R minimax n = sup R Bayes n; ; where the supremum is taken over all (Borel) probability measures on the parameter space fi. <p> This result can be obtained with limited effort from the general results in an early paper of Le Cam [14]. Special cases of the result were derived by Gallager [29] and Davisson and Leon-Garcia [23], and the general result is given in <ref> [33] </ref>. Theorem 1 [33] R minimax n = sup R Bayes n; ; where the supremum is taken over all (Borel) probability measures on the parameter space fi.
Reference: [34] <author> D. Haussler and A. Barron. </author> <title> How well do Bayes methods work for on-line prediction of f+1; 1g values? In Proceedings of the Third NEC Symposium on Computation and Cognition. </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: In computational learning theory, 2 this risk is the average additional loss suffered by an adaptive algorithm that predicts each observation before it arrives, based on the previous observations, as compared to an algorithm that makes predictions knowing the true distribution <ref> [34, 35] </ref>. <p> Since maximin minimax always, we have sup R Bayes 1;; 1+ R minimax 1; 1+ , and from this we obtain the result stated in the Theorem. 2 The method used in obtaining the upper bound in the above result is a familiar one (see e.g. <ref> [5, 34] </ref>).
Reference: [35] <author> D. Haussler, M. Kearns, and R. E. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <journal> Machine Learning, </journal> <volume> 14(1) </volume> <pages> 83-113, </pages> <year> 1994. </year>
Reference-contexts: In computational learning theory, 2 this risk is the average additional loss suffered by an adaptive algorithm that predicts each observation before it arrives, based on the previous observations, as compared to an algorithm that makes predictions knowing the true distribution <ref> [34, 35] </ref>.
Reference: [36] <author> D. Haussler and M. Opper. </author> <title> General bounds on the mutual information between a parameter and n conditionally independent observations. </title> <booktitle> In Proceedings of the Seventh Annual ACM Workshop on Computational Learning Theory, </booktitle> <year> 1995. </year>
Reference-contexts: Hence, the corollary shows exponential convergence. Finally, let us note that Theorem 2 and Corollary 1 can also be used to characterize the mutual information between fi fl and Y n (Bayes risk) in the general case when fi is uncountably infinite but finite dimensional. This was demonstrated in <ref> [36] </ref>. <p> In the 40 future we hope to further explore the applications of these results to specific estimation problems, such as the "concept learning" or "pattern classification" problems examined in current machine learning and neural network research. Some initial results along these lines can be found in <ref> [46, 36] </ref> (see also [28, 43]). There are also several other directions for further research one might pursue. <p> distribution in fi but is "close to" a distribution fi, and giving a more complete characterization of the mutual information I (fi fl ; Y n ) in terms of the metric entropy properties of fi for the infinite dimensional case, as was done for the finite dimensional case in <ref> [36] </ref>. 11 Appendix Here we give the proof of Lemma 5. Lemma 12 Assume 0 &lt; ff &lt; 1 and &gt; 0. Let P , R and U be any distributions on Y . Let c = dP 1+ dU .
Reference: [37] <author> I. Ibragimov and R. Hasminskii. </author> <title> On the information in a sample about a parameter. </title> <booktitle> In Second Int. Symp. on Information Theory, </booktitle> <pages> pages 295-309, </pages> <year> 1972. </year>
Reference-contexts: For cumulative relative entropy loss, the Bayes risk has a fundamental information theoretic interpretation: it is the mutual information between a random variable representing the choice of the parameter fl of the true distribution, and the random variable given by the n observations <ref> [37, 27, 18] </ref>. This provides a beautiful connection between information theory and statistics. This connection also extends to other fields, as is discussed in [18, 8]. <p> Hasminskii showed that I (fi fl ; Y n ) ~ (D=2) log n when Y is the real line and the conditional distributions P are a smooth family of densities indexed by a real-valued parameter vector in a compact set fi of dimension D, and certain other conditions apply <ref> [37] </ref>. In this case they were even able to estimate the lower order additive terms in this approximation, which involve the Fisher information and the entropy of the prior. Further related results were given by Efroimovich [27] and Clarke [17].
Reference: [38] <author> A. J. Izenman. </author> <title> Recent developments in nonparametric density estimation. </title> <journal> JASA, </journal> <volume> 86(413) </volume> <pages> 205-224, </pages> <year> 1991. </year>
Reference-contexts: Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 41]. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. <ref> [24, 38] </ref>).
Reference: [39] <author> A. N. Kolmogorov and V. M. Tihomirov. </author> <title> *-entropy and *-capacity of sets in functional spaces. </title> <journal> Amer. Math. Soc. Translations (Ser. </journal> <volume> 2), 17 </volume> <pages> 277-364, </pages> <year> 1961. </year>
Reference-contexts: Under this assumption, (fi; h) is a metric space. We show how bounds on the minimax risk can be obtained by looking at properties of this metric space. These are the the packing and covering numbers, and the associated metric entropy, introduced by Kolmogorov and Tikhomirov in <ref> [39] </ref> and commonly used in the theory of empirical processes (see e.g. [26, 48, 31, 13]). For the following definitions, let (S; ) be any complete separable metric space. Definition 1 (Metric entropy, also called Kolmogorov *-entropy [39]) A partition of S is a collection f i g of Borel subsets <p> covering numbers, and the associated metric entropy, introduced by Kolmogorov and Tikhomirov in <ref> [39] </ref> and commonly used in the theory of empirical processes (see e.g. [26, 48, 31, 13]). For the following definitions, let (S; ) be any complete separable metric space. Definition 1 (Metric entropy, also called Kolmogorov *-entropy [39]) A partition of S is a collection f i g of Borel subsets of S that are pairwise disjoint and whose union is S. The diameter of a set A S is given by diam (A) = sup x;y2A (x; y). <p> By M * (S; ) we denote the cardinality of the largest finite *-separated subset of S, or 1 if arbitrarily large such sets exist. The following lemma is easily verified <ref> [39] </ref>. <p> Kolmogorov and Tikhomirov also introduced abstract notions of the dimension and order of metric spaces in their seminal paper <ref> [39] </ref>. These can be used to measure the "massiveness" of both spaces indexed by a finite dimensional parameter vector and infinite dimensional function spaces. In the following, the metric is omitted from the notation, being understood from the context. Definition 3 The upper and lower metric dimensions [39] of S are <p> their seminal paper <ref> [39] </ref>. These can be used to measure the "massiveness" of both spaces indexed by a finite dimensional parameter vector and infinite dimensional function spaces. In the following, the metric is omitted from the notation, being understood from the context. Definition 3 The upper and lower metric dimensions [39] of S are defined by dim (S) = lim sup K * (S) * dim (S) = lim inf K * (S) * respectively. When dim (S) = dim (S), then this value is denoted dim (S) and called the metric dimension of S.
Reference: [40] <author> F. Komaki. </author> <title> On asymptotic properties of predictive distributions. </title> <type> Technical Report METR 94-21, </type> <institution> U. Yokyo, Math. and Eng. Physics, </institution> <year> 1994. </year> <month> 45 </month>
Reference-contexts: Amari has developed an extensive theory that relates the risk when fl is the true state of Nature to certain differential-geometric properties of the parameter space fi in the neighborhood of fl involving Fisher information and related quantities [2, 3] (see also <ref> [60, 40] </ref>). Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. [6, 10, 52, 59, 55]. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 41].
Reference: [41] <author> L. Gyorfi and I. Pali and E. van der Meulen. </author> <title> There is no universal source code for an infinite alphabet. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 40 </volume> <pages> 267-271, </pages> <year> 1994. </year>
Reference-contexts: In data compression, the cumulative relative entropy risk is the redundancy, which is the expected excess code length for the best adaptive coding method, as compared to the best coding method that has prior knowledge of the true distribution <ref> [18, 41, 44] </ref>. The minimax risk is called "information" channel capacity [21], p. 184. In mathematical finance and gambling theory, the cumulative relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution [9, 18]. <p> Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. [6, 10, 52, 59, 55]. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in <ref> [1, 41] </ref>. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. [24, 38]).
Reference: [42] <author> L. LeCam. </author> <title> Asymptotic methods in statistical decision theory. </title> <publisher> Springer, </publisher> <year> 1986. </year>
Reference-contexts: These results are then used further to obtain similar bounds for the risk under other loss functions, including the Hellinger and L 1 distance. Here the results are not as sharp as one can obtain by other methods, such as those of Le Cam <ref> [15, 42] </ref>, Birge [11, 12], Hasminskii and Ibragimov [32], and Wong and Shen [55], but these applications nevertheless illustrate the general utility of the method. <p> None of our results depend on the choice of the dominating measure -, hence for any distribution Q, the Radon-Nikodym derivative dQ d- will be abbreviated simply as dQ, following the convention in Le Cam's text <ref> [42] </ref>. Furthermore, all integrals in the results below are assumed, without specific notation, to be taken with respect to the measure -, unless otherwise indicated. <p> While this work is too extensive to summarize here, we do note that some authors have also taken the general approach that we take here in using notions of metric entropy (defined below), and specifically using the Hellinger distance in obtaining these bounds (e.g. <ref> [42, 11, 12, 32, 54, 13, 10] </ref>). The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [55] (see Corollary 1, p. 360) and Barron and Yang [10]. <p> This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including Le Cam <ref> [42] </ref>, Birge [11, 12], Hasminskii and Ibragimov [32], and van de Geer [54]. 9 4.1 Basic bounds Our main theorem gives bounds on I (fi fl ; Y n ) and D KL (P n fl jjM n; ) in terms of the logarithms of two Laplace transforms of the I <p> This general approach was developed by Le Cam and Birge <ref> [42, 11, 12] </ref>. We illustrate it by applying the above lemma to all of the standard cases for the asymptotic growth rate of the metric entropy K * (fi; h). Theorem 4 Assume there exists &gt; 0 such that R minimax 1; 1+ &lt; 1. 5 1. <p> The following bounds are well known for any distributions P and Q on Y (see e.g. <ref> [42] </ref>) D 2 In addition, using Lemma 4, it follows that D 2 y2Y dQ (y) D 2 Finally, using Lemma 5 with * = * n = 1 n 2= log n , it can easily be shown that for any distribution U such that C = R (dP )
Reference: [43] <author> R. Meir and N. Merhav. </author> <title> On the stochastic complexity of learning realizable and unrealizable rules. </title> <booktitle> Machine Learning, </booktitle> <address> 19(3):241-, </address> <year> 1995. </year>
Reference-contexts: Some initial results along these lines can be found in [46, 36] (see also <ref> [28, 43] </ref>). There are also several other directions for further research one might pursue.
Reference: [44] <author> N. Merhav and M. Feder. </author> <title> A strong version of the redundancy-capacity theorem of universal coding. </title> <journal> IEEE Trans. Info Th., </journal> <volume> 41(3):714-, </volume> <year> 1995. </year>
Reference-contexts: In data compression, the cumulative relative entropy risk is the redundancy, which is the expected excess code length for the best adaptive coding method, as compared to the best coding method that has prior knowledge of the true distribution <ref> [18, 41, 44] </ref>. The minimax risk is called "information" channel capacity [21], p. 184. In mathematical finance and gambling theory, the cumulative relative entropy risk measures the expected reduction in the logarithm of compounded wealth due to lack of knowledge of the true distribution [9, 18]. <p> Thus both this strengthened upper bound and the given lower bound hold on a set of measure 1 e fl in this case. Finally, we note that part (2) is related to part (1) in the same way that the strong redundancy-capacity theorem of universal coding in <ref> [44] </ref> is related to the usual theorems concerning average redundancy. It is possible to state a variant of Theorem 2 using the the D ff distances.
Reference: [45] <author> M. Opper and D. Haussler. </author> <title> Calculation of the learning curve of Bayes optimal classification algorithm for learning a perceptron with noise. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 75-87, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Finally, in statistical mechanics, the Bayes risk can be related to the free energy <ref> [45, 46] </ref>. In this paper, we provide upper and lower bounds on the Bayes risk for cumulative relative entropy loss in the form of Laplace integrals of the Hellinger distance between pairs of distributions in fP : 2 fig.
Reference: [46] <author> M. Opper and D. Haussler. </author> <title> Bounds for predictive errors in the statistical mechanics of in supervised learning. </title> <journal> Physical Review Letters, </journal> <volume> 75(20) </volume> <pages> 3772-3775, </pages> <year> 1995. </year>
Reference-contexts: Finally, in statistical mechanics, the Bayes risk can be related to the free energy <ref> [45, 46] </ref>. In this paper, we provide upper and lower bounds on the Bayes risk for cumulative relative entropy loss in the form of Laplace integrals of the Hellinger distance between pairs of distributions in fP : 2 fig. <p> In the 40 future we hope to further explore the applications of these results to specific estimation problems, such as the "concept learning" or "pattern classification" problems examined in current machine learning and neural network research. Some initial results along these lines can be found in <ref> [46, 36] </ref> (see also [28, 43]). There are also several other directions for further research one might pursue.
Reference: [47] <author> M. S. Pinsker. </author> <title> Information and Information Stability of Random Variables and Processes (Transl.). </title> <type> Holden Day, </type> <year> 1964. </year>
Reference-contexts: More general results, including the above corollary, follow from results in Pinsker's book <ref> [47] </ref> (see also [4]). Applying Theorem (1) and taking the supremum over in Corollary (2), it follows that if fi is finite then for all n, R minimax n log jfij and lim n!1 R minimax n = log jfij.
Reference: [48] <author> D. Pollard. </author> <title> Empirical Processes: Theory and Applications, </title> <booktitle> volume 2 of NSF-CBMS Regional Conference Series in Probability and Statistics. </booktitle> <institution> Institute of Math. Stat. and Am. Stat. Assoc., </institution> <year> 1990. </year>
Reference-contexts: These are the the packing and covering numbers, and the associated metric entropy, introduced by Kolmogorov and Tikhomirov in [39] and commonly used in the theory of empirical processes (see e.g. <ref> [26, 48, 31, 13] </ref>). For the following definitions, let (S; ) be any complete separable metric space. Definition 1 (Metric entropy, also called Kolmogorov *-entropy [39]) A partition of S is a collection f i g of Borel subsets of S that are pairwise disjoint and whose union is S.
Reference: [49] <author> A. Renyi. </author> <title> On measures of entropy and information. </title> <editor> In L. C. et.al., editor, </editor> <title> Fourth Berkeley Sym. </title> <journal> on Math., Stat. and Prob., </journal> <pages> pages 547-561. </pages> <year> 1960. </year>
Reference-contexts: In obtaining these bounds, we use several notions of "distance" between probability distributions based on the ff-affinities. One such family of distances are the I-divergences introduced by Renyi <ref> [49] </ref>.
Reference: [50] <author> A. Renyi. </author> <title> On the amount of information concerning an unknown parameter in a sequence of observations. </title> <journal> Publ. Math. Inst. Hungar. Acad. Sci., </journal> <volume> 9 </volume> <pages> 617-625, </pages> <year> 1964. </year>
Reference-contexts: It also follows that if fi is infinite, then lim n!1 R minimax n = 1. In the case that fi is finite, results of Renyi <ref> [50] </ref> show further that the difference I (fi fl ; Y n ) H (fi fl ) converges to zero exponentially fast in n.
Reference: [51] <author> J. Rissanen. </author> <title> Stochastic complexity and modeling. </title> <journal> The Annals of Statistics, </journal> <volume> 14(3) </volume> <pages> 1080-1100, </pages> <year> 1986. </year>
Reference-contexts: These results were extended to the Bayes and minimax risk in [19] (see also [7]). Related lower bounds, which are often quoted, were obtained by Rissanen <ref> [51] </ref>, based on certain asymptotic normality assumptions. Further extensions of this work are given by 7 Yamanishi [56, 58, 57].
Reference: [52] <author> J. Rissanen, T. Speed, and B. Yu. </author> <title> Density estimation by stochastic complexity. </title> <journal> IEEE Trans. Info. Th., </journal> <volume> 38 </volume> <pages> 315-323, </pages> <year> 1992. </year>
Reference-contexts: Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. <ref> [6, 10, 52, 59, 55] </ref>. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 41]. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. [24, 38]).
Reference: [53] <author> K. Symanzik. </author> <title> Proof and refinements of an inequality of Feynman. </title> <journal> J.Math. Phys., </journal> <volume> 6:1155-, </volume> <year> 1965. </year>
Reference-contexts: Hence both bounds hold on the complement of the union of these two sets, which has -measure at least 1 2e fl . We begin with the upper bounds. This requires the following lemma which has been previously utilized in the framework of Statistical Physics <ref> [53] </ref>. Lemma 1 Let P = P (w) be a measure on a set W and Q = Q (v) be a measure on a set V .
Reference: [54] <author> S. van deGeer. </author> <title> Hellinger-consistency of certain nonparametric maximum likelihood estimators. </title> <journal> Annals of Statistics, </journal> <volume> 21 </volume> <pages> 14-44, </pages> <year> 1993. </year>
Reference-contexts: While this work is too extensive to summarize here, we do note that some authors have also taken the general approach that we take here in using notions of metric entropy (defined below), and specifically using the Hellinger distance in obtaining these bounds (e.g. <ref> [42, 11, 12, 32, 54, 13, 10] </ref>). The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen [55] (see Corollary 1, p. 360) and Barron and Yang [10]. <p> This metric has been used to give bounds on the risk of estimation procedures in statistics by many authors, including Le Cam [42], Birge [11, 12], Hasminskii and Ibragimov [32], and van de Geer <ref> [54] </ref>. 9 4.1 Basic bounds Our main theorem gives bounds on I (fi fl ; Y n ) and D KL (P n fl jjM n; ) in terms of the logarithms of two Laplace transforms of the I divergence, one at the value ff = 1 (the relative entropy) and
Reference: [55] <author> W. Wong and X. Shen. </author> <title> Probability inequalities for likelihood ratios and convergence rates for sieve MLE's. </title> <journal> Annals of Statistics, </journal> <volume> 23(2) </volume> <pages> 339-362, </pages> <year> 1995. </year>
Reference-contexts: Here the results are not as sharp as one can obtain by other methods, such as those of Le Cam [15, 42], Birge [11, 12], Hasminskii and Ibragimov [32], and Wong and Shen <ref> [55] </ref>, but these applications nevertheless illustrate the general utility of the method. Finally, we discuss some possible further work in section 10. 2 Basic definitions, notation and assumptions The following notation and assumptions will be used throughout the paper. Let Y be a complete separable metric space. <p> Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. <ref> [6, 10, 52, 59, 55] </ref>. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 41]. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. [24, 38]). <p> The only authors we have found who have applied this methodology to the relative entropy risk are Wong and Shen <ref> [55] </ref> (see Corollary 1, p. 360) and Barron and Yang [10]. This work is somewhat complementary to ours, in that it treats instantaneous risk, whereas we focus on cumulative risk. <p> Then D KL (P jjQ) * D ff (P; R) + * + * =2 c ; where f ff (x) = 1 ff 2 We recently noticed that a related result is given in <ref> [55] </ref>, Theorem 5, although no explicit relationship with the ff affinities is given in the latter result. 17 The proof of this lemma is given in the appendix. Now let U be the Bayes strategy as defined above. Since R Bayes 1;; 1+ &lt; 1, U is well defined.
Reference: [56] <author> K. Yamanishi. </author> <title> A learning criterion for stochastic rules. </title> <booktitle> Machine Learning, 1992. Special Issue on the Proceedings of the 3nd Workshop on Computational Learning Theory. </booktitle> <pages> 46 </pages>
Reference-contexts: These results were extended to the Bayes and minimax risk in [19] (see also [7]). Related lower bounds, which are often quoted, were obtained by Rissanen [51], based on certain asymptotic normality assumptions. Further extensions of this work are given by 7 Yamanishi <ref> [56, 58, 57] </ref>. Amari has developed an extensive theory that relates the risk when fl is the true state of Nature to certain differential-geometric properties of the parameter space fi in the neighborhood of fl involving Fisher information and related quantities [2, 3] (see also [60, 40]).
Reference: [57] <author> K. Yamanishi. </author> <title> A decision-theoretic extension of stochastic complexity and its applica-tions to learning. </title> <type> Unpublished manuscript, </type> <year> 1995. </year>
Reference-contexts: These results were extended to the Bayes and minimax risk in [19] (see also [7]). Related lower bounds, which are often quoted, were obtained by Rissanen [51], based on certain asymptotic normality assumptions. Further extensions of this work are given by 7 Yamanishi <ref> [56, 58, 57] </ref>. Amari has developed an extensive theory that relates the risk when fl is the true state of Nature to certain differential-geometric properties of the parameter space fi in the neighborhood of fl involving Fisher information and related quantities [2, 3] (see also [60, 40]).
Reference: [58] <author> K. Yamanishi. </author> <title> A loss bound model for on-line stochastic prediction algorithms. </title> <journal> Information and Computation, </journal> <volume> 119 </volume> <pages> 39-54, </pages> <year> 1995. </year>
Reference-contexts: These results were extended to the Bayes and minimax risk in [19] (see also [7]). Related lower bounds, which are often quoted, were obtained by Rissanen [51], based on certain asymptotic normality assumptions. Further extensions of this work are given by 7 Yamanishi <ref> [56, 58, 57] </ref>. Amari has developed an extensive theory that relates the risk when fl is the true state of Nature to certain differential-geometric properties of the parameter space fi in the neighborhood of fl involving Fisher information and related quantities [2, 3] (see also [60, 40]).
Reference: [59] <author> B. Yu. </author> <title> Lower bounds on expected redundancy for nonparametric classes. </title> <journal> IEEE Trans. Info. Th., </journal> <volume> 42(1), </volume> <year> 1996. </year>
Reference-contexts: Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. <ref> [6, 10, 52, 59, 55] </ref>. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 41]. However, in the nonparametric case, more extensive work has been done in bounding the risk for other loss functions (see e.g. [24, 38]). <p> The method for obtaining the lower bound by choosing a discrete prior on a well-separated set of is also similar in many respects to standard lower bound methods, such as those that use Fano's inequality or Assouad's lemma (see e.g. <ref> [12, 10, 59] </ref>), but the 27 method is particularly clean in the present framework, giving a fairly good match to the upper bound. <p> Let us assume that the statistician observes a set of n observations y 1 ; : : : ; y n which are drawn independently from a density dP (y), 2 fi on the interval [0; 1]. As in 39 <ref> [59] </ref>, let fi be the Lipschitz class F p;ff (C; L) of densities satisfying sup y2 [0;1] jdP (y)j C and having derivatives dP (k) (y) of order k p with the Lipschitz condition on the p- th derivative jdP (y) dP (y 0 )j Ljy y 0 j ff for
Reference: [60] <author> H. Zhu and R. Rohwer. </author> <title> Information geometric measurements of generalization. </title> <type> Technical Report NCRG 4350, </type> <institution> Aston U., England, Neural computing research group, </institution> <year> 1995. </year> <month> 47 </month>
Reference-contexts: Amari has developed an extensive theory that relates the risk when fl is the true state of Nature to certain differential-geometric properties of the parameter space fi in the neighborhood of fl involving Fisher information and related quantities [2, 3] (see also <ref> [60, 40] </ref>). Some authors have also looked at the value of the relative entropy risk in nonparametric cases as well, e.g. [6, 10, 52, 59, 55]. Also, the issue of consistent estimation of a general probability distribution with respect to relative entropy is addressed in [1, 41]. <p> Jensen's inequality, it can be verified that when R Bayes 1;; 1+ &lt; 1, the minimizing ^ P , i.e. the Bayes strategy, is the distribution U = U defined by dU = fi d ( fl )dP 1+ 1 C ; where C ; = Y fi fl 1+ <ref> [60] </ref>.
References-found: 60

