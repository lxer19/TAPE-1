URL: http://stat.stanford.edu/~gareth/ftp/papers/bv.ps
Refering-URL: http://stat.stanford.edu/~gareth/
Root-URL: 
Title: Generalizations of the Bias/Variance Decomposition for Prediction Error  
Author: Gareth James and Trevor Hastie 
Date: February 26, 1997  
Affiliation: Dept. of Statistics, Stanford University  
Abstract: The bias and variance of a real valued random variable, using squared error loss, are well understood. However because of recent developments in classification techniques it has become desirable to extend these concepts to general random variables and loss functions. The 0-1 (misclassification) loss function with categorical random variables has been of particular interest. We explore the concepts of variance and bias and develop a decomposition of the prediction error into functions of the systematic and variable parts of our predictor. After providing some examples we conclude with a discussion of the various definitions that have been proposed.
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L. </author> <note> (1996a) Bagging predictors, in press, Machine Learning 13 Breiman, </note> <author> L. </author> <title> (1996b) Bias, Variance, and Arcing Classifiers, </title> <institution> Dept. of Statis--tics, University of California Berkeley, </institution> <type> Technical Report Dietterich, </type> <note> T.G. </note> <author> and Bakiri G. </author> <title> (1995) Solving Multiclass Learning Problems via Error-Correcting Output Codes, </title> <note> Journal of Artificial Intelligence Research 2 (1995) 263-286 Dietterich, </note> <author> T. G. and Kong, E. B. </author> <title> (1995) Error-Correcting Output Coding Corrects Bias and Variance, </title> <booktitle> Proceedings of the 12th International Conference on Machine Learning pp. </booktitle> <publisher> 313-321 Morgan Kaufmann Efron, B. </publisher> <year> (1978), </year> <title> Regression and anova with zero-one data, </title> <journal> J. Amer. Statist. Assoc. </journal> <pages> pp. 113-121. </pages>
Reference: <author> Freund, Y. and Schapire, R. </author> <title> (1996) Experiments with a new boosting algorithm, </title> <booktitle> "Machine Learning : Proceedings of the Thirteenth International Conference", </booktitle> <month> July, </month> <title> 1996 Friedman, J.H. (1996) On Bias, Variance, 0/1-loss, and the Curse of Dimensionality, </title> <institution> Dept. of Statistics, Stanford University, </institution> <note> Technical Report Kohavi, </note> <author> R. and Wolpert, D.H. </author> <title> (1996) Bias Plus Variance Decomposition for Zero-One Loss Functions, </title> <booktitle> "Machine Learning : Proceedings of the Thirteenth International Conference", </booktitle> <month> July, </month> <title> 1996 http://robotics.stanford.edu/users/ronnyk Tibshirani (1996) Bias, Variance and Prediction Error for Classification Rules, </title> <institution> Dept. of Statistics, University of Toronto, </institution> <type> Technical Report 14 </type>
References-found: 2

