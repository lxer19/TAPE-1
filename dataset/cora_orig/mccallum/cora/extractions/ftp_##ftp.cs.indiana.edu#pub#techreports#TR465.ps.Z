URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR465.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Email: ajcbik@cs.indiana.edu  
Title: A Strategy for Exploiting Implicit Loop Parallelism in Java Programs  
Author: Aart J.C. Bik and Dennis B. Gannon 
Address: Lindley Hall 215, Bloomington, Indiana 47405-4101, USA  
Affiliation: Computer Science Department, Indiana University  
Abstract: In this paper, we explore a strategy that can be used by a source to source restructuring compiler to exploit implicit loop parallelism in Java programs. First, the compiler must identify the parallel loops in a program. Thereafter, the compiler explicitly expresses this parallelism in the transformed program using the multi-threading mechanism of Java. Finally, after a single compilation of the transformed program into Java byte-code, speedup can be obtained on any platform on which the Java byte-code interpreter supports actual concurrent execution of threads, whereas threads only induce a slight overhead for serial execution. In addition, this approach can enable a compiler to explicitly express the scheduling policy of each parallel loop in the program. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ull-man. </author> <booktitle> Compilers Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: In this paper, we focus on the parallelization of stride-1 loops that have the following form: L1: for (int i = low; i &lt; high; i++) body (i); Note that conventional compiler techniques, like constant propagation, scalar forward substitution, induction variable substitution, and loop normalization <ref> [1, 11, 24, 29] </ref> may be useful to convert some other loop constructs into this form.
Reference: [2] <author> Ken Arnold and James Gosling. </author> <title> The Java Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1996. </year>
Reference-contexts: Thereafter, the compiler uses the multi-threading mechanism of the Java programming language (see e.g. <ref> [2, 12, 20, 21] </ref>) to explicitly express these parallel loops in the transformed program. In this manner, after a single compilation of the transformed program into Java byte-code, speedup can be obtained on any platform on which the Java byte-code interpreter supports actual concurrent execution of threads.
Reference: [3] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference: [4] <author> Utpal Banerjee. </author> <title> Loop Transformations for Restructuring Compilers: The Foundations. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1993. </year>
Reference: [5] <author> Utpal Banerjee. </author> <title> Loop Parallelization. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1994. </year>
Reference: [6] <author> D.W. Barron. </author> <title> The Java Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1996. </year>
Reference-contexts: In this manner, a compiled Java program will run on any platform on which a Java byte-code interpreter is available. This idea has already proven to be successful in the past. Some of first implementers of Pascal compilers <ref> [6] </ref>, for example, used so-called p-code as target language, forming the language of an abstract ma chine (the Virtual Stack Machine). The compiler itself, translating Pascal program into p-code, was entirely written in p-code.
Reference: [7] <author> David Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rice University, </institution> <year> 1987. </year>
Reference: [8] <author> Ron G. Cytron. </author> <title> Doacross, beyond vectorization for multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 836-844, </pages> <year> 1986. </year>
Reference-contexts: Even if data dependences are carried by a loop, some parallelism may result from executing the loop as a DO-ACROSS-loop, where a partial execution of some (parts) of the iterations is enforced using synchronization to satisfy the data dependences. In <ref> [8, 9] </ref>, this synchronization is modeled under the assumption that processors operate synchronously by using a particular delay d 0 between consecutive iterations of the DO-ACROSS-loop: the ith iteration is executed after a delay of (i 1) d.
Reference: [9] <author> Ron G. Cytron. </author> <title> Limited processor scheduling of doacross loops. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 226-234, </pages> <year> 1987. </year>
Reference-contexts: Even if data dependences are carried by a loop, some parallelism may result from executing the loop as a DO-ACROSS-loop, where a partial execution of some (parts) of the iterations is enforced using synchronization to satisfy the data dependences. In <ref> [8, 9] </ref>, this synchronization is modeled under the assumption that processors operate synchronously by using a particular delay d 0 between consecutive iterations of the DO-ACROSS-loop: the ith iteration is executed after a delay of (i 1) d.
Reference: [10] <author> Erik H. D'Hollander. </author> <title> Partitioning and labeling of index sets in DO loops with constant dependence vectors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 139-144, </pages> <year> 1989. </year> <booktitle> Volume 2: Software. </booktitle>
Reference: [11] <author> C.N. Fischer and R.J. LeBlanc. </author> <title> Crafting a Compiler. </title> <address> Benjamin-Cummings, Menlo Park, Califor-nia, </address> <year> 1988. </year>
Reference-contexts: In this paper, we focus on the parallelization of stride-1 loops that have the following form: L1: for (int i = low; i &lt; high; i++) body (i); Note that conventional compiler techniques, like constant propagation, scalar forward substitution, induction variable substitution, and loop normalization <ref> [1, 11, 24, 29] </ref> may be useful to convert some other loop constructs into this form.
Reference: [12] <author> David Flanagan. </author> <title> Java in a Nutshell. </title> <publisher> O'Reilly & Associates, </publisher> <address> Sebastopol, CA, </address> <year> 1996. </year>
Reference-contexts: Thereafter, the compiler uses the multi-threading mechanism of the Java programming language (see e.g. <ref> [2, 12, 20, 21] </ref>) to explicitly express these parallel loops in the transformed program. In this manner, after a single compilation of the transformed program into Java byte-code, speedup can be obtained on any platform on which the Java byte-code interpreter supports actual concurrent execution of threads.
Reference: [13] <author> C.A.R. Hoare. </author> <title> Monitors: An operating system structuring concept. </title> <journal> Communications of the ACM, </journal> <volume> 17(10) </volume> <pages> 549-557, </pages> <year> 1974. </year>
Reference-contexts: given to the threads that implement a parallel loop depends on thread scheduler in the implementation of the Java Virtual Machine. 3.4 RandomSync In the implementation of the pool, we already have exploited the fact that synchronized methods can be used to enforce mutual exclusion. 5 However, the implicit monitors <ref> [13] </ref> associated with all objects in Java make it also extremely simple to implement random synchronization for DO-ACROSS-like loops. The class RandomSync defines the implementation of each synchronization variable.
Reference: [14] <author> David J. Kuck. </author> <title> The Structure of Computers and Computations. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <booktitle> 1978. </booktitle> <volume> Volume 1. </volume>
Reference: [15] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Programming. </title> <publisher> The Benjamin/Cummings Publishing Company, </publisher> <address> Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: Finally, this approach enables the compiler to express the scheduling policy of each parallel loop in the program. However, since threads are lightweight processes sharing one address-space, the actual concurrent execution of threads is typically only supported on shared-address-space architectures <ref> [15] </ref>. Hence, the focus of this paper is to obtain speedup on such architectures. Future research, however, will focus on letting a restructuring compiler use the networking capabilities of Java in a message-passing like manner to take advantage of computing power that is available over a network.
Reference: [16] <author> Zhiyuan Li and Walid Abu-Sufah. </author> <title> On reducing data synchronization in multiprocessed loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36:105-109, </volume> <year> 1987. </year>
Reference-contexts: Different synchronization variables, implemented as bit-arrays, are used to synchronize the different static data dependences in the loop. The automatic generation of synchronization is addressed in <ref> [16, 17, 18, 19, 29] </ref>. On shared-address space architectures, parallel loops can be executed using fork/join-like parallelism. In this approach, a master thread executes the serial parts of a program, and initiates a number of slave threads, or workers, when a parallel loop is reached [28, 385-387]. <p> cases where adding a new method and private variables to the class MyClass is undesirable, the compiler can resort to simply leaving all loops serial. 3.5.2 Insertion of Random Synchronization For DO-ACROSS-like execution, the compiler adds the appropriate synchronization primitives to the loop-body according to methods described in the literature <ref> [16, 17, 18, 19, 29] </ref>). Signaling iteration i on the synchronization variable with number k is implemented as sync [k].doPost (i). Likewise, waiting for iteration j on the synchronization variable with number k is implemented as sync [k].doWait (j).
Reference: [17] <author> Samuel P. Midkiff. </author> <title> The Dependence Analysis and Synchronization of Parallel Programs. </title> <type> PhD thesis, </type> <institution> C.S.R.D., </institution> <year> 1993. </year>
Reference-contexts: Alternatively, synchronization can be enforced using the primitives testset/test (or advance/await) <ref> [17, ch6] </ref>[18, 19][28, p393-395], which can be implemented with a scalar synchronization variable [27, p84-86]. A more general form of synchronization in a DO-ACROSS-loop is provided by random synchronization with primitives post/wait [27, p75-83][29, p289-295]. <p> Different synchronization variables, implemented as bit-arrays, are used to synchronize the different static data dependences in the loop. The automatic generation of synchronization is addressed in <ref> [16, 17, 18, 19, 29] </ref>. On shared-address space architectures, parallel loops can be executed using fork/join-like parallelism. In this approach, a master thread executes the serial parts of a program, and initiates a number of slave threads, or workers, when a parallel loop is reached [28, 385-387]. <p> cases where adding a new method and private variables to the class MyClass is undesirable, the compiler can resort to simply leaving all loops serial. 3.5.2 Insertion of Random Synchronization For DO-ACROSS-like execution, the compiler adds the appropriate synchronization primitives to the loop-body according to methods described in the literature <ref> [16, 17, 18, 19, 29] </ref>). Signaling iteration i on the synchronization variable with number k is implemented as sync [k].doPost (i). Likewise, waiting for iteration j on the synchronization variable with number k is implemented as sync [k].doWait (j).
Reference: [18] <author> Samuel P. Midkiff and David A. Padua. </author> <title> Compiler generated synchronization for DO loops. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 544-551, </pages> <year> 1986. </year>
Reference-contexts: Different synchronization variables, implemented as bit-arrays, are used to synchronize the different static data dependences in the loop. The automatic generation of synchronization is addressed in <ref> [16, 17, 18, 19, 29] </ref>. On shared-address space architectures, parallel loops can be executed using fork/join-like parallelism. In this approach, a master thread executes the serial parts of a program, and initiates a number of slave threads, or workers, when a parallel loop is reached [28, 385-387]. <p> cases where adding a new method and private variables to the class MyClass is undesirable, the compiler can resort to simply leaving all loops serial. 3.5.2 Insertion of Random Synchronization For DO-ACROSS-like execution, the compiler adds the appropriate synchronization primitives to the loop-body according to methods described in the literature <ref> [16, 17, 18, 19, 29] </ref>). Signaling iteration i on the synchronization variable with number k is implemented as sync [k].doPost (i). Likewise, waiting for iteration j on the synchronization variable with number k is implemented as sync [k].doWait (j).
Reference: [19] <author> Samuel P. Midkiff and David A. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36:1485-1495, </volume> <year> 1987. </year>
Reference-contexts: Different synchronization variables, implemented as bit-arrays, are used to synchronize the different static data dependences in the loop. The automatic generation of synchronization is addressed in <ref> [16, 17, 18, 19, 29] </ref>. On shared-address space architectures, parallel loops can be executed using fork/join-like parallelism. In this approach, a master thread executes the serial parts of a program, and initiates a number of slave threads, or workers, when a parallel loop is reached [28, 385-387]. <p> cases where adding a new method and private variables to the class MyClass is undesirable, the compiler can resort to simply leaving all loops serial. 3.5.2 Insertion of Random Synchronization For DO-ACROSS-like execution, the compiler adds the appropriate synchronization primitives to the loop-body according to methods described in the literature <ref> [16, 17, 18, 19, 29] </ref>). Signaling iteration i on the synchronization variable with number k is implemented as sync [k].doPost (i). Likewise, waiting for iteration j on the synchronization variable with number k is implemented as sync [k].doWait (j).
Reference: [20] <author> Patrick Naughton. </author> <title> The Java Handbook. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: Thereafter, the compiler uses the multi-threading mechanism of the Java programming language (see e.g. <ref> [2, 12, 20, 21] </ref>) to explicitly express these parallel loops in the transformed program. In this manner, after a single compilation of the transformed program into Java byte-code, speedup can be obtained on any platform on which the Java byte-code interpreter supports actual concurrent execution of threads. <p> Clearly, the overhead associated with random synchronization is more substantial than the overhead of a truly parallel loop. Furthermore, this experiment illustrates the using the wrong scheduling policy may effectively serialize the parallel loop. 4.4 Pixel Initialization The following example is based on an applet example found in <ref> [20] </ref>: public class MemoryImager extends Applet - ... final static int d_x = 700; final static int d_y = 1000; ... public void generateImage () - int pixels [] = new int [d_x * d_y]; ... for (int x = 0; x &lt; d_x; x++) - int g = (x*2^y*2) &
Reference: [21] <author> Patrick Niemeyer and Joshua Peck. </author> <title> Exploring Java. </title> <publisher> O'Reilly & Associates, </publisher> <address> Sebastopol, CA, </address> <year> 1996. </year>
Reference-contexts: Thereafter, the compiler uses the multi-threading mechanism of the Java programming language (see e.g. <ref> [2, 12, 20, 21] </ref>) to explicitly express these parallel loops in the transformed program. In this manner, after a single compilation of the transformed program into Java byte-code, speedup can be obtained on any platform on which the Java byte-code interpreter supports actual concurrent execution of threads.
Reference: [22] <author> David A. Padua, David J. Kuck, and Duncan H. Lawrie. </author> <title> High speed multiprocessors and compilation techniques. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29:763-776, </volume> <year> 1980. </year>
Reference: [23] <author> David A. Padua and Michael J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 1184-1201, </pages> <year> 1986. </year>
Reference: [24] <author> Thomas W. Parsons. </author> <title> Introduction to Compiler Construction. </title> <publisher> Computer Science Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: In this paper, we focus on the parallelization of stride-1 loops that have the following form: L1: for (int i = low; i &lt; high; i++) body (i); Note that conventional compiler techniques, like constant propagation, scalar forward substitution, induction variable substitution, and loop normalization <ref> [1, 11, 24, 29] </ref> may be useful to convert some other loop constructs into this form.
Reference: [25] <author> Constantine D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: Whether all threads are actually executed on different physical processors or whether threads are scheduled competitively depends on the operating system. The way in which iterations of the parallel loop are assigned to the workers, on the other hand, is dependent on the scheduling policy <ref> [25, ch4] </ref>[27, p73-74][28, p387-392][29, 296-298] that is used. 2 In pre-scheduling, either a block of consecutive iterations is assigned statically to each worker (block-scheduling), or iterations are assigned statically in a cyclic fashion to the workers (cyclic scheduling). To reduce the potential of load imbalance, we can also use self-scheduling.
Reference: [26] <author> Constantine D. Polychronopoulos, David J. Kuck, and David A. Padua. </author> <title> Execution of parallel loops on parallel processor systems. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 519-527, </pages> <year> 1986. </year>
Reference: [27] <author> Michael J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: Alternatively, synchronization can be enforced using the primitives testset/test (or advance/await) [17, ch6][18, 19][28, p393-395], which can be implemented with a scalar synchronization variable <ref> [27, p84-86] </ref>. A more general form of synchronization in a DO-ACROSS-loop is provided by random synchronization with primitives post/wait [27, p75-83][29, p289-295]. <p> Alternatively, synchronization can be enforced using the primitives testset/test (or advance/await) [17, ch6][18, 19][28, p393-395], which can be implemented with a scalar synchronization variable [27, p84-86]. A more general form of synchronization in a DO-ACROSS-loop is provided by random synchronization with primitives post/wait <ref> [27, p75-83] </ref>[29, p289-295]. If data dependences are carried by the loop, a non-blocking post-statement that sets a bit corresponding to the current iteration is placed directly after the source statement of each static data dependence.
Reference: [28] <author> Michael J. Wolfe. </author> <title> High Performance Compilers for Parallel Computers. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1996. </year>
Reference-contexts: On shared-address space architectures, parallel loops can be executed using fork/join-like parallelism. In this approach, a master thread executes the serial parts of a program, and initiates a number of slave threads, or workers, when a parallel loop is reached <ref> [28, 385-387] </ref>. After all iterations of this loop have been executed, the workers synchronize using barrier synchronization. Whether all threads are actually executed on different physical processors or whether threads are scheduled competitively depends on the operating system.
Reference: [29] <author> H. Zima and B. Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1990. </year> <month> 14 </month>
Reference-contexts: Different synchronization variables, implemented as bit-arrays, are used to synchronize the different static data dependences in the loop. The automatic generation of synchronization is addressed in <ref> [16, 17, 18, 19, 29] </ref>. On shared-address space architectures, parallel loops can be executed using fork/join-like parallelism. In this approach, a master thread executes the serial parts of a program, and initiates a number of slave threads, or workers, when a parallel loop is reached [28, 385-387]. <p> In this paper, we focus on the parallelization of stride-1 loops that have the following form: L1: for (int i = low; i &lt; high; i++) body (i); Note that conventional compiler techniques, like constant propagation, scalar forward substitution, induction variable substitution, and loop normalization <ref> [1, 11, 24, 29] </ref> may be useful to convert some other loop constructs into this form. <p> cases where adding a new method and private variables to the class MyClass is undesirable, the compiler can resort to simply leaving all loops serial. 3.5.2 Insertion of Random Synchronization For DO-ACROSS-like execution, the compiler adds the appropriate synchronization primitives to the loop-body according to methods described in the literature <ref> [16, 17, 18, 19, 29] </ref>). Signaling iteration i on the synchronization variable with number k is implemented as sync [k].doPost (i). Likewise, waiting for iteration j on the synchronization variable with number k is implemented as sync [k].doWait (j).
References-found: 29

