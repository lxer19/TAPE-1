URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-94-25.ps.Z
Refering-URL: http://www.research.att.com/~yoav/publications.html
Root-URL: 
Title: Unsupervised learning of distributions on binary vectors using two layer networks  
Author: Yoav Freund David Haussler 
Address: Santa Cruz, CA 95064 USA  
Affiliation: Baskin Center for Computer Engineering Information Sciences University of California, Santa Cruz  
Date: June 22, 1994  
Pubnum: UCSC-CRL-94-25  
Abstract: We present a distribution model for binary vectors, called the influence combination model and show how this model can be used as the basis for unsupervised learning algorithms for feature selection. The model is closely related to the Harmonium model defined by Smolensky [RM86][Ch.6]. In the first part of the paper we analyze properties of this distribution representation scheme. We show that arbitrary distributions of binary vectors can be approximated by the combination model. We show how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits. We compare the combination model with the mixture model and with principle component analysis. In the second part of the paper we present two algorithms for learning the combination model from examples. The first algorithm is based on gradient ascent. Here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general Boltzmann machine. The second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time. This method is a variant of projection pursuit density estimation. In the third part of the paper we give experimental results for these learning methods on synthetic data and on natural data of handwritten digit images. 
Abstract-found: 1
Intro-found: 1
Reference: [AHS85] <author> D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. </author> <title> A learning algorithm for Boltzmann machines. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 147-169, </pages> <year> 1985. </year>
Reference-contexts: The goal of learning is reduced to finding the set of parameters for the combination model that maximizes the (log of the) probability of the set of examples S. In fact, this gives the standard learning algorithm for general Boltzmann machines <ref> [AHS85] </ref>. For a general Boltzmann machine this would require stochastic estimation of the parameters. As stochastic estimation is very time-consuming, the result is that learning is very slow. In this section we show that stochastic estimation need not be used for the combination model.
Reference: [CS89] <author> D. R. Cox and E. J. Snell. </author> <title> Analysis of binary data. </title> <publisher> Chapman and Hall, </publisher> <year> 1989. </year>
Reference: [DF84] <author> P. Diaconis and D. Freedman. </author> <title> Asymptotics of graphical projection pursuit. </title> <journal> Annals of Statistics, </journal> <volume> 12 </volume> <pages> 793-815, </pages> <year> 1984. </year>
Reference-contexts: It is easy to show that every one dimensional projection of a Gaussian distribution generates a Normal marginal distribution. Thus the marginal distribution that is generated by the real-valued combination model is a mixture of normal distributions. Diaconis and Friedman <ref> [DF84] </ref> have shown that, in some sense, most "well-behaved" distributions generate a marginal distribution that is close to normal when projected on a randomly chosen direction.
Reference: [DH73] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference: [DLR77] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Roy. Statist. Soc. B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: However, for the special case where there is only a single hidden unit in the model, a much faster method can be used. This method is an Expectation-Maximization (EM) method <ref> [DLR77] </ref>. EM is a general method for estimating the parameters of distribution models that have both observable and unobservable random variables. <p> It starts with some initial guess of the parameters init , and proceeds by iterating the following two steps. It can be shown <ref> [DLR77] </ref>, that each of these iterations improves the likelihood of the parameters. 1. Using the old setting of the parameters, old , as if they were the actual parameters, some statistics of the joint distribution of the hidden and the observable variables are calculated. 2.
Reference: [EH81] <author> B.S. </author> <title> Everitt and D.J. Hand. Finite mixture distributions. </title> <publisher> Chapman and Hall, </publisher> <year> 1981. </year>
Reference: [Fre87] <author> D. H. Freeman. </author> <title> Applied Catagorical Data Analysis. </title> <publisher> Marcel Dekker, </publisher> <year> 1987. </year>
Reference: [Fri87] <author> J. H. Friedman. </author> <title> Exploratory projection pursuit. </title> <journal> J. Amer. Stat.Assoc., </journal> <volume> 82(397) </volume> <pages> 599-608, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: Such methods would compute an approximation to the gradient by computing only the largest terms in the sum that defines it. 3.2 Projection Pursuit methods A statistical method that has a close relationship with the combination model is the Projection Pursuit (PP) technique <ref> [Hub85, FWS84, Fri87] </ref>. In this section we give a short overview of the technique, show how it relates to the combination model, and present a learning algorithm for the combination model based on Projection Pursuit methods. This algorithm is a greedy algorithm that generates the hidden units one by one. <p> The search for a description of the distribution of a sample in terms of its projections can be formalized in the context of maximal likelihood density estimation in the following way <ref> [Fri87] </ref>. Define p 0 (~x) to be the initial estimate of the density over R n , i.e. the Gaussian density with appropriate mean and covariance. Define G to be a family of functions from R to R and A to be the set of vectors of length 1, i.e. <p> The main problem of designing a projection pursuit method is finding a good projection index whose calculation can be performed efficiently. Various projection indices have been discussed in the literature <ref> [Hub85, Fri87] </ref>. Selection of a direction that has a high projection index is usually performed using gradient following methods. <p> Normally distributed. So called "structure removal" methods have been devised towards this goal <ref> [Hub85, Fri87] </ref>. These methods alter the sample in such a way that a specific single projection that has been interesting is made uninteresting while all orthogonal projections are left unchanged. <p> Projection Pursuit has proved itself successful in some experiments <ref> [Fri87] </ref>. However, the search for best density is performed in a greedy manner and might not succeed in finding the optimal density in PP m .
Reference: [FWS84] <author> J. H. Friedman, W.Stuetzle, and A. Schroeder. </author> <title> Projection pursuit density estimation. </title> <journal> J. Amer. Stat.Assoc., </journal> <volume> 79 </volume> <pages> 599-608, </pages> <year> 1984. </year>
Reference-contexts: Such methods would compute an approximation to the gradient by computing only the largest terms in the sum that defines it. 3.2 Projection Pursuit methods A statistical method that has a close relationship with the combination model is the Projection Pursuit (PP) technique <ref> [Hub85, FWS84, Fri87] </ref>. In this section we give a short overview of the technique, show how it relates to the combination model, and present a learning algorithm for the combination model based on Projection Pursuit methods. This algorithm is a greedy algorithm that generates the hidden units one by one. <p> Given a choice of ~ff (i) , the optimal choice of the function g i () in terms of maximizing the likelihood is the following <ref> [FWS84] </ref>. Define p ~ff (i) i (t) to be the marginal density on R generated by projecting the density p i on the direction ~ff (i) .
Reference: [Gem86] <author> Stuart Geman. </author> <title> Stochastic relaxation methods for image restoration and expert systems. In D.B. </title> <editor> Cooper, R.L.Launer, and D.E. McClure, editors, </editor> <title> Automated Image Analysis: Theory and Experiments. </title> <publisher> Academic Press, </publisher> <year> 1986. </year>
Reference: [GG84] <author> S Geman and D Geman. </author> <title> Stochastic relaxations, Gibbs distributions and the Bayesian restoration of images. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-742, </pages> <year> 1984. </year>
Reference: [GP87] <author> Hector Gefner and Judea Pearl. </author> <title> On the probabilistic semantics of connectionist networks. </title> <type> Technical Report CSD-870033, </type> <institution> UCLA Computer Science Department, </institution> <month> July </month> <year> 1987. </year>
Reference: [HKP91] <author> John Hertz, Anders Krogh, and Richard G. Palmer. </author> <title> Introduction To The Theory Of Neural Computation. </title> <publisher> Addison Wesley, </publisher> <year> 1991. </year>
Reference-contexts: The results are summarized in the following table and in Figure (4.1). 3 2 The algorithm used a standard momentum term (see <ref> [HKP91] </ref>, page 123) to accelerate the convergence. 3 The difference between the measurements of the quality of the true model on the test set and on the training set are due to the random fluctuations between the two sets of examples.
Reference: [Hop82] <author> J.J. </author> <title> Hopfield. Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad Sci. USA, </institution> <month> 79 </month> <pages> 2554-2558, </pages> <month> April </month> <year> 1982. </year>
Reference: [Hub85] <author> P.J. Huber. </author> <title> Projection pursuit (with discussion). </title> <journal> Ann. Stat., </journal> <volume> 13 </volume> <pages> 435-525, </pages> <year> 1985. </year>
Reference-contexts: Such methods would compute an approximation to the gradient by computing only the largest terms in the sum that defines it. 3.2 Projection Pursuit methods A statistical method that has a close relationship with the combination model is the Projection Pursuit (PP) technique <ref> [Hub85, FWS84, Fri87] </ref>. In this section we give a short overview of the technique, show how it relates to the combination model, and present a learning algorithm for the combination model based on Projection Pursuit methods. This algorithm is a greedy algorithm that generates the hidden units one by one. <p> The main problem of designing a projection pursuit method is finding a good projection index whose calculation can be performed efficiently. Various projection indices have been discussed in the literature <ref> [Hub85, Fri87] </ref>. Selection of a direction that has a high projection index is usually performed using gradient following methods. <p> Normally distributed. So called "structure removal" methods have been devised towards this goal <ref> [Hub85, Fri87] </ref>. These methods alter the sample in such a way that a specific single projection that has been interesting is made uninteresting while all orthogonal projections are left unchanged.
Reference: [Jol86] <author> I.T. Jolliffe. </author> <title> Principle Component Analysis. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: states of ~ h. 2 This interpretation of the real-valued model will be used in Section (3.5) in a Projection Pursuit algorithm for learning the combination model. 2.4 Comparison with principal components analysis Principal Component Analysis (PCA) is a popular method for the analysis of high order correlations (see e.g. <ref> [Jol86] </ref>). Many algorithms for unsupervised learning are based on this method, among them some learning rules for neural networks [San89, Oja89]. The method is based on the covariance matrix, which measures pairwise correlations among input bits.
Reference: [Nea90] <author> Radford M. Neal. </author> <title> Learning stochastic feedforward networks. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Toronto, </institution> <month> November </month> <year> 1990. </year>
Reference: [Now90] <author> S. Nowlan. </author> <title> Maximum likelihood competitive learning. </title> <editor> In D. Touretsky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 574-582. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: [Oja89] <author> E. Oja. </author> <title> Neural networks, principle components, and subspaces. </title> <journal> Int. J. Neural Systems, </journal> <volume> 1(1) </volume> <pages> 61-68, </pages> <year> 1989. </year>
Reference-contexts: Many algorithms for unsupervised learning are based on this method, among them some learning rules for neural networks <ref> [San89, Oja89] </ref>. The method is based on the covariance matrix, which measures pairwise correlations among input bits. The main assumption underlying the method is that the low dimension projections of the data that retain the largest amount of information are those projections that have the largest variance.
Reference: [PA87] <author> Carsten Peterson and James R. Anderson. </author> <title> A mean field theory learning algorithm for neural networks. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 995-1019, </pages> <year> 1987. </year> <note> 36 References </note>
Reference-contexts: However, here the clamped term is easy to calculate, it requires summing a logistic type function over all training examples. The same term is obtained by making the mean field approximation for the clamped phase in the general algorithm <ref> [PA87] </ref>, which is exact in this case.
Reference: [Pea88] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference: [RM86] <author> D. E. Rumelhart and J. L. McClelland. </author> <title> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </title> <booktitle> Volume 1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: The cross entropy is minimized when P = Q, and is then equal to the entropy of Q. The cross entropy can be estimated by taking the average value of minus log 1 The results are given using Hinton diagrams <ref> [RM86] </ref>, i.e. positive values are displayed as full rectangles, negative values as empty rectangles, and the area of the rectangle is proportional to the absolute value. 25 of the probability that the model assigns to each instance in the sample. This measure of error is also called the log-loss error.
Reference: [San89] <author> T.D. Sanger. </author> <title> Optimal unsupervised learning in a single-layer linear feedforward neural network. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 459-473, </pages> <year> 1989. </year>
Reference-contexts: Many algorithms for unsupervised learning are based on this method, among them some learning rules for neural networks <ref> [San89, Oja89] </ref>. The method is based on the covariance matrix, which measures pairwise correlations among input bits. The main assumption underlying the method is that the low dimension projections of the data that retain the largest amount of information are those projections that have the largest variance.
Reference: [Ser80] <author> R. J. Serfling. </author> <title> Approximation Theorems of Mathematical Statistics. </title> <publisher> John Wiley & Sons, </publisher> <year> 1980. </year> <month> 37 </month>
References-found: 24

