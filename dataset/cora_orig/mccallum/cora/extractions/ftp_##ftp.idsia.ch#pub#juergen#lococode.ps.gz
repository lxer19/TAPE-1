URL: ftp://ftp.idsia.ch/pub/juergen/lococode.ps.gz
Refering-URL: http://www.bmc.riken.go.jp/sensor/Allan/ICA/index.html
Root-URL: 
Email: hochreit@informatik.tu-muenchen.de  juergen@idsia.ch  
Title: Feature extraction through LOCOCODE  
Author: Sepp Hochreiter Jurgen Schmidhuber 
Date: May 1998  
Web: http://www7.informatik.tu-muenchen.de/~hochreit  http://www.idsia.ch/~juergen  
Address: 80290 Munchen, Germany  Corso Elvezia 36 6900 Lugano, Switzerland  
Affiliation: Fakultat fur Informatik Technische Universitat Munchen  IDSIA  
Abstract: Low-complexity coding and decoding" (Lococode) is a novel approach to sensory coding and unsupervised learning. Unlike previous methods it explicitly takes into account the information-theoretic complexity of the code generator: it computes lococodes that (1) convey information about the input data and (2) can be computed and decoded by low-complexity mappings. We implement Lococode by training autoassociators with Flat Minimum Search, a recent, general method for discovering low-complexity neural nets. It turns out that this approach can unmix an unknown number of independent data sources by extracting a minimal number of low-complexity features necessary for representing the data. Experiments show: unlike codes obtained with standard autoencoders, lococodes are based on feature detectors, never unstructured, usually sparse, sometimes factorial or local (depending on statistical properties of the data). Although Lococode is not explicitly designed to enforce sparse or factorial codes, it extracts optimal codes for difficult versions of the "bars" benchmark problem, whereas ICA and PCA do not. It produces familiar, biologically plausible feature detectors when applied to real world images, and codes with fewer bits per pixel than ICA and PCA. Unlike ICA it does not need to know the number of independent sources. As a preprocessor for a vowel recognition benchmark problem it sets the stage for excellent classification performance. Our results reveil an interesting, previously ignored connection between two important fields: regularizer research, and ICA-related research. They may represent a first step towards unification of regularization and unsupervised learning.
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S., Cichocki, A., and Yang, H. </author> <year> (1996). </year> <title> A new learning algorithm for blind signal separation. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8. </volume> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address> <note> 24 Baldi, </note> <author> P. and Hornik, K. </author> <year> (1989). </year> <title> Neural networks and principal component analysis: Learning from examples without local minima. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 53-58. </pages>
Reference: <author> Barlow, H. B. </author> <year> (1983). </year> <title> Understanding natural vision. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Barlow, H. B., Kaushal, T. P., and Mitchison, G. J. </author> <year> (1989). </year> <title> Finding minimum entropy codes. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 412-423. </pages>
Reference: <author> Barrow, H. G. </author> <year> (1987). </year> <title> Learning receptive fields. </title> <booktitle> In Proceedings of the IEEE 1st Annual Conference on Neural Networks, volume IV, </booktitle> <pages> pages 115-121. </pages> <publisher> IEEE. </publisher>
Reference: <author> Baumgartner, M. </author> <year> (1996). </year> <institution> Bilddatenvorverarbeitung mit neuronalen Netzen. Diploma thesis, Institut fur Informatik, Lehrstuhl Prof. Brauer, Technische Universitat Munchen. </institution>
Reference: <author> Becker, S. </author> <year> (1991). </year> <title> Unsupervised learning procedures for neural networks. </title> <journal> International Journal of Neural Systems, </journal> <volume> 2(1 </volume> & 2):17-33. 
Reference-contexts: Such codes can be advantageous for (1) data compression, (2) speeding up subsequent gradient descent learning <ref> (e.g., Becker 1991) </ref>, (3) simplifying subsequent Bayes classifiers (e.g., Schmidhuber et al. 1996). Other approaches favor local codes, e.g., Rumelhart and Zipser (1986), Barrow (1987), Kohonen (1988).
Reference: <author> Bell, A. J. and Sejnowski, T. J. </author> <year> (1995). </year> <title> An information-maximization approach to blind separation and blind deconvolution. </title> <journal> Neural Computation, </journal> <volume> 7(6) </volume> <pages> 1129-1159. </pages>
Reference: <author> Cardoso, J.-F. and Souloumiac, A. </author> <year> (1993). </year> <title> Blind beamforming for non Gaussian signals. </title> <journal> IEE Proceedings-F, </journal> <volume> 140(6) </volume> <pages> 362-370. </pages>
Reference: <author> Comon, P. </author> <year> (1994). </year> <title> Independent component analysis anew concept? Signal Processing, </title> <booktitle> 36(3) </booktitle> <pages> 287-314. </pages>
Reference: <author> Dayan, P. and Zemel, R. </author> <year> (1995). </year> <title> Competition and multiple cause models. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 565-579. </pages>
Reference-contexts: This reflects a basic assumption, namely, that the true input "causes" <ref> (e.g., Hinton et al. 1995, Dayan and Zemel 1995, Ghahramani 1995) </ref> are indeed few and simple. <p> Training and testing. Each of the 10 possible bars appears with probability 1 5 . In contrast to Dayan and Zemel's set-up (1995) we allow for bar type mixing. This makes the task harder <ref> (Dayan and Zemel 1995, p. 570) </ref>. To test Lococode's ability to reduce redundancy, we use many more HUs (namely 25) than the required minimum of 10. Dayan and Zemel report that an AA trained without FMS (and more than 10 HUs) "consistently failed". <p> Conclusion. Lococode solves a hard variant of the standard "bars" problem. It discovers the underlying statistics and extracts the essential, statistically independent features, even in presence of noise. Standard BP AAs accomplish none of these feats <ref> (Dayan and Zemel, 1995) </ref> | this has been confirmed by additional experiments conducted by ourselves. ICA and PCA also fail to extract the true input causes and the optimal features. Lococode achieves success solely by reducing information-theoretic (de)coding costs.
Reference: <author> Deco, G. and Brauer, W. </author> <year> (1995). </year> <title> Nonlinear higher-order statistical decorrelation by volume-conserving neur al architectures. </title> <booktitle> Neural Networks, </booktitle> <volume> 8(4) </volume> <pages> 525-535. </pages>
Reference: <author> Deco, G. and Parra, L. </author> <year> (1994). </year> <title> Nonlinear features extraction by unsupervised redundancy reduction with a stochastic neural network. </title> <type> Technical Report ZFE ST SN 41, </type> <institution> Siemens AG. </institution>
Reference: <author> DeMers, D. and Cottrell, G. </author> <year> (1993). </year> <title> Non-linear dimensionality reduction. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 580-587. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Field, B. A. O. D. J. </author> <year> (1996). </year> <title> Emergence of simple-cell receptive field properties by learning a sparse code for natural images. </title> <journal> Nature, </journal> <volume> 381(6583) </volume> <pages> 607-609. </pages>
Reference-contexts: It also produces biologically plausible sparse codes (standard AAs do not). FMS's objective function, however, does not contain explicit terms enforcing such codes <ref> (this contrasts previous methods, e.g., Olshausen and Field 1996) </ref>. The experiments show that equally-sized PCA codes, ICA codes, and lococodes convey ap 19 proximately the same information. Lococode, however, codes with fewer bits per pixel. Unlike PCA and ICA, it determines the code size automatically. <p> Depending on the statistical properties of the input, this can result in either local, factorial, or sparse codes, although biologically plausible sparseness is the most common case. Unlike the objective functions of previous methods <ref> (e.g., Olshausen and Field 1996) </ref>, however, Lococode's does not contain an explicit term enforcing, say, sparse codes | sparseness or factoriality are not viewed as a good things a priori.
Reference: <author> Field, D. J. </author> <year> (1987). </year> <title> Relations between the statistics of natural images and the response properties of cortical cells. </title> <journal> Journal of the Optical Society of America, </journal> <volume> 4 </volume> <pages> 2379-2394. </pages>
Reference: <author> Field, D. J. </author> <year> (1994). </year> <title> What is the goal of sensory coding? Neural Computation, </title> <booktitle> 6 </booktitle> <pages> 559-601. </pages>
Reference: <author> Flake, G. W. </author> <year> (1998). </year> <title> Square unit augmented, radially extended, multilayer perceptrons. </title> <editor> In Orr, G. B. and Muller, K.-R., editors, </editor> <title> Tricks of the Trade. </title> <publisher> Springer Verlag, </publisher> <address> Berlin. </address> <note> To appear in Lecture Notes in Computer Science. </note>
Reference: <author> Foldiak, P. </author> <year> (1990). </year> <title> Forming sparse representations by local anti-Hebbian learning. </title> <journal> Biological Cybernetics, </journal> <volume> 64 </volume> <pages> 165-170. </pages>
Reference: <author> Foldiak, P. and Young, M. P. </author> <year> (1995). </year> <title> Sparse coding in the primate cortex. </title> <editor> In Arbib, M. A., editor, </editor> <booktitle> The Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 895-898. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: <author> Ghahramani, Z. </author> <year> (1995). </year> <title> Factorial learning and the EM algorithm. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 617-624. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address> <note> 25 Hassibi, </note> <author> B. and Stork, D. G. </author> <year> (1993). </year> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 164-171. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This reflects a basic assumption, namely, that the true input "causes" <ref> (e.g., Hinton et al. 1995, Dayan and Zemel 1995, Ghahramani 1995) </ref> are indeed few and simple.
Reference: <author> Hastie, T. J. and Tibshirani, R. J. </author> <year> (1996). </year> <title> Discriminant adaptive nearest neighbor classification. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 18(6) </volume> <pages> 607-616. </pages>
Reference: <author> Hastie, T. J., Tibshirani, R. J., and Buja, A. </author> <year> (1993). </year> <title> Flexible discriminant analysis by optimal scoring. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories. </institution>
Reference: <author> Herrmann, M. </author> <year> (1997). </year> <title> On the merits of topography in neural maps. </title> <editor> In Kohonen, T., editor, </editor> <booktitle> Proceedings of the Workshop on Self-Organizing Maps, </booktitle> <pages> pages 112-117. </pages> <institution> Helsinki University of Technology. </institution>
Reference: <author> Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. </author> <year> (1995). </year> <title> The wake-sleep algorithm for unsupervised neural networks. </title> <journal> Science, </journal> <volume> 268 </volume> <pages> 1158-1160. </pages>
Reference-contexts: This reflects a basic assumption, namely, that the true input "causes" <ref> (e.g., Hinton et al. 1995, Dayan and Zemel 1995, Ghahramani 1995) </ref> are indeed few and simple.
Reference: <author> Hinton, G. E. and Ghahramani, Z. </author> <year> (1997). </year> <title> Generative models for discovering sparse distributed representations. </title> <journal> Philosophical Transactions of the Royal Society B, </journal> <volume> 352 </volume> <pages> 1177-1190. </pages>
Reference: <author> Hinton, G. E. and Zemel, R. S. </author> <year> (1994). </year> <title> Autoencoders, minimum description length and Helmholtz free energy. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 3-10. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1995). </year> <title> Simplifying neural nets by discovering flat minima. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 529-536. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference-contexts: In contrast, weights in a "sharp" minimum require a high-precision specification. As a natural by-product of net complexity reduction, FMS automatically prunes weights and units, and reduces output sensitivity with respect to remaining weights and units. Previous FMS applications focused on supervised learning <ref> (Hochreiter and Schmidhuber 1995, 1997a) </ref>: FMS led to better stock market prediction results than "weight decay" and "optimal brain surgeon" (Hassibi and Stork 1993). In this paper, however, we will use it for unsupervised coding only. Architecture. We use a 3-layer feedforward net.
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1997a). </year> <title> Flat minima. </title> <journal> Neural Computation, </journal> <volume> 9(1) </volume> <pages> 1-42. </pages>
Reference-contexts: By minimizing coding/decoding costs Lococode will yield efficient, robust, noise-tolerant mappings for processing inputs and codes. Lococodes through FMS. To implement Lococode we apply Flat Minimum Search <ref> (FMS, Hochreiter and Schmidhuber 1997a) </ref> to an autoassociator (AA) whose hidden layer activations represent the code. FMS is a general, gradient-based method for finding networks that can be described with few bits of information. Coding each input via few simple component functions (CFs). <p> tiny first order derivatives of output units (which may cause floating point overflows), and (2) allows for proving that the FMS algorithm makes the Hessian entries of output units @ 2 y k @w ij @w uv decrease where the weight precisions jffiw ij j or jffiw uv j increase <ref> (Hochreiter and Schmidhuber 1997a) </ref>. Parameters and other details. * learning rate: conventional learning rate for error term E (just like backprop's). * : a positive "regularizer" (hyperparameter) scaling B's influence. is computed heuristi cally as described by Hochreiter and Schmidhuber (1997a). * : a value used for updating during learning. <p> We finish in phase 2. The experimental sections will indicate 2-phase learning by mentioning values of fl. * Pruning of weights and units: we judge a weight w ij as being pruned if its required precision <ref> (jffiw ij j in Hochreiter and Schmidhuber 1997a) </ref> for each input is 100 times lower (corresponding to 2 decimal digits) than the highest precision of the other weights for the same input.
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1997b). </year> <title> Low-complexity coding and decoding. </title> <editor> In Wong, K. M., King, I., and Yeung, D., editors, </editor> <booktitle> Theoretical Aspects of Neural Computation, </booktitle> <pages> pages 297-306. </pages> <publisher> Springer. </publisher>
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1997c). </year> <title> Unsupervised coding with LOCOCODE. </title> <editor> In Gerstner, W., Germond, A., Hasler, M., and Nicoud, J.-D., editors, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks, Lausanne, Switzerland, </booktitle> <pages> pages 655-660. </pages> <publisher> Springer. </publisher>
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1998). </year> <title> Lococode versus PCA and ICA. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks. To appear. </booktitle>
Reference: <author> Jutten, C. and Herault, J. </author> <year> (1991). </year> <title> Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture. </title> <booktitle> Signal Processing, </booktitle> <volume> 24(1) </volume> <pages> 1-10. </pages>
Reference: <author> Kohonen, T. </author> <year> (1988). </year> <title> Self-Organization and Associative Memory. </title> <publisher> Springer, second edition. </publisher>
Reference: <author> Kramer, M. </author> <year> (1991). </year> <title> Nonlinear principal component analysis using autoassociative neural networks. </title> <journal> AIChE Journal, </journal> <volume> 37 </volume> <pages> 233-243. </pages>
Reference: <author> Li, Z. </author> <year> (1995). </year> <title> A theory of the visual motion coding in the primary visual cortex. </title> <journal> Neural Computation, </journal> <volume> 8(4) </volume> <pages> 705-730. </pages>
Reference: <author> Linsker, R. </author> <year> (1988). </year> <title> Self-organization in a perceptual network. </title> <journal> IEEE Computer, </journal> <volume> 21 </volume> <pages> 105-117. </pages>
Reference-contexts: ICA and PCA also fail to extract the true input causes and the optimal features. Lococode achieves success solely by reducing information-theoretic (de)coding costs. Unlike previous approaches, it does not depend on explicit terms enforcing independence (e.g., Schmidhu-ber 1992), zero mutual information among code components <ref> (e.g., Linsker 1988, Deco and Parra 1994) </ref>, or sparseness (e.g., Field 1994, Zemel and Hinton 1994, Olshausen and Field 1996, Zemel 1993, Hinton and Ghahramani 1997). Lococode vs. ICA.
Reference: <author> Molgedey, L. and Schuster, H. G. </author> <year> (1994). </year> <title> Separation of independent signals using time-delayed correlations. </title> <journal> Phys. Reviews Letters, </journal> <volume> 72(23) </volume> <pages> 3634-3637. </pages>
Reference: <author> Mozer, M. C. </author> <year> (1991). </year> <title> Discovering discrete distributed representations with iterative competitive learning. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 627-634. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <address> 26 Nadal, </address> <note> J.-P. </note> <author> and Parga, N. </author> <year> (1997). </year> <title> Redundancy reduction and independent component analysis: Conditions on cumulants and adaptive approaches. </title> <journal> Neural Computation, </journal> <volume> 9(7) </volume> <pages> 1421-1456. </pages>
Reference: <author> Oja, E. </author> <year> (1989). </year> <title> Neural networks, principal components, and subspaces. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(1) </volume> <pages> 61-68. </pages>
Reference-contexts: Comparison. In sections 4.3 and 4.4 we compare Lococode to simple variants of "independent component analysis" (ICA, e.g., Jutten and Herault 1991, Cardoso and Souloumiac 1993, Molgedey and Schuster 1994, Comon 1994, Bell and Sejnowski 1995, Amari et al. 1996, Nadal and Parga 1997) and "principal component analysis" <ref> (PCA, e.g., Oja 1989) </ref>. ICA is realized by Car-doso's (1993) JADE (Joint Approximate Diagonalization of Eigen-matrices) algorithm (we used 7 the Matlab JADE version obtained via FTP from sig.enst.fr). JADE is based on whitening and subsequent joint diagonalization of 4th-order cumulant matrices.
Reference: <author> Oja, E. </author> <year> (1991). </year> <title> Data compression, feature extraction, and autoassociation in feedforward neural networks. </title> <editor> In Kohonen, T., Makisara, K., Simula, O., and Kangas, J., editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 737-745. </pages> <publisher> Elsevier Science Publishers B.V., North-Holland. </publisher>
Reference: <author> Pajunen, P. </author> <year> (1998). </year> <title> Blind source separation using algorithmic information theory. </title> <editor> In Fyfe, C., editor, </editor> <booktitle> Proceedings of Independence and Artificial Neural Networks (I & ANN), </booktitle> <pages> pages 26-31. </pages> <publisher> ICSC Academic Press. </publisher>
Reference: <author> Palm, G. </author> <year> (1992). </year> <title> On the information storage capacity of local learning rules. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 703-711. </pages>
Reference: <author> Redlich, A. N. </author> <year> (1993). </year> <title> Redundancy reduction as a strategy for unsupervised learning. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 289-304. </pages>
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference: <author> Robinson, A. J. </author> <year> (1989). </year> <title> Dynamic Error Propagation Networks. </title> <type> PhD thesis, </type> <institution> Trinity Hall and Cambridge University Engineering Department. </institution>
Reference-contexts: Lococode's, however, are found automatically. The final column shows coding efficiency measured in bits per pixels (for code components mapped to 100 discrete intervals) and reconstruction error (for this discrete code). Lococode exhibits superior coding efficiency. Task. We recognize vowels, using vowel data from Scott Fahlman's CMU benchmark collection <ref> (see also Robinson 1989) </ref>. There are 11 vowels and 15 speakers. Each speaker spoke each vowel 6 times. Data from the first 8 speakers is used for training. The other data is used for testing. This means 528 frames for training and 462 frames for testing.
Reference: <author> Rumelhart, D. E. and Zipser, D. </author> <year> (1986). </year> <title> Feature discovery by competitive learning. </title> <booktitle> In Parallel Distributed Processing, </booktitle> <pages> pages 151-193. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Saund, E. </author> <year> (1994). </year> <title> Unsupervised learning of mixtures of multiple causes in binary data. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 27-34. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Saund, E. </author> <year> (1995). </year> <title> A multiple cause mixture model for unsupervised learning. </title> <booktitle> Neural Computation. </booktitle>
Reference: <author> Schmidhuber, J. </author> <year> (1992). </year> <title> Learning factorial codes by predictability minimization. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 863-879. </pages>
Reference: <author> Schmidhuber, J. </author> <year> (1997a). </year> <title> Discovering neural nets with low Kolmogorov complexity and high generalization capability. </title> <booktitle> Neural Networks, </booktitle> <volume> 10(5) </volume> <pages> 857-873. </pages>
Reference-contexts: By minimizing coding/decoding costs Lococode will yield efficient, robust, noise-tolerant mappings for processing inputs and codes. Lococodes through FMS. To implement Lococode we apply Flat Minimum Search <ref> (FMS, Hochreiter and Schmidhuber 1997a) </ref> to an autoassociator (AA) whose hidden layer activations represent the code. FMS is a general, gradient-based method for finding networks that can be described with few bits of information. Coding each input via few simple component functions (CFs). <p> tiny first order derivatives of output units (which may cause floating point overflows), and (2) allows for proving that the FMS algorithm makes the Hessian entries of output units @ 2 y k @w ij @w uv decrease where the weight precisions jffiw ij j or jffiw uv j increase <ref> (Hochreiter and Schmidhuber 1997a) </ref>. Parameters and other details. * learning rate: conventional learning rate for error term E (just like backprop's). * : a positive "regularizer" (hyperparameter) scaling B's influence. is computed heuristi cally as described by Hochreiter and Schmidhuber (1997a). * : a value used for updating during learning. <p> We finish in phase 2. The experimental sections will indicate 2-phase learning by mentioning values of fl. * Pruning of weights and units: we judge a weight w ij as being pruned if its required precision <ref> (jffiw ij j in Hochreiter and Schmidhuber 1997a) </ref> for each input is 100 times lower (corresponding to 2 decimal digits) than the highest precision of the other weights for the same input.
Reference: <author> Schmidhuber, J. </author> <year> (1997b). </year> <title> Low-complexity art. </title> <journal> Leonardo, Journal of the International Society for the Arts, Sciences, and Technology, </journal> <volume> 30(2) </volume> <pages> 97-103. </pages>
Reference-contexts: Finally, encouraged by our successful application to vowel classification, we intend to look at more complex pattern recognition tasks. We also intend to look at alternative Lococode implementations besides FMS-based AAs. Finally we would like to improve our understanding of the relationship between low-complexity codes, low-complexity art <ref> (see Schmidhuber, 1997b) </ref> and informal notions such as "beauty" and "good art". 6 ACKNOWLEDGMENTS We would like to thank Peter Dayan, Manfred Opper, Nic Schraudolph, Rich Zemel, and several anonymous reviewers for helpful discussions and for comments on a draft of this paper.
Reference: <author> Schmidhuber, J., Eldracher, M., and Foltin, B. </author> <year> (1996). </year> <title> Semilinear predictability minimization produces well-known feature detectors. </title> <journal> Neural Computation, </journal> <volume> 8(4) </volume> <pages> 773-786. </pages>
Reference-contexts: Such codes can be advantageous for (1) data compression, (2) speeding up subsequent gradient descent learning (e.g., Becker 1991), (3) simplifying subsequent Bayes classifiers <ref> (e.g., Schmidhuber et al. 1996) </ref>. Other approaches favor local codes, e.g., Rumelhart and Zipser (1986), Barrow (1987), Kohonen (1988). They can help to achieve (1) minimal crosstalk, (2) subsequent gradient descent speedups, (3) facilitation of post training analysis, (4) simultaneous representation of different data items.
Reference: <author> Schmidhuber, J. and Prelinger, D. </author> <year> (1993). </year> <title> Discovering predictable classifications. </title> <journal> Neural Computation, </journal> <volume> 5(4) </volume> <pages> 625-635. </pages>
Reference: <author> Schraudolph, N. and Sejnowski, T. J. </author> <year> (1993). </year> <title> Unsupervised discrimination of clustered data via optimization of binary information gain. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> pages 499-506. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference: <author> Schraudolph, N. N. </author> <year> (1998). </year> <title> On centering neural network weight updates. </title> <editor> In Orr, G. B. and Muller, K.-R., editors, </editor> <title> Tricks of the Trade. </title> <publisher> Springer Verlag, </publisher> <address> Berlin. </address> <note> To appear in Lecture Notes in Computer Science. </note>
Reference: <author> Solomonoff, R. </author> <year> (1964). </year> <title> A formal theory of inductive inference. Part I. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22. </pages>
Reference-contexts: Generating this code and using it for dealing with everyday events, however, would be extremely inefficient. A previous argument for ignoring coding costs (e.g., Zemel 1993, Zemel and Hinton 1994, Hin-ton and Zemel 1994), based on the principle of minimum description length <ref> (MDL, e.g., Solomonoff 1964, Wallace and Boulton 1968, Rissanen 1978) </ref>, focuses on hypothetical costs of transmitting the data from some sender to a receiver | how many bits are necessary to enable the receiver to reconstruct the data? It goes more or less like this: "Total transmission cost is the number
Reference: <author> Tenenbaum, J. B. and Freeman, W. T. </author> <year> (1997). </year> <title> Separating style and content. </title> <editor> In Mozer, M. C., Jordan, M. I., and Petsche, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9, </booktitle> <pages> pages 662-668. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Turney, P. D. </author> <year> (1993). </year> <title> Exploiting context when learning to classify. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 402-407. </pages> <month> ftp://ai.iit.nrc.ca/pub/ksl-papers/NRC-35058.ps.Z. </month>
Reference: <author> Wallace, C. S. and Boulton, D. M. </author> <year> (1968). </year> <title> An information theoretic measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11(2) </volume> <pages> 185-194. </pages>
Reference: <author> Watanabe, S. </author> <year> (1985). </year> <title> Pattern Recognition: </title> <booktitle> Human and Mechanical. </booktitle> <address> Willey, New York. </address>
Reference: <author> Zemel, R. S. </author> <year> (1993). </year> <title> A minimum description length framework for unsupervised learning. </title> <type> PhD thesis, </type> <institution> University of Toronto. </institution>
Reference-contexts: In fact, the most compact code of the possible environmental inputs would be the "true" probabilistic causal model corresponding to our universe's most basic physical laws. Generating this code and using it for dealing with everyday events, however, would be extremely inefficient. A previous argument for ignoring coding costs <ref> (e.g., Zemel 1993, Zemel and Hinton 1994, Hin-ton and Zemel 1994) </ref>, based on the principle of minimum description length (MDL, e.g., Solomonoff 1964, Wallace and Boulton 1968, Rissanen 1978), focuses on hypothetical costs of transmitting the data from some sender to a receiver | how many bits are necessary to enable
Reference: <author> Zemel, R. S. and Hinton, G. E. </author> <year> (1994). </year> <title> Developing population codes by minimizing description length. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 11-18. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 28 </pages>
References-found: 62

