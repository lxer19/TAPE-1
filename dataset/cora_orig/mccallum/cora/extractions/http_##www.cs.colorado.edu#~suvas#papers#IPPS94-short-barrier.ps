URL: http://www.cs.colorado.edu/~suvas/papers/IPPS94-short-barrier.ps
Refering-URL: http://www.cs.colorado.edu/~suvas/Papers.html
Root-URL: http://www.cs.colorado.edu
Email: (grunwald,suvas)@cs.colorado.edu  
Title: Efficient Barriers for Distributed Shared Memory Computers  
Author: Dirk Grunwald Suvas Vajracharya 
Address: Boulder, CO 80309-0430  
Affiliation: Department of Computer Science University of Colorado  
Abstract: In this paper, we present two new barrier algorithms that offer the best performance we have recorded on the KSR-1 distributed cache multiprocessor. We discuss the trade-offs and the performance of seven algorithms on two architectures. The new barrier algorithms adapt well to a hierarchical caching memory model and take advantage of parallel communication offered by most multiprocessor interconnection networks. Performance results are shown for a 256-processor KSR-1 and a 20-processor Sequent Symmetry. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T.E. Anderson. </author> <title> The performance of spinlock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The hotspot problem can be somewhat alleviated in cache coherent machines because a copy of the counter is copied to local memory and updated via broadcasts; this reduces the amount of network communication. Various centralized barrier and lock designs make effective use of local caches <ref> [1] </ref>. However, processors writeing to the barrier contend between themselves, and arrival still requires O (N) time. 2.2 Software Combining Trees Yew et al. [12] proposed a combining tree barrier to reduce the occurence of hot spots. A software combining tree spreads the congestion over a tree of variables.
Reference: [2] <author> Seif Haridi Erik Hagersten and David H.D. Warren. </author> <title> Cache and interconnect architectures in multiprocessors. The cache-coherence protocol of the data diffusion machine, </title> <year> 1990. </year>
Reference: [3] <author> Allan Gottlieb, Ralph Grishman, Clyde P. Kruskal, Kevin P. McAuliffe, Larray Rudolph, and Marc Snir. </author> <title> The nyu ultracomputer: Designing a mimd, shared-memory parallel machine. </title> <booktitle> Proceedings of 9th Annual International Symposium on Computer, </booktitle> <volume> 10(3) </volume> <pages> 27-42, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Once the rendezvous has been achieved, the counter is reset to zero for the next rendezvous. There are two disadvantages to this approach. First, the counter must be updated atomically, either via explicit locking or hardware operations such as fetch and <ref> [3] </ref>. Second, all processes must contend with each other to read and write a single memory location. As mentioned, this causes hot-spots, or points of high traffic congestion. Consequently, this barrier is not scalable since each read and a write involves serialized actions.
Reference: [4] <author> Dirk Grunwald and Suvas Vajracharya. </author> <title> Efficient barriers for shared memory computers. </title> <type> CU-CS 703-94., </type> <institution> Univ. of Colorado, Boulder, Campus Box 430, Univ. of Colorado, Boulder CO, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: The parameter f varies between two and the number of bytes in memory word. Due to space considerations, we can not show the full algorithm in this paper; interested readers are referred to an extended version of this paper <ref> [4] </ref> for more information. Furthermore, implementations of these algorithms are available; contact the authors for more information. As with other algorithms, a P fi log f P matrix is used to hold pre-determined scheduling information about the role of each processor in the barrier. <p> Again, the full algorithm is described in the longer version of this pa per <ref> [4] </ref>. Like the Software Combining Tree, the last process to arrive at a node goes on to participate at higher level node (s). <p> In <ref> [4] </ref>, we present data that verifies their results, and show that the Static f -way Tournament is slightly faster than the other algorithms for a larger number of processors. <p> We are currently expanding on the algorithms presented here; currently, we have found that the Dynamic f-way algorithm is faster than the other algorithms on the BBN TC-2000; see <ref> [4] </ref> for more details. We are also considering the interaction of the barrier algorithms with operating system rescheduling; this will necessitate the recomputation of the predetermined information associated with the different processors. This work was funded in part by NSF grant No.
Reference: [5] <author> D. Hensgen, R. Finkel, and U. Manber. </author> <title> Two algorithms for barrier synchronization. </title> <journal> Intl. Journal of Parallel Programming, </journal> <volume> 17(1), </volume> <year> 1988. </year>
Reference-contexts: To alleviate these problems, Brooks described the butterfly barrier [7]; this communication pattern is similar to the exchange-swap operation used in hypercube interconnection networks. Hensgen et al <ref> [5] </ref> improved the butterfly-network for situations where the number of processors accessing the barrier are not a power of two. Their dissemination barrier uses the communication structure shown in Figure 1.
Reference: [6] <author> W. Daniel Hillis and G.L. Steele. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29, No.12:1170-1183, </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: 1 Introduction Barriers are a synchronization tool for parallel computers, including shared and distributed (message-passing) address-space architectures. No processor may pass the barrier until all processes have arrived at the barrier; this synchronization tool is used in many algorithms, and is central to the data-parallel programming model <ref> [6] </ref>. There are numerous barrier algorithms for message-passing and shared-address space computers. Some architectures, such as the Thinking Machines CM-5, provide special hardware support for barrier synchronization. On architectures lacking such hardware support, scalable barriers must be implemented in software, using the underlying communication network.
Reference: [7] <author> Edward D. Brooks III. </author> <title> The butterfly barrier. </title> <journal> Intl. Journal of Parallel Programming, </journal> <volume> 15(4) </volume> <pages> 295-307, </pages> <year> 1986. </year>
Reference-contexts: Second, both methods spin at a remote memory location, leading to unnecessary contention for the interconnection bandwidth on machines that are not broadcast-based and lack cache-coherency. To alleviate these problems, Brooks described the butterfly barrier <ref> [7] </ref>; this communication pattern is similar to the exchange-swap operation used in hypercube interconnection networks. Hensgen et al [5] improved the butterfly-network for situations where the number of processors accessing the barrier are not a power of two. Their dissemination barrier uses the communication structure shown in Figure 1.
Reference: [8] <author> Boris D. Lubachecsky. </author> <title> Synchronization barrier and relation tools for shared memory parallel programs. </title> <booktitle> In Proc. of the 1989 Int. Conf. on Parallel Processing, pages II-175-II-179. </booktitle> <institution> Penn State, </institution> <year> 1989. </year>
Reference-contexts: We believe the communication structure of a barrier algorithm must match the physical interconnection network for best performance. 2.4 Tournament Algorithm To counter the additional communication in the dissemination algorithm, Hensgen et al also developed the tournament algorithm, apparently at the suggestion of Lubachevsky <ref> [8] </ref>. As with the dissemination algorithm, the tournament algorithm avoids special hardware by using a pre-determined communication structure; however, the tournament algorithm incurs only O (log 2 P ) total network transactions, as illustrated by the communication structure shown in Figure 1.
Reference: [9] <author> John Mellor-Crummey and Michael Scott. </author> <title> Algorithms for scalable synchronization on shared memory multiprocessors. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: In this paper, we describe several existing barrier implementations, propose two new barrier algorithm and describe the performance of each algorithm on a Cache Only Memory Access (COMA) architecture, the KSR-1, and a shared-bus cache-consistent architecture (the Sequent Symmetry). We show that conclusions drawn in previous studies of barriers <ref> [9] </ref> are not necessarily true on COMA architectures, highlighting the need for a formal model to predict the performance of barrier algorithms. 2 Previous Barrier Algorithms As computer architectures have evolved, numerous barrier algorithms have been proposed. A central barrier suffices for small-scale cache-coherent multiprocessors with 2-10 processors. <p> Processors that are not group leaders busy-wait, awaiting wakeup notification of the barrier completion. A number of methods can be used for wakeup notification. In broadcast cache-coherent architectures (such as the KSR or Sequent), a central busy-flag can be used <ref> [9] </ref>. On architectures where non-local memory references must use the network, a tree-based wake-up algorithm is used. The counters at each node are reset when the barrier is reinitialized. <p> During each synchronization `round', each processor either waits on a locally available flag or signals the partner via writing the flag. The flags are reinitialized by sense-reversing the flag after each round. In a previous study <ref> [9] </ref>, the dissemination algorithm achieved the best performance of a variety of barrier algorithms on non-uniform memory shared-address machines without cache-coherency and broadcasting, such as BBN Butterfly. On such architectures, the `local spinning' provides an advantage for this algorithm. <p> Wakeup notification is done using either a global wakeup flag for broadcast-based machines or a tree-wakeup algorithm. Reinitialization is done by sense reversing the flags. 2.5 MCS-Tree Algorithm Mellor-Crummey et al <ref> [9] </ref> proposed a variation on the tournament algorithm called the MCS-Tree barrier. Figure 2 depicts the communication structure for the MCS-Tree barrier algorithm, and Figure 3 presents an alternate view for a larger number of processors. In this algorithm readers (parents) and writers (children) are statically pre-determined during barrier creation. <p> For all the tree-based algorithms, we used a central global flag that is broadcasted to the waiting processes during the wakeup phase of the algorithms. This method was shown to be more appropriate in a previous study <ref> [9] </ref> for cache-coherent machines that provide fast broadcast operations. On the KSR, we implemented the broadcast using the _pstsp (post-store subpage) instruction. In theory, we can use this instruction to update the contents of the modified word in a single tour of the network. <p> Note that the dissemination barrier suffers a sharp increase in the time to rendezvous between 32 and 33 processors; this is when inter-cluster communication occurs in the KSR-1 interconnection network. Previous studies <ref> [9] </ref> have shown that the MCS algorithm was faster than other algorithms such as the tournament algorithm. However, there is a greater degree of parallelism in the tournament algorithm, and that parallelism can be supported on the KSR-1. This was also recently noted by Ramachandran et al [11].
Reference: [10] <author> G. Pfister and V. Norton. </author> <title> Hot spot contention and combining in multistage interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):943-948, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: A central barrier suffices for small-scale cache-coherent multiprocessors with 2-10 processors. As the promise of larger-scale systems including tens and hundreds of processors was realized, it was noted that centralized barriers limited system performance. Concentrated communication to a single memory location can induce a hot spot <ref> [10] </ref> in the network. Both hardware solutions, including combining networks, and software solutions [12] have been proposed to alleviate hotspots. Several barrier algorithms have been described that distribute the communication, either over different cache locations, network connections or processor clusters.
Reference: [11] <author> Umakishore Ramachandran, Gautam Shah, S. Ravikumar, and Jeyakumar Muthuku-marasamy. </author> <title> Scalability study of the ksr-1. </title> <type> GIT-CC 93/03, </type> <institution> Georgia Inst. of Technology, </institution> <year> 1993. </year>
Reference-contexts: Previous studies [9] have shown that the MCS algorithm was faster than other algorithms such as the tournament algorithm. However, there is a greater degree of parallelism in the tournament algorithm, and that parallelism can be supported on the KSR-1. This was also recently noted by Ramachandran et al <ref> [11] </ref>. Furthermore, the tournament algorithm involves less inter-cluster communication, which is very important on the KSR-1 intra-cluster memory references take 150 machine cycles, while inter-cluster references take 600 cycles.
Reference: [12] <author> Pen Yew, N. Tzeng, and Ducan Lawrie. </author> <title> Distributing host-spot addressing in large-scale multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 388-395, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: As the promise of larger-scale systems including tens and hundreds of processors was realized, it was noted that centralized barriers limited system performance. Concentrated communication to a single memory location can induce a hot spot [10] in the network. Both hardware solutions, including combining networks, and software solutions <ref> [12] </ref> have been proposed to alleviate hotspots. Several barrier algorithms have been described that distribute the communication, either over different cache locations, network connections or processor clusters. Most of these algorithms use some tree-based structure to distribute the communication. In the remainder of this section we describe several such algorithms. <p> Various centralized barrier and lock designs make effective use of local caches [1]. However, processors writeing to the barrier contend between themselves, and arrival still requires O (N) time. 2.2 Software Combining Trees Yew et al. <ref> [12] </ref> proposed a combining tree barrier to reduce the occurence of hot spots. A software combining tree spreads the congestion over a tree of variables. Arriving processors are divided into pre-determined groups. <p> During barrier creation, processor i selects processor b i1 4 c as its parent, and will write to byte (i 1) mod 4 of the parents flag. The four-fold fan-in was selected because Yew et al <ref> [12] </ref> reported this resulted in the best speedup in their software combining tree and four bytes are Algorithm easily packed into a 32-bit word. During notification, each processor spins on its flag-word until all bytes are cleared by the arriving children and then informs its own parent.
References-found: 12

