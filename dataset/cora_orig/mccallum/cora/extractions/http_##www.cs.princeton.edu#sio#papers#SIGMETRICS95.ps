URL: http://www.cs.princeton.edu/sio/papers/SIGMETRICS95.ps
Refering-URL: http://www.cs.princeton.edu/sio/
Root-URL: http://www.cs.princeton.edu
Title: A Study of Integrated Prefetching and Caching Strategies  
Author: Pei Cao Edward W. Felten Anna R. Karlin Kai Li 
Abstract: Prefetching and caching are effective techniques for improving the performance of file systems, but they have not been studied in an integrated fashion. This paper proposes four properties that optimal integrated strategies for prefetch-ing and caching must satisfy, and then presents and studies two such integrated strategies, called aggressive and conservative. We prove that the performance of the conservative approach is within a factor of two of optimal and that the performance of the aggressive strategy is a factor significantly less than twice that of the optimal case. We have evaluated these two approaches by trace-driven simulation with a collection of file access traces. Our results show that the two integrated prefetching and caching strategies are indeed close to optimal and that these strategies can reduce the running time of applications by up to 50%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Mary Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John Ousterhout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-211, </pages> <month> Oc-tober </month> <year> 1991. </year>
Reference-contexts: LRU-OBL and OPT-OBL model the above replacement algorithms with the addition of sequential one-block looka-head (OBL) prefetching. Traditional file systems use OBL to take advantage of the fact that files are often accessed sequentially <ref> [1] </ref>. OBL prefetches block K+2 of a file whenever the last two references to the file were to block K and block K+1. Existing file systems mostly use the combination of OBL with LRU replacement 2 ; we call the resultant algorithm LRU-OBL. <p> We collected one set by tracing several applications running on a DEC 5000/200 workstation under Ultrix 4.3. The other set is from the Sprite file system traces from the University of California at Berke-ley <ref> [1] </ref>. We instrumented our Ultrix 4.3 kernel to collect file access traces from a set of read-dominated applications on which existing file systems perform poorly.
Reference: [2] <author> L. A. Belady. </author> <title> A study of replacement algorithms for virtural storage. </title> <journal> IBM Systems Journal, </journal> <pages> pages 5 78-101, </pages> <year> 1966. </year>
Reference-contexts: at most 1.024 times that of optimal for the Ultrix traces (as F varies) and at most 1.02 times that of optimal for the Sprite traces. 5 Related Work Caching has been studied extensively in the past and there is a large body of literature on caching ranging from theory <ref> [2, 8] </ref>, to architecture [21] to file systems [11, 16, 4], etc. Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures [20, 5, 3, 22, 24], to file systems [19, 10, 23] to databases [6, 17] and beyond. <p> Several questions deserve further study. First, our results are based on full knowledge of file accesses. Although this is unrealistic, we feel that understanding the off-line case is a necessary step towards a full understanding of the online case. We can draw an analogy with Belady's famous result <ref> [2] </ref> showing that MIN is the optimal off-line demand paging strategy. In fact, we have not solved the off-line problem: one direction for future research is to either find a polynomial time algorithm for computing the optimal o*ine prefetching and caching strategy or to show that the problem is NP-complete.
Reference: [3] <author> David Callahan, Ken Kennedy, and Allan Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures <ref> [20, 5, 3, 22, 24] </ref>, to file systems [19, 10, 23] to databases [6, 17] and beyond. Sequential one-block looka head was first proposed in [22]. Few of these studies considered the interaction between prefetching and caching.
Reference: [4] <author> Pei Cao, Edward W. Felten, and Kai Li. </author> <title> Implementation and performance of application-controlled file caching. </title> <type> Technical report, </type> <institution> Department of Computer Science, Princeton University, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: LRU-demand specifies that the block that is least recently used should be replaced when necessary; OPT-demand specifies that the block that will be referenced furthest in the future should be replaced. Traditional file systems use LRU as replacement algorithm, although recent research <ref> [4] </ref> has shown that with application knowledge it is possible to make replacement decisions that are close to optimal. LRU-OBL and OPT-OBL model the above replacement algorithms with the addition of sequential one-block looka-head (OBL) prefetching. <p> the Ultrix traces (as F varies) and at most 1.02 times that of optimal for the Sprite traces. 5 Related Work Caching has been studied extensively in the past and there is a large body of literature on caching ranging from theory [2, 8], to architecture [21] to file systems <ref> [11, 16, 4] </ref>, etc. Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures [20, 5, 3, 22, 24], to file systems [19, 10, 23] to databases [6, 17] and beyond. Sequential one-block looka head was first proposed in [22].
Reference: [5] <author> Tien-Fu Chen and Jean-Loup Baer. </author> <title> Reducing memory latency via non-blocking and prefetching caches. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 51-61, </pages> <month> October </month> <year> 1992. </year> <note> Also available as U. Washington CS TR 92-06-03. </note>
Reference-contexts: Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures <ref> [20, 5, 3, 22, 24] </ref>, to file systems [19, 10, 23] to databases [6, 17] and beyond. Sequential one-block looka head was first proposed in [22]. Few of these studies considered the interaction between prefetching and caching.
Reference: [6] <author> Kenneth M. Curewitz, P. Krishnan, and Jeffrey Scott Vitter. </author> <title> Practical prefetching via data compression. </title> <booktitle> In Proc. 1993 ACM-SIGMOD Conference on Management of Data, </booktitle> <pages> pages 257-266, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Some of these let users or applications provide information about future accesses and use this information to guide prefetching [18], while others try to predict future accesses based on patterns observed in previous accesses <ref> [6, 10] </ref>. LRU is typically used as the cache replacement algorithm, even when information about the future reference string is available. One problem with such approaches is "thrashing". <p> Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures [20, 5, 3, 22, 24], to file systems [19, 10, 23] to databases <ref> [6, 17] </ref> and beyond. Sequential one-block looka head was first proposed in [22]. Few of these studies considered the interaction between prefetching and caching. In file systems, perhaps the most straightforward approach to prefetching is using large I/O units (i.e. blocks), as in extent-based or similar file systems [14]. <p> Tait and Duchamp's work [23] tries to detect user's file access patterns and exploit the patterns to prefetch files from servers. Palmer and Zdonik's work on Fido [17] tries to train an associative memory to recognize access patterns in order to prefetch. Vitter and Krishnan's work <ref> [6] </ref> tries to use compression techniques to predict future file accesses from past access history. All these studies had promising results with respect to prediction.
Reference: [7] <author> Carla Schlatter Ellis and David Kotz. </author> <title> Prefetching in file system for MIMD multiprocessors. </title> <booktitle> In 1989 International Conference on Parallel Processing, </booktitle> <pages> pages 306-314, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: With such a model, cache management becomes substantially less important. There are a number of papers on prefetching in parallel I/O systems <ref> [7, 25] </ref>. Although our work focuses on prefetching with a single disk or server, the "Do No Harm" and "First Opportunity" principles apply to prefetching algorithms in the parallel context as well. We believe these principles are important to avoid the thrashing problem [25].
Reference: [8] <author> Edward G. Coffman, Jr. and Peter J. Denning. </author> <title> Operating Systems Theory. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1973. </year>
Reference-contexts: at most 1.024 times that of optimal for the Ultrix traces (as F varies) and at most 1.02 times that of optimal for the Sprite traces. 5 Related Work Caching has been studied extensively in the past and there is a large body of literature on caching ranging from theory <ref> [2, 8] </ref>, to architecture [21] to file systems [11, 16, 4], etc. Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures [20, 5, 3, 22, 24], to file systems [19, 10, 23] to databases [6, 17] and beyond.
Reference: [9] <author> Jim Gray. </author> <title> The Benchmark Handbook. </title> <address> Morgen-Kaufman, San Mateo, Ca., </address> <year> 1991. </year>
Reference-contexts: [13], searching for four keywords in a 40MB snapshot of news articles; postgres-join: the Postgres relational database system (version 4.0.1) developed at the University of California at Berke-ley, performing a join between an indexed 32MB relation and a non-indexed 3.2MB relation (the relations are those used in the Wisconsin Benchmark <ref> [9] </ref>). Since the result relation is small, most of the file accesses are reads. The Sprite traces consist of five sets, recording about 40 clients' file activities over a period of 48 hours (traces 1, 2 and 3) or 24 hours (traces 4 and 5).
Reference: [10] <author> Jim Griffioen and Randy Appleton. </author> <title> Reducing file system latency using a predictive approach. </title> <booktitle> In Conference Proceedings of the USENIX Summer 1994 Technical Conference, </booktitle> <pages> pages 197-208, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Some of these let users or applications provide information about future accesses and use this information to guide prefetching [18], while others try to predict future accesses based on patterns observed in previous accesses <ref> [6, 10] </ref>. LRU is typically used as the cache replacement algorithm, even when information about the future reference string is available. One problem with such approaches is "thrashing". <p> Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures [20, 5, 3, 22, 24], to file systems <ref> [19, 10, 23] </ref> to databases [6, 17] and beyond. Sequential one-block looka head was first proposed in [22]. Few of these studies considered the interaction between prefetching and caching. <p> In file systems, perhaps the most straightforward approach to prefetching is using large I/O units (i.e. blocks), as in extent-based or similar file systems [14]. However, this approach and one-block-lookahead are often too limited and only benefit applications that make sequential references to large files <ref> [10] </ref>. Recently there have been a number of research projects on prefetching in file systems. Patterson's Transparent-Informed Prefetching [19] showed that prefetching using hints from applications is an effective way of exploiting I/O con-currency in disk arrays. Griffioen and Appleton's work [10] tries to predict future file accesses based on past <p> benefit applications that make sequential references to large files <ref> [10] </ref>. Recently there have been a number of research projects on prefetching in file systems. Patterson's Transparent-Informed Prefetching [19] showed that prefetching using hints from applications is an effective way of exploiting I/O con-currency in disk arrays. Griffioen and Appleton's work [10] tries to predict future file accesses based on past accesses using "probability graphs", and prefetch accordingly. These papers demonstrated the benefits of prefetching. However they did not address the interaction between caching and prefetching and did not investigate the combined cache management problem.
Reference: [11] <author> John H. Howard, Michael Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 6(1) 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: the Ultrix traces (as F varies) and at most 1.02 times that of optimal for the Sprite traces. 5 Related Work Caching has been studied extensively in the past and there is a large body of literature on caching ranging from theory [2, 8], to architecture [21] to file systems <ref> [11, 16, 4] </ref>, etc. Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures [20, 5, 3, 22, 24], to file systems [19, 10, 23] to databases [6, 17] and beyond. Sequential one-block looka head was first proposed in [22].
Reference: [12] <author> James J. Kistler and M. Satyanarayanan. </author> <title> Disconnected operation in the Coda file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 6(1) 1-25, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Although we did not address the write-back problem in this paper, we are working on algorithms for it. Finally, we note that the stashing approach in mobile computing environment <ref> [12] </ref> is similar to prefetching, although the purpose is quite different: stashing is concerned with the availability of files, while prefetching is more concerned with latency hiding. 6 Conclusions and Future Work This paper presents a theoretical study and performance evaluation via simulation of two prefetching strategies that address the interaction
Reference: [13] <author> Udi Manber and Sun Wu. GLIMPSE: </author> <title> A tool to search through entire file systems. </title> <booktitle> In Conference Proceedings of the USENIX Winter 1994 Technical Conference, </booktitle> <pages> pages 23-32, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: symbols (cscope1) in a 18MB software package, searching for four text strings (cscope2) in the same 18MB software package, and searching for four text strings (cscope3) on a 10MB software package; dinero: a cache simulator written by Mark Hill, running on the cc trace; glimpse: a text information retrieval system <ref> [13] </ref>, searching for four keywords in a 40MB snapshot of news articles; postgres-join: the Postgres relational database system (version 4.0.1) developed at the University of California at Berke-ley, performing a join between an indexed 32MB relation and a non-indexed 3.2MB relation (the relations are those used in the Wisconsin Benchmark [9]).
Reference: [14] <author> L. W. McVoy and S. R. Kleiman. </author> <title> Extent-like performance from a UNIX file system. </title> <booktitle> In 1991 Winter USENIX, </booktitle> <pages> pages 33-43, </pages> <year> 1991. </year>
Reference-contexts: Sequential one-block looka head was first proposed in [22]. Few of these studies considered the interaction between prefetching and caching. In file systems, perhaps the most straightforward approach to prefetching is using large I/O units (i.e. blocks), as in extent-based or similar file systems <ref> [14] </ref>. However, this approach and one-block-lookahead are often too limited and only benefit applications that make sequential references to large files [10]. Recently there have been a number of research projects on prefetching in file systems.
Reference: [15] <author> Jeffrey C. Mogul. </author> <title> A better update policy. </title> <booktitle> In Proceedings of 1994 Summer USENIX, </booktitle> <pages> pages 99-111, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: On the other hand, Tullsen and Eggers [24] showed that thrashing is a problem when prefetching in bus-based multiprocessor caches, suggesting that the "Do No Harm" rule applies in those systems as well. Recent work on update policies <ref> [15] </ref> (i.e. policies on write-backs of dirty blocks) is directly related to our work on prefetching. Although we did not address the write-back problem in this paper, we are working on algorithms for it.
Reference: [16] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout. </author> <title> Caching in the Sprite file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 6(1) 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: the Ultrix traces (as F varies) and at most 1.02 times that of optimal for the Sprite traces. 5 Related Work Caching has been studied extensively in the past and there is a large body of literature on caching ranging from theory [2, 8], to architecture [21] to file systems <ref> [11, 16, 4] </ref>, etc. Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures [20, 5, 3, 22, 24], to file systems [19, 10, 23] to databases [6, 17] and beyond. Sequential one-block looka head was first proposed in [22].
Reference: [17] <author> Mark Palmer and Stanley B. Zdonik. </author> <title> Fido: A cache that learns to fetch. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Data Bases, </booktitle> <pages> pages 255-264, </pages> <month> Septem-ber </month> <year> 1991. </year>
Reference-contexts: Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures [20, 5, 3, 22, 24], to file systems [19, 10, 23] to databases <ref> [6, 17] </ref> and beyond. Sequential one-block looka head was first proposed in [22]. Few of these studies considered the interaction between prefetching and caching. In file systems, perhaps the most straightforward approach to prefetching is using large I/O units (i.e. blocks), as in extent-based or similar file systems [14]. <p> There have also been many studies focusing on how to predict future accesses from past accesses. Tait and Duchamp's work [23] tries to detect user's file access patterns and exploit the patterns to prefetch files from servers. Palmer and Zdonik's work on Fido <ref> [17] </ref> tries to train an associative memory to recognize access patterns in order to prefetch. Vitter and Krishnan's work [6] tries to use compression techniques to predict future file accesses from past access history. All these studies had promising results with respect to prediction.
Reference: [18] <author> Hugo Patterson, Garth Gibson, and M. Satyanarayanan. </author> <title> Transparent informed prefetching. </title> <booktitle> ACM Operating Systems Review, </booktitle> <pages> pages 21-34, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Combining OBL with optimal replacement yields the OPT-OBL algorithm. The last two algorithms are intended to model approaches taken in recent research projects on prefetching in file systems. Some of these let users or applications provide information about future accesses and use this information to guide prefetching <ref> [18] </ref>, while others try to predict future accesses based on patterns observed in previous accesses [6, 10]. LRU is typically used as the cache replacement algorithm, even when information about the future reference string is available. One problem with such approaches is "thrashing".
Reference: [19] <author> R. Hugo Patterson and Garth A. Gibson. </author> <title> Exposing I/O concurrency with informed prefetching. </title> <booktitle> In Proc. Third International Conf. on Parallel and Distributed Information Systems, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: It stops when one-third of the cache contains blocks that have been prefetched but haven't been referenced yet 3 . 2 The prefetched block is typically inserted in the most-recently-used end of the LRU list when the fetch finishes. 3 In <ref> [19] </ref> 150 blocks is the throttling limit, out of a cache of 400 blocks. Hence in our simulation we set the throttling limit to be 4.2 File Access Traces We used two sets of traces. <p> The average reference time and disk access time for each Ultrix trace is shown in Table 1. one-third of the cache. Also, LRU-throttled attempts to simulate the algorithm described in <ref> [19] </ref>, which is intended for applications that do not reuse their data. <p> Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures [20, 5, 3, 22, 24], to file systems <ref> [19, 10, 23] </ref> to databases [6, 17] and beyond. Sequential one-block looka head was first proposed in [22]. Few of these studies considered the interaction between prefetching and caching. <p> However, this approach and one-block-lookahead are often too limited and only benefit applications that make sequential references to large files [10]. Recently there have been a number of research projects on prefetching in file systems. Patterson's Transparent-Informed Prefetching <ref> [19] </ref> showed that prefetching using hints from applications is an effective way of exploiting I/O con-currency in disk arrays. Griffioen and Appleton's work [10] tries to predict future file accesses based on past accesses using "probability graphs", and prefetch accordingly. These papers demonstrated the benefits of prefetching.
Reference: [20] <author> Anne Rogers and Kai Li. </author> <title> Software support for speculative loads. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 38-50, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures <ref> [20, 5, 3, 22, 24] </ref>, to file systems [19, 10, 23] to databases [6, 17] and beyond. Sequential one-block looka head was first proposed in [22]. Few of these studies considered the interaction between prefetching and caching.
Reference: [21] <author> Alan J. Smith. </author> <title> Second bibliography on cache memories. </title> <journal> Computer Architecture News, </journal> <volume> 19(4) </volume> <pages> 154-182, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: that of optimal for the Ultrix traces (as F varies) and at most 1.02 times that of optimal for the Sprite traces. 5 Related Work Caching has been studied extensively in the past and there is a large body of literature on caching ranging from theory [2, 8], to architecture <ref> [21] </ref> to file systems [11, 16, 4], etc. Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures [20, 5, 3, 22, 24], to file systems [19, 10, 23] to databases [6, 17] and beyond. Sequential one-block looka head was first proposed in [22].
Reference: [22] <author> Alan Jay Smith. </author> <title> Sequential program prefetching in memory hierarchies. </title> <journal> IEEE Computer, </journal> <volume> 11(12) </volume> <pages> 7-21, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures <ref> [20, 5, 3, 22, 24] </ref>, to file systems [19, 10, 23] to databases [6, 17] and beyond. Sequential one-block looka head was first proposed in [22]. Few of these studies considered the interaction between prefetching and caching. <p> Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures [20, 5, 3, 22, 24], to file systems [19, 10, 23] to databases [6, 17] and beyond. Sequential one-block looka head was first proposed in <ref> [22] </ref>. Few of these studies considered the interaction between prefetching and caching. In file systems, perhaps the most straightforward approach to prefetching is using large I/O units (i.e. blocks), as in extent-based or similar file systems [14].
Reference: [23] <author> Carl D. Tait and Dan Duchamp. </author> <title> Detection and exploitation of file working sets. </title> <type> Technical Report CUCS-050-90, </type> <institution> Computer Science Department, Columbia University, </institution> <year> 1990. </year>
Reference-contexts: Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures [20, 5, 3, 22, 24], to file systems <ref> [19, 10, 23] </ref> to databases [6, 17] and beyond. Sequential one-block looka head was first proposed in [22]. Few of these studies considered the interaction between prefetching and caching. <p> These papers demonstrated the benefits of prefetching. However they did not address the interaction between caching and prefetching and did not investigate the combined cache management problem. There have also been many studies focusing on how to predict future accesses from past accesses. Tait and Duchamp's work <ref> [23] </ref> tries to detect user's file access patterns and exploit the patterns to prefetch files from servers. Palmer and Zdonik's work on Fido [17] tries to train an associative memory to recognize access patterns in order to prefetch.
Reference: [24] <author> Dean M. Tullsen and Susan J. Eggers. </author> <title> Limitations of cache prefetching on a bus-based multiprocessor. </title> <booktitle> In Proceedings of the 1993 International Symposium on Computer Architecture, </booktitle> <pages> pages 278-288, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Prefetching has also been studied extensively in various domains, ranging from, uni-processor and multi-processor architectures <ref> [20, 5, 3, 22, 24] </ref>, to file systems [19, 10, 23] to databases [6, 17] and beyond. Sequential one-block looka head was first proposed in [22]. Few of these studies considered the interaction between prefetching and caching. <p> File systems, on the other hand, can change their cache management algorithms freely and can spare more cycles for calculating a good replacement or prefetching decision, as the potential savings are substantial. On the other hand, Tullsen and Eggers <ref> [24] </ref> showed that thrashing is a problem when prefetching in bus-based multiprocessor caches, suggesting that the "Do No Harm" rule applies in those systems as well. Recent work on update policies [15] (i.e. policies on write-backs of dirty blocks) is directly related to our work on prefetching.
Reference: [25] <author> Kun-Lung Wu, Philip S. Yu, and James Z. Teng. </author> <title> Performance comparision of thrashing control policies for concurrent mergesorts with parallel prefetching. </title> <booktitle> In Proceedings of 1993 ACM SIGMETRICS, </booktitle> <pages> pages 171-182, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: With such a model, cache management becomes substantially less important. There are a number of papers on prefetching in parallel I/O systems <ref> [7, 25] </ref>. Although our work focuses on prefetching with a single disk or server, the "Do No Harm" and "First Opportunity" principles apply to prefetching algorithms in the parallel context as well. We believe these principles are important to avoid the thrashing problem [25]. <p> Although our work focuses on prefetching with a single disk or server, the "Do No Harm" and "First Opportunity" principles apply to prefetching algorithms in the parallel context as well. We believe these principles are important to avoid the thrashing problem <ref> [25] </ref>. Prefetching in uni-processor and multi-processor computer architectures is similar to prefetching in file systems. However, in these systems there is little flexibility in cache management, as the cache is usually direct-mapped or has very limited associativity.
References-found: 25

