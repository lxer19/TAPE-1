URL: http://www.cis.ohio-state.edu/~kchen/jnc98.ps
Refering-URL: http://www.cis.ohio-state.edu/~kchen/on-line.html
Root-URL: 
Title: A Method of Combining Multiple Probabilistic Classifiers through Soft Competition on Different Feature Sets  
Author: Ke Chen a;b; and Huisheng Chi a 
Keyword: Combination of multiple classifiers, soft competition, different feature sets, Expectation-Maximization (EM) algorithm, speaker identification  
Address: Columbus, OH 43210-1277, USA  
Affiliation: Center for Information Science Peking University, Beijing 100871, China b Department of Computer and Information Science and Center for Cognitive Science The Ohio State University,  
Note: a National Laboratory of Machine Perception and  Neurocomputing An International Journal, Vol. 20, 1998. (in press)  
Abstract: A novel method is proposed for combining multiple probabilistic classifiers on different feature sets. In order to achieve the improved classification performance, a generalized finite mixture model is proposed as a linear combination scheme and implemented based on radial basis function networks. In the linear combination scheme, soft competition on different feature sets is adopted as an automatic feature rank mechanism so that different feature sets can be always simultaneously used in an optimal way to determine linear combination weights. For training the linear combination scheme, a learning algorithm is developed based on Expectation-Maximization (EM) algorithm. The proposed method has been applied to a typical real world problem, viz. speaker identification, in which different feature sets often need consideration simultaneously for robustness. Simulation results show that the proposed method yields good performance in speaker identification.
Abstract-found: 1
Intro-found: 1
Reference: <author> Agnew, C. </author> <year> (1985). </year> <title> Multiple probability assessments by dependent experts, </title> <journal> J. Am. Stat. Assoc. </journal> <volume> 80(3): </volume> <pages> 343347. </pages>
Reference: <author> Atal, B. S. </author> <year> (1974). </year> <title> Effectiveness of linear prediction characteristics of the speech waves for automatic speaker identification and verification, </title> <journal> J. Acoust. Soc. Am. </journal> <volume> 55(6): </volume> <pages> 13041312. </pages>
Reference-contexts: For text-independent speaker identification, it is generally agreed that the voiced parts of an utterance, especially vowels and nasals, are more effective in contrast to the unvoiced parts <ref> (Atal 1974, Sambur 1975, Reynolds 1992, Campbell 1997) </ref>. In simulations, therefore, only the voiced parts of a sentence remained regardless of their contents by using a common energy measure (Rabiner & Juang 1993). The length of the Hamming analysis window was 64 ms without overlapping.
Reference: <author> Battiti, R. & Colla, A. M. </author> <year> (1994). </year> <title> Democracy in neural nets: voting schemes for classification, </title> <booktitle> Neural Networks 7(4): </booktitle> <pages> 691708. </pages>
Reference: <author> Bennani, Y. </author> <year> (1995). </year> <title> A modular and hybrid connectionist system for speaker identification, </title> <booktitle> Neural Computation 7(4): </booktitle> <pages> 791798. </pages>
Reference: <author> Bennani, Y. & Gallinari, P. </author> <year> (1994). </year> <title> Connectionist approaches for automatic speaker recognition, </title> <booktitle> Proc. ESCA Workshop on Automatic Speaker Recognition, Identification and Verification, </booktitle> <pages> pp. 95102. </pages>
Reference: <author> Blum, A. </author> <year> (1997). </year> <title> Selection of relevant features and examples in machine learning, </title> <journal> Artificial Intelligence 97(2): </journal> <volume> 245 272. </volume>
Reference-contexts: For a classification task, numerous types of features can be extracted from the same raw data by means of different methods. A selection technique is often adopted to find an optimal feature set for use in classification <ref> (Blum 1997) </ref>. Sometimes, however, it is impossible to find such an optimal feature set. Instead several different feature sets can result in similar classification performance so that none of them can be optimal or robust for a specific classification task.
Reference: <author> Breiman, L. </author> <year> (1992). </year> <title> Stacked regression, </title> <type> Tech. Rep. </type> <institution> TR-367, Department of Statistics, University of California, Berke-ley. </institution>
Reference: <author> Breiman, L. </author> <year> (1996). </year> <title> Bagging predictors, </title> <booktitle> Machine Learning 26(2): </booktitle> <pages> 123140. </pages>
Reference-contexts: Each classifier itself may not produce the desired performance, but an optimal combination of such classifiers may lead to a highly reliable performance. More recently, this issue has been examined by different researchers <ref> (Freund & Schapire 1996, Breiman 1996, Ji & Ma 1997) </ref> and their methods have turned out to be a hopeful way to develop new pattern recognition systems.
Reference: <author> Campbell, F. P. </author> <year> (1997). </year> <title> Speaker recognition: A tutorial, </title> <booktitle> Proceedings of the IEEE 85(9): </booktitle> <pages> 14371463. </pages>
Reference: <author> Chatterjee, S. & Chatterjee, S. </author> <year> (1987). </year> <title> On combining expert opinions, </title> <journal> Am. J. Math. Management Sci. </journal> <volume> 7(1): </volume> <pages> 271295. </pages>
Reference: <author> Chen, K., Wang, L. & Chi, H. </author> <year> (1997). </year> <title> Methods of combining multiple classifiers with different features and their applications to text-indpendent speaker identification, </title> <journal> Internat. J. Pattern Recognition Artif. Intell. </journal> <volume> 11(3): 417 445. </volume>
Reference-contexts: In fact, our method can be used to combine multiple different types of classifiers. As a result, an extensive study on this topic has been done and experimental results show that our method also yields good performance in different circumstances <ref> (Chen et al. 1997) </ref>. On the other hand, Kanal (1993) argued that the research on combination of multiple classifiers provides a new perspective of pattern recognition. His argument suggested that we can build a number of different and complementary classifiers instead of developing a single high performance classifier.
Reference: <author> Chen, K., Xie, D. & Chi, H. </author> <year> (1995). </year> <title> Speaker identification based on hierarchical mixture of experts, </title> <booktitle> Proc. World Congress on Neural Networks, </booktitle> <address> Washington D.C., </address> <pages> pp. </pages> <month> I493I496. </month>
Reference-contexts: The hierarchical mixture of experts (HME) is a modular neural network architecture recently proposed by Jordan & Jocobs (1994). Our earlier work showed that the HME architecture yielded better identification results in text-dependent speaker identification in contrast to conventional methods <ref> (Chen et al. 1995, Chen, Xie & Chi 1996a) </ref>. Based on our earlier work, we employed the HME architecture as an individual classifier.
Reference: <author> Chen, K., Xie, D. & Chi, H. </author> <year> (1996a). </year> <title> A modified HME architecture for text-dependent speaker identification, </title> <journal> IEEE Trans. Neural Networks 7(5): </journal> <volume> 13091313. </volume>
Reference-contexts: A so-called sequence-based test method is often adopted in a practical speaker identification system based on the single-digit based test to identify personal identity <ref> (Chen, Xie & Chi 1996a) </ref>. In our simulations, we first produced a sequence consisting of five digits at random (it may be viewed as a password), then asked a speaker to utter the digit-sequence on line.
Reference: <author> Chen, K., Xie, D. & Chi, H. </author> <year> (1996b). </year> <title> Speaker identification using time-delay HMEs, </title> <journal> Internat. J. Neural Systems 7(1): </journal> <volume> 2943. </volume>
Reference: <author> Chen, K., Xu, L. & Chi, H. </author> <year> (1996). </year> <title> Improved learning algorithms for mixtures of experts in multiway classification, Neural Networks (in revision) </title> . 
Reference: <author> Dempster, A. P., Laird, N. M. & Rubin, D. B. </author> <year> (1977). </year> <title> Maximum-likelihood from incomplete data via the EM algorithm, </title> <journal> J. Royal Stat. Soc. </journal> <volume> B 39(1): </volume> <pages> 138. </pages>
Reference-contexts: maximum a posterior (MAP) principle as 1mM Thus, the sample D is classified as class m fl . 3 EM Algorithm for Maximum Likelihood Learning In this section, we present a maximum likelihood learning method for parameter estimation in the linear combination scheme on the basis of Expectation-Maximization (EM) algorithm <ref> (Dempster et al. 1977) </ref>. 1 When x k cannot be modeled as a Gaussian distribution, a generalized linear model or a multilayer perceptron with a softmax output layer can be used as the parametric form of ff kj (x k ). <p> For the log-likelihood function, we apply an EM algorithm <ref> (Dempster et al. 1977) </ref> for parameter estimation by introducing a set of indicators as missing data or unobserved data to observed data in X . To facilitate the presentation, we skip the detailed derivation of the EM algorithm here and put it in Appendix. <p> In general, an EM algorithm consists of two consecutive steps: Expectation step (E-step) and Maximization step (M-step) <ref> (Dempster et al. 1977) </ref>. In the E-step, the task is to acquire the expectation of the missing data on the condition of the observed data. At the sth iteration, the expectation is calculated based on the Bayesian rule as follows.
Reference: <author> Doddington, G. </author> <year> (1986). </year> <title> Speaker recognition identifying people by their voice, </title> <booktitle> Proceedings of the IEEE 73(11): </booktitle> <pages> 16511664. </pages>
Reference-contexts: In simulations, we chose isolated digits as the fixed text used in both training and test. The method has been extensively used to test a text-dependent speaker identification system <ref> (Doddington 1986, Soong & Rosenberg 1988, Chen, Xie & Chi 1996a) </ref>. The acoustic database consists of 10 isolated digits from `0' to `9' uttered in Chinese (Mandarin dialect). All the utterances were recorded in three different sessions and 20 male speakers were registered in the database. <p> However, it does not indicate that the system is robust since there is little variation of speaker's voice recorded in the same session. In contract, the performance of a speaker identification system should be evaluated by utterances recorded in different sessions as suggested by many researchers <ref> (Doddington 1986, Furui 1997, Campbell 1997) </ref>. In simulations, an HME classifier worked on a specific feature set to handle the utterances of a digit. <p> Their average lengths of the sentences were about 4.4 and 5.0 seconds, respectively. All utterances were recorded in a quiet room and sampled at 11.025 KHz sampling frequency in 16 bit precision. In simulations, we adopted three speech spectral feature sets commonly used in text-independent speaker identification <ref> (Doddington 1986, Furui 1997, Campbell 1997) </ref>, i.e., 24-order LPC based cepstrum (24-LPCCEP), 20-order Mel-scale cepstrum (24-MELCEP), and 24-order LPC coefficients (24-LPCCOE).
Reference: <author> Duda, R. & Hart, P. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis, </title> <publisher> John wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: As illustrated in Figure 1 (a), in general, such a classification system is composed of three stages: preprocessing, feature extraction, and classification. In particular, feature extraction is necessary to avoid a so-called curse of dimensionality problem <ref> (Duda & Hart 1973) </ref> which may lead to prohibitively expensive computation in the stage of classification. Therefore, the performance of a classification system highly depends upon a feature set used.
Reference: <author> Freund, Y. & Schapire, R. </author> <year> (1996). </year> <title> Experiments with a new boosting algorithm, </title> <booktitle> Proc. 13th Int. Conf. Machine Learning, </booktitle> <pages> pp. 148156. </pages>
Reference-contexts: Each classifier itself may not produce the desired performance, but an optimal combination of such classifiers may lead to a highly reliable performance. More recently, this issue has been examined by different researchers <ref> (Freund & Schapire 1996, Breiman 1996, Ji & Ma 1997) </ref> and their methods have turned out to be a hopeful way to develop new pattern recognition systems.
Reference: <author> Furui, S. </author> <year> (1994). </year> <title> An overview of speaker recognition technology, </title> <booktitle> Proc. ESCA Workshop on Automatic Speaker Recognition, Identification and Verification, </booktitle> <pages> pp. 19. </pages>
Reference-contexts: For this problem, several different spectrum feature sets have been turned out useful, but none of them can be regarded as an optimal or robust one. It has been suggested that multiple spectrum feature sets need consideration simultaneously for robustness in speaker identification <ref> (Furui 1994, Furui 1997) </ref>. As a result, a technique that efficiently utilizes different feature sets becomes a solution to pattern classification on different feature sets. For simultaneous use of different feature sets, a traditional method is to lump different feature vectors together into a single composite feature vector. <p> Different feature sets have been simultaneously considered for robustness in speaker identification. To use multiple features simultaneously, a traditional method is to lump two or more different feature vectors together as a single composite feature vector <ref> (Furui 1994, Furui 1997) </ref>. However, the performance of the composite-feature based system is not significantly improved. To some extent, the use of a composite feature set results in the curse of dimensionality problem.
Reference: <author> Furui, S. </author> <year> (1997). </year> <title> Recent advances in speaker recognition, </title> <journal> Pattern Recognition Letters 18(9): </journal> <volume> 859872. </volume>
Reference-contexts: Speaker identification is a rather hard task for learning since a person's voice always changes in time. In addition, many other factors, e.g. short-term sickness, emotion, fatigue, and the words spoken, may significantly alter personal voice characteristics <ref> (Furui 1997) </ref>. Although there is a long history to explore speaker's feature, the unique robust feature has still not been discovered so far.
Reference: <author> Gelfand, A., Ballick, B. & Dey, D. </author> <year> (1995). </year> <title> Modeling expert opinion arising as a partial probabilistic specification, </title> <journal> J. Am. Stat. Assoc. </journal> <volume> 90(5): </volume> <pages> 598604. </pages>
Reference: <author> Hashem, S. </author> <year> (1993). </year> <title> Optimal linear combinations of neural networks, </title> <type> Tech. Rep. SMS 94-4, </type> <institution> School of Industrial Engineering, Purdue University. </institution>
Reference: <author> He, J., Liu, L. & Palm, G. </author> <year> (1994). </year> <title> A text-independent speaker identification system based on neural networks, </title> <booktitle> Proc. Int. Conf. Spoken Language Processing, Yokohama, </booktitle> <pages> pp. </pages> <note> 181185. 15 Ho, </note> <author> T., Hull, J. & Srihari, S. </author> <year> (1994). </year> <title> Decision combination in multiple classifier systems, </title> <journal> IEEE Trans Pattern Anal. Machine Intell. </journal> <volume> 16(1): </volume> <pages> 6675. </pages>
Reference-contexts: The EM learning algorithms have been also developed for parameter estimation in such models <ref> (Jordan & Jocobs 1994, Chen, Xu & Chi 1996) </ref>. 5 To present the EM algorithm, we assume that N classifiers have been already trained on K (K N ) different feature sets extracted from the the same training set, Given a cross-validation set, X = f (D (t) ; y (t) <p> Based on our earlier work, we employed the HME architecture as an individual classifier. Therefore, 40 HMEs were used in simulations so that for each each digit four two-level HMEs with 2-8 structure 2 were trained on the four different feature sets using the EM algorithm <ref> (Jordan & Jocobs 1994, Chen, Xu & Chi 1996) </ref>, respectively. For a digit, five utterances of each speaker recorded in the first session were merely used for training and all the utterances of this digit recorded in other two sessions were used for test. <p> The length of the Hamming analysis window was 64 ms without overlapping. Here, we emphasize that the size of analysis window was slightly larger than the common 9 one (normally 16 ~ 32 ms) since it was found that the identification performance was degraded with a normal analysis window <ref> (He et al. 1994) </ref>. Whenever the short-time energy of a speech frame was higher than a pre-specified threshold, spectral feature vectors would be calculated based on the different feature extraction methods. In addition, samples were also pre-emphasized by the filter H (z) = 1 0:97z 1 . <p> Finally, we chose HME with 2-9 structure for the problem. We first trained three individual HMEs with 2-9 structure on the three different feature sets, respectively, using the EM algorithm <ref> (Jordan & Jocobs 1994, Chen, Xu & Chi 1996) </ref>. All the short-time speech frames in SET-1 were used to train individual HME classifiers. Moreover, all the short-time speech frames in SET-2 as a cross-validation data set were used to train our linear combination scheme.
Reference: <author> Huang, Y. & Suen, C. </author> <year> (1995). </year> <title> A method of combining multiple experts for the recognition of unconstrained handwritten numerals, </title> <journal> IEEE Trans Pattern Anal. Machine Intell. </journal> <volume> 17(1): </volume> <pages> 9094. </pages>
Reference: <author> Jacobs, R. A. </author> <year> (1995). </year> <title> Methods for combining experts' probability assessments, </title> <booktitle> Neural Computation 7(5): </booktitle> <pages> 867888. </pages>
Reference-contexts: From the viewpoint of statistics, combination of multiple classifiers can be viewed as combination of multiple probability distributions if each classifier is interpreted as an estimator of probability distribution. In general, there are two frameworks to perform such a combination <ref> (Jacobs 1995) </ref>. One is that of a decision maker who consults several classifiers regarding some events. The classifiers express their opinions in the form of probability distributions. The decision maker must aggregate the classifiers' distributions into a single distribution that can be used to make the final decision. <p> In terms of pattern classification on different feature sets, combination techniques under the framework of a decision maker, e.g. a supra Bayesian procedure <ref> (Jacobs 1995, Xu et al. 1992) </ref>, can be directly used for this problem since outputs of the classifiers are merely considered for combination regardless of their inputs. <p> These techniques are based on constrained or unconstrained least-squares regression with model selection to make a system achieve good generalization properties. However, the existing techniques of linear opinion pools with weights as veridical probabilities <ref> (Jacobs 1995, Xu et al. 1995, Xu & Jordan 1993) </ref> cannot be used to deal with a problem of pattern classification on different feature sets unless a single composite feature set is used since the process of generating weights used for linear combination highly depends upon inputs of multiple classifiers.
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J. & Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixtures of local experts, </title> <booktitle> Neural Computation 3(1): </booktitle> <pages> 7987. </pages>
Reference-contexts: Previous comparative studies showed that the method yielded the best recognition results for several benchmark OCR problems among numerous combination methods ranging from voting 2 It refers to that there are two mixture of experts (ME) modules in the structure and eight experts in each ME module <ref> (Jacobs et al. 1991, Jordan & Jocobs 1994) </ref>. 8 to evidence-reasoning based combination methods (Xu et al. 1992, Perrone 1993). In simulations, we also used the same cross-validation set and test sets used in our method for training and test. <p> Several issues with respect to our linear combination scheme are worth to be addressed. First of all, our method can be viewed as an extension of the existing models on linear opinion pools with weights as veridical probabilities <ref> (Jacobs et al. 1991, Xu & Jordan 1993) </ref> in terms of pattern classification on different feature sets.
Reference: <author> Ji, C. & Ma, S. </author> <year> (1997). </year> <title> Combinations of weak classifiers, </title> <journal> IEEE Trans. Neural Networks 7(1): </journal> <volume> 3242. </volume>
Reference: <author> Jordan, M. I. & Jocobs, R. A. </author> <year> (1994). </year> <title> Hierarchical mixture of experts and EM algorithm, </title> <booktitle> Neural Computation 6(2): </booktitle> <pages> 181214. </pages>
Reference-contexts: The EM learning algorithms have been also developed for parameter estimation in such models <ref> (Jordan & Jocobs 1994, Chen, Xu & Chi 1996) </ref>. 5 To present the EM algorithm, we assume that N classifiers have been already trained on K (K N ) different feature sets extracted from the the same training set, Given a cross-validation set, X = f (D (t) ; y (t) <p> Based on our earlier work, we employed the HME architecture as an individual classifier. Therefore, 40 HMEs were used in simulations so that for each each digit four two-level HMEs with 2-8 structure 2 were trained on the four different feature sets using the EM algorithm <ref> (Jordan & Jocobs 1994, Chen, Xu & Chi 1996) </ref>, respectively. For a digit, five utterances of each speaker recorded in the first session were merely used for training and all the utterances of this digit recorded in other two sessions were used for test. <p> In simulations, we still adopted the HME architecture as individual classifiers because of its good performance in classification <ref> (Jordan & Jocobs 1994) </ref>. For model selection in the current problem, we examined six different HME structures ranging for two-level to four level using a two-fold cross-validation method. Finally, we chose HME with 2-9 structure for the problem. <p> Finally, we chose HME with 2-9 structure for the problem. We first trained three individual HMEs with 2-9 structure on the three different feature sets, respectively, using the EM algorithm <ref> (Jordan & Jocobs 1994, Chen, Xu & Chi 1996) </ref>. All the short-time speech frames in SET-1 were used to train individual HME classifiers. Moreover, all the short-time speech frames in SET-2 as a cross-validation data set were used to train our linear combination scheme.
Reference: <author> Kanal, L. </author> <year> (1993). </year> <title> On pattern, categories, and alternative realities, </title> <journal> Pattern Recognition Letters 14(3): </journal> <volume> 241255. </volume>
Reference: <author> Meir, R. </author> <year> (1994). </year> <title> Bias, variance, and the combination of estimators, </title> <type> Tech. Rep. 922, </type> <institution> Department of Electrical Engineering, Technion, Haifa, Israel. </institution>
Reference: <author> O'Shaughnessy, D. </author> <year> (1986). </year> <title> Speaker recognition, </title> <journal> IEEE ASSP Magazine 3(4): </journal> <volume> 417. </volume>
Reference: <author> Perrone, M. P. </author> <year> (1993). </year> <title> Improving regression estimation: averaging methods of variance reduction with extensions to general convex measure optimization, </title> <type> Ph.D. Thesis, </type> <institution> Department of Physics, Brown University. </institution>
Reference: <author> LeBlanc, M. & Tibshirani, R. </author> <year> (1993). </year> <title> Combining estimates in regression and classification, </title> <type> Tech. Rep., </type> <institution> Department of Preventive Medicine and Biostatistics, University of Toronto. </institution>
Reference: <author> Rabiner, L. R. & Juang, B. H. </author> <year> (1993). </year> <title> Fundamentals of Speech Recognition, </title> <address> Englewood Cliffs, </address> <publisher> Prentice-Hall, </publisher> <address> New Jersey. </address>
Reference-contexts: In simulations, we adopted four common features used in text-dependent speaker identification, i.e. 19-order delta-cepstrum, 19-order LPC based cepstrum, 19-order Mel-scale cep-strum, and 15-order LPC coefficients <ref> (Rabiner & Juang 1993) </ref>. The hierarchical mixture of experts (HME) is a modular neural network architecture recently proposed by Jordan & Jocobs (1994). <p> In simulations, therefore, only the voiced parts of a sentence remained regardless of their contents by using a common energy measure <ref> (Rabiner & Juang 1993) </ref>. The length of the Hamming analysis window was 64 ms without overlapping.
Reference: <author> Reynolds, D. A. </author> <year> (1992). </year> <title> A Gaussian mixture modeling approach to text-independent speaker identification, </title> <type> Ph.D. Thesis, </type> <institution> Department of Electrical Engineering, Georgia Institute of Technology. </institution>
Reference: <author> Rogova, G. </author> <year> (1994). </year> <title> Combining the results of several neural network classifiers, </title> <booktitle> Neural Networks 7(5): </booktitle> <pages> 777781. </pages>
Reference: <author> Sambur, M. R. </author> <year> (1975). </year> <title> Selection of acoustic features for speaker identification, </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing 23(1): </journal> <volume> 176182. </volume>
Reference: <author> Soong, F. K. & Rosenberg, A. E. </author> <year> (1988). </year> <title> On the use of instantaneous and transitional spectral information in speaker recognition, </title> <journal> IEEE Trans. Acoust., Speech, Singal Processing 36(6): </journal> <volume> 871879. </volume>
Reference: <author> Tresp, V. & Taniguchi, M. </author> <year> (1995). </year> <title> Combining estimators using non-constant weithting functions, </title> <editor> in G. Tesauro, D. Tourezsky & T. Leen (eds), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <pages> pp. 419426. </pages>
Reference-contexts: In their linear combination scheme, the weighting functions are also dependent on the input. However, estimators in their method are allowed to work on disjoint regions of input 11 space and the method is applied in a regression task for better generalization <ref> (Tresp & Taniguchi 1995) </ref>. In contract, our linear combination scheme works on different representation forms of input space and all the estimators in our method still work on the same input space though different feature sets are used for pattern classification.
Reference: <author> Waibel, A., Hanazawa, T., Hinton, G., Shikano, K. & Lang, K. </author> <year> (1989). </year> <title> Phoneme recognition using time-delay neural networks, </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing 37(3): </journal> <volume> 328339. </volume>
Reference: <author> Wolpert, D. H. </author> <year> (1992). </year> <title> Stacked generalization, </title> <booktitle> Neural Networks 5(2): </booktitle> <pages> 241259. </pages>
Reference: <author> Xu, L. & Jordan, M. I. </author> <year> (1993). </year> <title> EM learning on a generalized finite mixture model for combining multiple classifiers, </title> <booktitle> Proc. World Congress on Neural Networks, </booktitle> <address> San Diego, </address> <pages> pp. </pages> <month> IV227IV230. </month>
Reference: <author> Xu, L., Jordan, M. I. & Hinton, G. E. </author> <year> (1995). </year> <title> An alternative model for mixture of experts, </title> <editor> in G. Tesauro, D. Tourezsky & T. Leen (eds), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference-contexts: In terms of pattern classification on different feature sets, combination techniques under the framework of a decision maker, e.g. a supra Bayesian procedure <ref> (Jacobs 1995, Xu et al. 1992) </ref>, can be directly used for this problem since outputs of the classifiers are merely considered for combination regardless of their inputs. <p> These techniques are based on constrained or unconstrained least-squares regression with model selection to make a system achieve good generalization properties. However, the existing techniques of linear opinion pools with weights as veridical probabilities <ref> (Jacobs 1995, Xu et al. 1995, Xu & Jordan 1993) </ref> cannot be used to deal with a problem of pattern classification on different feature sets unless a single composite feature set is used since the process of generating weights used for linear combination highly depends upon inputs of multiple classifiers.

References-found: 44

