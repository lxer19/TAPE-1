URL: ftp://ftp.cc.gatech.edu/pub/ai/students/carlos/RLI/tech-report.ps
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Email: carlos@cc.gatech.edu rich@cs.umass.edu ashwin@cc.gatech.edu  
Title: Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces  
Author: Juan Carlos Santamara Richard S. Sutton Ashwin Ram Andrew G. Barto and Richard S. Sutton. 
Note: This work was supported by the NSF grant ECS-9511805 to  
Address: Atlanta, GA 30332-0280  Amherst, MA 01002  
Affiliation: College of Computing, Georgia Institute of Technology,  Lederle Graduate Research Center, University of Massachusetts,  
Date: December 1996  
Pubnum: COINS Technical Report 96-088  
Abstract: A key element in the solution of reinforcement learning problems is the value function. The purpose of this function is to measure the long-term utility or value of any given state and it is important because an agent can use it to decide what to do next. A common problem in reinforcement learning when applied to systems having continuous states and action spaces is that the value function must operate with a domain consisting of real-valued variables, which means that it should be able to represent the value of infinitely many state and action pairs. For this reason, function approximators are used to represent the value function when a close-form solution of the optimal policy is not available. In this paper, we extend a previously proposed reinforcement learning algorithm so that it can be used with function approximators that generalize the value of individual experiences across both, state and action spaces. In particular, we discuss the benefits of using sparse coarse-coded function approximators to represent value functions and describe in detail three implementations: CMAC, instance-based, and case-based. Additionally, we discuss how function approximators having different degrees of resolution in different regions of the state and action spaces may influence the performance and learning efficiency of the agent. We propose a simple and modular technique that can be used to implement function approximators with non-uniform degrees of resolution so that it can represent the value function with higher accuracy in important regions of the state and action spaces. We performed extensive experiments in the double integrator and pendulum swing up systems to demonstrate the proposed ideas. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. S. Albus. </author> <title> A new approach to manipulator control: The cerebellar model articulation controller (cmac). Journal of Dynamic Systems, Measurement, </title> <journal> and Control, </journal> <volume> 97(3) </volume> <pages> 220-227, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: However, the conditions for convergence in these algorithms rarely hold in quantized versions of continuous state and action spaces. 3.3.2 Cerebellar Model Articulation Controller Cerebellar Model Articulation Controller or CMAC is a class of sparse coarse-coded memory that models cerebellar functionality <ref> [1] </ref>. Each input or state-action pair activates a specific set of memory locations or features, the arithmetic sum of whose contents is the value of the stored Q-value. <p> representation, r t+1 = ((x t x d ) T Q (x t x d ) + u T t Ru t ), where Q and R are the positive definite 2 fi 2 and 1 fi 1 matrices respectively defined as Q = 0 0 # and R = <ref> [1] </ref>. The one-step reward function penalizes the agent more heavily when the distance between the current and desired states is large and also when the action applied is large. <p> 2 +ff 2 t ) or in vectorial representation, r t+1 = (x T t Qx t +u T t Ru t ), where Q and R are the positive definite 2 fi 2 and 1 fi 1 matrices respectively defined as Q = " 0 1 and R = <ref> [1] </ref>. The dynamics of the pendulum is given by the following equations, d! = 4 ml 2 (ff + mlg sin ()) dt where m = 1=3 and l = 3=2 are the mass and length of the bar respectively, and g = 9:8 is the gravity.
Reference: [2] <author> C. G. Atkeson. </author> <title> Memory-based learning control. </title> <booktitle> In Proceedings of the 1991 American Control Conference, </booktitle> <volume> volume 3, </volume> <pages> pages 2131-2136, </pages> <address> Boston, MA, </address> <year> 1991. </year>
Reference-contexts: Although, memory-based function approximators have not been widely used in conjunction with reinforcement learning, they are common in other tasks such as classification (e.g., [8]) and robot control (e.g., <ref> [2] </ref>) (but see [16, 14, 11]). In a memory-based function approximator, each memory element represents some of the state-action pairs or a case the agent has experienced before.
Reference: [3] <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 835-846, </pages> <year> 1983. </year>
Reference-contexts: involving specific state-action pairs to other regions of the state and action spaces? A common approach that have been used to represent the value function is to quantize the state and action spaces into a finite number of cells and aggregate all states and actions within 1 each cell (e.g., <ref> [10, 23, 3] </ref>. This is one of the simplest forms of generalization in which all the states and actions within a cell have the same value. Thus, the value function is approximated as a table in which each cell has a specific value.
Reference: [4] <author> R. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference: [5] <author> D. P. Bertsekas. </author> <title> Dynamic Programming and Optimal Control, volume 1. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1995. </year> <month> 41 </month>
Reference-contexts: The derivation follows from the solution to the Hamilton-Bellman-Jacobi partial differential equation (see, for example, [21] or <ref> [5] </ref> for details). <p> The last equation is known as the continuous-time Ricatti equation and it is used to find the unknown matrix P , which then is used in Equation 29 to find K also called the gain matrix <ref> [5, 17] </ref>. 8 K = R 1 B T P (28) 7 The constant x d corresponds to the desired goal state and the constant u d corresponds to the required force to maintain the system at the desired state (i.e., once the system is in x d , applying u
Reference: [6] <author> P. Cishosz. </author> <title> Truncating temporal differences: on the efficient implementation of td() for rein-forcement learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 287-318, </pages> <year> 1996. </year>
Reference-contexts: In naives CMAC implementations, TD () updates are proportional to the number of tiles in the CMAC since each tile holds an eligibility trace. More efficient implementations keep a list of non-zero traces and perform the updates only on those tiles (e.g., <ref> [6] </ref>). A description of the structure and basic operations of the CMAC follows: * Tile structure: A CMAC consists of a set of N tilings: f T i ; i = 1; ; N g.
Reference: [7] <author> P. Kanerva. </author> <title> Sparse distributed memory and related models. </title> <editor> In M. H. Hassoun, editor, </editor> <title> Associative Neural Memories: Theory and Implementation, chapter 3. </title> <publisher> Oxford University Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: The more two different inputs share memory locations, the similar their contents will be. The resolution, storage, and computational efficiency vary according to the specific type of function ap-proximator and its implementation. A detailed description of sparse distributed memory and related models is found in <ref> [7] </ref>. The following subsections describe three types of sparse coarse-coded function approximators and outline their advantages and disadvantages. 3.3.1 Lookup Tables The lookup table is one of the most common function approximators used to represent the Q-function when the states and actions spaces are discrete.
Reference: [8] <author> D. Kibler and D. W. Aha. </author> <title> Instance-based prediction of real-valued attributes. </title> <journal> Computational Intelligence, </journal> <volume> 5(2) </volume> <pages> 51-57, </pages> <year> 1989. </year>
Reference-contexts: However, the storage is proportional to the total number of tiles. 3.3.3 Memory-Based Function Approximators Another class of sparse coarse-coded memory is memory-based. Although, memory-based function approximators have not been widely used in conjunction with reinforcement learning, they are common in other tasks such as classification (e.g., <ref> [8] </ref>) and robot control (e.g., [2]) (but see [16, 14, 11]). In a memory-based function approximator, each memory element represents some of the state-action pairs or a case the agent has experienced before.
Reference: [9] <author> L. J. Lin. </author> <title> Self-improving reactive agents based on reinforcement learning. </title> <booktitle> Machine Learning, </booktitle> <address> 8(3-4):293-321, </address> <year> 1992. </year>
Reference-contexts: Another approach is to avoid the problems associated with quantityzing the state space altogether by using other types of function approximators, such as neural networks, that do not rely on quantization and can been used to generalize the value function across states (e.g., <ref> [9, 18] </ref>). The approach consists of associating one function approximator to represent the value of all the states and one specific action. For this reason it is useful for systems with continuous states and discrete actions. <p> There is no proof showing the convergence of TD () for more complex function approximators, but this has not stopped researchers for trying these methods using different classes of non-linear function approximators. Successful results have been obtained with multi-layer neural networks (e.g., <ref> [9] </ref>, [18]) and sparse coarse coding methods such as Cerebellar Model Articulation Controllers (CMACs) (e.g., [23], [24]). <p> Then, the one-step search can be performed by simply evaluating each function ap-proximator at the given state. The best action is the one associated with the function producing the maximum value (see, for example, <ref> [9, 18, 23] </ref>). However, such technique becomes quickly impractical when the number of actions increases due to storage implications and computational efficiency because the number of function approximators is proportional to the number of actions.
Reference: [10] <author> S. Mahadevan and J. Connell. </author> <title> Scaling reinforcement learning to robotics by exploiting the subsumption architecture. </title> <booktitle> In Proceedings of the Eight International Workshop on Machine Learning, </booktitle> <volume> volume 1, </volume> <pages> pages 328-332. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: involving specific state-action pairs to other regions of the state and action spaces? A common approach that have been used to represent the value function is to quantize the state and action spaces into a finite number of cells and aggregate all states and actions within 1 each cell (e.g., <ref> [10, 23, 3] </ref>. This is one of the simplest forms of generalization in which all the states and actions within a cell have the same value. Thus, the value function is approximated as a table in which each cell has a specific value.
Reference: [11] <author> R. A. McCallum, G. Tesauro, D. Touretzky, and T. Leen. </author> <title> Instance-based state identification for reinforcement learning. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 7, </volume> <year> 1995. </year>
Reference-contexts: Although, memory-based function approximators have not been widely used in conjunction with reinforcement learning, they are common in other tasks such as classification (e.g., [8]) and robot control (e.g., [2]) (but see <ref> [16, 14, 11] </ref>). In a memory-based function approximator, each memory element represents some of the state-action pairs or a case the agent has experienced before.
Reference: [12] <author> A. W. Moore and C. G. Atkeson. </author> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. </title> <journal> Machine Learning, </journal> <volume> 21(3) </volume> <pages> 199-233, </pages> <year> 1995. </year>
Reference-contexts: Third, the rate of convergence 9 of the learning algorithm becomes extremely slow as the number of states and actions increases. Additionally, the quantized state and action spaces often create convergence problems because they form a non-Markovian representation of the dynamics of the system (see <ref> [12] </ref>). Lookup tables efficiently support one-step searches and TD updates. The best action at a given state is found by indexing the table holding the state constant and performing a sweep across all possible actions values (Equation 4).
Reference: [13] <author> K. S. Narendra and A. M. Annaswamy. </author> <title> Stable Adaptive Systems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference: [14] <author> J. Peng. </author> <title> Efficient Dynamic Programming-Based Learning for Control. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Northeastern University, </institution> <year> 1993. </year>
Reference-contexts: Although, memory-based function approximators have not been widely used in conjunction with reinforcement learning, they are common in other tasks such as classification (e.g., [8]) and robot control (e.g., [2]) (but see <ref> [16, 14, 11] </ref>). In a memory-based function approximator, each memory element represents some of the state-action pairs or a case the agent has experienced before.
Reference: [15] <author> J. Peng and R. J. Williams. </author> <title> Incremental multi-step q-learning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 189-195, </pages> <address> Aberdeen, Scotland, </address> <year> 1994. </year>
Reference-contexts: Also, Peng and Williams described a way to use TD () in a learning algorithm they call Q () <ref> [15] </ref>, which results in a learning algorithm with faster convergence rate.
Reference: [16] <author> A. Ram and J. C. Santamara. </author> <title> Continuous case-based reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 90(1-2):25-77, </volume> <year> 1997. </year>
Reference-contexts: Although, memory-based function approximators have not been widely used in conjunction with reinforcement learning, they are common in other tasks such as classification (e.g., [8]) and robot control (e.g., [2]) (but see <ref> [16, 14, 11] </ref>). In a memory-based function approximator, each memory element represents some of the state-action pairs or a case the agent has experienced before.
Reference: [17] <author> R. J. Richards. </author> <title> An Introduction to Dynamics and Control. </title> <publisher> Longman, </publisher> <address> New York, NY, </address> <year> 1979. </year>
Reference-contexts: The last equation is known as the continuous-time Ricatti equation and it is used to find the unknown matrix P , which then is used in Equation 29 to find K also called the gain matrix <ref> [5, 17] </ref>. 8 K = R 1 B T P (28) 7 The constant x d corresponds to the desired goal state and the constant u d corresponds to the required force to maintain the system at the desired state (i.e., once the system is in x d , applying u
Reference: [18] <author> G. A. Rummery and M. Niranjan. </author> <title> On-line q-learning using connectionist systems. </title> <type> Technical Report CUED/F-INFEG/TR66, </type> <institution> Cambridge University Department, </institution> <year> 1994. </year>
Reference-contexts: Another approach is to avoid the problems associated with quantityzing the state space altogether by using other types of function approximators, such as neural networks, that do not rely on quantization and can been used to generalize the value function across states (e.g., <ref> [9, 18] </ref>). The approach consists of associating one function approximator to represent the value of all the states and one specific action. For this reason it is useful for systems with continuous states and discrete actions. <p> There is no proof showing the convergence of TD () for more complex function approximators, but this has not stopped researchers for trying these methods using different classes of non-linear function approximators. Successful results have been obtained with multi-layer neural networks (e.g., [9], <ref> [18] </ref>) and sparse coarse coding methods such as Cerebellar Model Articulation Controllers (CMACs) (e.g., [23], [24]). <p> Then, the one-step search can be performed by simply evaluating each function ap-proximator at the given state. The best action is the one associated with the function producing the maximum value (see, for example, <ref> [9, 18, 23] </ref>). However, such technique becomes quickly impractical when the number of actions increases due to storage implications and computational efficiency because the number of function approximators is proportional to the number of actions.
Reference: [19] <author> K. S. Shanmugam. </author> <title> Digital and Analog Communication Systems. </title> <publisher> Longman, </publisher> <address> New York, NY, </address> <year> 1979. </year>
Reference-contexts: For that purpose, more quantization levels are allocated to those regions where the amplitude of the signal is more frequently used, which reduces the average error performed in the quantization process (see, for example, <ref> [19] </ref>). We proposed a technique that takes advantages of the simplicity and efficiency of the uniform versions of function approximators and yet are equivalent to the more elaborate, but more accurate non-uniform counterparts.
Reference: [20] <author> S. P. Singh and R. S. Sutton. </author> <title> Reinforcement learning with replacing eligibility traces. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 123-158, </pages> <year> 1996. </year>
Reference-contexts: The variable e t represents the proportion of blame or "eligibility" the associated component in W t has in the current error taking into account past information (for details, see, <ref> [20] </ref>). In order to improve the Q-values, the policy must be chosen in such a way that both improve as the agent collects information. <p> The TD () and the eligibilities updates are proportional to the number of tiles in the CMAC. This may deteriorate the performance of the CMAC in situations where some of the tiles are never used. The generalization and resolution of 2 Equation 12 corresponds to replace of eligibilities (see <ref> [20] </ref>). 3 In naive CMAC implementations, there is only one active tile per tiling.
Reference: [21] <author> R. F. Stengel. </author> <title> Optimal Control and Estimation. </title> <publisher> Dover Publications, </publisher> <address> Mineola, NY, </address> <year> 1994. </year>
Reference-contexts: The derivation follows from the solution to the Hamilton-Bellman-Jacobi partial differential equation (see, for example, <ref> [21] </ref> or [5] for details).
Reference: [22] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference: [23] <author> R. S. Sutton. </author> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 8, </volume> <year> 1996. </year>
Reference-contexts: involving specific state-action pairs to other regions of the state and action spaces? A common approach that have been used to represent the value function is to quantize the state and action spaces into a finite number of cells and aggregate all states and actions within 1 each cell (e.g., <ref> [10, 23, 3] </ref>. This is one of the simplest forms of generalization in which all the states and actions within a cell have the same value. Thus, the value function is approximated as a table in which each cell has a specific value. <p> Successful results have been obtained with multi-layer neural networks (e.g., [9], [18]) and sparse coarse coding methods such as Cerebellar Model Articulation Controllers (CMACs) (e.g., <ref> [23] </ref>, [24]). <p> Then, the one-step search can be performed by simply evaluating each function ap-proximator at the given state. The best action is the one associated with the function producing the maximum value (see, for example, <ref> [9, 18, 23] </ref>). However, such technique becomes quickly impractical when the number of actions increases due to storage implications and computational efficiency because the number of function approximators is proportional to the number of actions. <p> However, a small fraction, *, of the time, an action is chosen randomly uniformly from the operating range. This policy is called *-greedy-policy (see <ref> [23] </ref>). Both performing and learning are put together in the SARSA algorithm ([18, 23]). <p> A query is performed by first activating all the features that contain the state-action input and then summing the values of all the activated features. Figure 3 shows a bidimensional example of a CMAC organization. CMACs have been widely used in conjunction with reinforcement learning <ref> [26, 23, 24] </ref>. The size of a CMAC depends on the number of tilings and the size of each tiling. Tilings are usually large to provide enough resolution and they grow exponentially with the number of variables.
Reference: [24] <author> C. L. Tham. </author> <title> Reinforcement learning of multiple tasks using a hierarchical cmac architecture. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 15(4) </volume> <pages> 247-274, </pages> <year> 1995. </year>
Reference-contexts: Successful results have been obtained with multi-layer neural networks (e.g., [9], [18]) and sparse coarse coding methods such as Cerebellar Model Articulation Controllers (CMACs) (e.g., [23], <ref> [24] </ref>). <p> A query is performed by first activating all the features that contain the state-action input and then summing the values of all the activated features. Figure 3 shows a bidimensional example of a CMAC organization. CMACs have been widely used in conjunction with reinforcement learning <ref> [26, 23, 24] </ref>. The size of a CMAC depends on the number of tilings and the size of each tiling. Tilings are usually large to provide enough resolution and they grow exponentially with the number of variables.
Reference: [25] <author> J. N. Tsitsiklis and B. Van Roy. </author> <title> Analysis of temporal-difference learning with function approximation. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 9, </volume> <year> 1996. </year>
Reference-contexts: A condition for the convergence is that all states are visited and all actions are executed infinitely often. Tsitsiklis and Van Roy <ref> [25] </ref> shows that the value function associated with a given policy converges for any linear function approximator and TD () updates.
Reference: [26] <author> C. J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Univeristy of Cambridge, </institution> <address> England, </address> <year> 1989. </year> <month> 43 </month>
Reference-contexts: Alternatively, the action-value function (or simply, the Q-function as Watkins <ref> [26] </ref> defines it) measures the expected return of executing action u at state x t , and then following the policy () for selecting actions in subsequent states; therefore, the name of action-value function. <p> Sutton defines a whole family of update formulas for temporal difference learning called TD (), where 0 1 is a weight used to measure the relevance of previous predictions in the current error. The convergence of TD () has been proved under different conditions and assumptions. Watkins <ref> [26] </ref> shows that the Q-function estimates asymptotically converge to their optimal values in systems having discrete and finite state and action spaces when TD (0) is used to perform the updates. A condition for the convergence is that all states are visited and all actions are executed infinitely often. <p> Q (x; u) = ff (r t+1 + fl max ^ Q (y; a) ^ Q (x; u)) (8) Watkins showed that under certain conditions this update rule can be used to asymptotically learn the optimal Q-values in systems having discrete state and action spaces <ref> [26] </ref>. Also, Peng and Williams described a way to use TD () in a learning algorithm they call Q () [15], which results in a learning algorithm with faster convergence rate. <p> A query is performed by first activating all the features that contain the state-action input and then summing the values of all the activated features. Figure 3 shows a bidimensional example of a CMAC organization. CMACs have been widely used in conjunction with reinforcement learning <ref> [26, 23, 24] </ref>. The size of a CMAC depends on the number of tilings and the size of each tiling. Tilings are usually large to provide enough resolution and they grow exponentially with the number of variables.
References-found: 26

