URL: http://www.cse.ogi.edu/Sparse/paper/stoltz.cprop.94.ps
Refering-URL: http://www.cse.ogi.edu/Sparse/sparse.cprop.html
Root-URL: http://www.cse.ogi.edu
Email: stoltz@cse.ogi.edu  
Phone: (503) 690-1121 ext. 7404  
Title: Constant Propagation: A Fresh Demand-Driven Look  
Author: Eric Stoltz Michael Wolfe, and Michael P. Gerlek 
Note: grants and stuff here...  
Date: September 12, 1993  
Address: P.O. Box 91000 Portland, OR 97291-1000  
Affiliation: Department of Computer Science and Engineering Oregon Graduate Institute of Science Technology  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Jean-Paul Tremblay and Paul G. Sorenson. </author> <title> The Theory and Practice of Compiler Writing. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1985. </year>
Reference-contexts: 1 Introduction A Constant Propagation Primer Constant propagation is a static technique employed by the compiler to determine values which do not change regardless of which program path is taken. In fact, it is a generalization of constant folding <ref> [1] </ref>, the deduction at compile time that the value of an expression is constant, and is frequently used as a preliminary to other optimizations. The results can often be propagated to other expressions, enabling further applications of the technique.
Reference: [2] <author> J. Kam and J. Ullman. </author> <title> Monotone data flow anlaysis frameworks. </title> <journal> Acta Informatica 7, </journal> <pages> pages 305-317, </pages> <year> 1977. </year>
Reference-contexts: It should be noted that constant propagation is usually applied only to integer values, since operations on real-valued expressions are not always well-defined, and often architecturally dependent. Although in general constant S1: x = 2 + 3 propagation is an undecidable problem <ref> [2] </ref>, it is nonetheless extremely useful and profitable for a number of optimizations. These include dead code elimination [3], which can prevent unnecessary compilation, array- and loop-bound propagation, and procedure integration and in-lining, which we believe to be a major source of detectable constants.
Reference: [3] <author> Mark N. Wegman and F. Kenneth Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Although in general constant S1: x = 2 + 3 propagation is an undecidable problem [2], it is nonetheless extremely useful and profitable for a number of optimizations. These include dead code elimination <ref> [3] </ref>, which can prevent unnecessary compilation, array- and loop-bound propagation, and procedure integration and in-lining, which we believe to be a major source of detectable constants. Due to these benefits, constant propagation is an integral component of modern optimizing commercial compilers [4, 5]. <p> Top (&gt;) is the initial state for all nodes. When comparing two lattice element values, the meet operator (u) is applied, as given in Table 1. These foundations are standard with many constant propagation methods <ref> [3, 6, 7] </ref>, and originally introduced by Kildall [8]. Each symbol has its lattice element initialized to &gt;, which indicates that it has an as yet undetermined value. <p> By starting values at &gt;, an optimistic approach is taken, which assumes all symbols can be determined to be constant until proven otherwise. Previous methods perform the analysis as an iterative data-flow problem [9], in which iterations continue until a fixed point is reached <ref> [3, 7] </ref>. <p> i u constant j = constant i , i = j constant i u constant j = ?, i 6= j Table 1: Rules for meet (u) operator. 3 alternative demand-driven recursive algorithm offers advantages over the traditional approach. 2.2 Previous Methods 2.2.1 Classification As explained by Wegman and Zadeck <ref> [3] </ref>, constant propagation algorithms can be classified in two ways, (i) entire graph or sparse graph representation, and (ii) simple or conditional constant detection. This naturally creates four classes of algorithms. <p> The distinction between the four types of algorithms is explained well by Wegman and Zadeck <ref> [3] </ref>, and the reader is referred to their paper for more detail. We will look at the algorithm which they present, since it incorporates both sparse graph representation and conditional code. <p> Practically, however, this is undesirable (managing the symbol table explosion alone precludes this option), so the SSA properties are maintained by providing unique links between each use and its one definition. Instead of providing def-use links as is the common implementation <ref> [3, 11] </ref>, we provide use-def links, giving rise to an SSA graph comprised of factored use-def chains (FUD chains). This approach yields several advantages, such as constant space per node and an ideal form with which to perform demand-driven analysis.
Reference: [4] <author> Steve S. Muchnick. </author> <title> Optimizing compilers for SPARC. </title> <booktitle> Sun Technology, </booktitle> <pages> pages 161-173, </pages> <year> 1988. </year> <month> Summer. </month>
Reference-contexts: These include dead code elimination [3], which can prevent unnecessary compilation, array- and loop-bound propagation, and procedure integration and in-lining, which we believe to be a major source of detectable constants. Due to these benefits, constant propagation is an integral component of modern optimizing commercial compilers <ref> [4, 5] </ref>. The rest of the paper is organized as follows. In Section 2 we examine the standard frame-work employed to perform constant propagation and relate it to previous methods and algorithms. We also cover the class of constants found, and the running times of these methods.
Reference: [5] <author> D. Blickstein, P. Craig, C. Davidson, R. Faiman, K. Glossop, R. Grove, S. Hobbs, and W. Noyce. </author> <title> The GEM optimizing compiler system. </title> <journal> Digital Technical Journal, </journal> <volume> 4 </volume> <pages> 121-136, </pages> <year> 1992. </year> <note> Special Issue. 22 </note>
Reference-contexts: These include dead code elimination [3], which can prevent unnecessary compilation, array- and loop-bound propagation, and procedure integration and in-lining, which we believe to be a major source of detectable constants. Due to these benefits, constant propagation is an integral component of modern optimizing commercial compilers <ref> [4, 5] </ref>. The rest of the paper is organized as follows. In Section 2 we examine the standard frame-work employed to perform constant propagation and relate it to previous methods and algorithms. We also cover the class of constants found, and the running times of these methods.
Reference: [6] <author> David Callahan, Keith D. Cooper, Ken Kennedy, and Linda Torczon. </author> <title> Interprocedural constant propagation. </title> <booktitle> In Proceedings of Sigplan Symposium on Compiler Construction, </booktitle> <volume> volume 21, </volume> <month> June </month> <year> 1986. </year>
Reference-contexts: Top (&gt;) is the initial state for all nodes. When comparing two lattice element values, the meet operator (u) is applied, as given in Table 1. These foundations are standard with many constant propagation methods <ref> [3, 6, 7] </ref>, and originally introduced by Kildall [8]. Each symbol has its lattice element initialized to &gt;, which indicates that it has an as yet undetermined value.
Reference: [7] <author> Dan Grove and Linda Torczon. </author> <title> Interprocedural constant propagation: A study of jump function implementations. </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 90-99, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Top (&gt;) is the initial state for all nodes. When comparing two lattice element values, the meet operator (u) is applied, as given in Table 1. These foundations are standard with many constant propagation methods <ref> [3, 6, 7] </ref>, and originally introduced by Kildall [8]. Each symbol has its lattice element initialized to &gt;, which indicates that it has an as yet undetermined value. <p> By starting values at &gt;, an optimistic approach is taken, which assumes all symbols can be determined to be constant until proven otherwise. Previous methods perform the analysis as an iterative data-flow problem [9], in which iterations continue until a fixed point is reached <ref> [3, 7] </ref>.
Reference: [8] <author> G. A. Kildall. </author> <title> A unified approach to global program optimization. </title> <booktitle> In Conference Record of the First ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 194-206, </pages> <month> October </month> <year> 1973. </year>
Reference-contexts: Top (&gt;) is the initial state for all nodes. When comparing two lattice element values, the meet operator (u) is applied, as given in Table 1. These foundations are standard with many constant propagation methods [3, 6, 7], and originally introduced by Kildall <ref> [8] </ref>. Each symbol has its lattice element initialized to &gt;, which indicates that it has an as yet undetermined value. After analysis is complete, all symbols will either be ?, which means it cannot be determined to be constant, constant, or &gt;, indicating un-executable code. <p> We find the same class of simple constants as in the case of other non-conditional solvers, such as Kildall <ref> [8] </ref> and Reif and Lewis [15]. * When at a merge node, we take the meet of the demanded classification of the - arguments.
Reference: [9] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: We note that values can only move down in the lattice, due to the meet operator. By starting values at &gt;, an optimistic approach is taken, which assumes all symbols can be determined to be constant until proven otherwise. Previous methods perform the analysis as an iterative data-flow problem <ref> [9] </ref>, in which iterations continue until a fixed point is reached [3, 7]. <p> A non-reducible graph contains loops with multiple entries this leads to problems both in loop detection (we classify loops according to the natural loop <ref> [9] </ref> definition), as well as control dependence problems (in a non-reducible graph, the transitive control dependence of a node can skip over the immediate dominator).
Reference: [10] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently computing Static Single Assignment form and the control dependence graph. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: It is clear that propagating information about each variable to every node in a graph is inefficient, since not all nodes contain references or definitions of the symbol under consideration. A sparse representation, on the other hand, such as Static Single Assignment (SSA) <ref> [10] </ref>, Dependence Flow Graphs (DFG) [11], or Program Dependence Graphs (PDG) [12], have all shown the virtue of operating on a sparse graph for analysis. The distinction between simple (all paths) constants and conditional constants can be seen in Figure 3. <p> The -function is itself considered a new definition of the variable. For details on SSA graph construction, which include two key phases known as -placement and renaming, the reader is referred to the paper by Cytron et al. <ref> [10] </ref>. <p> This algorithm relies heavily on the concept of control dependence. Informally, X is control dependent on Y if one path from Y must reach X, while another path may avoid X. Cytron et al. <ref> [10] </ref> showed that control dependence is equivalent to dominance frontiers in the reverse CFG. We compute control dependence only on the forward CFG, eliminating back edges. Not infrequently, fl-functions can be reduced. This can occur in two ways: 1. The same predicate occurs more than once in a fl-function.
Reference: [11] <author> Richard Johnson and Keshav Pingali. </author> <title> Dependence-based program analysis. </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 78-89, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: It is clear that propagating information about each variable to every node in a graph is inefficient, since not all nodes contain references or definitions of the symbol under consideration. A sparse representation, on the other hand, such as Static Single Assignment (SSA) [10], Dependence Flow Graphs (DFG) <ref> [11] </ref>, or Program Dependence Graphs (PDG) [12], have all shown the virtue of operating on a sparse graph for analysis. The distinction between simple (all paths) constants and conditional constants can be seen in Figure 3. <p> Practically, however, this is undesirable (managing the symbol table explosion alone precludes this option), so the SSA properties are maintained by providing unique links between each use and its one definition. Instead of providing def-use links as is the common implementation <ref> [3, 11] </ref>, we provide use-def links, giving rise to an SSA graph comprised of factored use-def chains (FUD chains). This approach yields several advantages, such as constant space per node and an ideal form with which to perform demand-driven analysis. <p> It may well be that dead code is best identified using edges instead of nodes, as pointed out by Wegman and Zadeck. * Derived assertions. Traditional SSA form has been criticized for lacking a method to propagate constants determined by predicate analysis <ref> [11] </ref>. In the following fragment if (x 1 =1) then i 0 = x 1 else j 0 = x 1 endif it is desirable to be able to assign i 0 constant value.
Reference: [12] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: A sparse representation, on the other hand, such as Static Single Assignment (SSA) [10], Dependence Flow Graphs (DFG) [11], or Program Dependence Graphs (PDG) <ref> [12] </ref>, have all shown the virtue of operating on a sparse graph for analysis. The distinction between simple (all paths) constants and conditional constants can be seen in Figure 3.
Reference: [13] <author> Eric Stoltz, Michael P. Gerlek, and Michael Wolfe. </author> <title> Extended SSA with factored use-def chains to support optimization and parallelism. </title> <booktitle> In 1994 ACM Conf. Proceedings Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1994. </year> <note> to appear. </note>
Reference: [14] <author> Michael Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 162-174, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Each basic block contains a list of intermediate code tuples, which themselves are linked together as part of the data-flow graph. The data-flow graph consists of tuples of the form &lt;op,left,right,ssalink&gt;, as described by Wolfe <ref> [14] </ref>, where op is the operation code and left and right are the two operands (both are not always required, e.g. a unary minus). The ssalink is used for fetches, including arguments of -functions, as well as indexed stores (which are not discussed further in this paper). <p> Variables which cycle around loops are detected by induction variable analysis. Induction variables are used traditionally for strength reduction, and more recently for dependence analysis with regard to subscripts. We have developed tools for detecting and classifying induction variables (including non-linear induction variables as described by Wolfe <ref> [14] </ref>) based on strongly-connected regions in the SSA data-flow graph. [citations in the final paper.] These techniques make use of an exit function, the -function, which holds the exit value of a variable when leaving any loop.
Reference: [15] <author> John H. Reif and Harry R. Lewis. </author> <title> Symbolic evaluation and the global value graph. </title> <booktitle> In Conference Record of the Fourth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 104-118, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: We find the same class of simple constants as in the case of other non-conditional solvers, such as Kildall [8] and Reif and Lewis <ref> [15] </ref>. * When at a merge node, we take the meet of the demanded classification of the - arguments.
Reference: [16] <author> Robert A. Ballance, Arthur B. Maccabe, and Karl J. Ottenstein. </author> <title> The program dependence web: A representation supporting control-, data-, and demand-driven interpretation of imperative languages. </title> <booktitle> In Proc. ACM SIGPLAN '90 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Examine Figure 4 (b). When attempting to classify x 1 , the value is demanded from the use-def SSA link of y 2 , which points to the -function. But a -function is not interpretable <ref> [16] </ref>. Thus we have no information about which path may or may not be taken. Since the predicate P in our example determines the path taken, if P is constant, we can determine which argument of the -function to evaluate. <p> Of course, if P is not constant, the best we can do is take the meet of the -arguments. 10 Thus, we need to extend the -function to include this additional information. We extend the SSA form to a gated single assignment form (GSA), introduced by Ballance et al. <ref> [16] </ref>, which allows us to evaluate conditionals based upon their predicates. Figure 4 (c) shows a simple program converted to GSA form. Briefly, -functions are reclassified into two distinct functions: and fl-functions. Loop-header nodes which contain a -function are renamed as -functions, while all other -functions are converted to fl-functions.
Reference: [17] <author> Paul Havlak. </author> <title> Construction of thinned gated single-assignment form. </title> <booktitle> In Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year> <month> 23 </month>
Reference-contexts: Loop-header nodes which contain a -function are renamed as -functions, while all other -functions are converted to fl-functions. Several important notes are necessary: * We provide the complete algorithm to convert -functions to fl- and -functions in Appendix A. A similar method is employed by Havlak <ref> [17] </ref>, which is aimed at value-numbering, and as such thins the fl-function to eliminate paths which cannot reach a merge point. <p> There are interesting tradeoffs, with unclear answers presently. * Value-numbering. Although not per se constant propagation, the structure of GSA lends itself particularly well to implementing value numbering, as has been shown by Havlak <ref> [17] </ref>. We have presented a new demand-driven method of performing conditional constant prop-agation, which works on sparse data-flow graphs, finds the same class of constants as previous algorithms, but avoids evaluating expressions more than once.
References-found: 17

