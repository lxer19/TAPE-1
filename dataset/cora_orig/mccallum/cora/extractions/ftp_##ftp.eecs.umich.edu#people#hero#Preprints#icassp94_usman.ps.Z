URL: ftp://ftp.eecs.umich.edu/people/hero/Preprints/icassp94_usman.ps.Z
Refering-URL: http://www.eecs.umich.edu/~hero/det_est.html
Root-URL: http://www.cs.umich.edu
Title: RECURSIVE CR BOUNDS: ALGEBRAIC AND STATISTICAL ACCELERATION  
Author: Mohammad Usman Alfred O. Hero 
Address: Ann Arbor MI 48109-2122  
Affiliation: 1 Dept. of Electrical Engineering and Computer Science The University of Michigan,  
Abstract: Computation of the Cramer-Rao bound involves inversion of the Fisher information matrix (FIM). The inversion can become computationally intractable when the number of unknown parameters is large. Hero has presented a recursive, monotonically convergent and computationally efficient algorithm to invert sub-matrices of the FIM corresponding to a small region of interest in image reconstruction [1]. The convergence rate of this algorithm depends on a splitting matrix which can be interpreted as a complete-data FIM. In this paper we investigate the acceleration of the algorithm using several different choices of the complete-data FIM. We also present a conjugate gradient based algorithm which achieves a much faster convergence rate at the expense of monotone convergence. We apply the methods developed in this paper to emission tomography. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.O. Hero, J.A. Fessler and W.L. </author> <note> Rogers </note>
Reference-contexts: Often we require a bound on few estimator components of interest, called the Region Of Interest (ROI). Hero et al. <ref> [1] </ref> presents a recursive and computationally efficient method for calculating a small portion of matrix F Y 1 which corresponds to a q fiq ROI. <p> In this paper we obtain an optimal F X by purely algebraic, non-statistical approach, as a solution to a constrained optimization problem. The main advantage of the algorithm of <ref> [1, 2] </ref> is its monotone convergence which generates a valid improving lower bound on estimator covariance at each iteration of the algorithm. However, the price paid for monotone convergence is slow linear convergence rate. <p> RECURSIVE ALGORITHMS 3.1. Monotone Convergent Algorithm Assume that we want to calculate the CR-Bound for only the first component of , corresponding to the top left entry of F 1 Y . The algorithm can be easily extended to q parameters, q n [5]. Let e 1 = <ref> [1; 0; 0; :::; 0] </ref> T . <p> ICASSP-94 1 c fl IEEE 1994 B (l) = e T In <ref> [1] </ref> it is shown that if F X dominates F Y in the sense of positive semi-definitiveness of F X F Y then the algorithm generates a sequence of approximations B (l) which monotonically converges to 1fi1 sub-matrix B of F Y 1 as l ! 1. <p> Non-monotone Convergent Algorithm: Con jugate Gradient Conjugate gradient is an algorithm to solve the linear sys tem of equation F Y x = b [8, 7]. If we substitute b = <ref> [1; 0; 0; :::; 0] </ref> T then it is easy to recognize that the solution to such a system of equations will be the first column of F 1 Y . <p> The following conjugate gradient algorithm is applicable to symmetric, positive definite matrices F Y [8]. Algorithm INITIALIZATIONS: u 0 = 0; b = <ref> [1; 0; 0; :::; 0] </ref> T ; r 0 = b RECURSION: FOR i := 0 UNTIL |r (i)| &lt; Tolerance DO IF i = 0 DO ELSE ff i = &lt; r i1 ; r i1 &gt; END IF &lt; r i ; r i &gt; u i+1 = u
References-found: 1

