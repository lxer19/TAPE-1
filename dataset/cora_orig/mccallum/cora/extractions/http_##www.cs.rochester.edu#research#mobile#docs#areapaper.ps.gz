URL: http://www.cs.rochester.edu/research/mobile/docs/areapaper.ps.gz
Refering-URL: http://www.cs.rochester.edu/research/mobile/docs/
Root-URL: 
Title: Issues in Autonomous Mobile Robot Navigation  
Author: Amit Singhal 
Date: May 5, 1997  
Affiliation: Computer Science Department University of Rochester  
Abstract: 1 The author would like to express his gratitude to everybody who read preliminary versions of this report and gave invaluable feedback; especially Chris Brown, who spent innumerable hours poring over this paper. It would not have been possible without him. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Aoki et al. </author> <title> Acquisition of Optimal Action Selection to Avoid Moving Obstacles in Autonomous Mobile Robot. </title> <booktitle> Proceedings of ICRA-96, </booktitle> <address> Minneapolis, MN, </address> <month> 22-28 Apr, </month> <year> 1996, </year> <pages> pp 2055-60. </pages>
Reference-contexts: A second group of approaches has been proposed to deal with unexpected obstacles and unknown trajectories. This approach, commonly called collision avoidance or collision detection, is based on reactive navigation and is not subject to the hardness constraints imposed on the planning problem <ref> [1] </ref>, [7]. While most reactive navigation systems tend to be based on ad-hoc designs, they can also be learned from observations made of the environment [7], [80], [77]. While neither of the above require any prior knowledge of the obstacle motion, they have their drawbacks. <p> However, when there is unpredictability in the estimation of these attributes of an obstacle, a path planning approach is not guaranteed to generate a collision free path <ref> [1] </ref>. In this case, a reactive or hybrid approach to motion planning is employed. 2.5.1 Reactive Approaches Reactive approaches to motion planning were the first ones to be used in dynamic environments. <p> A fuzzy set A in a universe of discourse X is defined by its membership function A (x). For each x 2 X, there exists a value A (x) 2 <ref> [0; 1] </ref> of the membership function representing the degree of the membership of x in X. In fuzzy systems, membership functions associated with linguistic variables are used to fuzzify physical quantities. A fuzzy rule base that characterizes the relationships between fuzzy inputs and fuzzy outputs is created. <p> Since the design process is usually ad-hoc, it is very hard to maintain a fuzzy control system for obstacle avoidance if the environmental constraints change. To alleviate this problem, Aoki et al. proposed a reinforcement learning approach that generates an optimal set of fuzzy rules <ref> [1] </ref>. The initial training phase allows the robot to explore its environment and learn how to avoid obstacles by providing a reward or punishment feedback when it avoids or collides with obstacles respectively.
Reference: [2] <author> D. Apostolopoulus, H. Schempf, and J. West. </author> <title> Mobile Robot for Automatic In stallation of Floor Tiles. </title> <booktitle> Proceedings of ICRA-96, </booktitle> <address> Minneapolis, MN, </address> <month> 22-28 Apr, </month> <year> 1996, </year> <pages> pp 3652-7. </pages>
Reference-contexts: Mobile autonomous robots are very much in the research and development phase. Even though there have been a few commercially viable robot systems created in the past decade <ref> [2] </ref>, [6], a general purpose mobile robot that truly exhibits the intelligence required to learn new tasks and perform them in an autonomous manner eludes us. <p> The computational limitation notwithstanding, there exists a plethora of tractable implementations of the problems posed by researchers in neural modeling, sensor modeling, data fusion and control theory. Apostolopoulos et al. describe a vision guided autonomous floor-tile laying mobile robot that possesses integrated sensor and actuator control <ref> [2] </ref>. Blanche is a mobile robot designed for structured office or factory environments and uses an off-line path planner to generate collision-free maneuvers with the additional assumption that the vehicle's location is always known [16]. <p> However, most of these highly successful endeavors have been in highly structured and static environments. Apostolopoulos et al. describe an autonomous mobile robot for floor tile laying <ref> [2] </ref>. The robot is capable of omni-directional locomotion and uses stereo cameras and light-striping for external sensing. It is also equipped with an internal dead-reckoning system based on its wheel encoders. A sample trial of the robot consists of laying a floor of tiles in a simple grid pattern.
Reference: [3] <author> S. Azami, S. Katahara, and M. Aoki. </author> <title> Route Guidance Sign Identification using 2-D Structural Description. </title> <booktitle> Proceedings of the 1996 IEEE Intelligent Vehicles Symposium, </booktitle> <address> Tokyo, Japan, </address> <month> 19-20 Sep, </month> <year> 1996, </year> <pages> pp 153-8. </pages>
Reference-contexts: A different approach to road scene identification and traffic sign recognition uses structure data obtained from a route guidance sign (RGS) database and performs template matching against images obtained from a moving camera to recognize traffic and road signs <ref> [3] </ref>. Each object in the RGS database is modeled as a two level recursive structure. The shape of the object is modeled as a rectangle, and the characters are represented as circumscribed rectangles. Segmentation (based on intensity and color) is used to extract the signs from road scenes.
Reference: [4] <author> D.H. Ballard and C.M. Brown. </author> <title> Computer Vision, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1982. </year>
Reference: [5] <author> Y. Bar-Shalom and T.E. Fortmann. </author> <title> Tracking and Data Association, </title> <publisher> Academic Press Limited, </publisher> <address> San Diego, CA, </address> <year> 1988. </year>
Reference: [6] <author> J. Borenstein, H.R. Everett, and L. Feng. </author> <title> Navigating Mobile Robots : Systems and Techniques, </title> <publisher> A K Peters Ltd., </publisher> <address> Wellesley, MA, </address> <year> 1996. </year>
Reference-contexts: Today's high performance world demands precise and scientific solutions from the robotics industry. The newer robots are more mobile, carry larger payloads, perform a varied number of tasks, are capable of learning new operations, and most importantly, do all this independently of a human controller <ref> [6] </ref>, [36], [57], [74], [77]. However, to claim that we have robots that truly exhibit even a majority of the above mentioned traits would be an outright lie. Mobile autonomous robots are very much in the research and development phase. <p> Mobile autonomous robots are very much in the research and development phase. Even though there have been a few commercially viable robot systems created in the past decade [2], <ref> [6] </ref>, a general purpose mobile robot that truly exhibits the intelligence required to learn new tasks and perform them in an autonomous manner eludes us. <p> Perhaps the most interesting result of a survey of the vast body of literature on robot localization is that no generalized elegant solution to the problem exists. The many different methodologies proposed can be divided into the following two groups <ref> [6] </ref> : relative and absolute position measurements. Most implementations of localization algorithms in current use combine multiple approaches from the two categories. Relative Position Methods Relative position methods rely on two approaches : odometry and inertial navigation. <p> Inertial navigation employs gyroscopes and accelerometers to continuously measure minute accelerations along the three directional axes. The velocity and position are estimated by integrating the measurements over time (once to obtain velocity and twice to obtain position) <ref> [6] </ref>. The major advantage of inertial navigation is that, like odometry, it is completely self contained. However, it cannot be used for an extended period of time as any positioning error increases without bound after integration, due to imprecision and drift. <p> The calculations for trilateration are almost similar to those for triangulation. Global Positioning Systems use trilateration techniques based on time of flight information for uniquely coded radio signals. Distances from the robot to at least three satellites (beacons) are required to determine latitude, longitude and elevation <ref> [6] </ref>. For both methods, the only a-priori information required is the exact location of each of the active beacons. One problem with these methods is that the beacons need to be extremely powerful to ensure omni-directional transmission over large distances. <p> Other problems include lack of recognition due to ambient lighting, marginal visibility, and competition from other objects in the environment with similar features. Another disadvantage is that artificial landmark recognition requires careful engineering of the environment, something that may not always be possible <ref> [6] </ref>. Natural landmark recognition attempts to overcome the last disadvantage of the above method by trying to recognize distinctive features that are an inherent part of the environment. The environment must be known in advance but does not have to be engineered in any manner. <p> This is especially true if one uses natural landmarks in an outdoor environment since the time required to recognize non-geometric and arbitrarily shaped landmarks (typically present in outdoor environments) is much higher than that required for recognizing linear landmarks (peg. doors and windows in the indoor environment) <ref> [6] </ref>. Landmark navigation approaches that rely on fast recognition of a limited number of landmarks (eg. doors and windows) require a series of observations before a unique position can be determined. <p> There need to be enough stationary, easily distinguishable features that can be used for matching. The sensor map and the terrain maps must be accurate enough to be useful. Also, correlation (the most common matching technique) requires a significant amount of sensing and processing power to be computationally feasible <ref> [6] </ref>. 1.2.2 Goal Specification and Goal Recognition Strictly speaking, goal specification and goal recognition are two different problems. Goal specification involves identifying the goals to the mobile robot navigation system. This is usually a task of the human designer. <p> Some other partially successful attempts at autonomous indoor mobile robot navigation are described in greater detail in [14], [51], [81]. The presence of mobile obstacles, unstructured terrain and multiple (possibly) dependent sensors introduce uncertainties and complexities that pose a much harder problem than navigation in a static structured domain <ref> [6] </ref>, [44]. Most of the current research work is targeted towards finding better solutions to the autonomous navigation problem in less and less constrained environments. CMU's NAVLAB project [36] and Dickmanns' VaMoRs project [18] lead the autonomous on-road driving research efforts in the world. <p> The representation chosen for modeling the environment is a modification of the Occupancy Grids proposed by Elfes in [20]. Though occupancy grids are a recent contribution to the representation literature, they have gained instant popularity among researchers developing probabilistic navigation systems <ref> [6] </ref>, [21]. We use a modified version of the occupancy grids, called dynamic occupancy grids as our representation of the environment. Section 3.2 presents the details of this representation. Pearl describes a Bayes rule based network approach for modeling causal relationships in [64].
Reference: [7] <author> H.R. Beom and H.S. Cho. </author> <title> A Sensor Based Navigation for a Mobile Robot Using Fuzzy Logic and Reinforcement Learning. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 25, no. 3, </volume> <month> Mar </month> <year> 1995, </year> <pages> pp 464-77. </pages>
Reference-contexts: A second group of approaches has been proposed to deal with unexpected obstacles and unknown trajectories. This approach, commonly called collision avoidance or collision detection, is based on reactive navigation and is not subject to the hardness constraints imposed on the planning problem [1], <ref> [7] </ref>. While most reactive navigation systems tend to be based on ad-hoc designs, they can also be learned from observations made of the environment [7], [80], [77]. While neither of the above require any prior knowledge of the obstacle motion, they have their drawbacks. <p> commonly called collision avoidance or collision detection, is based on reactive navigation and is not subject to the hardness constraints imposed on the planning problem [1], <ref> [7] </ref>. While most reactive navigation systems tend to be based on ad-hoc designs, they can also be learned from observations made of the environment [7], [80], [77]. While neither of the above require any prior knowledge of the obstacle motion, they have their drawbacks. Learning can be a very time consuming process, and has to be repeated whenever there is a change in domain constraints. <p> When all information regarding obstacle sizes, locations, motions etc. is known a-priori, we can use a path planning approach to generate a collision free path to reach the goal location <ref> [7] </ref>. However, when there is unpredictability in the estimation of these attributes of an obstacle, a path planning approach is not guaranteed to generate a collision free path [1]. <p> The sensors are polled every 0.2 seconds to update the robots view of its surrounding environment. Empirical studies show the effectiveness of this approach with completely collision free trajectories for the robot in the presence of moving robots. Beom and Cho <ref> [7] </ref> had earlier demonstrated how to use reinforcement learning techniques to learn obstacle avoidance and goal seeking behaviors. Their approach learned deterministic rules for the two behaviors and then used fuzzy logic to determine which behavior to execute at a given time.
Reference: [8] <author> B. Bouilly and T. Simeon. </author> <title> A Sensor Based Motion Planner for Mobile Robot Navigation with Uncertainty. </title> <booktitle> Reasoning With Uncertainty in Robotics 1995, </booktitle> <address> Am-sterdam, The Netherlands, </address> <month> 4-6 Dec, </month> <year> 1995, </year> <pages> pp 235-47. </pages>
Reference-contexts: A path planner then generates a route that takes the mobile obstacles into account. The sampling rate of the sensors depends on the uncertainty associated with the prediction of the obstacle trajectories [59]. Hybrid approaches to this problem have also been studied and found favor in many implementations <ref> [8] </ref>. In the remainder of the chapter, we will look at some of this recent work in much more detail and try to give an in-depth understanding of the current state-of-the-art in these areas. 2.3 Landmark Detection Many different methodologies for robot localization in structured indoor environments exist.
Reference: [9] <author> V. Brajovic and T. Kanade. </author> <title> Computational Sensors for Global Operations. </title> <booktitle> Pro ceedings of the 23rd Image Understanding Workshop, </booktitle> <address> Monterey, CA, 13-16 Nov, </address> <year> 1994, </year> <pages> pp 621-30. </pages>
Reference-contexts: Computational sensors is another very new and active sensor research area. Here, raw sensing and computation are integrated together on the sensor itself, eliminating the need to transmit sensor information to some other computational resource for processing <ref> [9] </ref>. Most of the computational sensor research to date has been limited to local sensory data preprocessing (eg. pixel-wise adaptation and convolution with a small kernel). This does not reduce the sensor-processor data transfer bottleneck by any appreciable amount, resulting in continued excessive latencies.
Reference: [10] <author> V. Brajovic and T. Kanade. </author> <title> A Sorting Image Sensor : An Example of Massively Parallel Intensity-to-Time Processing for Low-Latency Computational Sensors. </title> <booktitle> Proceedings of ICRA-96, </booktitle> <address> Minneapolis, MN, </address> <month> 22-28 Apr, </month> <year> 1996, </year> <pages> pp 1638-43. </pages>
Reference-contexts: The sorting sensor computes a histogram of the image intensities and assigns an ordinal number to each input in the histogram. With limited guidance from the user, it can perform simple operations such as segmentation and labeling <ref> [10] </ref>. The evolution of new sensors, some using completely different paradigms, creates a constant flux in multi-sensor fusion and integration methodologies.
Reference: [11] <author> R.A. Brooks and J.H. Connell. </author> <title> Asynchronous Distributed Control System for a Mobile Robot. </title> <booktitle> Proceedings of SPIE, </booktitle> <volume> vol. 727, </volume> <month> Oct </month> <year> 1986, </year> <pages> pp 77-84. </pages>
Reference-contexts: In this approach, the robot has to constantly poll its sensors to find the next goal state (the 10 next ball to pick up) until it has picked up all the balls. The subsumption architecture was the first attempt at a generalized formalization of this approach <ref> [11] </ref>. In the subsumption architecture, each robot is associated with a repertoire of behaviors that it can exhibit. Goal recognition is performed via polling the environment and selecting the next goal (and the corresponding behavior that achieves this goal). <p> The subsumed behavior may be resumed at a later time <ref> [11] </ref>. 1.2.3 Path Planning Once the goals of a robot have been identified, the next task is to navigate towards them. The two major motion strategies used for mobile robot navigation are reactive and planning [27]. In reactive navigation, only the goal is known (and sometimes its location). <p> Kortenkamp first proposed the use of a Bayesian Network (as opposed to statistical Bayesian methods) for performing multisensor fusion for topological map building in [43]. For more details on the theoretical foundations and implementations of Bayesian networks, please refer to section 3.3.1. Brooks <ref> [11] </ref> had earlier argued very persuasively for using topological maps as a means of dealing with uncertainty in mobile robot navigation. Elfes [20] later proposed another framework called occupancy grids which was also shown to be very efficient in dealing with the above problem.
Reference: [12] <author> C.M. Brown et al. </author> <title> Distributed Data Fusion Using Kalman Filtering : A Robotics Approach. </title> <booktitle> Data Fusion in Robotics and Machine Intelligence, </booktitle> <publisher> Academic Press Limited, </publisher> <address> San Diego, CA, </address> <year> 1992, </year> <pages> pp 267-309. </pages>
Reference-contexts: The sensor fusion literature can be broadly categorized into two main approaches : statistical fusion techniques and probabilistic fusion techniques. Statistical fusion techniques are based on least-squares approximation methods. Some of the representative techniques in this class include least squares optimization, Kalman filtering, and regularization techniques <ref> [12] </ref>, [39]. Probability theory, with its inherent notions of uncertainty and confidence, has found widespread popularity in the multisensor fusion community. The probabilistic models proposed by various researchers can be classified into four broad methodologies : Bayesian reasoning, evidence theory, robust statistics, and recursive operators [21], [39]. <p> [z (j) h (j; k)] 2 Other estimation rules that have been proven to be equivalent to least squares estimation (under some assumptions about the underlying prior and posterior probability distribution functions) include maximum likelihood estimation, Bayesian estimation, maximum a posteriori estimation, minimum mean square error estimation and linear estimation <ref> [12] </ref>. <p> One can use the extended Kalman filter updating rules provided in <ref> [12] </ref> to update the state estimate ^ x (k + 1jk + 1) and the state covariance P (k + 1jk + 1). In lay terms, this approach can be described as an algorithm that repeatedly performs a cycle of prediction, observation, data validation, and updating. <p> Another advantage of Kalman filters is that they readily lend themselves to decentralized architectures, with each node 31 responsible for prediction of local estimates. A communications protocol can be used to transfer this local information to other nodes for a global updation process <ref> [12] </ref>. The Kalman filter's main utility lies in low level fusion of redundant sensor data. However, there may exist situations where sensor data is not redundant but complementary in nature.
Reference: [13] <author> J. Canny. </author> <title> The Theory of Robot Motion Planning, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: Path-planning is known to be an NP-hard problem in a dynamic environment [67]. Another result shows that the above problem is PSPACE-hard even in the presence of constraints such as velocity bounds <ref> [13] </ref>. Despite the hardness of the path planning problem, various methods have been proposed to attack the problem in a dynamic environment. The first approach relies on a space-time formulation. <p> We have already noted that the problem of path planning in the presence of dynamic obstacles is NP-hard, even when the obstacles are convex polygons moving with bounded constant linear velocities without rotation <ref> [13] </ref>, [67]. However, this has not stopped researchers from proposing many different path planning algorithms made tractable by imposing certain additional constraints on the system. The Basic Path Planner The canonical path planning task in a static a-priori known environment can be performed in three steps [42] : 1.
Reference: [14] <author> C.C. Chang, K.T. Song. </author> <title> Sensor-Based Motion Planning of a Mobile Robot in a Dynamic Environment. </title> <booktitle> Proceedings of the 1996 IEEE IECON, </booktitle> <address> Taipei, Taiwan, 5-10 Aug, </address> <year> 1996, </year> <pages> pp 766-71. </pages>
Reference-contexts: The robot is also incapable of laying odd and cut tiles and creating intricate tile patterns. Some other partially successful attempts at autonomous indoor mobile robot navigation are described in greater detail in <ref> [14] </ref>, [51], [81]. The presence of mobile obstacles, unstructured terrain and multiple (possibly) dependent sensors introduce uncertainties and complexities that pose a much harder problem than navigation in a static structured domain [6], [44]. <p> Another area of active interest is path planning in the presence of dynamic obstacles. Various approaches have been suggested to tackle this problem, ranging from fully reactive and reflexive avoidance <ref> [14] </ref> to predicting obstacle motion [59]. The first approach uses continuous sensor polling (usually proximity sensors only) to detect any obstacles that enter the robot's personal space. The personal space is an enclosed area surrounding the robot that should ideally be kept free of any obstacles at all times. <p> The personal space is an enclosed area surrounding the robot that should ideally be kept free of any obstacles at all times. When some object invades the robot's personal space, the robot takes evasive action and avoids the obstacle <ref> [14] </ref>. In the second approach, the robot periodically polls its sensors (typically vision and long range sensors) to detect moving obstacles and predict their trajectories. A path planner then generates a route that takes the mobile obstacles into account.
Reference: [15] <author> D. Chung and F.L. Merat. </author> <title> Neural Network Based Sensor Array Signal Processing. </title> <booktitle> Proceedings of MFI-96, </booktitle> <address> Washington, D.C., </address> <year> 1996, </year> <pages> pp 757-64. </pages>
Reference-contexts: The evolution of new sensors, some using completely different paradigms, creates a constant flux in multi-sensor fusion and integration methodologies. Some commonly 15 used approaches to sensor fusion include Bayesian estimation [66], Kalman filter and its variants [55], fuzzy rule based systems [78], decision theory [45], neural networks <ref> [15] </ref> and Bayesian networks [43]. Even though we have so many different techniques for performing sensor data fusion, we still lack a framework that can combine sensor data from multiple sensing modalities effectively. Each of the above methods works only within their defined constraints and underlying assumptions.
Reference: [16] <author> I.J. Cox. </author> <title> Blanche An Experiment in Guidance and Navigation of an Au tonomous Mobile Robot. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 7, no. 3, </volume> <year> 1991, </year> <pages> pp 193-204. </pages>
Reference-contexts: Blanche is a mobile robot designed for structured office or factory environments and uses an off-line path planner to generate collision-free maneuvers with the additional assumption that the vehicle's location is always known <ref> [16] </ref>. CMU's NAVLAB project has made major national headlines with its first completely autonomous cross country trip. It uses computer vision algorithms to detect road edges, lane markers and other vehicles on the road to perform collision free driving [36]. <p> The drawback is that the position error grows boundlessly due to slippage and drift unless an independent reference is used periodically to reduce the error <ref> [16] </ref>. Inertial navigation employs gyroscopes and accelerometers to continuously measure minute accelerations along the three directional axes. The velocity and position are estimated by integrating the measurements over time (once to obtain velocity and twice to obtain position) [6].
Reference: [17] <author> J.L. Crowley. </author> <title> Mathematical Foundations of Navigation and Perception For an Autonomous Mobile Robot. </title> <booktitle> Reasoning With Uncertainty in Robotics 1995, </booktitle> <address> Ams-terdam, The Netherlands, </address> <month> 4-6 Dec, </month> <year> 1995, </year> <pages> pp 9-51. </pages>
Reference-contexts: These sensor discrepancies have to be handled in some framework to allow the robot to visualize a unified view of its environment. Most advances in sensor fusion have largely entailed rediscovery and adaptation of techniques from estimation theory <ref> [17] </ref>. The early work characterized the sensor fusion problem as one of incremental combination of geometric information. Most of these techniques were ad-hoc and did not characterize the uncertainties in the system. <p> They proposed the use of Bayesian estimation theory and derived a combination function that was an equivalent form of Kalman Filter. This caused a rapid paradigm shift towards probabilistic estimation theories closely related to Bayesian estimation, maximum likelihood estimation and least squares methods <ref> [17] </ref>. While these methods have been successful in combining sensor data that determine a common subset of the robot state vector (eg. robot location derived from GPS, odometry and gyroscope readings), there seems to be no obvious extension to multi-modal sensor fusion processes.
Reference: [18] <author> E.D. Dickmanns and N. Muller. </author> <title> Scene Recognition and Landmark Navigation for Road Vehicles. </title> <booktitle> Proceedings 2nd IFAC Conference on Intelligent Autonomous Vehicles, </booktitle> <address> Espoo, Finland, </address> <month> 12-14 June, </month> <year> 1995, </year> <pages> pp 199-204. </pages>
Reference-contexts: Other factors that further complicate outdoor navigation algorithms include the object recognition problem, the presence of (possibly random moving) obstacles, the uncertainties generated by multiple sensing modalities used by a robot for observation, and the weaknesses of sensing devices themselves along with poor theory and control of sensors <ref> [18] </ref>. 1.1.1 Computational Power Though we keep building faster CPUs and cheaper memory, most computer scientists believe that we will soon reach a plateau in the exponential growth of computational power witnessed in the last decade [23]. <p> Indoor environments exhibit a high degree of deterministic linear and non-linear visual features, providing robots with good structure and organization for image processing tasks. Outdoor environments tend to be unstructured and defeat computer vision solutions developed for the former <ref> [18] </ref>. How can one approach the problem of navigation in unstructured outdoor terrain? Most of the commonly used approaches to landmark and object recognition fail in the absence of linear visual features. <p> Most of the current research work is targeted towards finding better solutions to the autonomous navigation problem in less and less constrained environments. CMU's NAVLAB project [36] and Dickmanns' VaMoRs project <ref> [18] </ref> lead the autonomous on-road driving research efforts in the world. Both use edge detection algorithms to track roads and driving lanes. Object recognition algorithms are used for detecting obstacles and other cars and localization is done using road features like intersections, lane changes, corners, and bridges as landmarks. <p> Most of these approaches classify multiple features as a single landmark and detect these via a variety of methods such as Hough transform [51], edge detection <ref> [18] </ref>, [36], [69], and neural networks [22], [54]. Geometric Models Simple geometric models such as circles, ellipses or polygons are often used as artificial landmarks because they can be detected easily in real time using edge detection algorithms. <p> With at least 50% of the landmark visible, the error in the aspect angle is under 7% but degrades quickly when more of the landmark is occluded [51]. 19 Road Tracking Road edge detection and tracking is one of the most common navigational methods for on-road autonomous vehicles <ref> [18] </ref>, [69]. The NAVLAB project has extended these road detection capabilities to allow driving on dirt paths also [36]. Localization can be performed by using intersections, corners and curved roads as landmarks and keeping track of their sequence of occurrence to periodically update the position estimate generated via odometry methods. <p> NAVLAB has been able to achieve speeds in excess of 50 mph while driving on multi-lane highways. The systems developed for the NAVLAB project work very well in outdoor environments where fine precision navigation and position estimation is not required. VaMoRs <ref> [18] </ref>, an on-road test vehicle developed by Dickmanns' research group at the University of Munich tracks road edges, lane markers and the like for performing navigation. The vehicle employs two active bifocal vision systems, one for look-ahead and the other for look-back.
Reference: [19] <author> A. Dubrawski and J.L. Crowley. </author> <title> Self-Supervised Neural System for Reactive Navigation. </title> <booktitle> Proceedings 1994 IEEE International Conference on Robotics and Automation, </booktitle> <address> San Diego, CA, </address> <month> 8-13 May, </month> <year> 1994, </year> <pages> pp 2076-81. </pages>
Reference-contexts: This approach can be further extended to learn the environment during the initial explorations and trials. This learned partial information can then be used by the goal recognition algorithm to guide the robot when one of the known landmarks is specified as a goal <ref> [19] </ref>. The above approaches to goal specification and recognition identify a goal as a landmark or location that the robot is aiming to reach. A goal can also be something more abstract and task directed, eg. pick up all the balls from the playing field. <p> Based on these observations, a robot can change its speed and orientation, avoid obstacles, or perform some other task. Reactive navigation works especially well in the presence of dynamic obstacles and in unknown environments <ref> [19] </ref>. It has the added advantage of high flexibility, although it can be less efficient than a path planning strategy as it may take a long time to achieve the goal based purely on sampling the environment state.
Reference: [20] <author> A. Elfes. </author> <title> Occupancy Grids : A Probabilistic Framework for Robot Perception and Navigation. </title> <type> Ph.D. thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: For more details on the theoretical foundations and implementations of Bayesian networks, please refer to section 3.3.1. Brooks [11] had earlier argued very persuasively for using topological maps as a means of dealing with uncertainty in mobile robot navigation. Elfes <ref> [20] </ref> later proposed another framework called occupancy grids which was also shown to be very efficient in dealing with the above problem. Other probabilistic approaches to sensor fusion have employed fuzzy logic and rule based systems with varied success [78]. <p> The representation chosen for modeling the environment is a modification of the Occupancy Grids proposed by Elfes in <ref> [20] </ref>. Though occupancy grids are a recent contribution to the representation literature, they have gained instant popularity among researchers developing probabilistic navigation systems [6], [21]. We use a modified version of the occupancy grids, called dynamic occupancy grids as our representation of the environment. <p> This is especially important in our approach since we rely purely on the information present in the dynamic occupancy grids for performing navigation. Dynamic Occupancy Grids (DOG) are based on the occupancy grid framework proposed by Elfes in <ref> [20] </ref>. A description of the occupancy grids, presented in [20], follows. 3.2.1 Introduction to Occupancy Grids Occupancy grids are a stochastic tessellated representation of perceived spatial information about the robot's operating environment. The environment is explicitly divided into small square spatial areas called cells. <p> This is especially important in our approach since we rely purely on the information present in the dynamic occupancy grids for performing navigation. Dynamic Occupancy Grids (DOG) are based on the occupancy grid framework proposed by Elfes in <ref> [20] </ref>. A description of the occupancy grids, presented in [20], follows. 3.2.1 Introduction to Occupancy Grids Occupancy grids are a stochastic tessellated representation of perceived spatial information about the robot's operating environment. The environment is explicitly divided into small square spatial areas called cells. <p> The second drawback is that there exist subsets of configurations that are indistinguishable under a single sensor observation. In this case, numerical solutions to the above equations may not computable, and the navigation system may have to rely on close-formed solutions <ref> [20] </ref>. 3.2.2 Modified Occupancy Grids We use the occupancy grid approach developed by Elfes [20] and modify it to represent a more generalized environment than the one represented by Elfes as follows: 1. We associate a set of state vectors with each cell (instead of a single state vari able). <p> In this case, numerical solutions to the above equations may not computable, and the navigation system may have to rely on close-formed solutions <ref> [20] </ref>. 3.2.2 Modified Occupancy Grids We use the occupancy grid approach developed by Elfes [20] and modify it to represent a more generalized environment than the one represented by Elfes as follows: 1. We associate a set of state vectors with each cell (instead of a single state vari able).
Reference: [21] <author> A. Elfes. </author> <title> Robot Navigation: Integrating Perception, Environmental Constraints and Task Execution Within a Probabilistic Framework. Reasoning with Uncertainty in Robotics. </title> <booktitle> International Workshop, RUR'95 Proceedings, </booktitle> <address> Amsterdam, Netherlands, </address> <month> 4-6 Dec, </month> <year> 1995, </year> <pages> pp 93-129. </pages>
Reference-contexts: Multi-sensor fusion and integration research is a two pronged effort, one focusing on representations (eg. occupancy grids <ref> [21] </ref>) and the other focusing on methodologies (eg. Bayesian networks [43]). At the same time, physicists and electrical engineers are also involved in inventing new sensor technologies, the most notable of the recent ones being GPS [34]. Computational sensors is another very new and active sensor research area. <p> Probability theory, with its inherent notions of uncertainty and confidence, has found widespread popularity in the multisensor fusion community. The probabilistic models proposed by various researchers can be classified into four broad methodologies : Bayesian reasoning, evidence theory, robust statistics, and recursive operators <ref> [21] </ref>, [39]. In the remainder of this section, we analyze some of the representative work performed using these two sensor fusion paradigms. 2.4.1 Statistical Techniques Statistical techniques have their root in the least squares estimation. <p> The representation chosen for modeling the environment is a modification of the Occupancy Grids proposed by Elfes in [20]. Though occupancy grids are a recent contribution to the representation literature, they have gained instant popularity among researchers developing probabilistic navigation systems [6], <ref> [21] </ref>. We use a modified version of the occupancy grids, called dynamic occupancy grids as our representation of the environment. Section 3.2 presents the details of this representation. Pearl describes a Bayes rule based network approach for modeling causal relationships in [64].
Reference: [22] <author> A. de la Escalera et. al. </author> <title> Neural Traffic Sign Recognition for Autonomous Vehicles. </title> <booktitle> Proceedings of IECON'94, </booktitle> <address> Bologna, Italy, 5-9 Sept, </address> <year> 1994, </year> <pages> pp 841-6. </pages>
Reference-contexts: Most of these approaches classify multiple features as a single landmark and detect these via a variety of methods such as Hough transform [51], edge detection [18], [36], [69], and neural networks <ref> [22] </ref>, [54]. Geometric Models Simple geometric models such as circles, ellipses or polygons are often used as artificial landmarks because they can be detected easily in real time using edge detection algorithms. <p> Neural Networks Another popular methodology for detecting artificial landmarks uses multiple neural networks that are trained to recognize these landmarks quickly in a sequence of input images <ref> [22] </ref>, [53], [54]. Most of the commonly used neural network approaches for artificial landmark classification in the outdoor environment employ neural networks 21 that are trained to detect and classify road and traffic signs. <p> To test the system, fractal models of very different textured objects (such as road, sky, grass, stop sign and left turn sign) were used. Obviously, this cannot be used to perform localization. A similar approach to traffic sign recognition was proposed by Escalera et al. in <ref> [22] </ref>. They create a system solely targeted at detecting and classifying traffic signs. The algorithm has two main stages. In the first stage, color and shape information about traffic signs is used to segment and extract the regions of interest from input images.
Reference: [23] <author> K. Eshraghian. </author> <title> Challenges in Future Technologies. </title> <booktitle> Proceedings of the 14th IEEE VLSI Test Symposium, </booktitle> <address> Princeton, NJ, 28 Apr-1 May, </address> <year> 1996. </year>
Reference-contexts: the weaknesses of sensing devices themselves along with poor theory and control of sensors [18]. 1.1.1 Computational Power Though we keep building faster CPUs and cheaper memory, most computer scientists believe that we will soon reach a plateau in the exponential growth of computational power witnessed in the last decade <ref> [23] </ref>. The processing power required for real time image processors, computer vision, biomedical systems, telecommunications and machine learning far exceeds that of present day supercomputers. <p> CMOS technology has almost reached its limits and any new hardware design paradigm (possibly DNA 3 computing [37] or Photonics [33]) that may provide the computational resources we need is still years away from being commercially viable <ref> [23] </ref>. The computational limitation notwithstanding, there exists a plethora of tractable implementations of the problems posed by researchers in neural modeling, sensor modeling, data fusion and control theory. Apostolopoulos et al. describe a vision guided autonomous floor-tile laying mobile robot that possesses integrated sensor and actuator control [2].
Reference: [24] <author> H.R. Everett. </author> <title> Sensors for Mobile Robots. Theory and Application, A.K. </title> <publisher> Peters Ltd., </publisher> <address> Wellesley, MA, </address> <year> 1995. </year>
Reference-contexts: However, it cannot be used for an extended period of time as any positioning error increases without bound after integration, due to imprecision and drift. Highly accurate gyroscopes in current use are also prohibitively expensive, though newer products may bring this cost down <ref> [24] </ref>. 7 Absolute Position Methods Some of the commonly used absolute position measurement methods include triangulation of active beacons, trilateration of active beacons, artificial landmark recognition, natural landmark recognition, and model matching.
Reference: [25] <author> C. Fennema and A.R. Hanson. </author> <title> Experiments in Autonomous Navigation. </title> <booktitle> Proceed ings of the IEEE International Workshop on Intelligent Motion Control, Istanbul, Turkey, </booktitle> <month> 20-22 Aug, </month> <year> 1990, </year> <pages> pp 29-37. </pages>
Reference-contexts: While this approach is easy to use, it may require engineering of the environment and can only be used when the locations of the objects or landmarks are known a-priori and the robot is given an accurate map <ref> [25] </ref>. An extension to the above approach attempts to use user-friendly goal specifications to identify the goals to a robot [69]. The position of the goal is not given by its coordinates but by a verbal description of the route to it.
Reference: [26] <author> T. Fraichard and C. Laugier. </author> <title> Dynamic Trajectory Planning, Path-Velocity De composition and Adjacent Paths. </title> <booktitle> Proceedings of IJCAI-93, </booktitle> <address> Chambery, France, </address> <month> 28 Aug-3 Sep, </month> <year> 1993, </year> <pages> pp 1592-7. </pages>
Reference-contexts: The first approach relies on a space-time formulation. Given the predicted motion of all the obstacles in the environment, a reachability region containing all possible locations for a robot in some future time is created. This is then used to guide the robot to a desired location <ref> [26] </ref>, [40], [79]. The major problem with this approach is the amount of time required to construct the reachability region.
Reference: [27] <author> K. Fujimura. </author> <title> Motion Planning in Dynamic Environments, </title> <publisher> Springer-Verlag, </publisher> <address> Tokyo, Japan, </address> <year> 1991. </year>
Reference-contexts: The subsumed behavior may be resumed at a later time [11]. 1.2.3 Path Planning Once the goals of a robot have been identified, the next task is to navigate towards them. The two major motion strategies used for mobile robot navigation are reactive and planning <ref> [27] </ref>. In reactive navigation, only the goal is known (and sometimes its location). However, no explicit encoding of a path that can be followed to achieve that goal is generated. The robot periodically polls its sensors to determine the current state of its location and environment. <p> The other motion planning technique is reactive navigation, where such a global path is not generated. Motion planning is accomplished via selecting from a series of actions whose global effect is the movement of the robot from the initial location to the final location <ref> [27] </ref>. The capability of motion planning in dynamic environments is very important for autonomous robots. A relevant issue that plays an important role in deciding which motion planning strategy to employ is information about the environment. <p> There are many different approaches to performing collision avoidance. Some of the simplest approaches utilizes a set of rules that define the robot's actions given the obstacle's location and path of motion <ref> [27] </ref>. For example, if a robot detects an obstacle to its left, and there is no obstacle on its right, then the collision avoidance rule can guide the robot to move to the right to avoid the obstacle. <p> The system then switches over to the obstacle avoidance behavior. Once the robot's proximity is determined to be clear of obstacles, the path planner is evoked again to generate a new path from the robot's current location to the goal <ref> [27] </ref>. Obviously, this approach is very inefficient when many mobile objects are present in the environment since the path planning algorithm is restarted every time a moving obstacle is detected and avoided. <p> Time Minimal Path Planning To incorporate the characteristics of dynamic obstacles into the path planner, time enlarged approaches to the planning problem were proposed <ref> [27] </ref>. To keep the space and computation tractable, most of these methods perform local path planning instead of global path planning. Fujimura describes such a time-minimal path planner in [27]. The first step involves construction of an accessibility graph that captures the movements of all the entities in the system. <p> Planning To incorporate the characteristics of dynamic obstacles into the path planner, time enlarged approaches to the planning problem were proposed <ref> [27] </ref>. To keep the space and computation tractable, most of these methods perform local path planning instead of global path planning. Fujimura describes such a time-minimal path planner in [27]. The first step involves construction of an accessibility graph that captures the movements of all the entities in the system. The graph consists of annotated nodes, each node representing an object (the robot, the obstacles or the goal) in the environment.
Reference: [28] <author> R.F. Gans. </author> <title> Following Behavior Using Moving Potential Fields. </title> <type> Computer Science Technical Report TR 603, </type> <institution> University of Rochester, Rochester, </institution> <address> NY, </address> <month> Jan </month> <year> 1996. </year>
Reference-contexts: Some of the key techniques being studied for robot control include classical open loop control and closed loop feedback control, potential field control <ref> [28] </ref>, behavior based subsumption control [56], model-and-planner approach [49] and various combinations and adaptations of these. Mataric uses a behavior based approach to perform cooperative tasks with multiple robots.
Reference: [29] <author> R. Greiner and R. Isukapalli. </author> <title> Learning to Select Useful Landmarks. </title> <journal> IEEE Trans actions on Systems, Man, and Cybernetics Part B : Cybernetics, </journal> <volume> vol. 26, no. 3, </volume> <month> June </month> <year> 1996, </year> <pages> pp 437-449. </pages>
Reference-contexts: The environment must be known in advance but does not have to be engineered in any manner. Some added disadvantages include increased computation 8 time due to more complex object recognition tasks and decreased reliability <ref> [29] </ref>, [75]. This is especially true if one uses natural landmarks in an outdoor environment since the time required to recognize non-geometric and arbitrarily shaped landmarks (typically present in outdoor environments) is much higher than that required for recognizing linear landmarks (peg. doors and windows in the indoor environment) [6]. <p> On the flip side of the coin, one can increase the number of natural landmarks enabling the robot to uniquely determine its location after each observation. However, increasing the number of objects that need to be recognized increases the time required for recognition <ref> [29] </ref>. Thrun proposes a learning technique where the robot determines a balance between the two opposite approaches mentioned above [75]. Model matching, also known as map based positioning, involves matching the robot sensor outputs to a map or model of the environment. <p> Here we present some of the most interesting ideas for natural landmark detection with application to outdoor environments. Landmark Learning Approach Many researchers are of the view that selection of landmarks used by a robot for localization should be determined by the robot itself <ref> [29] </ref>, [75]. Humans perceive the world differently from robots, and structures that make themselves readily available to us for landmark based localization may not be easily detected by robots.
Reference: [30] <author> W.E.L. </author> <title> Grimson. Object Recognition by Computer: The Role of Geometric Con straints, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Object recognition algorithms have gained popularity because of the visual structure and shape associated with artificial features. There exist various algorithms that perform very well on objects with linear shapes and boundaries <ref> [30] </ref>. However, linear 16 constraints cannot be imposed on most environments, hence the need for recognizing objects with non-linear boundaries. Two of the most successful approaches to non-linear object recognition are those of Nayar [60] and Nelson [61]. <p> Some of the other most notable work in object recognition has been done using 3D geometric models <ref> [30] </ref>. The explicit geometric model representation makes it easy 23 to effectively use geometric constraints and methods for easy recognition. The major drawback is that these models are not always easy to obtain. [30] cites various attempts at automatic acquisition of shape and structure geometric models using sensor data. <p> Some of the other most notable work in object recognition has been done using 3D geometric models <ref> [30] </ref>. The explicit geometric model representation makes it easy 23 to effectively use geometric constraints and methods for easy recognition. The major drawback is that these models are not always easy to obtain. [30] cites various attempts at automatic acquisition of shape and structure geometric models using sensor data. However, none of these approaches has yielded satisfactory results and they do not lend themselves to generalizations outside their particular geometric schemata [61].
Reference: [31] <author> R. Gutsche, C. </author> <title> Laloni and F.M. Wahl. Path Planning for Mobile Vehicles Within Dynamical Worlds Using Statistical Data. </title> <booktitle> Proceedings of IROS '94, </booktitle> <address> Munich, Germany, </address> <month> 12-16 Sep, </month> <year> 1994, </year> <pages> pp 454-61. </pages>
Reference-contexts: However, uncertainties can be introduced into this internal representation by the presence of moving obstacles, multiple conflicting sensors etc. <ref> [31] </ref>. To take advantage of the properties of the two approaches, most mobile robots use a combination of reactive and planning navigation. Usually, once the robot has 11 determined its location and its new goal, a path is generated. <p> The learning algorithm takes about 30 trials to produce a limited motion model to aid in prediction of obstacle velocities. 41 Statistical Path Planning Some recent work on path planning in the presence of unknown or partially known obstacles has been performed by Gutsche <ref> [31] </ref>. This approach is similar to that proposed by Fujimura, except that a global planner is also incorporated into the system. This global planner generates an initial path taking into account all the stationary objects in the system and the dynamic objects with partially or completely known motion characteristics.
Reference: [32] <author> G. Hager, M. Mintz. </author> <title> Searching for Information. Proceedings of the AAAI Work shop on Spatial Reasoning and Multisensor Fusion, </title> <address> Los Altos, CA, 5-7 Oct, </address> <year> 1987, </year> <pages> pp 313-22. </pages>
Reference: [33] <author> A.B. Harvey. </author> <title> New Directions at NSF. </title> <booktitle> Proceedings of the SPIE, </booktitle> <volume> vol. 2527, </volume> <month> July </month> <year> 1995, </year> <pages> pp 370-74. </pages>
Reference-contexts: The processing power required for real time image processors, computer vision, biomedical systems, telecommunications and machine learning far exceeds that of present day supercomputers. CMOS technology has almost reached its limits and any new hardware design paradigm (possibly DNA 3 computing [37] or Photonics <ref> [33] </ref>) that may provide the computational resources we need is still years away from being commercially viable [23]. The computational limitation notwithstanding, there exists a plethora of tractable implementations of the problems posed by researchers in neural modeling, sensor modeling, data fusion and control theory.
Reference: [34] <author> R.E. Helmick, J.E. Conte, and T.R. Rice. </author> <title> Absolute Sensor Alignment using GPS. </title> <booktitle> Proceedings of the SPIE, </booktitle> <volume> vol. 2739, </volume> <year> 1996, </year> <month> pp168-79. </month>
Reference-contexts: Bayesian networks [43]). At the same time, physicists and electrical engineers are also involved in inventing new sensor technologies, the most notable of the recent ones being GPS <ref> [34] </ref>. Computational sensors is another very new and active sensor research area. Here, raw sensing and computation are integrated together on the sensor itself, eliminating the need to transmit sensor information to some other computational resource for processing [9]. <p> Its biggest disadvantage is that it provides absolute coordinates (longitude, latitude and elevation) only and cannot be used without a geometric map of the environment. Another drawback is that GPS signals cannot be received indoors <ref> [34] </ref>, making GPS a highly unlikely candidate for generalized robot localization. Current research in landmark detection algorithms is focused on detecting unstructured and arbitrary landmarks such as trees [35] and distant peaks [71], [76]. All of these implementations match the input scenes against stored topographical maps.
Reference: [35] <author> R. Jarvis. </author> <title> An All-Terrain Intelligent Autonomous Vehicle With Sensor-Fusion Based Navigation Capabilities. </title> <journal> Control Engineering Practice, </journal> <volume> vol. 4, no. 4, </volume> <month> Apr </month> <year> 1996, </year> <pages> pp 481-6. </pages>
Reference-contexts: Another drawback is that GPS signals cannot be received indoors [34], making GPS a highly unlikely candidate for generalized robot localization. Current research in landmark detection algorithms is focused on detecting unstructured and arbitrary landmarks such as trees <ref> [35] </ref> and distant peaks [71], [76]. All of these implementations match the input scenes against stored topographical maps. A second approach recently proposed by Thrun views landmarks as features learned by the robot during an initial training phase [75].
Reference: [36] <author> T. Jochem and D. Pomerleau. </author> <title> Life in the Fast Lane: The Evolution of an Adap tive Vehicle Control System. </title> <journal> AI Magazine, </journal> <volume> vol. 17, no. 2, </volume> <year> 1996, </year> <pages> pp 11-50. </pages>
Reference-contexts: Today's high performance world demands precise and scientific solutions from the robotics industry. The newer robots are more mobile, carry larger payloads, perform a varied number of tasks, are capable of learning new operations, and most importantly, do all this independently of a human controller [6], <ref> [36] </ref>, [57], [74], [77]. However, to claim that we have robots that truly exhibit even a majority of the above mentioned traits would be an outright lie. Mobile autonomous robots are very much in the research and development phase. <p> CMU's NAVLAB project has made major national headlines with its first completely autonomous cross country trip. It uses computer vision algorithms to detect road edges, lane markers and other vehicles on the road to perform collision free driving <ref> [36] </ref>. As a result of the above research successes, practical mobile robots now exist. 1.1.2 Object and Landmark Recognition The common thread amongst all of the research highlighted in the previous paragraph is the assumption on the visual structure of the environment. <p> Most of the current research work is targeted towards finding better solutions to the autonomous navigation problem in less and less constrained environments. CMU's NAVLAB project <ref> [36] </ref> and Dickmanns' VaMoRs project [18] lead the autonomous on-road driving research efforts in the world. Both use edge detection algorithms to track roads and driving lanes. <p> Most of these approaches classify multiple features as a single landmark and detect these via a variety of methods such as Hough transform [51], edge detection [18], <ref> [36] </ref>, [69], and neural networks [22], [54]. Geometric Models Simple geometric models such as circles, ellipses or polygons are often used as artificial landmarks because they can be detected easily in real time using edge detection algorithms. <p> The NAVLAB project has extended these road detection capabilities to allow driving on dirt paths also <ref> [36] </ref>. Localization can be performed by using intersections, corners and curved roads as landmarks and keeping track of their sequence of occurrence to periodically update the position estimate generated via odometry methods. CMU's NAVLAB project [36] is the leader in on-road autonomous driving vehicle research in the United States. <p> NAVLAB project has extended these road detection capabilities to allow driving on dirt paths also <ref> [36] </ref>. Localization can be performed by using intersections, corners and curved roads as landmarks and keeping track of their sequence of occurrence to periodically update the position estimate generated via odometry methods. CMU's NAVLAB project [36] is the leader in on-road autonomous driving vehicle research in the United States. In addition to using vision based algorithms for landmark detection, it uses a neural network system to learn how to drive by simply observing the actions of a human teacher. <p> As an example, intersections are detected by the failure to observe a continuous road boundary on one side of the road. ALVINN, another research effort involving the NAVLAB project uses neural networks specially trained for recognizing and driving on different road types <ref> [36] </ref>. Depending on the environment the vehicle is operating in, localization may be performed in different manners.
Reference: [37] <author> L. Kari. </author> <title> DNA Computers, Tomorrow's Reality. </title> <journal> Bulletin of the European Associ ation for Theoretical Computer Science, </journal> <volume> no. 59, </volume> <month> June </month> <year> 1996, </year> <pages> pp 256-66. </pages>
Reference-contexts: The processing power required for real time image processors, computer vision, biomedical systems, telecommunications and machine learning far exceeds that of present day supercomputers. CMOS technology has almost reached its limits and any new hardware design paradigm (possibly DNA 3 computing <ref> [37] </ref> or Photonics [33]) that may provide the computational resources we need is still years away from being commercially viable [23]. The computational limitation notwithstanding, there exists a plethora of tractable implementations of the problems posed by researchers in neural modeling, sensor modeling, data fusion and control theory.
Reference: [38] <author> G. Kamberova, M. Mintz. </author> <title> Robust Multisensor Fusion : A Decision Theoretic Approach. </title> <booktitle> Proceedings of the 1990 DARPA Image Understanding Workshop, </booktitle> <year> 1990, </year> <pages> pp 867-73. </pages>
Reference: [39] <author> M. Kam, X. Zhu and P. Kalata. </author> <title> Sensor Fusion for Mobile Robot Navigation. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 85, no. 1, </volume> <month> Jan </month> <year> 1997, </year> <pages> pp 108-19. </pages>
Reference-contexts: The sensor fusion literature can be broadly categorized into two main approaches : statistical fusion techniques and probabilistic fusion techniques. Statistical fusion techniques are based on least-squares approximation methods. Some of the representative techniques in this class include least squares optimization, Kalman filtering, and regularization techniques [12], <ref> [39] </ref>. Probability theory, with its inherent notions of uncertainty and confidence, has found widespread popularity in the multisensor fusion community. The probabilistic models proposed by various researchers can be classified into four broad methodologies : Bayesian reasoning, evidence theory, robust statistics, and recursive operators [21], [39]. <p> filtering, and regularization techniques [12], <ref> [39] </ref>. Probability theory, with its inherent notions of uncertainty and confidence, has found widespread popularity in the multisensor fusion community. The probabilistic models proposed by various researchers can be classified into four broad methodologies : Bayesian reasoning, evidence theory, robust statistics, and recursive operators [21], [39]. In the remainder of this section, we analyze some of the representative work performed using these two sensor fusion paradigms. 2.4.1 Statistical Techniques Statistical techniques have their root in the least squares estimation. <p> This is fixed by using an extended Kalman filter that linearizes the problem around the predicted state via a second order approximation. Most Kalman filters in use are variants of the extended Kalman filter and have been modified to suit the task under study. In <ref> [39] </ref>, an extended Kalman filter for performing low level sensor data fusion is described. The paper also cites many actual implementations of sensor fusion algorithms that use this extended Kalman filter approach. <p> Empirical studies show that the fused estimate of the position was always better than that given by odometry or the gyroscope alone. For more information on other applications, please refer to <ref> [39] </ref>. Here, we present the approach and framework developed for sensor fusion. <p> Another drawback of Kalman filters is that they seem to fail in the presence of mobile obstacles and dynamic environments. This makes sense since Kalman filters use "weighting" measures for prediction of new states and dynamic obstacles bring in new characteristics that cannot be captured using these weighting measures <ref> [39] </ref>. Kalman filters can also not be used when sensor models are unknown and in the presence of other uncertainties. This is because they do not have the expressive power to make probabilistic predictions. <p> This is because they do not have the expressive power to make probabilistic predictions. Kam et al. advocate the use of probabilistic sensor fusion methodologies in the presence of unknown characteristics about the navigation environment or the sensor model <ref> [39] </ref>. 2.4.2 Probabilistic Techniques Bayesian reasoning and inference procedures have been widely used in other areas of sciences for a long time but have only recently gained popularity in multisensor fusion. <p> Here, we present Kortenkamp's work in sensor fusion as leading example of Bayes Net research. We plan to use this model to develop our own framework for unified sensor fusion in the next chapter. 32 Bayesian Networks Bayesian estimation has long been a popular method for sensor data fusion <ref> [39] </ref>. Bayesian networks were first used by Kortenkamp in [43] as a way of formalizing the Bayesian estimation approach. Both are based on Bayes Rule but Bayesian networks have the added feature of an explicit representation that is very powerful and captures causal relationships between various entities in the system.
Reference: [40] <author> K. Kant and S.W. Zucker. </author> <title> Toward Efficient Trajectory Planning: The Path Velocity Decomposition. </title> <journal> International Journal of Robotics Research, </journal> <volume> vol. 5, no. 3, </volume> <month> Fall </month> <year> 1986, </year> <pages> pp 72-89. </pages>
Reference-contexts: The first approach relies on a space-time formulation. Given the predicted motion of all the obstacles in the environment, a reachability region containing all possible locations for a robot in some future time is created. This is then used to guide the robot to a desired location [26], <ref> [40] </ref>, [79]. The major problem with this approach is the amount of time required to construct the reachability region.
Reference: [41] <author> K. Kluge and C. Thorpe. </author> <title> The YARF System for Vision-Based Road Following. </title> <booktitle> Mathematical and Computer Modeling, </booktitle> <volume> vol. 22, no. </volume> <pages> 4-7, </pages> <month> Aug </month> <year> 1995, </year> <pages> pp 213-33. </pages>
Reference-contexts: The YARF component system of the NAVLAB tracks lane markings and other features and uses position tracking windows to detect intersections and lane changes <ref> [41] </ref>. Feature detection is done using selective application of specialized segmentation routines. As an example, intersections are detected by the failure to observe a continuous road boundary on one side of the road.
Reference: [42] <author> W.S. Ko, L.D. Senevieatne, and S.W.E. Earles. </author> <title> Space Representation and Map Building ATriangulation Model to Path Planning with Obstacle Avoidance. </title> <booktitle> Proceedings of the 1993 IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <address> Yokohama, Japan, </address> <month> 26-30 July, </month> <year> 1993, </year> <pages> pp 2222-7. </pages>
Reference-contexts: Another drawback is that the radio signals transmitted by these active beacons are susceptible to distortion from refraction and reflection due to atmospheric and geographic conditions, giving a different measurement than that associated with the optimal straight line path <ref> [42] </ref>. In artificial landmark recognition, uniquely determinable and specially designed markers or objects are placed in various a-priori known locations in the environment for the sole purpose of enabling robot navigation [75]. <p> However, this has not stopped researchers from proposing many different path planning algorithms made tractable by imposing certain additional constraints on the system. The Basic Path Planner The canonical path planning task in a static a-priori known environment can be performed in three steps <ref> [42] </ref> : 1. Free Space Generation : The free space regions of the environment, taking into account the positions of the robot and the obstacles is generated. 39 2. Model Building : A representation such as a visibility graph which models the free space regions is created. 3. <p> Solution Path Searching : A search method finds the shortest collision free path from the model or representation constructed in the previous step. Ko et al. give an example of a path planner based on triangulation of obstacles for model generation <ref> [42] </ref>. The planner first reduces the robot to a point object by correspondingly enlarging the obstacles (the configuration space method) and generates a graph that models the free space as triangles with obstacles at each of the vertices (the incenter of each triangle forms a node in the model graph).
Reference: [43] <author> D. Kortenkamp et. al. </author> <title> Mobile Robot Exploration and Navigation of Indoor Spaces Using Sonar and Vision. </title> <booktitle> Proceedings of CIRFFSS'94, </booktitle> <address> Houston, TX, </address> <month> 21-24 Mar, </month> <year> 1994, </year> <pages> pp 509-19. </pages>
Reference-contexts: Multi-sensor fusion and integration research is a two pronged effort, one focusing on representations (eg. occupancy grids [21]) and the other focusing on methodologies (eg. Bayesian networks <ref> [43] </ref>). At the same time, physicists and electrical engineers are also involved in inventing new sensor technologies, the most notable of the recent ones being GPS [34]. Computational sensors is another very new and active sensor research area. <p> Some commonly 15 used approaches to sensor fusion include Bayesian estimation [66], Kalman filter and its variants [55], fuzzy rule based systems [78], decision theory [45], neural networks [15] and Bayesian networks <ref> [43] </ref>. Even though we have so many different techniques for performing sensor data fusion, we still lack a framework that can combine sensor data from multiple sensing modalities effectively. Each of the above methods works only within their defined constraints and underlying assumptions. <p> These range from pure odometry or inertial navigation systems to a combination of sonar, vision and other sensing modalities for place recognition <ref> [43] </ref>. Indoor environments can usually be structured to fit the requirements of one or more of the prevalent methodologies without much hardship and, as such, no longer pose a very interesting problem to researchers. <p> Kortenkamp first proposed the use of a Bayesian Network (as opposed to statistical Bayesian methods) for performing multisensor fusion for topological map building in <ref> [43] </ref>. For more details on the theoretical foundations and implementations of Bayesian networks, please refer to section 3.3.1. Brooks [11] had earlier argued very persuasively for using topological maps as a means of dealing with uncertainty in mobile robot navigation. <p> We plan to use this model to develop our own framework for unified sensor fusion in the next chapter. 32 Bayesian Networks Bayesian estimation has long been a popular method for sensor data fusion [39]. Bayesian networks were first used by Kortenkamp in <ref> [43] </ref> as a way of formalizing the Bayesian estimation approach. Both are based on Bayes Rule but Bayesian networks have the added feature of an explicit representation that is very powerful and captures causal relationships between various entities in the system. <p> When a sufficiently large amount of sample data is gathered, this conditional probability table can be calculated from the contingency tables. The most important (and one of the first) work using Bayesian networks for sensor fusion was first published by Kortenkamp in <ref> [43] </ref>. Kortenkamp uses Bayesian networks for combining sensor data from sonar and vision sensors to build a topological map of the world. Topological maps were chosen because of the ease of expressing uncertainties in their framework and because of their compactness. <p> Tables 2.1 and 2.2 give the individual conditional probabilities that were observed by the robot in <ref> [43] </ref>. Neither the vision, nor the sonar sensor can independently recognize every location in the world unambiguously given their sensor measurements. However, using the Bayesian network of figure 2.3 results in the combined conditional probability table shown in table 2.3. <p> To solve autonomous navigation problems in such situations would require the use of many more sensors besides vision and sonar. Secondly, the experimental space of <ref> [43] </ref> consisted on only seven distinct gateways. There is a question of how well the system would scale to larger spaces. It is certainly reasonable to expect system degradation as the space size increases. It is not quite clear how a Bayesian network could adapt to increasing space sizes.
Reference: [44] <author> S. Kristensen. </author> <title> Sensor Planning with Bayesian Decision Theory. Reasoning with Uncertainty in Robotics. </title> <booktitle> International Workshop, RUR'95 Proceedings, </booktitle> <address> Amster-dam, Netherlands, </address> <month> 4-6 Dec, </month> <year> 1995, </year> <pages> pp 353-67. 71 </pages>
Reference-contexts: The presence of mobile obstacles, unstructured terrain and multiple (possibly) dependent sensors introduce uncertainties and complexities that pose a much harder problem than navigation in a static structured domain [6], <ref> [44] </ref>. Most of the current research work is targeted towards finding better solutions to the autonomous navigation problem in less and less constrained environments. CMU's NAVLAB project [36] and Dickmanns' VaMoRs project [18] lead the autonomous on-road driving research efforts in the world.
Reference: [45] <author> S. Kristensen and H.I. Christensen. </author> <title> Decision-Theoretic Multisensor Planning and Integration for Mobile Robot Navigation. </title> <booktitle> Proceedings of MFI-96, </booktitle> <address> Washington, D.C., </address> <year> 1996, </year> <pages> pp 517-24. </pages>
Reference-contexts: The evolution of new sensors, some using completely different paradigms, creates a constant flux in multi-sensor fusion and integration methodologies. Some commonly 15 used approaches to sensor fusion include Bayesian estimation [66], Kalman filter and its variants [55], fuzzy rule based systems [78], decision theory <ref> [45] </ref>, neural networks [15] and Bayesian networks [43]. Even though we have so many different techniques for performing sensor data fusion, we still lack a framework that can combine sensor data from multiple sensing modalities effectively. Each of the above methods works only within their defined constraints and underlying assumptions.
Reference: [46] <author> J.C. Latombe. </author> <title> Robot Motion Planning, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1991. </year>
Reference-contexts: Traditional computational algorithms used for problem solving in computer science cannot apply in this domain. This led Latombe to propose that robot algorithms be viewed as a blend of basic control issues (controllability and observability) and computational issues (calculability and complexity) <ref> [46] </ref>. From this perspective, we can classify research in artificially intelligent robotics into three main branches. The first branch attempts to gain cognizance of the brain's intelligent processes (learning, thinking, reasoning etc.).
Reference: [47] <author> J.C. Latombe. </author> <title> Robot Algorithms. </title> <booktitle> Workshop on the Algorithmic Foundations of Robotics, </booktitle> <address> San Francisco, CA, </address> <month> 17-19 Feb, </month> <year> 1994, </year> <pages> pp 1-19. </pages>
Reference-contexts: The biggest obstacle preventing the robotics community from creating artificially intelligent mechanical beings is the enormity of the human thinking, learning and acting processes that need to be modeled. One can say that robotics, like computer science, is fundamentally about algorithms <ref> [47] </ref>. However, robots are much more versatile mechanically than computers. They are equipped with sensors (providing observability) and actuators (providing controllability), forcing interaction with the system, and not just passive data compliance. Traditional computational algorithms used for problem solving in computer science cannot apply in this domain.
Reference: [48] <author> J. Leonard and H.F. Durrant-Whyte. </author> <title> Mobile Robot Localization by Tracking Geometric Beacons. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 7, no. 3, </volume> <year> 1991, </year> <pages> pp 376-82. </pages>
Reference-contexts: However, a modular approach can help reduce the complexity of the navigation problem by tackling smaller portions of it somewhat independently of each other and then combining the solutions. Leonard and Durrant-Whyte <ref> [48] </ref> summarized the general problem of autonomous mobile robot navigation using three questions : "Where am I?", "Where am I going?", and "How do I get there?". The first problem is commonly known as robot localization, the second as goal recognition, and the third as path planning.
Reference: [49] <author> L.F. Lee, A. Kean. </author> <title> An architecture for autonomous flying vehicles: a preliminary report. </title> <booktitle> Proceedings of PRICAI'96, Cairns, </booktitle> <address> Australia, 26-30 Aug, </address> <year> 1996, </year> <pages> pp 613-24. </pages>
Reference-contexts: Some of the key techniques being studied for robot control include classical open loop control and closed loop feedback control, potential field control [28], behavior based subsumption control [56], model-and-planner approach <ref> [49] </ref> and various combinations and adaptations of these. Mataric uses a behavior based approach to perform cooperative tasks with multiple robots.
Reference: [50] <author> W. Li. </author> <title> Perception-Action Behavior Control of a Mobile Robot in Uncertain En vironments Using Fuzzy Logic. </title> <booktitle> Proceedings of IROS-94, </booktitle> <address> Munich, Germany, </address> <month> 12-16 Sep, </month> <year> 1994, </year> <pages> pp 439-46. </pages>
Reference-contexts: A major reason for this is that path planning in the presence of dynamic obstacles is a very hard problem. So in keeping with chronological history, we discuss these approaches first. Most reactive approaches to motion planning are souped up collision detection and avoidance methodologies <ref> [50] </ref>. The most common collision detection and avoidance strategies work by creating a personal space around the robot and detecting obstacles that cross into this space. Once an obstacle is detected, and obstacle avoidance algorithm is executed that steers the robot away from a possible collision. <p> Such rule based collision avoidance approaches lend themselves quite easily to fuzzy logic reasoning systems for collision avoidance <ref> [50] </ref>. While such systems are easy to use, they are built using purely ad-hoc techniques that can be very environment dependent. We have implemented a similar obstacle avoidance system on the mobile wheelchair platform. <p> We have implemented a similar obstacle avoidance system on the mobile wheelchair platform. This work is described in more detail in section 4.1. 37 Fuzzy Logic Systems Fuzzy logic control is based on the theory of fuzzy sets <ref> [50] </ref>. A fuzzy set A in a universe of discourse X is defined by its membership function A (x). For each x 2 X, there exists a value A (x) 2 [0; 1] of the membership function representing the degree of the membership of x in X.
Reference: [51] <author> C.C. Lin and R.L. Tummala. </author> <title> Mobile Robot Navigation using Artificial Land marks. </title> <journal> Journal of Robotic Systems, vol.14, </journal> <volume> no. 2, </volume> <month> Feb </month> <year> 1997, </year> <pages> pp 93-106. </pages>
Reference-contexts: The robot is also incapable of laying odd and cut tiles and creating intricate tile patterns. Some other partially successful attempts at autonomous indoor mobile robot navigation are described in greater detail in [14], <ref> [51] </ref>, [81]. The presence of mobile obstacles, unstructured terrain and multiple (possibly) dependent sensors introduce uncertainties and complexities that pose a much harder problem than navigation in a static structured domain [6], [44]. <p> In the first approach, simple regularly shaped polygonal or circular patterns are used as landmarks. A perspective projection of these landmarks in a robot mounted camera's image space can then be used to compute the location estimate <ref> [51] </ref>. Object recognition algorithms have gained popularity because of the visual structure and shape associated with artificial features. There exist various algorithms that perform very well on objects with linear shapes and boundaries [30]. <p> Most of these approaches classify multiple features as a single landmark and detect these via a variety of methods such as Hough transform <ref> [51] </ref>, edge detection [18], [36], [69], and neural networks [22], [54]. Geometric Models Simple geometric models such as circles, ellipses or polygons are often used as artificial landmarks because they can be detected easily in real time using edge detection algorithms. <p> Circles are especially good as they project ellipses into the camera space when viewed at an angle by the robot. These projections can then be used to compute the location of the robot. One of the best algorithms that employs this technique was recently proposed by Lin and Tummala <ref> [51] </ref>. Figure 2.1 shows the artificial 3-D landmark pattern used to perform localization. It consists of two disks (a smaller one some distance in front of 18 the larger one) at an a-priori known location in the environment. <p> The second source of error is occlusion of the landmark. With at least 50% of the landmark visible, the error in the aspect angle is under 7% but degrades quickly when more of the landmark is occluded <ref> [51] </ref>. 19 Road Tracking Road edge detection and tracking is one of the most common navigational methods for on-road autonomous vehicles [18], [69]. The NAVLAB project has extended these road detection capabilities to allow driving on dirt paths also [36].
Reference: [52] <author> R.C. Luo, M.G. Kay. </author> <title> Data Fusion and Sensor Integration : State-of-the-Art 1990s. </title> <booktitle> Data Fusion in Robotics and Machine Intelligence, </booktitle> <publisher> Academic Press Limited, </publisher> <address> San Diego, CA, </address> <year> 1992, </year> <pages> pp 7-136. </pages>
Reference-contexts: Multisensor fusion refers to any stage in the integration process where there is actual combination (fusion) of different sources of sensory information into one representational format <ref> [52] </ref>. Although the above distinction is not standard in the literature, it allows us to concentrate on a particular aspect of the general problem of combining multiple sensors (ie. sensor fusion). <p> Multiple sensors allow us to acquire information that is more accurate, especially if certain features are occluded for a particular individual sensor, in less time and cost. 29 These advantages correspond, respectively, to the notions of redundancy, complementarity, timeliness, and cost of the information provided the system <ref> [52] </ref>. The sensor fusion literature can be broadly categorized into two main approaches : statistical fusion techniques and probabilistic fusion techniques. Statistical fusion techniques are based on least-squares approximation methods. Some of the representative techniques in this class include least squares optimization, Kalman filtering, and regularization techniques [12], [39].
Reference: [53] <author> R.C. Luo, H. Potlapalli and D.W. Hislop. </author> <title> Outdoor Landmark Recognition Using Fractal Based Vision and Neural Networks. </title>
Reference-contexts: Neural Networks Another popular methodology for detecting artificial landmarks uses multiple neural networks that are trained to recognize these landmarks quickly in a sequence of input images [22], <ref> [53] </ref>, [54]. Most of the commonly used neural network approaches for artificial landmark classification in the outdoor environment employ neural networks 21 that are trained to detect and classify road and traffic signs.
Reference: [54] <author> R.C. Luo and H. Potlapalli. </author> <title> Landmark Recognition Using Projection Learning for Mobile Robot Navigation. </title> <booktitle> Proceedings of the 1994 IEEE International Conference on Neural Networks, </booktitle> <address> Orlando, FL, 27 June-2 July, </address> <year> 1994, </year> <pages> pp 2703-8. </pages>
Reference-contexts: Most of these approaches classify multiple features as a single landmark and detect these via a variety of methods such as Hough transform [51], edge detection [18], [36], [69], and neural networks [22], <ref> [54] </ref>. Geometric Models Simple geometric models such as circles, ellipses or polygons are often used as artificial landmarks because they can be detected easily in real time using edge detection algorithms. <p> Neural Networks Another popular methodology for detecting artificial landmarks uses multiple neural networks that are trained to recognize these landmarks quickly in a sequence of input images [22], [53], <ref> [54] </ref>. Most of the commonly used neural network approaches for artificial landmark classification in the outdoor environment employ neural networks 21 that are trained to detect and classify road and traffic signs. Luo et al. present a fractal based approach to detecting and classifying street signs and mile markers in [54]. <p> <ref> [54] </ref>. Most of the commonly used neural network approaches for artificial landmark classification in the outdoor environment employ neural networks 21 that are trained to detect and classify road and traffic signs. Luo et al. present a fractal based approach to detecting and classifying street signs and mile markers in [54]. The task of locating landmarks in a cluttered scene is very complex because the perceived size of the landmarks depends on the distance between the camera and landmark. The appearance of a landmark may also change due to variations in lighting, viewing direction, and occlusions. <p> In the first stage, color and shape information about traffic signs is used to segment and extract the regions of interest from input images. In the second stage, a neural network is used to classify these traffic signs. Another approach proposed by Luo and Potlapalli in <ref> [54] </ref> uses self-organizing neural networks to detect and classify mile marker signs and cautionary signs to safely navigate the robot through its path.
Reference: [55] <author> S. Maeyama, N. Ishikawa and S. Yuta. </author> <title> Rule Based Filtering and Fusion of Odom etry and Gyroscope for a Fail Safe Dead Reckoning System of a Mobile Robot. </title> <booktitle> Proceedings of MFI-96, </booktitle> <address> Washington, D.C., </address> <month> 8-11 Dec, </month> <year> 1996, </year> <month> pp541-8. </month>
Reference-contexts: The evolution of new sensors, some using completely different paradigms, creates a constant flux in multi-sensor fusion and integration methodologies. Some commonly 15 used approaches to sensor fusion include Bayesian estimation [66], Kalman filter and its variants <ref> [55] </ref>, fuzzy rule based systems [78], decision theory [45], neural networks [15] and Bayesian networks [43]. Even though we have so many different techniques for performing sensor data fusion, we still lack a framework that can combine sensor data from multiple sensing modalities effectively. <p> These can be structured (eg. buildings, lamp-posts, doors, windows, fire hydrants etc.) or unstructured (eg. trees, mountain peaks, etc.) in terms of their shape. Most natural landmark detection algorithms use structured landmarks for localization purposes as they can be detected relatively easily using a variety of image processing algorithms <ref> [55] </ref>, [69]. Another approach to landmark detection employs neural networks to classify input (visual, range data, or some other sensory data) patterns and perform localization based on the classification results [68], [75]. Thompson's group at the University of Utah is at the forefront of unstructured landmark detection for robot localization. <p> In [39], an extended Kalman filter for performing low level sensor data fusion is described. The paper also cites many actual implementations of sensor fusion algorithms that use this extended Kalman filter approach. Maeyama et al. <ref> [55] </ref> use this Kalman filter approach for performing sensor fusion for navigation in an outdoor world composed of sidewalks, trees, hedges, and walls. The recursive Kalman filter update rule is used to fuse low level position information obtained from odometry and gyroscopic data.
Reference: [56] <author> M.J. Mataric. </author> <title> Behavior-Based Control: Examples from Navigation, Learning, and Group Behavior. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> vol. 9, </volume> <pages> nos. 2-3, </pages> <year> 1997. </year> <month> 72 </month>
Reference-contexts: Some of the key techniques being studied for robot control include classical open loop control and closed loop feedback control, potential field control [28], behavior based subsumption control <ref> [56] </ref>, model-and-planner approach [49] and various combinations and adaptations of these. Mataric uses a behavior based approach to perform cooperative tasks with multiple robots. <p> This approach differs from purely reactive control in that there isn't just a simple functional mapping between stimuli and appropriate responses, but also a distributed aspect wherein multiple behaviors may be active at any given time and a central policy decision chooses one of these to execute <ref> [56] </ref>. Multi-sensor fusion and integration research is a two pronged effort, one focusing on representations (eg. occupancy grids [21]) and the other focusing on methodologies (eg. Bayesian networks [43]).
Reference: [57] <editor> A.L. Meyrowitz, D.R. Blidberg, </editor> <title> and R.C. Michelson. Autonomous Vehicles. </title> <journal> Pro ceedings of the IEEE, </journal> <volume> vol. 84, no. 8, </volume> <month> Aug </month> <year> 1996, </year> <pages> pp 1147-1164. </pages>
Reference-contexts: Today's high performance world demands precise and scientific solutions from the robotics industry. The newer robots are more mobile, carry larger payloads, perform a varied number of tasks, are capable of learning new operations, and most importantly, do all this independently of a human controller [6], [36], <ref> [57] </ref>, [74], [77]. However, to claim that we have robots that truly exhibit even a majority of the above mentioned traits would be an outright lie. Mobile autonomous robots are very much in the research and development phase. <p> NAVLAB's road tracker is also capable of detecting dirt roads and furrows for limited rough terrain navigation. Recently, a host of papers on unmanned aerial and underwater vehicles have also come to the forefront of autonomous vehicle research, foremost among them being the Ocean Explorer and Cruise Missile projects <ref> [57] </ref>. Ocean Explorer uses a topographical map and model-matching techniques to perform underwater navigation. Cruise missiles originally used a similar model-matching approach but now use a military-grade GPS system 14 for navigation. <p> Some of the current areas of focused research include robotic control, sensing and path planning (both in the presence and absence of a-priori knowledge of the environment) <ref> [57] </ref>. Some of the key techniques being studied for robot control include classical open loop control and closed loop feedback control, potential field control [28], behavior based subsumption control [56], model-and-planner approach [49] and various combinations and adaptations of these.
Reference: [58] <author> H. Murase and S.K. Nayar. </author> <title> Visual Learning and Recognition of 3D Objects from Appearance. </title> <journal> International Journal of Computer Vision, </journal> <volume> vol. 14, no. 1, </volume> <month> Jan </month> <year> 1995, </year> <pages> pp 5-24. </pages>
Reference-contexts: We discuss these in more detail. Nayar's Object Recognition Algorithm Nayar et al. describe an object recognition algorithm capable of recognizing 100 complex three-dimensional objects in real time (under one second for each input image) [60]. They use the appearance based representation described in <ref> [58] </ref> to encode brightness variations in the image of an object caused by its three-dimensional space, surface reflectance properties, and illumination conditions.
Reference: [59] <author> Y.S. Nam, B.H. Lee, and M.S. Kim. </author> <title> View-Time Based Moving Obstacle Avoid ance Using Stochastic Prediction of Obstacle Motion. </title> <booktitle> Proceedings. IEEE International Conference on Robotics and Automation, </booktitle> <address> Minneapolis, MN, </address> <month> 22-28 Apr, </month> <year> 1996, </year> <pages> pp 1081-6. </pages>
Reference-contexts: Another area of active interest is path planning in the presence of dynamic obstacles. Various approaches have been suggested to tackle this problem, ranging from fully reactive and reflexive avoidance [14] to predicting obstacle motion <ref> [59] </ref>. The first approach uses continuous sensor polling (usually proximity sensors only) to detect any obstacles that enter the robot's personal space. The personal space is an enclosed area surrounding the robot that should ideally be kept free of any obstacles at all times. <p> A path planner then generates a route that takes the mobile obstacles into account. The sampling rate of the sensors depends on the uncertainty associated with the prediction of the obstacle trajectories <ref> [59] </ref>. Hybrid approaches to this problem have also been studied and found favor in many implementations [8].
Reference: [60] <author> S.K. Nayar, S.A. Nene and H. Murase. </author> <title> Real-Time 100 Object Recognition Sys tem. </title> <booktitle> Proceedings of the 1996 Image Understanding Workshop, </booktitle> <address> Palm Springs, CA, </address> <month> 12-15 Feb, </month> <year> 1996, </year> <pages> pp 1223-7. </pages>
Reference-contexts: There exist various algorithms that perform very well on objects with linear shapes and boundaries [30]. However, linear 16 constraints cannot be imposed on most environments, hence the need for recognizing objects with non-linear boundaries. Two of the most successful approaches to non-linear object recognition are those of Nayar <ref> [60] </ref> and Nelson [61]. Nayar uses appearance representations to encode a combined effect of the object's shape, reflectance properties, pose in the scene, and the illumination conditions into a compact model [60]. <p> Two of the most successful approaches to non-linear object recognition are those of Nayar <ref> [60] </ref> and Nelson [61]. Nayar uses appearance representations to encode a combined effect of the object's shape, reflectance properties, pose in the scene, and the illumination conditions into a compact model [60]. Nelson uses a feature matching technique to identify objects in a cluttered scene by computing the intermediate-level features (specifically 2-D boundary fragments)of objects in the input image and searching for a match in a pre-constructed database [61]. <p> Appearance based approaches generate and store some feature representative models of the objects that need to be recognized by the system. These model representations, commonly called databases, can be be very large [61] or very compact <ref> [60] </ref>. Recognition is usually performed by detecting salient features in the input image and matching these against the representations stored in the database. The salient features in the input image are dependent on the object recognition algorithm. <p> In Nelson's object recognition algorithm, the salient features are a set of 2-D boundary fragments associated with the objects [61]. In Nayar's system, the salient feature is the projection of the input image into the eigenspace of all the training images in the database <ref> [60] </ref>. While neither of these two object recognition systems has been used for landmark detection, there is an obvious application of these algorithms to the landmark detection problem motivating us to detail these further. <p> Arrows are recognized using thinning and template matching operations. The drawbacks of this method include segmentation problems due to clutter and occlusion and lack of generalizability to more complex shaped landmarks. The best candidates for a generalized object recognition algorithm seem to be the appearance based methods developed in <ref> [60] </ref> and [61]. We discuss these in more detail. Nayar's Object Recognition Algorithm Nayar et al. describe an object recognition algorithm capable of recognizing 100 complex three-dimensional objects in real time (under one second for each input image) [60]. <p> object recognition algorithm seem to be the appearance based methods developed in <ref> [60] </ref> and [61]. We discuss these in more detail. Nayar's Object Recognition Algorithm Nayar et al. describe an object recognition algorithm capable of recognizing 100 complex three-dimensional objects in real time (under one second for each input image) [60]. They use the appearance based representation described in [58] to encode brightness variations in the image of an object caused by its three-dimensional space, surface reflectance properties, and illumination conditions.
Reference: [61] <author> R.C. Nelson. </author> <title> Memory-Based Recognition for 3-D Objects. </title> <booktitle> Proceedings ARPA Image Understanding Workshop, </booktitle> <address> Palm Springs, CA, </address> <month> Feb </month> <year> 1996, </year> <pages> pp 1305-1310. </pages>
Reference-contexts: Outdoor environments tend to be unstructured and defeat computer vision solutions developed for the former [18]. How can one approach the problem of navigation in unstructured outdoor terrain? Most of the commonly used approaches to landmark and object recognition fail in the absence of linear visual features. Nelson's <ref> [61] </ref> memory-based object recognition algorithm recognizes objects with non-linear visual features but requires a training phase where precise multiple (around a sphere) views of each object must be stored in a database. <p> However, linear 16 constraints cannot be imposed on most environments, hence the need for recognizing objects with non-linear boundaries. Two of the most successful approaches to non-linear object recognition are those of Nayar [60] and Nelson <ref> [61] </ref>. Nayar uses appearance representations to encode a combined effect of the object's shape, reflectance properties, pose in the scene, and the illumination conditions into a compact model [60]. <p> Nelson uses a feature matching technique to identify objects in a cluttered scene by computing the intermediate-level features (specifically 2-D boundary fragments)of objects in the input image and searching for a match in a pre-constructed database <ref> [61] </ref>. Another area of active interest is path planning in the presence of dynamic obstacles. Various approaches have been suggested to tackle this problem, ranging from fully reactive and reflexive avoidance [14] to predicting obstacle motion [59]. <p> Appearance based approaches generate and store some feature representative models of the objects that need to be recognized by the system. These model representations, commonly called databases, can be be very large <ref> [61] </ref> or very compact [60]. Recognition is usually performed by detecting salient features in the input image and matching these against the representations stored in the database. The salient features in the input image are dependent on the object recognition algorithm. <p> The salient features in the input image are dependent on the object recognition algorithm. In Nelson's object recognition algorithm, the salient features are a set of 2-D boundary fragments associated with the objects <ref> [61] </ref>. In Nayar's system, the salient feature is the projection of the input image into the eigenspace of all the training images in the database [60]. <p> However, none of these approaches has yielded satisfactory results and they do not lend themselves to generalizations outside their particular geometric schemata <ref> [61] </ref>. These approaches also tend to be dependent on whole image segmentation and degrade quickly in the presence of occlusions and clutter. <p> The drawbacks of this method include segmentation problems due to clutter and occlusion and lack of generalizability to more complex shaped landmarks. The best candidates for a generalized object recognition algorithm seem to be the appearance based methods developed in [60] and <ref> [61] </ref>. We discuss these in more detail. Nayar's Object Recognition Algorithm Nayar et al. describe an object recognition algorithm capable of recognizing 100 complex three-dimensional objects in real time (under one second for each input image) [60]. <p> As is obvious from this list of assumptions, such a system will have degraded performance in outdoor environments where occlusions are present, illumination conditions vary easily, and clutter and background changes cause problems for the segmentation algorithms. Nelson's Object Recognition Algorithm Nelson <ref> [61] </ref> describes an appearance-based object recognition system that is not restricted by the assumptions constraining Nayar's work. The object recognition system uses intermediate-level features representing 2-D boundary fragments as normalized keys and descriptors for constructing and indexing the database.
Reference: [62] <author> R.C. Nelson. </author> <title> Experiments on (Intelligent) Brute Force Methods for Appearance Based Object Recognition. </title> <booktitle> 1997 DARPA Image Understanding Workshop, </booktitle> <address> New Orleans, LA, </address> <month> May </month> <year> 1997, </year> <note> to appear. </note>
Reference-contexts: A Hough-transform like evidence combination scheme is used to update the confidence in various object hypotheses generated by this search and select the best one. Extensive testing of the system is reported in <ref> [62] </ref>. The base case used for testing composed of a database consisting of six different objects. A recognition rate of 99% was observed with test images that were free of clutter and occlusions.
Reference: [63] <author> J.T.Nielson. </author> <title> CALCM The Untold Story of the Weapon Used to Start the Gulf War. </title> <journal> IEEE Aerospace and Electronic Systems Magazine, </journal> <volume> vol. 9, no. 7, </volume> <month> Jul </month> <year> 1994, </year> <pages> pp 18-22. </pages>
Reference-contexts: Geometric maps represent the world in a global coordinate system, while topological maps represent the world as a network of nodes and arcs [74]. Cruise missiles used a terrain correlation technique for model matching based navigation before the advent of military GPS <ref> [63] </ref>. Disadvantages of map based positioning systems arise from the specific requirements for satisfactory navigation. There need to be enough stationary, easily distinguishable features that can be used for matching. The sensor map and the terrain maps must be accurate enough to be useful. <p> Model Matching Approach Another approach for navigation and localization in unstructured environments performs model matching between sensory inputs and an a-priori known topographical map [76]. This was the first navigational and localization system used by the Cruise Missile before the advent of military GPS <ref> [63] </ref>. 28 Researchers at the University of Utah are at the forefront of using natural terrain features as landmarks for robot localization purposes. Thompson and Pick describe a system that detects and tracks peaks and valleys in input images to and matches these against a topographical map [76].
Reference: [64] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. 2nd ed., </title> <publisher> Morgan Kauf mann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1988. </year>
Reference-contexts: These conditional probabilities do not have to be known a-priori and can be learned using statistical sampling techniques or supervised learning approaches. One of the most popular approaches to sampling techniques uses Gibbs sampling for generation of training data <ref> [64] </ref>. In this approach, a small set of complete training data (sensor observations and actual environment state) serves as a base for a sample generator to generate a large set of training data from incomplete samples. The incomplete samples can have missing sensor observations or missing environment observations. <p> We use a modified version of the occupancy grids, called dynamic occupancy grids as our representation of the environment. Section 3.2 presents the details of this representation. Pearl describes a Bayes rule based network approach for modeling causal relationships in <ref> [64] </ref>. This approach, called Bayesian Networks, explicitly encodes uncertainty information associated with related entities in a system and describes the interactions between these entities in a very clear and formal manner. <p> Lastly, since our dynamic occupancy grids can represent different entities (and this requires different PDFs) at different times, the method used to generate these PDFs must also be adaptive in the same sense. Pearl describes Bayesian networks in <ref> [64] </ref> and motivates their use for tasks that require causal relationships to be modeled. The PDFs required by the dynamic occupancy grids are definitely causally dependent on the sensory inputs. <p> Given this definition, a Bayesian network can be thought of as a knowledge base. It explicitly represents our beliefs about the system and the relationships between the various entities of the system. Pearl presents a detailed description of Bayesian networks and associated theory in <ref> [64] </ref> which we summarize here. Bayesian networks operate by propagating beliefs through the network once some evidence about the existence of an entity is asserted. <p> For our analysis here, we deal with discretized probabilities. However, Pearl shows how the very same technique can also be extended to propagate continuous PDFs rather than discretized probability arrays <ref> [64] </ref>. 50 Consider a general Bayesian network shown in figure 3.4. This Bayesian network shows a typical node X with one parent, U and m children, Y 1 , Y 2 , . . . , Y m .
Reference: [65] <author> N.S.V. Rao. </author> <title> Robot Navigation in Unknown Generalized Polygonal Terrains Using Vision Sensors. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> vol. 25, no. 6, </volume> <pages> pp 947-62. </pages>
Reference: [66] <author> N.S.V. Rao. </author> <title> Distributed Decision Fusion Using Empirical Estimation. </title> <booktitle> Proceed ings of MFI-96, </booktitle> <address> Washington, D.C., </address> <month> 8-11 Dec, </month> <year> 1996, </year> <pages> pp 697-704. </pages>
Reference-contexts: With limited guidance from the user, it can perform simple operations such as segmentation and labeling [10]. The evolution of new sensors, some using completely different paradigms, creates a constant flux in multi-sensor fusion and integration methodologies. Some commonly 15 used approaches to sensor fusion include Bayesian estimation <ref> [66] </ref>, Kalman filter and its variants [55], fuzzy rule based systems [78], decision theory [45], neural networks [15] and Bayesian networks [43]. Even though we have so many different techniques for performing sensor data fusion, we still lack a framework that can combine sensor data from multiple sensing modalities effectively.
Reference: [67] <author> J. Reif and M. Sharir. </author> <title> Motion Planning in the Presence of Moving Obstacles. </title> <booktitle> Proceedings of the 26th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <address> Portland, OR, </address> <year> 1985, </year> <pages> pp 144-54. </pages>
Reference-contexts: Path planning refers to generating a route from one location in the environment to another, and path planning algorithms attempt to generate a collision free motion trajectory for the robot over a time period. Path-planning is known to be an NP-hard problem in a dynamic environment <ref> [67] </ref>. Another result shows that the above problem is PSPACE-hard even in the presence of constraints such as velocity bounds [13]. Despite the hardness of the path planning problem, various methods have been proposed to attack the problem in a dynamic environment. The first approach relies on a space-time formulation. <p> We have already noted that the problem of path planning in the presence of dynamic obstacles is NP-hard, even when the obstacles are convex polygons moving with bounded constant linear velocities without rotation [13], <ref> [67] </ref>. However, this has not stopped researchers from proposing many different path planning algorithms made tractable by imposing certain additional constraints on the system. The Basic Path Planner The canonical path planning task in a static a-priori known environment can be performed in three steps [42] : 1.
Reference: [68] <author> M. Sekiguchi, H. Okada and N. Watanabe. </author> <title> Neural Network Based Landmark Detection for Mobile Robot. </title> <booktitle> Proceedings of the SPIE, </booktitle> <volume> vol. 2760, </volume> <month> Apr </month> <year> 1996, </year> <pages> pp 216-23. </pages>
Reference-contexts: Another approach to landmark detection employs neural networks to classify input (visual, range data, or some other sensory data) patterns and perform localization based on the classification results <ref> [68] </ref>, [75]. Thompson's group at the University of Utah is at the forefront of unstructured landmark detection for robot localization. They detect mountain peaks in the input images and match their locations against a topographical map to estimate the robot position.
Reference: [69] <author> F. Shibata et al. </author> <title> Mobile Robot Navigation by User-Friendly Goal Specification. </title> <booktitle> Proceedings. 5th IEEE International Workshop on Robot and Human Communication, </booktitle> <address> Tsukuba, Japan, 11-14 Nov, </address> <year> 1996, </year> <pages> pp 439-44. </pages>
Reference-contexts: An extension to the above approach attempts to use user-friendly goal specifications to identify the goals to a robot <ref> [69] </ref>. The position of the goal is not given by its coordinates but by a verbal description of the route to it. Thus one can specify a goal by a series of statements such as " turn right and go to the second intersection, and...". <p> The major advantage of this approach is that accurate a-priori knowledge about the locations of the goals is not required. It also breaks the path planning problem into easily identifiable components, (each statement representing a subgoal), reducing the complexity of motion planning and making it tractable <ref> [69] </ref>. Another extension to the above attempts to deal with unknown environments by identifying goals (landmarks or objects) without specifying their locations or a path to them. <p> Most natural landmark detection algorithms use structured landmarks for localization purposes as they can be detected relatively easily using a variety of image processing algorithms [55], <ref> [69] </ref>. Another approach to landmark detection employs neural networks to classify input (visual, range data, or some other sensory data) patterns and perform localization based on the classification results [68], [75]. Thompson's group at the University of Utah is at the forefront of unstructured landmark detection for robot localization. <p> Most of these approaches classify multiple features as a single landmark and detect these via a variety of methods such as Hough transform [51], edge detection [18], [36], <ref> [69] </ref>, and neural networks [22], [54]. Geometric Models Simple geometric models such as circles, ellipses or polygons are often used as artificial landmarks because they can be detected easily in real time using edge detection algorithms. <p> With at least 50% of the landmark visible, the error in the aspect angle is under 7% but degrades quickly when more of the landmark is occluded [51]. 19 Road Tracking Road edge detection and tracking is one of the most common navigational methods for on-road autonomous vehicles [18], <ref> [69] </ref>. The NAVLAB project has extended these road detection capabilities to allow driving on dirt paths also [36]. Localization can be performed by using intersections, corners and curved roads as landmarks and keeping track of their sequence of occurrence to periodically update the position estimate generated via odometry methods.
Reference: [70] <author> R. Smith and P. Cheeseman. </author> <title> On the Estimation and Representation of Spatial Uncertainty. </title> <journal> International Journal of Robotics Research, </journal> <volume> vol. 5, no. 4, </volume> <month> Winter </month> <year> 1987, </year> <pages> pp 56-68. </pages>
Reference-contexts: Most of these techniques were ad-hoc and did not characterize the uncertainties in the system. The first work on incorporating uncertainty in an explicit manner in sensor fusion was performed by Smith and Cheeseman <ref> [70] </ref>. They proposed the use of Bayesian estimation theory and derived a combination function that was an equivalent form of Kalman Filter. This caused a rapid paradigm shift towards probabilistic estimation theories closely related to Bayesian estimation, maximum likelihood estimation and least squares methods [17].
Reference: [71] <author> S.H. Suh et al. </author> <title> CAD-MAP and Estimation of ALV Positions in Mountainous Areas. </title> <journal> Robotica, </journal> <volume> vol. 12, pt. 4, </volume> <pages> pp 287-97. </pages>
Reference-contexts: Another drawback is that GPS signals cannot be received indoors [34], making GPS a highly unlikely candidate for generalized robot localization. Current research in landmark detection algorithms is focused on detecting unstructured and arbitrary landmarks such as trees [35] and distant peaks <ref> [71] </ref>, [76]. All of these implementations match the input scenes against stored topographical maps. A second approach recently proposed by Thrun views landmarks as features learned by the robot during an initial training phase [75].
Reference: [72] <author> K.T. Sutherland and W.B. Thompson. </author> <title> Inexact Navigation. </title> <booktitle> Proceedings of ICRA 93, </booktitle> <address> Atlanta, GA, </address> <month> 2-6 May, </month> <year> 1993, </year> <pages> pp 1-7. </pages>
Reference-contexts: Features extracted using these processes still have a great deal of ambiguity associated with them. It is extremely hard to extract these landmarks without a-priori knowledge about the approximate viewing position and direction. Thompson cites a bottoms-up approach presented in <ref> [72] </ref> that first analyzes the uncertainty of localization at various regions in the environment, and then develops simple heuristics for selecting landmarks likely to minimize the uncertainty.
Reference: [73] <author> H. Suzuki and A. Arimoto. </author> <title> A Recursive Method of Trajectory Planning for a Point-Like Mobile Robot in Transient Environment Utilizing Paint Procedure. </title> <journal> Journal of the Robotics Society of Japan, </journal> <volume> vol. 8, </volume> <year> 1990, </year> <pages> pp 9-19. </pages>
Reference-contexts: To reduce the amount of computation time involved, Suzuki and Arimoto proposed a divide and conquer strategy for solving the above problem by decomposing it into subgoals that could be solved easily and then concatenated to obtain the final plan <ref> [73] </ref>. The above approach is extremely constrained by the precise demand of prior knowledge about all obstacle motions and trajectories. A second group of approaches has been proposed to deal with unexpected obstacles and unknown trajectories.
Reference: [74] <author> S. Thrun and A. Bucken. </author> <title> Learning Maps for Indoor Mobile Robot Navigation. </title> <type> Technical Report CMU-CS-96-121, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1996. </year>
Reference-contexts: Today's high performance world demands precise and scientific solutions from the robotics industry. The newer robots are more mobile, carry larger payloads, perform a varied number of tasks, are capable of learning new operations, and most importantly, do all this independently of a human controller [6], [36], [57], <ref> [74] </ref>, [77]. However, to claim that we have robots that truly exhibit even a majority of the above mentioned traits would be an outright lie. Mobile autonomous robots are very much in the research and development phase. <p> The two most often used map representations are topological and geometric. Geometric maps represent the world in a global coordinate system, while topological maps represent the world as a network of nodes and arcs <ref> [74] </ref>. Cruise missiles used a terrain correlation technique for model matching based navigation before the advent of military GPS [63]. Disadvantages of map based positioning systems arise from the specific requirements for satisfactory navigation. There need to be enough stationary, easily distinguishable features that can be used for matching.
Reference: [75] <author> S, Thrun. </author> <title> A Bayesian Approach to Landmark Discovery and Active Perception in Mobile Robot Navigation. </title> <type> Technical Report CMU-CS-96-122, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1996. </year>
Reference-contexts: In artificial landmark recognition, uniquely determinable and specially designed markers or objects are placed in various a-priori known locations in the environment for the sole purpose of enabling robot navigation <ref> [75] </ref>. The advantage of such a method is that the robot location can be uniquely determined even in adverse conditions. Triangulation can also be performed (when three or more landmarks are visible) to get exact positions. <p> The environment must be known in advance but does not have to be engineered in any manner. Some added disadvantages include increased computation 8 time due to more complex object recognition tasks and decreased reliability [29], <ref> [75] </ref>. This is especially true if one uses natural landmarks in an outdoor environment since the time required to recognize non-geometric and arbitrarily shaped landmarks (typically present in outdoor environments) is much higher than that required for recognizing linear landmarks (peg. doors and windows in the indoor environment) [6]. <p> However, increasing the number of objects that need to be recognized increases the time required for recognition [29]. Thrun proposes a learning technique where the robot determines a balance between the two opposite approaches mentioned above <ref> [75] </ref>. Model matching, also known as map based positioning, involves matching the robot sensor outputs to a map or model of the environment. This technique is often also employed to update global maps in a dynamic environment, and to build a global map from multiple local maps. <p> All of these implementations match the input scenes against stored topographical maps. A second approach recently proposed by Thrun views landmarks as features learned by the robot during an initial training phase <ref> [75] </ref>. Multiple neural networks are used as feature detectors and trained by providing a series of images from the test environment as inputs. After training, this set of feature detectors can be used to perform localization by determining the subset of features visible from various locations in the environment [75]. <p> phase <ref> [75] </ref>. Multiple neural networks are used as feature detectors and trained by providing a series of images from the test environment as inputs. After training, this set of feature detectors can be used to perform localization by determining the subset of features visible from various locations in the environment [75]. The majority of artificial landmark detection research can be classified into two approaches, the first based on geometric models and the second on object recognition. In the first approach, simple regularly shaped polygonal or circular patterns are used as landmarks. <p> Another approach to landmark detection employs neural networks to classify input (visual, range data, or some other sensory data) patterns and perform localization based on the classification results [68], <ref> [75] </ref>. Thompson's group at the University of Utah is at the forefront of unstructured landmark detection for robot localization. They detect mountain peaks in the input images and match their locations against a topographical map to estimate the robot position. <p> Here we present some of the most interesting ideas for natural landmark detection with application to outdoor environments. Landmark Learning Approach Many researchers are of the view that selection of landmarks used by a robot for localization should be determined by the robot itself [29], <ref> [75] </ref>. Humans perceive the world differently from robots, and structures that make themselves readily available to us for landmark based localization may not be easily detected by robots. <p> In order to overcome these problems the selection of landmarks can be learned by the robot based on some simple rules. Thrun <ref> [75] </ref> uses this approach to train a set of neural networks as feature detectors. The landmarks are chosen based on their utility for robot localization. A qualitative measure of the goodness of natural landmarks aims at landmarks that are stationary, reliably recognizable and sufficiently unique.
Reference: [76] <author> W.B. Thompson et al. </author> <title> Geometric Reasoning for Map-Based Localization. </title> <institution> Com puter Science Technical Report UUCS-96-006, University of Utah, </institution> <address> Salt Lake City, UT, </address> <month> May, </month> <year> 1996. </year>
Reference-contexts: Another drawback is that GPS signals cannot be received indoors [34], making GPS a highly unlikely candidate for generalized robot localization. Current research in landmark detection algorithms is focused on detecting unstructured and arbitrary landmarks such as trees [35] and distant peaks [71], <ref> [76] </ref>. All of these implementations match the input scenes against stored topographical maps. A second approach recently proposed by Thrun views landmarks as features learned by the robot during an initial training phase [75]. <p> They detect mountain peaks in the input images and match their locations against a topographical map to estimate the robot position. The major drawback of using distant features for localization is the high uncertainty in the position estimate ( 30m) <ref> [76] </ref>. We describe some of the more interesting and successful research efforts in further detail. 2.3.1 Artificial Landmark Detection Here, we analyze some of the artificial landmark detection approaches that do not rely on object recognition methodologies. <p> Any change in the environment will cause us to retrain our networks, resulting in a method that is not suitable for use in dynamic environments. Model Matching Approach Another approach for navigation and localization in unstructured environments performs model matching between sensory inputs and an a-priori known topographical map <ref> [76] </ref>. This was the first navigational and localization system used by the Cruise Missile before the advent of military GPS [63]. 28 Researchers at the University of Utah are at the forefront of using natural terrain features as landmarks for robot localization purposes. <p> Thompson and Pick describe a system that detects and tracks peaks and valleys in input images to and matches these against a topographical map <ref> [76] </ref>. The strategy is to independently extract patterns from the image and the map that are likely to correspond to the same topographic features. A ridge contour finding algorithm is used to detect peaks, saddles and valleys in images.
Reference: [77] <author> T. Tsubochi, A. Hirose, and S. Arimoto. </author> <title> A Navigation Scheme with Learning for a Mobile Robot Among Multiple Moving Obstacles. </title> <booktitle> Proceedings of IROS '93. Intelligent Robots for Flexibility, p. </booktitle> <volume> 3, vol. 2317, </volume> <year> 1993, </year> <pages> pp 2234-2240. </pages>
Reference-contexts: Today's high performance world demands precise and scientific solutions from the robotics industry. The newer robots are more mobile, carry larger payloads, perform a varied number of tasks, are capable of learning new operations, and most importantly, do all this independently of a human controller [6], [36], [57], [74], <ref> [77] </ref>. However, to claim that we have robots that truly exhibit even a majority of the above mentioned traits would be an outright lie. Mobile autonomous robots are very much in the research and development phase. <p> While most reactive navigation systems tend to be based on ad-hoc designs, they can also be learned from observations made of the environment [7], [80], <ref> [77] </ref>. While neither of the above require any prior knowledge of the obstacle motion, they have their drawbacks. Learning can be a very time consuming process, and has to be repeated whenever there is a change in domain constraints. <p> It then uses reactive navigation to move around the obstacle and get back on the original path, from where the path-following algorithm can take over once again. Some newer approaches also predict the path of moving obstacles and account for that in the path generated for the robot <ref> [77] </ref>. 1.3 Motivating a Survey The previous sections were aimed at providing the reader with a high level description of some of the issues facing autonomous mobile robot navigation, and the attempts made at generating solutions for these. <p> The major drawbacks of this approach are that the motion of all the objects has to be completely known a-priori, and the amount of computational effort required to generate a path quickly increases as more obstacles with complex motions are added to the environment. Tsubouchi <ref> [77] </ref> uses an approach very similar to that proposed by Fujimura for performing path planning when the motion of the obstacles is partially known. A motion planning algorithm repeats the following steps in a continuous cycle : 1.
Reference: [78] <author> P. Wide and D. Driankov. </author> <title> A Fuzzy Approach to Multi-Sensor Data Fusion for Quality Profile Classification. </title> <booktitle> Proceedings of MFI-96, </booktitle> <address> Washington, D.C., </address> <year> 1996, </year> <pages> pp 215-21. </pages>
Reference-contexts: The evolution of new sensors, some using completely different paradigms, creates a constant flux in multi-sensor fusion and integration methodologies. Some commonly 15 used approaches to sensor fusion include Bayesian estimation [66], Kalman filter and its variants [55], fuzzy rule based systems <ref> [78] </ref>, decision theory [45], neural networks [15] and Bayesian networks [43]. Even though we have so many different techniques for performing sensor data fusion, we still lack a framework that can combine sensor data from multiple sensing modalities effectively. <p> Elfes [20] later proposed another framework called occupancy grids which was also shown to be very efficient in dealing with the above problem. Other probabilistic approaches to sensor fusion have employed fuzzy logic and rule based systems with varied success <ref> [78] </ref>. Here, we present Kortenkamp's work in sensor fusion as leading example of Bayes Net research.
Reference: [79] <author> C.H. Wu. </author> <title> A Coordinated Motion Planner Among Moving Machines and Objects. </title> <booktitle> Proceedings of IEEE Symposium on Intelligent Vehicles 1995, </booktitle> <address> New York, NY, </address> <month> 25-26 Sep, </month> <year> 1995, </year> <pages> pp 188-93. 74 </pages>
Reference-contexts: The first approach relies on a space-time formulation. Given the predicted motion of all the obstacles in the environment, a reachability region containing all possible locations for a robot in some future time is created. This is then used to guide the robot to a desired location [26], [40], <ref> [79] </ref>. The major problem with this approach is the amount of time required to construct the reachability region.
Reference: [80] <author> B. Yamauchi and R. Beer. </author> <title> Spatial Learning in Dynamic Environments. </title> <journal> IEEE Transactions on Man, Systems and Cybernetics, </journal> <volume> vol. 26, no. 3, </volume> <month> Jun </month> <year> 1996, </year> <pages> pp 496-505. </pages>
Reference-contexts: While most reactive navigation systems tend to be based on ad-hoc designs, they can also be learned from observations made of the environment [7], <ref> [80] </ref>, [77]. While neither of the above require any prior knowledge of the obstacle motion, they have their drawbacks. Learning can be a very time consuming process, and has to be repeated whenever there is a change in domain constraints.
Reference: [81] <author> S. Yuta. </author> <title> Experimental Research on the Autonomous Mobile Robot Which Nav igates in the Real World. </title> <booktitle> Proceedings of the Second International Conference on Mechatronics and Machine Vision in Practice, </booktitle> <address> Hong Kong, 12-14 Sept, </address> <year> 1995, </year> <pages> pp 59-66. </pages>
Reference-contexts: The robot is also incapable of laying odd and cut tiles and creating intricate tile patterns. Some other partially successful attempts at autonomous indoor mobile robot navigation are described in greater detail in [14], [51], <ref> [81] </ref>. The presence of mobile obstacles, unstructured terrain and multiple (possibly) dependent sensors introduce uncertainties and complexities that pose a much harder problem than navigation in a static structured domain [6], [44].
References-found: 81

