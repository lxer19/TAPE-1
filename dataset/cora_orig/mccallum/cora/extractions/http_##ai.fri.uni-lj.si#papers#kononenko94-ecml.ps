URL: http://ai.fri.uni-lj.si/papers/kononenko94-ecml.ps
Refering-URL: http://ai.fri.uni-lj.si/papers/index.html
Root-URL: 
Email: e-mail: igor.kononenko@ninurta.fer.uni-lj.si  
Title: Estimating Attributes: Analysis and Extensions of RELIEF  
Author: Igor Kononenko 
Address: Trzaska 25, SLO-61001 Ljubljana, Slovenia  
Affiliation: University of Ljubljana, Faculty of Electrical Engineering Computer Science,  
Abstract: In the context of machine learning from examples this paper deals with the problem of estimating the quality of attributes with and without dependencies among them. Kira and Rendell (1992a,b) developed an algorithm called RELIEF, which was shown to be very efficient in estimating attributes. Original RELIEF can deal with discrete and continuous attributes and is limited to only two-class problems. In this paper RELIEF is analysed and extended to deal with noisy, incomplete, and multi-class data sets. The extensions are verified on various artificial and one well known real-world problem.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Breiman L., Friedman J.H., Olshen R.A., Stone C.J.: </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsforth International Group 1984 </booktitle>
Reference-contexts: The idea is to estimate the difference between the prior entropy of classes C and posterior entropy, given values V of an attribute: Gain = X P (C) log 2 P (C) V P (V ) fi C ! Information gain and similar estimates like gini index <ref> (Breiman et al., 1984) </ref>, distance measure (Mantaras, 1989), and j-measure (Smyth & Goodman, 1990) assume that attributes are independent and therefore are not applicable in domains with strong dependencies between attributes. Kira and Rendell (1992a,b) developed an algorithm called RELIEF, which was shown to be very efficient in estimating attributes. <p> For discrete attributes the difference is either 1 (the values are different) or 0 (the values are equal), while for continuous attributes the difference is the actual difference normalized to the interval <ref> [0; 1] </ref>. Normalization with m guarantees that all weights are in the interval [1; 1]. Function diff is used also for calculating the distance between instances to find the nearest neighbours. The total distance is simply the sum of differences over all attributes. <p> For discrete attributes the difference is either 1 (the values are different) or 0 (the values are equal), while for continuous attributes the difference is the actual difference normalized to the interval [0; 1]. Normalization with m guarantees that all weights are in the interval <ref> [1; 1] </ref>. Function diff is used also for calculating the distance between instances to find the nearest neighbours. The total distance is simply the sum of differences over all attributes. Obviously, the algorithm tries to approximate the difference (2). Parameter m represents the number of instances for approximating probabilities. <p> samecl Using equalities P samecl = X P (C) P samecljeqval = X P fi C 2 we obtain: W [A] = P eqval fi Gini 0 (A) P samecl (1 P samecl ) (4) where Gini 0 X P fi C 2 C 2 is highly correlated with gini-index <ref> (Breiman et al., 1984) </ref> for classes C and values V of attribute A. <p> In this paper we did not address the problem of multi valued attributes. Information gain (1) and gini-index tend to overestimate multi valued attributes and various normalization heuristics are needed to avoid this tendency (e.g. gain ratio (Quinlan, 1986) and binarization of attributes <ref> (Kononenko et al., 1984) </ref>). RELIEF with k-nearest hits/misses implicitly uses prior probability that two instances have equal values (P eqval ) (see equation (4)) for such normalization, which seems to be appropriate.
Reference: 2. <author> Hunt E., Martin J & Stone P.: </author> <title> Experiments in Induction. </title> <address> New York: </address> <publisher> Academic Press 1966 </publisher>
Reference: 3. <author> Kira K. & Rendell L.: </author> <title> A practical approach to feature selection. </title> <booktitle> In: Proc. Intern. Conf. on Machine Learning. </booktitle> <address> (Aberdeen, July 1992) D.Sleeman & P.Edwards (eds.), </address> <publisher> Morgan Kaufmann 1992, pp.249-256 </publisher>
Reference: 4. <author> Kira K. & Rendell L.: </author> <title> The feature selection problem: traditional methods and new algorithm. </title> <booktitle> In: Proc. AAAI'92. </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992 </year>
Reference: 5. <author> Kononenko I., Bratko I., Roskar E.: </author> <title> Experiments in inductive learning of medical diagnostic rules. </title> <booktitle> In: Proc. International School for the Synthesis of Expert Knowledge Workshop. Bled, Slovenia, </booktitle> <month> August </month> <year> 1984 </year>
Reference: 6. <author> Kononenko I.: </author> <title> Semi-naive Bayesian classifier. </title> <booktitle> In: Proc. European Working Session on Learning, (Porto, </booktitle> <month> March </month> <year> 1991), </year> <editor> Y.Kodratoff (ed.), </editor> <publisher> Springer Verlag 1991, pp.206-219 </publisher>
Reference: 7. <author> Mantaras R.L.: </author> <title> ID3 Revisited: A distance based criterion for attribute selection. </title> <booktitle> In: Proc. Int. Symp. Methodologies for Intelligent Systems. </booktitle> <address> Charlotte, North Carolina, U.S.A., </address> <month> Oct. </month> <year> 1989 </year>
Reference: 8. <author> Quinlan R.: </author> <title> Induction of decision trees. </title> <booktitle> Machine learning 1, </booktitle> <month> 81-106 </month> <year> (1986) </year>
Reference-contexts: In such cases efficient heuristic algorithms are needed to discover the dependencies. Information gain was proposed as a measure for estimating the attribute's quality by Hunt et al. (1966) and later used by many authors <ref> (Quinlan, 1986) </ref>. <p> In this paper we did not address the problem of multi valued attributes. Information gain (1) and gini-index tend to overestimate multi valued attributes and various normalization heuristics are needed to avoid this tendency (e.g. gain ratio <ref> (Quinlan, 1986) </ref> and binarization of attributes (Kononenko et al., 1984)). RELIEF with k-nearest hits/misses implicitly uses prior probability that two instances have equal values (P eqval ) (see equation (4)) for such normalization, which seems to be appropriate.
Reference: 9. <author> Smyth P. & Goodman R.M.: </author> <title> Rule induction using information theory. </title> <editor> In. G.Piatetsky-Shapiro & W.Frawley (eds.): </editor> <title> Knowledge Discovery in Databases. MIT Press 1990 This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: between the prior entropy of classes C and posterior entropy, given values V of an attribute: Gain = X P (C) log 2 P (C) V P (V ) fi C ! Information gain and similar estimates like gini index (Breiman et al., 1984), distance measure (Mantaras, 1989), and j-measure <ref> (Smyth & Goodman, 1990) </ref> assume that attributes are independent and therefore are not applicable in domains with strong dependencies between attributes. Kira and Rendell (1992a,b) developed an algorithm called RELIEF, which was shown to be very efficient in estimating attributes.
References-found: 9

