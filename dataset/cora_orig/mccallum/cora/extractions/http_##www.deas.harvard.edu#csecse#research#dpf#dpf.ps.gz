URL: http://www.deas.harvard.edu/csecse/research/dpf/dpf.ps.gz
Refering-URL: http://www.deas.harvard.edu/csecse/research/dpf/root.html
Root-URL: 
Title: The DPF Benchmark Suite: A Tool for Parallel Software Evaluation  
Author: Yu Hu S. Lennart Johnsson Dimitris Kehagias Nadia Shalaby 
Address: Cambridge, Massachusetts  
Affiliation: Parallel Computing Research Group Center for Research in Computing Technology Harvard University  
Date: September 1995  
Pubnum: TR-36-95  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J.B. Anderson, C.A. Traynor, and B.M. Boghosian. </author> <title> Quantum chemistry by random walk: Exact treatment of many-electron systems. </title> <journal> Journal of Chemical Physics, </journal> <volume> 95(10) </volume> <pages> 7418-7425, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: The size L = d (1 locality) of the locality window is determined by the real input parameter locality , which must lie in the interval <ref> [0; 1] </ref>. When locality = 0, this mode degenerates to the random mode; when locality = 1, it degenerates to an identity mapping. <p> For a thorough description of the algorithmic methodology see <ref> [1] </ref>. To load-balance the calculation, high-weight particles are split and low-weight particles are killed at each step. <p> To mitigate this problem, a walker-cancellation scheme is used, as described in <ref> [1] </ref>. This amounts to an N-body problem on the walkers, and it is implemented using the sequential spread algorithm every n c (cancellation number) iteration steps. Thus, by making n c large, this code is a good test of the scan-and-send load-balancing algorithm.
Reference: [2] <author> D. Bailey, E. Barszcz, J. Barton, D. Browning, R. Carter, L. Dagum, R. Fatoohi, S. Fineberg, P. Frederickson, T. Lasinski, R. Schreiber, H. Simon, V. Venkatakrishnan, and S. Weeratunga. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report RNR-94-007, </type> <institution> NASA Ames Research Center, Moffett Field, California, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: The objective of the DPF benchmark suite is to serve as an instrument in the evaluation of data parallel compilers and scalable architectures. The NAS kernels [4] and the Livermore loops [47] are examples of "classical" benchmarks designed for uniprocessors. The NAS parallel benchmarks <ref> [2] </ref>, are "paper and pencil" benchmarks aimed at evaluating systems. The benchmarks specify the task to be performed and allows the implementor to choose algorithms as well as programming model. The NAS parallel benchmarks 2.0 [3] consists of MPI-based source implementations.
Reference: [3] <author> D. Bailey, T. Harris, W. Saphir, R. Wijngaartand, A. Woo, and M. Yarrow. </author> <title> The NAS parallel benchmarks 2.0. </title> <type> Technical Report NAS-95-020, </type> <institution> NASA Ames Research Center, Moffett Field, California, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: The NAS parallel benchmarks [2], are "paper and pencil" benchmarks aimed at evaluating systems. The benchmarks specify the task to be performed and allows the implementor to choose algorithms as well as programming model. The NAS parallel benchmarks 2.0 <ref> [3] </ref> consists of MPI-based source implementations. The Perfect Club benchmarks [16, 11, 61] is a collection of Fortran 77 application codes for the evaluation of sequential architectures. The Parkbench project [25] has assembled a collection of sequential and parallel benchmarks for the message passing programming paradigm.
Reference: [4] <author> David H. Bailey and J. Barton. </author> <title> The NAS kernel benchmark program. </title> <type> Technical Report NASA Technical Memorandum 86711, </type> <institution> NASA Ames Research Center, Moffett Field, California, </institution> <year> 1985. </year>
Reference-contexts: The selection of application codes in the DPF benchmark suite reflects this fact. The objective of the DPF benchmark suite is to serve as an instrument in the evaluation of data parallel compilers and scalable architectures. The NAS kernels <ref> [4] </ref> and the Livermore loops [47] are examples of "classical" benchmarks designed for uniprocessors. The NAS parallel benchmarks [2], are "paper and pencil" benchmarks aimed at evaluating systems. The benchmarks specify the task to be performed and allows the implementor to choose algorithms as well as programming model.
Reference: [5] <author> G.G. </author> <title> Batrouni and R.T. </title> <journal> Scalettar. Physics Review B46, </journal> <volume> 9051, </volume> <year> 1992. </year>
Reference-contexts: qptransport indirect addressing Scatter w/ combine fem-3D CMSSL partitioned scatter utility pic-gather-scatter CMF send add or FORALL w/ indirect addressing qmc CMF send overwrite AAB md SPREAD n-body CSHIFT, SPREAD, broadcast via PM AAPC diff-2D FORALL Table 17: Implementation techniques for some common communication patterns in application kernels demonstrated in <ref> [5, 24, 67] </ref>. The implementation uses 3-D single-precision real arrays, with the first axis representing the time axes, and the other two axes representing the two spatial axes of the grid. Nearest neighbor CSHIFTs are heavily used along the spatial axes.
Reference: [6] <author> R. Blankenbecler, </author> <title> R.L. Sugar, and D.J. </title> <journal> Scalapino. Physics Review D24, </journal> <volume> 2278, </volume> <year> 1981. </year>
Reference-contexts: The solution uses a grid-based Monte Carlo method <ref> [6, 23] </ref>. The parallelization in this code is accomplished by assigning a different configuration disorder to each node. The code is "embarrassingly" parallel. Each node simulates a different physical system and there is no communication between the nodes. The benchmark only tests one disorder and on one node.
Reference: [7] <author> Bruce Boghosian. </author> <title> CM-5 performance metrics suite. TMC internal memo, </title> <year> 1993. </year>
Reference-contexts: We characterize these benchmarks to assess their performance according to some inherent properties that inevitably dictate their computational structure and communication pattern. A preliminary classification of a similar set of applications is listed in <ref> [7] </ref>. Single node performance is at least as important as the communication related performance on most architectures. The DPF application kernels contain two "embarrassingly parallel" codes, fermion and gmo, which do not include any interprocessor communication.
Reference: [8] <author> Ralph G. Brickner, William George, S. Lennart Johnsson, and Alan Ruttenberg. </author> <title> A stencil compiler for the Connection Machine models CM-2/200. </title> <editor> In H.J. Sips, editor, </editor> <booktitle> Fourth International Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 68-78. </pages> <institution> Delft Univ. of Tech., F. of Appl. Physics, </institution> <year> 1993. </year>
Reference-contexts: DPF: A Data Parallel Fortran Benchmark Suite 36 Some of these issues, related to the data parallel programming paradigm, are explored in [29], while some compiler techniques are explored in <ref> [8] </ref>. The benchmark takes up 8n x n y n z bytes of memory and 9 (n x 2)(n y 2)(n z 2) FLOPs per iteration.
Reference: [9] <author> Marina C. Chen, James Cowie, and Janet Wu. </author> <title> Craft: A framework for F90 compiler optimization. </title> <booktitle> In 5th Workshop on Compilers for Parallel Computers, Malaga, </booktitle> <address> Spain, </address> <month> June </month> <year> 1995, 1995. </year> <title> DPF: A Data Parallel Fortran Benchmark Suite 58 </title>
Reference-contexts: 1 Introduction 1.1 Motivation, Functionality and Scope The goal in developing the Data Parallel Fortran (DPF) benchmark suite was to produce a means for evaluating data parallel Fortran compilers, such as the emerging High Performance Fortran (HPF) [18] compilers, Fortran-90 [49] compilers, the Fortran-Y <ref> [9] </ref> compiler 1 , as well as the Connection Machine Fortran (CMF) [64] compiler from Thinking Machines Corp. At the time the benchmarks were developed, CMF was the only data parallel Fortran language with a production quality compiler available. Hence, the benchmarks were all written in CMF.
Reference: [10] <author> James C. Cooley and J.W. Tukey. </author> <title> An algorithm for the machine computation of complex fourier series. </title> <journal> Math. Comp, </journal> <volume> 19 </volume> <pages> 291-301, </pages> <year> 1965. </year>
Reference-contexts: Aligning the 1-D arrays with the 2-D arrays result in poor load-balance for the computation of rotation factors, while not aligning the arrays yields good load-balance, but results in a potentially high communication cost. 3.7 Fast Fourier Transform These routines compute the complex-to-complex Cooley-Tukey FFT <ref> [10] </ref>. One-, two-, or three-dimensional transforms can be carried out. An input parameter determines the axis, or combination of axes for the transform. The forward transform uses a negative twiddle exponent, and the inverse transform uses a positive twiddle exponent.
Reference: [11] <author> George Cybenko, Lyle Kipp, Lynn Pointer, and David Kuck. </author> <title> Supercomputer performance evaluation and the Perfect Benchmarks. </title> <booktitle> In 1990 Procedings of the International Conference on Supercomputing, </booktitle> <pages> pages 254 - 266. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1990. </year>
Reference-contexts: The NAS parallel benchmarks [2], are "paper and pencil" benchmarks aimed at evaluating systems. The benchmarks specify the task to be performed and allows the implementor to choose algorithms as well as programming model. The NAS parallel benchmarks 2.0 [3] consists of MPI-based source implementations. The Perfect Club benchmarks <ref> [16, 11, 61] </ref> is a collection of Fortran 77 application codes for the evaluation of sequential architectures. The Parkbench project [25] has assembled a collection of sequential and parallel benchmarks for the message passing programming paradigm.
Reference: [12] <author> G. Dahlquist, A. Bjorck, and N. Anderson. </author> <title> Numerical Methods. Series in Automatic Computation. </title> <publisher> Prentice Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1974. </year>
Reference-contexts: The single instance version of this benchmark corresponds to the well known LINPACK [14] benchmark. 3.3 QR Factorization and Solution This benchmark is a collection of routines for solving dense linear systems of equations using Householder transformations <ref> [12, 20] </ref>, commonly referred to as the QR routines. <p> The Poisson's equation is discretized with a centered five-point stencil. The matrix-vector product in the Conjugate Gradient algorithm <ref> [12, 20, 55] </ref> takes the form of a stencil evaluation on the two-dimensional grid. In addition, one reduction and one broadcast is required for the Conjugate Gradient method. Compared to diff-3D this DPF benchmark is dominated by the evaluation of a two-dimensional stencil instead of a three-dimensional stencil.
Reference: [13] <author> T.J. Dekker and W. Hoffman. </author> <title> Rehabilitation of the Gauss-Jordan algorithm. </title> <journal> Numerische Mathematik, </journal> <volume> 54 </volume> <pages> 591-599, </pages> <year> 1989. </year>
Reference-contexts: Thus, both the upper and lower triangular matrices are brought to zero. An analysis of the numerical behavior of the algorithm can be found in <ref> [13] </ref>. For a detailed algorithmic description see [65]. Rather than replacing the original matrix with the identity matrix, this space is used to accumulate the inverse solution.
Reference: [14] <author> Jack J. Dongarra, J. R. Bunch, Cleve Moler, and G.W. Stewart. </author> <title> Linpack User's Guide. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, PA, </address> <year> 1979. </year>
Reference-contexts: The single instance version of this benchmark corresponds to the well known LINPACK <ref> [14] </ref> benchmark. 3.3 QR Factorization and Solution This benchmark is a collection of routines for solving dense linear systems of equations using Householder transformations [12, 20], commonly referred to as the QR routines.
Reference: [15] <author> Alan Edelman, Steve Heller, and S. Lennart Johnsson. </author> <title> Index transformation algorithms in a linear algebra framework. </title> <journal> IEEE Trans. Parallel and Distributed Computing, </journal> <volume> 5(11), </volume> <month> November </month> <year> 1994. </year>
Reference-contexts: The communication pattern portrayed is global-local transpose, which is essentially all-to-all personalized communication (AAPC). Thus, apart from being an indispensable operation in linear algebra and other numerous applications, the benchmark may be used to measure the bisection bandwidth, and was studied in a class of permutations in <ref> [15, 60] </ref>. 3 Library Functions for Linear Algebra The Connection Machine Scientific Software Library (CMSSL) was created for data parallel languages and distributed memory architectures.
Reference: [16] <author> M. Berry et. al. </author> <title> The PERFECT Club benchmarks: Effective performance evaluation of supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3:5 - 40, </volume> <year> 1989. </year>
Reference-contexts: The NAS parallel benchmarks [2], are "paper and pencil" benchmarks aimed at evaluating systems. The benchmarks specify the task to be performed and allows the implementor to choose algorithms as well as programming model. The NAS parallel benchmarks 2.0 [3] consists of MPI-based source implementations. The Perfect Club benchmarks <ref> [16, 11, 61] </ref> is a collection of Fortran 77 application codes for the evaluation of sequential architectures. The Parkbench project [25] has assembled a collection of sequential and parallel benchmarks for the message passing programming paradigm.
Reference: [17] <author> S. Aoki et. al. </author> <title> Int. </title> <journal> Journal Mod. Physics C2, </journal> <pages> pages 829-947, </pages> <year> 1991. </year>
Reference-contexts: The integer loops over the four dimensions of the lattice, so x is a neighboring lattice point along dimension . This method is described in detail in <ref> [17] </ref>, which is also an excellent source of bibliography on the subject. The qcd-kernel benchmark is implemented as a multiple-instance code, with 5-D arrays for lattice data, or more for nonscalar fields.
Reference: [18] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran; language specification, version 1.0. </title> <journal> Scientific Programming, </journal> <volume> 2(1 </volume> - 2):1-170, 1993. 
Reference-contexts: 1 Introduction 1.1 Motivation, Functionality and Scope The goal in developing the Data Parallel Fortran (DPF) benchmark suite was to produce a means for evaluating data parallel Fortran compilers, such as the emerging High Performance Fortran (HPF) <ref> [18] </ref> compilers, Fortran-90 [49] compilers, the Fortran-Y [9] compiler 1 , as well as the Connection Machine Fortran (CMF) [64] compiler from Thinking Machines Corp. At the time the benchmarks were developed, CMF was the only data parallel Fortran language with a production quality compiler available.
Reference: [19] <author> William George, Ralph G. Brickner, and S. Lennart Johnsson. </author> <title> Polyshift communications software for the Connection Machine systems CM-2 and CM-200. </title> <journal> Scientific Programming, </journal> <volume> 3(1) </volume> <pages> 83-99, </pages> <month> Spring </month> <year> 1994. </year>
Reference-contexts: Nearest neighbor CSHIFTs are heavily used along the spatial axes. The CSHIFTs are interleaved with computations in a way that precludes the use of a linear stencil formulation as well as the use of polyshift (PSHIFT <ref> [19] </ref>). The main computations are scalar operations among 2-D parallel arrays, i.e., the 3-D arrays with the time axes locally (and sequentially) indexed.
Reference: [20] <author> Gene Golub and Charles vanLoan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> second edition, </address> <year> 1989. </year>
Reference-contexts: The single instance version of this benchmark corresponds to the well known LINPACK [14] benchmark. 3.3 QR Factorization and Solution This benchmark is a collection of routines for solving dense linear systems of equations using Householder transformations <ref> [12, 20] </ref>, commonly referred to as the QR routines. <p> (r + 1) double, complex 68mn 92mn + 16m (r + 1) Table 11: Memory usage for QR factorization and solution 3.4 Gauss-Jordan Matrix Inversion Given a square matrix A, the Gauss-Jordan routines compute the inverse matrix of A, A 1 , via the Gauss-Jordan elimination algorithm with partial pivoting <ref> [20, 70] </ref>. Pivoting is required if the system is not symmetric positive definite. 4 The FLOP counts include higher order terms only. DPF: A Data Parallel Fortran Benchmark Suite 20 The pivot element is chosen from the pivot row, and the columns are (in effect) permuted. <p> the Conjugate Gradient method. 3.5.1 Parallel Cyclic Reduction Given a tridiagonal matrix A, represented as three 1-D arrays, and an n fi r matrix B with r right hand sides, the pcr benchmark solves the irreducible tridiagonal system of linear equations AX = B using the parallel cyclic reduction method <ref> [20, 26, 28] </ref>, which performs the reduction and obtains the solution in one pass. Parallel implementation issues are discussed in [35, 37, 40]. <p> However, with each step, the square root of the sum of the squares of the off-diagonal elements decreases, eventually approaching zero. Thus, the matrix approaches a diagonal matrix, and the diagonal elements approach the eigenvalues. For a detailed description of this method see <ref> [20] </ref> and [58]. Implementation details can be found in [65]. <p> The Poisson's equation is discretized with a centered five-point stencil. The matrix-vector product in the Conjugate Gradient algorithm <ref> [12, 20, 55] </ref> takes the form of a stencil evaluation on the two-dimensional grid. In addition, one reduction and one broadcast is required for the Conjugate Gradient method. Compared to diff-3D this DPF benchmark is dominated by the evaluation of a two-dimensional stencil instead of a three-dimensional stencil.
Reference: [21] <author> A. Greenberg, J. Mesirov, and J. Sethian. </author> <title> Programming direct N-body solvers on Connection Machines. </title> <type> Technical Report 245, </type> <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: The model problem is a two-dimensional vortex method for computing viscous, incompressible flow. The different codes in the suite allows for a comparison of different ways of programming the all-to-all communication. The different ways of programming the direct method are described in detail with code listings in <ref> [21] </ref>. The data structures consist of 2-D arrays with the first dimension local to a node, holding the particle information, and the second parallel, representing the number of particles. The all-to-all communication for the direct method is implemented with eight different communication patterns: 1. Broadcast 2. Broadcast with fill 3.
Reference: [22] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <year> 1990. </year>
Reference-contexts: Similarly, in diff-1D and diff-2D, the constituents of the dominant loop are timed separately. Where applicable, we attempt to quantify performance using the following attributes: FLOP count: In counting the number of FLOPs we adopt the operation counts suggested in <ref> [22] </ref>, and summarized in Table 3, for addition, subtraction, multiplication, division, square root, logarithms, exponentials and trigonometric functions. For reduction and parallel prefix operations, such as the intrinsic SUM and segmented Scans, we use the sequential FLOP count, i.e., N 1 for N element 1-D arrays.
Reference: [23] <author> J.E. </author> <title> Hirsch. </title> <journal> Physics Review B31, </journal> <volume> 4403, </volume> <year> 1985. </year>
Reference-contexts: The solution uses a grid-based Monte Carlo method <ref> [6, 23] </ref>. The parallelization in this code is accomplished by assigning a different configuration disorder to each node. The code is "embarrassingly" parallel. Each node simulates a different physical system and there is no communication between the nodes. The benchmark only tests one disorder and on one node.
Reference: [24] <author> J.E. Hirsch, </author> <title> R.L. Sugar, D.J. </title> <journal> Scalapino, and R. Blankenbecler. Physics Review B26, </journal> <volume> 5033, </volume> <year> 1982. </year>
Reference-contexts: qptransport indirect addressing Scatter w/ combine fem-3D CMSSL partitioned scatter utility pic-gather-scatter CMF send add or FORALL w/ indirect addressing qmc CMF send overwrite AAB md SPREAD n-body CSHIFT, SPREAD, broadcast via PM AAPC diff-2D FORALL Table 17: Implementation techniques for some common communication patterns in application kernels demonstrated in <ref> [5, 24, 67] </ref>. The implementation uses 3-D single-precision real arrays, with the first axis representing the time axes, and the other two axes representing the two spatial axes of the grid. Nearest neighbor CSHIFTs are heavily used along the spatial axes.
Reference: [25] <author> Roger Hockney and Michael Berry. </author> <title> Public international benchmarks for parallel computers: PARKBENCH committe report-1. </title> <type> Technical report, Netlib, </type> <institution> Oak Ridge National Laboratory, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: The NAS parallel benchmarks 2.0 [3] consists of MPI-based source implementations. The Perfect Club benchmarks [16, 11, 61] is a collection of Fortran 77 application codes for the evaluation of sequential architectures. The Parkbench project <ref> [25] </ref> has assembled a collection of sequential and parallel benchmarks for the message passing programming paradigm. To our knowledge, the DPF suite is the first benchmark suite focused entirely on data parallel software environments. The DPF benchmark suite is organized into two main groups: library functions and application kernels.
Reference: [26] <author> Roger W. Hockney. </author> <title> A fast direct solution of Poisson's equation using Fourier analysis. </title> <journal> J. ACM, </journal> <volume> 12 </volume> <pages> 95-113, </pages> <year> 1965. </year> <title> DPF: A Data Parallel Fortran Benchmark Suite 59 </title>
Reference-contexts: the Conjugate Gradient method. 3.5.1 Parallel Cyclic Reduction Given a tridiagonal matrix A, represented as three 1-D arrays, and an n fi r matrix B with r right hand sides, the pcr benchmark solves the irreducible tridiagonal system of linear equations AX = B using the parallel cyclic reduction method <ref> [20, 26, 28] </ref>, which performs the reduction and obtains the solution in one pass. Parallel implementation issues are discussed in [35, 37, 40].
Reference: [27] <author> Roger W. Hockney and J.W. Eastwood. </author> <title> Computer Simulation Using Particles. </title> <publisher> McGraw-Hill, </publisher> <year> 1988. </year>
Reference-contexts: The communication pattern involves SPREADs to implement the send-spread-reduce algorithm. This benchmark gives separate timing instrumentation for each of the following stages in the code (described for n s iteration steps and n particles): Verlet: updates particle positions and velocities using the Verlet integration method <ref> [27] </ref>; the FLOP count is n s (15n) Spreads: spreads particle information; no computation PE:Forces evaluates force and potential; FLOP count is n s (30n 2 ) DPF: A Data Parallel Fortran Benchmark Suite 45 Dist:BC computes the difference between interacting particles; this is separated from potential and force evaluation because <p> The total FLOP count is the sum of the above stages which amounts to n s (23n + 51n 2 ). 4.9 A generic direct N-body solver, long range forces The DPF benchmark n-body consists of a suite of two-dimensional N-body solvers all of which directly compute all pairwise interactions <ref> [27] </ref>. The model problem is a two-dimensional vortex method for computing viscous, incompressible flow. The different codes in the suite allows for a comparison of different ways of programming the all-to-all communication. The different ways of programming the direct method are described in detail with code listings in [21].
Reference: [28] <author> Roger W. Hockney and C.R. Jesshope. </author> <title> Parallel Computers 2. </title> <editor> Adam Hilger, </editor> <year> 1988. </year>
Reference-contexts: the Conjugate Gradient method. 3.5.1 Parallel Cyclic Reduction Given a tridiagonal matrix A, represented as three 1-D arrays, and an n fi r matrix B with r right hand sides, the pcr benchmark solves the irreducible tridiagonal system of linear equations AX = B using the parallel cyclic reduction method <ref> [20, 26, 28] </ref>, which performs the reduction and obtains the solution in one pass. Parallel implementation issues are discussed in [35, 37, 40]. <p> This system is solved using parallel cyclic reduction, which requires log 2 P communications for P processing elements <ref> [28] </ref>.
Reference: [29] <author> Yu Hu and S. Lennart Johnsson. </author> <title> Implementing O(N ) N-body algorithms efficiently in data parallel languages. </title> <journal> Journal of Scientific Programming, </journal> <note> 1996. Also available as TR-24-94, </note> <institution> Harvard University, Division of Applied Sciences, </institution> <month> September, </month> <year> 1994. </year>
Reference-contexts: DPF: A Data Parallel Fortran Benchmark Suite 36 Some of these issues, related to the data parallel programming paradigm, are explored in <ref> [29] </ref>, while some compiler techniques are explored in [8]. The benchmark takes up 8n x n y n z bytes of memory and 9 (n x 2)(n y 2)(n z 2) FLOPs per iteration. <p> The cells are ordered according to a hierarchical (two-level) row or column major ordering in three dimensions. Using a coordinate-sort for the particles as described in <ref> [29] </ref> should improve the locality in the send and get operations between the 1-D particle arrays the 3-D grid arrays. 4.11 QCD kernel: staggered fermion Conjugate Gradient method The qcd-kernel benchmark represents the kernel of a staggered fermion Conjugate Gradient algorithm for Quantum Chromo-Dynamics (QCD), which involves the computation of R <p> The local data movement can be eliminated via restructuring the code using the array aliasing feature of CMF <ref> [29] </ref>. DPF: A Data Parallel Fortran Benchmark Suite 55 The main loop also includes inner products of "3-D" vectors in computing the scaling factor and the convergence error.
Reference: [30] <author> Zdenek Johan, Kapil K Mathur, S. Lennart Johnsson, and Thomas J.R. Hughes. </author> <title> An efficient communication strategy for Finite Element Methods on the Connection Machine CM-5 system. </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, </booktitle> <volume> 113 </volume> <pages> 363-387, </pages> <year> 1994. </year>
Reference-contexts: A partitioning of the unstructured mesh is performed in an attempt to minimize the surface area (communication) for the collection of elements stored in the memory of a processing element. The partitioning is carried out by a CMSSL routine using a parallel implementation of the recursive spectral bisection technique <ref> [30, 31, 32] </ref>. The associated gather and scatter operations required for the sparse matrix-vector multiplication and assembly of residuals are also performed using CMSSL routines.
Reference: [31] <author> Zdenek Johan, Kapil K. Mathur, S. Lennart Johnsson, and Thomas J.R. Hughes. </author> <title> Mesh decomposition and communication procedures for finite element applications on the Connection Machine CM-5 System. </title> <editor> In W Gentzsch and U Harms, editors, </editor> <booktitle> High-Performance Computing and Networking, </booktitle> <volume> volume 2, </volume> <pages> pages 233-240. </pages> <note> Springer-Verlag, Lecture Notes in Computer Science, </note> <year> 1994. </year>
Reference-contexts: A partitioning of the unstructured mesh is performed in an attempt to minimize the surface area (communication) for the collection of elements stored in the memory of a processing element. The partitioning is carried out by a CMSSL routine using a parallel implementation of the recursive spectral bisection technique <ref> [30, 31, 32] </ref>. The associated gather and scatter operations required for the sparse matrix-vector multiplication and assembly of residuals are also performed using CMSSL routines.
Reference: [32] <author> Zdenek Johan, Kapil K. Mathur, S. Lennart Johnsson, and Thomas J.R. Hughes. </author> <title> Scalability of finite element applications on distributed-memory parallel computers. </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, </booktitle> <volume> 119(1 - 2):61 - 72, </volume> <month> November </month> <year> 1994. </year>
Reference-contexts: A partitioning of the unstructured mesh is performed in an attempt to minimize the surface area (communication) for the collection of elements stored in the memory of a processing element. The partitioning is carried out by a CMSSL routine using a parallel implementation of the recursive spectral bisection technique <ref> [30, 31, 32] </ref>. The associated gather and scatter operations required for the sparse matrix-vector multiplication and assembly of residuals are also performed using CMSSL routines.
Reference: [33] <author> Claes Johnson. </author> <title> Numerical solutions of partial differential equations by the Finite Element Method. </title> <publisher> Cambridge University Press, </publisher> <year> 1987. </year>
Reference-contexts: The mesh is represented as an unstructured mesh. Specifically, this code evaluates the elemental stiffness matrices and computes the displacements and the stresses at the quadrature points using the Conjugate Gradient method on unassembled stiffness matrices <ref> [33, 34] </ref>. A partitioning of the unstructured mesh is performed in an attempt to minimize the surface area (communication) for the collection of elements stored in the memory of a processing element.
Reference: [34] <author> Claes Johnson and Anders Szepessy. </author> <title> On the convergence of a finite element method for a nonlinear hyperbolic conservation law. </title> <journal> Mathematics of Computation, </journal> <volume> 49(180) </volume> <pages> 427-444, </pages> <year> 1987. </year>
Reference-contexts: The mesh is represented as an unstructured mesh. Specifically, this code evaluates the elemental stiffness matrices and computes the displacements and the stresses at the quadrature points using the Conjugate Gradient method on unassembled stiffness matrices <ref> [33, 34] </ref>. A partitioning of the unstructured mesh is performed in an attempt to minimize the surface area (communication) for the collection of elements stored in the memory of a processing element.
Reference: [35] <author> S. Lennart Johnsson. </author> <title> Solving narrow banded systems on ensemble architectures. </title> <journal> ACM TOMS, </journal> <volume> 11(3) </volume> <pages> 271-288, </pages> <month> November </month> <year> 1985. </year>
Reference-contexts: Parallel implementation issues are discussed in <ref> [35, 37, 40] </ref>. The code can handle multiple instances of the system AX = B, specifically: 1. single instance, DPF: A Data Parallel Fortran Benchmark Suite 21 2. multiple instances along one dimension, 3. multiple instances along two dimensions.
Reference: [36] <author> S. Lennart Johnsson. </author> <title> Solving tridiagonal systems on ensemble architectures. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 8(3) </volume> <pages> 354-392, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: The execution time for this benchmark is dominated by the time taken by the CMSSL routines. The substructuring is carried out as a Gauss-Jordan elimination, which only requires a single communication in order to yield a reduced size tridiagonal system with one equation per processing node <ref> [36, 40] </ref>. This system is solved using parallel cyclic reduction, which requires log 2 P communications for P processing elements [28].
Reference: [37] <author> S. Lennart Johnsson. </author> <title> Scientific computation on large scale parallel computers. </title> <booktitle> Lecture Notes for course CS 231, </booktitle> <publisher> Harvard University, </publisher> <month> May </month> <year> 1995. </year>
Reference-contexts: Parallel implementation issues are discussed in <ref> [35, 37, 40] </ref>. The code can handle multiple instances of the system AX = B, specifically: 1. single instance, DPF: A Data Parallel Fortran Benchmark Suite 21 2. multiple instances along one dimension, 3. multiple instances along two dimensions.
Reference: [38] <author> S. Lennart Johnsson, Tim Harris, and Kapil K. Mathur. </author> <title> Matrix multiplication on the Connection Machine. </title> <booktitle> In Supercomputing 89, </booktitle> <pages> pages 326-332. </pages> <publisher> ACM, </publisher> <month> November </month> <year> 1989. </year>
Reference-contexts: Given data parallel arrays x, y and A containing multiple instances <ref> [38] </ref> of the vectors x and y and the matrix A, respectively, the matrix-vector routines perform the operation y y + Ax for each instance. The benchmark kernels support all four standard floating-point data types.
Reference: [39] <author> S. Lennart Johnsson and Ching-Tien Ho. </author> <title> Spanning graphs for optimum broadcasting and personalized communication in hypercubes. </title> <journal> IEEE Trans. Computers, </journal> <volume> 38(9) </volume> <pages> 1249-1268, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: These communication types include stencils, gather, scatter, reduction, broadcast (SPREAD), all-to-all broadcast (AAB) <ref> [39] </ref>, all-to-all personalized communication (AAPC) [39], butterfly, Scan, circular shift (CSHIFT), send, get, and sort. It should be noted that more complex patterns (such as stencils and AAB) can be implemented by more than one simpler communication function (for instance CSHIFTs, SPREADs, etc.). <p> These communication types include stencils, gather, scatter, reduction, broadcast (SPREAD), all-to-all broadcast (AAB) <ref> [39] </ref>, all-to-all personalized communication (AAPC) [39], butterfly, Scan, circular shift (CSHIFT), send, get, and sort. It should be noted that more complex patterns (such as stencils and AAB) can be implemented by more than one simpler communication function (for instance CSHIFTs, SPREADs, etc.).
Reference: [40] <author> S. Lennart Johnsson and Ching-Tien Ho. </author> <title> Optimizing tridiagonal solvers for alternating direction methods on Boolean cube multiprocessors. </title> <journal> SIAM J. on Scientific and Statistical Computing, </journal> <volume> 11(3) </volume> <pages> 563-592, </pages> <year> 1990. </year>
Reference-contexts: Parallel implementation issues are discussed in <ref> [35, 37, 40] </ref>. The code can handle multiple instances of the system AX = B, specifically: 1. single instance, DPF: A Data Parallel Fortran Benchmark Suite 21 2. multiple instances along one dimension, 3. multiple instances along two dimensions. <p> The execution time for this benchmark is dominated by the time taken by the CMSSL routines. The substructuring is carried out as a Gauss-Jordan elimination, which only requires a single communication in order to yield a reduced size tridiagonal system with one equation per processing node <ref> [36, 40] </ref>. This system is solved using parallel cyclic reduction, which requires log 2 P communications for P processing elements [28].
Reference: [41] <author> S. Lennart Johnsson, Michel Jacquemin, and Robert L. Krawitz. </author> <title> Communication efficient multi-processor FFT. </title> <journal> Journal of Computational Physics, </journal> <volume> 102(2) </volume> <pages> 381-397, </pages> <month> October </month> <year> 1992. </year> <title> DPF: A Data Parallel Fortran Benchmark Suite 60 </title>
Reference-contexts: A detailed analysis of the parallel FFT can be found in <ref> [41, 59] </ref>.
Reference: [42] <author> Jim Douglas Jr and H. Rachford. </author> <title> On the numerical solution of heat conduction problems in two and three space variables. </title> <journal> Trans. American Mathematical Society, </journal> <volume> 82 </volume> <pages> 421-439, </pages> <year> 1956. </year>
Reference-contexts: P , scalar arithmetic will dominate the performance of this code, while for n x P , it is very sensitive to the communication performance. 4.2.2 2-D Diffusion Equation: ADI Method This diffusion equation simulation is the integration of the two-dimensional heat equation using the alternating direction implicit algorithm (ADI) <ref> [42, 54, 55, 69] </ref>, which is an unconditionally stable implicit method.
Reference: [43] <institution> Kendall Square Research, Waltham, MA. Kendall Square Research Technical Summary (of KSR-1), </institution> <year> 1992. </year>
Reference-contexts: For such memory architectures, the parallel code efficiency is affected by how the data reference pattern interplays with maintaining cache coherence through hardware <ref> [43, 44] </ref>, run-time system [57], or compiler [56] features.
Reference: [44] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessy. </author> <title> The DASH prototype: Logic overhead and performance. </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 41-61, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: We also expect the DPF benchmarks to be useful for the evaluation of compilers and supporting software systems on the so-called distributed shared memory architectures, such as the Stanford Dash and Flash architectures <ref> [44] </ref>, the SGI Origin 2000, and the HP/Convex SPP/2000. For such memory architectures, the parallel code efficiency is affected by how the data reference pattern interplays with maintaining cache coherence through hardware [43, 44], run-time system [57], or compiler [56] features. <p> For such memory architectures, the parallel code efficiency is affected by how the data reference pattern interplays with maintaining cache coherence through hardware <ref> [43, 44] </ref>, run-time system [57], or compiler [56] features.
Reference: [45] <author> Woody Lichtenstein and S. Lennart Johnsson. </author> <title> Block cyclic dense linear algebra. </title> <journal> SIAM Journal of Scientific Computing, </journal> <volume> 14(6) </volume> <pages> 1257-1286, </pages> <year> 1993. </year>
Reference-contexts: The factorization method is Gaussian elimination with or without partial pivoting. Each factored system is solved using forward elimination followed by back substitution. The details of the parallel implementation are given in <ref> [45, 65] </ref>. SPREADs are used for the rank-one or higher updates, and reduction is used for the selection of the pivot row. <p> Whereas load-balance is an issue for all phases of the algorithm, the "O (n 3 )" operations can be load-balanced fairly easily, while the "O (n 2 )" operations do not load-balance well <ref> [45] </ref>. Load-balance for the block allocation used by the CMF compiler is accomplished through a block-cyclic elimination order. The lu benchmark uses 3-D double-precision real arrays. Array A is of shape n fi n fi i and array B is of shape n fi r fi i.
Reference: [46] <author> P. S. Lomdahl, P. Tamayo, N. Gronbech-Jensen, and D. M. Beazley. </author> <title> 50 GFlops molecular dynamics on the Connection Machine 5. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 520-527. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: Utilizing the symmetry, each cell only needs to fetch directly 13 neighbor cells instead of all 26 in the neighbor-cell interaction. The fetching of neighbor cells is performed via CSHIFT. No linear-ordering <ref> [46] </ref> is imposed on the order of fetching the 13 neighbors, therefore the number of CSHIFTs is not minimized.
Reference: [47] <author> Frank McMahon. </author> <title> The Livermore Fortran kernels: A test of numerical performance range. </title> <booktitle> In Performance Evaluation of Supercomputers, </booktitle> <pages> pages 143 - 186. </pages> <publisher> North Holland, </publisher> <year> 1988. </year>
Reference-contexts: The selection of application codes in the DPF benchmark suite reflects this fact. The objective of the DPF benchmark suite is to serve as an instrument in the evaluation of data parallel compilers and scalable architectures. The NAS kernels [4] and the Livermore loops <ref> [47] </ref> are examples of "classical" benchmarks designed for uniprocessors. The NAS parallel benchmarks [2], are "paper and pencil" benchmarks aimed at evaluating systems. The benchmarks specify the task to be performed and allows the implementor to choose algorithms as well as programming model.
Reference: [48] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard. </title> <year> 1994. </year>
Reference-contexts: On scalable architectures, the functions mentioned above are usually implemented as part of a collective communication library, which may be part of the run-time system or be a separate library. Several of the functions mentioned are incorporated in the emerging Message-Passing Interface (MPI) standard <ref> [48] </ref>. The DPF benchmark suite contains codes that either implement this functionality in CMF or calls its supporting Run-Time System.
Reference: [49] <author> Michael Metcalf and John Reid. </author> <title> Fortran 90 Explained. </title> <publisher> Oxford Scientific Publications, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction 1.1 Motivation, Functionality and Scope The goal in developing the Data Parallel Fortran (DPF) benchmark suite was to produce a means for evaluating data parallel Fortran compilers, such as the emerging High Performance Fortran (HPF) [18] compilers, Fortran-90 <ref> [49] </ref> compilers, the Fortran-Y [9] compiler 1 , as well as the Connection Machine Fortran (CMF) [64] compiler from Thinking Machines Corp. At the time the benchmarks were developed, CMF was the only data parallel Fortran language with a production quality compiler available.
Reference: [50] <author> Henri J. Nussbaumer. </author> <title> Fast Fourier Transform and Convolution Algorithms. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: Only the leading term in the FLOP count is given. For a more detailed accounting of the number of arithmetic operations for the radix-2 Cooley-Tukey transform see, for instance, <ref> [50] </ref>. The bit-reversal operation defines a communication pattern that has many properties, studied in for example, [60] which are common with matrix transposition. They both have large demands on the network bandwidth and often cause severe network contention for many common networks and routers.
Reference: [51] <author> Pelle Olsson. </author> <title> The numerical behavior of high-order finite difference methods. </title> <journal> Journal of Scientific Computing, </journal> <volume> 9 </volume> <pages> 445-466, </pages> <year> 1994. </year>
Reference-contexts: This can be verified via Taylor series expansion [55] and guarantees fourth-order overall convergence. This somehwat peculiar operator can be shown to yield a stable linear system and thus guarantee convergence of the solution <ref> [51, 52, 53, 62] </ref>. As shown in Figure 1, the maximum number of neighbors on either side of the main diagonal is four. The communication pattern for the difference operations therefore involves a 16-point stencil with four stencil points coming from each direction: north, east, south and west.
Reference: [52] <author> Pelle Olsson. </author> <title> Summation by parts, projections, and stability. i. </title> <journal> Mathematics of Computation, </journal> <volume> 64 </volume> <pages> 1035-1065, </pages> <year> 1995. </year>
Reference-contexts: This can be verified via Taylor series expansion [55] and guarantees fourth-order overall convergence. This somehwat peculiar operator can be shown to yield a stable linear system and thus guarantee convergence of the solution <ref> [51, 52, 53, 62] </ref>. As shown in Figure 1, the maximum number of neighbors on either side of the main diagonal is four. The communication pattern for the difference operations therefore involves a 16-point stencil with four stencil points coming from each direction: north, east, south and west.
Reference: [53] <author> Pelle Olsson. </author> <title> Summation by parts, projections, and stability. ii. </title> <journal> Mathematics of Computation, </journal> <volume> 65, </volume> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: This can be verified via Taylor series expansion [55] and guarantees fourth-order overall convergence. This somehwat peculiar operator can be shown to yield a stable linear system and thus guarantee convergence of the solution <ref> [51, 52, 53, 62] </ref>. As shown in Figure 1, the maximum number of neighbors on either side of the main diagonal is four. The communication pattern for the difference operations therefore involves a 16-point stencil with four stencil points coming from each direction: north, east, south and west.
Reference: [54] <author> D. W. Peaceman and H. H. Rachford Jr. </author> <title> The numerical solution of parabolic and elliptic differential equations. </title> <journal> J. SIAM, </journal> <volume> 3 </volume> <pages> 28-41, </pages> <year> 1955. </year>
Reference-contexts: P , scalar arithmetic will dominate the performance of this code, while for n x P , it is very sensitive to the communication performance. 4.2.2 2-D Diffusion Equation: ADI Method This diffusion equation simulation is the integration of the two-dimensional heat equation using the alternating direction implicit algorithm (ADI) <ref> [42, 54, 55, 69] </ref>, which is an unconditionally stable implicit method.
Reference: [55] <author> William H. Press, P. Flannery, Saul A Teukolsky, and William Vetterling. </author> <title> Numerical recipies in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: P , scalar arithmetic will dominate the performance of this code, while for n x P , it is very sensitive to the communication performance. 4.2.2 2-D Diffusion Equation: ADI Method This diffusion equation simulation is the integration of the two-dimensional heat equation using the alternating direction implicit algorithm (ADI) <ref> [42, 54, 55, 69] </ref>, which is an unconditionally stable implicit method. <p> The Poisson's equation is discretized with a centered five-point stencil. The matrix-vector product in the Conjugate Gradient algorithm <ref> [12, 20, 55] </ref> takes the form of a stencil evaluation on the two-dimensional grid. In addition, one reduction and one broadcast is required for the Conjugate Gradient method. Compared to diff-3D this DPF benchmark is dominated by the evaluation of a two-dimensional stencil instead of a three-dimensional stencil. <p> The stencils for the x- and y-dimensions are shown in Figure 1 in matrix form. The 3's indicate that the first six stencils are of third-order accuracy, whereas the remaining stencils are sixth-order accurate. This can be verified via Taylor series expansion <ref> [55] </ref> and guarantees fourth-order overall convergence. This somehwat peculiar operator can be shown to yield a stable linear system and thus guarantee convergence of the solution [51, 52, 53, 62]. As shown in Figure 1, the maximum number of neighbors on either side of the main diagonal is four.
Reference: [56] <author> Martin C. Rinard, Daniel J. Scales, and Monica S. Lam. </author> <title> Jade: a high-level, machine independent language for parallel programming. </title> <booktitle> Computer, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: For such memory architectures, the parallel code efficiency is affected by how the data reference pattern interplays with maintaining cache coherence through hardware [43, 44], run-time system [57], or compiler <ref> [56] </ref> features. Yet another group of the DPF codes contain constructs related to an execution model in which one processor is primarily responsible for the execution control of single-thread programs, such as a typical HPF (no extrinsic procedures) and CMF (no local-global features) program.
Reference: [57] <author> Daniel J. Scales and Monica S. Lam. </author> <title> The design and evaluation of a shared object system for distributed memory machines. </title> <booktitle> In First symposium on operating systems design and implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: For such memory architectures, the parallel code efficiency is affected by how the data reference pattern interplays with maintaining cache coherence through hardware [43, 44], run-time system <ref> [57] </ref>, or compiler [56] features. Yet another group of the DPF codes contain constructs related to an execution model in which one processor is primarily responsible for the execution control of single-thread programs, such as a typical HPF (no extrinsic procedures) and CMF (no local-global features) program.
Reference: [58] <author> Gautam Schroff and Robert Schreiber. </author> <title> On the convergence of the cyclic Jacobi method for parallel block orderings. </title> <type> Technical Report 88-11, </type> <institution> Renssleaer Polytechnic Institute, Dept. of Computer Science, </institution> <month> May </month> <year> 1988. </year>
Reference-contexts: However, with each step, the square root of the sum of the squares of the off-diagonal elements decreases, eventually approaching zero. Thus, the matrix approaches a diagonal matrix, and the diagonal elements approach the eigenvalues. For a detailed description of this method see [20] and <ref> [58] </ref>. Implementation details can be found in [65].
Reference: [59] <author> Nadia Shalaby. </author> <title> Optimal parallel fast Fourier transforms for different computational models. </title> <type> Technical Report TR-33-95, </type> <institution> Harvard University, </institution> <month> November </month> <year> 1995. </year> <title> DPF: A Data Parallel Fortran Benchmark Suite 61 </title>
Reference-contexts: A detailed analysis of the parallel FFT can be found in <ref> [41, 59] </ref>.
Reference: [60] <author> Nadia Shalaby and S. Lennart Johnsson. </author> <title> A vector space framework for parallel stable permutations. </title> <type> Technical Report TR-32-95, </type> <institution> Harvard University, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: The communication pattern portrayed is global-local transpose, which is essentially all-to-all personalized communication (AAPC). Thus, apart from being an indispensable operation in linear algebra and other numerous applications, the benchmark may be used to measure the bisection bandwidth, and was studied in a class of permutations in <ref> [15, 60] </ref>. 3 Library Functions for Linear Algebra The Connection Machine Scientific Software Library (CMSSL) was created for data parallel languages and distributed memory architectures. <p> Only the leading term in the FLOP count is given. For a more detailed accounting of the number of arithmetic operations for the radix-2 Cooley-Tukey transform see, for instance, [50]. The bit-reversal operation defines a communication pattern that has many properties, studied in for example, <ref> [60] </ref> which are common with matrix transposition. They both have large demands on the network bandwidth and often cause severe network contention for many common networks and routers.
Reference: [61] <author> Priyamvada Sinvhal-Sharma, Lawrence Rauchwerger, and John Larson. </author> <title> Perfect benchmarks: </title> <type> Instrumented version. Technical Report CSRD-TR-1152, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana/Champaign, </institution> <year> 1991. </year>
Reference-contexts: The NAS parallel benchmarks [2], are "paper and pencil" benchmarks aimed at evaluating systems. The benchmarks specify the task to be performed and allows the implementor to choose algorithms as well as programming model. The NAS parallel benchmarks 2.0 [3] consists of MPI-based source implementations. The Perfect Club benchmarks <ref> [16, 11, 61] </ref> is a collection of Fortran 77 application codes for the evaluation of sequential architectures. The Parkbench project [25] has assembled a collection of sequential and parallel benchmarks for the message passing programming paradigm.
Reference: [62] <author> Bo Strand. </author> <title> Summation by parts for finite difference approximations for d/dx. </title> <journal> Journal of Computational Physics, </journal> <volume> 110 </volume> <pages> 47-67, </pages> <year> 1994. </year>
Reference-contexts: This can be verified via Taylor series expansion [55] and guarantees fourth-order overall convergence. This somehwat peculiar operator can be shown to yield a stable linear system and thus guarantee convergence of the solution <ref> [51, 52, 53, 62] </ref>. As shown in Figure 1, the maximum number of neighbors on either side of the main diagonal is four. The communication pattern for the difference operations therefore involves a 16-point stencil with four stencil points coming from each direction: north, east, south and west.
Reference: [63] <institution> Thinking Machines Corp. </institution> <type> CM-5 Technical Summary, </type> <year> 1991. </year>
Reference-contexts: Some of the benchmarks focus on evaluating how well the local memory hierarchy (consisting of registers and caches and main memory) is utilized in a distributed memory multiprocessor. For example, in the Connection Machine CM-5 <ref> [63] </ref>, the nodes are best viewed as having a register-oriented vector architecture with respect to the data-parallel programming model. However, the main memory consists of dynamic random access memory (DRAM) that is operated in DPF: A Data Parallel Fortran Benchmark Suite 4 page mode.
Reference: [64] <institution> Thinking Machines Corp. </institution> <note> CM Fortran Reference Manual, Version 2.1, </note> <year> 1993. </year>
Reference-contexts: Scope The goal in developing the Data Parallel Fortran (DPF) benchmark suite was to produce a means for evaluating data parallel Fortran compilers, such as the emerging High Performance Fortran (HPF) [18] compilers, Fortran-90 [49] compilers, the Fortran-Y [9] compiler 1 , as well as the Connection Machine Fortran (CMF) <ref> [64] </ref> compiler from Thinking Machines Corp. At the time the benchmarks were developed, CMF was the only data parallel Fortran language with a production quality compiler available. Hence, the benchmarks were all written in CMF.
Reference: [65] <institution> Thinking Machines Corp. CMSSL for CM Fortran, </institution> <note> Version 3.1, </note> <year> 1993. </year>
Reference-contexts: Moreover, main memory is addressed through a translation table that holds a strict subset of DRAM pages. Optimum performance for some computations therefore requires yet another level of blocking. Several of the Connection Machine Scientific Software Library (CMSSL) <ref> [65] </ref> routines (used in some of the DPF codes) attempts to exploit locality of reference in this four level hierarchy (registers, DRAM pages, mapped pages and remote memory). <p> Where possible, the interface conventions are kept identical with those of CMSSL. In many cases, different layouts are accepted and analyzed before calling the common interface. Documentation of the CMSSL interface with all its conventions can be found in <ref> [65] </ref>. The description of each benchmark function includes a FLOP count, the amount of user allocated memory, and a characterization of data reference patterns for remote and local references. We summarize some of the important properties of our implementations of the linear algebra benchmarks by means of three tables. <p> The factorization method is Gaussian elimination with or without partial pivoting. Each factored system is solved using forward elimination followed by back substitution. The details of the parallel implementation are given in <ref> [45, 65] </ref>. SPREADs are used for the rank-one or higher updates, and reduction is used for the selection of the pivot row. <p> The CMF version of the QR routines only supports single-instance computation and performs the Householder transformations without column pivoting. The corresponding CMSSL version implements a more general interface. Below, we discuss the CMF version. For a detailed description of the CMSSL version see <ref> [65] </ref>. Both the factorization and the solution routines make use of masks. An alternative would be to use array sections. <p> Thus, both the upper and lower triangular matrices are brought to zero. An analysis of the numerical behavior of the algorithm can be found in [13]. For a detailed algorithmic description see <ref> [65] </ref>. Rather than replacing the original matrix with the identity matrix, this space is used to accumulate the inverse solution. For single-precision real n fi n matrices, the FLOP count is n (n + 2 + 2n 2 ), and the memory requirement is 28n 2 + 16n bytes. <p> The three diagonals representing A have the same shape and are 1-D, 2-D, or 3-D arrays, for the first, second and third case respectively. One of the array dimensions is the problem axis <ref> [65] </ref> of extent n, i.e., the axis along which the system will be solved. The other dimensions are the instance axes [65] and the product of their extents is the number i of instances. <p> One of the array dimensions is the problem axis <ref> [65] </ref> of extent n, i.e., the axis along which the system will be solved. The other dimensions are the instance axes [65] and the product of their extents is the number i of instances. For a single right hand side per instance, the array B is also 1-D, 2-D, or 3-D for the three cases respectively and has the same shape as each diagonal of A. <p> Thus, the matrix approaches a diagonal matrix, and the diagonal elements approach the eigenvalues. For a detailed description of this method see [20] and [58]. Implementation details can be found in <ref> [65] </ref>. <p> The associated gather and scatter operations required for the sparse matrix-vector multiplication and assembly of residuals are also performed using CMSSL routines. The finite element mesh is represented both as a collection of elements in a standard format <ref> [65] </ref>, and as a collection of nodes: * Each node is associated with one or more elements of the element nodes array, ien, defined by ien (m; n) = the node number of the mth node of the nth mesh element.
Reference: [66] <institution> Thinking Machines Corp. </institution> <note> DPEAC Reference Manual, CMOST Version 7.1, </note> <year> 1993. </year>
Reference-contexts: In other cases, rather than resorting to library calls, some segment of the code, critical to the benchmark performance, is identified and implemented in the lower level language C/DPEAC <ref> [66] </ref>. This code version is termed C/DPEAC and is assumed to give the programmer finer control over the underlying architecture. Some DPF benchmarks provide several versions at the same level of optimization.
Reference: [67] <author> J. Tobochnik, G. G. Batrouni, and H. Gould. </author> <booktitle> Computers in Physics 6, </booktitle> <volume> 673, </volume> <year> 1992. </year>
Reference-contexts: qptransport indirect addressing Scatter w/ combine fem-3D CMSSL partitioned scatter utility pic-gather-scatter CMF send add or FORALL w/ indirect addressing qmc CMF send overwrite AAB md SPREAD n-body CSHIFT, SPREAD, broadcast via PM AAPC diff-2D FORALL Table 17: Implementation techniques for some common communication patterns in application kernels demonstrated in <ref> [5, 24, 67] </ref>. The implementation uses 3-D single-precision real arrays, with the first axis representing the time axes, and the other two axes representing the two spatial axes of the grid. Nearest neighbor CSHIFTs are heavily used along the spatial axes.
Reference: [68] <author> C.A. Traynor, J.B. Anderson, and B.M. Boghosian. </author> <title> A quantum Monte Carlo calculation of the ground state energy of the hydrogen molecule. </title> <journal> Journal of Chemical Physics, </journal> <volume> 94(5) </volume> <pages> 3657-3664, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: For a thorough description of the algorithmic methodology see [1]. To load-balance the calculation, high-weight particles are split and low-weight particles are killed at each step. This kill-and-split algorithm is implemented with Scans and sends, and is described in <ref> [68] </ref>, The relevant code fragment follows: subroutine kill_and_split (w_new, z_new, z_initial, & n_particles, n_dimensions, n_walkers, n_ensemble, sum_min, count) integer n_particles, n_dimensions, n_walkers, n_ensemble integer particle, dimension, ier, ensemble, count real*8 z_initial (n_particles, n_dimensions) cmf$ layout z_initial (:serial, :serial) real*8 z_new (n_particles, n_dimensions, n_walkers, n_ensemble) real*8 w_new (n_walkers, n_ensemble) CMF$ layout z_new
Reference: [69] <author> E.L. Wachspress and G.J. Habetler. </author> <title> An alternating direction implicit iteration technique. </title> <journal> J. SIAM, </journal> <volume> 8 </volume> <pages> 403-424, </pages> <year> 1960. </year>
Reference-contexts: P , scalar arithmetic will dominate the performance of this code, while for n x P , it is very sensitive to the communication performance. 4.2.2 2-D Diffusion Equation: ADI Method This diffusion equation simulation is the integration of the two-dimensional heat equation using the alternating direction implicit algorithm (ADI) <ref> [42, 54, 55, 69] </ref>, which is an unconditionally stable implicit method.
Reference: [70] <author> J.H. Wilkinson. </author> <title> Error analysis of direct methods of matrix invesion. </title> <journal> Journal of Association of Computing Machinery, </journal> <volume> 8 </volume> <pages> 281-330, </pages> <year> 1961. </year>
Reference-contexts: (r + 1) double, complex 68mn 92mn + 16m (r + 1) Table 11: Memory usage for QR factorization and solution 3.4 Gauss-Jordan Matrix Inversion Given a square matrix A, the Gauss-Jordan routines compute the inverse matrix of A, A 1 , via the Gauss-Jordan elimination algorithm with partial pivoting <ref> [20, 70] </ref>. Pivoting is required if the system is not symmetric positive definite. 4 The FLOP counts include higher order terms only. DPF: A Data Parallel Fortran Benchmark Suite 20 The pivot element is chosen from the pivot row, and the columns are (in effect) permuted.
References-found: 70

