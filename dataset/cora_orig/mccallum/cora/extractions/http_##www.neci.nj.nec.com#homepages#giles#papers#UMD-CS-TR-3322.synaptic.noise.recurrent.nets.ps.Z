URL: http://www.neci.nj.nec.com/homepages/giles/papers/UMD-CS-TR-3322.synaptic.noise.recurrent.nets.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/papers/
Root-URL: http://www.neci.nj.nec.com
Email: fkamjim,giles,horneg@research.nj.nec.com  
Title: An Analysis of Noise in Recurrent Neural Networks: Convergence and Generalization  
Author: Kam Jim C. Lee Giles Bill G. Horne 
Date: Revised June 1995  
Note: Also with  
Address: 4 Independence Way Princeton, NJ 08540  College Park, MD 20742  
Affiliation: NEC Research Institute, Inc.  Institute for Advanced Computer Studies University of Maryland  
Abstract: There has been much interest in applying noise to feedforward neural networks in order to observe their effect on network performance. We extend these results by introducing and analyzing various methods of injecting synaptic noise into dynamically-driven recurrent networks during training. We present theoretical results which show that applying a controlled amount of noise during training may improve convergence and generalization performance. In addition, we analyze the effects of various noise parameters (additive vs. multiplicative, cumulative vs. non-cumulative, per time step vs. per string) and predict that best overall performance can be achieved by injecting additive noise at each time step. Noise contributes a second-order gradient term to the error function which can be viewed as an anticipatory agent to aid convergence. This term appears to find promising regions of weight space in the beginning stages of training when the training error is large and should improve convergence on error surfaces with local minima. Synaptic noise also enhances the error function by favoring internal representations where state nodes are operating in the saturated regions of the sigmoid discriminant function, thus improving generalization to longer strings. To substantiate these predictions, we present simulations on learning the dual parity grammar from temporal strings for all noise models, and present simulations on learning a randomly generated 6-state grammar using the predicted best noise model. 
Abstract-found: 1
Intro-found: 1
References-found: 0

