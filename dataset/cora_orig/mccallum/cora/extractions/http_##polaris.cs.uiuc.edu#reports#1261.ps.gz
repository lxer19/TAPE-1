URL: http://polaris.cs.uiuc.edu/reports/1261.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: The Cedar System and an Initial Performance Study  
Author: D. Kuck, E. Davidson D. Lawrie A. Sameh C.-Q Zhu A. Veidenbaum, J. Konicek, P. Yew, K. Gallivan, W. Jalby H. Wijshoff R. Bramley U.M. Yang P. Emrath, D. Padua, R. Eigenmann, J. Hoeflinger, G. Jaxon Z. Li T. Murphy, J. Andrews, S. Turner 
Address: Urbana, IL, 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois  
Abstract: In this paper, we give an overview of the Cedar mutliprocessor and present recent performance results. These include the performance of some computational kernels and the Perfect Benchmarks R fl . We also present a methodology for judging parallel system performance and apply this methodology to Cedar, Cray YMP-8, and Thinking Machines CM-5. 
Abstract-found: 1
Intro-found: 1
Reference: [Add1] <author> CSRD Staff. </author> <type> Perfect Report 2: Addendum 1. </type> <institution> Center for Supercomputing Research and Development, University of Illinois, </institution> <year> 1991. </year>
Reference-contexts: Cedar The previous discussion has ignored absolute performance in terms of time or megaflops. Table 3 shows the megaflops generated by Cedar automatable versions <ref> [Add1] </ref>, as well as Cray YMP/8 baseline compiler MFLOPS to Cedar MFLOPS ratios. The harmonic mean for the MFLOPS on the YMP/8 is 23.7, 7.4 times that of Cedar. It should be remembered that the ratios of clock speeds of the two systems is 170ns/6ns = 28.33.
Reference: [AnGa93] <author> J. Andrews and K. Gallivan. </author> <title> Analysis of a Cedar Implementation of trfd, </title> <note> CSRD Report in preparation, </note> <institution> University of Illinois. </institution>
Reference-contexts: If we change the algorithm used in the code and exploit the hierarchical SDOALL/CDOALL control structure an execution time of 31 secs. results [YaGa93]. The execution time of TRFD was reduced to 11.5 secs. by implementing high performance kernels to efficiently exploit the clusters' caches and vector registers <ref> [AnGa93] </ref>. The improved version was shown to have almost four times the number of page faults relative to the one-cluster version and was spending close to 50% of the time in virtual memory activity.
Reference: [BrBo91] <author> R. Bramley and J. Bordner. </author> <title> Sequential Optimization and Data Distribution for arc2d on the Cedar Hierarchical Multiprocessor, </title> <type> CSRD Report No. 1128, </type> <institution> University of Illi-nois, </institution> <year> 1991. </year>
Reference-contexts: Careful consideration of ARC2D reveals a substantial number of unnecessary computations. Primarily due to their elimination but also due to aggressive data distribution into cluster memory the execution time is reduced to 68 secs. <ref> [BrBo91] </ref>. If a hand-coded parallel random number generator is used, QCD can be improved to yield a speed improvement of 20.8 rather than the 1.8 reported for the automatable code. FLO52, DYFESM, and TRFD require more elaborate analyses and modifications.
Reference: [EABM91] <author> Emrath, P., et al. </author> <title> The Xylem Operating System. Procs. </title> <booktitle> of ICPP'91, </booktitle> <volume> vol. 1, pg. </volume> <pages> 67-70, </pages> <year> 1991. </year>
Reference-contexts: All of these make use of the abstractions provided by the Xylem kernel <ref> [EABM91] </ref> which links the four separate operating systems in Alliant clusters into the Cedar OS. Xylem exports virtual memory, scheduling, and file system services for Cedar. A program for Cedar can be written using explicit parallelism and memory hierarchy placement directives.
Reference: [EHJL91] <author> Eigenmann, et al. </author> <title> Restructuring Fortran Programs for Cedar. Procs. </title> <booktitle> of ICPP'91, </booktitle> <volume> vol. 1, </volume> <pages> pp. 57-66, </pages> <year> 1991. </year>
Reference-contexts: This access is supported by language extensions and run-time library functions. The Cedar Fortran language extensions and compilation for Cedar are described next. Cedar Fortran is fully described in [Hoef91] and description of the Cedar Compiler project can be found in <ref> [EHJL91, EHLP91, EHJP92] </ref>. 3.1 Fortran Extensions Parallel Loops A programmer can express parallelism using DOALL loop constructs. A DOALL is a loop in which iterations are independent and therefore can be executed in parallel. <p> These transformations include array privatization, parallel reductions, advanced induction variable substitution, runtime data dependence tests, balanced stripmining, and paralleliza-tion in the presence of SAVE and RETURN statements. Many of these transformations require advanced symbolic and interprocedural analysis methods. The transformations have been described in more detail in <ref> [EHLP91, EHJL91, EHJP92] </ref> 4 Cedar Performance Examples of Cedar performance are discussed in this section. Given the complexity of the Cedar architecture, compilers, OS, and of the codes themselves it is very difficult to isolate the performance effects of various architectural features at the level of full codes.
Reference: [EHJP92] <author> Eigenmann, et al. </author> <title> The Cedar Fortran Project. </title> <type> CSRD Report No. 1262, </type> <institution> University of Illinois, </institution> <year> 1992. </year>
Reference-contexts: This access is supported by language extensions and run-time library functions. The Cedar Fortran language extensions and compilation for Cedar are described next. Cedar Fortran is fully described in [Hoef91] and description of the Cedar Compiler project can be found in <ref> [EHJL91, EHLP91, EHJP92] </ref>. 3.1 Fortran Extensions Parallel Loops A programmer can express parallelism using DOALL loop constructs. A DOALL is a loop in which iterations are independent and therefore can be executed in parallel. <p> These transformations include array privatization, parallel reductions, advanced induction variable substitution, runtime data dependence tests, balanced stripmining, and paralleliza-tion in the presence of SAVE and RETURN statements. Many of these transformations require advanced symbolic and interprocedural analysis methods. The transformations have been described in more detail in <ref> [EHLP91, EHJL91, EHJP92] </ref> 4 Cedar Performance Examples of Cedar performance are discussed in this section. Given the complexity of the Cedar architecture, compilers, OS, and of the codes themselves it is very difficult to isolate the performance effects of various architectural features at the level of full codes.
Reference: [EHLP91] <author> Eigenmann, et al. </author> <title> Experience in the automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pp. 65-83, </pages> <year> 1991. </year>
Reference-contexts: This access is supported by language extensions and run-time library functions. The Cedar Fortran language extensions and compilation for Cedar are described next. Cedar Fortran is fully described in [Hoef91] and description of the Cedar Compiler project can be found in <ref> [EHJL91, EHLP91, EHJP92] </ref>. 3.1 Fortran Extensions Parallel Loops A programmer can express parallelism using DOALL loop constructs. A DOALL is a loop in which iterations are independent and therefore can be executed in parallel. <p> These transformations include array privatization, parallel reductions, advanced induction variable substitution, runtime data dependence tests, balanced stripmining, and paralleliza-tion in the presence of SAVE and RETURN statements. Many of these transformations require advanced symbolic and interprocedural analysis methods. The transformations have been described in more detail in <ref> [EHLP91, EHJL91, EHJP92] </ref> 4 Cedar Performance Examples of Cedar performance are discussed in this section. Given the complexity of the Cedar architecture, compilers, OS, and of the codes themselves it is very difficult to isolate the performance effects of various architectural features at the level of full codes.
Reference: [FWPS92] <author> Ferng, W., et al. </author> <title> Basic Sparse Matrix Computations on Massively Parallel Computers. </title> <type> AHPCRC Preprint 92-084, </type> <institution> University of Minnesota, </institution> <month> July, </month> <year> 1992. </year>
Reference-contexts: Cedar exhibits scalable intermediate performance for smaller matrices, evidently ranging well below the smallest actual runs of N = 1K. No unacceptable performance was observed in the data that was gathered. In <ref> [FWPS92] </ref>, a number of linear algebra experiments are reported on the CM-5. For comparison, we quote data for matrix-vector products with bandwidths 3 and 11. The CM-5 used does not have floating-point accelerators. <p> The CM-5 used does not have floating-point accelerators. For problem sizes run, 16K N 256K, high performance was not achieved relative to 32, 256, or 512 processors. The communication structure of the CM-5 evidently causes these performance difficulties <ref> [FWPS92] </ref>. The CM-5 exhibits scalable intermediate performance with these three processor counts for problems evidently smaller than 16K for bandwidth 11 and evidently much smaller problems for bandwidth 3. No unacceptable performance was observed in the ranges reported.
Reference: [GJTV91] <author> K. Gallivan, et al. </author> <title> Preliminary Performance Analysis of the Cedar Multiprocessor Memory System. </title> <booktitle> Proc. 1991 ICPP, </booktitle> <volume> Vol. I, </volume> <pages> pp. 71-75, </pages> <year> 1991. </year>
Reference-contexts: The 32 CE observed performance yields 74 % efficiency compared to the effective peak and is consistent with the observed maximum bandwidth of memory system characterization benchmarks <ref> [GJTV91] </ref>. 1 cl. 2 cl. 3 cl. 4 cl.
Reference: [GJWY93] <author> K. Gallivan, et al. </author> <title> Comments on a Cedar Implementation of flo52, </title> <note> CSRD Report in preparation, </note> <institution> University of Illinois. </institution>
Reference: [GKLS83] <author> Gajski, D., et al. </author> <title> CEDAR aLarge Scale Multiprocessor, Procs. </title> <booktitle> 1983 ICPP, </booktitle> <pages> pp. 524-529, </pages> <year> 1983. </year>
Reference-contexts: The Cedar project brought together a group of people in the areas of computer architecture, paralleliz-ing compilers, operating systems, and parallel algorithms/applications to help solve the real problems associated with building a "complete" parallel system, and to study the effects of interaction among these components on such a machine <ref> [GKLS83, KDLS86] </ref>. The machine has been in full operation since late 1990. The Cedar experience includes the architecture, compiler, OS, and application perspectives and this paper attempts to summarize these for the architecture community. We describe the machine organization in Section 2, concentrating on the unique aspects of Cedar.
Reference: [GoGV90] <author> Gornish, E., et al. </author> <title> Compiler-directed Data Prefetching in Multiprocessors with Memory Hierarchies. Procs. </title> <booktitle> ICS'90, Amsterdam, The Netherlands, </booktitle> <volume> vol. 1, </volume> <pages> pp. 342-353, </pages> <year> 1990. </year>
Reference-contexts: The compiler then attempts to float the prefetch instructions in order to overlap prefetch operations with computation. This rarely succeeds and thus most of the time prefetch is started immediately before the vector instruction. More aggressive methods are being investigated <ref> [GoGV90] </ref>. Data privatization Cedar Fortran uses data declared local to a loop in place of scalar and array expansion. In all Perfect programs we have found loop-local data placement to be an important factor in reducing data access latencies.
Reference: [Hoef91] <author> Hoeflinger, J. </author> <title> Cedar Fortran Programmer's Handbook. </title> <institution> Center for Supercomputing Research and Development, University of Illi-nois, </institution> <year> 1991. </year>
Reference-contexts: This access is supported by language extensions and run-time library functions. The Cedar Fortran language extensions and compilation for Cedar are described next. Cedar Fortran is fully described in <ref> [Hoef91] </ref> and description of the Cedar Compiler project can be found in [EHJL91, EHLP91, EHJP92]. 3.1 Fortran Extensions Parallel Loops A programmer can express parallelism using DOALL loop constructs. A DOALL is a loop in which iterations are independent and therefore can be executed in parallel.
Reference: [KDLS86] <author> Kuck, D., et al. </author> <title> Parallel Supercomputing Today and the Cedar Approach, </title> <journal> Science, </journal> <volume> vol. 231, </volume> <pages> pp. 967-974, </pages> <month> Feb. 28, </month> <year> 1986. </year>
Reference-contexts: The Cedar project brought together a group of people in the areas of computer architecture, paralleliz-ing compilers, operating systems, and parallel algorithms/applications to help solve the real problems associated with building a "complete" parallel system, and to study the effects of interaction among these components on such a machine <ref> [GKLS83, KDLS86] </ref>. The machine has been in full operation since late 1990. The Cedar experience includes the architecture, compiler, OS, and application perspectives and this paper attempts to summarize these for the architecture community. We describe the machine organization in Section 2, concentrating on the unique aspects of Cedar.
Reference: [KTVZ91] <author> Konicek, J., et al. </author> <title> The Organization of the Cedar System. Procs. </title> <booktitle> ICPP'91, </booktitle> <volume> vol. 1, pg. </volume> <pages> 49-56, </pages> <year> 1991. </year>
Reference-contexts: Each cluster is a slightly modified Alliant FX/8 system with eight processors. In this section we first summarize the features of these clusters and then describe the unique features of Cedar. For a more detailed overall description of Cedar see <ref> [KTVZ91] </ref>. Alliant clusters The organization of an Alliant FX/8 cluster is shown in Figure 2. Each Alliant FX/8 contains 8 computational elements (CEs). The CEs are connected to a 4-way interleaved shared cache which in turn is connected to an interleaved cluster memory.
Reference: [Lawr75] <author> Lawrie, D., </author> <title> Access and Alignment of Data in an Array Processor. </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-24, no. 12, </volume> <pages> pp. 1145-1155, </pages> <month> Dec. </month> <year> 1975. </year>
Reference-contexts: Global Network The Cedar network was designed to support simultaneous vector loads/stores from global memory by all processors. It is a multistage shu*e-exchange network as shown in Fig ure 1. The network is self-routing, buffered and packet-switched. Routing is based on the tag control scheme proposed in <ref> [Lawr75] </ref>, and provides a unique path between any pair of input/output ports. Each network packet consists of one to four 64-bit words, the first word containing routing and control information and the memory address. The network is constructed with 8 fi 8 crossbar switches with 64-bit wide data paths.
Reference: [MaEG92] <author> B. Marsolf, P. Emrath, and K. Galli-van. </author> <title> Investigation of the Page Fault Performance of Cedar, </title> <type> CSRD Report No. 1263, </type> <institution> University of Illinois, </institution> <year> 1992. </year>
Reference-contexts: The extra faults are TLB miss faults as each additional cluster of a multicluster version first accesses pages for which a valid PTE exists in global memory. Based on analysis of the virtual memory performance of Cedar, <ref> [MaEG92] </ref>, a distributed memory version of the code was developed to mitigate this problem and yielded a final execution time of 7.5 secs. SPICE also benefits significantly from algorithmic attention.
Reference: [Turn93] <author> S. Turner. </author> <title> Performance Analysis of Interconnection Networks, </title> <type> PhD Thesis in preparation, </type> <year> 1993. </year>
Reference-contexts: We have shown via detailed simulations that this degradation is not inherent in the type of network used but is a result of specific implementation constraints <ref> [Turn93] </ref>. 4.2 Cedar Performance Using the Perfect Codes All the results presented in this section were collected in single-user mode to avoid the non-determinism of multiprogramming. The results are shown in Table 3 and have speed improvements versus uniprocessor scalar versions of the same codes.
Reference: [YaGa93] <author> U. M. Yang and K. Gallivan. </author> <title> Analysis of a Cedar Implementation of dyfesm, </title> <type> CSRD Report No. 1284, </type> <institution> University of Illinois, </institution> <year> 1993. </year>
Reference-contexts: If we change the algorithm used in the code and exploit the hierarchical SDOALL/CDOALL control structure an execution time of 31 secs. results <ref> [YaGa93] </ref>. The execution time of TRFD was reduced to 11.5 secs. by implementing high performance kernels to efficiently exploit the clusters' caches and vector registers [AnGa93].
Reference: [ZhYe87] <author> Zhu, C-Q. and Yew, P-C., </author> <title> A Scheme to Enforce Data Dependence on Large Multiprocessor Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. SE-13, no. 6, </volume> <pages> pp. 726-739, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: However, given multistage interconnection networks it is impossible to provide standard lock cycles and very inefficient to perform multiple memory accesses for synchronization. Cedar implements a set of indivisible synchronization instructions in each memory module. These include Test-And-Set and Cedar synchronization instructions based on <ref> [ZhYe87] </ref>, which are performed by a special processor in each memory module. Cedar synchronization instructions implement Test And-Operate, where Test is any relational operation on 32-bit data (e.g. ) and Operate is a Read, Write, Add, Subtract, or Logical operation on 32-bit data.
References-found: 20

