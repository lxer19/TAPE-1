URL: http://www.eecs.umich.edu/~tyson/Postscript/micro30a.ps.gz
Refering-URL: http://www.eecs.umich.edu/~tyson/publications.html
Root-URL: http://www.cs.umich.edu
Email: tyson@eecs.umich.edu  taustin@ichips.intel.com  
Title: Accuracy and Performance of Memory Communication Through Renaming  
Author: Gary S. Tyson Todd M. Austin 
Address: Michigan  
Affiliation: The University of  Intel Microcomputer Research Labs  
Note: Improving the  
Abstract: Copyright 1997 IEEE. Published in the Proceedings of Micro-30, December 1-3, 1997 in Research Triangle Park, North Carolina. Personal use of thismaterial is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE.Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966. Abstract As processors continue to exploit more instruction level parallelism, a greater demand is placed on reducing the effects of memory access latency. In this paper, we introduce a novel modification of the processor pipeline called memory renaming. Memory renaming applies register access techniques to load instructions, reducing the effect of delays caused by the need to calculate effective addresses for the load and all preceding stores before the data can be fetched. Memory renaming allows the processor to speculatively fetch values when the producer of the data can be reliably determined without the need for an effective address. This work extends previous studies of data value and dependence speculation. When memory renaming is added to the processor pipeline, renaming can be applied to 30% to 50% of all memory references, translating to an overall improvement in execution time of up to 41%. Furthermore, this improvement is seen across all memory segments including the heap segment which has often been difficult to manage efficiently. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Intel boosts pentium pro to 200 mhz. </institution> <type> Microprocessor Report, 9(17), </type> <month> June </month> <year> 1995. </year>
Reference-contexts: Whereas, dependencies between register-register instructions can be identified by examining the operand fields, memory dependencies cannot be determined until much later in the pipeline (when the effective address is calculated). As a result, mechanisms specific to loads and stores (e.g., the MOB in the Pentium Pro <ref> [1] </ref>) are required to resolve these memory dependencies later in the pipeline and enforce memory access semantics. To date, the only effective solution for dealing with ambiguous memory dependencies requires stalling loads until no earlier unknown store address exists.
Reference: [2] <author> Todd M. Austin and Guri S. Sohi. </author> <title> Zero-cycle loads: Mi-croarchitecture support for reducing load latency. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 82-92, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Section 5 provides performance analysis for a cycle level simulation of the techniques. In section 6 we state conclusions and identify future research directions for this work. 2 Background A number of studies have targeted the reduction of memory latency. Austin and Sohi <ref> [2] </ref> employed a simple, fast address calculation early in the pipeline to effectively hide the memory latency. This was achieved by targeting the simple base+offset addressing modes used in references to global and stack data.
Reference: [3] <author> Doug Burger, Todd M. Austin, and Steve Bennett. </author> <title> Evaluating future microprocessors: The simplescalar tool set. </title> <note> In UW - Madison Technical Report #1308, </note> <month> Jul </month> <year> 1996. </year>
Reference-contexts: The Fortran codes were first converted to C using AT&T F2C version 1994.09.27. All experiments were performed on an extended version of the SimpleScalar <ref> [3] </ref> tool set. The tool set employs the SimpleScalar instruction set, which is a (virtual) MIPS-like architecture [6]. Table 1: Benchmark Application Descriptions Bench- Instr Loads Value Addr Prod mark (Mil.) (Mil.) Locality Loc. <p> We varied the the confidence mechanism, mis-speculation recovery mechanism, and key system parameters to see what affect these parameters had on performance. 5.1 Methodology Our baseline simulator is detailed in Table 2. It is from the SimpleScalar simulation suite (simulator sim-outorder) <ref> [3] </ref>. The simulator executes only user-level instructions, performing a detailed timing simulation of an 4-way superscalar microprocessor with two levels of instruction and data cache memory. The simulator implements an out-of-order issue execution model.
Reference: [4] <author> Thomas M. Conte, Kishore N. Menezes, Patrick M. Mills, and Burzin A. Patel. </author> <title> Optimization of instruction fetch mechanisms for high issue rates. </title> <booktitle> In 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 333-344, </pages> <month> Jun </month> <year> 1995. </year>
Reference-contexts: The data cache modeled is a four-ported 32k two-way set-associative non-blocking cache. We found early on that instruction fetch bandwidth was a critical performance bottleneck. To mitigate this problem, we implemented a limited variant of the collapsing buffer described in <ref> [4] </ref>. Our implementation supports two predictions per cycle within the same instruction cache block, which provides significantly more instruction fetch bandwidth and better pipeline resource utilization.
Reference: [5] <author> Peter Dahl and Matthew O'Keefe. </author> <title> Reducing memory traffic with cregs. </title> <booktitle> In Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 100-111, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Austin and Sohi [2] employed a simple, fast address calculation early in the pipeline to effectively hide the memory latency. This was achieved by targeting the simple base+offset addressing modes used in references to global and stack data. Dahl and O'Keefe <ref> [5] </ref> incorporated address bits associated with each register to provide a hardware mechanism to disambiguate memory references dynamically.
Reference: [6] <author> G. Kane and J. Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: The Fortran codes were first converted to C using AT&T F2C version 1994.09.27. All experiments were performed on an extended version of the SimpleScalar [3] tool set. The tool set employs the SimpleScalar instruction set, which is a (virtual) MIPS-like architecture <ref> [6] </ref>. Table 1: Benchmark Application Descriptions Bench- Instr Loads Value Addr Prod mark (Mil.) (Mil.) Locality Loc.
Reference: [7] <author> Robert Keller. </author> <title> Look-ahead processors. </title> <journal> In ACM Computing Surveys, </journal> <pages> pages 177-195, </pages> <month> December </month> <year> 1975. </year>
Reference-contexts: A new mechanism is then employed which uses an identifier associated with the store-load pair to address the value, bypassing the normal addressing mechanism. The term memory renaming comes from the similarity this approach has to the abstraction of operand specifiers performed in register renaming. <ref> [7] </ref>. In this paper, we will examine the characteristics of the memory reference stream and propose a novel architectural modification to the pipeline to enable speculative execution of load instructions early in the pipeline (before address calculation).
Reference: [8] <author> Mikko H. Lipasti, Christopher B. Wilkerson, and John Paul Shen. </author> <title> Value locality and load value prediction. </title> <booktitle> In Proceedings of the 17th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: This allowed the compiler to be more aggressive in placing frequently referenced data in the register file (even when aliasing may be present), which can dramatically reduce the number of memory operations that must be executed. Lipasti and Shen <ref> [8] </ref> described a mechanism in which the value of a load instruction is predicted based on the previous values loaded by that instruction. <p> The predictor works quite well, predicting correctly as many as 76% of the program's memory dependencies anaverage of 62% for all the programs. Unlike many of the value predictor mechanisms <ref> [8] </ref>, dependence predictors work well, even better, on floating point programs. To better understand where the dependence predictor was finding its dependence locality, we broke down the correct predictions by the segment in which the reference data resided.
Reference: [9] <author> Andreas Moshovos, Scott Breach, T. N. Vijaykumar, and Gurindar Sohi. </author> <title> Dynamic speculation and synchronization of data dependences. </title> <booktitle> In 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 181-193, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: This enables load instructions to proceed speculatively without their address operands when effective address computation for a particular load instruction remains constant (as in global variable references). Finally, Moshovos, Breach, Vijaykumar and Sohi <ref> [9] </ref> used a memory reorder buffer incorporating data dependence speculation. Data dependence speculation allows load instructions to bypass preceding stores before ambiguous dependencies are resolved; this greatly increases the flexibility of the dynamic instruction scheduling to find memory instruction ready to execute.
Reference: [10] <author> Yiannakis Sazeidis, Stamatis Vassiliadis, and James Smith. </author> <title> The performance potential of data dependence speculation and collapsing. </title> <booktitle> In Proceedings of the 29th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 238-247, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: Using this approach they were able to achieve a speedup in execution of between 3% (for a simple implementation) to 16% (with infinite resources and perfect prediction). Sazeides, Vassiliadis and Smith <ref> [10] </ref> used address speculation on load instructions to remove the dependency caused by the calculation of the effective address. This enables load instructions to proceed speculatively without their address operands when effective address computation for a particular load instruction remains constant (as in global variable references).
Reference: [11] <author> Tse-Yu Yeh and Yale Patt. </author> <title> Two-level adaptive branch prediction. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 51-61, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: holds for global ac Fetch Interface fetches any 4 instructions in up to two cache block per cycle, separated by at most two branches Instruction Cache 32k 2-way set-associative, 32 byte blocks, 6 cycle miss latency Branch Predictor 8 bit global history indexing a 4096 entry pattern history table (GAp <ref> [11] </ref>) with 2-bit saturating counters, 8 cycle mis-prediction penalty Out-of-Order Issue out-of-order issue of up to 8 operations per cycle, 256 entry re-order buffer, 128 entry Mechanism load/store queue, loads may execute when all prior store addresses are known Architected Registers 32 integer, 32 floating point Functional Units 8-integer ALU, 4-load/store
References-found: 11

