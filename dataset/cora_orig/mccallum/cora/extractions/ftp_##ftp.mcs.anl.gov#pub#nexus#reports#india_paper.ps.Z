URL: ftp://ftp.mcs.anl.gov/pub/nexus/reports/india_paper.ps.Z
Refering-URL: http://www.cs.washington.edu/education/courses/590o/
Root-URL: 
Email: foster@mcs.anl.gov carl@compbio.caltech.edu tuecke@mcs.anl.gov  
Title: The Nexus Task-parallel Runtime System  
Author: Ian Foster Carl Kesselman Steven Tuecke 
Address: Argonne, IL 60439 Pasadena, CA 91125 Argonne, IL 60439  
Affiliation: Math Computer Science Beckman Institute Math Computer Science Argonne National Laboratory Caltech Argonne National Laboratory  
Abstract: A runtime system provides a parallel language compiler with an interface to the low-level facilities required to support interaction between concurrently executing program components. Nexus is a portable runtime system for task-parallel programming languages. Distinguishing features of Nexus include its support for multiple threads of control, dynamic processor acquisition, dynamic address space creation, a global memory model via interprocessor references, and asynchronous events. In addition, it supports heterogeneity at multiple levels, allowing a single computation to utilize different programming languages, executables, processors, and network protocols. Nexus is currently being used as a compiler target for two task-parallel languages: Fortran M and Composi- tional C ++ . In this paper, we present the Nexus design, outline techniques used to implement Nexus on parallel computers, show how it is used in compilers, and compare its performance with that of another runtime system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Peter Buhr and R. Stroobosscher. </author> <title> The system: Providing light-weight concurrency on sharedmemory multiprocessor systems running Unix. </title> <journal> Software Practice and Experience, </journal> <pages> pages 929964, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The drawback to this approach is that POSIX was designed as an application program interface, with features such as real-time scheduling support that may add overhead for parallel systems. A lower-level interface designed specifically as a compiler target would most likely result in better performance <ref> [1] </ref> and will be investigated in future research. To summarize, the mapping of computation to physical processors is determined by both the mapping of threads to contexts and the mapping of contexts to nodes. The relationship between nodes, contexts, and threads is illustrated in Fig. 1. Global Pointers.
Reference: [2] <author> R. Butler and E. Lusk. </author> <title> Monitors, message, and clusters: The p4 parallel programming system. </title> <note> Parallel Computing (to appear), </note> <year> 1994. </year>
Reference-contexts: P4 has several special multiprotocol implementations, such as a version for the Paragon that allows the nodes to use both NX and TCP <ref> [2] </ref>. But it does not allow arbitrary mixing of protocols. 3 Performance Studies In this section, we present results of some preliminary Nexus performance studies. We note that the thrust of our development effort to date has been to provide a correct implementation of Nexus.
Reference: [3] <author> K. Mani Chandy and Carl Kesselman. </author> <title> CC ++ : A declarative concurrent object oriented program-ming notation. In Research Directions in Object Oriented Programming. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Later in this paper, we present some preliminary performance results that address this question. As we describe in this paper, Nexus is already in use as a compiler target for two task-parallel languages: Fortran M [7] (FM) and Compositional C ++ <ref> [3] </ref> (CC ++ ). Our initial experiences have been gratifying in that the resulting compilers are considerably simpler than earlier prototypes that did not use Nexus services. Space does not permit a detailed discussion of related work.
Reference: [4] <editor> David Culler et al. </editor> <booktitle> Parallel programming in SplitC. In Proc. Supercomputing '93. ACM, </booktitle> <year> 1993. </year>
Reference-contexts: As an example, a handler that implements the get and put operations found in Split-C <ref> [4] </ref> can take advantage of this optimization. 2.2 Implementation In order to support heterogeneity, the Nexus implementation encapsulates thread and communication functions in thread and protocol modules, respectively, that implement a standard interface to low- level mechanisms (Fig. 2).
Reference: [5] <author> J. Dongarra, G. Geist, R. Manchek, and V. Sunderam. </author> <title> Integrated PVM framework supports het-erogeneous network computing. </title> <booktitle> In Computers in Physics, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: During a remote service request, data can be transferred between contexts by the use of a buffer. Data is inserted into a buffer and removed from a buffer through the use of packing and unpacking functions similar to those found in PVM and MPI <ref> [5, 6] </ref>. Invok- ing a remote service request is a three-step process: 1. The remote service request is initialized by providing a global pointer to an address in the destination context and the identifier for the handler in the remote context. A buffer is returned from the initialization operation. 2. <p> Although some existing message-passing systems support limited network heterogeneity, none do so with the same generality. For example, PVM3 allows processors in a parallel computer to communicate with external computers by sending messages to the pvmd daemon process which acts as a message forwarder <ref> [5] </ref>. However, this approach is not optimal on machines such as the IBM SP1 and the Intel Paragon, whose nodes are able to support TCP directly, and it limits PVM programs to using just one protocol in addition to TCP. <p> Watson Research Center to explore this possibility.) The experiments reported here compare the performance of a CC ++ program compiled to use Nexus and a similar C ++ program using PVM <ref> [5] </ref> for communication. The CC ++ program uses a function call through a CC ++ global pointer to transfer an array of double-precision floating-point numbers between two processor objects (Nexus contexts).
Reference: [6] <author> Message Passing Interface Forum. </author> <title> Document for a standard messge-passing interface, </title> <month> March </month> <year> 1994. </year> <note> (available from netlib). </note>
Reference-contexts: During a remote service request, data can be transferred between contexts by the use of a buffer. Data is inserted into a buffer and removed from a buffer through the use of packing and unpacking functions similar to those found in PVM and MPI <ref> [5, 6] </ref>. Invok- ing a remote service request is a three-step process: 1. The remote service request is initialized by providing a global pointer to an address in the destination context and the identifier for the handler in the remote context. A buffer is returned from the initialization operation. 2.
Reference: [7] <author> Ian Foster and K. Mani Chandy. </author> <title> Fortran M: A language for modular parallel programming. </title> <journal> J. Parallel and Distributed Computing, </journal> <note> 1994. to appear. </note>
Reference-contexts: Later in this paper, we present some preliminary performance results that address this question. As we describe in this paper, Nexus is already in use as a compiler target for two task-parallel languages: Fortran M <ref> [7] </ref> (FM) and Compositional C ++ [3] (CC ++ ). Our initial experiences have been gratifying in that the resulting compilers are considerably simpler than earlier prototypes that did not use Nexus services. Space does not permit a detailed discussion of related work.
Reference: [8] <author> Ian Foster, Carl Kesselman, Robert Olson, and Steve Tuecke. </author> <title> Nexus: An interoperability toolkit for parallel and distributed computer systems. </title> <type> Technical Report ANL/MCS-TM-189, </type> <institution> Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: We expect to examine these issues in future research. 2.1 Core Abstractions The Nexus interface is organized around five basic abstractions: nodes, contexts, threads, global pointers, and remote service requests. The associated services provide direct support for light-weight threading, address space management, communication, and synchronization <ref> [8] </ref>. A computation consists of a set of threads, each executing in an address space called a context. An individual thread executes a sequential program, which may read and write data shared with other threads executing in the same context.
Reference: [9] <author> M. Haines, D. Cronk, and P. Mehrotra. </author> <title> On the design of Chant: A talking threads package. </title> <type> Technical Report 94-25, </type> <institution> ICASE, </institution> <year> 1994. </year>
Reference-contexts: Our initial experiences have been gratifying in that the resulting compilers are considerably simpler than earlier prototypes that did not use Nexus services. Space does not permit a detailed discussion of related work. However, we note that the Chant system <ref> [9] </ref> has similar design goals (but adopts different solutions). 2 Nexus Design and Implementation Before describing the Nexus interface and implementation, we review the requirements and assumptions that motivated the Nexus design. Nexus is intended as a general-purpose runtime system for task-parallel languages.
Reference: [10] <author> IEEE. </author> <title> Threads extension for portable operating systems (draft 6), </title> <month> February </month> <year> 1992. </year>
Reference-contexts: Nexus provides a routine for creating threads within the context of the currently executing thread. The number of threads that can be created within a context is limited only by the resources available. The thread routines in Nexus are modeled after a subset of the POSIX thread specification <ref> [10] </ref>. The operations supported include thread creation, termination, and yielding the current thread. Mutexes and condition variables are also provided for synchronization between threads within a context.
Reference: [11] <author> Thorsten von Eicken, David Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active mes-sages: a mechanism for integrated communica-tion and computation. </title> <booktitle> In Proc. 19th Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: A remote service request is not a remote procedure call, because there is no acknowledgement or return value from the call, and the thread that initiated the request does not block. Remote service requests are similar in some respects to active messages <ref> [11] </ref>. They also differ in significant ways, however. Because active message handlers are designed to execute within an interrupt handler, there are restrictions on the ways in which they can modify the environment of a node. For example, they cannot call memory allocation routines.
References-found: 11

