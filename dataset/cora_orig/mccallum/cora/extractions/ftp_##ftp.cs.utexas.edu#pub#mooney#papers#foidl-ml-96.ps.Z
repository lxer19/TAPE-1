URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/foidl-ml-96.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: fmecaliff,mooneyg@cs.utexas.edu  
Title: Advantages of Decision Lists and Implicit Negatives in Inductive Logic Programming  
Author: Mary Elaine Califf and Raymond J. Mooney 
Keyword: inductive logic programming  
Date: January 20, 1996  
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas at Austin  
Abstract: This paper demonstrates the capabilities of Foidl, an inductive logic programming (ILP) system whose distinguishing characteristics are the ability to produce first-order decision lists, the use of an output completeness assumption to provide implicit negative examples, and the use of intensional background knowledge. The development of Foidl was originally motivated by the problem of learning to generate the past tense of English verbs; however, this paper demonstrates its superior performance on two different sets of benchmark ILP problems. Tests on the finite element mesh design problem show that Foidl's decision lists enable it to produce better results than all other ILP systems whose results on this problem have been reported. Tests with a selection of list-processing problems from Bratko's introductory Prolog text demonstrate that the combination of implicit negatives and intensionality allow Foidl to learn correct programs from far fewer examples than Foil.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., Ling, C. X., Matwin, S., & Lapointe, S. </author> <year> (1993). </year> <title> Learning singly recursive relations from small datasets. </title> <booktitle> In Workshop on Inductive Logic - IJCAI93, </booktitle> <pages> pp. 47-58. </pages>
Reference-contexts: However, these assumptions prevent its use on the tasks considered here because they typically involve non-functional background relations. Also, although Filp assumes that its target relation is function, the definitions learned consist of unordered sets of clauses. Other systems, most notably Lopster (Lapointe & Matwin, 1992) and Crustacean <ref> (Aha, Ling, Matwin, & Lapointe, 1993) </ref>, have addressed learning relations from very small sets of examples.
Reference: <author> Bergadano, F., & Gunetti, D. </author> <year> (1993). </year> <title> An interactive system to learn functional logic programs. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1044-1049 Chambery, France. </address>
Reference-contexts: clearly can exploit its ability to use implicit negative examples to learn these relations from far fewer positive examples than Foil requires (75 for member, 81 for del, 81 for insert, and 202 for sublist). 5 Related Work The first learning system to focus on learning functions only was Filp <ref> (Bergadano & Gunetti, 1993) </ref>. It assumes that the target and all background relations are functional and uses this 9 knowledge to limit its search for literals. However, these assumptions prevent its use on the tasks considered here because they typically involve non-functional background relations.
Reference: <author> Bergadano, F., Gunetti, D., & Trinchero, U. </author> <year> (1993). </year> <title> The difficulties of learning logic programs with cut. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 91-107. </pages>
Reference-contexts: clearly can exploit its ability to use implicit negative examples to learn these relations from far fewer positive examples than Foil requires (75 for member, 81 for del, 81 for insert, and 202 for sublist). 5 Related Work The first learning system to focus on learning functions only was Filp <ref> (Bergadano & Gunetti, 1993) </ref>. It assumes that the target and all background relations are functional and uses this 9 knowledge to limit its search for literals. However, these assumptions prevent its use on the tasks considered here because they typically involve non-functional background relations.
Reference: <author> Bratko, I. </author> <year> (1990). </year> <title> Prolog Programming for Artificial Intelligence. </title> <publisher> Addison Wesley, Reading:MA. </publisher>
Reference-contexts: These are a sequence of list-processing examples and exercises from Chapter 3 of <ref> (Bratko, 1990) </ref>. For each problem, the background provided consists of the relations encountered previously. In his experiments, Quinlan uses two universes: all lists on three elements of length up to three and all lists on four elements of length up to four.
Reference: <author> Cameron-Jones, R. M., & Quinlan, J. R. </author> <year> (1994). </year> <title> Efficient top-down induction of logic programs. </title> <journal> SIGART Bulletin, </journal> <volume> 5 (1), </volume> <pages> 33-42. </pages>
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-284. </pages> <note> 11 Cohen, </note> <author> W. W. </author> <year> (1993). </year> <title> Pac-learning a resticted class of recursive logic programs. </title> <booktitle> In Proceed--ings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 86-92 Wash-ington, D.C. </address>
Reference-contexts: In the original algorithm of Rivest (1987) and in CN2 <ref> (Clark & Niblett, 1989) </ref>, rules are learned in the order they appear in the final decision list (i.e. new rules are appended to the end of the list as they are learned).
Reference: <author> De Raedt, L., & Bruynooghe, M. </author> <year> (1993). </year> <title> A theory of clausal discovery. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1058-1063 Chambery, France. </address>
Reference-contexts: The non-monotonic semantics used to eliminate the need for negative examples in Claudien <ref> (De Raedt & Bruynooghe, 1993) </ref> has the same effect as an output completeness assumption in the case where all arguments of the target relation are outputs.
Reference: <author> Dolsak, B., & Muggleton, S. </author> <year> (1992). </year> <title> The application of inductive logic programming to finite-element mesh design. </title> <editor> In Muggleton, S. (Ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 453-472. </pages> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Lapointe, S., & Matwin, S. </author> <year> (1992). </year> <title> Sub-unification: A tool for efficient induction of recursive programs. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 273-281 Aberdeen, Scotland. </address>
Reference-contexts: However, these assumptions prevent its use on the tasks considered here because they typically involve non-functional background relations. Also, although Filp assumes that its target relation is function, the definitions learned consist of unordered sets of clauses. Other systems, most notably Lopster <ref> (Lapointe & Matwin, 1992) </ref> and Crustacean (Aha, Ling, Matwin, & Lapointe, 1993), have addressed learning relations from very small sets of examples.
Reference: <editor> Lavrac, N., & Dzeroski, S. (Eds.). </editor> <year> (1994). </year> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood. </publisher>
Reference-contexts: Foidl's results are slightly better than FFoil's overall, but not significantly so. Both systems also perform much better than results reported for mFoil and Golem (21% and 19% correct respectively) <ref> (Lavrac & Dzeroski, 1994) </ref>. 4.2 List-processing Programs For another test of Foidl's capabilities, we used a selection of the list-processing problems to which Quinlan and Cameron-Jones (1993) applied Foil. These are a sequence of list-processing examples and exercises from Chapter 3 of (Bratko, 1990).
Reference: <author> Mooney, R. J., & Califf, M. E. </author> <year> (1995). </year> <title> Induction of first-order decision lists: Results on learning the past tense of English verbs. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3, </volume> <pages> 1-24. </pages>
Reference-contexts: These characteristics allow Foidl to perform well on the past tense problem, bettering all previous results <ref> (Mooney & Califf, 1995) </ref>. In this paper, we present results that show that the Foidl algorithm is useful for more than the problem for which it was originally designed.
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1992). </year> <title> Efficient induction of logic programs. </title> <editor> In Muggleton, S. (Ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 281-297. </pages> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Quinlan, J. R. </author> <title> (submitted). Learning first-order definitions of functions. </title> <journal> Artificial Intelligence. </journal>
Reference: <author> Quinlan, J. R., & Cameron-Jones, R. M. </author> <year> (1993). </year> <title> FOIL: A midterm report. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pp. 3-20 Vienna. </pages>
Reference: <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 (3), </volume> <pages> 239-266. </pages>
Reference-contexts: Foidl is based on Foil <ref> (Quinlan, 1990) </ref> but has three distinguishing characteristics: 1) it uses intensional background knowledge; 2) it avoids the need for explicit negative examples by using an output completeness assumption to create implicit negatives; and 3) it is able to create first-order decision lists (ordered lists of clauses, each ending in a cut).
Reference: <author> Rivest, R. L. </author> . <year> (1987). </year> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2 (3), </volume> <pages> 229-246. </pages>
Reference-contexts: As described above, these are ordered sets of clauses each ending in a cut. When answering an output query, the cuts simply eliminate all but the first answer produced when trying the clauses in order. Therefore, this representation is similar to propositional decision lists <ref> (Rivest, 1987) </ref>, which are ordered lists of pairs (rules) of the form (t i ; c i ) where the test t i is a conjunction of features and c i is a category label and an example is assigned to the category of the first pair whose test it satisfies.
Reference: <author> Webb, G. I., & Brkic, N. </author> <year> (1993). </year> <title> Learning decision lists by prepending inferred rules. </title> <booktitle> In Proceedings of the Australian Workshop on Machine Learning and Hybrid Systems, </booktitle> <pages> pp. </pages> <address> 6-10 Melbourne, Australia. </address>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1994). </year> <title> Combining top-down and bottom-up methods in inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 343-351 New Brunswick, NJ. </address> <month> 12 </month>
References-found: 18

