URL: http://www-plateau.cs.berkeley.edu/ftp/pub/papers/bsmith-thesis.ps.gz
Refering-URL: http://www-plateau.cs.berkeley.edu/ftp/pub/papers/
Root-URL: http://www.cs.berkeley.edu
Title: Implementation Techniques for Continuous Media Systems and Applications  
Author: by Brian Christopher Smith 
Degree: A Dissertation Submitted in Partial Satisfaction of the Requirements for the Degree of Doctor of Philosophy in Computer Science in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA at Berkeley Committee in charge: Professor Lawrence A. Rowe (chair) Professor Carlo H. Sequin Professor Alice Agogino  
Date: 1994  
Address: Berkeley)1990  
Affiliation: A.B. (University of California at Berkeley)1986 M.S. (University of California at  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> A. Albanese, </author> <type> personal communication, </type> <year> 1994 </year>
Reference-contexts: (type and reference frames for each MPEG frame) Table 4-3: media-dependent header information Media-Independent Header Magic Number: CMT Clipfile Media Type: VIDEO Format: JPEG Version: 1.0 NumFrames: 80 FOT Address: 1024722 FTT Address: 1031890 Media-Dependent Header Width: 320 Height: 240 Q-factor: 75 Max Frame Size: 18624 Raw Data FOT FOT <ref> [1] </ref>: 12756 FOT [79]: 1007122 FTT FTT [1]: 0.0416 FTT [80]: 3.3333 128 1024722 Byte Address 74 formats, a single sample is too small a unit for efficient transmission (e.g., 1-2 byte samples for audio). In such cases, larger groups of samples may be used as frames. <p> frame) Table 4-3: media-dependent header information Media-Independent Header Magic Number: CMT Clipfile Media Type: VIDEO Format: JPEG Version: 1.0 NumFrames: 80 FOT Address: 1024722 FTT Address: 1031890 Media-Dependent Header Width: 320 Height: 240 Q-factor: 75 Max Frame Size: 18624 Raw Data FOT FOT <ref> [1] </ref>: 12756 FOT [79]: 1007122 FTT FTT [1]: 0.0416 FTT [80]: 3.3333 128 1024722 Byte Address 74 formats, a single sample is too small a unit for efficient transmission (e.g., 1-2 byte samples for audio). In such cases, larger groups of samples may be used as frames.
Reference: [2] <author> A. Albanese, J. Blomer, J. Edmonds, M. Luby, M. Sudan, </author> <title> Priority Encoding Transmission, </title> <booktitle> Symposium on Foundations of Computer Science, </booktitle> <month> October, </month> <year> 1994. </year>
Reference-contexts: Yavatkar and Manoj study two FEC strategies (XOR and replication) in a mutlicasting simulation study [85]. The Priority Encoded Transmission (PET) project <ref> [2] </ref>, implements FEC in a particularly novel way. Each frame in a group of frames is assigned a numerical priority in the range (0..1], with smaller values yielding higher priorities. The group is then encoded into packets that are sent to the destination. <p> Systems that use no ow control typically use FEC or assume that ow control is provided by the network layer. For example, the Starlight Network Server [77] assumes that sufficient network bandwidth is available and the PET project <ref> [2] </ref> treats the network as a fundamentally lossy transmission medium where ow control is not necessary. The Tenet protocol [22] suite falls into this class, allowing applications to reserve bandwidth before they initiate transmission. Cyclic-UDP falls into category (2), using measurements to estimate the avail 151 able channel bandwidth.
Reference: [3] <author> D. P. Anderson, George Homsy, </author> <title> A Continuous Media I/O Server and Its Synchronization Mechanism, </title> <booktitle> IEEE Computer, 1991 Oct, </booktitle> <volume> Vol. 24 Num. 10, </volume> <pages> pp. 51-57. </pages>
Reference: [4] <author> F. Arman, A. Hsu, M. Y. Chiu, </author> <title> Image processing on compressed data for large video databases. </title> <booktitle> Proceedings of ACM Multimedia 93, </booktitle> <address> (Anaheim, CA, </address> <month> August 1-6, </month> <year> 1993). </year> <institution> Association for Computing Machinery Press, </institution> <address> New York, </address> <note> 1993 pp. 267-72. </note>
Reference-contexts: Their technique is applicable to high and low pass filtering, but does not adapt well to the block by block encoding nature of most compression technologies. 65 Lastly, Arman <ref> [4] </ref> has developed a technique for detecting scene breaks in motion JPEG video data that operates on the compressed representation. His technique compares two images using the dot product of two vectors formed from the DCT coefficients of a selected subset of corresponding block in the test images.
Reference: [5] <author> A. Banerjea, B. Mah, </author> <title> The Real-Time Channel Administration Protocol, Network and Operating System Support for Digital Audio and Video: </title> <booktitle> Second International Workshop, </booktitle> <address> (Heidelberg, Germany, </address> <month> November, </month> <title> 1992), </title> <publisher> Springer-Verlag, </publisher> <address> Berlin; New York. </address> <pages> pp. 160-170 </pages>
Reference-contexts: The Tenet research group [22] has defined a protocol suite that provides statistical guarantees on network performance to implement strategy (2). Clients reserve resources in advance at nodes along the route between source and destination using the Real-Time Channel Administration Protocol, RCAP <ref> [5] </ref>, specifying constraints on delay, jitter, and bandwidth. If the connection can be accommodated, it is accepted and a channel is established. Cyclic-UDP uses strategy (3) and with prioritization to retransmit the optimal packets.
Reference: [6] <author> J. Bates, </author> <title> Presentation Support for Distributed Multimedia Applications, </title> <type> Ph. D Thesis, </type> <institution> Computer Laboratory, University of Cambridge, </institution> <year> 1993. </year>
Reference: [7] <author> J. Bates, J. Bacon, </author> <title> A development platform for multimedia applications in a distributed, ATM network environment, </title> <booktitle> Proceedings of IEEE International Conference on Multimedia Computing and Systems, </booktitle> <address> Boston, MA, USA, May 15-19, </address> <publisher> 1994 IEEE Computer Society Press, Los Alamitos, CA, </publisher> <pages> pp. 154-163. </pages>
Reference: [8] <author> E. Biersack, </author> <title> Performance Evaluation Of Forward Error Correction in an ATM Environment, </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <month> 160 May </month> <year> 1993, </year> <title> Vol.11, </title> <journal> Num. </journal> <volume> 4, </volume> <pages> pp. 631-640. </pages>
Reference: [9] <author> J. Bolot, T. Turletti, </author> <title> Feedback Control of Video Codecs for a Packet Switched Network, Network and Operating System Support for Digital Audio and Video: </title> <booktitle> Fourth International Workshop, </booktitle> <address> Lancaster, UK, Novem-ber, 1993. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin; New York pp. </address> <pages> 13-15 </pages>
Reference-contexts: The second way cyclic-UDP differs from previous work is that it works with a variety of encoding schemes and has been tested in LAN, MAN, and WAN environments. Other best effort protocols reported in the literature are either untested 158 proposals <ref> [31, 9] </ref>, have been tested only in one environment [68, 20, 38, 19, 76], have been simulated, but not implemented [85, 31, 9], or use custom compression schemes [15]. Future Work Cyclic-UDP can be extended in two ways. <p> Other best effort protocols reported in the literature are either untested 158 proposals [31, 9], have been tested only in one environment [68, 20, 38, 19, 76], have been simulated, but not implemented <ref> [85, 31, 9] </ref>, or use custom compression schemes [15]. Future Work Cyclic-UDP can be extended in two ways. First, cyclic-UDP can be extended to support the lower latency communication required by video conferencing applications.
Reference: [10] <author> P. Brady, </author> <title> Effects of Transmission Delay on Conversational Behavior on Echo-Free Telephone Circuits, </title> <journal> The Bell System Technical Journal, </journal> <volume> Vol. 50, Num.1, </volume> <month> January, </month> <year> 1971, </year> <pages> pp. 115-134. </pages>
Reference-contexts: The CMT system uses this approach. Although buffering is suitable for most playback applications, this solution is not viable for many interesting classes of applications. For example, conferencing applications need low end-to-end latency and can only buffer a maximum of about 800 milliseconds worth of data <ref> [64, 41, 10] </ref>. This implies that the network delay can not exceed 800 milliseconds. Furthermore, in some playback applications the receiver may have extremely limited buffering capabilities due to cost constraints. In both these situations, delay and jitter must be bounded.
Reference: [11] <author> M. C. Buchanan, P. T. Zellweger, </author> <title> Automatic temporal layout mechanisms, </title> <booktitle> Proceedings of ACM Multimedia 93, </booktitle> <address> (Anaheim, CA, </address> <month> August 1-6, </month> <year> 1993). </year> <institution> Association for Computing Machinery Press, </institution> <address> New York, </address> <note> 1993 pp. 341-350 </note>
Reference: [12] <author> M. C. Buchanan, P. T. Zellweger, </author> <title> Scheduling multimedia documents using temporal constraints, Network and Operating System Support for Digital Audio and Video: </title> <booktitle> Third International Workshop, </booktitle> <address> (La Jolla, California, USA November 12-13, 1992). </address> <publisher> Springer-Verlag, </publisher> <address> Berlin; New York pp. </address> <pages> 237-249. </pages>
Reference: [13] <author> D. Bulterman, </author> <title> Synchronization of Multi-Sourced Multi-media Data for Heterogeneous Target Systems, Network and Operating System Support for Digital Audio and Video: </title> <booktitle> Third International Workshop, </booktitle> <address> (La Jolla, California, USA November 12-13, 1992). </address> <publisher> Springer-Verlag, </publisher> <address> Berlin; New York pp. </address> <pages> 110-120 </pages>
Reference-contexts: The language can be used to create multimedia, hyper-media, and computer supported cooperative work applications. Other systems are closer in goals to CMT. The Amsterdan Multimedia Frame B C Display Buttons Start 102 work (AMF) is one such system <ref> [13, 65, 30] </ref>. AMF defines a data model that provides a method for collecting data objects into multimedia documents called CMIFs. The CMIF format provides two views of multimedia data, the hierarchical view and the channel view.
Reference: [14] <author> Gordon Chaffee, </author> <type> personal communication, </type> <year> 1994 </year>
Reference: [15] <author> S. Chakrabafti, R. Wang, </author> <title> Adaptive Control for Packet Video, </title> <booktitle> Proceedings of the 1994 International Conference on Multimedia Computing and Systems, </booktitle> <address> Boston, Mass, </address> <month> May </month> <year> 1994, </year> <pages> pp. 56-62 </pages>
Reference-contexts: Other best effort protocols reported in the literature are either untested 158 proposals [31, 9], have been tested only in one environment [68, 20, 38, 19, 76], have been simulated, but not implemented [85, 31, 9], or use custom compression schemes <ref> [15] </ref>. Future Work Cyclic-UDP can be extended in two ways. First, cyclic-UDP can be extended to support the lower latency communication required by video conferencing applications. A method for implementing such an extension was proposed at the end of chapter 5, and will not be repeated here.
Reference: [16] <author> S. F. Chang, W. L. Chen, D. G. Messerschmitt, </author> <title> Video Compositing in the DCT Domain, </title> <booktitle> IEEE Workshop on Visual Signal Processing and Communi 161 cations, Rayleigh, </booktitle> <address> North Carolina, </address> <month> Sept. </month> <year> 1992 </year>
Reference: [17] <author> S. F. Chang and D. G. Messerschmitt, </author> <title> A New Approach to Decoding and Compositing Motion Compensated DCT-Based Images, </title> <booktitle> IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Minneapolis, </address> <publisher> Minnesota, </publisher> <pages> pp. 421-424, </pages> <month> April, </month> <year> 1993. </year>
Reference: [18] <author> B. Chitprasert, K. R. Rao, </author> <title> Discrete Cosine Transform Filtering, </title> <booktitle> Signal Processing, </booktitle> <volume> Vol. 19, Num.3, </volume> <pages> pp. 233-245, </pages> <month> March </month> <year> 1990 </year>
Reference-contexts: The results of sections 2.2 and 2.3 of the previous chapter show how two compressed images can be pixel-wise multiplied and composited, which provides a basis for using a channel techniques on compressed images. In the area of DCT domain image processing, Chitprasert and Rao <ref> [18] </ref> presented a convolution algorithm for the DCT, and showed how it could be used for image processing in certain special cases.
Reference: [19] <author> L. Delgrossi, C. Halstrick, et. al., </author> <title> Media Scaling for Audio Vidual Communication with the Heidelberg Transport System, </title> <booktitle> Proceedings of ACM Multimedia 93, </booktitle> <address> (Anaheim, CA, </address> <month> August 1-6, </month> <year> 1993). </year> <institution> Association for Computing Machinery Press, </institution> <address> New York, </address> <note> 1993 pp. 99-104 </note>
Reference-contexts: This strategy reduces the ow of video data in response to network congestion [38], dropping extra frames when access to the network is delayed. UDP+resends uses strategy (3) to reduce the rate at which frames are generated at the source. HeiTP <ref> [19] </ref> uses media specific protocols to compensate for network congestion in response to feedback messages from the destination. And in the simulation study by Yavatkar and Manoj [85], several strategies for ow control based on destination feedback messages are investigated. <p> Other best effort protocols reported in the literature are either untested 158 proposals [31, 9], have been tested only in one environment <ref> [68, 20, 38, 19, 76] </ref>, have been simulated, but not implemented [85, 31, 9], or use custom compression schemes [15]. Future Work Cyclic-UDP can be extended in two ways. First, cyclic-UDP can be extended to support the lower latency communication required by video conferencing applications.
Reference: [20] <author> A. Eleftheridas, S. Pejhan, D. Anastassiou, </author> <title> Algorithms and Performance Evaluation of the XPhone Multimedia Communication System, </title> <booktitle> Proceedings of ACM Multimedia 93, </booktitle> <address> (Anaheim, CA, </address> <month> August 1-6, </month> <year> 1993). </year> <institution> Association for Computing Machinery Press, </institution> <address> New York, </address> <note> 1993 pp. 311-320 </note>
Reference-contexts: TCP/IP [75] uses this paradigm; [31] reports efforts to extend TCP/IP to support multimedia applications by prioritizing packets in the routers, but this work is still embryonic. TCP/IP is the transport mechanism used in the Xphone system <ref> [20] </ref>, a video conferencing prototype being developed at Columbia. 150 The idea of strategy (4) is to send redundant information that allows lost data to be reconstructed provided a subset of the original information is sent, a technique called forward error correction (FEC) [8,72]. <p> And in the simulation study by Yavatkar and Manoj [85], several strategies for ow control based on destination feedback messages are investigated. TCP/IP is the only system I know of that uses window-based ow control. The Xphone system <ref> [20] </ref>, which sends audio and motion-JPEG data using TCP/IP, also uses window-based ow control by extension. <p> Furthermore, in some playback applications the receiver may have extremely limited buffering capabilities due to cost constraints. In both these situations, delay and jitter must be bounded. The Tenet protocol suite allows applications to specify delay and jitter bounds 152 at connection setup. In Xphone <ref> [20] </ref> and vat, the Internet audio conferencing tool, network jitter is absorbed at the application by dynamically adjusting the amount of data buffered at the destination and adjusting the delay between capture and display of data: if many samples are arriving after the playback point, more data is buffered and the <p> Other best effort protocols reported in the literature are either untested 158 proposals [31, 9], have been tested only in one environment <ref> [68, 20, 38, 19, 76] </ref>, have been simulated, but not implemented [85, 31, 9], or use custom compression schemes [15]. Future Work Cyclic-UDP can be extended in two ways. First, cyclic-UDP can be extended to support the lower latency communication required by video conferencing applications.
Reference: [21] <author> C. Federighi, L. A. Rowe, </author> <title> A Distributed Hierarchical Storage Manager for a Video-on-Demand System, </title> <booktitle> IS&T/SPIE Symposium on Electronic Imaging: Science & Technology, </booktitle> <address> San Jose, California, </address> <month> February, </month> <year> 1994. </year>
Reference-contexts: Striping has several advantages: 1. It allows large video sequences to be split into many small chunks for easier management. 2. It gives greater exibility when CMSs are used as caches for data stored on a tertiary archive <ref> [21, 47] </ref>. 3. It allows segments to be reused and shared, saving valuable disk storage. CMT is highly portable, since it is built using generic Unix operating system fa cilities, conventional networks (i.e., any network supporting TCP/IP and UDP/IP protocols), and off-the-shelf hardware. <p> It has been ported to most Unix systems, VMS, Windows 3.1, Chicago, UNICOS (the Cray operating system), and VxWorks. CMT has served as the basis for a Video-on-Demand System <ref> [21] </ref> and a video conferencing system [66]. Many 157 researchers have expressed interest in using CMT for their own research, and efforts are underway to distribute a new version of the toolkit. Future Work CMT has great promise, but poses many challenges.
Reference: [22] <author> D. Ferrari, A. Banerjea, H. Zhang, </author> <title> Network Support for Multimedia - A discussion of the Tenet Approach, </title> <type> Tech Report TR-92-072, </type> <institution> Internation Computer Science Institute, Berkeley, </institution> <address> CA, </address> <month> October, </month> <note> 1992; to appear in Computer Networks and ISDN Systems, special issue on Multimedia Networking, </note> <year> 1994. </year>
Reference-contexts: So when the network is overloaded by a CM application, CM data is delayed to the point where frames arrive so late as to be useless. Other researchers are developing real-time protocols to solve this problem <ref> [22] </ref>, but they require changes in OS kernels and network routers and gateways that are not yet available. Hence, I developed a best-effort protocol for CMT that utilizes the existing IP infrastructure. <p> When the destination receives a block, it updates the portion of the screen that contains the block. Nv also periodically resends background blocks, even if they have not changed recently, just in case the original was lost in transmission. The Tenet research group <ref> [22] </ref> has defined a protocol suite that provides statistical guarantees on network performance to implement strategy (2). Clients reserve resources in advance at nodes along the route between source and destination using the Real-Time Channel Administration Protocol, RCAP [5], specifying constraints on delay, jitter, and bandwidth. <p> For example, the Starlight Network Server [77] assumes that sufficient network bandwidth is available and the PET project [2] treats the network as a fundamentally lossy transmission medium where ow control is not necessary. The Tenet protocol <ref> [22] </ref> suite falls into this class, allowing applications to reserve bandwidth before they initiate transmission. Cyclic-UDP falls into category (2), using measurements to estimate the avail 151 able channel bandwidth. Jeffay and Stone allow multiple audio frames, but only a single video frame, to be queued at the sender.
Reference: [23] <editor> J. D. Foley, et. al., </editor> <booktitle> Computer Graphics: Principles and Practice, second edition, </booktitle> <address> Reading, Mass. </address> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference: [24] <author> J. D. Foley, A. Van Dam, </author> <title> Fundamentals of Interactive Computer Graphics, 162 Second Edition, </title> <publisher> Addison-Wesley Publishing Company. </publisher>
Reference: [25] <author> R. Frederick, </author> <title> Experiences with Real-time Software Video Compression, </title> <booktitle> Proceedings of the Packet Video Workshop (Portland, </booktitle> <address> Oregon, USA, Sep-tember 26-27, </address> <year> 1994) </year>
Reference-contexts: Jeffay and Stone [38] use this strategy in a system based on UDP, and the Heidelberg Transport System [33] uses this strategy, but on top of ST-II [77]. Nv <ref> [25] </ref>, a video conferencing tool popular on the Internet, uses strategy (1) in combination with a custom compression technique that is robust to packet loss. In this technique, the source captures an image and breaks it into small blocks.
Reference: [26] <editor> D. R. Fuhrmann, et. al, </editor> <title> Experimental Evaluation of Psychophysical Distortion Metrics for JPEG-Encoded Image, </title> <booktitle> Proceedings of the SPIE - The International Society for Optical Engineering,1993, Vol.1913, </booktitle> <pages> pp. 179-190 </pages>
Reference: [27] <author> T. A. Funkhouser, C. H. Sequin, </author> <title> Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments, </title> <booktitle> Proceeding of SIGGRAPH 93, </booktitle> <address> Anaheim, CA, USA, </address> <month> August 1-6, </month> <year> 1993, </year> <institution> Association for Computing Machinery, </institution> <address> New York, NY, </address> <year> 1993. </year> <pages> pp. 247-254. </pages>
Reference-contexts: This feature allows fractional frames to be retrieved, and media recorded at different or irregular frame rates to be combined. Irregular frame rates can occur in video digitized by a non real-time capture system (e.g., desktop video capture systems or real-time video generation systems such as <ref> [27] </ref>) or in media such as animation or slide show that may have no regular period for frame update. Other formats specify a single frame rate for the entire video sequence, which forces resampling of video recorded at different or irregular rates.
Reference: [28] <author> S. Gibbs, </author> <title> Application Construction and Component Design in an Object-Oriented Multimedia Framework, Network and Operating System Support for Digital Audio and Video: </title> <booktitle> Third International Workshop, </booktitle> <address> (La Jolla, Cali-fornia, USA November 12-13, 1992). </address> <publisher> Springer-Verlag, </publisher> <address> Berlin; New York pp. </address> <pages> 351-355 </pages>
Reference: [29] <author> T. Grogan, </author> <title> Image Quality Evaluation With A Contour-based Perceptual Model, </title> <booktitle> Proceedings of the SPIE - The International Society for Optical Engineering,1992, Vol.1666, </booktitle> <pages> pp. 188-197 </pages>
Reference: [30] <author> L. Hardman, D. Bulterman, G. Van Rossum, </author> <title> The Amsterdam Hypermedia Model: Extending Hypertext to Support Real Multimedia, Hypermedia, </title> <booktitle> 1993, Vol.5, Num.1, </booktitle> <pages> pp. 47-69. </pages>
Reference-contexts: The language can be used to create multimedia, hyper-media, and computer supported cooperative work applications. Other systems are closer in goals to CMT. The Amsterdan Multimedia Frame B C Display Buttons Start 102 work (AMF) is one such system <ref> [13, 65, 30] </ref>. AMF defines a data model that provides a method for collecting data objects into multimedia documents called CMIFs. The CMIF format provides two views of multimedia data, the hierarchical view and the channel view.
Reference: [31] <author> B. Heinrichs, R. Karabek, </author> <title> TCP/IP Supporting Real-Time Applications: The Predictive Delay Control Algorithm, Network and Operating System Support for Digital Audio and Video: </title> <booktitle> Second International Workshop, </booktitle> <address> (Heidel-berg, U.K., </address> <month> November, </month> <title> 1992), </title> <publisher> Springer-Verlag, </publisher> <address> Berlin; New York. </address> <pages> pp. 45-47 163 </pages>
Reference-contexts: Cyclic-UDP uses strategy (3) and with prioritization to retransmit the optimal packets. Surprisingly few other systems use this paradigm, mostly due to fears that the latency will be too large for use in conferencing applications. TCP/IP [75] uses this paradigm; <ref> [31] </ref> reports efforts to extend TCP/IP to support multimedia applications by prioritizing packets in the routers, but this work is still embryonic. <p> The second way cyclic-UDP differs from previous work is that it works with a variety of encoding schemes and has been tested in LAN, MAN, and WAN environments. Other best effort protocols reported in the literature are either untested 158 proposals <ref> [31, 9] </ref>, have been tested only in one environment [68, 20, 38, 19, 76], have been simulated, but not implemented [85, 31, 9], or use custom compression schemes [15]. Future Work Cyclic-UDP can be extended in two ways. <p> Other best effort protocols reported in the literature are either untested 158 proposals [31, 9], have been tested only in one environment [68, 20, 38, 19, 76], have been simulated, but not implemented <ref> [85, 31, 9] </ref>, or use custom compression schemes [15]. Future Work Cyclic-UDP can be extended in two ways. First, cyclic-UDP can be extended to support the lower latency communication required by video conferencing applications.
Reference: [32] <author> R. Herrtwich, </author> <title> The HeiProjects: An Updated Survey, </title> <type> Technical Report, </type> <institution> IBM European Networking Center, </institution> <address> Heidelberg, Germany October, </address> <year> 1992 </year>
Reference: [33] <author> R. Herrtwich, L. Delgrossi, </author> <title> Beyond ST-II: Fulfilling the Requirements of Multimedia Communication, Network and Operating System Support for Digital Audio and Video: </title> <booktitle> Third International Workshop, </booktitle> <address> La Jolla, California, USA November 12-13, 1992, </address> <publisher> Springer-Verlag, </publisher> <address> Berlin; New York pp. </address> <pages> 23-29 </pages>
Reference-contexts: Rather than recovering from errors, these systems initiate ow control when error rates exceed a threshold value in an attempt to reduce future errors. Jeffay and Stone [38] use this strategy in a system based on UDP, and the Heidelberg Transport System <ref> [33] </ref> uses this strategy, but on top of ST-II [77]. Nv [25], a video conferencing tool popular on the Internet, uses strategy (1) in combination with a custom compression technique that is robust to packet loss. In this technique, the source captures an image and breaks it into small blocks.
Reference: [34] <author> A. Hopper, </author> <title> Pandora - an experimental system for multimedia applications, </title> <institution> Olivetti Research Laboratory, </institution> <address> Trumpington St., Cambridge, UK, CB21QA, </address> <month> June, </month> <year> 1990. </year>
Reference: [35] <author> B. K. P. Horn, </author> <title> Robot Vision, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass, </address> <year> 1986 </year>
Reference: [36] <author> E. Israel, </author> <title> The X-Window system server: X version 11, release 5. </title> <publisher> Digital Press, </publisher> <address> Bedford, MA, </address> <year> 1992. </year>
Reference-contexts: Infrastructure CMT is an extension to the Tcl/Tk graphical user interface (GUI) toolkit. Tcl (a tool command language, pronounced tickle) provides an embedded interpreter for programs written in the C programming language]. Tk (pronounced tee kay) is Tcls GUI toolkit for the X window system <ref> [36] </ref>. Together, Tcl and Tk provide a ex-ible environment for building GUIs. Common user interface components, such as buttons, menu bars, and the like, are provided by Tk as commands in the Tcl language.
Reference: [37] <author> A. K. Jain, </author> <title> Fundamentals of Digital Image Processing, </title> <publisher> Prentice-Hall, Inc. </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989 </year>
Reference: [38] <author> K. Jeffay, D. L. Stone, T. Talley, and F. D. Smith, </author> <title> Adaptive, best-effort delivery of digital audio and video across packet switched networks, Network and Operating System Support for Digital Audio and Video: </title> <booktitle> Third International Workshop, </booktitle> <address> (La Jolla, California, USA, </address> <month> November 12-13, </month> <title> 1992), </title> <publisher> Springer-Verlag, </publisher> <address> Berlin; New York, </address> <pages> pp. 1-12, </pages>
Reference-contexts: Rather than recovering from errors, these systems initiate ow control when error rates exceed a threshold value in an attempt to reduce future errors. Jeffay and Stone <ref> [38] </ref> use this strategy in a system based on UDP, and the Heidelberg Transport System [33] uses this strategy, but on top of ST-II [77]. <p> Cyclic-UDP falls into category (2), using measurements to estimate the avail 151 able channel bandwidth. Jeffay and Stone allow multiple audio frames, but only a single video frame, to be queued at the sender. This strategy reduces the ow of video data in response to network congestion <ref> [38] </ref>, dropping extra frames when access to the network is delayed. UDP+resends uses strategy (3) to reduce the rate at which frames are generated at the source. HeiTP [19] uses media specific protocols to compensate for network congestion in response to feedback messages from the destination. <p> Other best effort protocols reported in the literature are either untested 158 proposals [31, 9], have been tested only in one environment <ref> [68, 20, 38, 19, 76] </ref>, have been simulated, but not implemented [85, 31, 9], or use custom compression schemes [15]. Future Work Cyclic-UDP can be extended in two ways. First, cyclic-UDP can be extended to support the lower latency communication required by video conferencing applications.
Reference: [39] <author> B. W. Kernighan, D. M. Ritchie, </author> <title> The C Programming Language, </title> <address> Engle-wood Cliffs, N.J., Prentice-Hall,1978. </address>
Reference-contexts: CMT currently includes support for software decoding of MPEG compressed video streams [44], Sun audio files, and hardware decoding of motion JPEG video streams [57,82] using the Parallax XVideo hardware [56]. The code is written in the C programming language <ref> [39] </ref> and an extended version of the Tcl/Tk language [54,55], which is described below. CMT is composed of approximately 3100 lines in Tcl and 130,000 lines of code in C, 90,000 of which implemented the Tcl/Tk language.
Reference: [40] <author> S. A. Klein, et. al. </author> <title> Relevance of Human VIsion to JPEG-DCT Compression, </title> <booktitle> Proceedings of the SPIE - The International Society for Optical Engineering,1992, Vol.1666, </booktitle> <pages> pp. 200-215 </pages>
Reference: [41] <author> E. Klemmer, </author> <title> Subject Evaluation of Transmission Delay in Telephone Conversations, </title> <journal> The Bell System Technical Journal, </journal> <volume> Vol. 46, July-August, </volume> <year> 1967, </year> <month> 164 </month>
Reference-contexts: The CMT system uses this approach. Although buffering is suitable for most playback applications, this solution is not viable for many interesting classes of applications. For example, conferencing applications need low end-to-end latency and can only buffer a maximum of about 800 milliseconds worth of data <ref> [64, 41, 10] </ref>. This implies that the network delay can not exceed 800 milliseconds. Furthermore, in some playback applications the receiver may have extremely limited buffering capabilities due to cost constraints. In both these situations, delay and jitter must be bounded.
Reference: [42] <author> S. G. Kochan et. al, </author> <title> UNIX Networking, Hayden Books, Carmel, </title> <booktitle> IN, </booktitle> <year> 1989 </year>
Reference: [43] <author> T. Lane, </author> <title> Independent JPEG Group Software Codec, Internet Software Distribution, </title> <address> URL ftp://ftp.uu.net/graphics/jpeg/jpegsrc.v4.tar.Z </address>
Reference: [44] <author> D. Le Gall, </author> <title> MPEG: a video compression standard for multimedia applications, </title> <journal> Communications of the ACM, April 1991, </journal> <volume> Vol. 34, Num.4, </volume> <pages> pp. 46-58. </pages>
Reference-contexts: A 352 by 240 MPEG video sequence sent across an eighteen hop Internet route from UC Berkeley to Cornell University was played at over 17 fps. These experiments are described in more detail in the next chapter. CMT currently includes support for software decoding of MPEG compressed video streams <ref> [44] </ref>, Sun audio files, and hardware decoding of motion JPEG video streams [57,82] using the Parallax XVideo hardware [56]. The code is written in the C programming language [39] and an extended version of the Tcl/Tk language [54,55], which is described below. <p> The clipfile/storyboard representation facilitates assembly and synchronization editing because the information needed to perform these edits is kept separate from the CM data. In other formats such as AVI from Microsoft [80,59], MPEG <ref> [44] </ref>, and the JMovie structure from Parallax [56], synchronization and assembly information is maintained by interleaving data within a single file. <p> A simple example will illustrate the advantages of media-specific network protocols. The MPEG video standard <ref> [44] </ref> uses differential coding between frames to achieve high compression ratios. MPEG defines three types of video frames: I-frames, P- frames, and B-frames.
Reference: [45] <author> J. S. Lim, </author> <title> Two-dimensional signal and image processing, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1990 </year>
Reference: [46] <author> T. D. C. Little, A. Ghafoor, </author> <title> Synchronization and Storage Models for Multimedia Objects, </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> Vol. 8, Num.3, </volume> <pages> pp. 413-427, </pages> <month> April </month> <year> 1990 </year>
Reference: [47] <author> T. D.C. Little, D. Venkatesh, </author> <title> Probabilistic Assignment of Movie to Storage Devices in a Video-On-Demand System, Network and Operating System Support for Digital Audio and Video: </title> <booktitle> Fourth International Workshop, </booktitle> <address> Lan-caster, UK, </address> <month> November, </month> <title> 1993. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin; New York pp. </address> <pages> 213-224 </pages>
Reference-contexts: Striping has several advantages: 1. It allows large video sequences to be split into many small chunks for easier management. 2. It gives greater exibility when CMSs are used as caches for data stored on a tertiary archive <ref> [21, 47] </ref>. 3. It allows segments to be reused and shared, saving valuable disk storage. CMT is highly portable, since it is built using generic Unix operating system fa cilities, conventional networks (i.e., any network supporting TCP/IP and UDP/IP protocols), and off-the-shelf hardware.
Reference: [48] <author> W. Mackay, G. Davenport, </author> <title> Virtual Video Editing in Interactive Multimedia Applications, </title> <journal> Communications of the ACM, July 1989, </journal> <volume> Vol. 32 Num. 7, </volume> <pages> pp. 802-810 </pages>
Reference: [49] <author> D. Mills, </author> <title> Measured performance of the network time protocol in the internet system, Network Working Group, </title> <note> RFC 1128 (October 1988). </note>
Reference-contexts: This section describes how distributed LTS and stream objects are used to control CMS and CMX. The clocks of all three processes are assumed to be synchronized. The current implementation uses NTP <ref> [49] </ref> for synchronization, but any synchronization protocol that keeps the clocks synchronized to within about 100 milliseconds would suffice. After creating the necessary LTS and stream objects, the Application distributes them to CMX using the Tcl-DP DOMS described above.
Reference: [50] <author> T. J. Mowbray, T. Brando, </author> <title> Interoperability and CORBA-based open systems, </title> <journal> Object Magazine, Sept.-Oct. 1993, </journal> <volume> Vol. 3, Num.3, </volume> <pages> pp. 50-54. </pages>
Reference: [51] <institution> Multimedia System Services, </institution> <note> Version 1.0, </note> <institution> Hewlett-Packard, IBM Corp., SunSoft, Inc. Response to Interactive Multimedia Association Request for 165 Technology, IMA Compatibility Project, </institution> <year> 1993 </year>
Reference-contexts: The LOS is analogous to the CMX and CMS processes in CMT. CMT has no analog of the GOS, since each application competes for the primary shared resource, network bandwidth, on a best-effort basis. The Interactive Multimedia Associations (IMA) Multimedia System Services (MSS) proposal <ref> [51] </ref>, defines a standard way to create and to control complex information ow graphs where nodes in the graph represent sources, sinks, and processors of multimedia data and edges in the graph represent CM connections between the nodes.
Reference: [52] <author> C. Nicolaou, </author> <title> An Architecture for Real-Time Multimedia Communication Systems, </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> Vol. 8, Num.3, </volume> <pages> pp. 391-400, </pages> <month> April </month> <year> 1990 </year>
Reference: [53] <author> N. B. Nill, </author> <title> A Visual Model Weighted Cosine Transform for Image Compression and Quality Assessment, </title> <journal> IEEE Transaction on Communication, </journal> <volume> Vol. 33, Num.6, </volume> <month> June </month> <year> 1985 </year>
Reference: [54] <author> J. Ousterhout, </author> <title> Tcl: An Embeddable Command Language, </title> <booktitle> Proc. of the 1990 USENIX Winter Conference, </booktitle> <month> January </month> <year> 1990, </year> <pages> pp. 133-146 </pages>
Reference: [55] <author> J. Ousterhout, </author> <title> An X11 Toolkit Based on the Tcl Language, </title> <booktitle> Proc. of the 1991 USENIX Winter Conference, </booktitle> <month> January </month> <year> 1991. </year>
Reference: [56] <institution> Parallax Video Software Developers Guide, Parallax Graphics, Inc. </institution> <address> Santa Clara, CA. </address> <year> 1993 </year>
Reference-contexts: These experiments are described in more detail in the next chapter. CMT currently includes support for software decoding of MPEG compressed video streams [44], Sun audio files, and hardware decoding of motion JPEG video streams [57,82] using the Parallax XVideo hardware <ref> [56] </ref>. The code is written in the C programming language [39] and an extended version of the Tcl/Tk language [54,55], which is described below. CMT is composed of approximately 3100 lines in Tcl and 130,000 lines of code in C, 90,000 of which implemented the Tcl/Tk language. <p> The clipfile/storyboard representation facilitates assembly and synchronization editing because the information needed to perform these edits is kept separate from the CM data. In other formats such as AVI from Microsoft [80,59], MPEG [44], and the JMovie structure from Parallax <ref> [56] </ref>, synchronization and assembly information is maintained by interleaving data within a single file. To perform assembly or synchronization editing, a new file must be produced for each edit, which can be a time consuming process owing to the large size of video data.
Reference: [57] <author> W. B. Pennebaker, </author> <title> JPEG still image data compression standard, </title> <publisher> Van Nos-trand Reinhold, </publisher> <address> New York, </address> <year> 1992. </year>
Reference: [58] <author> H. A. Peterson, </author> <title> An Improved Model for DCT Coefficient Quantization, </title> <booktitle> Proceedings of the SPIE - The International Society for Optical Engineering,1993, Vol.1913, </booktitle> <pages> pp. 191-201 </pages>
Reference: [59] <author> C. Petzold, </author> <title> Video for Windows brings interleaved audio and full-motion digital video to the PC, </title> <journal> Microsoft Systems Journal Vol. </journal> <volume> 8, Num.1, </volume> <month> Jan, </month> <year> 1993, </year> <pages> pp. 43-52. </pages>
Reference: [60] <institution> Pixie profiling software, Digital Equipment Corporation, </institution> <year> 1991. </year>
Reference: [61] <author> T. Porter and T. Duff, </author> <title> Compositing Digital Images, </title> <booktitle> SIGGRAPH 84 Proceedings, </booktitle> <volume> Vol. 18, </volume> <pages> pp. 253-259, </pages> <month> July </month> <year> 1984 </year>
Reference: [62] <author> W. Pratt, </author> <title> Multimedia System Services, Version 1.0, Interactive Multimedia 166 Association Compatibility Project. </title> <type> Technical Contact: Dr. William Pratt, </type> <institution> SunSoft Inc., Mountain View, </institution> <address> CA. william.pratt@sun.com </address>
Reference: [63] <author> K. R. Rao and P. Kip, </author> <title> Discrete Cosine Transform -- Algorithms, Advantages, Applications, </title> <publisher> Academic Press, Inc. </publisher> <address> London, </address> <year> 1990 </year>
Reference: [64] <author> R. Riesz, E. Klemmer, </author> <title> Subject Evaluation of Delay and Echo Suppressors in Telephone Communications, </title> <journal> The Bell System Technical Journal, </journal> <volume> Vol. 42, </volume> <month> November, </month> <year> 1963, </year> <pages> pp. 2919-2941. </pages>
Reference-contexts: The CMT system uses this approach. Although buffering is suitable for most playback applications, this solution is not viable for many interesting classes of applications. For example, conferencing applications need low end-to-end latency and can only buffer a maximum of about 800 milliseconds worth of data <ref> [64, 41, 10] </ref>. This implies that the network delay can not exceed 800 milliseconds. Furthermore, in some playback applications the receiver may have extremely limited buffering capabilities due to cost constraints. In both these situations, delay and jitter must be bounded.
Reference: [65] <author> G. Rossum, et. al., CMIFed: </author> <title> A Presentation Environment for Portable Hypermedia Documents, </title> <booktitle> Proceedings of ACM Multimedia 93, </booktitle> <address> (Anaheim, CA, </address> <month> August 1-6, </month> <year> 1993). </year> <institution> Association for Computing Machinery Press, </institution> <address> New York, </address> <note> 1993 pp. 183-188 </note>
Reference-contexts: The language can be used to create multimedia, hyper-media, and computer supported cooperative work applications. Other systems are closer in goals to CMT. The Amsterdan Multimedia Frame B C Display Buttons Start 102 work (AMF) is one such system <ref> [13, 65, 30] </ref>. AMF defines a data model that provides a method for collecting data objects into multimedia documents called CMIFs. The CMIF format provides two views of multimedia data, the hierarchical view and the channel view.
Reference: [66] <author> L. A. Rowe, </author> <type> personal communication, </type> <year> 1994 </year>
Reference-contexts: It has been ported to most Unix systems, VMS, Windows 3.1, Chicago, UNICOS (the Cray operating system), and VxWorks. CMT has served as the basis for a Video-on-Demand System [21] and a video conferencing system <ref> [66] </ref>. Many 157 researchers have expressed interest in using CMT for their own research, and efforts are underway to distribute a new version of the toolkit. Future Work CMT has great promise, but poses many challenges.
Reference: [67] <author> L. A. Rowe, K. Patel, B. C. Smith, </author> <title> MPEG video in software: representation, transmission and playback, </title> <booktitle> IS&T/SPIE Symposium on Electronic Imaging: Science & Technology, </booktitle> <address> San Jose, California, </address> <month> February, </month> <year> 1994. </year>
Reference-contexts: Details of this type of processing are presented elsewhere <ref> [67] </ref>. Scheduling of frames for playback is performed by converting the logical start and end times of the frame to system time using the LTS associated with the frames stream, and a high priority at-event is created to play the frame. <p> For example, a media-independent protocol might decide not to send an I-frame in an MPEG video stream, making the sequence undecodable up to the next I-frame. A better solution is to drop frames in dependency order. For example, an MPEG-specific protocol reported in <ref> [67] </ref> drops B-frames first, then P-frames, and finally the I-frame. More exotic MPEG-specific protocols could even transform P-frames to I-frames in the file server, possibly using compressed domain processing techniques based on the methods presented in chapter 3, if sufficient CPU resources are available. <p> Network frame drops typically occur during a congestion episode on the network. Server frame drops typically occur during software decoding because the CPU is not fast enough to play the stream at the full data rate or because the machine is executing a CPU-intensive process <ref> [67] </ref>. In either case, the CMX sends a backoff message to the CMS process telling it to reduce the rate of CM data transmission. Like the missing packet resend request, the backoff message is sent as a Tcl-DP RDO over the control channel.
Reference: [68] <author> L. A. Rowe, B. C. Smith, </author> <title> A Continuous Media Player, Network and Operating System Support for Digital Audio and Video: </title> <booktitle> Third International Workshop, </booktitle> <address> (La Jolla, California, USA November 12-13, 1992). </address> <publisher> Springer-Verlag, </publisher> <address> Berlin; New York pp. </address> <pages> 334-344 </pages>
Reference-contexts: A retransmission scheme that allows the lost packet to be resent will improve throughput, as shown by the next protocol. UDP+Resends UDP+resends was the first CM protocol used in CMT <ref> [68] </ref>. UDP+resends provides fragmentation, limited retransmission, and ow control. A CM Connection (CMC) is established between the CMS process and CMX process that consists of two channels: a data channel and a control channel. The data channel carries CM data and uses the UDP protocol. <p> Other best effort protocols reported in the literature are either untested 158 proposals [31, 9], have been tested only in one environment <ref> [68, 20, 38, 19, 76] </ref>, have been simulated, but not implemented [85, 31, 9], or use custom compression schemes [15]. Future Work Cyclic-UDP can be extended in two ways. First, cyclic-UDP can be extended to support the lower latency communication required by video conferencing applications.
Reference: [69] <institution> RTV Toolkit Software Guide, </institution> <note> Version 2.2, </note> <institution> Parallax Graphics, Inc. </institution> <address> Santa Clara, CA. </address> <year> 1993 </year>
Reference-contexts: Deinterlacing. NTSC video is transmitted as a sequence of fields (designated odd and even) that contain every other line on the screen. Some capture systems <ref> [69] </ref> grab and compress video as a sequence of fields, whereas most applications, systems, and hardware store, process, and transport video as a sequence of frames (the composite of two fields). Consequently, the captured video fields must be converted to frames, a slow, off-line process.
Reference: [70] <author> J. A. Saghri, et. al, </author> <title> Image Quality Measure Based on a Human Visual System Model, </title> <journal> Optical Engineering, </journal> <volume> Vol. 28, Num.7, </volume> <month> July </month> <year> 1989 </year>
Reference: [71] <author> R. Sedgewick, </author> <title> Algorithms, 2nd ed., 1988, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass </address>
Reference-contexts: The only exceptions are when the P- or B-frames use a reference frame not in the input array. The code in figure 5-11 initializes the dependents and refCount fields of the chainArray. The other values are initialized as specified in table 5-6. After initialization, a heap <ref> [71] </ref> that contains the eligible frames is built so that the item at the top of the heap is the next frame to send.
Reference: [72] <author> L. Shaw-Min, </author> <title> Forward Error Correction Codes for MPEG2 over ATM, </title> <journal> IEEE 167 Transactions on Circuits and Systems for Video Technology, April 1994, </journal> <volume> Vol. 4, Num.2, </volume> <pages> pp. 200-203 </pages>
Reference: [73] <author> E. E. Smith, J. D. Lehman, </author> <title> Interactive Video: Implications of the Literature for Science Education, </title> <journal> Journal of Computers in Mathematics and Science Teaching, </journal> <volume> Vol. 8, Num. 1, </volume> <month> Fall, </month> <year> 1988, </year> <pages> pp. 25-32 </pages>
Reference: [74] <author> R. Steinmetz, J. C. Fritzche, </author> <title> Abstractions for Continuous Media Programming, </title> <journal> Computer Communications, </journal> <volume> Vol. 15, Num.6, </volume> <month> July </month> <year> 1992, </year> <pages> pp. 396-402 </pages>
Reference-contexts: The literature is replete with requirements papers, language definitions, object-oriented extensions, data ow models, and sketchy design documents for toolkits, but few designs have been implemented. Steinmetz reviews some of the models <ref> [74] </ref>. For example, Bates and Bacon [6,7] have developed a language for describing a distributed multimedia presentation in terms of named pipelines that specify the connectivity of objects. The language also allows the binding of user-defined events in the CM data stream (e.g., frame 52 plays) with actions.
Reference: [75] <author> W. Stevens, </author> <title> TCP/IP Illustrated, </title> <publisher> Addison-Wesley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: CMT plays synchronized audio and video. Synchronization is maintained even on unreliable networks with significant network contention. The system uses two custom designed connection protocols: 1) a control connection based on TCP/IP and 2) a data transmission connection based on UDP/IP <ref> [75] </ref>. The protocol dynamically adjusts the video frames per second (fps) sent by CMS to achieve the highest perceived quality of playback given the available network and computing resources. Experiments show that the system performs well on both local area networks (LANs) and wide area networks (WANs). <p> Protocols that provide this service are called best effort protocols. One might wonder why I developed a special protocol for CM data delivery. Indeed, some research systems [31,20] use TCP/IP <ref> [75] </ref> for communication, including early versions of CMT. Experience with this implementation revealed that CM applications based on TCP are unstable with respect to the real-time bandwidth required by a media stream. When the required bandwidth is well below the available bandwidth, TCP connections perform well. <p> EstBW is adaptively computed by the protocol using measurements made by the CMX process. The values measured are listed in table 5-3. The computation of meanDelay and delayDeviation are similar to what is used in TCP <ref> [75] </ref>. The only difference is that TCP estimates round trip delay whereas cyclic-UDP measures end-to-end delay. I use the following scheme to estimate end-to-end delay. When each packet is transmitted, the sendTime field of the packet is set using the value on the senders system clock. <p> The minimum value of this difference in recent history is assumed to be the skew on the system clock. The skew is subtracted from the difference to get the measured delay, M. MeanDelay and delayDeviation are computed from M using the following formulas (taken directly from <ref> [75] </ref>): EQ 5-2 EQ 5-4 Loss and recvdBW are computed over a period known as an epoch which is typically 100-200 milliseconds. During each epoch, the source tracks how many bytes it transmits and the destination tracks how many bytes it receives (bytes-Recvd). <p> Cyclic-UDP uses strategy (3) and with prioritization to retransmit the optimal packets. Surprisingly few other systems use this paradigm, mostly due to fears that the latency will be too large for use in conferencing applications. TCP/IP <ref> [75] </ref> uses this paradigm; [31] reports efforts to extend TCP/IP to support multimedia applications by prioritizing packets in the routers, but this work is still embryonic.
Reference: [76] <author> D. Stone, K. Jeffay, </author> <title> Queue Monitoring: A Delay Jitter Management Policy, Network and Operating System Support for Digital Audio and Video: </title> <booktitle> Fourth International Workshop, </booktitle> <address> Lancaster, UK November,1993. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin; New York pp. </address> <pages> 151-162 </pages>
Reference-contexts: Other best effort protocols reported in the literature are either untested 158 proposals [31, 9], have been tested only in one environment <ref> [68, 20, 38, 19, 76] </ref>, have been simulated, but not implemented [85, 31, 9], or use custom compression schemes [15]. Future Work Cyclic-UDP can be extended in two ways. First, cyclic-UDP can be extended to support the lower latency communication required by video conferencing applications.
Reference: [77] <author> F. Tobagi, J. Pang, </author> <title> Starworks *TM -- A Video Applications Server, </title> <booktitle> Proceedings of IEEE Compcon 93 (San Francisco, </booktitle> <address> California, USA, </address> <month> February, </month> <year> 1993). </year>
Reference-contexts: Jeffay and Stone [38] use this strategy in a system based on UDP, and the Heidelberg Transport System [33] uses this strategy, but on top of ST-II <ref> [77] </ref>. Nv [25], a video conferencing tool popular on the Internet, uses strategy (1) in combination with a custom compression technique that is robust to packet loss. In this technique, the source captures an image and breaks it into small blocks. <p> Systems that use no ow control typically use FEC or assume that ow control is provided by the network layer. For example, the Starlight Network Server <ref> [77] </ref> assumes that sufficient network bandwidth is available and the PET project [2] treats the network as a fundamentally lossy transmission medium where ow control is not necessary. The Tenet protocol [22] suite falls into this class, allowing applications to reserve bandwidth before they initiate transmission.
Reference: [78] <author> C. Topolic, </author> <title> Experimental Internet Stream Protocol, Version 2 (ST-II); RFC-1190, Internet Request for Comments, Num. </title> <type> 1190, </type> <institution> Network Information Center, </institution> <month> October </month> <year> 1990 </year>
Reference: [79] <author> D. Verma, H. Zhang, </author> <title> Design documents for RTIP/RMTP, </title> <note> unpublished manuscript (1991). </note>
Reference-contexts: frames for each MPEG frame) Table 4-3: media-dependent header information Media-Independent Header Magic Number: CMT Clipfile Media Type: VIDEO Format: JPEG Version: 1.0 NumFrames: 80 FOT Address: 1024722 FTT Address: 1031890 Media-Dependent Header Width: 320 Height: 240 Q-factor: 75 Max Frame Size: 18624 Raw Data FOT FOT [1]: 12756 FOT <ref> [79] </ref>: 1007122 FTT FTT [1]: 0.0416 FTT [80]: 3.3333 128 1024722 Byte Address 74 formats, a single sample is too small a unit for efficient transmission (e.g., 1-2 byte samples for audio). In such cases, larger groups of samples may be used as frames.
Reference: [80] <author> R. V. Volkman, </author> <title> The digital video interface for Windows multimedia, </title> <journal> Windows-DOS Developers Journal, </journal> <volume> Vol. 3, Num.9, </volume> <month> Sept., </month> <year> 1992, </year> <pages> pp. 5-14 </pages>
Reference-contexts: media-dependent header information Media-Independent Header Magic Number: CMT Clipfile Media Type: VIDEO Format: JPEG Version: 1.0 NumFrames: 80 FOT Address: 1024722 FTT Address: 1031890 Media-Dependent Header Width: 320 Height: 240 Q-factor: 75 Max Frame Size: 18624 Raw Data FOT FOT [1]: 12756 FOT [79]: 1007122 FTT FTT [1]: 0.0416 FTT <ref> [80] </ref>: 3.3333 128 1024722 Byte Address 74 formats, a single sample is too small a unit for efficient transmission (e.g., 1-2 byte samples for audio). In such cases, larger groups of samples may be used as frames.
Reference: [81] <author> S. Voran, </author> <title> The Development of Objective Video Quality Measures that Emulate Human Perception, </title> <booktitle> Proceedings of IEEE Global Telecommunications Conference: GLOBECOM 91, </booktitle> <address> Phoenix, AZ, </address> <month> December </month> <year> 1991 </year> <month> 168 </month>
Reference: [82] <author> G. K. Wallace, </author> <title> The JPEG Still Picture Compression Standard, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 34, Num. 4, </volume> <pages> pp. 30-44, </pages> <month> April </month> <year> 1991. </year>
Reference: [83] <author> A. B. Watson, </author> <title> DCT Quantization Matrices Optimized for Individual Images, </title> <booktitle> Proceedings of the SPIE - The International Society for Optical Engineering,1993, Vol.1913, </booktitle> <pages> pp. 202-216 </pages>
Reference: [84] <author> A. Watson, </author> <title> OMG (Object Management Group) architecture and CORBA (common object request broker architecture) specification, </title> <booktitle> IEE Colloquium on Distributed Object Management, Digest Num.1994/007, </booktitle> <address> London, UK, </address> <month> Jan. </month> <year> 1994, </year> <pages> pp. 4/1. </pages>
Reference: [85] <author> R. Yavatkar, L. Manoj, </author> <title> Optimistic Strategies for Large Scale Dissemination of Multimedia Information, </title> <booktitle> Proceedings of ACM Multimedia 93, </booktitle> <address> (Anaheim, CA, </address> <month> August 1-6, </month> <year> 1993). </year> <institution> Association for Computing Machinery Press, </institution> <address> New York, </address> <year> 1993, </year> <pages> pp. 13-20 </pages>
Reference-contexts: Yavatkar and Manoj study two FEC strategies (XOR and replication) in a mutlicasting simulation study <ref> [85] </ref>. The Priority Encoded Transmission (PET) project [2], implements FEC in a particularly novel way. Each frame in a group of frames is assigned a numerical priority in the range (0..1], with smaller values yielding higher priorities. The group is then encoded into packets that are sent to the destination. <p> UDP+resends uses strategy (3) to reduce the rate at which frames are generated at the source. HeiTP [19] uses media specific protocols to compensate for network congestion in response to feedback messages from the destination. And in the simulation study by Yavatkar and Manoj <ref> [85] </ref>, several strategies for ow control based on destination feedback messages are investigated. TCP/IP is the only system I know of that uses window-based ow control. The Xphone system [20], which sends audio and motion-JPEG data using TCP/IP, also uses window-based ow control by extension. <p> Other best effort protocols reported in the literature are either untested 158 proposals [31, 9], have been tested only in one environment [68, 20, 38, 19, 76], have been simulated, but not implemented <ref> [85, 31, 9] </ref>, or use custom compression schemes [15]. Future Work Cyclic-UDP can be extended in two ways. First, cyclic-UDP can be extended to support the lower latency communication required by video conferencing applications.
Reference: [86] <author> C. Zetzsche, </author> <title> Principal Features of Human Vision in the Context of Image Quality Models, </title> <booktitle> Third International Conference on Image Processing and its Applications, Warwick, </booktitle> <address> UK, </address> <month> July </month> <year> 1989. </year>
Reference: [87] <author> J. Ziv, A. Lempel, </author> <title> A universal algorithm for sequential data compression IEEE Transactions on Information Theory, </title> <month> May </month> <year> 1977, </year> <title> Vol. </title> <journal> IT-23, Num. </journal> <volume> 3, </volume> <pages> pp. 337-343. 169 </pages>
References-found: 87

