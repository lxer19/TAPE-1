URL: http://www.cs.wisc.edu/~shoup/papers/factorimpl.ps.Z
Refering-URL: http://www.cs.wisc.edu/~shoup/papers/
Root-URL: 
Email: shoup@bellcore.com  
Title: A New Polynomial Factorization Algorithm and its Implementation  
Author: Victor Shoup 
Address: 445 South St., Morristown, NJ 07960  
Affiliation: Bellcore,  
Abstract: We consider the problem of factoring univariate polynomials over a finite field. We demonstrate that the new baby step/giant step factoring method, recently developed by Kaltofen & Shoup, can be made into a very practical algorithm. We describe an implementation of this algorithm, and present the results of empirical tests comparing this new algorithm with others. When factoring polynomials modulo large primes, the algorithm allows much larger polynomials to be factored using a reasonable amount of time and space than was previously possible. For example, this new software has been used to factor a "generic" polynomial of degree 2048 modulo a 2048-bit prime in under 12 days on a Sun SPARC-station 10, using 68 MB of main memory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: We shall assume that multiplication of two degree d polynomials uses O (M (d)) scalar operations, where M (d) = d log d loglog d. This running-time bound is attained using the Fast Fourier Transform (FFT) <ref> [1] </ref>, and (as we shall later see) is quite realistic in practice. To measure the space complexity of factoring algorithms, we will count the number of scalars (elements of F p ) that need to be stored. <p> Using an algorithm of Yun [23], the square-free factorization stage can be accomplished essentially in the time to compute a GCD. Using the recursive "Half-GCD" method (see <ref> [1, Chapter 8] </ref>), this takes O (M (n) log n) scalar operations. The space requirement is O (n) scalars. We do not discuss this any further in this paper. <p> Using standard techniques (see <ref> [1, Chapter 8] </ref>) this can be done as follows. Let h = x n f (x 1 ) (the "reverse" of f ). <p> T 2 T 2 + T 1 Just as the pre-conditioned modular-multiplication algorithm, this algorithm uses 2n RED-steps, 2n CRT-steps, and 3 2 k k FFT-steps. 7.6 Computing GCD's For large n, we use an asymptotically fast GCD algorithm, based on the "Half-GCD" algorithm in <ref> [1, Chapter 8] </ref>. <p> For such machines, different software techniques should be used. In the default version of LIP, numbers are represented as vectors of type long (which are 32-bit integers) with radix R = 2 30 ; that is, the vector A [0]; A <ref> [1] </ref>; : : :; A [n 1], where each A [i] is between 0 and R 1, represents the integer A [0] + A [1] R + + A [n 1] R n1 : A fundamental problem that arises in integer multiplication and Chinese remaindering is to compute A A + <p> numbers are represented as vectors of type long (which are 32-bit integers) with radix R = 2 30 ; that is, the vector A [0]; A <ref> [1] </ref>; : : :; A [n 1], where each A [i] is between 0 and R 1, represents the integer A [0] + A [1] R + + A [n 1] R n1 : A fundamental problem that arises in integer multiplication and Chinese remaindering is to compute A A + B s, where A and B are multi-precision integers and s is a single-precision integer (i.e., in the range 0 through R 1). <p> First, we add the 22 union d_or_rep doubled; long rep [2]; -; const long mask = (1 &lt;< 26)-1; const double offset = 4503599627370496.0; // 2^52 inline void extract (long& hi, long& lo, double x) - y.d = x; hi = ((y.rep [0] &lt;< 6) | (((unsigned long) y.rep <ref> [1] </ref>) &gt;> 26)) & mask; lo = y.rep [1] & mask; - inline void add (long& hi, long& lo, long a) lo= lo + a; hi = hi + (lo &gt;> 26); lo = lo & mask; - inline void AddMul0 (long& hi, long& lo, long a, long b, long c, <p> doubled; long rep [2]; -; const long mask = (1 &lt;< 26)-1; const double offset = 4503599627370496.0; // 2^52 inline void extract (long& hi, long& lo, double x) - y.d = x; hi = ((y.rep [0] &lt;< 6) | (((unsigned long) y.rep <ref> [1] </ref>) &gt;> 26)) & mask; lo = y.rep [1] & mask; - inline void add (long& hi, long& lo, long a) lo= lo + a; hi = hi + (lo &gt;> 26); lo = lo & mask; - inline void AddMul0 (long& hi, long& lo, long a, long b, long c, long d) - double x = double (a)*double <p> This version of AddMul0 uses just one floating-point multiply, as opposed to two floating-point multiplies and one integer multiply in the default version. Depending on the "endian"-ness of the machine, the roles of y.rep [0] and y.rep <ref> [1] </ref> in extract may have to be reversed, which is accomplished with a compile-time flag. Many machines (especially modern RISC processors, such as SPARC, MIPS, and RS/6000) have separate floating-point and integer units, allowing floating-point and integer operations to run in parallel.
Reference: [2] <author> E. R. Berlekamp. </author> <title> Factoring polynomials over large finite fields. </title> <journal> Math. Comp., </journal> <volume> 24(111) </volume> <pages> 713-735, </pages> <year> 1970. </year>
Reference-contexts: To measure the space complexity of factoring algorithms, we will count the number of scalars (elements of F p ) that need to be stored. Berlekamp's null-space method <ref> [2] </ref> reduces the factoring problem to that of finding elements in the null space of a certain linear map defined on a vector space of dimension n over F p . <p> This allows us to directly compute a 52-bit product in floating point. The only tricky part is efficiently extracting the low-order and high-order 26 bits of this product. We use the following method. First, we add the 22 union d_or_rep doubled; long rep <ref> [2] </ref>; -; const long mask = (1 &lt;< 26)-1; const double offset = 4503599627370496.0; // 2^52 inline void extract (long& hi, long& lo, double x) - y.d = x; hi = ((y.rep [0] &lt;< 6) | (((unsigned long) y.rep [1]) &gt;> 26)) & mask; lo = y.rep [1] & mask; -
Reference: [3] <author> W. Bosma, J. Cannon, C. Playoust, and A. Steel. </author> <title> Solving problems with MAGMA. </title> <type> Preprint, </type> <year> 1994. </year>
Reference-contexts: Also, the MAGMA computer algebra system was used to factor the n = 300 benchmark <ref> [3] </ref>. This was done using an implementation of the null-space method on a Sun MP670, which is somewhat faster than our SPARC-station ELC. The total running time was 110 hours.
Reference: [4] <author> R. P. Brent and H. T. Kung. </author> <title> Fast algorithms for manipulating formal power series. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 25 </volume> <pages> 581-595, </pages> <year> 1978. </year>
Reference-contexts: Moreover, note that f and h remain fixed throughout many instances of the problem, which we can use to our advantage. To solve the modular-composition problem, we use the method of Brent & Kung <ref> [4] </ref>, which is itself a baby step/giant step technique. We first choose a parameter t (1 t n), and build a table of powers h i mod f for 0 i t using t multiplications by h modulo f .
Reference: [5] <author> D. G. Cantor and H. Zassenhaus. </author> <title> A new algorithm for factoring polynomials over finite fields. </title> <journal> Math. Comp., </journal> <volume> 36(154) </volume> <pages> 587-592, </pages> <year> 1981. </year> <month> 35 </month>
Reference-contexts: The exponent 3 in the running time can be reduced using asymptotically fast (but generally impractical) matrix techniques. Cantor and Zassenhaus's degree-separation method <ref> [5] </ref> works by first partially factoring the polynomial so as to separate irreducible factors of differing degree, and then completing the factorization (if necessary) by separating irreducible factors of the same degree.
Reference: [6] <author> T. Cormen, C. Leiserson, and R. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: CRT-step We define one CRT-step as the operation of applying the Chinese Remainder Theorem to residues modulo q 1 ; : : : ; q ` , and reducing the result modulo p. FFT-step In the standard iterative implementation of the FFT (see <ref> [6, Chapter 32] </ref>), the basic arithmetic step is the following "butterfly" operation: (s; t) (u + v w; u v w): We define one FFT-step as the operation of executing one such butterfly operation for all of the ` FFT-primes. <p> The cost of computing FFT (T ) or FFT 1 (S) is 2 k1 k FFT-steps (see <ref> [6, Chapter 32] </ref>). With this notation, we can describe our polynomial-multiplication algorithm and its complexity quite concisely. The input consists of two polynomials g; h 2 F p [x] of degree at most n 1. Let k = dlog 2 ne.
Reference: [7] <author> R. Dentzer. </author> <title> The libI software library. </title> <note> Available via anonymous FTP from ftp.iwr.uni-heidelberg.de in pub/IntArith, </note> <year> 1994. </year>
Reference-contexts: The MIPS processor also has an integer multiply instruction. For the assembly-code, we used the software library libI (version 2.1), written by Ralf Dentzer <ref> [7] </ref>. This library is written in C, but contains highly optimized assembly-language code for the equivalent of our AddMul routine. For multi-precision integer multiplication, on these machines, libI is as fast or faster than just about any other available software.
Reference: [8] <author> J. L. Dornstetter. </author> <title> On the equivalence between Berlekamp's and Euclid's algorithms. </title> <journal> IEEE Trans. Inf. Theory, </journal> <volume> IT-33:428-431, </volume> <year> 1987. </year>
Reference-contexts: The Berlekamp-Massey step can be carried out with essentially a GCD computation, taking O (M (m) log m) scalar operations <ref> [8] </ref>. If the resulting polynomial has degree less than m, we can test if it actually annihilates h using the modular-composition algorithm. 4.1 Power Projection It remains to discuss the implementation of the power projection step.
Reference: [9] <author> E. Kaltofen and A. Lobo. </author> <title> Factoring high-degree polynomials by the black box Berlekamp algorithm. </title> <booktitle> In Proc. Internat. Symp. </booktitle> <institution> Symbolic Alebraic Comput., </institution> <year> 1994. </year>
Reference-contexts: For large p, this algorithm is asymptotically the fastest known. However, it appears to be quite impractical, and our experience indicates that it will be slower than the null-space method for n up to at least several thousands. Kaltofen and Lobo introduced a black-box variant of the null-space method <ref> [9] </ref> by applying Wiedemann's linear system solving techniques [22] to the null-space method. The black-box variant can be implemented in a variety of ways, achieving a variety of simultaneous time/space bounds (including those of the above three methods).
Reference: [10] <author> E. Kaltofen and V. Pan. </author> <title> Processor efficient parallel solution of linear systems over an abstract field. </title> <booktitle> In Proc. 3rd Ann. ACM Symp. Parallel Algor. Architecture, </booktitle> <pages> pages 180-191, </pages> <year> 1991. </year>
Reference-contexts: In the Berlekamp-Massey step, the output polynomial will depend on the random choices made in the first step. The "success" probability that the algorithm correctly outputs the polynomial h fl can be bounded from below in two different ways: this probability is at least 1 m=p <ref> [10] </ref>, which is a useful bound when p is large with respect to m; for small p, this probability is bounded from below by a constant times 1=dlog p me [22].
Reference: [11] <author> E. Kaltofen and V. Shoup. </author> <title> Subquadratic-time factoring of polynomials over finite fields. </title> <booktitle> In 27th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 398-406, </pages> <year> 1995. </year>
Reference-contexts: In the case where p is very small, especially the extreme case where p = 2, the algorithmic issues are quite different from those in the case we are considering here. We demonstrate that the new baby step/giant step factoring method, recently developed by Kaltofen & Shoup <ref> [11] </ref>, can be made into a very practical algorithm. We describe an implementation of this algorithm, and present the results of empirical tests comparing this new algorithm with others. <p> The black-box variant can be implemented in a variety of ways, achieving a variety of simultaneous time/space bounds (including those of the above three methods). Kaltofen and Shoup introduced a baby step/giant step technique <ref> [11] </ref> that can be applied to either the degree-separation method or the black-box variant of the null-space method. This new method yields a factoring algorithm that for small p is asymptotically the fastest known, using O (n 1:82 log p) scalar operations. <p> The first topic concerns application of the baby step/giant step technique to the black-box variant of the null-space method. We refer the reader to <ref> [11] </ref> for the details of how this is done.
Reference: [12] <author> M. Kaminski, D. G. Kirkpatrick, and N. H. Bshouty. </author> <title> Addition requirements for matrix and transposed matrix products. </title> <journal> Journal of Algorithms, </journal> <volume> 9 </volume> <pages> 354-364, </pages> <year> 1988. </year>
Reference-contexts: This is just the modular-composition problem. By the "transposition principle" <ref> [12] </ref>, under certain technical restrictions, an algorithm for modular composition can be transformed into one with the same time complexity for power projection. Thus, the power projection problem might also be called the transposed modular-composition problem. <p> In our application, h remains fixed for many such applications. In the previous subsection, we presented an algorithm for the problem of applying A to a given vector. Generally, any circuit that computes the map A can essentially be reversed to compute the transpose map A T (see, e.g., <ref> [12] </ref>). Using this transformation technique, we have derived from our pre-conditioned modular-multiplication algorithm the following algorithm for applying A T to a vector; we offer no other proof of correctness other than the validity of this transformation technique (and the fact that it does indeed work in practice).
Reference: [13] <author> F. Lehmann, M. Maurer, V. Muller, and V. Shoup. </author> <title> Counting the number of points on elliptic curves over finite fields of characteristic greater than three. </title> <booktitle> In First Algorithmic Number Theory Symposium, </booktitle> <year> 1994. </year>
Reference-contexts: In this paper, we shall concentrate on the case where p is large; for concreteness, say log 2 p = (n). This case arises, for example, in algorithms for counting points on elliptic curves over F p <ref> [13] </ref>. In the case where p is very small, especially the extreme case where p = 2, the algorithmic issues are quite different from those in the case we are considering here. <p> For example, this software has been used to count the number of points over F p where p is a 914-bit (375 decimal digit) prime in a week's time on a network of workstations, which represents the state of the art for this problem. These results are reported in <ref> [13] </ref>. 1.3 Overview of algorithm We now present an overview of the new algorithm.
Reference: [14] <author> A. K. Lenstra. </author> <title> Documentation of LIP. </title> <note> Available via anonymous FTP from flash.bellcore.com in pub/lenstra, </note> <year> 1994. </year>
Reference-contexts: One of our goals in this implementation was to write code that is highly portable, while still attaining reasonable performance. 8.1 Multi-precision arithmetic To implement multi-precision arithmetic, we used a customized version of LIP, a C software library for multi-precision arithmetic written by Arjen Lenstra <ref> [14] </ref>. The default version of LIP is highly portable. However, by setting a flag at compile time, an alternative set of routines is used that is a bit less portable than the default, but on many machines is quite a bit faster.
Reference: [15] <author> R. Lidl and H. Niederreiter. </author> <title> Finite Fields. </title> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: This fact follows, for example, from <ref> [15, Theorem 3.20] </ref>. Here is a high-level description of the new algorithm. The details of how each step is to be implemented are deferred until later. Let f 2 F p [x] be the polynomial (assumed square-free) to be factored, with n = deg (f ).
Reference: [16] <author> M. Monagan. </author> <title> Von zur Gathen's factorization challenge. </title> <journal> SIGSAM Bulletin, </journal> <volume> 27(2) </volume> <pages> 13-18, </pages> <year> 1993. </year>
Reference-contexts: The benchmarks are chosen so that they are easy to describe and to generate, but yet appear to behave as "generic" inputs. In response to von zur Gathen's challenge, Monagan <ref> [16] </ref> implemented the null-space method in Maple on a DEC station 3100. The largest of von zur Gathen's benchmarks reported to be factored in [16] was the n = 200 benchmark, which took approximately 27 hours. <p> In response to von zur Gathen's challenge, Monagan <ref> [16] </ref> implemented the null-space method in Maple on a DEC station 3100. The largest of von zur Gathen's benchmarks reported to be factored in [16] was the n = 200 benchmark, which took approximately 27 hours.
Reference: [17] <author> P. Montgomery. </author> <title> An FFT Extension of the Elliptic Curve Method of Factorization. </title> <type> PhD thesis, </type> <institution> Univ. of California-Los Angeles, </institution> <year> 1992. </year>
Reference-contexts: In x8, we discuss some of the implementation details of long-integer arithmetic and related problems. We certainly do not claim that all of the techniques in xx6-8 are original. Similar work has also been done by others (see Montgomery <ref> [17] </ref> in the context of integer factorization, and Morain [18] in the context of counting points on elliptic curves). In x9, we discuss the results of timing experiments with our software, including a precise description of our factoring benchmarks.
Reference: [18] <author> F. Morain. Implantation de l'algorithme de Schoof-Elkies-Atkin. </author> <type> Preprint, </type> <year> 1994. </year>
Reference-contexts: In x8, we discuss some of the implementation details of long-integer arithmetic and related problems. We certainly do not claim that all of the techniques in xx6-8 are original. Similar work has also been done by others (see Montgomery [17] in the context of integer factorization, and Morain <ref> [18] </ref> in the context of counting points on elliptic curves). In x9, we discuss the results of timing experiments with our software, including a precise description of our factoring benchmarks.
Reference: [19] <author> V. Shoup. </author> <title> Fast construction of irreducible polynomials over finite fields. </title> <journal> J. Symbolic Comp., </journal> <volume> 17 </volume> <pages> 371-391, </pages> <year> 1994. </year>
Reference-contexts: For large m this is quite costly, requiring O (mM (n) + m 2 n) scalar operations and space for O (nm) scalars. An asymptotically faster and more space-efficient approach is described in Shoup <ref> [19] </ref>, using O (m 1=2 M (n) + mn) scalar operations and space for O (m 1=2 n) scalars. However, the techniques used there prove only the existence of an algorithm, and do not give an explicit algorithm. We give one here. The algorithm has two basic steps.
Reference: [20] <author> J. von zur Gathen. </author> <title> A polynomial factorization challenge. </title> <journal> SIGSAM Bulletin, </journal> <volume> 26(2) </volume> <pages> 22-24, </pages> <year> 1992. </year>
Reference-contexts: Von zur Gathen <ref> [20] </ref> has suggested a "polynomial factorization challenge" consisting of a family of benchmarks for polynomial factorization algorithms. For each n, von zur Gathen's nth benchmark consists of a degree n polynomial modulo an (n + 2)-bit prime. <p> polynomial factorization. 30 Table 8: Factorization pattern of F n mod P n n degrees of factors 64 1, 4, 6, 7, 17, 29 256 1, 1, 1, 1, 2, 5, 102, 143 1024 1, 1, 2, 5, 42, 42, 44, 49, 77, 120, 198, 443 Joachim von zur Gathen <ref> [20] </ref> has proposed a set of benchmarks by which to judge polynomial factorization algorithms. For every n, von zur Gathen's nth benchmark is to factor x n + x + 1 modulo the first prime b2 n c.
Reference: [21] <author> J. von zur Gathen and V. Shoup. </author> <title> Computing Frobenius maps and factoring polynomials. </title> <journal> Computational Complexity, </journal> <volume> 2 </volume> <pages> 187-224, </pages> <year> 1992. </year>
Reference-contexts: For the large p case, this method is in theory and in practice much slower than the null-space method; however, it is uses much less space. Von zur Gathen and Shoup introduced a fast conjugation technique <ref> [21] </ref> to speed up the degree-separation method. This technique yields new and asymptotically fast algorithms for computing successive pth powers in polynomial quotient rings over F p . <p> This method is described in x2. For the equal-degree factorization step, we apply the fast trace computation technique of <ref> [21] </ref>; this is discussed in x3. This stage of the algorithm uses in the worst case O (n 2 log n + M (n) log n log p) scalar operations and space for O (n 1:5 ) scalars. Again, the implied constants here are quite small. <p> For the large values of p we are mainly considering in this paper, this is not a serious issue. We discuss the Trace Computation step next, and the Factor Extraction step in x3.2. 3.1 Trace Computation In <ref> [21] </ref>, a fast trace computation technique was introduced. It was observed that for nonnegative integers a and b, T a+b = T a (x p b This leads to a repeated-squaring type of algorithm for computing T k (g) mod f using O (log k) modular compositions. Algorithm 5.2 in [21] <p> <ref> [21] </ref>, a fast trace computation technique was introduced. It was observed that for nonnegative integers a and b, T a+b = T a (x p b This leads to a repeated-squaring type of algorithm for computing T k (g) mod f using O (log k) modular compositions. Algorithm 5.2 in [21] is such an algorithm. We describe here an algorithm that is slightly more efficient. The inputs are the integer k, and the polynomials g, f , and (x p mod f ). <p> This is convenient, as then we need to build just one power table per loop iteration, cutting down significantly on the running time. In comparison, Algorithm 5.2 in <ref> [21] </ref>, computes (per loop iteration) either 2 compositions with the same argument, or 4 compositions with two different arguments. <p> We can then recursively apply this technique on the two factors. 9 It follows that the expected total cost of the recursion is O (M (n) log r (log p + log n)) scalar operations (see, e.g., <ref> [21, Lemma 4.1] </ref> for a detailed analysis of this type of recursion). The minimum-polynomial extraction method This method avoids the costly computation of a (p1)=2-th power modulo f , and for small r (which is expected on random inputs) is significantly faster than the simple extraction method. <p> Now we recursively apply this method twice with f f 1 ; h (h mod f 1 ); h fl h fl and 2 : It follows (see again <ref> [21, Lemma 4.1] </ref>) that the expected total cost of the minimum-polynomial extraction method is O (log r (M (r) log p + M (n)(r 1=2 + log n) + rn)) scalar operations and space for O (r 1=2 n) scalars.
Reference: [22] <author> D. </author> <title> Wiedemann. Solving sparse linear systems over finite fields. </title> <journal> IEEE Trans. Inf. Theory, </journal> <volume> IT-32:54-62, </volume> <year> 1986. </year>
Reference-contexts: However, it appears to be quite impractical, and our experience indicates that it will be slower than the null-space method for n up to at least several thousands. Kaltofen and Lobo introduced a black-box variant of the null-space method [9] by applying Wiedemann's linear system solving techniques <ref> [22] </ref> to the null-space method. The black-box variant can be implemented in a variety of ways, achieving a variety of simultaneous time/space bounds (including those of the above three methods). <p> the polynomial h fl can be bounded from below in two different ways: this probability is at least 1 m=p [10], which is a useful bound when p is large with respect to m; for small p, this probability is bounded from below by a constant times 1=dlog p me <ref> [22] </ref>. The Berlekamp-Massey step can be carried out with essentially a GCD computation, taking O (M (m) log m) scalar operations [8].
Reference: [23] <author> D. Y. Y. Yun. </author> <title> On square-free decomposition algorithms. </title> <booktitle> In Proc. ACM Symp. Symbolic and Algebraic Comp., </booktitle> <pages> pages 26-35, </pages> <year> 1976. </year> <month> 36 </month>
Reference-contexts: Using an algorithm of Yun <ref> [23] </ref>, the square-free factorization stage can be accomplished essentially in the time to compute a GCD. Using the recursive "Half-GCD" method (see [1, Chapter 8]), this takes O (M (n) log n) scalar operations. The space requirement is O (n) scalars.
References-found: 23

