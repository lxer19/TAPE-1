URL: http://www-anw.cs.umass.edu/People/singh/Papers/summerschool.ps.Z
Refering-URL: http://www-anw.cs.umass.edu/People/singh/Papers/
Root-URL: 
Title: On the Computational Economics of Reinforcement Learning  
Author: Andrew G. Barto Satinder Pal Singh 
Address: Amherst, MA 01003  Amherst, MA 01003  
Affiliation: Dept. of Computer and Information Science University of Massachusetts  Dept. of Computer and Information Science University of Massachusetts  
Abstract: Following terminology used in adaptive control, we distinguish between indirect learning methods, which learn explicit models of the dynamic structure of the system to be controlled, and direct learning methods, which do not. We compare an existing indirect method, which uses a conventional dynamic programming algorithm, with a closely related direct reinforcement learning method by applying both methods to an infinite horizon Markov decision problem with unknown state-transition probabilities. The simulations show that although the direct method requires much less space and dramatically less computation per control action, its learning ability in this task is superior to, or compares favorably with, that of the more complex indirect method. Although these results do not address how the methods' performances compare as problems become more difficult, they suggest that given a fixed amount of computational power available per control action, it may be better to use a direct reinforcement learning method augmented with indirect techniques than to devote all available resources to a computation-ally costly indirect method. Comprehensive answers to the questions raised by this study depend on many factors making up the eco nomic context of the computation.
Abstract-found: 1
Intro-found: 1
Reference: <author> C. W. Anderson. </author> <title> Strategy learning with multilayer connectionist representations. </title> <type> Technical report TR87-509.3, </type> <institution> GTE Laboratories, Incorporated, </institution> <address> Waltham, MA, </address> <year> 1987. </year> <note> (This is a corrected version of the report published in Proceedings of the Fourth International Workshop on Machine Learning,103-114, </note> <institution> 1987, </institution> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann.) </publisher> <editor> A. G. Barto and P. Anandan. </editor> <title> Pattern recognizing stochastic learning automata. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 15 </volume> <pages> 360-375, </pages> <year> 1985. </year>
Reference: <author> A. G. Barto and S. P. Singh. </author> <title> Reinforcement learning and dynamic programming. </title> <booktitle> In Proceedings of the Sixth Yale Workshop on Adaptive and Learning Systems, </booktitle> <address> New Haven, CT, </address> <month> Aug </month> <year> 1990. </year>
Reference: <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <title> Neu--ronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 835-846, </pages> <year> 1983. </year> <note> Reprinted in J. A. </note>
Reference: <author> Anderson and E. Rosenfeld, </author> <title> Neurocomputing: </title> <booktitle> Foundations of Research, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference: <author> A. G. Barto, R. S. Sutton, and P. S. Brouwer. </author> <title> Associative search network: A reinforcement learning associative memory. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 40 </volume> <pages> 201-211, </pages> <year> 1981. </year>
Reference: <author> A. G. Barto, R. S. Sutton, and C. Watkins. </author> <title> Learning and sequential decision making. </title> <editor> In M. Gabriel and J. W. Moore, editors, </editor> <title> Learning and Computational Neuroscience. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <note> To appear. </note>
Reference: <author> A. G. Barto, R. S. Sutton, and C. Watkins. </author> <title> Sequential decision problems and neural networks. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA, 1990. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> D. P. Bertsekas. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: A policy that achieves this objective is an optimal policy which, although not always unique, is denoted U fl = (u fl 1 ; . . . ; u fl be shown that for the formulation given here, all optimal policies are stationary <ref> (e.g., Bertsekas, 1987) </ref>. <p> methods also require estimating the payoffs r k ij , for each combination of current state, i, next state, j, and action, k. 4 Indirect methods are based on the certainty equivalence principle of computing and using policies that would be optimal if the current transition probability estimates were correct <ref> (Bertsekas, 1987) </ref>. Most of the methods for the adaptive control of Markov processes described in the engineering literature are indirect (e.g., Borkar and Varaiya, 1979; Kumar and Lin, 1982; Mandl, 1974; Riordon, 1969; Sato-Abe-Takeda, 1982, 1985, 1988).
Reference: <author> V. Borkar and P. Varaiya. </author> <title> Adaptive control of markov chains I: Finite parameter set. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 24 </volume> <pages> 953-957, </pages> <year> 1979. </year>
Reference: <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: However these results demonstrate that direct reinforcement learning methods are not necessarily less capable than much more complex indirect methods, and they raise questions, which we discuss below, about the computational economics of learning. 1 This distinction parallels that between parametric and non-parametric approaches to pattern classification <ref> (e.g., Duda and Hart, 1973) </ref>. Watkins (1989) made a similar distinction between model-based and primitive learning methods, terminology we adopted in Barto and Singh (1990). 2 A control design procedure is any method for determining a control rule based on a system model and performance specifications.
Reference: <author> Y. El-Fattah. </author> <title> Recursive algorithms for adaptive control of finite markov chains. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 11 </volume> <pages> 135-144, </pages> <year> 1981. </year>
Reference: <author> G. C. Goodwin and K. S. </author> <title> Sin. Adaptive Filtering Prediction and Control. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1984. </year>
Reference-contexts: The simulations described in this paper were motivated by a simple, but nevertheless unanswered, question about the relative efficiency of two approaches to learning how to solve a particular type of stochastic control problem. Following terminology used in adaptive control <ref> (e.g., Goodwin and Sin, 1984) </ref>, we distinguish between indirect learning methods, which learn explicit models of the dynamic structure of the system to be controlled, and direct learning methods, which do not. 1 Indirect methods estimate unknown parameters describing the system to be controlled and define a control rule in terms
Reference: <author> S. E. Hampson. </author> <title> Connectionist Problem Solving: Computational Aspects of Biological Learning. </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1989. </year>
Reference: <author> J. H. Holland. </author> <title> Escaping brittleness: The possibility of general-purpose learning algorithms applied to rule-based systems. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Volume II, </volume> <pages> pages 593-623. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1986. </year>
Reference: <author> D. H. Jacobson and D. Q. Mayne. </author> <title> Differential Dynamic Programming. </title> <publisher> Elsevier, </publisher> <address> New York, </address> <year> 1970. </year>
Reference: <author> M. I. Jordan and R. A. Jacobs. </author> <title> Learning to control an unstable system with forward modeling. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA, 1990. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> A. H. Klopf. </author> <title> Brain function and adaptive systems| A heterostatic theory. </title> <type> Technical Report AFCRL-72-0164, </type> <institution> Air Force Cambridge Research Laboratories, Bedford, </institution> <address> MA, </address> <year> 1972. </year> <booktitle> (A summary appears in Proceedings of the International Conference on Systems, Man, and Cybernetics, </booktitle> <year> 1974, </year> <journal> IEEE Systems, Man, and Cybernetics Society, Dallas, </journal> <note> TX.) </note> <author> A. H. Klopf. </author> <title> The Hedonistic Neuron: A Theory of Memory, Learning, </title> <booktitle> and Intelligence. </booktitle> <address> Hemishere, Washington, D.C., </address> <year> 1982. </year>
Reference: <author> P. R. Kumar and Woei Lin. </author> <title> Optimal adaptive controllers for unknown markov chains. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 25 </volume> <pages> 765-774, </pages> <year> 1982. </year>
Reference: <author> S. Lakshmivarahan. </author> <title> Learning Algorithms and Applications. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference: <author> P. Mandl. </author> <title> Estimation and control in markov chains. </title> <booktitle> Advances in Applied Probability, </booktitle> <volume> 6 </volume> <pages> 40-60, </pages> <year> 1974. </year>
Reference: <author> K. Narendra and M. A. L. Thathachar. </author> <title> Learning Automata: An Introduction. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: This idea plays a fundamental role in theories of animal learning and is elaborated mathematically in the theory of learning automata <ref> (Narendra and Thathachar, 1989) </ref>. Embedding this idea within a framework for associative learning includes a role for stimulus patterns in eliciting actions (Barto, Sutton, and Brouwer, 1981; Barto and Anandan, 1985; Klopf, 1982).
Reference: <author> J. S. Riordon. </author> <title> An adaptive automaton controller for discrete-time markov processes. </title> <journal> Automatica, </journal> <volume> 5 </volume> <pages> 721-730, </pages> <year> 1969. </year>
Reference: <author> M. Sato, K. Abe, and H. Takeda. </author> <title> Learning control of finite markov chains with unknown transition probabilities. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 27 </volume> <pages> 502-505, </pages> <year> 1982. </year>
Reference-contexts: This algorithm is an extension of previous research by the same authors <ref> (Sato, Abe, and Takeda, 1982, 1985) </ref>. In Section 6, we combine a component of this algorithm with Q Learning to produce a comparable direct method. The method of Sato et al. explicitly estimates the unknown state-transition probabilities by keeping counts of state transitions observed while controlling the system.
Reference: <author> M. Sato, K. Abe, and H. Takeda. </author> <title> An asymptotically optimal learning controller for finite markov chains with unknown transition probabilities. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 30 </volume> <pages> 1147-1149, </pages> <year> 1985. </year>
Reference: <author> M. Sato, K. Abe, and H. Takeda. </author> <title> Learning control of finite markov chains with explicit trade-off between estimation and control. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 18 </volume> <pages> 677-684, </pages> <year> 1988. </year>
Reference: <author> R. S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Mas-sachusetts, </institution> <address> Amherst, MA, </address> <year> 1984. </year>
Reference: <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference: <author> R. S. Sutton. </author> <title> Integrating architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Although control architectures based on reinforcement learning can be quite complex, including components permitting off-line, look-ahead planning <ref> (Sutton, 1990) </ref>, reinforcement learning is usually regarded as a very simple direct method for adjusting behavior. <p> One way of combining direct and indirect methods that retains many of the advantages of each approach is illustrated by Sutton's DYNA architecture <ref> (Sutton, 1990) </ref>. 9 CONCLUSION The simulation results described in this paper show that although the direct EQ algorithm requires less space and much less computation per control action than the indirect method of Sato et al., its learning ability when applied to a test problem is superior to, or compares favorably
Reference: <author> C. J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, Eng-land, </address> <year> 1989. </year>
Reference-contexts: The direct method we implemented replaces the DP component of the Sato et al. method with Watkins' Q-Learning algorithm for incrementally approximating the results of DP <ref> (Watkins, 1989) </ref>. We call the resulting direct reinforcement learning algorithm the Exploratory Q-Learning, or EQ, algorithm. We selected the method of Sato et al. for this study because its action-selection component is readily adaptable to direct methods.
Reference: <author> P. J. Werbos. </author> <title> Advanced forecasting methods for global crisis warning and models of intelligence. </title> <journal> General Systems Yearbook, </journal> <volume> 22 </volume> <pages> 25-38, </pages> <year> 1977. </year>
Reference: <author> P. J. Werbos. </author> <title> Building and understanding adaptive systems: A statistical/numerical approach to factory automation and brain research. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <year> 1987. </year>
Reference: <author> P. J. Werbos. </author> <title> Generalization of back propagation with applications to a recurrent gas market model. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 339-356, </pages> <year> 1988. </year>
Reference: <author> P. J. Werbos. </author> <title> Neural networks for control and system identification. </title> <booktitle> In Proceedings of the 28th Conference on Decision and Control, </booktitle> <pages> pages 260-265, </pages> <address> Tampa, Florida, </address> <year> 1989. </year>
Reference: <author> R. M. Wheeler and K. S. Narendra. </author> <title> Decentralized learning in finite markov chains. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 31 </volume> <pages> 519-526, </pages> <year> 1986. </year>
Reference: <author> I. H. Witten. </author> <title> An adaptive optimal controller for discrete-time Markov environments. </title> <journal> Information and Control, </journal> <volume> 34 </volume> <pages> 286-295, </pages> <year> 1977a. </year>
Reference: <author> I. H. Witten. </author> <title> Exploring, modelling and controlling discrete sequential environments. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 9 </volume> <pages> 715-735, </pages> <year> 1977b. </year>
References-found: 36

