URL: http://www.cse.psu.edu/~barlow/rank.ps
Refering-URL: http://www.cse.psu.edu/~barlow/papers.html
Root-URL: http://www.cse.psu.edu
Title: AN EFFICIENT RANK DETECTION PROCEDURE FOR MODIFYING THE ULV DECOMPOSITION  
Author: PETER A. YOON JESSE L. BARLOW 
Address: Azusa, CA 91702-7000 USA  Park PA, 16802-6106, USA  
Affiliation: Department of Computer Science Azusa Pacific University  Department of Computer Science and Engineering The Pennsylvania State University University  
Date: 37 (1997), 000-000.  
Note: BIT  
Abstract: The ULV decomposition (ULVD) is an important member of a class of rank-revealing two-sided orthogonal decompositions used to approximate the singular value decomposition (SVD). The problem of adding and deleting rows from the ULVD (called updating and downdating respectively) is considered. The ULVD can be updated and downdated much faster than the SVD, hence its utility. When updating or downdating the ULVD, it is necessary to compute its numerical rank. In this paper, we propose an efficient algorithm which almost always maintains rank-revealing structure of the decomposition after an update or downdate without Lanczos or power iterations. Moreover, we can monitor the accuracy with which the ULVD approximates the singular value decomposition by tracking exact Frobenius norms of the two small blocks of the lower triangular factor in the decomposition. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J.L. Barlow and S.L. Handy. </author> <title> The direct solution of weighted and equality constrained least squares problems. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 704-716, </pages> <year> 1988. </year>
Reference-contexts: Then, kL 1 k fl fl kL 1 11 wk fl fl 12 P.A. YOON AND J.L. BARLOW Proof. It is easy to verify that L 1 = L 1 11 w Then taking norms and using Lemma 3.3 in <ref> [1] </ref> yields (3.6). Two of the values in Lemma 3.2 can be read off of an LQ factorization of the matrix L in (3.5). Lemma 3.3. Let L be defined in (3.5). Suppose e L = e L 11 0 where Q is orthogonal. <p> Thus this important parameter is computed directly from our algorithms. We constructed three example of problems of the form (4.1)-(4.2). For each example, we constructed a 110 fi 6 matrix X big and a 110-vector b big . Their entries were chosen from a uniform <ref> [0; 1] </ref> distribution. We then chose 89 rows from A big = at random and multiplied them by a small constant j and chose a value * F &gt; j. These example was run in MATLAB on a SUN SPARCstation 5 in IEEE double precision arithmetic. Example 4.1.
Reference: 2. <author> J.L. Barlow and P.A. Yoon. </author> <title> An efficient rank detection procedure for modifying the ULV decomposition. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, The Pennsylvania State University, University Park, </institution> <address> PA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Our tests show that our new algorithms are as robust as the ones in [12] and [3], but are much more automatic. Our results are based on those in the Ph.D. thesis of the first author [15]. Some of these results are summarized without proof in the expository paper <ref> [2] </ref>. The paper is structured as follows. In x2, we outline our algorithms for modifying the ULVD. Our algorithms for bounding kL 1 (t + 1)k from bounds for kL 1 (t)k are given in x3.
Reference: 3. <author> J.L. Barlow, P.A. Yoon, and H. Zha. </author> <title> An algorithm and a stability theory for downdating the ULV decomposition. </title> <journal> BIT, </journal> <volume> 36 </volume> <pages> 14-40, </pages> <year> 1996. </year>
Reference-contexts: An algorithm for updating was given by Stewart [12]. A mixed forward stable algorithm for downdating is given by Barlow, Yoon, and Zha <ref> [3] </ref> that does not require keeping U (t). The authors of [3] also give a thorough discussion of the stability and regularity issues for downdating the ULVD. Both procedures require only O (p 2 ) flops and are thus considerably faster than modifying the SVD. <p> An algorithm for updating was given by Stewart [12]. A mixed forward stable algorithm for downdating is given by Barlow, Yoon, and Zha <ref> [3] </ref> that does not require keeping U (t). The authors of [3] also give a thorough discussion of the stability and regularity issues for downdating the ULVD. Both procedures require only O (p 2 ) flops and are thus considerably faster than modifying the SVD. <p> The conditions (1.5) and (1.7) can be verified by computing kF (t)k F , kG (t)k F , and kL 1 (t)k at each new value of t. Barlow, Yoon, and Zha <ref> [3] </ref> give methods for updating kF (t)k F and kG (t)k F that require O (p) flops altogether. The computation of kL 1 (t)k is more complicated and there lies the main contribution of this paper. <p> Some applications are discussed by Fierro, Vanhamme, and Van Huffel [8]. Our tests show that our new algorithms are as robust as the ones in [12] and <ref> [3] </ref>, but are much more automatic. Our results are based on those in the Ph.D. thesis of the first author [15]. Some of these results are summarized without proof in the expository paper [2]. The paper is structured as follows. In x2, we outline our algorithms for modifying the ULVD. <p> In that context, we may assume that ` 1 . Since the values of 2 and g can be small, they can also be inaccurate because of accumulated rounding errors. We can define the function down22 using a version of the algorithm described in <ref> [3] </ref> applied to R. <p> In that case, we solve a "nearby" downdating problem. We have ff = ~g = 0, and ~ 2 = fl g + fi h which is consistent with the choice of the orthogonal matrix Q. The significance of the latter regularity condition is discussed in <ref> [3, Proposition 3.2] </ref>. 2.2 Algorithms for Updating and Downdating the ULVD We now make notational simplifications of (1.2)-(1.6) and (1.7) by writing A = A (t); U = U (t); C = C (t); V = V (t); C = @ k L (t) 0 0 mp 0 0 0 A <p> Such a procedure is given in <ref> [3] </ref>. Also compute F (1) = ^ U T Step 2. For each integer i, let f (i) (i) 1 G (i) e 1 . <p> The assumption that L T L z 1 z T 1 is positive semi-definite is sufficient to insure that j` (3) kk j j 2 j and thus that step 4 is well defined. Occasionally this assumption is violated, see the discussion in x4 or the examples in <ref> [3] </ref>. Algorithm 2.2 (Updating Procedure for the ULVD). An updating algorithm for the ULVD may be obtained by making three simple changes to Algorithm 2.1. <p> Similar formulas can be given for the algorithms in <ref> [3] </ref> and [12]. The updating of these two norms contributes only to the O (p) term in the complexity of our algorithms. 3 Tracking kL 1 k and Related Issues Let ^ L denote the fi principal submatrix of ^ C as defined in (2.4). <p> To determine which of or 1 is correct, we must estimate k ^ L 1 k. For the algorithms in <ref> [3] </ref> and [12], we must use Lanczos or power iterations, but for Algorithms 2.1 and 2.2 we can get a simple estimate very easily. A sharp bound for k ^ L 1 k is stated in x3.1 and proven in x3.2. <p> Then ^ k ^ L 1 k 1 oe ( ^ C). We give the proof of this theorem in x3.2. The estimate in Theorem 3.1 is not really free. The updating and downdating algorithms in this paper require about k 2 more flops than those in <ref> [3] </ref>, [12]. Thus the real cost of this estimate is about one power iteration. If k b L 1 k 1 &lt; *, then step 8 of Algorithms 2.1 and 2.2 is completed by com puting the ULVD of ^ L. <p> We show how to get the formulas for Algorithm 2.1 and a similar analysis will obtain them for Algorithm 2.2. Unlike the results in x3.1 for the Euclidean norm of L 1 , the Frobenius norm can be tracked in O (p 2 ) for the Algorithms in <ref> [3] </ref> or [12] using an algorithm similar to that outlined here. Lemma 3.5. Assume the hypothesis and terminology of Lemma 3.3. Then, kL 1 k 2 11 k 2 1 Proof. <p> Algorithms 2.1 and 2.2 obtain about the same accuracy as the algorithms from <ref> [3] </ref> and [12] in terms of computed subspaces. There is no reason to expect that the algorithms in this paper are more accurate or less accurate than the ones in [3] or [12]. Their advantages are the avoidance of the condition estimation step. <p> Algorithms 2.1 and 2.2 obtain about the same accuracy as the algorithms from <ref> [3] </ref> and [12] in terms of computed subspaces. There is no reason to expect that the algorithms in this paper are more accurate or less accurate than the ones in [3] or [12]. Their advantages are the avoidance of the condition estimation step. That is, Algorithms 2.1 and 2.2 are more automatic and obtain subspaces that are as good. <p> This is roughly the same error using the ULVD modification algorithms from <ref> [3] </ref>, [12]. The solid line in the graph "Rank Estimates" of Fig. 4.1 gives the value of k obtained by the SVD. The same value was always obtained our ULVD procedures, so it is not separately graphed. Notice that k varies greatly. <p> Notice that k varies greatly. The asterisk "*" indicates that step 7 was necessary in the downdate. The "+"indicates that L T L z 1 z T 1 became indefinite and a step of corrected semi-normal equations as discussed in <ref> [3] </ref>, [4], [11] was necessary to get a more accurate value of z 1 . The last graph in each figure "Norm Estimates" plots the kL 1 (t)k fl ^ where ^ is defined in Theorem 3.1.
Reference: 4. <author> A. Bjorck, H. Park, and L. Elden. </author> <title> Accurate downdating of least squares solutions. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 15 </volume> <pages> 549-568, </pages> <year> 1992. </year>
Reference-contexts: Notice that k varies greatly. The asterisk "*" indicates that step 7 was necessary in the downdate. The "+"indicates that L T L z 1 z T 1 became indefinite and a step of corrected semi-normal equations as discussed in [3], <ref> [4] </ref>, [11] was necessary to get a more accurate value of z 1 . The last graph in each figure "Norm Estimates" plots the kL 1 (t)k fl ^ where ^ is defined in Theorem 3.1.
Reference: 5. <author> C. Davis and W.M. Kahan. </author> <title> The rotation of eigenvectors by a perturbation III. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 7 </volume> <pages> 1-46, </pages> <year> 1970. </year>
Reference-contexts: To insure that, Fierro and Bunch [7] show that the Davis-Kahan <ref> [5] </ref> sin measure of error in subspaces is bounded by j sin j = kW T oe k+1 (A (t))kF (t)k k+1 (A (t)) A simple use of norm inequalities yields the bound kW T oe k+1 (A (t)) : Thus, a small value of j assures us of accurate subspaces.
Reference: 6. <author> D. K. Faddeev, V. N. Kublanovskaja, and V. N. Faddeva. </author> <title> Solution of linear algebraic systems with rectangular matrices. </title> <journal> Trudy Mat. Inst. Steklov, </journal> <volume> 96 </volume> <pages> 93-111, </pages> <year> 1968. </year>
Reference-contexts: The ULV decomposition (ULVD) is such an approximation. The ULVD is a special case of the two-sided orthogonal decompositions defined by Faddeev, Kublanovskaya, and Faddeeva <ref> [6] </ref> and Hanson and Lawson [10]. The most familiar formulation is due to Stewart [12]. A slightly different formulation is given below.
Reference: 7. <author> R. D. Fierro and J. R. Bunch. </author> <title> Orthogonal projection and total least squares. </title> <journal> Numer. Linear Algebra Appl., </journal> <volume> 2 </volume> <pages> 135-154, </pages> <year> 1995. </year>
Reference-contexts: To insure that, Fierro and Bunch <ref> [7] </ref> show that the Davis-Kahan [5] sin measure of error in subspaces is bounded by j sin j = kW T oe k+1 (A (t))kF (t)k k+1 (A (t)) A simple use of norm inequalities yields the bound kW T oe k+1 (A (t)) : Thus, a small value of j <p> example, we computed the two error measures j sin 1 (t)j = kW T 1 (t)V 2 (t)k: The value sin 1 (t) is the accumulated error from approximation and rounding in the updated and downdated ULVD, sin 2 (t) is the local approximation error discussed by Fierro and Bunch <ref> [7] </ref> and given in (1.10). In the graph "Noise Space Errors" in Fig. 4.1, the solid line is log 10 j sin 1 (t)j, and the dashed dot line is log 10 j sin 2 (t)j.
Reference: 8. <author> R.D. Fierro, L Vanhamme, and S. Van Huffel. </author> <title> Total least squares algorithms based on rank-revealing complete orthogonal decompositions. </title> <editor> In S. Van Huf-fel, editor, </editor> <booktitle> Recent advances in total least squares techniques and errors-in-variables modeling, </booktitle> <pages> pages 99-116, </pages> <address> Philadelph, 1997. </address> <publisher> SIAM Publications. </publisher>
Reference-contexts: YOON AND J.L. BARLOW For the first procedure, its only potential weakness is that it can be too pessimistic, but it is always possible to detect that and correct for it. Some applications are discussed by Fierro, Vanhamme, and Van Huffel <ref> [8] </ref>. Our tests show that our new algorithms are as robust as the ones in [12] and [3], but are much more automatic. Our results are based on those in the Ph.D. thesis of the first author [15]. <p> MODIFYING THE ULV DECOMPOSITION 19 Acknowledgments Hongyuan Zha showed the first author how to shorten the proof of Theorem 3.1. The formulation of ULVD in this paper seemed natural to us after comments to the first author by Ricardo Fierro. Sabine Van Huffel gave us access to reference <ref> [8] </ref> and provided other constructive help. Finally, we thank Lars Elden for his patience, and an anonymous referee for some helpful comments.
Reference: 9. <author> M. Gu and S.C. Eisenstat. </author> <title> A divide-and-conquer algorithm for the symmetric tridiagonal eigenproblem. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 16 </volume> <pages> 172-191, </pages> <year> 1995. </year>
Reference-contexts: The adaptive fast multipole procedure of Gu and Eisenstat <ref> [9] </ref> requires O (n 2 ) flops, but no robust software is available for it, and its flop count is kn 2 + O (n) where k is a very large constant that depends upon machine precision.
Reference: 10. <author> R. J. Hanson and R. J. Lawson. </author> <title> Extensions and applications of the householder algorithm for solving linear least squares problems. </title> <journal> Mathematics of Computation, </journal> <volume> 23 </volume> <pages> 787-812, </pages> <year> 1969. </year>
Reference-contexts: The ULV decomposition (ULVD) is such an approximation. The ULVD is a special case of the two-sided orthogonal decompositions defined by Faddeev, Kublanovskaya, and Faddeeva [6] and Hanson and Lawson <ref> [10] </ref>. The most familiar formulation is due to Stewart [12]. A slightly different formulation is given below. <p> For each integer i, let f (i) (i) 1 G (i) e 1 . Find an orthogonal matrix ^ U 3 2 R (k+1)fi (k+1) such that 0 g 11 = ^ U T ff 1 g T g 11 where L (2) is lower triangular. See <ref> [10] </ref> for such a procedure. Define F (2) = (I e 1 e T Step 3.
Reference: 11. <author> H. Park and L. Elden. </author> <title> Downdating the rank-revealing URV decomposition. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 16 </volume> <pages> 138-156, </pages> <year> 1995. </year>
Reference-contexts: Notice that k varies greatly. The asterisk "*" indicates that step 7 was necessary in the downdate. The "+"indicates that L T L z 1 z T 1 became indefinite and a step of corrected semi-normal equations as discussed in [3], [4], <ref> [11] </ref> was necessary to get a more accurate value of z 1 . The last graph in each figure "Norm Estimates" plots the kL 1 (t)k fl ^ where ^ is defined in Theorem 3.1.
Reference: 12. <author> G.W. Stewart. </author> <title> Updating a rank-revealing ULV decomposition. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 14 </volume> <pages> 494-499, </pages> <address> 1993. 20 P.A. </address> <note> YOON AND J.L. BARLOW </note>
Reference-contexts: The ULV decomposition (ULVD) is such an approximation. The ULVD is a special case of the two-sided orthogonal decompositions defined by Faddeev, Kublanovskaya, and Faddeeva [6] and Hanson and Lawson [10]. The most familiar formulation is due to Stewart <ref> [12] </ref>. A slightly different formulation is given below. <p> For each value of t, the ULVD of A (t+1) is formed from that of A (t) by adding a row, thus performing an update operation on C (t), and deleting a row, thus performing a downdate. An algorithm for updating was given by Stewart <ref> [12] </ref>. A mixed forward stable algorithm for downdating is given by Barlow, Yoon, and Zha [3] that does not require keeping U (t). The authors of [3] also give a thorough discussion of the stability and regularity issues for downdating the ULVD. <p> Some applications are discussed by Fierro, Vanhamme, and Van Huffel [8]. Our tests show that our new algorithms are as robust as the ones in <ref> [12] </ref> and [3], but are much more automatic. Our results are based on those in the Ph.D. thesis of the first author [15]. Some of these results are summarized without proof in the expository paper [2]. The paper is structured as follows. <p> Both algorithms use steps 1-3 to reduce the updating/downdating problem to that for a 2 fi 2 upper triangular matrix, do the appropriate modification in step 4, and use steps 5-7 to restore lower triangular form. Using an illustration technique from Stewart <ref> [12] </ref>, we show the orthogonal transformations necessary for steps 1-7 of Algorithm 2.1 on a 6 fi 6 matrix with k = 3. Figure 2.1 is steps 1-3, Figure 2.2 is steps 4-6, and Figure 2.3 is step 7. <p> Similar formulas can be given for the algorithms in [3] and <ref> [12] </ref>. The updating of these two norms contributes only to the O (p) term in the complexity of our algorithms. 3 Tracking kL 1 k and Related Issues Let ^ L denote the fi principal submatrix of ^ C as defined in (2.4). <p> To determine which of or 1 is correct, we must estimate k ^ L 1 k. For the algorithms in [3] and <ref> [12] </ref>, we must use Lanczos or power iterations, but for Algorithms 2.1 and 2.2 we can get a simple estimate very easily. A sharp bound for k ^ L 1 k is stated in x3.1 and proven in x3.2. <p> Then ^ k ^ L 1 k 1 oe ( ^ C). We give the proof of this theorem in x3.2. The estimate in Theorem 3.1 is not really free. The updating and downdating algorithms in this paper require about k 2 more flops than those in [3], <ref> [12] </ref>. Thus the real cost of this estimate is about one power iteration. If k b L 1 k 1 &lt; *, then step 8 of Algorithms 2.1 and 2.2 is completed by com puting the ULVD of ^ L. <p> That can be determined if ^ &lt; *, but the ULVD of ^ L fails to isolate a small last row as in (3.4). In that case, the process of determining that ULVD will produce a new estimate for k ^ L 1 k <ref> [12] </ref>. 3.2 Proof of Theorem 3.1 The proof of Theorem 3.1 requires three lemmas. Lemma 3.2. Let L = p 1 0 fl p where L 11 is nonsingular and fl 6= 0. Then, kL 1 k fl fl kL 1 11 wk fl fl 12 P.A. YOON AND J.L. <p> Unlike the results in x3.1 for the Euclidean norm of L 1 , the Frobenius norm can be tracked in O (p 2 ) for the Algorithms in [3] or <ref> [12] </ref> using an algorithm similar to that outlined here. Lemma 3.5. Assume the hypothesis and terminology of Lemma 3.3. Then, kL 1 k 2 11 k 2 1 Proof. <p> Algorithms 2.1 and 2.2 obtain about the same accuracy as the algorithms from [3] and <ref> [12] </ref> in terms of computed subspaces. There is no reason to expect that the algorithms in this paper are more accurate or less accurate than the ones in [3] or [12]. Their advantages are the avoidance of the condition estimation step. <p> Algorithms 2.1 and 2.2 obtain about the same accuracy as the algorithms from [3] and <ref> [12] </ref> in terms of computed subspaces. There is no reason to expect that the algorithms in this paper are more accurate or less accurate than the ones in [3] or [12]. Their advantages are the avoidance of the condition estimation step. That is, Algorithms 2.1 and 2.2 are more automatic and obtain subspaces that are as good. We computed the TLS solutions x T LS (t) from W 2 (t) and y T LS (t) from V 2 (t). <p> This is roughly the same error using the ULVD modification algorithms from [3], <ref> [12] </ref>. The solid line in the graph "Rank Estimates" of Fig. 4.1 gives the value of k obtained by the SVD. The same value was always obtained our ULVD procedures, so it is not separately graphed. Notice that k varies greatly.
Reference: 13. <author> S. Van Huffel and J. Vandewalle. </author> <title> The Total Least Squares Problem: Computational Aspects and Analysis. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: For that reason, our numerical ex periments with them often produced poor estimates. 4 Numerical Examples and Conclusions We base our examples on the total least squares (TLS) problem which we outline below. For a complete description see the book by Van Huffel and Van-dewalle <ref> [13] </ref>. We assume that in (1.1), n 2. <p> It is recovered from the SVD of the matrix A (t) = X (t) b (t) :(4.3) Specifically, the vector y T LS (t) is constructed from the W 2 (t) in (1.9) using the algorithm discussed in <ref> [13, pp.80-81] </ref>. We use the ULVD of A (t) to construct an approximate solution ~y T LS (t) from V 2 (t) again using the algorithm in [13, pp.80-81]. <p> (t) :(4.3) Specifically, the vector y T LS (t) is constructed from the W 2 (t) in (1.9) using the algorithm discussed in <ref> [13, pp.80-81] </ref>. We use the ULVD of A (t) to construct an approximate solution ~y T LS (t) from V 2 (t) again using the algorithm in [13, pp.80-81]. For ~y T LS (t), we have (X (t) + ~ E (t))~y T LS (t) = b (t) + ~ f (t); where ae T LS (t) = k ~ E (t) ~ f (t) k F = kG (t)k F ; 16 P.A. YOON AND J.L.
Reference: 14. <author> J.H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford University Press, </publisher> <address> London, </address> <year> 1965. </year>
Reference-contexts: ! l h ! b l 0 l h f f f g g 1 C C A B B B 0 0 1 2 0 0 l l w h 0 0 0 g f f f g g g C C C A classical theorem on rank-one modifications <ref> [14, pp.94-97] </ref> tells us that if A has k singular values greater than * then the ^ A resulting from Algorithms 2.1 or 2.2 has either 1 or singular values greater than *. To determine which of or 1 is correct, we must estimate k ^ L 1 k.
Reference: 15. <author> P.A. Yoon. </author> <title> Modifying Two-Sided Orthogonal Decompositions: Algorithms, Implementation, and Applications. </title> <type> PhD thesis, </type> <institution> The Pennsylvania State University, University Park, </institution> <address> PA, </address> <year> 1996. </year>
Reference-contexts: Some applications are discussed by Fierro, Vanhamme, and Van Huffel [8]. Our tests show that our new algorithms are as robust as the ones in [12] and [3], but are much more automatic. Our results are based on those in the Ph.D. thesis of the first author <ref> [15] </ref>. Some of these results are summarized without proof in the expository paper [2]. The paper is structured as follows. In x2, we outline our algorithms for modifying the ULVD. Our algorithms for bounding kL 1 (t + 1)k from bounds for kL 1 (t)k are given in x3.
References-found: 15

