URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/nl-icnn96/nl-icnn96-letter.ps.gz
Refering-URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/nl-icnn96/nl-icnn96-6.html
Root-URL: 
Email: lawrence@elec.uq.edu.au, fgiles,sandiwayg@research.nj.nec.com  
Title: Can Recurrent Neural Networks Learn Natural Language Grammars? W&Z recurrent neural networks are able to
Author: Steve Lawrence C. Lee Giles, Sandiway Fong 
Address: 4 Independence Way, Princeton, NJ 08540  
Affiliation: NEC Research Institute,  
Date: 1853-1858, June 2-6, 1996.  
Note: Appears in International Conference on Neural Networks, ICNN 96, Washington DC, pp.  Elman and  
Abstract: Recurrent neural networks are complex parametric dynamic systems that can exhibit a wide range of different behavior. We consider the task of grammatical inference with recurrent neural networks. Specifically, we consider the task of classifying natural language sentences as grammatical or ungrammatical can a recurrent neural network be made to exhibit the same kind of discriminatory power which is provided by the Principles and Parameters linguistic framework, or Government and Binding theory? We attempt to train a network, without the bifurcation into learned vs. innate components assumed by Chomsky, to produce the same judgments as native speakers on sharply grammatical/ungrammatical data. We consider how a recurrent neural network could possess linguistic capability, and investigate the properties of Elman, Narendra & Parthasarathy (N&P) and Williams & Zipser (W&Z) recurrent networks, and Frasconi-Gori-Soda (FGS) locally recurrent networks in this setting. We show that both 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N.A. Chomsky. </author> <title> Three models for the description of language. </title> <journal> IRE Transactions on Information Theory, </journal> <volume> IT-2:113-124, </volume> <year> 1956. </year>
Reference-contexts: Our task differs from these in that the grammar is considerably more complex. In the Chomsky hierarchy of phrase structured grammars, the simplest grammar and its associated automata are regular grammars and finite state automata (FSA). However, it has been firmly established <ref> [1] </ref> that the syntactic structures of natural language cannot be parsimoniously described by regular languages.
Reference: [2] <author> N.A. Chomsky. </author> <title> Lectures on Government and Binding. </title> <publisher> Foris Publications, </publisher> <year> 1981. </year>
Reference-contexts: The language faculty has impressive discriminatory power, in the sense that a single word, as seen in the example above, can result in sharp differences in acceptability or interpretation. Given this and many other examples across various languages, some linguists (chiefly Chomsky <ref> [2] </ref>) have hypothesized that such knowledge is only partially acquired: the lack of variation across speakers, and indeed, languages for certain classes of data suggests that there exists a fixed or innate component of the language system modulated by minor language-specific parametric variations, e.g. surface word order.
Reference: [3] <author> N.A. Chomsky. </author> <title> Knowledge of Language: Its Nature, Origin, and Use. </title> <type> Prager, </type> <year> 1986. </year>
Reference-contexts: Language and Its Acquisition One of the most important questions for the study of human language is: How do people manage to learn such a complex rule system? A system so complex that linguists to date have been unable to construct a formal description <ref> [3] </ref>. Let us consider for a moment the kind of knowledge about language that native speakers often take for granted.
Reference: [4] <author> A. Cleeremans, D. Servan-Schreiber, and J.L. McClelland. </author> <title> Finite state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 372-381, </pages> <year> 1989. </year>
Reference-contexts: However, finite-state models cannot represent hierarchical structures as found in natural language 1 [20]. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference <ref> [4] </ref> [11]. Recurrent networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [7] [13].
Reference: [5] <author> J. P. Crutchfield and K. Young. </author> <title> Computation at the onset of chaos. </title> <editor> In W. Zurek, editor, </editor> <title> Complexity, Entropy and the Physics of Information. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: The algorithm is currently only practical for relatively small grammars [20]. It has been shown that recurrent networks have the representational power required for hierarchical solutions [8], and that they are Turing equivalent [25]. The recurrent neural networks investigated in this paper constitute complex, dynamical systems. Crutchfield and Young <ref> [5] </ref> have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling. They have shown that these systems are not regular, but are finitely described by indexed context-free grammars. Several modern computational linguistic grammatical theories fall in this class [15] [23]. 1.2.
Reference: [6] <author> C. Darken and J.E. Moody. </author> <title> Note on learning rate schedules for stochastic optimization. </title> <editor> In R.P. Lippmann, J.E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 832-838. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Moody and Darkin have proposed "search then converge" learning rate schedules of the form <ref> [6] </ref>: (t) = 0 = (1 + t=t ) where (t) is the learning rate at time t, 0 is the initial learning rate, and t is a constant. We have found the use of learning rate schedules to improve performance considerably as shown in table 1. 5. Sigmoid functions.
Reference: [7] <author> J.L. Elman. </author> <title> Structured representations and connectionist models. </title> <booktitle> In 6th Annual Proceedings of the Cognitive Science Society, </booktitle> <pages> pages 17-25, </pages> <year> 1984. </year>
Reference-contexts: In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [4] [11]. Recurrent networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: <ref> [7] </ref> [13]. Neural network models have been shown to be able to account for a variety of phenomena in phonology [10] [27], morphology [12] [24] and role assignment [18] [26]. Induction of simpler grammars has been addressed often - eg. [28] [11] on learning Tomita languages.
Reference: [8] <author> J.L. Elman. </author> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <journal> Machine Learning, </journal> 7(2/3):195-226, 1991. 
Reference-contexts: The algorithm is currently only practical for relatively small grammars [20]. It has been shown that recurrent networks have the representational power required for hierarchical solutions <ref> [8] </ref>, and that they are Turing equivalent [25]. The recurrent neural networks investigated in this paper constitute complex, dynamical systems. Crutchfield and Young [5] have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling. <p> We have used the local-output version where the output of a node, y (t) = f (wy (t 1) + P n 2. Narendra and Parthasarathy [19]. A recurrent network with feedback connections from each output node to all hidden nodes. 3. Elman <ref> [8] </ref>. A recurrent network with feedback from each hidden node to all hidden nodes. 4. Williams and Zipser [29]. A recurrent network where all nodes are connected to all other nodes.
Reference: [9] <author> P. Frasconi, M. Gori, and G. </author> <title> Soda. Local feedback multilayered networks. </title> <journal> Neural Computation, </journal> <volume> 4(1) </volume> <pages> 120-130, </pages> <year> 1992. </year>
Reference-contexts: Obviously, a word, e.g. to, may be part of more than one part-of-speech. 3. Network Models We tested the following models. We expect the FGS architecture to be unable to perform the task and include it primarily as a control case. 1. Frasconi-Gori-Soda locally recurrent networks <ref> [9] </ref>. A multi-layer perception augmented with local feedback around each hidden node. We have used the local-output version where the output of a node, y (t) = f (wy (t 1) + P n 2. Narendra and Parthasarathy [19].
Reference: [10] <author> M. Gasser and C. Lee. </author> <title> Networks that learn phonology. </title> <type> Technical report, </type> <institution> Computer Science Department, Indiana University, </institution> <year> 1990. </year>
Reference-contexts: Recurrent networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [7] [13]. Neural network models have been shown to be able to account for a variety of phenomena in phonology <ref> [10] </ref> [27], morphology [12] [24] and role assignment [18] [26]. Induction of simpler grammars has been addressed often - eg. [28] [11] on learning Tomita languages. Our task differs from these in that the grammar is considerably more complex.
Reference: [11] <author> C. Lee Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, and Y.C. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 393-405, </pages> <year> 1992. </year>
Reference-contexts: However, finite-state models cannot represent hierarchical structures as found in natural language 1 [20]. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [4] <ref> [11] </ref>. Recurrent networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [7] [13]. <p> Neural network models have been shown to be able to account for a variety of phenomena in phonology [10] [27], morphology [12] [24] and role assignment [18] [26]. Induction of simpler grammars has been addressed often - eg. [28] <ref> [11] </ref> on learning Tomita languages. Our task differs from these in that the grammar is considerably more complex. In the Chomsky hierarchy of phrase structured grammars, the simplest grammar and its associated automata are regular grammars and finite state automata (FSA).
Reference: [12] <author> M. Hare, D. Corina, and G.W. Cottrell. </author> <title> Connectionist perspective on prosodic structure. </title> <type> Technical Report CRL Newsletter Volume 3 Number 2, </type> <institution> Centre for Research in Language, University of California, </institution> <address> San Diego, </address> <year> 1989. </year>
Reference-contexts: Recurrent networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [7] [13]. Neural network models have been shown to be able to account for a variety of phenomena in phonology [10] [27], morphology <ref> [12] </ref> [24] and role assignment [18] [26]. Induction of simpler grammars has been addressed often - eg. [28] [11] on learning Tomita languages. Our task differs from these in that the grammar is considerably more complex.
Reference: [13] <author> Catherine L. Harris and J.L. Elman. </author> <title> Representing variable information with simple recurrent networks. </title> <booktitle> In 6th Annual Proceedings of the Cognitive Science Society, </booktitle> <pages> pages 635-642, </pages> <year> 1984. </year>
Reference-contexts: In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [4] [11]. Recurrent networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [7] <ref> [13] </ref>. Neural network models have been shown to be able to account for a variety of phenomena in phonology [10] [27], morphology [12] [24] and role assignment [18] [26]. Induction of simpler grammars has been addressed often - eg. [28] [11] on learning Tomita languages.
Reference: [14] <author> B. G. Horne and C. Lee Giles. </author> <title> An experimental comparison of recurrent neural networks. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 697-704. </pages> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: The fact that the more powerful Elman and W&Z networks do provide increased performance suggests that they are able to find structure in the data which cannot be modeled by the remaining networks. Another comparison of recurrent neural network architectures, that of Giles and Horne <ref> [14] </ref>, compared various networks on randomly generated 6 and 64-state finite memory machines.
Reference: [15] <author> A. K. Joshi. </author> <title> Tree adjoining grammars: how much context-sensitivity is required to provide reasonable structural descriptions? In L. </title> <editor> Karttunen D. R. Dowty and A. M. Zwicky, editors, </editor> <booktitle> Natural Language Parsing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1985. </year>
Reference-contexts: Crutchfield and Young [5] have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling. They have shown that these systems are not regular, but are finitely described by indexed context-free grammars. Several modern computational linguistic grammatical theories fall in this class <ref> [15] </ref> [23]. 1.2. Language and Its Acquisition One of the most important questions for the study of human language is: How do people manage to learn such a complex rule system? A system so complex that linguists to date have been unable to construct a formal description [3].
Reference: [16] <author> H. Lasnik and J. Uriagereka. </author> <title> A Course in GB Syntax: Lectures on Binding and Empty Categories. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: Data Our data consists of 552 English positive and negative examples taken from an introductory GB-linguistics textbook by Lasnik and Uriagereka <ref> [16] </ref>. Most of these examples are organized into minimal pairs like the example I am happy for John to be here/*I am happy John to be here that we have seen earlier.
Reference: [17] <author> Steve Lawrence, C. Lee Giles, and Sandiway Fong. </author> <title> On the applicability of neural network and machine learning methodologies to natural language processing. </title> <institution> Technical Report UMIACS-TR-95-64 and CS-TR-3479, Institute for Advanced Computer Studies, University of Maryland, College Park MD 20742, </institution> <year> 1995. </year>
Reference-contexts: In order to make the number of weights in each architecture approximately equal we have used only single word inputs for the W&Z model but two word inputs for the others. This reduction in dimensionality for the W&Z network improved performance. More details can be found in <ref> [17] </ref>. Our goal was to train a network using only a small temporal input window. Initially, we were unable to do this.
Reference: [18] <author> R. Miikkulainen and M. Dyer. </author> <title> Encoding input/output representations in connectionist cognitive systems. </title> <editor> In D. S. Touretzky, G. E. Hinton, and T. J. Sejnowski, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 188-195, </pages> <address> Los Altos, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [10] [27], morphology [12] [24] and role assignment <ref> [18] </ref> [26]. Induction of simpler grammars has been addressed often - eg. [28] [11] on learning Tomita languages. Our task differs from these in that the grammar is considerably more complex.
Reference: [19] <author> K.S. Narendra and K. Parthasarathy. </author> <title> Identification and control of dynamical systems using neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(1) </volume> <pages> 4-27, </pages> <year> 1990. </year>
Reference-contexts: Frasconi-Gori-Soda locally recurrent networks [9]. A multi-layer perception augmented with local feedback around each hidden node. We have used the local-output version where the output of a node, y (t) = f (wy (t 1) + P n 2. Narendra and Parthasarathy <ref> [19] </ref>. A recurrent network with feedback connections from each output node to all hidden nodes. 3. Elman [8]. A recurrent network with feedback from each hidden node to all hidden nodes. 4. Williams and Zipser [29]. A recurrent network where all nodes are connected to all other nodes.
Reference: [20] <author> F. Pereira and Y. Schabes. </author> <title> Inside-outside re-estimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th annual meeting of the ACL, </booktitle> <pages> pages 128-135, </pages> <address> Newark, </address> <year> 1992. </year>
Reference-contexts: The most successful stochastic language models have been based on finite-state descriptions such as n-grams or hidden Markov models. However, finite-state models cannot represent hierarchical structures as found in natural language 1 <ref> [20] </ref>. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [4] [11]. Recurrent networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [7] [13]. <p> Lee Giles is also with the Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742. 1 The inside-outside re-estimation algorithm is an extension of hidden Markov models intended to be useful for learning hierarchical systems. The algorithm is currently only practical for relatively small grammars <ref> [20] </ref>. It has been shown that recurrent networks have the representational power required for hierarchical solutions [8], and that they are Turing equivalent [25]. The recurrent neural networks investigated in this paper constitute complex, dynamical systems.
Reference: [21] <author> D. M. Pesetsky. </author> <title> Paths and Categories. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1982. </year>
Reference-contexts: give or persuade would be representative of other classes. 3 Flushing out the 2 As is conventional, we use the asterisk to mark examples of ungrammaticality. 3 Following classical GB theory, these classes are synthesized from the theta-grids of individual predicates via the Canonical Structural Realization (CSR) mechanism of Pesetsky <ref> [21] </ref>. subcategorization requirements along these lines for lexical items in the training set resulted in 9 classes for verbs, 4 for nouns and adjectives, and 2 for prepositions.
Reference: [22] <author> J.B. Pollack. </author> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 227-252, </pages> <year> 1991. </year>
Reference-contexts: Certain phenomena (eg. center embedding) are more compactly described by context-free grammars which are recognized by push-down automata, while others (eg. crossed-serial dependencies and agreement) are better described by context-sensitive grammars which are recognized by linear bounded automata <ref> [22] </ref>. fl http://www.neci.nj.nec.com/homepages/lawrence, http://www.elec.uq.edu.au/~lawrence y Steve Lawrence is also with Electrical and Computer Engineering, University of Queensland, St. Lucia Qld 4072, Australia.
Reference: [23] <author> C. Pollard. </author> <title> Generalised context-free grammars, head grammars and natural language. </title> <type> PhD thesis, </type> <institution> Department of Linguistics, Stanford University, </institution> <address> Palo Alto, CA, </address> <year> 1984. </year>
Reference-contexts: Crutchfield and Young [5] have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling. They have shown that these systems are not regular, but are finitely described by indexed context-free grammars. Several modern computational linguistic grammatical theories fall in this class [15] <ref> [23] </ref>. 1.2. Language and Its Acquisition One of the most important questions for the study of human language is: How do people manage to learn such a complex rule system? A system so complex that linguists to date have been unable to construct a formal description [3].
Reference: [24] <author> D.E. Rumelhart and J.L. McClelland. </author> <title> Parallel Distributed Processing: </title> <journal> Explorations in the Microstructure of Cognition, </journal> <volume> volume 1, 2. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: Recurrent networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [7] [13]. Neural network models have been shown to be able to account for a variety of phenomena in phonology [10] [27], morphology [12] <ref> [24] </ref> and role assignment [18] [26]. Induction of simpler grammars has been addressed often - eg. [28] [11] on learning Tomita languages. Our task differs from these in that the grammar is considerably more complex.
Reference: [25] <author> H.T. Siegelmann and E.D. Sontag. </author> <title> On the computational power of neural nets. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50(1) </volume> <pages> 132-150, </pages> <year> 1995. </year>
Reference-contexts: The algorithm is currently only practical for relatively small grammars [20]. It has been shown that recurrent networks have the representational power required for hierarchical solutions [8], and that they are Turing equivalent <ref> [25] </ref>. The recurrent neural networks investigated in this paper constitute complex, dynamical systems. Crutchfield and Young [5] have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling. They have shown that these systems are not regular, but are finitely described by indexed context-free grammars.
Reference: [26] <author> M. F. St. John and J.L. McClelland. </author> <title> Learning and applying contextual constraints in sentence comprehension. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 5-46, </pages> <year> 1990. </year>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [10] [27], morphology [12] [24] and role assignment [18] <ref> [26] </ref>. Induction of simpler grammars has been addressed often - eg. [28] [11] on learning Tomita languages. Our task differs from these in that the grammar is considerably more complex.
Reference: [27] <author> D. S. Touretzky. </author> <title> Towards a connectionist phonology: The "many maps" approach to sequence manipulation. </title> <booktitle> In Proceedings of the 11th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 188-195, </pages> <year> 1989. </year>
Reference-contexts: Recurrent networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [7] [13]. Neural network models have been shown to be able to account for a variety of phenomena in phonology [10] <ref> [27] </ref>, morphology [12] [24] and role assignment [18] [26]. Induction of simpler grammars has been addressed often - eg. [28] [11] on learning Tomita languages. Our task differs from these in that the grammar is considerably more complex.
Reference: [28] <author> R.L. Watrous and G.M. Kuhn. </author> <title> Induction of finite-state languages using second-order recurrent networks. </title> <booktitle> Neural Computation, </booktitle> <address> 4(3):406, </address> <year> 1992. </year>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [10] [27], morphology [12] [24] and role assignment [18] [26]. Induction of simpler grammars has been addressed often - eg. <ref> [28] </ref> [11] on learning Tomita languages. Our task differs from these in that the grammar is considerably more complex. In the Chomsky hierarchy of phrase structured grammars, the simplest grammar and its associated automata are regular grammars and finite state automata (FSA).
Reference: [29] <author> R.J. Williams and D. Zipser. </author> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 270-280, </pages> <year> 1989. </year>
Reference-contexts: Narendra and Parthasarathy [19]. A recurrent network with feedback connections from each output node to all hidden nodes. 3. Elman [8]. A recurrent network with feedback from each hidden node to all hidden nodes. 4. Williams and Zipser <ref> [29] </ref>. A recurrent network where all nodes are connected to all other nodes. For input to the neural networks, the data was encoded into a fixed length window made up of segments containing eight separate inputs, corresponding to the classifications noun, verb, adjective, etc.
Reference: [30] <author> R.J. Williams and D. Zipser. </author> <title> Gradient-based learning algorithms for recurrent connectionist networks. </title> <editor> In Y. Chauvin and D.E. Rumelhart, editors, </editor> <title> Backpropagation: Theory, Architectures, and Applications. </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1990. </year>
Reference-contexts: The size of the window was variable from one word to the length of the longest sentence. 4. Gradient Descent We have used backpropagation-through-time <ref> [30] </ref> to train the globally-recurrent networks and the gradient descent algorithm described by the authors for the FGS network. The error surface of a multi-layer network is generally non-convex, non-quadratic, and often has large dimensionality. We found the standard gradient descent algorithms to be impractical for our problem.
References-found: 30

