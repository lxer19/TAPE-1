URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-73.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.93.html
Root-URL: 
Title: An Evaluation of State Sharing Techniques in Distributed Operating Systems  
Author: Ranjit John Mustaque Ahamad Umakishore Ramachandran R. Ananthanarayanan Ajay Mohindra 
Note: This work was supported in part by the National Science Foundation under grants CCR-8619886, CCR-9106627, MIP-9058430 and ARPA contract N00174-93-K-0150.  
Address: Atlanta, GA 30332  Mountain View, CA  Yorktown Heights, NY  
Affiliation: College of Computing Georgia Institute of Technology  Silicon Graphics Inc.  IBM T. J. Watson Research Center  
Abstract: A shared memory abstraction in distributed systems (DSM) provides ease of programming but could be costly to implement. Many protocols have been proposed recently that are based on different approaches for exploiting program semantics. We have implemented four different protocols that embody the different memory semantics and have evaluated them using applications that capture a wide range of state sharing patterns. Our main goal is to quantify the relative performance of these protocols with respect to representative applications. One of the results is that memory systems that provide weaker consistency or provide consistency only at well defined synchronization points (e.g., release consistency), cannot completely avoid the overhead associated with DSM. The disparity between the performance using these protocols and programmer controlled state sharing varied from being insignificant to as high as 45% depending on application characteristics and system size. The use of synchronization information in conjunction with user directives will bridge this performance gap but may decrease the programming ease of DSM. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <type> Technical Report CS-1051, </type> <institution> University of Wisconsin, Madison, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: M 2 , the third memory system we implement makes use of synchronization operations in the coherence activities for shared data. This approach has been advocated by several researchers <ref> [11, 1, 14, 7, 17] </ref>. In our implementation, we buffer the writes to shared data locally and propagate them to other nodes at certain synchronization points in the program. 4. In our last system, called M 3 , user provided information is used to direct coherence activities.
Reference: [2] <author> Mustaque Ahamad, Phillip W. Hutto, and Ranjit John. </author> <title> Implementing and programming causal distributed shared memory. </title> <booktitle> In 11th International Conference on Distributed Compututing, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: The programming task is simplified by DSM because both local and remote state can be accessed uniformly. Since nodes in a distributed system do not physically share memory, the DSM abstraction has to be implemented in software. Many researchers have investigated <ref> [18, 6, 14, 7, 2, 20] </ref> distributed shared memories. Several approaches have been investigated to mitigate the performance problems of DSM systems. These include weaker consistency for shared data [19, 2], use of synchronization information in maintaining coherence [14, 7, 17] and user-directed coherence [6, 3]. <p> Many researchers have investigated [18, 6, 14, 7, 2, 20] distributed shared memories. Several approaches have been investigated to mitigate the performance problems of DSM systems. These include weaker consistency for shared data <ref> [19, 2] </ref>, use of synchronization information in maintaining coherence [14, 7, 17] and user-directed coherence [6, 3]. These approaches have some impact on programming but it is claimed that they avoid unnecessary communication and hence provide better performance. <p> The second memory system, M 1 , is based on the weak consistency approach. In par-ticular, it is derived from causal orderings [16, 8], established because of interactions between processors. M 1 implements causal memory <ref> [2] </ref> which guarantees that all processors view memory operations in an order that is consistent with causality. Memory operations that are not causally ordered can be seen by different processors in different order which results in weaker consistency. 3.
Reference: [3] <author> R. Ananthanarayanan, M. Ahamad, and R.J. LeBlanc. </author> <title> Coherence, synchronization and state-sharing in distributed shared memory applications. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <year> 1993. </year>
Reference-contexts: Many researchers have investigated [18, 6, 14, 7, 2, 20] distributed shared memories. Several approaches have been investigated to mitigate the performance problems of DSM systems. These include weaker consistency for shared data [19, 2], use of synchronization information in maintaining coherence [14, 7, 17] and user-directed coherence <ref> [6, 3] </ref>. These approaches have some impact on programming but it is claimed that they avoid unnecessary communication and hence provide better performance. However, there has been no study to investigate the relative gains made possible in distributed systems by these different approaches. <p> An application programmer provides explicit associations between synchronization variables and corresponding shared data. M 3 makes use of such associations to perform the minimal amount of consistency activity needed for the correct execution of the program, and thus comes close to message passing in terms of the communication requirements <ref> [7, 3] </ref>. In our study, the first three systems (M 0 to M 2 ) differ from the last one because they are all page based.
Reference: [4] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report Report RNR-91-002, </type> <institution> NAS Systems Division, Applied Research Branch, NASA Ames Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: We have implemented DSM systems based on all the four above approaches using a uniform set of mechanisms. A comprehensive evaluation of these DSM systems requires a set of applications that capture a range of data sharing patterns. We have chosen the NAS kernels <ref> [4] </ref> which represent the access patterns of many scientific computations. In addition, we have implemented SOR, matrix multiplication, and a branch-and-bound solution to the traveling salesperson problem. The results of our experiments lead to several interesting insights.
Reference: [5] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Experience with distributed program-ming in Orca. </title> <booktitle> In In Intl. Conf. on Computer Languages, </booktitle> <year> 1990. </year>
Reference-contexts: Our implementation is for a 13 city tour and is similar to the one reported in <ref> [5] </ref>. It uses a branch-and-bound method. A set of partial tours are generated and processes evaluate these partial tours in parallel. They all share a work queue that stores the partial tours and the value of the best tour that has been found so far to discard bad tours.
Reference: [6] <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the 2nd ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 168-177, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: The programming task is simplified by DSM because both local and remote state can be accessed uniformly. Since nodes in a distributed system do not physically share memory, the DSM abstraction has to be implemented in software. Many researchers have investigated <ref> [18, 6, 14, 7, 2, 20] </ref> distributed shared memories. Several approaches have been investigated to mitigate the performance problems of DSM systems. These include weaker consistency for shared data [19, 2], use of synchronization information in maintaining coherence [14, 7, 17] and user-directed coherence [6, 3]. <p> Many researchers have investigated [18, 6, 14, 7, 2, 20] distributed shared memories. Several approaches have been investigated to mitigate the performance problems of DSM systems. These include weaker consistency for shared data [19, 2], use of synchronization information in maintaining coherence [14, 7, 17] and user-directed coherence <ref> [6, 3] </ref>. These approaches have some impact on programming but it is claimed that they avoid unnecessary communication and hence provide better performance. However, there has been no study to investigate the relative gains made possible in distributed systems by these different approaches. <p> In addition, we have implemented SOR, matrix multiplication, and a branch-and-bound solution to the traveling salesperson problem. The results of our experiments lead to several interesting insights. One of the results is that unless the programmer provides information that is used in coherence maintenance (e.g. annotations in Munin <ref> [6] </ref> or associations of Midway [7]), DSM protocols will have performance overheads. These overheads manifest as extra messages, the amount of data transferred in these messages and the additional processing introduced by them. Thus, reducing the number of messages is the key to improving performance of a DSM system. <p> A node records all modifications to a page in a 4 shadow copy (transparent to the program). Prior to exiting a synchronization region (as in release consistency), an XOR of the original page and its shadow is generated for each dirty page (similar to the diff in <ref> [6] </ref>). An optimization that is employed in this implementation is to limit the size of the diff messages by recognizing when there is no change in the page beyond a certain offset, and truncating the diff message after this offset. <p> There are several other observations we would like to make about this study in general. Appropriateness of Applications: The NAS kernels come from the domain of numerical applications and several applications having similar characteristics (e.g. set of linear equations, partial differential equations etc.) have been studied in DSM systems <ref> [6, 18] </ref>. In addition, the TSP and SOR applications have been investigated extensively in evaluating DSM systems. We believe these applications do capture the data sharing patterns of many applications that can be run on distributed platforms. <p> For example, updates were not sent to nodes that did not need them in M 2 and readonly data was tagged to reduce the size of version vectors for M 1 . These optimizations are fundamentally different from the directives of M 3 or the annotations used in Munin <ref> [6] </ref>. This is because correct execution of the application does not depend on these optimizations. We believe such optimizations can be easily automated in the compiler and do not require any change in the application programming.
Reference: [7] <author> Brian N. Bershad and Matthew J. Zekauskas. Midway: </author> <title> Shared memory parallel pro-gramming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: The programming task is simplified by DSM because both local and remote state can be accessed uniformly. Since nodes in a distributed system do not physically share memory, the DSM abstraction has to be implemented in software. Many researchers have investigated <ref> [18, 6, 14, 7, 2, 20] </ref> distributed shared memories. Several approaches have been investigated to mitigate the performance problems of DSM systems. These include weaker consistency for shared data [19, 2], use of synchronization information in maintaining coherence [14, 7, 17] and user-directed coherence [6, 3]. <p> Many researchers have investigated [18, 6, 14, 7, 2, 20] distributed shared memories. Several approaches have been investigated to mitigate the performance problems of DSM systems. These include weaker consistency for shared data [19, 2], use of synchronization information in maintaining coherence <ref> [14, 7, 17] </ref> and user-directed coherence [6, 3]. These approaches have some impact on programming but it is claimed that they avoid unnecessary communication and hence provide better performance. However, there has been no study to investigate the relative gains made possible in distributed systems by these different approaches. <p> M 2 , the third memory system we implement makes use of synchronization operations in the coherence activities for shared data. This approach has been advocated by several researchers <ref> [11, 1, 14, 7, 17] </ref>. In our implementation, we buffer the writes to shared data locally and propagate them to other nodes at certain synchronization points in the program. 4. In our last system, called M 3 , user provided information is used to direct coherence activities. <p> An application programmer provides explicit associations between synchronization variables and corresponding shared data. M 3 makes use of such associations to perform the minimal amount of consistency activity needed for the correct execution of the program, and thus comes close to message passing in terms of the communication requirements <ref> [7, 3] </ref>. In our study, the first three systems (M 0 to M 2 ) differ from the last one because they are all page based. <p> The results of our experiments lead to several interesting insights. One of the results is that unless the programmer provides information that is used in coherence maintenance (e.g. annotations in Munin [6] or associations of Midway <ref> [7] </ref>), DSM protocols will have performance overheads. These overheads manifest as extra messages, the amount of data transferred in these messages and the additional processing introduced by them. Thus, reducing the number of messages is the key to improving performance of a DSM system. <p> M 2 , on the other hand, sends the changes to all the nodes that cache the changed data. Thus, the approach taken in M 2 is similar to release consistency [11], whereas the one taken in M 3 is similar to entry consistency <ref> [7] </ref> and lazy release consistency [14]. However, lazy release consistency must propagate information about changes to all shared data whereas the explicit associations of M 3 allow it to send only the data that will be accessed in a synchronization region.
Reference: [8] <author> Kenneth Birman, Andre Schiper, and Pat Stephenson. </author> <title> Lightweight causal and atomic group multicast. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> (9, </volume> 3):272-314, 1991. 
Reference-contexts: This would require coherence maintenance of the state and hence would be similar to a shared memory abstraction. 1 2. The second memory system, M 1 , is based on the weak consistency approach. In par-ticular, it is derived from causal orderings <ref> [16, 8] </ref>, established because of interactions between processors. M 1 implements causal memory [2] which guarantees that all processors view memory operations in an order that is consistent with causality.
Reference: [9] <author> J. S. Chase, F. G. Amador, H. M. Levy E. D. Lazowska, and R. J. Littlefield. </author> <title> The amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th Symposium on Operating System Principles, </booktitle> <year> 1989. </year>
Reference-contexts: The application completes when all tours have been explored. 4.3 Successive Over Relaxation (SOR) SOR is an iterative method for solving discretized Laplace equations on a grid (size 128fi128). The program is based on the parallel Red/Black SOR algorithm as described in <ref> [9] </ref>. The grid is partitioned among the processors and all the communication occurs between neighboring processors. Only the boundary elements of the grid need to be communicated between iterations. 7 4.4 Matrix Multiply (MM) Lastly, we implemented everyone's favorite matrix multiply using input matrices of sizes 256x256.
Reference: [10] <author> B. D. Fleisch and G. J. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <booktitle> In Proceedings of the ACM Symposium on Operating System Principles, </booktitle> <year> 1989. </year>
Reference-contexts: We have implemented the invalidation based protocol used in the Ivy system [18] with a fixed 3 manager. We added several optimizations to the basic protocol. In particular, we control thrashing between nodes by pinning a page <ref> [10] </ref>, and also avoid the re-sending of page data due to double faults [15]. 3.2 M 1 Weaker Consistency The consistency guarantees provided by M 1 are much weaker than M 0 .
Reference: [11] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <year> 1990. </year> <month> 17 </month>
Reference-contexts: M 2 , the third memory system we implement makes use of synchronization operations in the coherence activities for shared data. This approach has been advocated by several researchers <ref> [11, 1, 14, 7, 17] </ref>. In our implementation, we buffer the writes to shared data locally and propagate them to other nodes at certain synchronization points in the program. 4. In our last system, called M 3 , user provided information is used to direct coherence activities. <p> In particular, if the programs are written with some synchronization model in mind, it is possible to defer consistency actions to certain synchronization points. For example, release consistency <ref> [11] </ref> and buffered consistency [17] allow consistency actions to be deferred until the exit point to a synchronization region, while lazy release consistency [14] allows them to be deferred until the entry point to a synchronization region. The implementation of M 2 is as follows. <p> This determination is made at runtime based on the state of the queue associated with the synchronization variable. M 2 , on the other hand, sends the changes to all the nodes that cache the changed data. Thus, the approach taken in M 2 is similar to release consistency <ref> [11] </ref>, whereas the one taken in M 3 is similar to entry consistency [7] and lazy release consistency [14].
Reference: [12] <author> Ranjit John and Mustaque Ahamad. </author> <title> Evaluation of causal distributed shared memory. </title> <type> Technical Report GT-CC-94-34, </type> <institution> Georgia Institute of Technology, </institution> <year> 1994. </year>
Reference-contexts: This is done when a page is brought into the memory of a node as a result of a page fault. We use version vectors <ref> [12] </ref> to detect when a page is overwritten. Version vectors are maintained by each process and a version is associated with each page. Cached pages that have an older version than the values in the version vector received with an incoming page are locally invalidated. <p> This indeed was seen in our experiments. For example, when we ran CGM (size 14000 x 14000) and SOR (size 512 x 512) <ref> [12] </ref>, better speedups were observed for all the memory systems. However, there are differences between the memory systems when it comes to data granularity (the amount of shared data processed between communication events).
Reference: [13] <author> Eric Jul, Henry Levy, Norman Hutchinson, and Andrew Black. </author> <title> Fine-grained mobility in the Emerald system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <month> Feb </month> <year> 1988. </year>
Reference-contexts: The first memory system, M 0 , provides a sequentially consistent memory. Since it provides standard shared memory semantics, it is the easiest to program and also provides a reference point to evaluate the performance of other approaches. 1 Migration can be used to realize locality benefits <ref> [13] </ref> but cannot be used when more than one node attempts to make the state of the callee local. This would require coherence maintenance of the state and hence would be similar to a shared memory abstraction. 1 2.
Reference: [14] <author> P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software dis-tributed shared memory. </title> <journal> SIGARCH Computer Architecture News, </journal> <volume> 20(2), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: The programming task is simplified by DSM because both local and remote state can be accessed uniformly. Since nodes in a distributed system do not physically share memory, the DSM abstraction has to be implemented in software. Many researchers have investigated <ref> [18, 6, 14, 7, 2, 20] </ref> distributed shared memories. Several approaches have been investigated to mitigate the performance problems of DSM systems. These include weaker consistency for shared data [19, 2], use of synchronization information in maintaining coherence [14, 7, 17] and user-directed coherence [6, 3]. <p> Many researchers have investigated [18, 6, 14, 7, 2, 20] distributed shared memories. Several approaches have been investigated to mitigate the performance problems of DSM systems. These include weaker consistency for shared data [19, 2], use of synchronization information in maintaining coherence <ref> [14, 7, 17] </ref> and user-directed coherence [6, 3]. These approaches have some impact on programming but it is claimed that they avoid unnecessary communication and hence provide better performance. However, there has been no study to investigate the relative gains made possible in distributed systems by these different approaches. <p> M 2 , the third memory system we implement makes use of synchronization operations in the coherence activities for shared data. This approach has been advocated by several researchers <ref> [11, 1, 14, 7, 17] </ref>. In our implementation, we buffer the writes to shared data locally and propagate them to other nodes at certain synchronization points in the program. 4. In our last system, called M 3 , user provided information is used to direct coherence activities. <p> For example, release consistency [11] and buffered consistency [17] allow consistency actions to be deferred until the exit point to a synchronization region, while lazy release consistency <ref> [14] </ref> allows them to be deferred until the entry point to a synchronization region. The implementation of M 2 is as follows. A node records all modifications to a page in a 4 shadow copy (transparent to the program). <p> M 2 , on the other hand, sends the changes to all the nodes that cache the changed data. Thus, the approach taken in M 2 is similar to release consistency [11], whereas the one taken in M 3 is similar to entry consistency [7] and lazy release consistency <ref> [14] </ref>. However, lazy release consistency must propagate information about changes to all shared data whereas the explicit associations of M 3 allow it to send only the data that will be accessed in a synchronization region.
Reference: [15] <author> R. E. Kessler and M. Livny. </author> <title> An analysis of distributed shared memory algorithms. </title> <booktitle> In Proceedings of the 9th International Conference on Distributed Computing Systems, </booktitle> <year> 1989. </year>
Reference-contexts: We added several optimizations to the basic protocol. In particular, we control thrashing between nodes by pinning a page [10], and also avoid the re-sending of page data due to double faults <ref> [15] </ref>. 3.2 M 1 Weaker Consistency The consistency guarantees provided by M 1 are much weaker than M 0 . It differs from M 0 because it guarantees that processes only agree on the order of those memory operations that are ordered by causality.
Reference: [16] <author> Leslie Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: This would require coherence maintenance of the state and hence would be similar to a shared memory abstraction. 1 2. The second memory system, M 1 , is based on the weak consistency approach. In par-ticular, it is derived from causal orderings <ref> [16, 8] </ref>, established because of interactions between processors. M 1 implements causal memory [2] which guarantees that all processors view memory operations in an order that is consistent with causality.
Reference: [17] <author> J. Lee and U. Ramachandran. </author> <title> Architectural primitives for a scalable shared memory multiprocessor. </title> <booktitle> In Proceedings of the 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 103-114, </pages> <year> 1991. </year>
Reference-contexts: Many researchers have investigated [18, 6, 14, 7, 2, 20] distributed shared memories. Several approaches have been investigated to mitigate the performance problems of DSM systems. These include weaker consistency for shared data [19, 2], use of synchronization information in maintaining coherence <ref> [14, 7, 17] </ref> and user-directed coherence [6, 3]. These approaches have some impact on programming but it is claimed that they avoid unnecessary communication and hence provide better performance. However, there has been no study to investigate the relative gains made possible in distributed systems by these different approaches. <p> M 2 , the third memory system we implement makes use of synchronization operations in the coherence activities for shared data. This approach has been advocated by several researchers <ref> [11, 1, 14, 7, 17] </ref>. In our implementation, we buffer the writes to shared data locally and propagate them to other nodes at certain synchronization points in the program. 4. In our last system, called M 3 , user provided information is used to direct coherence activities. <p> In particular, if the programs are written with some synchronization model in mind, it is possible to defer consistency actions to certain synchronization points. For example, release consistency [11] and buffered consistency <ref> [17] </ref> allow consistency actions to be deferred until the exit point to a synchronization region, while lazy release consistency [14] allows them to be deferred until the entry point to a synchronization region. The implementation of M 2 is as follows.
Reference: [18] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The programming task is simplified by DSM because both local and remote state can be accessed uniformly. Since nodes in a distributed system do not physically share memory, the DSM abstraction has to be implemented in software. Many researchers have investigated <ref> [18, 6, 14, 7, 2, 20] </ref> distributed shared memories. Several approaches have been investigated to mitigate the performance problems of DSM systems. These include weaker consistency for shared data [19, 2], use of synchronization information in maintaining coherence [14, 7, 17] and user-directed coherence [6, 3]. <p> For example, when a reference is made to a page that is not cached, the fault handler requests the page from the appropriate data manager (there is a one-to-one correspondence between a given page and a data manager which is similar to the fixed manager concept in <ref> [18] </ref>). The implementation of the synchronization constructs (locks, semaphores and barriers) is centralized. For a given synchronization variable, a single server maintains its state and the queue of processes blocked on it. <p> We have implemented the invalidation based protocol used in the Ivy system <ref> [18] </ref> with a fixed 3 manager. We added several optimizations to the basic protocol. <p> There are several other observations we would like to make about this study in general. Appropriateness of Applications: The NAS kernels come from the domain of numerical applications and several applications having similar characteristics (e.g. set of linear equations, partial differential equations etc.) have been studied in DSM systems <ref> [6, 18] </ref>. In addition, the TSP and SOR applications have been investigated extensively in evaluating DSM systems. We believe these applications do capture the data sharing patterns of many applications that can be run on distributed platforms.
Reference: [19] <author> Richard J. Lipton and Jonathan S. Sandberg. </author> <title> PRAM: A scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, Department of Computer Science, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Many researchers have investigated [18, 6, 14, 7, 2, 20] distributed shared memories. Several approaches have been investigated to mitigate the performance problems of DSM systems. These include weaker consistency for shared data <ref> [19, 2] </ref>, use of synchronization information in maintaining coherence [14, 7, 17] and user-directed coherence [6, 3]. These approaches have some impact on programming but it is claimed that they avoid unnecessary communication and hence provide better performance.
Reference: [20] <author> Umakishore Ramachandran, Mustaque Ahamad, and M. Yousef A. Khalidi. </author> <title> Coherence of Distributed Shared Memory: Unifying Synchronization and Data Transfer. </title> <booktitle> In Eighteenth Annual International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: The programming task is simplified by DSM because both local and remote state can be accessed uniformly. Since nodes in a distributed system do not physically share memory, the DSM abstraction has to be implemented in software. Many researchers have investigated <ref> [18, 6, 14, 7, 2, 20] </ref> distributed shared memories. Several approaches have been investigated to mitigate the performance problems of DSM systems. These include weaker consistency for shared data [19, 2], use of synchronization information in maintaining coherence [14, 7, 17] and user-directed coherence [6, 3].
Reference: [21] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Limits to low-latency communication on high-speed networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2), </volume> <month> May </month> <year> 1993. </year> <month> 18 </month>
Reference-contexts: However, faster network will make the difference among the memory systems less significant. This is because the protocols that send large messages (pages) will benefit more from the increased network speed <ref> [21] </ref>. 7 Concluding Remarks A wider acceptance of the DSM abstraction as a state sharing mechanism in distributed systems and the numerous approaches for implementing it motivated our work.
References-found: 21

