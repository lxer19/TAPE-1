URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94442.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: e-mail: mpourzan@lip.ens-lyon.fr  e-mail: btouranc@cs.utk.edu  
Phone: Tel. (+33) 72 72 85 03 Fax (+33) 72 72 80 80  Tel (1) 615 974 8295, Fax (1) 615 974 8296,  
Title: A Parallel Performance Study of Jacobi-like Eigenvalue Solution  
Author: Makan Pourzandi Bernard Tourancheau yz 
Note: This work was supported by MRE grant No. 974, the CNRS-NSF grant No. 950.22/ 07 and the research program C3. On leave from  This work was supported in part by the National Science Foundation under grant ASC-871728, the National Science Foundation Science and Technology Center Cooperative Agreement CCR-8809615, the DARPA and ARO under contract DAAL03-91-C-0047, PRC C 3 CNRS-NSF grant 950.223/07, Archipel SA and MRE under grant 974, and DRET.  
Date: March 24, 1994  
Address: 69364 Lyon Cedex 07, France  Knoxville, TN 37996-1301, USA  4 allee d'Italie, 69364 Lyon Cedex 07, France.  
Affiliation: Laboratoire de l'Informatique du Parallelisme, Unite de Recherche Associee 1398 du CNRS Ecole Normale Superieure de Lyon,  The University of Tennessee Computer Science Department,  LIP, CNRS URA 1398, ENS Lyon,  
Abstract: In this report we focus on Jacobi like resolution of the eigen-problem for a real symmetric matrix from a parallel performance point of view: we try to optimize the algorithm working on the communication intensive part of the code. We discuss several parallel implementations and propose an implementation which overlaps the communications by the computations to reach a better efficiency. We show that the overlapping implementation can lead to significant improvements. We conclude by presenting our future work. 
Abstract-found: 1
Intro-found: 1
Reference: [DCHH88] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and R. Hanson. </author> <title> An Extended Set of Fortran Basic Linear Algebra Subroutines. </title> <journal> ACM Transaction on Mathematical Software, </journal> <volume> 1(14) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Roughly, we have T over comm = T shuffle and T noover comm = T shuffle + T gossip . 7 Experimental results In this subsection, we comment our experimental results that have been done on an iPSC/860 hypercube. We use the level 1 BLAS subroutines <ref> [DCHH88] </ref> to update the columns on sequential and parallel versions. In general, the performances of these subroutines increase with the size of the data treated. This implies some performance gain for longer vectors. In Figure 7, we present the speedup for all versions.
Reference: [DS86] <author> J. Dongarra and DC. Sorensen. </author> <title> Linear algebra on high performance computers. </title> <journal> Parallel Computing, </journal> <volume> 85 </volume> <pages> 221-236, </pages> <year> 1986. </year>
Reference-contexts: For the iPSC/860 ff is 136 s for long messages (larger than 100 bytes) and 75 s for short ones. fi is 3,2 s for a word of 4 bytes <ref> [DS86, Dun90] </ref>.
Reference: [DT92] <author> F. Desprez and B. Tourancheau. LOCCS: </author> <title> Low Overhead Communication and Computation Subroutines. </title> <type> Technical Report 92-44, </type> <institution> Laboratoire d'Informatique du Parallelisme-ENSL, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Then, we discuss the same implementation but using our hand coded gossiping algorithm, following the works of [Fra90, JH89] leading to a very efficient solution. Afterwards, we present the same algorithm with overlapping of the communications by the computations. We use for that, a general methodology, developed in <ref> [DT92, PT93] </ref> and a tuned implementation. In the next section, we compare the experimental results of all algorithms. We show that the overlapping implementation can lead to a 6% improvement of the execution timings and that represents a decrease of 35% of the total communication time. <p> We intend to overlap the shu*e communications and try to take advantage of the matrix symmetry to reduce the computation time. Finally, we are working on a more general version of our gossiping procedure to include it in the LOCCS library <ref> [DT92] </ref>. We run several tests on the Paragon machine.
Reference: [Dun90] <author> T.H Dunigan. </author> <title> Performance of the intel iPSC/860. </title> <type> Technical Report TM-11491, </type> <institution> Oak Ridge National Laboratory, </institution> <month> June 90. </month>
Reference-contexts: The Direct Connect Module (DCM) does the inter-nodes communications. Using the DCM, the communications are independent from the i860 and are routed in circuit switched mode [Int90, MMM91, SB77]. 2 There are several technical reports concerning the iPSC/860 architecture and communication performances <ref> [Dun90, MM91] </ref>. One can easily refer to them to have more details. The i860 is a 40 MHz RISC type processor with 8k bytes of cache memory. It uses two arithmetic units (adder and multiplier) and a graphic unit. These units could be used in pipelined and chained modes. <p> Actually the performances are 11.5 Mflops with our present compilers (64 bits) for the average vector length of our experiments (512 words). The gap between the peak and sustain performance is principally due to memory delays (cache miss, page-translation-miss, DRAM access delays : : : ) <ref> [Dun90] </ref>. 3 Analysis Model Let t a be the time to perform a floating point operation (double precision, addition or multiplication). The time to communicate between 2 neighbor nodes is modelized by ff+Lflfi where ff is the startup time and fi is the time to transmit a word. <p> For the iPSC/860 ff is 136 s for long messages (larger than 100 bytes) and 75 s for short ones. fi is 3,2 s for a word of 4 bytes <ref> [DS86, Dun90] </ref>.
Reference: [Ebe86] <author> P. J. Eberlein. </author> <title> Comments on some parallel Jacobi orderings. </title> <type> Technical Report 86-16, </type> <institution> Dept. Comp. Sci., State University of New York at Buffalo, </institution> <year> 1986. </year>
Reference-contexts: Luk and Park [LP89b] proved the convergence for odd-even Jacobi sets by proving it is equivalent to column-cycling orderings. The odd-even ordering, however, is not optimal for parallel computation in that it completes a sweep in n sweeps instead of (n 1). We use the caterpillar-track ordering <ref> [Ebe86, EP90] </ref> which which is identical to the odd-even ordering [LP89a]. In Figure 4, we show this parallel ordering for n = 8. We embed a ring in the hypercube and do the communications through this ring.
Reference: [Ebe87] <author> P. J. Eberlein. </author> <title> On one-sided Jacobi methods for parallel computation. </title> <journal> SIAM J. ALG. DISC. METH., </journal> <volume> 8(4) </volume> <pages> 790-796, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: In this report we focus on Jacobi like resolution of the eigen-problem for a real symmetric matrix from a parallel performance point of view: we try to optimize the algorithm working on the communication intensive part of the code. We discuss several parallel implementations <ref> [Ebe87, EP90, Fou89, LP89a] </ref> and propose an implementation which overlaps the communications by the computations to reach a better efficiency. We first present briefly our target machine and our analysis model in the sections 2 and 3. In Chapter 4, we present the sequential Jacobi like resolution [Jac46, Mod88, Wil65].
Reference: [EP90] <author> P. J. Eberlein and H. Park. </author> <title> Efficient implementation of Jacobi algorithms and Jacobi sets on distributed memory arrchitectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> (8):358-366, 1990. 
Reference-contexts: In this report we focus on Jacobi like resolution of the eigen-problem for a real symmetric matrix from a parallel performance point of view: we try to optimize the algorithm working on the communication intensive part of the code. We discuss several parallel implementations <ref> [Ebe87, EP90, Fou89, LP89a] </ref> and propose an implementation which overlaps the communications by the computations to reach a better efficiency. We first present briefly our target machine and our analysis model in the sections 2 and 3. In Chapter 4, we present the sequential Jacobi like resolution [Jac46, Mod88, Wil65]. <p> Luk and Park [LP89b] proved the convergence for odd-even Jacobi sets by proving it is equivalent to column-cycling orderings. The odd-even ordering, however, is not optimal for parallel computation in that it completes a sweep in n sweeps instead of (n 1). We use the caterpillar-track ordering <ref> [Ebe86, EP90] </ref> which which is identical to the odd-even ordering [LP89a]. In Figure 4, we show this parallel ordering for n = 8. We embed a ring in the hypercube and do the communications through this ring. <p> At each stage k of the ordering, a processor p, according to k, sends a column to a neighbor (p + 1 or p 1 whether k is even or odd) and receives a column from the another neighbor <ref> [EP90] </ref>. The communication schema is shown in Figure 3. Each block represents a processor. The numbers in each block are the column numbers housing in this processor and the arrows indicate the communication at each stage. We only consider the case with an 6 even number of matrix columns.
Reference: [FH60] <author> G. E. Forsythe and P. Henrici. </author> <title> The cyclic Jacobi method for computing the principal values of a complex matrix. </title> <journal> Trans. Amer. Math. Soc., </journal> <volume> 94 </volume> <pages> 1-23, </pages> <year> 1960. </year>
Reference-contexts: The use of various orderings does make a difference in convergence rates, which in some instances is quite striking [ME93]. But, quadric convergence is always observed in real symmetric cases. Convergence for symmetric matrices has been proven by Forsythe and Henrici <ref> [FH60] </ref> for column-cycling ordering. Luk and Park [LP89b] proved the convergence for odd-even Jacobi sets by proving it is equivalent to column-cycling orderings. The odd-even ordering, however, is not optimal for parallel computation in that it completes a sweep in n sweeps instead of (n 1).
Reference: [Fou89] <author> David E. Foulser. </author> <title> A blocked Jacobi method for the symmetric eigen-problem. </title> <type> Technical Report RR-680, </type> <institution> Dept. of Computer Science, Yale University, </institution> <month> February </month> <year> 1989. </year>
Reference-contexts: In this report we focus on Jacobi like resolution of the eigen-problem for a real symmetric matrix from a parallel performance point of view: we try to optimize the algorithm working on the communication intensive part of the code. We discuss several parallel implementations <ref> [Ebe87, EP90, Fou89, LP89a] </ref> and propose an implementation which overlaps the communications by the computations to reach a better efficiency. We first present briefly our target machine and our analysis model in the sections 2 and 3. In Chapter 4, we present the sequential Jacobi like resolution [Jac46, Mod88, Wil65].
Reference: [Fra90] <author> P. </author> <type> Fraigniaud. </type> <institution> Communications intensives dans les architectures a memoires distribuees et Algorithme parallele pour la recherche de racines de polynomes. </institution> <type> PhD thesis, </type> <institution> Ecole Normale Superieure de Lyon, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Afterwards, we discuss the parallel implementations (section 5). We discuss our implementation on the Intel machine iPSC/860 hypercube, using the Intel gossiping procedure. Then, we discuss the same implementation but using our hand coded gossiping algorithm, following the works of <ref> [Fra90, JH89] </ref> leading to a very efficient solution. Afterwards, we present the same algorithm with overlapping of the communications by the computations. We use for that, a general methodology, developed in [DT92, PT93] and a tuned implementation. In the next section, we compare the experimental results of all algorithms. <p> We follow the works of <ref> [Fra90, JH89] </ref> for the implementation of our gossiping procedure on a hypercube network. me = mynode () for i = 0 to d do Destination = me xor 2 i Exchange message of length L fl 2 i with the Destination processor endfor of dimension d.
Reference: [GL90] <author> G. H. Golub and C. F. V. Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <year> 1990. </year> <note> 2nd edition. </note>
Reference-contexts: For an nfi n real symmetric matrix A with elements a pq the Jacobi method <ref> [GL90] </ref> systematically reduces the norm of the non-diagonal elements: of f (A) = p q6=p pq (1) by a sequence of plane (Jacobi) rotations. We call a Jacobi sweep, every n (n1) plane rotations reducing all non diagonal elements. <p> Thus a sequence of matrices A (k) is produced such that lim k!1 A (k) = D, a diagonal matrix consisting of the eigenvalues of A, and lim J (1)T J (2)T J (3)T : : : J (k)T = V; is a matrix consisting of the eigenvectors of A <ref> [GL90, TY91] </ref>. We skip the anni hilation of a (k) pq when a (k) pq &lt; ", because the reduction in off (A) is not worth the cost. This leads to the algorithm which is called the Jacobi threshold method. <p> In the raw-cyclic scheme, we simply pick (p; q) in raw-by-raw fashion. For instance in the case n = 4 the following rotations (1; 2); (1; 3); (1; 4); (2; 3); (2; 4); (3; 4) are made in a complete sweep <ref> [GL90, TY91] </ref>. We present the corresponding sequential algorithm on Figure 1. <p> n (n 1) is a parallel ordering of the set f (i; j)j1 i j ng if for s = 1 : n 1 the rotation set Rot (s) = f (i r ; j r )jr = 1 + n (s 1)=2 : ns=2g consists of non conflicting rotations <ref> [GL90] </ref>. Most of the empirical results for parallel Jacobi-type algorithms that are found in the literature use odd-even ordering. Convergence has been proved on some cases, but, only for odd-even orderings or the orderings equivalent to odd-even one. <p> As in <ref> [GL90] </ref>, we assume this amount of computation is constant, let it be C te flops (for our implementation C te 53 flops). As we mentioned in subsection 4, when executing the rotation (p; q), only rows and columns p and q are altered. <p> Therefore the maximum computation complexity for each sweep k is: max (T k comp ) = n (n 1) (6n + C te ) + 2n C te 3) + n (2 2 There is no rigorous theory that enables one to predict the number of sweeps <ref> [GL90] </ref>. But Brent and Luk have argued heuristically that the number of sweeps is proportional to log 2 (n).
Reference: [Int90] <institution> Intel Corporation. </institution> <note> iPSC/860 User's Guide, </note> <month> June </month> <year> 1990. </year>
Reference-contexts: Each node of the iPSC provides an i860 processor, a Direct Connect Module and 16 Megabytes of memory. The Direct Connect Module (DCM) does the inter-nodes communications. Using the DCM, the communications are independent from the i860 and are routed in circuit switched mode <ref> [Int90, MMM91, SB77] </ref>. 2 There are several technical reports concerning the iPSC/860 architecture and communication performances [Dun90, MM91]. One can easily refer to them to have more details. The i860 is a 40 MHz RISC type processor with 8k bytes of cache memory.
Reference: [Jac46] <author> C.G.J. </author> <title> Jacobi. Uber ein leichtes Verfahren. </title> <type> 1846. 15 </type>
Reference-contexts: We first present briefly our target machine and our analysis model in the sections 2 and 3. In Chapter 4, we present the sequential Jacobi like resolution <ref> [Jac46, Mod88, Wil65] </ref>. Afterwards, we discuss the parallel implementations (section 5). We discuss our implementation on the Intel machine iPSC/860 hypercube, using the Intel gossiping procedure.
Reference: [JH89] <author> S. Johnsson and C. T. Ho. </author> <title> Optimum broadcasting and personalized communication in hypercubes. </title> <journal> IEEE Trans. Comp., </journal> <volume> 38(9) </volume> <pages> 1249-1268, </pages> <year> 1989. </year>
Reference-contexts: Afterwards, we discuss the parallel implementations (section 5). We discuss our implementation on the Intel machine iPSC/860 hypercube, using the Intel gossiping procedure. Then, we discuss the same implementation but using our hand coded gossiping algorithm, following the works of <ref> [Fra90, JH89] </ref> leading to a very efficient solution. Afterwards, we present the same algorithm with overlapping of the communications by the computations. We use for that, a general methodology, developed in [DT92, PT93] and a tuned implementation. In the next section, we compare the experimental results of all algorithms. <p> We follow the works of <ref> [Fra90, JH89] </ref> for the implementation of our gossiping procedure on a hypercube network. me = mynode () for i = 0 to d do Destination = me xor 2 i Exchange message of length L fl 2 i with the Destination processor endfor of dimension d.
Reference: [LP89a] <author> F. T. Luk and H. Park. </author> <title> On parallel Jacobi orderings. </title> <journal> SIAM J. SCI. STAT. COMPUT., </journal> <volume> 10(1) </volume> <pages> 18-26, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: In this report we focus on Jacobi like resolution of the eigen-problem for a real symmetric matrix from a parallel performance point of view: we try to optimize the algorithm working on the communication intensive part of the code. We discuss several parallel implementations <ref> [Ebe87, EP90, Fou89, LP89a] </ref> and propose an implementation which overlaps the communications by the computations to reach a better efficiency. We first present briefly our target machine and our analysis model in the sections 2 and 3. In Chapter 4, we present the sequential Jacobi like resolution [Jac46, Mod88, Wil65]. <p> The odd-even ordering, however, is not optimal for parallel computation in that it completes a sweep in n sweeps instead of (n 1). We use the caterpillar-track ordering [Ebe86, EP90] which which is identical to the odd-even ordering <ref> [LP89a] </ref>. In Figure 4, we show this parallel ordering for n = 8. We embed a ring in the hypercube and do the communications through this ring.
Reference: [LP89b] <author> F. T. Luk and H. Park. </author> <title> A proof of convergence for two parallel Jacobi SVD algorithms. </title> <journal> IEEE Trans. Comput., </journal> <volume> 38(6) </volume> <pages> 806-811, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: The use of various orderings does make a difference in convergence rates, which in some instances is quite striking [ME93]. But, quadric convergence is always observed in real symmetric cases. Convergence for symmetric matrices has been proven by Forsythe and Henrici [FH60] for column-cycling ordering. Luk and Park <ref> [LP89b] </ref> proved the convergence for odd-even Jacobi sets by proving it is equivalent to column-cycling orderings. The odd-even ordering, however, is not optimal for parallel computation in that it completes a sweep in n sweeps instead of (n 1).
Reference: [ME93] <author> M. Mantharam and P. J. Eberlein. </author> <title> New Jacobi-sets for parallel computations. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 437-454, </pages> <year> 1993. </year>
Reference-contexts: Convergence has been proved on some cases, but, only for odd-even orderings or the orderings equivalent to odd-even one. The use of various orderings does make a difference in convergence rates, which in some instances is quite striking <ref> [ME93] </ref>. But, quadric convergence is always observed in real symmetric cases. Convergence for symmetric matrices has been proven by Forsythe and Henrici [FH60] for column-cycling ordering. Luk and Park [LP89b] proved the convergence for odd-even Jacobi sets by proving it is equivalent to column-cycling orderings.
Reference: [MM91] <author> C.L. McCreary and M.E Mcradle. </author> <title> Modeling communication delay on the iPSC/2 and iPSC/860 hypercubes. </title> <type> Technical Report CSE-91-12, </type> <institution> Aubran University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: The Direct Connect Module (DCM) does the inter-nodes communications. Using the DCM, the communications are independent from the i860 and are routed in circuit switched mode [Int90, MMM91, SB77]. 2 There are several technical reports concerning the iPSC/860 architecture and communication performances <ref> [Dun90, MM91] </ref>. One can easily refer to them to have more details. The i860 is a 40 MHz RISC type processor with 8k bytes of cache memory. It uses two arithmetic units (adder and multiplier) and a graphic unit. These units could be used in pipelined and chained modes.
Reference: [MMM91] <author> C.L. McCreary, M.E. Mcardle, and J.D. McCreary. </author> <title> Broadcast communication delay metric for iPSC/2 and iPSC/860 hypercubes. </title> <month> July </month> <year> 1991. </year>
Reference-contexts: Each node of the iPSC provides an i860 processor, a Direct Connect Module and 16 Megabytes of memory. The Direct Connect Module (DCM) does the inter-nodes communications. Using the DCM, the communications are independent from the i860 and are routed in circuit switched mode <ref> [Int90, MMM91, SB77] </ref>. 2 There are several technical reports concerning the iPSC/860 architecture and communication performances [Dun90, MM91]. One can easily refer to them to have more details. The i860 is a 40 MHz RISC type processor with 8k bytes of cache memory.
Reference: [Mod88] <author> J. J. Modi. </author> <title> Parallel Algorithms and Matrix Computation. </title> <publisher> OXFORD: CLARENDON Press, </publisher> <year> 1988. </year>
Reference-contexts: We first present briefly our target machine and our analysis model in the sections 2 and 3. In Chapter 4, we present the sequential Jacobi like resolution <ref> [Jac46, Mod88, Wil65] </ref>. Afterwards, we discuss the parallel implementations (section 5). We discuss our implementation on the Intel machine iPSC/860 hypercube, using the Intel gossiping procedure. <p> This leads to the algorithm which is called the Jacobi threshold method. To save computing time, one chooses a definite order for the rotations. One can use for instance the raw- (or column-) cyclic method <ref> [Mod88, Wil65] </ref>. In the raw-cyclic scheme, we simply pick (p; q) in raw-by-raw fashion. For instance in the case n = 4 the following rotations (1; 2); (1; 3); (1; 4); (2; 3); (2; 4); (3; 4) are made in a complete sweep [GL90, TY91].
Reference: [PT93] <author> M. Pourzandi and B. Tourancheau. </author> <title> Overlapping in Gaussian elimination on iPSC/860. </title> <editor> In M. A. Yaghoubi, editor, </editor> <booktitle> Proceeding of International Congress on Computational Methods in Engineering, </booktitle> <volume> volume 4, </volume> <pages> pages 183-193, </pages> <institution> University of Shiraz, Iran, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Then, we discuss the same implementation but using our hand coded gossiping algorithm, following the works of [Fra90, JH89] leading to a very efficient solution. Afterwards, we present the same algorithm with overlapping of the communications by the computations. We use for that, a general methodology, developed in <ref> [DT92, PT93] </ref> and a tuned implementation. In the next section, we compare the experimental results of all algorithms. We show that the overlapping implementation can lead to a 6% improvement of the execution timings and that represents a decrease of 35% of the total communication time.
Reference: [Sam71] <author> A. Sameh. </author> <title> On Jacobi and Jacobi-like algorithms for a parallel computer. </title> <journal> Math. Comp., </journal> (25):579-590, 1971. 
Reference-contexts: Our parallel Jacobi method consists in doing concurrent Jacobi rotations at each of the processors of our computing system. We resume our parallel Jacobi algorithm for each processor in Figure 2. As noted in <ref> [Sam71] </ref>, all the processors in a parallel machine can and should do their own Jacobi rotations at the same time. In concurrent rotations the transformations are done on the original columns. Each rotation (p; q) affects the columns and rows p; q.
Reference: [SB77] <author> H. Sulivan and T.R. Bashkow. </author> <title> A large scale, homogeneous, fully distributed parallel machine. </title> <booktitle> In Proceeding of the fourth Symposium on computer architectures, </booktitle> <pages> pages 105-117, </pages> <year> 1977. </year>
Reference-contexts: Each node of the iPSC provides an i860 processor, a Direct Connect Module and 16 Megabytes of memory. The Direct Connect Module (DCM) does the inter-nodes communications. Using the DCM, the communications are independent from the i860 and are routed in circuit switched mode <ref> [Int90, MMM91, SB77] </ref>. 2 There are several technical reports concerning the iPSC/860 architecture and communication performances [Dun90, MM91]. One can easily refer to them to have more details. The i860 is a 40 MHz RISC type processor with 8k bytes of cache memory.
Reference: [TMLZ93] <author> C. Trefftz, P. K. McKinley, T. Y. Li, and Z. Zeng. </author> <title> A scalable eigenvalue solver for symmetric tridiagonal matrices. </title> <editor> In R. Sincovec, D. E. Keyes, M. R. Leuze, L. R. Petzold, and D. A. Reed, editors, </editor> <booktitle> Proceedings of the sixth SIAM Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 602-609, </pages> <year> 1993. </year>
Reference-contexts: Large eigenvalue problems occur in a wide variety of applications, including the dynamic analysis of large-scale structures such as aircraft and spacecraft, the prediction of structural responses in solid and soil mechanics, the study of solar convection, the modal analysis of electronic circuits, and the statistical analysis of data <ref> [TMLZ93] </ref>. Thus the need for faster methods to solve these large eigenvalue problems becomes very important.
Reference: [TY91] <author> P. Tervola and W. Yeung. </author> <title> Parallel Jacobi algorithm for matrix diag-onalization on transputer networks. </title> <journal> Parallel Computing, </journal> (17):155-163, 1991. 
Reference-contexts: Thus a sequence of matrices A (k) is produced such that lim k!1 A (k) = D, a diagonal matrix consisting of the eigenvalues of A, and lim J (1)T J (2)T J (3)T : : : J (k)T = V; is a matrix consisting of the eigenvectors of A <ref> [GL90, TY91] </ref>. We skip the anni hilation of a (k) pq when a (k) pq &lt; ", because the reduction in off (A) is not worth the cost. This leads to the algorithm which is called the Jacobi threshold method. <p> In the raw-cyclic scheme, we simply pick (p; q) in raw-by-raw fashion. For instance in the case n = 4 the following rotations (1; 2); (1; 3); (1; 4); (2; 3); (2; 4); (3; 4) are made in a complete sweep <ref> [GL90, TY91] </ref>. We present the corresponding sequential algorithm on Figure 1.
Reference: [Wil65] <author> J. H. Wilkinson. </author> <title> The Algebric Eigenvalue Problem. </title> <publisher> OXFORD: CLARENDON Press, </publisher> <year> 1965. </year> <month> 16 </month>
Reference-contexts: We first present briefly our target machine and our analysis model in the sections 2 and 3. In Chapter 4, we present the sequential Jacobi like resolution <ref> [Jac46, Mod88, Wil65] </ref>. Afterwards, we discuss the parallel implementations (section 5). We discuss our implementation on the Intel machine iPSC/860 hypercube, using the Intel gossiping procedure. <p> This leads to the algorithm which is called the Jacobi threshold method. To save computing time, one chooses a definite order for the rotations. One can use for instance the raw- (or column-) cyclic method <ref> [Mod88, Wil65] </ref>. In the raw-cyclic scheme, we simply pick (p; q) in raw-by-raw fashion. For instance in the case n = 4 the following rotations (1; 2); (1; 3); (1; 4); (2; 3); (2; 4); (3; 4) are made in a complete sweep [GL90, TY91].
References-found: 26

