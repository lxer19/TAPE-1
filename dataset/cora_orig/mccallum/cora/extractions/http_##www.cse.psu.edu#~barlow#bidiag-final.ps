URL: http://www.cse.psu.edu/~barlow/bidiag-final.ps
Refering-URL: http://www.cse.psu.edu/~barlow/papers.html
Root-URL: http://www.cse.psu.edu
Title: MORE ACCURATE BIDIAGONAL REDUCTION FOR COMPUTING THE SINGULAR VALUE DECOMPOSITION where B is bidiagonal, U
Author: JESSE L. BARLOW 
Note: A ffiA UBV T kffiAk 2 M f(n)kAk 2  
Abstract: Bidiagonal reduction is the preliminary stage for the fastest stable algorithms for computing the singular value decomposition. However, the best error bounds on bidiagonal reduction methods are of the form 1. Introduction. We consider the problem of reducing an m fi n matrix A to 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Anda and H. Park. </author> <title> Fast plane rotations with dynamic scaling. </title> <journal> SIAM J. Matrix Anal., </journal> <volume> 15 </volume> <pages> 162-174, </pages> <year> 1994. </year>
Reference-contexts: The use of fast Givens rotations as described in Hammarling [22], Gentleman [16], Bareiss [2], Barlow [3], or Anda and Park <ref> [1] </ref> would produce an algorithm with 8n 3 + O (n 2 ) flops, approximately the same as the Golub-Kahan Householder-based procedure.
Reference: [2] <author> E.H. Bareiss. </author> <title> Numerical solution of the weighted linear least squares problem by G-transformations. </title> <type> Technical Report 82-03-NAM-03, </type> <institution> Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, Illinois, </institution> <month> April </month> <year> 1982. </year>
Reference-contexts: The use of fast Givens rotations as described in Hammarling [22], Gentleman [16], Bareiss <ref> [2] </ref>, Barlow [3], or Anda and Park [1] would produce an algorithm with 8n 3 + O (n 2 ) flops, approximately the same as the Golub-Kahan Householder-based procedure.
Reference: [3] <author> J.L. Barlow. </author> <title> Stability analysis of the G-algorithm and a note on its application to sparse least squares problems. </title> <journal> BIT, </journal> <volume> 25 </volume> <pages> 507-520, </pages> <year> 1985. </year>
Reference-contexts: The second rowwise error bound (2.3) can only be shown for algorithms that do some kind of row and column permutations for stability. Similar results for Givens based algorithms are given by Barlow <ref> [3] </ref> and Barlow and Handy [5]. The Givens based algorithm in [3] could be substituted for the Cox-Higham algorithm. Gulliksen [21] has given a new framework for orthogonal transformations with this property. <p> The second rowwise error bound (2.3) can only be shown for algorithms that do some kind of row and column permutations for stability. Similar results for Givens based algorithms are given by Barlow <ref> [3] </ref> and Barlow and Handy [5]. The Givens based algorithm in [3] could be substituted for the Cox-Higham algorithm. Gulliksen [21] has given a new framework for orthogonal transformations with this property. Cox and Higham demonstrate that Householder's original version of Householder transformation must be used for these bounds to hold. <p> The use of fast Givens rotations as described in Hammarling [22], Gentleman [16], Bareiss [2], Barlow <ref> [3] </ref>, or Anda and Park [1] would produce an algorithm with 8n 3 + O (n 2 ) flops, approximately the same as the Golub-Kahan Householder-based procedure.
Reference: [4] <author> J.L. Barlow and J.W. Demmel. </author> <title> Computing accurate eigensystems of scaled diagonally dominant matrices. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 27 </volume> <pages> 762-791, </pages> <year> 1990. </year>
Reference-contexts: There are now a number of very good algorithms for computing the singular value decomposition of bidiagonal matrices. We know that the "zero-shift" Q-R algorithm [13], bisection <ref> [4] </ref>, and the dqds algorithm [15] can compute all of the singular values of B to relative accuracy. <p> 0:57735 2:22045e 19 2:22045e 19 0 0:57735 6:66134e 19 6:66134e 19 C A : If we perform Algorithm 3.1 on A without the preprocessing in x2, we obtain B = B @ 0 0:408248 0:912871 0 0 0 0 0 C A : The use of the bisection routine in <ref> [4] </ref> obtain the singular values = diag (1; 1; 3:58323e 17; 0): The computed singular vector matrices are V = B @ 4:44089e 15 1 1:11022e 16 3:33067e 16 4:44089e 15 8:32667e 16 0:894427 0:447214 C A 0 B 1 2:22045e 15 1:7791e 31 0 1:28198e 15 0:57735 0:408248 0:707107 1 <p> The important effect is that of the error ffiC in Theorems 4.9 and 4.10. To characterize the effect of the error ffiC in (4.17), we use a generalization of results that have been published by Barlow and Demmel <ref> [4] </ref> , Demmel and Veselic [14], and Gu [20]. This version was proven by Barlow and Slapnicar [6] in a work in preparation. <p> The next theorem gives such a bound. Its proof follows that of Theorem 4 in <ref> [4, p.773] </ref>. Lemma 4.13. Let ( i (i); x i (i)) be the ith eigenpair of the Hermitian matrix in (4.24). Let S (ffi) be defined by (4.26).
Reference: [5] <author> J.L. Barlow and S.L. Handy. </author> <title> The direct solution of weighted and equality constrained least squares problems. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 9 </volume> <pages> 704-716, </pages> <year> 1988. </year>
Reference-contexts: The second rowwise error bound (2.3) can only be shown for algorithms that do some kind of row and column permutations for stability. Similar results for Givens based algorithms are given by Barlow [3] and Barlow and Handy <ref> [5] </ref>. The Givens based algorithm in [3] could be substituted for the Cox-Higham algorithm. Gulliksen [21] has given a new framework for orthogonal transformations with this property. Cox and Higham demonstrate that Householder's original version of Householder transformation must be used for these bounds to hold.
Reference: [6] <author> J.L. Barlow and I. Slapnicar. </author> <title> Optimal perturbation bounds for the Hermitian eigenvalue problem. </title> <note> in preparation, </note> <year> 1996. </year>
Reference-contexts: To characterize the effect of the error ffiC in (4.17), we use a generalization of results that have been published by Barlow and Demmel [4] , Demmel and Veselic [14], and Gu [20]. This version was proven by Barlow and Slapnicar <ref> [6] </ref> in a work in preparation. <p> Thus we can obtain (4.27)-(4.28) by integrating over each of the intervals in which i (i) is analytic. The proof of the following proposition was given by Slapnicar <ref> [6] </ref>. Proposition 4.14. Let A; A 2 &lt; mfin ; m n, and let ffi A = kAk 2 , and E A = A=ffi A . Define A (i) = A + iE A for i 2 [0; ffi A ]. <p> The error bounds on computed singular vectors of C are also better for a bidi agonal reduction satisfying Theorem 4.9. For such bounds see the papers by Veselic and Demmel [14] or Barlow and Slapnicar <ref> [6] </ref>. 5. Numerical Tests. We performed two sets of numerical tests on the bidiag-onal reduction algorithms. Our tests compared the singular values to those obtained by the Jacobi method described by Demmel and Veselic [14]. The two test sets are as follows Example 5.1 (Test Set 1).
Reference: [7] <author> J.L. Barlow and H. Zha. </author> <title> Growth factors in Gaussian elimination ,the euclidean norm, and orthogonal matrices. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 19 </volume> <year> 807-815,1998. </year>
Reference-contexts: Unfortunately, the bound on growth for Gaussian elimination for a given row ordering is no better for orthogonal matrices than it is for all matrices. The following result is proven by Barlow and Zha <ref> [7] </ref>. Proposition 4.4. Let X 2 &lt; nfin be nonsingular and have the P-L-R factorization X = P LR by Gaussian elimination with the row ordering P where L is lower triangular and R is upper triangular.
Reference: [8] <author> P.A. Businger and G.H. Golub. </author> <title> Linear least squares solutions by Householder transformations. </title> <journal> Numerische Mathematik, </journal> <volume> 7 </volume> <pages> 269-278, </pages> <year> 1965. </year>
Reference-contexts: The procedure is as follows. 1. Reorder the rows of A so that kA (1; : )k 1 : : : kA (m; : )k 1 : 2. Using the maximal column pivoting algorithm of Businger and Golub <ref> [8] </ref>, factor A into A = U 0 C T where U 0 2 &lt; mfim is orthogonal, P is a permutation matrix, C 2 &lt; nfin is lower triangular. This particular Householder factorization algorithm has very strong numerical stability properties.
Reference: [9] <author> T.F. Chan. </author> <title> An improved algorithm for computing the singular value decomposition. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 8 </volume> <pages> 72-83, </pages> <year> 1982. </year>
Reference: [10] <author> A. J. Cox and N. J. Higham. </author> <title> Stability of Householder QR factorization for weighted least squares problems. in Numerical Analysis 1997, </title> <booktitle> Proceedings of the 17th Dundee Biennial Conference, </booktitle> <month> June </month> <year> 24-27,1997, </year> <editor> D.F. Griffiths, D.J. Higham, and G.A. Watson, eds., </editor> <volume> 24 0 10 20 30 40 50 60 70 80 90 -60 -40 -20 0 Base 10 Logarithms of Sing. </volume> <editor> Val. </editor> <title> of Matrix of Size 90: Test Set One Log 10 of Ith Singular Value Index of Singular Value Fig. 5.4. Singular Values of R 90 in Example 5.1 on logarithmic scale Addison-Wesley Longman, </title> <address> Dorset, UK, </address> <year> 1998. </year>
Reference-contexts: In this paper, we present a bidiagonal reduction method that will often preserve more of the accuracy in the singular value decomposition. The reduction is computed in two stages. In the first stage, using a Householder factorization method of Cox and Higham <ref> [10] </ref>, we reduce A to a lower triangular matrix C 2 &lt; nfin . <p> k 2 " M ae A " M f (n) + O (" 2 M );(1.3) D A = diag (d (A) m ); d i = kA (i; : )k 2 :(1.4) Here f (n) is a modestly sized functions and ae A is a growth factor given in <ref> [10] </ref>. A similar reduction is recommended by Demmel and Veselic [14] before using the Jacobi method. Thus the difference in our algorithm is the reduction of C. In the second stage, we apply a new bidiagonal reduction algorithm to C. <p> They are * Givens transformations are used in the construction of the right orthogonal matrix V . (Clearly, 2 fi 2 Householder transformations could also be used.) * The matrix A is preprocessed by Householder factorization with maximal column pivoting using a new procedure due to Cox and Higham <ref> [10] </ref>. * The first column of V is not e 1 , the first column of the identity matrix. * The computation of the matrices U and V are interleaved in a different man ner to preserve accuracy in the small columns. <p> In x4 we prove the bounds (1.2)-(1.3). In x5, we give some tests and a conclusion. 2. Reduction to triangular form. Before giving the bidiagonal reduction procedure, we preprocess A by reducing it to lower triangular form using a Householder transformation based procedure due to Cox and Higham <ref> [10] </ref>. It is based upon the row and column pivoting procedure of Powell and Reid [30], but uses a simpler form of pivoting. The procedure is as follows. 1. <p> This particular Householder factorization algorithm has very strong numerical stability properties. If we let C be the computed lower triangular factor, then Cox and Higham <ref> [10] </ref> (based on the analysis of a more complicated pivoting strategy by Powell and Reid [30]) showed that for some orthgonal matrix U 0 , A + A = U 0 C T kAe j k 2 c 1 (m; n)kAe j k 2 " M + O (" 2 and
Reference: [11] <author> J.W. Demmel and W.B. Gragg. </author> <title> On computing accurate singular values and eigenvalues of matrices with acyclic graphs. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 185 </volume> <pages> 203-217, </pages> <year> 1993. </year>
Reference-contexts: We also know that it is not reasonable to expect any algorithm to compute all of the singular values of a matrix to relative accuracy unless that matrix has an acyclic graph <ref> [11] </ref> or is totally sign compound [12]. Thus, it is not surprising that no algorithm can be expected to produce the bidiagonal form of a general matrix to relative accuracy in fixed precision arithmetic.
Reference: [12] <author> J. Demmel, M. Gu, S. Eisenstat, I. Slapnicar, K. Veselic and Z. Drmac, </author> <title> Computing the Singular Value Decomposition with High Relative Accuracy, </title> <note> LAPACK Working Note No. </note> <month> 119, </month> <pages> pp. 1-46, </pages> <year> (1997), </year> <note> submitted to Linear Algebra and Its Applications. </note>
Reference-contexts: We also know that it is not reasonable to expect any algorithm to compute all of the singular values of a matrix to relative accuracy unless that matrix has an acyclic graph [11] or is totally sign compound <ref> [12] </ref>. Thus, it is not surprising that no algorithm can be expected to produce the bidiagonal form of a general matrix to relative accuracy in fixed precision arithmetic.
Reference: [13] <author> J.W. Demmel and W.H. Kahan. </author> <title> Accurate singular values of bidiagonal matrices. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 11 </volume> <pages> 873-912, </pages> <year> 1990. </year>
Reference-contexts: There are now a number of very good algorithms for computing the singular value decomposition of bidiagonal matrices. We know that the "zero-shift" Q-R algorithm <ref> [13] </ref>, bisection [4], and the dqds algorithm [15] can compute all of the singular values of B to relative accuracy. <p> Quite recently, there have been a number of papers on the singular values and vectors of matrices under structured perturbations. We give two results below that are relevant to singular values for the perturbation given in Theorem 4.9. The first is due to Kahan <ref> [13] </ref>. Lemma 4.11. Let B = bidiag (fl (1: n); OE (2: n)) 2 &lt; nfin , let ~ B = bidiag (~fl (1: n); ~ OE (2: n)) 2 &lt; nfin , and let o 1. <p> Relative Error Plots from Example 5.1 * Algorithm J The Jacobi method described in [14]. * Algorihtm G The bidiagonalization method of Algorithm 3.2 followed by the bisection routine of Demmel and Kahan <ref> [13] </ref>. * Algorihtm H The bidiagonalization method of Algorithm 3.1 followed by the same bisection routine.
Reference: [14] <author> J.W. Demmel and K. Veselic. </author> <title> Jacobi's method is more accurate than QR. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13 </volume> <pages> 1204-1243, </pages> <year> 1992. </year>
Reference-contexts: A similar reduction is recommended by Demmel and Veselic <ref> [14] </ref> before using the Jacobi method. Thus the difference in our algorithm is the reduction of C. In the second stage, we apply a new bidiagonal reduction algorithm to C. <p> The growth factor ae V is bounded provided that the s fi s principal submatrix of the corresponding 2 Krylov matrix is nonsingular. If that submatrix is singular, the standard backward error bounds from [17] apply. This is not as good a bound as the Jacobi method achieves <ref> [14, 27, 28] </ref>, but can be much better than that achieved by the standard algorithm. Moreover, the algorithm can be implemented in less than 2 8 27 n 3 + O (n 2 ) more flops than the Lawson-Hanson-Chan SVD [26, pp.107-120],[9]. <p> The important effect is that of the error ffiC in Theorems 4.9 and 4.10. To characterize the effect of the error ffiC in (4.17), we use a generalization of results that have been published by Barlow and Demmel [4] , Demmel and Veselic <ref> [14] </ref>, and Gu [20]. This version was proven by Barlow and Slapnicar [6] in a work in preparation. <p> As pointed out by Demmel and Veselic <ref> [14] </ref>, only an algorithm with columnwise backward error bounds can take advantage of that fact. The error bounds on computed singular vectors of C are also better for a bidi agonal reduction satisfying Theorem 4.9. For such bounds see the papers by Veselic and Demmel [14] or Barlow and Slapnicar [6]. <p> out by Demmel and Veselic <ref> [14] </ref>, only an algorithm with columnwise backward error bounds can take advantage of that fact. The error bounds on computed singular vectors of C are also better for a bidi agonal reduction satisfying Theorem 4.9. For such bounds see the papers by Veselic and Demmel [14] or Barlow and Slapnicar [6]. 5. Numerical Tests. We performed two sets of numerical tests on the bidiag-onal reduction algorithms. Our tests compared the singular values to those obtained by the Jacobi method described by Demmel and Veselic [14]. <p> For such bounds see the papers by Veselic and Demmel <ref> [14] </ref> or Barlow and Slapnicar [6]. 5. Numerical Tests. We performed two sets of numerical tests on the bidiag-onal reduction algorithms. Our tests compared the singular values to those obtained by the Jacobi method described by Demmel and Veselic [14]. The two test sets are as follows Example 5.1 (Test Set 1). We use the set R k ; k = 25; : : : ; 90 where R k was the Cholesky factor of the Hilbert matrix of dimension k. <p> Relative Error Plots from Example 5.1 * Algorithm J The Jacobi method described in <ref> [14] </ref>. * Algorihtm G The bidiagonalization method of Algorithm 3.2 followed by the bisection routine of Demmel and Kahan [13]. * Algorihtm H The bidiagonalization method of Algorithm 3.1 followed by the same bisection routine.
Reference: [15] <author> K.V. Fernando and B.N. Parlett. </author> <title> Accurate singular values and differential qd algorithms. </title> <journal> Numer. Math., </journal> <volume> 67 </volume> <pages> 191-229, </pages> <year> 1994. </year>
Reference-contexts: There are now a number of very good algorithms for computing the singular value decomposition of bidiagonal matrices. We know that the "zero-shift" Q-R algorithm [13], bisection [4], and the dqds algorithm <ref> [15] </ref> can compute all of the singular values of B to relative accuracy.
Reference: [16] <author> W.M. Gentleman. </author> <title> Least squares computations by Givens rotations without square roots. </title> <journal> Lin. Alg. Appl., </journal> <volume> 10 </volume> <pages> 329-336, </pages> <year> 1973. </year>
Reference-contexts: The use of fast Givens rotations as described in Hammarling [22], Gentleman <ref> [16] </ref>, Bareiss [2], Barlow [3], or Anda and Park [1] would produce an algorithm with 8n 3 + O (n 2 ) flops, approximately the same as the Golub-Kahan Householder-based procedure.
Reference: [17] <author> G.H. Golub and W.M. Kahan. </author> <title> Calculating the singular values and pseudoinverse of a matrix. </title> <journal> SIAM J. Num. Anal. Ser. B, </journal> <volume> 2 </volume> <pages> 205-224, </pages> <year> 1965. </year>
Reference-contexts: The growth factor ae V is bounded provided that the s fi s principal submatrix of the corresponding 2 Krylov matrix is nonsingular. If that submatrix is singular, the standard backward error bounds from <ref> [17] </ref> apply. This is not as good a bound as the Jacobi method achieves [14, 27, 28], but can be much better than that achieved by the standard algorithm. <p> Using fast versions of Givens rotations, that additional overhead may be reduced to about 8=27n 3 + O (n 2 ) flops. Our procedure for bidiagonal reduction has some important differences from the Golub-Kahan Householder transformation based procedure <ref> [17] </ref>. <p> We now give algorithms for computing the bidiagonal reduction of C. 3. Bidiagonal Reduction Algorithms. 3.1. The Golub-Kahan Bidiagonal Reduction. The usual bidiagonal reduction algorithm is that given by Golub and Kahan <ref> [17, pp.208-210,Theorem 1] </ref>, see also Golub and Reinsch [19, pp.404-405]. It is given below for the square matrix C. Since C is lower triangular, it is exactly rank deficient only if it has zero diagonals. <p> = fl k e 1 ; 3. 4 % The bidiagonal reduction of C is given by C = U BV T % where B = bidiag (fl 1 ; : : : ; fl n ; OE 2 ; : : : ; OE n ): Golub and Kahan <ref> [17] </ref> used Householder transformations to describe their algorithm. In that case, it requires 4n 3 + O (n 2 ) flops if the matrices U and V are kept in factored form. If U and V are accumulated, it requires 8n 3 + O (n 2 ) flops. 3.2.
Reference: [18] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations, Third Edition. </title> <publisher> The Johns Hopkins Press, </publisher> <address> Baltimore,MD, </address> <year> 1996. </year>
Reference-contexts: It is well known that matrix V from bidiagonal reduction is the orthgonal factor of the Krylov matrix K <ref> [18, pp.472-473] </ref>. That allows us to find the following bound of the growth factor for bidiagonal reduction. A caveat to all of our results is that the s fi s matrix K 11 be nonsingular. Proposition 4.7. Let V be as in Lemma 4.2. <p> This theorem is given in [24, pp.423-424,problem 18]. Lemma 4.12. Let A 2 &lt; mfin and let B 2 &lt; nfip where p n. Then for i = 1; 2; : : : ; p, Standard bounds on eigenvalue perturbation <ref> [18, Chapter 8] </ref> and taking square roots leads to 1 0:5g V (n)" M + O (" 2 n (V ) oe 2 M ): The errors in B and non-orthogonality of the transformations U and V , make only small relative changes in the singular values.
Reference: [19] <author> G.H. Golub and C. Reinsch. </author> <title> Singular value decomposition and least squares solutions. </title> <journal> Numer. Math, </journal> <volume> 14 </volume> <pages> 403-20, </pages> <year> 1970. </year>
Reference-contexts: We now give algorithms for computing the bidiagonal reduction of C. 3. Bidiagonal Reduction Algorithms. 3.1. The Golub-Kahan Bidiagonal Reduction. The usual bidiagonal reduction algorithm is that given by Golub and Kahan [17, pp.208-210,Theorem 1], see also Golub and Reinsch <ref> [19, pp.404-405] </ref>. It is given below for the square matrix C. Since C is lower triangular, it is exactly rank deficient only if it has zero diagonals. Minor and obvious modifications to the bidiagonal reduction algorithms in this section are necessary if C has zero diagonals.
Reference: [20] <author> M. Gu. </author> <title> Studies in Numerical Linear Algebra. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1994. </year>
Reference-contexts: The important effect is that of the error ffiC in Theorems 4.9 and 4.10. To characterize the effect of the error ffiC in (4.17), we use a generalization of results that have been published by Barlow and Demmel [4] , Demmel and Veselic [14], and Gu <ref> [20] </ref>. This version was proven by Barlow and Slapnicar [6] in a work in preparation.
Reference: [21] <author> M. Gulliksson. </author> <title> Backward error analysis for the constrained and weighted linear least squares problem when using the weighted QR factorization. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 16 </volume> <pages> 675-687, </pages> <year> 1995. </year>
Reference-contexts: Similar results for Givens based algorithms are given by Barlow [3] and Barlow and Handy [5]. The Givens based algorithm in [3] could be substituted for the Cox-Higham algorithm. Gulliksen <ref> [21] </ref> has given a new framework for orthogonal transformations with this property. Cox and Higham demonstrate that Householder's original version of Householder transformation must be used for these bounds to hold. The bound does not hold if Parlett's [29] version of the Householder transformation is used.
Reference: [22] <author> S.J. Hammarling. </author> <title> A note on the modifications to the Givens plane rotation. </title> <journal> J. Inst. Math. Appl., </journal> <volume> 13 </volume> <pages> 215-218, </pages> <year> 1974. </year> <month> 25 </month>
Reference-contexts: The use of fast Givens rotations as described in Hammarling <ref> [22] </ref>, Gentleman [16], Bareiss [2], Barlow [3], or Anda and Park [1] would produce an algorithm with 8n 3 + O (n 2 ) flops, approximately the same as the Golub-Kahan Householder-based procedure.
Reference: [23] <author> N.J. Higham. </author> <title> Accuracy and Stability of Numerical Algorithms. </title> <publisher> SIAM Publications, </publisher> <address> Philadel--phia, </address> <year> 1996. </year>
Reference: [24] <author> R.A. Horn and C.A. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge,UK, </address> <year> 1985. </year>
Reference-contexts: The following result shows that the non-orthogonality of U and V in Theorem 4.9 causes only a small relative change in the singular values. This theorem is given in <ref> [24, pp.423-424,problem 18] </ref>. Lemma 4.12. Let A 2 &lt; mfin and let B 2 &lt; nfip where p n.
Reference: [25] <author> T. Kato. </author> <title> A Short Introduction to Perturbation Theory for Linear Operators. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: In Kato <ref> [25, Theorem II.6.1,p.139] </ref>, it is shown that the eigenvalues of H (i) in S (ffi) are real analytic, even when they are multiple. <p> In Kato [25, Theorem II.6.1,p.139], it is shown that the eigenvalues of H (i) in S (ffi) are real analytic, even when they are multiple. Moreover, Kato <ref> [25, p.143] </ref> goes on to point out that there are only a finite number of i where i (i) is multiple, so that i (i) is continuous and piecewise analytic throughout the interval [0; ffi].
Reference: [26] <editor> C.L. Lawson and R.J. Hanson. </editor> <title> Solving Least Squares Problems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliff, NJ, </address> <year> 1974. </year>
Reference-contexts: Moreover, the algorithm can be implemented in less than 2 8 27 n 3 + O (n 2 ) more flops than the Lawson-Hanson-Chan SVD <ref> [26, pp.107-120] </ref>,[9]. Using fast versions of Givens rotations, that additional overhead may be reduced to about 8=27n 3 + O (n 2 ) flops. Our procedure for bidiagonal reduction has some important differences from the Golub-Kahan Householder transformation based procedure [17]. <p> Step one requires the minimum length solution of the least squares problem ( C T 21 )z = e 1 : Since C T 11 is already upper triangular, a procedure in Lawson and Hanson <ref> [26, pp.77-83] </ref> would allow us to compute the orthogonal factorization of the above matrix in 2s (n s) 2 + O (n 2 ) flops.
Reference: [27] <author> R. Mathias. </author> <title> Fast accurate eigensystem computation by Jacobi methods. </title> <journal> SIAM J. Matrix Anal., </journal> <volume> 16 </volume> <year> 977-1003,1995. </year>
Reference-contexts: The growth factor ae V is bounded provided that the s fi s principal submatrix of the corresponding 2 Krylov matrix is nonsingular. If that submatrix is singular, the standard backward error bounds from [17] apply. This is not as good a bound as the Jacobi method achieves <ref> [14, 27, 28] </ref>, but can be much better than that achieved by the standard algorithm. Moreover, the algorithm can be implemented in less than 2 8 27 n 3 + O (n 2 ) more flops than the Lawson-Hanson-Chan SVD [26, pp.107-120],[9].
Reference: [28] <author> R. Mathias. </author> <title> Fast accurate eigenvalue methods for graded positive definite matrices Numer. </title> <journal> Math., </journal> <volume> 74 </volume> <year> 85-103,1996. </year>
Reference-contexts: The growth factor ae V is bounded provided that the s fi s principal submatrix of the corresponding 2 Krylov matrix is nonsingular. If that submatrix is singular, the standard backward error bounds from [17] apply. This is not as good a bound as the Jacobi method achieves <ref> [14, 27, 28] </ref>, but can be much better than that achieved by the standard algorithm. Moreover, the algorithm can be implemented in less than 2 8 27 n 3 + O (n 2 ) more flops than the Lawson-Hanson-Chan SVD [26, pp.107-120],[9].
Reference: [29] <author> B.N. Parlett. </author> <title> Analysis of algorithms for reflectors in bisectors. </title> <journal> SIAM Review, </journal> <volume> 13 </volume> <pages> 197-208, </pages> <year> 1971. </year>
Reference-contexts: Gulliksen [21] has given a new framework for orthogonal transformations with this property. Cox and Higham demonstrate that Householder's original version of Householder transformation must be used for these bounds to hold. The bound does not hold if Parlett's <ref> [29] </ref> version of the Householder transformation is used.
Reference: [30] <author> M.J.D. Powell and J.K. Reid. </author> <title> On applying Householder's method to linear least squares problems. </title> <booktitle> In Proc. IFIP Congress, </booktitle> <pages> pages 122-126, </pages> <year> 1968. </year>
Reference-contexts: Reduction to triangular form. Before giving the bidiagonal reduction procedure, we preprocess A by reducing it to lower triangular form using a Householder transformation based procedure due to Cox and Higham [10]. It is based upon the row and column pivoting procedure of Powell and Reid <ref> [30] </ref>, but uses a simpler form of pivoting. The procedure is as follows. 1. Reorder the rows of A so that kA (1; : )k 1 : : : kA (m; : )k 1 : 2. <p> This particular Householder factorization algorithm has very strong numerical stability properties. If we let C be the computed lower triangular factor, then Cox and Higham [10] (based on the analysis of a more complicated pivoting strategy by Powell and Reid <ref> [30] </ref>) showed that for some orthgonal matrix U 0 , A + A = U 0 C T kAe j k 2 c 1 (m; n)kAe j k 2 " M + O (" 2 and j Ak 2 c 2 (m; n)ae A ke T M ); i = 1;

References-found: 30

