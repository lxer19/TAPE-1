URL: ftp://mancos.cs.utah.edu/papers/munin.ps.Z
Refering-URL: ftp://mancos.cs.utah.edu/papers/munin.html
Root-URL: 
Title: Techniques for Reducing Consistency-Related Communication in Distributed Shared Memory Systems  
Author: John B. Carter, John K. Bennett and Willy Zwaenepoel 
Note: This research was supported in part by the National Science Foundation under Grants CDA-8619893, CCR-9010351, CCR-9116343, by the IBM Corporation under Research Agreement No. 20170041, by the Texas Advanced Technology Program under Grants 003604014 and 003604012, and by a NASA Graduate Fellowship.  
Address: Houston, TX 77251-1892  
Affiliation: Computer Systems Laboratory Rice University  
Abstract: Distributed shared memory (DSM) is a software abstraction of shared memory on a distributed memory machine. One of the key problems in building an efficient DSM system is to reduce the amount of communication needed to keep the distributed memories consistent. In this paper we present four techniques for doing so: (1) software release consistency; (2) multiple consistency protocols; (3) write-shared protocols; and (4) an update-with-timeout mechanism. These techniques have been implemented in the Munin DSM system. We compare the performance of seven application programs on Munin, first to their performance when implemented using message passing and then to their performance when running on a conventional DSM system that does not embody the above techniques. On a 16-processor cluster of workstations, Munin's performance is within 5% of message passing for four out of the seven applications. For the other three, performance is within 29% to 33%. Detailed analysis of two of these three applications indicates that the addition of a function shipping capability would bring their performance within 7% of the message passing performance. Compared to a conventional DSM system, Munin achieves performance improvements ranging from a few to several hundred percent, depending on the application. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal and A. Gupta. </author> <title> Memory-reference characteristics of multiprocessor applications under MACH. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 215-225, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: To understand how shared memory programs characteristically access shared data, we studied the access behavior of a suite of shared memory parallel programs. The results of this study [8] and others <ref> [1, 23, 26, 50, 54, 56] </ref> support the notion that using the flexibility of a software implementation to support multiple consistency protocols can improve the performance of DSM. They also suggest the types of access patterns that should be supported: conventional, read-only, migratory, write-shared, and synchronization 1 . <p> The average number of different objects accessed between synchronization points is small [8], so the DSM system only needs to enqueue changes infrequently. Furthermore, when a shared data object needs to be updated, it usually resides on a small number of other processors <ref> [1, 50] </ref>. Thus, the number of update messages required to maintain consistency is small. 2.4 Update Timeout Mechanism Update protocols suffer from the fact that updates to a particular data item are propagated to all of its replicas, including those that are no longer being used.
Reference: [2] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Thus, Midway is able to detect access violations without taking page faults, which eliminates the time spent handling interrupts. 9.2 Hardware DSMs Recently, a number of designs for hardware distributed shared memory machines have been published <ref> [2, 9, 13, 22, 41, 57, 58] </ref>. We limit our discussion to those systems that are most related to the work presented in this paper. We have adopted from the DASH project [41] the concept of release consistency. <p> The GalacticaNet design includes a provision to time out updates to stale data, which is shown to have a significant effect on performance when there is a large number of processors. The APRIL machine addresses the problem of high latencies in distributed shared memory multiprocessors in a different way <ref> [2] </ref>. APRIL provides sequential consistency, but relies on extremely fast processor switching to overlap memory latency with computation. For APRIL to be successful at reducing the impact of read misses, there must be several threads ready to run on each processor.
Reference: [3] <author> A. Agarwal, R. Simoni, J. Hennessy, and M. Horowitz. </author> <title> An evaluation of directory schemes for cache coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: This approach also allows Munin to exploit locality of reference when maintaining directory information, since the need to maintain a single consistent directory entry, as has been proposed for most scalable shared-memory multiprocessors <ref> [3, 17, 18] </ref>, is eliminated. 3.5 Synchronization Support Synchronization objects are accessed in a fundamentally different way than ordinary data [8]. Thus, Munin provides efficient implementations of locks, barriers, and condition variables that directly use V's communication primitives rather than synchronizing through shared memory.
Reference: [4] <author> M. Ahamad, P.W. Hutto, and R. John. </author> <title> Implementing and programming causal distributed shared memory. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 274-281, </pages> <month> May </month> <year> 1991. </year> <month> 37 </month>
Reference: [5] <author> T.E. Anderson. </author> <title> The performance of spin lock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Doing so reduces the number of messages required to implement synchronization, especially compared to conventional spinlock algorithms <ref> [5] </ref>, and thereby reduces the amount of time that threads spend blocked at synchronization points. 2.3 Write-Shared Protocol The write-shared protocol is designed specifically to mitigate the effect of false sharing, as discussed in Sections 1 and 2.2.
Reference: [6] <author> J. Archibald and J.-L. Baer. </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: The only way for processors to communicate is through explicit message passing. Distributed memory machines are easier to build, especially for large configurations, because unlike shared memory machines they do not require complex and expensive hardware cache controllers <ref> [6, 36, 44, 48, 52] </ref>. The shared memory programming model is, however, more attractive since most application programmers find it difficult to program machines using a message passing paradigm that requires them to explicitly partition data and manage communication.
Reference: [7] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Orca: A language for parallel programming of distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: A comparative evaluation of the two approaches is the subject of an ongoing Ph.D. dissertation [37]. A variety of systems have sought to present an object-oriented interface to shared memory. We use the Orca <ref> [7] </ref> and the Midway system [11] as examples of this approach. Other noteworthy systems include Clouds [21], Emerald [34], and Amber [19].
Reference: [8] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Multiple consistency protocols are used to keep memory consistent in accordance with the observation that no single consistency protocol is the best for all applications, or even for all data items in a single application <ref> [8, 26] </ref>. 3. Write-shared protocols address the problem of false sharing in DSM by allowing multiple processes to write concurrently into a shared page, with the updates being merged at the appropriate synchronization point, in accordance with the definition of release consistency. 4. <p> Sequential consistency essentially requires that any update to shared data become visible to all other processors before the updating processor is allowed to issue another write to shared data [42]. This requirement imposes severe restrictions on possible performance optimizations. These restrictions have led to both theoretical [43] and empirical <ref> [8, 59] </ref> arguments that DSM systems based on sequential consistency require a substantial amount of communication and are thus inefficient. Therefore, we chose to explore a more relaxed notion of consistency in DSM systems. <p> To understand how shared memory programs characteristically access shared data, we studied the access behavior of a suite of shared memory parallel programs. The results of this study <ref> [8] </ref> and others [1, 23, 26, 50, 54, 56] support the notion that using the flexibility of a software implementation to support multiple consistency protocols can improve the performance of DSM. <p> Read-only data is provided as a special case of conventional data for debugging purposes. Migratory data is accessed multiple times by a single thread, including one or more writes, before another thread accesses the data <ref> [8, 56] </ref>. This access pattern is typical of shared data that is accessed only inside a critical section or via a work queue. <p> The average number of different objects accessed between synchronization points is small <ref> [8] </ref>, so the DSM system only needs to enqueue changes infrequently. Furthermore, when a shared data object needs to be updated, it usually resides on a small number of other processors [1, 50]. <p> Munin uses variables rather than pages as the basic unit of granularity because this better reflects the way data is used and reduces the amount of false sharing between unrelated variables <ref> [8] </ref>. Munin's object directory is structured as a hash table that maps a virtual address to an entry that describes the data located at that address. The object directory on the Munin root node is initialized from the shared data description table located in the executable file. <p> allows Munin to exploit locality of reference when maintaining directory information, since the need to maintain a single consistent directory entry, as has been proposed for most scalable shared-memory multiprocessors [3, 17, 18], is eliminated. 3.5 Synchronization Support Synchronization objects are accessed in a fundamentally different way than ordinary data <ref> [8] </ref>. Thus, Munin provides efficient implementations of locks, barriers, and condition variables that directly use V's communication primitives rather than synchronizing through shared memory. More elaborate synchronization mechanisms, such as monitors and atomic integers, can be built using these basic mechanisms.
Reference: [9] <author> J.K. Bennett, S. Dwarkadas, J.A. Greenwood, and E. Speight. </author> <title> Willow: A scalable shared memory multiprocessor. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 336-345, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Thus, Midway is able to detect access violations without taking page faults, which eliminates the time spent handling interrupts. 9.2 Hardware DSMs Recently, a number of designs for hardware distributed shared memory machines have been published <ref> [2, 9, 13, 22, 41, 57, 58] </ref>. We limit our discussion to those systems that are most related to the work presented in this paper. We have adopted from the DASH project [41] the concept of release consistency.
Reference: [10] <author> B.N. Bershad, E.D. Lazowska, and H.M. Levy. </author> <title> PRESTO: A system for object-oriented parallel programming. </title> <journal> Software: Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Other than that, Munin provides thread, synchronization, and data sharing facilities like those found in many shared memory parallel programming systems, e.g., Presto <ref> [10] </ref>. Munin does not currently support thread migration.
Reference: [11] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In COMPCON '93, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: A comparative evaluation of the two approaches is the subject of an ongoing Ph.D. dissertation [37]. A variety of systems have sought to present an object-oriented interface to shared memory. We use the Orca [7] and the Midway system <ref> [11] </ref> as examples of this approach. Other noteworthy systems include Clouds [21], Emerald [34], and Amber [19]. <p> On an orthogonal issue, Orca's consistency management uses an efficient, reliable, ordered broadcast protocol. For reasons of scalability, Munin does not rely on broadcast, although support for efficient multicast could improve the performance of some aspects of Munin. Midway <ref> [11] </ref> proposes a DSM system with entry consistency, a memory consistency model weaker than release consistency. The goal of Midway is to minimize communication costs by aggressively exploiting the relationship between shared variables and the synchronization objects that protect them.
Reference: [12] <author> R. Bisiani and M. Ravishankar. </author> <title> PLUS: A distributed shared-memory system. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 115-124, </pages> <month> May </month> <year> 1990. </year>
Reference: [13] <author> H. Burkhardt, S. Frank, B. Knobe, and J. Rothnie. </author> <title> Overview of the KSR1 computer system. </title> <type> Technical Report KSR-TR-9002001, </type> <institution> Kendall Square Research, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Thus, Midway is able to detect access violations without taking page faults, which eliminates the time spent handling interrupts. 9.2 Hardware DSMs Recently, a number of designs for hardware distributed shared memory machines have been published <ref> [2, 9, 13, 22, 41, 57, 58] </ref>. We limit our discussion to those systems that are most related to the work presented in this paper. We have adopted from the DASH project [41] the concept of release consistency. <p> Because APRIL performs many low-level consistency operations in very fast trap handling software, it would be possible to adopt several of our techniques to their hardware cache consistency mechanism. The Data Diffusion Machine (DDM) [55] and the KSR-1 <ref> [13] </ref> use hardware DSM to support an all-cache model for memory, wherein the entire memory of the system is treated as a large cache of the global virtual address space, similar to the way our software DSM treats memory.
Reference: [14] <author> C. S. Burrus and T. W. Parks. </author> <title> DFT/FFT and Convolution Algorithms. </title> <publisher> Wiley-Interscience, </publisher> <year> 1985. </year>
Reference-contexts: communication goes from 23 megabytes in about 30,000 messages for the Munin version, to 110 megabytes in 231,000 messages for the conventional version. 6.5 Fast Fourier Transform Program Description The Fast Fourier Transform (FFT) program used in the evaluation is based on the Cooley-Tukey Radix 2 Decimation in Time algorithm <ref> [14] </ref>. It recursively subdivides the problem into its even and odd components, until the input is of length 2. For this base case, the output is an elementary function known as a Butterfly, a linear combination of its inputs.
Reference: [15] <author> J.B. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: This section provides a brief overview of aspects of the implementation of Munin that are relevant to its evaluation. A more detailed description of the Munin prototype appears elsewhere <ref> [15, 16] </ref>. 3.1 Writing A Munin Program Munin programmers write parallel programs using threads, as they would on many shared memory multiprocessors. All of the current applications were written in C. Synchronization is supported by library routines for the manipulation of locks, barriers and condition variables.
Reference: [16] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Experience with release consistent memories indicates that because of the need to handle arbitrary thread preemption, most shared memory parallel programs are free of data races even when written assuming a sequentially consistent memory <ref> [16, 28] </ref>. More formally, the following constraints on the memory subsystem ensure release consistency: 1. Before an ordinary read or write is allowed to perform with respect to any other processor, all previous acquire accesses must be performed. 2. <p> This section provides a brief overview of aspects of the implementation of Munin that are relevant to its evaluation. A more detailed description of the Munin prototype appears elsewhere <ref> [15, 16] </ref>. 3.1 Writing A Munin Program Munin programmers write parallel programs using threads, as they would on many shared memory multiprocessors. All of the current applications were written in C. Synchronization is supported by library routines for the manipulation of locks, barriers and condition variables.
Reference: [17] <author> L. Censier and P. Feautrier. </author> <title> A new solution to coherence problems in multicache systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> December </month> <year> 1978. </year>
Reference-contexts: This approach also allows Munin to exploit locality of reference when maintaining directory information, since the need to maintain a single consistent directory entry, as has been proposed for most scalable shared-memory multiprocessors <ref> [3, 17, 18] </ref>, is eliminated. 3.5 Synchronization Support Synchronization objects are accessed in a fundamentally different way than ordinary data [8]. Thus, Munin provides efficient implementations of locks, barriers, and condition variables that directly use V's communication primitives rather than synchronizing through shared memory.
Reference: [18] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This approach also allows Munin to exploit locality of reference when maintaining directory information, since the need to maintain a single consistent directory entry, as has been proposed for most scalable shared-memory multiprocessors <ref> [3, 17, 18] </ref>, is eliminated. 3.5 Synchronization Support Synchronization objects are accessed in a fundamentally different way than ordinary data [8]. Thus, Munin provides efficient implementations of locks, barriers, and condition variables that directly use V's communication primitives rather than synchronizing through shared memory.
Reference: [19] <author> J.S. Chase, F.G. Amador, E.D. Lazowska, H.M. Levy, and R.J. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: A variety of systems have sought to present an object-oriented interface to shared memory. We use the Orca [7] and the Midway system [11] as examples of this approach. Other noteworthy systems include Clouds [21], Emerald [34], and Amber <ref> [19] </ref>. In general, the object-oriented nature 34 allows the compiler and the runtime system to carry out a number of powerful optimizations, but the programs have to be written in the particular object model supported.
Reference: [20] <author> D.R. Cheriton. </author> <title> The V distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: These techniques have been incorporated in the Munin DSM system. Munin has been implemented on a network of SUN-3/60 workstations running the V-System <ref> [20] </ref>. <p> Munin was evaluated on a network of SUN-3/60 workstations running the V-System <ref> [20] </ref> connected via an isolated 10 megabit per second Ethernet. This section provides a brief overview of aspects of the implementation of Munin that are relevant to its evaluation. <p> A Munin system thread installs itself as the page fault handler for the Munin program. As a result, the underlying V kernel <ref> [20] </ref> forwards to this system thread all memory exceptions. The Munin thread also interacts with the V kernel to communicate with the other Munin nodes over the network, and to manipulate the virtual memory system as part of maintaining the consistency of shared memory.
Reference: [21] <author> P. Dasgupta, R.C. Chen, S. Menon, M. Pearson, R. Ananthanarayanan, U. Ramachandran, M. Ahamad, R. LeBlanc Jr., W. Applebe, J.M. Bernabeu-Auban, P.W. Hutto, M.Y.A. Khalidi, and C.J. Wilekn-loh. </author> <title> The design and implementation of the Clouds distributed operating system. </title> <journal> Computing Systems Journal, </journal> <volume> 3, </volume> <month> Winter </month> <year> 1990. </year>
Reference-contexts: When a thread attempts to write to replicated data, a message is transmitted to invalidate all other copies of the data. The thread that generated the miss blocks until all invalidation messages are acknowledged. This single owner consistency protocol is typical of what existing DSM systems provide <ref> [21, 27, 42] </ref>, and is what we use exclusively to represent a conventional DSM system in our performance evaluation. Once read-only data has been initialized, no further updates occur. Thus, the consistency protocol simply consists of replication on demand. <p> It is up to the programmer or the compiler to lay out the program data structures in the shared address space such that false sharing is reduced. The directory management scheme in our implementation is largely borrowed from Ivy's dynamic distributed manager scheme. Both Clouds <ref> [21] </ref> and Mirage [27] allow part of shared memory to be locked down at a particular processor. In Clouds, the programmer can request that a segment of shared memory be locked on a processor. <p> A variety of systems have sought to present an object-oriented interface to shared memory. We use the Orca [7] and the Midway system [11] as examples of this approach. Other noteworthy systems include Clouds <ref> [21] </ref>, Emerald [34], and Amber [19]. In general, the object-oriented nature 34 allows the compiler and the runtime system to carry out a number of powerful optimizations, but the programs have to be written in the particular object model supported.
Reference: [22] <author> G.S. Delp, </author> <title> A.S. Sethi, and D.J. Farber. An analysis of MemNet: An experiment in high-speed shared-memory local networking. </title> <booktitle> In Proceedings of the Sigcomm '88 Symposium, </booktitle> <pages> pages 165-174, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Thus, Midway is able to detect access violations without taking page faults, which eliminates the time spent handling interrupts. 9.2 Hardware DSMs Recently, a number of designs for hardware distributed shared memory machines have been published <ref> [2, 9, 13, 22, 41, 57, 58] </ref>. We limit our discussion to those systems that are most related to the work presented in this paper. We have adopted from the DASH project [41] the concept of release consistency.
Reference: [23] <author> C. Dubnicki and T. LeBlanc. </author> <title> Adjustable block size coherent caches. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 170-180, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: To understand how shared memory programs characteristically access shared data, we studied the access behavior of a suite of shared memory parallel programs. The results of this study [8] and others <ref> [1, 23, 26, 50, 54, 56] </ref> support the notion that using the flexibility of a software implementation to support multiple consistency protocols can improve the performance of DSM. They also suggest the types of access patterns that should be supported: conventional, read-only, migratory, write-shared, and synchronization 1 .
Reference: [24] <author> M. Dubois and C. Scheurich. </author> <title> Memory access dependencies in shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 16(6) </volume> <pages> 660-673, </pages> <month> June </month> <year> 1990. </year> <month> 38 </month>
Reference-contexts: Therefore, we chose to explore a more relaxed notion of consistency in DSM systems. Among the various relaxed memory models that have been developed <ref> [24, 29, 31, 43] </ref>, we opted for the release consistency model developed as part of the DASH project [29, 41]. Release consistency exploits the fact that programmers use synchronization to separate accesses to shared variables by different threads.
Reference: [25] <author> S. Dwarkadas, P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 144-155, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: This latter form of data can support spinlocks and message-passing fairly effectively. Our support for multiple protocols is more general, without added cost, and Munin's separate synchronization package removes the need to support data-driven memory. Lazy release consistency <ref> [38, 25] </ref>, as used in TreadMarks [39], is an algorithm for implementing release consistency different from the one presented in this paper.
Reference: [26] <author> S.J. Eggers and R.H. Katz. </author> <title> A characterization of sharing in parallel programs and its application to coherency protocol evaluation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 373-383, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Multiple consistency protocols are used to keep memory consistent in accordance with the observation that no single consistency protocol is the best for all applications, or even for all data items in a single application <ref> [8, 26] </ref>. 3. Write-shared protocols address the problem of false sharing in DSM by allowing multiple processes to write concurrently into a shared page, with the updates being merged at the appropriate synchronization point, in accordance with the definition of release consistency. 4. <p> To understand how shared memory programs characteristically access shared data, we studied the access behavior of a suite of shared memory parallel programs. The results of this study [8] and others <ref> [1, 23, 26, 50, 54, 56] </ref> support the notion that using the flexibility of a software implementation to support multiple consistency protocols can improve the performance of DSM. They also suggest the types of access patterns that should be supported: conventional, read-only, migratory, write-shared, and synchronization 1 .
Reference: [27] <author> B. Fleisch and G. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 211-223, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: When a thread attempts to write to replicated data, a message is transmitted to invalidate all other copies of the data. The thread that generated the miss blocks until all invalidation messages are acknowledged. This single owner consistency protocol is typical of what existing DSM systems provide <ref> [21, 27, 42] </ref>, and is what we use exclusively to represent a conventional DSM system in our performance evaluation. Once read-only data has been initialized, no further updates occur. Thus, the consistency protocol simply consists of replication on demand. <p> It is up to the programmer or the compiler to lay out the program data structures in the shared address space such that false sharing is reduced. The directory management scheme in our implementation is largely borrowed from Ivy's dynamic distributed manager scheme. Both Clouds [21] and Mirage <ref> [27] </ref> allow part of shared memory to be locked down at a particular processor. In Clouds, the programmer can request that a segment of shared memory be locked on a processor.
Reference: [28] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Performance evaluations of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Experience with release consistent memories indicates that because of the need to handle arbitrary thread preemption, most shared memory parallel programs are free of data races even when written assuming a sequentially consistent memory <ref> [16, 28] </ref>. More formally, the following constraints on the memory subsystem ensure release consistency: 1. Before an ordinary read or write is allowed to perform with respect to any other processor, all previous acquire accesses must be performed. 2.
Reference: [29] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Software release consistency is a software implementation of release consistency <ref> [29] </ref>, specifically aimed at reducing the number of messages required to maintain consistency in a software DSM system. Roughly speaking, release consistency requires memory to be consistent only at specific synchronization points. 2. <p> Therefore, we chose to explore a more relaxed notion of consistency in DSM systems. Among the various relaxed memory models that have been developed <ref> [24, 29, 31, 43] </ref>, we opted for the release consistency model developed as part of the DASH project [29, 41]. Release consistency exploits the fact that programmers use synchronization to separate accesses to shared variables by different threads. <p> Therefore, we chose to explore a more relaxed notion of consistency in DSM systems. Among the various relaxed memory models that have been developed [24, 29, 31, 43], we opted for the release consistency model developed as part of the DASH project <ref> [29, 41] </ref>. Release consistency exploits the fact that programmers use synchronization to separate accesses to shared variables by different threads. The system then only needs to guarantee that memory is consistent at (select) synchronization points. This ability to allow temporary, but harmless, inconsistencies is what gives release consistency its power.
Reference: [30] <author> J. R. Goodman, M. K. Vernon, and P.J. Woest. </author> <title> Efficient synchronization primitives for large-scale cache-coherent multiprocessor. </title> <booktitle> In Proceedings of the 3rd Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 64-75, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: All of Munin's synchronization primitives cause their invoking thread to block on an "acquire" and cause the local delayed update queue to be purged on a "release". 3.5.1 Locks Munin employs a queue-based implementation of locks similar to existing implementations on shared memory multiprocessors <ref> [30, 45] </ref>. This allows a thread to request ownership of a lock and block awaiting a reply, without repeated queries. The system associates an ownership "token" and a distributed queue with each lock.
Reference: [31] <author> J.R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report CS-1006, </type> <institution> University of Wisconsin-Madison, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: Therefore, we chose to explore a more relaxed notion of consistency in DSM systems. Among the various relaxed memory models that have been developed <ref> [24, 29, 31, 43] </ref>, we opted for the release consistency model developed as part of the DASH project [29, 41]. Release consistency exploits the fact that programmers use synchronization to separate accesses to shared variables by different threads.
Reference: [32] <author> D. Hensgen, R. Finkel, and U. Manber. </author> <title> Two algorithms for barrier synchronization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(1) </volume> <pages> 1-17, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: When all of the threads have arrived at the barrier, the barrier manager replies to each waiting thread to let it resume. We considered using a distributed barrier mechanism similar to those designed for scalable multiprocessor systems <ref> [32] </ref>, but for the small size of the prototype implementation, a simple centralized scheme was more practical and efficient.
Reference: [33] <author> D.B. Johnson and W. Zwaenepoel. </author> <title> The Peregrine high-performance rpc system. </title> <journal> Software: Practice and Experience, </journal> <volume> 23(2) </volume> <pages> 201-221, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: In particular, both processor and network speeds have improved by a factor of fifteen to twenty in the past four years. Interprocessor communication is still a high latency operation, but there are indications that latencies can be improved by an order of magnitude through careful protocol implementation <ref> [33, 53] </ref>. At the same time, DRAM latencies are improving very slowly, so some form of cache will be present on essentially all future high-performance platforms. Finally, hardware DSM systems are becoming increasingly common.
Reference: [34] <author> E. Jul, H. Levy, N. Hutchinson, and A. Black. </author> <title> Fine-grained mobility in the Emerald system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The specific protocol varies from system to system. For instance, Ivy [42] supports a page-based write-invalidate protocol (see Section 1) while Emerald <ref> [34] </ref> uses object-oriented language support to handle shared object invocations. Each of these systems, however, treats all shared data the same way. <p> A variety of systems have sought to present an object-oriented interface to shared memory. We use the Orca [7] and the Midway system [11] as examples of this approach. Other noteworthy systems include Clouds [21], Emerald <ref> [34] </ref>, and Amber [19]. In general, the object-oriented nature 34 allows the compiler and the runtime system to carry out a number of powerful optimizations, but the programs have to be written in the particular object model supported.
Reference: [35] <author> A.R. Karlin, M.S. Manasse, L. Rudolph, and D.D. Sleator. </author> <title> Competitive snoopy caching. </title> <booktitle> In Proceedings of the 16th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 244-254, </pages> <year> 1986. </year>
Reference-contexts: This effect is a major reason that existing commercial multiprocessors use invalidation-based protocols. We address this problem with a timeout algorithm similar to the competitive snoopy caching algorithm devised by Karlin <ref> [35] </ref>. The goal of the update timeout mechanism is to invalidate replicas of a cached variable that have not been accessed recently upon receipt of an update. An example of the problem and our solution are illustrated in Figure 8.
Reference: [36] <author> R. Katz, S. Eggers, D. Wood, C.L. Perkins, and R. Sheldon. </author> <title> Implementing a cache consistency protocol. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 276-283, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: The only way for processors to communicate is through explicit message passing. Distributed memory machines are easier to build, especially for large configurations, because unlike shared memory machines they do not require complex and expensive hardware cache controllers <ref> [6, 36, 44, 48, 52] </ref>. The shared memory programming model is, however, more attractive since most application programmers find it difficult to program machines using a message passing paradigm that requires them to explicitly partition data and manage communication.
Reference: [37] <author> P. Keleher. </author> <title> Distributed Shared Memory Using Lazy Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <year> 1994. </year>
Reference-contexts: Simulation studies indicate that lazy release consistency reduces the number of message required to maintain consistency, but the implementation is more expensive in terms of protocol and memory overhead. A comparative evaluation of the two approaches is the subject of an ongoing Ph.D. dissertation <ref> [37] </ref>. A variety of systems have sought to present an object-oriented interface to shared memory. We use the Orca [7] and the Midway system [11] as examples of this approach. Other noteworthy systems include Clouds [21], Emerald [34], and Amber [19].
Reference: [38] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: This latter form of data can support spinlocks and message-passing fairly effectively. Our support for multiple protocols is more general, without added cost, and Munin's separate synchronization package removes the need to support data-driven memory. Lazy release consistency <ref> [38, 25] </ref>, as used in TreadMarks [39], is an algorithm for implementing release consistency different from the one presented in this paper.
Reference: [39] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: This latter form of data can support spinlocks and message-passing fairly effectively. Our support for multiple protocols is more general, without added cost, and Munin's separate synchronization package removes the need to support data-driven memory. Lazy release consistency [38, 25], as used in TreadMarks <ref> [39] </ref>, is an algorithm for implementing release consistency different from the one presented in this paper. Instead of updating every cached copy of a data item whenever the modifying thread performs a release operation, only the cached copies on the processor that next acquires the released lock are updated.
Reference: [40] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: In this paper we explore four techniques for achieving this goal. These techniques are discussed in detail in this section. 2.1 Software Release Consistency 2.1.1 Release Consistency Conventional DSM systems, as described in Section 1, employ the sequential consistency model <ref> [40] </ref> as the basis for their consistency protocols. Sequential consistency essentially requires that any update to shared data become visible to all other processors before the updating processor is allowed to issue another write to shared data [42]. This requirement imposes severe restrictions on possible performance optimizations.
Reference: [41] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Therefore, we chose to explore a more relaxed notion of consistency in DSM systems. Among the various relaxed memory models that have been developed [24, 29, 31, 43], we opted for the release consistency model developed as part of the DASH project <ref> [29, 41] </ref>. Release consistency exploits the fact that programmers use synchronization to separate accesses to shared variables by different threads. The system then only needs to guarantee that memory is consistent at (select) synchronization points. This ability to allow temporary, but harmless, inconsistencies is what gives release consistency its power. <p> Thus, Midway is able to detect access violations without taking page faults, which eliminates the time spent handling interrupts. 9.2 Hardware DSMs Recently, a number of designs for hardware distributed shared memory machines have been published <ref> [2, 9, 13, 22, 41, 57, 58] </ref>. We limit our discussion to those systems that are most related to the work presented in this paper. We have adopted from the DASH project [41] the concept of release consistency. <p> We limit our discussion to those systems that are most related to the work presented in this paper. We have adopted from the DASH project <ref> [41] </ref> the concept of release consistency. The differences between DASH's implementation of release consistency and Munin's implementation of release consistency were explained in detail in Section 2.1. DASH uses a write-invalidate protocol for all consistency maintenance.
Reference: [42] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year> <month> 39 </month>
Reference-contexts: To illustrate this challenge, Figure 2 shows how a conventional DSM system implements the 1 shared memory abstraction <ref> [42] </ref>. The global shared address space is divided in pages, depicted by a shaded box in the global address space in Figure 2. The local memory of each processor is used as a cache on the global shared address space. <p> If the access is a read, then the page becomes replicated in read-only mode. If the access is a write, then all other copies of the pages are invalidated. Throughout the rest of this paper, the term conventional DSM <ref> [42] </ref> refers to a DSM system that employs a page-based write-invalidate consistency protocol, such as the one just described. <p> Sequential consistency essentially requires that any update to shared data become visible to all other processors before the updating processor is allowed to issue another write to shared data <ref> [42] </ref>. This requirement imposes severe restrictions on possible performance optimizations. These restrictions have led to both theoretical [43] and empirical [8, 59] arguments that DSM systems based on sequential consistency require a substantial amount of communication and are thus inefficient. <p> An update protocol based on release consistency can, however, buffer writes, which reduces substantially the amount of communication required. 2.2 Multiple Consistency Protocols Most DSM systems employ a single protocol to maintain the consistency of all shared data. The specific protocol varies from system to system. For instance, Ivy <ref> [42] </ref> supports a page-based write-invalidate protocol (see Section 1) while Emerald [34] uses object-oriented language support to handle shared object invocations. Each of these systems, however, treats all shared data the same way. <p> When a thread attempts to write to replicated data, a message is transmitted to invalidate all other copies of the data. The thread that generated the miss blocks until all invalidation messages are acknowledged. This single owner consistency protocol is typical of what existing DSM systems provide <ref> [21, 27, 42] </ref>, and is what we use exclusively to represent a conventional DSM system in our performance evaluation. Once read-only data has been initialized, no further updates occur. Thus, the consistency protocol simply consists of replication on demand. <p> It is this property that ensures the correctness of the distributed ownership version of the update mechanism. The probable owner is used to determine the identity of the Munin node that currently owns the data <ref> [42] </ref>. The owner node is used by the conventional and migratory protocols to arbitrate write access to the data. <p> with false sharing. 9 Related Work This section compares our work with a number of existing software and hardware DSM systems, focusing on the mechanisms used by these other systems to reduce the amount of communication necessary to provide shared memory. 9.1 Software DSMs Ivy was the first DSM system <ref> [42] </ref>. It uses a single-writer, write-invalidate protocol for all data, with virtual memory pages as the units of consistency. This protocol is used as the baseline conventional protocol in our experiments.
Reference: [43] <author> R.J. Lipton and J.S. Sandberg. </author> <title> PRAM: A scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Sequential consistency essentially requires that any update to shared data become visible to all other processors before the updating processor is allowed to issue another write to shared data [42]. This requirement imposes severe restrictions on possible performance optimizations. These restrictions have led to both theoretical <ref> [43] </ref> and empirical [8, 59] arguments that DSM systems based on sequential consistency require a substantial amount of communication and are thus inefficient. Therefore, we chose to explore a more relaxed notion of consistency in DSM systems. <p> Therefore, we chose to explore a more relaxed notion of consistency in DSM systems. Among the various relaxed memory models that have been developed <ref> [24, 29, 31, 43] </ref>, we opted for the release consistency model developed as part of the DASH project [29, 41]. Release consistency exploits the fact that programmers use synchronization to separate accesses to shared variables by different threads.
Reference: [44] <author> T. Lovett and S. Thakkar. </author> <title> The Symmetry multiprocessor system. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 303-310, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The only way for processors to communicate is through explicit message passing. Distributed memory machines are easier to build, especially for large configurations, because unlike shared memory machines they do not require complex and expensive hardware cache controllers <ref> [6, 36, 44, 48, 52] </ref>. The shared memory programming model is, however, more attractive since most application programmers find it difficult to program machines using a message passing paradigm that requires them to explicitly partition data and manage communication.
Reference: [45] <author> J.M. Mellor-Crummey and M.L. Scott. </author> <title> Synchronization without contention. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 269-278, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: All of Munin's synchronization primitives cause their invoking thread to block on an "acquire" and cause the local delayed update queue to be purged on a "release". 3.5.1 Locks Munin employs a queue-based implementation of locks similar to existing implementations on shared memory multiprocessors <ref> [30, 45] </ref>. This allows a thread to request ownership of a lock and block awaiting a reply, without repeated queries. The system associates an ownership "token" and a distributed queue with each lock.
Reference: [46] <author> R.G. Minnich and D.J. Farber. </author> <title> The Mether system: A distributed shared memory for SunOS 4.0. </title> <booktitle> In Proceedings of the Summer 1989 USENIX Conference, </booktitle> <pages> pages 51-60, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: In both cases, the goal is to avoid extensive communication due to false sharing. The combination of software release consistency and write-shared protocols addresses the adverse effects of false sharing without introducing the delays caused by locking parts of shared memory to a processor. Mether <ref> [46] </ref> supports a number of special shared memory segments in fixed locations in the virtual address space of each machine in the system. In an attempt to support efficient memory-based spinlocks, Mether supports several different shared memory segments, each with different protocol characteristics.
Reference: [47] <author> B. Nitzberg and V. Lo. </author> <title> Distributed shared memory: A survey of issues and algorithms. </title> <journal> IEEE Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference: [48] <author> M. Papamarcos and J. Patel. </author> <title> A low overhead coherence solution for multiprocessors with private cache memories. </title> <booktitle> In Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 348-354, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: The only way for processors to communicate is through explicit message passing. Distributed memory machines are easier to build, especially for large configurations, because unlike shared memory machines they do not require complex and expensive hardware cache controllers <ref> [6, 36, 44, 48, 52] </ref>. The shared memory programming model is, however, more attractive since most application programmers find it difficult to program machines using a message passing paradigm that requires them to explicitly partition data and manage communication.
Reference: [49] <author> U. Ramachandran, M. Ahamad, and Y.A. Khalidi. </author> <title> Unifying synchronization and data transfer in maintaining coherence of distributed shared memory. </title> <type> Technical Report GIT-CS-88/23, </type> <institution> Georgia Institute of Technology, </institution> <month> June </month> <year> 1988. </year>
Reference: [50] <author> R.L. Sites and A.Agarwal. </author> <title> Multiprocessor cache analysis using ATUM. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 186-195, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: To understand how shared memory programs characteristically access shared data, we studied the access behavior of a suite of shared memory parallel programs. The results of this study [8] and others <ref> [1, 23, 26, 50, 54, 56] </ref> support the notion that using the flexibility of a software implementation to support multiple consistency protocols can improve the performance of DSM. They also suggest the types of access patterns that should be supported: conventional, read-only, migratory, write-shared, and synchronization 1 . <p> The average number of different objects accessed between synchronization points is small [8], so the DSM system only needs to enqueue changes infrequently. Furthermore, when a shared data object needs to be updated, it usually resides on a small number of other processors <ref> [1, 50] </ref>. Thus, the number of update messages required to maintain consistency is small. 2.4 Update Timeout Mechanism Update protocols suffer from the fact that updates to a particular data item are propagated to all of its replicas, including those that are no longer being used.
Reference: [51] <author> M. Stumm and S. Zhou. </author> <title> Algorithms implementing distributed shared memory. </title> <journal> IEEE Computer, </journal> <volume> 24(5) </volume> <pages> 54-64, </pages> <month> May </month> <year> 1990. </year>
Reference: [52] <editor> C.P. Thacker, L.C. Stewart, and E.H. Satterthewaite, Jr. Firefly: </editor> <title> A multiprocessor workstation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8) </volume> <pages> 909-920, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The only way for processors to communicate is through explicit message passing. Distributed memory machines are easier to build, especially for large configurations, because unlike shared memory machines they do not require complex and expensive hardware cache controllers <ref> [6, 36, 44, 48, 52] </ref>. The shared memory programming model is, however, more attractive since most application programmers find it difficult to program machines using a message passing paradigm that requires them to explicitly partition data and manage communication.
Reference: [53] <author> A.C. Thekkath and H. Levy. </author> <title> Limits to low-latency communications on high-speed networks. </title> <journal> acm Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In particular, both processor and network speeds have improved by a factor of fifteen to twenty in the past four years. Interprocessor communication is still a high latency operation, but there are indications that latencies can be improved by an order of magnitude through careful protocol implementation <ref> [33, 53] </ref>. At the same time, DRAM latencies are improving very slowly, so some form of cache will be present on essentially all future high-performance platforms. Finally, hardware DSM systems are becoming increasingly common.
Reference: [54] <author> J.E. Veenstra and R.J. Fowler. </author> <title> A performance evaluation of optimal hybrid cache coherency protocols. </title> <booktitle> In Proceedings of the 5th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 149-160, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: To understand how shared memory programs characteristically access shared data, we studied the access behavior of a suite of shared memory parallel programs. The results of this study [8] and others <ref> [1, 23, 26, 50, 54, 56] </ref> support the notion that using the flexibility of a software implementation to support multiple consistency protocols can improve the performance of DSM. They also suggest the types of access patterns that should be supported: conventional, read-only, migratory, write-shared, and synchronization 1 .
Reference: [55] <author> D.H.D. Warren and S. Haridi. </author> <title> The Data Diffusion machine A shared virtual memory architecture for parallel execution of logic programs. </title> <booktitle> In Proceedings of the 1988 International Conference on Fifth Generation Computer Systems, </booktitle> <pages> pages 943-952, </pages> <address> Tokyo, Japan, </address> <month> December </month> <year> 1988. </year>
Reference-contexts: This approach is analogous to the copy of last resort used in cache-only multiprocessors <ref> [55] </ref>. Munin implements a dynamic ownership protocol to distribute the task of data ownership across the nodes that use the data. <p> Because APRIL performs many low-level consistency operations in very fast trap handling software, it would be possible to adopt several of our techniques to their hardware cache consistency mechanism. The Data Diffusion Machine (DDM) <ref> [55] </ref> and the KSR-1 [13] use hardware DSM to support an all-cache model for memory, wherein the entire memory of the system is treated as a large cache of the global virtual address space, similar to the way our software DSM treats memory.
Reference: [56] <author> W.-D. Weber and A. Gupta. </author> <title> Analysis of cache invalidation patterns in multiprocessors. </title> <booktitle> In Proceedings of the 3rd Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: To understand how shared memory programs characteristically access shared data, we studied the access behavior of a suite of shared memory parallel programs. The results of this study [8] and others <ref> [1, 23, 26, 50, 54, 56] </ref> support the notion that using the flexibility of a software implementation to support multiple consistency protocols can improve the performance of DSM. They also suggest the types of access patterns that should be supported: conventional, read-only, migratory, write-shared, and synchronization 1 . <p> Read-only data is provided as a special case of conventional data for debugging purposes. Migratory data is accessed multiple times by a single thread, including one or more writes, before another thread accesses the data <ref> [8, 56] </ref>. This access pattern is typical of shared data that is accessed only inside a critical section or via a work queue.
Reference: [57] <author> A. Wilson and R. LaRowe. </author> <title> Hiding shared memory reference latency on the GalacticaNet distributed shared memory architecture. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4) </volume> <pages> 351-367, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Thus, Midway is able to detect access violations without taking page faults, which eliminates the time spent handling interrupts. 9.2 Hardware DSMs Recently, a number of designs for hardware distributed shared memory machines have been published <ref> [2, 9, 13, 22, 41, 57, 58] </ref>. We limit our discussion to those systems that are most related to the work presented in this paper. We have adopted from the DASH project [41] the concept of release consistency. <p> DASH uses a write-invalidate protocol for all consistency maintenance. We instead use the flexibility of its software implementation to also attack the problem of read misses by using update protocols and migration when appropriate. The GalacticaNet system <ref> [57] </ref> also demonstrated that support for an update-based protocol that exploits the flexibility of a relaxed consistency protocol can improve performance by reducing the number 35 of read misses and attendant processor stalls.
Reference: [58] <author> L.D. Wittie, G. Hermannsson, and A. Li. </author> <title> Eager sharing for efficient massive parallelism. </title> <booktitle> In 1992 International Conference on Parallel Processing, </booktitle> <pages> pages 251-255, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Thus, Midway is able to detect access violations without taking page faults, which eliminates the time spent handling interrupts. 9.2 Hardware DSMs Recently, a number of designs for hardware distributed shared memory machines have been published <ref> [2, 9, 13, 22, 41, 57, 58] </ref>. We limit our discussion to those systems that are most related to the work presented in this paper. We have adopted from the DASH project [41] the concept of release consistency.
Reference: [59] <author> R.N. Zucker and J.-L. Baer. </author> <title> A performance study of memory consistency models. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-12, </pages> <month> May </month> <year> 1992. </year> <month> 40 </month>
Reference-contexts: Sequential consistency essentially requires that any update to shared data become visible to all other processors before the updating processor is allowed to issue another write to shared data [42]. This requirement imposes severe restrictions on possible performance optimizations. These restrictions have led to both theoretical [43] and empirical <ref> [8, 59] </ref> arguments that DSM systems based on sequential consistency require a substantial amount of communication and are thus inefficient. Therefore, we chose to explore a more relaxed notion of consistency in DSM systems.
References-found: 59

