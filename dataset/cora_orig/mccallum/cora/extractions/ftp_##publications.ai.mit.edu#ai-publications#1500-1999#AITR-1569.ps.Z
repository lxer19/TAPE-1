URL: ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1569.ps.Z
Refering-URL: http://www.ai.mit.edu/people/deniz/papers.html
Root-URL: 
Title: From Genetic Algorithms To Efficient Optimization  
Author: Deniz Yuret 
Note: This publication can be retrieved by anonymous ftp to publications.ai.mit.edu. Copyright c Massachusetts Institute of Technology, 1996  
Date: 1569 May, 1994  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY  
Pubnum: A.I. Technical Report No.  
Abstract: The work described in this thesis began as an inquiry into the nature and use of optimization programs based on "genetic algorithms." That inquiry led, eventually, to three powerful heuristics that are broadly applicable in gradient-ascent programs: First, remember the locations of local maxima and restart the optimization program at a place distant from previously located local maxima. Second, adjust the size of probing steps to suit the local nature of the terrain, shrinking when probes do poorly and growing when probes do well. And third, keep track of the directions of recent successes, so as to probe preferentially in the direction of most rapid ascent. These algorithms lie at the core of a novel optimization program that illustrates the power to be had from deploying them together. The efficacy of this program is demonstrated on several test problems selected from a variety of fields, including De Jong's famous test-problem suite, the traveling salesman problem, the problem of coordinate registration for image guided surgery, the energy minimization problem for determining the shape of organic molecules, and the problem of assessing the structure of sedimentary deposits using seismic data. This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for this research was provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N00014-91-J-4038. 
Abstract-found: 1
Intro-found: 1
Reference: [Back and Hoffmeister, 1992] <author> Back, T. and Hoffmeister, F. </author> <year> (1992). </year> <note> A User's Guide to GENEsYs 1.0. </note> <institution> University of Dortmund. Software package documentation. </institution>
Reference-contexts: Also, De Jong's functions are quite popular in genetic algorithms literature, so it is possible to make direct comparisons. I should emphasize that all these results are only obtained from particular implementations of these algorithms. In particular the GENEsYs package <ref> [Back and Hoffmeister, 1992] </ref> was used for the genetic algorithms, the ASA package [Ingber, 1993] was used for simulated annealing, and the programs from "Numerical Recipes in C" [Press et al., 1992] were used for implementations of Powell's method and downhill simplex.
Reference: [Cormen et al., 1990] <author> Cormen, T., Leiserson, C., and Rivest, R. </author> <year> (1990). </year> <title> Introduction to Algorithms, chapter 17. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Simulated annealing is one of the popular methods for optimization in spaces with a lot of local maxima. It allows non-optimal moves in a probabilistic fashion. Many other algorithms for optimization belong to the class of greedy algorithms <ref> [Cormen et al., 1990] </ref>. The algorithms in this class always make the choice that looks best at the moment. They hold on to the best point found so far and never step down when climbing a hill. This is exactly the reason why greedy algorithms get stuck at local maxima.
Reference: [De Jong, 1975] <author> De Jong, K. </author> <year> (1975). </year> <title> An analysis of the behaviour of a class of genetic adaptive systems. </title> <type> PhD thesis, </type> <institution> University of Michigan. [de la Maza and Yuret, 1994] de la Maza, M. and Yuret, </institution> <address> D. </address> <year> (1994). </year> <title> Dynamic hill climbing. </title> <journal> AI Expert, </journal> <volume> 9(3). </volume>
Reference-contexts: De Jong first suggested this set in his work "An analysis of the behaviour of a class of genetic adaptive systems" <ref> [De Jong, 1975] </ref>. He chose these functions because they represent the common difficulties in optimization problems in an isolated manner. By running the comparisons on these functions, one can make judgments about the strengths and weaknesses of particular algorithms.
Reference: [Dennis and Schnabel, 1983] <author> Dennis, J. E. and Schnabel, R. B. </author> <year> (1983). </year> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: For a general introduction to computational methods in optimization see [Polak, 1971], or <ref> [Dennis and Schnabel, 1983] </ref>. The work described in this thesis started with explorations with genetic-algorithms. <p> For problems where the solution consists of traveling a certain distance in each dimension, this gives us a simple running time of: fi (#dimensions fi log 2 (distance)) However, this is not the main advantage. There are several algorithms in the literature <ref> [Dennis and Schnabel, 1983, Polak, 1971] </ref> that do better under certain assumptions about their objective function. Typical numerical optimization algorithms perform a sequence of line optimizations when faced with high dimensional problems. The advantage of this algorithm is the fact that it does not perform line optimizations.
Reference: [Grimson et al., 1994] <author> Grimson, W., Lozano-Perez, T., III, W. W., Ettinger, G., White, S., and Kikinis., R. </author> <year> (1994). </year> <title> An automatic registration method for frameless stereotaxy, image guided surgery, and enhanced reality visualization. </title> <booktitle> In Computer Vision and Pattern Recognition Conference, </booktitle> <address> Seattle. </address>
Reference-contexts: The problem is to find the transformation that will align these two data sets. 5.2 Image guided surgery There is recent study on frameless guidance systems to aid neurosurgeons in planning operations <ref> [Grimson et al., 1994] </ref>. A method has been developed for registering clinical data, such as segmented MRI or CT reconstructions, with the actual head of the patient, for which position and orientation information is provided by a laser scanning device.
Reference: [Hillis, 1990] <author> Hillis, W. D. </author> <year> (1990). </year> <title> Co-evolving parasites improve simulated evolution as an optimizing procedure. </title> <journal> Physica. </journal>
Reference-contexts: Most proposed solutions end up modifying the objective function, which can result in the total loss of the global optimum [Winston, 1992], or are specific to the particular problem at hand and difficult to generalize <ref> [Hillis, 1990] </ref>. Also, the large number of individuals involved in the typical genetic program results in a large number of function evaluations. Especially in problems with a high number of parameters, this considerably slows the genetic algorithm down. <p> For problems with a high number of dimensions (e.g. the molecule energy minimization problem with 153 dimensions), I chose random points instead. 2.5 How does nature do it There are various other approaches to the problem of local maxima inspired by natural phenomena. In this section Hillis' co-evolving parasites <ref> [Hillis, 1990] </ref> inspired by biology, and the simulated annealing algorithm [Kirkpatrick et al., 1983] inspired by physics, will be discussed. Hillis used simulated evolution to study the problem of finding optimum sorting networks. To tackle the local maxima problem he proposed to use co-evolving parasites.
Reference: [Holland, 1975] <author> Holland, J. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor. </address>
Reference-contexts: Algorithms that do not do well on this test function will do poorly on noisy data. * Foxholes is an example of many (in this case 25) local optima. Many standard optimization algorithms get stuck in the first peak they find. standard genetic algorithm <ref> [Holland, 1975] </ref>, simulated annealing [Kirkpatrick et al., 1983], Powell's method [Powell, 1964], and downhill simplex [Nelder and Mead, 1965]. Note that the standard genetic algorithm is the least efficient of all. It usually does the most number of function evaluations.
Reference: [Ingber, 1993] <author> Ingber, L. </author> <year> (1993). </year> <title> Adaptive Simulated Annealing (ASA). [ftp.caltech.edu: /pub/ingber/asa.Z]. Software package documentation. </title>
Reference-contexts: I should emphasize that all these results are only obtained from particular implementations of these algorithms. In particular the GENEsYs package [Back and Hoffmeister, 1992] was used for the genetic algorithms, the ASA package <ref> [Ingber, 1993] </ref> was used for simulated annealing, and the programs from "Numerical Recipes in C" [Press et al., 1992] were used for implementations of Powell's method and downhill simplex. Other implementations, or other parameter settings may give different results. Furthermore, the topic of optimization has a vast literature.
Reference: [Kirkpatrick et al., 1983] <author> Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P. </author> <year> (1983). </year> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680. </pages>
Reference-contexts: In this section Hillis' co-evolving parasites [Hillis, 1990] inspired by biology, and the simulated annealing algorithm <ref> [Kirkpatrick et al., 1983] </ref> inspired by physics, will be discussed. Hillis used simulated evolution to study the problem of finding optimum sorting networks. To tackle the local maxima problem he proposed to use co-evolving parasites. In his implementation, the evolution is not limited to the sorting networks. <p> Algorithms that do not do well on this test function will do poorly on noisy data. * Foxholes is an example of many (in this case 25) local optima. Many standard optimization algorithms get stuck in the first peak they find. standard genetic algorithm [Holland, 1975], simulated annealing <ref> [Kirkpatrick et al., 1983] </ref>, Powell's method [Powell, 1964], and downhill simplex [Nelder and Mead, 1965]. Note that the standard genetic algorithm is the least efficient of all. It usually does the most number of function evaluations. The numerical algorithms, Powell's method and downhill simplex, do well in terms of efficiency.
Reference: [Lin, 1965] <author> Lin, S. </author> <year> (1965). </year> <title> Computer solution of the tsp. </title> <journal> Bell System Technical Journal, </journal> <volume> 44 </volume> <pages> 2245-2269. </pages>
Reference-contexts: I compared the results I obtained from a very simple implementation of my algorithm with results from simulated annealing. The simulated annealing program [Press et al., 1992] uses an efficient set of moves suggested by Lin <ref> [Lin, 1965] </ref>. The moves consist of two types: (a) A section of path is removed and then replaced with the same cities running in the opposite order; or (b) a section of path is removed and then replaced in between two cities on another, randomly chosen, part of the path.
Reference: [Nelder and Mead, 1965] <author> Nelder, J. A. and Mead, R. </author> <year> (1965). </year> <title> A simplex method for function minimization. </title> <journal> Computer Journal, </journal> <volume> 7 </volume> <pages> 308-313. </pages>
Reference-contexts: Many standard optimization algorithms get stuck in the first peak they find. standard genetic algorithm [Holland, 1975], simulated annealing [Kirkpatrick et al., 1983], Powell's method [Powell, 1964], and downhill simplex <ref> [Nelder and Mead, 1965] </ref>. Note that the standard genetic algorithm is the least efficient of all. It usually does the most number of function evaluations. The numerical algorithms, Powell's method and downhill simplex, do well in terms of efficiency.
Reference: [Polak, 1971] <author> Polak, E. </author> <year> (1971). </year> <title> Computational Methods in Optimization. </title> <publisher> Academic Press, </publisher> <address> New York. </address> <month> 52 </month>
Reference-contexts: What makes global optimization a challenging problem are the potential existence of many local maxima, a high number of dimensions, uncooperative twists and turns in the search space, and an objective function that is costly to compute. For a general introduction to computational methods in optimization see <ref> [Polak, 1971] </ref>, or [Dennis and Schnabel, 1983]. The work described in this thesis started with explorations with genetic-algorithms. <p> For problems where the solution consists of traveling a certain distance in each dimension, this gives us a simple running time of: fi (#dimensions fi log 2 (distance)) However, this is not the main advantage. There are several algorithms in the literature <ref> [Dennis and Schnabel, 1983, Polak, 1971] </ref> that do better under certain assumptions about their objective function. Typical numerical optimization algorithms perform a sequence of line optimizations when faced with high dimensional problems. The advantage of this algorithm is the fact that it does not perform line optimizations. <p> Note that the gradient of the energy function typically can be computed. In that case 42 an algorithm that can use this information such as conjugate gradient <ref> [Press et al., 1992, Polak, 1971] </ref>, will be more efficient. The comparison is made with Powell's method because it is known as one of the most efficient algorithms among the ones that do not use the gradient. minimization problem. The first example starts from a near optimal configuration.
Reference: [Powell, 1964] <author> Powell, M. </author> <year> (1964). </year> <title> An efficient method for finding the minimum of a function of several variables without calculating derivatives. </title> <journal> Computer Journal, </journal> <volume> 7 </volume> <pages> 155-162. </pages>
Reference-contexts: Many standard optimization algorithms get stuck in the first peak they find. standard genetic algorithm [Holland, 1975], simulated annealing [Kirkpatrick et al., 1983], Powell's method <ref> [Powell, 1964] </ref>, and downhill simplex [Nelder and Mead, 1965]. Note that the standard genetic algorithm is the least efficient of all. It usually does the most number of function evaluations. The numerical algorithms, Powell's method and downhill simplex, do well in terms of efficiency.
Reference: [Press et al., 1992] <author> Press, W., Teukolsky, S., Vetterling, W., and Flannery, B. </author> <year> (1992). </year> <title> Numerical Recipes in C: </title> <booktitle> the art of scientific computing, chapter 10. </booktitle> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <note> second edition. </note>
Reference-contexts: I should emphasize that all these results are only obtained from particular implementations of these algorithms. In particular the GENEsYs package [Back and Hoffmeister, 1992] was used for the genetic algorithms, the ASA package [Ingber, 1993] was used for simulated annealing, and the programs from "Numerical Recipes in C" <ref> [Press et al., 1992] </ref> were used for implementations of Powell's method and downhill simplex. Other implementations, or other parameter settings may give different results. Furthermore, the topic of optimization has a vast literature. <p> Note that the gradient of the energy function typically can be computed. In that case 42 an algorithm that can use this information such as conjugate gradient <ref> [Press et al., 1992, Polak, 1971] </ref>, will be more efficient. The comparison is made with Powell's method because it is known as one of the most efficient algorithms among the ones that do not use the gradient. minimization problem. The first example starts from a near optimal configuration. <p> I compared the results I obtained from a very simple implementation of my algorithm with results from simulated annealing. The simulated annealing program <ref> [Press et al., 1992] </ref> uses an efficient set of moves suggested by Lin [Lin, 1965].
Reference: [Teng, 1994] <author> Teng, S. </author> <year> (1994). </year> <title> Voronoi diagrams. </title> <booktitle> Lecture Notes for Computational Geometry, </booktitle> <publisher> M.I.T. </publisher>
Reference-contexts: It is shown that in very high number of dimensions, the distance of a random point to its nearest neighbor is very likely to be close to the maximum. The problem is related to the nearest neighbor problem <ref> [Teng, 1994] </ref>. Thus it is no surprise that Voronoi diagrams which are generally used in finding nearest neighbors are 18 also the key in solving the distant point problem. Figure 2-3 illustrates the basic principle in two dimensions. A Voronoi diagram consists of convex regions that enclose every given point.
Reference: [Wang, 1994] <author> Wang, E. </author> <year> (1994). </year> <title> Conformational search of macrocyclic molecules. </title> <type> Masters thesis proposal, </type> <institution> M.I.T. </institution>
Reference-contexts: Figure 5-5 illustrates this structure. There are a total of 51 atoms in the molecule. Treating their x, y and z coordinates as separate parameters of the energy function, we have an optimization problem of 153 dimensions. Recently, a novel method to solve this problem efficiently was proposed <ref> [Wang, 1994] </ref>. First, approximate solutions are found by combining smaller chains together. Then, these approximate solutions are further refined in an optimization loop. Figure 5-6 contains the comparison of Procedure 2-1 with Powell's method in this loop as well as a sample run starting from a random initial conformation.
Reference: [Williamson, 1990] <author> Williamson, P. </author> <year> (1990). </year> <title> Tomographic inversion in reflection seismology. </title> <journal> Geophysical Journal International. </journal>
Reference-contexts: lines are the traces of Procedure 2-1. just the value at node A effects a part of the domain for one source and the remaining part for the other. 5.4 Refraction tomography There is recent study in mathematical geophysics, on assessing the structure of sedimentary deposits using refraction seismic data <ref> [Williamson, 1990] </ref>. The synthetic example we have worked on is a rectangular section of earth that is 8000 meters long and 500 meters deep. There are 15 sources and 41 receivers for a total of 615 seismic rays.
Reference: [Winston, 1992] <author> Winston, P. </author> <year> (1992). </year> <booktitle> Artificial Intelligence, chapter 25. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, third edition. </address>
Reference-contexts: In particular, local optima attract populations like magnets. The right way to maintain 10 information available to the optimization algorithm. diversity is not clear. Most proposed solutions end up modifying the objective function, which can result in the total loss of the global optimum <ref> [Winston, 1992] </ref>, or are specific to the particular problem at hand and difficult to generalize [Hillis, 1990]. Also, the large number of individuals involved in the typical genetic program results in a large number of function evaluations. <p> The distant explorers are outperformed and gradually eliminated. Finally, progress comes to a halt when diversity ceases to exist. The initial approach taken for this research was making diversity a part of the fitness function <ref> [Winston, 1992, Yuret, 1992] </ref>. This works by populating the local maxima in order to avoid them. When diversity is part of the fitness function, the individuals that are close to an already populated local maxima are punished, and distant explorers are rewarded. There are several difficulties with this approach.
Reference: [Yuret, 1992] <author> Yuret, D. </author> <year> (1992). </year> <title> Evolution of evolution: an exploratory work on genetic algorithms. </title> <type> Undergraduate thesis, </type> <institution> M.I.T. </institution>
Reference-contexts: The distant explorers are outperformed and gradually eliminated. Finally, progress comes to a halt when diversity ceases to exist. The initial approach taken for this research was making diversity a part of the fitness function <ref> [Winston, 1992, Yuret, 1992] </ref>. This works by populating the local maxima in order to avoid them. When diversity is part of the fitness function, the individuals that are close to an already populated local maxima are punished, and distant explorers are rewarded. There are several difficulties with this approach. <p> When diversity is part of the fitness function, the individuals that are close to an already populated local maxima are punished, and distant explorers are rewarded. There are several difficulties with this approach. First, it is not clear how to combine fitness with diversity, and experiments <ref> [Yuret, 1992] </ref> suggest that an implementation good for one particular problem has difficulty with another. Second, as the number of discovered local maxima increases, the size of the population has to increase also.
Reference: [Yuret and de la Maza, 1993] <author> Yuret, D. and de la Maza, M. </author> <year> (1993). </year> <title> Dynamic hill climbing: Overcoming the limitations of optimization techniques. </title> <booktitle> In The Second Turkish Symposium on Artificial Intelligence and Neural Networks, </booktitle> <pages> pages 208-212. 53 </pages>
Reference-contexts: This way, the algorithm can keep the size of the population in the reproductive cycle, thus the number of function evaluations, manageable. This scheme was analyzed in previous work <ref> [Yuret and de la Maza, 1993] </ref>. The population is separated into two groups; young and old. The size of the young population, which contribute to reproduction, is held constant. Individuals that survive more than a maximum number of generations are moved into the old population.
References-found: 20

