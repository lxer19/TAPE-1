URL: ftp://ilk.kub.nl/pub/antalb/benelearn-95.ps.gz
Refering-URL: http://ilk.kub.nl/~antalb/pubs-time.html
Root-URL: 
Title: The Profit of Learning Exceptions  
Author: Antal van den Bosch, Ton Weijters, Jaap van den Herik Walter Daelemans 
Address: PO Box 616, NL-6200 MD Maastricht, the Netherlands  PO Box 90153, NL-5000 LE Tilburg, the Netherlands  
Affiliation: University of Maastricht, MATRIKS, Department of Computer Science  Institute for Language Technology and AI (ITK), Tilburg University,  
Abstract: For many classification tasks, the set of available task instances can be roughly divided into regular instances and exceptions. We investigate three learning algorithms that apply a different method of learning with respect to regularities and exceptions, viz. (i) backpropagation, (ii) cascade back-propagation (a constructive version of back-propagation), and (iii) information-gain tree (an inductive decision-tree algorithm). We compare the bias of the algorithms towards learning regularities and exceptions, using a task-independent metric for the typicality of instances. We have found that information-gain tree is best capable of learning exceptions. However, it outperforms backpropagation and cascade back-propagation only when trained on very large training sets.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., Kibler, D., and Albert, M. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 7, </volume> <pages> pp. 37-66. </pages>
Reference: <author> Daelemans, W., and Van den Bosch, A. </author> <year> (1992). </year> <title> A neural network for hyphenation. </title> <editor> In I. Aleksander and J. Taylor (Eds.), </editor> <booktitle> Artificial Neural Networks 2, </booktitle> <volume> volume 2, </volume> <pages> pp 1647-1650. </pages> <address> Amsterdam: </address> <publisher> North-Holland. </publisher>
Reference: <author> Daelemans, W., Van den Bosch, A., and Weijters, T. </author> <year> (1995). </year> <note> ig-tree: A variant of ibl. submitted. </note>
Reference-contexts: The algorithm is then able to classify new instances by matching them to stored parts of the reduced instances. In our experiments, the standard ig-tree algorithm was used <ref> (Daelemans et al., 1995) </ref>. Since ig tree is a symbolic learning algorithm, the 7-letter input patterns were not encoded by binary values, but by the letters themselves. 4 Results An experiment is a 10-fold cross-validation application of one of the three learning algorithms to one of the four data sets.
Reference: <author> Fahlman, S. E. and Lebiere, C. </author> <year> (1990). </year> <title> The Cascade-correlation Learning Architecture. </title> <type> Technical Report CMU-CS-90-100, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference: <author> Rumelhart, D.E., Hinton, G.E., and Williams, R.J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. Mc-Clelland (Eds.), </editor> <booktitle> Parallel Distributed Processing, volume 1: Foundations, </booktitle> <pages> pp. 318-362. </pages> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Learning takes place by adjusting the weights of the connections between all units according to the error signal at the output layer <ref> (Rumelhart, Hinton, and Williams, 1986) </ref>. The number of connections follows from the numbers of layers and units predefined beforehand. In our experiments with bp, we adopted the coding technique used by Sejnowski and Rosenberg (1987), i.e., each of the 7 input letters are locally coded.
Reference: <author> Sejnowski, T.J., and Rosenberg, C.S. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> pp. 145-168. </pages>
Reference-contexts: We refer to the full data set of 89,019 words as d4. Instead of using words as instances, we used the windowing technique <ref> (cf. Sejnowski and Rosenberg, 1987) </ref> to convert words into fixed-length instances. An instance consists of a focus letter surrounded by 3 left context letters and 3 right context letters, including blanks before and after words. Dataset d1 thus contains 1,729 instances, d2 16,980, d3 168,549, and d4 750,874.
Reference: <author> Treiman, R., and Zukowski, A. </author> <year> (1990). </year> <title> Toward an understanding of English syllabification. </title> <journal> Journal of Memory and Language, </journal> <volume> 29, </volume> <pages> pp. 66-85. </pages> <editor> 8 Van den Bosch, A., Weijters, A., and Van den Herik, H.J. </editor> <year> (1995). </year> <title> Scaling effects with greedy and lazy machine-learning algorithms. </title> <booktitle> In Proceedings of the Dutch AI Conference, </booktitle> <address> NAIC-95, </address> <publisher> forthcoming. </publisher>
Reference-contexts: For English hyphenation, a fairly large number of cases exist that obey to a few simple, pronunciation-based principles <ref> (Treiman and Zukowski, 1990) </ref>. Moreover, the morphological principle introduces a large amount of periphery, since it states that morphological boundaries must receive hyphens, regardless of any other applicable principle.
Reference: <author> Zhang, J. </author> <year> (1992). </year> <title> Selecting typical instances in instance-based learning. </title> <booktitle> In Proceedings of the International Machine Learning Conference 1992, </booktitle> <pages> pp. 470-479. </pages>
References-found: 8

