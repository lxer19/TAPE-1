URL: http://www.cs.ualberta.ca/~upal/cluster/p3/p3.ps
Refering-URL: http://www.cs.ualberta.ca/~upal/cluster/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Comparison of Bayesian and Neural Net Unsupervised Classification Techniques  
Author: Muhammad Afzal Upal Eric Neufeld 
Abstract: Unsupervised classification is the classification of data into a number of classes in such a way that data in each class are all similar to each other. In the past there have been few if any studies done to compare the performance of different unsupervised classification techniques. In this paper we review Bayesian and neural net approaches to unsupervised classification and present results of experiments that we did to compare Autoclass, a Bayesian classification system, and ART2, a neural net classification algorithm.
Abstract-found: 1
Intro-found: 1
Reference: [CKS + 88a] <author> Peter Cheeseman, James Kelly, Matthew Self, John Stutz, Will Taylor, and Donn Freeman. </author> <title> Autoclass: A bayesian classification system. </title> <editor> In Michael B. Morgan, editor, </editor> <booktitle> Proceedings of Fifth International Conference On Machine Learning, </booktitle> <pages> pages 54-64, </pages> <address> San Mateo, California, 1988. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: The intractable complexity of the these calculations has often led researchers to consider alternative methods of classification that promise a sub-optimal classification. In the next section we will see how Autoclass solves this problem. 2.1 Autoclass Cheeseman et al. <ref> [CKS + 88a] </ref> [CKS + 88b] [CSH91] have adopt a different approach. Instead of opting for simpler alternatives to the classification problem, they adopt a different strategy that involves making simplifying assumptions about the classification model. <p> Iris is the only data-set from among the data-sets that were available to us that Cheeseman et al. had originally tested Autoclass on. They had reported that Autoclass was able to discover all three classes with high probability <ref> [CKS + 88a] </ref> [CKS + 88b] and [CSH91], but we were unable to repeat their results, despite repeated experiments.
Reference: [CKS + 88b] <author> Peter Cheeseman, James Kelly, Matthew Self, John Stutz, Will Taylor, and Donn Freeman. </author> <title> Bayesian classification. </title> <booktitle> In Proceedings of 7th International Conference on Artificial Intelligence, </booktitle> <address> Saint Paul, </address> <publisher> Minnesota, </publisher> <pages> pages 607-611, </pages> <year> 1988. </year>
Reference-contexts: The intractable complexity of the these calculations has often led researchers to consider alternative methods of classification that promise a sub-optimal classification. In the next section we will see how Autoclass solves this problem. 2.1 Autoclass Cheeseman et al. [CKS + 88a] <ref> [CKS + 88b] </ref> [CSH91] have adopt a different approach. Instead of opting for simpler alternatives to the classification problem, they adopt a different strategy that involves making simplifying assumptions about the classification model. <p> Iris is the only data-set from among the data-sets that were available to us that Cheeseman et al. had originally tested Autoclass on. They had reported that Autoclass was able to discover all three classes with high probability [CKS + 88a] <ref> [CKS + 88b] </ref> and [CSH91], but we were unable to repeat their results, despite repeated experiments.
Reference: [CSH91] <author> Peter Cheeseman, John Stutz, and Robin Hanson. </author> <title> Bayesian classification with correlation and inheritance. </title> <editor> In John Mylopoulos and Ray Reiter, editors, </editor> <booktitle> Proceedings of 12th International Joint Conference On Artificial Intelligence, </booktitle> <address> Sydney, Australia, </address> <pages> pages 692-698, </pages> <address> San Mateo, California, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The intractable complexity of the these calculations has often led researchers to consider alternative methods of classification that promise a sub-optimal classification. In the next section we will see how Autoclass solves this problem. 2.1 Autoclass Cheeseman et al. [CKS + 88a] [CKS + 88b] <ref> [CSH91] </ref> have adopt a different approach. Instead of opting for simpler alternatives to the classification problem, they adopt a different strategy that involves making simplifying assumptions about the classification model. <p> Cheeseman et al. <ref> [CSH91] </ref> have described different models including, single discrete model, single real attribute model, independent attribute model, fully co-variant discrete model, fully co-variant real model, block co-variance and the model for a population which is mixture of all these models. <p> Iris is the only data-set from among the data-sets that were available to us that Cheeseman et al. had originally tested Autoclass on. They had reported that Autoclass was able to discover all three classes with high probability [CKS + 88a] [CKS + 88b] and <ref> [CSH91] </ref>, but we were unable to repeat their results, despite repeated experiments.
Reference: [Dem77] <author> A. P. Dempster. </author> <title> Maximum likelihood from incomplete data via the em algorithm. </title> <journal> Royal Journal of Statistical Society, Series B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: Expectation Maximization (EM) algorithm of Dempster and Laird <ref> [Dem77] </ref> is used in order to find the optimal value of . Another problem is to determine optimal number of classes.
Reference: [Fis36] <author> R Fisher. </author> <title> The use of multiple measurements in taxonomic problems. </title> <booktitle> Annual Eugenics, </booktitle> <volume> 7 </volume> <pages> 179-188, </pages> <year> 1936. </year>
Reference-contexts: This database has been used by Ross Quinlan is his study on decision trees [Qui87]. There are no missing values in the data-base. 4.3 Iris Data This is the classical classification data set first used by Fisher in his analysis of linear discrimination function <ref> [Fis36] </ref>. The data contains 150 cases from three species of Iris, 50 cases from each class.
Reference: [FM89] <author> Douglas H. Fisher and Kathleen B. McKusick. </author> <title> An empirical comparison of id3 and back propagation. </title> <editor> In N. S. Sridharan, editor, </editor> <booktitle> Proceedings of Eleventh International Conference On Artificial Intelligence, </booktitle> <pages> pages 788-793, </pages> <address> San Mateo, California, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction In the face of rapid advancements in the fields of machine learning and classification, comparing classification methods is becoming a science in its own right. In the last few years various studies comparing of a variety of classification techniques have been reported [MSTG89] <ref> [FM89] </ref> [WK89] [WK91] and [FSK + 93]. These studies have exposed strengths and weaknesses of the classification methods. Another interesting result to come out of these studies has been the identification of some domains and tests that favor certain classification methods.
Reference: [FSK + 93] <author> C. Feng, A. Sutherland, R. King, S. Muggleton, and R. Henry. </author> <title> Comparison of machine learning classifiers to statistics and neural networks. </title> <booktitle> In Fourth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 41-52, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction In the face of rapid advancements in the fields of machine learning and classification, comparing classification methods is becoming a science in its own right. In the last few years various studies comparing of a variety of classification techniques have been reported [MSTG89] [FM89] [WK89] [WK91] and <ref> [FSK + 93] </ref>. These studies have exposed strengths and weaknesses of the classification methods. Another interesting result to come out of these studies has been the identification of some domains and tests that favor certain classification methods. <p> In our communications with them it turned out that they were also unable to reproduce the results they had reported earlier. 2 5.2 Cost Analysis of Misclassification There has been a recent trend in classification literature <ref> [FSK + 93] </ref> towards assigning costs to the classification errors. For example it would be more costly to the credit granting agency if classification system advises the agency to award credit to a bad credit risk case rather than if it advises to refuse credit to a good credit risk.
Reference: [Fuk90] <author> Keinosuke Fukunaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: i 2 c l ) replacing P (X i ) by its value in equation 1 we get, P (X i 2 c j jX i ) = P m It has been shown that Bayesian classifier is optimal in that it minimizes the probability of the error of classification <ref> [Fuk90] </ref>. But this optimal performance comes at the cost of the complexity of calculations. Often times the problem is that we cannot measure the prior probability of classes and we have to resort to estimating them subjectively.
Reference: [GCR91] <author> Stephen Grossberg, Gail Carpenter, and David Rosen. </author> <title> Fuzzy art: Fast stable learning and categorization of analog pattern by an adaptive resonance system. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 759-770, </pages> <year> 1991. </year>
Reference-contexts: But ART2 does not deal with missing information either, therefore, for ART2 we employed the technique known as complement coding suggested to us by Gaudiano 1 for dealing with incomplete information, which is a variation on Complement coding scheme described by Grossberg et al. in <ref> [GCR91] </ref>, the technique involves concatenating complement of the n-dimensional input vectors to the vectors themselves to make them 2n dimensional and replacing the don't care values and their complements by zero. Our experiments show that in case of this small data set this technique produced encouraging results.
Reference: [Gro76] <author> Stephen Grossberg. </author> <title> Adaptive pattern classification and universal recoding i: Parallel development and coding of neural feature dtectors. </title> <journal> Biological Cybernetics, </journal> <volume> 23 </volume> <pages> 121-134, </pages> <year> 1976. </year>
Reference-contexts: weights keep getting more fine tuned. 3.1 Adaptive Resonance Theory However, a competitive learning model does not always learn a stable code in response to arbitrary input patterns and problems arise especially if too many input patterns are presented to the network or if input patterns form too many clusters <ref> [Gro76] </ref>. The reason is that as new input patterns arrive old patterns are washed away, due to the plasticity or instability of competitive learning model. Grossberg's Adaptive Resonance Theory (henceforth ART) addresses exactly this plasticity-stability problem [Gro76], and forms the basis of ART1, ART2 and ART3 from which we have included <p> patterns are presented to the network or if input patterns form too many clusters <ref> [Gro76] </ref>. The reason is that as new input patterns arrive old patterns are washed away, due to the plasticity or instability of competitive learning model. Grossberg's Adaptive Resonance Theory (henceforth ART) addresses exactly this plasticity-stability problem [Gro76], and forms the basis of ART1, ART2 and ART3 from which we have included ART2 in our study. The basic structure of an ART2 network is shown in Figure 4. ART2 has two layers, F1 and F2.
Reference: [Koh84] <author> Tuevo Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: The change in the weights is propotional to the magnitude of the error. Although Rosenbaltt mentioned using perceptrons as self-organizing units [Ros58], it was Teovo Kohonen who developed his self-organizing maps that exploit the idea of competitive learning <ref> [Koh84] </ref> advanced by Grossberg and Malsburg [Von88]. The basic difference between competitive learning and perceptron learning is that activation pattern induced by input pattern is used to directly modify the weights, thereby reducing the need for an external teacher to provide the feedback (see Figure 2).
Reference: [Mic88] <author> D Michie. </author> <title> The fifth generation's unbridged gap. </title> <editor> In Rolf Herken, editor, </editor> <booktitle> The Universal Turing Machine: A Half Century Survey, </booktitle> <pages> pages 466-489. </pages> <publisher> Oxford University Press, </publisher> <address> London, </address> <year> 1988. </year>
Reference-contexts: We obtained these data-bases from UCI repository for machine learning data-bases [Mur94]. 4.1 Shuttle Landing Control Data This is a small databases containing only 15 instances and about 30 percent of information consists of the don't care values. This data-base has been used by Michie <ref> [Mic88] </ref>. Since both Autoclass and ART2 don't handle don't care values, we treated the don't care values as missing values in Autoclass.
Reference: [MSTG89] <author> Raymond Mooney, Jude Shavlik, Geoffrey Towell, and Allen Gove. </author> <title> An experimental comparison of symbolic and connectionist learning algorithms. </title> <editor> In N. S. Sridharan, editor, </editor> <booktitle> Proceedings of Eleventh International Conference On Artificial Intelligence, </booktitle> <pages> pages 775-780, </pages> <address> San Mateo, Cali-fornia, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction In the face of rapid advancements in the fields of machine learning and classification, comparing classification methods is becoming a science in its own right. In the last few years various studies comparing of a variety of classification techniques have been reported <ref> [MSTG89] </ref> [FM89] [WK89] [WK91] and [FSK + 93]. These studies have exposed strengths and weaknesses of the classification methods. Another interesting result to come out of these studies has been the identification of some domains and tests that favor certain classification methods.
Reference: [Mur94] <author> Patrick M Murphy. </author> <title> UCI Repository of machine learning databses [Machine readable data repository]. </title> <institution> Department of Information and Computer Science, University of California at Irvine, </institution> <address> Irvine, CA, </address> <year> 1994. </year>
Reference-contexts: All the data-sets have been previously used by other scientists studying classification. We obtained these data-bases from UCI repository for machine learning data-bases <ref> [Mur94] </ref>. 4.1 Shuttle Landing Control Data This is a small databases containing only 15 instances and about 30 percent of information consists of the don't care values. This data-base has been used by Michie [Mic88].
Reference: [Qui87] <author> Ross Quinlan. </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27 </volume> <pages> 221-234, </pages> <year> 1987. </year> <note> 10 Afzal Upal </note>
Reference-contexts: This database has been used by Ross Quinlan is his study on decision trees <ref> [Qui87] </ref>. There are no missing values in the data-base. 4.3 Iris Data This is the classical classification data set first used by Fisher in his analysis of linear discrimination function [Fis36]. The data contains 150 cases from three species of Iris, 50 cases from each class.
Reference: [Ros58] <author> Frank Rosenblatt. </author> <title> The perceptrons: A probabilistic model for information storage and organization in the brain. </title> <journal> Psychological Review, </journal> <volume> 65 </volume> <pages> 386-408, </pages> <year> 1958. </year>
Reference-contexts: Perhaps this explains why learning and classification are the central issues of neural networks research. Ever since the inception of back-coupled perceptrons by Rosenblatt <ref> [Ros58] </ref>, research in learning has dominated the work in neural networks. Years of research has produced a variety of neural models and although details of these models vary, they all share the basic structure. <p> The difference between the two outputs is termed as error and is fed back to adjust the weights. The change in the weights is propotional to the magnitude of the error. Although Rosenbaltt mentioned using perceptrons as self-organizing units <ref> [Ros58] </ref>, it was Teovo Kohonen who developed his self-organizing maps that exploit the idea of competitive learning [Koh84] advanced by Grossberg and Malsburg [Von88].
Reference: [Von88] <author> C Von der Malsburg. </author> <title> Self organization of orientation sensitive cells in the striate cortex. </title> <journal> Kyber-netik, </journal> <volume> 14 </volume> <pages> 85-100, </pages> <year> 1988. </year>
Reference-contexts: The change in the weights is propotional to the magnitude of the error. Although Rosenbaltt mentioned using perceptrons as self-organizing units [Ros58], it was Teovo Kohonen who developed his self-organizing maps that exploit the idea of competitive learning [Koh84] advanced by Grossberg and Malsburg <ref> [Von88] </ref>. The basic difference between competitive learning and perceptron learning is that activation pattern induced by input pattern is used to directly modify the weights, thereby reducing the need for an external teacher to provide the feedback (see Figure 2). The network has two layers F1 and F2.
Reference: [WK89] <author> Sholom M. Weiss and Ioannis Kapouleas. </author> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classificationmethods. </title> <editor> In N. S. Sridharan, editor, </editor> <booktitle> Proceedings of Eleventh International Conference On Artificial Intelligence, </booktitle> <pages> pages 781-787, </pages> <address> San Mateo, Cali-fornia, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction In the face of rapid advancements in the fields of machine learning and classification, comparing classification methods is becoming a science in its own right. In the last few years various studies comparing of a variety of classification techniques have been reported [MSTG89] [FM89] <ref> [WK89] </ref> [WK91] and [FSK + 93]. These studies have exposed strengths and weaknesses of the classification methods. Another interesting result to come out of these studies has been the identification of some domains and tests that favor certain classification methods.
Reference: [WK91] <author> Sholom M. Weiss and C. A. </author> <title> Kulikowski. Computer Systems that Learn: Classification and Prediction Methods From Statistics, Neural Networks, Machine Learning and Expert Systems. </title> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, California, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction In the face of rapid advancements in the fields of machine learning and classification, comparing classification methods is becoming a science in its own right. In the last few years various studies comparing of a variety of classification techniques have been reported [MSTG89] [FM89] [WK89] <ref> [WK91] </ref> and [FSK + 93]. These studies have exposed strengths and weaknesses of the classification methods. Another interesting result to come out of these studies has been the identification of some domains and tests that favor certain classification methods.
References-found: 19

