URL: http://cobar.cs.umass.edu/mlc94/papers/pfahringer.ps
Refering-URL: 
Root-URL: 
Email: E-mail: bernhard@ai.univie.ac.at  
Title: CIPF 2.0: A Robust Constructive Induction System  
Author: Bernhard Pfahringer 
Address: Schottengasse 3, A-1010 Vienna, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: We describe CIPF 2.0, a propositional constructive learner which is able to cope with both noise and representation mismatch in training examples simultaneously. CIPF 2.0's abilities stem from coupling the robust selective learner C4.5 (and its production rule generator) with a sophisticated constructive induction component. An important new general constructive operator incorporated into CIPF 2.0 is the simplified Kramer operator which abstracts combinations of two attributes into a single new boolean attribute. The so-called Minimum Description Length (MDL) principle acts as a powerful control heuristic guiding the search in the possibly vast representation space.
Abstract-found: 1
Intro-found: 1
Reference: [Angluin & Laird 87] <author> Angluin D., Laird P.: </author> <title> Learning from Noisy Examples, </title> <journal> Machine Learning, </journal> <volume> 2(4), pp.343-370, </volume> <year> 1987. </year>
Reference-contexts: This over-abundance in the representation space may quickly result in unwieldy, overly complex induced rule sets or attributes when learning without appropriate control. Such rule sets can be difficult for the user to comprehend and may yield mediocre results when classifying unseen examples. In analogy to noise fitting <ref> [Angluin & Laird 87] </ref> this phenomenon can be called language fitting. Typical examples of such behaviour are published in the section on AQ17-HCI in the Monk report [Thrun et al. 91], which describes three artificial learning problems for evaluating and comparing different algorithms.
Reference: [Bloedorn et al. 93] <author> Bloedorn E., Wnek J., Michalski R.S.: </author> <title> Mul tistrategy Constructive Induction: </title> <editor> AQ17-MCI, in Michalski R.S. and Tecuci G.(eds.), </editor> <booktitle> Proceedings of the Second International Workshop on Multistrategy Learning (MSL-93), </booktitle> <address> Harpers Ferry, W.VA., pp.188-206, </address> <year> 1993. </year>
Reference-contexts: The following subsections will describe these three essential parts forming CIPF's top-level loop which consists of modules for constructive induction, selective induction, and evaluation respectively. 2.1 CONSTRUCTIVE INDUCTION IN CIPF Just like the multi-strategy system AQ17-MCI <ref> [Bloedorn et al. 93] </ref>, CIPF takes an operator-based approach to constructive induction. It supplies a (still growing) list of generally useful CI operators plus an interface allowing for user-supplied special operators. For instance, these operators might encode possibly relevant background knowledge.
Reference: [Breiman et al. 84] <author> Breiman L., Friedman J.H., Olshen R.A., Stone C.J.: </author> <title> Classification and Regression Trees, </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, CA, </address> <publisher> The Wadsworth Statis-tics/Probability Series, </publisher> <year> 1984. </year>
Reference-contexts: Our simplifications allow for efficient implementation with tightly limited search (linear in the number of pairs of values), but still seem to yield useful (though maybe not always immediately the absolute best) abstractions. This operator can also be seen as a generalization of the technique described in <ref> [Breiman et al. 84] </ref> for computing optimal binary splits for single attributes: we handle combinations of two attributes. 4 EXPERIMENTS In the following experiments, CIPF 2.0's performance was usually averaged over ten runs randomly choosing the appropriate number of training examples and randomly reversing the class attribute (from yes to no
Reference: [Dietterich & Michalski 81] <author> Dietterich T.G., Michalski R.S.: </author> <title> In ductive Learning of Structural Descriptions: Evaluation Criteria and Comparative Review of Selected Methods, </title> <journal> Artificial Intelligence, </journal> <volume> 16(3), pp.257-294, </volume> <year> 1981. </year>
Reference-contexts: For example, decision trees encode axis-parallel nested hyper-rectangles. Two different problems may cause irregular distributions of learning examples in the original representation space: noise and/or an inadequate description language. As a remedy for the latter problem constructive induction has been introduced, e.g. in <ref> [Dietterich & Michalski 81] </ref> and [Mehra et al. 89]. The basic idea is to somehow transform the original representation space into a space where the learning examples exhibit (more) regularities. Usually this is done by introducing new attributes and forgetting old ones.
Reference: [Fuernkranz 93] <author> Fuernkranz J.: </author> <title> A numerical analysis of the KRK domain. </title> <note> Working Note, 1993. Available upon request. </note>
Reference-contexts: As such it has been used intensivly in inductive logic programming. There are a few hundred thousand different possible examples. <ref> [Fuernkranz 93] </ref> is a theoretical study including various approximate theories and showing a test-set size of 5000 to be sufficient for estimating accur-racies of induced theories. KRK is very easily represented for CIPF. <p> Noise% 100 Ex 250 Ex 500 Ex first best first best first best 0 98.6 98.6 98.5 98.6 99.1 99.3 20 92.4 93.2 96.2 96.5 97.2 97.8 Induced theories (with no noise present) usually resemble the approximate theories given in <ref> [Fuernkranz 93] </ref>. A sample theory derived by CIPF from 100 training examples is given in figure 1. This approximate theory was tested with 5000 test examples yielding an accuracy of 98.4%. This is consistent with [Fuernkranz 93] which proves a theory consisting of the first three clauses 1,2,3 to be 98.451% <p> 97.2 97.8 Induced theories (with no noise present) usually resemble the approximate theories given in <ref> [Fuernkranz 93] </ref>. A sample theory derived by CIPF from 100 training examples is given in figure 1. This approximate theory was tested with 5000 test examples yielding an accuracy of 98.4%. This is consistent with [Fuernkranz 93] which proves a theory consisting of the first three clauses 1,2,3 to be 98.451% correct.
Reference: [Furnkranz & Widmer 94] <author> Furnkranz J., Widmer G.: </author> <title> Incremen tal Reduced Error Pruning. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning (ML-94), </booktitle> <address> New Brunswick, N.J., </address> <year> 1994. </year>
Reference: [Kramer 94] <author> Kramer S.: CN2-MCI: </author> <title> A Two-Step Method for Constructive Induction, </title> <type> OeFAI Tech Report TR-94-13, </type> <institution> Vi-enna, </institution> <year> 1994. </year>
Reference-contexts: The main improvements in CIPF 2.0 are the use of the well-known sophisticated decision tree learner C4.5 [Quinlan 93] as the basic induction module and the inclusion of a new general operator for constructive induction, a simplified, more efficient version of the operator described in <ref> [Kramer 94] </ref>. These modifications allow for robust constructive induction handling both noise and an inadequate language. <p> Also, the strategy includes no user-settable parameters, and it does not require a secondary training set (train-test set) to evaluate the quality of constructed attributes, like e.g. AQ17-MCI or various forms of reduced error pruning [F urnkranz & Widmer 94] do. 3 THE SIMPLIFIED KRAMER OPERATOR In <ref> [Kramer 94] </ref> a new, general constructive induction operator is introduced, which essentially abstracts the extensional product of the set of possible values of two given attributes to a new boolean attribute. <p> We take care of immediately rejecting trivial constructions like tautologies or projections of one of the arguments. CIPF 2.0 introduces the following simplifications. In <ref> [Kramer 94] </ref> a heuristic chooses a few good rules and from these rules a few pairs of co-occuring attributes are taken as input for an involved A*-search for the best split according to another heuristic estimating split values.
Reference: [Matheus & Rendell 89] <author> Matheus C.J., Rendell L.A.: </author> <title> Construc tive Induction On Decision Trees, </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, 645-650, </address> <year> 1989. </year>
Reference-contexts: CI operators in CIPF 2.0: * Compare attributes of the same type: is attribute A1 Equal to/Different from attribute A2. * Conjoin possible values of nominal attributes occuring in good rules into a subset of useful values for the respective attribute. * Conjoin two attributes occuring in a good rule <ref> [Matheus & Rendell 89] </ref>.
Reference: [Mehra et al. 89] <author> Mehra P., Rendell L.A., Wah B.W.: </author> <title> Principled Constructive Induction, </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, 651-656, </address> <year> 1989. </year>
Reference-contexts: For example, decision trees encode axis-parallel nested hyper-rectangles. Two different problems may cause irregular distributions of learning examples in the original representation space: noise and/or an inadequate description language. As a remedy for the latter problem constructive induction has been introduced, e.g. in [Dietterich & Michalski 81] and <ref> [Mehra et al. 89] </ref>. The basic idea is to somehow transform the original representation space into a space where the learning examples exhibit (more) regularities. Usually this is done by introducing new attributes and forgetting old ones.
Reference: [Pagallo & Haussler 90] <author> Pagallo G., Haussler D.: </author> <title> Boolean Fea ture Discovery in Empirical Learning, </title> <journal> Machine Learning, </journal> <volume> 5(1), pp.71-99, </volume> <year> 1990. </year>
Reference-contexts: But in every test-run the initially induced theory was rewritten into a concise and easily comprehensible form like exemplified by the above given sample rule-set. 4.3 PARITY-5 AND OTHER BOOLEAN PROBLEMS PARITY-5 is one of the boolean functions used in <ref> [Pagallo & Haussler 90] </ref> to compare several learning methods. These functions are also used in [Wnek & Michalski 94] to test AQ17-HCI, where one can find more detailed descriptions, too. <p> A possible explanation is the bad initial performance of C4.5 yielding random theories. The CI part of CIPF is not able to introduce useful new attributes in such a case. We have also experimented with the other three boolean functions used in <ref> [Pagallo & Haussler 90] </ref>. the MUX-11, DNF-3, and DNF-4. Both MUX-11 and DNF-3 show a more graceful degradation of classification performance in % correct) . even &lt;= ci1006. odd &lt;= not ci1006. ci1006 &lt;= ci989=t, a4=0. ci344 &lt;= a2=a3.
Reference: [Pfahringer 94a] <author> Pfahringer B.: </author> <title> Controlling Constructive Induc tion in CiPF: An MDL Approach, </title> <booktitle> in Proceedings of the Eu-ropean Conference on Machine Learning (ECML94), </booktitle> <year> 1994. </year>
Reference-contexts: Usually this is done by introducing new attributes and forgetting old ones. So constructive induction is searching for an adequate representation language for the learning task at hand. In <ref> [Pfahringer 94a] </ref> we reported on our constructive learner CIPF and its relative success in noise-free domains due to rigid control based on the Minimum Description Length [Rissanen 78] principle.
Reference: [Pfahringer 94b] <author> Pfahringer B.: </author> <title> Robust Constructive Induction. </title> <type> OeFAI Tech Report TR-94-11, </type> <institution> Vienna, </institution> <year> 1994. </year>
Reference-contexts: These modifications allow for robust constructive induction handling both noise and an inadequate language. A more detailed description of CIPF 2.0 can be found in <ref> [Pfahringer 94b] </ref>. 2 THE ARCHITECTURE OF CIPF Most implemented constructive induction systems can be described in terms of three different modules working together: a constructive induction module, a selective learner, and an evaluation component.
Reference: [Quinlan & Rivest 89] <author> Quinlan J.R, Rivest R.L.: </author> <title> Inferring Deci sion Trees using the Minimum Description Length Principle, </title> <booktitle> in Information and Computation, </booktitle> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: Now we can answer the question of which rule set will be chosen as the final result of induction. Instead of using some ad-hoc measures of accuracy and quality or some user-supplied evaluation functions we have identified the so-called Minimum Description Length Principle <ref> [Rissanen 78, Quinlan & Rivest 89] </ref> as a very well-performing evaluator when it comes to choosing the one best rule set from all the induced rule sets. For any set of rules generated by the selective learner a cost can be computed.
Reference: [Quinlan & Cameron-Jones 93] <author> Quinlan J.R., Cameron-Jones R.M.: </author> <title> FOIL: A Midterm Report, in Brazdil P.B.(ed.), Machine Learning: </title> <publisher> ECML-93, Springer, </publisher> <address> Berlin, pp.3-20, </address> <year> 1993. </year>
Reference-contexts: research directions for CIPF 2.0 include adding again support for numerical attributes (as was already present in CIPF 1.0), application to especially medical databases, and the definition of constructive operators dealing with structured objects, hopefully giving CiPF some of the representational power found in inductive logic programming systems like FOIL <ref> [Quinlan & Cameron-Jones 93] </ref>. The simplified Kramer operator will also be generalized to construct multi-valued attributes in addition to binary attributes. We experienced the necessity for such more diverse abstractions especially in the context of multiple-class learning problems.
Reference: [Quinlan 93] <author> Quinlan J.R.: C4.5: </author> <title> Programs for Machine Learn ing, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: In [Pfahringer 94a] we reported on our constructive learner CIPF and its relative success in noise-free domains due to rigid control based on the Minimum Description Length [Rissanen 78] principle. The main improvements in CIPF 2.0 are the use of the well-known sophisticated decision tree learner C4.5 <ref> [Quinlan 93] </ref> as the basic induction module and the inclusion of a new general operator for constructive induction, a simplified, more efficient version of the operator described in [Kramer 94]. These modifications allow for robust constructive induction handling both noise and an inadequate language. <p> Recursive application of these operators may yield complex new attributes. 2.2 CIPF'S SELECTIVE LEARNER Due to CIPF 1.0's weaknesses in handling noise CIPF 2.0 now incorporates C4.5 <ref> [Quinlan 93] </ref> as its selective learner, a sophisticated decision tree algorithm well able to deal with noise. C4.5 also includes a rule-generator transforming decision trees into sets of production rules. These rules are then processed and analyzed by CIPF 2.0's module for constructive induction. <p> The rule set with minimum cost is supposed to be (and in the experiments reported below most often really is) the best theory for the training data. The precise formula used to apply the MDL Principle in CIPF is the same one as used by C4.5 <ref> [Quinlan 93] </ref> for simplifying rule sets. It is of course necessary to also assess and include cost for constructed attributes into the total cost of theories.
Reference: [Rissanen 78] <author> Rissanen J.: </author> <title> Modeling by Shortest Data Descrip tion, </title> <journal> in Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: So constructive induction is searching for an adequate representation language for the learning task at hand. In [Pfahringer 94a] we reported on our constructive learner CIPF and its relative success in noise-free domains due to rigid control based on the Minimum Description Length <ref> [Rissanen 78] </ref> principle. The main improvements in CIPF 2.0 are the use of the well-known sophisticated decision tree learner C4.5 [Quinlan 93] as the basic induction module and the inclusion of a new general operator for constructive induction, a simplified, more efficient version of the operator described in [Kramer 94]. <p> Now we can answer the question of which rule set will be chosen as the final result of induction. Instead of using some ad-hoc measures of accuracy and quality or some user-supplied evaluation functions we have identified the so-called Minimum Description Length Principle <ref> [Rissanen 78, Quinlan & Rivest 89] </ref> as a very well-performing evaluator when it comes to choosing the one best rule set from all the induced rule sets. For any set of rules generated by the selective learner a cost can be computed.
Reference: [Thrun et al. 91] <author> Thrun S.B., et.al.: </author> <title> The MONK's Problems: A Performance Comparison of Different Learning Algorithms, </title> <type> CMU Tech Report, </type> <institution> CMU-CS-91-197, </institution> <year> 1991. </year>
Reference-contexts: In analogy to noise fitting [Angluin & Laird 87] this phenomenon can be called language fitting. Typical examples of such behaviour are published in the section on AQ17-HCI in the Monk report <ref> [Thrun et al. 91] </ref>, which describes three artificial learning problems for evaluating and comparing different algorithms. We have made similar experiences with early versions of CIPF lacking sophisticated control. <p> Nonetheless the final result is most often better than the initial result, and for the rare cases where it is not, differences are marginal, e.g. an accuracy of 80% instead of 82%. 4.1 MONK'S PROBLEMS The Monk's problems <ref> [Thrun et al. 91] </ref> are three artificially constructed problems in a space formed by six nominal attributes having from two to four possible values. There is a total of 432 different possible examples. The three problems are abbreviated to Monk1, Monk2, and Monk3 in the following.
Reference: [Wnek & Michalski 94] <author> Wnek J., Michalski R.S.: </author> <title> Hypothesis Driven Constructive Induction in AQ17-HCI: A Method and Experiments, </title> <journal> Machine Learning, </journal> <volume> 14(2), pp.139-168, </volume> <year> 1994. </year>
Reference-contexts: These functions are also used in <ref> [Wnek & Michalski 94] </ref> to test AQ17-HCI, where one can find more detailed descriptions, too. For PARITY-5 the concept to be learned is even/odd parity of the first five boolean attributes of a given example in the presence of 27 irrelevant random boolean attributes per example.
References-found: 18

