URL: http://www.cs.duke.edu/~mlittman/docs/routing-iwannt.ps
Refering-URL: http://www.cs.duke.edu/~mlittman/topics/routing-page.html
Root-URL: 
Title: A Distributed Reinforcement Learning Scheme for Network Routing  
Author: Michael Littman*, Justin Boyan 
Address: Pittsburgh, PA  Morristown, NJ  
Affiliation: Carnegie Mellon University School of Computer Science  also Cognitive Science Research Group, Bellcore  
Abstract: In this paper we describe a self-adjusting algorithm for packet routing in which a reinforcement learning method is embedded into each node of a network. Only local information is used at each node to keep accurate statistics on which routing policies lead to minimal routing times. In simple experiments involving a 36-node irregularly-connected network, this learning approach proves superior to routing based on precomputed shortest paths.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Bellman. </author> <title> On a routing problem. </title> <journal> Quarterly of Applied Mathematics, </journal> <volume> 16(1) </volume> <pages> 87-90, </pages> <year> 1958. </year>
Reference-contexts: Each point represents the median over three trials of the mean 3 In fact, it is interesting to note that the Q-learning update rule is mathematically very much like the well-known Bellman-Ford shortest paths algorithm <ref> [1, 2] </ref>, except our path relaxation steps are performed asynchronously. packet delivery time (after learning has settled). When the load is very low, our learning algorithm routes nearly as efficiently as the shortest path policy.
Reference: [2] <author> L. R. Ford, Jr. </author> <title> Flows in Networks. </title> <publisher> Princeton University Press, </publisher> <year> 1962. </year>
Reference-contexts: Each point represents the median over three trials of the mean 3 In fact, it is interesting to note that the Q-learning update rule is mathematically very much like the well-known Bellman-Ford shortest paths algorithm <ref> [1, 2] </ref>, except our path relaxation steps are performed asynchronously. packet delivery time (after learning has settled). When the load is very low, our learning algorithm routes nearly as efficiently as the shortest path policy.
Reference: [3] <author> L.-J. Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: In the field of reinforcement learning, the Q-function, Q x (y; d), is often approximated by a neural network (see e.g. <ref> [3, 6] </ref>); this can allow the learner 2 We denote the function by Q because it corresponds to the "Q-function" used in the reinforcement learning technique of Q-learning [7]. to incorporate diverse parameters of the system, such as local queue size and time of day, into its distance estimation.
Reference: [4] <author> H. </author> <title> Rudin. On routing and delta routing: A taxonomy and performance comparison of techniques for packet-switched networks. </title> <journal> IEEE Transactions on Communications, </journal> <volume> COM-24(1):43-59, </volume> <month> January </month> <year> 1976. </year>
Reference-contexts: In this paper, we present an example of such an algorithm for the problem of routing packets efficiently in a communication network with an irregular topology and unpredictable usage patterns <ref> [4, 5] </ref>. The algorithm must learn a routing policy which balances minimizing the number of "hops" a packet will take with the possibility of congestion along popular routes. It does this by experimenting with different routing policies and collecting statistics on which policies minimize total delivery time.
Reference: [5] <author> A. Tanenbaum. </author> <title> Computer Networks. </title> <publisher> Prentice-Hall, </publisher> <address> second edition edition, </address> <year> 1989. </year>
Reference-contexts: In this paper, we present an example of such an algorithm for the problem of routing packets efficiently in a communication network with an irregular topology and unpredictable usage patterns <ref> [4, 5] </ref>. The algorithm must learn a routing policy which balances minimizing the number of "hops" a packet will take with the possibility of congestion along popular routes. It does this by experimenting with different routing policies and collecting statistics on which policies minimize total delivery time.
Reference: [6] <author> G. Tesauro. </author> <title> Practial issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8(3/4), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: In the field of reinforcement learning, the Q-function, Q x (y; d), is often approximated by a neural network (see e.g. <ref> [3, 6] </ref>); this can allow the learner 2 We denote the function by Q because it corresponds to the "Q-function" used in the reinforcement learning technique of Q-learning [7]. to incorporate diverse parameters of the system, such as local queue size and time of day, into its distance estimation.
Reference: [7] <author> C. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, </address> <year> 1989. </year>
Reference-contexts: Although in principle such an approach could be very expensive in terms of learning time and storage space, the algorithm we describe here is a variant of a reinforcement learning algorithm called Q-learning <ref> [7] </ref> which adapts to changes in network traffic and requires little more space than that needed to represent a complete routing policy. <p> In the field of reinforcement learning, the Q-function, Q x (y; d), is often approximated by a neural network (see e.g. [3, 6]); this can allow the learner 2 We denote the function by Q because it corresponds to the "Q-function" used in the reinforcement learning technique of Q-learning <ref> [7] </ref>. to incorporate diverse parameters of the system, such as local queue size and time of day, into its distance estimation.
References-found: 7

