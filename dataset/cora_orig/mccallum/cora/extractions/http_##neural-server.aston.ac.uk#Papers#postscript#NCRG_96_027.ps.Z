URL: http://neural-server.aston.ac.uk/Papers/postscript/NCRG_96_027.ps.Z
Refering-URL: http://www.cs.toronto.edu/~carl/gp.html
Root-URL: 
Email: d.barber@aston.ac.uk c.k.i.williams@aston.ac.uk  
Title: Gaussian Processes for Bayesian Classification via Hybrid Monte Carlo  
Author: David Barber and Christopher K. I. Williams 
Address: B4 7ET, UK  
Affiliation: Neural Computing Research Group Department of Computer Science and Applied Mathematics Aston University, Birmingham  
Date: 1997  
Note: To appear in: Advances in Neural Information Processing Systems 9, eds. M. C. Mozer, M. I. Jordan and T. Petsche. MIT Press,  
Abstract: The full Bayesian method for applying neural networks to a prediction problem is to set up the prior/hyperprior structure for the net and then perform the necessary integrals. However, these integrals are not tractable analytically, and Markov Chain Monte Carlo (MCMC) methods are slow, especially if the parameter space is high-dimensional. Using Gaussian processes we can approximate the weight space integral analytically, so that only a small number of hyperparameters need be integrated over by MCMC methods. We have applied this idea to classification problems, obtaining ex cellent results on the real-world problems investigated so far. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Duane, S., A. D. Kennedy, B. J. Pendleton, and D. </author> <month> Roweth </month> <year> (1987). </year> <title> Hybrid Monte Carlo. </title> <journal> Physics Letters B 195, </journal> <pages> 216-222. </pages>
Reference: <author> Green, P. J.and Silverman, B. W. </author> <year> (1994). </year> <title> Nonparametric regression and generalized linear models. </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Neal, R. M. </author> <year> (1996). </year> <title> Bayesian Learning for Neural Networks. </title> <booktitle> Springer. Lecture Notes in Statistics 118. </booktitle>
Reference-contexts: Gaussian Processes (GPs) achieve just that, being examples of stochastic process priors over functions that allow the efficient computation of predictions. It is also possible to show that a large class of neural network models converge to GPs in the limit of an infinite number of hidden units <ref> (Neal, 1996) </ref>. In previous work (Williams and Rasmussen, 1996) we have applied GP priors over functions to the problem of predicting a real-valued output, and found that the method has comparable performance to other state-of-the-art methods. This paper extends the use of GP priors to classification problems. <p> The w l parameters in equation 8 allow a different length scale on each input dimension. For irrelevant inputs, the corresponding w l will become small, and the model will ignore that input. This is closely related to the Automatic Relevance Determination (ARD) idea of MacKay and Neal <ref> (Neal, 1996) </ref>.
Reference: <author> Ripley, B. </author> <year> (1996). </year> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge. </publisher>
Reference-contexts: For the Pima Indians diabetes problem we have used the data as made available by Prof. Ripley, with his training/test split of 200 and 332 examples respectively <ref> (Ripley, 1996) </ref>. The baseline error obtained by simply classifying each record as coming from a diabetic gives rise to an error of 33%. Again, the GP method is comparable with the best alternative performance, with an error of around 20%. <p> Network (2) used two hidden units and the predictive approach (Ripley, 1994), which uses Laplace's approximation to weight each network local minimum. Network (3) had one hidden unit and was trained with maximum likelihood; the results were worse for nets with two or more hidden units <ref> (Ripley, 1996) </ref>. Table 2: Percentage classification error on the Forensic Glass task. 5 All available from http://markov.stats.ox.ac.uk/pub/PRNN. Our method is readily extendable to multiple class problems by using the softmax function.
Reference: <author> Ripley, B. D. </author> <year> (1994). </year> <title> Flexible Non-linear Approaches to Classification. </title> <editor> In V. Cherkassy, J. H. Friedman, and H. Wechsler (Eds.), </editor> <booktitle> From Statistics to Neural Networks, </booktitle> <pages> pp. 105-126. </pages> <publisher> Springer. </publisher>
Reference-contexts: Network (2) used two hidden units and the predictive approach <ref> (Ripley, 1994) </ref>, which uses Laplace's approximation to weight each network local minimum. Network (3) had one hidden unit and was trained with maximum likelihood; the results were worse for nets with two or more hidden units (Ripley, 1996).
Reference: <author> Sacks, J., W. J. Welch, T. J. Mitchell, and H. P. </author> <title> Wynn (1989). Design and analysis of computer experiments. </title> <booktitle> Statistical Science 4(4), </booktitle> <pages> 409-435. </pages>
Reference: <author> Williams, C. K. I. </author> <title> Computing with infinite networks. This volume. </title>
Reference: <author> Williams, C. K. I. and C. E. </author> <title> Rasmussen (1996). Gaussian processes for regression. </title> <editor> In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pp. 514-520. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: It is also possible to show that a large class of neural network models converge to GPs in the limit of an infinite number of hidden units (Neal, 1996). In previous work <ref> (Williams and Rasmussen, 1996) </ref> we have applied GP priors over functions to the problem of predicting a real-valued output, and found that the method has comparable performance to other state-of-the-art methods. This paper extends the use of GP priors to classification problems. <p> This covariance function has been studied by Sacks et al (1989) and can be obtained from a network of Gaussian radial basis functions in the limit of an infinite number of hidden units <ref> (Williams, 1996) </ref>. The w l parameters in equation 8 allow a different length scale on each input dimension. For irrelevant inputs, the corresponding w l will become small, and the model will ignore that input.
References-found: 8

