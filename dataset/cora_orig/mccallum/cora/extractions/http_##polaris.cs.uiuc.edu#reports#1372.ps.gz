URL: http://polaris.cs.uiuc.edu/reports/1372.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Email: moreira@csrd.uiuc.edu,  cdp@csrd.uiuc.edu,  
Phone: (217) 244-0049  (217) 244-4144 Fax: (217) 244-1351  
Title: On the Implementation and Effectiveness of Autoscheduling  
Author: Jose E. Moreira Constantine D. Polychronopoulos 
Note: This work was supported in part by the National Science Foundation under grant NSF CCR 89-57310, and the Office of Naval Research under grant N00014-94-1-0234.  
Date: 1372  
Address: 1308 W. Main St. Urbana, IL 61801-2307 USA  
Affiliation: Center for Supercomputing Research and Development and Coordinated Science Laboratory University of Illinois at Urbana-Champaign  
Pubnum: CSRD Report No.  
Abstract: Autoscheduling is a scheduling methodology that provides efficient support for multiprocessing and multiprogramming in multiprocessors by exploiting parallelism at different levels of granularity. Autoscheduling operates on the hierarchical task graph (HTG), an intermediate program representation that encapsulates the information on control and data dependences at all levels. In this paper we discuss the fundamentals of autoscheduling and describe how the HTG can be used by an au-toscheduling compiler to generate object code that exploits the parallelism present in the program. We also discuss several implementations issues for autoscheduling in shared memory processors. Validation and performance measurements of the autoscheduled code are accomplished by means of an instruction-level multiprocessor simulator. Our results show that autoscheduling can achieve high levels of efficiency on a multiprocessor and enables the exploitation of additional parallelism in a program, resulting in better speedups. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: The execution of a program begins with its START task in the ready queue and terminates when its STOP task finishes. The activation frames of a parallel program cannot be implemented with the simple stack structure normally used in sequential programs <ref> [1] </ref>, since several instances of subroutines and loop iterations can be active at the same time. Instead, a cactus-stack [27], equivalent to a (dynamic) tree of activation frames, is used. For example, consider the code in Figure 2 (a).
Reference: [2] <author> Thomas Anderson et al. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 95-109, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Substantial effort has been put into the development of lightweight threads to reduce management overhead, allowing processors to switch between tasks in a very short time, and to exploit more levels of parallelism. Examples of such work can be found in <ref> [3, 2, 5, 10, 19, 11] </ref>. In particular, [3] presents data structure and algorithm alternatives for thread management in shared memory multiprocessors, including centralized and distributed ready queues. Some memory management issues for parallel programs are addressed in [40].
Reference: [3] <author> Thomas Anderson, Edward Lazowska, and Henry Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12), </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: Substantial effort has been put into the development of lightweight threads to reduce management overhead, allowing processors to switch between tasks in a very short time, and to exploit more levels of parallelism. Examples of such work can be found in <ref> [3, 2, 5, 10, 19, 11] </ref>. In particular, [3] presents data structure and algorithm alternatives for thread management in shared memory multiprocessors, including centralized and distributed ready queues. Some memory management issues for parallel programs are addressed in [40]. <p> Substantial effort has been put into the development of lightweight threads to reduce management overhead, allowing processors to switch between tasks in a very short time, and to exploit more levels of parallelism. Examples of such work can be found in [3, 2, 5, 10, 19, 11]. In particular, <ref> [3] </ref> presents data structure and algorithm alternatives for thread management in shared memory multiprocessors, including centralized and distributed ready queues. Some memory management issues for parallel programs are addressed in [40].
Reference: [4] <author> Carl J. Beckmann and Constantine D. Polychronopoulos. </author> <title> Microarchitecture support for dynamic scheduling of acyclic task graphs. </title> <booktitle> In Proceedings 25th Annual International Symposium on Microar-chitecture, </booktitle> <pages> pages 140-148, </pages> <address> Portland, Oregon, December 1992. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The (embedded) scheduler can therefore be highly optimized for each particular program, making parallel processing affordable at fine-granularity levels. Although the abstract model of autoscheduling has been previously developed and analyzed <ref> [31, 21, 13, 4, 7] </ref>, in this paper we address the design issues and implementation details of autoscheduling for shared-memory architectures. We implement the model and conduct experiments to measure the the effectiveness of the approach for the first time. <p> The STOP node is shaded. 3.1 Bit-Vector Algorithm for Task Scheduling One of the alternatives for implementing execution tags is with bit-vectors <ref> [4] </ref>. Let T G be a given task graph (a level of an HTG), with n nodes. Let x y be an arc in the CFG of T G, from node x to node y. <p> To minimize accesses to shared memory, DONE and CONT are packed into one bit-vector SATISFIED [1::2n] which can be stored in a single word. SATISFIED [1::n] = DONE [1::n], and SATISFIED [n + 1::2n] = CONT [1::n]. The algorithms described in <ref> [4, 7] </ref> can be used to minimize the number of bits necessary for each task graph in a program and to guarantee that any bit-vector will fit in one word. 10 The update of control and data dependences and the testing of execution tags are implemented by performing simple bit-wise logical
Reference: [5] <author> David L. Black. </author> <title> Scheduling support for concurrency and parallelism in the mach operating system. </title> <journal> IEEE Computer Magazine, </journal> <pages> pages 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Substantial effort has been put into the development of lightweight threads to reduce management overhead, allowing processors to switch between tasks in a very short time, and to exploit more levels of parallelism. Examples of such work can be found in <ref> [3, 2, 5, 10, 19, 11] </ref>. In particular, [3] presents data structure and algorithm alternatives for thread management in shared memory multiprocessors, including centralized and distributed ready queues. Some memory management issues for parallel programs are addressed in [40].
Reference: [6] <author> William Blume and Rudolf Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the Perfect Benchmarks programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6), </volume> <month> November </month> <year> 1992. </year>
Reference-contexts: Dynamic allocation of activation frames is necessary to support reentrant code, recursion, and (most important in our case) concurrency. Variable privatization has been demonstrated to be an important feature for parallelization of programs <ref> [6] </ref>. To implement variable privatization with static allocation, it is necessary to expand the loop local variables by adding one dimension to their structure and distributing them among the processors along that dimension. The major disadvantage of doing 15 this is that the maximum number of processors must be known.
Reference: [7] <author> Carl J. Beckmann. </author> <title> Hardware and Software for Functional and Fine Grain Parallelism. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, </institution> <year> 1993. </year>
Reference-contexts: The (embedded) scheduler can therefore be highly optimized for each particular program, making parallel processing affordable at fine-granularity levels. Although the abstract model of autoscheduling has been previously developed and analyzed <ref> [31, 21, 13, 4, 7] </ref>, in this paper we address the design issues and implementation details of autoscheduling for shared-memory architectures. We implement the model and conduct experiments to measure the the effectiveness of the approach for the first time. <p> The efficiency in scheduling provided by the drive code makes affordable parallel processing of tasks as small as 100 instructions without the need of special hardware, and can be applied to instruction level parallelism with hardware support <ref> [7] </ref>. Moreover, it allows a program to control the size of parallel tasks at run-time, in order to better suit the dynamic environment conditions. <p> It represents a program in a hierarchical structure, thus facilitating task-granularity control. Information on control and data dependences allows the exploitation of functional (task level) parallelism in addition to data (loop level) parallelism. The definitions, properties, and construction mechanism of the HTG presented here are from <ref> [31, 12, 21, 7] </ref>. The hierarchical task graph is a directed acyclic graph HT G = (HV; HE) with unique nodes START and STOP 2 HV . <p> Node A is a loop node, B and C are compound loop nodes, and D is a compound node. The arcs of an HTG are the union of control and data dependence arcs. The optimizations described in <ref> [7, 21] </ref> can be used to eliminate redundant arcs, creating a minimum set of dependences that enforce correct execution and thus allow a more efficient autoscheduled execution. <p> The candidates are all the nodes that are data and control dependent on node x, plus all the nodes that are data dependent on nodes bypassed by branch x y, minus those nodes bypassed by branch x y <ref> [7] </ref>. A bit-vector DONE [1::n] is used to represent the data dependences. DONE [i] is set to TRUE whenever the data dependences originating from node i are satisfied. Another bit-vector, CONT [1::n] is used to represent the control dependences. <p> To minimize accesses to shared memory, DONE and CONT are packed into one bit-vector SATISFIED [1::2n] which can be stored in a single word. SATISFIED [1::n] = DONE [1::n], and SATISFIED [n + 1::2n] = CONT [1::n]. The algorithms described in <ref> [4, 7] </ref> can be used to minimize the number of bits necessary for each task graph in a program and to guarantee that any bit-vector will fit in one word. 10 The update of control and data dependences and the testing of execution tags are implemented by performing simple bit-wise logical <p> The theoretical foundations for the HTG and autoscheduling were developed in [13, 21]. The functions of the entry and exit blocks, and a method for granularity control, were defined in [31]. Efficient implementation schemes for the scheduling operations and algorithms for optimizing the HTG are presented in <ref> [7, 21] </ref>. It is shown in [39] how the HTG can be used for instruction scheduling in superscalar processors, in order to exploit instruction level parallelism. <p> The motivation to exploit functional parallelism comes from measurements of the dynamic behavior of programs, such as those in [8, 29], in which large amounts of parallelism have been found beyond that available from totally parallel loops alone. In <ref> [7] </ref>, however, it is shown that the amount of functional parallelism exploitable through the HTG is modest for scientific programs (in the order of 2 to 4). 29 Another effort to exploit functional parallelism is described in [33], where scheduling and allocation algorithms for the exploitation of functional parallelism on distributed
Reference: [8] <author> Ding-Kai Chen. </author> <title> MaxPar: An execution driven simulator for studying parallel systems. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: It is shown in [39] how the HTG can be used for instruction scheduling in superscalar processors, in order to exploit instruction level parallelism. The motivation to exploit functional parallelism comes from measurements of the dynamic behavior of programs, such as those in <ref> [8, 29] </ref>, in which large amounts of parallelism have been found beyond that available from totally parallel loops alone.
Reference: [9] <author> David E. Culler et al. </author> <title> Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV). </booktitle> <address> April 8-11, 1991. Santa Clara, CA, </address> <pages> pages 164-175, </pages> <year> 1991. </year>
Reference-contexts: Discussion of other approaches to software implementation of data flow models can be found in [38] for the J-Machine and in <ref> [9] </ref> for a machine built with commercial microprocessors. The P-RISC architecture [24] uses special processor instructions, multithreading, and a token queue to exploit fine grain parallelism, with data flow capability.
Reference: [10] <author> Richard Draves et al. </author> <title> Using continuations to implement thread management and communication in operating systems. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 122-136, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Substantial effort has been put into the development of lightweight threads to reduce management overhead, allowing processors to switch between tasks in a very short time, and to exploit more levels of parallelism. Examples of such work can be found in <ref> [3, 2, 5, 10, 19, 11] </ref>. In particular, [3] presents data structure and algorithm alternatives for thread management in shared memory multiprocessors, including centralized and distributed ready queues. Some memory management issues for parallel programs are addressed in [40].
Reference: [11] <author> Derek Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support for shared-memory parallel computing. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(1), </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: Substantial effort has been put into the development of lightweight threads to reduce management overhead, allowing processors to switch between tasks in a very short time, and to exploit more levels of parallelism. Examples of such work can be found in <ref> [3, 2, 5, 10, 19, 11] </ref>. In particular, [3] presents data structure and algorithm alternatives for thread management in shared memory multiprocessors, including centralized and distributed ready queues. Some memory management issues for parallel programs are addressed in [40].
Reference: [12] <author> M. Girkar and C. D. Polychronopoulos. </author> <title> The HTG: An intermediate representation for programs based on control and data dependences. </title> <type> Technical Report 1046, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: It represents a program in a hierarchical structure, thus facilitating task-granularity control. Information on control and data dependences allows the exploitation of functional (task level) parallelism in addition to data (loop level) parallelism. The definitions, properties, and construction mechanism of the HTG presented here are from <ref> [31, 12, 21, 7] </ref>. The hierarchical task graph is a directed acyclic graph HT G = (HV; HE) with unique nodes START and STOP 2 HV .
Reference: [13] <author> Milind Girkar and Constantine Polychronopoulos. </author> <title> Automatic detection and generation of unstructured parallelism in ordinary programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2), </volume> <month> April </month> <year> 1992. </year>
Reference-contexts: The (embedded) scheduler can therefore be highly optimized for each particular program, making parallel processing affordable at fine-granularity levels. Although the abstract model of autoscheduling has been previously developed and analyzed <ref> [31, 21, 13, 4, 7] </ref>, in this paper we address the design issues and implementation details of autoscheduling for shared-memory architectures. We implement the model and conduct experiments to measure the the effectiveness of the approach for the first time. <p> Performance results are presented in Section 6, related work is discussed in Section 7, and concluding remarks are given in Section 8. 3 2 Task Management in Autoscheduling The approach discussed here is based on the autoscheduling abstract architectural model of [31]. This model uses the hierarchical task graph <ref> [13] </ref> of a program (described below) as its execution vehicle. The efficiency in scheduling provided by the drive code makes affordable parallel processing of tasks as small as 100 instructions without the need of special hardware, and can be applied to instruction level parallelism with hardware support [7]. <p> The goal, as in autoscheduling, is to execute a parallel program at the appropriate granularity. The partitioning algorithm attempts to minimize the estimated parallel execution time, based on dependence information and execution-profiling information. The theoretical foundations for the HTG and autoscheduling were developed in <ref> [13, 21] </ref>. The functions of the entry and exit blocks, and a method for granularity control, were defined in [31]. Efficient implementation schemes for the scheduling operations and algorithms for optimizing the HTG are presented in [7, 21].
Reference: [14] <author> V. G. Grafe and J. E. Hoch. </author> <title> The Epsilon-2 Hybrid Dataflow Architecture. </title> <booktitle> In Proceedings of the 35 th IEEE Computer Society Conference. </booktitle> <address> Feb 26-Mar 2, 1990. San Francisco, CA, </address> <pages> pages 88-93, </pages> <year> 1990. </year>
Reference-contexts: Examples of machines specifically designed for macro data flow operation include the EM-4 [34], Epsilon-2 <ref> [14] </ref>, OSCAR [17], and Harray [41, 42]. The theory for construction and partitioning of a program dependence graph (PDG) and its use in the exploitation of loop and functional parallelism is presented in [35]. The goal, as in autoscheduling, is to execute a parallel program at the appropriate granularity.
Reference: [15] <author> Anoop Gupta, Andrew Tucker, and Luis Stevens. </author> <title> Making effective use of shared-memory multi-processors: The process control approach. </title> <type> Technical Report CSL-TR-91-475A, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1991. </year>
Reference-contexts: Some memory management issues for parallel programs are addressed in [40]. Issues on the partitioning of the physical set of processors of a machine, and their allocation to parallel processes under simultaneous execution on a multiprogramming environment, are discussed in <ref> [15] </ref>. 8 Conclusion We have argued that autoscheduling can be efficiently implemented on a shared memory multiprocessor based on commercial microprocessors.
Reference: [16] <author> W. W. Hwu, S. A. Mahlke, W. Y. Chen, P. P. Chang, N. J. Warter, R. A. Bringmann, R. G Ouellette, R. E. Hank, T. Kiyohara, G. E. Haab, J. G. Holm, and D. M. Lavery. </author> <title> The Superblock: An effective technique for VLIW and superscalar compilation. </title> <journal> Journal of Supercomputing, </journal> <volume> 7(1) </volume> <pages> 229-248, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: A distributed queue consists of one task queue per processor assigned to the process. Each processor can enqueue and dequeue tasks from any processor in the partition, using the 2 Given that the minimal task size in our approach is not smaller than a superblock <ref> [16] </ref>, we can still take full advantage of the most aggressive register allocation policies. 12 following dequeueing and enqueueing policies: * Dequeueing: A processor first tries to dequeue a task from its local queue.
Reference: [17] <author> H. Kasahara et al. </author> <title> A multi-grain parallelizing compilation scheme for OSCAR. </title> <booktitle> In Proceedings of the Fourth International Workshop on Languages and Compilers for Parallel Computing. </booktitle> <address> Santa Clara, CA, </address> <year> 1991. </year>
Reference-contexts: Examples of machines specifically designed for macro data flow operation include the EM-4 [34], Epsilon-2 [14], OSCAR <ref> [17] </ref>, and Harray [41, 42]. The theory for construction and partitioning of a program dependence graph (PDG) and its use in the exploitation of loop and functional parallelism is presented in [35]. The goal, as in autoscheduling, is to execute a parallel program at the appropriate granularity.
Reference: [18] <author> Evangelos Markatos. </author> <title> Scheduling for Locality in Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Rochester, </institution> <year> 1993. </year>
Reference: [19] <author> Brian Marsh et al. </author> <title> First-class user-level threads. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 110-121, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Substantial effort has been put into the development of lightweight threads to reduce management overhead, allowing processors to switch between tasks in a very short time, and to exploit more levels of parallelism. Examples of such work can be found in <ref> [3, 2, 5, 10, 19, 11] </ref>. In particular, [3] presents data structure and algorithm alternatives for thread management in shared memory multiprocessors, including centralized and distributed ready queues. Some memory management issues for parallel programs are addressed in [40].
Reference: [20] <author> Cathy McCann, Raj Vaswany, and John Zahorjan. </author> <title> A dynamic processor allocation policy for mul-tiprogrammed shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2), </volume> <month> May </month> <year> 1993. </year>
Reference: [21] <author> Milind Girkar. </author> <title> Functional Parallelism: Theoretical Foundations and Implementation. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: The (embedded) scheduler can therefore be highly optimized for each particular program, making parallel processing affordable at fine-granularity levels. Although the abstract model of autoscheduling has been previously developed and analyzed <ref> [31, 21, 13, 4, 7] </ref>, in this paper we address the design issues and implementation details of autoscheduling for shared-memory architectures. We implement the model and conduct experiments to measure the the effectiveness of the approach for the first time. <p> It represents a program in a hierarchical structure, thus facilitating task-granularity control. Information on control and data dependences allows the exploitation of functional (task level) parallelism in addition to data (loop level) parallelism. The definitions, properties, and construction mechanism of the HTG presented here are from <ref> [31, 12, 21, 7] </ref>. The hierarchical task graph is a directed acyclic graph HT G = (HV; HE) with unique nodes START and STOP 2 HV . <p> Node A is a loop node, B and C are compound loop nodes, and D is a compound node. The arcs of an HTG are the union of control and data dependence arcs. The optimizations described in <ref> [7, 21] </ref> can be used to eliminate redundant arcs, creating a minimum set of dependences that enforce correct execution and thus allow a more efficient autoscheduled execution. <p> The goal, as in autoscheduling, is to execute a parallel program at the appropriate granularity. The partitioning algorithm attempts to minimize the estimated parallel execution time, based on dependence information and execution-profiling information. The theoretical foundations for the HTG and autoscheduling were developed in <ref> [13, 21] </ref>. The functions of the entry and exit blocks, and a method for granularity control, were defined in [31]. Efficient implementation schemes for the scheduling operations and algorithms for optimizing the HTG are presented in [7, 21]. <p> The theoretical foundations for the HTG and autoscheduling were developed in [13, 21]. The functions of the entry and exit blocks, and a method for granularity control, were defined in [31]. Efficient implementation schemes for the scheduling operations and algorithms for optimizing the HTG are presented in <ref> [7, 21] </ref>. It is shown in [39] how the HTG can be used for instruction scheduling in superscalar processors, in order to exploit instruction level parallelism.
Reference: [22] <author> Jose E. Moreira. </author> <title> Autoscheduling Compilers. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, </institution> <month> August </month> <year> 1994. </year> <note> In preparation. </note>
Reference-contexts: data or control dependent on any other task. 5 The Simulator The correctness and quality of the code generated by the autoscheduling compiler as well as the performance of the target architecture is measured by actually executing the autoscheduling code, by means of an instruction-level simulator of a shared-memory multiprocessor <ref> [22] </ref>. The instruction set architecture used in the simulator is a three address load/store architecture. All arithmetic and logic operations are executed between registers, with two registers as sources and a third register as destination. The instruction set architecture does not specify the number of registers in the processor.
Reference: [23] <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind. </author> <title> *T: A multithreaded massively parallel architecture. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture. </booktitle> <address> Gold Coast, Australia., </address> <pages> pages 156-167, </pages> <year> 1992. </year>
Reference-contexts: The P-RISC architecture [24] uses special processor instructions, multithreading, and a token queue to exploit fine grain parallelism, with data flow capability. The *T architecture <ref> [23] </ref> represents the step following P-RISC in dynamic data flow; it is implemented with (almost) stock microprocessors, and it is is even more compatible with conventional parallel machines based on von Neumann processors.
Reference: [24] <author> Rishiyur S. Nikhil and Arvind. </author> <booktitle> Can dataflow subsume von Neumann computing? In Proceedings of the 16 th International Symposium on Computer Architecture. </booktitle> <month> May, </month> <year> 1989, </year> <title> Jerusalem, </title> <booktitle> Israel, </booktitle> <pages> pages 46-53, </pages> <year> 1989. </year>
Reference-contexts: In this way, the equivalent of thread creation and scheduling can be achieved at much lower costs, effectively implementing a macro-data flow model of execution <ref> [24, 26] </ref>, in which the execution of tasks is constrained only by data and control dependences, and by the availability of processors. 2.1 The Hierarchical Task Graph The hierarchical task graph (HTG) is an intermediate program representation that encapsulates data and control dependences at various granularity levels. <p> Discussion of other approaches to software implementation of data flow models can be found in [38] for the J-Machine and in [9] for a machine built with commercial microprocessors. The P-RISC architecture <ref> [24] </ref> uses special processor instructions, multithreading, and a token queue to exploit fine grain parallelism, with data flow capability.
Reference: [25] <author> Gregory M. Papadopoulos and David E. Culler. Monsoon: </author> <title> an Explicit Token-Store Architecture. </title> <booktitle> In Proceedings of the 17 th International Symposium on Computer Architecture. </booktitle> <address> May 28-31, 1990, Seattle, Washington, </address> <pages> pages 82-91, </pages> <year> 1990. </year>
Reference-contexts: ....... ....... ....... ....... ...... ....... ....... ....... ....... ....... ....... ....... ...... ....... ....... ....... ....... ....... ....... ....... ............................................................................................................................................................................................................... * * * parallelism (ff = 1). 28 7 Related Work Current research on macro-data flow systems includes the explicit token store architecture (ETS) used in the Monsoon machine <ref> [25, 26] </ref>, in which multithreading with very efficient fork and join operations is used to implement a data flow model of execution.
Reference: [26] <author> Gregory M. Papadopoulos and Kenneth R. Traub. </author> <title> Multithreading: A Revisionist View of Dataflow Architectures. </title> <booktitle> In Proceedings of the 18 th International Symposium on Computer Architecture. </booktitle> <month> May 27-30, </month> <year> 1991, </year> <institution> Toronto, </institution> <address> Canada, </address> <pages> pages 342-351, </pages> <year> 1991. </year>
Reference-contexts: In this way, the equivalent of thread creation and scheduling can be achieved at much lower costs, effectively implementing a macro-data flow model of execution <ref> [24, 26] </ref>, in which the execution of tasks is constrained only by data and control dependences, and by the availability of processors. 2.1 The Hierarchical Task Graph The hierarchical task graph (HTG) is an intermediate program representation that encapsulates data and control dependences at various granularity levels. <p> ....... ....... ....... ....... ...... ....... ....... ....... ....... ....... ....... ....... ...... ....... ....... ....... ....... ....... ....... ....... ............................................................................................................................................................................................................... * * * parallelism (ff = 1). 28 7 Related Work Current research on macro-data flow systems includes the explicit token store architecture (ETS) used in the Monsoon machine <ref> [25, 26] </ref>, in which multithreading with very efficient fork and join operations is used to implement a data flow model of execution.
Reference: [27] <author> Per Stenstrom. </author> <title> VLSI Support for Cactus Stack Oriented Memory Organization. </title> <booktitle> In Proceedings of the 21 st Annual Hawaii International Conference on System Sciences, </booktitle> <volume> vol I., </volume> <pages> pages 211-220, </pages> <year> 1988. </year>
Reference-contexts: The activation frames of a parallel program cannot be implemented with the simple stack structure normally used in sequential programs [1], since several instances of subroutines and loop iterations can be active at the same time. Instead, a cactus-stack <ref> [27] </ref>, equivalent to a (dynamic) tree of activation frames, is used. For example, consider the code in Figure 2 (a). If it is executed serially, its run-time stack evolves as shown in Figure 2 (b), where only the frame at the top of the stack is active.
Reference: [28] <author> Paul Petersen and David Padua. </author> <title> Machine independent evaluation of parallelizing compilers. </title> <type> Technical Report 1173, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference: [29] <author> Paul Marx Petersen. </author> <title> Evaluation of Programs and Parallelizing Compilers Using Dynamic Analysis Techniques. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> January </month> <year> 1993. </year> <month> 32 </month>
Reference-contexts: It is shown in [39] how the HTG can be used for instruction scheduling in superscalar processors, in order to exploit instruction level parallelism. The motivation to exploit functional parallelism comes from measurements of the dynamic behavior of programs, such as those in <ref> [8, 29] </ref>, in which large amounts of parallelism have been found beyond that available from totally parallel loops alone.
Reference: [30] <author> Constantine D. Polychronopoulos. </author> <title> On Program Restructuring, Scheduling, and Communications for Parallel Processor Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: In the buddy system, space for 16 activation frames is allocated from a heap and deallocated as soon as the frame is no longer needed. 3.5 Implementing DOALL Tasks The current implementation of autoscheduling uses self-scheduling (SS) or guided self-scheduling (GSS) <ref> [30] </ref>), as selected by the user, to distribute the iterations of a DOALL loop to P processors.
Reference: [31] <author> Constantine D. Polychronopoulos. Autoscheduling: </author> <title> Control flow and data flow come together. </title> <type> Technical Report 1058, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: The (embedded) scheduler can therefore be highly optimized for each particular program, making parallel processing affordable at fine-granularity levels. Although the abstract model of autoscheduling has been previously developed and analyzed <ref> [31, 21, 13, 4, 7] </ref>, in this paper we address the design issues and implementation details of autoscheduling for shared-memory architectures. We implement the model and conduct experiments to measure the the effectiveness of the approach for the first time. <p> Performance results are presented in Section 6, related work is discussed in Section 7, and concluding remarks are given in Section 8. 3 2 Task Management in Autoscheduling The approach discussed here is based on the autoscheduling abstract architectural model of <ref> [31] </ref>. This model uses the hierarchical task graph [13] of a program (described below) as its execution vehicle. <p> It represents a program in a hierarchical structure, thus facilitating task-granularity control. Information on control and data dependences allows the exploitation of functional (task level) parallelism in addition to data (loop level) parallelism. The definitions, properties, and construction mechanism of the HTG presented here are from <ref> [31, 12, 21, 7] </ref>. The hierarchical task graph is a directed acyclic graph HT G = (HV; HE) with unique nodes START and STOP 2 HV . <p> The partitioning algorithm attempts to minimize the estimated parallel execution time, based on dependence information and execution-profiling information. The theoretical foundations for the HTG and autoscheduling were developed in [13, 21]. The functions of the entry and exit blocks, and a method for granularity control, were defined in <ref> [31] </ref>. Efficient implementation schemes for the scheduling operations and algorithms for optimizing the HTG are presented in [7, 21]. It is shown in [39] how the HTG can be used for instruction scheduling in superscalar processors, in order to exploit instruction level parallelism.
Reference: [32] <author> Constantine D. Polychronopoulos et al. </author> <title> Parafrase-2: an environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 1(1) </volume> <pages> 45-72, </pages> <year> 1989. </year>
Reference: [33] <author> Shankar Ramswamy and Prithviraj Banerjee. </author> <title> Processor allocation and scheduling of macro dataflow graphs on distributed memory multicomputers by the paradigm compiler. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <pages> pages 134-138, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: In [7], however, it is shown that the amount of functional parallelism exploitable through the HTG is modest for scientific programs (in the order of 2 to 4). 29 Another effort to exploit functional parallelism is described in <ref> [33] </ref>, where scheduling and allocation algorithms for the exploitation of functional parallelism on distributed memory machines are developed. The allocation and scheduling of tasks to processors are done statically, at compile time, based on a cost model for computation and communication.
Reference: [34] <author> Shuichi Sakai, Yoshinori Yamaguchi, Kei Hiraki, Yuetsu Kodama, and Toshigtsu Yuba. </author> <title> An Architecture of a Dataflow Single Chip Processor. </title> <booktitle> In Proceedings of the 16 th International Symposium on Computer Architecture. </booktitle> <month> May, </month> <year> 1989, </year> <title> Jerusalem, </title> <booktitle> Israel, </booktitle> <pages> pages 46-53, </pages> <year> 1989. </year>
Reference-contexts: The *T architecture [23] represents the step following P-RISC in dynamic data flow; it is implemented with (almost) stock microprocessors, and it is is even more compatible with conventional parallel machines based on von Neumann processors. Examples of machines specifically designed for macro data flow operation include the EM-4 <ref> [34] </ref>, Epsilon-2 [14], OSCAR [17], and Harray [41, 42]. The theory for construction and partitioning of a program dependence graph (PDG) and its use in the exploitation of loop and functional parallelism is presented in [35].
Reference: [35] <author> Vivek Sarkar. </author> <title> Automatic partitioning of a program dependence graph into parallel tasks. </title> <journal> IBM Journal of Research and Development, </journal> 35(5/6):779-804, September/October 1991. 
Reference-contexts: Examples of machines specifically designed for macro data flow operation include the EM-4 [34], Epsilon-2 [14], OSCAR [17], and Harray [41, 42]. The theory for construction and partitioning of a program dependence graph (PDG) and its use in the exploitation of loop and functional parallelism is presented in <ref> [35] </ref>. The goal, as in autoscheduling, is to execute a parallel program at the appropriate granularity. The partitioning algorithm attempts to minimize the estimated parallel execution time, based on dependence information and execution-profiling information. The theoretical foundations for the HTG and autoscheduling were developed in [13, 21].
Reference: [36] <author> Vivek Sarkar and David Cann. </author> <title> POSC a partitioning and optimizing SISAL compiler. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 148-163, </pages> <year> 1990. </year>
Reference: [37] <author> Dale A. Schouten. </author> <title> Design and Implementation of Autoscheduled Threads. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> August </month> <year> 1994. </year> <note> In preparation. </note>
Reference-contexts: number of processors in the partition, at the time the compound task starts execution: mode = 8 &gt; &gt; : 1 otherwise where Q n is the number of tasks in the queue, P is the number of processors currently assigned to the process, and ff is a given threshold <ref> [37] </ref>. In general, the best value for ff will be machine and application dependent. However, the value of ff can be fine-tuned by the operating system if performance monitoring of the processors is available. This issue will not be considered further in this paper. <p> Definition of the kernel operations for autoscheduling is beyond the scope of this paper. Work in this area can be found in <ref> [37] </ref>. Adding a processor to a partition consists of assigning the process address space to it and making it 19 jump to the task fetch loop: the processor starts fetching tasks from the task ready queue and executes them.
Reference: [38] <author> Ellen Spertus and William J. Dally. </author> <title> Experiences Implementing Dataflow on a General-Purpose Parallel Computer. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <month> August 12-16, </month> <year> 1991., </year> <pages> pages II-231-235, </pages> <year> 1991. </year>
Reference-contexts: Discussion of other approaches to software implementation of data flow models can be found in <ref> [38] </ref> for the J-Machine and in [9] for a machine built with commercial microprocessors. The P-RISC architecture [24] uses special processor instructions, multithreading, and a token queue to exploit fine grain parallelism, with data flow capability.
Reference: [39] <author> David R. Wallace. </author> <title> Low level scheduling using the hierarchical task graph. </title> <booktitle> In Proceedings of the 1992 International Conference on Supercomputing. </booktitle> <address> Washington, D.C., </address> <pages> pages 72-81, </pages> <year> 1992. </year>
Reference-contexts: The functions of the entry and exit blocks, and a method for granularity control, were defined in [31]. Efficient implementation schemes for the scheduling operations and algorithms for optimizing the HTG are presented in [7, 21]. It is shown in <ref> [39] </ref> how the HTG can be used for instruction scheduling in superscalar processors, in order to exploit instruction level parallelism.
Reference: [40] <author> M. Weiss, Z. Fhang, C. R. Morgan, and P. </author> <title> Belmont. Dynamic scheduling and memory management for parallel programs. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 161-165, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Examples of such work can be found in [3, 2, 5, 10, 19, 11]. In particular, [3] presents data structure and algorithm alternatives for thread management in shared memory multiprocessors, including centralized and distributed ready queues. Some memory management issues for parallel programs are addressed in <ref> [40] </ref>.
Reference: [41] <editor> H. Yamana et al. </editor> <booktitle> System architecture of parallel processing system -Harray-. In Proceedings of the 1988 International Conference on Supercomputing. </booktitle> <address> St. Malo, France, </address> <year> 1988. </year>
Reference-contexts: Examples of machines specifically designed for macro data flow operation include the EM-4 [34], Epsilon-2 [14], OSCAR [17], and Harray <ref> [41, 42] </ref>. The theory for construction and partitioning of a program dependence graph (PDG) and its use in the exploitation of loop and functional parallelism is presented in [35]. The goal, as in autoscheduling, is to execute a parallel program at the appropriate granularity.
Reference: [42] <editor> H. Yamana et al. </editor> <booktitle> Parallel Processing System -Harray-. Computing Systems in Engineering, </booktitle> <volume> 1(1) </volume> <pages> 111-130, </pages> <year> 1990. </year>
Reference-contexts: Examples of machines specifically designed for macro data flow operation include the EM-4 [34], Epsilon-2 [14], OSCAR [17], and Harray <ref> [41, 42] </ref>. The theory for construction and partitioning of a program dependence graph (PDG) and its use in the exploitation of loop and functional parallelism is presented in [35]. The goal, as in autoscheduling, is to execute a parallel program at the appropriate granularity.
References-found: 42

