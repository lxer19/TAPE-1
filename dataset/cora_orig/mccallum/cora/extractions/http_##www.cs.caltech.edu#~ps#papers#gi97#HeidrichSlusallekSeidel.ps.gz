URL: http://www.cs.caltech.edu/~ps/papers/gi97/HeidrichSlusallekSeidel.ps.gz
Refering-URL: http://www.cs.caltech.edu/~ps/papers/gi97/
Root-URL: http://www.cs.caltech.edu
Email: e-mail: fheidrich,slusallek,seidelg@informatik.uni-erlangen.de  
Phone: Phone: +49.9131.859926 Fax: +49.9131.859931  
Title: An Image-Based Model for Realistic Lens Systems in Interactive Computer Graphics  
Author: Wolfgang Heidrich, Philipp Slusallek, Hans-Peter Seidel 
Keyword: Lens Systems, Image-Based Rendering, Hardware Acceleration, Interactive Computer Graphics  
Address: Am Weichselgarten 9 91058 Erlangen, Germany  
Affiliation: Computer Graphics Group University of Erlangen  
Abstract: Many applications, such as realistic rendering, virtual and augmented reality, and virtual studios, require an accurate simulation of real lens and camera systems at interactive rates, including depth of field and geometric aberrations, in particular distortions. Unfortunately, camera models used in Computer Graphics are either too simple to describe these effects or too expensive to simulate for interactive use. In this paper, we introduce an image-based lens model that is powerful enough to simulate sophisticated properties of real lens systems, yet fast enough for interactive graphics. By exploiting coherence, common graphics hardware can be used to yield high frame rates. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Max Born and Emil Wolf. </author> <title> Principles of Optics. </title> <publisher> Perga-mon Press, Oxford, </publisher> <year> 1993. </year>
Reference-contexts: The simplest model of lenses with circular symmetry and finite aperture is the thin lens approximation. This model is used in optics and lens design to describe some of the properties of simple lens systems <ref> [1] </ref>. The fundamental assumption of the thin lens model is that the lens is of negligible thickness. As a consequence, light passing through the lens is refracted only in a single plane, the principal plane, and moves in a straight line otherwise.
Reference: [2] <author> Robert L. Cook, Thomas Porter, and Loren Carpenter. </author> <title> Distributed ray tracing. </title> <booktitle> In Computer Graphics (SIG-GRAPH '84 Proceedings), </booktitle> <pages> pages 134-145, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: In the past, most approaches for simulating these properties have been based on off-line rendering methods such as distribution ray-tracing <ref> [2, 9] </ref>. Efforts for improved lens and camera models for interactive computer graphics have mainly been restricted to the simulation of depth of field [8, 13]. <p> This distance can be easily derived from the well-known relationship 1 + s 0 = f 2.3 Rendering Thin Lenses Typically, rendering of thin lenses is done using distribution ray-tracing <ref> [2] </ref>. For each sample point on the film, rays are cast through random sample points on the principal plane, and the resulting color values are averaged.
Reference: [3] <author> Johannes Flugge. Das photographische Objektiv. Springer Wien, </author> <year> 1955. </year>
Reference-contexts: In particular, we have used an achromatic doublet as shown in the top row of in the bottom row. The achromatic doublet is a real lens system that has been developed and used in real cameras in the 1920s <ref> [3] </ref>. The top left of Figure 6 shows an image rendered with the algorithms described in this paper, while the top right shows reference images of the same scene rendered using distribution ray-tracing. In both images the barrel distortions of the lens are obvious.
Reference: [4] <author> James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. </author> <title> Computer Graphics, Principles and Practice, Second Edition. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1990. </year> <title> Overview of research to date. bottom row. The images in the left column have been rendered using the algorithm described in this paper, while distribution ray-tracing has been used for the images on the right. </title>
Reference-contexts: This projection can be described as a perspective transformation, whose parameters are the dimensions of the film and the distance of the film plane from the hole, along with additional near and far clipping planes (see, for example <ref> [4] </ref>). Usually, when dealing with perspective transformations, we do not think of it in terms of a pinhole camera, but as of a perspective projection with a center of projection (COP) and some virtual image plane in front of it.
Reference: [5] <author> Andrew Glassner. </author> <booktitle> Principles of Digital Image Synthesis, </booktitle> <volume> volume 2. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference: [6] <author> Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. </author> <booktitle> The lumigraph. In Computer Graphics (SIGGRAPH '96 Proceedings), </booktitle> <pages> pages 43-54, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Similarly to [9], the model uses the real geometry of lenses and computes an accurate approximation of the exposure on the film plane. However, instead of using ray-tracing, our model approximates the light field <ref> [10, 6] </ref> between the lens and the film plane. <p> Each of the images generated by these perspective projections represent a 2-dimensional slice of the 4-dimensional light field in front of the lens system, as described in <ref> [10, 6] </ref>. In the following we call this light field the scene light field. Due to the properties of the thin lens model, this slice is identical to a slice of the camera light field between the lens system and the film plane, defined by the refracted rays.
Reference: [7] <author> Keith D. Gremban, Charles E. Thorpe, and Takeo Kanade. </author> <title> Geometric camera calibration using systems of linear equations. </title> <booktitle> In 1988 IEEE International Conference on Robotics and Animation, </booktitle> <volume> volume 1, </volume> <pages> pages 562-567, </pages> <year> 1988. </year>
Reference-contexts: The central task in approximating this transformation with a perspective projection is to find a good virtual COP. This problem is also known in computer vision, where the aberrations of camera lenses have to be removed based on the measured distortions on a calibration grid <ref> [7] </ref>. The approach proposed by the authors of [7] is to choose the COP so that its distance from all rays is minimal in the least-squares sense. For our purposes, this approach works well as long as there is no hierarchical subdivision. <p> This problem is also known in computer vision, where the aberrations of camera lenses have to be removed based on the measured distortions on a calibration grid <ref> [7] </ref>. The approach proposed by the authors of [7] is to choose the COP so that its distance from all rays is minimal in the least-squares sense. For our purposes, this approach works well as long as there is no hierarchical subdivision. As soon as subdivision is performed, however, intolerable dis-continuities occur between adjacent quad-tree cells.
Reference: [8] <author> Paul E. Haeberli and Kurt Akeley. </author> <title> The accumulation buffer: Hardware support for high-quality rendering. </title> <booktitle> In Computer Graphics (SIGGRAPH '90 Proceedings), </booktitle> <pages> pages 309-318, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: In the past, most approaches for simulating these properties have been based on off-line rendering methods such as distribution ray-tracing [2, 9]. Efforts for improved lens and camera models for interactive computer graphics have mainly been restricted to the simulation of depth of field <ref> [8, 13] </ref>. <p> The intersection point is then connected to the chosen sample point on the principal plane. An alternative approach is to select a fixed set of sample points on the principal plane <ref> [8] </ref>. All rays passing through a single sample point p i on the principal plane represent a perspective projection with p i as the COP (see Figure 3a). <p> All rays with the same line style form a slice of the respective light field. the slices. Averaging of slices from different sample points p i to form the final image can be done using an accumulation buffer <ref> [8] </ref>. It should be noted that the simple averaging of the slices of the light field does not yield the correct exposure on the film, as pointed out by [9]. In Section 3.5 we will show how the algorithm can be extended to compute the correct exposure on the film.
Reference: [9] <author> Craig Kolb, Don Mitchell, and Pat Hanrahan. </author> <title> A realistic camera model for computer graphics. </title> <booktitle> In Computer Graphics (SIGGRAPH '95 Proceedings), </booktitle> <pages> pages 317-324, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: In the past, most approaches for simulating these properties have been based on off-line rendering methods such as distribution ray-tracing <ref> [2, 9] </ref>. Efforts for improved lens and camera models for interactive computer graphics have mainly been restricted to the simulation of depth of field [8, 13]. <p> In this paper, we describe an image-based camera model for interactive graphics, which is capable of simulating a variety of properties of real lens systems at high frame rates. Similarly to <ref> [9] </ref>, the model uses the real geometry of lenses and computes an accurate approximation of the exposure on the film plane. However, instead of using ray-tracing, our model approximates the light field [10, 6] between the lens and the film plane. <p> It should be noted that the simple averaging of the slices of the light field does not yield the correct exposure on the film, as pointed out by <ref> [9] </ref>. In Section 3.5 we will show how the algorithm can be extended to compute the correct exposure on the film. Another approach to rendering depth of field effects of thin lens models is post-filtering as used in [12] and [13]. <p> The full geometric model correctly simulates all kinds of geometric aberrations, and is the only model that is capable of handling lenses without rotational symmetry, for example bi-focal or progressive addition lenses (PALs) used for eye glasses [11]. This model has been used in <ref> [9] </ref> to generate accurate simulations of complex lens systems using distribution ray-tracing. <p> It has been pointed out in <ref> [9] </ref>, that this is actually not a correct model for the exposure on the film. Exposure is the integral of irradiance incident on the film over time. <p> Assuming that a scene is static, that is, that the irradiance is constant over the exposure time, and that the shutter opens and closes instantaneously, the exposure is simply the irradiance times the exposure interval. Following the notation in <ref> [9] </ref>, the irradiance E (x 0 ) at a point x 0 on the film is given as E (x 0 ) = x 00 cos 0 cos 00 where x 00 is a point on the image-sided surface of the lens, and dA 00 is the differential area around this
Reference: [10] <author> Marc Levoy and Pat Hanrahan. </author> <title> Light field rendering. </title> <booktitle> In Computer Graphics (SIGGRAPH '96 Proceedings), </booktitle> <pages> pages 31-42, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Similarly to [9], the model uses the real geometry of lenses and computes an accurate approximation of the exposure on the film plane. However, instead of using ray-tracing, our model approximates the light field <ref> [10, 6] </ref> between the lens and the film plane. <p> Each of the images generated by these perspective projections represent a 2-dimensional slice of the 4-dimensional light field in front of the lens system, as described in <ref> [10, 6] </ref>. In the following we call this light field the scene light field. Due to the properties of the thin lens model, this slice is identical to a slice of the camera light field between the lens system and the film plane, defined by the refracted rays. <p> Due to the properties of the thin lens model, this slice is identical to a slice of the camera light field between the lens system and the film plane, defined by the refracted rays. For the light field, we use the same parameterization as <ref> [10] </ref>. Each point in this 4-dimensional space corresponds to a ray from the film plane (two parametric directions) to the principal plane (another two parametric directions).
Reference: [11] <author> J. Loos, G. Greiner, H.-P. Seidel, P. Slusallek, and E. Wirsching. </author> <title> Advanced spectacle lens design by combining wavefront tracing and variational design. </title> <type> Technical report, </type> <institution> Computer Graphics Group, University of Erlan-gen, </institution> <year> 1995. </year> <type> Technical Report 3/1995. </type>
Reference-contexts: The full geometric model correctly simulates all kinds of geometric aberrations, and is the only model that is capable of handling lenses without rotational symmetry, for example bi-focal or progressive addition lenses (PALs) used for eye glasses <ref> [11] </ref>. This model has been used in [9] to generate accurate simulations of complex lens systems using distribution ray-tracing.
Reference: [12] <author> M. Potmesil and I. Chakravarty. </author> <title> A lens and aperture camera model for synthetic image generation. </title> <booktitle> In Computer Graphics (SIGGRAPH '81 Proceedings), </booktitle> <pages> pages 297-305, </pages> <month> August </month> <year> 1981. </year>
Reference-contexts: In Section 3.5 we will show how the algorithm can be extended to compute the correct exposure on the film. Another approach to rendering depth of field effects of thin lens models is post-filtering as used in <ref> [12] </ref> and [13]. These methods require a filtering step to be performed in software, which causes high CPU loads and additional data transfers between the graphics board and main memory, and therefore leads to performance penalties.
Reference: [13] <author> M. Shinya. </author> <title> Post-filtering for depth of field simulation with ray distribution buffer. </title> <booktitle> In Proceedings of Graphics Interface '94, </booktitle> <pages> pages 59-66. </pages> <booktitle> Canadian Information Processing Society, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: In the past, most approaches for simulating these properties have been based on off-line rendering methods such as distribution ray-tracing [2, 9]. Efforts for improved lens and camera models for interactive computer graphics have mainly been restricted to the simulation of depth of field <ref> [8, 13] </ref>. <p> In Section 3.5 we will show how the algorithm can be extended to compute the correct exposure on the film. Another approach to rendering depth of field effects of thin lens models is post-filtering as used in [12] and <ref> [13] </ref>. These methods require a filtering step to be performed in software, which causes high CPU loads and additional data transfers between the graphics board and main memory, and therefore leads to performance penalties.
Reference: [14] <author> O. N. Stavroudis. </author> <title> The Optics of Rays, Wavefronts and Caustics, volume 38 of Pure and Applied Physics. </title> <publisher> Academic Press, </publisher> <year> 1972. </year>
References-found: 14

