URL: http://www.cs.cornell.edu/Info/People/coleman/PAPERS/CTC96-TR258.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/coleman/papers.html
Root-URL: 
Title: Structure and Efficient Hessian Calculation  
Author: Thomas F. Coleman Arun Verma 
Keyword: Hessian matrix, automatic differentiation, structured computation, sparsity.  
Date: August 26, 1996  
Abstract: Cornell Theory Center Technical Report CTC96-TR258 Abstract Modern methods for numerical optimization calculate (or approximate) the matrix of second derivatives, the Hessian matrix, at each iteration. The recent arrival of robust software for automatic differentiation allows for the possibility of automatically computing the Hessian matrix, and the gradient, given a code to evaluate the objective function itself. However, for large-scale problems direct application of automatic differentiation may be unacceptably expensive. Recent work has shown that this cost can be dramatically reduced in the presence of sparsity. In this paper we show that for structured problems it is possible to apply automatic differentiation tools in an economical way even in the absence of sparsity in the Hessian. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. M. Averick, J. J. More, C. H. Bischof, A. Carle, and A. Griewank, </author> <title> Computing large sparse Jacobian matrices using automatic differentiation, </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 15 (1994), </volume> <pages> pp. 285-294. </pages>
Reference-contexts: It is now possible to have first and second derivatives automatically computed given a code that computes the objective function. The difficulty is one of computational cost: straightforward application of automatic differentiation tools may be inordinately expensive for large problems. Results obtained in <ref> [1, 6] </ref> show that for the related sparse Jacobian problem, the cost can be dramatically reduced if sparsity is exploited. In principle similar techniques [5] can be applied to the sparse fl Presented at the International Conference on Nonlinear Programming, Beijing, September, 1996. <p> 2 ) with corresponding Newton equations, J E @ ffi y 1 1 0 0 F (x) A ;(4) where J E = 4 A x y 2 I A 3 The point here is that the "extended" Jacobian matrix J E is sparse and clearly sparse AD techniques, e.g., <ref> [1, 6, 9] </ref> can be applied with respect to F E (x; y) = @ A (x)y 2 y 1 1 3 to efficiently determine J E .
Reference: [2] <author> C. H. Bischof, A. Bouaricha, P. M. Khademi, and J. J. </author> <title> More, Computing gradients in large-scale optimization using automatic differentiation, </title> <type> Preprint MCS-P4880195, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Second, the structural ideas discussed above can, of course, be applied to the special case of gradient computation. This can be particularily useful when only the forward mode of AD is available, e.g., <ref> [2] </ref>. Third, the structural ideas discussed above can also be applied to the case where F (x) is a gradient function, rf (x), of a scalar-valued function f (x). In this case the computed Jacobian matrix of F corresponds to the Hessian matrix of f .
Reference: [3] <author> C. H. Bischof, A. Carle, G. F. Corliss, and A. Griewank, ADIFOR: </author> <title> Automatic differentiation in a source translation environment, </title> <booktitle> in Proceedings of the International Symposium on Symbolic and Algebraic Computation, </booktitle> <editor> P. S. Wang, ed., </editor> <address> New York, 1992, </address> <publisher> ACM Press, </publisher> <pages> pp. 294-302. </pages>
Reference-contexts: F E then r 2 xy g = r 2 yx g = 0; therefore, there is additional structure in the extended Hessian matrix: H S 0 0 I ~ F x yy g 0 xx g A : 4 Conclusions The arrival of robust, reliable automatic differentiation tools, e.g., <ref> [3, 8] </ref>, is a major new development in scientific computing. The potential impact on numerical optimization is enormous. This paper is concerned with the efficient determination of Hessian matrices, and Newton steps, in large-scale optimization problems.
Reference: [4] <author> C. </author> <title> Broyden, A class of methods for solving nonlinear simultaneous equations, </title> <journal> Mathematics of Computation, </journal> <volume> 19 (1965), </volume> <pages> pp. 577-593. </pages>
Reference-contexts: We consider a composite function of the form described above. In particular, the function ~ F is defined to be the Broyden <ref> [4] </ref> function (the Jacobian is tridiagonal). Function f is chosen to be a simple scalar-valued function with a tridiagonal Hessian matrix. The structure of A is based on the 5-point Laplacian defined on a regular p p n grid.
Reference: [5] <author> T. F. Coleman and J. Y. Cai, </author> <title> The cyclic coloring problem and estimation of sparse Hessian matrices, </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 7 (1986), </volume> <pages> pp. 221-235. </pages>
Reference-contexts: The difficulty is one of computational cost: straightforward application of automatic differentiation tools may be inordinately expensive for large problems. Results obtained in [1, 6] show that for the related sparse Jacobian problem, the cost can be dramatically reduced if sparsity is exploited. In principle similar techniques <ref> [5] </ref> can be applied to the sparse fl Presented at the International Conference on Nonlinear Programming, Beijing, September, 1996. This research was partially supported by the Applied Mathematical Sciences Research Program (KC-04-02) of the Office of Energy Research of the U.S. <p> The potential impact on numerical optimization is enormous. This paper is concerned with the efficient determination of Hessian matrices, and Newton steps, in large-scale optimization problems. If there is sparsity in the Hessian matrix then graph coloring techniques <ref> [5, 6] </ref> can be used to guide the use of AD software the efficiency gains can be significant. However, our thesis is that many large- scale problems exhibit structure at a high, accessible, level. Such problems often have dense Hessian matrices, rendering direct application of sparse AD techniques impotent.
Reference: [6] <author> T. F. Coleman and A. Verma, </author> <title> The efficient computation of sparse Jacobian matrices using automatic differentiation, </title> <type> Tech. Report TR95-1557, </type> <institution> Computer Science Department, Cornell University, </institution> <month> November </month> <year> 1995. </year> <title> [7] , Structure and efficient Jacobian calculation, in Computational Differentiation: Techniques, Applications, and Tools, </title> <editor> M. Berz, C. Bischof, G. Corliss, and A. Griewank, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1996, </year> <pages> pp. 149-159. </pages>
Reference-contexts: It is now possible to have first and second derivatives automatically computed given a code that computes the objective function. The difficulty is one of computational cost: straightforward application of automatic differentiation tools may be inordinately expensive for large problems. Results obtained in <ref> [1, 6] </ref> show that for the related sparse Jacobian problem, the cost can be dramatically reduced if sparsity is exploited. In principle similar techniques [5] can be applied to the sparse fl Presented at the International Conference on Nonlinear Programming, Beijing, September, 1996. <p> 2 ) with corresponding Newton equations, J E @ ffi y 1 1 0 0 F (x) A ;(4) where J E = 4 A x y 2 I A 3 The point here is that the "extended" Jacobian matrix J E is sparse and clearly sparse AD techniques, e.g., <ref> [1, 6, 9] </ref> can be applied with respect to F E (x; y) = @ A (x)y 2 y 1 1 3 to efficiently determine J E . <p> For example, the work required by the bi-coloring technique developed in <ref> [6] </ref> is of order !(F E ) = !(F ) where is a "bi-chromatic number" dependent on the sparsity of J E , and !() denotes the work required to evaluate the argument. Typically, &lt;< min (m; n). <p> The crucial observation here is that while the Jacobian of F is often dense in these cases, the Jacobian of the extended function F E , J E , is typically very sparse. Hence, the sparse AD techniques developed in <ref> [6] </ref>, for example, can be applied in combination with AD software to compute J E in an efficient manner. <p> First, the matrix H is likely to be dense, due to the action of A 1 , whereas under reasonable assumptions H E will be sparse. Second, matrix H E can be obtained using automatic differentiation applied to function GF E (x; y; w). Sparse AD techniques <ref> [6] </ref> can be applied to GF E (x; y; w) to allow for the economical calculation of H E . Third, it is also possible to obtain matrix H E without explicitly applying a sparse AD technique to the structured gradient function GF E . <p> Experiment 1 : Computing H E versus H In Figure 2 we compare the time to compute H directly, i.e., applying automatic differentiation directly to the function f to obtain the Hessian matrix H, versus the sparse AD computation of H E using the bi-coloring technique proposed in <ref> [6] </ref>. Experiments were performed using the AD-software package ADOL-C [8]. Fig. 2. ADOL-C experiment Clearly, exploiting sparsity is a big win and the advantage grows with problem size. <p> The potential impact on numerical optimization is enormous. This paper is concerned with the efficient determination of Hessian matrices, and Newton steps, in large-scale optimization problems. If there is sparsity in the Hessian matrix then graph coloring techniques <ref> [5, 6] </ref> can be used to guide the use of AD software the efficiency gains can be significant. However, our thesis is that many large- scale problems exhibit structure at a high, accessible, level. Such problems often have dense Hessian matrices, rendering direct application of sparse AD techniques impotent.
Reference: [8] <author> A. Griewank, D. Juedes, and J. Utke, ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 22 (1996), </volume> <pages> pp. 131-167. </pages>
Reference-contexts: Experiments were performed using the AD-software package ADOL-C <ref> [8] </ref>. Fig. 2. ADOL-C experiment Clearly, exploiting sparsity is a big win and the advantage grows with problem size. <p> F E then r 2 xy g = r 2 yx g = 0; therefore, there is additional structure in the extended Hessian matrix: H S 0 0 I ~ F x yy g 0 xx g A : 4 Conclusions The arrival of robust, reliable automatic differentiation tools, e.g., <ref> [3, 8] </ref>, is a major new development in scientific computing. The potential impact on numerical optimization is enormous. This paper is concerned with the efficient determination of Hessian matrices, and Newton steps, in large-scale optimization problems.
Reference: [9] <author> A. K. M. Hossain and T. Steihaug, </author> <title> Computing a sparse Jacobian matrix by rows and columns, </title> <type> Tech. Report 109, </type> <institution> Department of Informatics, University of Bergen, Bergen, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: 2 ) with corresponding Newton equations, J E @ ffi y 1 1 0 0 F (x) A ;(4) where J E = 4 A x y 2 I A 3 The point here is that the "extended" Jacobian matrix J E is sparse and clearly sparse AD techniques, e.g., <ref> [1, 6, 9] </ref> can be applied with respect to F E (x; y) = @ A (x)y 2 y 1 1 3 to efficiently determine J E .
Reference: [10] <institution> MATLAB 4.2c for UNIX, The Mathworks, Inc., </institution> <address> 24 Prime Park Way, Natick, Massachusetts 01760. </address>
References-found: 9

