URL: http://www.ai.mit.edu/people/holly/papers/ms-thesis.ps.Z
Refering-URL: http://www.ai.mit.edu/people/holly/papers/papers.html
Root-URL: 
Title: Robot Communication: Issues and Implementations  
Author: by Holly A. Yanco Lynn Andrea Stein 
Degree: B.A. Wellesley College (1991) Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Master of Science in Computer Science and Engineering at the  All rights reserved. Author  Certified by  Class of 1957 Assistant Professor of Computer Science Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Departmental Committee on Graduate Students  
Date: May 1994  May 12, 1994  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1994.  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [ AAAI , 1991 ] <institution> Proceedings of the Ninth National Conference on Artificial Intelligence, </institution> <address> Anaheim, California, </address> <month> July </month> <year> 1991. </year>
Reference: [ Arkin and Hobbs, 1993 ] <author> Ronald C. Arkin and J. David Hobbs. </author> <title> Dimensions of communication and social organization in multi-agent robotic systems. </title> <editor> In Meyer et al. [ Meyer et al., </editor> <year> 1993 </year> <month> ] , pages 486-493. </month>
Reference-contexts: For other examples of systems that use provided languages, see [ Fukuda and Kawauchi, 1990 ] , [ Matsumoto et al., 1990 ] , [ Shin and Epstein, 1985 ] , [ Arkin et al., 1993 ] , <ref> [ Arkin and Hobbs, 1993 ] </ref> , and [ Wei, 1993 ] Other systems use no communication between the robots. For example, Mataric [ Mataric, 1993a, Mataric, 1994, Mataric, 1993b ] discusses the building up of large social behaviors through the use of low level primitives.
Reference: [ Arkin et al., 1993 ] <author> Ronald C. Arkin, Tucker Balch, and Elizabeth Nitz. </author> <title> Communication of behavioral state in multi-agent retrieval tasks. </title> <booktitle> In Proceedings of the 1993 International Conference on Robotics and Automation, </booktitle> <pages> pages 588-594, </pages> <year> 1993. </year>
Reference-contexts: There is a global environment for communication; each creature can write a symbol to the global environment, but only one symbol can appear at a time. (This writing to the global environment is similar to communication using shared memory in robots in <ref> [ Arkin et al., 1993 ] </ref> .) Communicating and listening effectively adds to the fitness of a creature | and only the most fit creatures get to breed while the least fit creatures die. <p> For other examples of systems that use provided languages, see [ Fukuda and Kawauchi, 1990 ] , [ Matsumoto et al., 1990 ] , [ Shin and Epstein, 1985 ] , <ref> [ Arkin et al., 1993 ] </ref> , [ Arkin and Hobbs, 1993 ] , and [ Wei, 1993 ] Other systems use no communication between the robots.
Reference: [ Bond and Gasser, 1988 ] <editor> Alan H. Bond and Les Gasser, editors. </editor> <title> An analysis of problems and research in DAI, </title> <booktitle> chapter 1, </booktitle> <pages> pages 3-35. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1988. </year>
Reference-contexts: Clearly, we fall into the camp that advocates explicit communication. 7.5 Distributed Artificial Intelligence The Distributed Artificial Intelligence community has explored many of the same issues that are now currently being studied in the area of multiple robotics. Some survey articles are <ref> [ Bond and Gasser, 1988 ] </ref> , [ Durfee et al., 1992 ] , [ Decker, 1987 ] , [ Durfee et al., 1989 ] and [ Decker, 1987 ] .
Reference: [ Chapman and Kaelbling, 1991 ] <author> David Chapman and Leslie Pack Kaelbling. </author> <title> Input generalization in delayed reinforcement learning: an algorithm and performance comparisons. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 726-731, </pages> <year> 1991. </year>
Reference-contexts: Two of the papers that address the issue of input generalization in reinforcement learning are [ Mahadevan and Connell, 1991 ] and <ref> [ Chapman and Kaelbling, 1991 ] </ref> . [ Chapman and Kaelbling, 1991 ] developed a tree-structured reinforcement table to allow for input generalization At the outset, the algorithm assumes that all input space is identical. <p> Two of the papers that address the issue of input generalization in reinforcement learning are [ Mahadevan and Connell, 1991 ] and <ref> [ Chapman and Kaelbling, 1991 ] </ref> . [ Chapman and Kaelbling, 1991 ] developed a tree-structured reinforcement table to allow for input generalization At the outset, the algorithm assumes that all input space is identical. As the learner sees examples, it builds a binary tree using input 30 bits that are judged to be relevant.
Reference: [ Chomsky, 1986 ] <author> Noam Chomsky. </author> <title> Knowledge of language: its nature, origin, and use. </title> <type> Praeger, </type> <year> 1986. </year>
Reference-contexts: We introduce a simple, fixed position grammar that the robot uses to learn a 55 compositional ASRL. Note that the robot does not create this simple grammar; it is provided to the robot. While we need not look to humans for justification, Chom-sky <ref> [ Chomsky, 1986 ] </ref> argues persuasively that humans are born with an innate grammar; i.e. children do not need to learn most of the structure of a language before starting to learn the language. We discuss the grammar we implemented in the next section.
Reference: [ Decker, 1987 ] <author> Keith S. Decker. </author> <title> Distributed problem-solving techniques: a survey. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, SMC-17(5):729-740, September/October 1987. </journal> <volume> 84 </volume>
Reference-contexts: Some survey articles are [ Bond and Gasser, 1988 ] , [ Durfee et al., 1992 ] , <ref> [ Decker, 1987 ] </ref> , [ Durfee et al., 1989 ] and [ Decker, 1987 ] . In DAI, problems are solved by breaking them down into parts to be solved by 72 multiple agents, usually software agents. <p> Some survey articles are [ Bond and Gasser, 1988 ] , [ Durfee et al., 1992 ] , <ref> [ Decker, 1987 ] </ref> , [ Durfee et al., 1989 ] and [ Decker, 1987 ] . In DAI, problems are solved by breaking them down into parts to be solved by 72 multiple agents, usually software agents. Cooperation between agents is necessary since it is difficult to break down problems into independent portions.
Reference: [ Drescher, 1991 ] <author> Gary L. Drescher. </author> <title> Made-up Minds. </title> <publisher> The MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Both [ Drescher, 1993 ] and <ref> [ Drescher, 1991 ] </ref> address the development of primitives using the Schema mechanism which builds up representations of observations about the world. [ Pierce, 1991 ] and [ Pierce and 15 Kuipers, 1991 ] discuss how robots can learn to use their motors to move around the world.
Reference: [ Drescher, 1993 ] <author> Gary L. Drescher. </author> <title> The schema mechanism. </title> <editor> In S.J. Hanson, W. Rem-mele, and R.L. Rivest, editors, </editor> <title> Machine Learning: From Theory to Applications. (Cooperative research at Siemens and MIT). </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year> <note> Lecture Notes in Computer Science. </note>
Reference-contexts: Just as children seem to learn to interact with the world through actions before they start to talk about the world, a robotic system could be developed that learned how to act in the world and then start to learn to talk about what it has learned. Both <ref> [ Drescher, 1993 ] </ref> and [ Drescher, 1991 ] address the development of primitives using the Schema mechanism which builds up representations of observations about the world. [ Pierce, 1991 ] and [ Pierce and 15 Kuipers, 1991 ] discuss how robots can learn to use their motors to move around
Reference: [ Durfee et al., 1989 ] <author> Edmund H. Durfee, Victor R. Lesser, and Daniel D. Corkill. </author> <title> Cooperative distributed problem solving. </title> <editor> In Avron Barr, Paul R. Cohen, and Edward A. Feigenbaum, editors, </editor> <booktitle> The Handbook of Artificial Intelligence, volume VI, chapter 17, </booktitle> <pages> pages 83-147. </pages> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Some survey articles are [ Bond and Gasser, 1988 ] , [ Durfee et al., 1992 ] , [ Decker, 1987 ] , <ref> [ Durfee et al., 1989 ] </ref> and [ Decker, 1987 ] . In DAI, problems are solved by breaking them down into parts to be solved by 72 multiple agents, usually software agents. Cooperation between agents is necessary since it is difficult to break down problems into independent portions.
Reference: [ Durfee et al., 1992 ] <author> Edmund H. Durfee, Victor R. Lesser, and Daniel D. Corkill. </author> <title> Distributed problem solving. </title> <editor> In Stuart C. Shapiro, editor, </editor> <booktitle> Enclyclopedia of AI, </booktitle> <pages> pages 379-388. </pages> <publisher> John Wiley and Sons, Inc., </publisher> <address> second edition, </address> <year> 1992. </year>
Reference-contexts: Clearly, we fall into the camp that advocates explicit communication. 7.5 Distributed Artificial Intelligence The Distributed Artificial Intelligence community has explored many of the same issues that are now currently being studied in the area of multiple robotics. Some survey articles are [ Bond and Gasser, 1988 ] , <ref> [ Durfee et al., 1992 ] </ref> , [ Decker, 1987 ] , [ Durfee et al., 1989 ] and [ Decker, 1987 ] . In DAI, problems are solved by breaking them down into parts to be solved by 72 multiple agents, usually software agents.
Reference: [ Fukuda and Kawauchi, 1990 ] <author> T. Fukuda and Y. Kawauchi. </author> <title> Communication and distributed intelligence for cellular robotic system CEBOT. </title> <booktitle> In 1990 Japan-USA Symposium on Flexible Automation, </booktitle> <pages> pages 1085-1092, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: For other examples of systems that use provided languages, see <ref> [ Fukuda and Kawauchi, 1990 ] </ref> , [ Matsumoto et al., 1990 ] , [ Shin and Epstein, 1985 ] , [ Arkin et al., 1993 ] , [ Arkin and Hobbs, 1993 ] , and [ Wei, 1993 ] Other systems use no communication between the robots.
Reference: [ Huber and Durfee, 1993 ] <author> Marcus J. Huber and Edmund H. Durfee. </author> <title> Observational uncertainty in plan recognition among interacting robots. </title> <booktitle> In IJCAI [ IJCAI , 1993 ] , pages 68-75. </booktitle>
Reference-contexts: The robots broadcast their locations, but there is no discussion of what any robot should do next. Communication between agents can either be implicit or explicit. In implicit communication, an agent needs to deduce what is being communicated without the benefit of being told directly. <ref> [ Huber and Durfee, 1993 ] </ref> uses implicit communication in his plan recognition system. One robot observes another robot moving through the world and tries to determine what goal the other robot is trying to reach.
Reference: [ IJCAI , 1993 ] <institution> International Joint Conference on Artificial Intelligence Workshop on Dynamically Interacting Robots, </institution> <address> Chambery, France, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Only recently has there been a workshop solely on multiple robotic systems at one of the major artificial intelligence conferences <ref> [ IJCAI , 1993 ] </ref> . Many different issues have been explored, but here we will concentrate on communication issues. There are several approaches to communication in multi-agent robot systems. Some researchers believe that communication is not necessary. Other researchers rely on implicit communication.
Reference: [ Kaelbling, 1993 ] <author> Leslie Pack Kaelbling. </author> <title> Learning in Embedded Systems. </title> <publisher> The MIT Press: A Bradford Book, </publisher> <year> 1993. </year> <title> Revised version of Ph.D. </title> <type> Thesis, </type> <institution> Stanford University, </institution> <year> 1990. </year>
Reference-contexts: Our work in this thesis introduces a reinforcement method, task-based reinforcement, that does not need to assign credit to particular robots. For some overviews of reinforcement learning, see <ref> [ Kaelbling, 1993 ] </ref> , [ Watkins, 1989 ] , [ Sutton, 1992 ] , and [ Mataric, 1991 ] . 22 3.2 Kaelbling's Interval Estimation The reinforcement learning method used in this research is Kaelbling's interval estimation [ Kaelbling, 1993 ] . <p> For some overviews of reinforcement learning, see <ref> [ Kaelbling, 1993 ] </ref> , [ Watkins, 1989 ] , [ Sutton, 1992 ] , and [ Mataric, 1991 ] . 22 3.2 Kaelbling's Interval Estimation The reinforcement learning method used in this research is Kaelbling's interval estimation [ Kaelbling, 1993 ] .
Reference: [ Langton et al., 1991 ] <editor> C.G. Langton, C. Taylor, J.D. Farmer, and S. Rasmussen, editors. </editor> <booktitle> Artificial Life II, volume X of Santa Fe Institute Studies in the Sciences of 85 Complexity. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1991. </year> <booktitle> Proceedings of the 2nd interdisciplinary work-shop on synthesis and simulation of living systems held in February 1990 in Los Alamos, </booktitle> <address> NM. </address>
Reference-contexts: Artificial life is concerned with discovering how organisms can evolve and adapt in different environments. (For examples of work done in this field, see [ Langton, 1989 ] , <ref> [ Langton et al., 1991 ] </ref> , [ Langton, 1994 ] .) Creatures are simulated by a computer program and set loose to survive in a simulated world. Most creatures select from an action set based upon an evolved mechanism for choosing actions.
Reference: [ Langton, 1989 ] <editor> Christopher G. Langton, editor. </editor> <booktitle> Artificial Life, volume VI of Santa Fe Institute Studies in the Sciences of Complexity. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1989. </year> <booktitle> Proceedings of the interdisciplinary workshop on the synthesis and simulatino of living systems held in September 1987 in Los Alamos, </booktitle> <address> NM. </address>
Reference-contexts: Artificial life is concerned with discovering how organisms can evolve and adapt in different environments. (For examples of work done in this field, see <ref> [ Langton, 1989 ] </ref> , [ Langton et al., 1991 ] , [ Langton, 1994 ] .) Creatures are simulated by a computer program and set loose to survive in a simulated world. Most creatures select from an action set based upon an evolved mechanism for choosing actions.
Reference: [ Langton, 1994 ] <editor> Christopher G. Langton, editor. </editor> <booktitle> Artificial Life III. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Artificial life is concerned with discovering how organisms can evolve and adapt in different environments. (For examples of work done in this field, see [ Langton, 1989 ] , [ Langton et al., 1991 ] , <ref> [ Langton, 1994 ] </ref> .) Creatures are simulated by a computer program and set loose to survive in a simulated world. Most creatures select from an action set based upon an evolved mechanism for choosing actions.
Reference: [ MacLennan and Berghardt, 1993 ] <author> Bruce J. MacLennan and Gordon M. Berghardt. </author> <title> Synthetic ethology and the evolution of cooperative communication. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 2(2) </volume> <pages> 161-187, </pages> <year> 1993. </year>
Reference-contexts: The languages are slightly mutated at the time of splitting to introduce new languages into the pool. MacLennan describes a program of "synthetic ethology", which he defines as the creation of simulated worlds and organisms for the purpose of studying the development of behaviors <ref> [ MacLennan, 1991, MacLennan and Berghardt, 1993 ] </ref> . One of these behaviors is communication. He describes a world where each creature has a local environment that only it can sense.
Reference: [ MacLennan, 1991 ] <author> Bruce MacLennan. </author> <title> Synthetic ethology: an approach to the study of communication, pages 631-658. Volume X of Langton et al. </title> [ <editor> Langton et al., </editor> <booktitle> 1991 ] , 1991. Proceedings of the 2nd interdisciplinary workshop on synthesis and simulation of living systems held in February 1990 in Los Alamos, </booktitle> <address> NM. </address>
Reference-contexts: The languages are slightly mutated at the time of splitting to introduce new languages into the pool. MacLennan describes a program of "synthetic ethology", which he defines as the creation of simulated worlds and organisms for the purpose of studying the development of behaviors <ref> [ MacLennan, 1991, MacLennan and Berghardt, 1993 ] </ref> . One of these behaviors is communication. He describes a world where each creature has a local environment that only it can sense.
Reference: [ Mahadevan and Connell, 1991 ] <author> Sridhar Mahadevan and Jonathan Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <booktitle> In AAAI [ AAAI , 1991 ] , pages 768-773. </booktitle>
Reference-contexts: So, if there are irrelevant input bits, the algorithm will explore the extra space without making an attempt to judge whether or not it should be exploring in that area. Two of the papers that address the issue of input generalization in reinforcement learning are <ref> [ Mahadevan and Connell, 1991 ] </ref> and [ Chapman and Kaelbling, 1991 ] . [ Chapman and Kaelbling, 1991 ] developed a tree-structured reinforcement table to allow for input generalization At the outset, the algorithm assumes that all input space is identical. <p> As the learner sees examples, it builds a binary tree using input 30 bits that are judged to be relevant. In <ref> [ Mahadevan and Connell, 1991 ] </ref> , they show that learning individual behaviors results in better performance than "monolithic" learning. The advantage of learning individual behaviors comes from the ability to have separate reinforcement functions for each behavior.
Reference: [ Martin and Sargent, 1991 ] <author> Fred Martin and Randy Sargent. </author> <title> The MIT sensor robot: User's guide and technical reference. </title> <month> October </month> <year> 1991. </year>
Reference-contexts: A.2 Hardware Bert and Ernie, two of the robots used in this research, are Sensor Robots designed by Fred Martin at the Media Laboratory at the Massachusetts Institute of Technology <ref> [ Martin and Sargent, 1991 ] </ref> . We have six of the Sensor Robots, but have used only three at a time in this research due to hardware failures.
Reference: [ Mataric, 1991 ] <editor> Maja J. Mataric. </editor> <title> A comparative analysis of reinforcement learning methods. </title> <type> Memo 1322, </type> <institution> Massachusetts Institute of Technology Artificial Intelligence Laboratory, Cambridge, Massachusetts, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: Our work in this thesis introduces a reinforcement method, task-based reinforcement, that does not need to assign credit to particular robots. For some overviews of reinforcement learning, see [ Kaelbling, 1993 ] , [ Watkins, 1989 ] , [ Sutton, 1992 ] , and <ref> [ Mataric, 1991 ] </ref> . 22 3.2 Kaelbling's Interval Estimation The reinforcement learning method used in this research is Kaelbling's interval estimation [ Kaelbling, 1993 ] .
Reference: [ Mataric, 1993a ] <editor> Maja J. Mataric. </editor> <title> Designing emergent behaviors: local interactions to collective intelligence. </title> <editor> In Meyer et al. [ Meyer et al., </editor> <year> 1993 </year> <month> ] , pages 432-441. 86 </month>
Reference-contexts: For example, Mataric <ref> [ Mataric, 1993a, Mataric, 1994, Mataric, 1993b ] </ref> discusses the building up of large social behaviors through the use of low level primitives. There is no communication in this system. The robots broadcast their locations, but there is no discussion of what any robot should do next.
Reference: [ Mataric, 1993b ] <editor> Maja J. Mataric. </editor> <title> Kin recognition, similarity, and group behavior. </title> <booktitle> In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 705-710, </pages> <address> Boulder, Colorado, </address> <year> 1993. </year>
Reference-contexts: For example, Mataric <ref> [ Mataric, 1993a, Mataric, 1994, Mataric, 1993b ] </ref> discusses the building up of large social behaviors through the use of low level primitives. There is no communication in this system. The robots broadcast their locations, but there is no discussion of what any robot should do next.
Reference: [ Mataric, 1994 ] <editor> Maja J. Mataric. </editor> <title> Interaction and Intelligent Behavior. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: For example, Mataric <ref> [ Mataric, 1993a, Mataric, 1994, Mataric, 1993b ] </ref> discusses the building up of large social behaviors through the use of low level primitives. There is no communication in this system. The robots broadcast their locations, but there is no discussion of what any robot should do next.
Reference: [ Matsumoto et al., 1990 ] <author> A. Matsumoto, H. Asama, and Y. Ishida. </author> <title> Communication in the autonomous and decentralized robot system ACTRESS. </title> <booktitle> In Proceedings of the IEEE International Workshop on Intelligent Robots and Systems, </booktitle> <pages> pages 835-840, </pages> <address> Tsuchura, Japan, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: For other examples of systems that use provided languages, see [ Fukuda and Kawauchi, 1990 ] , <ref> [ Matsumoto et al., 1990 ] </ref> , [ Shin and Epstein, 1985 ] , [ Arkin et al., 1993 ] , [ Arkin and Hobbs, 1993 ] , and [ Wei, 1993 ] Other systems use no communication between the robots.
Reference: [ Meyer et al., 1993 ] <editor> Jean-Arcady Meyer, Herbert L. Roitblat, and Stweart W. Wil-son, editors. </editor> <booktitle> From Animals to Animats 2:Proceedings of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Honolulu, Hawaii, </address> <month> December </month> <year> 1993. </year>
Reference: [ ML, 1993 ] <editor> Machine Learning: </editor> <booktitle> Proceedings of the Tenth International Conference, </booktitle> <address> Amherst, Massachusetts, </address> <month> June </month> <year> 1993. </year>
Reference: [ Mullender, 1993 ] <author> Sape Mullender, </author> <title> editor. Distributed Systems. </title> <publisher> Addison-Wesley: ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: Since the communication is assumed to be perfect, we assume that the signal is received by the intended robotic audience. The issues we ignore by using assumptions 2 and 3 have been and are currently extensively researched in the Distributed Systems area (see, for example, <ref> [ Mullender, 1993 ] </ref> ). Assumption 4 We do not address plan recognition in which an agent attempts to determine the goals of another agent by watching the other agent.
Reference: [ Parker, 1993 ] <author> Lynne E. Parker. </author> <title> Adaptive action selection for cooperative agent teams. </title> <editor> In Meyer et al. [ Meyer et al., </editor> <year> 1993 </year> <month> ] , pages 442-450. </month>
Reference-contexts: To our knowledge, no other systems have explored the development of languages by robots. communicating. The work in this thesis has concentrated on the third option. Most work in the field concentrates on the first option. An example of a provided communication language is <ref> [ Parker, 1994, Parker, 1993 ] </ref> . This work investigates the use of heterogeneous multiple agents to complete tasks more effectively than a single super-robot. This system uses communication in the form of broadcast messages.
Reference: [ Parker, 1994 ] <author> Lynne E. Parker. </author> <title> Heterogeneous multi-robot cooperation. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: To our knowledge, no other systems have explored the development of languages by robots. communicating. The work in this thesis has concentrated on the third option. Most work in the field concentrates on the first option. An example of a provided communication language is <ref> [ Parker, 1994, Parker, 1993 ] </ref> . This work investigates the use of heterogeneous multiple agents to complete tasks more effectively than a single super-robot. This system uses communication in the form of broadcast messages. <p> language and give robots ability to adapt * Fast startup * Since language was developed by other robots, probably will not require much adaptation at the outset * Adaptable * Much less human involvement basis for communication. 71 however, the language is not required for the task to be completed. <ref> [ Parker, 1994 ] </ref> shows that even with the breakdown of communication, the tasks can be completed, but not as efficiently.
Reference: [ Pierce and Kuipers, 1991 ] <author> David Pierce and Benjamin Kuipers. </author> <title> Learning hill-climbing functions as a strategy for generating behaviors in a mobile robot. </title> <editor> In Jean-Arcady Meyer and Stewart W. Wilson, editors, </editor> <booktitle> From Animals to Ani-mats:Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 327-336, </pages> <address> Paris, France, </address> <month> September </month> <year> 1991. </year> <month> 87 </month>
Reference: [ Pierce, 1991 ] <author> David Pierce. </author> <title> Learning turn and travel actions with an uninterpreted sensiomotor apparatus. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 391-404. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1991. </year>
Reference-contexts: Both [ Drescher, 1993 ] and [ Drescher, 1991 ] address the development of primitives using the Schema mechanism which builds up representations of observations about the world. <ref> [ Pierce, 1991 ] </ref> and [ Pierce and 15 Kuipers, 1991 ] discuss how robots can learn to use their motors to move around the world. Assumption 7 All reinforcement is immediate. Together with Assumption 6, this means that reinforcement will always pertain to the most recent action taken.
Reference: [ Sargent and Martin, 1991 ] <author> Randy Sargent and Fred Martin. </author> <title> ic: Multi-tasking interactive C for the 6811. </title> <note> ic Version 2.5, </note> <month> October </month> <year> 1991. </year>
Reference-contexts: A 6v battery strapped to the underside of the chassis supplies the power for the robot. The robots are shown in figure A-1. The primary computational resource is an on-board Motorola 6811 microprocessor. The programming environment is ic, a multi-tasking interactive C compiler and interpreter developed by Randy Sargent <ref> [ Sargent and Martin, 1991 ] </ref> . ic allows a Sen 75 Figure A-1: Two of the Sensor Robots used: Bert and Ernie sor Robot to be addressed through a serial line from a host computer as well as the downloading of programs for autonomous activity.
Reference: [ Shavlik and Dietterich, 1990 ] <author> Jude W. Shavlik and Thomas G. Dietterich, </author> <title> editors. </title> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Positive reinforcement is a reward such as an ice cream cone. Negative reward is a penalty such as not being allowed to play Nintendo. 1 For an overview of machine learning, see <ref> [ Shavlik and Dietterich, 1990 ] </ref> . 20 and two outputs, "Spin" and "Straight." In reinforcement learning, the learning agents try to maximize future rewards based on experience.
Reference: [ Shewchuk, 1991 ] <author> John P. Shewchuk. </author> <type> Ph.D. thesis proposal. </type> <institution> Department of Computer Science, Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1991. </year>
Reference-contexts: The followers need to interpret the signal by determining the concept that maps to each 1 Our initial experiments in the development of the basic ASRL were inspired by the work of John Shewchuk <ref> [ Shewchuk, 1991 ] </ref> 33 signal. The learning problem is to match the signals to concepts. In our experiments, concepts map directly to actions due to Assumption 6 in Chapter 2. The ASRL evolves as the leader and followers are learning the task.
Reference: [ Shin and Epstein, 1985 ] <author> K. Shin and M. Epstein. </author> <title> Communication primitives for a distributed multi-robot system. </title> <booktitle> In Proceedings of the IEEE Robotics and Automation Conference, </booktitle> <pages> pages 910-917, </pages> <year> 1985. </year>
Reference-contexts: For other examples of systems that use provided languages, see [ Fukuda and Kawauchi, 1990 ] , [ Matsumoto et al., 1990 ] , <ref> [ Shin and Epstein, 1985 ] </ref> , [ Arkin et al., 1993 ] , [ Arkin and Hobbs, 1993 ] , and [ Wei, 1993 ] Other systems use no communication between the robots.
Reference: [ Sutton, 1984 ] <author> Richard S. Sutton. </author> <title> Temporal credit assignment in reinforcement learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <month> February </month> <year> 1984. </year>
Reference-contexts: Together with Assumption 6, this means that reinforcement will always pertain to the most recent action taken. This assumption simplifies the learning problem and allows us to focus on the development of ASRLs. It could be relaxed through the use of temporal differencing methods such as <ref> [ Sutton, 1984 ] </ref> . 2.2 Language development scenario The language development experiments were performed with the following experimental scenario, illustrated in figure 2-1. We have a group of robots that need to perform a task. <p> Delayed reinforcement requires the agent to assign credit to particular moves that it made during the course of the game. Temporal differencing (TD) is a reinforcement method which can handle this credit assignment problem. For examples of TD work, see <ref> [ Sutton, 1984 ] </ref> , [ Sutton, 1988 ] , [ Watkins, 1989 ] , [ Watkins and Dayan, 1992 ] , and [ Tesauro, 1992 ] . We assumed immediate reinforcement to avoid this problem of temporal credit assignment.
Reference: [ Sutton, 1988 ] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: Delayed reinforcement requires the agent to assign credit to particular moves that it made during the course of the game. Temporal differencing (TD) is a reinforcement method which can handle this credit assignment problem. For examples of TD work, see [ Sutton, 1984 ] , <ref> [ Sutton, 1988 ] </ref> , [ Watkins, 1989 ] , [ Watkins and Dayan, 1992 ] , and [ Tesauro, 1992 ] . We assumed immediate reinforcement to avoid this problem of temporal credit assignment.
Reference: [ Sutton, 1992 ] <author> Richard S. Sutton, </author> <title> editor. </title> <journal> Machine Learning: Special issue on reinforcement learning, </journal> <volume> volume 8 </volume> <pages> 3-4. </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Our work in this thesis introduces a reinforcement method, task-based reinforcement, that does not need to assign credit to particular robots. For some overviews of reinforcement learning, see [ Kaelbling, 1993 ] , [ Watkins, 1989 ] , <ref> [ Sutton, 1992 ] </ref> , and [ Mataric, 1991 ] . 22 3.2 Kaelbling's Interval Estimation The reinforcement learning method used in this research is Kaelbling's interval estimation [ Kaelbling, 1993 ] .
Reference: [ Tan, 1993 ] <author> Ming Tan. </author> <title> Multi-agent reinforcement learning: independent vs. </title> <booktitle> cooperative agents. In ML [ ML, </booktitle> <pages> 1993 ] , pages 330-337. </pages>
Reference-contexts: The issue of multiple agent reinforcement learning has also been addressed in <ref> [ Tan, 1993 ] </ref> . Tan defines three ways that agents can communicate to cooperate using reinforcement learning. The first way is by communicating instantaneous information, such as a sensation, an action taken or a reward received. The second way is by transmitting episodes.
Reference: [ Tesauro, 1992 ] <author> Gerald Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <booktitle> In Sutton [ Sutton, </booktitle> <pages> 1992 ] , pages 33-53. </pages>
Reference-contexts: Temporal differencing (TD) is a reinforcement method which can handle this credit assignment problem. For examples of TD work, see [ Sutton, 1984 ] , [ Sutton, 1988 ] , [ Watkins, 1989 ] , [ Watkins and Dayan, 1992 ] , and <ref> [ Tesauro, 1992 ] </ref> . We assumed immediate reinforcement to avoid this problem of temporal credit assignment. The robots are told if they have performed the task properly as soon as they try to perform it.
Reference: [ Watkins and Dayan, 1992 ] <author> Christopher J.C.H. Watkins and Peter Dayan. </author> <note> Q-learning (technical note). In Sutton [ Sutton, </note> <year> 1992 </year> <month> ] , pages 55-68. </month>
Reference-contexts: Temporal differencing (TD) is a reinforcement method which can handle this credit assignment problem. For examples of TD work, see [ Sutton, 1984 ] , [ Sutton, 1988 ] , [ Watkins, 1989 ] , <ref> [ Watkins and Dayan, 1992 ] </ref> , and [ Tesauro, 1992 ] . We assumed immediate reinforcement to avoid this problem of temporal credit assignment. The robots are told if they have performed the task properly as soon as they try to perform it.
Reference: [ Watkins, 1989 ] <author> Christopher J.C.H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <month> May </month> <year> 1989. </year> <month> 88 </month>
Reference-contexts: Temporal differencing (TD) is a reinforcement method which can handle this credit assignment problem. For examples of TD work, see [ Sutton, 1984 ] , [ Sutton, 1988 ] , <ref> [ Watkins, 1989 ] </ref> , [ Watkins and Dayan, 1992 ] , and [ Tesauro, 1992 ] . We assumed immediate reinforcement to avoid this problem of temporal credit assignment. The robots are told if they have performed the task properly as soon as they try to perform it. <p> Our work in this thesis introduces a reinforcement method, task-based reinforcement, that does not need to assign credit to particular robots. For some overviews of reinforcement learning, see [ Kaelbling, 1993 ] , <ref> [ Watkins, 1989 ] </ref> , [ Sutton, 1992 ] , and [ Mataric, 1991 ] . 22 3.2 Kaelbling's Interval Estimation The reinforcement learning method used in this research is Kaelbling's interval estimation [ Kaelbling, 1993 ] .
Reference: [ Wei, 1993 ] <author> Gerhard Wei. </author> <title> Action selections and learning in multi-agent environ-ments. </title> <editor> In Meyer et al. [ Meyer et al., </editor> <year> 1993 </year> <month> ] , pages 502-510. </month>
Reference-contexts: For other examples of systems that use provided languages, see [ Fukuda and Kawauchi, 1990 ] , [ Matsumoto et al., 1990 ] , [ Shin and Epstein, 1985 ] , [ Arkin et al., 1993 ] , [ Arkin and Hobbs, 1993 ] , and <ref> [ Wei, 1993 ] </ref> Other systems use no communication between the robots. For example, Mataric [ Mataric, 1993a, Mataric, 1994, Mataric, 1993b ] discusses the building up of large social behaviors through the use of low level primitives. There is no communication in this system.
Reference: [ Werner and Dyer, 1991 ] <author> Gregory M. Werner and Michael G. Dyer. </author> <booktitle> Evolution of communication in artificial organisms, </booktitle> <pages> pages 659-687. </pages> <editor> Volume X of Langton et al. [ Langton et al., </editor> <booktitle> 1991 ] , 1991. Proceedings of the 2nd interdisciplinary workshop on synthesis and simulation of living systems held in February 1990 in Los Alamos, </booktitle> <address> NM. </address>
Reference-contexts: There is no adaptation within the lifetime of creatures in <ref> [ Werner and Dyer, 1991 ] </ref> | language is genetically hard-coded. In this work, creatures must communicate in order to breed and carry on their genetic line.
Reference: [ Whitehead, 1991 ] <author> Steven D. Whitehead. </author> <title> A complexity analysis of cooperative mechanisms in reinforcement learning. </title> <booktitle> In AAAI [ AAAI , 1991 ] , pages 607-613. </booktitle>
Reference-contexts: However, the work in communication development in artificial systems is still relevant for robotics if we look at multiple generations as multiple learning steps within a lifetime. 7.3 Multi-agent reinforcement learning Whitehead <ref> [ Whitehead, 1991 ] </ref> provides an analysis of complexity in multi-agent reinforcement learning. The issue of multiple agent reinforcement learning has also been addressed in [ Tan, 1993 ] . Tan defines three ways that agents can communicate to cooperate using reinforcement learning.
Reference: [ Yanco and Stein, 1993 ] <author> Holly Yanco and Lynn Andrea Stein. </author> <title> An adaptable communication protocol for cooperating mobile robots. </title> <editor> In Meyer et al. [ Meyer et al., </editor> <year> 1993 </year> <month> ] , pages 478-485. 89 </month>
Reference-contexts: We call this simple language type the basic ASRL 1 . Portions of this chapter were joint work with Lynn Andrea Stein <ref> [ Yanco and Stein, 1993 ] </ref> . We gain simplicity at the expense of learning times. In later chapters, we will present two other ASRL types that are more complicated and require more structure to learn, but that converge more quickly than the basic ASRL.
References-found: 49

