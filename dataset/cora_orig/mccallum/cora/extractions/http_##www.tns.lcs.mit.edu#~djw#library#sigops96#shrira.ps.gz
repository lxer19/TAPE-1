URL: http://www.tns.lcs.mit.edu/~djw/library/sigops96/shrira.ps.gz
Refering-URL: http://www.tns.lcs.mit.edu/~djw/library/sigops96/index.html
Root-URL: 
Title: How to Scale Transactional Storage Systems  
Author: Liuba Shrira, Barbara Liskov, Miguel Castro and Atul Adya 
Date: July 15, 1996  
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science, MIT  
Abstract: Applications of the future will need to support large numbers of clients and will require scalable storage systems that allow state to be shared reliably. Recent research in distributed file systems provides technology that increases the scalability of storage systems. But file systems only support sharing with weak consistency guarantees and can not support applications that require transactional consistency. The challenge is how to provide scalable storage systems that support transactional applications. We are developing technology for scalable transactional storage systems. Our approach combines scalable caching and coherence techniques developed in serverless file systems and DSM systems, with recovery techniques developed in traditional databases. This position paper describes the design rationale for split caching, a new scalable memory management technique for network-based transactional object storage systems, and fragment reconstruction, a new coherence protocol that supports fine-grained sharing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Adya, R. Gruber, B. Liskov, and U. Maheshwari. </author> <title> Efficient Optimistic Concurrency Control Using Loosely Synchronized Clocks. </title> <booktitle> In Proceedings of ACM SIGMOD, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Split caching relies on a cache coherence protocol called fragment reconstruction that avoids the penalties of false sharing. The protocol works in the presence of concurrency control techniques that support fine-grained sharing (e.g., adaptive call-back locking [3] or the optimistic approach used in Thor <ref> [1] </ref>); such techniques are desirable because they avoid conflicts due to false sharing. In such systems, when a transaction commits new versions of some objects, this may cause pages in other client caches to become fragments, i.e., pages containing stale copies of the objects modified by those transactions. <p> The server responds with a copy of the requested object. Clients cache objects across transaction boundaries. Thor uses an optimistic concurrency control and cache consistency protocol based on loosely synchronized clocks and invalidation messages <ref> [1] </ref>.
Reference: [2] <author> B.Liskov, A.Adya, M.Castro, M.Day, S.Ghemawat, R.Gruber, U.Maheshwari, A.Myers, and L.Shrira. </author> <title> Safe and Efficient Sharing of Persistent Objects in Thor. </title> <booktitle> In Proceedings of ACM SIGMOD, </booktitle> <year> 1996. </year>
Reference-contexts: Therefore, we instead invalidate those stale copies, and use fragment reconstruction to bring those pages up to date using the fragment and the information in the mcache. Our work has been done in the context of Thor <ref> [2] </ref>. Thor supports object-based sharing and uses an mcache architecture to optimize updates. We describe a revised Thor design that incorporates split caching and fragment reconstruction. <p> The paper is organized as follows. Section 2 describes the Thor architecture. The new split caching architecture and page reconstruction protocol are described in Section 3. We conclude in Section 4. 2 Thor Architecture Our work is done in the context of the Thor object-oriented database system <ref> [2] </ref>. Thor has a distributed client/server architecture. Persistent objects are stored on disk at the servers. Application code runs at clients. Applications interact with Thor within atomic transactions that read and write persistent objects.
Reference: [3] <author> M. Carey, M. Franklin, and M. Zaharioudakis. </author> <title> Fine-Grained Sharing in a Page Server OODBMS. </title> <booktitle> In Proceedings of ACM SIGMOD, </booktitle> <year> 1994. </year>
Reference-contexts: Split caching avoids installation reads by fetching the containing pages from client caches. Split caching relies on a cache coherence protocol called fragment reconstruction that avoids the penalties of false sharing. The protocol works in the presence of concurrency control techniques that support fine-grained sharing (e.g., adaptive call-back locking <ref> [3] </ref> or the optimistic approach used in Thor [1]); such techniques are desirable because they avoid conflicts due to false sharing. <p> Simulation studies show that this scheme outperforms the best pessimistic scheme (adaptive callback locking <ref> [3] </ref>) on almost all workloads [9]. 2.2 Server Organization A server stores persistent objects and a stable transaction log on disk; it also has some volatile memory. The disk is organized as a collection of large pages that contain many objects. These pages are the unit of disk transer.
Reference: [4] <author> Michael J. Carey, Michael J. Franklin, Miron Livny, and Eugene J. Shekita. </author> <title> Data caching tradeoffs in client-server DBMS architectures. </title> <booktitle> In Proceedings of the ACM SIGMOD, </booktitle> <pages> pages 357-366, </pages> <year> 1991. </year>
Reference-contexts: Earlier research indicates that bringing such pages up to date by propagating the new object versions at commit time is not efficient <ref> [4] </ref>. Therefore, we instead invalidate those stale copies, and use fragment reconstruction to bring those pages up to date using the fragment and the information in the mcache. Our work has been done in the context of Thor [2].
Reference: [5] <author> M. Dahlin, R. Wang, T.Anderson, and D. Patterson. </author> <title> Cooperative Caching:Using Remote Client Memory to Improve File System Performance. </title> <booktitle> In Proceedings of OSDI, </booktitle> <year> 1994. </year>
Reference-contexts: Cooperative caching avoids disk access by taking advantage of emerging high speed local area networks to provide remote access to huge primary memories available in workstations. However, current cooperative caching techniques only work for file systems <ref> [5] </ref> and virtual memory systems [6], and do not provide support for transactions and fine-grained sharing (i.e., sharing of objects that are smaller than pages). <p> Cooperative caching addresses 1 this problem by letting clients fetch data from other client caches and by managing the aggregated client caches cooperatively. However, cooperative caching techniques <ref> [5] </ref> as used in file systems are not appropriate for transactional systems. In particular, caching modified data in client caches to optimize disk updates as in XFS [14] is not satisfactory since modified data is vulnerable to client machine crashes. <p> The aggregate memory at the clients will do at least as well as the server cache, and is likely to do better because it is much bigger than the server cache. This observation has motivated the work on cooperative caching in the xFS file system <ref> [5] </ref> and in traditional client/server databases [7]. 2. Work on the mcache has shown that for most workloads, performance improves as memory is shifted from the server cache to the mcache [8]. <p> The new architecture is based on the split caching global memory management scheme, and the transactional fragment reconstruction coherence protocol. Split caching is of interest because it provides the first global memory management architecture for distributed transactional object storage systems. Like cooperative caching in serverless file system <ref> [5] </ref>, split caching increases the scalability of the storage system by exploiting remote memory access. In addition, the decoupled optimization of disk reads and disk writes in split caching architecture also reflects transactional storage reliability requirements.
Reference: [6] <author> M. Feeley, W. Morgan, F. Pighin, A. Karlin, H. Levy, and C. Thekkath. </author> <title> Implementing global memory management in a workstation cluster. </title> <booktitle> In Proceedings of SOSP, </booktitle> <year> 1995. </year>
Reference-contexts: Cooperative caching avoids disk access by taking advantage of emerging high speed local area networks to provide remote access to huge primary memories available in workstations. However, current cooperative caching techniques only work for file systems [5] and virtual memory systems <ref> [6] </ref>, and do not provide support for transactions and fine-grained sharing (i.e., sharing of objects that are smaller than pages).
Reference: [7] <author> M. Franklin, M. Carey, and M. Livny. </author> <title> Global Memory Management in Client-Server DBMS Architectures. </title> <booktitle> In Proceedings of 18th VLDB Conf., </booktitle> <year> 1992. </year>
Reference-contexts: This observation has motivated the work on cooperative caching in the xFS file system [5] and in traditional client/server databases <ref> [7] </ref>. 2. Work on the mcache has shown that for most workloads, performance improves as memory is shifted from the server cache to the mcache [8].
Reference: [8] <author> S. Ghemawat. </author> <title> The Modified Object Buffer: A Storage Management Technique for Object-Oriented Databases. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1995. </year>
Reference-contexts: Servers cache only new committed versions of recently modified objects in a large object cache called the mcache. The mcache is recoverable and therefore modified objects survive server crashes. The mcache allows disk updates to be performed more efficiently <ref> [13, 8] </ref>. However, when objects need to be moved from the mcache to the database on disk, it is necessary to first perform an installation read [13], or iread , to obtain the containing pages, since the mcache contains the modified objects but not the containing pages. <p> As transactions commit, modifications are written to the log and also inserted in the mcache. To satisfy a fetch request, the server first checks the mcache, since it may contain the latest version of an object. The mcache-based server architecture improves the efficiency of disk updates for small objects <ref> [13, 8] </ref>. It avoids the cost of synchronous commit time installation reads that obtain pages from disk in order to install the modifications on their containing pages. Studies show that installation reads performed at commit time can reduce the scalability of the server significantly [13, 15]. <p> Performance studies show that for most workloads the mcache architecture outperforms other architectures <ref> [13, 8] </ref>, including conventional architectures in which the server stores modified pages, and the clients ship entire pages to the server at commit. 3 The Split Caching Architecture The new caching architecture is based on the Thor architecture described in the previous section, except that clients fetch and evict pages instead <p> This observation has motivated the work on cooperative caching in the xFS file system [5] and in traditional client/server databases [7]. 2. Work on the mcache has shown that for most workloads, performance improves as memory is shifted from the server cache to the mcache <ref> [8] </ref>. This result is consistent with the first observation: since the server page cache is not very effective, it makes sense to shift the memory to the mcache (where it can be used to delay installing changes in the database). 3.
Reference: [9] <author> R. Gruber. </author> <title> Optimism vs. Locking: A Study of Concurrency Control for Client-Server Object-Oriented Databases. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1996. </year>
Reference-contexts: Simulation studies show that this scheme outperforms the best pessimistic scheme (adaptive callback locking [3]) on almost all workloads <ref> [9] </ref>. 2.2 Server Organization A server stores persistent objects and a stable transaction log on disk; it also has some volatile memory. The disk is organized as a collection of large pages that contain many objects. These pages are the unit of disk transer.
Reference: [10] <author> M.Feeley, J.Chase, V.Narasayya, and H.Levy. </author> <title> Integrating Coherency and Recoverability in Distributed Systems. </title> <booktitle> In Proceedings of OSDI, </booktitle> <year> 1994. </year>
Reference-contexts: In addition, the decoupled optimization of disk reads and disk writes in split caching architecture also reflects transactional storage reliability requirements. The fragment reconstruction coherence protocol is attractive because, like the log-based coherency protocol of Feeley et.al, <ref> [10] </ref>, it avoids the penalties of false sharing in a transactional object storage system. However, the fragment reconstruction protocol supports storage systems with multiple servers and cooperative caching.
Reference: [11] <author> D. Muntz and P. Honeyman. </author> <title> Multi-level Caching in Distributed File Systems or Your Cache ain't nothin' but trash. </title> <booktitle> In Winter Usenix Technical Conference, </booktitle> <year> 1992. </year>
Reference-contexts: The new architecture is based on the following observations: 1. Earlier research has shown that when there is a large number of clients with caches, and when clients are sharing a large database whose size is much larger than the server memory, the server cache is relatively ineffective <ref> [11] </ref>. It is unlikely to satisfy client requests with pages fetched earlier for that client since they are already present in that client's cache.
Reference: [12] <author> J. O'Toole and L. Shrira. </author> <title> Shared data management needs adaptive methods. </title> <booktitle> In Proceedings of IEEE Workshop on Hot Topics in Operating Systems, </booktitle> <year> 1995. </year>
Reference-contexts: This optimizes server disk writes. Clients fetch and evict pages and these pages are used for fetches and installation reads. Like other cooperative caching schemes, split caching allows the clients to fetch pages from other client caches. In addition, like the hybrid caching scheme <ref> [12] </ref>, split caching allows the servers to avoid installation reads by taking advantage of pages in the client cache. In contrast to hybrid caching, split caching fetches pages at installation time, rather then commit time, which is more efficient when pages are updated repeatedly.
Reference: [13] <author> James O'Toole and Liuba Shrira. </author> <title> Opportunistic Log: Efficient Installation Reads in a Reliable Object Server. </title> <booktitle> In Proceedings of OSDI, </booktitle> <year> 1994. </year>
Reference-contexts: Servers cache only new committed versions of recently modified objects in a large object cache called the mcache. The mcache is recoverable and therefore modified objects survive server crashes. The mcache allows disk updates to be performed more efficiently <ref> [13, 8] </ref>. However, when objects need to be moved from the mcache to the database on disk, it is necessary to first perform an installation read [13], or iread , to obtain the containing pages, since the mcache contains the modified objects but not the containing pages. <p> The mcache is recoverable and therefore modified objects survive server crashes. The mcache allows disk updates to be performed more efficiently [13, 8]. However, when objects need to be moved from the mcache to the database on disk, it is necessary to first perform an installation read <ref> [13] </ref>, or iread , to obtain the containing pages, since the mcache contains the modified objects but not the containing pages. Installation reads form a significant part of the disk load in the mcache architecture. Split caching avoids installation reads by fetching the containing pages from client caches. <p> As transactions commit, modifications are written to the log and also inserted in the mcache. To satisfy a fetch request, the server first checks the mcache, since it may contain the latest version of an object. The mcache-based server architecture improves the efficiency of disk updates for small objects <ref> [13, 8] </ref>. It avoids the cost of synchronous commit time installation reads that obtain pages from disk in order to install the modifications on their containing pages. Studies show that installation reads performed at commit time can reduce the scalability of the server significantly [13, 15]. <p> It avoids the cost of synchronous commit time installation reads that obtain pages from disk in order to install the modifications on their containing pages. Studies show that installation reads performed at commit time can reduce the scalability of the server significantly <ref> [13, 15] </ref>. Instead, installation reads are performed asynchronously by a background thread that moves modified objects from the mcache to the disk using a read-modify-write cycle. First, the modified page is read from disk, if necessary. <p> Performance studies show that for most workloads the mcache architecture outperforms other architectures <ref> [13, 8] </ref>, including conventional architectures in which the server stores modified pages, and the clients ship entire pages to the server at commit. 3 The Split Caching Architecture The new caching architecture is based on the Thor architecture described in the previous section, except that clients fetch and evict pages instead
Reference: [14] <author> T.Anderson, M. Dahlin, J. Neefe, D. Patterson, and R. Wang. </author> <title> Serverless Network File Systems Performance. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(1), </volume> <year> 1996. </year>
Reference-contexts: Transaction commits are handled at servers (using a two-phase commit protocol if the transaction used objects from more than one server); when a transaction commits, its modifications are recorded persistently and become visible to other transactions. Note that the need to support transactions makes a "serverless" architecture <ref> [14] </ref> undesirable because the cost of distributed commit protocols increases with the number of participating servers. Traditionally, server caches are used both to avoid disk reads and optimize disk updates. <p> However, cooperative caching techniques [5] as used in file systems are not appropriate for transactional systems. In particular, caching modified data in client caches to optimize disk updates as in XFS <ref> [14] </ref> is not satisfactory since modified data is vulnerable to client machine crashes. Split caching decouples the optimization of disk reads and disk updates, providing a new way of structuring the cooperative caches in transactional storage systems. Clients fetch pages from other clients' caches to avoid disk reads.
Reference: [15] <author> Seth J. White and David J. DeWitt. </author> <title> Implementing Crash recovery in Quickstore: a performance study. </title> <booktitle> In Proceedings of ACM SIGMOD, </booktitle> <pages> pages 187-198, </pages> <year> 1995. </year>
Reference-contexts: It avoids the cost of synchronous commit time installation reads that obtain pages from disk in order to install the modifications on their containing pages. Studies show that installation reads performed at commit time can reduce the scalability of the server significantly <ref> [13, 15] </ref>. Instead, installation reads are performed asynchronously by a background thread that moves modified objects from the mcache to the disk using a read-modify-write cycle. First, the modified page is read from disk, if necessary.
References-found: 15

