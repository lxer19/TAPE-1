URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/MLjournal5.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Email: baveja@cs.colorado.edu  TOMMI JAAKKOLA tommi@cse.ucsc.edu  mlittman@cs.duke.edu  szepes@sol.cc.u-szeged.hu  
Title: Algorithms  
Author: SATINDER SINGH MICHAEL L. LITTMAN CSABA SZEPESV ARI 
Keyword: reinforcement-learning, on-policy, convergence, Markov decision processes  
Address: Boulder, CO 80309-0430  Santa Cruz, CA 95064  Durham, NC 27708-0129  Szeged 6720, Aradi vrt tere 1. Hungary  
Affiliation: Department of Computer Science University of Colorado  Computer Science Department University of California  Department of Computer Science Duke University  Bolyai Institute of Mathematics "Jozsef Attila" University of Szeged  
Note: Reinforcement-Learning  
Abstract: Convergence Results for Single-Step On-Policy Abstract. An important application of reinforcement learning (RL) is to finite-state control problems and one of the most difficult problems in learning for control is balancing the exploration/exploitation tradeoff. Existing theoretical results for RL give very little guidance on reasonable ways to perform exploration. In this paper, we examine the convergence of single-step on-policy RL algorithms for control. On-policy algorithms cannot separate exploration from learning and therefore must confront the exploration problem directly. We prove convergence results for several related on-policy algorithms with both decaying exploration and persistent exploration. We also provide examples of exploration strategies that can be followed during learning that result in convergence to both optimal values and optimal policies. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Andrew G. Barto, S. J. Bradtke, and Satinder Singh (1995). </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72(1) </volume> <pages> 81-138. </pages>
Reference: <author> Richard Bellman (1957). </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference: <author> Dimitri P. </author> <title> Bertsekas (1995). Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, Massachusetts. </address> <note> Volumes 1 and 2. </note>
Reference: <author> Justin A. Boyan and Andrew W. Moore.(1995). </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 369-376, </pages> <address> Cambridge, MA. </address> <publisher> The MIT Press. </publisher>
Reference: <author> Leo Breiman.(1992). </author> <title> Probability. </title> <institution> Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania. </institution>
Reference: <author> Peter Dayan (1992). </author> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8(3) </booktitle> <pages> 341-362. </pages>
Reference: <author> Peter Dayan and Terrence J. </author> <title> Sejnowski (1994). TD() converges with probability 1. </title> <journal> Machine Learning, </journal> <volume> 14(3). </volume>
Reference: <author> Peter Dayan and Terrence J. </author> <title> Sejnowski (1996). Exploration bonuses and dual control. </title> <journal> Machine Learning, </journal> <volume> 25 </volume> <pages> 5-22. </pages>
Reference: <author> Vijaykumar Gullapalli and Andrew G. </author> <title> Barto (1994). Convergence of indirect adaptive asynchronous value iteration algorithms. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 695-702, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Tommi Jaakkola, Michael I. Jordan, and Satinder Singh (1994). </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6(6) </volume> <pages> 1185-1201, </pages> <month> November. </month>
Reference: <author> George H. </author> <title> John (1994). When the best move isn't optimal: Q-learning with exploration. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> page 1464, </pages> <address> Seattle, WA. </address>
Reference: <author> George H. </author> <title> John (1995). When the best move isn't optimal: Q-learning with exploration. </title> <type> Unpublished manuscript, </type> <note> available through URL ftp://starry.stanford.edu/pub/gjohn/papers/rein-nips.ps. </note>
Reference: <author> Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. </author> <title> Moore (1996). Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285. </pages>
Reference: <author> P. R. Kumar and P. P. </author> <month> Varaiya </month> <year> (1986). </year> <title> Stochastic Systems: Estimation, Identification, and Adaptive Control. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Michael L. </author> <title> Littman and Csaba Szepesvari (1996). A generalized reinforcement-learning model: Convergence and applications. </title> <editor> In Lorenza Saitta, editor, </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 310-318. </pages>
Reference: <author> Michael Lederman Littman (1996). </author> <title> Algorithms for Sequential Decision Making. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University, </institution> <note> February. Also Technical Report CS-96-09. </note>
Reference: <author> Martin L. </author> <title> Puterman (1994). Markov Decision Processes|Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY. </address>
Reference: <author> G. A. </author> <month> Rummery </month> <year> (1994). </year> <title> Problem solving with reinforcement learning. </title> <type> PhD thesis, </type> <institution> Cambridge University Engineering Department. </institution>
Reference: <author> G. A. Rummery and M. </author> <title> Niranjan (1994). On-line Q-learning using connectionist systems. </title> <type> Technical Report CUED/F-INFENG/TR 166, </type> <institution> Cambridge University Engineering Department. </institution>
Reference: <author> Satinder P. Singh and Richard S. </author> <title> Sutton (1996). Reinforcement learning with replacing eligibility traces. </title> <journal> Machine Learning, </journal> 22(1/2/3):123-158. 
Reference: <author> Satinder Pal Singh and Richard C. </author> <title> Yee (1994). An upper bound on the loss from approximate optimal-value functions. Machine Learning, 16:227. CONVERGENCE OF ON-POLICY RL ALGORITHMS 17 Rich Sutton and Andy Barto (1997). An Introduction to Reinforcement Learning. </title> <publisher> The MIT Press, forthcoming. </publisher>
Reference: <author> Richard S. </author> <title> Sutton (1988). Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44. </pages>
Reference: <author> Richard S. </author> <title> Sutton (1996). Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <editor> In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <address> Cambridge, MA. </address> <publisher> The MIT Press. </publisher>
Reference: <author> Csaba Szepesvari and Michael L. </author> <title> Littman (1996). Generalized Markov decision processes: Dynamic-programming and reinforcement-learning algorithms. </title> <type> Technical Report CS-96-11, </type> <institution> Brown University, Providence, RI. </institution>
Reference: <author> Sebastian B. </author> <title> Thrun (1992). The role of exploration in learning control. </title> <editor> In David A. White and Donald A. Sofge, editors, </editor> <title> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY. </address>
Reference: <author> John N. </author> <title> Tsitsiklis (1994). Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16(3) </volume> <pages> 185-202, </pages> <month> September </month> <year> 1994. </year>
Reference: <author> John N. Tsitsiklis and Benjamin Van Roy (1996). </author> <title> An analysis of temporal-difference learning with function approximation. </title> <type> Technical Report LIDS-P-2322, </type> <institution> Massachusetts Institute of Technology, </institution> <month> March. </month> <note> Available through URL http://web.mit.edu/bvr/www/td.ps. To appear in IEEE Transactions on Automatic Control. </note>
Reference: <author> Christopher J. C. H. </author> <title> Watkins (1989). Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, UK. </address>
Reference: <author> Christopher J. C. H. Watkins and Peter Dayan (1992). </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292. </pages>
Reference: <author> Ronald J. Williams and Leemon C. Baird, </author> <title> III (1993). Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-14, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA, </address> <month> November. </month>
References-found: 30

