URL: http://www.cis.upenn.edu/~ungar/papers/yale.ps
Refering-URL: http://www.cis.upenn.edu/~ungar/papers.html
Root-URL: 
Title: Estimating Prediction Intervals for Artificial Neural Networks  
Author: Lyle H. Ungar Richard D. De Veaux Evelyn Rosengarten 
Affiliation: University of Pennsylvania Williams College University of Pennsylvania  
Abstract: Neural networks can be viewed as nonlinear models, where the weights are parameters to be estimated. In general two parameter estimation methods are used: nonlinear regression, corresponding to the standard backpropagation algorithm, and Bayesian estimation, in which the model parameters are considered as being random variables drawn from a prior distribution, which is updated based on the observed data. These two estimation methods suggest different methods of calculating prediction intervals for neural networks. We present some preliminary observations comparing the ability of the two methods to provide accurate prediction intervals. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baxt, W.G. and H. White, </author> <title> (1995) "Bootstrapping confidence intervals for clinical input variable effects in a network trained to identify the presence of acute myocardial infarction", </title> <booktitle> Neural Computation 7 624-638. </booktitle>
Reference-contexts: The Bayesian method has the further virtue that the regularization is explicit (which has been done for backpropagation networks as well) and that the degree of regularization can be estimated as part of the modeling procedure. Bootstrapping is another potential method of estimating prediction intervals <ref> (see e.g. Baxt and White, 1995) </ref>, which can be done with either model. It can be very effective when sufficient data and computa tional power are available.
Reference: <author> De Veaux, R.D., </author> <title> D.C. Psichogios and L.H. Ungar, (1993) "A Tale of Two Non-parametric Estimation Schemes: MARS and Neural Networks," </title> <booktitle> 4th Intl. Conf. on Artificial Intelligence and Statistics, </booktitle> <month> Jan. </month> <year> 1993. </year>
Reference: <author> Ding, A.A., and Hwang, J.T.G. </author> <title> (1995) "Construct prediction intervals using artificial neural networks," </title> <journal> Proceedings of the Section on Physical and Engineering Sciences, </journal> <pages> 73-78. </pages>
Reference-contexts: Thus, the networks can be thought of as doing nonlinear regression. Standard methods exist for estimating the prediction uncertainties of nonlinear regression (see e.g Seber and Wild, 1989), based on local linearizations of the model. These methods have been applied to neural nets <ref> (see e.g Ding and Hwang, 1995) </ref>, but although they work well on small problems, they are often less reliable on the larger problems typically addressed using neural networks. Recently (MacKay 1992, Neal 1994), a Bayesian approach has been proposed for estimating the parameters in neural nets.
Reference: <author> Donaldson, J.R., and Schnabel, R.B., </author> <title> (1987) "Computational experience with confidence regions and confidence intervals for nonlinear least squares", </title> <journal> Technometrics, </journal> <volume> 29 (1) 67-82. </volume>
Reference-contexts: A number of variants of the equation have been proposed, using different estimates of the covariance of the model parameters, but all give similar results <ref> (see e.g. Donaldson and Schnabel, 1987) </ref>. 3 Neural Nets as Bayesian Es timators Bayesians take a different view of the modeling and estimation problem: the parameters are drawn from a distribution. During the estimation phase, one uses observations z = fx; yg to update the (prior) distribution of .
Reference: <author> Friedman, J.H. </author> <title> (1991) "Multivariate adaptive regression splines," </title> <journal> Annals of Statistics, </journal> <volume> 19 (1) 1-141. </volume>
Reference: <author> Neal, </author> <title> R.M. (1994) "Bayesian Learning for Neural Networks," </title> <type> Ph.D. Thesis, </type> <institution> Dept. of Computer Science, University of Toronto, </institution> <note> Leonard J., </note> <author> Kramer, M., and Ungar, </author> <title> L.H. (1992) "Using radial basis functions to approximate a function and its error bounds." </title> <journal> IEEE Transactions on Neural Nets, </journal> <volume> 3(4), </volume> <pages> 624-627. </pages>
Reference-contexts: However, recently, a different approach to estimating the parameters in neural nets has been implemented <ref> (Neal, 1994) </ref>. Neal's method is complex and computationally demanding method, but seems to get excellent results. The key to his method lies in sampling from and averaging over a posterior distribution of models.
Reference: <author> MacKay, D.J.C., </author> <title> (1992) "A Practical Bayesian Framework for Backpropagation Networks." </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 448-472. </pages>
Reference-contexts: These methods have been applied to neural nets (see e.g Ding and Hwang, 1995), but although they work well on small problems, they are often less reliable on the larger problems typically addressed using neural networks. Recently <ref> (MacKay 1992, Neal 1994) </ref>, a Bayesian approach has been proposed for estimating the parameters in neural nets. The parameters in the neural network are considered as being drawn from a distribution (e.g. Gaussian) which is characterized by a set of hyperparameters (e.g. the mean and variance of the Gaussian). <p> One can do this in three ways: * Analytical, involving closed form expressions for the posterior, typically using conjugate families of densities * Monte Carlo methods - Gibbs sampling * Gaussian approximation The first implementations of Bayesian learning <ref> (MacKay, 1992) </ref> used a Gaussian approximation for the posterior distribution of the network parameters (weights) and single-valued estimates for the hyper-parameters (mean and variances). However, recently, a different approach to estimating the parameters in neural nets has been implemented (Neal, 1994).
Reference: <author> Seber, G.A.F. and C.J. Wild. </author> <title> Nonlinear regression. </title> <address> New York. </address> <publisher> Wiley, </publisher> <year> 1989. </year>
Reference-contexts: The parameters (network weights), are typically estimated by minimizing a residual sum of squares prediction error, using gradient descent or conjugate gradient methods. Thus, the networks can be thought of as doing nonlinear regression. Standard methods exist for estimating the prediction uncertainties of nonlinear regression <ref> (see e.g Seber and Wild, 1989) </ref>, based on local linearizations of the model. These methods have been applied to neural nets (see e.g Ding and Hwang, 1995), but although they work well on small problems, they are often less reliable on the larger problems typically addressed using neural networks.
Reference: <author> Weigend, A.S., M. Mangeas, </author> <title> and A.N. Srivastava, (1995) "Nonlinear gated experts for time series: discovering regimes and avoiding overfitting" International Journal of Neural Systems 6 373-399. </title>
Reference: <author> White, H., </author> <type> (1993) personal communication. </type>
Reference: <author> Address correspondence to Lyle H. </author> <type> Ungar, </type> <institution> 311A Towne Bldg., Department of Computer and Information Science, University of Pennsylvania, </institution> <address> Philadel-phia, PA 19104. (ungar@cis.upenn.edu) </address>
References-found: 11

