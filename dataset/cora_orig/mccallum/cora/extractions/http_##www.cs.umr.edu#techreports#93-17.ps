URL: http://www.cs.umr.edu/techreports/93-17.ps
Refering-URL: http://www.cs.umr.edu/techreports/
Root-URL: 
Phone: 2  
Title: Parallel Algorithm Fundamentals and Analysis  
Author: CSC - Bruce McMillin ? Hanan Lutfiyya ?? Grace Tsai Jun-Lin Liu 
Keyword: Key Words: Algorithm Design, Embeddings, Speedup, Class NC, Reasoning, Operational Evaluation  
Address: Rolla, MO 65401 USA  Ontario London, Ontario N6A 5B7 Canada  
Affiliation: 1 Department of Computer Science University of Missouri-Rolla  Department of Computer Science University of Western  
Abstract: This session explores, through the use of formal methods, the "intuition" used in creating a parallel algorithm design and realizing this design on distributed memory hardware. The algorithm class NC and the LSTM machine are used to show why some algorithms realize their promise of speedup better than others and the algorithm class NP is used to show why other algorithms will never be good for parallelization. The realities of algorithm design are presented through partitioning and mapping issues and models. Finally, correctness through cooperative axiomatic reasoning provides an additional basis for understanding parallel algorithm design and specification and is used for run-time assurance of distributed computing systems through operational evaluation. This paper appears, in its entirety, in the Proceedings of the International Summer Institute on Parallel Computer Architectures, Languages, and Algorithms, July 5-10, 1993, Prague, Czech Republic, IEEE Computer Society Press. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> G.M. </author> <title> Amdahl. Validity of the single processor approach to achieving large scale computing capabilities. </title> <booktitle> AFIPS Conference Proceedings, </booktitle> <volume> 30 </volume> <pages> 483-485, </pages> <year> 1967. </year>
Reference-contexts: Then the speedup is linear, as N grows, the speedup S = N . Between these two extremes, are two measures of what occurs when system bottlenecks, overhead, imperfect parallel decomposition occur. Amdahl's law <ref> [1] </ref> treats every program as consisting of a sequential component s and a parallel component p = 1 s. The crucial observation is that a program's speedup will be limited, severely, by the amount of non-parallelizable code.
Reference: 2. <author> R. Apt and W. Roever. </author> <title> A proof system for communicating sequential processes. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 2(3) </volume> <pages> 359-385, </pages> <year> 1981. </year>
Reference-contexts: The first is the sequential proofs of each individual process that makes assumptions about the effects of the communication commands. The second part is to ensure that the assumptions are "legitimate". This will be discussed later. This approach is taken in <ref> [2] </ref> and [21]. The second approach allows us to prove properties of the individual processes using the axioms and rules of inference applicable to the statements in the individual processes.
Reference: 3. <author> J. Backus. </author> <title> Can programming be liberated from the von Neumann style? a functional style and its algebra of programs. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 613-641, </pages> <year> 1979. </year>
Reference-contexts: Matrix multiplication is, fundamentally, a collection of inner products of the elements of the multiplier and multiplicand matrices. This is expressed below, in a version of matrix multiplication expressed in FP <ref> [3] </ref>.
Reference: 4. <author> W. Briggs. </author> <title> Multigrid Tutorial. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, Pennsylvania, </address> <year> 1987. </year>
Reference-contexts: Thus, solving the errors at a coarser level increases the speedup of the solution by damping out the errors faster, along with increasing convergence rate due to better guesses. Fig. 15. Error Frequency Reduction Using Multgrid As illustrated in Figure 16 and as described by <ref> [4] </ref>, there are many ways to implement the multi-grid idea. In the figure, level 0 represents the finest array of points, while level 3 is the coarsest. In the V-cycle, level 0 does a set number of iterations of equation 7, then passes its residuals to level 1.
Reference: 5. <author> M. Clint. </author> <title> Program proving: </title> <journal> coroutines,. Acta Informatica, </journal> <volume> 2 </volume> <pages> 50-63, </pages> <year> 1973. </year>
Reference-contexts: This is done with use of "dummy" or auxiliary variables that relate program variables of one process to program variables of another. The need for such variables has been independently recognized by many. The first reference that shows the usefulness of auxiliary variables is found in <ref> [5] </ref>. Overall Proof Approach . As discussed before a CSP program is made up of component sequential processes executing in parallel. In general, to prove properties about the program, first properties of each component process are derived in isolation.
Reference: 6. <author> G. Cybenko, D. W. Krumme, and K. N. Venkataraman. </author> <title> Fixed hypercube embedding. </title> <journal> Information Process ing Letters, </journal> <volume> 25 </volume> <pages> 35-39, </pages> <year> 1987. </year>
Reference-contexts: It has been known for a long time that the general graph embedding problem (i.e., subgraph iso-morphism problem) is NP-complete. It was shown that the embedding of general graphs into the binary hypercube is also NP-complete <ref> [6] </ref>. However, with rich interconnection structure the hypercube contains as a subgraph many the regular structures (i.e., rings, two-dimensional meshes, higher-dimensional meshes, and almost complete binary trees). Most of the mapping research in these years has dealt with effectively simulating these regular structures in the hypercubes, (for example, [36]).
Reference: 7. <author> E. Dijkstra. </author> <title> A Discipline of Programming. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1976. </year>
Reference-contexts: A sequential proof for it only proves facts about it running in isolation. With only one process running, communication commands deadlock. Thus, any predicate Q may be assumed to be true upon termination of a communication command because termination never occurs. The Law of the Excluded Miracle <ref> [7] </ref> states that the statement false should never be derived. This is the requirement to ensure a sound logic. The communication axiom does violate the Law of the Excluded Miracle.
Reference: 8. <author> E. Edmiston and R. A. Wagner. </author> <title> Parallelization of the dynamic programming algorithm for comparison of sequences. </title> <booktitle> In Proceedings of Int'l Conf. on Parallel Processing, </booktitle> <pages> pages 78-80, </pages> <year> 1987. </year>
Reference-contexts: A parallel version of the dynamic programming algorithm is quite straightforward to derive <ref> [8] </ref>.
Reference: 9. <author> M. Fischer. </author> <title> A theoretician's view of fault tolerant distributed computing. Fault-Tolerant Distributed Com puting, </title> <booktitle> Lecture Notes in Computer Science 448, </booktitle> <pages> pages 1-9, </pages> <year> 1990. </year>
Reference-contexts: Separating the problem from its solution is an important contribution of having a theoretical foundation in that it opens the door to alternative solutions <ref> [9] </ref>. There are numerous examples of the use of mathematical models in the computer science literature. One example from the study of network topology is being able to compute the information carrying capacity of a network.
Reference: 10. <author> G. C. Fox and W. Furmaski. </author> <title> Load balancing loosely synchronous problems with a neural network. </title> <type> Technical report, </type> <institution> California Institute of Technology, </institution> <address> Pasedena, CA, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: Classically <ref> [10] </ref>, the goal of load balancing, given a process/communication digraph G (P; C), where P is the set of processes and C is the set of directed arcs C (i; j), is to find a partition G = G 0 [ G 1 [ ::: [ G T 1 of G
Reference: 11. <author> W. M. Gentleman. </author> <title> Some complexity results for matrix computations on parallel computers. </title> <journal> Journal of the ACM, </journal> <volume> 25(1) </volume> <pages> 112-115, </pages> <month> January </month> <year> 1978. </year>
Reference-contexts: Optimal Matrix Multiplication (in the abstract sense) As another mesh problem, consider Gentleman's Algorithm <ref> [11] </ref> which is an explicit parallel solution using a 2D mesh of processors to multiply two matrices. Assume we have N 2 processors arranged in an N fi N mesh.
Reference: 12. <author> J. Gustafson. </author> <title> Reevaluating Amdahl's law. </title> <journal> Communications of the ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <year> 1988. </year>
Reference-contexts: Thus the maximum speedup is lim S lim 1 N or S 2 no matter how many processors are used! These results seem disappointing. However, <ref> [12] </ref> in 1988 observed that programs are made parallel, for the most part, as they are have run times which grow as the problem scales. <p> Thus, we can calculate a scaled speedup S s , as S s = s + p Experimental results using this speedup measure report scaled speedups of 1020 on a 1024 processor machine <ref> [12] </ref>. There is still much debate, however, on the usefulness of this model. B.
Reference: 13. <author> C. Hoare. </author> <title> An axiomatic basis for computer programming. </title> <journal> Communications of the ACM, </journal> <volume> 12(10) </volume> <pages> 576-583, </pages> <year> 1969. </year> <editor> B. McMillin et al: </editor> <title> Parallel Algorithm Fundamentals and Analysis 39 </title>
Reference-contexts: The interpretation of the theorem is as follows: if P is true before the execution of S and if the execution of S terminates, then Q is true after the execution of S. P is said to be the precondition and Q the postcondition <ref> [13] </ref>. A statement, S, is partially correct with respect to the precondition P and a postcondition Q, if, whenever, P is true of S prior to execution, and if S terminates then Q is true of S after the execution of S terminates. <p> The following are common to all the axiomatic systems and apply to reasoning about sequential programs. The basis of the axiomatic approach to sequential programming can be found in <ref> [13] </ref>. The skip axiom is simple, since execution of the skip statement has no effect on any program or auxiliary variables. &lt; P &gt; skip &lt; P &gt; The axiom states that anything about the program and logical variables that holds before executing skip also holds after it has terminated.
Reference: 14. <author> C. Hoare. </author> <title> Communicating sequential processes. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 666-677, </pages> <year> 1978. </year>
Reference-contexts: Given that the only control we have in parallel programming, at the system level, is process creation and send/receive communication, all examples can be constructed using this primitive set of operations. Later we will present a more formal model of this in Hoare's CSP <ref> [14] </ref>. 1.2 PARALLEL SORTING Consider the problem of sorting an array a into ascending order using the a (very simple) Sequential Sorting Algorithm (Exchange Sort). <p> What is necessary is a way of classifying algorithms by their parallel complexity. The class N C is one such class. To explore the class N C, we need to first examine the fundamental nature of parallel processes. 2.3 CSP Hoare's model of concurrent programming, Communicating Sequential Processes (CSP) <ref> [14] </ref>, is a model reflecting properties that should be in all concurrent programming languages. It was not intended to be used as a programming language per se, but it does reflect Hoare's concerns of proving the correctness of programs. <p> Hoare has suggested the following three properties that every concurrent language should have: the ability to express parallelism, communication primitives and non-determinism. This section provides an informal brief description of the syntax and meaning of CSP commands. Full details of CSP are contained in <ref> [14] </ref>. Communicating Sequential Processes (CSP) was proposed as a preliminary solution to the problem of defining a synchronous message-based language. A CSP program consists of a static collection of processes. <p> Thus, P will hold if the repetition terminates. The repetition ends when no boolean guard is true, so :b 1 ^ :b 2 ^ ::: ^ :b n will also hold at that time. [21] does not have distributed termination which is contrary to Hoare's original version of CSP <ref> [14] </ref>. Distributed termination provides the means for automatic termination of a loop in one process because another process has terminated. It is assumed that termination of all loops occurs when all boolean guards are false. 30 ISIPCALA'93 Example 5.
Reference: 15. <author> J. Hong. </author> <title> Computation, Computability, Similarity and Duality. </title> <publisher> Pittman, </publisher> <address> London, </address> <year> 1986. </year>
Reference-contexts: If the Turing Machine is the abstract computational model for a sequential program, what is the corresponding model for a concurrent program and how does this model relate to the sequential Turing Machine model? From <ref> [15] </ref>, the fundamental measures of complexity are parallel time, space, and sequential time. If we have an abstract model which provides these three measures, then we can succinctly define speedup and characterize classes of algorithms which are amenable to parallelism.
Reference: 16. <author> K Huang and J. Abraham. </author> <title> Fault-tolerant algorithms and their applications to solving Laplace equations. </title> <booktitle> Proceedings of the 1984 International Conference on Parallel Processing, </booktitle> <pages> pages 117-122, </pages> <month> August, </month> <year> 1984. </year>
Reference-contexts: Recovery and reconfiguration are different issues. Work in concurrent detection methods includes self-checking software [38] and recovery blocks [31], which instrument the software with assertions on the program's state, watchdog processor [28], which monitors intermediate data of a computation, and algorithm-based fault tolerance <ref> [16] </ref> which imposes an additional structure on the data to detect errors. These methods define structure for fault tolerance, but do not, generally, give a methodology for instantiating the structure.
Reference: 17. <author> K. Hwang and Briggs F. </author> <title> Computer Architecture and Parallel Processing. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: If we assume that we can decompose the job into N parts, then speedup is just how much faster the decomposed job runs on N processors. Speedup measures address both the optimal and expected performance. Minsky's conjecture <ref> [17] </ref> forms a lower bound on what we can reasonably expect from a parallel program. The key observation is that as N grows, the performance becomes dominated by system bottlenecks and communication. Thus, perhaps the best speedup, S is O (log 2 N ). <p> Techniques such as simulated annealing [23] provide good results, but are computationally complex. Parallel computing can help speed their evaluation. 3 Interconnection Networks and Embeddings In the presentation so far, we have assumed that all processors are connected to each other (a completely connected network). The crossbar switch <ref> [17] </ref> attempts to connect each processor to each other processor. However, the number of switch elements grows as the square of the number of processors, making this technology infeasible for large multicomputer networks. The bus interconnection [17], by contrast, is inexpensive, but exhibits a performance bottleneck as interprocessor communication grows. <p> The crossbar switch <ref> [17] </ref> attempts to connect each processor to each other processor. However, the number of switch elements grows as the square of the number of processors, making this technology infeasible for large multicomputer networks. The bus interconnection [17], by contrast, is inexpensive, but exhibits a performance bottleneck as interprocessor communication grows. Multistage interconnection networks attempt to minimize the cost of interconnecting processors by providing a subset of possible interconnection patterns between the processors, at any one time. Examples of multistage interconnection networks are shown in Figure 6.
Reference: 18. <institution> IBM. IBM Scalable PowerParallel System 9076-SP1, </institution> <year> 1993. </year>
Reference-contexts: The multistage interconnect is the basis for many commercial and research parallel processors such as PASM [34] and the IBM RS/6000-based POWERparallel 1 (SP1) System <ref> [18] </ref>. However, if we examine the examples of Section 1 the communication patterns between processors are all nearest neighbor. Indeed, the most natural parallel algorithms result from domain decomposition into spatially local communication patterns such as mesh, ring, or tree.
Reference: 19. <author> E. Lander and J. P. Mesirov. </author> <title> Protein sequence comparison on a data parallel computer. </title> <booktitle> In Proceeding of the International Conf. on Parallel Processing, </booktitle> <pages> pages 257-263, </pages> <year> 1988. </year>
Reference-contexts: This dynamic programming algorithm can best be understood by considering the matrix C r;s = max &gt; &gt; &lt; 0 C r1;s + g where the gap constant g &lt; 0, and D is a correlation function between single elements <ref> [19] </ref>. A parallel version of the dynamic programming algorithm is quite straightforward to derive [8].
Reference: 20. <author> J Laprie and B. Littlewood. </author> <title> Probabilistic assessment of safety-critical software: </title> <booktitle> Why and how? Communi cations of the ACM, </booktitle> <volume> 35(2) </volume> <pages> 13-21, </pages> <year> 1992. </year>
Reference-contexts: It is also assumed that fl ij ffi ij is invariantly true throughout program execution. 6 Operational Evaluation It is important for both life-critical, and non-life-critical distributed systems to meet their specification at run time <ref> [20] </ref>. Large, complex, distributed systems, are subject to individual component failures which can cause system failure. Fault tolerance is an important technique to improve system reliability. The fault detection aspect identifies individual faulty components (processors) before they can affect, negatively, overall system reliability.
Reference: 21. <author> G.M. Levin and D. Gries. </author> <title> A proof technique for communicating sequential processes. </title> <journal> Acta Informatica, </journal> <volume> 15 </volume> <pages> 281-302, </pages> <year> 1981. </year>
Reference-contexts: The first is the sequential proofs of each individual process that makes assumptions about the effects of the communication commands. The second part is to ensure that the assumptions are "legitimate". This will be discussed later. This approach is taken in [2] and <ref> [21] </ref>. The second approach allows us to prove properties of the individual processes using the axioms and rules of inference applicable to the statements in the individual processes. <p> No system is more powerful than the other. However, there are very different approaches to thinking about the verification of the program and the applicability in a practical environment. The proof system presented in <ref> [21] </ref> is presented here for its relative ease of use. Axioms and Inference Rules Used For Sequential Reasoning . In addition to the axioms and inference rules of predicate logic, there is one axiom or inference rule for each type of statement, as well as some statement-independent inference rules. <p> Thus, P will hold if the repetition terminates. The repetition ends when no boolean guard is true, so :b 1 ^ :b 2 ^ ::: ^ :b n will also hold at that time. <ref> [21] </ref> does not have distributed termination which is contrary to Hoare's original version of CSP [14]. Distributed termination provides the means for automatic termination of a loop in one process because another process has terminated. <p> Otherwise, this unrestricted use of auxiliary variables would destroy the soundness of the proof system. Hence, auxiliary variables are not necessary to the computation, but they are necessary for verification. The proof system in <ref> [21] </ref> allows for auxiliary variables to be global i.e. variables that can be shared between distinct processes. Global auxiliary variables (GAVs) are used to record part of the history of the communication sequence. Shared reference to auxiliary variables allow for assertions relating the different communication sequences.
Reference: 22. <author> H. Lutfiyya and B. McMillin. </author> <title> Comparison of three axiomatic proof systems. </title> <institution> UMR Department of Computer Science Technical Report CSC91-13, </institution> <year> 1991. </year>
Reference-contexts: These properties are then used to prove properties of the entire program. This is the approach of [35]. It has been shown <ref> [22] </ref> that it is irrelevant as to which axiomatic proof systems of program verification is chosen. This was done by showing that the axiomatic systems are equivalent in the sense that they allow us to prove the same properties. No system is more powerful than the other.
Reference: 23. <author> H. Lutfiyya, B. McMillin, P. Poshyanonda, and C. Dagli. </author> <title> Composite stock cutting through simulated an nealing. </title> <journal> Journal of Mathematical and Computer Modeling, </journal> <volume> 16(1) </volume> <pages> 57-74, </pages> <year> 1992. </year>
Reference-contexts: However, parallel computers are useful in evaluating expensive hueristics for approximation to the solution of N P-Complete problems. Techniques such as simulated annealing <ref> [23] </ref> provide good results, but are computationally complex. Parallel computing can help speed their evaluation. 3 Interconnection Networks and Embeddings In the presentation so far, we have assumed that all processors are connected to each other (a completely connected network).
Reference: 24. <author> H. Lutfiyya, B. McMillin, and Alan Su. </author> <title> Formal derivation of an error-detecting distributed data scheduler using CHANGELING. </title> <booktitle> In Formal Methods in Programming, </booktitle> <address> Novosibirisk, Russia, </address> <month> July </month> <year> 1993. </year> <note> Also as UMR Department of Computer Science Technical Report Number CSC 92-14. </note>
Reference-contexts: Likewise, removing some of the assertions may result in certain GAVs no longer being required. Furthermore, certain assertions may be too expensive to evaluate in the operational environment and may be deleted for that reason. We applied this transformation to several concurrent applications including concurrent database transactions schedules <ref> [24] </ref>, bitonic sorting [25], and concurrent branch and bound [27] and obtained performance and error coverage data on each. 7 Summary This paper has covered a broad expanse of topics in an effort to provide both an informal basis for constructing parallel applications and a formal basis for reasoning about these
Reference: 25. <author> H. Lutfiyya, M. Schollmeyer, and B. McMillin. </author> <title> Fault-tolerant distributed sort generated from a verification proof outline. </title> <editor> In H. Kopetz and Y. Kakuda, editors, </editor> <booktitle> Responsive Computer Systems Dependable Computing and Fault-Tolerance, </booktitle> <volume> volume 7. </volume> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <note> Also as a Short Talk in the 14th ICSE, </note> <institution> Melbourne, Australia and UMR Department of Computer Science Technical Report C.Sc. </institution> <month> 91-12. </month>
Reference-contexts: Furthermore, certain assertions may be too expensive to evaluate in the operational environment and may be deleted for that reason. We applied this transformation to several concurrent applications including concurrent database transactions schedules [24], bitonic sorting <ref> [25] </ref>, and concurrent branch and bound [27] and obtained performance and error coverage data on each. 7 Summary This paper has covered a broad expanse of topics in an effort to provide both an informal basis for constructing parallel applications and a formal basis for reasoning about these parallel applications and
Reference: 26. <author> H. Lutfiyya, M. Schollmeyer, and B. McMillin. </author> <title> Formal generation of executable assertions for application oriented fault tolerance. </title> <institution> UMR Department of Computer Science Technical Report Number CSC 92-15, </institution> <year> 1992. </year>
Reference-contexts: Non-interference and the rule of satisfaction can be used to show that the soundness and completeness properties of the original GAA system will hold in the new HAA proof system. Theorem 28. <ref> [26] </ref> The history of auxiliary variables approach (HAA) retains the properties of the global auxiliary variables approach (GAA). Reliable Communication of State Information The HAA proof system provides for direct transformation of assertions from the verification environment into executable assertions for the non-faulty distributed operational environment.
Reference: 27. <author> H. Lutfiyya, A. Sun, and B. McMillin. </author> <title> A fault tolerant branch and bound algorithm derived from program verification. </title> <journal> IEEE Computers Software and Applications Conference(COMPSAC), </journal> <pages> pages 182-187, </pages> <year> 1992. </year>
Reference-contexts: Furthermore, certain assertions may be too expensive to evaluate in the operational environment and may be deleted for that reason. We applied this transformation to several concurrent applications including concurrent database transactions schedules [24], bitonic sorting [25], and concurrent branch and bound <ref> [27] </ref> and obtained performance and error coverage data on each. 7 Summary This paper has covered a broad expanse of topics in an effort to provide both an informal basis for constructing parallel applications and a formal basis for reasoning about these parallel applications and how they are mapped onto a
Reference: 28. <author> A. Mahmood, E. McCluskey, and D. Lu. </author> <title> Concurrent fault detection using a watchdog processor and asser tions. </title> <booktitle> IEEE 1983 International Test Conference, </booktitle> <pages> pages 622-628, </pages> <year> 1983. </year>
Reference-contexts: This paper focuses on detecting the occurrence of errors. Recovery and reconfiguration are different issues. Work in concurrent detection methods includes self-checking software [38] and recovery blocks [31], which instrument the software with assertions on the program's state, watchdog processor <ref> [28] </ref>, which monitors intermediate data of a computation, and algorithm-based fault tolerance [16] which imposes an additional structure on the data to detect errors. These methods define structure for fault tolerance, but do not, generally, give a methodology for instantiating the structure.
Reference: 29. <author> B. McMillin and L. Ni. </author> <title> Reliable distributed sorting through the application-oriented fault tolerance paradigm. </title> <journal> IEEE Trans. of Parallel and Distributed Computing, </journal> <volume> 3(4) </volume> <pages> 411-420, </pages> <year> 1992. </year>
Reference-contexts: These methods define structure for fault tolerance, but do not, generally, give a methodology for instantiating the structure. Application-oriented fault tolerance <ref> [29] </ref>, by contrast, provides a heuristic approach, based on the "Natural Constraints," to choosing executable assertions from the software specification. These executable assertions [38], in the form of source language statements, are inserted into a program for monitoring the run-time execution behavior of the program.
Reference: 30. <author> S. Owicki and D. Gries. </author> <title> An axiomatic proof technique for parallel programs I. </title> <journal> Acta Informatica, </journal> <volume> 6 </volume> <pages> 319-340, </pages> <year> 1976. </year>
Reference-contexts: This necessitates the need for a Proof of Non-interference. This consists of showing that for each assertion T in process i , it must be shown that T is invariant over any parallel execution. This is the non-interference property of <ref> [30] </ref>. 32 ISIPCALA'93 Asynchronous Message Passing Systems The proof systems that have been discussed up to this point are designed for synchronous programming primitives. Our work uses an extension of work discussed in [33].
Reference: 31. <author> B. Randall. </author> <title> System structure for software fault tolerance. </title> <journal> IEEE Transactions of Software Engineering, </journal> <volume> SE-1(2):220-232, </volume> <year> 1975. </year>
Reference-contexts: Once the faults are identified, reconfiguration and recovery [37] are used to deal with the fault. This paper focuses on detecting the occurrence of errors. Recovery and reconfiguration are different issues. Work in concurrent detection methods includes self-checking software [38] and recovery blocks <ref> [31] </ref>, which instrument the software with assertions on the program's state, watchdog processor [28], which monitors intermediate data of a computation, and algorithm-based fault tolerance [16] which imposes an additional structure on the data to detect errors.
Reference: 32. <author> D. Riggins, B. McMillin, M. Underwood, L. Reeves, and E. Lu. </author> <title> Modeling of supersonic combustor flows using parallel computing. </title> <booktitle> Computer Systems in Engineering, </booktitle> <volume> 3 </volume> <pages> 217-219, </pages> <year> 1992. </year>
Reference-contexts: Consider the mode fluids problem <ref> [32] </ref> of cavity-driven flow whose physical domain chosen is shown in Figure 10. The pair of non-linear coupled differential equations 1,2 that describe this flow are easily solved sequentially using a standard second-order central differencing scheme.
Reference: 33. <author> R. Schlichting and F. Schneider. </author> <title> Using message passing for distributed programming: Proof rules and disci plines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(3) </volume> <pages> 402-431, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: This is the non-interference property of [30]. 32 ISIPCALA'93 Asynchronous Message Passing Systems The proof systems that have been discussed up to this point are designed for synchronous programming primitives. Our work uses an extension of work discussed in <ref> [33] </ref>. The work of [33] describes how to extend the notion of a "satisfaction proof" and "non-interference proof" for asynchronous message-passing primitives. <p> This is the non-interference property of [30]. 32 ISIPCALA'93 Asynchronous Message Passing Systems The proof systems that have been discussed up to this point are designed for synchronous programming primitives. Our work uses an extension of work discussed in <ref> [33] </ref>. The work of [33] describes how to extend the notion of a "satisfaction proof" and "non-interference proof" for asynchronous message-passing primitives.
Reference: 34. <editor> H.J. Siegel et al. PASM: </editor> <title> A partionable SIMD/MIMD system for image processing and pattern recognition. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30:934-947, </volume> <month> December </month> <year> 1981. </year>
Reference-contexts: Thus, each processor can communicate with each other processor using n hops in the switch, however, as mentioned above, only a subset of simultaneous connections are possible. The multistage interconnect is the basis for many commercial and research parallel processors such as PASM <ref> [34] </ref> and the IBM RS/6000-based POWERparallel 1 (SP1) System [18]. However, if we examine the examples of Section 1 the communication patterns between processors are all nearest neighbor. Indeed, the most natural parallel algorithms result from domain decomposition into spatially local communication patterns such as mesh, ring, or tree.
Reference: 35. <author> N. Soundararahan. </author> <title> Axiomatic semantics of communicating sequential processes. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(6) </volume> <pages> 647-662, </pages> <year> 1984. </year>
Reference-contexts: These properties are then used to prove properties of the entire program. This is the approach of <ref> [35] </ref>. It has been shown [22] that it is irrelevant as to which axiomatic proof systems of program verification is chosen. This was done by showing that the axiomatic systems are equivalent in the sense that they allow us to prove the same properties. <p> This is formalized in the following definitions. Definition 23. For a process i , h i denotes the sequence of all communications that process i has so far participated in as the receiving process. Thus, h i is a list consisting of tuples (these are different from the <ref> [35] </ref> tuples; all future reference to tuples will refer to the following tuples) representing matching communication pairs of the form [; (V ar; V al); T; C] where is a process from which i receives from, Var is the variable that is transmitting to i with formal parameter Val.
Reference: 36. <editor> Q. F. Stout. Hypercubes and pyramids. In V. Cantoni and S. Levialdi, editors, </editor> <booktitle> Pyramidal Systems for Computer Vision. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: However, with rich interconnection structure the hypercube contains as a subgraph many the regular structures (i.e., rings, two-dimensional meshes, higher-dimensional meshes, and almost complete binary trees). Most of the mapping research in these years has dealt with effectively simulating these regular structures in the hypercubes, (for example, <ref> [36] </ref>). Let f be an embedding function which maps a guest graph G into a host graph H. jV G j denotes the 14 ISIPCALA'93 Fig. 7. Some Interconnection Topologies cardinality of the set V G . Terminology related to the mapping problem are formally defined as follows. Definition 10.
Reference: 37. <author> R. Yanney and J. Hayes. </author> <title> Distributed recovery in fault tolerance multiprocessor networks. </title> <booktitle> 4th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 514-525, </pages> <year> 1984. </year>
Reference-contexts: Early attempts at improving system reliability used fault-masking methods; these methods make the hardware tolerant of faults through the multiplicity of processing resources. In contrast, concurrent fault detection methods attempt to locate component errors which can lead to system failure. Once the faults are identified, reconfiguration and recovery <ref> [37] </ref> are used to deal with the fault. This paper focuses on detecting the occurrence of errors. Recovery and reconfiguration are different issues.
Reference: 38. <author> S. Yau and R. Cheung. </author> <title> Design of self-checking software. </title> <booktitle> Proc. Int'l Conf. on Reliability Software, </booktitle> <pages> pages 450-457, </pages> <month> April </month> <year> 1975. </year>
Reference-contexts: Once the faults are identified, reconfiguration and recovery [37] are used to deal with the fault. This paper focuses on detecting the occurrence of errors. Recovery and reconfiguration are different issues. Work in concurrent detection methods includes self-checking software <ref> [38] </ref> and recovery blocks [31], which instrument the software with assertions on the program's state, watchdog processor [28], which monitors intermediate data of a computation, and algorithm-based fault tolerance [16] which imposes an additional structure on the data to detect errors. <p> These methods define structure for fault tolerance, but do not, generally, give a methodology for instantiating the structure. Application-oriented fault tolerance [29], by contrast, provides a heuristic approach, based on the "Natural Constraints," to choosing executable assertions from the software specification. These executable assertions <ref> [38] </ref>, in the form of source language statements, are inserted into a program for monitoring the run-time execution behavior of the program.
References-found: 38

