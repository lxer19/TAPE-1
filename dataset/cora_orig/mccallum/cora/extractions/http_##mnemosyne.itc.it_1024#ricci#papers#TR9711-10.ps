URL: http://mnemosyne.itc.it:1024/ricci/papers/TR9711-10.ps
Refering-URL: http://mnemosyne.itc.it:1024/ricci/tech-reports-list.html
Root-URL: 
Email: Email: ricci@irst.itc.it, aha@aic.nrl.navy.mil  
Phone: Phone: ++39 461 314334 FAX: ++39 461 302040  
Title: Bias, Variance, and Error Correcting Output Codes for Local Learners  
Author: Francesco Ricci David W. Aha 
Keyword: Case-based learning, classification, error-correcting output codes,  
Note: bias and variance  
Address: 38050 Povo (TN), Italy  Code 5510 Washington, DC 20375 USA  
Affiliation: Istituto per la Ricerca Scientifica e Tecnologica  Navy Center for Applied Research in Artificial Intelligence Naval Research Laboratory,  
Abstract: This paper focuses on a bias variance decomposition analysis of a local learning algorithm, the nearest neighbor classifier, that has been extended with error correcting output codes. This extended algorithm often considerably reduces the 0-1 (i.e., classification) error in comparison with nearest neighbor (Ricci & Aha, 1997). The analysis presented here reveals that this performance improvement is obtained by drastically reducing bias at the cost of increasing variance. We also show that, even in classification problems with few classes (m5), extending the codeword length beyond the limit that assures column separation yields an error reduction. This error reduction is not only in the variance, which is due to the voting mechanism used for error-correcting output codes, but also in the bias. 
Abstract-found: 1
Intro-found: 1
Reference: [ Aha and Bankert, 1997 ] <author> D. W. Aha and R. L. Bankert. </author> <title> Cloud classification using error-correcting output codes. </title> <booktitle> Artificial Intelligence Applications: Natural Science, Agriculture, and Environmental Science, </booktitle> <volume> 11 </volume> <pages> 13-28, </pages> <year> 1997. </year>
Reference-contexts: of the error for archival data sets is difficult to estimate because one often does not know the Bayes optimal 4 Increasing codeword length is an example of increasing the emphasis on "voting" [ Breiman, 1996a ] , which has already been used within ECOC classifiers with promis ing results <ref> [ Kong and Dietterich, 1995; Aha and Bankert, 1997 ] </ref> . 5 Only one set of codewords was used for each codeword length. Fig. 1. Bias and Error of IB1 ecoc when Varying Codeword Length for Clouds204-500 error rate.
Reference: [ Aha, 1992 ] <author> D. W. Aha. </author> <title> Tolerating noisy, irrelevant, and novel attributes in instance-based learning algorithms. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 36 </volume> <pages> 267-287, </pages> <year> 1992. </year>
Reference-contexts: More information on these data sets, which have only numeric-valued features, can be found in [ Ricci and Aha, 1997 ] . 4.1 Archived Data Sets The results of the experiments conducted on archived data are shown in Table 3. IB1 is an implementation of the nearest neighbor classifier <ref> [ Aha, 1992 ] </ref> ) and is used as a baseline.
Reference: [ Atkeson et al., 1997 ] <author> C. Atkeson, A. Moore, and S. Schaal. </author> <title> Locally weighted learning. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11 </volume> <pages> 11-73, </pages> <year> 1997. </year>
Reference-contexts: In fact, research in regression theory has shown that variance can be decreased by combining the influences of instances nearby a query [ Geman et al., 1992 ] . We therefore replaced IB1 with a distance-weighted k-nearest neighbor classifier <ref> [ Atkeson et al., 1997 ] </ref> . Applied to the LED display task, both this algorithm and its ECOC variant yields variance and bias equal to zero.
Reference: [ Bottou and Vapnik, 1992 ] <author> Leon Bottou and Vladimir Vapnik. </author> <title> Local learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 888-900, </pages> <year> 1992. </year>
Reference: [ Breiman et al., 1984 ] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: This data set is similar to the "ringnorm" data set described in [ Breiman, 1996a ] . Its Bayes optimal error rate is 0.161. Wave This data set is similar to that used in CART <ref> [ Breiman et al., 1984 ] </ref> . It has 22 features, ten classes and 300 instances. The instances belonging to class k have the 2k-th feature equal to t and the two features 2k 1 and 2k + 1 equal to t=2. All the other features are 0. <p> All the other features are 0. A random noise is added to each feature with 0 mean and 1 standard deviation. We set t = 3; the Bayes optimal error rate is 0.047. LED This is the classic LED display dataset used in CART <ref> [ Breiman et al., 1984 ] </ref> . There are seven boolean features and ten classes. The Bayes optimal error rate is 0.274. Table 4.
Reference: [ Breiman, 1996a ] <author> L. Breiman. </author> <title> Bias, variance, and arcing classifiers. </title> <type> Technical Report 460, </type> <institution> University of California, Berkeley, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: synthesized data sets, while in Section 5 we discuss ways in which variance can be reduced and our approach can be applied to tasks with few features. 2 The Bias and Variance Decomposition This section reviews the bias/variance decomposition of the error of a classifier, following the definitions given in <ref> [ Breiman, 1996a ] </ref> and [ Friedman, 1996 ] . <p> Other classifiers skip this step and immediately build ^ Y (xjT ). For both cases an aggregate classifier Y A (x) can be defined as <ref> [ Breiman, 1996a ] </ref> : Y A (x) = arg max ^ f i (x) (5) When the approximations ^ f i (x) or ^ f i (xjT ) differ from f i (x) or f i (xjT ) this yields a different behavior from the Bayes optimal classifier and a <p> Let U be the set of instances at which ^ Y is unbiased and B its complement. At this point we can define the bias and variance of zero-one loss functions as follows <ref> [ Breiman, 1996a ] </ref> : Bias ( ^ Y ) = P ( ^ Y (x) 6= Y (x); x 2 B) P (Y B (x) 6= Y (x); x 2 B) (8) V ar ( ^ Y ) = P ( ^ Y (x) 6= Y (x); x 2 U <p> by cross validating the choice of codewords. 4.2 Synthetic Classification Tasks As stated earlier, the bias/variance decomposition of the error for archival data sets is difficult to estimate because one often does not know the Bayes optimal 4 Increasing codeword length is an example of increasing the emphasis on "voting" <ref> [ Breiman, 1996a ] </ref> , which has already been used within ECOC classifiers with promis ing results [ Kong and Dietterich, 1995; Aha and Bankert, 1997 ] . 5 Only one set of codewords was used for each codeword length. Fig. 1. <p> Class i is multivariate normal with mean zero and covariance matrix 2 fl i 1 times the identity, i = 1; : : :; N . This data set is similar to the "ringnorm" data set described in <ref> [ Breiman, 1996a ] </ref> . Its Bayes optimal error rate is 0.161. Wave This data set is similar to that used in CART [ Breiman et al., 1984 ] . It has 22 features, ten classes and 300 instances. <p> However, this technique has limitations, and we now describe and report additional investigations for two of them. 5.1 Reducing variance Kong and Dietterich [1995] showed that the variance error of C4.5 ecoc can be reduced by "voting" multiple repetitions of the same classifier built with bootstrapped training sets <ref> [ Breiman, 1996a ] </ref> . In our case (i.e., with a local classifier like IB1 ecoc ), there is also another way: "smoothing" the local classifier.
Reference: [ Breiman, 1996b ] <author> Leo Breiman. </author> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24(2) </volume> <pages> 123-140, </pages> <year> 1996. </year>
Reference-contexts: First, assuming that the Bayes optimal error rate to be zero means that we overestimate both variance and bias. This follows from Equation 10 and because all the addenda are positive. Moreover, the nearest neighbor classifier is known to have high bias <ref> [ Breiman, 1996b ] </ref> , as the decomposition of the error on artificial data sets shows (see Table 4).
Reference: [ Dietterich and Bakiri, 1991 ] <author> T. G. Dietterich and G. Bakiri. </author> <title> Error-correcting output codes: A general method for improving multiclass inductive learning programs. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 572-577, </pages> <address> Anaheim, CA, 1991. </address> <publisher> AAAI Press. </publisher>
Reference: [ Dietterich and Bakiri, 1995 ] <author> T. G. Dietterich and G. Bakiri. </author> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 263-286, </pages> <year> 1995. </year>
Reference-contexts: Types Name Atomic Distributed One-Per-Class ECOC Earthling 1 1000 1111111 Martian 2 0100 0000111 Venusian 3 0010 0011001 Italian 4 0001 0101010 3 Error Correcting Output Codes Error correcting output codes (ECOC) is a multiclass classification technique where each class is encoded as a string of codeletters called a codeword <ref> [ Dietterich and Bakiri, 1995 ] </ref> . Given a test instance, each of its codeletters is predicted, and the class whose codeword has smallest Hamming distance to the predicted codeword is assigned. <p> Only on CLOUDS98 did IB1 ecoc yield higher errors than IB1 opc , because the class encoding space defined by only four classes is too small to generate sufficiently distinctive codewords. 3 3 The exhaustive codes technique <ref> [ Dietterich and Bakiri, 1995 ] </ref> , used in this case, gen erates only seven bit functions. Table 3. <p> Apparently this technique cannot be used for data sets with a small number of classes. The maximal number of codewords that are both column separated and row separated is 2 k1 1, where k is the number of classes <ref> [ Dietterich and Bakiri, 1995 ] </ref> . We already noted that for data sets like CLOUDS98, which has four classes, only seven different bit functions can be created. But each bit function is learned with a randomized feature selection algorithm.
Reference: [ Friedman, 1996 ] <author> J. H. Friedman. </author> <title> On bias, variance, 0/1 loss, and the curse-of-dimensionality. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: Section 5 we discuss ways in which variance can be reduced and our approach can be applied to tasks with few features. 2 The Bias and Variance Decomposition This section reviews the bias/variance decomposition of the error of a classifier, following the definitions given in [ Breiman, 1996a ] and <ref> [ Friedman, 1996 ] </ref> . In a classification problem one assumes that there exist two random variables, X and Y , where X describes the input parameters (i.e., instances) and Y is a discrete variable with a finite number of values, Y 2 f1; : : :; kg, called classes. <p> The goal is to produce a classifier ^ Y 2 f1; : : : ; kg that minimizes the misclassification error (risk) E [r (X)] where r (x) = i=1 and where 1 () is a function that takes the value 1 if the argument is true and 0 otherwise <ref> [ Friedman, 1996 ] </ref> .
Reference: [ Geman et al., 1992 ] <author> S. Geman, A. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: In our case (i.e., with a local classifier like IB1 ecoc ), there is also another way: "smoothing" the local classifier. In fact, research in regression theory has shown that variance can be decreased by combining the influences of instances nearby a query <ref> [ Geman et al., 1992 ] </ref> . We therefore replaced IB1 with a distance-weighted k-nearest neighbor classifier [ Atkeson et al., 1997 ] . Applied to the LED display task, both this algorithm and its ECOC variant yields variance and bias equal to zero.
Reference: [ Kohavi and Wolpert, 1996 ] <author> R. Kohavi and D. H. Wolpert. </author> <title> Bias plus variance decomposition for zero-one loss functions. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 275-283, </pages> <address> Bari, Italy, 1996. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: [ Kong and Dietterich, 1995 ] <author> E. B. Kong and T. G. Dietterich. </author> <title> Error-correcting output coding corrects bias and variance. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 313-321, </pages> <address> Tahoe City, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In contrast, local algorithms will not benefit from ECOCs because, independent of each output bit's class partition, they will generate classification predictions based on the same (local) information in the training set, and this causes the output bit predictions to be correlated <ref> [ Kong and Dietterich, 1995; Wettschereck and Dietterich, 1992 ] </ref> . <p> Ricci and Aha [1997] extended these results to the local learning algorithm IB1, an implementation of the nearest neighbor classifier. ECOCs can work only if the bit functions have different bias errors <ref> [ Kong and Dietterich, 1995 ] </ref> . We [ Ricci and Aha, 1997 ] showed that the bias errors made by local learners on different bit functions can be decorrelated by selecting different features for each function. <p> of the error for archival data sets is difficult to estimate because one often does not know the Bayes optimal 4 Increasing codeword length is an example of increasing the emphasis on "voting" [ Breiman, 1996a ] , which has already been used within ECOC classifiers with promis ing results <ref> [ Kong and Dietterich, 1995; Aha and Bankert, 1997 ] </ref> . 5 Only one set of codewords was used for each codeword length. Fig. 1. Bias and Error of IB1 ecoc when Varying Codeword Length for Clouds204-500 error rate.
Reference: [ Maron and Moore, 1997 ] <author> O. Maron and A. W. Moore. </author> <title> The racing algorithm: model selection for lazy learners. </title> <journal> Artificial Intelligence Review, </journal> <pages> pages 193-225, </pages> <year> 1997. </year>
Reference-contexts: We [ Ricci and Aha, 1997 ] showed that the bias errors made by local learners on different bit functions can be decorrelated by selecting different features for each function. To perform feature selection, we modified the schemata racing algorithm <ref> [ Maron and Moore, 1997; Ricci and Aha, 1997 ] </ref> .
Reference: [ Quinlan, 1993 ] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: They found that ECOCs significantly increased the classification accuracy of two learning algorithms for several multiclass tasks (i.e., with k &gt; 2 classes). Subsequently, Kong and Dietterich [1995] analyzed these algorithms and reported that ECOCs reduce both bias and variance. The algorithms investigated in these studies were C4.5 <ref> [ Quinlan, 1993 ] </ref> and error backpropagation (BP) [ Rumelhart and McClelland, 1986 ] . These are examples of global learning algorithms in that they generate classification predictions by examining the mappings of instances to classes in the entire training set.
Reference: [ Ricci and Aha, 1997 ] <author> F. Ricci and D. W. Aha. </author> <title> Extending local learners with error-correcting output codes. </title> <type> Technical Report AIC-97-001, </type> <institution> Naval Research Laboratory, </institution> <note> Navy Center for Applied Research in Artificial Intelligence, </note> <month> January </month> <year> 1997. </year>
Reference-contexts: In <ref> [ Ricci and Aha, 1997 ] </ref> , we confirmed this analysis and introduced a method to overcome this problem; use a feature selection approach, independently for each output bit, to ensure that different local information is used to generate classification predictions for different output bits. <p> Given a test instance, each of its codeletters is predicted, and the class whose codeword has smallest Hamming distance to the predicted codeword is assigned. We will now detail this technique and its integration with a nearest neighbor classifier <ref> [ Ricci and Aha, 1997 ] </ref> . Let k be the number of classes and l an integer. <p> Ricci and Aha [1997] extended these results to the local learning algorithm IB1, an implementation of the nearest neighbor classifier. ECOCs can work only if the bit functions have different bias errors [ Kong and Dietterich, 1995 ] . We <ref> [ Ricci and Aha, 1997 ] </ref> showed that the bias errors made by local learners on different bit functions can be decorrelated by selecting different features for each function. To perform feature selection, we modified the schemata racing algorithm [ Maron and Moore, 1997; Ricci and Aha, 1997 ] . <p> We [ Ricci and Aha, 1997 ] showed that the bias errors made by local learners on different bit functions can be decorrelated by selecting different features for each function. To perform feature selection, we modified the schemata racing algorithm <ref> [ Maron and Moore, 1997; Ricci and Aha, 1997 ] </ref> . <p> The bias set B and its complement U can be obtained based on these estimates. A summary description of the archived data sets is shown in Table 2. More information on these data sets, which have only numeric-valued features, can be found in <ref> [ Ricci and Aha, 1997 ] </ref> . 4.1 Archived Data Sets The results of the experiments conducted on archived data are shown in Table 3. IB1 is an implementation of the nearest neighbor classifier [ Aha, 1992 ] ) and is used as a baseline. <p> The other algorithms (i.e., IB1 atomic (standard single-codeletter encoding), IB1 opc (one-per-class encoding), and IB1 ecoc (error correcting encoding)) differ from IB1 in that they all employ a feature selection component (i.e., a distinct feature selection task for each output bit <ref> [ Ricci and Aha, 1997 ] </ref> ) and a distinct class encoding. The Bayes optimal error rate is not reported here because it is assumed to be zero, as explained above. In each these data sets IB1 ecoc recorded smaller errors than IB1, IB1 atomic , and IB1 opc .
Reference: [ Rumelhart and McClelland, 1986 ] <editor> D. E. Rumelhart and J. L. McClelland, editors. </editor> <booktitle> Parallel Distributed Processing: Exploration in the Miscrostructure of Cognition. </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Subsequently, Kong and Dietterich [1995] analyzed these algorithms and reported that ECOCs reduce both bias and variance. The algorithms investigated in these studies were C4.5 [ Quinlan, 1993 ] and error backpropagation (BP) <ref> [ Rumelhart and McClelland, 1986 ] </ref> . These are examples of global learning algorithms in that they generate classification predictions by examining the mappings of instances to classes in the entire training set.
Reference: [ Wettschereck and Dietterich, 1992 ] <author> D. Wettschereck and T. G. Dietterich. </author> <title> Improving the performance of radial basis function networks by learning center locations. </title> <editor> In J. Moody, S. Hanson, and R. Lippman, editors, </editor> <booktitle> Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: In contrast, local algorithms will not benefit from ECOCs because, independent of each output bit's class partition, they will generate classification predictions based on the same (local) information in the training set, and this causes the output bit predictions to be correlated <ref> [ Kong and Dietterich, 1995; Wettschereck and Dietterich, 1992 ] </ref> .
References-found: 18

