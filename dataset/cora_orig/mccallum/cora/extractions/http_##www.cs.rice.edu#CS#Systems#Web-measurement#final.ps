URL: http://www.cs.rice.edu/CS/Systems/Web-measurement/final.ps
Refering-URL: http://www.cs.rice.edu/CS/Systems/Web-measurement/index.html
Root-URL: 
Title: Measuring the Capacity of a Web Server  
Author: Gaurav Banga and Peter Druschel 
Address: Houston, TX 77005  
Affiliation: Department of Computer Science Rice University  
Abstract: The widespread use of the World Wide Web and related applications places interesting performance demands on network servers. The ability to measure the effect of these demands is important for tuning and optimizing the various software components that make up a Web server. To measure these effects, it is necessary to generate realistic HTTP client requests. Unfortunately, accurate generation of such traffic in a testbed of limited scope is not trivial. In particular, the commonly used approach is unable to generate client request-rates that exceed the capacity of the server being tested even for short periods of time. This paper examines pitfalls that one encounters when measuring Web server capacity using a synthetic workload. We propose and evaluate a new method for Web traffic generation that can generate bursty traffic, with peak loads that exceed the capacity of the server. Finally, we use the proposed method to measure the performance of a Web server. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Almeida, V. Almeida, and D. Yates. </author> <title> Measuring the Behavior of a World-Wide Web Server. </title> <type> Technical Report TR-96-025, </type> <institution> Boston University, CS Dept., </institution> <address> Boston MA, </address> <year> 1996. </year>
Reference-contexts: However, this approach suffers from the experimental difficulties involved in non-intrusive measurement of a live system and the inherent irreproducibility of live workloads. Recently, there has been some effort towards Web server evaluation through generation of synthetic HTTP client traffic, based on invariants observed in real Web traffic <ref> [26, 28, 29, 30, 1] </ref>. Unfortunately, there are pitfalls that arise in generating heavy and realistic Web traffic using a limited number of client machines. These problems can lead to significant deviation of benchmarking conditions from reality and fail to predict the performance of a given Web server.
Reference: [2] <author> Apache. </author> <note> http://www.apache.org/. </note>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching [5, 6, 7, 23, 31], HTTP protocol enhancements [4, 20, 25, 18], better HTTP servers and proxies <ref> [2, 33, 7] </ref> and server OS implementations [16, 17, 10, 24].
Reference: [3] <author> M. F. Arlitt and C. L. Williamson. </author> <title> Web Server Workload Characterization: The Search for Invariants. </title> <booktitle> In Proceedings of the ACM SIGMETRICS '96 Conference, </booktitle> <address> Philadelphia, PA, </address> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: terms of request file types, transfer sizes, locality of reference in URLs requested and other fl This paper will appear in the Proceedings of the USENIX Symposium on Internet Technologies and Systems, Monterey, Dec 1997. y This work was supported in part by National Science Foundation Grant CCR-9503098 related statistics <ref> [3, 5, 6, 8, 9, 12] </ref>. Some researchers have tried to evaluate the performance of Web servers and proxies using real workloads directly [13, 15]. However, this approach suffers from the experimental difficulties involved in non-intrusive measurement of a live system and the inherent irreproducibility of live workloads. <p> This is not possible using a simple scheme for request generation. Moreover, we have shown that the effect of such burstiness on server performance is significant. 6 Related Work There is much existing work towards characterizing the invariants in WWW traffic. Most recently, Arlitt and Williamson <ref> [3] </ref> characterized several aspects of Web server workloads such as request file type distribution, transfer sizes, locality of reference in the requested URLs and related statistics. Crovella and Bestavros [8] looked at Self-Similarity in WWW traffic.
Reference: [4] <author> G. Banga, F. Douglis, and M. Rabinovich. </author> <title> Optimistic Deltas for WWW Latency Reduction. </title> <booktitle> In Proceedings of the 1997 Usenix Technical Conference, </booktitle> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching [5, 6, 7, 23, 31], HTTP protocol enhancements <ref> [4, 20, 25, 18] </ref>, better HTTP servers and proxies [2, 33, 7] and server OS implementations [16, 17, 10, 24].
Reference: [5] <author> A. Bestavros, R. Carter, M. Crovella, C. Cunha, A. Heddaya, and S. Mirdad. </author> <title> Application-Level Document Caching in the Internet. </title> <type> Technical Report TR-95-002, </type> <institution> Boston University, CS Dept., </institution> <address> Boston MA, </address> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching <ref> [5, 6, 7, 23, 31] </ref>, HTTP protocol enhancements [4, 20, 25, 18], better HTTP servers and proxies [2, 33, 7] and server OS implementations [16, 17, 10, 24]. <p> terms of request file types, transfer sizes, locality of reference in URLs requested and other fl This paper will appear in the Proceedings of the USENIX Symposium on Internet Technologies and Systems, Monterey, Dec 1997. y This work was supported in part by National Science Foundation Grant CCR-9503098 related statistics <ref> [3, 5, 6, 8, 9, 12] </ref>. Some researchers have tried to evaluate the performance of Web servers and proxies using real workloads directly [13, 15]. However, this approach suffers from the experimental difficulties involved in non-intrusive measurement of a live system and the inherent irreproducibility of live workloads. <p> Our work is intended to complement the existing work done on Web workload characterization <ref> [5, 6, 7, 23, 31] </ref>, and can easily be used in conjunction with it. 4.1 Basic Architecture The basic architecture of our testbed is shown in Figure 3. A set of P client machines are connected to the server machine being tested.
Reference: [6] <author> H. Braun and K. Claffy. </author> <title> Web Traffic Characterization: An Assessment of the Impact of Caching Documents from NCSA's Web Server. </title> <booktitle> In Proceedings of the Second International WWW Conference, </booktitle> <address> Chicago, IL, </address> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching <ref> [5, 6, 7, 23, 31] </ref>, HTTP protocol enhancements [4, 20, 25, 18], better HTTP servers and proxies [2, 33, 7] and server OS implementations [16, 17, 10, 24]. <p> terms of request file types, transfer sizes, locality of reference in URLs requested and other fl This paper will appear in the Proceedings of the USENIX Symposium on Internet Technologies and Systems, Monterey, Dec 1997. y This work was supported in part by National Science Foundation Grant CCR-9503098 related statistics <ref> [3, 5, 6, 8, 9, 12] </ref>. Some researchers have tried to evaluate the performance of Web servers and proxies using real workloads directly [13, 15]. However, this approach suffers from the experimental difficulties involved in non-intrusive measurement of a live system and the inherent irreproducibility of live workloads. <p> Our work is intended to complement the existing work done on Web workload characterization <ref> [5, 6, 7, 23, 31] </ref>, and can easily be used in conjunction with it. 4.1 Basic Architecture The basic architecture of our testbed is shown in Figure 3. A set of P client machines are connected to the server machine being tested.
Reference: [7] <author> A. Chankhunthod, P. B. Danzig, C. Neerdaels, M. F. Schwartz, and K. J. Worrell. </author> <title> A Hierarchical Internet Object Cache. </title> <booktitle> In Proceedings of the 1996 Usenix Technical Conference, </booktitle> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching <ref> [5, 6, 7, 23, 31] </ref>, HTTP protocol enhancements [4, 20, 25, 18], better HTTP servers and proxies [2, 33, 7] and server OS implementations [16, 17, 10, 24]. <p> Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching [5, 6, 7, 23, 31], HTTP protocol enhancements [4, 20, 25, 18], better HTTP servers and proxies <ref> [2, 33, 7] </ref> and server OS implementations [16, 17, 10, 24]. <p> Also, packet losses due to congestion are absent in LAN-based testbeds. Maltzahn et al. [13] discovered a large difference in Squid proxy performance from the idealized numbers reported in <ref> [7] </ref>. A lot of this degradation is attributed to such WAN effects, which tend to keep server resources such as memory tied up for extended periods of time. <p> Our work is intended to complement the existing work done on Web workload characterization <ref> [5, 6, 7, 23, 31] </ref>, and can easily be used in conjunction with it. 4.1 Basic Architecture The basic architecture of our testbed is shown in Figure 3. A set of P client machines are connected to the server machine being tested.
Reference: [8] <author> M. Crovella and A. Bestavros. </author> <title> Self-Similarity in World Wide Web Traffic: Evidence and Possible Causes. </title> <booktitle> In Proceedings of the ACM SIGMETRICS '96 Conference, </booktitle> <address> Philadelphia, PA, </address> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: terms of request file types, transfer sizes, locality of reference in URLs requested and other fl This paper will appear in the Proceedings of the USENIX Symposium on Internet Technologies and Systems, Monterey, Dec 1997. y This work was supported in part by National Science Foundation Grant CCR-9503098 related statistics <ref> [3, 5, 6, 8, 9, 12] </ref>. Some researchers have tried to evaluate the performance of Web servers and proxies using real workloads directly [13, 15]. However, this approach suffers from the experimental difficulties involved in non-intrusive measurement of a live system and the inherent irreproducibility of live workloads. <p> As a result, HTTP request traffic arriving at a server is bursty with the burstiness being observable at several scales of observation <ref> [8] </ref>, and with peak rates exceeding the average rate by factors of 8 to 10 [15, 27]. Furthermore, peak request rates can easily exceed the capacity of the server. <p> In this way, any request arrival process can be generated whose peak request rate is lower than or equal to the maximum raw request rate of the system. In particular, the system can be parameterized to generate self-similar traffic <ref> [8] </ref>. 5 Quantitative Evaluation In this section we present experimental data to quantify the problems identified in Section 3, and to evaluate the performance of our proposed method. We measure the request generation limitations of the naive approach and evaluate the S-Client based request generation method proposed in Section 4. <p> Most recently, Arlitt and Williamson [3] characterized several aspects of Web server workloads such as request file type distribution, transfer sizes, locality of reference in the requested URLs and related statistics. Crovella and Bestavros <ref> [8] </ref> looked at Self-Similarity in WWW traffic.
Reference: [9] <author> C. Cunha, A. Bestavros, and M. Crovella. </author> <title> Characteristics of WWW Client-Based Traces. </title> <type> Technical Report TR-95-010, </type> <institution> Boston University, CS Dept., </institution> <address> Boston MA, </address> <year> 1995. </year>
Reference-contexts: terms of request file types, transfer sizes, locality of reference in URLs requested and other fl This paper will appear in the Proceedings of the USENIX Symposium on Internet Technologies and Systems, Monterey, Dec 1997. y This work was supported in part by National Science Foundation Grant CCR-9503098 related statistics <ref> [3, 5, 6, 8, 9, 12] </ref>. Some researchers have tried to evaluate the performance of Web servers and proxies using real workloads directly [13, 15]. However, this approach suffers from the experimental difficulties involved in non-intrusive measurement of a live system and the inherent irreproducibility of live workloads.
Reference: [10] <institution> DIGITAL UNIX Tuning Parameters for Web Servers. </institution> <address> http://www.digital.com/info/internet/docu ment/ias/tuning.html. </address>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching [5, 6, 7, 23, 31], HTTP protocol enhancements [4, 20, 25, 18], better HTTP servers and proxies [2, 33, 7] and server OS implementations <ref> [16, 17, 10, 24] </ref>.
Reference: [11] <author> P. Druschel and G. Banga. </author> <title> Lazy Receiver Processing (LRP): A Network Subsystem Architecture for Server Systems. </title> <booktitle> In Proc. 2nd Symp. on Operating Systems Design and Implementation, </booktitle> <address> Seattle, WA, </address> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: The effect of such bursts is to temporarily overload the server. It is important to evaluate Web server performance under overload. For instance, it is a well known fact that many Unix and non-Unix based network subsystems suffer from poor overload behavior <ref> [11, 19] </ref>. Under heavy network load these interrupt-driven systems can enter a state called receiver-livelock [22].
Reference: [12] <author> T. T. Kwan, R. E. McGrath, and D. A. Reed. </author> <title> User access patterns to NCSA's World-wide Web server. </title> <type> Technical Report UIUCDCS-R-95-1934, </type> <institution> Dept. of Computer Science, Univ. IL., </institution> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: terms of request file types, transfer sizes, locality of reference in URLs requested and other fl This paper will appear in the Proceedings of the USENIX Symposium on Internet Technologies and Systems, Monterey, Dec 1997. y This work was supported in part by National Science Foundation Grant CCR-9503098 related statistics <ref> [3, 5, 6, 8, 9, 12] </ref>. Some researchers have tried to evaluate the performance of Web servers and proxies using real workloads directly [13, 15]. However, this approach suffers from the experimental difficulties involved in non-intrusive measurement of a live system and the inherent irreproducibility of live workloads.
Reference: [13] <author> C. Maltzahn, K. J. Richardson, and D. Grunwald. </author> <title> Performance Issues of Enterprise Level Web Proxies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS '97 Conference, </booktitle> <address> Seattle, WA, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Some researchers have tried to evaluate the performance of Web servers and proxies using real workloads directly <ref> [13, 15] </ref>. However, this approach suffers from the experimental difficulties involved in non-intrusive measurement of a live system and the inherent irreproducibility of live workloads. <p> In particular, the simple method does not model high and variable WAN delays which are known to cause long SYN-RCVD queues in the server's listening socket. Also, packet losses due to congestion are absent in LAN-based testbeds. Maltzahn et al. <ref> [13] </ref> discovered a large difference in Squid proxy performance from the idealized numbers reported in [7]. A lot of this degradation is attributed to such WAN effects, which tend to keep server resources such as memory tied up for extended periods of time.
Reference: [14] <author> M. K. McKusick, K. Bostic, M. J. Karels, and J. S. Quarterman. </author> <title> The Design and Implementation of the 4.4BSD Operating System. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1996. </year>
Reference-contexts: The description provides background for the discussion in the following sections. For simplicity, we focus our discussion on a BSD <ref> [14, 32] </ref> based network subsystem. The working of many other implementations of TCP/IP, such as those found in Unix System V and Windows NT, is similar.
Reference: [15] <author> J. C. Mogul. </author> <title> Network behavior of a busy web server and its clients. </title> <type> Technical Report WRL 95/5, </type> <institution> DEC Western Research Laboratory, </institution> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference-contexts: Some researchers have tried to evaluate the performance of Web servers and proxies using real workloads directly <ref> [13, 15] </ref>. However, this approach suffers from the experimental difficulties involved in non-intrusive measurement of a live system and the inherent irreproducibility of live workloads. <p> As a result, HTTP request traffic arriving at a server is bursty with the burstiness being observable at several scales of observation [8], and with peak rates exceeding the average rate by factors of 8 to 10 <ref> [15, 27] </ref>. Furthermore, peak request rates can easily exceed the capacity of the server. By contrast, in the simple request generation method, a small number of clients have independent think time distributions with small mean and variance. As a result, the generated traffic has little burstiness. <p> The primary factor in preventing client bottlenecks from affecting server performance results is to limit the number of simulated clients per client machine. In addition, it is important to use an efficient implementation of TCP/IP (in particular, an efficient PCB table <ref> [15] </ref> implementation) on the client machines, and to avoid I/O operations in the simulated clients that could affect the rate of HTTP transactions in uncontrolled ways. For example, writing logging information to disk can affect the client behavior in complex and undesirable ways.
Reference: [16] <author> J. C. Mogul. </author> <title> Operating system support for busy internet servers. </title> <booktitle> In Proceedings of the Fifth Workshop on Hot Topics in Operating Systems (HotOS-V), </booktitle> <address> Orcas Island, WA, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching [5, 6, 7, 23, 31], HTTP protocol enhancements [4, 20, 25, 18], better HTTP servers and proxies [2, 33, 7] and server OS implementations <ref> [16, 17, 10, 24] </ref>.
Reference: [17] <author> J. C. </author> <title> Mogul. </title> <type> Personal communication, </type> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching [5, 6, 7, 23, 31], HTTP protocol enhancements [4, 20, 25, 18], better HTTP servers and proxies [2, 33, 7] and server OS implementations <ref> [16, 17, 10, 24] </ref>.
Reference: [18] <author> J. C. Mogul, F. Douglis, A. Feldmann, and B. Kr-ishnamurthy. </author> <title> Potential benefits of delta encoding and data compression for HTTP. In Proceedings of the SIGCOMM '97 Conference, </title> <address> Cannes, France, </address> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching [5, 6, 7, 23, 31], HTTP protocol enhancements <ref> [4, 20, 25, 18] </ref>, better HTTP servers and proxies [2, 33, 7] and server OS implementations [16, 17, 10, 24].
Reference: [19] <author> J. C. Mogul and K. K. Ramakrishnan. </author> <title> Eliminating receive livelock in an interrupt-driven kernel. </title> <booktitle> In Proc. of the 1996 Usenix Technical Conference, </booktitle> <pages> pages 99-111, </pages> <year> 1996. </year>
Reference-contexts: The effect of such bursts is to temporarily overload the server. It is important to evaluate Web server performance under overload. For instance, it is a well known fact that many Unix and non-Unix based network subsystems suffer from poor overload behavior <ref> [11, 19] </ref>. Under heavy network load these interrupt-driven systems can enter a state called receiver-livelock [22].
Reference: [20] <author> V. N. Padmanabhan and J. C. Mogul. </author> <title> Improving HTTP Latency. </title> <booktitle> In Proceedings of the Second International WWW Conference, </booktitle> <address> Chicago, IL, </address> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching [5, 6, 7, 23, 31], HTTP protocol enhancements <ref> [4, 20, 25, 18] </ref>, better HTTP servers and proxies [2, 33, 7] and server OS implementations [16, 17, 10, 24].
Reference: [21] <author> J. B. Postel. </author> <title> Transmission Control Protocol. </title> <type> RFC 793, </type> <month> Sept. </month> <year> 1981. </year>
Reference-contexts: Digital Unix 32767, Solaris 1000). In Section 3, we will see how this fact interacts with WWW request generation. 4 This TIME-WAIT period should be set equal to twice the Maximum Segment Lifetime (MSL) of a packet on the Internet (RFC 793 <ref> [21] </ref> specifies the MSL as 2 minutes, but many implementations use a much shorter value.) 3 Problems in Generating Synthetic HTTP requests This section identifies problems that arise when trying to measure the performance of a Web server, using a testbed consisting of a limited number of client machines.
Reference: [22] <author> K. K. Ramakrishnan. </author> <title> Scheduling issues for interfacing to high speed networks. </title> <booktitle> In Proc. Globe-com'92 IEEE Global Telecommunications Conference, </booktitle> <pages> pages 622-626, </pages> <address> Orlando, FL, </address> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: It is important to evaluate Web server performance under overload. For instance, it is a well known fact that many Unix and non-Unix based network subsystems suffer from poor overload behavior [11, 19]. Under heavy network load these interrupt-driven systems can enter a state called receiver-livelock <ref> [22] </ref>. In this state, the system spends all its resources processing incoming network packets (in this case TCP SYN packets), only to discard them later because there is no CPU time left to service the receiving application programs (in this case the Web server).
Reference: [23] <author> M. Seltzer and J. Gwertzman. </author> <title> The Case for Geographical Pushcaching. </title> <booktitle> In Proceedings of the 1995 Workshop on Hot Operating Systems, </booktitle> <year> 1995. </year>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching <ref> [5, 6, 7, 23, 31] </ref>, HTTP protocol enhancements [4, 20, 25, 18], better HTTP servers and proxies [2, 33, 7] and server OS implementations [16, 17, 10, 24]. <p> Our work is intended to complement the existing work done on Web workload characterization <ref> [5, 6, 7, 23, 31] </ref>, and can easily be used in conjunction with it. 4.1 Basic Architecture The basic architecture of our testbed is shown in Figure 3. A set of P client machines are connected to the server machine being tested.
Reference: [24] <institution> Solaris 2 TCP/IP. </institution> <address> http://www.sun.com/sunsoft/solaris/networking /tcpip.html. </address>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching [5, 6, 7, 23, 31], HTTP protocol enhancements [4, 20, 25, 18], better HTTP servers and proxies [2, 33, 7] and server OS implementations <ref> [16, 17, 10, 24] </ref>.
Reference: [25] <author> M. Spasojevic, M. Bowman, and A. Spector. </author> <title> Using a Wide-Area File System Within the World-Wide Web. </title> <booktitle> In Proceedings of the Second International WWW Conference, </booktitle> <address> Chicago, IL, </address> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching [5, 6, 7, 23, 31], HTTP protocol enhancements <ref> [4, 20, 25, 18] </ref>, better HTTP servers and proxies [2, 33, 7] and server OS implementations [16, 17, 10, 24].
Reference: [26] <author> SPECWeb96. </author> <note> http://www.specbench.org/osg/web96/. </note>
Reference-contexts: However, this approach suffers from the experimental difficulties involved in non-intrusive measurement of a live system and the inherent irreproducibility of live workloads. Recently, there has been some effort towards Web server evaluation through generation of synthetic HTTP client traffic, based on invariants observed in real Web traffic <ref> [26, 28, 29, 30, 1] </ref>. Unfortunately, there are pitfalls that arise in generating heavy and realistic Web traffic using a limited number of client machines. These problems can lead to significant deviation of benchmarking conditions from reality and fail to predict the performance of a given Web server. <p> We expect that this request generation methodology, in conjunction with a representative HTTP request data set like the one used in the SPECWeb benchmark <ref> [26] </ref> and a representative temporal characterization of HTTP traffic, will result in a benchmark that can more accurately predict actual Web server performance. The rest of this paper is organized as follows. <p> To reduce cost and for ease of control of the experiment, P must be kept low. All the popular Web benchmarking efforts that we know of use a load generation scheme similar to this <ref> [26, 28, 29, 30] </ref>. Several problems arise when trying to use the simple scheme described above to generate realistic HTTP requests. <p> WebStone is very similar to the simple scheme that we described in Section 3 and suffers from its limitations. Recently SPEC has released SPECWeb96 <ref> [26] </ref>, which is a standardized Web server benchmark with a workload derived from the study of some typical servers on the Internet. The request generation method of this benchmark is also similar to that of the simple scheme and so it too suffers from the same limitations.
Reference: [27] <author> W. Stevens. </author> <title> TCP/IP Illustrated Volume 3. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1996. </year>
Reference-contexts: As a result, HTTP request traffic arriving at a server is bursty with the burstiness being observable at several scales of observation [8], and with peak rates exceeding the average rate by factors of 8 to 10 <ref> [15, 27] </ref>. Furthermore, peak request rates can easily exceed the capacity of the server. By contrast, in the simple request generation method, a small number of clients have independent think time distributions with small mean and variance. As a result, the generated traffic has little burstiness.
Reference: [28] <institution> Web 66. </institution> <note> http://web66.coled.umn.edu/gstone/info.html. </note>
Reference-contexts: However, this approach suffers from the experimental difficulties involved in non-intrusive measurement of a live system and the inherent irreproducibility of live workloads. Recently, there has been some effort towards Web server evaluation through generation of synthetic HTTP client traffic, based on invariants observed in real Web traffic <ref> [26, 28, 29, 30, 1] </ref>. Unfortunately, there are pitfalls that arise in generating heavy and realistic Web traffic using a limited number of client machines. These problems can lead to significant deviation of benchmarking conditions from reality and fail to predict the performance of a given Web server. <p> To reduce cost and for ease of control of the experiment, P must be kept low. All the popular Web benchmarking efforts that we know of use a load generation scheme similar to this <ref> [26, 28, 29, 30] </ref>. Several problems arise when trying to use the simple scheme described above to generate realistic HTTP requests. <p> Web server benchmarking efforts have much more recent origins. SGI's WebStone [30] was one of the earliest Web server benchmarks and is the de facto industry standard, although there have been several other efforts <ref> [28, 29] </ref>. WebStone is very similar to the simple scheme that we described in Section 3 and suffers from its limitations. Recently SPEC has released SPECWeb96 [26], which is a standardized Web server benchmark with a workload derived from the study of some typical servers on the Internet.
Reference: [29] <author> WebCompare. </author> <note> http://webcompare.iworld.com/. </note>
Reference-contexts: However, this approach suffers from the experimental difficulties involved in non-intrusive measurement of a live system and the inherent irreproducibility of live workloads. Recently, there has been some effort towards Web server evaluation through generation of synthetic HTTP client traffic, based on invariants observed in real Web traffic <ref> [26, 28, 29, 30, 1] </ref>. Unfortunately, there are pitfalls that arise in generating heavy and realistic Web traffic using a limited number of client machines. These problems can lead to significant deviation of benchmarking conditions from reality and fail to predict the performance of a given Web server. <p> To reduce cost and for ease of control of the experiment, P must be kept low. All the popular Web benchmarking efforts that we know of use a load generation scheme similar to this <ref> [26, 28, 29, 30] </ref>. Several problems arise when trying to use the simple scheme described above to generate realistic HTTP requests. <p> Web server benchmarking efforts have much more recent origins. SGI's WebStone [30] was one of the earliest Web server benchmarks and is the de facto industry standard, although there have been several other efforts <ref> [28, 29] </ref>. WebStone is very similar to the simple scheme that we described in Section 3 and suffers from its limitations. Recently SPEC has released SPECWeb96 [26], which is a standardized Web server benchmark with a workload derived from the study of some typical servers on the Internet.
Reference: [30] <institution> WebStone. http://www.sgi.com/Products/WebFORCE/Resources /res webstone.html. </institution>
Reference-contexts: However, this approach suffers from the experimental difficulties involved in non-intrusive measurement of a live system and the inherent irreproducibility of live workloads. Recently, there has been some effort towards Web server evaluation through generation of synthetic HTTP client traffic, based on invariants observed in real Web traffic <ref> [26, 28, 29, 30, 1] </ref>. Unfortunately, there are pitfalls that arise in generating heavy and realistic Web traffic using a limited number of client machines. These problems can lead to significant deviation of benchmarking conditions from reality and fail to predict the performance of a given Web server. <p> To reduce cost and for ease of control of the experiment, P must be kept low. All the popular Web benchmarking efforts that we know of use a load generation scheme similar to this <ref> [26, 28, 29, 30] </ref>. Several problems arise when trying to use the simple scheme described above to generate realistic HTTP requests. <p> Eventually, a point is reached where the bottleneck in a Web transaction is no longer the server but the client. Designers of commercial Web server benchmarks have also noticed this pitfall. The WebStone benchmark <ref> [30] </ref> explicitly warns about this potential problem, but gives no systematic method to avoid it. The primary factor in preventing client bottlenecks from affecting server performance results is to limit the number of simulated clients per client machine. <p> Web server benchmarking efforts have much more recent origins. SGI's WebStone <ref> [30] </ref> was one of the earliest Web server benchmarks and is the de facto industry standard, although there have been several other efforts [28, 29]. WebStone is very similar to the simple scheme that we described in Section 3 and suffers from its limitations.
Reference: [31] <author> S. Williams, M. Abrams, C. R. Standridge, G. Ab--dulla, and E. A. Fox. </author> <title> Removal Policies in Network Caches for World-Wide Web Documents. </title> <booktitle> In Proceedings of the SIGCOMM '96 Conference, </booktitle> <address> Palo Alto, CA, </address> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching <ref> [5, 6, 7, 23, 31] </ref>, HTTP protocol enhancements [4, 20, 25, 18], better HTTP servers and proxies [2, 33, 7] and server OS implementations [16, 17, 10, 24]. <p> Our work is intended to complement the existing work done on Web workload characterization <ref> [5, 6, 7, 23, 31] </ref>, and can easily be used in conjunction with it. 4.1 Basic Architecture The basic architecture of our testbed is shown in Figure 3. A set of P client machines are connected to the server machine being tested.
Reference: [32] <author> G. Wright and W. Stevens. </author> <title> TCP/IP Illustrated Volume 2. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1995. </year>
Reference-contexts: The description provides background for the discussion in the following sections. For simplicity, we focus our discussion on a BSD <ref> [14, 32] </ref> based network subsystem. The working of many other implementations of TCP/IP, such as those found in Unix System V and Windows NT, is similar.
Reference: [33] <author> Zeus. </author> <note> http://www.zeus.co.uk/. </note>
Reference-contexts: Improving the performance of the Web has been the subject of much recent research, addressing various aspects of the problem such as better Web caching [5, 6, 7, 23, 31], HTTP protocol enhancements [4, 20, 25, 18], better HTTP servers and proxies <ref> [2, 33, 7] </ref> and server OS implementations [16, 17, 10, 24].
References-found: 33

