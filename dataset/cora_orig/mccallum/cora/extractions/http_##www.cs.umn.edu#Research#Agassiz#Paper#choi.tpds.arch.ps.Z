URL: http://www.cs.umn.edu/Research/Agassiz/Paper/choi.tpds.arch.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: Email: lchoi@csrd.uiuc.edu  Email: yew@cs.umn.edu  
Title: Hardware and Compiler-Directed Cache Coherence in Large-Scale Multiprocessors: Design Considerations and Performance Study 1  
Author: Lynn Choi Pen-Chung Yew 
Keyword: Cache Coherence, Memory Systems, Performance Evaluation, Computer Architecture, Shared-Memory Multiprocessors.  
Address: 1308 West Main Street Urbana, IL 61801  Building  200 Union Street, SE Minneapolis, MN 55455-0159  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  4-192 EE/CS  Department of Computer Science University of Minnesota  
Abstract: In this paper, we study a hardware-supported, compiler-directed (HSCD) cache coherence scheme, which can be implemented on a large-scale multiprocessor using off-the-shelf microprocessors, such as the Cray T3D. The scheme can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures. Several system related issues, including critical sections, inter-thread communication, and task migration have also been addressed. The cost of the required hardware support is minimal and proportional to the cache size. The necessary compiler algorithms, including intra- and interprocedural array data flow analysis, have been implemented on the Polaris parallelizing compiler [33]. From our simulation study using the Perfect Club benchmarks [5], we found that in spite of the conservative analysis made by the compiler, the performance of the proposed HSCD scheme can be comparable to that of a full-map hardware directory scheme. Given its comparable performance and reduced hardware cost, the proposed scheme can be a viable alternative for large-scale multiprocessors such as the Cray T3D, which rely on users to maintain data coherence. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve, V. S. Adve, M. D. Hill, and M. K. Vernon. </author> <title> Comparison of Hardware and Software Cache Coherence Schemes. </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 298-308, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Lilja [28] compared the performance of the version control scheme [13] with directory schemes, and analyzed the directory overhead of several implementations. Both studies show that the performance of those HSCD schemes can be comparable to that of directory schemes. Adve <ref> [1] </ref> used an analytical model to compare the performance of compiler-directed and directory-based techniques. They concluded that the performance of compiler-directed schemes depends on the characteristics of the workloads.
Reference: [2] <author> A. Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> Proceedings of Workshop on Scalable Shared Memory Multiprocessors, </booktitle> <year> 1991. </year>
Reference-contexts: Since the address tag overhead is much more expensive than the data storage, 9 the extra hardware overhead for TPI scheme is reasonably small. Table 1 compares the storage overhead of our TPI scheme with a full-map directory scheme [8] as well as the LimitLess directory scheme <ref> [2] </ref>. It shows both the cache (SRAM) and memory overhead (DRAM) in terms of the cache line size (L), node cache size (C), node memory size (M) and number of processors (P).
Reference: [3] <author> J. Archibald and J. Baer. </author> <title> An Economical Solution to the Cache Coherence Problem. </title> <booktitle> Proceedings of The 11th Annual International Symposium on Computer Architectur, </booktitle> <pages> pages 355-362, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: This coherence mechanism is similar to that of the Cray T3D, where the compiler generates noncacheable loads for shared memory references and cacheable loads for private references. 2. Full-Map Directory Scheme (HW) This scheme uses a simple, three-state (invalid, read-shared, write-exclusive) invalidation-based protocol with a full-map directory <ref> [8, 3] </ref> but without broadcasting. It gives a performance comparison to the hardware directory protocols.
Reference: [4] <author> R. Ballance, A. Maccabe, and K. Ottenstein. </author> <title> The Program Dependence Web: a Representation Supporting Control Data- and Demand-Driven Interpretation of Imperative Languages. </title> <booktitle> Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: We use the demand-driven symbolic analysis using GSA form <ref> [4] </ref>. 8 * Interprocedural analysis Previous HSCD schemes invalidate the entire cache at pro-cedure boundaries to avoid side effects caused by procedure calls. We use a complete interprocedural analysis to avoid such invalidations and to exploit locality across procedure boundaries. <p> First, we construct a procedure call graph. Then, based on the bottom-up scan of the call graph, we analyze each procedure and propagate its side effects to its callers. The per-procedure analysis is based on the following steps. First, we transform the source program into GSA form <ref> [4] </ref>. Then, we construct a modified flow graph, called the epoch flow graph [18]. It contains the epoch boundary information as well as the control flows of the program.
Reference: [5] <author> M. Berry and others. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall, </month> <year> 1989. </year> <month> 28 </month>
Reference-contexts: This difference will create less write traffic in the hardware directory schemes than in TPI scheme. 4 Performance Evaluation 4.1 Experimental Methodology In this section, we describe our simulation study to evaluate our proposed scheme. We use six programs from the Perfect Club benchmark suite <ref> [5] </ref> as our target benchmarks. They are first parallelized by the Polaris compiler. In the parallelized code, the parallelism is expressed in terms of DOALL loops. We then mark the Time-Read operations in the parallelized source codes using our compiler algorithms [15], which are also implemented in the Polaris compiler. <p> The cost of the required hardware support is minimal and proportional to the cache size. The necessary compiler algorithms, including intra- and interprocedural array data flow analysis, have been implemented on the Polaris paralleling compiler [33]. The results of our simulation study using the Perfect Club Benchmarks <ref> [5] </ref> show that both hardware directory schemes and the TPI scheme have comparable number of unnecessary cache misses. In hardware schemes these misses result from the false-sharing effect while in our proposed scheme they come from the conservative assumptions made by the compiler.
Reference: [6] <author> W. C. Brantley, K. P. McAuliffe, and J. Weiss. </author> <title> RP3 processor-memory element. </title> <booktitle> Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 782-789, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: They, instead, provide software mechanisms while relying mostly on users to maintain data coherence either through language extensions or message-passing paradigms. In several early multiprocessor systems, such as the CMU C.mmp [38], the NYU Ultracom-puter [23], the IBM RP3 <ref> [6] </ref>, and the Illinois Cedar [27], compiler-directed techniques were used to solve the cache coherence problem. In this approach, cache coherence is maintained locally without the need for interprocessor communication or hardware directories. <p> In this paper, we investigated a hardware-supported, compiler-directed (HSCD) cache coherence scheme, called the two-phase invalidation (TPI) scheme which relies mostly on compiler analysis, yet also provides a reasonable amount of hardware support. This approach has a long history of predecessors, including C.mmp [38], IBM's RP3 <ref> [6] </ref>, Illinois Cedar [27], and several recently proposed schemes [10, 14, 12, 13, 18, 21, 29, 30]. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf 27 microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [7] <institution> IBM Inc. </institution> <note> Edited by C. May, </note> <author> E. Silha, R. Simpson, and H. Warren. </author> <title> The PowerPC Architecture: A Specification for a New Family of RISC Processors. </title> <month> March </month> <year> 1995. </year>
Reference-contexts: Similarly, the Flush instruction (DCBF) can be used for the IBM PowerPC 600 series microprocessor <ref> [7] </ref>. 5 2.2 Two-phase invalidation scheme (TPI) In our proposed HSCD scheme, called the Two-Phase Invalidation (TPI) scheme 3 , each epoch is assigned a unique epoch number.
Reference: [8] <author> L. M. Censier and P. Feautrier. </author> <title> A New Solution to Coherence Problems in Multicache Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> December, </month> <year> 1978. </year>
Reference-contexts: Since the address tag overhead is much more expensive than the data storage, 9 the extra hardware overhead for TPI scheme is reasonably small. Table 1 compares the storage overhead of our TPI scheme with a full-map directory scheme <ref> [8] </ref> as well as the LimitLess directory scheme [2]. It shows both the cache (SRAM) and memory overhead (DRAM) in terms of the cache line size (L), node cache size (C), node memory size (M) and number of processors (P). <p> or 8-bit timetag is large enough to achieve a very good performance. 9 As noted by [32, 39], the tag overhead in the design of the on-chip caches are significant, occupying area comparable to the data portion, especially for set-associative caches with 64-bit addresses. 12 Full-map LimitLess Directory Two-phase Directory <ref> [8] </ref> (bits) (DIR NB i )[2] (bits) Invalidation (bits) Cache Overhead (SRAM) 2*C*P 2*C*P 8*L*C*P Memory Overhead (DRAM) (P+2)*M*P (i+2)*M*P None Total (P = 1024, i = 10) 4MB SRAM / 4MB SRAM / 64MB SRAM only 64.5GB DRAM 3GB DRAM Table 1: Cache and memory overhead of different hardware cache <p> This coherence mechanism is similar to that of the Cray T3D, where the compiler generates noncacheable loads for shared memory references and cacheable loads for private references. 2. Full-Map Directory Scheme (HW) This scheme uses a simple, three-state (invalid, read-shared, write-exclusive) invalidation-based protocol with a full-map directory <ref> [8, 3] </ref> but without broadcasting. It gives a performance comparison to the hardware directory protocols.
Reference: [9] <author> Yung-Chin Chen. </author> <title> Cache Design and Performance in a Large-Scale Shared-Memory Multiprocessor System. </title> <type> Technical report, </type> <institution> Univ. of Illinois, Dept. of Elec. Eng ., 1993. </institution> <type> Ph.D. Thesis. </type>
Reference-contexts: Both studies show that the performance of those HSCD schemes can be comparable to that of directory schemes. Adve [1] used an analytical model to compare the performance of compiler-directed and directory-based techniques. They concluded that the performance of compiler-directed schemes depends on the characteristics of the workloads. Chen <ref> [9] </ref> showed that a simple invalidation scheme can achieve performance comparable to that of a directory scheme and discussed several different write policies. Most 2 of those studies, however, assumed perfect compile-time memory disambiguation and complete control dependence information. <p> This can be accomplished by using write-through caches. However, this will produce more redundant write traffic. By organizing the write buffer as a cache, as in the DEC Alpha 21164 processor [19], such redundant write traffic can be reduced effectively <ref> [9] </ref>. Note that ordinary write buffers can help hide latencies but cannot eliminate redundant write traffic. For a write-back cache configuration, TPI scheme should force all global writes to be written back to the main memory at synchronization points. <p> A write-through write-allocate policy is used for both TPI and SC schemes, while a write-back cache is used for the hardware directory protocol. These write policies are chosen to deliver the best performance for each type of coherence scheme <ref> [9] </ref>. A weak consistency model is used for all the coherence schemes. It is assumed that each processor can handle basic arithmetic and logical operations in one cycle and that it has synchronization operations to support parallel language constructs. <p> Therefore, redundant writes are not merged. The use of write buffers will decrease only the stall times of the CPU, and not the network traffic. The assumption of the infinite write buffer will decrease the CPU stall times during the simulation compared to a fixed size write buffer. Chen <ref> [9] </ref> studied the issue of write buffer design for the compiler-directed schemes and found that 8 words of write merging write buffers will reduce the traffic significantly, and that the write through with the write merging write buffer is a better choice than a write back cache implementation for compiler-directed schemes. <p> In TRFD, there is a significant number of redundant writes that increases the overall network traffic of TPI substantially compared to HW. This 13 Although compiler-directed schemes can employ write-back at task boundaries, it increases the latency of the invalidation and results in more bursty traffic <ref> [9] </ref>. 18 Program Average Miss Latency Two-Phase Invalidation Hardware Directory 16 bytes 64 bytes 16 bytes 64 bytes SPEC77 136.2 356.3 136.4 355.5 OCEAN 136.2 354.3 136.4 353.6 FLO52 136.2 355.1 136.6 361.2 QCD 136.0 354.7 145.5 405.4 TRFD 136.0 352.4 149.1 418.6 Table 5: Average miss penalty of TPI and <p> SPEC77 136.2 356.3 136.4 355.5 OCEAN 136.2 354.3 136.4 353.6 FLO52 136.2 355.1 136.6 361.2 QCD 136.0 354.7 145.5 405.4 TRFD 136.0 352.4 149.1 418.6 Table 5: Average miss penalty of TPI and HW schemes. additional write traffic can be eliminated effectively by organizing a write buffer as a cache <ref> [9] </ref>. A similar technique can also be employed to remove redundant write traffic for update-based coherence protocols. The third type of network traffic is for coherence transactions in the directory protocol. This extra traffic is relatively small compared to the read and write traffic for the benchmarks considered.
Reference: [10] <author> H. Cheong. </author> <title> Life Span Strategy A Compiler-Based Approach to Cache Coherence. </title> <booktitle> Proceedings of the 1992 International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Several compiler-directed cache coherence schemes <ref> [10, 12, 13, 14, 18, 21, 29, 30] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> Several compiler-directed cache coherence schemes [10, 12, 13, 14, 18, 21, 29, 30] have been recently proposed. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. They require a more precise program analysis to maintain coherence on a reference basis <ref> [10, 11, 18] </ref> instead of a program region basis compared to the previous schemes. In addition, these schemes require hardware support to maintain local runtime cache states. In this regard, the terminology software cache coherence is a misnomer. It is a hardware approach with strong compiler support. <p> The guarded execution technique can be used to further optimize code generation [15, 16]. The compiler marking algorithms developed here are general enough to be applicable to other compiler-directed coherence schemes <ref> [10, 12] </ref>. 3 Hardware implementation issues Off-chip secondary cache implementation Since most of today's multiprocessors use off-the-shelf microprocessors, it would be more cost effective if the proposed TPI scheme can directly be implemented using existing microprocessors. necessary to determine a cache hit for a Time-Read operation. <p> This approach has a long history of predecessors, including C.mmp [38], IBM's RP3 [6], Illinois Cedar [27], and several recently proposed schemes <ref> [10, 14, 12, 13, 18, 21, 29, 30] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf 27 microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [11] <author> H. Cheong and A. Veidenbaum. </author> <title> Stale Data Detection and Coherence Enforcement Using Flow Analysis. </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, I, </booktitle> <address> Architecture:138-145, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: Several compiler-directed cache coherence schemes [10, 12, 13, 14, 18, 21, 29, 30] have been recently proposed. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. They require a more precise program analysis to maintain coherence on a reference basis <ref> [10, 11, 18] </ref> instead of a program region basis compared to the previous schemes. In addition, these schemes require hardware support to maintain local runtime cache states. In this regard, the terminology software cache coherence is a misnomer. It is a hardware approach with strong compiler support.
Reference: [12] <author> H. Cheong and A. Veidenbaum. </author> <title> A Cache Coherence Scheme with Fast Selective Invalidation. </title> <booktitle> Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Several compiler-directed cache coherence schemes <ref> [10, 12, 13, 14, 18, 21, 29, 30] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> The guarded execution technique can be used to further optimize code generation [15, 16]. The compiler marking algorithms developed here are general enough to be applicable to other compiler-directed coherence schemes <ref> [10, 12] </ref>. 3 Hardware implementation issues Off-chip secondary cache implementation Since most of today's multiprocessors use off-the-shelf microprocessors, it would be more cost effective if the proposed TPI scheme can directly be implemented using existing microprocessors. necessary to determine a cache hit for a Time-Read operation. <p> This approach has a long history of predecessors, including C.mmp [38], IBM's RP3 [6], Illinois Cedar [27], and several recently proposed schemes <ref> [10, 14, 12, 13, 18, 21, 29, 30] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf 27 microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [13] <author> H. Cheong and A. Veidenbaum. </author> <title> A Version Control Approach To Cache Coherence. </title> <booktitle> Proceedings of the 1989 ACM/SIGARCH International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Several compiler-directed cache coherence schemes <ref> [10, 12, 13, 14, 18, 21, 29, 30] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> Several studies have compared the performance of directory schemes and some recent HSCD schemes. Min and Baer [31] compared the performance of a directory scheme and a timestamp-based scheme assuming infinite cache size and single-word cache lines. Lilja [28] compared the performance of the version control scheme <ref> [13] </ref> with directory schemes, and analyzed the directory overhead of several implementations. Both studies show that the performance of those HSCD schemes can be comparable to that of directory schemes. Adve [1] used an analytical model to compare the performance of compiler-directed and directory-based techniques. <p> This approach has a long history of predecessors, including C.mmp [38], IBM's RP3 [6], Illinois Cedar [27], and several recently proposed schemes <ref> [10, 14, 12, 13, 18, 21, 29, 30] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf 27 microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [14] <author> T. Chiueh. </author> <title> A Generational Approach to Software-Controlled Multiprocessor Cache Coherence. </title> <booktitle> Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <year> 1993. </year>
Reference-contexts: Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Several compiler-directed cache coherence schemes <ref> [10, 12, 13, 14, 18, 21, 29, 30] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> This approach has a long history of predecessors, including C.mmp [38], IBM's RP3 [6], Illinois Cedar [27], and several recently proposed schemes <ref> [10, 14, 12, 13, 18, 21, 29, 30] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf 27 microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [15] <author> Lynn Choi. </author> <title> Hardware and Compiler Support for Cache Coherence in Large-Scale Multiprocessors. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Dept., University of Illinois, </institution> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: We call this sequence of events a stale reference sequence and the read reference in (c) a potentially stale data reference. This sequence can be detected easily by a compiler using 4 a modified def-use chain analysis used in standard data-flow analysis techniques <ref> [15, 16] </ref>. Software cache-bypass scheme (SC) Once all the potentially stale data references are identified by the compiler, cache coherence can be enforced if we guarantee that all such references access up-to-date data from main memory rather than from the stale cache copies. <p> The details of compiler algorithms are beyond the scope of this paper and are described in <ref> [15, 16] </ref>. Compiler implementation All the compiler algorithms explained earlier have been implemented on the Polaris parallelizing compiler [33]. Figure 6 shows the flowchart of our reference marking algorithm. First, we construct a procedure call graph. <p> We then transform the program in the GSA form back to the original program with the reference marking information, and generate appropriate cache and memory operations. The guarded execution technique can be used to further optimize code generation <ref> [15, 16] </ref>. <p> They are first parallelized by the Polaris compiler. In the parallelized code, the parallelism is expressed in terms of DOALL loops. We then mark the Time-Read operations in the parallelized source codes using our compiler algorithms <ref> [15] </ref>, which are also implemented in the Polaris compiler. After compiler marking, we instrument the benchmarks to generate memory events, which are used for the simulation. Figure 8 shows the experimentation tools used in our simulations.
Reference: [16] <author> Lynn Choi and Pen-Chung Yew. </author> <title> Compiler Analysis for Cache Coherence: Interprocedural Array Data-Flow Analysis and Its Impacts on Cache Performance. </title> <journal> submitted to IEEE Transactions on Parallel and Distributed Systems, </journal> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: We call this sequence of events a stale reference sequence and the read reference in (c) a potentially stale data reference. This sequence can be detected easily by a compiler using 4 a modified def-use chain analysis used in standard data-flow analysis techniques <ref> [15, 16] </ref>. Software cache-bypass scheme (SC) Once all the potentially stale data references are identified by the compiler, cache coherence can be enforced if we guarantee that all such references access up-to-date data from main memory rather than from the stale cache copies. <p> The details of compiler algorithms are beyond the scope of this paper and are described in <ref> [15, 16] </ref>. Compiler implementation All the compiler algorithms explained earlier have been implemented on the Polaris parallelizing compiler [33]. Figure 6 shows the flowchart of our reference marking algorithm. First, we construct a procedure call graph. <p> We then transform the program in the GSA form back to the original program with the reference marking information, and generate appropriate cache and memory operations. The guarded execution technique can be used to further optimize code generation <ref> [15, 16] </ref>.
Reference: [17] <author> Lynn Choi and Pen-Chung Yew. </author> <title> Compiler and Hardware Support for Cache Coherence in Large-Sca le Multiprocessors: Design Considerations and Performance Study. </title> <booktitle> To appear in the 23rd Annual ACM International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference: [18] <author> Lynn Choi and Pen-Chung Yew. </author> <title> A Compiler-Directed Cache Coherence Scheme with Improved Intertask Locality. </title> <booktitle> Proceedings of the ACM/IEEE Supercomputing'94, </booktitle> <pages> pages 773-782, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Several compiler-directed cache coherence schemes <ref> [10, 12, 13, 14, 18, 21, 29, 30] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> Several compiler-directed cache coherence schemes [10, 12, 13, 14, 18, 21, 29, 30] have been recently proposed. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. They require a more precise program analysis to maintain coherence on a reference basis <ref> [10, 11, 18] </ref> instead of a program region basis compared to the previous schemes. In addition, these schemes require hardware support to maintain local runtime cache states. In this regard, the terminology software cache coherence is a misnomer. It is a hardware approach with strong compiler support. <p> The per-procedure analysis is based on the following steps. First, we transform the source program into GSA form [4]. Then, we construct a modified flow graph, called the epoch flow graph <ref> [18] </ref>. It contains the epoch boundary information as well as the control flows of the program. Given a source program unit and its epoch flow graph, G, we identify the target references in each epoch to utilize both spatial and temporal reuses inside a task. <p> This approach has a long history of predecessors, including C.mmp [38], IBM's RP3 [6], Illinois Cedar [27], and several recently proposed schemes <ref> [10, 14, 12, 13, 18, 21, 29, 30] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf 27 microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [19] <author> Digital Equipment Corp. </author> <title> Alpha 21164 Microprocessor: Hardware Reference Manual. </title> <year> 1994. </year>
Reference-contexts: Note that this is true for all the shared-memory multiprocessors. This can be accomplished by using write-through caches. However, this will produce more redundant write traffic. By organizing the write buffer as a cache, as in the DEC Alpha 21164 processor <ref> [19] </ref>, such redundant write traffic can be reduced effectively [9]. Note that ordinary write buffers can help hide latencies but cannot eliminate redundant write traffic. For a write-back cache configuration, TPI scheme should force all global writes to be written back to the main memory at synchronization points.
Reference: [20] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview. </title> <year> 1991. </year>
Reference-contexts: 1 Introduction Many commercially available large-scale multiprocessors, such as the Cray T3D and the Intel Paragon, do not provide hardware-coherent caches due to the expensive hardware required for such mechanisms <ref> [24, 20] </ref>. They, instead, provide software mechanisms while relying mostly on users to maintain data coherence either through language extensions or message-passing paradigms.
Reference: [21] <author> E. Darnell and K. Kennedy. </author> <title> Cache Coherence Using Local Knowledge. </title> <booktitle> Proceedings of the Supercomputing '93, </booktitle> <pages> pages 720-729, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Several compiler-directed cache coherence schemes <ref> [10, 12, 13, 14, 18, 21, 29, 30] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> This approach has a long history of predecessors, including C.mmp [38], IBM's RP3 [6], Illinois Cedar [27], and several recently proposed schemes <ref> [10, 14, 12, 13, 18, 21, 29, 30] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf 27 microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [22] <author> C. Dubnicki and T. LeBlanc. </author> <title> Adjustable Block Size Coherent Caches. </title> <booktitle> Proceedings the 19th Annual International Symposium on Computer Archtecture, </booktitle> <pages> pages 170-180, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Maintaining sharing information per cache word in directory schemes will increase directory storage significantly since the memory requirement is proportional to the total memory size instead of the total cache size. Optimizations to reduce the storage overhead may result in very complicated hardware protocols <ref> [22] </ref>. However, more fine-grained sharing information can be incorporated in this HSCD scheme more easily because the coherence enforcement is done locally. A cache memory has two components: data storage and address tag storage.
Reference: [23] <author> J. Edler, A. Gottlieb, C. P. Kruskal, K. P. McAuliffe, et al. </author> <title> Issues related to MIMD shared-memory computers: the NYU Ultracomputer approach. </title> <booktitle> Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 126-135, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: They, instead, provide software mechanisms while relying mostly on users to maintain data coherence either through language extensions or message-passing paradigms. In several early multiprocessor systems, such as the CMU C.mmp [38], the NYU Ultracom-puter <ref> [23] </ref>, the IBM RP3 [6], and the Illinois Cedar [27], compiler-directed techniques were used to solve the cache coherence problem. In this approach, cache coherence is maintained locally without the need for interprocessor communication or hardware directories.
Reference: [24] <author> Cray Research Inc. </author> <title> Cray T3D System Architecture Overview. </title> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Many commercially available large-scale multiprocessors, such as the Cray T3D and the Intel Paragon, do not provide hardware-coherent caches due to the expensive hardware required for such mechanisms <ref> [24, 20] </ref>. They, instead, provide software mechanisms while relying mostly on users to maintain data coherence either through language extensions or message-passing paradigms.
Reference: [25] <institution> MIPS Technology Inc. </institution> <note> R10000 User's Manual, Alpha Revision 2.0. </note> <month> March </month> <year> 1995. </year>
Reference-contexts: To overcome these limitations, we propose a hardware scheme that keeps track of the local cache states at runtime. 2 This operation can be implemented in the MIPS R10000 processor with a cache block invalidate (Index Write Back Invalidate) followed by a regular load operation <ref> [25] </ref>. Similarly, the Flush instruction (DCBF) can be used for the IBM PowerPC 600 series microprocessor [7]. 5 2.2 Two-phase invalidation scheme (TPI) In our proposed HSCD scheme, called the Two-Phase Invalidation (TPI) scheme 3 , each epoch is assigned a unique epoch number.
Reference: [26] <author> C. P. Kruskal and M. Snir. </author> <title> The Performance of Multistage Interconnection Networks for Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(12):1091-1098, </volume> <month> Sept., </month> <year> 1987. </year>
Reference-contexts: The memory system provides a one-cycle cache hit latency and a 100-cycle miss latency, assuming no network load. The network delays are simulated using an analytical delay model for indirect multistage networks <ref> [26] </ref>. The execution-driven simulator instruments the application codes to generate events that reflect the behavior of the codes executing on the target architecture. Simulated events include global and local memory accesses, parallel loop setup and scheduling operations, and synchronization operations. <p> size on-chip 64 KB, direct-mapped on epoch counter overflow two-phase reset mechanism line size 4 32-bit word cache line timetag size 8-bits number of processors 16 Cache and system organization ALU operations 1 CPU cycle cache hit 1 CPU cycle base miss latency 100 CPU cycles network delay analytic model <ref> [26] </ref> memory bus cycle 3 CPU cycles off-chip timetag access 2 CPU cycles off-chip timetag access penalty 8 (3 + 2 + 3) CPU cycles Table 2: Parameters used for typical simulations.
Reference: [27] <author> D. Kuck, E. Davidson, et al. </author> <title> The Cedar System and an Initial Performance Study. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Com puter Architecture, </booktitle> <pages> pages 213-223, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: They, instead, provide software mechanisms while relying mostly on users to maintain data coherence either through language extensions or message-passing paradigms. In several early multiprocessor systems, such as the CMU C.mmp [38], the NYU Ultracom-puter [23], the IBM RP3 [6], and the Illinois Cedar <ref> [27] </ref>, compiler-directed techniques were used to solve the cache coherence problem. In this approach, cache coherence is maintained locally without the need for interprocessor communication or hardware directories. The C.mmp was the first to allow read-only shared data to be kept in private caches while leaving read-write data uncached. <p> In this paper, we investigated a hardware-supported, compiler-directed (HSCD) cache coherence scheme, called the two-phase invalidation (TPI) scheme which relies mostly on compiler analysis, yet also provides a reasonable amount of hardware support. This approach has a long history of predecessors, including C.mmp [38], IBM's RP3 [6], Illinois Cedar <ref> [27] </ref>, and several recently proposed schemes [10, 14, 12, 13, 18, 21, 29, 30]. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf 27 microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [28] <author> D. J. Lilja. </author> <title> Cache Coherence in Large-Scale Shared-Memory Multiprocessors: Issues and Comparisons. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(3) </volume> <pages> 303-338, </pages> <month> Sep. </month> <year> 1993. </year> <month> 29 </month>
Reference-contexts: Several studies have compared the performance of directory schemes and some recent HSCD schemes. Min and Baer [31] compared the performance of a directory scheme and a timestamp-based scheme assuming infinite cache size and single-word cache lines. Lilja <ref> [28] </ref> compared the performance of the version control scheme [13] with directory schemes, and analyzed the directory overhead of several implementations. Both studies show that the performance of those HSCD schemes can be comparable to that of directory schemes.
Reference: [29] <author> A. Louri and H. Sung. </author> <title> A Compiler Directed Cache Coherence Scheme with Fast and Parallel Explicit Invalidation. </title> <booktitle> Proceedings of the 1992 International Conference on Parallel Processing, I, </booktitle> <address> Architecture:2-9, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Several compiler-directed cache coherence schemes <ref> [10, 12, 13, 14, 18, 21, 29, 30] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> This approach has a long history of predecessors, including C.mmp [38], IBM's RP3 [6], Illinois Cedar [27], and several recently proposed schemes <ref> [10, 14, 12, 13, 18, 21, 29, 30] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf 27 microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [30] <author> S. L. Min and J.-L. Baer. </author> <title> A Timestamp-based Cache Coherence Scheme. </title> <booktitle> Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <address> I:23-32, </address> <year> 1989. </year>
Reference-contexts: Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Several compiler-directed cache coherence schemes <ref> [10, 12, 13, 14, 18, 21, 29, 30] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> This approach has a long history of predecessors, including C.mmp [38], IBM's RP3 [6], Illinois Cedar [27], and several recently proposed schemes <ref> [10, 14, 12, 13, 18, 21, 29, 30] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf 27 microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [31] <author> S. L. Min and J.-L. Baer. </author> <title> Design and Analysis of a Scalable Cache Coherence Scheme Based on Clocks and Timestamps. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(1) </volume> <pages> 25-44, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: It is a hardware approach with strong compiler support. We call them hardware-supported compiler-directed (HSCD) coherence schemes, which is distinctly different from a pure hardware directory scheme and a pure software scheme. Several studies have compared the performance of directory schemes and some recent HSCD schemes. Min and Baer <ref> [31] </ref> compared the performance of a directory scheme and a timestamp-based scheme assuming infinite cache size and single-word cache lines. Lilja [28] compared the performance of the version control scheme [13] with directory schemes, and analyzed the directory overhead of several implementations.
Reference: [32] <author> J. M. Mulder, N. T. Quach, and M. J. Flynn. </author> <title> An area model for on-chip memories and its application. </title> <journal> Journal of Solid State Circuits, </journal> <volume> 26 </volume> <pages> 98-106, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: unit size since each access unit (such as a character, integer, or floating point data) is a distinct variable analyzed by the compiler. 8 Our experimental results in Section 4 show that a 4-bit or 8-bit timetag is large enough to achieve a very good performance. 9 As noted by <ref> [32, 39] </ref>, the tag overhead in the design of the on-chip caches are significant, occupying area comparable to the data portion, especially for set-associative caches with 64-bit addresses. 12 Full-map LimitLess Directory Two-phase Directory [8] (bits) (DIR NB i )[2] (bits) Invalidation (bits) Cache Overhead (SRAM) 2*C*P 2*C*P 8*L*C*P Memory Overhead
Reference: [33] <author> D. A. Padua, R. Eigenmann, J. Hoeflinger, P. Peterson, P. Tu, S. Weatherford, and K. Faign. </author> <title> Polaris: A New-Generation Parallelizing Compiler for MPPs. In CSRD Rept. No. </title> <type> 1306. </type> <institution> Univ. of Illinois at Urbana-Champaign., </institution> <month> June, </month> <year> 1993. </year>
Reference-contexts: The cost of the required hardware support is small and proportional to the cache size. To study the compiler analysis techniques for the proposed scheme, we have implemented the compiler algorithms on the Polaris parallelizing compiler <ref> [33] </ref>. By performing execution-driven simulations on the Perfect Club Benchmarks, we evaluate the performance of our scheme compared to a hardware directory scheme. In Section 2, we describe an overview of our cache coherence scheme and discuss our compiler analysis techniques and their implementation. <p> The details of compiler algorithms are beyond the scope of this paper and are described in [15, 16]. Compiler implementation All the compiler algorithms explained earlier have been implemented on the Polaris parallelizing compiler <ref> [33] </ref>. Figure 6 shows the flowchart of our reference marking algorithm. First, we construct a procedure call graph. Then, based on the bottom-up scan of the call graph, we analyze each procedure and propagate its side effects to its callers. The per-procedure analysis is based on the following steps. <p> The cost of the required hardware support is minimal and proportional to the cache size. The necessary compiler algorithms, including intra- and interprocedural array data flow analysis, have been implemented on the Polaris paralleling compiler <ref> [33] </ref>. The results of our simulation study using the Perfect Club Benchmarks [5] show that both hardware directory schemes and the TPI scheme have comparable number of unnecessary cache misses.
Reference: [34] <author> D. K. Poulsen and P.-C. Yew. </author> <title> Execution-Driven Tools for Parallel Simulation of Parallel Architectures and Applications. </title> <booktitle> Proceedings of the Supercomputing 93, </booktitle> <pages> pages 860-869, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: After compiler marking, we instrument the benchmarks to generate memory events, which are used for the simulation. Figure 8 shows the experimentation tools used in our simulations. Simulation Execution-driven simulations <ref> [34] </ref> are used to verify the compiler algorithms and to evaluate the performance of our proposed coherence scheme. All the simulations assume a 16-processor, physically distributed shared-memory multiprocessor.
Reference: [35] <author> J. Torrellas, M. S. Lam, and J. L. Hennessy. </author> <title> False Sharing and Spatial Locality in Multiprocessor Caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-43(6):651-663, </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: On the other hand, code generation, such as address calculation, will increase the frequency of private references due to spill code. Shared data consistency prevents optimization of the references to the shared data <ref> [35] </ref>. 14 Cache and system organization CPU single-issue processor cache size on-chip 64 KB, direct-mapped on epoch counter overflow two-phase reset mechanism line size 4 32-bit word cache line timetag size 8-bits number of processors 16 Cache and system organization ALU operations 1 CPU cycle cache hit 1 CPU cycle base
Reference: [36] <author> D. M. Tullsen and S. J. Eggers. </author> <title> Limitations of Cache Prefetching on a Bus-Based Multiprocessors. </title> <booktitle> Proceedings of The 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 278-288, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The true sharing misses are necessary to satisfy cache coherence, while false sharing misses are unnecessary misses resulting from lack of compile time information or the false-sharing effect of hardware protocols. False sharing misses are identified during simulations using the method in <ref> [36] </ref>. If an invalidation is caused by an access to a word that the local processor had not used since getting the block into its cache, then it is a false sharing invalidation. Any subsequent invalidation miss on this block will be counted also as a false sharing miss.
Reference: [37] <author> A. V. Veidenbaum. </author> <title> A Compiler-Assisted Cache Coherence Solution for Multiprocessors. </title> <booktitle> Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 1029-1035, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: A set of perfectly-nested parallel loops is considered as a single epoch. Multiple epochs may occur due to intervening code in non-perfectly-nested parallel loops. Figure 1 shows a parallel program and its corresponding epochs. Stale reference sequence The following sequence of events creates a stale reference at runtime <ref> [37] </ref>: (1) Processor P i reads or writes to memory location x at time T 1 , and brings a copy of x in its cache; (2) Another processor P j (j 6= i) writes to x later at time T 2 (&gt; T 1 ), and 3 creates a new
Reference: [38] <author> C. G. Bell W. A. Wulf. </author> <title> C.mmp amulti-mini processor. </title> <booktitle> Proceedings of the Fall Joint Computer Conference, </booktitle> <pages> pages 765-777, </pages> <month> December </month> <year> 1972. </year>
Reference-contexts: They, instead, provide software mechanisms while relying mostly on users to maintain data coherence either through language extensions or message-passing paradigms. In several early multiprocessor systems, such as the CMU C.mmp <ref> [38] </ref>, the NYU Ultracom-puter [23], the IBM RP3 [6], and the Illinois Cedar [27], compiler-directed techniques were used to solve the cache coherence problem. In this approach, cache coherence is maintained locally without the need for interprocessor communication or hardware directories. <p> In this paper, we investigated a hardware-supported, compiler-directed (HSCD) cache coherence scheme, called the two-phase invalidation (TPI) scheme which relies mostly on compiler analysis, yet also provides a reasonable amount of hardware support. This approach has a long history of predecessors, including C.mmp <ref> [38] </ref>, IBM's RP3 [6], Illinois Cedar [27], and several recently proposed schemes [10, 14, 12, 13, 18, 21, 29, 30]. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf 27 microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [39] <author> H. Wang, T. Sun, and Q. Yang. </author> <title> CAT Caching Address Tags: A Technique for Reducing Area Cost of On-Chip Caches. </title> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 381-390, </pages> <month> June </month> <year> 1995. </year> <month> 30 </month>
Reference-contexts: unit size since each access unit (such as a character, integer, or floating point data) is a distinct variable analyzed by the compiler. 8 Our experimental results in Section 4 show that a 4-bit or 8-bit timetag is large enough to achieve a very good performance. 9 As noted by <ref> [32, 39] </ref>, the tag overhead in the design of the on-chip caches are significant, occupying area comparable to the data portion, especially for set-associative caches with 64-bit addresses. 12 Full-map LimitLess Directory Two-phase Directory [8] (bits) (DIR NB i )[2] (bits) Invalidation (bits) Cache Overhead (SRAM) 2*C*P 2*C*P 8*L*C*P Memory Overhead
References-found: 39

