URL: http://www.media.mit.edu/~nitin/papers/Espace2_Assets96.ps
Refering-URL: http://www.media.mit.edu/~nitin/papers/publications.html
Root-URL: http://www.media.mit.edu
Email: nitin@cc.gatech.edu, arthur.murphy@arch.gatech.edu  
Title: Designing Audio Environments Not Audio Interfaces  
Author: Nitin Nick Sawhney and Arthur Murphy 
Address: Atlanta, GA 30332-0165 USA  
Affiliation: Graphics, Visualization, and Usability Center School of Literature, Communication, and Culture The Georgia Institute of Technology  
Abstract-found: 0
Intro-found: 1
Reference: [Aro91] <author> Barry Arons. Hyperspeech: </author> <title> Navigating in speech-only hypermedia. </title> <booktitle> Hypertext '91 Proceedings, </booktitle> <month> December </month> <year> 1991. </year>
Reference-contexts: The actual hypertext content of the WWW is not made accessible in this prototype. Even though a text-to-speech reader could be added to the browser, the system would still function as an audio interface to a visual browser, and would not provide efficient access for the visually-impaired. Hyperspeech <ref> [Aro91] </ref> is a speech-only hypermedia application that uses speech recognition to maneuver in a database of digitally recorded speech segments without a visual display. Recorded audio interviews were segmented by topic, and hypertext-style links were manually added to connect logically related comments and ideas. <p> Interactive Audio Skimming It is claimed that speech exists only temporally - the ear cannot browse around a set of recordings the way the eye can scan a screen of text and images <ref> [Aro91] </ref>. Yet speech could be controlled by interfaces that permit faster scanning of speech and indicate (aurally) the length and depth of speech nodes. The user must have full control over the playback of the audio recordings, not unlike the tape transport controls on modern audio/video playback devices.
Reference: [Aro93] <author> Barry Arons. SpeechSkimmer: </author> <title> Interactively Skimming Recorded Speech. </title> <booktitle> Proceedings of UIST 93, </booktitle> <publisher> ACM Press, </publisher> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: The impending end of a sentence could also be exaggerated by artificially lowering the pitch of the recording. Pitch correction techniques can be utilized to ensure that speedup playback still produces intelligible output <ref> [Aro93] </ref>. Hyperlinked Audio Navigation All audio content can be conceived of as nodes within a hypertextual framework. Audio nodes can be grouped within other abstract containers and links between the audio content of individual nodes can be established.
Reference: [Bre90] <author> A.S. Bregman. </author> <title> Auditory Scene Analysis: The perceptual organization of sound. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Dynamic Audio Streaming The "cocktail-party effect" provides the justification that humans can in fact monitor several audio streams simultaneously, selectively focusing on any one and placing the rest in the background <ref> [Bre90] </ref>. Multiple streams of simultaneous audio can be used in audio environments to present prerecorded and live broadcast information content, permitting the user to attentively listen to any one, while being aware of changes in the other streams.
Reference: [BVM93] <author> Jay David Bolter, J.C. Verlinden, and C. van der Mast. </author> <title> Voice annotation in virtual environments. </title> <type> GIT GVU Technical Report, </type> <year> 1993. </year>
Reference-contexts: Hyperspeech was not designed for effective use by the visually-impaired, and provided only a constrained form of navigation of hyperlinked audio content. Navigation via exploration was demonstrated in a research project at Georgia Techs GVU Lab <ref> [BVM93] </ref>, where voice annotations were utilized in virtual environments, to augment existing 3D simulations. When a visual marker in the VR environment was activated its recorded audio contents were presented, while the user is still navigating and interacting with other parts of the virtual environment.
Reference: [Coh93] <author> Jonathan Cohen. </author> <title> Monitoring background activities. Auditory Display: Sonification, Audification, and Auditory Interfaces. </title> <address> Reading MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Sound textures can be specially designed or algorithmically generated to deliver a perception of object persistence or subtle changes in object attributes. Contextual Awareness Continuous audio can also indicate the presence of background activity <ref> [Coh93] </ref> or the sense of location within an audio environment. Ambient textures or looped musical sequences can be associated as unique signature sounds to specific audio spaces or container objects.
Reference: [CMK88] <author> John M. Carroll, Robert L. Mack, and Wendy A. Kellog. </author> <title> Interface Metaphors and User Interface Design. Handbook of Human-Computer Interaction, </title> <editor> M. Helander (ed), </editor> <publisher> Elsevier Science Publishers B.V. (North Holland), </publisher> <year> 1988. </year>
Reference-contexts: These metaphors must be evaluated on the basis of how well the metaphor is known, abstractness, transparency, clarity, richness, and scope <ref> [CMK88] </ref>. The metaphor must permit an abstract representation of system functionality in a unified and unambiguous manner. This is a difficult task, and although several approaches are possible, no one approach may be suitable for all applications.
Reference: [DAHC86] <author> Jr. D. Austin Henderson and Stuart K. Card. Rooms. </author> <title> The use of multiple virtual workspaces to reduce space contention in a windows-based graphical user interface. </title> <journal> ACM Transactions on Graphics, </journal> <pages> pp. 211-243, </pages> <month> July </month> <year> 1986. </year> <month> 8 </month>
Reference-contexts: This is a difficult task, and although several approaches are possible, no one approach may be suitable for all applications. Audio Rooms The concept of rooms was first developed for use in 3D graphical systems at Xerox PARC <ref> [DAHC86] </ref>. Recently the rooms metaphor have been described in the context of audio interfaces [ME91] and has been utilized in several audio-related projects [LR95] [SS95] [SME95]. Here each room serves as a container for grouping some related applications or data.
Reference: [Die95] <author> Andreas Dieberger. </author> <title> Magic Features in (Spatial) Metaphors. </title> <journal> SigLink Newsletter, </journal> <volume> Volume 4(3), </volume> <month> December </month> <year> 1995. </year>
Reference-contexts: Users transition between rooms via doorways, yet it can be a tedious task to find information if a user must go through several doorways to get to it. There must be some capability for users to transport themselves into different rooms via links or keywords. Specially designed magic features <ref> [Die95] </ref> in the interface, such as a transporter (from Star Trek), could add useful functionality to rooms, at the expense of intentionally breaking the original rooms metaphor.
Reference: [EMS94] <author> W. Keith Edwards, Elizabeth D. Mynatt, and Kathryn Stockton. </author> <title> Providing Access to Graphical User Interfaces - Not Graphical Screens. </title> <booktitle> ACM Proceedings on ASSETS 94, </booktitle> <month> November </month> <year> 1994. </year>
Reference: [Gav89] <author> William W. Gaver. </author> <title> The Sonic Finder: An interface that uses auditory icons. </title> <booktitle> Human Computer Interaction, </booktitle> <volume> 4 </volume> <pages> 67-94, </pages> <year> 1989. </year>
Reference: [ME91] <author> Elizabeth D. Mynatt and W. Keith Edwards. </author> <title> New metaphors for nonvisual interfaces. In Extraordinary Human-Computer Interaction, </title> <year> 1991. </year>
Reference-contexts: Audio Rooms The concept of rooms was first developed for use in 3D graphical systems at Xerox PARC [DAHC86]. Recently the rooms metaphor have been described in the context of audio interfaces <ref> [ME91] </ref> and has been utilized in several audio-related projects [LR95] [SS95] [SME95]. Here each room serves as a container for grouping some related applications or data. Users may place objects in these rooms according to their own preferences.
Reference: [MB95] <author> Michael C. Albers and Eric Bergman. </author> <title> The Audible Web: Auditory Enhancements for Mosaic. </title> <booktitle> Proceedings of CHI 95, </booktitle> <pages> pp. 318-319, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Later in this paper we will consider a lte r na tiv e a ppr o ac h es ba s ed o n a u dio environments. Access to Hyperlinked Information The Audible Web <ref> [MB95] </ref> is a recent prototype that adds auditory cues to Mosaic (a WorldWide-Web browser) to aid monitoring of data transfer in progress, provide feedback for user actions and provide content feedback to aid navigation of the WWW.
Reference: [LR95] <institution> Mauricio Lumbreras and Gustavo Rossi. </institution>
Reference-contexts: A spatial sound system can provide a strong spatial metaphor by placing individual voices in particular spatial locations or allowing the user to create a personalized map of the conversations in space. 3D spatialization has been utilized in several applications [SME95] <ref> [LR95] </ref> for presenting live conversations or recorded audio sources around a listener. <p> Audio Rooms The concept of rooms was first developed for use in 3D graphical systems at Xerox PARC [DAHC86]. Recently the rooms metaphor have been described in the context of audio interfaces [ME91] and has been utilized in several audio-related projects <ref> [LR95] </ref> [SS95] [SME95]. Here each room serves as a container for grouping some related applications or data. Users may place objects in these rooms according to their own preferences.
References-found: 13

