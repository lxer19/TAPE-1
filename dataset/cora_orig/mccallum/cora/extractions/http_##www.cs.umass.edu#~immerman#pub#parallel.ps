URL: http://www.cs.umass.edu/~immerman/pub/parallel.ps
Refering-URL: http://www.cs.umass.edu/~immerman/pub_immerman.html
Root-URL: 
Title: EXPRESSIBILITY AND PARALLEL COMPLEXITY  
Author: NEIL IMMERMAN 
Keyword: Key words. Computational complexity, parallel complexity, first-order express-ibility, polynomial-time hierarchy  
Date: 18 (1989), 625-638.  
Note: SIAM J. of Comput.  AMS(MOS) subject classification. 68025  
Abstract: It is shown that the time needed by a concurrent-read, concurrent-write parallel random access machine (CRAM) to check if an input has a certain property is the same as the minimal depth of a first-order inductive definition of the property. This in turn is equal to the number of "iterations" of a first-order sentence needed to express the property. The second contribution of this paper is the introduction of a purely syntactic uniformity notion for circuits. It is shown that an equivalent definition for the uniform circuit classes AC i ; i 1 is given by first-order sentences "iterated" log i n times. Similarly, uniform AC 0 is defined to be the first-order expressible properties (which in turn is equal to constant time on a CRAM by our main theorem). A corollary of our main result is a new characterization of the Polynomial-Time Hierarchy (PH): PH is equal to the set of languages accepted by a CRAM using exponentially many processors and constant time. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. M. BARRINGTON, N. IMMERMAN, AND H. STRAUBING, </author> <booktitle> On uniformity within NC 1 , Proc. 3rd Annual Symposium on Structure in Complexity Theory (1988), </booktitle> <pages> pp. 47-59. </pages>
Reference-contexts: Define the complexity class FO to be the set of all first-order expressible problems. We will see in x5 that FO is a uniform version of the circuit class AC 0 . (See also <ref> [1] </ref> where it is shown that FO is equal to deterministic log time uniform AC 0 .) Example 2.1 An example of a first-order expressible property is addition. 2 In order to turn addition into a yes/no question, we can let our input have the vocabulary t a = hA; B; <p> The formula ' contains d alternating blocks of quantifiers. In view of the above results, we propose the following: Definition 5.5 Let (uniform) AC 0 def = FO <ref> [1] </ref> = CRAM [1] . Since we first made this suggestion, much evidence concerning the appropriateness of Definition 5.5 has appeared. In particular, see [1] for a study of low-level uniformity. It is shown there that FO is equal to deterministic log time uniform AC 0 . <p> The formula ' contains d alternating blocks of quantifiers. In view of the above results, we propose the following: Definition 5.5 Let (uniform) AC 0 def = FO <ref> [1] </ref> = CRAM [1] . Since we first made this suggestion, much evidence concerning the appropriateness of Definition 5.5 has appeared. In particular, see [1] for a study of low-level uniformity. It is shown there that FO is equal to deterministic log time uniform AC 0 . <p> In view of the above results, we propose the following: Definition 5.5 Let (uniform) AC 0 def = FO <ref> [1] </ref> = CRAM [1] . Since we first made this suggestion, much evidence concerning the appropriateness of Definition 5.5 has appeared. In particular, see [1] for a study of low-level uniformity. It is shown there that FO is equal to deterministic log time uniform AC 0 . In [11] we introduced the notion of first-order translations. <p> characterization of PH as a parallel complexity class: Corollary 6.3 PH is equal to the set of properties checkable by a CRAM using exponentially many processors and constant time 9 : PH = k=1 CRAM <ref> [1] </ref>-PROC [2 n k ] : Proof The inclusion SO CRAM [1]-PROC [2 n O [1] ] follows just as in the proof of Lemma 3.2. A processor number is now large enough to give values to all the relational variables as well as to all the first-order variables. <p> A processor number is now large enough to give values to all the relational variables as well as to all the first-order variables. Thus, as in Lemma 3.2, the CRAM can evaluate each first or second-order quantifier in three steps. The inclusion CRAM <ref> [1] </ref>-PROC [2 n O [1] ] SO follows just as in the proof of Lemma 3.1.
Reference: [2] <author> P. BEAME, </author> <title> Limits on the power of concurrent-write parallel machines, </title> <booktitle> Proc. 18th ACM Symposium on Theory of Computing (1986), </booktitle> <pages> pp. 169-176. </pages>
Reference-contexts: I (n)= log n words of memory contain log n bits each of the input, or even if all I (n) bits are placed in the first word, then all our results remain unchanged. Note that this is not true of the models used in <ref> [2] </ref>, for example.
Reference: [3] <author> A. CHANDRA, L. STOCKMEYER, AND U. VISHKIN, </author> <title> Constant depth reducibility, </title> <journal> SIAM J. of Comput. </journal> <volume> 13 (1984), </volume> <pages> pp. 423-439. </pages>
Reference-contexts: Of those we have considered, the answer is yes, with the following interesting exception. (The UGAP problem is the set of undirected graphs for which there exists a path from vertex 0 to vertex n 1.) Fact 5.6 <ref> [3] </ref>. UGAP is nonuniform AC 0 reducible to UNDIR-CYCLE. Now UNDIR-CYCLE is in DSPACE [log n] [8], but UGAP is not known to be in DSPACE [log n]. Of course, Remark 5.7 If UGAP is uniform AC 0 reducible to UNDIR-CYCLE, then UGAP is in DSPACE [log n]. <p> Now UNDIR-CYCLE is in DSPACE [log n] [8], but UGAP is not known to be in DSPACE [log n]. Of course, Remark 5.7 If UGAP is uniform AC 0 reducible to UNDIR-CYCLE, then UGAP is in DSPACE [log n]. We mention one more interesting justification of Definition 5.5. In <ref> [3] </ref> it is shown that the obvious bounds, nonuniform NC i nonuniform AC i can be improved to Fact 5.8 ([3]) nonuniform NC i nonuniform AC-DEPTH [log i n= log log n] : When i = 1 this bound is optimal because nonuniform AC-DEPTH [log n= log log n] is necessary
Reference: [4] <author> H. ENDERTON, </author> <title> A Mathematical Introduction to Logic, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: In x7 we give some suggestions for future work in this area. 2 Background and definitions 2.1 First-order logic We begin this section by making some precise definitions concerning first-order logic. For more information see <ref> [4] </ref>. A vocabulary t = hR a 1 1 ; ; R k ; c 1 ; ; c r i is a tuple of relation symbols and constant symbols. R a i i is a relation symbol of arity a i . <p> In [11] we introduced the notion of first-order translations. These reductions consist of a fixed first-order formula translating all instances of one problem to instances of another. (First-order translations are interpreta tions between theories, cf. <ref> [4] </ref>, that are also reductions in the complexity 8 In [17] Stockmeyer and Vishkin showed that nonuniform AC 0 is equal to constant time on a nonuniform CRAM.
Reference: [5] <author> R. FAGIN, </author> <title> Generalized first-order spectra and polynomial-time recognizable sets, in Complexity of Computation, </title> <editor> R. Karp, ed., </editor> <booktitle> SIAM-AMS Proc., 7 (1974), </booktitle> <pages> pp. 27-41. </pages>
Reference: [6] <author> F. FICH, P. RAGDE, AND A. WIGDERSON, </author> <title> Relations between concurrent-write models of parallel computation, </title> <booktitle> Proc. 3rd Annual 20 ACM Symposium on Principles of Distributed Computing (1984), </booktitle> <pages> pp. 179-189. </pages>
Reference-contexts: The factor of two would be eliminated if we adopted a weaker parallel machine model allowing only common writes 7 , and such that the memory location accessed by a processor could be determined by a very simple computation on the processor number. 7 See <ref> [6] </ref> for an earlier proof that a common write machine can simulate a CRAM with a linear increase in time and a squaring of the number of processors. 14 The additional two variables arise for various bookkeeping reasons.
Reference: [7] <author> D. HAREL AND D. KOZEN, </author> <title> A programming language for the inductive sets, </title> <booktitle> and applications, Proc. 9th International Colloquium on Automata Languages, and Programming, Springer-Verlag Lecture Notes in Computer Science, 140 (1982), </booktitle> <pages> pp. 313-329. </pages>
Reference-contexts: Let IND-DEPTH [t (n)] be the set of problems expressible as a uniform induction that requires depth of recursion at most t (n) for structures of size n. In <ref> [7] </ref>, Harel and Kozen introduce a programming language called IND, which is closely tied to inductive definitions. They prove that the execution time for their IND programs is equal to the depth of the inductive definitions that describe the programs' input output behavior.
Reference: [8] <author> J.-W. HONG, </author> <title> On some deterministic space complexity problems, </title> <journal> SIAM J. Comput., </journal> <volume> 11 (1982), </volume> <pages> pp. 591-601. </pages>
Reference-contexts: UGAP is nonuniform AC 0 reducible to UNDIR-CYCLE. Now UNDIR-CYCLE is in DSPACE [log n] <ref> [8] </ref>, but UGAP is not known to be in DSPACE [log n]. Of course, Remark 5.7 If UGAP is uniform AC 0 reducible to UNDIR-CYCLE, then UGAP is in DSPACE [log n]. We mention one more interesting justification of Definition 5.5.

Reference: [14] <author> Y.N.MOSCHOVAKIS, </author> <title> Elementary Induction on Abstract Structures, </title> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1974. </year>
Reference-contexts: we let ' n = [QB] t (n) M 0 , for n = 1; 2; , then for all structures G of vocabulary t with jGj = n, G 2 C , G j= ' n : A more traditional way to iterate formulas is by making inductive definitions, <ref> [14] </ref>, [10]. Let IND-DEPTH [t (n)] be the set of problems expressible as a uniform induction that requires depth of recursion at most t (n) for structures of size n. In [7], Harel and Kozen introduce a programming language called IND, which is closely tied to inductive definitions. <p> In the remainder of this paper we write IND [t (n)] to signify IND-TIME [t (n)] as well as IND-DEPTH [t (n)]. The following fact relates IND [t (n)] to FO [t (n)]. This fact follows easily from Moschovakis' Canonical Form for Positive Formulas, <ref> [14] </ref>. Fact 2.4 ([10], [14]) For all t (n), IND [t (n)] FO [t (n)] : (In particular, a property in IND [t (n)] is expressible as a FO [t (n)] property in which M 0 false, cf. <p> In the remainder of this paper we write IND [t (n)] to signify IND-TIME [t (n)] as well as IND-DEPTH [t (n)]. The following fact relates IND [t (n)] to FO [t (n)]. This fact follows easily from Moschovakis' Canonical Form for Positive Formulas, <ref> [14] </ref>. Fact 2.4 ([10], [14]) For all t (n), IND [t (n)] FO [t (n)] : (In particular, a property in IND [t (n)] is expressible as a FO [t (n)] property in which M 0 false, cf.
Reference: [15] <author> L. RUZZO, </author> <title> On uniform circuit complexity, </title> <journal> J. Comput. System. Sci., </journal> <volume> 21 (1981), </volume> <pages> pp. 365-383. </pages>
Reference-contexts: Until now, a principal unaesthetic feature of the theory of complexity via boolean circuits was that one had resorted to Turing machines to define 2 the uniformity conditions for circuits <ref> [15] </ref>. As a corollary to Theorem 1.1, we obtain a purely syntactic uniformity notion for circuits. In x5 we describe this result as well as other relations between circuits and first-order complexity.
Reference: [16] <author> L. STOCKMEYER, </author> <title> The polynomial-time hierarchy, </title> <journal> Theoretical Com-put. Sci., </journal> <volume> 3 (1977), </volume> <pages> pp. 1-22. </pages>
Reference: [17] <author> L. STOCKMEYER AND U. VISHKIN, </author> <title> Simulation of parallel random access machines by circuits, </title> <journal> SIAM J. of Comput. </journal> <volume> 13 (1984), </volume> <pages> pp. 409-422. </pages>
Reference-contexts: We now state our main result. (See x2 for relevant definitions. In particular, the iteration of a first-order sentence is defined in x2.2, and the CRAM is defined in x2.3. The definition of the CRAM differs from the standard definition of the CRCW PRAM in <ref> [17] </ref> only in that a processor may shift a word of local memory by any polynomial number of bits in unit time. <p> For t (n) log n, the equivalence of (1) and (2) in Theorem 1.1 may also be obtained by combining a result of Ruzzo and Tompa relating CRAMs to alternating Turing machines <ref> [17, Thm. 3] </ref>, together with a result of ours relating alternating Turing machines to first-order expressibility [9, Thm. B.4]. <p> ^ B (y) ^ (8z:y &lt; z &lt; x)A (z) _ B (z)] Then with standing for exclusive or, we can express PLUS, PLUS (x) A (x) B (x) CARRY (x) Thus the sentence expressing the addition property is PLUS (k). 2 2 This is a standard construction, see e.g., <ref> [17] </ref>. 4 2.2 Iterating first-order sentences To describe properties that are not in AC 0 , we need languages that are more expressive than FO. We now recall the definition of the complexity classes FO [t (n)] 3 . <p> follows that P n (x; y) [QB] d1+logne (false) ; and thus E fl 2 FO [log n] as claimed. 2 2.3 Concurrent random access machines We define the concurrent random access machine (CRAM) to be essentially the concurrent read, concurrent write parallel random access machine (CRCW PRAM) described in <ref> [17] </ref>. A CRAM is a synchronous parallel machine such that any number of processors may read or write into any word of global memory at any step. <p> Each processor also has a local register containing its processor number. The difference between the CRAM and the CRCW PRAM described in <ref> [17] </ref> is that we also include a Shift instruction. Shift (x; y) causes the word x to be shifted y bits to the right. Without Shift, CRAM [t (n)] would be too weak to simulate FO [t (n)] for t (n) &lt; log n. <p> In [11] we introduced the notion of first-order translations. These reductions consist of a fixed first-order formula translating all instances of one problem to instances of another. (First-order translations are interpreta tions between theories, cf. [4], that are also reductions in the complexity 8 In <ref> [17] </ref> Stockmeyer and Vishkin showed that nonuniform AC 0 is equal to constant time on a nonuniform CRAM. This, together with Fact 5.4, gives a nonuniform version of Theorem 1.1. 16 theoretic sense.) It follows from Definition 5.5 that first-order translations are exactly uniform AC 0 reductions.
Reference: [18] <author> A. C.-C. YAO, </author> <title> Separating the polynomial-time hierarchy by oracles, </title> <booktitle> Proc. 26th Annual IEEE Symposium on Foundations of Comput. Sci. </booktitle> <year> (1985), </year> <pages> pp. 1-10. </pages>
Reference-contexts: shown that the obvious bounds, nonuniform NC i nonuniform AC i can be improved to Fact 5.8 ([3]) nonuniform NC i nonuniform AC-DEPTH [log i n= log log n] : When i = 1 this bound is optimal because nonuniform AC-DEPTH [log n= log log n] is necessary for Parity <ref> [18] </ref>. We next show that the same bound holds in the uniform case: Theorem 5.9 For t (n) log n, ASPACE [log n]TIME [t (n)] FO [t (n)= log log n] : Proof This is a log log n factor improvement of Theorem B.3 in [9].
References-found: 13

