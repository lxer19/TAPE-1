URL: http://www.cis.udel.edu/~case/papers/journal-nfex.ps
Refering-URL: http://www.cis.udel.edu/~case/colt.html
Root-URL: http://www.cis.udel.edu
Email: case@cis.udel.edu  sanjay@iscs.nus.sg  fstephan@ira.uka.de  
Title: Vacillatory and BC Learning on Noisy Data  
Author: John Case Sanjay Jain Frank Stephan 
Date: March 2, 1997  
Address: 19716, USA  Singapore 119260 Republic of Singapore  Im Neuenheimer Feld 294 Ruprecht-Karls-Universitat Heidelberg 69120 Heidelberg  
Affiliation: Computer and Information Sciences Department University of Delaware Newark, DE  Department of Information Systems and Computer Science National University of Singapore  Mathematisches Institut  
Abstract: In an earlier paper, Frank Stephan introduced a form of noisy data which nonetheless uniquely determines the true data: correct information occurs infinitely often while incorrect information occurs only finitely often. The present paper considers the effects of this form of noise on vacillatory and behaviorally correct learning of grammars | both from positive data alone and from informant (positive and negative data). For learning from informant, the noise, in effect, destroys negative data. Various noisy-data hierarchies are exhibited, which, in some cases, are known to collapse when there is no noise. Noisy behaviorally correct learning is shown to obey a very strong "subset principle". It is shown, in many cases, how much power is needed to overcome the effects of noise. For example, the best we can do to simulate, in the presence of noise, the noise-free, no mind change cases takes infinitely many mind changes. One technical result is proved by a priority argument. 
Abstract-found: 1
Intro-found: 1
Reference: [Ang80] <author> D. Angluin. </author> <title> Inductive inference of formal languages from positive data. </title> <journal> Information and Control, </journal> <volume> 45 </volume> <pages> 117-135, </pages> <year> 1980. </year>
Reference-contexts: Theorem 5 implies a very strong subset principle on noisy behaviorally correct learning from positive information only. It is stronger than that from Angluin's characterization <ref> [Ang80] </ref> of (uniformly decidable classes) learnable Ex style, with no noise, and positive information only. <p> The following theorem (Theorem 5) provides a very strong subset principle on NoisyTxtBc a , stronger than that from Angluin's characterization <ref> [Ang80] </ref> of (uniformly decidable classes in) TxtEx. 4 Even at the TxtBc levels, noise is problematic.
Reference: [AS83] <author> D. Angluin and C. Smith. </author> <title> A survey of inductive inference: Theory and methods. </title> <journal> Computing Surveys, </journal> <volume> 15 </volume> <pages> 237-289, </pages> <year> 1983. </year>
Reference-contexts: In particular he considered a machine, which reads more and more positive information on an r.e. set and produces in the limit a grammar to generate this set. This is called Ex style identification. From then on many variants of this concept have been considered <ref> [AS83, BB75, CL82, OSW86] </ref>. 1 Barzdin [Bar74] and Case and Smith [CS83] considered the notion of behaviorally correct in-ference which is motivated by the fact that no algorithm can check the equivalence of grammars.
Reference: [Bar74] <author> J. M. Barzdin. </author> <title> Two theorems on the limiting synthesis of functions. In Theory of Algorithms and Programs, Latvian State University, </title> <journal> Riga, </journal> <volume> 210 </volume> <pages> 82-88, </pages> <year> 1974. </year> <note> In Russian. </note>
Reference-contexts: This is called Ex style identification. From then on many variants of this concept have been considered [AS83, BB75, CL82, OSW86]. 1 Barzdin <ref> [Bar74] </ref> and Case and Smith [CS83] considered the notion of behaviorally correct in-ference which is motivated by the fact that no algorithm can check the equivalence of grammars. <p> We call each instance of ? 6= M (I [n]) 6= M (I [n + 1]) as a mind change by M on I. * <ref> [Bar74, CS83] </ref>. M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. InfBc a -identification is defined similarly. * [Cas88, Cas96, Cas97, BP73].
Reference: [BB75] <author> L. Blum and M. Blum. </author> <title> Toward a mathematical theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 28 </volume> <pages> 125-155, </pages> <year> 1975. </year>
Reference-contexts: In particular he considered a machine, which reads more and more positive information on an r.e. set and produces in the limit a grammar to generate this set. This is called Ex style identification. From then on many variants of this concept have been considered <ref> [AS83, BB75, CL82, OSW86] </ref>. 1 Barzdin [Bar74] and Case and Smith [CS83] considered the notion of behaviorally correct in-ference which is motivated by the fact that no algorithm can check the equivalence of grammars. <p> Convergence on information sequences is defined similarly. Definition 1 (a) Suppose a; b 2 N [ fflg. Below, for each of several learning criteria J , we define what it means for a machine M to J -identify a language L from a text T or informant I. * <ref> [Gol67, CS83, BB75] </ref> M TxtEx a b -identifies L from text T iff (9i : W i = a L)[M (T )# = i] and card (fn : ? 6= M (T [n]) 6= M (T [n + 1])g) b. <p> We call each instance of ? 6= M (T [n]) 6= M (T [n + 1]) as a mind change by M on T . * <ref> [Gol67, CS83, BB75] </ref> M InfEx a b -identifies L from informant I iff (9i : W i = a L)[M (I)# = i] and card (fn : ? 6= M (I [n]) 6= M (I [n + 1])g) b. <p> Lemma 1 (Based on <ref> [BB75] </ref>) Suppose J 2 fTxtEx a ; TxtFex a b ; TxtOex a b ; TxtBc a g. If M J - identifies L then there exists a J -locking sequence for M on L.
Reference: [BCJ96] <author> G. Baliga, J. Case, and S. Jain. </author> <title> Synthesizing enumeration techniques for language learning. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, Desenzano del Garda, Italy, </booktitle> <pages> pages 169-180. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1996. </year>
Reference-contexts: If a 2 N , it follows that L 1 = 2a L 2 ; if a = fl, it follows that L 1 = fl L 2 . 4 This latter subset principle, for preventing overgeneralization, is further discussed, for example, in <ref> [Ber85, Cas96, Cas97, KLHM93, Wex93, BCJ96, ZLK95, KB92] </ref>.
Reference: [BCJS94] <author> G. Baliga, J. Case, S. Jain, and M. Suraj. </author> <title> Machine learning of higher order programs. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 59(2) </volume> <pages> 486-500, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: By Claim 1, g (; l; L [l]) is a grammar for X L ;l = L. It is now easy to verify that L 2 InfEx (M 0 ). It follows that NoisyInfBc InfEx. If one considers the definition of GenEx a b from <ref> [BCJS94] </ref>, then one can show that GenInfEx a 0 NoisyGenInfEx a NoisyInfBc a . Parts (a), (b) of Theorem 16 can then also be proved using the fact that GenInfEx n+1 0 InfBc n 6= ; and GenInfEx 1 0 InfOex fl fl 6= ;.
Reference: [Ber85] <author> R. Berwick. </author> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year> <month> 24 </month>
Reference-contexts: If a 2 N , it follows that L 1 = 2a L 2 ; if a = fl, it follows that L 1 = fl L 2 . 4 This latter subset principle, for preventing overgeneralization, is further discussed, for example, in <ref> [Ber85, Cas96, Cas97, KLHM93, Wex93, BCJ96, ZLK95, KB92] </ref>.
Reference: [BJS96] <author> G. Baliga, S. Jain, and A. Sharma. </author> <title> Learning from multiple sources of inaccurate data. </title> <journal> SIAM Journal of Computing, </journal> <note> 1996. To appear. </note>
Reference-contexts: On the other hand, if one is missing negative information [Cas88, Cas96, Cas97, CJS94] or has suitable complexity constraints [CJS95], then vacillatory inference increases learning power. Many real-world applications of learning or inductive inference have to deal with faulty data, so it is natural to study this phenomenon <ref> [BJS96, FJ96, OSW86] </ref>. Many of these notions of noise have the disadvantage that noisy data does not specify uniquely the object to be learned. Stephan [Ste95] introduced a notion of noise in order to overcome this difficulty: correct information occurs infinitely often while incorrect information occurs only finitely often.
Reference: [Blu67] <author> M. Blum. </author> <title> A machine independent theory of the complexity of recursive functions. </title> <journal> Journal of the ACM, </journal> <volume> 14 </volume> <pages> 322-336, </pages> <year> 1967. </year>
Reference-contexts: W i denotes the domain of ' i . W i is considered as the language enumerated by the i-th program in ' system, and we say that i is a grammar or index for W i . denotes a standard Blum complexity measure <ref> [Blu67] </ref> for the programming system '. W i;s = fx &lt; s : i (x) &lt; sg. L is called a single valued total language iff (8x)(9!y)[hx; yi 2 L]. SVT = fL : L is a single valued total language g.
Reference: [BP73] <author> J. M. Barzdin and K. Podnieks. </author> <title> The theory of inductive inference. </title> <booktitle> In Mathematical Foundations of Computer Science, High Tatras, Czechoslovakia, </booktitle> <pages> pages 9-15, </pages> <year> 1973. </year>
Reference-contexts: So, it turns out, the learner can learn more languages if infinitely many guesses are allowed under the condition that almost all of these guessed grammars generate the same correct set. Barzdin and Podnieks <ref> [BP73] </ref> introduced the notion of vacillatory inference which is a restriction of behaviorally correct inference in the sense that the learner may change its mind infinitely often, but only between finitely many grammars. <p> M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. InfBc a -identification is defined similarly. * <ref> [Cas88, Cas96, Cas97, BP73] </ref>. M TxtFex a b -identifies L from text T iff (9S : card (S) b ^ (8i 2 S)[W i = a L])(8 1 n)[M (T [n]) 2 S]. <p> TxtEx fl 0 n InfFex n fl 6= ;. (b) [CS83] TxtEx n+1 InfEx fl n 6= ;. TxtEx S n InfEx fl n 6= ;. (c) <ref> [BP73, CS83] </ref> InfFex a fl = InfEx a . (d) [Cas88, Cas96, Cas97] TxtFex n+1 TxtFex fl n 6= ;. TxtFex fl S n TxtFex fl n 6= ;. (e) [CS83] InfFex fl fl InfBc. (f ) [CS83] TxtBc n+1 InfBc n 6= ;. <p> This contrasts sharply with the non-noisy case. Fex style criteria, in situations taking into account noise (as here), missing information (as in [Cas88, Cas96, Cas97, CJS94]), or complexity constraints (as in [CJS95]), provide a hierarchy; but, unconstrained, do not (as in <ref> [BP73, CS83] </ref>). Theorem 14 Suppose n 1. (a) (NoisyInfFex n+1 " NoisyTxtFex n+1 ) TxtOex fl n 6= ;. (b) (NoisyInfFex fl " NoisyTxtFex fl ) S n TxtOex fl n 6= ;. Proof. (a) Let Null L = fy : h0; yi 2 Lg.
Reference: [Cas88] <author> J. </author> <title> Case. The power of vacillation. </title> <editor> In D. Haussler and L. Pitt, editors, </editor> <booktitle> Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 133-142. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1988. </year>
Reference-contexts: They showed that, for learning recursive functions, vacillatory inference is not more powerful than Ex style learning in the limit. On the other hand, if one is missing negative information <ref> [Cas88, Cas96, Cas97, CJS94] </ref> or has suitable complexity constraints [CJS95], then vacillatory inference increases learning power. Many real-world applications of learning or inductive inference have to deal with faulty data, so it is natural to study this phenomenon [BJS96, FJ96, OSW86]. <p> Theorem 16 says that behaviorally correct learning from noisy informant can be simulated by Ex style learning from a noise free informant. Hence, for informant data, noise destroys the advantage of behaviorally correct over Ex style learning! If one is missing negative information <ref> [Cas88, Cas96, Cas97, CJS94] </ref> or has suitable complexity 2 constraints [CJS95], then Fex b+1 style learning is more powerful than Fex b . Theorem 14 implies that, one also gets such a hierarchy result for Fex b style learning from noisy informant. <p> M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. InfBc a -identification is defined similarly. * <ref> [Cas88, Cas96, Cas97, BP73] </ref>. M TxtFex a b -identifies L from text T iff (9S : card (S) b ^ (8i 2 S)[W i = a L])(8 1 n)[M (T [n]) 2 S]. <p> TxtEx fl 0 n InfFex n fl 6= ;. (b) [CS83] TxtEx n+1 InfEx fl n 6= ;. TxtEx S n InfEx fl n 6= ;. (c) [BP73, CS83] InfFex a fl = InfEx a . (d) <ref> [Cas88, Cas96, Cas97] </ref> TxtFex n+1 TxtFex fl n 6= ;. TxtFex fl S n TxtFex fl n 6= ;. (e) [CS83] InfFex fl fl InfBc. (f ) [CS83] TxtBc n+1 InfBc n 6= ;. <p> Interestingly, as we see by Corollary 2 to the following theorem (Theorem 14), the hierarchy NoisyInfFex 1 NoisyInfFex 2 : : : NoisyInfFex fl is proper. This contrasts sharply with the non-noisy case. Fex style criteria, in situations taking into account noise (as here), missing information (as in <ref> [Cas88, Cas96, Cas97, CJS94] </ref>), or complexity constraints (as in [CJS95]), provide a hierarchy; but, unconstrained, do not (as in [BP73, CS83]).
Reference: [Cas96] <author> J. </author> <title> Case. </title> <booktitle> The power of vacillation in language learning. Technical Report LP-96-08, Logic, Philosophy and Linguistics Series of the Institute for Logic, Language and Computation, </booktitle> <institution> University of Amsterdam, </institution> <year> 1996. </year>
Reference-contexts: They showed that, for learning recursive functions, vacillatory inference is not more powerful than Ex style learning in the limit. On the other hand, if one is missing negative information <ref> [Cas88, Cas96, Cas97, CJS94] </ref> or has suitable complexity constraints [CJS95], then vacillatory inference increases learning power. Many real-world applications of learning or inductive inference have to deal with faulty data, so it is natural to study this phenomenon [BJS96, FJ96, OSW86]. <p> Theorem 16 says that behaviorally correct learning from noisy informant can be simulated by Ex style learning from a noise free informant. Hence, for informant data, noise destroys the advantage of behaviorally correct over Ex style learning! If one is missing negative information <ref> [Cas88, Cas96, Cas97, CJS94] </ref> or has suitable complexity 2 constraints [CJS95], then Fex b+1 style learning is more powerful than Fex b . Theorem 14 implies that, one also gets such a hierarchy result for Fex b style learning from noisy informant. <p> M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. InfBc a -identification is defined similarly. * <ref> [Cas88, Cas96, Cas97, BP73] </ref>. M TxtFex a b -identifies L from text T iff (9S : card (S) b ^ (8i 2 S)[W i = a L])(8 1 n)[M (T [n]) 2 S]. <p> TxtEx fl 0 n InfFex n fl 6= ;. (b) [CS83] TxtEx n+1 InfEx fl n 6= ;. TxtEx S n InfEx fl n 6= ;. (c) [BP73, CS83] InfFex a fl = InfEx a . (d) <ref> [Cas88, Cas96, Cas97] </ref> TxtFex n+1 TxtFex fl n 6= ;. TxtFex fl S n TxtFex fl n 6= ;. (e) [CS83] InfFex fl fl InfBc. (f ) [CS83] TxtBc n+1 InfBc n 6= ;. <p> TxtFex fl S n TxtFex fl n 6= ;. (e) [CS83] InfFex fl fl InfBc. (f ) [CS83] TxtBc n+1 InfBc n 6= ;. TxtBc fl S n InfBc n 6= ;. (g) <ref> [CL82, Cas96, Cas97] </ref> TxtFex 2n fl TxtBc n . (h) [CL82, Cas96, Cas97] TxtEx 2n+1 0 TxtBc n 6= ;. <p> TxtFex fl S n TxtFex fl n 6= ;. (e) [CS83] InfFex fl fl InfBc. (f ) [CS83] TxtBc n+1 InfBc n 6= ;. TxtBc fl S n InfBc n 6= ;. (g) <ref> [CL82, Cas96, Cas97] </ref> TxtFex 2n fl TxtBc n . (h) [CL82, Cas96, Cas97] TxtEx 2n+1 0 TxtBc n 6= ;. <p> If a 2 N , it follows that L 1 = 2a L 2 ; if a = fl, it follows that L 1 = fl L 2 . 4 This latter subset principle, for preventing overgeneralization, is further discussed, for example, in <ref> [Ber85, Cas96, Cas97, KLHM93, Wex93, BCJ96, ZLK95, KB92] </ref>. <p> (x) 6= f (x + 1)g) = min (W f (0) )g; let L = fL : L represents some f 2 Cg.) (c) Follows from the fact that InfFex fl fl InfBc. (d) The idea is essentially the same as used to prove TxtFex 2n fl TxtBc n from <ref> [CL82, Cas96, Cas97] </ref>. Suppose M is given. M 0 () is defined as follows. Let S be the least n elements in Pos ()W M ();jj (if Pos ()W M ();jj contains less than n elements, then S = Pos ()W M ();jj ). <p> The argument to prove that M 0 NoisyInfBc n -identifies every language NoisyInfFex 2n fl - identified by M is essentially the same as used by <ref> [CL82, Cas96, Cas97] </ref>. We omit the details. (e) For a 2 N [ fflg, let L a = fL : L = a N g. Clearly, L a 2 NoisyInfEx a 0 . <p> Interestingly, as we see by Corollary 2 to the following theorem (Theorem 14), the hierarchy NoisyInfFex 1 NoisyInfFex 2 : : : NoisyInfFex fl is proper. This contrasts sharply with the non-noisy case. Fex style criteria, in situations taking into account noise (as here), missing information (as in <ref> [Cas88, Cas96, Cas97, CJS94] </ref>), or complexity constraints (as in [CJS95]), provide a hierarchy; but, unconstrained, do not (as in [BP73, CS83]).
Reference: [Cas97] <author> J. </author> <title> Case. The power of vacillation in language learning. </title> <journal> SIAM Journal on Computing, </journal> <note> 1997. To appear. </note>
Reference-contexts: They showed that, for learning recursive functions, vacillatory inference is not more powerful than Ex style learning in the limit. On the other hand, if one is missing negative information <ref> [Cas88, Cas96, Cas97, CJS94] </ref> or has suitable complexity constraints [CJS95], then vacillatory inference increases learning power. Many real-world applications of learning or inductive inference have to deal with faulty data, so it is natural to study this phenomenon [BJS96, FJ96, OSW86]. <p> Theorem 16 says that behaviorally correct learning from noisy informant can be simulated by Ex style learning from a noise free informant. Hence, for informant data, noise destroys the advantage of behaviorally correct over Ex style learning! If one is missing negative information <ref> [Cas88, Cas96, Cas97, CJS94] </ref> or has suitable complexity 2 constraints [CJS95], then Fex b+1 style learning is more powerful than Fex b . Theorem 14 implies that, one also gets such a hierarchy result for Fex b style learning from noisy informant. <p> M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. InfBc a -identification is defined similarly. * <ref> [Cas88, Cas96, Cas97, BP73] </ref>. M TxtFex a b -identifies L from text T iff (9S : card (S) b ^ (8i 2 S)[W i = a L])(8 1 n)[M (T [n]) 2 S]. <p> TxtEx fl 0 n InfFex n fl 6= ;. (b) [CS83] TxtEx n+1 InfEx fl n 6= ;. TxtEx S n InfEx fl n 6= ;. (c) [BP73, CS83] InfFex a fl = InfEx a . (d) <ref> [Cas88, Cas96, Cas97] </ref> TxtFex n+1 TxtFex fl n 6= ;. TxtFex fl S n TxtFex fl n 6= ;. (e) [CS83] InfFex fl fl InfBc. (f ) [CS83] TxtBc n+1 InfBc n 6= ;. <p> TxtFex fl S n TxtFex fl n 6= ;. (e) [CS83] InfFex fl fl InfBc. (f ) [CS83] TxtBc n+1 InfBc n 6= ;. TxtBc fl S n InfBc n 6= ;. (g) <ref> [CL82, Cas96, Cas97] </ref> TxtFex 2n fl TxtBc n . (h) [CL82, Cas96, Cas97] TxtEx 2n+1 0 TxtBc n 6= ;. <p> TxtFex fl S n TxtFex fl n 6= ;. (e) [CS83] InfFex fl fl InfBc. (f ) [CS83] TxtBc n+1 InfBc n 6= ;. TxtBc fl S n InfBc n 6= ;. (g) <ref> [CL82, Cas96, Cas97] </ref> TxtFex 2n fl TxtBc n . (h) [CL82, Cas96, Cas97] TxtEx 2n+1 0 TxtBc n 6= ;. <p> If a 2 N , it follows that L 1 = 2a L 2 ; if a = fl, it follows that L 1 = fl L 2 . 4 This latter subset principle, for preventing overgeneralization, is further discussed, for example, in <ref> [Ber85, Cas96, Cas97, KLHM93, Wex93, BCJ96, ZLK95, KB92] </ref>. <p> (x) 6= f (x + 1)g) = min (W f (0) )g; let L = fL : L represents some f 2 Cg.) (c) Follows from the fact that InfFex fl fl InfBc. (d) The idea is essentially the same as used to prove TxtFex 2n fl TxtBc n from <ref> [CL82, Cas96, Cas97] </ref>. Suppose M is given. M 0 () is defined as follows. Let S be the least n elements in Pos ()W M ();jj (if Pos ()W M ();jj contains less than n elements, then S = Pos ()W M ();jj ). <p> The argument to prove that M 0 NoisyInfBc n -identifies every language NoisyInfFex 2n fl - identified by M is essentially the same as used by <ref> [CL82, Cas96, Cas97] </ref>. We omit the details. (e) For a 2 N [ fflg, let L a = fL : L = a N g. Clearly, L a 2 NoisyInfEx a 0 . <p> Interestingly, as we see by Corollary 2 to the following theorem (Theorem 14), the hierarchy NoisyInfFex 1 NoisyInfFex 2 : : : NoisyInfFex fl is proper. This contrasts sharply with the non-noisy case. Fex style criteria, in situations taking into account noise (as here), missing information (as in <ref> [Cas88, Cas96, Cas97, CJS94] </ref>), or complexity constraints (as in [CJS95]), provide a hierarchy; but, unconstrained, do not (as in [BP73, CS83]).
Reference: [CJS94] <author> J. Case, S. Jain, and A. Sharma. </author> <title> Vacillatory learning of nearly minimal size grammers. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 49(2) </volume> <pages> 189-207, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: They showed that, for learning recursive functions, vacillatory inference is not more powerful than Ex style learning in the limit. On the other hand, if one is missing negative information <ref> [Cas88, Cas96, Cas97, CJS94] </ref> or has suitable complexity constraints [CJS95], then vacillatory inference increases learning power. Many real-world applications of learning or inductive inference have to deal with faulty data, so it is natural to study this phenomenon [BJS96, FJ96, OSW86]. <p> Theorem 16 says that behaviorally correct learning from noisy informant can be simulated by Ex style learning from a noise free informant. Hence, for informant data, noise destroys the advantage of behaviorally correct over Ex style learning! If one is missing negative information <ref> [Cas88, Cas96, Cas97, CJS94] </ref> or has suitable complexity 2 constraints [CJS95], then Fex b+1 style learning is more powerful than Fex b . Theorem 14 implies that, one also gets such a hierarchy result for Fex b style learning from noisy informant. <p> Interestingly, as we see by Corollary 2 to the following theorem (Theorem 14), the hierarchy NoisyInfFex 1 NoisyInfFex 2 : : : NoisyInfFex fl is proper. This contrasts sharply with the non-noisy case. Fex style criteria, in situations taking into account noise (as here), missing information (as in <ref> [Cas88, Cas96, Cas97, CJS94] </ref>), or complexity constraints (as in [CJS95]), provide a hierarchy; but, unconstrained, do not (as in [BP73, CS83]).
Reference: [CJS95] <author> J. Case, S. Jain, and A. Sharma. </author> <title> Complexity issues for vacillatory function identification. </title> <journal> Infor mation and Computation, </journal> <volume> 116(2) </volume> <pages> 174-192, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: They showed that, for learning recursive functions, vacillatory inference is not more powerful than Ex style learning in the limit. On the other hand, if one is missing negative information [Cas88, Cas96, Cas97, CJS94] or has suitable complexity constraints <ref> [CJS95] </ref>, then vacillatory inference increases learning power. Many real-world applications of learning or inductive inference have to deal with faulty data, so it is natural to study this phenomenon [BJS96, FJ96, OSW86]. <p> Hence, for informant data, noise destroys the advantage of behaviorally correct over Ex style learning! If one is missing negative information [Cas88, Cas96, Cas97, CJS94] or has suitable complexity 2 constraints <ref> [CJS95] </ref>, then Fex b+1 style learning is more powerful than Fex b . Theorem 14 implies that, one also gets such a hierarchy result for Fex b style learning from noisy informant. Suppose a is a natural number or a fl. <p> This contrasts sharply with the non-noisy case. Fex style criteria, in situations taking into account noise (as here), missing information (as in [Cas88, Cas96, Cas97, CJS94]), or complexity constraints (as in <ref> [CJS95] </ref>), provide a hierarchy; but, unconstrained, do not (as in [BP73, CS83]). Theorem 14 Suppose n 1. (a) (NoisyInfFex n+1 " NoisyTxtFex n+1 ) TxtOex fl n 6= ;. (b) (NoisyInfFex fl " NoisyTxtFex fl ) S n TxtOex fl n 6= ;.
Reference: [CKKK95] <author> J. Case, S. Kaufmann, E. Kinber, and M. Kummer. </author> <title> Learning recursive functions from approximations. </title> <editor> In Paul Vitanyi, editor, </editor> <booktitle> Computational Learning Theory, Second European Conference, </booktitle> <address> EuroCOLT'95, Barcelona, Spain, </address> <pages> pages 140-153. </pages> <publisher> Springer-Verlag, </publisher> <month> March </month> <year> 1995. </year> <booktitle> Lecture Notes in Artificial Intelligence 904. </booktitle>
Reference-contexts: It would be good to assuage the difficulty of learning from noisy data, in the future, by finding natural forms of "innate knowledge" or additional information (as, for example, was done for noise free function learning in <ref> [CKKK95] </ref>). Acknowledgements Frank Stephan was supported by the Deutsche Forschungsgemeinschaft (DFG) Grant No. Am 60/9-1. We are grateful to Tom Nordahl for his information on schizophrenia.
Reference: [CL82] <author> J. Case and C. Lynes. </author> <title> Machine inductive inference and language identification. </title> <editor> In M. Nielsen and E. M. Schmidt, editors, </editor> <booktitle> Proceedings of the 9th International Colloquium on Automata, Languages and Programming, </booktitle> <pages> pages 107-115. </pages> <publisher> Springer-Verlag, </publisher> <year> 1982. </year> <note> Lecture Notes in Computer Science 140. </note>
Reference-contexts: In particular he considered a machine, which reads more and more positive information on an r.e. set and produces in the limit a grammar to generate this set. This is called Ex style identification. From then on many variants of this concept have been considered <ref> [AS83, BB75, CL82, OSW86] </ref>. 1 Barzdin [Bar74] and Case and Smith [CS83] considered the notion of behaviorally correct in-ference which is motivated by the fact that no algorithm can check the equivalence of grammars. <p> TxtFex fl S n TxtFex fl n 6= ;. (e) [CS83] InfFex fl fl InfBc. (f ) [CS83] TxtBc n+1 InfBc n 6= ;. TxtBc fl S n InfBc n 6= ;. (g) <ref> [CL82, Cas96, Cas97] </ref> TxtFex 2n fl TxtBc n . (h) [CL82, Cas96, Cas97] TxtEx 2n+1 0 TxtBc n 6= ;. <p> TxtFex fl S n TxtFex fl n 6= ;. (e) [CS83] InfFex fl fl InfBc. (f ) [CS83] TxtBc n+1 InfBc n 6= ;. TxtBc fl S n InfBc n 6= ;. (g) <ref> [CL82, Cas96, Cas97] </ref> TxtFex 2n fl TxtBc n . (h) [CL82, Cas96, Cas97] TxtEx 2n+1 0 TxtBc n 6= ;. <p> TxtEx fl 0 n TxtBc n 6= ;. (i) InfEx 1 TxtBc fl 6= ;. (j) [Sha95] InfEx a 0 TxtEx a . (k) [CS83] InfOex n fl = InfEx n fl . (l) <ref> [CS83, CL82, Gol67] </ref> TxtOex fl 2 n InfBc n 6= ;. TxtOex fl 2 TxtBc fl 6= ;. Moreover parts (a) and (b) can be shown using subsets of SVT as a diagonalizing class. Theorem 2 Suppose n 2 N and b 2 N [ fflg. <p> It follows from Theorem 5 that L =2 NoisyTxtBc fl . (c) Let L = fL : L is finite g [ fN g. Clearly, L 2 NoisyTxtOex fl 2 " NoisyInfOex fl 2 (by using the grammars for ; and N ). However L 62 TxtBc fl <ref> [Gol67, CL82] </ref>. Corollary 1 NoisyInfFex fl = NoisyInfOex fl . NoisyTxtFex fl NoisyTxtOex fl . <p> Clearly, L a 2 NoisyTxtEx a 0 . It was shown in <ref> [CL82] </ref> that L 2n+1 62 TxtBc n and L fl 62 S n TxtBc n . (f) Let L; L 1 ; L 2 be r.e. sets such that L 1 L L 2 and card (L 2 L) = card (L L 1 ) = n + 1. <p> (x) 6= f (x + 1)g) = min (W f (0) )g; let L = fL : L represents some f 2 Cg.) (c) Follows from the fact that InfFex fl fl InfBc. (d) The idea is essentially the same as used to prove TxtFex 2n fl TxtBc n from <ref> [CL82, Cas96, Cas97] </ref>. Suppose M is given. M 0 () is defined as follows. Let S be the least n elements in Pos ()W M ();jj (if Pos ()W M ();jj contains less than n elements, then S = Pos ()W M ();jj ). <p> The argument to prove that M 0 NoisyInfBc n -identifies every language NoisyInfFex 2n fl - identified by M is essentially the same as used by <ref> [CL82, Cas96, Cas97] </ref>. We omit the details. (e) For a 2 N [ fflg, let L a = fL : L = a N g. Clearly, L a 2 NoisyInfEx a 0 . <p> We omit the details. (e) For a 2 N [ fflg, let L a = fL : L = a N g. Clearly, L a 2 NoisyInfEx a 0 . It was shown in <ref> [CL82] </ref> that L 2n+1 62 TxtBc n and L fl 62 S n TxtBc n . Interestingly, as we see by Corollary 2 to the following theorem (Theorem 14), the hierarchy NoisyInfFex 1 NoisyInfFex 2 : : : NoisyInfFex fl is proper. This contrasts sharply with the non-noisy case. <p> Proof. Clearly, Var 2n (L) 2 NoisyInfEx 2n NoisyInfBc n TxtBc n . This proves part (a) and (b). Also, Var fl (L) 2 NoisyInfEx fl InfEx fl InfBc. This proves part (c). Case and Lynes <ref> [CL82] </ref> showed that Var 2n+1 (N ) =2 TxtBc n NoisyInfBc n , and Var n+1 (N ) =2 TxtFex n fl = TxtOex n fl . Their proof generalizes to any infinite L. This proves (d), (e) and (f).
Reference: [CS83] <author> J. Case and C. Smith. </author> <title> Comparison of identification criteria for machine inductive inference. </title> <journal> Theoretical Computer Science, </journal> <volume> 25 </volume> <pages> 193-220, </pages> <year> 1983. </year>
Reference-contexts: This is called Ex style identification. From then on many variants of this concept have been considered [AS83, BB75, CL82, OSW86]. 1 Barzdin [Bar74] and Case and Smith <ref> [CS83] </ref> considered the notion of behaviorally correct in-ference which is motivated by the fact that no algorithm can check the equivalence of grammars. <p> Convergence on information sequences is defined similarly. Definition 1 (a) Suppose a; b 2 N [ fflg. Below, for each of several learning criteria J , we define what it means for a machine M to J -identify a language L from a text T or informant I. * <ref> [Gol67, CS83, BB75] </ref> M TxtEx a b -identifies L from text T iff (9i : W i = a L)[M (T )# = i] and card (fn : ? 6= M (T [n]) 6= M (T [n + 1])g) b. <p> We call each instance of ? 6= M (T [n]) 6= M (T [n + 1]) as a mind change by M on T . * <ref> [Gol67, CS83, BB75] </ref> M InfEx a b -identifies L from informant I iff (9i : W i = a L)[M (I)# = i] and card (fn : ? 6= M (I [n]) 6= M (I [n + 1])g) b. <p> We call each instance of ? 6= M (I [n]) 6= M (I [n + 1]) as a mind change by M on I. * <ref> [Bar74, CS83] </ref>. M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. InfBc a -identification is defined similarly. * [Cas88, Cas96, Cas97, BP73]. <p> If lim n!1 Last b (M; T [n])#, then we say that Last b (M; T ) = lim n!1 Last b (M; T [n]). Other wise Last b (M; T ) is undefined. Last b (M; I) is defined similarly. * <ref> [CS83] </ref>. M TxtOex a b -identifies L from text T iff Last b (M; T ) is defined and (9i 2 Last b (M; T ))[W i = a L]. <p> The following theorem gives some of the results from the literature when there is no noise in the input data. Theorem 1 Let a 2 N [ fflg and n 2 N . (a) <ref> [CS83] </ref> TxtEx n+1 0 InfFex n fl 6= ;. TxtEx fl 0 n InfFex n fl 6= ;. (b) [CS83] TxtEx n+1 InfEx fl n 6= ;. <p> Theorem 1 Let a 2 N [ fflg and n 2 N . (a) <ref> [CS83] </ref> TxtEx n+1 0 InfFex n fl 6= ;. TxtEx fl 0 n InfFex n fl 6= ;. (b) [CS83] TxtEx n+1 InfEx fl n 6= ;. TxtEx S n InfEx fl n 6= ;. (c) [BP73, CS83] InfFex a fl = InfEx a . (d) [Cas88, Cas96, Cas97] TxtFex n+1 TxtFex fl n 6= ;. TxtFex fl S n TxtFex fl n 6= ;. (e) [CS83] InfFex fl fl <p> TxtEx fl 0 n InfFex n fl 6= ;. (b) [CS83] TxtEx n+1 InfEx fl n 6= ;. TxtEx S n InfEx fl n 6= ;. (c) <ref> [BP73, CS83] </ref> InfFex a fl = InfEx a . (d) [Cas88, Cas96, Cas97] TxtFex n+1 TxtFex fl n 6= ;. TxtFex fl S n TxtFex fl n 6= ;. (e) [CS83] InfFex fl fl InfBc. (f ) [CS83] TxtBc n+1 InfBc n 6= ;. <p> 6= ;. (b) <ref> [CS83] </ref> TxtEx n+1 InfEx fl n 6= ;. TxtEx S n InfEx fl n 6= ;. (c) [BP73, CS83] InfFex a fl = InfEx a . (d) [Cas88, Cas96, Cas97] TxtFex n+1 TxtFex fl n 6= ;. TxtFex fl S n TxtFex fl n 6= ;. (e) [CS83] InfFex fl fl InfBc. (f ) [CS83] TxtBc n+1 InfBc n 6= ;. TxtBc fl S n InfBc n 6= ;. (g) [CL82, Cas96, Cas97] TxtFex 2n fl TxtBc n . (h) [CL82, Cas96, Cas97] TxtEx 2n+1 0 TxtBc n 6= ;. <p> TxtEx S n InfEx fl n 6= ;. (c) [BP73, CS83] InfFex a fl = InfEx a . (d) [Cas88, Cas96, Cas97] TxtFex n+1 TxtFex fl n 6= ;. TxtFex fl S n TxtFex fl n 6= ;. (e) <ref> [CS83] </ref> InfFex fl fl InfBc. (f ) [CS83] TxtBc n+1 InfBc n 6= ;. TxtBc fl S n InfBc n 6= ;. (g) [CL82, Cas96, Cas97] TxtFex 2n fl TxtBc n . (h) [CL82, Cas96, Cas97] TxtEx 2n+1 0 TxtBc n 6= ;. <p> TxtEx fl 0 n TxtBc n 6= ;. (i) InfEx 1 TxtBc fl 6= ;. (j) [Sha95] InfEx a 0 TxtEx a . (k) <ref> [CS83] </ref> InfOex n fl = InfEx n fl . (l) [CS83, CL82, Gol67] TxtOex fl 2 n InfBc n 6= ;. TxtOex fl 2 TxtBc fl 6= ;. Moreover parts (a) and (b) can be shown using subsets of SVT as a diagonalizing class. <p> TxtEx fl 0 n TxtBc n 6= ;. (i) InfEx 1 TxtBc fl 6= ;. (j) [Sha95] InfEx a 0 TxtEx a . (k) [CS83] InfOex n fl = InfEx n fl . (l) <ref> [CS83, CL82, Gol67] </ref> TxtOex fl 2 n InfBc n 6= ;. TxtOex fl 2 TxtBc fl 6= ;. Moreover parts (a) and (b) can be shown using subsets of SVT as a diagonalizing class. Theorem 2 Suppose n 2 N and b 2 N [ fflg. <p> NoisyTxtEx fl 0 n TxtBc n 6= ;. (f ) NoisyTxtEx n+1 0 NoisyTxtBc n 6= ;. Proof. (a), (b) Case and Smith <ref> [CS83] </ref> showed that there exist L; L 0 ; L 00 SVT such that L 2 TxtEx n+1 InfOex n fl , L 0 2 TxtEx fl S n InfOex n fl , and L 00 2 TxtEx S n TxtEx fl n . (a), (b), now follows from Theorem 11. <p> NoisyInfEx fl 0 n TxtBc n 6= ;. Proof. (a), (b) Follow using Theorem 3 and the facts that InfEx n+1 0 6 InfEx n = InfFex n fl , InfEx fl 0 6 n InfEx n = S n InfFex n fl <ref> [CS83] </ref> and InfEx 0 [K] 6 S n InfEx fl n . (Gasarch and Pleszkoch [GP89] showed that InfEx 0 [K] 6 S n InfEx n . Cylindrification of their result gives InfEx 0 [K] S n InfEx fl n . <p> This contrasts sharply with the non-noisy case. Fex style criteria, in situations taking into account noise (as here), missing information (as in [Cas88, Cas96, Cas97, CJS94]), or complexity constraints (as in [CJS95]), provide a hierarchy; but, unconstrained, do not (as in <ref> [BP73, CS83] </ref>). Theorem 14 Suppose n 1. (a) (NoisyInfFex n+1 " NoisyTxtFex n+1 ) TxtOex fl n 6= ;. (b) (NoisyInfFex fl " NoisyTxtFex fl ) S n TxtOex fl n 6= ;. Proof. (a) Let Null L = fy : h0; yi 2 Lg. <p> Proof. For a 2 N [ fflg, L a = fL : card (L) = 1 ^ (8 1 x 2 L) [W x = a L]g. It is easy to verify that L a 2 NoisyTxtBc a . Adopting the techniques used by Case and Smith <ref> [CS83] </ref> to show Bc n+1 6 Bc n and Bc 6 Ex fl , one can show that L 0 62 InfOex fl fl , L n+1 62 InfBc n and L fl 62 S n InfBc n . We omit the details. <p> The proof of Bc n+1 Bc n 6= ; and Bc InfOex fl fl 6= ; from <ref> [CS83] </ref> can be easily adopted to show that L n+1 62 InfBc n , L fl 62 S n InfBc n , and L 1 =2 InfOex fl fl . We omit the details. (c) Let L 1 x;y = fhw; zi : x z yg.
Reference: [FJ96] <author> M. A. Fulk and S. Jain. </author> <title> Learning in the presence of inaccurate information. </title> <institution> Theoretical Computer Science A, 161(1-2):235-261, </institution> <month> 15 July </month> <year> 1996. </year>
Reference-contexts: On the other hand, if one is missing negative information [Cas88, Cas96, Cas97, CJS94] or has suitable complexity constraints [CJS95], then vacillatory inference increases learning power. Many real-world applications of learning or inductive inference have to deal with faulty data, so it is natural to study this phenomenon <ref> [BJS96, FJ96, OSW86] </ref>. Many of these notions of noise have the disadvantage that noisy data does not specify uniquely the object to be learned. Stephan [Ste95] introduced a notion of noise in order to overcome this difficulty: correct information occurs infinitely often while incorrect information occurs only finitely often. <p> We told the mathematician and psychiatrist Tom Nordahl about Theorem 8 after he had contacted us inquiring about <ref> [FJ96] </ref>. He was interested in the possible relevance to schizophrenia. <p> The idea of the proof is to convert a noisy text for L 2 SVT , limit effectively, into a text for L (similar technique was also used in <ref> [FJ96, FJO94] </ref>). This is done as follows.
Reference: [FJO94] <author> M. Fulk, S. Jain, and D. Osherson. </author> <title> Open problems in systems that learn. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 49(3) </volume> <pages> 589-604, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: The idea of the proof is to convert a noisy text for L 2 SVT , limit effectively, into a text for L (similar technique was also used in <ref> [FJ96, FJO94] </ref>). This is done as follows.
Reference: [Gol67] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: 1 Introduction Gold <ref> [Gol67] </ref> introduced the notion of learning in the limit. In particular he considered a machine, which reads more and more positive information on an r.e. set and produces in the limit a grammar to generate this set. This is called Ex style identification. <p> Convergence on information sequences is defined similarly. Definition 1 (a) Suppose a; b 2 N [ fflg. Below, for each of several learning criteria J , we define what it means for a machine M to J -identify a language L from a text T or informant I. * <ref> [Gol67, CS83, BB75] </ref> M TxtEx a b -identifies L from text T iff (9i : W i = a L)[M (T )# = i] and card (fn : ? 6= M (T [n]) 6= M (T [n + 1])g) b. <p> We call each instance of ? 6= M (T [n]) 6= M (T [n + 1]) as a mind change by M on T . * <ref> [Gol67, CS83, BB75] </ref> M InfEx a b -identifies L from informant I iff (9i : W i = a L)[M (I)# = i] and card (fn : ? 6= M (I [n]) 6= M (I [n + 1])g) b. <p> TxtEx fl 0 n TxtBc n 6= ;. (i) InfEx 1 TxtBc fl 6= ;. (j) [Sha95] InfEx a 0 TxtEx a . (k) [CS83] InfOex n fl = InfEx n fl . (l) <ref> [CS83, CL82, Gol67] </ref> TxtOex fl 2 n InfBc n 6= ;. TxtOex fl 2 TxtBc fl 6= ;. Moreover parts (a) and (b) can be shown using subsets of SVT as a diagonalizing class. Theorem 2 Suppose n 2 N and b 2 N [ fflg. <p> It follows from Theorem 5 that L =2 NoisyTxtBc fl . (c) Let L = fL : L is finite g [ fN g. Clearly, L 2 NoisyTxtOex fl 2 " NoisyInfOex fl 2 (by using the grammars for ; and N ). However L 62 TxtBc fl <ref> [Gol67, CL82] </ref>. Corollary 1 NoisyInfFex fl = NoisyInfOex fl . NoisyTxtFex fl NoisyTxtOex fl .
Reference: [GP89] <author> W. I. Gasarch and M. B. Pleszkoch. </author> <title> Learning via queries to an oracle. </title> <editor> In R. Rivest, D. Haussler, and M. K. Warmuth, editors, </editor> <booktitle> Proceedings of the Second Annual Workshop on Computational Learning Theory, </booktitle> <address> Santa Cruz, California, </address> <pages> pages 214-229. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1989. </year>
Reference-contexts: Suppose I is an identification criterion considered in this paper. Then I [A] denotes the identification criteria formed from I by allowing the learning machines access to oracle A. Gasarch and Pleszkoch <ref> [GP89] </ref>, building on earlier work of L. Adleman and M. Blum, were first in print to consider the notion of learning with oracle. Several proofs in this paper depend on the concept of locking sequence. <p> Proof. (a), (b) Follow using Theorem 3 and the facts that InfEx n+1 0 6 InfEx n = InfFex n fl , InfEx fl 0 6 n InfEx n = S n InfFex n fl [CS83] and InfEx 0 [K] 6 S n InfEx fl n . (Gasarch and Pleszkoch <ref> [GP89] </ref> showed that InfEx 0 [K] 6 S n InfEx n . Cylindrification of their result gives InfEx 0 [K] S n InfEx fl n .
Reference: [KB92] <author> S. Kapur and G. Bilardi. </author> <title> Language learning without overgeneralization. </title> <booktitle> In Proceedings of the Ninth Annual Symposium on Theoretical Aspects of Computer Science, Lecture Notes in Computer Science 577. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: If a 2 N , it follows that L 1 = 2a L 2 ; if a = fl, it follows that L 1 = fl L 2 . 4 This latter subset principle, for preventing overgeneralization, is further discussed, for example, in <ref> [Ber85, Cas96, Cas97, KLHM93, Wex93, BCJ96, ZLK95, KB92] </ref>.
Reference: [KLHM93] <author> S. Kapur, B. Lust, W. Harbert, and G. Martohardjono. </author> <title> Universal grammar and learnability theory: The case of binding domains and the `subset principle'. </title> <editor> In E. Reuland and W Abraham, editors, </editor> <booktitle> Knowledge and Language, </booktitle> <volume> Volume I, </volume> <pages> pages 185-216. </pages> <publisher> Kluwer, </publisher> <year> 1993. </year>
Reference-contexts: If a 2 N , it follows that L 1 = 2a L 2 ; if a = fl, it follows that L 1 = fl L 2 . 4 This latter subset principle, for preventing overgeneralization, is further discussed, for example, in <ref> [Ber85, Cas96, Cas97, KLHM93, Wex93, BCJ96, ZLK95, KB92] </ref>.
Reference: [LZ92] <author> S. Lange and T. Zeugmann. </author> <title> Types of monotonic language learning and their characterization. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, Pennsylvania, </address> <pages> pages 377-390. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: Mukouchi [Muk92] and Lange and Zeugmann <ref> [LZ92] </ref> present a subset principle for one-shot learning. 10 Theorem 6 Suppose a 2 N [ fflg and n 2 N . (a) TxtEx 1 NoisyTxtBc fl 6= ;. (b) InfEx 0 NoisyTxtBc fl 6= ;. (c) TxtEx n+1 0 NoisyTxtBc n 6= ;.
Reference: [LZ93] <author> S. Lange and T. Zeugmann. </author> <title> Monotonic versus non-monotonic language learning. </title> <booktitle> In Proceedings of the Second International Workshop on Nonmonotonic and Inductive Logic, </booktitle> <pages> pages 254-269. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year> <booktitle> Lecture Notes in Artificial Intelligence 659. </booktitle>
Reference-contexts: Theorem 8 Suppose a; b 2 N [ fflg. (a) NoisyInfFex a b TxtFex a b . (b) NoisyInfOex a b TxtOex a b . (c) NoisyInfBc a TxtBc a . Proof. An idea similar to that used in this proof was also used by Lange and Zeugmann <ref> [LZ93] </ref>. The proof is based on the fact that any text T for L can be translated into a noisy informant I T 12 for L via (i; 1); if i 2 content (T [hi; ji]); (i; 0); if i =2 content (T [hi; ji]).
Reference: [Muk92] <author> Y. Mukouchi. </author> <title> Characterization of finite identification. </title> <editor> In K. P. Jantke, editor, </editor> <booktitle> Proceedings of the Third International Workshop on Analogical and Inductive Inference, </booktitle> <address> Dagstuhl Castle, Germany, </address> <pages> pages 260-267, </pages> <month> October </month> <year> 1992. </year> <month> 25 </month>
Reference-contexts: Mukouchi <ref> [Muk92] </ref> and Lange and Zeugmann [LZ92] present a subset principle for one-shot learning. 10 Theorem 6 Suppose a 2 N [ fflg and n 2 N . (a) TxtEx 1 NoisyTxtBc fl 6= ;. (b) InfEx 0 NoisyTxtBc fl 6= ;. (c) TxtEx n+1 0 NoisyTxtBc n 6= ;.
Reference: [Odi89] <author> P. Odifreddi. </author> <title> Classical Recursion Theory. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1989. </year>
Reference-contexts: proved by a priority argument, says that for some r.e. set L, Var n+1 (L) cannot be learned Ex style from a noise free informant (with final program correct except at up to n arguments). 2 Notations and Identification Criteria The recursion theoretic notions are from the books of Odifreddi <ref> [Odi89] </ref> and Soare [Soa87]. N = f0; 1; 2; : : :g is the set of all natural numbers, and this paper considers r.e. subsets L of N . All conventions regarding range of variables apply, with or without decorations 1 , unless otherwise specified.
Reference: [OSW86] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems that Learn, An Introduction to Learning Theory for Cognitive and Computer Scientists. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: In particular he considered a machine, which reads more and more positive information on an r.e. set and produces in the limit a grammar to generate this set. This is called Ex style identification. From then on many variants of this concept have been considered <ref> [AS83, BB75, CL82, OSW86] </ref>. 1 Barzdin [Bar74] and Case and Smith [CS83] considered the notion of behaviorally correct in-ference which is motivated by the fact that no algorithm can check the equivalence of grammars. <p> On the other hand, if one is missing negative information [Cas88, Cas96, Cas97, CJS94] or has suitable complexity constraints [CJS95], then vacillatory inference increases learning power. Many real-world applications of learning or inductive inference have to deal with faulty data, so it is natural to study this phenomenon <ref> [BJS96, FJ96, OSW86] </ref>. Many of these notions of noise have the disadvantage that noisy data does not specify uniquely the object to be learned. Stephan [Ste95] introduced a notion of noise in order to overcome this difficulty: correct information occurs infinitely often while incorrect information occurs only finitely often. <p> Let M 0 ; M 1 ; : : : be a recursive enumeration of total learning machines such that, for all L 2 InfEx n , there exists an i, such that L InfEx n (M i ). (There exists such an enumeration. For example see <ref> [OSW86] </ref>.) Then one of the following two properties will be satisfied for each i. (A) M i (I L ) diverges. (B) There is an m such that X m i L is infinite and (8L 0 : L L 0 X m Note that this implies L 6 InfEx n
Reference: [Sha95] <author> A. Sharma. </author> <type> Personal Communication, </type> <year> 1995. </year>
Reference-contexts: TxtBc fl S n InfBc n 6= ;. (g) [CL82, Cas96, Cas97] TxtFex 2n fl TxtBc n . (h) [CL82, Cas96, Cas97] TxtEx 2n+1 0 TxtBc n 6= ;. TxtEx fl 0 n TxtBc n 6= ;. (i) InfEx 1 TxtBc fl 6= ;. (j) <ref> [Sha95] </ref> InfEx a 0 TxtEx a . (k) [CS83] InfOex n fl = InfEx n fl . (l) [CS83, CL82, Gol67] TxtOex fl 2 n InfBc n 6= ;. TxtOex fl 2 TxtBc fl 6= ;.
Reference: [Soa87] <author> R. Soare. </author> <title> Recursively Enumerable Sets and Degrees. </title> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: priority argument, says that for some r.e. set L, Var n+1 (L) cannot be learned Ex style from a noise free informant (with final program correct except at up to n arguments). 2 Notations and Identification Criteria The recursion theoretic notions are from the books of Odifreddi [Odi89] and Soare <ref> [Soa87] </ref>. N = f0; 1; 2; : : :g is the set of all natural numbers, and this paper considers r.e. subsets L of N . All conventions regarding range of variables apply, with or without decorations 1 , unless otherwise specified.
Reference: [SRN96] <author> R. Salo, L. Robertson, and T. Nordahl. </author> <title> Normal sustained effects of selective attention are absent in schizophrenic patients withdrawn from medication. </title> <journal> Psychiatry Research, </journal> <volume> 62 </volume> <pages> 121-130, </pages> <year> 1996. </year>
Reference-contexts: Tom told us that schizophrenics, compared to normals and in contexts requiring some conscious processing, have trouble ignoring irrelevant data and also do not exhibit a kind of normal inhibitory use of negative information (i.e., they do not exhibit negative priming) <ref> [SRN96] </ref>. Furthermore, schizophrenics' deficit in inhibitory processes may occur at a later stage of processing than their difficulties with filtering out "noise" [SRN96]. <p> conscious processing, have trouble ignoring irrelevant data and also do not exhibit a kind of normal inhibitory use of negative information (i.e., they do not exhibit negative priming) <ref> [SRN96] </ref>. Furthermore, schizophrenics' deficit in inhibitory processes may occur at a later stage of processing than their difficulties with filtering out "noise" [SRN96]. Theorem 8, then, provides the beginnings of a possible mathematical, causal explanation: schizophrenia, in effect, gives people noisy input and, then, their deficient, net behavior is subsumable by that of a noise-free (r) normal who just ignores negative information.
Reference: [Ste95] <author> F. Stephan. </author> <title> Noisy inference and oracles. In Algorithmic Learning Theory, </title> <booktitle> 6th International Workshop, </booktitle> <address> ALT'95, Fukuoka, Japan, </address> <pages> pages 185-200. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1995. </year> <booktitle> Lecture Notes in Artificial Intelligence 997. </booktitle>
Reference-contexts: Many real-world applications of learning or inductive inference have to deal with faulty data, so it is natural to study this phenomenon [BJS96, FJ96, OSW86]. Many of these notions of noise have the disadvantage that noisy data does not specify uniquely the object to be learned. Stephan <ref> [Ste95] </ref> introduced a notion of noise in order to overcome this difficulty: correct information occurs infinitely often while incorrect information occurs only finitely often. Many theorems are presented below comparing the learning power for vacillatory and behaviorally correct criteria with or without Stephan's version of noise in the input data. <p> Many theorems are presented below comparing the learning power for vacillatory and behaviorally correct criteria with or without Stephan's version of noise in the input data. Stephan <ref> [Ste95] </ref> showed that the learning power of Ex style learning of grammars from noisy positive and negative data (noisy informant) is exactly characterized by noise free, one-shot (no mind change) learning (from informant) provided the latter learning machines have access to an oracle for K, the halting problem (see Theorem 3 <p> Definition 3 <ref> [Ste95] </ref> An information sequence I is a noisy information sequence (or noisy informant) for L iff (8x) [occur (I; (x; L (x))) = 1 ^ occur (I; (x; L (x))) &lt; 1]. <p> For the criteria of noisy inference discussed in this paper one can prove the existence of a locking sequence as was done in <ref> [Ste95, Theorem 2, proof for NoisyEx Ex 0 [K] </ref> ]. <p> Thus, for all but finitely many T , M 0 () 2 Last b (M; T ), and W M 0 () = n L. Thus M 0 TxtFex n b -identifies L. 3 Simulating Identification from Noisy Data Using Oracles Stephan <ref> [Ste95] </ref> showed that NoisyInfEx = InfEx 0 [K]. His proof also shows, Theorem 3 Suppose a 2 N [ fflg. NoisyInfEx a = InfEx a 0 [K]. One direction of Theorem 3 can be generalized: learning from noisy informant can be simulated by one-shot (finite) learning with suitable oracle. <p> Let L = fL : card (L) &lt; 1g. Clearly, L 2 TxtEx fl 0 . However, for all n, L 62 NoisyTxtBc n by Theorem 5. (d) The proof of TxtEx a 0 NoisyTxtBc a is identical to that of Theorem 23 in <ref> [Ste95] </ref>. (e) Let L K = fhx; yi : x 2 K; y 2 N g and L x = fhx; yi : y 2 N g. Let L = fL K g [ fL x : x 62 Kg. <p> Thus presence of noise changes the hierarchy structure of common identification criteria. As we have seen in this paper, the introduction of noise (as defined in this paper and from <ref> [Ste95] </ref>), in many cases, increases the difficulty of learning, sometimes in interesting ways.
Reference: [Wex93] <author> K Wexler. </author> <title> The subset principle is an intensional principle. </title> <editor> In E. Reuland and W Abraham, editors, </editor> <booktitle> Knowledge and Language, </booktitle> <volume> Volume I, </volume> <pages> pages 217-239. </pages> <publisher> Kluwer, </publisher> <year> 1993. </year>
Reference-contexts: If a 2 N , it follows that L 1 = 2a L 2 ; if a = fl, it follows that L 1 = fl L 2 . 4 This latter subset principle, for preventing overgeneralization, is further discussed, for example, in <ref> [Ber85, Cas96, Cas97, KLHM93, Wex93, BCJ96, ZLK95, KB92] </ref>.
Reference: [ZLK95] <author> Thomas Zeugmann, Steffen Lange, and Shyam Kapur. </author> <title> Characterizations of monotonic and dual monotonic language learning. </title> <journal> Information and Computation, </journal> <volume> 120(2) </volume> <pages> 155-173, </pages> <month> August 1 </month> <year> 1995. </year> <month> 26 </month>
Reference-contexts: If a 2 N , it follows that L 1 = 2a L 2 ; if a = fl, it follows that L 1 = fl L 2 . 4 This latter subset principle, for preventing overgeneralization, is further discussed, for example, in <ref> [Ber85, Cas96, Cas97, KLHM93, Wex93, BCJ96, ZLK95, KB92] </ref>.
References-found: 35

