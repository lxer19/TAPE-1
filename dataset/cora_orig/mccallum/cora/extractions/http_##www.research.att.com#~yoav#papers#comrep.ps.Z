URL: http://www.research.att.com/~yoav/papers/comrep.ps.Z
Refering-URL: http://www.bell-labs.com/user/seung/papers/index.html
Root-URL: 
Email: yoav@research.att.com  seung@bell-labs.com  shamir@cs.huji.ac.il  tishby@cs.huji.ac.il  
Title: Selective sampling using the Query by Committee algorithm Running title: Selective sampling using Query by Committee  
Author: Yoav Freund H. Sebastian Seung Eli Shamir Naftali Tishby 
Date: July 1995  
Address: Murray Hill, New Jersey  Murray Hill, New Jersey  Jerusalem  Jerusalem  
Affiliation: AT&T Laboratories  Bell Laboratories Lucent Technologies  Institute of Computer Science Hebrew University,  Institute of Computer Science and Center for Neural Computation Hebrew University,  
Abstract: We analyze the "query by committee" algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of perceptrons. Keywords: selective sampling, query learning, Bayesian Learning, experimental design fl Yoav Freund, Room 2B-428, AT&T Laboratories, 700 Mountain Ave., Murray Hill, NJ, 07974. Telephone:908-582-3164.
Abstract-found: 1
Intro-found: 1
Reference: [AD92] <author> A. C. Atkinson and A. N. Donev. </author> <title> Optimum Experimental Designs. </title> <publisher> Oxford science publications, </publisher> <year> 1992. </year>
Reference-contexts: The problem of selecting the optimal examples for learning is closely related to the problem of experimental design in statistics (see e.g. <ref> [Fed72, AD92] </ref>). Experimental design is the analysis of methods for selecting sets of experiments, which correspond to membership queries in the context of learning theory.
Reference: [Ang88] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: In this paradigm the learner is passive and has no control over the information that it receives. In contrast, in the query paradigm, the learner is given the power to ask questions. What does the learner gain from this additional power? Study of the use of queries in learning <ref> [Val84, Ang88] </ref>, has mostly concentrated on algorithms for exact identification of the target concept. This type of analysis concentrates on the worst case behavior of the algorithm, and no probabilistic assumptions are made.
Reference: [Bar92] <author> Ian Barland. </author> <title> Some ideas on learning with directional feedback. </title> <type> Master's thesis, </type> <institution> University of California at Santa Cruz, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: The proof is based on finding the convex version space that produces the smallest value of G (F ~x ). This body is constructed of two isomorphic cones connected at their bases, we call this body a "two-cone". Barland <ref> [Bar92, Theorem5] </ref>, analyzes a similar problem.
Reference: [Bau91] <author> E. Baum. </author> <title> Neural net algorithms that learn in polynomial time from examples and queries. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2 </volume> <pages> 5-19, </pages> <year> 1991. </year>
Reference-contexts: This added capability enables the learner to maintain its sensitivity to the input distribution, while reducing the number of labels that it needs to know. Baum <ref> [Bau91] </ref>, proposed a learning algorithm that uses membership queries to avoid the intractability of learning neural networks with hidden units. His algorithm is proved to work for networks with at most four hidden units, and there is experimental evidence [BL92] that it works for larger networks.
Reference: [BEHW89] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> J. ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: As the distribution of the instances is within D from the uniform distribution, the probability of this set is at least D cos 1 (ff). On the other hand, as the VC dimension of the d dimensional perceptron is d we can use the classical uniform convergence bounds from <ref> [BEHW89] </ref>. Theorem 2.1 in [BEHW89] guarantees that a hypothesis that is consistent with m labeled examples, chosen independently at random from an arbitrary distribution, has error smaller than * with probability 1 ffi if m max 4 log ffi 8d log * : Combining these two arguments, we get the statement <p> On the other hand, as the VC dimension of the d dimensional perceptron is d we can use the classical uniform convergence bounds from <ref> [BEHW89] </ref>. Theorem 2.1 in [BEHW89] guarantees that a hypothesis that is consistent with m labeled examples, chosen independently at random from an arbitrary distribution, has error smaller than * with probability 1 ffi if m max 4 log ffi 8d log * : Combining these two arguments, we get the statement of the theorem.
Reference: [BF87] <author> T. Bonnesen and W. Fenchel. </author> <title> Theory of Convex Bodies. </title> <publisher> BCS Associates, </publisher> <address> Moscow, Idaho, USA, </address> <year> 1987. </year>
Reference: [BL92] <author> E. B. Baum and K. Lang. </author> <title> Query learning can work poorly when a human oracle is used. </title> <booktitle> In International Joint Conference in Neural Networks, </booktitle> <address> Beijing, China, </address> <year> 1992. </year>
Reference-contexts: Baum [Bau91], proposed a learning algorithm that uses membership queries to avoid the intractability of learning neural networks with hidden units. His algorithm is proved to work for networks with at most four hidden units, and there is experimental evidence <ref> [BL92] </ref> that it works for larger networks. However, when Baum and Lang tried to use this algorithm to train a network for classifying handwritten characters, they encountered an unexpected problem [BL92]. <p> His algorithm is proved to work for networks with at most four hidden units, and there is experimental evidence <ref> [BL92] </ref> that it works for larger networks. However, when Baum and Lang tried to use this algorithm to train a network for classifying handwritten characters, they encountered an unexpected problem [BL92]. The problem was that many of the images generated by the algorithm as queries did not contain any recognizable character, they were artificial combinations of character images that had no natural meaning.
Reference: [CAL90] <author> David Cohn, Les Atlas, and Richard Ladner. </author> <title> Training connectionist networks with queries and selective sampling. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 27 </pages>
Reference-contexts: In the lines of work described above, queries are explicitly constructed. In contrast, our work is derived within the query filtering paradigm. In this paradigm, proposed by <ref> [CAL90] </ref>, the learner is given access to a stream of inputs drawn at random from the input distribution. The learner sees every input, but chooses whether or not to query the teacher for the label. <p> Initially, most examples will be informative for the learner, but as the process continues, the prediction capabilities of the learner improve, and it discards most of the examples as non-informative, thus saving the human teacher a large amount of work. In <ref> [CAL90] </ref> there are several suggestions for query filters together with some empirical tests of their performance on simple problems. Seung et al. [SOS92] have suggested a filter called "query by committee," (QBC) and analytically calculated its performance for some perceptron-type learning problems.
Reference: [DE95] <author> Ido Dagan and Sean P. Engelson. </author> <title> Committee-based sampling for training probabilistic classfiers. </title> <editor> In Priedits and Russel, editors, </editor> <booktitle> The XII International Conference on Machine Learning, </booktitle> <pages> pages 150-157. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Though theoretical results for such models are lacking, there is empirical evidence that extensions of the QBCalgorithm can be used to learn noisy and probabilistic models, such as hidden Markov models <ref> [DE95] </ref>. We believe that the more general "agnostic" learning scenario and the noisy learning problem are related. 26 It seems useful, in this context, to extend the size of the committee and use more refined definitions for "disagreement" among the committee members.
Reference: [ER90] <author> Bonnie Eisenberg and Ronald L. Rivest. </author> <title> On the sample complexity of pac-learning using random and chosen examples. </title> <booktitle> In Proceedings of the 1990 Workshop on Computational Learning Theory, </booktitle> <pages> pages 154-162, </pages> <year> 1990. </year>
Reference-contexts: We assume that both the examples and the target concept are chosen randomly. In particular, we show that queries can help accelerate learning of concept classes that are already learnable from just unlabeled data. This question was previously studied by Eisenberg and Rivest <ref> [ER90] </ref> in the PAC learning framework. They give a negative result, and show that, for a natural set of concept classes, which they call "dense in themselves", queries are essentially useless.
Reference: [Fed72] <author> V. V. Fedorov. </author> <title> Theory of Optimal Experiments. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: The problem of selecting the optimal examples for learning is closely related to the problem of experimental design in statistics (see e.g. <ref> [Fed72, AD92] </ref>). Experimental design is the analysis of methods for selecting sets of experiments, which correspond to membership queries in the context of learning theory.
Reference: [HKS94] <author> David Haussler, Michael Kearns, and Robert E. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 83-113, </pages> <year> 1994. </year>
Reference-contexts: In Section 7 we give a broader view on using unlabeled examples for accelerating learning, and in Section 8 we summarize and point to some potential future directions. 2 Preliminaries We work in a Bayesian model of concept learning <ref> [HKS94] </ref>. As in the PAC model, we denote by X an arbitrary sample space over which a distribution D is defined. In this paper we concentrate on the case where X is a Euclidean space R d . <p> The random choice of h is made according to the prior distribution P restricted to the version space. It is a simple observation (see <ref> [HKS94] </ref>), that the expected error of this prediction error is at most twice larger than the expected error of the optimal prediction rule which is the Bayes rule. <p> From the definition of t n we get that (1 *) * ln 2 (n+1) 2 3ffi = 2 (n + 1) 2 : Summing this probability over all possible values of n from zero to infinity we get the statement of the lemma. In <ref> [HKS94] </ref> it was shown that if the VC-dimension of a concept class is d, then the expected information gain from m random examples is bounded by (d + 1) log (m=d). Here we show that the probability that the information gain is much larger than that is very small.
Reference: [KR90] <author> W. Kinzel and P. Rujan. </author> <title> Improving a network generalization ability by selecting examples. </title> <journal> Europhys. Lett., </journal> <volume> 13 </volume> <pages> 473-477, </pages> <year> 1990. </year>
Reference-contexts: In the context of Bayesian estimation a very general measure of the quality of a query is the reduction in the entropy of the posterior distribution that is induced by the answer to the query. Similar suggestions have been made in the perceptron learning literature <ref> [KR90] </ref>. A different experimental design criterion is the accuracy with which the outcome of future experiments, chosen from some constrained domain, can be predicted using the hypothesis. This criterion is very similar to criteria used in learning theory. Both criteria are important for us in this paper.
Reference: [Lin56] <author> D. V. Lindley. </author> <title> On a measure of the information provided by an experiment. </title> <journal> Ann. Math. Statist., </journal> <volume> 27 </volume> <pages> 986-1005, </pages> <year> 1956. </year>
Reference-contexts: One natural criterion is the accuracy with which the parameters that define the hypothesis can be estimated <ref> [Lin56] </ref>. In the context of Bayesian estimation a very general measure of the quality of a query is the reduction in the entropy of the posterior distribution that is induced by the answer to the query. Similar suggestions have been made in the perceptron learning literature [KR90].
Reference: [McD89] <author> C. McDiarmid. </author> <title> On the method of bounded differences. In Survey of Combinatorics, </title> <booktitle> 10th British Combinatorial Conference, </booktitle> <year> 1989. </year>
Reference-contexts: As the instantaneous information gain is bounded between 0 and 1, we get that g Y i 1 g. We can thus use Hoeffding's bound on the tails of bounded step 12 sub-martingales <ref> [McD89] </ref> 5 from which we know that for any * &gt; 0 Pr ( i=1 g ) g+* ( 1 g * n Setting * = g and taking logs we get Pr ( i=1 Y i gn) 1 (1+)g n Choosing = 1=2 we get the bound Lemma 2 The <p> Fix a sequence of examples ~ X, recall that ~ X M denotes the first m examples. Then Pr c2P I (h ~ X M ; c ( ~ X M )i) (d + 1)(log d d : (7) 5 The bound as it appears in <ref> [McD89] </ref> is given for martingales. However, it is easily checked that it is also true for super martingales.
Reference: [Mit82] <author> T.M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18(2), </volume> <year> 1982. </year>
Reference-contexts: We use ~ X 1:::m to denote the sequence of the first m elements in ~ X. We use the terminology of Mitchell <ref> [Mit82] </ref>, and define the version space generated by the sequence of labeled examples h ~ X 1:::m ; c ( ~ X 1:::m )i to be the set of concepts c 0 2 C that are consistent with c on ~ X, i.e. that c 0 (x i ) = c
Reference: [Sau72] <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> J. Combinatorial Theory (A), </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference-contexts: However, it is easily checked that it is also true for super martingales. Reversing the sign of the Y i we get an equivalent theorem for sub-martingales. 13 Proof : From Sauer's Lemma <ref> [Sau72] </ref> we know that the number of different labelings created by m examples is at most P d m (em=d) d .
Reference: [Smi85] <author> Peter Smith. </author> <title> Convexity Methods in Variational Calculus. </title> <publisher> Research studies press, John Wiley & sons, </publisher> <year> 1985. </year>
Reference-contexts: In terms of the volume function F , for t &gt; 0, 9 Details on how the Frechet derivative is defined and calculated can be found in standard books on variational analysis, such as <ref> [Smi85] </ref>. F increases when K decreases and vice versa. Thus if the variation (t) is non-negative for points below the pivot point, non-positive for points above the pivot point, and R +1 0 (t) 2 dt is sufficiently small then G (K (t) + (t)) &lt; 0 as desired.
Reference: [SOS92] <author> H.S Seung, M. Opper, and H. Sompolinsky. </author> <title> Query by committee. </title> <booktitle> In Proceedings of the Fifth Workshop on Computational Learning Theory, </booktitle> <pages> pages 287-294, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In [CAL90] there are several suggestions for query filters together with some empirical tests of their performance on simple problems. Seung et al. <ref> [SOS92] </ref> have suggested a filter called "query by committee," (QBC) and analytically calculated its performance for some perceptron-type learning problems. For these problems, they found that the prediction error decreases exponentially fast in the number of queries. <p> In the next section we present such a method. 4 The Query by Committee learning algorithm Seung, Opper and Sompolinsky <ref> [SOS92] </ref> have devised an algorithm for learning by queries which they called "Query by Committee" and we shall refer to as the QBC algorithm. <p> If the two predictions differ, it calls Label with input x, and adds the labeled example to the set of labeled examples that define the version space. It then proceeds to the next iteration. In <ref> [SOS92] </ref> Seung et al. treat the query by committee algorithm as an on-line learning algorithm, and analyze the rate at which the error of the two Gibbs learners reduces as a function of the number of queries made. <p> This gives a rigorous proof to the results given by Seung et al. in <ref> [SOS92] </ref> which were obtained using the replica method of statistical mechanics. It also generalizes their results by relaxing the requirements on the distribution of the examples and on the prior distribution. In addition, we show that exact knowledge of the prior distribution is not required.
Reference: [Val84] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year> <month> 28 </month>
Reference-contexts: In this paradigm the learner is passive and has no control over the information that it receives. In contrast, in the query paradigm, the learner is given the power to ask questions. What does the learner gain from this additional power? Study of the use of queries in learning <ref> [Val84, Ang88] </ref>, has mostly concentrated on algorithms for exact identification of the target concept. This type of analysis concentrates on the worst case behavior of the algorithm, and no probabilistic assumptions are made.
References-found: 20

