URL: ftp://ftp.cs.wisc.edu/wwt/sigmetrics95_am.ps
Refering-URL: http://www.cs.wisc.edu/~david/david.html
Root-URL: 
Email: falvy,davidg@cs.wisc.edu  
Title: Active Memory: A New Abstraction for Memory-System Simulation  
Author: Alvin R. Lebeck and David A. Wood 
Address: 1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Date: May 1995.  
Note: Appears in: "Proceedings of the 1995 ACM SIGMETRICS Conference,"  Reprinted by permission of ACM.  
Abstract: This paper describes the active memory abstraction for memory-system simulation. In this abstraction|designed specifically for on-the-fly simulation, memory references logically invoke a user-specified function depending upon the reference's type and accessed memory block state. Active memory allows simulator writers to specify the appropriate action on each reference, including "no action" for the common case of cache hits. Because the abstraction hides implementation details, implementations can be carefully tuned for particular platforms, permitting much more efficient on-the-fly simulation than the traditional trace-driven abstraction. Our SPARC implementation, Fast-Cache, executes simple data cache simulations two or three times faster than a highly-tuned trace-driven simulator and only 2 to 7 times slower than the original program. Fast-Cache implements active memory by performing a fast table look up of the memory block state, taking as few as 3 cycles on a Super-SPARC for the no-action case. Modeling the effects of Fast-Cache's additional lookup instructions qualitatively shows that Fast-Cache is likely to be the most efficient simulator for miss ratios between 3% and 40%. fl The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Wright Laboratory Avionics Directorate or the U.S. Government. This work is supported in part by NSF PYI Award CCR-9157366, NSF Grants CDA-9024618 and MIP-9225097, donations from Thinking Machines Corp., Digital Equipment Corp., Xerox Corp., and by Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant #F33615-94-1-1525 and ARPA order no. B550 c fl 1993 ACM. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and that notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> SuperSPARC User's Guide, </author> <year> 1992. </year> <note> Alpha Edition. </note>
Reference-contexts: This seems excessively pessimistic given that our host|a SPARCstation 10/51|has a unified 1-megabyte direct-mapped second-level cache backing up the 16-kilobyte 4-way-associative first-level data cache. Instead, we assume C hostmiss is the first-level cache miss penalty, or 5 cycles <ref> [1] </ref>. <p> Fpppp is the only benchmark with a non-negligible instruction cache miss ratio (3.7%) and E p E predicts the number of instruction cache misses within 15% for Fast-Cache and 10% for Fast-Cache-Indirect. To further evaluate this model we used the reference counter of the SuperSPARC second-level cache controller <ref> [1] </ref> to 3 Due to Shade's large slowdowns, we used smaller input data sets for fpppp, tomcatv, and xlisp.
Reference: [2] <author> Anita Borg, R. E. Kessler, and David W. Wall. </author> <title> Generation and Analysis of Very Long Address Traces. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 270-281, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: However, software reference generation techniques have improved to the point that regenerating the trace is nearly as efficient as reading it from disk or tape [9]. On-the-fly simulation techniques|which combine steps 1-4|have become popular because they eliminate I/O overhead, context switches, and large storage requirements <ref> [4, 14, 3, 2] </ref>. Most on-the-fly simulation systems work by instrumenting a program to calculate each reference's effective address and then invoke the simulator. For typical RISC instruction sets, the effective address calculation is trivial, requiring at most one additional instruction per reference. <p> Although simple, this abstraction requires that the simulator either (i) perform a procedure call to process each reference, with the commensurate overhead to save and restore registers [4, 14], or (ii) buffer the reference in memory, incurring buffer management overhead and memory system delays caused by cache pollution <ref> [2, 22] </ref>. Furthermore, this overhead is almost always wasted, because in most simulations the common case, e.g., a cache hit, requires no action. Clearly, optimizing the lookup (step 2) to quickly detect these "no action" cases can significantly improve simulation performance.
Reference: [3] <author> Robert F. Cmelik and David Keppel. Shade: </author> <title> A Fast Instruction-Set Simulator for Execution Profiling. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 128-137, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: However, software reference generation techniques have improved to the point that regenerating the trace is nearly as efficient as reading it from disk or tape [9]. On-the-fly simulation techniques|which combine steps 1-4|have become popular because they eliminate I/O overhead, context switches, and large storage requirements <ref> [4, 14, 3, 2] </ref>. Most on-the-fly simulation systems work by instrumenting a program to calculate each reference's effective address and then invoke the simulator. For typical RISC instruction sets, the effective address calculation is trivial, requiring at most one additional instruction per reference. <p> For our benchmarks, Fast-Cache-Indirect executes 3.3 to 8 times slower than the original program. This is 1.1 to 1.7 times slower than Fast-Cache. To validate our instruction cache models we used Shade <ref> [3] </ref> to measure the instruction cache performance of the instrumented programs. 3 Because the code expansion is not exactly a power of two, we validate the scaled model by simulating caches of the next larger and smaller powers of two and interpolate.
Reference: [4] <author> Helen Davis, Stephen R. Goldschmidt, and John Hennessy. </author> <title> Multiprocessor Simulation and Tracing Using Tango. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages II99-107, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: However, software reference generation techniques have improved to the point that regenerating the trace is nearly as efficient as reading it from disk or tape [9]. On-the-fly simulation techniques|which combine steps 1-4|have become popular because they eliminate I/O overhead, context switches, and large storage requirements <ref> [4, 14, 3, 2] </ref>. Most on-the-fly simulation systems work by instrumenting a program to calculate each reference's effective address and then invoke the simulator. For typical RISC instruction sets, the effective address calculation is trivial, requiring at most one additional instruction per reference. <p> Unfortunately, most on-the-fly simulation systems continue to use the reference trace abstraction. Although simple, this abstraction requires that the simulator either (i) perform a procedure call to process each reference, with the commensurate overhead to save and restore registers <ref> [4, 14] </ref>, or (ii) buffer the reference in memory, incurring buffer management overhead and memory system delays caused by cache pollution [2, 22]. Furthermore, this overhead is almost always wasted, because in most simulations the common case, e.g., a cache hit, requires no action.
Reference: [5] <author> Jeffrey D. Gee, Mark D. Hill, Dionisios N. Pnevmatikatos, and Alan Jay Smith. </author> <title> Cache Performance of the SPEC92 Benchmark Suite. </title> <journal> IEEE Micro, </journal> <volume> 13(4) </volume> <pages> 17-27, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Simulation is the most-widely-used method to evaluate memory-system performance. However, current simulation techniques are discouragingly slow; simulation times can be as much as two or three orders of magnitude slower than the execution time of the original program. Gee, et al. <ref> [5] </ref>, estimate that 17 months of processing time were used to obtain miss ratios for the SPEC92 benchmarks [20]. Fortunately, much of the inefficiency can be eliminated using a new simulation abstraction. <p> This is because most of the SPEC benchmarks have extremely low instruction cache miss ratios on the SPARCstation 10/51 <ref> [5] </ref>. Thus, Fast-Cache's code expansion has very little effect on their performance. In contrast, for codes with more significant instruction cache miss ratios, such as fpppp, instruction cache behavior has a noticeable impact. <p> We call this the scaled cache model. We can estimate the effect of a scaled cache using design target miss ratios [19] and other available data <ref> [5] </ref>. Design target miss ratios predict that decreasing the cache size by a factor of E increases the number of misses by p E. Data gathered by Gee, et al. [5] indicates that decreasing the instruction cache block size by E increases the number of instruction cache misses by E. <p> We can estimate the effect of a scaled cache using design target miss ratios [19] and other available data <ref> [5] </ref>. Design target miss ratios predict that decreasing the cache size by a factor of E increases the number of misses by p E. Data gathered by Gee, et al. [5] indicates that decreasing the instruction cache block size by E increases the number of instruction cache misses by E. Thus we expect that the number of instruction cache misses will be equal to E p E times the original number of instruction cache misses.
Reference: [6] <author> Mark D. Hill and Alan Jay Smith. </author> <title> Evaluating Associativity in CPU Caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-38(12):1612-1630, </volume> <month> December </month> <year> 1989. </year> <title> Appears in: </title> <booktitle> "Proceedings of the 1995 ACM SIGMETRICS Conference," </booktitle> <month> May </month> <year> 1995. </year> <note> Reprinted by permission of ACM. </note>
Reference-contexts: The active memory abstraction enables efficient simulation of a broad range of memory systems. Complex simulations can benefit from both the NULL handler and direct invocation of simulator functions. For example, many simulators that evaluate multiple cache configurations <ref> [6, 23, 12] </ref> use the property of inclusion [12] to limit the search for caches that contain a given block. No action is required for blocks that are contained in all simulated caches. An active memory implementation optimizes these references with the NULL handler.
Reference: [7] <author> Raj Jain. </author> <title> The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling. </title> <publisher> John Wiley & Sons, </publisher> <year> 1991. </year>
Reference-contexts: Traditionally, the first step was considered difficult and inefficient, usually requiring either expensive hardware monitors or slow instruction-level simulators <ref> [7] </ref>. The reference trace abstraction helps amortize this overhead by cleanly separating reference generation (step 1) from simulation (steps 2-4). Reference traces can be saved and reused for multiple simulations, with the added benefit of guaranteeing reproducible results.
Reference: [8] <author> Norman P. Jouppi and Steven J. E. Wilton. </author> <title> Tradeoffs in Two-Level On-Chip Caching. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 34-45, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Trap-driven simulation will be more efficient than Fast-Cache for some studies, such as large second-level caches or TLBs. However, Fast-Cache will be better for complete memory hierarchy simulations, since first-level caches are unlikely to be much larger than 64 kilobytes <ref> [8] </ref>. Furthermore, if the hardware is available, we can implement the active memory abstraction using the trap-driven technique as well. Thus the active memory abstraction gives the best performance over most of the design space.
Reference: [9] <author> James R. Larus. </author> <title> Efficient Program Tracing. </title> <journal> IEEE Computer, </journal> <volume> 26(5) </volume> <pages> 52-61, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Puzak [15] extended this work to set-associative memories by filtering references to a direct-mapped cache. However, software reference generation techniques have improved to the point that regenerating the trace is nearly as efficient as reading it from disk or tape <ref> [9] </ref>. On-the-fly simulation techniques|which combine steps 1-4|have become popular because they eliminate I/O overhead, context switches, and large storage requirements [4, 14, 3, 2]. Most on-the-fly simulation systems work by instrumenting a program to calculate each reference's effective address and then invoke the simulator.
Reference: [10] <author> James R. Larus and Eric Schnarr. EEL: </author> <title> Machine-Independent Executable Editing. </title> <booktitle> In Proceedings of the SIG-PLAN '95 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <month> June </month> <year> 1995. </year> <note> To Appear. </note>
Reference-contexts: Most of the registers are saved in the normal way using the SPARC register windows. However the stub must save the condition codes, if live, and some of the global registers. The table lookup instructions could be inserted with any instrumentation methodology. Fast-Cache uses the EEL system <ref> [10] </ref>, which takes an executable SPARC binary file, adds instrumentation code, and produces an executable that runs on the same machine.
Reference: [11] <author> M. Martonosi, A. Gupta, and T. Anderson. </author> <title> Effectiveness of Trace Sampling for Performance Debugging Tools. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 248-259, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Furthermore, this overhead is almost always wasted, because in most simulations the common case, e.g., a cache hit, requires no action. Clearly, optimizing the lookup (step 2) to quickly detect these "no action" cases can significantly improve simulation performance. MemSpy <ref> [11] </ref> builds on this observation by saving only the registers necessary to determine if a reference is a hit or a miss; hits branch around the remaining register saves and miss processing. MemSpy's optimization improves performance, but sacrifices trace-driven simulation's clean abstraction. <p> To simplify the discussion, we lump effective address calculation and action lookup into a single lookup term. Similarly, we lump action simulation and metric update into a single miss processing term. For trace-driven simulation, we consider an on-the-fly simulator that performs a procedure call to perform the lookup <ref> [21, 11] </ref>. To maintain a clean interface between the reference generator and the simulator, processor state is saved before invoking the simulator.
Reference: [12] <author> R. L. Mattson, J. Gecsei, D. R. Schultz, and I. L. Traiger. </author> <title> Evaluation Techniques for Storage Hierarchies. </title> <journal> IBM Systems Journal, </journal> <volume> 9(2) </volume> <pages> 78-117, </pages> <year> 1970. </year>
Reference-contexts: The active memory abstraction enables efficient simulation of a broad range of memory systems. Complex simulations can benefit from both the NULL handler and direct invocation of simulator functions. For example, many simulators that evaluate multiple cache configurations <ref> [6, 23, 12] </ref> use the property of inclusion [12] to limit the search for caches that contain a given block. No action is required for blocks that are contained in all simulated caches. An active memory implementation optimizes these references with the NULL handler. <p> The active memory abstraction enables efficient simulation of a broad range of memory systems. Complex simulations can benefit from both the NULL handler and direct invocation of simulator functions. For example, many simulators that evaluate multiple cache configurations [6, 23, 12] use the property of inclusion <ref> [12] </ref> to limit the search for caches that contain a given block. No action is required for blocks that are contained in all simulated caches. An active memory implementation optimizes these references with the NULL handler.
Reference: [13] <author> Ann Marie Grizzaffi Maynard, Colette M. Donnely, and Bret R. Olszewski. </author> <title> Contrasting Characteristics and Cache Performance of Technical and Multi-User Commercial Work-loads. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 145-156, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: This analysis indicates that Fast-Cache is likely to perform poorly for applications with high instruction cache miss ratios, such as the operating system or large commercial codes <ref> [13] </ref>. To reduce instruction cache pollution, we present an alternative implementation, Fast-Cache-Indirect, which inserts only two instructions| a jump-and-link plus effective address calculation|per memory reference. This reduces the code expansion from a factor of 4 to 1.6, for typical codes.
Reference: [14] <author> A. K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year> <note> Also available as Rice COMP TR 89-93. </note>
Reference-contexts: However, software reference generation techniques have improved to the point that regenerating the trace is nearly as efficient as reading it from disk or tape [9]. On-the-fly simulation techniques|which combine steps 1-4|have become popular because they eliminate I/O overhead, context switches, and large storage requirements <ref> [4, 14, 3, 2] </ref>. Most on-the-fly simulation systems work by instrumenting a program to calculate each reference's effective address and then invoke the simulator. For typical RISC instruction sets, the effective address calculation is trivial, requiring at most one additional instruction per reference. <p> Unfortunately, most on-the-fly simulation systems continue to use the reference trace abstraction. Although simple, this abstraction requires that the simulator either (i) perform a procedure call to process each reference, with the commensurate overhead to save and restore registers <ref> [4, 14] </ref>, or (ii) buffer the reference in memory, incurring buffer management overhead and memory system delays caused by cache pollution [2, 22]. Furthermore, this overhead is almost always wasted, because in most simulations the common case, e.g., a cache hit, requires no action.
Reference: [15] <author> T. R. Puzak. </author> <title> Analysis of Cache Replacement Algorithms. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <month> February </month> <year> 1985. </year> <title> Ph. D. </title> <type> Thesis, </type> <institution> Dept. of Electrical and Computer Engineering. </institution>
Reference-contexts: Smith [18] proposed deleting references to the n most recently used blocks. The subsequent trace can be used to obtain approximate miss counts for fully associative memories that use LRU replacement with more than n blocks. Puzak <ref> [15] </ref> extended this work to set-associative memories by filtering references to a direct-mapped cache. However, software reference generation techniques have improved to the point that regenerating the trace is nearly as efficient as reading it from disk or tape [9].
Reference: [16] <author> Steven K. Reinhardt, Babak Falsafi, and David A. Wood. </author> <title> Kernel Support for the Wisconsin Wind Tunnel. </title> <booktitle> In Proceedings of the Usenix Symposium on Microkernels and Other Kernel Architectures, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: Trap-driven simulators represent the other extreme, incurring no overhead for cache hits. Unfortunately, target cache misses cause memory system exceptions that invoke the kernel, resulting in miss processing overhead of approximately 250 cycles on highly tuned systems <ref> [25, 24, 16] </ref>. Therefore, trap-driven simulation performance will be highly dependent on the target miss ratio. Fast-Cache's lookup overhead is roughly 4 cycles (the mean lookup overhead for typical programs).
Reference: [17] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The ATOM cache simulator performs a similar optimization more cleanly, using the OM liveness analysis to detect, and save, caller-save registers used in the simulator routines [21]. However, ATOM still incurs unnecessary procedure linkage overhead in the no-action cases. A recent alternative technique, trap-driven simulation <ref> [17, 25] </ref>, optimizes "no action" cases to their logical extreme. Trap-driven simulators exploit the characteristics of the simulation platform to implement effective address calculation and lookup (steps 1 and 2) in hardware. <p> Portability suffers because these simulators require operating system and hardware support that is not readily available on most machines. Generality is lacking because current trap-driven simulators do not simulate arbitrary memory systems: the Wisconsin Wind Tunnel does not simulate stack references <ref> [17] </ref>, while Tapeworm II does not simulate any data references [25]. Furthermore, the overhead of memory exceptions can overwhelm the benefits of "free" lookups for simulations with non-negligible miss ratios.
Reference: [18] <author> A. J. Smith. </author> <title> Two Methods for Efficient Analysis of Memory Address Trace Data. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 3(12), </volume> <month> January </month> <year> 1977. </year>
Reference-contexts: Many techniques have been developed to improve trace-driven simulation time by reducing the size of reference traces. Some accomplish this by filtering out references that would hit in the simulated cache. Smith <ref> [18] </ref> proposed deleting references to the n most recently used blocks. The subsequent trace can be used to obtain approximate miss counts for fully associative memories that use LRU replacement with more than n blocks. Puzak [15] extended this work to set-associative memories by filtering references to a direct-mapped cache.
Reference: [19] <author> Alan Jay Smith. </author> <title> Line (block) size choice for CPU cache memories. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(9):1063-1075, </volume> <month> September </month> <year> 1987. </year>
Reference-contexts: We call this the scaled cache model. We can estimate the effect of a scaled cache using design target miss ratios <ref> [19] </ref> and other available data [5]. Design target miss ratios predict that decreasing the cache size by a factor of E increases the number of misses by p E.
Reference: [20] <institution> SPEC. SPEC Newsletter, </institution> <month> Dec </month> <year> 1991. </year>
Reference-contexts: Gee, et al. [5], estimate that 17 months of processing time were used to obtain miss ratios for the SPEC92 benchmarks <ref> [20] </ref>. Fortunately, much of the inefficiency can be eliminated using a new simulation abstraction. The traditional approach|trace-driven simulation|employs a reference trace abstraction: a reference generator produces a list of memory addresses which are consumed by the simulation. <p> We can use Equation 2 to get a rough idea of the relative performance of the various simulation techniques. Figure 2 shows simulator slowdown versus target miss ratio, using a CP I orig of 1.22 and reference ratio r = 0:25 (derived from the SPEC92 benchmark program compress <ref> [20] </ref>). The simulator parameters are miss processing overhead, C miss , of 250 for trap-driven, 3 for trace-driven, and 55 for Fast-Cache, and lookup overhead, C lookup , of 0 for trap-driven, 25 for trace-driven, 4 for Fast-Cache. The results in Figure 2 confirm our expectations. <p> ) Refs (10 9 ) r f cc CPI Compress 0.08 0.02 0.25 0.95 1.22 Fpppp 5.41 2.58 0.48 0.83 1.22 Tomcatv 1.65 0.67 0.41 0.52 1.61 Xlisp 5.82 1.53 0.26 0.98 1.38 Table 2: Benchmark Characteristics To validate our model, we use 4 programs from the SPEC92 benchmark suite <ref> [20] </ref>: compress, fpppp, tomcatv, and xlisp. All programs operate on the SPEC input files, and are compiled with gcc version 2.6.0 or f77 version 1.4 at optimization level -O4. Program characteristics are shown in Table 2.
Reference: [21] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM A System for Building Customized Program Analysis Tools. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 196-205, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Measurements on a SPARCstation 10/51 show that simple data-cache simulations run only 2 to 7 times slower than the original program. This is comparable to many execution-time profilers and two to three times faster than published numbers for highly optimized trace-driven simulators <ref> [21] </ref>. As described in Section 4, Fast-Cache efficiently implements this abstraction by inserting 9 instructions before each memory reference to look up a memory block's state and invoke the user-specified handler. <p> The ATOM cache simulator performs a similar optimization more cleanly, using the OM liveness analysis to detect, and save, caller-save registers used in the simulator routines <ref> [21] </ref>. However, ATOM still incurs unnecessary procedure linkage overhead in the no-action cases. A recent alternative technique, trap-driven simulation [17, 25], optimizes "no action" cases to their logical extreme. <p> To simplify the discussion, we lump effective address calculation and action lookup into a single lookup term. Similarly, we lump action simulation and metric update into a single miss processing term. For trace-driven simulation, we consider an on-the-fly simulator that performs a procedure call to perform the lookup <ref> [21, 11] </ref>. To maintain a clean interface between the reference generator and the simulator, processor state is saved before invoking the simulator.
Reference: [22] <author> Craig B. Stunkel and W. Kent Fuchs. TRAPEDS: </author> <title> Producing Traces for Multicomputers Via Execution Driven Simulation. </title> <booktitle> In Proceedings of the 1989 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 70-78, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Although simple, this abstraction requires that the simulator either (i) perform a procedure call to process each reference, with the commensurate overhead to save and restore registers [4, 14], or (ii) buffer the reference in memory, incurring buffer management overhead and memory system delays caused by cache pollution <ref> [2, 22] </ref>. Furthermore, this overhead is almost always wasted, because in most simulations the common case, e.g., a cache hit, requires no action. Clearly, optimizing the lookup (step 2) to quickly detect these "no action" cases can significantly improve simulation performance.
Reference: [23] <author> R. A. Sugumar and S. G. Abraham. </author> <title> Efficient Simulation of Multiple Cache Configurations using Binomial Trees. </title> <type> Technical Report CSE-TR-111-91, </type> <year> 1991. </year>
Reference-contexts: The active memory abstraction enables efficient simulation of a broad range of memory systems. Complex simulations can benefit from both the NULL handler and direct invocation of simulator functions. For example, many simulators that evaluate multiple cache configurations <ref> [6, 23, 12] </ref> use the property of inclusion [12] to limit the search for caches that contain a given block. No action is required for blocks that are contained in all simulated caches. An active memory implementation optimizes these references with the NULL handler.
Reference: [24] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Hardware and Software Support for Efficient Exception Handling. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 110-119, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Trap-driven simulators represent the other extreme, incurring no overhead for cache hits. Unfortunately, target cache misses cause memory system exceptions that invoke the kernel, resulting in miss processing overhead of approximately 250 cycles on highly tuned systems <ref> [25, 24, 16] </ref>. Therefore, trap-driven simulation performance will be highly dependent on the target miss ratio. Fast-Cache's lookup overhead is roughly 4 cycles (the mean lookup overhead for typical programs).
Reference: [25] <author> Richard Uhlig, David Nagle, Trevor Mudge, and Stuart Sechrest. </author> <title> Trap-Driven Simulation with TapewormII. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 132-144, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The ATOM cache simulator performs a similar optimization more cleanly, using the OM liveness analysis to detect, and save, caller-save registers used in the simulator routines [21]. However, ATOM still incurs unnecessary procedure linkage overhead in the no-action cases. A recent alternative technique, trap-driven simulation <ref> [17, 25] </ref>, optimizes "no action" cases to their logical extreme. Trap-driven simulators exploit the characteristics of the simulation platform to implement effective address calculation and lookup (steps 1 and 2) in hardware. <p> Generality is lacking because current trap-driven simulators do not simulate arbitrary memory systems: the Wisconsin Wind Tunnel does not simulate stack references [17], while Tapeworm II does not simulate any data references <ref> [25] </ref>. Furthermore, the overhead of memory exceptions can overwhelm the benefits of "free" lookups for simulations with non-negligible miss ratios. The active memory abstraction|described in detail in the next section|combines the efficiency of trap-driven simulation with the generality and portability of trace-driven simulation. <p> Trap-driven simulators represent the other extreme, incurring no overhead for cache hits. Unfortunately, target cache misses cause memory system exceptions that invoke the kernel, resulting in miss processing overhead of approximately 250 cycles on highly tuned systems <ref> [25, 24, 16] </ref>. Therefore, trap-driven simulation performance will be highly dependent on the target miss ratio. Fast-Cache's lookup overhead is roughly 4 cycles (the mean lookup overhead for typical programs).
Reference: [26] <author> Robert Wahbe, Steven Lucco, Thomas E. Anderson, and Su-san L. Graham. </author> <title> Efficient Software-Based Fault Isolation. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 203-216, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: These numbers are approximate, of course, since inserting additional instructions may introduce or eliminate pipeline interlocks and affect the superscalar issue rate <ref> [26] </ref>. This sequence could be further optimized on the SuperSPARC by scheduling independent instructions from the original program with the Fast-Cache inserted instructions. If the condition codes are live, we cannot use a branch instruction.
Reference: [27] <author> David A. Wood, Mark D. Hill, and R. E. Kessler. </author> <title> A Model for Estimating Trace-Sample Miss Ratios. </title> <booktitle> In Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 79-89, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Second, the upper bound is conservative, significantly overestimating the slowdown due to data cache pollution. The upper bound is overly pessimistic because (i) not all Fast-Cache data references will actually miss, and (ii) when they do miss the probability of replacing a live block is approximately one-third, not one <ref> [27] </ref>.
References-found: 27

