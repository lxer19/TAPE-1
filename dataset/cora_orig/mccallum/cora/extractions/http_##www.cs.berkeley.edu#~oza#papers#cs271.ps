URL: http://www.cs.berkeley.edu/~oza/papers/cs271.ps
Refering-URL: http://www.cs.berkeley.edu/~oza/papers.html
Root-URL: 
Email: oza@cs.berkeley.edu  
Title: Markov Decision Problems  
Author: Nikunj C. Oza 
Address: Berkeley, CA 94720-1776  
Affiliation: Computer Science Division University of California  
Abstract: Markov Decision Problems (MDPs) are the foundation for many problems that are of interest to researchers in Artificial Intelligence and Operations Research. In this paper, we will review what is known about algorithms for solving MDPs as well as the complexity of solving MDPs in general. We will argue that, even though there are theoretically efficient algorithms for solving MDPs, these algorithms are inefficient enough in practice that efficient algorithms for finding exact and approximate solutions to MDPs are still needed. We will review some aspects of MDPs that we hope can potentially be exploited by these currently unknown algorithms.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Boutilier, Craig, Dearden, Richard, and Goldszmidt, Moises, </author> <title> Exploiting Structure in Policy Construction Proceedings IJCAI-95, </title> <institution> Montreal, Canada, IJCAI, </institution> <year> 1995. </year>
Reference-contexts: This allows prioritized sweeping to focus on states that need updating the most. Structure in the state space should allow us to aggregate states and decompose problems into weakly-coupled subproblems, leading to simpler computation. Some in the planning <ref> [1] </ref> and reinforcement learning communities have explored this possibility. For example, in the top grid of Figure 2, the first three states of the second row can be considered one state because the desired action in each state is the same.
Reference: [2] <author> Dean, Thomas, Kaelbling, Leslie, Kirman, Jak, and Nicholson, Ann, </author> <title> Planning With Deadlines in Stochastic Domains, </title> <booktitle> Proceedings AAAI-93, </booktitle> <address> Washington, D.C., </address> <publisher> AAAI, </publisher> <year> 1993, </year> <pages> 574-579. </pages>
Reference-contexts: It may be possible to avoid exploring the entire state space, such as in MDPs that have low discount factors or have relatively short, straight paths to a terminating state (if one exists). Dean, Kaelbling, Kirman, and Nicholson <ref> [2] </ref> solve MDPs using an algorithm that takes advantage of knowledge of the start state and assumes that there are one or more states that are considered goal states.
Reference: [3] <author> Moore, Andrew and Atkeson, Chris Prioritized Sweeping: </author> <title> Reinforcement Learning with Less Data and Less Time Machine Learning 13(1), </title> <booktitle> 1993, </booktitle> <pages> 103-130. </pages>
Reference-contexts: They do not give error bounds on the algorithms performance, but their empirical data shows that it works well. This scheme is also similar to well-established schemes for reinforcement learning of MDPs such as temporal-difference (TD) learning [5] and prioritized sweeping <ref> [3] </ref>. Prioritized sweeping is especially interesting because it updates the value function on neighboring states in decreasing order of the size of the change in the value function and performs as many updates as possible in the time allotted.
Reference: [4] <author> Russell, Stuart, Norvig, Peter, </author> <title> Artificial Intelligence A Modern Approach, </title> <publisher> (Prentice Hall, </publisher> <address> New Jersey, </address> <year> 1995). </year>
Reference-contexts: In the second grid of Figure 2, however, these states should not be considered one state, because the action is different in state (2,2), so we can break up the single state into the original three states. Dynamic Belief Networks <ref> [4] </ref> represent the states of an MDP in terms of state variables. Not only does this constitute an exponential reduction in the size of the representation, but it allows us to represent conditional independencies among the state variables to simplify the 12 probability calculations.
Reference: [5] <author> Sutton, </author> <title> Rich Learning to Predict by the Method of Temporal Differences Machine Learning 3, </title> <booktitle> 1988, </booktitle> <pages> 9-44. </pages>
Reference-contexts: They do not give error bounds on the algorithms performance, but their empirical data shows that it works well. This scheme is also similar to well-established schemes for reinforcement learning of MDPs such as temporal-difference (TD) learning <ref> [5] </ref> and prioritized sweeping [3]. Prioritized sweeping is especially interesting because it updates the value function on neighboring states in decreasing order of the size of the change in the value function and performs as many updates as possible in the time allotted.
References-found: 5

