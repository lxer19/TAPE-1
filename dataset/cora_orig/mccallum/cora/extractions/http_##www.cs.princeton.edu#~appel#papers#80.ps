URL: http://www.cs.princeton.edu/~appel/papers/80.ps
Refering-URL: http://www.cs.princeton.edu/~appel/papers/
Root-URL: http://www.cs.princeton.edu
Title: Concise specifications of locally optimal code generators  
Author: Andrew W. Appel 
Note: Limitations and possible improvements to the specification language are discussed.  
Address: Princeton, NJ 08544  
Affiliation: Department of Computer Science Princeton University  
Abstract: Encoding a complex architecture as a grammar for a dynamic-programming code-generator-generator shows the expressive power of the technique. Each instruction, addressing mode, register and class can be expressed individually in the grammar. The grammar can be factored much more readily than with the Graham-Glanville LR(1) algorithm, so it can be much more concise. Twig specifications for the VAX and MC68020 are described, and the corresponding code generators select very good (and under the right assumptions, optimal) instruction sequences. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> References </institution>
Reference: 1. <author> A. V. Aho, M. Ganapathi, and S. W. K. Tjiang, </author> <title> ``Code generation using tree matching and dynamic programming,'' </title> <journal> ACM Trans. Prog. Lang. and Systems, </journal> <note> vol. (to appear), </note> <year> 1989. </year>
Reference-contexts: This paper describes techniques for using Aho, Ganapathi, and Tjiang's Twig <ref> [1] </ref>, a code-generator generator that uses tree matching and dynamic programming. The instruction sets of the DEC VAX and Motorola 68020 are used as a illustrative examples. The VAX architecture can be described with only 112 rules; the 68020 is somewhat more complicated. <p> For this purpose, attribute grammars are convenient [17]. An attribute grammar describing a given machine can be much shorter than the equivalent context-free grammar. 1.3. Optimal instruction selection from formal instruction set descriptions Combining grammar-based machine descriptions with dynamic programming, as in the Twig system <ref> [1] </ref>, leads to concise and powerful instruction selection specifications. <p> This paper demonstrates the concise and powerful nature of code generators based on dynamic-programming parsers by using the VAX and MC68020 instruction sets as illustrative examples. 2. Overview of Twig Aho, Ganapathi, and Tjiang's code-generator generator, Twig, is described in <ref> [1] </ref> and [18]. This section provides just a brief overview of the system. - 4 - The Twig system takes a specification in the form of an annotated grammar and produces a code generator. <p> argument, and emits the appropriate assembly-language phrase: - 22 - emitlocation (t) Tree t; - switch (t-&gt;kind) -case 1: /* reg */ emit ("(r%d)",t-&gt;reg); givereg (t); break; case 4: /* MEM (OP (PLUS,name,reg)) */ emit ("*"); t=t-&gt;u.child [0]; /* fall through */ case 2: /* OP (PLUS,name,reg) */ emitname (t-&gt;u.child <ref> [1] </ref>); emit ("(r%d)",t-&gt;u.child [2]->reg); givereg (t-&gt;u.child [2]->reg); break; case 5: /* MEM (OP (PLUS,reg,name)) */ emit ("*"); t=t-&gt;u.child [0]; /* fall through */ case 3: /* OP (PLUS,reg,name) */ emitname (t-&gt;u.child [2]); emit ("(r%d)",t-&gt;u.child [1]->reg); givereg (t-&gt;u.child [1]->reg); break; case 6: /* name */ emitname (t); break; - 4.3.5. <p> entire instruction is built, so a separate procedure traverses the tree looking at the kind fields of the nodes: - 25 - emitcomputation (t) Tree t; -switch (t-&gt;kind) -case 31: /* OP (binop, operand, operand) */ -emit ("%s%c3"t",vaxop [t-&gt;u.child [0]->op], t-&gt;u.child [0]->u.op.sizecode [t->x.size]); emitoperand (t-&gt;u.child [2]); emit (","); emitoperand (t-&gt;u.child <ref> [1] </ref>); emit (","); break; case 32: /* UNOP (unop,operand) */ -emit ("%s%c"t",vaxop [t-&gt;u.child [0]->op],sizecode [t->x.size]); emitoperand (t-&gt;u.child [1]); emit (","); break; case 33: /* UNOP (cvtop,operand) */ -emit ("cvt%c%c"t", t-&gt;u.child [0]->u.cvt.sizecodefrom [t->u.child [1]->x.size], t-&gt;u.child [0]->u.cvt.sizecodeto [t->x.size]); emitoperand (t-&gt;u.child [1]); emit (","); break; case 34: /* UNOP (cvtopu,computation) */ emitcomputation (t-&gt;u.child [1]); <p> the nodes: - 25 - emitcomputation (t) Tree t; -switch (t-&gt;kind) -case 31: /* OP (binop, operand, operand) */ -emit ("%s%c3"t",vaxop [t-&gt;u.child [0]->op], t-&gt;u.child [0]->u.op.sizecode [t->x.size]); emitoperand (t-&gt;u.child [2]); emit (","); emitoperand (t-&gt;u.child <ref> [1] </ref>); emit (","); break; case 32: /* UNOP (unop,operand) */ -emit ("%s%c"t",vaxop [t-&gt;u.child [0]->op],sizecode [t->x.size]); emitoperand (t-&gt;u.child [1]); emit (","); break; case 33: /* UNOP (cvtop,operand) */ -emit ("cvt%c%c"t", t-&gt;u.child [0]->u.cvt.sizecodefrom [t->u.child [1]->x.size], t-&gt;u.child [0]->u.cvt.sizecodeto [t->x.size]); emitoperand (t-&gt;u.child [1]); emit (","); break; case 34: /* UNOP (cvtopu,computation) */ emitcomputation (t-&gt;u.child [1]); break; case 35: /* OP (LSHIFT,operand,operand) */ emit ("ashl"t"); emitoperand (t-&gt;u.child [2]); emit (","); emitoperand (t-&gt;u.child [1]); <p> [t-&gt;u.child [0]->op], t-&gt;u.child [0]->u.op.sizecode [t->x.size]); emitoperand (t-&gt;u.child [2]); emit (","); emitoperand (t-&gt;u.child <ref> [1] </ref>); emit (","); break; case 32: /* UNOP (unop,operand) */ -emit ("%s%c"t",vaxop [t-&gt;u.child [0]->op],sizecode [t->x.size]); emitoperand (t-&gt;u.child [1]); emit (","); break; case 33: /* UNOP (cvtop,operand) */ -emit ("cvt%c%c"t", t-&gt;u.child [0]->u.cvt.sizecodefrom [t->u.child [1]->x.size], t-&gt;u.child [0]->u.cvt.sizecodeto [t->x.size]); emitoperand (t-&gt;u.child [1]); emit (","); break; case 34: /* UNOP (cvtopu,computation) */ emitcomputation (t-&gt;u.child [1]); break; case 35: /* OP (LSHIFT,operand,operand) */ emit ("ashl"t"); emitoperand (t-&gt;u.child [2]); emit (","); emitoperand (t-&gt;u.child [1]); emit (","); break; case 36: /* OP (RSHIFT,reg,reg) */ -struct tree q; getreg (&q); emit ("subl3"tr%d,$32,r%d"nextzv"tr%d,r%d,r%d," , t-&gt;u.child [2]->reg,q.reg,t->u.child [2]->reg,q.reg, t-&gt;u.child <p> <ref> [1] </ref>); emit (","); break; case 32: /* UNOP (unop,operand) */ -emit ("%s%c"t",vaxop [t-&gt;u.child [0]->op],sizecode [t->x.size]); emitoperand (t-&gt;u.child [1]); emit (","); break; case 33: /* UNOP (cvtop,operand) */ -emit ("cvt%c%c"t", t-&gt;u.child [0]->u.cvt.sizecodefrom [t->u.child [1]->x.size], t-&gt;u.child [0]->u.cvt.sizecodeto [t->x.size]); emitoperand (t-&gt;u.child [1]); emit (","); break; case 34: /* UNOP (cvtopu,computation) */ emitcomputation (t-&gt;u.child [1]); break; case 35: /* OP (LSHIFT,operand,operand) */ emit ("ashl"t"); emitoperand (t-&gt;u.child [2]); emit (","); emitoperand (t-&gt;u.child [1]); emit (","); break; case 36: /* OP (RSHIFT,reg,reg) */ -struct tree q; getreg (&q); emit ("subl3"tr%d,$32,r%d"nextzv"tr%d,r%d,r%d," , t-&gt;u.child [2]->reg,q.reg,t->u.child [2]->reg,q.reg, t-&gt;u.child [1]->reg); givereg (&q); givereg (t-&gt;u.child [1]); givereg (t-&gt;u.child [2]); break; case 37: <p> <ref> [1] </ref>); emit (","); break; case 33: /* UNOP (cvtop,operand) */ -emit ("cvt%c%c"t", t-&gt;u.child [0]->u.cvt.sizecodefrom [t->u.child [1]->x.size], t-&gt;u.child [0]->u.cvt.sizecodeto [t->x.size]); emitoperand (t-&gt;u.child [1]); emit (","); break; case 34: /* UNOP (cvtopu,computation) */ emitcomputation (t-&gt;u.child [1]); break; case 35: /* OP (LSHIFT,operand,operand) */ emit ("ashl"t"); emitoperand (t-&gt;u.child [2]); emit (","); emitoperand (t-&gt;u.child [1]); emit (","); break; case 36: /* OP (RSHIFT,reg,reg) */ -struct tree q; getreg (&q); emit ("subl3"tr%d,$32,r%d"nextzv"tr%d,r%d,r%d," , t-&gt;u.child [2]->reg,q.reg,t->u.child [2]->reg,q.reg, t-&gt;u.child [1]->reg); givereg (&q); givereg (t-&gt;u.child [1]); givereg (t-&gt;u.child [2]); break; case 37: /* OP (RSHIFT,reg,CONST) */ emit ("extzv"t$%d,$%d,r%d,", t-&gt;u.child [2]->u.ivalue, 32-t-&gt;u.child [2]->u.ivalue, t-&gt;u.child [1]->reg); givereg (t-&gt;u.child [1]); break; case <p> /* UNOP (cvtopu,computation) */ emitcomputation (t-&gt;u.child <ref> [1] </ref>); break; case 35: /* OP (LSHIFT,operand,operand) */ emit ("ashl"t"); emitoperand (t-&gt;u.child [2]); emit (","); emitoperand (t-&gt;u.child [1]); emit (","); break; case 36: /* OP (RSHIFT,reg,reg) */ -struct tree q; getreg (&q); emit ("subl3"tr%d,$32,r%d"nextzv"tr%d,r%d,r%d," , t-&gt;u.child [2]->reg,q.reg,t->u.child [2]->reg,q.reg, t-&gt;u.child [1]->reg); givereg (&q); givereg (t-&gt;u.child [1]); givereg (t-&gt;u.child [2]); break; case 37: /* OP (RSHIFT,reg,CONST) */ emit ("extzv"t$%d,$%d,r%d,", t-&gt;u.child [2]->u.ivalue, 32-t-&gt;u.child [2]->u.ivalue, t-&gt;u.child [1]->reg); givereg (t-&gt;u.child [1]); break; case 1: case 2: case 3: case 4: case 5: case 6: case 7: case 8: /* location */ emit ("movab"t"); emitlocation (t); emit (","); break; case 11: <p> emitoperand (t-&gt;u.child <ref> [1] </ref>); emit (","); break; case 36: /* OP (RSHIFT,reg,reg) */ -struct tree q; getreg (&q); emit ("subl3"tr%d,$32,r%d"nextzv"tr%d,r%d,r%d," , t-&gt;u.child [2]->reg,q.reg,t->u.child [2]->reg,q.reg, t-&gt;u.child [1]->reg); givereg (&q); givereg (t-&gt;u.child [1]); givereg (t-&gt;u.child [2]); break; case 37: /* OP (RSHIFT,reg,CONST) */ emit ("extzv"t$%d,$%d,r%d,", t-&gt;u.child [2]->u.ivalue, 32-t-&gt;u.child [2]->u.ivalue, t-&gt;u.child [1]->reg); givereg (t-&gt;u.child [1]); break; case 1: case 2: case 3: case 4: case 5: case 6: case 7: case 8: /* location */ emit ("movab"t"); emitlocation (t); emit (","); break; case 11: case 12: case 13: case 14: case 15: /* ioperand */ emit ("mova%c"t",sizecode [sizeioperand (t)]); emitioperand (t); emit (","); break; case
Reference: 2. <author> R. Sethi, </author> <title> ``Complete register allocation problems,'' </title> <journal> SIAM J. Computing, </journal> <volume> vol. 4, no. 3, </volume> <pages> pp. 226-248, </pages> <publisher> SIAM, </publisher> <year> 1975. </year>
Reference-contexts: considered only trees -- graphs in which each source of a value has only one use. (The ``trivial'' sources, such as constants and the addresses of program variables, may have several uses; each use will be a leaf of the tree.) For more general graphs, optimal instruction selection becomes NP-complete <ref> [2, 3] </ref>. For trees, it is possible to find optimal solutions (with appropriate restrictions on the scope of the problem). For example, the problem of optimal register allocation [4, 5] can be solved using a simple bottom-up labelling algorithm. <p> (OP (PLUS,name,reg)) */ emit ("*"); t=t-&gt;u.child [0]; /* fall through */ case 2: /* OP (PLUS,name,reg) */ emitname (t-&gt;u.child [1]); emit ("(r%d)",t-&gt;u.child <ref> [2] </ref>->reg); givereg (t-&gt;u.child [2]->reg); break; case 5: /* MEM (OP (PLUS,reg,name)) */ emit ("*"); t=t-&gt;u.child [0]; /* fall through */ case 3: /* OP (PLUS,reg,name) */ emitname (t-&gt;u.child [2]); emit ("(r%d)",t-&gt;u.child [1]->reg); givereg (t-&gt;u.child [1]->reg); break; case 6: /* name */ emitname (t); break; - 4.3.5. Index mode Many of the VAX addressing modes can be augmented by an ``index,'' which is a register multiplied by a constant (1, 2, 4, or 8, depending on the operation code). <p> can't be emitted until an entire instruction is built, so a separate procedure traverses the tree looking at the kind fields of the nodes: - 25 - emitcomputation (t) Tree t; -switch (t-&gt;kind) -case 31: /* OP (binop, operand, operand) */ -emit ("%s%c3"t",vaxop [t-&gt;u.child [0]->op], t-&gt;u.child [0]->u.op.sizecode [t->x.size]); emitoperand (t-&gt;u.child <ref> [2] </ref>); emit (","); emitoperand (t-&gt;u.child [1]); emit (","); break; case 32: /* UNOP (unop,operand) */ -emit ("%s%c"t",vaxop [t-&gt;u.child [0]->op],sizecode [t->x.size]); emitoperand (t-&gt;u.child [1]); emit (","); break; case 33: /* UNOP (cvtop,operand) */ -emit ("cvt%c%c"t", t-&gt;u.child [0]->u.cvt.sizecodefrom [t->u.child [1]->x.size], t-&gt;u.child [0]->u.cvt.sizecodeto [t->x.size]); emitoperand (t-&gt;u.child [1]); emit (","); break; case 34: /* UNOP <p> [t-&gt;u.child [0]->op],sizecode [t->x.size]); emitoperand (t-&gt;u.child [1]); emit (","); break; case 33: /* UNOP (cvtop,operand) */ -emit ("cvt%c%c"t", t-&gt;u.child [0]->u.cvt.sizecodefrom [t->u.child [1]->x.size], t-&gt;u.child [0]->u.cvt.sizecodeto [t->x.size]); emitoperand (t-&gt;u.child [1]); emit (","); break; case 34: /* UNOP (cvtopu,computation) */ emitcomputation (t-&gt;u.child [1]); break; case 35: /* OP (LSHIFT,operand,operand) */ emit ("ashl"t"); emitoperand (t-&gt;u.child <ref> [2] </ref>); emit (","); emitoperand (t-&gt;u.child [1]); emit (","); break; case 36: /* OP (RSHIFT,reg,reg) */ -struct tree q; getreg (&q); emit ("subl3"tr%d,$32,r%d"nextzv"tr%d,r%d,r%d," , t-&gt;u.child [2]->reg,q.reg,t->u.child [2]->reg,q.reg, t-&gt;u.child [1]->reg); givereg (&q); givereg (t-&gt;u.child [1]); givereg (t-&gt;u.child [2]); break; case 37: /* OP (RSHIFT,reg,CONST) */ emit ("extzv"t$%d,$%d,r%d,", t-&gt;u.child [2]->u.ivalue, 32-t-&gt;u.child [2]->u.ivalue, t-&gt;u.child [1]->reg); <p> */ emitcomputation (t-&gt;u.child [1]); break; case 35: /* OP (LSHIFT,operand,operand) */ emit ("ashl"t"); emitoperand (t-&gt;u.child <ref> [2] </ref>); emit (","); emitoperand (t-&gt;u.child [1]); emit (","); break; case 36: /* OP (RSHIFT,reg,reg) */ -struct tree q; getreg (&q); emit ("subl3"tr%d,$32,r%d"nextzv"tr%d,r%d,r%d," , t-&gt;u.child [2]->reg,q.reg,t->u.child [2]->reg,q.reg, t-&gt;u.child [1]->reg); givereg (&q); givereg (t-&gt;u.child [1]); givereg (t-&gt;u.child [2]); break; case 37: /* OP (RSHIFT,reg,CONST) */ emit ("extzv"t$%d,$%d,r%d,", t-&gt;u.child [2]->u.ivalue, 32-t-&gt;u.child [2]->u.ivalue, t-&gt;u.child [1]->reg); givereg (t-&gt;u.child [1]); break; case 1: case 2: case 3: case 4: case 5: case 6: case 7: case 8: /* location */ emit ("movab"t"); emitlocation (t); emit (","); break; case 11: case 12: case
Reference: 3. <author> J. Bruno and R. Sethi, </author> <title> ``Code generation for a one-register machine,'' </title> <journal> J. ACM, </journal> <volume> vol. 23, no. 3, </volume> <pages> pp. 502-510, </pages> <publisher> ACM, </publisher> <year> 1976. </year>
Reference-contexts: considered only trees -- graphs in which each source of a value has only one use. (The ``trivial'' sources, such as constants and the addresses of program variables, may have several uses; each use will be a leaf of the tree.) For more general graphs, optimal instruction selection becomes NP-complete <ref> [2, 3] </ref>. For trees, it is possible to find optimal solutions (with appropriate restrictions on the scope of the problem). For example, the problem of optimal register allocation [4, 5] can be solved using a simple bottom-up labelling algorithm. <p> Here is the declaration of the type Tree: typedef struct tree *Tree; struct tree -char op; char reg, kind; union -int size; Label proclabel; . . . other fields - x; union -Tree child <ref> [3] </ref>; Label label; int ivalue; double fvalue; struct temp *temp; . . . other fields - u; The fields named x.size, x.proclabel, u.label, u.ivalue, u.fvalue, and u.temp correspond to the similarly named attributes described in a previous section.
Reference: 4. <author> A. P. Ershov, </author> <title> ``On programming of arithmetic operations,'' </title> <journal> Comm. ACM, </journal> <volume> vol. 1, no. 8, </volume> <pages> pp. 3-6, </pages> <publisher> ACM, </publisher> <year> 1958. </year>
Reference-contexts: For trees, it is possible to find optimal solutions (with appropriate restrictions on the scope of the problem). For example, the problem of optimal register allocation <ref> [4, 5] </ref> can be solved using a simple bottom-up labelling algorithm.
Reference: 5. <author> R. Sethi and J. D. Ullman, </author> <title> ``The generation of optimal code for arithmetic expressions,'' </title> <journal> J. Assoc. Computing Machinery, </journal> <pages> pp. 715-728, </pages> <publisher> ACM, </publisher> <year> 1970. </year>
Reference-contexts: For trees, it is possible to find optimal solutions (with appropriate restrictions on the scope of the problem). For example, the problem of optimal register allocation <ref> [4, 5] </ref> can be solved using a simple bottom-up labelling algorithm. <p> The DEFAULT_COST for each node is (roughly) the sum of the costs of the children. That is, the space values of the children are summed, the time values are summed, and the sideEffect values are unioned. In addition, a variant [21] of the Sethi-Ullman algorithm <ref> [5] </ref> for minimizing register-usage is used. This will require reordering the computation of the children of expression-nodes (as long as there are no side-effects among the reordered children). The cost of the optimal ordering is computed in DEFAULT_COST, yielding the cost value maxregs.
Reference: 6. <author> A. V. Aho and S. C. Johnson, </author> <title> ``Optimal code generation for expression trees,'' </title> <journal> J. ACM, </journal> <volume> vol. 23, no. 3, </volume> <pages> pp. 488-501, </pages> <publisher> ACM, </publisher> <year> 1976. </year>
Reference-contexts: For example, the problem of optimal register allocation [4, 5] can be solved using a simple bottom-up labelling algorithm. Optimal instruction selection for machines with one class of general registers is solved using a bottom-up dynamic programming algorithm <ref> [6] </ref> (the algorithm uses dynamic programming in that - 2 - the optimal solution for each node is determined from the optimal solutions of its descendants). For machines with complicated addressing modes, this algorithm becomes unwieldy, with hundreds of cases. <p> When several different ways of matching the same node are found, only the cheapest is remembered; the running time, therefore, is proportional to the number of patterns (a constant) times the size of the tree. In essence, this is the Aho-Johnson algorithm <ref> [6] </ref> for instruction selection. The problem with this algorithm is that a tree-pattern must represent an entire instruction; as illustrated above, some instructions require many patterns to describe all the different ways they can be used. <p> Actually, each nonterminal that can stand for an expression would have to be replicated. To cope with this large but consistent set of nonterminal symbols, perhaps a way of specifying vectors of nonterminals would be appropriate. The Aho-Johnson register-allocation algorithm <ref> [6] </ref> operates on such vectors, but without the generality of context-free grammars. This would produce specifications just as concise as the ones in this paper, but they would more accurately select optimal instruction sequences.
Reference: 7. <author> D. E. Knuth, </author> <title> ``A generalization of Dijkstra's algorithm,'' </title> <journal> Information Processing Letters, </journal> <volume> vol. 6, </volume> <pages> pp. 1-5, </pages> <year> 1977. </year>
Reference-contexts: For machines with complicated addressing modes, this algorithm becomes unwieldy, with hundreds of cases. Dynamic programming can be extended to machines with several classes of registers <ref> [7] </ref> by an algorithm analogous to the CYK algorithm [8, 9] for parsing by dynamic programming.
Reference: 8. <author> T. Kasami, </author> <title> ``An efficient recognition and syntax algorithm for context-free languages,'' </title> <type> Scientific Report AFCRL-65-758, </type> <institution> Air Force Cambridge Research Lab, Bedford, </institution> <address> Mass., </address> <year> 1965. </year>
Reference-contexts: For machines with complicated addressing modes, this algorithm becomes unwieldy, with hundreds of cases. Dynamic programming can be extended to machines with several classes of registers [7] by an algorithm analogous to the CYK algorithm <ref> [8, 9] </ref> for parsing by dynamic programming. While these algorithms are powerful and useful, it is important to note that they select optimal code sequences only for the particular trees they are given; and those trees are not necessarily optimal versions of the programs they represent. 1.2.
Reference: 9. <author> D. H. Younger, </author> <title> ``Recognition and parsing of context-free languages in time n 3 ,'' Information and Control, </title> <journal> vol. </journal> <volume> 10, no. 2, </volume> <pages> pp. 189-208, </pages> <year> 1967. </year>
Reference-contexts: For machines with complicated addressing modes, this algorithm becomes unwieldy, with hundreds of cases. Dynamic programming can be extended to machines with several classes of registers [7] by an algorithm analogous to the CYK algorithm <ref> [8, 9] </ref> for parsing by dynamic programming. While these algorithms are powerful and useful, it is important to note that they select optimal code sequences only for the particular trees they are given; and those trees are not necessarily optimal versions of the programs they represent. 1.2.
Reference: 10. <author> C. Gordon Bell and Allen Newell, </author> <title> Computer Structures: Readings and Examples, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: Formal description of instruction sets The automatic generation of instruction selectors from a specification of the target machine requires a formal specification language for target-machine semantics. The ISP language for specifying machine instruction-sets <ref> [10] </ref> has been used to automatically generate instruction selectors for intermediate representations based on trees [11] and peephole optimizers for more general graphs [12, 13]. These target-code generators rely on heuristics to match patterns (derived from the semantics of target-machine instructions) to portions of the intermediate representation tree (or graph).
Reference: 11. <author> R. G. G. Cattell, </author> <title> ``Formalization and automatic derivation of code generators,'' </title> <type> Ph.D. Thesis, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> April </month> <year> 1978. </year>
Reference-contexts: Formal description of instruction sets The automatic generation of instruction selectors from a specification of the target machine requires a formal specification language for target-machine semantics. The ISP language for specifying machine instruction-sets [10] has been used to automatically generate instruction selectors for intermediate representations based on trees <ref> [11] </ref> and peephole optimizers for more general graphs [12, 13]. These target-code generators rely on heuristics to match patterns (derived from the semantics of target-machine instructions) to portions of the intermediate representation tree (or graph).
Reference: 12. <author> Christopher W. Fraser, </author> <title> ``A compact, machine-independent peephole optimizer,'' </title> <booktitle> Sixth ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pp. 1-6, </pages> <publisher> ACM, </publisher> <year> 1979. </year>
Reference-contexts: The ISP language for specifying machine instruction-sets [10] has been used to automatically generate instruction selectors for intermediate representations based on trees [11] and peephole optimizers for more general graphs <ref> [12, 13] </ref>. These target-code generators rely on heuristics to match patterns (derived from the semantics of target-machine instructions) to portions of the intermediate representation tree (or graph).
Reference: 13. <author> J. W. Davidson, </author> <title> ``Simplifying Code Generation Through Peephole Optimization,'' </title> <type> TR 81-19, </type> <institution> Department of Computer Science, University of Arizona, Tucson, Arizona, </institution> <year> 1981. </year>
Reference-contexts: The ISP language for specifying machine instruction-sets [10] has been used to automatically generate instruction selectors for intermediate representations based on trees [11] and peephole optimizers for more general graphs <ref> [12, 13] </ref>. These target-code generators rely on heuristics to match patterns (derived from the semantics of target-machine instructions) to portions of the intermediate representation tree (or graph).
Reference: 14. <author> R. Steven Glanville and Susan L. Graham, </author> <title> ``A New Method for Compiler Code Generation,'' </title> <booktitle> Fifth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 231-240, </pages> <publisher> ACM, </publisher> <year> 1978. </year>
Reference-contexts: The heuristic used on trees leads to a sequence of pattern-matches identical to that made by an LR parser; this led to the formalization of target-machine semantics as LR (1) grammars <ref> [14] </ref>. Instruction selection is thus reduced to LR (1) parsing, which is efficient and well-understood. Unfortunately, the LR (1) grammars tend to be very large: typical descriptions of the VAX instruction set use hundreds of productions [15, 16]. <p> One a machine with many addressing modes (like the VAX), a single instruction like mov requires thousands of patterns. It is this problem which motivated the Graham-Glanville approach <ref> [14] </ref>, which uses context-free grammars to describe instruction sets. In our example instruction set, the sum of a register and a constant (either of which can be zero) is used in several instructions; this could be abstracted as a displacement.
Reference: 15. <author> Susan L. Graham, Robert R. Henry, and Robert A. Schulman, </author> <title> ``An Experiment in Table Driven Code Generation,'' </title> <booktitle> Tenth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 32-43, </pages> <publisher> ACM, </publisher> <year> 1983. </year>
Reference-contexts: Instruction selection is thus reduced to LR (1) parsing, which is efficient and well-understood. Unfortunately, the LR (1) grammars tend to be very large: typical descriptions of the VAX instruction set use hundreds of productions <ref> [15, 16] </ref>. Context-free grammars have difficulty in efficiently describing certain features of real instruction sets, particularly those involving operand size constraints. For this purpose, attribute grammars are convenient [17]. An attribute grammar describing a given machine can be much shorter than the equivalent context-free grammar. 1.3. <p> No claim is made that this is the ``optimally concise'' specification of a machine architecture. Indeed, section 7 summarizes the redundancies, along with some possible improvements. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Authors Rules Remarks iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Appel 112 Present work. Dynamic programming, attribute grammar. 63 terminals, 20 nonterminals, 112 productions. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii 458Graham, Henry, Schulman <ref> [ 15] </ref> LR (1) Grammar. Before type replication: 115 terminals, 96 nonterminals, 458 productions. After type replication: 219 terminals, 148 nonterminals, 1073 productions. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii 222Fraser, Davidson [23] Peephole-optimization based code generator. 38 instruction-descriptions, 15 address-mode descriptions each replicated up to 4 times, 29 miscellaneous, 100 translation rules.
Reference: 16. <author> Philippe Aigrain, Susan L. Graham, Robert R. Henry, Marshall K. McKusick, and Eduardo Pelegri-Llopart, </author> <title> ``Experience with a Graham-Glanville style code generator,'' </title> <booktitle> Proc. Sigplan '84 Symp. on Compiler Construction (Sigplan Notices), </booktitle> <volume> vol. 19, no. 6, </volume> <pages> pp. 13-24, </pages> <publisher> ACM, </publisher> <year> 1984. </year>
Reference-contexts: Instruction selection is thus reduced to LR (1) parsing, which is efficient and well-understood. Unfortunately, the LR (1) grammars tend to be very large: typical descriptions of the VAX instruction set use hundreds of productions <ref> [15, 16] </ref>. Context-free grammars have difficulty in efficiently describing certain features of real instruction sets, particularly those involving operand size constraints. For this purpose, attribute grammars are convenient [17]. An attribute grammar describing a given machine can be much shorter than the equivalent context-free grammar. 1.3. <p> After type replication: 219 terminals, 148 nonterminals, 1073 productions. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii 222Fraser, Davidson [23] Peephole-optimization based code generator. 38 instruction-descriptions, 15 address-mode descriptions each replicated up to 4 times, 29 miscellaneous, 100 translation rules. Does not include floating point or 16-bit word datatypes. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii 146Aigrain, Graham, Henry, McKusick, Pelegri-Llopart <ref> [16] </ref> Improvement of Graham, Henry, Schulman. 146 ``meta-pacs'' are automatically transformed into 662 LR (1) produc tions on 246 grammar symbols. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii 238Kessler [24] Architecture analysis to find ``idioms.'' 238 instruction descriptions are automatically reduced to 111 instruction families; from these, 1273 idioms are generated.
Reference: 17. <author> M. Ganapathi, </author> <title> ``Retargetable Code Generation and Optimization using Attribute Grammars,'' </title> <type> PhD. Thesis, </type> <institution> Univ. of Wisconsin, Madison, </institution> <address> Wis., </address> <year> 1980. </year>
Reference-contexts: Unfortunately, the LR (1) grammars tend to be very large: typical descriptions of the VAX instruction set use hundreds of productions [15, 16]. Context-free grammars have difficulty in efficiently describing certain features of real instruction sets, particularly those involving operand size constraints. For this purpose, attribute grammars are convenient <ref> [17] </ref>. An attribute grammar describing a given machine can be much shorter than the equivalent context-free grammar. 1.3. Optimal instruction selection from formal instruction set descriptions Combining grammar-based machine descriptions with dynamic programming, as in the Twig system [1], leads to concise and powerful instruction selection specifications.
Reference: 18. <author> Steven W. K. Tjiang, </author> <title> ``Twig Reference Manual,'' </title> <institution> CSTR-120, ATT Bell Laboratories, </institution> <address> Murray Hill, NJ, </address> <year> 1986. </year>
Reference-contexts: This paper demonstrates the concise and powerful nature of code generators based on dynamic-programming parsers by using the VAX and MC68020 instruction sets as illustrative examples. 2. Overview of Twig Aho, Ganapathi, and Tjiang's code-generator generator, Twig, is described in [1] and <ref> [18] </ref>. This section provides just a brief overview of the system. - 4 - The Twig system takes a specification in the form of an annotated grammar and produces a code generator.
Reference: 19. <author> C. M. Hoffmann and M. J. O'Donnell, </author> <title> ``Pattern matching in trees,'' </title> <journal> Journal of the ACM, </journal> <volume> vol. 29, </volume> <pages> pp. 68-95, </pages> <year> 1982. </year>
Reference-contexts: Twig implements this dynamic programming algorithm, given a grammar annotated with cost and instruction-emission information. Twig also uses a clever pattern-matching algorithm <ref> [19] </ref> to avoid comparing every pattern to every node in the tree, but a description of that is beyond the scope of this paper. - 7 - 2.2. Format A Twig specification has several parts.
Reference: 20. <institution> VAX Architecture Handbook, Digital Equipment Corp., Maynard, </institution> <address> Mass., </address> <year> 1979. </year>
Reference-contexts: The VAX instruction set The VAX has a rather complex, though mostly consistent, instruction set <ref> [20] </ref>. The operands of VAX instructions may have several forms, known as ``addressing modes.'' The instructions themselves may be three-address (operating on two operands and placing the result in a third) or two-address (placing the result in the second source operand).
Reference: 21. <author> Andrew W. Appel and Kenneth J. Supowit, </author> <title> ``Generalizations of the Sethi-Ullman algorithm for register allocation,'' </title> <journal> Software Practice/Experience, </journal> <volume> vol. 17, no. 6, </volume> <pages> pp. 417-421, </pages> <year> 1987. </year>
Reference-contexts: The DEFAULT_COST for each node is (roughly) the sum of the costs of the children. That is, the space values of the children are summed, the time values are summed, and the sideEffect values are unioned. In addition, a variant <ref> [21] </ref> of the Sethi-Ullman algorithm [5] for minimizing register-usage is used. This will require reordering the computation of the children of expression-nodes (as long as there are no side-effects among the reordered children). The cost of the optimal ordering is computed in DEFAULT_COST, yielding the cost value maxregs.
Reference: 22. <editor> MC68020 32-Bit Microprocessor User's Manual, </editor> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1985. </year>
Reference-contexts: The MC68020 instruction set The Motorola MC68020 has a large, complex, and somewhat unorthogonal instruction set <ref> [22] </ref>. There are many addressing modes, each instruction takes only a particular subset of these modes, and there are several different forms of many instructions. (In particular, the MC68020 has several new addressing modes not found on its predecessor, the MC68010.) 5.1.
Reference: 23. <author> J. W. Davidson and Christopher W. Fraser, </author> <title> ``Automatic Generation of Peephole Optimizations,'' </title> <booktitle> Sigplan '84 Symposium on Compiler Construction, </booktitle> <pages> pp. 111-116, </pages> <publisher> ACM, </publisher> <year> 1984. </year>
Reference-contexts: Dynamic programming, attribute grammar. 63 terminals, 20 nonterminals, 112 productions. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii 458Graham, Henry, Schulman [ 15] LR (1) Grammar. Before type replication: 115 terminals, 96 nonterminals, 458 productions. After type replication: 219 terminals, 148 nonterminals, 1073 productions. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii 222Fraser, Davidson <ref> [23] </ref> Peephole-optimization based code generator. 38 instruction-descriptions, 15 address-mode descriptions each replicated up to 4 times, 29 miscellaneous, 100 translation rules.
Reference: 24. <author> Peter B. Kessler, </author> <title> ``Discovering machine-specific code improvements,'' </title> <booktitle> Proc. Sigplan '86 Symp. on Compiler Construction (Sigplan Notices), </booktitle> <volume> vol. 21, no. 7, </volume> <pages> pp. 249-254, </pages> <publisher> ACM, </publisher> <year> 1986. </year>
Reference-contexts: Does not include floating point or 16-bit word datatypes. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii 146Aigrain, Graham, Henry, McKusick, Pelegri-Llopart [16] Improvement of Graham, Henry, Schulman. 146 ``meta-pacs'' are automatically transformed into 662 LR (1) produc tions on 246 grammar symbols. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii 238Kessler <ref> [24] </ref> Architecture analysis to find ``idioms.'' 238 instruction descriptions are automatically reduced to 111 instruction families; from these, 1273 idioms are generated.
Reference: 25. <author> Kurt Keutzer and Wayne Wolf, </author> <title> ``Anatomy of a hardware compiler,'' </title> <booktitle> Proc. ACM SIGPLAN '88 Conf. on Prog. Lang. Design & Implementation,, </booktitle> <pages> pp. 95-104, </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1988. </year>
Reference-contexts: Each shared node (one with several in-edges) is made into an explicit temporary (perhaps using the ALLOC and TEMP nodes described in this paper). This tree is then matched using Twig; the match may be suboptimal, but at least a solution is generated. This technique has been used successfully <ref> [25] </ref> in a Twig-based compiler for standard-cell VLSI circuits. 8. Conclusion Dynamic programming is a robust and efficient technique for generating machine code from expression trees. Code generator specifications written for the Twig system correspond closely and elegantly to the instruction sets and addressing modes of complex-instruction-set computers.
References-found: 26

