URL: ftp://ftp.lcs.mit.edu/student-workshop/1995/abstracts/Toledo.ps
Refering-URL: http://www.cag.lcs.mit.edu/student95/proceedings.html
Root-URL: 
Email: sivan@theory.lcs.mit.edu  
Phone: 02139.  
Title: Out-of-Core Krylov-Subspace Methods methods are a class of iterative numerical methods for solving linear equations.
Author: Sivan Toledo SimpleKrylovMethod(A; t 
Web: Web: http://theory.lcs.mit.edu/~sivan  
Address: N00014-94-1-0985. Address: Room 243, 545 Technology Square, Cambridge MA  
Note: Krylov-subspace  Sivan Toledo is advised by Charles E. Leiserson and is supported in part by ARPA contract  Chronopoulos and Gear [1]  The  can be  
Affiliation: MIT Laboratory for Computer Science  
Abstract: This abstract describes efficient out-of-core Krylov-subspace methods. Krylov-subspace methods include popular iterative linear solvers such as conjugate gradient and GMRES. If the data structures of the algorithm do not fit within main memory, then an out-of-core algorithm must be used. An out-of-core algorithms tries to be computation-ally efficient but in addition attempts to move as little data as possible between the computer's primary and secondary memories. An efficient conjugate-gradient algorithm described in this abstract is more than 3 times faster than naive implementations that rely on paging to perform I/O. This abstract summarizes results from [3]. Most Krylov-subspace methods involve several types of operations in every iteration, namely multiplication of a vector by the matrix A, vector operations (additions and multiplication by scalars), and inner products. Krylov-subspace algorithms that do not compute inner products are called linear relaxation algorithms. Almost all Krylov-subspace methods maintain a number of n-vectors in addition to x (t) . Many reuse the space used to store these vectors, so the space required is only a small multiple of n words. A naive approach to out-of-core algorithms is to use the in-core algorithm and let an automatic system, for example paging, transfer data on demand between levels of the memory hierarchy. The naive approach causes almost the entire data-structure that represents the state of the iterative algorithm to be read from secondary memory in every iteration. In many Krylov-subspace algorithms the performance of this naive approach is limited by the I/O channel that transfers data to and from secondary memory, because the amount of work per iteration is small. We propose a novel implementation of Krylov-subspace algorithms, which is based on blockers [2]. Our method The main idea behind our method is to block the flow of information through inner products in Krylov-subspace algorithms. We simulate t iterations of the algorithm using three phases. In the first phase, we use linear relaxation to compute inner products, but we do not use them. Therefore, in Phase 1 information flows into the inner products but not out of them. In the second phase, we use the inner products from Phase 1 to compute the exact values of the inner products computed by the original Krylov-subspace algorithm. In the third phase we perform the Krylov-subspace algorithm. Since the values of the inner products are known, they are not computed, and hence, information flows out of these variables but not into them. we demonstrate our method on the following pseudocode that describes a simple Krylov-subspace algorithm for symmetric matrices A. Our method applies equally well to realistic algorithms such as conjugate gradient. Our method is related to algorithms by 1 for t 1 to t 3 ff t x T x In Phase 1 we compute inner products without ever using them. Because inner products are not used, this phase is essentially a linear relaxation algorithm. We use an efficient out-of-core linear relaxation algorithm to perform this phase. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. T. Chronopoulos and C. W. Gear. </author> <title> s-step iterative methods for symmetric linear systems. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 25 </volume> <pages> 153-168, </pages> <year> 1989. </year>
Reference-contexts: 1 Form the matrix Q where Q i;j = fi i+j2 2 w <ref> [1; 0; : : : ; 0] </ref> T 4 do v shift-down (w) 5 ff t w T B T Bw In Phase 3 we simple use the precomputed values of ff. This phase too is a linear relaxation algorithm that can often be executed efficiently out of core.
Reference: [2] <author> Charles E. Leiserson, Satish Rao, and Sivan Toledo. </author> <title> Efficient out-of-core algorithms for linear relaxation using blocking covers. </title> <booktitle> In Proceedings of the 34rd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 704-713, </pages> <month> November </month> <year> 1993. </year> <note> To appear in the Journal of Computer and System Sciences. </note>
Reference: [3] <author> Sivan A. Toledo. </author> <title> Quantitative Performance Modeling of Scientific Computations and Creating Locality in Numerical Algorithms. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1995. </year> <month> 2 </month>
References-found: 3

