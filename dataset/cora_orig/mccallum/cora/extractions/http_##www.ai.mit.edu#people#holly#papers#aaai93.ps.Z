URL: http://www.ai.mit.edu/people/holly/papers/aaai93.ps.Z
Refering-URL: http://www.ai.mit.edu/people/holly/papers/papers.html
Root-URL: 
Email: holly@ai.mit.edu  
Phone: (617)253-7884  
Title: Talking About the World: Cooperative Robots that Learn to Communicate robots have already learned simple
Author: Holly Yanco 
Note: The  
Address: 545 Technology Square, Room 741 Cambridge, MA 02139  
Affiliation: Artificial Intelligence Laboratory Massachusetts Institute of Technology  
Abstract: Models of the world can take many shapes. In this paper, we will discuss how groups of autonomous robots learn languages that can be used as a means for modeling the environment. In current work, the world will be the motivation for learning languages. Since the languages are grounded in the world, they can be used to talk about the world; in effect, the language is the means the robots use to model the world. This paper will explore the issues of learning to communicate solely through environment motivation. Additionally, we will discuss the possible uses of these languages for interacting with the world.
Abstract-found: 1
Intro-found: 1
Reference: [Kaelbling, 1990] <author> Leslie Pack Kaelbling. </author> <title> Learning in embedded systems. </title> <type> Technical Report TR-90-04, </type> <institution> Teleos Research, Palo Alto, California, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: Likewise, if either robot performs incorrectly, negative reinforcement () is issued. Based on this environmental feedback, the robots learn to select appropriate actions and communication signals. Both the action selection and the signal selection are learned using standard reinforcement learning techniques. (See, e.g., <ref> [Kaelbling, 1990] </ref> or [Sutton, 1992] for overviews of reinforcement learning.) The particular algorithm that we use is adapted from Kaelbling's interval estimation method [1990]. Interval estimation is a relatively simple form of reinforcement: A table of inputs fi actions is maintained.
Reference: [MacLennan, 1990] <author> Bruce MacLennan. </author> <title> Evolution of communication in a population of simple machines. </title> <type> Technical Report CS-90-99, </type> <institution> University of Tennessee, Knoxville, Tennessee, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: Work on the development of communication between groups of autonomous agents has also been done by <ref> [MacLennan, 1990] </ref> and [Werner and Dyer, 1990]. Their research addresses the problem of language learning with genetic algorithms. Language evolves over many generations of the community. Within an individual agent, however, language is fixed over its lifetime.
Reference: [Martin and Sargent, 1991] <author> Fred Martin and Randy Sargent. </author> <title> The MIT sensor robot: User's guide and technical reference. </title> <month> October </month> <year> 1991. </year>
Reference-contexts: In the context of this world motivation, we are exploring the development of context dependent languages and compositional languages. 2 The robots The robots used in this research are Sensor Robots designed by Fred Martin at the Media Laboratory at the Massachusetts Institute of Technology <ref> [Martin and Sargent, 1991] </ref>. Each robot is approximately 9 00 l fi 6 00 w fi 4 00 h, with a single circuit board containing most of the computational and sensory resources of the robot.
Reference: [Shewchuk, 1991] <author> John P. Shewchuk. </author> <type> Ph.D. thesis proposal. </type> <institution> Department of Computer Science, Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1991. </year>
Reference-contexts: This work is discussed in [Yanco and Stein, 1993]; it is also summarized below as background for the current work. Our initial work was inspired by Shewchuk's Ph.D. thesis <ref> [Shewchuk, 1991] </ref>. It addresses the design of appropriate reinforcement learning algorithms to learn languages for internal representation as well as for communication.
Reference: [Sutton, 1992] <author> Richard S. Sutton. </author> <title> Special issue on reinforcement learning. </title> <booktitle> Machine Learning, </booktitle> <pages> 8(3-4), </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Likewise, if either robot performs incorrectly, negative reinforcement () is issued. Based on this environmental feedback, the robots learn to select appropriate actions and communication signals. Both the action selection and the signal selection are learned using standard reinforcement learning techniques. (See, e.g., [Kaelbling, 1990] or <ref> [Sutton, 1992] </ref> for overviews of reinforcement learning.) The particular algorithm that we use is adapted from Kaelbling's interval estimation method [1990]. Interval estimation is a relatively simple form of reinforcement: A table of inputs fi actions is maintained.
Reference: [Werner and Dyer, 1990] <author> Gregory M. Werner and Michael G. Dyer. </author> <title> Evolution of communication in artificial organisms. </title> <type> Technical Report UCLA-AI-90-06, </type> <institution> University of California, Los Angles, California, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: Work on the development of communication between groups of autonomous agents has also been done by [MacLennan, 1990] and <ref> [Werner and Dyer, 1990] </ref>. Their research addresses the problem of language learning with genetic algorithms. Language evolves over many generations of the community. Within an individual agent, however, language is fixed over its lifetime.
Reference: [Yanco and Stein, 1993] <author> Holly Yanco and Lynn Andrea Stein. </author> <title> An adaptive communication protocol for cooperating mobile robots. </title> <booktitle> In From Animals to Animats: Proceedings of the Second International Conference on the Simulation of Adaptive Behavior, </booktitle> <pages> pages 478-485. </pages> <publisher> The MIT Press/Bradford Books, </publisher> <year> 1993. </year>
Reference-contexts: Under this method, the robots successfully developed languages for task communication and were able to adapt them when we changed the commands' meanings. This work is discussed in <ref> [Yanco and Stein, 1993] </ref>; it is also summarized below as background for the current work. Our initial work was inspired by Shewchuk's Ph.D. thesis [Shewchuk, 1991]. It addresses the design of appropriate reinforcement learning algorithms to learn languages for internal representation as well as for communication. <p> Using three robots, a two element language typically converges after an average of 27 iterations; the range is between 10 and 80 iterations. Larger language sizes have also been tested; as the language size increases, the learning time increases exponentially. Results are discussed in <ref> [Yanco and Stein, 1993] </ref>. 4.2 Adaptability of language Once the robots converge on a particular dialect, they continue to receive positive reinforcement as long as the environmental constraints do not change. If circumstances change, however, the robots may find that their previously successful actions no longer earn them positive feedback.
References-found: 7

