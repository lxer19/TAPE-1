URL: ftp://ftp.cs.umd.edu/pub/papers/papers/ncstrl.umcp/CS-TR-3516/CS-TR-3516.ps.Z
Refering-URL: http://www.cs.umd.edu/~elman/
Root-URL: http://www.cs.umd.edu
Title: A NOTE ON CONJUGATE GRADIENT CONVERGENCE  
Author: AARON E. NAIMAN IVO M. BABU SKA AND HOWARD C. ELMAN 
Keyword: Key words. conjugate gradient, convergence rates  
Note: AMS subject classifications. 65F10, 65G99, 65L10, 65L12, 65N22  
Abstract: The one-dimensional discrete Poisson equation on a uniform grid with n points produces a linear system of equations with a symmetric, positive-definite coefficient matrix. Hence, the conjugate gradient method can be used, and standard analysis gives an upper bound of O(n) on the number of iterations required for convergence. This paper introduces a systematically defined set of solutions dependent on a parameter fi, and for several values of fi, presents exact analytic expressions for the number of steps k(fi; o; n) needed to achieve accuracy o. The asymptotic behavior of these expressions has the form O(n ff ) as n ! 1 and O(o fl ) as o ! 0. In particular, two choices of fi corresponding to nonsmooth solutions give ff = 0, i.e., iteration counts independent of n; this is in contrast to the standard bounds. The standard asymptotic convergence behavior, ff = 1, is seen for a relatively smooth solution. Numerical examples illustrate and supplement the analysis. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> O. Axelsson, </author> <title> Iterative Solution Methods, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1994. </year>
Reference-contexts: grid with n points, the condition number is approximately 4n 2 = 2 , and this estimate leads to an upper bound of k o n (1.2) for the number of iterations to make the relative error smaller than a specified accuracy o (provided o is not too small), see <ref> [1, p. 567] </ref>. <p> NAIMAN, IVO M. BABU SKA AND HOWARD C. ELMAN for any distribution of at least p + 1 eigenvalues, there is an initial guess (depending on p) that produces equality in (1.3) <ref> [1, p. 561] </ref>, [4]. This observation can be used to derive sharp upper bounds on the error that are stronger than (1.1) [4].
Reference: [2] <author> G. H. Golub and C. F. V. Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD, </address> <note> second ed., </note> <year> 1989. </year>
Reference-contexts: In partic ular, the standard analysis leads to the upper bound 2 1 + 1 (1.1) for the relative error in the energy norm after p steps (see, e.g., <ref> [2, p. 525] </ref>), where is the condition number.
Reference: [3] <author> I. S. Gradshteyn and I. M. Ryzhik, </author> <title> Table of Integrals, Series and Products, </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <publisher> forth ed., </publisher> <year> 1980. </year>
Reference-contexts: Identity 4.2. p X (2k + 1) = 3 h 2 i Identity 4.3. p X (1) (2k + 1) = (1) (p + 1) Identity 4.4. n X sin 2 j i = 2 Proof. Invoke <ref> [3, (1.351.1)] </ref>. Theorem 4.5. <p> In this case, we have to minimize n X p X a k sin 2k+1 j i ; which is equivalent to minimizing (using the multiple angle formulae, see <ref> [3, (1.320.3)] </ref>) K = i=1 sin j i k=0 ! 2 subject to the constraint p X c k (2k + 1) = 0: This constrained minimization problem can be solved by the Lagrange multiplier technique. <p> Indeed any such ~ ` which will concern us will always be smaller than 2 (n + 1) in absolute value, and therefore, we need only deal with ~ ` = 0 as a special case. This relationship can be proven in a straightforward manner using <ref> [3, 1.342.2] </ref>, and expanding sin j n` into sin ` 2 1 1 j , as in [5, Appendix 3.B]. Theorem 5.2. <p> In this case, we have to minimize the expression (see (3.1)) n X p X a k sin 2k j i ; which, invoking <ref> [3, (1.320.1)] </ref>, is equivalent to minimizing K = i=1 1 k=0 ! 2 10 AARON E. NAIMAN, IVO M. BABU SKA AND HOWARD C. ELMAN Once again, due to the one additional constant introduced, we need a constraint equation to interrelate the parameters fc k g. <p> We note immediately that because of the constraint equation, B = 1. We have @J = i=1 2 cos 2j `i 1 k=0 !# p X c k cos ` 1 k=0 ! Rewriting the cosine product terms (see <ref> [3, 1.314.3] </ref>), we get @J = i=1 2 cos 2j `i + k=0 fi fl p X c k cos ` + 2 k=0 + 1 + (1) A: and rearranging @J = 2 i=1 # 1 cos ` + k=0 (" n X cos 2j (k+`)i + 2 ) p <p> p X p X c k c ` cos 2j ki cos 2j `i + 2 1 2 k=0 p X p X c k c ` + 2 1 2 k=0 p X p X c k c ` cos k cos ` 2 1 + A 2 Invoking <ref> [3, 1.314.3] </ref> we have K = n + 2 1 2 k=0 " n X cos 2j ki + 2 1 cos k + 2 k=0 `=0 (" n X cos 2j (k+`)i + 2 1 cos (k + `) + i=1 ! 1 + 2 #) 1 = (n +
Reference: [4] <author> A. Greenbaum, </author> <title> Comparison of splittings used with the conjugate gradient algorithm, </title> <journal> Nu-merische Mathematik, </journal> <volume> 33 (1979), </volume> <pages> pp. 181-194. </pages>
Reference-contexts: NAIMAN, IVO M. BABU SKA AND HOWARD C. ELMAN for any distribution of at least p + 1 eigenvalues, there is an initial guess (depending on p) that produces equality in (1.3) [1, p. 561], <ref> [4] </ref>. This observation can be used to derive sharp upper bounds on the error that are stronger than (1.1) [4]. <p> ELMAN for any distribution of at least p + 1 eigenvalues, there is an initial guess (depending on p) that produces equality in (1.3) [1, p. 561], <ref> [4] </ref>. This observation can be used to derive sharp upper bounds on the error that are stronger than (1.1) [4]. The conventional wisdom holds that (1.2) characterizes the behavior of the conjugate gradient method for the model problem in terms of both n and o , except in the case where the initial error contains only a small number of eigenvectors. In this paper, we examine this issue.
Reference: [5] <author> A. E. Naiman, </author> <title> Computer Solutions of Finite Element Linear Systems, </title> <type> PhD thesis, </type> <institution> University of Maryland, College Park, MD, </institution> <year> 1994. </year>
Reference-contexts: The work presented in this paper represents part of the first author's thesis, and additional details can be found there <ref> [5] </ref>. 2. The Model Problem and Solution Class. <p> Identity 4.1. " n X sin (2k + 1)j i sin (2` + 1)j i + 2 (2k + 1) sin 2 n + 1 ffi k` ; 1 This identity is proven in <ref> [5, Appendix 3.B] </ref>. Note: the second term is the same as the summand of the first, with i := n + 1. CONJUGATE GRADIENT CONVERGENCE 5 for 0 k; ` n, where ffi k` = ae 0; k 6= `; is the Kronecker delta. <p> This relationship can be proven in a straightforward manner using [3, 1.342.2], and expanding sin j n` into sin ` 2 1 1 j , as in <ref> [5, Appendix 3.B] </ref>. Theorem 5.2.
Reference: [6] <author> S. Wolfram, </author> <title> Mathematica: A System for Doing Mathematics by Computer, </title> <publisher> Addison-Wesley, </publisher> <address> Readwood City, CA, </address> <note> second ed., </note> <year> 1991. </year>
Reference-contexts: The following two identities come up in the derivation below. These identities were proven with the aid of Mathematica <ref> [6] </ref>. Recall that j i = i 2 (n+1) . Identity 7.2. n X sin (2k 1)j i = n + 2 1 k for k n.
References-found: 6

