URL: http://www.cs.tamu.edu/faculty/bhuyan/papers/IPPS98.ps
Refering-URL: http://www.cs.tamu.edu/faculty/bhuyan/
Root-URL: http://www.cs.tamu.edu
Email: ravig@cs.tamu.edu  akumar2@mipos2.intel.com  
Title: Impact of Switch Design on the Application Performance of Cache-Coherent Multiprocessors  
Author: L. Bhuyan, H. Wang, and R.Iyer fbhuyan, hwang, A. Kumar 
Address: College Station, TX 77843-3112, USA  2200 Mission College Blvd Santa Clara, CA 95052-8119, USA  
Affiliation: Department of Computer Science Texas A&M University  Intel Corporation  
Abstract: In this paper, the effect of switch design on the application performance of cache-coherent non-uniform memory access (CC-NUMA) multiprocessors is studied in detail. Wormhole routing and cut-through switching are evaluated for these shared-memory multiprocessors that employ multistage interconnection network (MIN) and full map directory-based cache coherence protocol. The switch design also considers virtual channels and varying number of input buffers per switch. Based on this, four different switch architectures are presented and compared. The evaluation is based on execution-driven simulation using five different applications to capture the random bursty nature of the network traffic arrival. The round-robin memory management policy is implemented. We show that the use of cut-through switching with buffers and virtual channels improves the average message latency tremendously. The waiting times of messages at various stages of switches are also presented. Finally, we show the variation of stall times and execution times for these applications by varying the switch delay and wire width. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Bhuyan and et al. </author> <title> Performance of multistage bus networks for a distributed shared memory multiprocessor. </title> <journal> IEEE Transactions on Prallel and Distributed Systems, </journal> <volume> 8(1):82 95, </volume> <month> January </month> <year> 1997. </year>
Reference-contexts: The effect of all these advances in the INs has to be judged from the dynamic changes during the execution of applications. Execution-based evaluations for mesh interconnection networks have been reported recently <ref> [1, 8, 14] </ref> to test the effectiveness of virtual channels and adaptive routing. These studies evaluate the multiprocessors at the network level and do not reflect the effect of design details of switch hardware.
Reference: [2] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl. Proteus: </author> <title> A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Multiple input channels can request the same output channel in this phase. This conflict is resolved in the second step. A similar arbitration policy is used in the Intel Cavallino switch [3]. 3. Simulator design Our simulator is based on PROTEUS <ref> [2] </ref> which implemented MIN using an analytical model. We have modified the simulator extensively to exactly model the MIN with wormhole routing. We have also incorporated the switch architectures with virtual channels and multi-flit buffers, as explained in section 2.
Reference: [3] <author> J. Carbonaro and F. Verhoorn. Cavallino: </author> <title> The teraflops router and nic. </title> <booktitle> Proc. Symp. High Performance Interconnects (Hot Interconnects 4), </booktitle> <month> Augest </month> <year> 1996. </year>
Reference-contexts: Torrellas and Zhang [13] recently reported detailed performance evaluation of the Cedar multiprocessor network. Besides being a simulation-based evaluation, our work differs from theirs in (a) switching: we adopt wormhole routing, virtual cut-through switching and virtual channels, as employed in commercial multiprocessor switches <ref> [3, 6] </ref> instead of packet switching in Cedar, (b) cache coherence: we employ a directory-based cache coherence whereas Cedar has no hardware support for cache coherence and (c) organization: Cedar is a UMA system where memory modules are centrally located and employ a forward network from processors to memories and a <p> In virtual cut-through, buffers are provided in the switch so that a message can be buffered when it is blocked. We use these switching techniques because they are used in the design of current multiprocessor switches <ref> [3, 6] </ref>. There are two types of messages that are transmitted over the MIN. One is a control message that consists of read, write and invalidation signals that are short and 4 flits long. <p> The input synchronization takes one to two cycles, and it takes one more cycle for transmission. As a result, the switch delay is about 4 cycles, which is similar to the time taken in SGI Spider and Intel Cavallino switches <ref> [3, 6] </ref>. Many times the requests arrive in bulk. For example, in case of a write miss on a block, the interface/coherence controller sends invalidation signals to processors having a copy of the block one after another. In such situations, providing some buffer in the switches will be helpful. <p> In this situation, the arbiter consists of two simple 4 to 1 crossbar arbiters. When the number of inputs is more (6 inputs with 4 VC's or buffers per input which is equivalent to 24 possible inputs per output in Cavallino and Spider <ref> [3, 6] </ref>), two-stage arbiters are needed to keep arbitration time within limit. However, we have no such problem with a simple 2x2 switch. Each of the virtual channels can again have space for multiple flits. <p> Multiple input channels can request the same output channel in this phase. This conflict is resolved in the second step. A similar arbitration policy is used in the Intel Cavallino switch <ref> [3] </ref>. 3. Simulator design Our simulator is based on PROTEUS [2] which implemented MIN using an analytical model. We have modified the simulator extensively to exactly model the MIN with wormhole routing. We have also incorporated the switch architectures with virtual channels and multi-flit buffers, as explained in section 2. <p> The switch parameters are set as explained in section 2. A flit size of 16 bits was considered which is same as the width of the link. The switch latency or delay is considered to be 1, 2 or 4 processor cycles. The SGI Spider and Cavallino <ref> [3, 6] </ref> both have 4 cycles of latency. Additionally, we consider 1 or 2 cycle delays for comparison since our aim is to design high-speed switches. The interface delay for protocol handling, etc. is 20 cycles, similar to DASH [9]. <p> The current switch delay in the commercial machines is about 5 switch core cycles (switch core operates slower than the CPU) for a 6x6 crossbar <ref> [3, 6] </ref>. With a 2x2 switch, we could obtain a faster switch because it takes much less time to arbitrate.
Reference: [4] <author> L. M. Censier and P. Feautrier. </author> <title> A new solution to coherence problems in multicache systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 27(12):11121118, </volume> <month> December </month> <year> 1978. </year>
Reference-contexts: We assumed message lengths of 8 and 40 bytes for control and data messages, respectively, similar to the SGI switch. 3.1. Cache coherence and synchronization We implemented the full-map directory-based cache coherence protocol <ref> [4] </ref> with some modifications for evaluation in this paper. In this scheme, each shared memory block is assigned to a node, called home node, which maintains the directory entries for that block.
Reference: [5] <author> W. J. Dally. </author> <title> Virtual-channel flow control. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2):194205, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: The IN has become quite advanced with the introduction of techniques such as virtual channel <ref> [5] </ref> and adaptive routing [7]. The effect of all these advances in the INs has to be judged from the dynamic changes during the execution of applications. Execution-based evaluations for mesh interconnection networks have been reported recently [1, 8, 14] to test the effectiveness of virtual channels and adaptive routing. <p> However, the arriving message is bypassed without being stored in the buffer if the output is free. Note that the output buffer is still one flit. We call such a switch a buffered cut-through (BCT) design and is shown in Fig.2.b. Virtual channels (VC's) <ref> [5] </ref> are used in wormhole networks to avoid deadlocks and to improve link utilization and network throughput. Fig. 2.c shows such a simple virtual channel (SVC) design with two VC's. Whenever there is blocking of a message, another message can be routed through the extra VC.
Reference: [6] <author> M. Galles. </author> <title> Scalable pipelined interconnect for distributed endpoint routing: The SGI SPIDER chip. </title> <booktitle> Proc. Symp. High Performance Interconnects (Hot Interconnects 4), </booktitle> <month> Augest </month> <year> 1996. </year>
Reference-contexts: Torrellas and Zhang [13] recently reported detailed performance evaluation of the Cedar multiprocessor network. Besides being a simulation-based evaluation, our work differs from theirs in (a) switching: we adopt wormhole routing, virtual cut-through switching and virtual channels, as employed in commercial multiprocessor switches <ref> [3, 6] </ref> instead of packet switching in Cedar, (b) cache coherence: we employ a directory-based cache coherence whereas Cedar has no hardware support for cache coherence and (c) organization: Cedar is a UMA system where memory modules are centrally located and employ a forward network from processors to memories and a <p> In virtual cut-through, buffers are provided in the switch so that a message can be buffered when it is blocked. We use these switching techniques because they are used in the design of current multiprocessor switches <ref> [3, 6] </ref>. There are two types of messages that are transmitted over the MIN. One is a control message that consists of read, write and invalidation signals that are short and 4 flits long. <p> The input synchronization takes one to two cycles, and it takes one more cycle for transmission. As a result, the switch delay is about 4 cycles, which is similar to the time taken in SGI Spider and Intel Cavallino switches <ref> [3, 6] </ref>. Many times the requests arrive in bulk. For example, in case of a write miss on a block, the interface/coherence controller sends invalidation signals to processors having a copy of the block one after another. In such situations, providing some buffer in the switches will be helpful. <p> In this situation, the arbiter consists of two simple 4 to 1 crossbar arbiters. When the number of inputs is more (6 inputs with 4 VC's or buffers per input which is equivalent to 24 possible inputs per output in Cavallino and Spider <ref> [3, 6] </ref>), two-stage arbiters are needed to keep arbitration time within limit. However, we have no such problem with a simple 2x2 switch. Each of the virtual channels can again have space for multiple flits. <p> The switch parameters are set as explained in section 2. A flit size of 16 bits was considered which is same as the width of the link. The switch latency or delay is considered to be 1, 2 or 4 processor cycles. The SGI Spider and Cavallino <ref> [3, 6] </ref> both have 4 cycles of latency. Additionally, we consider 1 or 2 cycle delays for comparison since our aim is to design high-speed switches. The interface delay for protocol handling, etc. is 20 cycles, similar to DASH [9]. <p> The current switch delay in the commercial machines is about 5 switch core cycles (switch core operates slower than the CPU) for a 6x6 crossbar <ref> [3, 6] </ref>. With a 2x2 switch, we could obtain a faster switch because it takes much less time to arbitrate.
Reference: [7] <author> P. Gawghan and S. Yalamanchi. </author> <title> Adaptive routing protocols for hypercube interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 26(5):1223, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: The IN has become quite advanced with the introduction of techniques such as virtual channel [5] and adaptive routing <ref> [7] </ref>. The effect of all these advances in the INs has to be judged from the dynamic changes during the execution of applications. Execution-based evaluations for mesh interconnection networks have been reported recently [1, 8, 14] to test the effectiveness of virtual channels and adaptive routing.
Reference: [8] <author> A. Kumar and L. Bhuyan. </author> <title> Evaluating virtual channels for cache coherent shared memory multiprocessors. </title> <booktitle> ACM International Conference on Supercomuting, </booktitle> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The effect of all these advances in the INs has to be judged from the dynamic changes during the execution of applications. Execution-based evaluations for mesh interconnection networks have been reported recently <ref> [1, 8, 14] </ref> to test the effectiveness of virtual channels and adaptive routing. These studies evaluate the multiprocessors at the network level and do not reflect the effect of design details of switch hardware.
Reference: [9] <author> D. Lenoski and et al. </author> <title> The DASH prototype: Logic overhead and performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1):4161, </volume> <month> January </month> <year> 1993. </year>
Reference-contexts: The interface provides services such as dividing the message into packets or flits, initializing header flit of every packet with necessary routing information, etc. It contains a coherence controller, memory controller, directory controller and a reply controller, as described in <ref> [9] </ref>. The schematic of the network is shown in Fig. 1. It is a multistage interconnection network (MIN) employing 2x2 switches. In general, an NxN MIN consists of log 2 N stages of 2x2 switches with N/2 such switches per stage. The interconnection between the stages is perfect shuffle. <p> The SGI Spider and Cavallino [3, 6] both have 4 cycles of latency. Additionally, we consider 1 or 2 cycle delays for comparison since our aim is to design high-speed switches. The interface delay for protocol handling, etc. is 20 cycles, similar to DASH <ref> [9] </ref>. As explained in section 2, the messages are of two different lengths in a cache coherent multiprocessor. We assumed message lengths of 8 and 40 bytes for control and data messages, respectively, similar to the SGI switch. 3.1. <p> We also observe that the invalidation overhead is significant. We can reduce the invalidation and write times by one network latency by incorporating dirty forward where the processor having a dirty copy sends the block directly to the requester instead of through the directory update, like in DASH <ref> [9] </ref>. The invalidation overhead is also highly dependent on the network design. To reduce this overhead we need to employ techniques such as broadcasting and multicasting. We can thus perform the memory access and many invalidations simultaneously.
Reference: [10] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Synchronization without contention. </title> <booktitle> Proceedings of Fourth Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 269278, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: We have modified the coherence protocol where the cache controllers detect if a message has arrived out-of-order and holds it to be serviced later. The synchronization method used in our simulations is based on spin-locks using test-and-set operation with exponential backoff <ref> [10] </ref>. Barriers used in many of the applications were implemented using a shared counter. We also experimented with other types of barriers to reduce contention, however, our experience suggests that the main overhead in synchronization is the contention for the lock itself, not for the shared variable in the barrier.
Reference: [11] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(1):544, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: The overhead due to contention on synchronization variables is significant for some applications, such as FFT and MP3D <ref> [11] </ref>. The network latency also plays an important role on the synchronization overhead. In this paper, we do not discuss the synchronization overhead associated with any of the simulations. We concentrate only on the read/write requests to data items and the coherence requests. 3.2. <p> During these stages, instead of using two separate input and output arrays, we interleave these arrays to avoid conflict misses. MP3D is a three-dimensional particle simulator used in rarefied fluid flow simulation. We used 25000 molecules with the default geometry provided with SPLASH <ref> [11] </ref> which uses a 14 fi 24 fi 4 (2646-cell) space containing a single flat sheet placed at an angle to the free stream. The simulation was done for 5 time steps.
Reference: [12] <author> Y. Tamir and H. Chi. </author> <title> Symmetric crossbar arbiters for VLSI communication switches. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1):1327, </volume> <month> January </month> <year> 1993. </year>
Reference-contexts: The amount of buffer needed per input and output for wormhole routing is only one flit that is essential for transmission over the links. Once the flit arrives, the arbitration starts and the connection is made. Such a simple crossbar arbiter takes one cycle arbitration time <ref> [12] </ref>, however, the grant signals are modified to ensure continuity of transmission due to wormhole routing. The input synchronization takes one to two cycles, and it takes one more cycle for transmission.
Reference: [13] <author> J. Torrellas and Z. Zheng. </author> <title> The performance of the cedar multistage switching network. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 8(4):321336, </volume> <month> April </month> <year> 1997. </year>
Reference-contexts: Also, the measurements are done at the execution time level instead of a detailed study at the switch or IN level. We consider a multistage interconnection network (MIN) in this paper, similar to the one employed in Butterfly and Cedar multiprocessors. Torrellas and Zhang <ref> [13] </ref> recently reported detailed performance evaluation of the Cedar multiprocessor network. <p> Figs.7 and 8 present the average message latencies for the second group of applications, namely, FFT and FWA. In these two figures, one can notice a huge waiting delay at stage 0 (interface) for the backward messages. A similar situation was observed in Cedar network <ref> [13] </ref> where there was a large delay at the input of the backward network. In our case, the same network is used both for forward and backward requests and we use wormhole or cut-through switching instead of packet switching in Cedar.
Reference: [14] <author> A. Vaidya, A. Sivasubramaniam, and C. Das. </author> <title> Performance benefits of virtual channels and adaptive routing: An application-driven study. </title> <booktitle> Proc. 11th International Conference on Supercomputing, </booktitle> <address> Vienna, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: The effect of all these advances in the INs has to be judged from the dynamic changes during the execution of applications. Execution-based evaluations for mesh interconnection networks have been reported recently <ref> [1, 8, 14] </ref> to test the effectiveness of virtual channels and adaptive routing. These studies evaluate the multiprocessors at the network level and do not reflect the effect of design details of switch hardware.
Reference: [15] <author> B. Vergese and et al. </author> <title> Operating system support for improving data locality on CC-NUMA computer servers. </title> <booktitle> Proc. ASPLOS-VII, </booktitle> <pages> pages 279289, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Avg. queue length at network inter face ule. Designing further optimal memory management policies for different applications has been examined in <ref> [15] </ref> and is beyond the scope of this paper. Given the access pattern, our aim is to reduce this delay at the interface by designing a smoother pipeline in the switch architecture.
References-found: 15

