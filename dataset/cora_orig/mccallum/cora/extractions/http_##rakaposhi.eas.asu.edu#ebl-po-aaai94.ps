URL: http://rakaposhi.eas.asu.edu/ebl-po-aaai94.ps
Refering-URL: http://rakaposhi.eas.asu.edu/yochan.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: suresh@enuxsa.eas.asu.edu, rao@asu.edu  
Title: Learning Explanation-Based Search Control Rules For Partial Order Planning  
Author: Suresh Katukam Subbarao Kambhampati 
Address: Tempe, AZ 85287-5406  
Affiliation: Department of Computer Science and Engineering Arizona State University,  
Date: August, 1994  
Note: To appear in Proc. 12th Natl. Conf. on Artificial Intelligence (AAAI-94),  
Abstract: This paper presents snlp+ebl, the first implementation of explanation based learning techniques for a partial order planner. We describe the basic learning framework of snlp+ebl, including regression, explanation propagation and rule generation. We then concentrate on snlp+ebl's ability to learn from failures and present a novel approach that uses stronger domain and planner specific consistency checks to detect, explain and learn from the failures of plans at depth limits. We will end with an empirical evaluation of the efficacy of this approach in improving planning performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Barrett and D.S. Weld. </author> <title> Partial Order Planning: Evaluating Possible Efficiency Gains. </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 67, No.1, </volume> <year> 1994. </year>
Reference-contexts: Accordingly, there has been a considerable amount of research directed towards applying explanation-based learning (EBL) techniques to planning [2, 10]. Much of this work has been concentrated on the state-based planning. Motivated by the known advantages of partial order (PO) planning over state based planning in plan generation <ref> [1] </ref> and reuse [5, 6], in this paper we address the problem of adapting EBL techniques to speed up partial order planning. The EBL frameworks for state-based planning, such as PRODIGY/EBL [10] and FailSafe [2] typically construct search control rules that aim to steer the planner away from unpromising paths. <p> In this paper, we address both these issues. Specifically, we describe snlp+ebl, a system that learns search control rules for snlp, a causal link based PO planner <ref> [1, 9] </ref>. We will start by describing the basic learning framework in snlp+ebl, including the details of regression, explanation propagation and search-control rule learning (Section 2). We will then concentrate on snlp+ebl's ability to learn from failures. <p> In Section 3.1, we describe the results of an empirical study which demonstrate the effectiveness of the search control rules learned by this method. 2 The snlp+ebl system 2.1 The base level planner As mentioned earlier, our base level planner is snlp, a causal link based PO planner described in <ref> [1, 9] </ref>. snlp searches in the space of partial plans. Each partial plan can be seen as a 5 tuple: hS; O; B; L; Gi where: S is the set of actions (also called steps) in the plan. <p> The rules learned from such failures were successful in improving performance of snlp in some synthetic domains (such as D m S 2fl described in <ref> [1] </ref>). Unfortunately however, learning from analytical failures alone turns out to be ineffective in other recursive domains such as blocks world or job-shop scheduling. The main reason for this is that many futile lines of reasoning either never end in analytical failures or cross depth limits much before they do. <p> Our results clearly show that snlp+ebl was able to outperform snlp significantly on these problem populations. 5 snlp+ebl also outperforms snlp+domax, showing that learning search-control rules 5 The experiments reported here were all done on the standard public domain snlp implementation of Barrett and Weld <ref> [1] </ref>. In addition, we also experimented with more optimized implementations of snlp including those that do not resolve positive threats (and hence are not systematic), and avoid separation by defining threats in terms of necessary codesignation [14].
Reference: [2] <author> N. Bhatnagar. </author> <title> On-line Learning From Search Failures PhD thesis, </title> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction One way of coping with the computational complexity of domain-independent planning involves application of learning techniques to speed up planning. Accordingly, there has been a considerable amount of research directed towards applying explanation-based learning (EBL) techniques to planning <ref> [2, 10] </ref>. Much of this work has been concentrated on the state-based planning. <p> The EBL frameworks for state-based planning, such as PRODIGY/EBL [10] and FailSafe <ref> [2] </ref> typically construct search control rules that aim to steer the planner away from unpromising paths. The search control rules are generated by analyzing the search space explored by the planner to locate failures, constructing explanations for those failures, and regressing the failure explanations over the planning decisions. <p> In our current work, we have concentrated on learning rejection rules (although the basic framework can be extended to include selection rules). Unlike systems such as PRODIGY/EBL, which commence learning only after the planning is completed, snlp+ebl does adaptive (intra-trial) learning (c.f. <ref> [2] </ref>), which combines a form of dependency directed backtracking with generation of search-control rules. The planner does depth first search both in the learning and non-learning phases. During the learning phase, snlp+ebl invokes the learning component whenever the planner encounters a failure. <p> EBL thus provides a way of strategically applying stronger consistency checks. 4 Related Work As we noted earlier, our work on snlp+ebl was motivated by the desire to adapt the EBL frameworks developed for state-based planning, such as PRODIGY/EBL [10] and FailSafe <ref> [2] </ref>, to partial order planning. Our use of domain axioms to detect and explain failures at depth limits is related to, and inspired by Bhatnagar's work on FailSafe [2]. Bhatnagar also advocates starting with over-general explanations of failure and relaxing the rules in response to future impasses. <p> on snlp+ebl was motivated by the desire to adapt the EBL frameworks developed for state-based planning, such as PRODIGY/EBL [10] and FailSafe <ref> [2] </ref>, to partial order planning. Our use of domain axioms to detect and explain failures at depth limits is related to, and inspired by Bhatnagar's work on FailSafe [2]. Bhatnagar also advocates starting with over-general explanations of failure and relaxing the rules in response to future impasses. The rules learned in snlp+ebl, in contrast, are always sound in that any path rejected by a rejection rule is guaranteed to fail.
Reference: [3] <author> M. Drummond and K. Curry. </author> <title> Exploiting Temporal coherence in nonlinear plan construction. </title> <journal> Computational Intelligence, </journal> <volume> 4(2) </volume> <pages> 341-348, </pages> <year> 1988. </year>
Reference-contexts: When such looping makes snlp cross the depth limit, snlp+ebl uses the npconditions based consistency check to detect and explain this implicit failure, and learn from that explanation. To keep the consistency check tractable, snlp+ebl utilizes a restricted representation for domain axioms (first proposed in <ref> [3] </ref>): each domain axiom is represented as a conjunction of literals, with a set of binding constraints. The table below lists a set of domain axioms for the blocks world. <p> The rules learned in snlp+ebl, in contrast, are always sound in that any path rejected by a rejection rule is guaranteed to fail. Domain axioms have been used by other researchers in the past to control search in PO planning (c.f. <ref> [7, 3] </ref>). Our use of domain axioms is closest to the work of Kambhampati [7], who uses an idea similar to npconditions to implement a minimal-conflict based heuristic for controlling refitting in plan reuse.
Reference: [4] <author> J. Gratch and G. DeJong. COMPOSER: </author> <title> A Probabilistic Solution to the Utility problem in Speed-up Learning. </title> <booktitle> In Proc. AAAI 92, </booktitle> <address> pp:235--240, </address> <year> 1992 </year>
Reference-contexts: The current work shows that EBL provides a way of strategically applying domain axiom based consistency checks. Finally, although we did not explicitly address monitoring the utility of learned rules and filtering bad rules, we believe that utility monitoring models developed for state-based planners <ref> [4, 10] </ref> will also apply for PO planners. 5 Conclusions and Future Work In this paper, we presented snlp+ebl, the first systematic implementation of explanation-based search control rule learning to a PO planner.
Reference: [5] <author> S. Kambhampati and S. Kedar. </author> <title> A unified framework for explanation based generalization of partially ordered and partially instantiated plans. </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 67, No. 2, </volume> <year> 1994. </year>
Reference-contexts: Much of this work has been concentrated on the state-based planning. Motivated by the known advantages of partial order (PO) planning over state based planning in plan generation [1] and reuse <ref> [5, 6] </ref>, in this paper we address the problem of adapting EBL techniques to speed up partial order planning. The EBL frameworks for state-based planning, such as PRODIGY/EBL [10] and FailSafe [2] typically construct search control rules that aim to steer the planner away from unpromising paths. <p> (as it would have, in the current case). 4 It is interesting to note that in a similar situation, Prodigy [11] seems to learn a more specific rule which depends on establishing Rule Generalization: Once a search control rule is made, it is generalized using the standard EBL process (c.f. <ref> [5, 10] </ref>). This process aims to replace any constants in the search control rule with variables, without affecting the rule correctness. In snlp+ebl this is accomplished by doing the original regression process in terms of variables and their bindings (snlp already provides support for this). <p> During generalization, any bindings that are forced by the initial and goal state specifications of the original problem are removed from the explanation, leaving only those binding constraints that were forced by the initial explanation of the failure <ref> [5] </ref>. In the example in Figure 1, the binding ?x A in the failure explanation of node B is stripped when making the generalized rule. The generalization process also needs to generalize step names occurring in the failure explanation.
Reference: [6] <author> S. Kambhampati and J. Chen. </author> <title> Relative Utility of EBG based Plan Reuse in Partial Ordering vs. Total Ordering Planning. </title> <booktitle> In Proc. AAAI-93, </booktitle> <address> pp:514--519, </address> <year> 1993. </year>
Reference-contexts: Much of this work has been concentrated on the state-based planning. Motivated by the known advantages of partial order (PO) planning over state based planning in plan generation [1] and reuse <ref> [5, 6] </ref>, in this paper we address the problem of adapting EBL techniques to speed up partial order planning. The EBL frameworks for state-based planning, such as PRODIGY/EBL [10] and FailSafe [2] typically construct search control rules that aim to steer the planner away from unpromising paths.
Reference: [7] <author> S. Kambhampati. </author> <title> Exploiting Causal Structure to Control Retrieval and Refitting during Plan reuse. </title> <journal> Computational Intelligence, </journal> <volume> 10(2), </volume> <month> May </month> <year> 1994. </year>
Reference-contexts: The rules learned in snlp+ebl, in contrast, are always sound in that any path rejected by a rejection rule is guaranteed to fail. Domain axioms have been used by other researchers in the past to control search in PO planning (c.f. <ref> [7, 3] </ref>). Our use of domain axioms is closest to the work of Kambhampati [7], who uses an idea similar to npconditions to implement a minimal-conflict based heuristic for controlling refitting in plan reuse. <p> Domain axioms have been used by other researchers in the past to control search in PO planning (c.f. [7, 3]). Our use of domain axioms is closest to the work of Kambhampati <ref> [7] </ref>, who uses an idea similar to npconditions to implement a minimal-conflict based heuristic for controlling refitting in plan reuse. The current work shows that EBL provides a way of strategically applying domain axiom based consistency checks.
Reference: [8] <author> S. Katukam. </author> <title> Learning Explanation-Based Search Control Rules for Partial Order Planning. </title> <type> Masters Thesis, </type> <institution> Arizona State University, </institution> <address> Tempe, AZ, </address> <year> 1994. </year> <month> (forthcoming). </month>
Reference-contexts: Figure 2 contains a partial outline of the procedure for regressing arbitrary constraints of an explanation over an establishment decision involving step addition. A complete description of the regression rules for this and other planning decisions is beyond the scope of this paper, and can be found in <ref> [8] </ref>. Propagating Failure Explanations: Once an initial explanation of the failure has been identified, it is propagated up the failure branch to learn search control rules, as well as to do a form of dependency directed backtracking. Figure 3 provides the outline of this procedure.
Reference: [9] <author> D. McAllester and D. Rosenblitt. </author> <title> Systematic Nonliner Planning In Proc. </title> <booktitle> AAAI-91, </booktitle> <year> 1991. </year>
Reference-contexts: In this paper, we address both these issues. Specifically, we describe snlp+ebl, a system that learns search control rules for snlp, a causal link based PO planner <ref> [1, 9] </ref>. We will start by describing the basic learning framework in snlp+ebl, including the details of regression, explanation propagation and search-control rule learning (Section 2). We will then concentrate on snlp+ebl's ability to learn from failures. <p> In Section 3.1, we describe the results of an empirical study which demonstrate the effectiveness of the search control rules learned by this method. 2 The snlp+ebl system 2.1 The base level planner As mentioned earlier, our base level planner is snlp, a causal link based PO planner described in <ref> [1, 9] </ref>. snlp searches in the space of partial plans. Each partial plan can be seen as a 5 tuple: hS; O; B; L; Gi where: S is the set of actions (also called steps) in the plan.
Reference: [10] <author> S. Minton. </author> <title> Learning Effective Search Control Knowledge: An Explanation-Based Approach. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction One way of coping with the computational complexity of domain-independent planning involves application of learning techniques to speed up planning. Accordingly, there has been a considerable amount of research directed towards applying explanation-based learning (EBL) techniques to planning <ref> [2, 10] </ref>. Much of this work has been concentrated on the state-based planning. <p> Motivated by the known advantages of partial order (PO) planning over state based planning in plan generation [1] and reuse [5, 6], in this paper we address the problem of adapting EBL techniques to speed up partial order planning. The EBL frameworks for state-based planning, such as PRODIGY/EBL <ref> [10] </ref> and FailSafe [2] typically construct search control rules that aim to steer the planner away from unpromising paths. <p> (as it would have, in the current case). 4 It is interesting to note that in a similar situation, Prodigy [11] seems to learn a more specific rule which depends on establishing Rule Generalization: Once a search control rule is made, it is generalized using the standard EBL process (c.f. <ref> [5, 10] </ref>). This process aims to replace any constants in the search control rule with variables, without affecting the rule correctness. In snlp+ebl this is accomplished by doing the original regression process in terms of variables and their bindings (snlp already provides support for this). <p> The problems all had randomly generated initial states consisting of 3 to 8 blocks (using the procedure outlined in Minton's thesis <ref> [10] </ref>). The first test set contained 30 problems all of which had random 3-block stacks in the goal state. The second test set contained 100 randomly generated goal states (using the procedure in [10]) with 2 to 6 goals. <p> randomly generated initial states consisting of 3 to 8 blocks (using the procedure outlined in Minton's thesis <ref> [10] </ref>). The first test set contained 30 problems all of which had random 3-block stacks in the goal state. The second test set contained 100 randomly generated goal states (using the procedure in [10]) with 2 to 6 goals. For each test set, the planner was run on a set of randomly generated problems drawn from the same distribution (20 for the first set and 50 for the second). <p> EBL thus provides a way of strategically applying stronger consistency checks. 4 Related Work As we noted earlier, our work on snlp+ebl was motivated by the desire to adapt the EBL frameworks developed for state-based planning, such as PRODIGY/EBL <ref> [10] </ref> and FailSafe [2], to partial order planning. Our use of domain axioms to detect and explain failures at depth limits is related to, and inspired by Bhatnagar's work on FailSafe [2]. Bhatnagar also advocates starting with over-general explanations of failure and relaxing the rules in response to future impasses. <p> The current work shows that EBL provides a way of strategically applying domain axiom based consistency checks. Finally, although we did not explicitly address monitoring the utility of learned rules and filtering bad rules, we believe that utility monitoring models developed for state-based planners <ref> [4, 10] </ref> will also apply for PO planners. 5 Conclusions and Future Work In this paper, we presented snlp+ebl, the first systematic implementation of explanation-based search control rule learning to a PO planner.
Reference: [11] <author> S. Minton, J.G. Carbonell, C.A. Knoblock, D.R. Kuokka, O. Etzioni and Y. Gil. Explanation-BasedLearning: </author> <title> A Problem Solving Perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40:63--118, </volume> <year> 1989. </year>
Reference-contexts: 4 3 A more eager learning possibility would be to extend additional planning effort and see if B will have failed even if initial state were giving the open condition (as it would have, in the current case). 4 It is interesting to note that in a similar situation, Prodigy <ref> [11] </ref> seems to learn a more specific rule which depends on establishing Rule Generalization: Once a search control rule is made, it is generalized using the standard EBL process (c.f. [5, 10]).
Reference: [12] <author> N.J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga, </publisher> <address> Palo Alto, </address> <year> 1980. </year>
Reference-contexts: Set d parent (d); Goto Step 1. tor applications, and thus regression over planning decisions is very close to regression over operators <ref> [12] </ref>. In contrast, decisions in the PO planners correspond to addition of generalized constraints (steps, orderings, bindings, causal links) to the partial plan. snlp+ebl provides a sound and complete framework for regressing explanations over these decisions.
Reference: [13] <author> J.S. Penberthy and D.S. Weld. UCPOP: </author> <title> A sound, complete partial order planner for ADL. </title> <booktitle> In Proc. </booktitle> <address> KRR-92, </address> <year> 1992. </year>
Reference-contexts: Although our EBL framework was developed in the context of snlp we believe that it can be easily extended to more powerful PO planners such as UCPOP <ref> [13] </ref>. Learning from domain axiom based failures alone may not be sufficient in domains which do not have any strong implicit domain theory. We are currently working towards identifying other types of stronger consistency checks which can be used to complement the domain axiom based techniques in such domains.
Reference: [14] <author> M. Peot and D. Smith. </author> <title> Threat removal strategies for Nonlinear Planning. </title> <booktitle> In Proc. 11th AAAI, </booktitle> <year> 1993. </year>
Reference-contexts: In addition, we also experimented with more optimized implementations of snlp including those that do not resolve positive threats (and hence are not systematic), and avoid separation by defining threats in terms of necessary codesignation <ref> [14] </ref>.
References-found: 14

