URL: http://ferdowsi.eecs.berkeley.edu/~nlachang/Research/icassp95.ps.gz
Refering-URL: http://ferdowsi.eecs.berkeley.edu/papers.html
Root-URL: http://www.cs.berkeley.edu
Email: e-mail: nlachang@eecs.Berkeley.EDU, avz@eecs.Berkeley.EDU  
Title: ARBITRARY VIEW GENERATION FOR THREE-DIMENSIONAL SCENES FROM UNCALIBRATED VIDEO CAMERAS  
Author: Nelson L. Chang and Avideh Zakhor 
Address: Berkeley, CA 94720 USA  
Affiliation: Department of Electrical Engineering and Computer Sciences University of California,  
Abstract: This paper focuses on the representation and arbitrary view generation of three dimensional (3-D) scenes. In contrast to existing methods that construct a full 3-D model or those that exploit geometric invariants, our representation consists of dense depth maps at several preselected viewpoints from an image sequence. Furthermore, instead of using multiple calibrated stationary cameras or range data, we derive our depth maps from image sequences captured by an uncalibrated camera. We propose an adaptive matching algorithm which assigns various confidence levels to different regions. Nonuniform bicubic spline interpolation is then used to fill in low confidence regions in the depth maps. Once the depth maps are computed at preselected viewpoints, the intensity and depth at these locations are used to reconstruct arbitrary views of the 3-D scene. Experimental results are presented to verify our approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. H. Chien and J. K. Aggarwal, </author> <title> "Identification of 3D objects from multiple silhouettes using quadtrees/octrees," Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> vol. 36, no. </volume> <pages> 2-3, pp. 256-273, </pages> <month> Nov.- Dec. </month> <year> 1986. </year>
Reference-contexts: Existing work in this area can be classified into three classes: in the first class, a full 3-D model of the scene is constructed by volumetric intersection and then reprojected in order to generate the desired view <ref> [1] </ref>. The main difficulty with this approach is that of registering and combining the 2-D information to generate a full 3-D model. In the second class, views are generated by exploiting certain invariants in the geometry of the problem [2].
Reference: [2] <author> A. Shashua, </author> <title> "Projective structure from two uncalibrated images: Structure from motion and recognition," </title> <type> Tech. Rep. 1363, </type> <institution> MIT AI Laboratory, </institution> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: The main difficulty with this approach is that of registering and combining the 2-D information to generate a full 3-D model. In the second class, views are generated by exploiting certain invariants in the geometry of the problem <ref> [2] </ref>. This approach however does not correctly reconstruct points that become deoccluded. The third class of AVG algorithms attempts to deal with occluded/deoccluded regions in the scene better than the second class while not resorting to a full 3-D representation of the first class.
Reference: [3] <author> S. E. Chen and L. Williams, </author> <title> "View interpolation for image synthesis," </title> <booktitle> in Proceedings of SIGGRAPH, </booktitle> <address> New York, 1-6 Aug. </address> <year> 1993. </year>
Reference-contexts: Generally, a set of 2 1 2 -D surfaces is first estimated and then combined to generate the desired view. For example, Chen and Williams <ref> [3] </ref> measure range and camera transformation to establish pixel correspondence and then apply morphing to interpolate intermediate views. Similarly, Skerjanc and Liu [4] compute depth with known camera positions in order to synthesize inter mediate pictures.
Reference: [4] <author> R. Skerjanc and J. Liu, </author> <title> "A three camera approach for calculating disparity and synthesizing intermediate pictures," Signal Processing: </title> <journal> Image Communication, </journal> <volume> vol. 4, </volume> <pages> pp. 55-64, </pages> <year> 1991. </year>
Reference-contexts: Generally, a set of 2 1 2 -D surfaces is first estimated and then combined to generate the desired view. For example, Chen and Williams [3] measure range and camera transformation to establish pixel correspondence and then apply morphing to interpolate intermediate views. Similarly, Skerjanc and Liu <ref> [4] </ref> compute depth with known camera positions in order to synthesize inter mediate pictures. This work was supported by an Air Force Laboratory Graduate Fellowship, PYI-NSF grant MIP-9057466, ONR young investigator award N00014-92-J-1732, and Sun Microsystems. Our approach to AVG falls into this third category [5].
Reference: [5] <author> N. L. Chang, </author> <title> "View reconstruction from uncalibrated cameras for three-dimensional scenes," </title> <type> Master's thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1994. </year>
Reference-contexts: Similarly, Skerjanc and Liu [4] compute depth with known camera positions in order to synthesize inter mediate pictures. This work was supported by an Air Force Laboratory Graduate Fellowship, PYI-NSF grant MIP-9057466, ONR young investigator award N00014-92-J-1732, and Sun Microsystems. Our approach to AVG falls into this third category <ref> [5] </ref>. However, unlike existing techniques, we use a sequence of images captured by a hand held, uncalibrated camcorder. Uncalibrated cameras with unknown position are used to avoid the difficult and time-consuming step of calibration and therefore increase the flexibility of the image acquisition process. <p> A second artifact occurs in regions of constant intensity where disparities are incorrectly matched because the block size is again too small. Other artifacts occur in occluded regions and near depth discontinuities; see <ref> [5] </ref> for more details. It is straightforward to identify most of these artifacts and subsequently assign confidence levels to different regions in the scene. These confidence levels are important for locating the regions to ignore when combining multiple depth maps together. <p> These confidence levels are important for locating the regions to ignore when combining multiple depth maps together. To detect aperture ambiguity, a gradient-based edge detector is used to locate the horizontal edges <ref> [5] </ref>. Points in the image near these edge pixels are marked as possibly spurious. To identify constant intensity regions, a small window is used to find regions where the intensity variance is lower than a prespecified threshold. <p> A low variance suggests that the block consists of low texture and nearly constant intensity. Occluded regions consist of the unmapped points from matching two images in both directions. Performing the match in both directions also helps to validate the matches <ref> [5] </ref>. In the end, the scene will consist of low confidence regions marked according to the different artifacts: constant intensity, aperture ambiguity, occlusion, and inconsistencies in matching. <p> To avoid too sparse a depth map, we attempt to improve estimates in these regions. We propose an adaptive matching approach whereby a small block size is used to match regions near boundaries and a larger block size is used to match constant intensity regions <ref> [5] </ref>. This overcomes the well-known tradeoff between good boundary localization with a small window and improved matching in low textured regions with a large window. The final result consists of fairly dense and reasonably accurate disparities. 2.2. <p> For this task, we propose to estimate the translation parameter between maps and scale by the reciprocal. The relationship between disparities u m;i and relative motion b m may be derived <ref> [5] </ref> to get the linear least squares solution b m = i=1 (u 1;i )(u m;i ) i=1 (u 1;i ) 2 where b 1 is assumed to be one. Then b m is precisely the scaling factor ff m by which we need to adjust the m-th depth map. <p> Points outside the range median k are discarded. The remaining points are combined in a weighted average based on confidence levels <ref> [5] </ref>. Depth information from matching a vertically-related pair of images is also included in combination to overcome spurious esti mates due to horizontal aperture ambiguity. 2.4. Cubic B-Spline Approximation The depth map after the combination stage is fairly accurate in many regions. <p> The points of the reference frame arrays are considered not as discrete independent points, but rather as vertices of a deformable wire mesh <ref> [5] </ref> to overcome possible inconsistencies after transformation. Neighboring points in the reference frame are viewed as connected to one another. <p> Generally, introducing more reference frames helps to reduce the size of these holes. For the remaining holes, the region around each point is grown until a sufficient number of points exists within the region <ref> [5] </ref>. 4. RESULTS We shall now examine some results using the techniques described above. The object of interest is a mug placed atop a stool.
Reference: [6] <author> U. R. Dhond and J. K. Aggarwal, </author> <title> "Structure from stereo|a review," </title> <journal> IEEE Trans. Sys. Man Cyber., </journal> <volume> vol. 19, no. 6, </volume> <pages> pp. 1489-1509, </pages> <year> 1989. </year>
Reference-contexts: In the remainder of this section, each step will be discussed in detail. 2.1. Depth Estimation In the first step of the representation process, local dense depth maps are generated by matching the reference frame and each neighboring frame. Existing stereo matching techniques <ref> [6] </ref> cannot be used because they assume correspondence or known camera positions. Similarly, structure-from-motion algorithms [7] estimate the structure of only a small set of feature points in the scene.
Reference: [7] <author> R. Szeliski and S. B. Kang, </author> <title> "Recovering 3D shape and motion from image streams using non-linear least squares," </title> <type> Tech. Rep. CRL 93/3, </type> <institution> Digital Equipment Corporation: Cambridge Research Lab, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Depth Estimation In the first step of the representation process, local dense depth maps are generated by matching the reference frame and each neighboring frame. Existing stereo matching techniques [6] cannot be used because they assume correspondence or known camera positions. Similarly, structure-from-motion algorithms <ref> [7] </ref> estimate the structure of only a small set of feature points in the scene. We shall assume local perfect translation between every pair of images to reduce the depth estimation problem to a 1-D correspondence matching problem [8]. <p> The ICASSP Detroit, Michigan 8-12 May 1995 final step in the representation process is to estimate the relative camera motion between reference frames using an approach like <ref> [7] </ref>. Once the relative motion between all reference frames is known, a geometric relationship may be constructed among the different reference frames. This enables us to select the reference frames needed to use in the reconstruction stage.
Reference: [8] <author> B. K. P. Horn, </author> <title> Robot Vision. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Similarly, structure-from-motion algorithms [7] estimate the structure of only a small set of feature points in the scene. We shall assume local perfect translation between every pair of images to reduce the depth estimation problem to a 1-D correspondence matching problem <ref> [8] </ref>. In this case, the epipolar lines of the two images are parallel with the scan lines of the image. <p> However, at least two reference frames are needed to properly reconstruct the desired view to reduce noise and to recover occluded regions in the scene. 3.2. Generation of View Estimates The notion of applying motion parameters to a frame has been addressed in conventional computer vision literature <ref> [8] </ref>. Let (u 1 ; v 1 ) be the projection of a point in the scene onto the image plane.
References-found: 8

