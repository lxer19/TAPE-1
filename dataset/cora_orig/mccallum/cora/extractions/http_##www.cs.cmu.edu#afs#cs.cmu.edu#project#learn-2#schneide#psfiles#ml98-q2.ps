URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-2/schneide/psfiles/ml98-q2.ps
Refering-URL: http://www.cs.cmu.edu/~schneide/papers.html
Root-URL: 
Email: fawm,schneide,jab,msleeg@cs.cmu.edu  
Title: Q2: Memory-based active learning for optimizing noisy continuous functions learning field expands beyond prediction and
Author: Andrew W. Moore Jeff G. Schneider Justin A. Boyan and Mary S. Lee 
Note: These capabilities are directly applicable to industrial processes, and may become increasingly valuable elsewhere as the machine  
Address: Pittsburgh, PA 15213 6413 Howe Street http://www.cs.cmu.edu/~AUTON Pittsburgh, PA 15206  
Affiliation: CMU Computer Science Robotics 1 Schenley Park Research Inc. 2  
Abstract: This paper introduces a new algorithm, Q2, for optimizing the expected output of a multi-input noisy continuous function. Q2 is designed to need only a few experiments, it avoids strong assumptions on the form of the function, and it is autonomous in that it requires little problem-specific tweaking. Four existing approaches to this problem (response surface methods, numerical optimization, supervised learning, and evolutionary methods) all have inadequacies when the requirement of "black box" behavior is combined with the need for few experiments. Q2 uses instance-based determination of a convex region of interest for performing experiments. In conventional instance-based approaches to learning, a neighborhood was defined by proximity to a query point. In contrast, Q2 defines the neighborhood by a new geometric procedure that captures the size and shape of the zone of possible optimum locations. Q2 also optimizes weighted combinations of outputs, and finds inputs to produce target outputs. We compare Q2 with other optimizers of noisy functions on several problems, including a simulated noisy process with both non-linear continuous dynamics and discrete-event queueing components. Results are encouraging in terms of both speed and autonomy. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. E. P. Box and N. R. Draper. </author> <title> Empirical Model-Building and Response Surfaces. </title> <publisher> Wiley, </publisher> <year> 1987. </year>
Reference-contexts: Ingenious simplex transformations let the simplex shrink near the optimum, grow in large linear zones, and ooze along ridges. Experiment design & response surface meth-ods: Current RSM practice is described in the classic reference <ref> [1] </ref>. It proceeds by cautious steepest ascent hill-climbing. A region of interest (ROI) is established at a starting point and experiments are made at positions that can best be used to identify local function properties with low-order polynomial regression. <p> The worst scoring datapoint is in the top left. optimum. 4. Choose the point in ROI that keeps the regression as orthogonal <ref> [1] </ref> as possible, mimicking es tablished RSM practice. 5. Choose the point in ROI as far away from any previous datapoints (in or out of ROI) as possible. Option 5 is best empirically. <p> The results are given in Figure 18, and show a significant win for Q2. Q2 and the PMAX's also have far more repeatable results than the Amoebas. We also applied conventional RSM to this task, using a star design prescribed by <ref> [1] </ref>.
Reference: [2] <author> D. A. Cohn, Z. Ghahramani, and M. I. Jordan. </author> <title> Active learning with statistical models. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: In this paper we also assume that there are no long term dynamics, i.e. the output of the n'th experiment depends only on the n'th chosen x, not on previous x values or the time. Unlike <ref> [2, 6] </ref> we only try to find the optimum, not to model the g function. 3 POSSIBLE APPROACHES Many disciplines have methods that are relevant to noisy optimization. Space permits only a brief survey.
Reference: [3] <author> A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. </author> <title> Bayesian Data Analysis. </title> <publisher> Chapman and Hall, </publisher> <year> 1995. </year>
Reference-contexts: Details In this short paper, many details have been omitted. Some regressions predict a minimum or a saddlepoint, instead of a maximum. We have special-purpose techniques to deal with this. The Bayesian analysis is largely standard, and also omitted: see <ref> [3] </ref> for more details. Some confidence measures require Monte Carlo integration.
Reference: [4] <author> L. P. Kaelbling. </author> <title> Learning in Embedded Systems. </title> <type> PhD. Thesis; Technical Report No. </type> <institution> TR-90-04, Stanford University, Department of Computer Science, </institution> <month> June </month> <year> 1990. </year> <note> in [7]. </note>
Reference-contexts: Variations of PMAX include taking the next experiment not at the predicted optimum, but instead where the confidence intervals are widest [6], or where the top of the confidence interval is maximized [9], or in accordance with the Interval Estimation heuristic <ref> [4] </ref> or similar criteria [13]. Empirically, we have found that PMAX using locally weighted regression as the function approximator is often faster than more sophisticated alternatives [9]. However it has some serious drawbacks: * In conventional function approximation one must solve the bias-variance tradeoff.
Reference: [5] <author> H. Kushner and D. Clark. </author> <title> Stochastic Approximation Methods for Constrained and Unconstrained Systems. </title> <publisher> Springer-Verlag, </publisher> <year> 1978. </year>
Reference-contexts: Furthermore, current numerical methods cannot survive noise. Stochastic approximation: The algorithm of [12] finds roots without the use of derivative estimates. Keifer-Wolfowitz (KW) <ref> [5] </ref> is a related algorithm for noisy optimization. It estimates the gradient by performing experiments in both directions along each dimension of the input space. Based on the estimate, it moves its experiment center and repeats. It uses decreasing step sizes to ensure convergence.
Reference: [6] <author> D. J. C. MacKay. </author> <title> Bayesian Model Comparison and Backprop Nets. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> April </month> <year> 1992. </year>
Reference-contexts: In this paper we also assume that there are no long term dynamics, i.e. the output of the n'th experiment depends only on the n'th chosen x, not on previous x values or the time. Unlike <ref> [2, 6] </ref> we only try to find the optimum, not to model the g function. 3 POSSIBLE APPROACHES Many disciplines have methods that are relevant to noisy optimization. Space permits only a brief survey. <p> This approach has been used with a decision-tree approxima-tor [13], with neural nets (in many commercial products), and with locally weighted regression [9]. Variations of PMAX include taking the next experiment not at the predicted optimum, but instead where the confidence intervals are widest <ref> [6] </ref>, or where the top of the confidence interval is maximized [9], or in accordance with the Interval Estimation heuristic [4] or similar criteria [13]. Empirically, we have found that PMAX using locally weighted regression as the function approximator is often faster than more sophisticated alternatives [9].
Reference: [7] <author> S. Mahadevan, N. Marchalleck, T. Das, and A. Gosavi. </author> <title> Self-Improving Factory Simulation using Continuous-Time Average-Reward Reinforcement Learning. </title> <booktitle> In Proceedings of the 14th International Conference on Machine Learning (ICML '97), </booktitle> <address> Nashville, TN. </address> <publisher> Mor-gan Kaufmann, </publisher> <month> July </month> <year> 1997. </year>
Reference-contexts: Next, we examine a domain where experiments are time-consuming. Figure 19 shows a generalization of the multi-buffer machine task described in <ref> [7] </ref> (this makes 10 products instead of 5). There are two inputs defining a simple parameterized policy for when to service the machine. Services are costly, but unscheduled breakdown is much worse. <p> The results are shown for runs of only 24 experiments. Q2 learns a good policy in these 24 experiments, i.e. a total of only 24 fi 10000 simulation steps. This compares favorably with the tens of millions of simulation steps needed for reinforcement learning in <ref> [7] </ref>, but Q2 is unlikely to find as good a policy as their semi-MDP formulation. The final results show Q2 being used for root-finding instead of optimization. The hand position in Figure 21 is a noisy function of 1 and 2 .
Reference: [8] <author> A. W. Moore, D. J. Hill, and M. P. Johnson. </author> <title> An Empirical Investigation of Brute Force to choose Features, Smoothers and Function Approximators. </title> <editor> In S. Han-son, S. Judd, and T. Petsche, editors, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Volume 3. </volume> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Empirically, we have found that PMAX using locally weighted regression as the function approximator is often faster than more sophisticated alternatives [9]. However it has some serious drawbacks: * In conventional function approximation one must solve the bias-variance tradeoff. This is often determined automatically using cross-validation <ref> [8] </ref>, but this proves difficult with a set of very few, weirdly distributed datapoints obtained during optimization. Empirically we have observed dismal performance when attempting this.
Reference: [9] <author> A. W. Moore and J. Schneider. </author> <title> Memory-based Stochastic Optimization. </title> <editor> In D. Touretzky, M. Mozer, and M. Hasselmo, editors, </editor> <booktitle> Neural Information Processing Systems 8, </booktitle> <year> 1996. </year>
Reference-contexts: The next experiment is taken at the point that maximizes the estimate of g. This approach has been used with a decision-tree approxima-tor [13], with neural nets (in many commercial products), and with locally weighted regression <ref> [9] </ref>. Variations of PMAX include taking the next experiment not at the predicted optimum, but instead where the confidence intervals are widest [6], or where the top of the confidence interval is maximized [9], or in accordance with the Interval Estimation heuristic [4] or similar criteria [13]. <p> with a decision-tree approxima-tor [13], with neural nets (in many commercial products), and with locally weighted regression <ref> [9] </ref>. Variations of PMAX include taking the next experiment not at the predicted optimum, but instead where the confidence intervals are widest [6], or where the top of the confidence interval is maximized [9], or in accordance with the Interval Estimation heuristic [4] or similar criteria [13]. Empirically, we have found that PMAX using locally weighted regression as the function approximator is often faster than more sophisticated alternatives [9]. <p> confidence intervals are widest [6], or where the top of the confidence interval is maximized <ref> [9] </ref>, or in accordance with the Interval Estimation heuristic [4] or similar criteria [13]. Empirically, we have found that PMAX using locally weighted regression as the function approximator is often faster than more sophisticated alternatives [9]. However it has some serious drawbacks: * In conventional function approximation one must solve the bias-variance tradeoff. This is often determined automatically using cross-validation [8], but this proves difficult with a set of very few, weirdly distributed datapoints obtained during optimization.
Reference: [10] <author> A. W. Moore, J. Schneider, J. Boyan, and M. S. Lee. Q2: </author> <title> A memory-based active learning algorithm for Blackbox Noisy Optimization. </title> <note> In preparation, </note> <year> 1998. </year>
Reference-contexts: We have special-purpose techniques to deal with this. The Bayesian analysis is largely standard, and also omitted: see [3] for more details. Some confidence measures require Monte Carlo integration. These details will be discussed in a forthcoming technical report <ref> [10] </ref>. 5 RESULTS We begin by comparing Q2 with four versions of Amoeba and three versions of PMAX on the function f 1 from Figure 1 with noise of 0.3 added to each evaluation 2 .
Reference: [11] <author> W. Press, S. Teukolsky, W. Vetterling, and B. Flan-nery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: Unlike [2, 6] we only try to find the optimum, not to model the g function. 3 POSSIBLE APPROACHES Many disciplines have methods that are relevant to noisy optimization. Space permits only a brief survey. Numerical analysis: Numerical methods such as Newton-Raphson or Levenberg-Marquardt <ref> [11] </ref> have fast convergence properties, but they must be applied carefully to prevent oscillations or divergence to infinity, which violates our desire for black box autonomy. Furthermore, current numerical methods cannot survive noise. Stochastic approximation: The algorithm of [12] finds roots without the use of derivative estimates. <p> However, it can attempt wild experiments if there is noise, and discards the data it collects after each gradient estimate is made. Amoeba (see below) is a similar approach, but in our experience is superior to KW. Amoeba search: Amoeba <ref> [11] </ref> searches k-d space using a simplex (i.e. a k-dimensional tetrahedron). The function is evaluated at each vertex. The worst-performing vertex is reflected through the hyperplane defined by the remaining vertices to produce a new simplex that has moved up the estimated gradient. <p> Amoeba is the classic search algorithm 2 These tasks are available from http://www.cs.cmu.edu/~AUTON from <ref> [11] </ref>. Amoeba2 is the same except it is made resistant to noise by doing two evaluations and taking their average at each simplex vertex. Amoeba4 and Amoeba8 similarly average four and eight evaluations at each vertex. All the Amoebas begin with a medium-sized simplex started randomly in input space. <p> Algorithms like Newton's method, golden ratio search and conjugate gradient <ref> [11] </ref> maintain a region expected to contain an optimum and in which future experiments will occur. Q2 tries to do the same thing with two innovations. First, it can derive a ROI from a previous dataset irrespective of how that dataset was collected. Second, Q2 can survive noise.
Reference: [12] <author> H. Robbins and S. Monro. </author> <title> A stochastic approximation method. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22 </volume> <pages> 400-407, </pages> <year> 1951. </year>
Reference-contexts: Numerical analysis: Numerical methods such as Newton-Raphson or Levenberg-Marquardt [11] have fast convergence properties, but they must be applied carefully to prevent oscillations or divergence to infinity, which violates our desire for black box autonomy. Furthermore, current numerical methods cannot survive noise. Stochastic approximation: The algorithm of <ref> [12] </ref> finds roots without the use of derivative estimates. Keifer-Wolfowitz (KW) [5] is a related algorithm for noisy optimization. It estimates the gradient by performing experiments in both directions along each dimension of the input space. Based on the estimate, it moves its experiment center and repeats.
Reference: [13] <author> M. Salganicoff and L. H. Ungar. </author> <title> Active Exploration and Learning in Real-Valued Spaces using Multi-Armed Bandit Allocation Indices. </title> <booktitle> In Proceedings of the 12th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Based on the data from the experiments so far, it uses a non-linear function approximator to estimate the underlying function g (x). The next experiment is taken at the point that maximizes the estimate of g. This approach has been used with a decision-tree approxima-tor <ref> [13] </ref>, with neural nets (in many commercial products), and with locally weighted regression [9]. <p> Variations of PMAX include taking the next experiment not at the predicted optimum, but instead where the confidence intervals are widest [6], or where the top of the confidence interval is maximized [9], or in accordance with the Interval Estimation heuristic [4] or similar criteria <ref> [13] </ref>. Empirically, we have found that PMAX using locally weighted regression as the function approximator is often faster than more sophisticated alternatives [9]. However it has some serious drawbacks: * In conventional function approximation one must solve the bias-variance tradeoff.
References-found: 13

