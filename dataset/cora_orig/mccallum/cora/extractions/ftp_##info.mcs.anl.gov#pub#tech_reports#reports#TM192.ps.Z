URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/TM192.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts95.htm
Root-URL: http://www.mcs.anl.gov
Email: bischof@rz.rwth-aachen.de.  carle@rice.edu.  hovland@mcs.anl.gov.  
Title: ADIFOR 2.0 Users' Guide (Revision D)  
Author: by Christian Bischof, Alan Carle, Paul Hovland, Peyvand Khademi, flfl and Andrew Mauer flfl 
Address: Aachen, Seffenter Weg 23, 52074 Aachen,  MS 134, 6100 Main Street, Houston, TX 77005,  Cass Ave., Argonne, IL 60439,  
Affiliation: Computing Center, Technical University  Department of Computational and Applied Mathematics, Rice University,  Mathematics and Computer Science Division, Argonne National Laboratory, 9700 S.  flfl  at Argonne National Laboratory.  
Date: 1998  
Note: June  Address:  Address:  Address:  Work performed while employed  
Abstract: Mathematics and Computer Science Division Technical Memorandum No. 192 and Center for Research on Parallel Computation Technical Report CRPC-95516-S This work was supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38, by the National Aerospace Agency under Purchase Order L25935D, Cooperative Agreement No. NCCW-0027 and Cooperative Agreement No. NCC-1-212, and by the National Science Foundation, through the Center for Research on Parallel Computation, under Cooperative Agreement No. CCR-9120008. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Ham-marling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: It shows a simple Newton iteration being used to minimize Rosenbrock's function. The routines DLANGE and DGESV from the LAPACK package <ref> [1, 2] </ref> are used to compute the norm of y and to solve the linear system dy dx s = y. Our goal will be to replace the subroutine fprime, which approximates dy dx by using central divided differences, with an ADIFOR-generated derivative code. <p> For univariant functions, one may obtain any value in the interval [lim inf f 0 ; lim sup f 0 ]. For example, a generalized gradient for jxj at 0 is any number in <ref> [1; 1] </ref>. The values we choose to return as "derivative" values at points of nondifferentiability are generalized gradient values, provided that the chain rule for generalized gradients holds as a set inequality, rather than as an inclusion [11].
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Ham-marling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK User's Guide Release 2.0. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1994. </year>
Reference-contexts: It shows a simple Newton iteration being used to minimize Rosenbrock's function. The routines DLANGE and DGESV from the LAPACK package <ref> [1, 2] </ref> are used to compute the norm of y and to solve the linear system dy dx s = y. Our goal will be to replace the subroutine fprime, which approximates dy dx by using central divided differences, with an ADIFOR-generated derivative code.
Reference: [3] <author> B. M. Averick, R. G. Carter, J. J. More, and G. L. Xue. </author> <title> The MINPACK-2 test problem collection. </title> <type> Preprint ANL-MCS-P153-0692, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: Note that the ADIFOR-generated code remains unchanged. As a more realistic example, we consider the swirling flow problem, part of the MINPACK-2 test problem collection <ref> [3] </ref>, which was made available to us by Jorge More of Argonne National Laboratory. Here we solve a nonlinear system of equations F (x) = 0 for F : R n ! R n .
Reference: [4] <author> Brett Averick, Jorge More, Christian Bischof, Alan Carle, and Andreas Griewank. </author> <title> Computing large sparse Jacobian matrices using automatic differentiation. </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 15(2) </volume> <pages> 285-294, </pages> <year> 1994. </year>
Reference-contexts: the Jacobian is nonzero, c and FALSE otherwise. do i = 1, n if (nonzero (j,i)) then jac (j,i) = g_fvec (color (i),j) else jac (j,i) = 0.0 endif enddo enddo Experimental results using this approach on a suite of problems from the MINPACK test set collection are presented in <ref> [4, 10] </ref>. A.5 Case 4: Sparse Jacobian, two independent variables, one dependent variable The coating thickness problem, conveyed to us by Janet Rogers of the National Institute of Standards and Technology, presents many alternatives for using ADIFOR-generated subroutines.
Reference: [5] <author> Christian Bischof, Ali Bouaricha, Peyvand Khademi, and Jorge More. </author> <title> Computing gradients in large-scale optimization using automatic differentiation. </title> <type> Preprint MCS-P488-0195, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: Experimental results with partially separable functions from the MINPACK test set collection are presented in <ref> [5] </ref>. 59 Appendix B ADIntrinsics 1.5: Exception Handling Support for ADIFOR 2.0 B.1 Introduction In ADIFOR parlance, an "exception" is an event that occurs when an elementary function is evaluated at a point where the function result is defined, but the derivative is not.
Reference: [6] <author> Christian Bischof, Alan Carle, George Corliss, Andreas Griewank, and Paul Hovland. ADIFOR: </author> <title> Generating derivative codes from Fortran programs. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 11-29, </pages> <year> 1992. </year>
Reference-contexts: Familiarity with UNIX 1 and FORTRAN 77 is assumed. We strongly suggest that you, before reading this manual, have a look at the overview papers of ADIFOR 2.0 [7] and ADIFOR 1.0 <ref> [6] </ref>. They provide an overview of the philosophy of ADIFOR, references to successful applications of ADIFOR, and a perspective of how automatic differentiation relates to other approaches for computing derivatives. The ADIFOR 2.0 system consists of the ADIFOR Preprocessor, the ADIntrinsics template expander and library, and the SparsLinC library.
Reference: [7] <author> Christian Bischof, Alan Carle, Peyvand Khademi, and Andrew Mauer. ADIFOR 2.0: </author> <title> Automatic Differentiation of Fortran 77 Programs. </title> <journal> IEEE Computational Science & Engineering, </journal> <volume> 3(3) </volume> <pages> 18-32, </pages> <month> Fall, </month> <year> 1996. </year>
Reference-contexts: This paper describes step by step how to use version 2.0 (Revision D) of the ADIFOR system to generate derivative code. Familiarity with UNIX 1 and FORTRAN 77 is assumed. We strongly suggest that you, before reading this manual, have a look at the overview papers of ADIFOR 2.0 <ref> [7] </ref> and ADIFOR 1.0 [6]. They provide an overview of the philosophy of ADIFOR, references to successful applications of ADIFOR, and a perspective of how automatic differentiation relates to other approaches for computing derivatives. <p> A primary reason is that the forward mode of automatic differentiation upon which ADIFOR is mainly based (see <ref> [7] </ref>) requires roughly g p operations for every assignment statement in the original function.
Reference: [8] <author> Christian Bischof, George Corliss, and Andreas Griewank. </author> <title> ADIFOR exception handling. </title> <type> Technical Report ANL/MCS-TM-159, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: For most functions, there are several reasonable interpretations of what should be done when an exception occurs. ADIFOR 2.0, by default, chooses the approach that was deemed appropriate for most cases, based on the arguments presented in <ref> [8, 23] </ref>. However, only a person familiar with the code can decide whether this choice is the correct one for a particular given instance. Hence, when an exception occurs, one should examine the derivative code generated by ADIFOR 2.0 to make sure that the default values had the desired effect. <p> In case of a complex-valued argument, abs (x) is treated like sqrt (re (x)**2 + im (x)**2), and hence has the same exceptional behavior like the real-valued square root. The following principles were considered in designing the ADIFOR exception-handling mechanism (see <ref> [8, 23] </ref> for more background information): Generalized Gradient: Many algorithms for optimizing nonsmooth functions use generalized gradient values. A generalized gradient is any value in the convex hull of derivative values in the neighborhood of the point of nondifferentiability.
Reference: [9] <author> Christian Bischof, Larry Green, Kitty Haigler, and Tim Knauff. </author> <title> Parallel calculation of sensitivity derivatives for aircraft design using automatic differentiation. </title> <booktitle> In Proceedings of the 5th AIAA/NASA/USAF/ISSMO Symposium on Multidisciplinary Analysis and Optimization, </booktitle> <pages> AIAA 94-4261, pages 73-84. </pages> <institution> American Institute of Aeronautics and Astronautics, </institution> <year> 1994. </year>
Reference-contexts: In this fashion, the ADIFOR interface provides a mechanism for accomodating memory/runtime tradeoffs. An example of a parallel "derivative stripmining" technique based on this approach is presented in <ref> [9] </ref>. A.3 Case 2: Dense Jacobian, multiple independent and mul tiple dependent variables The second example involves a code that models adiabatic flow [25], a commonly used module in chemical engineering.
Reference: [10] <author> Christian Bischof, Peyvand Khademi, Ali Bouaricha, and Alan Carle. </author> <title> Computation of gradients and Jacobians by transparent exploitation of sparsity in automatic differentiation. </title> <type> Preprint MCS-P519-0595, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: the Jacobian is nonzero, c and FALSE otherwise. do i = 1, n if (nonzero (j,i)) then jac (j,i) = g_fvec (color (i),j) else jac (j,i) = 0.0 endif enddo enddo Experimental results using this approach on a suite of problems from the MINPACK test set collection are presented in <ref> [4, 10] </ref>. A.5 Case 4: Sparse Jacobian, two independent variables, one dependent variable The coating thickness problem, conveyed to us by Janet Rogers of the National Institute of Standards and Technology, presents many alternatives for using ADIFOR-generated subroutines.
Reference: [11] <author> Frank H Clark. </author> <title> Optimization and Nonsmooth Analysis. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1983. </year> <note> 94 Revision D ADIFOR 2.0 User's Guide February 16, </note> <year> 1998 </year>
Reference-contexts: The values we choose to return as "derivative" values at points of nondifferentiability are generalized gradient values, provided that the chain rule for generalized gradients holds as a set inequality, rather than as an inclusion <ref> [11] </ref>. Continuity of Catastrophe: The value at the point of nondifferentiability should in some sense be the limit of what happens in a neighborhood. For example, the derivative of asin (x) at 1 should be INFINITY.
Reference: [12] <author> Thomas F. Coleman. </author> <title> Large Sparse Numerical Optimization, </title> <booktitle> volume 165 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: As it turns out, less than 7 % of the total operations performed with gradient objects in the ADIFOR code involve nonzeros. On the other hand, by using a graph-coloring algorithm designed to identify structurally orthogonal columns (we used the one described in <ref> [12] </ref>), we can determine that this Jacobian can be grouped into 14 sets of structurally orthogonal columns, independent of the size of the problem.
Reference: [13] <author> Thomas F. Coleman, Burton S. Garbow, and Jorge J. </author> <title> More. Software for estimating sparse Jacobian matrices. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 10(3) </volume> <pages> 329-345, </pages> <year> 1984. </year>
Reference-contexts: Thus, if we compute a Jacobian J with n columns by setting g p = n, its computation will require roughly n times as many operations as the original function evaluation, independent of whether J is dense or sparse. However, it is well known <ref> [13, 17] </ref> that the number of function evaluations that are required to compute an approximation to the Jacobian by finite differences can be much less than n if J is sparse. Fortunately, the same idea can be applied to greatly reduce the running time of ADIFOR-generated derivative code as well.
Reference: [14] <author> A. R. Conn, N. I. M. Gould, and P. L. Toint. </author> <title> An introduction to the structure of large scale nonlinear optimization problems and the LANCELOT project. </title> <type> Report 89-19, </type> <institution> Namur University, </institution> <address> Namur, Belgium, </address> <year> 1989. </year>
Reference-contexts: However, it has the disadvantage that the initialization routine might have to be changed if fnc or np is altered. A.6 Computing Gradients of Partially Separable Functions A particular class of functions that arises often in optimization contexts is that of the so-called partially separable functions <ref> [14, 19, 20, 21, 22] </ref>.
Reference: [15] <author> Wayne H. Enright and John D. Pryce. </author> <title> Two FORTRAN packages for assessing initial value methods. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 13(1) </volume> <pages> 1-22, </pages> <year> 1987. </year>
Reference-contexts: Nonetheless, the general seeding ideas presented here for the nonsparse case apply equally as well to the sparse case. A.2 Case 1: Dense Jacobian, one independent, one depen dent variable Our first example is adapted from Problem C2 in the STDTST set of test problems for stiff ODE solvers <ref> [15] </ref> and was brought to our attention by George Corliss of Marquette University.
Reference: [16] <author> Herbert Fischer. </author> <title> Special problems in automatic differentiation. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 43 - 50. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Similarly, the value of dy dx j x=1 will be 1:0 rather than 4:0. This "anomaly" stems from the fact that automatic differentiation differentiates the statements executed in the course of program execution. This issue, as well as other subtle pitfalls, is discussed in <ref> [16] </ref>. 36 Chapter 8 Potential Problems Users may encounter several problems while trying to process programs with ADIFOR 2.0.
Reference: [17] <author> D. Goldfarb and P.L. Toint. </author> <title> Optimal estimation of Jacobian and Hessian matrices that arise in finite difference calculations. </title> <journal> Mathematics of Computation, </journal> <volume> 43 </volume> <pages> 69-88, </pages> <year> 1984. </year>
Reference-contexts: Thus, if we compute a Jacobian J with n columns by setting g p = n, its computation will require roughly n times as many operations as the original function evaluation, independent of whether J is dense or sparse. However, it is well known <ref> [13, 17] </ref> that the number of function evaluations that are required to compute an approximation to the Jacobian by finite differences can be much less than n if J is sparse. Fortunately, the same idea can be applied to greatly reduce the running time of ADIFOR-generated derivative code as well.
Reference: [18] <author> Andreas Griewank. </author> <title> On automatic differentiation. </title> <booktitle> In Mathematical Programming: Recent Developments and Applications, </booktitle> <pages> pages 83-108, </pages> <address> Amsterdam, 1989. </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Argonne National Laboratory 9700 S. Cass Avenue Argonne IL 60439 hovland@mcs.anl.gov 2 Chapter 2 Some Preliminaries Automatic differentiation is a technique for computing the derivatives of functions described by computer programs. See <ref> [18, 24] </ref> for an introduction to automatic differentiation. ADIFOR implements automatic differentiation by transforming a collection of FORTRAN 77 subroutines that compute a function f into new FORTRAN 77 subroutines that compute the derivatives of the outputs of f with respect to a specified set of inputs of f .
Reference: [19] <author> Andreas Griewank and Philippe L. Toint. </author> <title> On the unconstrained optimization of partially separable objective functions. </title> <editor> In M. J. D. Powell, editor, </editor> <booktitle> Nonlinear Optimization 1981, </booktitle> <pages> pages 301-312, </pages> <address> London, 1981. </address> <publisher> Academic Press. </publisher>
Reference-contexts: However, it has the disadvantage that the initialization routine might have to be changed if fnc or np is altered. A.6 Computing Gradients of Partially Separable Functions A particular class of functions that arises often in optimization contexts is that of the so-called partially separable functions <ref> [14, 19, 20, 21, 22] </ref>. <p> Any f with a sparse Hessian belongs to this class of problem <ref> [19] </ref>, regardless of whether the partially separable structure is expressed explicitly in the code. For many Jacobian computations, the final Jacobian is itself sparse, implying that there is much sparsity to be exploited in the intermediate computations.
Reference: [20] <author> Andreas Griewank and Philippe L. Toint. </author> <title> Partitioned variable metric updates for large structured optimization problems. </title> <journal> Numerische Mathematik, </journal> <volume> 39 </volume> <pages> 119-137, </pages> <year> 1982. </year>
Reference-contexts: However, it has the disadvantage that the initialization routine might have to be changed if fnc or np is altered. A.6 Computing Gradients of Partially Separable Functions A particular class of functions that arises often in optimization contexts is that of the so-called partially separable functions <ref> [14, 19, 20, 21, 22] </ref>.
Reference: [21] <author> M. Lescrenier. </author> <title> Partially separable optimization and parallel computing. </title> <journal> Ann. Oper. Res., </journal> <volume> 14 </volume> <pages> 213-224, </pages> <year> 1988. </year>
Reference-contexts: However, it has the disadvantage that the initialization routine might have to be changed if fnc or np is altered. A.6 Computing Gradients of Partially Separable Functions A particular class of functions that arises often in optimization contexts is that of the so-called partially separable functions <ref> [14, 19, 20, 21, 22] </ref>.
Reference: [22] <author> J. J. </author> <title> More. On the performance of algorithms for large-scale bound constrained problems. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <title> Large-Scale Numerical Optimization. </title> <publisher> SIAM, </publisher> <year> 1991. </year>
Reference-contexts: However, it has the disadvantage that the initialization routine might have to be changed if fnc or np is altered. A.6 Computing Gradients of Partially Separable Functions A particular class of functions that arises often in optimization contexts is that of the so-called partially separable functions <ref> [14, 19, 20, 21, 22] </ref>.
Reference: [23] <author> Gordon Pusch, Christian Bischof, and Alan Carle. </author> <title> On automatic differentiation of codes with complex arithmetic with respect ot real variables. </title> <type> Technical Report ANL/MCS-TM-188, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: For most functions, there are several reasonable interpretations of what should be done when an exception occurs. ADIFOR 2.0, by default, chooses the approach that was deemed appropriate for most cases, based on the arguments presented in <ref> [8, 23] </ref>. However, only a person familiar with the code can decide whether this choice is the correct one for a particular given instance. Hence, when an exception occurs, one should examine the derivative code generated by ADIFOR 2.0 to make sure that the default values had the desired effect. <p> In case of a complex-valued argument, abs (x) is treated like sqrt (re (x)**2 + im (x)**2), and hence has the same exceptional behavior like the real-valued square root. The following principles were considered in designing the ADIFOR exception-handling mechanism (see <ref> [8, 23] </ref> for more background information): Generalized Gradient: Many algorithms for optimizing nonsmooth functions use generalized gradient values. A generalized gradient is any value in the convex hull of derivative values in the neighborhood of the point of nondifferentiability.
Reference: [24] <author> Louis B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> volume 120 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: Argonne National Laboratory 9700 S. Cass Avenue Argonne IL 60439 hovland@mcs.anl.gov 2 Chapter 2 Some Preliminaries Automatic differentiation is a technique for computing the derivatives of functions described by computer programs. See <ref> [18, 24] </ref> for an introduction to automatic differentiation. ADIFOR implements automatic differentiation by transforming a collection of FORTRAN 77 subroutines that compute a function f into new FORTRAN 77 subroutines that compute the derivatives of the outputs of f with respect to a specified set of inputs of f .
Reference: [25] <author> J. M. Smith and H. C. Van Ness. </author> <title> Introduction to Chemical Engineering. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1975. </year> <month> 95 </month>
Reference-contexts: An example of a parallel "derivative stripmining" technique based on this approach is presented in [9]. A.3 Case 2: Dense Jacobian, multiple independent and mul tiple dependent variables The second example involves a code that models adiabatic flow <ref> [25] </ref>, a commonly used module in chemical engineering. This code models the separation of a pressurized mixture of hydrocarbons into liquid and vapor components in a distillation column, where pressure (and, as a result, temperature) decrease. This example was communicated to us by Larry Biegler of Carnegie-Mellon University.
References-found: 25

