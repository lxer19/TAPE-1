URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94451-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Interprocedural Symbolic Analysis  
Author: by Paul Havlak Keith D. Cooper, Associate Professor Robert S. Cartwright, 
Degree: A Thesis Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Approved, Thesis Committee: Ken Kennedy, Noah Harding Professor Computer Science Chair  Professor Computer Science John Bennett, Assistant Professor  
Date: May, 1994  
Address: Houston, Texas  
Affiliation: RICE UNIVERSITY  Computer Science  Electrical and Computer Engineering  
Abstract-found: 0
Intro-found: 1
Reference: [AC72] <author> F. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <booktitle> In Design and Optimization of Compilers, </booktitle> <pages> pages 1-30. </pages> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: Section 2.6 discusses related work on computing array side effects. Section 2.7 discusses future work, and we conclude in Section 2.8. 2.2 Bounded Sections A simple way to improve dependence analysis around a call site is to perform inline expansion, replacing the called procedure with its body <ref> [AC72] </ref>. This precisely represents the effects of the procedure as a sequence of ordinary statements, which are readily understood by existing dependence analyzers. <p> * combines all distance and direction vectors between two references (forward and backward) into one vector, and * fails to test for dependence carried by particular levels (i.e., freezing outer loops to test for inner-loop-carried dependences). 4.7 Related Work Basic Blocks Value numbering on basic blocks has a long history <ref> [AC72, ASU86] </ref>. It was generally employed in common subexpression elimination; that is, detecting multiple evaluations of the same operation and replacing them with one evaluation. The saved result can then be used in place of the other occurrences.
Reference: [ACK86] <author> Randy Allen, David Callahan, and Ken Kennedy. </author> <title> An implementation of interprocedural analysis in a vectorizing Fortran compiler. </title> <type> Technical Report TR86-38, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1986. </year>
Reference-contexts: The overriding concern in the implementation is that it be effective and efficient enough to be incorporated in a practical compilation system. Algorithm 2.1 summarizes the steps of the analysis, which is integrated with the three-phase interprocedural analysis and optimization structure of pfc <ref> [ACK86, CCKT86] </ref>. Regular section analysis adds less than 8000 lines to pfc, a roughly 150,000-line PL/I program which runs under IBM VM/CMS. This project successfully demonstrated that interprocedural subarray analysis is useful and inexpensive.
Reference: [AH82] <author> Marc Auslander and Martin Hopkins. </author> <title> An overview of the PL.8 compiler. </title> <booktitle> In Proceedings of the SIGPLAN '82 Symposium on Compiler Construction, </booktitle> <month> June </month> <year> 1982. </year>
Reference-contexts: Several other annotations on the control flow graph are easily derived, the most useful are described below. 1 More sophisticated exception handling than this is difficult for a compiler to analyze; witness the simplification of Pl/1 to Pl/8 by removing all non-fatal exceptions <ref> [AH82] </ref>. 38 SUBROUTINE P I = f () RETURN20 I = 530 GOTO 50 CONTINUE40 PRINT I50 GOTO 10 End Start I = f () 20 30 I = 5 40 PRINT I End 1 Start I = f () 20 I = 5 40 PRINT I End Control Flow G
Reference: [AK84] <author> J. R. Allen and K. Kennedy. </author> <title> PFC: A program to convert Fortran to parallel form. </title> <editor> In K. Hwang, editor, </editor> <booktitle> Supercomputers: Design and Applications, </booktitle> <pages> pages 186-203. </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Silver Spring, MD, </address> <year> 1984. </year>
Reference-contexts: Regular sections describe side effects to common substructures of arrays such as elements, rows, columns and diagonals [CK88a, Cal87]. This chapter describes our adaptation of regular sections and the design and implementation of array side effect analysis in the Rice Parallel Fortran 11 Converter (pfc) <ref> [AK84] </ref>, an automatic parallelization system that can also export dependence information to the ParaScope programming environment [KMT91b]. The overriding concern in the implementation is that it be effective and efficient enough to be incorporated in a practical compilation system. <p> While this method should prove powerful in detecting exactly equivalent expressions, it is inadequate to the more general comparisons needed for dependence testing and other symbolic analysis clients. PFC The Parallel Fortran Converter (pfc) takes Fortran 77 and converts it to vector, parallel, or parallel/vector form <ref> [AK84, AK87] </ref>. As it evolved from a single-procedure 92 vectorizer to a whole-program parallelization system, several symbolic analysis systems, all using symbolic expressions, were implemented. The original symbolic analysis method relies on forward substitution and simplification of expression trees [All83].
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of FORTRAN programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: While this method should prove powerful in detecting exactly equivalent expressions, it is inadequate to the more general comparisons needed for dependence testing and other symbolic analysis clients. PFC The Parallel Fortran Converter (pfc) takes Fortran 77 and converts it to vector, parallel, or parallel/vector form <ref> [AK84, AK87] </ref>. As it evolved from a single-procedure 92 vectorizer to a whole-program parallelization system, several symbolic analysis systems, all using symbolic expressions, were implemented. The original symbolic analysis method relies on forward substitution and simplification of expression trees [All83].
Reference: [All83] <author> J. R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and Its Application to Program Transformations. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: As it evolved from a single-procedure 92 vectorizer to a whole-program parallelization system, several symbolic analysis systems, all using symbolic expressions, were implemented. The original symbolic analysis method relies on forward substitution and simplification of expression trees <ref> [All83] </ref>. Whenever there is a single definition of an integer variable reaching a use, the right-hand-side of the expression is substituted in place of the use | provided that none of the variables in the replacement expression are redefined between the two points. The resulting expression is then simplified.
Reference: [ASU86] <author> Alfred V. Aho, Ravi I. Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: Several components of analysis and transformation could exploit knowledge of j, k, and n in the following fragment. subroutine foo (j,k,n) call init (j,k,n) do i = 1, n A (i+k) := ... enddo return end Common subexpression elimination deletes redundant computations of equivalent expressions <ref> [ASU86] </ref>. If j and k can be proven equivalent, then the two subscript expressions are also the same and their value need only be computed once per iteration. Constant propagation replaces variable references and expressions with equivalent constants [WZ91]. <p> A system which did handle pointers could benefit from a preliminary scalar symbolic analysis (e.g., to recognize pointers that step through memory like auxiliary induction variables). A commonly used method of connecting definitions of variables with their uses is using traditional bit-vector data-flow analysis <ref> [ASU86] </ref>. This builds def-use chains linking every definition with all reachable uses of the same variable (where reachable implies no intervening definition). <p> 7 These measurements were taken inside the ParaScope programming environment, optimized with gcc version 2.4.5, running on a Sun MicroSystems Sparc 10 with 64 Mbytes of memory. 62 63 64 3.6 Related Work 3.6.1 Groundwork Control-flow graphs, dominator trees, control-dependence graphs and SSA form are covered extensively in the literature <ref> [ASU86, LT79, CFR + 91] </ref>. However, while construction of loop-nesting trees is well-known tool, it is seldom documented. We use the algorithm of Tarjan's algorithm for testing reducibility [Tar74] extended to recognizing irreducible loops in a way that isolates the problems they cause. <p> (use) node and delete the edge. (This handling also applies to the special edges from conditional expressions in branches to uses as the predicates of fl functions.) 2 Other researchers have expression edges going the opposite direction, from operators to uses [AWZ88], or have subexpressions pictured below their parent expressions <ref> [ASU86] </ref>. We adopt our conventions for ease of combination with dataflow graphs depicted in the traditional way. 72 4. For each copy node (operator node employing the identity function), combine the copy node with its input and delete the input edge. 5. <p> We need to detect when values vary with loops, and which loops are involved. Dags without Merges For program fragments with straight-line code (basic blocks) or with branching code and no merges (extended basic blocks), value graphs are the same dags used in tra 74 ditional value numbering <ref> [ASU86] </ref>. Expressions with congruent value nodes have the same value on the same execution of the basic block. To demonstrate this, first consider congruent zero-height value dags (leaf nodes). In this case, there are no input edges, but the function labels are identical. <p> When a particular combination of operator and input value numbers is not found in the list, it is added. Whether we have newly added it or found an old copy, we return the position in the list as the value number <ref> [ASU86] </ref>. Value hashing is exactly this method, optimized by substituting a hash table for the list. So long as we have no cycles in our value graph, it will discover exactly the same congruences as the partitioning method. <p> * combines all distance and direction vectors between two references (forward and backward) into one vector, and * fails to test for dependence carried by particular levels (i.e., freezing outer loops to test for inner-loop-carried dependences). 4.7 Related Work Basic Blocks Value numbering on basic blocks has a long history <ref> [AC72, ASU86] </ref>. It was generally employed in common subexpression elimination; that is, detecting multiple evaluations of the same operation and replacing them with one evaluation. The saved result can then be used in place of the other occurrences. <p> We need only define the predicates to be propagated and appropriate functions for the effects of control-flow components <ref> [ASU86] </ref>. A dataflow analysis framework consists of * A set S of values to be propagated.
Reference: [AWZ88] <author> B. Alpern, M. N. Wegman, and F. K. Zadeck. </author> <title> Detecting equality of variables in programs. </title> <booktitle> In Proceedings of the Fifteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 1-11, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: However, to pattern-match only on the -functions and their inputs would mistakenly find b 3 equivalent to a 3 and c 3 . To be conservative, we must assume that -assignments at different program points have independent values <ref> [AWZ88] </ref>. SSA form was introduced by researchers at IBM and at Brown University [RWZ88, AWZ88]. The -assignments are direct descendants of Reif et al.'s birthpoints, used to refine def-use chains for symbolic analysis [RT81, RL86]. The construction of SSA form makes it a good basis for a dataflow graph. <p> To be conservative, we must assume that -assignments at different program points have independent values [AWZ88]. SSA form was introduced by researchers at IBM and at Brown University <ref> [RWZ88, AWZ88] </ref>. The -assignments are direct descendants of Reif et al.'s birthpoints, used to refine def-use chains for symbolic analysis [RT81, RL86]. The construction of SSA form makes it a good basis for a dataflow graph. <p> Examining the statement defining a1, GSA form gives us no direct way to tell us whether or not that statement executes. 3.4.1 Definition of TGSA Form Alpern et al. introduced the first gated version of static single-assignment form as high-level SSA form <ref> [AWZ88] </ref>. Ballance et al. introduced the terminology of gated single-assignment form [BMO90]. TGSA form is an extension of high-level SSA form to unstructured code, for which we use the more convenient GSA-form notation [Hav93]. Detailed comparisons with the prior versions are given in Section 3.6. <p> level for control dependences in any reducible control-flow graph. (We also compute these levels in irreducible graphs, but their interpretation is more problematic when the carrying loop is irreducible.) 3.6.2 High-level SSA form Alpern et al. presented SSA form and proposed extensions to improve handling of conditional merges and loops <ref> [AWZ88] </ref>. For structured programs, their if , enter , and exit are exactly equivalent to the TGSA-form versions of fl; , and , respectively. <p> The same value partitioning methods apply to TGSA form, extending these results to unstructured programs. 3.6.3 Original GSA form Ballance et al. introduced GSA form as a component of their program dependence web (PDW) [BMO90]. GSA form was inspired both by high-level SSA form <ref> [AWZ88] </ref> and PDGs with valve nodes [CF89]. 65 I = 1 I = 2 J = 1 J = 2 I 1 = 1 I 2 = 2 J 1 = 1 J 2 = 2 J 0 = fl (P; fl (Q; ?; J 1 ); fl (R; J 2 <p> In Fortran and related languages, values are built up by expression evaluation and loads and stores of variables. By combining expression trees with the global, GSA-form dataflow graph, we produce a value graph <ref> [AWZ88] </ref> for the procedure's computations that enables powerful symbolic pattern-matching and rewriting techniques. <p> edge, fuse the source (definition) node with the sink (use) node and delete the edge. (This handling also applies to the special edges from conditional expressions in branches to uses as the predicates of fl functions.) 2 Other researchers have expression edges going the opposite direction, from operators to uses <ref> [AWZ88] </ref>, or have subexpressions pictured below their parent expressions [ASU86]. We adopt our conventions for ease of combination with dataflow graphs depicted in the traditional way. 72 4. <p> To recognize congruence without actually extracting slices and comparing them, we employ the following, equivalent definition <ref> [AWZ88] </ref>. <p> The set of all value nodes can therefore be partitioned completely into non-overlapping subsets (equivalence classes) of mutually congruent nodes. 3 Almost-linear algorithms exist for finding this partition, starting with the assumption that all nodes are congruent (putting them all in one class) and progressively splitting congruence classes <ref> [AWZ88] </ref>. Termination occurs when no nodes assumed in the same class can be proven not congruent. 3 Just because a relation is an equivalence relation doesn't mean that it is the equivalence relation (i.e., equality of value). However, congruence does happen to be connected to equality. <p> The saved result can then be used in place of the other occurrences. SSA Form Global common subexpression elimination (over whole procedures) was one motivation in the development of static single-assignment form by Wegman, Zadeck, and others <ref> [AWZ88, RWZ88] </ref>. Their work inspired our use of a value graph to represent symbolic expressions. However, the method used for detecting congruence requires the whole value graph to be built before any congruences are detected. <p> This severely limits rewriting of the graph, as it would have to be reanalyzed after rewriting to see if new congruences were exposed. A few tricks are shown for handling commutative operators <ref> [AWZ88] </ref>, but they are unlikely to generalize. While this method should prove powerful in detecting exactly equivalent expressions, it is inadequate to the more general comparisons needed for dependence testing and other symbolic analysis clients.
Reference: [Bal89] <author> Vasanth Balasundaram. </author> <title> Interactive Parallelization of Numerical Scientific Programs. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> July </month> <year> 1989. </year> <note> Available as Rice COMP TR89-95. 155 </note>
Reference-contexts: Operations on descriptors should be linear or, at worst, quadratic in the rank of the array. Researchers at Rice have defined several variants of regular sections to represent common access patterns while satisfying these constraints <ref> [Cal87, CK88a, Bal89, BK89] </ref>. Original Regular Sections Callahan's thesis proposed two regular section frameworks. He dismissed the first, resembling Li and Yew's atom images, due to the difficulty of devising efficient standardization and meet operations [Cal87]. <p> Intersection is implemented using standard dependence tests, which also take time proportional to the number of subscripts. Data Access Descriptors Concurrently with the original work reported in this chapter, Balasundaram and Kennedy developed Data Access Descriptors (DADs) as a general technique for describing data access <ref> [Bal89, BK89, Bal90] </ref>. DADs represent information about both the shapes of array accesses and their traversal order; for our comparison we are interested only in the shapes. <p> The implementors of the PIPS system use a similar general framework, but specialize it to common cases and claim to achieve much greater efficiency in practice [IJT91, Iri93]. Simple Sections The simple section framework of Balasundaram's Data Access Descriptors was developed to represent predicates on subscripts <ref> [BK89, Bal89] </ref>. It can also be used to represent predicates on other values (although we are unaware of prior work in extending it thus). Simple sections represent predicates of the form L xy U , i.e., pairwise linear inequalities where the coefficients are constrained to be 1 or -1.
Reference: [Bal90] <author> Vasanth Balasundaram. </author> <title> A mechanism for keeping useful internal information in parallel programming tools: the Data Access Descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 154-170, </pages> <year> 1990. </year>
Reference-contexts: Intersection is implemented using standard dependence tests, which also take time proportional to the number of subscripts. Data Access Descriptors Concurrently with the original work reported in this chapter, Balasundaram and Kennedy developed Data Access Descriptors (DADs) as a general technique for describing data access <ref> [Bal89, BK89, Bal90] </ref>. DADs represent information about both the shapes of array accesses and their traversal order; for our comparison we are interested only in the shapes.
Reference: [Ban78] <author> J. Banning. </author> <title> A Method for Determining the Side Effects of Procedure Calls. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1978. </year>
Reference-contexts: Classical Methods The classical methods of interprocedural summary dataflow analysis compute mod and use sets indicating which parameters and global variables may be modified or used in the procedure <ref> [Ban78, Bar77, CK85] </ref>. Such summary information costs only 29 two bits per variable. Meet and intersection may be implemented using single-bit or bit-vector logical operations. <p> Note that mod and ref are conservatively large; they include all variables that may be accessed, but can include some that are not. For this reason, we call them may-summary problems <ref> [Ban79, Ban78] </ref>. Mod and ref are also called flow-insensitive problems because their precise solution does not depend on the structure of control flow [Ban78]. The side effect information for a procedure is built from the effects of its statements and calls to other procedures. <p> For this reason, we call them may-summary problems [Ban79, Ban78]. Mod and ref are also called flow-insensitive problems because their precise solution does not depend on the structure of control flow <ref> [Ban78] </ref>. The side effect information for a procedure is built from the effects of its statements and calls to other procedures. <p> A loop-independent anti-dependence (read-write) from B to C is disproven by a comparison of loop-invariant symbols. Again, i2 &gt; il is the important fact. 140 6.9 Related Work 6.9.1 Flow-Sensitive Analysis Banning Banning introduced the distinction between flow-sensitive and flow-insensitive inter-procedural dataflow problems <ref> [Ban79, Ban78] </ref>. For flow-insensitive problems, the control structure inside a procedure is irrelevant to summarizing the effects of imbedded calls. For example, a variable is in the mod set of a procedure if it is potentially modified via any statement or call site.
Reference: [Ban79] <author> J. Banning. </author> <title> An efficient way to find the side effects of procedure calls and the aliases of variables. </title> <booktitle> In Conference Record of the Sixth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> San Antonio, TX, </address> <month> January </month> <year> 1979. </year>
Reference-contexts: Note that mod and ref are conservatively large; they include all variables that may be accessed, but can include some that are not. For this reason, we call them may-summary problems <ref> [Ban79, Ban78] </ref>. Mod and ref are also called flow-insensitive problems because their precise solution does not depend on the structure of control flow [Ban78]. The side effect information for a procedure is built from the effects of its statements and calls to other procedures. <p> A loop-independent anti-dependence (read-write) from B to C is disproven by a comparison of loop-invariant symbols. Again, i2 &gt; il is the important fact. 140 6.9 Related Work 6.9.1 Flow-Sensitive Analysis Banning Banning introduced the distinction between flow-sensitive and flow-insensitive inter-procedural dataflow problems <ref> [Ban79, Ban78] </ref>. For flow-insensitive problems, the control structure inside a procedure is irrelevant to summarizing the effects of imbedded calls. For example, a variable is in the mod set of a procedure if it is potentially modified via any statement or call site.
Reference: [Ban86] <author> U. Banerjee. </author> <title> A direct parallelization of CALL statements a review. CSRD Rpt. </title> <type> 576, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> April </month> <year> 1986. </year>
Reference-contexts: Operations on these regions are expensive in the worst case; the meet operation requires finding the convex hull of the combined set of inequalities and intersection uses a potentially exponential linear inequality solver <ref> [Ban86] </ref>. A succession of meet operations can also produce complicated regions with potentially as many inequalities as the number of primitive accesses merged together.
Reference: [Bar77] <author> J. Barth. </author> <title> An interprocedural data flow analysis algorithm. </title> <booktitle> In Conference Record of the Fourth ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Los Angeles, </address> <month> January </month> <year> 1977. </year>
Reference-contexts: Classical Methods The classical methods of interprocedural summary dataflow analysis compute mod and use sets indicating which parameters and global variables may be modified or used in the procedure <ref> [Ban78, Bar77, CK85] </ref>. Such summary information costs only 29 two bits per variable. Meet and intersection may be implemented using single-bit or bit-vector logical operations.
Reference: [BBDS93] <author> David Bailey, Eric Barszcz, Leonardo Dagum, and Horst Simon. </author> <title> NAS parallel benchmark results. </title> <type> Technical Report RNR-92-002, </type> <institution> NASA Ames Research Center, </institution> <month> February </month> <year> 1993. </year>
Reference: [BC86] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and par-allelization. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 162-175, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: One straightforward translation method is to linearize the subscripts for the referenced section of a formal parameter, adding the offset of the passed location of the actual parameter <ref> [BC86] </ref>. <p> Reference list methods are simple and precise, but are asymptotically as expensive as in-line expansion. Linearization Burke and Cytron proposed representing each multidimensional array reference by linearizing its subscript expressions to a one-dimensional address expression. Their method also retains bounds information for loop induction variables occurring in the expressions <ref> [BC86] </ref>. They describe two ways of implementing the meet operation. One merely keeps a list of the individual address expressions. The constructs a composite expression that can be polynomial in the loop induction variables. The disadvantages of the first method are described above.
Reference: [BCHT90] <author> P. Briggs, K. Cooper, M. W. Hall, and L. Torczon. </author> <title> Goal-directed inter-procedural optimization. </title> <type> Technical Report TR90-147, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: IF (P) THEN j = 1 ELSE j = 2 S4 ENDIF * call sites When there are multiple calls to a procedure, constraints for all context must be merged (alternatively, the procedure can be cloned <ref> [BCHT90] </ref> and the cloned copies optimized for the different contexts). SUBROUTINE bar () ... CALL foo (1, 4) ... CALL foo (2, 0) ... 99 SUBROUTINE foo (j, k) ... <p> While the direct speedups from reduced code size and saving the test would be small, indirect benefits can be large due to reduced uncertainty. This is especially true in library code, where different combinations of values enable very different functionality. 6.6.2 Cloning and Inlining Apply goal-directed methods of <ref> [BCHT90] </ref>. First, a backwards propagation to determine which sets of values are important. An initial approximation is the ref set; a better one is the set of variables used in the slices for array subscripts, conditional branches and array and loop bounds. <p> However, merging such sets appears to be more difficult [CH78]. The best approach may be to have a backwards pass of analysis that determines which values are particularly interesting, then restrict the forward analysis to compute predicates only on the interesting values (like goal-directed cloning) <ref> [BCHT90] </ref>. More array analysis. The same pattern-matching techniques applied in value numbering scalar computations can be applied to array operations. We already support value numbering of subscripted array references treated as access and update functions [DSvH93].
Reference: [BCT92] <author> Preston Briggs, Keith D. Cooper, and Linda Torczon. Rematerializa-tion. </author> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 311-321, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Static single-assignment (SSA) form enables efficient construction of program data-flow information [CFR + 91]. From constant propagation to register allocation, SSA-based algorithms have proven more powerful and efficient than those based on traditional def-use edges <ref> [WZ91, BCT92] </ref>. A distinctive feature of SSA form is the placement of a minimal number of pseudo-assignments at program merge points so that no statement, except a pseudo-assignment, is reached by multiple definitions. These pseudo-assignments use - functions to select which of the merged definitions flows to successive uses.
Reference: [BGNP93] <editor> Utpal Banerjee, David Gelernter, Alex Nicolau, and David Padua, editors. </editor> <booktitle> Proc. Sixth Workshop on Languages and Compilers for Parallel Computing, volume 768 of Lecture Notes in Computer Science, </booktitle> <address> Port-land, OR, August 1993. </address> <publisher> Springer Verlag. </publisher>
Reference: [BK89] <author> V. Balasundaram and K. Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism-enhancing transformations. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year> <month> 156 </month>
Reference-contexts: Operations on descriptors should be linear or, at worst, quadratic in the rank of the array. Researchers at Rice have defined several variants of regular sections to represent common access patterns while satisfying these constraints <ref> [Cal87, CK88a, Bal89, BK89] </ref>. Original Regular Sections Callahan's thesis proposed two regular section frameworks. He dismissed the first, resembling Li and Yew's atom images, due to the difficulty of devising efficient standardization and meet operations [Cal87]. <p> Intersection is implemented using standard dependence tests, which also take time proportional to the number of subscripts. Data Access Descriptors Concurrently with the original work reported in this chapter, Balasundaram and Kennedy developed Data Access Descriptors (DADs) as a general technique for describing data access <ref> [Bal89, BK89, Bal90] </ref>. DADs represent information about both the shapes of array accesses and their traversal order; for our comparison we are interested only in the shapes. <p> The implementors of the PIPS system use a similar general framework, but specialize it to common cases and claim to achieve much greater efficiency in practice [IJT91, Iri93]. Simple Sections The simple section framework of Balasundaram's Data Access Descriptors was developed to represent predicates on subscripts <ref> [BK89, Bal89] </ref>. It can also be used to represent predicates on other values (although we are unaware of prior work in extending it thus). Simple sections represent predicates of the form L xy U , i.e., pairwise linear inequalities where the coefficients are constrained to be 1 or -1.
Reference: [BKK + 89] <author> V. Balasundaram, K. Kennedy, U. Kremer, K. S. M c Kinley, and J. Subhlok. </author> <title> The ParaScope Editor: An interactive parallel programming tool. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: We ran the programs through regular section analysis and dependence analysis in pfc, then examined the resulting dependence graphs by hand and with the ParaScope editor <ref> [BKK + 89] </ref>. 2.5.1 Benchmarks LINPACK Analysis of Linpack provides a basis for comparison with other methods for analyzing interprocedural array side effects. Both Li and Yew [LY88a] and Triolet [Tri85] found several parallel calls in Linpack using their implementations in the University of Illinois translator, Parafrase.
Reference: [BMO90] <author> R. Ballance, A. Maccabe, and K. Ottenstein. </author> <title> The Program Dependence Web: a representation supporting control-, data-, and demand-driven interpretation of imperative languages. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <address> White Plains, New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: consists of the basic blocks from G CF with a different set of edges. 3 Given a G CF edge e with label L from a branch node B, 3 Another common form, the factored control dependence graph, contains these nodes plus region nodes wherever there are control dependence merges <ref> [BMO90, FOW87] </ref>. 42 procedure build control deps (G CF , G CD ) N CD := N CF foreach edge (v; w) 2 E CF do lab := label of edge y := w ` := INDEPENDENT while (y 6 v) do z := deepest common header (y; v) if (` <p> Ballance et al. introduced the terminology of gated single-assignment form <ref> [BMO90] </ref>. TGSA form is an extension of high-level SSA form to unstructured code, for which we use the more convenient GSA-form notation [Hav93]. Detailed comparisons with the prior versions are given in Section 3.6. <p> We focus on the asymptotic and empirical performance of GSA form, because the prior work used a different algorithm and gave no experiments <ref> [BMO90] </ref>. Per Procedure Mean 90 th Quant. 99 th Quant. <p> TGSA form is identical to high-level SSA form for structured code. The same value partitioning methods apply to TGSA form, extending these results to unstructured programs. 3.6.3 Original GSA form Ballance et al. introduced GSA form as a component of their program dependence web (PDW) <ref> [BMO90] </ref>.
Reference: [Bou92] <author> Raymond T. Boute. </author> <title> The euclidean definition of the functions div and mod. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 14(2) </volume> <pages> 127-144, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: When merges remove all relations of a variable to others, the separate stride and alignment information is no longer redundant. 5 Using the Euclidean definition of mod and div, for which x == S (x div S) + (x mod S), 0 (x mod S) &lt; jxj <ref> [Bou92] </ref>. 113 foreach x 2 ref, in canonical order do if (x == c, a constant) then do nothing else if (parent (x) == nil) then // x is a root variable C x := 1; K x := 0 else // we have (c p p + c x x
Reference: [Cal87] <author> D. Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> March </month> <year> 1987. </year>
Reference-contexts: We must determine the subarrays that are accessed in order to safely exploit the parallelism. Callahan and Kennedy proposed a method called regular section analysis for tracking interprocedural subarray side-effects. Regular sections describe side effects to common substructures of arrays such as elements, rows, columns and diagonals <ref> [CK88a, Cal87] </ref>. This chapter describes our adaptation of regular sections and the design and implementation of array side effect analysis in the Rice Parallel Fortran 11 Converter (pfc) [AK84], an automatic parallelization system that can also export dependence information to the ParaScope programming environment [KMT91b]. <p> Atom Images Li and Yew extended Parafrase to compute sets of atom images describing the side effects of procedures [LY88a, LY88b]. Like the original version of regular sections described in Callahan's thesis <ref> [Cal87] </ref>, these record subscript expressions that are linear in loop induction variables along with bounds on the induction variables. Any reference with linear subscript expressions in a triangular iteration space can be precisely represented, and they keep a separate atom image for each reference. <p> Operations on descriptors should be linear or, at worst, quadratic in the rank of the array. Researchers at Rice have defined several variants of regular sections to represent common access patterns while satisfying these constraints <ref> [Cal87, CK88a, Bal89, BK89] </ref>. Original Regular Sections Callahan's thesis proposed two regular section frameworks. He dismissed the first, resembling Li and Yew's atom images, due to the difficulty of devising efficient standardization and meet operations [Cal87]. <p> Original Regular Sections Callahan's thesis proposed two regular section frameworks. He dismissed the first, resembling Li and Yew's atom images, due to the difficulty of devising efficient standardization and meet operations <ref> [Cal87] </ref>. <p> Original Regular Sections Callahan's thesis proposed two regular section frameworks. He dismissed the first, resembling Li and Yew's atom images, due to the difficulty of devising efficient standardization and meet operations [Cal87]. Restricted Regular Sections The second framework, restricted regular sections <ref> [Cal87, CK88a] </ref>, is limited to access patterns in which each subscript is * a procedure-invariant expression (with constants and procedure inputs), * unknown (and assumed to vary over the entire range of the dimension), or * unknown but diagonal with one or more other subscripts.
Reference: [Cal88] <author> D. Callahan. </author> <title> The program summary graph and flow-sensitive interpro-cedural data flow analysis. </title> <booktitle> In Proceedings of the ACM SIGPLAN 88 Conference on Program Language Design and Implementation, </booktitle> <pages> pages 47-56, </pages> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: These and other flow-sensitive problems rely on some control-flow information. How much information should be attached to the call graph node for each procedure? In prior work, this has ranged from a relatively complete representation [Mye81] to one highly tailored for Kill and Use <ref> [Cal88] </ref>. Concurrently with other researchers at Rice [HMBCR93], we have decided to take a middle road of a lightweight control-flow graph for each procedure, with annotations to the control-flow nodes as needed for each particular problem. <p> Symbolic evaluation of conditional branch predicates can improve all interprocedural solutions, and with carefully built initial information, we can converge to a stable solution in one interprocedural phase. 134 6.7.1 Kill and Use Callahan's Program Summary Graph has separate sets of entry, exit, call and return nodes for every variable <ref> [Cal88] </ref>. To model the PSG, we must attach these variable reference nodes to their corresponding procedure-summary control-flow graph nodes. Dataflow analysis is performed in the initial phase, with entry and exit nodes treated as using and defining all formal parameters and global variables. <p> The example of Myers's supergraph also shows that algorithms which appear exponential in theory may be efficient on programs encountered in practice. Callahan Callahan's Program Summary Graph (PSG) is a pruned version of the supergraph tuned to the solution of the flow-sensitive summary problems kill and use <ref> [Cal88] </ref>. <p> Basic alias and flow-sensitive side-effect solutions are prerequisites to more extensive analysis. Must-alias, kill, use and even live solutions can be approximated much more efficiently than the worst case given by Myers and can have substantial payoff <ref> [Mye81, Cal88] </ref>. Constant propagation can be very useful, but requires at least an intraprocedural symbolic representation [CCKT86, GT93]. Propagation of constant ranges is not quite so simple or effective as that of pure constants, but can have a significant effect in proving loop bounds to be positive.
Reference: [CC77] <author> Patrick Cousot and Radhia Cousot. </author> <title> Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints. </title> <booktitle> In Conference Record of the Fourth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 238-252, </pages> <address> Los Angeles, </address> <month> January </month> <year> 1977. </year>
Reference-contexts: Handling of recursion turns out not to be a major issue. Iterative techniques can guarantee convergence to a fixed point solution using Cousot's technique of widening operators <ref> [CC77, Cou81] </ref>. Li and Yew proposed a preparatory analysis of recursive programs that guarantees termination in three iterations [LY88b, LY88c]. Either of these methods may be adapted for regular sections. 2.6.1 Summary Methods Summary methods describe each kind of side effect using a single subarray descriptor for each array. <p> The restricted section lattice has the finite descending chain property, which we originally thought necessary for efficient handling of recursive programs. However, we can obtain fixed-point solutions on more complicated lattices by applying the techniques of Cousot or of Li and Yew <ref> [CC77, LY88c] </ref> Bounded Regular Sections Anticipating that restricted regular sections would not be precise enough for effective parallelization, Callahan and Kennedy proposed an implementation of regular sections 32 with bounds. That proposal led to our development of the bounded regular section framework in this chapter. <p> However, one can simulate the efficiency of a finite-depth lattice. Since loop-variant values ( operators) are the reason why we may continually propagate, we need a special merge operator, called a widening operator, that is guaranteed to converge in a finite number of steps <ref> [CC77] </ref>. Different widening operators can be defined for each variety of predicates.
Reference: [CCKT86] <author> D. Callahan, K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural constant propagation. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: Constant propagation replaces variable references and expressions with equivalent constants [WZ91]. Interprocedural versions need ways of representing values that are not constant yet <ref> [CCKT86, GT93] </ref>. In the example, the values of j, k, and n passed to init are constant if the values passed into foo are constant. Test elision removes branches and (unexecutable) dead code when comparisons and boolean expressions can be evaluated [WZ91]. <p> The overriding concern in the implementation is that it be effective and efficient enough to be incorporated in a practical compilation system. Algorithm 2.1 summarizes the steps of the analysis, which is integrated with the three-phase interprocedural analysis and optimization structure of pfc <ref> [ACK86, CCKT86] </ref>. Regular section analysis adds less than 8000 lines to pfc, a roughly 150,000-line PL/I program which runs under IBM VM/CMS. This project successfully demonstrated that interprocedural subarray analysis is useful and inexpensive. <p> During the interprocedural phase, after producing the classical scalar side effect solution, but before propagating regular sections, we check to see if CLOBBER may change M. If so, we change S1's array side effect to A (?). A similar technique has proven successful for interprocedural constant propagation in pfc <ref> [Tor85, CCKT86] </ref>. 2 Another pass is needed to examine call sites for building the call graph, but this can be combined with the initial information gathering. <p> When interprocedural constant propagation was added to pfc, symbolic expressions were also required to represent not-yet-constant values (that might become constant during the propagation phase) <ref> [CCKT86] </ref>. The data structures used resemble a value graph, but are built in an ad-hoc manner whose computational complexity is hard to control. Because these symbolic expressions are mainly used for representing but not for comparing values, they were not designed to maximize congruence of subtrees. <p> Must-alias, kill, use and even live solutions can be approximated much more efficiently than the worst case given by Myers and can have substantial payoff [Mye81, Cal88]. Constant propagation can be very useful, but requires at least an intraprocedural symbolic representation <ref> [CCKT86, GT93] </ref>. Propagation of constant ranges is not quite so simple or effective as that of pure constants, but can have a significant effect in proving loop bounds to be positive.
Reference: [CF89] <author> Robert S. Cartwright and Matthias Felleisen. </author> <title> The semantics of program dependence. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, Oregon, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: GSA form was inspired both by high-level SSA form [AWZ88] and PDGs with valve nodes <ref> [CF89] </ref>. 65 I = 1 I = 2 J = 1 J = 2 I 1 = 1 I 2 = 2 J 1 = 1 J 2 = 2 J 0 = fl (P; fl (Q; ?; J 1 ); fl (R; J 2 ; ?)) I 1 = 1 <p> Groups at Wisconsin and Rice have developed semantics for PDGs <ref> [HPR88, CF89, Sel92] </ref>. Selke gives semantics for programs with arrays and arbitrary control flow, and shows how to insert valve nodes efficiently. Despite the greater maturity of PDGs as a program representation, they are inferior to TGSA form for value numbering.
Reference: [CFR + 91] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: (z == w) then ` := level (w) add (v; y) with level ` and label lab to E CD y := immediate post-dominator of y Algorithm 3.2 Building Control Dependences control dependence edges, also labeled L, go from B to every node that must execute if e is taken <ref> [CFR + 91] </ref>. Algorithm 3.2 gives a method for building control dependences, extended by us to mark each dependence with the loop carrying it [CFS90a]. <p> This is allowed by the Fortran standard. If two variables are later found to be aliased, one or both modified and both referenced, we give up on all computations involving those variables. 44 * Subscripted array references are treated as compositional functions of mono lithic variables <ref> [CFR + 91] </ref>. . <p> This may not seem like much of a problem, but if there were n conditionally executed definitions of x and m uses, we could end up with O (nm) def-use chains. Static single-assignment (SSA) form enables efficient construction of program data-flow information <ref> [CFR + 91] </ref>. From constant propagation to register allocation, SSA-based algorithms have proven more powerful and efficient than those based on traditional def-use edges [WZ91, BCT92]. <p> Algorithm 3.3 gives the method for adding -assignments, modified for construction of GSA form as described later. While the worst-case complexity is cubic in the size of the program, SSA construction takes linear time in practice <ref> [CFR + 91] </ref>. Edges linking definitions to uses are added during a walk over the pre-dominator tree, in time proportional to the number of edges [CFR + 91]. <p> While the worst-case complexity is cubic in the size of the program, SSA construction takes linear time in practice <ref> [CFR + 91] </ref>. Edges linking definitions to uses are added during a walk over the pre-dominator tree, in time proportional to the number of edges [CFR + 91]. <p> Linking definitions to uses in SSA form is often referred to as renaming, because each use of a variable is now reached by a unique definition, so that the different live ranges (a definition and its uses) can be thought of a separate variable <ref> [CFR + 91] </ref>. 3.4 GSA Form: Dataflow and Conditionals Gated Single-Assignment (GSA) form extends SSA form with information about branch predicates controlling merges. That is, the most visible difference is the replacement of functions with new functions giving not just the merged definitions but the predicates determining which definition reaches. <p> While our methods may take exponential time and space for contrived examples, they take linear time for programs satisfying loose and reasonable structural requirements. Auxiliary Analyses and Data Structures. Efficient methods exist for computing each of pre- and post-, Tarjan intervals and control dependences <ref> [LT79, Tar74, CFR + 91, CFS90a] </ref>. While the asymptotic time bounds range from almost linear to quadratic, for practical purposes, these methods are linear in the number of G CF edges (E CF ). <p> The addition of functions for SSA form affects the size of the dataflow graph. While the number of additional assignments can be quadratic in the number of original references, it is linear throughout extensive practical experiments <ref> [CFR + 91] </ref>. The number of edges in G SSA DU is linear in the number of references if the arity of 60 merges is constant. In practice, G SSA DU should be far smaller than traditional def-use chains, which are often quadratic in the number of references. <p> The average time re quired per G CF element on the entire sample was 2.4 ms; the average of the procedure averages was 3.1 ms. 7 These results confirm prior results that SSA form has time and space requirements linear in the size of the procedure <ref> [CFR + 91] </ref>. <p> 7 These measurements were taken inside the ParaScope programming environment, optimized with gcc version 2.4.5, running on a Sun MicroSystems Sparc 10 with 64 Mbytes of memory. 62 63 64 3.6 Related Work 3.6.1 Groundwork Control-flow graphs, dominator trees, control-dependence graphs and SSA form are covered extensively in the literature <ref> [ASU86, LT79, CFR + 91] </ref>. However, while construction of loop-nesting trees is well-known tool, it is seldom documented. We use the algorithm of Tarjan's algorithm for testing reducibility [Tar74] extended to recognizing irreducible loops in a way that isolates the problems they cause.
Reference: [CFS90a] <author> R. Cytron, J. Ferrante, and V. Sarkar. </author> <title> Compact representations for control dependence. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on 157 Program Language Design and Implementation, </booktitle> <pages> pages 337-351, </pages> <address> White Plains, New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Algorithm 3.2 gives a method for building control dependences, extended by us to mark each dependence with the loop carrying it <ref> [CFS90a] </ref>. Loop-carried control dependence edges run from conditional branches out of a loop to the header block of the loop, and are labeled with the level of the header. Note that statements post-dominating the header, but not post-dominating the exit branch, must lie inside the loop. <p> These changes cause threefold growth in G CF at worst. The dominance and loop nesting trees are easily updated, and control dependence construction proceeds normally on the augmented G CF <ref> [CFS90a] </ref>. As none of the new nodes are branches, control dependence edges among the original G CF nodes are unaffected. Construction of SSA form proceeds normally except for tweaks to the placement phase, given in Algorithm 3.3. <p> While our methods may take exponential time and space for contrived examples, they take linear time for programs satisfying loose and reasonable structural requirements. Auxiliary Analyses and Data Structures. Efficient methods exist for computing each of pre- and post-, Tarjan intervals and control dependences <ref> [LT79, Tar74, CFR + 91, CFS90a] </ref>. While the asymptotic time bounds range from almost linear to quadratic, for practical purposes, these methods are linear in the number of G CF edges (E CF ).
Reference: [CFS90b] <author> R. Cytron, J. Ferrante, and V. Sarkar. </author> <title> Experience using control dependence in PTRAN. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Note that statements post-dominating the header, but not post-dominating the exit branch, must lie inside the loop. Ignoring the loop-carried edges leaves the acyclic forward control-dependence graph F CD <ref> [CFS90b] </ref>. 3.3 Data Flow We ultimately require a representation of the flow of values through variables and expression trees that captures the essentials of what is computed but disregards if it is computed. Such a representation will help us to compare computations that are executed in different contexts. <p> The postbody node for each loop then terminates each complete iteration, and is post-dominated by the header. Wherever the sink of a loop-exit edge has other in-edges, we split the loop-exit edge with the addition of a postexit node (PE) <ref> [CFS90b] </ref>. These changes cause threefold growth in G CF at worst. The dominance and loop nesting trees are easily updated, and control dependence construction proceeds normally on the augmented G CF [CFS90a].
Reference: [CH78] <author> P. Cousot and N. Halbwachs. </author> <title> Automatic discovery of linear restraints among variables of a program. </title> <booktitle> In Conference Record of the Fifth ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 84-96, </pages> <year> 1978. </year>
Reference-contexts: Predicates have therefore usually been defined on the current contents of variables <ref> [CH78, Kar76, IJT91] </ref>. This complicates analysis in two major ways. Assignments remove predicates. When a variable is assigned a new value (unrelated to its old one), then it must be removed from the predicate set for that point. <p> However, constant bounds have an unbounded lattice. In order to guarantee convergence in iterative propagation, we must introduce a widening operator, applied when propagating around loops, that takes the lattice values to ? in a bounded number of operations <ref> [CH78] </ref>. The method proposed array section subscripts in Chapter 2 can be applied here; simply limit the number of times a lattice value saved at a loop header can be lowered; if the limit is exceeded, replace the lattice value with ?. <p> More sophisticated predicates. The value of affine inequalities on arbitrary numbers of variables (or value numbers) should be investigated. Efficient implementations exist for composing inequalities into a convex set [Pug91]. However, merging such sets appears to be more difficult <ref> [CH78] </ref>. The best approach may be to have a backwards pass of analysis that determines which values are particularly interesting, then restrict the forward analysis to compute predicates only on the interesting values (like goal-directed cloning) [BCHT90]. More array analysis.
Reference: [CHT91] <author> K. Cooper, M. W. Hall, and L. Torczon. </author> <title> An experiment with inline substitution. </title> <journal> Software|Practice and Experience, </journal> <volume> 21(6) </volume> <pages> 581-601, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: However, even if the whole program becomes no larger, the loop nest which contained the call may grow dramatically, causing explosive growth in resource requirements due to the non-linearity of array dependence analysis and other single-procedure compilation algorithms <ref> [CHT91] </ref>. To gain some of the benefits of inline expansion without its drawbacks, we must find another representation for the effects of the called procedure. For dependence analysis, we are interested in the memory locations modified or used by a procedure. <p> Goff, Kennedy and Tseng studied the performance of dependence tests on Riceps and other benchmarks [GKT91]. Some Riceps and 23 Riceps candidate codes have also been examined in a study on the utility of inline expansion of procedure calls <ref> [CHT91] </ref>. The six programs studied here are two Riceps codes linpackd and track) and four codes from the inlining study. 3 2.5.2 Precision The precision of regular sections, or their correspondence to the true access sets, is largely a function of the programming style being analyzed. <p> More symbolic analysis would improve the practicality of the entire method. Overall, the additional analysis time is comparable to that required to analyze programs after heuristically-determined inline expansion in Cooper, Hall and Torczon's study <ref> [CHT91] </ref>. Absolute timings for the array side effect analyses implemented in Parafrase by Triolet and by Li and Yew, have not been published, though Li and Yew do state that their method runs 2.6 times faster than Triolet's [LY88a]. <p> This deficiency certainly resulted in more dependences for the larger programs in Perfect. 7 In the inlining study at Rice, none of the commercial compilers was able to detect the parallel call in dogleg even after inlining, presumably due to complicated control flow <ref> [CHT91] </ref>. 27 routine calls in Parallel Calls name DO loops IP Li-Yew RS GBCO 8 1 1 1 GECO 8 1 1 1 PBCO 8 1 1 1 POCO 8 1 1 1 PPCO 8 1 1 1 SICO 1 1 1 1 SPCO 1 1 1 1 TRCO 4 1 <p> And even in non-recursive Fortran, the main focus of this work, exhaustive inlining can lead to excessive growth in code size | an exponential amount of growth in the worst case <ref> [CHT91] </ref>. Inlining: * cannot always be applied (potential array reshapes) * can slow program execution (anomalies in register allocation) * frequently slows down compile time (nonlinear algorithms) These are not fatal criticisms of inlining as a program transformation.
Reference: [CK85] <author> K. Cooper and K. Kennedy. </author> <title> Efficient computation of flow insensitive interprocedural summary information. </title> <booktitle> In Proceedings of the SIGPLAN '84 Symposium on Compiler Construction, SIGPLAN Notices Vol. </booktitle> <volume> 19, No. 6, </volume> <month> July </month> <year> 1985. </year>
Reference-contexts: Classical Methods The classical methods of interprocedural summary dataflow analysis compute mod and use sets indicating which parameters and global variables may be modified or used in the procedure <ref> [Ban78, Bar77, CK85] </ref>. Such summary information costs only 29 two bits per variable. Meet and intersection may be implemented using single-bit or bit-vector logical operations.
Reference: [CK88a] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 517-550, </pages> <year> 1988. </year>
Reference-contexts: We must determine the subarrays that are accessed in order to safely exploit the parallelism. Callahan and Kennedy proposed a method called regular section analysis for tracking interprocedural subarray side-effects. Regular sections describe side effects to common substructures of arrays such as elements, rows, columns and diagonals <ref> [CK88a, Cal87] </ref>. This chapter describes our adaptation of regular sections and the design and implementation of array side effect analysis in the Rice Parallel Fortran 11 Converter (pfc) [AK84], an automatic parallelization system that can also export dependence information to the ParaScope programming environment [KMT91b]. <p> Unfortunately, our experiences with pfc indicate that this summary information is too coarse for dependence testing and the effective detection of parallelism <ref> [CK88a] </ref>. The problem is that the only access sets representable in this method are "the whole array" and "none of the array" (see Figure 2.2). <p> Operations on descriptors should be linear or, at worst, quadratic in the rank of the array. Researchers at Rice have defined several variants of regular sections to represent common access patterns while satisfying these constraints <ref> [Cal87, CK88a, Bal89, BK89] </ref>. Original Regular Sections Callahan's thesis proposed two regular section frameworks. He dismissed the first, resembling Li and Yew's atom images, due to the difficulty of devising efficient standardization and meet operations [Cal87]. <p> Original Regular Sections Callahan's thesis proposed two regular section frameworks. He dismissed the first, resembling Li and Yew's atom images, due to the difficulty of devising efficient standardization and meet operations [Cal87]. Restricted Regular Sections The second framework, restricted regular sections <ref> [Cal87, CK88a] </ref>, is limited to access patterns in which each subscript is * a procedure-invariant expression (with constants and procedure inputs), * unknown (and assumed to vary over the entire range of the dimension), or * unknown but diagonal with one or more other subscripts.
Reference: [CK88b] <author> K. Cooper and K. Kennedy. </author> <title> Interprocedural side-effect analysis in linear time. </title> <booktitle> In Proceedings of the ACM SIGPLAN 88 Conference on Program Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: However, the proposed Fortran 90 standard allows recursion [X3J89], so it must be handled someday. Unfortunately, a straightforward iterative propagation of regular sections will not terminate, since the lattice has unbounded depth. Li and Yew [LY88b] and Cooper and Kennedy <ref> [CK88b] </ref> describe approaches for propagating subarrays that are efficient regardless of the depth of the lattice. However, it may be more convenient to implement a simple iterative technique while simulating a bounded-depth lattice. <p> Meet and intersection may be implemented using single-bit or bit-vector logical operations. Also, there exist algorithms that compute complete solutions, in which the number of meets is linear in the number of procedures and call sites in the program, even when recursion is permitted <ref> [CK88b] </ref>. Unfortunately, our experiences with pfc indicate that this summary information is too coarse for dependence testing and the effective detection of parallelism [CK88a]. The problem is that the only access sets representable in this method are "the whole array" and "none of the array" (see Figure 2.2). <p> We think that our method of two interprocedural passes will be acceptable for several reasons. Primarily, we believe that flow-insensitive interprocedural analysis is trivial in analysis costs and stable in its solutions. * The classical flow-insensitive problems of mod, ref and alias have extremely efficient solutions <ref> [CK88b, CK89] </ref>. * Mod, ref and alias can be expected to change infrequently across program edits | adding the first and deleting the last references of a variable are rare decisions, and aliases are rare both in their occurrence and in their insertion and deletion. 129 * More modern languages than <p> Banning's work has inspired linear-time algorithms for and commercial implementations of flow-insensitive interprocedural analysis <ref> [CK88b, LM94] </ref>. While his approximate solutions for flow-sensitive problems are a valuable starting point, more precise methods may be profitable today. Myers Myers addresses flow-sensitive problems by fusing the internal control flow graphs of each procedure into one program supergraph [Mye81]. <p> Many interesting languages, such as Fortran, C, and C++, do not allow nesting of procedures (c n == 1). Furthermore, the number of formal parameters f p to a procedure is generally bounded by some small constant c p (such an assumption is made in <ref> [CK88b] </ref>). The global variables that can be aliased to each other arise only when formal parameters are exposed as globals to nested procedures, so h p is bounded by (c n 1) fl c p .
Reference: [CK89] <author> Keith D. Cooper and Ken Kennedy. </author> <title> Fast interprocedural alias analysis. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 49-59, </pages> <address> Austin, Texas, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: We think that our method of two interprocedural passes will be acceptable for several reasons. Primarily, we believe that flow-insensitive interprocedural analysis is trivial in analysis costs and stable in its solutions. * The classical flow-insensitive problems of mod, ref and alias have extremely efficient solutions <ref> [CK88b, CK89] </ref>. * Mod, ref and alias can be expected to change infrequently across program edits | adding the first and deleting the last references of a variable are rare decisions, and aliases are rare both in their occurrence and in their insertion and deletion. 129 * More modern languages than <p> His experiments suggest that his methods may be practical. The improved complexity analysis of Myers's methods for flow-sensitive analysis suggest further experiments using our annotated call graph. Aliases are usually rare in practice, so the Must-Alias computation should not be much more expensive than the May-Alias computation <ref> [CK89] </ref>. An initial interprocedural phase could gather alias information and allow a pruned supergraph, something like the PSG described later, to be used for more efficient solution. The solutions to flow-sensitive analysis could be improved through symbolic evaluation of conditional branches.
Reference: [CKB93] <author> Philip L. Campbell, Ksheerabdhi Krishna, and Robert A. Ballance. </author> <title> Refining and defining the Program Dependence Web. </title> <type> Technical Report TR 93-6, </type> <institution> Department of Computer Science, University of New Mexico, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: The Program Dependence Web researchers have recently developed another revised definition of GSA form, which retains the major differences between with thinned GSA form described above <ref> [CKB93] </ref>. 3.6.4 Program Dependence Graphs Ferrante, Ottenstein and Warren introduced the program dependence graph, comprising data dependences and their now-standard formulation of control dependence [FOW87]. Groups at Wisconsin and Rice have developed semantics for PDGs [HPR88, CF89, Sel92].
Reference: [CKPK90] <author> G. Cybenko, L. Kipp, L. Pointer, and D. Kuck. </author> <title> Supercomputer performance evaluation and the Perfect benchmarks. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year> <month> 158 </month>
Reference-contexts: We omitted linpackd from the experiments of later chapters because it is not a genuine application program, and we used another version of track included in the Perfect Club benchmarks <ref> [CKPK90] </ref>. 24 program IP IP % name Lines Procs only +RS Change efie 1254 18 209 232 +10 euler 1113 13 117 138 +15 vortex 540 19 65 87 +25 track 1711 34 191 225 +15 dogleg 4199 48 272 377 +28 linpackd 355 10 28 44 +36 total 9172 142
Reference: [CKT86] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the IR n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference: [Cou81] <author> Patrick Cousot. </author> <title> Semantic foundations of program analysis. </title> <editor> In S. S. Muchnick and M. D. Jones, editors, </editor> <title> Program Flow Analysis: </title> <booktitle> Theory and Applications, </booktitle> <pages> pages 303-342. </pages> <address> Prentice-Hall,New Jersey, </address> <year> 1981. </year>
Reference-contexts: Handling of recursion turns out not to be a major issue. Iterative techniques can guarantee convergence to a fixed point solution using Cousot's technique of widening operators <ref> [CC77, Cou81] </ref>. Li and Yew proposed a preparatory analysis of recursive programs that guarantees termination in three iterations [LY88b, LY88c]. Either of these methods may be adapted for regular sections. 2.6.1 Summary Methods Summary methods describe each kind of side effect using a single subarray descriptor for each array.
Reference: [DBMS79] <author> J. J. Dongarra, J. R. Bunch, C. B. Moler, and G. W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: For these experiments, our candidates for realistic programs are the Linpack library of linear algebra subroutines <ref> [DBMS79] </ref> and the Rice Compiler Evaluation Program Suite.
Reference: [DJ92] <author> Babak Dehbonei and Pierre Jouvelot. </author> <title> Semantical interprocedural analysis by partial symbolic evaluation. </title> <booktitle> In Proceedings of the ACM SIG-PLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: While this is a valid approach for testing the power of regular sections in describing array side effects, better symbolic analysis is needed for analyzing realistic programs. Dehbonei and Jouvelot Dehbonei and Jouvelot describe a partial symbolic evaluation method based on symbolic expressions with guards <ref> [DJ92] </ref>. Each value is represented by a list of guards (logical expressions) followed by a list of symbolic expressions of the same length. <p> Efficient analysis of equality predicates is generally limited to linear expressions, whereas the pattern-matching techniques of Chapter 4 apply to any complete expression | that is, any expression free of hidden, unanalyzed references or side effects. 5.2.1 Predicates on Variables or on Values? Previous research has, with few exceptions <ref> [DJ92] </ref>, generally relied either on symbolic expressions or symbolic predicates, but not both. Predicates have therefore usually been defined on the current contents of variables [CH78, Kar76, IJT91]. This complicates analysis in two major ways. Assignments remove predicates. <p> We hope in this way to gain the benefits both of pattern-matching arbitrary expressions and of manipulating simple linear relations. Prior work has relied either solely on symbolic relations [Iri93] or symbolic expressions [HP93], or else has combined them in one representation <ref> [DJ92] </ref>. Combining our analysis with the first two methods could improve all. We believe that the third method produces a unified representation that is too difficult to manipulate for the exploitation of symbolic facts. <p> We expect significant 145 benefits could be achieve by combining our pattern matching for arbitrary expressions with their linear inequality analysis. Dehbonei Dehbonei and Jouvelot describe a partial symbolic evaluation method for interproce-dural semantic analysis <ref> [DJ92] </ref>. Their symbolic representation of a program value is a pair of symbolic expressions; the first representing the conditions under which the program expression executes and the second giving the value if it executes.
Reference: [DSvH93] <author> Raja Das, Joel Saltz, and Reinhard von Hanxleden. </author> <title> Slicing analysis and indirect accesses to distributed arrays. </title> <editor> In Banerjee et al. </editor> <booktitle> [BGNP93], </booktitle> <pages> pages 152-168. </pages>
Reference-contexts: Run-time analysis involves the insertion of code, perhaps by the compiler, that answers questions about the program's meaning as it runs. Increasing use of run-time techniques, especially for parallelization, create more opportunities for optimization. Examples include data race detection and scheduling of run-time data-dependent loops <ref> [HKMC90, DSvH93] </ref>. * Interactive tools. The analysis used for compilation can also help the authors and maintainers of programs. Analysis and transformation must be cleanly separated; a programmer will be unhappy if the smart editor changes the program, even by just propagating constants, without being asked [KMT91a]. <p> They form the basis for manipulation of non-constant expressions in: * interprocedural constant propagation [GT93] * interprocedural array section analysis [Tsa94] * interprocedural performance estimation * symbolic dependence testing within procedures * optimization of run-time preprocessing <ref> [DSvH93] </ref> The implementation is stable enough to run on most of the well-known Fortran benchmark applications listed in Appendix A. We hope it will serve as a foundation both for further enhancement of symbolic analysis and for its application in program optimization. <p> then add in all terms of variant and compare against the bounds. 4.5.4 Slicing Both the nave value graph (obtained by unioning G GSA DU with the expression forest) and the collapsed value graph (obtained by collapsing copies and congruent nodes in the nave value graph) are useful for slicing <ref> [DSvH93] </ref>. Given a program expression, chasing backwards from its value node will give everything that affects its value. While this will omit the decision whether or not to execute the expression, that omission may be useful in recognizing identical slices that can be combined. <p> Some method for symbolic expression manipulation is needed for dependence testing. Our value numbering techniques fill this need, while also supporting common subexpression elimination and test elision. Complicated code inserted by advanced transformations, such as run-time preprocessing of loops, can also benefit from recognition of redundant computations <ref> [DSvH93] </ref>. Gated single-assignment form does not show many direct benefits in our experiments. New analysis and transformations may exploit its unique properties in the future, however [Wol92]. Both GSA form and value numbering show generally linear behavior in our experiments. <p> More array analysis. The same pattern-matching techniques applied in value numbering scalar computations can be applied to array operations. We already support value numbering of subscripted array references treated as access and update functions <ref> [DSvH93] </ref>.
Reference: [Fie92] <author> John Field. </author> <title> A simple rewriting semantics for realistic imperative programs and its application to program analysis. </title> <booktitle> In Proceedings of the ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 98-107, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Perhaps rewriting systems can be defined for PDGs that have the same power as value numbering, but the practical efficiency of such an approach is uncertain. 3.6.5 Semantics Recent work by John Field gives a formal treatment of graph rewriting on a representation resembling original GSA form <ref> [Fie92] </ref>.
Reference: [FOW87] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: consists of the basic blocks from G CF with a different set of edges. 3 Given a G CF edge e with label L from a branch node B, 3 Another common form, the factored control dependence graph, contains these nodes plus region nodes wherever there are control dependence merges <ref> [BMO90, FOW87] </ref>. 42 procedure build control deps (G CF , G CD ) N CD := N CF foreach edge (v; w) 2 E CF do lab := label of edge y := w ` := INDEPENDENT while (y 6 v) do z := deepest common header (y; v) if (` <p> The Program Dependence Web researchers have recently developed another revised definition of GSA form, which retains the major differences between with thinned GSA form described above [CKB93]. 3.6.4 Program Dependence Graphs Ferrante, Ottenstein and Warren introduced the program dependence graph, comprising data dependences and their now-standard formulation of control dependence <ref> [FOW87] </ref>. Groups at Wisconsin and Rice have developed semantics for PDGs [HPR88, CF89, Sel92]. Selke gives semantics for programs with arrays and arbitrary control flow, and shows how to insert valve nodes efficiently.
Reference: [GKT91] <author> G. Goff, K. Kennedy, and C. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: ParaScope currently lacks an automatic parallel code generator, so we judge our success by comparing ParaScope dependence graphs built with symbolic information against those built without. A dependence tester using our symbolic analysis infrastructure should prove more effective in disproving dependences than the demonstrably successful method in pfc <ref> [GKT91] </ref>. However, the current 8 implementation of dependence analysis in ParaScope is not yet mature enough for a fair comparison of the underlying symbolic analysis systems. Efficiency A significant contribution of this work is a useful symbolic analysis method that is efficient enough to use in a compiler. <p> Our colleagues at Rice have already run several experiments on Riceps. Porterfield modeled cache performance using an adapted version of pfc [Por89]. Goff, Kennedy and Tseng studied the performance of dependence tests on Riceps and other benchmarks <ref> [GKT91] </ref>. Some Riceps and 23 Riceps candidate codes have also been examined in a study on the utility of inline expansion of procedure calls [CHT91]. <p> SSA form to build the value graph made no difference. While the overall reduction in dependence edges is small (just over three percent), for some programs the reduction is over 35 percent. These results are in the same range as those for the symbolic dependence analysis in pfc <ref> [GKT91] </ref>. <p> Unfortunately, ParaScope's dependence testing is still incomplete <ref> [GKT91] </ref>, limiting any comparison of the underlying symbolic analysis. <p> Forward substitution pushes expression information into subscripts, where it is conveniently available to the dependence tester. Some 12 percent of all subscript pairs proven independent and 25 percent of all subscript pairs whose dependence relations are precisely computed are enabled by symbolic analysis <ref> [GKT91] </ref>. Despite its success, this method has not been copied exactly for use in interproce-dural analysis within pfc or in the ParaScope system.
Reference: [GS90] <author> T. Gross and P. Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> Software|Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: In addition, this approach requires an intraprocedural dependence analysis framework capable of using array kill information, such as those described by Rosene [Ros90] and by Gross and Steenkiste <ref> [GS90] </ref>. While the computation of array kills does not necessarily require better symbolic information than that implemented in pfc, kill analysis is particularly sensitive to improvements. Flow-insensitive summaries of mod and ref sections describe elements that may be accessed.
Reference: [GT93] <author> Dan Grove and Linda Torczon. </author> <title> Interprocedural constant propagation: A study of jump function implementation. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 90-99, </pages> <address> Albuqueque, NM, </address> <month> June </month> <year> 1993. </year> <month> 159 </month>
Reference-contexts: Constant propagation replaces variable references and expressions with equivalent constants [WZ91]. Interprocedural versions need ways of representing values that are not constant yet <ref> [CCKT86, GT93] </ref>. In the example, the values of j, k, and n passed to init are constant if the values passed into foo are constant. Test elision removes branches and (unexecutable) dead code when comparisons and boolean expressions can be evaluated [WZ91]. <p> Experimental Implementation We have implemented the central components of our symbolic analysis method in the ParaScope programming environment for Fortran [KMT91b, KMT91a]. They form the basis for manipulation of non-constant expressions in: * interprocedural constant propagation <ref> [GT93] </ref> * interprocedural array section analysis [Tsa94] * interprocedural performance estimation * symbolic dependence testing within procedures * optimization of run-time preprocessing [DSvH93] The implementation is stable enough to run on most of the well-known Fortran benchmark applications listed in Appendix A. <p> All classes of related and referenced variables found contained two or three variables; the former were counted as one linear equality, the latter as three. Our value numbering infrastructure has also been used in other tests of interpro-cedural constant propagation <ref> [GT93] </ref>. 6.8.2 Effect on Dependence Testing Return value analysis alone did not eliminate any dependence edges in the current implementation. <p> However, the direct improvements in dependence information are marginal. Return expression analysis is value for its indirect effects via array section analysis [Tsa94] and constant propagation <ref> [GT93] </ref>. Constant range propagation should be added. <p> Must-alias, kill, use and even live solutions can be approximated much more efficiently than the worst case given by Myers and can have substantial payoff [Mye81, Cal88]. Constant propagation can be very useful, but requires at least an intraprocedural symbolic representation <ref> [CCKT86, GT93] </ref>. Propagation of constant ranges is not quite so simple or effective as that of pure constants, but can have a significant effect in proving loop bounds to be positive.
Reference: [Hag90] <author> Mohammad Reza Haghighat. </author> <title> Symbolic dependence analysis for high performance parallelizing compilers. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1990. </year> <note> Also available as CSRD Rpt. No. 995. </note>
Reference-contexts: Thinned GSA form omits predicates affecting whether or not a program point is reached, leaving only the predicates selecting among possible reaching values. This potentially increases the congruences to be found over Dehbonei and Jouvelot's method. Haghighat Haghighat's symbolic analysis also relies on symbolic expressions <ref> [Hag90, HP90, HP93] </ref>. He has highly developed methods for rewriting expressions and solving recurrences, while our methods seem stronger in representing and matching patterns of operators which are poorly understood (except for being known compositional). <p> Full GSA form could perhaps also be employed in this construction; thinned GSA form omits some of the predicates they are interested in propagating. Haghighat Haghighat's symbolic analysis relies purely on symbolic expressions <ref> [Hag90, HP90, HP93] </ref>. His methods for expression manipulation are more highly developed than 146 ours, while we have more efficient (or at least more explicitly described) techniques for building an initial expression from dataflow information.
Reference: [Hal90] <author> Mary Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: When one source procedure of a compiled program is edited, we need only rebuild that procedure's initial information. The interprocedural analysis is repeated in toto, but recompilation of an individual procedure is only required if the previous compilation relied on interprocedural solutions that have changed <ref> [Tor85, Hal90] </ref>. None of these incremental techniques are included in our implementation, but they will be important considerations in a production system. 9 1.3.2 Organization This dissertation describes the design and implementation of two symbolic analysis systems. Chapter 2 describes an implementation of interprocedural array section analysis. <p> This would also solve Callahan's difficulties Callahan with using the PSG to solve the Live problem. FIAT FIAT is an acronym of Framework for Interprocedural Analysis and Transformation, an interprocedural analysis system descended from the IR n program compiler and available in the ParaScope programming environment <ref> [HMBCR93, Hal90] </ref>. Its main data structure, the annotated call graph is fundamentally a variation of Myers's su-pergraph [Mye81]. However, FIAT supports a much richer set of annotations than Myers proposes, and provides tools for interprocedural analyses and transformations.
Reference: [Har77] <author> W. H. Harrison. </author> <title> Compiler analysis of the value ranges for variables. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-3(3):243-250, </volume> <month> May </month> <year> 1977. </year>
Reference-contexts: Many of the benefits of linear equalities seem to be achievable using only the symbolic expressions of Chapter 4, but a full comparison would require an implementation of Karr's method. 5.6.2 Inequality Predicates Symbolic Bounds Harrison gives ad-hoc techniques for propagating symbolic bounds information, especially for loop induction variables <ref> [Har77] </ref>. While his methods have inspired much later work, including much of our Chapter 2, it is difficult to assess the exact complexity and precision of his techniques. General Linear Inequalities Cousot and Halbwachs give a general framework for handling linear inequalities.
Reference: [Hav93] <author> Paul Havlak. </author> <title> Construction of thinned gated single-assignment form. </title> <editor> In Banerjee et al. </editor> <booktitle> [BGNP93], </booktitle> <pages> pages 477-499. </pages>
Reference-contexts: Ballance et al. introduced the terminology of gated single-assignment form [BMO90]. TGSA form is an extension of high-level SSA form to unstructured code, for which we use the more convenient GSA-form notation <ref> [Hav93] </ref>. Detailed comparisons with the prior versions are given in Section 3.6.
Reference: [HK91] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: This project successfully demonstrated that interprocedural subarray analysis is useful and inexpensive. However, the implementation has several limitations which could be improved by better support for the representation and comparison of symbolic subscripts. Most of these results were previously reported in <ref> [HK91] </ref>. The remainder of this chapter is organized as follows. Section 2.2 describes our framework for representing bounded subarrays. Sections 2.3 and 2.4 describe the construction of initial sections and their propagation, respectively. Section 2.5 examines the performance of regular section analysis on various Fortran applications.
Reference: [HKMC90] <author> R. Hood, K. Kennedy, and J. Mellor-Crummey. </author> <title> Parallel program debugging with on-the-fly anomaly detection. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Run-time analysis involves the insertion of code, perhaps by the compiler, that answers questions about the program's meaning as it runs. Increasing use of run-time techniques, especially for parallelization, create more opportunities for optimization. Examples include data race detection and scheduling of run-time data-dependent loops <ref> [HKMC90, DSvH93] </ref>. * Interactive tools. The analysis used for compilation can also help the authors and maintainers of programs. Analysis and transformation must be cleanly separated; a programmer will be unhappy if the smart editor changes the program, even by just propagating constants, without being asked [KMT91a].
Reference: [HMBCR93] <author> Mary W. Hall, John M. Mellor-Brummey, Alan Carle, and Rene G. Rodr iguez. Fiat: </author> <title> A framework for interprocedural analysis and transformations. </title> <editor> In Banerjee et al. </editor> <booktitle> [BGNP93], </booktitle> <pages> pages 522-545. </pages>
Reference-contexts: How much information should be attached to the call graph node for each procedure? In prior work, this has ranged from a relatively complete representation [Mye81] to one highly tailored for Kill and Use [Cal88]. Concurrently with other researchers at Rice <ref> [HMBCR93] </ref>, we have decided to take a middle road of a lightweight control-flow graph for each procedure, with annotations to the control-flow nodes as needed for each particular problem. <p> This would also solve Callahan's difficulties Callahan with using the PSG to solve the Live problem. FIAT FIAT is an acronym of Framework for Interprocedural Analysis and Transformation, an interprocedural analysis system descended from the IR n program compiler and available in the ParaScope programming environment <ref> [HMBCR93, Hal90] </ref>. Its main data structure, the annotated call graph is fundamentally a variation of Myers's su-pergraph [Mye81]. However, FIAT supports a much richer set of annotations than Myers proposes, and provides tools for interprocedural analyses and transformations.
Reference: [HP90] <author> M. Haghighat and C. Polychronopoulos. </author> <title> Symbolic dependence analysis for high performance parallelizing compilers. </title> <booktitle> In Proceedings of the Third Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Thinned GSA form omits predicates affecting whether or not a program point is reached, leaving only the predicates selecting among possible reaching values. This potentially increases the congruences to be found over Dehbonei and Jouvelot's method. Haghighat Haghighat's symbolic analysis also relies on symbolic expressions <ref> [Hag90, HP90, HP93] </ref>. He has highly developed methods for rewriting expressions and solving recurrences, while our methods seem stronger in representing and matching patterns of operators which are poorly understood (except for being known compositional). <p> Full GSA form could perhaps also be employed in this construction; thinned GSA form omits some of the predicates they are interested in propagating. Haghighat Haghighat's symbolic analysis relies purely on symbolic expressions <ref> [Hag90, HP90, HP93] </ref>. His methods for expression manipulation are more highly developed than 146 ours, while we have more efficient (or at least more explicitly described) techniques for building an initial expression from dataflow information.
Reference: [HP93] <author> Mohammad R. Haghighat and Constantine D. Polychronopoulos. </author> <title> Symbolic analysis: A basis for parallelization, optimization, and scheduling of programs. </title> <editor> In Banerjee et al. </editor> <booktitle> [BGNP93], </booktitle> <pages> pages 567-585. </pages>
Reference-contexts: Thinned GSA form omits predicates affecting whether or not a program point is reached, leaving only the predicates selecting among possible reaching values. This potentially increases the congruences to be found over Dehbonei and Jouvelot's method. Haghighat Haghighat's symbolic analysis also relies on symbolic expressions <ref> [Hag90, HP90, HP93] </ref>. He has highly developed methods for rewriting expressions and solving recurrences, while our methods seem stronger in representing and matching patterns of operators which are poorly understood (except for being known compositional). <p> We hope in this way to gain the benefits both of pattern-matching arbitrary expressions and of manipulating simple linear relations. Prior work has relied either solely on symbolic relations [Iri93] or symbolic expressions <ref> [HP93] </ref>, or else has combined them in one representation [DJ92]. Combining our analysis with the first two methods could improve all. We believe that the third method produces a unified representation that is too difficult to manipulate for the exploitation of symbolic facts. <p> Full GSA form could perhaps also be employed in this construction; thinned GSA form omits some of the predicates they are interested in propagating. Haghighat Haghighat's symbolic analysis relies purely on symbolic expressions <ref> [Hag90, HP90, HP93] </ref>. His methods for expression manipulation are more highly developed than 146 ours, while we have more efficient (or at least more explicitly described) techniques for building an initial expression from dataflow information.
Reference: [HPR88] <author> S. Horwitz, J. Prins, and T. Reps. </author> <title> On the adequacy of program dependence graphs for representing programs. </title> <booktitle> In Conference Record of the Fifteenth ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 146-157, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: Groups at Wisconsin and Rice have developed semantics for PDGs <ref> [HPR88, CF89, Sel92] </ref>. Selke gives semantics for programs with arrays and arbitrary control flow, and shows how to insert valve nodes efficiently. Despite the greater maturity of PDGs as a program representation, they are inferior to TGSA form for value numbering.
Reference: [IJT91] <author> F. Irigoin, P. Jouvelot, and R. Triolet. </author> <title> Semantical interprocedural par-allelization: An overview of the PIPS project. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year> <month> 160 </month>
Reference-contexts: Predicates have therefore usually been defined on the current contents of variables <ref> [CH78, Kar76, IJT91] </ref>. This complicates analysis in two major ways. Assignments remove predicates. When a variable is assigned a new value (unrelated to its old one), then it must be removed from the predicate set for that point. <p> The implementors of the PIPS system use a similar general framework, but specialize it to common cases and claim to achieve much greater efficiency in practice <ref> [IJT91, Iri93] </ref>. Simple Sections The simple section framework of Balasundaram's Data Access Descriptors was developed to represent predicates on subscripts [BK89, Bal89]. It can also be used to represent predicates on other values (although we are unaware of prior work in extending it thus). <p> Precise recompilation analysis on symbolic expressions for passed values is very difficult; minor changes can produce new expressions without conveying any worthwhile information. PIPS The PIPS system provides interprocedural analysis of side effects and symbolic predicates <ref> [IJT91, Iri93] </ref>. The symbolic information is exploited in several ways, including analysis of side effects to array regions (based on a previous implementation by Triolet [TIF86, Tri85]). Relying on predicates as the main form of symbolic information can have significant disadvantages.
Reference: [Iri93] <author> Francois Irigoin. </author> <title> Interprocedural analyses for programming environments. </title> <editor> In J. J. Dongarra and B. Tourancheau, editors, </editor> <booktitle> Environments and Tools for Parallel Scientific Computing. </booktitle> <publisher> Elsevier Science Publishers, </publisher> <year> 1993. </year>
Reference-contexts: The region method can maintain arbitrary convex array accesses precisely, but if the important array accesses prove to be regular sections, the additional complexity is wasted. Recently, researchers on the PIPS project have extended the region method to handle common cases more efficiently <ref> [Iri93] </ref>. 2.6.2 Reference Lists Some proposed methods do not summarize, but represent each reference separately. Descriptors are then lists of references, the meet operation is list concatenation (possibly with a check for duplicates), and translation and intersection are just the repeated 30 application of the corresponding operations on simple references. <p> The implementors of the PIPS system use a similar general framework, but specialize it to common cases and claim to achieve much greater efficiency in practice <ref> [IJT91, Iri93] </ref>. Simple Sections The simple section framework of Balasundaram's Data Access Descriptors was developed to represent predicates on subscripts [BK89, Bal89]. It can also be used to represent predicates on other values (although we are unaware of prior work in extending it thus). <p> We hope in this way to gain the benefits both of pattern-matching arbitrary expressions and of manipulating simple linear relations. Prior work has relied either solely on symbolic relations <ref> [Iri93] </ref> or symbolic expressions [HP93], or else has combined them in one representation [DJ92]. Combining our analysis with the first two methods could improve all. We believe that the third method produces a unified representation that is too difficult to manipulate for the exploitation of symbolic facts. <p> Precise recompilation analysis on symbolic expressions for passed values is very difficult; minor changes can produce new expressions without conveying any worthwhile information. PIPS The PIPS system provides interprocedural analysis of side effects and symbolic predicates <ref> [IJT91, Iri93] </ref>. The symbolic information is exploited in several ways, including analysis of side effects to array regions (based on a previous implementation by Triolet [TIF86, Tri85]). Relying on predicates as the main form of symbolic information can have significant disadvantages. <p> The worst case for exhaustive propagation of relational predicates ranges from cubic for pairwise linear equalities to exponential for general linear inequalities. While these seem too time-consuming for use in a compiler, some implementors have had good experience with propagating such information only on demand of dependence testing <ref> [Iri93] </ref>. Limiting analysis to cases where we know (from preliminary checking) that it may be useful will be increasingly important to producing powerful and fast compilers.
Reference: [Kar76] <author> M. Karr. </author> <title> Affine relationships among variables of a program. </title> <journal> Acta In-formatica, </journal> <volume> 6 </volume> <pages> 133-151, </pages> <year> 1976. </year>
Reference-contexts: While there are many published algorithms for performing symbolic analysis and global value numbering <ref> [Kar76, RT81, RWZ88] </ref>, their preliminary transformations and complexity make them difficult to integrate into pfc. Our implementation builds on pfc's existing dataflow analysis machinery to represent symbolic expressions by global value numbers. Leaf value numbers denote constants and the global and parameter values available on procedure entry. <p> Predicates have therefore usually been defined on the current contents of variables <ref> [CH78, Kar76, IJT91] </ref>. This complicates analysis in two major ways. Assignments remove predicates. When a variable is assigned a new value (unrelated to its old one), then it must be removed from the predicate set for that point. <p> The allowable values of the variables define an affine space, represented as a V fi V matrix of simultaneous equations, where V is the number of variables. <ref> [Kar76] </ref>. Conjoining affine spaces corresponds to our composition operation. Conjunction involves, in the worst case, putting one V fi V matrix under another and using row operations to convert to normal form. This operation takes O (V 2 ) time. Unioning affine spaces corresponds to our merge operation.
Reference: [Ken81] <author> K. Kennedy. </author> <title> A survey of data flow analysis techniques. </title> <editor> In S. S. Much-nick and M. D. Jones, editors, </editor> <title> Program Flow Analysis: </title> <booktitle> Theory and Applications, </booktitle> <pages> pages 1-54. </pages> <address> Prentice-Hall,New Jersey, </address> <year> 1981. </year>
Reference-contexts: Because of the 93 interprocedural nature of the analysis, we could not use AST expressions. Therefore, we implemented a value numbering method much like the one presented here, with the following differences: * It uses simple def-use chains <ref> [Ken81] </ref> instead of GSA form. If multiple definitions, with different value numbers, reach a use, then approximate bounds on the value are built. * The handling of potential interference from interprocedural mod is more primitive.
Reference: [KMT91a] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: The analysis used for compilation can also help the authors and maintainers of programs. Analysis and transformation must be cleanly separated; a programmer will be unhappy if the smart editor changes the program, even by just propagating constants, without being asked <ref> [KMT91a] </ref>. Larger-scale transformations Continually improving processor technology has made it harder to build memories with high enough bandwidth and low enough latency to keep the processor supplied with data. Computer architects address this problem in several ways. <p> Experimental Implementation We have implemented the central components of our symbolic analysis method in the ParaScope programming environment for Fortran <ref> [KMT91b, KMT91a] </ref>.
Reference: [KMT91b] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Interactive parallel programming using the ParaScope Editor. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 329-341, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Experimental Implementation We have implemented the central components of our symbolic analysis method in the ParaScope programming environment for Fortran <ref> [KMT91b, KMT91a] </ref>. <p> This chapter describes our adaptation of regular sections and the design and implementation of array side effect analysis in the Rice Parallel Fortran 11 Converter (pfc) [AK84], an automatic parallelization system that can also export dependence information to the ParaScope programming environment <ref> [KMT91b] </ref>. The overriding concern in the implementation is that it be effective and efficient enough to be incorporated in a practical compilation system. Algorithm 2.1 summarizes the steps of the analysis, which is integrated with the three-phase interprocedural analysis and optimization structure of pfc [ACK86, CCKT86].
Reference: [Knu73a] <author> Donald E. Knuth. </author> <booktitle> The Art of Computer Science Vol. 1: Fundamental Algorithms. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1973. </year>
Reference-contexts: When we encounter a non-root variable x, we derive from the relationship with its root p a point (p 1 ; x 1 ) that satisfies the relation. (The Euclidean greatest-common-divisor algorithm is given in <ref> [Knu73a] </ref>.) We then invent a value i, presumed to range over all the integers, and write p and x in terms of i so that each value of i corresponds to a (p; x) point satisfying their relation.
Reference: [Knu73b] <author> Donald E. Knuth. </author> <title> The Art of Computer Science Vol. 3: Searching and Sorting. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1973. </year>
Reference-contexts: construction of the GSA-form dataflow graph appears linear for all but a few outlying procedures. 87 It takes time proportional to the size of the keys to evaluate the hash function and, assuming a uniform and sufficiently pseudo-random hash distribution, constant time to perform the lookup in the hash table <ref> [Knu73b] </ref>. In numbering the whole value graph (assuming that we do no rewriting), we hash the operator name and input value numbers of each node in topological order, adding those which are not yet present in the table.
Reference: [Li90] <author> Zhiyuan Li. </author> <title> Private communication, </title> <month> October </month> <year> 1990. </year>
Reference-contexts: Starred entries (?) indicate parallel calls which were precisely summarized by regular section analysis, but which were not detected as parallel due to a deficiency in pfc's symbolic dependence test for triangular loops. One call in QRDC was mistakenly parallelized by Parafrase <ref> [Li90] </ref>. These results indicate, at least for Linpack, that there is no benefit to the generality of Triolet's and Li and Yew's methods. Regular section analysis obtains exactly the same precision, with a different number of loops parallelized only because of differences in dependence analysis and transformations.
Reference: [LM94] <author> Jon Loeliger and Robert Metzger. </author> <title> Developing an interprocedural optimizing compiler. </title> <journal> SIGPLAN Notices, </journal> <volume> 29(4) </volume> <pages> 41-48, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Banning's work has inspired linear-time algorithms for and commercial implementations of flow-insensitive interprocedural analysis <ref> [CK88b, LM94] </ref>. While his approximate solutions for flow-sensitive problems are a valuable starting point, more precise methods may be profitable today. Myers Myers addresses flow-sensitive problems by fusing the internal control flow graphs of each procedure into one program supergraph [Mye81].
Reference: [LT79] <author> T. Lengauer and R. E. Tarjan. </author> <title> A fast algorithm for finding dominators in a flowgraph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 1 </volume> <pages> 121-141, </pages> <year> 1979. </year>
Reference-contexts: Loops are assumed reducible, so that all cycles pass through a unique header. (In the implementation, irreducible loops are conservatively left alone.) We build tree representations of loop nesting and of the pre- and post-dominator relations <ref> [LT79, Tar74] </ref>. By saving a preorder numbering of each tree, we can test for transitive as well as immediate loop nesting and dominance in constant time. We augment loops, as shown in Figure 3.4, to provide suitable places for the placement of and nodes. <p> While our methods may take exponential time and space for contrived examples, they take linear time for programs satisfying loose and reasonable structural requirements. Auxiliary Analyses and Data Structures. Efficient methods exist for computing each of pre- and post-, Tarjan intervals and control dependences <ref> [LT79, Tar74, CFR + 91, CFS90a] </ref>. While the asymptotic time bounds range from almost linear to quadratic, for practical purposes, these methods are linear in the number of G CF edges (E CF ). <p> 7 These measurements were taken inside the ParaScope programming environment, optimized with gcc version 2.4.5, running on a Sun MicroSystems Sparc 10 with 64 Mbytes of memory. 62 63 64 3.6 Related Work 3.6.1 Groundwork Control-flow graphs, dominator trees, control-dependence graphs and SSA form are covered extensively in the literature <ref> [ASU86, LT79, CFR + 91] </ref>. However, while construction of loop-nesting trees is well-known tool, it is seldom documented. We use the algorithm of Tarjan's algorithm for testing reducibility [Tar74] extended to recognizing irreducible loops in a way that isolates the problems they cause.
Reference: [LY88a] <author> Z. Li and P.-C. Yew. </author> <title> Efficient interprocedural analysis for program parallelization and restructuring. </title> <booktitle> In ACM SIGPLAN PPEALS, </booktitle> <pages> pages 85-99, </pages> <year> 1988. </year>
Reference-contexts: Both Li and Yew <ref> [LY88a] </ref> and Triolet [Tri85] found several parallel calls in Linpack using their implementations in the University of Illinois translator, Parafrase. Linpack proves that useful numerical codes can be written in the modular programming style for which parallel calls can be detected. <p> Absolute timings for the array side effect analyses implemented in Parafrase by Triolet and by Li and Yew, have not been published, though Li and Yew do state that their method runs 2.6 times faster than Triolet's <ref> [LY88a] </ref>. <p> are collapsed to conserve space. 26 the 13 Perfect benchmarks indicate a reduction of 0.6 percent in the total size of the dependence graphs. 6 Parallelized Calls Table 2.3 examines the number of calls in Linpack which were parallelized after summary interprocedural analysis alone ("IP"), after Li and Yew's analysis <ref> [LY88a] </ref>, and after regular section analysis ("RS"). (Triolet's results from Parafrase resembled Li and Yew's.) Most (17) of these call sites were parallelized in ParaScope, based on pfc's dependence graph, with no transformations being necessary. <p> Linearization in its pure form is ill-suited to summarization, but might be a useful fallback for a true summary technique because of its ability to handle arbitrary reshapes. Atom Images Li and Yew extended Parafrase to compute sets of atom images describing the side effects of procedures <ref> [LY88a, LY88b] </ref>. Like the original version of regular sections described in Callahan's thesis [Cal87], these record subscript expressions that are linear in loop induction variables along with bounds on the induction variables.
Reference: [LY88b] <author> Z. Li and P.-C. Yew. </author> <title> Interprocedural analysis and program restructuring for parallel programs. </title> <journal> CSRD Rpt. </journal> <volume> No. </volume> <pages> 720, </pages> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: However, the proposed Fortran 90 standard allows recursion [X3J89], so it must be handled someday. Unfortunately, a straightforward iterative propagation of regular sections will not terminate, since the lattice has unbounded depth. Li and Yew <ref> [LY88b] </ref> and Cooper and Kennedy [CK88b] describe approaches for propagating subarrays that are efficient regardless of the depth of the lattice. However, it may be more convenient to implement a simple iterative technique while simulating a bounded-depth lattice. <p> Handling of recursion turns out not to be a major issue. Iterative techniques can guarantee convergence to a fixed point solution using Cousot's technique of widening operators [CC77, Cou81]. Li and Yew proposed a preparatory analysis of recursive programs that guarantees termination in three iterations <ref> [LY88b, LY88c] </ref>. Either of these methods may be adapted for regular sections. 2.6.1 Summary Methods Summary methods describe each kind of side effect using a single subarray descriptor for each array. They include the many variations of regular sections developed at Rice, described separately below. <p> Linearization in its pure form is ill-suited to summarization, but might be a useful fallback for a true summary technique because of its ability to handle arbitrary reshapes. Atom Images Li and Yew extended Parafrase to compute sets of atom images describing the side effects of procedures <ref> [LY88a, LY88b] </ref>. Like the original version of regular sections described in Callahan's thesis [Cal87], these record subscript expressions that are linear in loop induction variables along with bounds on the induction variables.
Reference: [LY88c] <author> Z. Li and P.-C. Yew. </author> <title> Program parallelization with interprocedural analysis. </title> <journal> The Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 225-244, </pages> <year> 1988. </year> <month> 161 </month>
Reference-contexts: Handling of recursion turns out not to be a major issue. Iterative techniques can guarantee convergence to a fixed point solution using Cousot's technique of widening operators [CC77, Cou81]. Li and Yew proposed a preparatory analysis of recursive programs that guarantees termination in three iterations <ref> [LY88b, LY88c] </ref>. Either of these methods may be adapted for regular sections. 2.6.1 Summary Methods Summary methods describe each kind of side effect using a single subarray descriptor for each array. They include the many variations of regular sections developed at Rice, described separately below. <p> The restricted section lattice has the finite descending chain property, which we originally thought necessary for efficient handling of recursive programs. However, we can obtain fixed-point solutions on more complicated lattices by applying the techniques of Cousot or of Li and Yew <ref> [CC77, LY88c] </ref> Bounded Regular Sections Anticipating that restricted regular sections would not be precise enough for effective parallelization, Callahan and Kennedy proposed an implementation of regular sections 32 with bounds. That proposal led to our development of the bounded regular section framework in this chapter.
Reference: [Mas92] <author> Vadim Maslov. Delinearization: </author> <title> An efficient way to break multiloop dependence equations. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 152-161, </pages> <address> San Francisco, </address> <month> June </month> <year> 1992. </year>
Reference: [Mye81] <author> E. Myers. </author> <title> A precise interprocedural data flow algorithm. </title> <booktitle> In Conference Record of the Eighth ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 219-230, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: These and other flow-sensitive problems rely on some control-flow information. How much information should be attached to the call graph node for each procedure? In prior work, this has ranged from a relatively complete representation <ref> [Mye81] </ref> to one highly tailored for Kill and Use [Cal88]. Concurrently with other researchers at Rice [HMBCR93], we have decided to take a middle road of a lightweight control-flow graph for each procedure, with annotations to the control-flow nodes as needed for each particular problem. <p> While his approximate solutions for flow-sensitive problems are a valuable starting point, more precise methods may be profitable today. Myers Myers addresses flow-sensitive problems by fusing the internal control flow graphs of each procedure into one program supergraph <ref> [Mye81] </ref>. There is no essential difference between the supergraph and our annotated call graph. Myers refers to our call and return nodes as cpoint and rpoint, respectively, and does not explicitly support as rich a set of annotations. <p> FIAT FIAT is an acronym of Framework for Interprocedural Analysis and Transformation, an interprocedural analysis system descended from the IR n program compiler and available in the ParaScope programming environment [HMBCR93, Hal90]. Its main data structure, the annotated call graph is fundamentally a variation of Myers's su-pergraph <ref> [Mye81] </ref>. However, FIAT supports a much richer set of annotations than Myers proposes, and provides tools for interprocedural analyses and transformations. Our scheme for symbolic and flow-sensitive analysis has been developed contemporaneously and in collaborative discussion with the FIAT authors. <p> Basic alias and flow-sensitive side-effect solutions are prerequisites to more extensive analysis. Must-alias, kill, use and even live solutions can be approximated much more efficiently than the worst case given by Myers and can have substantial payoff <ref> [Mye81, Cal88] </ref>. Constant propagation can be very useful, but requires at least an intraprocedural symbolic representation [CCKT86, GT93]. Propagation of constant ranges is not quite so simple or effective as that of pure constants, but can have a significant effect in proving loop bounds to be positive.
Reference: [Por89] <author> Allan Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year> <note> Available as Rice COMP TR88-93. </note>
Reference-contexts: RiCEPS The Rice Compiler Evaluation Program Suite is a collection of complete applications codes from a broad range of scientific disciplines. Our colleagues at Rice have already run several experiments on Riceps. Porterfield modeled cache performance using an adapted version of pfc <ref> [Por89] </ref>. Goff, Kennedy and Tseng studied the performance of dependence tests on Riceps and other benchmarks [GKT91]. Some Riceps and 23 Riceps candidate codes have also been examined in a study on the utility of inline expansion of procedure calls [CHT91].
Reference: [Pug91] <author> William Pugh. </author> <title> The Omega Test: a fast and practical integer programming algorithm for dependence analysis. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: More sophisticated predicates. The value of affine inequalities on arbitrary numbers of variables (or value numbers) should be investigated. Efficient implementations exist for composing inequalities into a convex set <ref> [Pug91] </ref>. However, merging such sets appears to be more difficult [CH78]. The best approach may be to have a backwards pass of analysis that determines which values are particularly interesting, then restrict the forward analysis to compute predicates only on the interesting values (like goal-directed cloning) [BCHT90]. More array analysis.
Reference: [RL86] <author> John H. Reif and Harry R. Lewis. </author> <title> Efficient symbolic analysis of programs. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 32(3) </volume> <pages> 280-313, </pages> <year> 1986. </year>
Reference-contexts: To be conservative, we must assume that -assignments at different program points have independent values [AWZ88]. SSA form was introduced by researchers at IBM and at Brown University [RWZ88, AWZ88]. The -assignments are direct descendants of Reif et al.'s birthpoints, used to refine def-use chains for symbolic analysis <ref> [RT81, RL86] </ref>. The construction of SSA form makes it a good basis for a dataflow graph. Pseudo-assignments of the form v := (v 1 ; ; v n ) are inserted so that exactly one definition of a variable v reaches any non- use of v.
Reference: [Ros90] <author> C. M. Rosene. </author> <title> Incremental Dependence Analysis. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> March </month> <year> 1990. </year> <note> Available as Rice COMP TR90-112. </note>
Reference-contexts: In addition, this approach requires an intraprocedural dependence analysis framework capable of using array kill information, such as those described by Rosene <ref> [Ros90] </ref> and by Gross and Steenkiste [GS90]. While the computation of array kills does not necessarily require better symbolic information than that implemented in pfc, kill analysis is particularly sensitive to improvements. Flow-insensitive summaries of mod and ref sections describe elements that may be accessed.
Reference: [RT81] <author> J. H. Reif and R. E. Tarjan. </author> <title> Symbolic program analysis in almost-linear time. </title> <journal> SIAM Journal on Computing, </journal> <volume> 11(1) </volume> <pages> 81-93, </pages> <month> February </month> <year> 1981. </year>
Reference-contexts: While there are many published algorithms for performing symbolic analysis and global value numbering <ref> [Kar76, RT81, RWZ88] </ref>, their preliminary transformations and complexity make them difficult to integrate into pfc. Our implementation builds on pfc's existing dataflow analysis machinery to represent symbolic expressions by global value numbers. Leaf value numbers denote constants and the global and parameter values available on procedure entry. <p> To be conservative, we must assume that -assignments at different program points have independent values [AWZ88]. SSA form was introduced by researchers at IBM and at Brown University [RWZ88, AWZ88]. The -assignments are direct descendants of Reif et al.'s birthpoints, used to refine def-use chains for symbolic analysis <ref> [RT81, RL86] </ref>. The construction of SSA form makes it a good basis for a dataflow graph. Pseudo-assignments of the form v := (v 1 ; ; v n ) are inserted so that exactly one definition of a variable v reaches any non- use of v. <p> Where values are merged, the shu*e operator combines the two lists. Because shu*e can be expensive, they relax their method to leave shu*e unevaluated. They also use a value graph to represent symbolic expressions; because, like SSA form, it descends from the work of Reif et al. <ref> [RT81] </ref>, it probably bears some resemblance to our value graph. Unevaluated shu*e operators are closely related to the dags of fl operators built in full GSA form. They both combine some of the conditions under which an expression executes with symbolic representations of the possible values. <p> Combining control conditions in the symbolic value makes identical values in different control contexts appear dif ferent. Dehbonei and Jouvelot build their symbolic values from a value graph that sounds like a relative of SSA form, being also derived from the earlier work of Reif et al. <ref> [RT81] </ref>. Full GSA form could perhaps also be employed in this construction; thinned GSA form omits some of the predicates they are interested in propagating. Haghighat Haghighat's symbolic analysis relies purely on symbolic expressions [Hag90, HP90, HP93].
Reference: [RWZ88] <author> B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> Global value numbers and redundant computations. </title> <booktitle> In Proceedings of the Fifteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 12-27, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: While there are many published algorithms for performing symbolic analysis and global value numbering <ref> [Kar76, RT81, RWZ88] </ref>, their preliminary transformations and complexity make them difficult to integrate into pfc. Our implementation builds on pfc's existing dataflow analysis machinery to represent symbolic expressions by global value numbers. Leaf value numbers denote constants and the global and parameter values available on procedure entry. <p> To be conservative, we must assume that -assignments at different program points have independent values [AWZ88]. SSA form was introduced by researchers at IBM and at Brown University <ref> [RWZ88, AWZ88] </ref>. The -assignments are direct descendants of Reif et al.'s birthpoints, used to refine def-use chains for symbolic analysis [RT81, RL86]. The construction of SSA form makes it a good basis for a dataflow graph. <p> The saved result can then be used in place of the other occurrences. SSA Form Global common subexpression elimination (over whole procedures) was one motivation in the development of static single-assignment form by Wegman, Zadeck, and others <ref> [AWZ88, RWZ88] </ref>. Their work inspired our use of a value graph to represent symbolic expressions. However, the method used for detecting congruence requires the whole value graph to be built before any congruences are detected.
Reference: [Sel92] <author> Rebecca P. Selke. </author> <title> A Semantic Framework for Program Dependence. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <year> 1992. </year>
Reference-contexts: Groups at Wisconsin and Rice have developed semantics for PDGs <ref> [HPR88, CF89, Sel92] </ref>. Selke gives semantics for programs with arrays and arbitrary control flow, and shows how to insert valve nodes efficiently. Despite the greater maturity of PDGs as a program representation, they are inferior to TGSA form for value numbering.
Reference: [Tar74] <author> R. E. Tarjan. </author> <title> Testing flow graph reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 9 </volume> <pages> 355-365, </pages> <year> 1974. </year>
Reference-contexts: Algorithm 3.1 builds a loop-nesting tree for G CF . It is essentially Tarjan's algorithm for recognizing flow-graph reducibility, except that it does not fail for irreducible graphs <ref> [Tar74] </ref>. Reducible graphs are those in which each nested strongly connected region has a unique entry node, the loop header, that pre-dominates all other nodes in the SCR. 2 We adapt Tarjan's method to recognize each irreducible loop and arbitrarily select one of its multiple entry nodes as the header. <p> Loops are assumed reducible, so that all cycles pass through a unique header. (In the implementation, irreducible loops are conservatively left alone.) We build tree representations of loop nesting and of the pre- and post-dominator relations <ref> [LT79, Tar74] </ref>. By saving a preorder numbering of each tree, we can test for transitive as well as immediate loop nesting and dominance in constant time. We augment loops, as shown in Figure 3.4, to provide suitable places for the placement of and nodes. <p> While our methods may take exponential time and space for contrived examples, they take linear time for programs satisfying loose and reasonable structural requirements. Auxiliary Analyses and Data Structures. Efficient methods exist for computing each of pre- and post-, Tarjan intervals and control dependences <ref> [LT79, Tar74, CFR + 91, CFS90a] </ref>. While the asymptotic time bounds range from almost linear to quadratic, for practical purposes, these methods are linear in the number of G CF edges (E CF ). <p> However, while construction of loop-nesting trees is well-known tool, it is seldom documented. We use the algorithm of Tarjan's algorithm for testing reducibility <ref> [Tar74] </ref> extended to recognizing irreducible loops in a way that isolates the problems they cause. Whether a particular control dependence is loop-independent or loop-carried may seem obvious in sufficiently structured code.
Reference: [Tar83] <author> Robert E. Tarjan. </author> <title> Data Structures and Network Algorithms. </title> <journal> Society for Industrial and Applied Mathematics (SIAM), </journal> <volume> Philadel-phia,Pennsylvania, </volume> <year> 1983. </year>
Reference-contexts: The modified version of S produced represents (R ^ S). Addition of a relation to S can union two previously unrelated classes of variables; this takes almost linear time using fast union-find data structures <ref> [Tar83] </ref>. Composition can therefore be done in time almost linear in the number of variables involved. If C (R) is the number of classes in the set of pairwise linear equalities R, then C (R ^ S) min (C (R); C (S)).
Reference: [TIF86] <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of CALL statements. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 176-185, </pages> <address> Palo Alto, CA, </address> <month> July </month> <year> 1986. </year> <month> 162 </month>
Reference-contexts: Convex Regions Triolet, Irigoin and Feautrier proposed to calculate linear inequalities bounding the set of array locations affected by a procedure call <ref> [TIF86, Tri85] </ref>. This representation and its intersection operation are precise for convex regions of access. Other patterns, such as array accesses with non-unit stride and non-convex results of meet operations, are given convex approximations. <p> PIPS The PIPS system provides interprocedural analysis of side effects and symbolic predicates [IJT91, Iri93]. The symbolic information is exploited in several ways, including analysis of side effects to array regions (based on a previous implementation by Triolet <ref> [TIF86, Tri85] </ref>). Relying on predicates as the main form of symbolic information can have significant disadvantages. The simplest form of predicates to manipulate, linear equalities and inequalities, miss many interesting cases, such as the equality of identical nonlinear expressions.
Reference: [Tor85] <author> L. Torczon. </author> <title> Compilation Dependences in an Ambitious Optimizing Compiler. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1985. </year>
Reference-contexts: When one source procedure of a compiled program is edited, we need only rebuild that procedure's initial information. The interprocedural analysis is repeated in toto, but recompilation of an individual procedure is only required if the previous compilation relied on interprocedural solutions that have changed <ref> [Tor85, Hal90] </ref>. None of these incremental techniques are included in our implementation, but they will be important considerations in a production system. 9 1.3.2 Organization This dissertation describes the design and implementation of two symbolic analysis systems. Chapter 2 describes an implementation of interprocedural array section analysis. <p> During the interprocedural phase, after producing the classical scalar side effect solution, but before propagating regular sections, we check to see if CLOBBER may change M. If so, we change S1's array side effect to A (?). A similar technique has proven successful for interprocedural constant propagation in pfc <ref> [Tor85, CCKT86] </ref>. 2 Another pass is needed to examine call sites for building the call graph, but this can be combined with the initial information gathering.
Reference: [TP93] <author> Peng Tu and David Padua. </author> <title> Automatic array privatization. </title> <editor> In Banerjee et al. </editor> <booktitle> [BGNP93], </booktitle> <pages> pages 500-521. </pages>
Reference-contexts: For arrays, improved subscript analysis should give better dependence information, both by traditional methods and indirectly by improving array section information needed for privatization <ref> [TP93] </ref>. For conditional branches, the tests may sometimes be evaluated symbolically, allowing the deletion of unexecuted code. While the direct speedups from reduced code size and saving the test would be small, indirect benefits can be large due to reduced uncertainty.
Reference: [Tri85] <author> R. Triolet. </author> <title> Interprocedural analysis for program restructuring with Parafrase. </title> <journal> CSRD Rpt. </journal> <volume> No. </volume> <pages> 538, </pages> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1985. </year>
Reference-contexts: Both Li and Yew [LY88a] and Triolet <ref> [Tri85] </ref> found several parallel calls in Linpack using their implementations in the University of Illinois translator, Parafrase. Linpack proves that useful numerical codes can be written in the modular programming style for which parallel calls can be detected. However, Linpack is a set of library routines, not a complete program. <p> Convex Regions Triolet, Irigoin and Feautrier proposed to calculate linear inequalities bounding the set of array locations affected by a procedure call <ref> [TIF86, Tri85] </ref>. This representation and its intersection operation are precise for convex regions of access. Other patterns, such as array accesses with non-unit stride and non-convex results of meet operations, are given convex approximations. <p> PIPS The PIPS system provides interprocedural analysis of side effects and symbolic predicates [IJT91, Iri93]. The symbolic information is exploited in several ways, including analysis of side effects to array regions (based on a previous implementation by Triolet <ref> [TIF86, Tri85] </ref>). Relying on predicates as the main form of symbolic information can have significant disadvantages. The simplest form of predicates to manipulate, linear equalities and inequalities, miss many interesting cases, such as the equality of identical nonlinear expressions.
Reference: [Tsa94] <author> Hariklia Tsalapatas. </author> <title> Interprocedural array side effect analysis. </title> <type> Master's thesis, </type> <institution> Rice University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Experimental Implementation We have implemented the central components of our symbolic analysis method in the ParaScope programming environment for Fortran [KMT91b, KMT91a]. They form the basis for manipulation of non-constant expressions in: * interprocedural constant propagation [GT93] * interprocedural array section analysis <ref> [Tsa94] </ref> * interprocedural performance estimation * symbolic dependence testing within procedures * optimization of run-time preprocessing [DSvH93] The implementation is stable enough to run on most of the well-known Fortran benchmark applications listed in Appendix A. <p> The bounded sections implemented here are both less expensive and less precise than DADs. Tsalapatas subsequently extended our methods to a DAD implementation in ParaScope, but no empirical comparison of the relative benefits of regular sections vs. DADs has been completed <ref> [Tsa94] </ref>. 2.7 Lessons More experiments are required to fully evaluate the performance of regular section analysis on complete applications and find new areas for improvement. Based on the studies conducted so far, extensions to provide better handling of conditionals and 33 flow-sensitive side effects seem promising. <p> Based on the studies conducted so far, extensions to provide better handling of conditionals and 33 flow-sensitive side effects seem promising. Both extensions require better support from the symbolic analysis infrastructure. Both have now been implemented, to differing degrees, by Tsalapatas <ref> [Tsa94] </ref>. 2.7.1 Conditional Symbolic Analysis Consider the following example, derived from the blas: SUBROUTINE D (N, DA, DX, INCX) DOUBLE PRECISION DA, DX (*) IF (INCX .LT. 0) THEN IX = (-N+1)*INCX + 1 ELSE IX = 1 ENDIF DO I = 1, N IX = IX + INCX ENDDO RETURN <p> However, the direct improvements in dependence information are marginal. Return expression analysis is value for its indirect effects via array section analysis <ref> [Tsa94] </ref> and constant propagation [GT93]. Constant range propagation should be added.
Reference: [Uni89] <author> J. Uniejewski. </author> <title> SPEC Benchmark Suite: designed for today's advanced systems. </title> <journal> SPEC Newsletter Volume 1, </journal> <note> Issue 1, SPEC, Fall 1989. </note>
Reference: [Wol92] <author> Michael Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> In Proceedings of the SIG-PLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 162-174, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Our implementation uses Algorithm 4.2 to recognize auxiliary induction variables varying at one loop level. For nested loops, we should use analytical techniques for identifying induction variables <ref> [Wol92] </ref>. Induction Variables An auxiliary induction variable's value will be an invariant iterative function of the placeholder; e.g., (M + 1). When this function is an invariant increment, this can be 84 normalized to a linear expression of the loop iteration count. <p> Complicated code inserted by advanced transformations, such as run-time preprocessing of loops, can also benefit from recognition of redundant computations [DSvH93]. Gated single-assignment form does not show many direct benefits in our experiments. New analysis and transformations may exploit its unique properties in the future, however <ref> [Wol92] </ref>. Both GSA form and value numbering show generally linear behavior in our experiments. However, the worst-case bounds are decidedly nonlinear, as borne out by a few procedures with tangled control flow.
Reference: [WZ91] <author> Mark N. Wegman and F. Kenneth Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <year> 1991. </year>
Reference-contexts: If j and k can be proven equivalent, then the two subscript expressions are also the same and their value need only be computed once per iteration. Constant propagation replaces variable references and expressions with equivalent constants <ref> [WZ91] </ref>. Interprocedural versions need ways of representing values that are not constant yet [CCKT86, GT93]. In the example, the values of j, k, and n passed to init are constant if the values passed into foo are constant. <p> In the example, the values of j, k, and n passed to init are constant if the values passed into foo are constant. Test elision removes branches and (unexecutable) dead code when comparisons and boolean expressions can be evaluated <ref> [WZ91] </ref>. If we have (n == 1) on entry to the do loop, then it only executes once and can be rewritten as straight-line code. If we have (n &lt; 1), then the loop does not execute at all and may be deleted. <p> Static single-assignment (SSA) form enables efficient construction of program data-flow information [CFR + 91]. From constant propagation to register allocation, SSA-based algorithms have proven more powerful and efficient than those based on traditional def-use edges <ref> [WZ91, BCT92] </ref>. A distinctive feature of SSA form is the placement of a minimal number of pseudo-assignments at program merge points so that no statement, except a pseudo-assignment, is reached by multiple definitions. These pseudo-assignments use - functions to select which of the merged definitions flows to successive uses. <p> v 1 := 1 if (P ) then v 2 := 2 v 3 := (v 2 ; v 1 ) Note that both v 1 and v 2 may both execute; we must know not only which nodes but which edges execute in order to select the correct input <ref> [WZ91] </ref>. The necessity of examining control-flow edges when interpreting SSA form limits its utility for many purposes.
Reference: [X3J89] <author> X3J3 Subcommittee of ANSI. </author> <title> American National Standard for Information Systems Programming Language Fortran: </title> <institution> S8 (X3.9-198x). American National Standards Institute, </institution> <address> New York, NY, </address> <year> 1989. </year>
Reference-contexts: a statement S 1 to a following statement S 2 , based on an array A, unless we can prove that M A S 2 == ;: Bounded regular sections comprise the same set of rectangular subarrays that can be written using triplet notation in the proposed Fortran 90 standard <ref> [X3J89] </ref>. They can represent sparse regions such as stripes and grids and dense regions such as columns, rows, and blocks. 2.2.1 Vectors of Ranges The descriptors for bounded regular sections are vectors of elements from the subscript lattice in Figure 2.1. <p> The final summary regular sections are built in order, so that incomplete regular sections need never be translated into a call site. However, the proposed Fortran 90 standard allows recursion <ref> [X3J89] </ref>, so it must be handled someday. Unfortunately, a straightforward iterative propagation of regular sections will not terminate, since the lattice has unbounded depth. Li and Yew [LY88b] and Cooper and Kennedy [CK88b] describe approaches for propagating subarrays that are efficient regardless of the depth of the lattice.
Reference: [YHR89] <author> Wuu Yang, Susan Horwitz, and Thomas Reps. </author> <title> Detecting program components with equivalent behaviors. </title> <type> Technical Report 840, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> April </month> <year> 1989. </year> <month> 163 </month>
Reference-contexts: Field's work still focuses on data-driven models of execution, where control dependence is converted to data dependence. (Doing this by use of control dependences is much more efficient than IF-conversion.) The program representation graph of Horwitz et al. also resembles GSA form <ref> [YHR89] </ref>, and is also used to recognize a form of equivalence between program fragments. 67 3.7 Summary The major result of this chapter is the introduction of thinned gated single-assignment (TGSA) form.
References-found: 94

