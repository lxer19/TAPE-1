URL: http://www.cs.arizona.edu/people/reading_group/papers/petal-asplos96.ps
Refering-URL: http://www.cs.arizona.edu/people/reading_group/papers/
Root-URL: http://www.cs.arizona.edu
Title: Petal: Distributed Virtual Disks  
Author: Edward K. Lee and Chandramohan A. Thekkath 
Address: 130 Lytton Ave, Palo Alto, CA 94301.  
Affiliation: Systems Research Center Digital Equipment Corporation  
Abstract: The ideal storage system is globally accessible, always available, provides unlimited performance and capacity for a large number of clients, and requires no management. This paper describes the design, implementation, and performance of Petal, a system that attempts to approximate this ideal in practice through a novel combination of features. Petal consists of a collection of network-connected servers that cooperatively manage a pool of physical disks. To a Petal client, this collection appears as a highly available block-level storage system that provides large abstract containers called virtual disks. A virtual disk is globally accessible to all Petal clients on the network. A client can create a virtual disk on demand to tap the entire capacity and performance of the underlying physical resources. Furthermore, additional resources, such as servers and disks, can be automatically incorporated into Petal. We have an initial Petal prototype consisting of four 225 MHz DEC 3000/700 workstations running Digital Unix and connected by a 155 Mbit/s ATM network. The prototype provides clients with virtual disks that tolerate and recover from disk, server, and network failures. Latency is comparable to a locally attached disk, and throughput scales with the number of servers. The prototype can achieve I/O rates of up to 3150 requests/sec and bandwidth up to 43.1 Mbytes/sec. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas E. Anderson, Michael D. Dahlin, Jeanna M. Neefe, David A. Patterson, Drew S. Roselli, and Randolph Y. Wang. </author> <title> Serverless network file systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(1) </volume> <pages> 41-79, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Cluster file systems such as the xFS <ref> [1] </ref> and parallel databases such as the Oracle Parallel Server may be able to take advantage of this fact by concurrently accessing a single virtual disk from multiple machines. <p> Although many of the systems above can tolerate disk failures, TickerTAIP is the only one that can tolerate node failures. In contrast, Petal supports wider distribution and can tolerate both node and network failures. The most closely related file systems include xFS <ref> [1] </ref>, Zebra [12], Echo [15], and AFS [16]. All these systems except xFS use a single meta-data server for a given partial subtree of the file system name space; ultimately limiting their scalability.
Reference: [2] <author> Thomas E. Anderson, Susan S. Owicki, James B. Saxe, and Charles P. Thacker. </author> <title> High-speed switch scheduling for local-area networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 319-352, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: An additional benefit is that the block-level interface is useful for supporting heterogeneous clients and client applications; that is, we can easily support many different types of file systems and databases. We have implemented Petal servers on Alpha workstations running Digital Unix connected by the Digital ATM network <ref> [2] </ref>. A Petal client interface exists for Digital Unix and is implemented as a kernel device driver, allowing all standard Unix applications, utilities, and file systems to run unmodified when using Petal.
Reference: [3] <author> Andrew D. Birrell and Bruce Jay Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: Clients view the storage system as a collection of virtual disks and access Petal services via a remote procedure call (RPC) <ref> [3] </ref> interface. A basic principle in the design of the Petal RPC interface was to maintain all state needed for ensuring the integrity of the storage system in the servers, and maintain only hints in the clients.
Reference: [4] <author> Luis-Felipe Cabrera and Darrel D. E. Long. Swift: </author> <title> Using distributed disk striping to provide high I/O data rates. </title> <journal> ACM Computing Systems, </journal> <volume> 4 </volume> <pages> 405-436, </pages> <month> Fall </month> <year> 1991. </year>
Reference-contexts: Related block-level storage systems include RAID-II [7], TickerTAIP [5], Logical Disk [8], Loge [10], Mime [6], Au-toRAID [19], and Swift <ref> [4] </ref>. Some of these systems support only simple algorithmic mappings between the address space seen by a client and the underlying physical disks. This mapping is usually completely specified when the system is configured.
Reference: [5] <author> Pei Cao, Swee Boon Lim, Shivakumar Venkataraman, and John Wilkes. </author> <title> The TickerTAIP parallel RAID architecture. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12(3) </volume> <pages> 236-269, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Related block-level storage systems include RAID-II [7], TickerTAIP <ref> [5] </ref>, Logical Disk [8], Loge [10], Mime [6], Au-toRAID [19], and Swift [4]. Some of these systems support only simple algorithmic mappings between the address space seen by a client and the underlying physical disks. This mapping is usually completely specified when the system is configured.
Reference: [6] <author> C. Chao, R. English, D. Jacobson, A. Stepanov, and J. Wilkes. Mime: </author> <title> A high performance parallel storage device with strong recovery guarantees. </title> <type> Technical Report HPL-CSP-92-9, </type> <institution> Hewlett-Packard Laboratories, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Related block-level storage systems include RAID-II [7], TickerTAIP [5], Logical Disk [8], Loge [10], Mime <ref> [6] </ref>, Au-toRAID [19], and Swift [4]. Some of these systems support only simple algorithmic mappings between the address space seen by a client and the underlying physical disks. This mapping is usually completely specified when the system is configured.
Reference: [7] <author> Peter M. Chen, Edward K. Lee, Ann L. Drapeau, Ken Lutz, Ethan L. Miller, Srinivasan Seshan, Ken Shirriff, David A. Patterson, and Randy H. Katz. </author> <title> Performance and design evaluation of the RAID-II storage server. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 2(3) </volume> <pages> 243-260, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Related block-level storage systems include RAID-II <ref> [7] </ref>, TickerTAIP [5], Logical Disk [8], Loge [10], Mime [6], Au-toRAID [19], and Swift [4]. Some of these systems support only simple algorithmic mappings between the address space seen by a client and the underlying physical disks. This mapping is usually completely specified when the system is configured.
Reference: [8] <author> Wiebren de Jonge, M. Frans Kaashoek, and Wilson C. Hsieh. </author> <title> The logical disk: A new approach to improving file systems. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-28, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Related block-level storage systems include RAID-II [7], TickerTAIP [5], Logical Disk <ref> [8] </ref>, Loge [10], Mime [6], Au-toRAID [19], and Swift [4]. Some of these systems support only simple algorithmic mappings between the address space seen by a client and the underlying physical disks. This mapping is usually completely specified when the system is configured.
Reference: [9] <author> Peter Druschel, Larry L. Peterson, and Bruce S. Davie. </author> <title> Experiences with a high-speed network adaptor: A software perspective. </title> <booktitle> In Proceedings of the 1994 SIGCOMM Symposium on Communications Architectures, Protocols and Applications, </booktitle> <pages> pages 2-13, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Our Petal servers run at user-level and we use the standard UNIX socket interface and UDP/IP protocol stacks. Techniques for streamlining these network accesses are well understood <ref> [9, 18] </ref>. As an experiment, we eliminated copying and checksums at the network layer for large read requests. For 64 Kbyte read requests, this optimization reduced CPU utilization to 48% and increased throughput from 43.1 Mbytes/s to 48.5 Mbytes/s. In this case, the throughput was limited by the disk controller.
Reference: [10] <author> R. M. English and A. A. Stepanov. Loge: </author> <title> A self-organizing disk controller. </title> <booktitle> In Proceedings of the Winter 1992 USENIX Conference, </booktitle> <pages> pages 237-251, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Related block-level storage systems include RAID-II [7], TickerTAIP [5], Logical Disk [8], Loge <ref> [10] </ref>, Mime [6], Au-toRAID [19], and Swift [4]. Some of these systems support only simple algorithmic mappings between the address space seen by a client and the underlying physical disks. This mapping is usually completely specified when the system is configured.
Reference: [11] <author> Garth A. Gibson, David F. Nagle, Khalil Amiri, Fay W. Chang, Eugene Feinberg, Howard Gobioff, Chen Lee, Berend Ozceri, Erik Riedel, and David Rochberg. </author> <title> A case for network-attached secure disks. </title> <type> Technical Report CMU-CS-96-142, </type> <institution> Department of Electrical and Computer Engineering, Carnegie-Mellon University, </institution> <month> June </month> <year> 1996. </year>
Reference-contexts: One alternative to Petal is to design distributed storage with a richer interface that is more like a file system as is being done in the CMU NASD project <ref> [11] </ref>. This could potentially result in a system that is more efficient overall; however, we currently believe that the simpler Petal interface is adequate and that higher level services can be efficiently built on top of it.
Reference: [12] <author> John H. Hartman and John K. Ousterhout. </author> <title> The Zebra striped network file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 274-310, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Although many of the systems above can tolerate disk failures, TickerTAIP is the only one that can tolerate node failures. In contrast, Petal supports wider distribution and can tolerate both node and network failures. The most closely related file systems include xFS [1], Zebra <ref> [12] </ref>, Echo [15], and AFS [16]. All these systems except xFS use a single meta-data server for a given partial subtree of the file system name space; ultimately limiting their scalability.
Reference: [13] <author> Hui-I Hsiao and David J. DeWitt. </author> <title> Chained declustering: A new availability strategy for multiprocessor database machines. </title> <type> Technical Report CS TR 854, </type> <institution> University of Wiscon-sin, Madison, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: A different set of data access and recovery modules exists for each type of redundancy scheme supported by the system. We currently support simple data striping without redundancy and a replication-based redundancy scheme called chained-declustering <ref> [13] </ref>. The desired redundancy scheme for a virtual disk is specified when the virtual disk is created. Subsequently, the redundancy scheme, and other attributes, can be transparently changed via a process called virtual disk reconfiguration. <p> To help guard against fencing off a heavily used subrange of the virtual disk, we construct the fenced region by collecting small non-contiguous ranges distributed throughout the virtual disk, instead of a single contiguous region. 2.4 Data Access and Recovery This section describes Petal's chained-declustered <ref> [13] </ref> data access and recovery modules. These modules give clients highly available access to data by automatically bypassing failed components. Dynamic load balancing eliminates system bottlenecks by ensuring uniform load distribution even in the face of component failures.
Reference: [14] <author> Leslie Lamport. </author> <title> The Part-Time Parliament. </title> <type> Technical Report 49, </type> <institution> Digital Equipment Corporation, Systems Research Center, </institution> <address> 130 Lytton Ave., Palo Alto, CA 94301-1044, </address> <month> Septem-ber </month> <year> 1989. </year>
Reference-contexts: This information is replicated across all Petal servers in the system. The global state manager is responsible for consistently maintaining this information, which is less than a megabyte in our current implementation. Our algorithm for maintaining global state is based on Leslie Lamport's Paxos, or part-time parliament algorithm <ref> [14] </ref> for implementing distributed, replicated state machines. The algorithm assumes that servers fail by ceasing to operate and that networks can reorder and lose messages.
Reference: [15] <author> Timothy Mann, Andrew D. Birrell, Andy Hisgen, Chuck Jerian, and Garret Swart. </author> <title> A coherent distributed file cache with directory write-behind. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12(2) </volume> <pages> 123-164, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Although many of the systems above can tolerate disk failures, TickerTAIP is the only one that can tolerate node failures. In contrast, Petal supports wider distribution and can tolerate both node and network failures. The most closely related file systems include xFS [1], Zebra [12], Echo <ref> [15] </ref>, and AFS [16]. All these systems except xFS use a single meta-data server for a given partial subtree of the file system name space; ultimately limiting their scalability.
Reference: [16] <author> M. Satyanarayanan. </author> <title> Scalable, secure, and highly available distributed file access. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 9-21, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Although many of the systems above can tolerate disk failures, TickerTAIP is the only one that can tolerate node failures. In contrast, Petal supports wider distribution and can tolerate both node and network failures. The most closely related file systems include xFS [1], Zebra [12], Echo [15], and AFS <ref> [16] </ref>. All these systems except xFS use a single meta-data server for a given partial subtree of the file system name space; ultimately limiting their scalability.
Reference: [17] <author> Daniel Stodolsky, Mark Holland, William V. Courtright II, and Garth A. Gibson. </author> <title> Parity-logging disk arrays. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12(3) </volume> <pages> 206-235, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Petal's framework is sufficiently general to incorporate other classes of redundancy schemes such as those based on parity <ref> [5 , 17] </ref>.
Reference: [18] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Limits to low-latency communication on high-speed networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Our Petal servers run at user-level and we use the standard UNIX socket interface and UDP/IP protocol stacks. Techniques for streamlining these network accesses are well understood <ref> [9, 18] </ref>. As an experiment, we eliminated copying and checksums at the network layer for large read requests. For 64 Kbyte read requests, this optimization reduced CPU utilization to 48% and increased throughput from 43.1 Mbytes/s to 48.5 Mbytes/s. In this case, the throughput was limited by the disk controller.
Reference: [19] <author> John Wilkes, Richard Golding, Carl Staelin, and Tim Sul-livan. </author> <title> The HP AutoRAID hierarchical storage system. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 96-108, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Related block-level storage systems include RAID-II [7], TickerTAIP [5], Logical Disk [8], Loge [10], Mime [6], Au-toRAID <ref> [19] </ref>, and Swift [4]. Some of these systems support only simple algorithmic mappings between the address space seen by a client and the underlying physical disks. This mapping is usually completely specified when the system is configured.
References-found: 19

