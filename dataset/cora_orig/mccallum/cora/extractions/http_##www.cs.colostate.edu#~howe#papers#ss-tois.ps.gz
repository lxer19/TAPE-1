URL: http://www.cs.colostate.edu/~howe/papers/ss-tois.ps.gz
Refering-URL: http://www.cs.colostate.edu/~howe/pubs.html
Root-URL: 
Email: e-mail: daniel@media.mit.edu  e-mail: howe@cs.colostate.edu  
Title: Experiences with Selecting Search Engines using Meta-Search  
Author: Daniel Dreilinger Adele E. Howe 
Note: The material contained in this document has not been submitted for publication elsewhere.  
Date: Submitted: August 8, 1996 Revised: December 2, 1996  
Web: Web: http://www.media.mit.edu/ daniel  Web: http://www.cs.colostate.edu/ howe  
Address: Cambridge, MA 02139  Fort Collins, CO 80523  
Affiliation: MIT Media Laboratory  Computer Science Dept., Colorado State University  
Abstract: Search engines are among the most useful and high profile resources on the Internet. The problem of finding information on the Internet has been replaced with the problem of knowing where search engines are, what they are designed to retrieve and how to use them. This paper describes and evaluates SavvySearch, a meta-search engine designed to intelligently select and interface with multiple remote search engines. The primary meta-search issue examined is the importance of carefully selecting and ranking remote search engines for user queries. We studied the efficacy of SavvySearch's incrementally acquired meta-index approach to selecting search engines by analyzing the effect of time and experience on performance. We also compared the meta-index approach to the simpler categorical approach and showed how much experience is required to surpass the simple scheme. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> InterNIC Database Administration. InterNIC meta-search engines. </institution> <note> http://ds2.internic.net/tools/meta.html. </note>
Reference-contexts: The precision is obtained at the high cost of network utilization, since all referenced documents must be retrieved. An alternative to automated meta-search is to allow the user to completely direct query dispatch. Tools such as All-In-One [7], CUSI [18], search.com [9], Infi-NET's META Search [27] and InterNIC <ref> [1] </ref> are essentially pages full of forms for sending queries to a number of different search engines. The selection process is entirely up to the user|they must type their query into a separate form for each query submission.
Reference: [2] <institution> DesignLab at University of Kansas. ProFusion meta-search. </institution> <note> http://www.designlab.ukans.edu/profusion/. 34 </note>
Reference-contexts: The selection process is entirely up to the user|they must type their query into a separate form for each query submission. Only one search engine is activated at a time, and the results appear in the native format of whichever search engine produced them. The ProFusion system <ref> [2] </ref> combines many features of the other meta-search engines. It supports both manual and automatic query dispatch and integrates and prunes the results.
Reference: [3] <author> C. Mic Bowman, Peter B. Danzig, Udi Manber, and Michael F. Schwartz. </author> <title> Scalable internet resource discovery: research problems and approaches. </title> <journal> CACM, </journal> <volume> 37(8), </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: One problem is that each of the search engines must cooperate with the meta-searcher by supplying up-to-date index information. As the number of databases increases, the administrative complexity may become prohibitive. The Harvest system <ref> [3, 4] </ref> comprises an integrated set of tools developed by the Research Group on Resource Discovery and Directory Service of the Internet Research Task Force.
Reference: [4] <author> C. Mic Bowman, Peter B. Danzig, Udi Manber, Michael F. Schwartz, Darren R. Hardy, and Duane P. Wessels. Harvest: </author> <title> A scalable, customizable discovery and access system. </title> <type> Technical report, </type> <institution> University of Colorado Boulder, </institution> <year> 1995. </year>
Reference-contexts: One problem is that each of the search engines must cooperate with the meta-searcher by supplying up-to-date index information. As the number of databases increases, the administrative complexity may become prohibitive. The Harvest system <ref> [3, 4] </ref> comprises an integrated set of tools developed by the Research Group on Resource Discovery and Directory Service of the Internet Research Task Force.
Reference: [5] <author> Tim Bray. </author> <title> The Open Text Web Index. </title> <address> http://www.opentext.com/. </address>
Reference-contexts: Several different approaches to meta-searching have been deployed; Section 2 introduces a general framework and background information about the various approaches. 1 At this time, these include Lycos [20], WebCrawler [22], Infoseek [6], Open Text <ref> [5] </ref>, Inktomi [16], Excite [29], and Alta Vista [21]. 2 In this paper, we adopt a pragmatic approach to the problem of information retrieval on the Web by focusing on the unusual requirements of one relatively new proposed solution: meta-search.
Reference: [6] <author> Infoseek Corporation. </author> <title> Infoseek Net Search. </title> <address> http://www.infoseek.com/. </address>
Reference-contexts: Several different approaches to meta-searching have been deployed; Section 2 introduces a general framework and background information about the various approaches. 1 At this time, these include Lycos [20], WebCrawler [22], Infoseek <ref> [6] </ref>, Open Text [5], Inktomi [16], Excite [29], and Alta Vista [21]. 2 In this paper, we adopt a pragmatic approach to the problem of information retrieval on the Web by focusing on the unusual requirements of one relatively new proposed solution: meta-search.
Reference: [7] <author> William Cross. </author> <title> All-In-One Search Page. </title> <note> http://www.albany.net/allinone/, 1995. </note>
Reference-contexts: Result verification prunes out unavailable resources and irrelevant documents. The precision is obtained at the high cost of network utilization, since all referenced documents must be retrieved. An alternative to automated meta-search is to allow the user to completely direct query dispatch. Tools such as All-In-One <ref> [7] </ref>, CUSI [18], search.com [9], Infi-NET's META Search [27] and InterNIC [1] are essentially pages full of forms for sending queries to a number of different search engines. The selection process is entirely up to the user|they must type their query into a separate form for each query submission.
Reference: [8] <author> C|NET. C|NET's shareware.com. </author> <note> http://www.shareware.com/. </note>
Reference-contexts: For example, Yahoo [14] and Point [23] search within smaller, human-reviewed collections of Web site descriptions. DejaNews [19] and the Stanford Information Filtering Tool [32, 31] specialize in searching archives of recent Usenet news articles. Tools such as FTPSearch [12] and C|Net's shareware.com <ref> [8] </ref> assist users in finding software and other items available via File Transfer Protocol. Still more search engines target email addresses, newspaper articles, technical reports, books, movie reviews, music recordings|a new database seems to appear daily to satisfy yet another specialized information need.
Reference: [9] <author> C|net. search.com. </author> <note> http://www.search.com/, 1994. </note>
Reference-contexts: The precision is obtained at the high cost of network utilization, since all referenced documents must be retrieved. An alternative to automated meta-search is to allow the user to completely direct query dispatch. Tools such as All-In-One [7], CUSI [18], search.com <ref> [9] </ref>, Infi-NET's META Search [27] and InterNIC [1] are essentially pages full of forms for sending queries to a number of different search engines. The selection process is entirely up to the user|they must type their query into a separate form for each query submission.
Reference: [10] <author> Daniel Dreilinger. </author> <note> SavvySearch Home Page. http://guaraldi.cs.colostate.edu:2000, 1995. </note>
Reference-contexts: For our own meta-search engine, we adopt the principle that Web resources should be used as efficiently as possible. Thus, the success of meta-searching depends critically on carefully selecting which resources to use. Our meta-search engine, SavvySearch <ref> [10] </ref>, carefully selects resources for an individual user's query and balances resource consumption against expected results quality.
Reference: [11] <author> Daniel Dreilinger. </author> <title> Description and evaluation of a meta-search agent. </title> <type> Master's thesis, </type> <institution> Computer Science Dept., Colorado State University, </institution> <month> Spring </month> <year> 1996. </year>
Reference-contexts: A large portion of the unique stems were misspellings, proper names, and 8 Search engines significant according to the Visit measure included Inktomi, Pathfinder, Tribal Voice, and Yahoo, in addition to the overall total. Excite, Inktomi, Point, and Yahoo were significant according to No Results. 9 See <ref> [11] </ref> for some examples of the performance of individual search engines. 25 logarithmic.
Reference: [12] <author> Tor Egge, Hugo Eide Gunnarsen, and Stig Sther Bakken. </author> <note> FTP Search v3.1. http://ftpsearch.unit.no/ftpsearch. </note>
Reference-contexts: For example, Yahoo [14] and Point [23] search within smaller, human-reviewed collections of Web site descriptions. DejaNews [19] and the Stanford Information Filtering Tool [32, 31] specialize in searching archives of recent Usenet news articles. Tools such as FTPSearch <ref> [12] </ref> and C|Net's shareware.com [8] assist users in finding software and other items available via File Transfer Protocol. Still more search engines target email addresses, newspaper articles, technical reports, books, movie reviews, music recordings|a new database seems to appear daily to satisfy yet another specialized information need.
Reference: [13] <author> David Eichmann. </author> <title> Ethical web agents. </title> <booktitle> In Electronic Proceedings of the Second World Wide Web Conference '94: Mosaic and the Web, </booktitle> <year> 1994. </year> <note> http://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/Agents/ eichmann.ethical/ethics.html. </note>
Reference-contexts: This operation must satisfy two conflicting goals: minimizing resource consumption and maximizing search quality [33]. Resource limitations make it impractical to send every query to every known search engine; programs that did so would be considered poor citizens of the Web <ref> [13] </ref>.
Reference: [14] <author> David Filo and Jerry Yang. </author> <note> Yahoo Home Page. http://www.yahoo.com/, 1994. </note>
Reference-contexts: They try, fairly successfully, to be comprehensive; as a result, any search may return an abundance of related and un-related information. The specialty search engines, on the other hand, are inadequate to most topics, but are more likely to quickly focus a search in their area. For example, Yahoo <ref> [14] </ref> and Point [23] search within smaller, human-reviewed collections of Web site descriptions. DejaNews [19] and the Stanford Information Filtering Tool [32, 31] specialize in searching archives of recent Usenet news articles.
Reference: [15] <author> Susan Gauch, Guijun Wang, and Mario Gomez. Profusion: </author> <title> Intelligent fusion from multiple, different search engines. </title> <journal> Journal of Universal Computer Science, </journal> <volume> 2(9), </volume> <month> Sept. </month> <year> 1996. </year>
Reference-contexts: a controlled experiment in which subjects were asked to rate the relevance of results from ten search engines (the six large search engines, Meta-Crawler, SavvySearch and both manual and automatic versions of ProFusion) on 12 queries, the ProFusion system returned the highest number of links judged relevant by the subjects <ref> [15] </ref>. 3 SavvySearch SavvySearch is an experimental meta-searching tool which serves as a single interface to many conventional search engines. The original version of SavvySearch was made available on the Web in March, 1995. Since then, one or more of its experimental versions have been publicly available. <p> In particular, the user population participating in the meta-index and categorical experiments was not the same. Because the interface differed, we needed to explain the differences by indicating that one was an experimental version. These results are consonant with the conclusions suggested by the ProFusion controlled experiment <ref> [15] </ref>. 31 A carefully generated taxonomy may return superior results. From our experiment, we have deter-mined that, for a large group of users, the categorical approach is superior to the learned meta-index scheme on Visits, our indirect measure of relevancy, unless considerable information is available about the query.
Reference: [16] <author> Paul Gauthier and Eric Brewer. </author> <note> Inktomi web services. http://inktomi.berkeley.edu/. 35 </note>
Reference-contexts: Several different approaches to meta-searching have been deployed; Section 2 introduces a general framework and background information about the various approaches. 1 At this time, these include Lycos [20], WebCrawler [22], Infoseek [6], Open Text [5], Inktomi <ref> [16] </ref>, Excite [29], and Alta Vista [21]. 2 In this paper, we adopt a pragmatic approach to the problem of information retrieval on the Web by focusing on the unusual requirements of one relatively new proposed solution: meta-search.
Reference: [17] <author> Luis Gravano, Hector Garca-Molina, and Anthony Tomasic. </author> <title> Precision and recall of GlOSS estimators for database discovery. </title> <booktitle> In Proceedings of the 3rd international Conference on Parallel and Distributed Information Systems (PDIS'94), </booktitle> <year> 1994. </year>
Reference-contexts: We survey a few of the best known of these efforts. The GlOSS (Glossary-of-Servers Server) project <ref> [17] </ref> uses a meta-index to direct simultaneous search of multiple databases (which they call the text-database discovery problem). A meta-index is constructed by integrating the indexes of each of the databases. For each database and each word, the number of documents containing that word is included in the meta-index. <p> These three mechanisms are described in the remainder of this section. 3.2.1 Meta-Index of Prior Query Experience As a substitute for the lack of direct access to the corpus, SavvySearch's meta-index, similar to that described in <ref> [17] </ref>, tracks the effectiveness of each search engine in responding to previous queries.
Reference: [18] <author> Martijn Koster. </author> <title> Configurable Unified Search Engine (CUSI). </title> <note> http://pubweb.nexor.co.uk/public/cusi/doc/about.html, 1994. </note>
Reference-contexts: Result verification prunes out unavailable resources and irrelevant documents. The precision is obtained at the high cost of network utilization, since all referenced documents must be retrieved. An alternative to automated meta-search is to allow the user to completely direct query dispatch. Tools such as All-In-One [7], CUSI <ref> [18] </ref>, search.com [9], Infi-NET's META Search [27] and InterNIC [1] are essentially pages full of forms for sending queries to a number of different search engines. The selection process is entirely up to the user|they must type their query into a separate form for each query submission.
Reference: [19] <author> Steve Madere. </author> <note> DejaNews research service. http://dejanews3.dejanews.com/, 1995. </note>
Reference-contexts: The specialty search engines, on the other hand, are inadequate to most topics, but are more likely to quickly focus a search in their area. For example, Yahoo [14] and Point [23] search within smaller, human-reviewed collections of Web site descriptions. DejaNews <ref> [19] </ref> and the Stanford Information Filtering Tool [32, 31] specialize in searching archives of recent Usenet news articles. Tools such as FTPSearch [12] and C|Net's shareware.com [8] assist users in finding software and other items available via File Transfer Protocol.
Reference: [20] <author> Michael Mauldin. Lycos, </author> <title> the catalog of the internet. </title> <note> http://www.lycos.com/, 1994. </note>
Reference-contexts: By automatically interfacing with multiple conventional search engines, meta-search engines add an additional level of abstraction to Web searching. Several different approaches to meta-searching have been deployed; Section 2 introduces a general framework and background information about the various approaches. 1 At this time, these include Lycos <ref> [20] </ref>, WebCrawler [22], Infoseek [6], Open Text [5], Inktomi [16], Excite [29], and Alta Vista [21]. 2 In this paper, we adopt a pragmatic approach to the problem of information retrieval on the Web by focusing on the unusual requirements of one relatively new proposed solution: meta-search.
Reference: [21] <author> Loius Monier and Mike Burrows. </author> <title> Alta Vista Search Engine. </title> <address> http://www.altavista.digital.com/. </address>
Reference-contexts: Several different approaches to meta-searching have been deployed; Section 2 introduces a general framework and background information about the various approaches. 1 At this time, these include Lycos [20], WebCrawler [22], Infoseek [6], Open Text [5], Inktomi [16], Excite [29], and Alta Vista <ref> [21] </ref>. 2 In this paper, we adopt a pragmatic approach to the problem of information retrieval on the Web by focusing on the unusual requirements of one relatively new proposed solution: meta-search.
Reference: [22] <author> Brian Pinkerton. </author> <note> WebCrawler Home Page. http://webcrawler.com/, 1994. </note>
Reference-contexts: By automatically interfacing with multiple conventional search engines, meta-search engines add an additional level of abstraction to Web searching. Several different approaches to meta-searching have been deployed; Section 2 introduces a general framework and background information about the various approaches. 1 At this time, these include Lycos [20], WebCrawler <ref> [22] </ref>, Infoseek [6], Open Text [5], Inktomi [16], Excite [29], and Alta Vista [21]. 2 In this paper, we adopt a pragmatic approach to the problem of information retrieval on the Web by focusing on the unusual requirements of one relatively new proposed solution: meta-search.
Reference: [23] <editor> Editor in Chief R.F. Holznagel. </editor> <title> Point Web Reviews. </title> <address> http://www.pointcom.com/ </address> . 
Reference-contexts: The specialty search engines, on the other hand, are inadequate to most topics, but are more likely to quickly focus a search in their area. For example, Yahoo [14] and Point <ref> [23] </ref> search within smaller, human-reviewed collections of Web site descriptions. DejaNews [19] and the Stanford Information Filtering Tool [32, 31] specialize in searching archives of recent Usenet news articles.
Reference: [24] <author> Gerard Salton. </author> <title> Automatic Text Processing: the Transformation, Analysis, and Retrieval of Information by Computer. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: The rank orderings of query terms supplied by users in the experiment agreed with Zipf's observations of word frequency <ref> [24, page 106-107] </ref>. Because the histogram of word usage (Figure 5) is log-linear, we analyzed the data in increasingly large groups according to word usage. While only 5.6% of all query terms were used just once, they contributed 50.4% of the unique stems in the meta-index.
Reference: [25] <author> Erik Selberg and Oren Etzioni. </author> <title> Multi-service search and comparison using the MetaCrawler. </title> <booktitle> In Proceedings of the 4th International World Wide Web Conference, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Empirical results indicate that no single search engine is likely to return more than 45% of the relevant results <ref> [25] </ref>. Consequently, working efficiently with the entire collection of search engines can be a challenge and burden for even the most experienced users. Meta-search engines, or tools which access multiple individual search engines, are designed to deal with these problems. <p> The system suggests modifications to the user's query so that they are not inundated with useless results. Then it helps the user identify relevant information providers. The registry of information providers includes a compact description of the contents of each provider that directs query refinement and routing. The MetaCrawler <ref> [26, 25] </ref> meta-search project at the University of Washington integrates a set of general Web search engines and dispatches queries to every one of them. MetaCrawler has demonstrated that precise and up-to-date rankings can be constructed by retrieving the HTML source of all referenced documents and applying further textual analysis.
Reference: [26] <author> Erik Selberg and Oren Etzioni. </author> <title> The MetaCrawler WWW Search Engine. </title> <note> http://metacrawler.cs.washington.edu:8080/home.html, 1995. </note>
Reference-contexts: The system suggests modifications to the user's query so that they are not inundated with useless results. Then it helps the user identify relevant information providers. The registry of information providers includes a compact description of the contents of each provider that directs query refinement and routing. The MetaCrawler <ref> [26, 25] </ref> meta-search project at the University of Washington integrates a set of general Web search engines and dispatches queries to every one of them. MetaCrawler has demonstrated that precise and up-to-date rankings can be constructed by retrieving the HTML source of all referenced documents and applying further textual analysis.
Reference: [27] <author> InfiNET Services. </author> <title> InfiNET META search. </title> <address> http://members.gnn.com/infinet/meta.htm. </address>
Reference-contexts: The precision is obtained at the high cost of network utilization, since all referenced documents must be retrieved. An alternative to automated meta-search is to allow the user to completely direct query dispatch. Tools such as All-In-One [7], CUSI [18], search.com [9], Infi-NET's META Search <ref> [27] </ref> and InterNIC [1] are essentially pages full of forms for sending queries to a number of different search engines. The selection process is entirely up to the user|they must type their query into a separate form for each query submission.
Reference: [28] <author> Mark A. Sheldon, Andrzej Duda, Ron Weiss, and David K. Gifford. </author> <title> Discover: A resource discovery system based on content routing. </title> <booktitle> In Proceedings of the Third International World Wide Web Conference. </booktitle> <publisher> Elsevier, North Holland, </publisher> <month> April </month> <year> 1995. </year>
Reference-contexts: Although Harvest engines only search a single database, it is possible to create custom gatherers which construct a composite index from multiple information repositories. Discover provides both query refinement and query routing to over 500 WAIS sites <ref> [28] </ref>. The system suggests modifications to the user's query so that they are not inundated with useless results. Then it helps the user identify relevant information providers. The registry of information providers includes a compact description of the contents of each provider that directs query refinement and routing.
Reference: [29] <institution> Architext Software. excite Netsearch. </institution> <note> http://www.excite.com/. 36 </note>
Reference-contexts: Several different approaches to meta-searching have been deployed; Section 2 introduces a general framework and background information about the various approaches. 1 At this time, these include Lycos [20], WebCrawler [22], Infoseek [6], Open Text [5], Inktomi [16], Excite <ref> [29] </ref>, and Alta Vista [21]. 2 In this paper, we adopt a pragmatic approach to the problem of information retrieval on the Web by focusing on the unusual requirements of one relatively new proposed solution: meta-search.
Reference: [30] <author> Ian H. Witten, Alistair Moffat, and Timothy C. Bell. </author> <title> Managing Gigabytes: Compressing and Indexing Documents and Images. </title> <publisher> Von Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: When a user submits a query, the search engine looks up the information for each query term in the inverted index. Search engines using the common tf idf (Term Frequency times Inverse Document Frequency) ranking algorithm exploit two important qualities of natural language text to perform accurate retrieval <ref> [30, pages 141-148] </ref>: Term Frequency if a term occurs frequently in a document, that document is considered more relevant to a query containing that term than other documents with fewer or no occurrences of the same term.
Reference: [31] <author> Tak Woon Yan. </author> <title> Stanford Information Filtering Tool (SIFT). </title> <address> http://sift.stanford.edu/. </address>
Reference-contexts: The specialty search engines, on the other hand, are inadequate to most topics, but are more likely to quickly focus a search in their area. For example, Yahoo [14] and Point [23] search within smaller, human-reviewed collections of Web site descriptions. DejaNews [19] and the Stanford Information Filtering Tool <ref> [32, 31] </ref> specialize in searching archives of recent Usenet news articles. Tools such as FTPSearch [12] and C|Net's shareware.com [8] assist users in finding software and other items available via File Transfer Protocol.
Reference: [32] <author> Tak Woon Yan and Hector Garcia-Molina. </author> <title> SIFT a tool for wide-area information dissemination. </title> <booktitle> In Proceedings of the 1995 USENIX Technical Conference, </booktitle> <pages> pages 177-186, </pages> <institution> Stanford University, </institution> <year> 1995. </year>
Reference-contexts: The specialty search engines, on the other hand, are inadequate to most topics, but are more likely to quickly focus a search in their area. For example, Yahoo [14] and Point [23] search within smaller, human-reviewed collections of Web site descriptions. DejaNews [19] and the Stanford Information Filtering Tool <ref> [32, 31] </ref> specialize in searching archives of recent Usenet news articles. Tools such as FTPSearch [12] and C|Net's shareware.com [8] assist users in finding software and other items available via File Transfer Protocol.
Reference: [33] <author> Shlomo Zilberstein. </author> <title> An anytime computation approach to information gathering. </title> <booktitle> In Working Notes of the AAAI Spring Symposium Series on Information Gathering from Distributed, Heterogeneous Environments, </booktitle> <address> Palo Alto, CA, </address> <year> 1995. </year> <month> 37 </month>
Reference-contexts: Users can subsequently request results from additional search engines if they wish to supplement their initial results. SavvySearch assists Web users in finding relevant information by submitting their queries to multiple search engines. This operation must satisfy two conflicting goals: minimizing resource consumption and maximizing search quality <ref> [33] </ref>. Resource limitations make it impractical to send every query to every known search engine; programs that did so would be considered poor citizens of the Web [13].
References-found: 33

