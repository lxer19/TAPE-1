URL: http://www.csc.calpoly.edu/~tpederse/lexacq.ps.gz
Refering-URL: http://www.csc.calpoly.edu/~tpederse/pubs.html
Root-URL: http://www.csc.calpoly.edu
Email: pedersen@seas.smu.edu  
Title: Automatic Acquisition of Noun and Verb Meanings  
Author: Ted Pedersen 
Date: June 19, 1995  
Address: Dallas, TX 75275  
Affiliation: Department of Computer Science and Engineering Southern Methodist University  
Pubnum: Technical Report 95-CSE-10  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Aho and J. Ullman. </author> <title> The Theory of Parsing, Translation, </title> <journal> and Compiling, </journal> <volume> vol. 1. </volume> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1972. </year>
Reference-contexts: The assignment of x as a noun and y as a verb results in a valid sentence. Since the algorithm is looking for a sentence it accepts the latter assignments. 2.2.2 Tomita's Algorithm Tomita's algorithm [57] extends the well known LR (k) parsing algorithm <ref> [1] </ref> to handle non-LR grammars. An LR parser is a shift reduce parser that is guided by a parsing table that tells what action should be taken next. <p> The increased lookahead causes the algorithm to look past the unknown word to whatever is on its right. A simple method to increase the lookahead in Earley's algorithm is described in <ref> [1] </ref> and a more efficient method is described in [38]. Tomita's algorithm can also be extended to have lookahead &gt; 1 by building the LR (k) parse table with k &gt; 1 using well known methods (e.g. [1]). <p> A simple method to increase the lookahead in Earley's algorithm is described in <ref> [1] </ref> and a more efficient method is described in [38]. Tomita's algorithm can also be extended to have lookahead &gt; 1 by building the LR (k) parse table with k &gt; 1 using well known methods (e.g. [1]). The idea of using lookahead to obtain the right context of an unknown word is not necessary if the parsing method is bidirectional (e.g. [54]) rather than exclusively left to right or right to left.
Reference: [2] <author> L. Asker, B. Gamback, and C. Samuelsson. </author> <title> EBL 2 : An approach to automatic lexical acquisition. </title> <booktitle> In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), </booktitle> <pages> pages 1172-1176, </pages> <address> Nantes, France, </address> <year> 1992. </year>
Reference-contexts: This paradigm is applied to NLP by assuming that a sentence with an unknown word is an example of a valid sentence. The system determines what properties the unknown word must have in order to make this assumption valid. EBL 2 <ref> [2] </ref> is an EBL based lexical acquisition system that combines phrase structure grammar with probabilistic information to acquire the syntax of unknown words. A corpus of text is divided into sentences that have unknown words and those with none. The sentences with no unknown words are parsed. <p> Given The x eats meat EBL 2 creates the partial parse P1 in figure 5 and finds that it matches G2. Thus x is classified as a noun with a confidence measure of .75. In the experimental results provided in <ref> [2] </ref> there are 62 syntactic and semantic classes that an unknown word can occupy: 5 for nouns, 10 for adjectives and 47 for verbs. The system was tested using a corpus of Swedish text.
Reference: [3] <author> R. Berwick. </author> <title> Learning word meanings from examples. </title> <booktitle> In Proceedings of the 8th International Joint Conference on Artificial Intelligence (IJCAI-83), </booktitle> <volume> volume 1, </volume> <pages> pages 459-461, </pages> <address> Karlsruhe, West Germany, </address> <month> August </month> <year> 1983. </year>
Reference-contexts: Other work in acquiring the semantics of unknown words <ref> [3, 23, 25, 53] </ref> requires much richer information be available either in the lexical entries or in a world knowledge source. <p> of co-occurence information is useful for language generation in that it helps avoid unlikely or unusual word pairings. [60] uses local word co-occurences to perform sense disambiguation. [50] shows that this technique can be used to resolve syntactic ambiguity. 4.2 Learning from Context Acquisition of lexical semantics is defined in <ref> [3, 23, 25, 53] </ref> as the process of classifying a new or unknown word into known semantic classes. These systems use world knowledge that is represented by script-like structures or concept hierarchies. <p> A place holder is inserted for elm. The system knows that propel requires an agent and a physical object. Since the car is the agent elm is classified as a physical object. 4.2.2 Berwick Methods to acquire the meaning of unknown verbs that appear in stories are described in <ref> [3, 4] </ref>. A process called analogical matching is used to determine the meaning of an unknown verb. The semantics of a word is determined by the role it plays in a causal network description of an event. Similar words play similar roles in similar events.
Reference: [4] <author> R. Berwick. </author> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: A place holder is inserted for elm. The system knows that propel requires an agent and a physical object. Since the car is the agent elm is classified as a physical object. 4.2.2 Berwick Methods to acquire the meaning of unknown verbs that appear in stories are described in <ref> [3, 4] </ref>. A process called analogical matching is used to determine the meaning of an unknown verb. The semantics of a word is determined by the role it plays in a causal network description of an event. Similar words play similar roles in similar events.
Reference: [5] <author> L. Boggess, A. Rajeev, and R. Davis. </author> <title> Disambiguation of prepositional phrases in automatically labelled technical text. </title> <booktitle> In Proceedings of the 9th National Conference on Artificial Intelligence (AAAI-91), </booktitle> <volume> volume 1, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: A large number of taggers trained on previously tagged text have been developed (e.g. [8, 13, 16, 21, 27, 37, 58]). Not all taggers specifically address the problem of unknown words, although any of them could utilize what <ref> [5] </ref> refers to as the last resort heuristic to determine the part of speech of words not in the training corpus. p (t i jw i ) = (number of occurrences of t i ) (number of open class words) This is the probability that given an unknown word w i
Reference: [6] <author> C. Cardie. </author> <title> A case-based approach to knowledge acquisition for domain-specific sentence analysis. </title> <booktitle> In Proceedings of the 11th National Conference on Artificial Intelligence (AAAI-93), </booktitle> <pages> pages 798-803, </pages> <year> 1993. </year>
Reference-contexts: Suppose that FRAME 00 had been acquired. Thus FRAME 11 and FRAME 00 may be generalized into a single frame. The target slot is generalized to inanimate since vehicle and building are subsumed by inanimate in the concept hierarchy. The end result is FRAME 30. 4.3.3 MayTag MayTag <ref> [6] </ref> employs a case based approach to lexical acquisition. MayTag is a lexical acquisition aid that is trained by a user prior to automatically processing sentences with unknown words. The lexicon contains the closed class words.
Reference: [7] <author> D. Carter. </author> <title> Lexical acquisition. </title> <editor> In H. Alshawi, editor, </editor> <booktitle> The Core Language Engine, </booktitle> <pages> pages 217-234. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: This is a very practical approach when the number of unknown words is small and normal processing is not frequently interrupted by the need to handle unknown words. 4.3.1 VEX Vocabulary Expander (VEX) <ref> [7] </ref> is the lexical acquisition tool of the Core Language Engine (CLE). <p> The example session in figure 25 is from <ref> [7] </ref>. Sentences 2),3), and 8) are valid usages and serve as positive examples while the remaining sentences are negative examples.
Reference: [8] <author> E. Charniak, C. Hendrickson, N. Jacobson, and M. Perkowitz. </author> <title> Equations for part-of-speech tagging. </title> <booktitle> In Proceedings of the 11th National Conference on Artificial Intelligence (AAAI-93), </booktitle> <year> 1993. </year>
Reference-contexts: An untagged text is input to the tagger. The probabilities previously computed are used to tag the input sentence. It is important that the training text be related to the text to be tagged. A large number of taggers trained on previously tagged text have been developed (e.g. <ref> [8, 13, 16, 21, 27, 37, 58] </ref>).
Reference: [9] <author> N. Chinchor, L. Hirschman, and D. Lewis. </author> <title> Evaluating message understanding systems: An analysis of the Third Message Understanding Conference (MUC-3). </title> <journal> Computational Linguistics, </journal> <volume> 19(3) </volume> <pages> 409-449, </pages> <month> September </month> <year> 1993. </year> <month> 42 </month>
Reference-contexts: is the case depends on the degree of specificity requested by the user. 3.5 Evaluation Metrics The performance of XXXXX in classifying unknown synonyms of known concepts and in placing unknown concepts into the concept hierarchies is evaluated using modifications of the metrics of the Third Message Understanding Conference (MUC-3) <ref> [9] </ref>. * Recall: degree of completeness of the attempted classifications. Recall = number of correct classif ications number of unknown words * Precision: degree of accuracy of the attempted classifications. P recision = number of correct classif ications number of concepts generated * Over generalization: degree of incorrect generation.
Reference: [10] <author> N. Chomsky. </author> <title> Syntactic Structures. </title> <publisher> Mouton, </publisher> <address> The Hague, </address> <year> 1957. </year>
Reference-contexts: However, this proves to be impossible since humans constantly add to and update their lexicons. Human language is known to be infinite in its lexical and grammatical generative capacity <ref> [10] </ref>. The lexicon of any non-trivial NLP system will always be incomplete. New words and usages are always being added to the human lexicon.
Reference: [11] <author> K. Chuch and R. Mercer. </author> <title> Introduction to the special issue on computational linguistics using large corpora. </title> <journal> Computational Linguistics, </journal> <volume> 19(1) </volume> <pages> 1-24, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: The effect of this heuristic is to assign the unknown word the tag that occurs most often in the open class words. 5 See <ref> [11] </ref> for a more complete list of available corpora. 12 PARTS [13] was one of the first taggers to specifically address unknown words. PARTS predicts proper nouns based upon capitalization before applying the tri-tag model.
Reference: [12] <author> M. Chung and D. Moldovan. </author> <title> Parallel memory-based parsing on a marker-passing computer. </title> <type> Technical Report PKPL 93-4, </type> <institution> University of Southern California, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: Sentences 2),3), and 8) are valid usages and serve as positive examples while the remaining sentences are negative examples. Given this information VEX is able to use inductive inferencing techniques to determine that use up acts as a transitive particle verb. 4.3.2 PALKA PARALLEL <ref> [12] </ref> is a memory based parser that runs on SNAP [43], a marker-passing parallel AI computer. The meaning structures used by PARALLEL are constructed by the lexical acquisition system PALKA [33] prior to parsing.
Reference: [13] <author> K. Church. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143, </pages> <address> Austin, TX, </address> <year> 1988. </year>
Reference-contexts: An untagged text is input to the tagger. The probabilities previously computed are used to tag the input sentence. It is important that the training text be related to the text to be tagged. A large number of taggers trained on previously tagged text have been developed (e.g. <ref> [8, 13, 16, 21, 27, 37, 58] </ref>). <p> The effect of this heuristic is to assign the unknown word the tag that occurs most often in the open class words. 5 See [11] for a more complete list of available corpora. 12 PARTS <ref> [13] </ref> was one of the first taggers to specifically address unknown words. PARTS predicts proper nouns based upon capitalization before applying the tri-tag model. This is a very successful method in English since any capitalized word that is not at the beginning of a sentence must be a proper noun.
Reference: [14] <author> K. Church, W. Gale, P. Hanks, and D. Hindle. </author> <title> Using statistics in lexical analysis. </title> <editor> In U. Zernik, editor, </editor> <title> Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1991. </year>
Reference-contexts: A general discussion of the various statistics than can be used in lexical analysis can be found in <ref> [14] </ref>. Of particular relevance to this dissertation is lexical co-occurence or collocation. These relations reveal that certain words are often used together in a given context while other word pairs simply don't occur.
Reference: [15] <author> K. Church and P. Hanks. </author> <title> Word association norms, mutual information and lexicography. </title> <booktitle> In Proceedings of the the 28th Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 76-83, </pages> <year> 1990. </year> <title> [16] de Marcken C.G. Parsing the LOB corpus. </title> <booktitle> In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 243-251, </pages> <year> 1990. </year>
Reference-contexts: A tagger is trained using a corpus of text where each word has been manually tagged with its part of speech. Probabilities of the co-occurrence of parts of speech are computed using the well known n-tag model (e.g. <ref> [15] </ref>) are computed from this data. An untagged text is input to the tagger. The probabilities previously computed are used to tag the input sentence. It is important that the training text be related to the text to be tagged.
Reference: [17] <author> G. DeJong. </author> <title> Skimming newspaper stories by computer. </title> <type> Technical Report 105, </type> <institution> Yale University, </institution> <year> 1977. </year>
Reference-contexts: These scripts describe the ordinary sequence of events associated with a concept. For example, a script for a vehicle accident ($VehAccident) is : [drive vehicle, lose control of vehicle, vehicle hits an object, vehicle stops]. When an unknown word is encountered by the parser SAM <ref> [17] </ref> FOUL-UP is called and finds the appropriate script based on keywords that are used in the text, especially verbs. FOUL-UP refers to the context of the surrounding script in order to infer the meaning of the unknown. The underlying framework of FOUL-UP is Schank's Conceptual Dependency Network [55].
Reference: [18] <author> J. Earley. </author> <title> An efficient context-free parsing algorithm. </title> <journal> Communications of the ACM, </journal> <volume> 13(2) </volume> <pages> 94-102, </pages> <year> 1970. </year>
Reference-contexts: The effect of acquiring the syntax of an unknown word is to select the proper concept hierarchy in which to classify or place the unknown. Both Earley's <ref> [18] </ref> and Tomita's [57] parsing algorithms treat an unknown word as an extreme case of lexical ambiguity. A lexically ambiguous word belongs to more than one part of speech. An unknown is extremely ambiguous since it could belong to any of the known part of speech. <p> This section shows how these algorithms acquire the syntax of unknown words while parsing. 2.2.1 Earley's Algorithm Earley's algorithm <ref> [18] </ref> parses any CFG in polynomial time and space by building a chart during parsing in order to avoid recomputing constituents that have already been found. There are three steps in Earley's parser: prediction, scanning, and completion. The predictor looks for rules that expand the current constituent being sought.
Reference: [19] <author> M. Evens, </author> <title> editor. Relational models of the lexicon: Representing knowledge in semantic networks. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: Taxonomic information is represented in XXXXX by concept hierarchies. 1 These are tree structures where parents are more general concepts than their children. Concept hierarchies are a common representation scheme in NLP <ref> [19, 28, 45, 52] </ref>, AI [48, 51, 59], linguistics [32, 47], computer hardware [34, 43], biology [24], and anthropology [36]. In XXXXX the acquisition of semantics is defined as locating the concept node in a concept hierarchy that defines an unknown word.
Reference: [20] <author> W. N. Francis and H. Kucera. </author> <title> Frequency Analysis of English Usage: Lexicon and Grammar. </title> <publisher> Houghton Mi*in, </publisher> <year> 1982. </year>
Reference-contexts: In this approach an unknown word can be converged upon from both the left and the right during normal processing (e.g. [46]). 2.3 Corpus Based Methods of Syntactic Acquistion The availability of large manually tagged corpora such as the 1 million word Brown Corpus <ref> [20] </ref>, the 4.5 million word Penn Treebank [39], and the 1 million word Lancaster-Oslo=Bergen (LOB) Corpus 5 [31] has spurred research in corpus based approaches to NLP. 2.3.1 Probabilistic Part of Speech Taggers Probabilistic part of speech tagging is a very successful method for acquiring the syntax of unknown words without
Reference: [21] <editor> R. Garside, G. Leech, and G. Sampson. </editor> <booktitle> The Computational Analysis of English. </booktitle> <address> Long-man, London, </address> <year> 1987. </year>
Reference-contexts: An untagged text is input to the tagger. The probabilities previously computed are used to tag the input sentence. It is important that the training text be related to the text to be tagged. A large number of taggers trained on previously tagged text have been developed (e.g. <ref> [8, 13, 16, 21, 27, 37, 58] </ref>).
Reference: [22] <author> D. Gerdemann and E. Hinrichs. </author> <title> Unicorn: A unification parser for attribute-value grammars. </title> <type> Technical Report CS-91-06, </type> <institution> Cognitive Science Deptartment, The Beckman Institute, </institution> <year> 1991. </year>
Reference-contexts: These templates are refined and merged as further examples of the unknown word are encountered. 4.2.4 MURRAY MURRAY [53] uses a knowledge base in conjunction with Earley's parsing algorithm to learn the syntax and semantics of an unknown word. MURRAY is an extension of the HPSG parser Unicorn <ref> [22] </ref> and processes sentences that deal with police reports of automobile accidents. As shown earlier Earley's algorithm predicts categories for the next word to be parsed. If the word is not found in the predicted categories MURRAY considers it unknown.
Reference: [23] <author> R. Granger. FOUL-UP: </author> <title> A program that figures out meanings of words from context. </title> <booktitle> In Proceedings of the the 5th International Joint Conference on Artificial Intelligence (IJCAI-77), </booktitle> <volume> volume 1, </volume> <pages> pages 172-178, </pages> <address> Cambridge, MA, </address> <month> August </month> <year> 1977. </year> <month> 43 </month>
Reference-contexts: Other work in acquiring the semantics of unknown words <ref> [3, 23, 25, 53] </ref> requires much richer information be available either in the lexical entries or in a world knowledge source. <p> of co-occurence information is useful for language generation in that it helps avoid unlikely or unusual word pairings. [60] uses local word co-occurences to perform sense disambiguation. [50] shows that this technique can be used to resolve syntactic ambiguity. 4.2 Learning from Context Acquisition of lexical semantics is defined in <ref> [3, 23, 25, 53] </ref> as the process of classifying a new or unknown word into known semantic classes. These systems use world knowledge that is represented by script-like structures or concept hierarchies. <p> The clues provided by the semantics of the known words in the sentence are 31 used to acquire the semantics of the unknown. XXXXX fits most closely into this category of system. 4.2.1 FOUL-UP FOUL-UP <ref> [23] </ref> uses a library of scripts to determine the semantics of an unknown word. These scripts describe the ordinary sequence of events associated with a concept. For example, a script for a vehicle accident ($VehAccident) is : [drive vehicle, lose control of vehicle, vehicle hits an object, vehicle stops]. <p> These traits are coarse grained roles such as Actor, Object, or Human. When an unknown word is encountered a place holder is inserted in the sentence and the meaning of the word is inferred from the expectations of the known words. Consider this example from <ref> [23] </ref> : Friday, a car swerved off Route 69. The car struck an elm. FOUL-UP knows all of the words in the story except elm. The first sentence triggers the selection of the script $VehAccident, which contains knowledge about vehicle accidents.
Reference: [24] <author> J. Gregg. </author> <title> THe Language of Taxonomy: an application of symbolic logic to the study of classificatory systems. </title> <publisher> Columbia University Press, </publisher> <address> New York, </address> <year> 1954. </year>
Reference-contexts: Taxonomic information is represented in XXXXX by concept hierarchies. 1 These are tree structures where parents are more general concepts than their children. Concept hierarchies are a common representation scheme in NLP [19, 28, 45, 52], AI [48, 51, 59], linguistics [32, 47], computer hardware [34, 43], biology <ref> [24] </ref>, and anthropology [36]. In XXXXX the acquisition of semantics is defined as locating the concept node in a concept hierarchy that defines an unknown word. If there is no such node then a node must be created and placed into the concept hierarchy.
Reference: [25] <author> P. Hastings. </author> <title> Automatic Acquisition of Word Meaning from Context. </title> <type> PhD thesis, </type> <institution> The University of Michigan, </institution> <year> 1994. </year>
Reference-contexts: Other work in acquiring the semantics of unknown words <ref> [3, 23, 25, 53] </ref> requires much richer information be available either in the lexical entries or in a world knowledge source. <p> XXXXX extends isolated concept hierarchies into semantic 30 networks by creating links between nouns and verbs that are attached in the text being processed. There is no need for detailed lexical entries or grammar rules to specify semantic relationships between concepts. With the exception of <ref> [25] </ref> all learning from context systems assume that the knowledge base is complete. XXXXX can add concepts to its knowledge base. XXXXX's knowledge base operates in open or closed mode. <p> of co-occurence information is useful for language generation in that it helps avoid unlikely or unusual word pairings. [60] uses local word co-occurences to perform sense disambiguation. [50] shows that this technique can be used to resolve syntactic ambiguity. 4.2 Learning from Context Acquisition of lexical semantics is defined in <ref> [3, 23, 25, 53] </ref> as the process of classifying a new or unknown word into known semantic classes. These systems use world knowledge that is represented by script-like structures or concept hierarchies. <p> The meaning of glump is taken to be the intersection of the sets of measures and physical objects. Thus glump is a block which is both a measure (e.g. go three blocks west) and a physical object (e.g. the block is full of stores). 4.2.5 Camille Camille <ref> [25] </ref> is an extension of the HPSG parser LINK [26]. If a sentence with unknown words is being parsed unknown words are represented by default definitions. The sentence is parsed using each default definition to see which definition leads to a successful parse. <p> The parsing mechanism assigns a default definition to unknown words. The default definition consists of every possible syntactic category that could be associated with the unknown word. The sentence is parsed to determine which category is valid for the unknown word. For example: Terrorists froobled the building (from <ref> [25] </ref>), where froobled is an unknown word. The parser determines that froobled is a transitive verb that is preceded by the agent terrorists and followed by the object building.
Reference: [26] <author> P. Hastings, S. Lytinen, and R. Lindsay. </author> <title> Learning words from context. </title> <booktitle> Machine Learning 91, </booktitle> <year> 1991. </year>
Reference-contexts: Thus glump is a block which is both a measure (e.g. go three blocks west) and a physical object (e.g. the block is full of stores). 4.2.5 Camille Camille [25] is an extension of the HPSG parser LINK <ref> [26] </ref>. If a sentence with unknown words is being parsed unknown words are represented by default definitions. The sentence is parsed using each default definition to see which definition leads to a successful parse.
Reference: [27] <author> D. Hindle and M. Rooth. </author> <title> Structural ambiguity and lexical relations. </title> <booktitle> In Proceedings of the the 29th Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 229-236, </pages> <address> Berkeley, CA, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: An untagged text is input to the tagger. The probabilities previously computed are used to tag the input sentence. It is important that the training text be related to the text to be tagged. A large number of taggers trained on previously tagged text have been developed (e.g. <ref> [8, 13, 16, 21, 27, 37, 58] </ref>).
Reference: [28] <author> G Hirst. </author> <title> Near-synonymy and the structure of lexical knowledge. </title> <booktitle> In Working Notes of the AAAI Spring Symposium on Representation and Acquisition of Lexical Knowledge, </booktitle> <pages> pages 51-56, </pages> <address> Palo Alto, CA, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: Taxonomic information is represented in XXXXX by concept hierarchies. 1 These are tree structures where parents are more general concepts than their children. Concept hierarchies are a common representation scheme in NLP <ref> [19, 28, 45, 52] </ref>, AI [48, 51, 59], linguistics [32, 47], computer hardware [34, 43], biology [24], and anthropology [36]. In XXXXX the acquisition of semantics is defined as locating the concept node in a concept hierarchy that defines an unknown word.
Reference: [29] <author> P. Jacobs. </author> <title> A knowledge framework for natural language analysis. </title> <booktitle> In Proceedings of the 10th International Joint Conference on Artificial Intelligence (IJCAI-87), </booktitle> <address> Milan, Italy, </address> <month> August </month> <year> 1987. </year>
Reference-contexts: RINA relies upon interaction with a user to provide negative and positive examples of a word's usage in order to converge upon a meaning. An automatic version of RINA is described in [30]. The parser TRUMP <ref> [29] </ref> calls RINA when it encounters a sentence with an unknown word. TRUMP produces a chart that represents the text and a context that presents the concepts referred to previously in the text.
Reference: [30] <author> P. Jacobs and U. Zelnik. </author> <title> Acquiring lexical knowledge from text: A case study. </title> <booktitle> In Proceedings of the 7th National Conference on Artificial Intelligence (AAAI-88), </booktitle> <volume> volume 2, </volume> <pages> pages 739-744, </pages> <address> St. Paul, MN, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: RINA relies upon interaction with a user to provide negative and positive examples of a word's usage in order to converge upon a meaning. An automatic version of RINA is described in <ref> [30] </ref>. The parser TRUMP [29] calls RINA when it encounters a sentence with an unknown word. TRUMP produces a chart that represents the text and a context that presents the concepts referred to previously in the text. In this example from [30]: Warnaco received another merger offer, valued at $36 a <p> An automatic version of RINA is described in <ref> [30] </ref>. The parser TRUMP [29] calls RINA when it encounters a sentence with an unknown word. TRUMP produces a chart that represents the text and a context that presents the concepts referred to previously in the text. In this example from [30]: Warnaco received another merger offer, valued at $36 a share where merger is an unknown word. TRUMP constructs a chart that contains all the possible meanings of the unknown word based upon morphological, syntactic, semantic, and contextual clues.
Reference: [31] <author> S. Johansson, E. Atwell, R. Garside, and G. Leech. </author> <title> The tagged LOB Corpus. User's manual. </title> <publisher> Longman, </publisher> <address> London, </address> <year> 1986. </year>
Reference-contexts: both the left and the right during normal processing (e.g. [46]). 2.3 Corpus Based Methods of Syntactic Acquistion The availability of large manually tagged corpora such as the 1 million word Brown Corpus [20], the 4.5 million word Penn Treebank [39], and the 1 million word Lancaster-Oslo=Bergen (LOB) Corpus 5 <ref> [31] </ref> has spurred research in corpus based approaches to NLP. 2.3.1 Probabilistic Part of Speech Taggers Probabilistic part of speech tagging is a very successful method for acquiring the syntax of unknown words without parsing.
Reference: [32] <author> P. Kay. </author> <title> Taxonomy and semantic contrast. </title> <booktitle> Language, </booktitle> <volume> 47(4) </volume> <pages> 866-887, </pages> <year> 1971. </year>
Reference-contexts: Semantic definitions attempt to relate the word to a real-world concept. There are many forms of semantic lexical representation but most include a taxonomic classifications of concepts. This is natural since here is significant evidence that human lexicons are largely organized as taxonomies of concepts <ref> [32] </ref>. This fact combined with the certainly that the lexicon of a NLP system will be incomplete means that adding concepts to a taxonomy is a critical issue in designing and maintaining NLP systems. <p> Taxonomic information is represented in XXXXX by concept hierarchies. 1 These are tree structures where parents are more general concepts than their children. Concept hierarchies are a common representation scheme in NLP [19, 28, 45, 52], AI [48, 51, 59], linguistics <ref> [32, 47] </ref>, computer hardware [34, 43], biology [24], and anthropology [36]. In XXXXX the acquisition of semantics is defined as locating the concept node in a concept hierarchy that defines an unknown word.
Reference: [33] <author> J.T. Kim and D. Moldovan. PALKA: </author> <title> A system for linguistic knowledge acquisition. </title> <type> Technical Report PKPL 92-8, </type> <institution> Dept. of Electrical Engineering-Systems, University of Southern California, </institution> <year> 1992. </year>
Reference-contexts: The meaning structures used by PARALLEL are constructed by the lexical acquisition system PALKA <ref> [33] </ref> prior to parsing. A traditional parser attempts to build the structure of a sentence and store it in the computer's memory. Memory based parsing reverses the process and begins with meaning representations stored in the computer. <p> Before PALKA is called a case frame for the event to be acquired is constructed manually and very general constraints are manually placed on the thematic roles. The example in figure 26 is from <ref> [33] </ref> and shows PALKA acquiring a case frame for bombing. FRAME 10 and the constraints on the thematic roles are created manually. A corpus of text is scanned for sentences that relate to the bombing case frame. Each relevant sentence from the corpus is extracted and broken into smaller clauses.
Reference: [34] <author> H. Kitano and T. Higuchi. </author> <title> High performance memory-based translation on IXM2 massively parallel associative memory processor. </title> <booktitle> In Proceedings of the 9th National Conference on Artificial Intelligence (AAAI-91), </booktitle> <volume> volume 1, </volume> <pages> pages 155-159, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Taxonomic information is represented in XXXXX by concept hierarchies. 1 These are tree structures where parents are more general concepts than their children. Concept hierarchies are a common representation scheme in NLP [19, 28, 45, 52], AI [48, 51, 59], linguistics [32, 47], computer hardware <ref> [34, 43] </ref>, biology [24], and anthropology [36]. In XXXXX the acquisition of semantics is defined as locating the concept node in a concept hierarchy that defines an unknown word. If there is no such node then a node must be created and placed into the concept hierarchy.
Reference: [35] <author> K. Kukich. </author> <title> Techniques for automatically correcting words in text. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4) </volume> <pages> 377-439, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: The word is badly misspelled. There is no possible way it could be a valid word. Again, the system would have to possess spelling rules to make this judgment. * The dictator is a very smmmart man. * Where is my xyljkkl? The interested reader is referred to <ref> [35] </ref> for a survey of work in spelling error detection and correction. 1.4.2 Starting Point of Lexicon The state of the lexicon at the time an unknown word is encountered is an important factor in determining how to acquire the meaning of the unknown.
Reference: [36] <author> G. Lakoff. Women, </author> <title> fire, and dangerous things: What categories reveal about the mind. </title> <publisher> The University of Chicago Press, </publisher> <address> Chicago, IL, </address> <year> 1987. </year>
Reference-contexts: Concept hierarchies are a common representation scheme in NLP [19, 28, 45, 52], AI [48, 51, 59], linguistics [32, 47], computer hardware [34, 43], biology [24], and anthropology <ref> [36] </ref>. In XXXXX the acquisition of semantics is defined as locating the concept node in a concept hierarchy that defines an unknown word. If there is no such node then a node must be created and placed into the concept hierarchy.
Reference: [37] <author> G. Leech, R. Garside, and E. Atwell. </author> <title> The automatic grammatical tagging of the LOB corpus. </title> <journal> Journal of the International Computer Archive of Modern English, </journal> <volume> 7 </volume> <pages> 13-33, </pages> <year> 1983. </year>
Reference-contexts: An untagged text is input to the tagger. The probabilities previously computed are used to tag the input sentence. It is important that the training text be related to the text to be tagged. A large number of taggers trained on previously tagged text have been developed (e.g. <ref> [8, 13, 16, 21, 27, 37, 58] </ref>).
Reference: [38] <author> H. Leiss. </author> <title> On Kilbury's modification of Earley's algorithm. </title> <journal> ACM Transactions on Pro--gramming Languages and Systems, </journal> <volume> 12(4) </volume> <pages> 610-640, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The increased lookahead causes the algorithm to look past the unknown word to whatever is on its right. A simple method to increase the lookahead in Earley's algorithm is described in [1] and a more efficient method is described in <ref> [38] </ref>. Tomita's algorithm can also be extended to have lookahead &gt; 1 by building the LR (k) parse table with k &gt; 1 using well known methods (e.g. [1]).
Reference: [39] <author> M. Marcus, B. Santorini, and M. A. Marcinkiewicz. </author> <title> Building a large annotated corpus of English: The Penn Treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 313-330, </pages> <year> 1993. </year>
Reference-contexts: this approach an unknown word can be converged upon from both the left and the right during normal processing (e.g. [46]). 2.3 Corpus Based Methods of Syntactic Acquistion The availability of large manually tagged corpora such as the 1 million word Brown Corpus [20], the 4.5 million word Penn Treebank <ref> [39] </ref>, and the 1 million word Lancaster-Oslo=Bergen (LOB) Corpus 5 [31] has spurred research in corpus based approaches to NLP. 2.3.1 Probabilistic Part of Speech Taggers Probabilistic part of speech tagging is a very successful method for acquiring the syntax of unknown words without parsing.
Reference: [40] <author> M. Meteer, R. Schwartz, and R. Weischedel. POST: </author> <title> Using probabilities in language processing. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence (IJCAI-91), </booktitle> <volume> volume 2, </volume> <pages> pages 960-965, </pages> <address> Sydney, Australia, </address> <year> 1991. </year>
Reference-contexts: PARTS predicts proper nouns based upon capitalization before applying the tri-tag model. This is a very successful method in English since any capitalized word that is not at the beginning of a sentence must be a proper noun. POST <ref> [40, 58] </ref> uses a model for tagging an unknown word that takes into account morphological and character clues of the unknown word. The morphological clues include the inflectional endings -ed, -s, -ing and 32 derivational endings, among them -ion, -al, -ive, and -ly.
Reference: [41] <author> G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. </author> <title> Five papers on WordNet, March 1993. </title> <type> CSL Report 43, </type> <institution> Princeton, NJ. </institution>
Reference-contexts: Using partial parse information in constructing the library of templates (e.g. [44]) seems more robust. 3 Semantic Acquisition in XXXXX The XXXXX knowledge base consists of concept hierarchies of nouns and verbs derived from WordNet <ref> [41] </ref>. The words in the lexicon are linked to the concept nodes that define them. Each noun and verb may be linked to multiple concept nodes in the concept hierarchies. A word that has multiple concept nodes for the same part of speech is said to have multiple senses.
Reference: [42] <author> T. Mitchell. </author> <title> Version spaces: A candidate elimination approach to rule learning. </title> <booktitle> In Proceedings of the the 5th International Joint Conference on Artificial Intelligence (IJCAI-77), </booktitle> <volume> volume 1, </volume> <pages> pages 305-310, </pages> <address> Cambridge, MA, </address> <month> August </month> <year> 1977. </year>
Reference-contexts: Systems that allow for human intervention have the advantage of receiving negative examples. A human can indicate that particular usages are valid or not, thus allowing for classical inductive inferencing techniques such as Mitchell's Version Spaces <ref> [42] </ref> to be used to determine a definition of the word that is consistent with all examples. In automatic approaches to lexical acquisition the examples given to the system are usually only positive, thus making inferencing more difficult.
Reference: [43] <author> D. Moldovan, W. Lee, C. Lin, and M. Chung. </author> <title> SNAP: </title> <booktitle> parallel processing applied to AI. Computer, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Taxonomic information is represented in XXXXX by concept hierarchies. 1 These are tree structures where parents are more general concepts than their children. Concept hierarchies are a common representation scheme in NLP [19, 28, 45, 52], AI [48, 51, 59], linguistics [32, 47], computer hardware <ref> [34, 43] </ref>, biology [24], and anthropology [36]. In XXXXX the acquisition of semantics is defined as locating the concept node in a concept hierarchy that defines an unknown word. If there is no such node then a node must be created and placed into the concept hierarchy. <p> Given this information VEX is able to use inductive inferencing techniques to determine that use up acts as a transitive particle verb. 4.3.2 PALKA PARALLEL [12] is a memory based parser that runs on SNAP <ref> [43] </ref>, a marker-passing parallel AI computer. The meaning structures used by PARALLEL are constructed by the lexical acquisition system PALKA [33] prior to parsing. A traditional parser attempts to build the structure of a sentence and store it in the computer's memory.
Reference: [44] <author> G. Neumann. </author> <title> Application of explanation-based learning for efficient processing of constraint-based grammars. </title> <booktitle> In Proceedings of the Tenth Conference on Artificial Intelligence for Applications, </booktitle> <pages> pages 208-214, </pages> <address> San Antonio, TX, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Given a large enough corpus these assumptions should prove valid. However, as reported above, it can be the case that only a small percentage of the sentences in a corpus are completely parsed. Using partial parse information in constructing the library of templates (e.g. <ref> [44] </ref>) seems more robust. 3 Semantic Acquisition in XXXXX The XXXXX knowledge base consists of concept hierarchies of nouns and verbs derived from WordNet [41]. The words in the lexicon are linked to the concept nodes that define them.
Reference: [45] <author> S. Nirenburg, V. Raskin, and B. Onyshkevych. Apologiae ontologiae. </author> <booktitle> In Working Notes of the AAAI Spring Symposium on Representation and Acquisition of Lexical Knowledge, </booktitle> <pages> pages 95-107, </pages> <address> Palo Alto, CA, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: Taxonomic information is represented in XXXXX by concept hierarchies. 1 These are tree structures where parents are more general concepts than their children. Concept hierarchies are a common representation scheme in NLP <ref> [19, 28, 45, 52] </ref>, AI [48, 51, 59], linguistics [32, 47], computer hardware [34, 43], biology [24], and anthropology [36]. In XXXXX the acquisition of semantics is defined as locating the concept node in a concept hierarchy that defines an unknown word.
Reference: [46] <author> T. Pedersen and W. Chen. </author> <title> Lexical acquistion via constraint solving. </title> <booktitle> In Working Notes of the AAAI Spring Symposium on Representation and Acquisition of Lexical Knowledge, </booktitle> <pages> pages 118-122, </pages> <address> Palo Alto, CA, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: A bidirectional parser works in both directions and may begin at any position in the sentence. In this approach an unknown word can be converged upon from both the left and the right during normal processing (e.g. <ref> [46] </ref>). 2.3 Corpus Based Methods of Syntactic Acquistion The availability of large manually tagged corpora such as the 1 million word Brown Corpus [20], the 4.5 million word Penn Treebank [39], and the 1 million word Lancaster-Oslo=Bergen (LOB) Corpus 5 [31] has spurred research in corpus based approaches to NLP. 2.3.1
Reference: [47] <author> C. Pollard and I. Sag. </author> <title> Head-Driven Phrase Structure Grammar. </title> <publisher> The University of Chicago Press, </publisher> <address> Chicago, IL, </address> <year> 1994. </year>
Reference-contexts: Taxonomic information is represented in XXXXX by concept hierarchies. 1 These are tree structures where parents are more general concepts than their children. Concept hierarchies are a common representation scheme in NLP [19, 28, 45, 52], AI [48, 51, 59], linguistics <ref> [32, 47] </ref>, computer hardware [34, 43], biology [24], and anthropology [36]. In XXXXX the acquisition of semantics is defined as locating the concept node in a concept hierarchy that defines an unknown word.
Reference: [48] <author> M. Quillian. </author> <title> The teachable language comprehender: A simulation program and theory of language. </title> <journal> Communications of the ACM, </journal> <volume> 12(8) </volume> <pages> 459-476, </pages> <month> August </month> <year> 1969. </year>
Reference-contexts: Taxonomic information is represented in XXXXX by concept hierarchies. 1 These are tree structures where parents are more general concepts than their children. Concept hierarchies are a common representation scheme in NLP [19, 28, 45, 52], AI <ref> [48, 51, 59] </ref>, linguistics [32, 47], computer hardware [34, 43], biology [24], and anthropology [36]. In XXXXX the acquisition of semantics is defined as locating the concept node in a concept hierarchy that defines an unknown word.
Reference: [49] <author> J. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: The global context is the word sense for the major constituents. During processing if the system encounters an unknown word it locates the most similar case and infers semantic and syntactic information for the unknown word. It determines the most similar case using Quinlan's C4.5 decision tree system <ref> [49] </ref>. In one experiment the lexicon begins with 129 closed class words.
Reference: [50] <author> P. </author> <title> Resnik. Selection and Information: A Class-Based Approach to Lexical Relationships. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <year> 1993. </year>
Reference-contexts: Perhaps the most closely related corpus based approaches are those that use measures of collocation information to extract relationships between words. XXXXX uses collocation information it uses it in conjunction with WordNet which is similar to <ref> [50] </ref>. However, the use of this type of information to determine the meaning of unknown nouns and verbs is unique. Collocation and some of the work done with it is discussed in section 4.1. <p> no "good" reason for that and it is simply the way that people have chosen to use the language. [56] shows that knowledge of co-occurence information is useful for language generation in that it helps avoid unlikely or unusual word pairings. [60] uses local word co-occurences to perform sense disambiguation. <ref> [50] </ref> shows that this technique can be used to resolve syntactic ambiguity. 4.2 Learning from Context Acquisition of lexical semantics is defined in [3, 23, 25, 53] as the process of classifying a new or unknown word into known semantic classes.
Reference: [51] <author> E. Rich and K. Knight. </author> <booktitle> Artificial Intelligence. </booktitle> <publisher> McGraw-Hill Inc., </publisher> <address> New York, NY, </address> <note> second edition, 1991. 45 </note>
Reference-contexts: Taxonomic information is represented in XXXXX by concept hierarchies. 1 These are tree structures where parents are more general concepts than their children. Concept hierarchies are a common representation scheme in NLP [19, 28, 45, 52], AI <ref> [48, 51, 59] </ref>, linguistics [32, 47], computer hardware [34, 43], biology [24], and anthropology [36]. In XXXXX the acquisition of semantics is defined as locating the concept node in a concept hierarchy that defines an unknown word.
Reference: [52] <author> C. Riesbeck and C. Martin. </author> <title> Direct memory access parsing. </title> <editor> In J. Kolodner and C. Ries--beck, editors, </editor> <booktitle> Experience, Memory, and Reasoning, </booktitle> <pages> pages 209-225. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1986. </year>
Reference-contexts: Taxonomic information is represented in XXXXX by concept hierarchies. 1 These are tree structures where parents are more general concepts than their children. Concept hierarchies are a common representation scheme in NLP <ref> [19, 28, 45, 52] </ref>, AI [48, 51, 59], linguistics [32, 47], computer hardware [34, 43], biology [24], and anthropology [36]. In XXXXX the acquisition of semantics is defined as locating the concept node in a concept hierarchy that defines an unknown word.
Reference: [53] <author> D. Russell. </author> <title> Language Acquisition in a Unification-Based Grammar Processing System Using a Real-World Knowledge Base. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1993. </year>
Reference-contexts: Other work in acquiring the semantics of unknown words <ref> [3, 23, 25, 53] </ref> requires much richer information be available either in the lexical entries or in a world knowledge source. <p> A lexicon may be in one of three states: empty, core, or complete <ref> [53] </ref>. Each state brings with it a separate set of problems in acquiring unknown words. The methods described in this dissertation assume that the lexicon has a complete set of closed class words and a base of the open class words. <p> of co-occurence information is useful for language generation in that it helps avoid unlikely or unusual word pairings. [60] uses local word co-occurences to perform sense disambiguation. [50] shows that this technique can be used to resolve syntactic ambiguity. 4.2 Learning from Context Acquisition of lexical semantics is defined in <ref> [3, 23, 25, 53] </ref> as the process of classifying a new or unknown word into known semantic classes. These systems use world knowledge that is represented by script-like structures or concept hierarchies. <p> This generalized pattern serves as a template. Multiple templates are created for an unknown word if TRUMP creates multiple candidates. These templates are refined and merged as further examples of the unknown word are encountered. 4.2.4 MURRAY MURRAY <ref> [53] </ref> uses a knowledge base in conjunction with Earley's parsing algorithm to learn the syntax and semantics of an unknown word. MURRAY is an extension of the HPSG parser Unicorn [22] and processes sentences that deal with police reports of automobile accidents. <p> Unification of lower bounds is likely to fail since these entries are quite specific. In this case the shared information in the lower bound is found and a more general lower bound that incorporates all the compatible features. Consider this example from <ref> [53] </ref>: The driver continued to go straight for three glumps, where glump is an unknown word. Earley's algorithm predicts that glumps must be a noun. The use of for indicates that glump might be a measure. The set of measures known in the knowledge base is: fblock, foot, mileg.
Reference: [54] <author> G. Satta and O. </author> <title> Stock. Bidirectional context-free grammar parsing for natural language processing. </title> <journal> Artificial Intelligence, </journal> <volume> 69 </volume> <pages> 123-164, </pages> <year> 1994. </year>
Reference-contexts: The idea of using lookahead to obtain the right context of an unknown word is not necessary if the parsing method is bidirectional (e.g. <ref> [54] </ref>) rather than exclusively left to right or right to left. A bidirectional parser works in both directions and may begin at any position in the sentence.
Reference: [55] <author> R. Schank and R. Abelson. </author> <title> Scripts, Plans, and Understanding. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1977. </year>
Reference-contexts: FOUL-UP refers to the context of the surrounding script in order to infer the meaning of the unknown. The underlying framework of FOUL-UP is Schank's Conceptual Dependency Network <ref> [55] </ref>. Each word expects to be combined with words that have certain traits. These traits are coarse grained roles such as Actor, Object, or Human.
Reference: [56] <author> F. Smadja. </author> <title> Macrocoding the lexicon with co-occurence knowledge. </title> <editor> In U. Zernik, editor, </editor> <title> Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1991. </year>
Reference-contexts: For instance, it is very common to mention black cats but quite unusual to talk about pink cats. Color is certainly an attribute that cats have and it is much more common that they are black than pink. In <ref> [56] </ref> it is argued that co-occurence information should be made a part of the lexicon as many English word combinations are idiosyncratic and don't correspond to any rational rules. <p> Why is it commonly said that a car is powerful but seldom that a car is strong? There is no "good" reason for that and it is simply the way that people have chosen to use the language. <ref> [56] </ref> shows that knowledge of co-occurence information is useful for language generation in that it helps avoid unlikely or unusual word pairings. [60] uses local word co-occurences to perform sense disambiguation. [50] shows that this technique can be used to resolve syntactic ambiguity. 4.2 Learning from Context Acquisition of lexical semantics
Reference: [57] <author> M. Tomita. </author> <title> Efficient Parsing for Natural Language. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1986. </year>
Reference-contexts: The effect of acquiring the syntax of an unknown word is to select the proper concept hierarchy in which to classify or place the unknown. Both Earley's [18] and Tomita's <ref> [57] </ref> parsing algorithms treat an unknown word as an extreme case of lexical ambiguity. A lexically ambiguous word belongs to more than one part of speech. An unknown is extremely ambiguous since it could belong to any of the known part of speech. <p> The assignment of x as a noun and y as a verb results in a valid sentence. Since the algorithm is looking for a sentence it accepts the latter assignments. 2.2.2 Tomita's Algorithm Tomita's algorithm <ref> [57] </ref> extends the well known LR (k) parsing algorithm [1] to handle non-LR grammars. An LR parser is a shift reduce parser that is guided by a parsing table that tells what action should be taken next. <p> A non-LR grammar has multiple entries (conflicts) for some actions. The parse table for the non-LR grammar in figure 1 is shown in figure 3. Both figures are taken from <ref> [57] </ref>. The parser keeps a stack that begins in state 0. The next word is input and its part of speech is determined. The action specified by the table for the state the parser is in and the part of speech of the current word is taken.
Reference: [58] <author> R. Weischedel, M. Meteer, R. Schwartz, L. Ramshaw, and J. Palmucci. </author> <title> Coping with ambiguity and unknown words through probabilistic models. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 359-382, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: An untagged text is input to the tagger. The probabilities previously computed are used to tag the input sentence. It is important that the training text be related to the text to be tagged. A large number of taggers trained on previously tagged text have been developed (e.g. <ref> [8, 13, 16, 21, 27, 37, 58] </ref>). <p> PARTS predicts proper nouns based upon capitalization before applying the tri-tag model. This is a very successful method in English since any capitalized word that is not at the beginning of a sentence must be a proper noun. POST <ref> [40, 58] </ref> uses a model for tagging an unknown word that takes into account morphological and character clues of the unknown word. The morphological clues include the inflectional endings -ed, -s, -ing and 32 derivational endings, among them -ion, -al, -ive, and -ly.
Reference: [59] <author> P. Winston. </author> <booktitle> Artificial Intelligence. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: Taxonomic information is represented in XXXXX by concept hierarchies. 1 These are tree structures where parents are more general concepts than their children. Concept hierarchies are a common representation scheme in NLP [19, 28, 45, 52], AI <ref> [48, 51, 59] </ref>, linguistics [32, 47], computer hardware [34, 43], biology [24], and anthropology [36]. In XXXXX the acquisition of semantics is defined as locating the concept node in a concept hierarchy that defines an unknown word.
Reference: [60] <author> D. Yarowsky. </author> <title> Word-sense disambiguation using statistical models of roget's categories trained on large corpora. </title> <booktitle> In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), </booktitle> <pages> pages 454-460, </pages> <address> Nantes, France, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: but seldom that a car is strong? There is no "good" reason for that and it is simply the way that people have chosen to use the language. [56] shows that knowledge of co-occurence information is useful for language generation in that it helps avoid unlikely or unusual word pairings. <ref> [60] </ref> uses local word co-occurences to perform sense disambiguation. [50] shows that this technique can be used to resolve syntactic ambiguity. 4.2 Learning from Context Acquisition of lexical semantics is defined in [3, 23, 25, 53] as the process of classifying a new or unknown word into known semantic classes.
Reference: [61] <author> U. Zernik. </author> <title> Language acquisition: Learning a hierarchy of phrases. </title> <booktitle> In Proceedings of the 10th International Joint Conference on Artificial Intelligence (IJCAI-87), </booktitle> <volume> volume 1, </volume> <pages> pages 125-132, </pages> <address> Milan, Italy, </address> <month> August </month> <year> 1987. </year> <month> 46 </month>
Reference-contexts: He asked her to make his bed against her will. 4) RINA: Frank made Corinne carry out a job on his behalf. 4.2.3 RINA RINA <ref> [61] </ref> is able to exploit clues found in the input sentence in order to acquire the meaning of unknown words. RINA queries the user for clarification and forms a hypothesis about the meaning of an unknown word based on the clues from the sentence. <p> RINA queries the user for clarification and forms a hypothesis about the meaning of an unknown word based on the clues from the sentence. RINA has a phrasal lexicon available that stores phrases in a hierarchy. The example session in figure 24 is from <ref> [61] </ref>. In 1) and 2) RINA does not realize that push around is a phrase that has a meaning that is not the sum of its parts. It assumes a literal meaning and asks the user to verify this assumption.
References-found: 60

