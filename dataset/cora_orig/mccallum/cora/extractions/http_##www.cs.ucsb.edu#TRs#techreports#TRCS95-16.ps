URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS95-16.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Heuristic Algorithms for Scheduling Iterative Task Computations on Distributed Memory Machines  
Author: Tao Yang Cong Fu 
Keyword: KEY WORDS: Scheduling, communication optimization, granularity, load balancing, iterative task graphs, directed acyclic graphs.  
Address: Santa Barbara, CA 93106.  
Affiliation: Department of Computer Science University of California  
Abstract: Many partitioned scientific programs can be modeled as iterative execution of computational tasks, represented by iterative task graphs (ITGs). In this paper, we consider the symbolic scheduling of ITGs on distributed memory architectures with nonzero communication overhead without searching the entire iteration space. An ITG may or may not have dependence cycles and we propose heuristic algorithms for mapping cyclic and acyclic ITGs, which incorporate techniques of software pipelining, graph unfolding, directed acyclic graph (DAG) scheduling and load balancing. We provide an analysis for computing near-optimal unfolding factors and comparing the performance of the proposed heuristic algorithms with the optimal solutions. We also study the stability of run-time performance when weights are not estimated accurately at compile-time. Our experiments study the scheduling performance of solving several scientific computing problems and analyze the effectiveness of optimization techniques used in our methods. We also present experimental results for mapping SOR-based iterative computation in solving sparse and banded matrix systems, and compare our approach with the multi-coloring method.
Abstract-found: 1
Intro-found: 1
Reference: [AJ86] <author> L. Adams, and H. Jordan, </author> <title> Is SOR color-blind?, </title> <journal> SIAM J. Sci. Stat. Comp, </journal> <volume> 7 (1986), </volume> <pages> pp 490-506. </pages>
Reference-contexts: Similar results for acyclic ITG scheduling can be seen in the right part of Fig. 14 on the role of the load balancing algorithm at Step 3. Iterative sparse matrix computation. In solving a sparse matrix system, the SOR iterative methods could be used <ref> [AJ86] </ref>. We use submatrix partitioning and a partitioned sparse matrix 27 of applying the DAG scheduling for one iteration. The top right is the average improvement ratio after the graphs are unfolded by a factor of 2. <p> The main difficulty in efficiently parallelizing the SOR methods stems from the intra-iteration data dependencies (cf. Jacobi method), as iterates values depend on computed values of the same iteration. The multi-coloring method has been proposed <ref> [AJ86] </ref> for executing SOR-based iterative computation. This scheme performs variable relabeling to increase the amount of exploitable parallelism since nodes assigned the same color can be computed concurrently. Adams and Jordan [AJ86] have shown that the multi-coloring method is numerically equivalent to performing several SOR iterations computed simultaneously in the absence <p> The multi-coloring method has been proposed <ref> [AJ86] </ref> for executing SOR-based iterative computation. This scheme performs variable relabeling to increase the amount of exploitable parallelism since nodes assigned the same color can be computed concurrently. Adams and Jordan [AJ86] have shown that the multi-coloring method is numerically equivalent to performing several SOR iterations computed simultaneously in the absence of convergence tests. However, in order to preserve inter-color data dependencies, processors must communicate between each computation phase associated with each color.
Reference: [AN88] <author> A. Aiken and A. Nicolau, </author> <title> Optimal Loop Parallelization, </title> <booktitle> SIGPLAN 88 Conf. on Programming Language Design and Implementation. </booktitle> <address> pp.308-317. </address>
Reference-contexts: The solution to such a problem is also useful for run-time compilation with the inspector/executor approach [SCMB]. Mapping multiple loops are studied in [RA93] for exploring fine grain parallelism. Software pipelining <ref> [AN88, GS92, L88, RE68, VGN92] </ref> is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques [PM91, N88] have also been developed to allow a compiler to explore more parallelism. <p> In Fig. 3 (d), task instances in different iterations are executed at the same time. iterations. * The idea of exploring parallelism within and across iterations has been proposed in the context of instruction-level software pipelining <ref> [AN88, L88] </ref>. In [GS92], a near-optimal scheduling algorithm for pipelining with no communication delay is proposed. <p> There is no prelude and postlude. However the periodic pattern is not unique. Another pattern is T 8 ; T 6 at [90; 180). The prelude is T 1 6 and the postlude is T 3 8 . Notice that in instruction-level software pipelining <ref> [AN88] </ref>, a global periodic pattern (a pattern which starts at the same time for all processors) is computed and this is not feasible in message-passing machines with asynchronous communication. In our approach, a local periodic pattern is produced. <p> Our approach is based on the idea of "periodic execution" in software pipelining. The difference is that in instruction-level pipelining, periodic pattern is global usually <ref> [AN88] </ref> while in a distributed memory machine, we use local period patterns to utilize 20 asynchronous parallelism. Our code does not have separate prelude and postlude parts, and in this way the code length can be shortened. The starting time of task instances depends on the iteration number k.
Reference: [B90] <author> S. H. Bokhari, </author> <title> Assignment Problems in Parallel and Distributed Computing., </title> <publisher> Kluwer Academic Publisher, </publisher> <year> 1990. </year>
Reference-contexts: The communication contention is hard to model. In [YG92], the processor distance information is incorporated in computing communication volume after virtual processor assignments of tasks in a DAG are determined. Then physical mapping can be adjusted <ref> [B90] </ref>. We are investigating a similar method for mapping ITGs. Acknowledgment This was supported in part by NSF RIA CCR-9409695, a startup fund from UCSB and ARPA contract DABT-63-93-C-0064.
Reference: [CFH95] <author> L. Carter, J. Ferrante and S. F. Hummel, </author> <title> Efficient parallelism via hierarchical tiling, </title> <booktitle> Proc. of Seventh SIAM Conf. on Parallel Processing for Scientific Computing, </booktitle> <year> 1995, </year> <pages> pp. 680-685. </pages>
Reference-contexts: Currently we are implementing a run-time support system for efficiently executing graph schedules. <ref> [CFH95] </ref> proposed a hierarchical tiling framework and our work is useful and can be extended for assisting performance prediction. Our algorithms have not considered the overhead of memory accessing and effect of caching and [WF93] discussed an approach for addressing this issue.
Reference: [CD73] <author> E. G. Coffman and P. J. Denning, </author> <title> Operating Systems Theory, </title> <publisher> Prentice Hall, </publisher> <year> 1973. </year>
Reference-contexts: Step 3: We assign tasks in the ITG G f to p processors such that load among those processors is balanced. Our load balancing method uses the classical list scheduling algorithm <ref> [CD73] </ref> with the highest-weight-task-first principle. The result of this mapping is the starting time fl i for each task and the processor assignment P A. Let the length of this schedule be LP T .
Reference: [CL95] <author> M. Cosnard and M. Loi, </author> <title> Automatic Task Graph Generation Techniques, </title> <booktitle> Proc. of the Hawaii International Conference on System Sciences, IEEE, </booktitle> <volume> Vol II. </volume> <year> 1995. </year> <pages> pp. 113-122. </pages> <note> A revised version will appear in Parallel Processing Letters, a special isssue on partitioning and scheduling. </note>
Reference-contexts: 1 Introduction Parallel programming on distributed memory machines requires coarse grain program partitioning since there is a large startup overhead in message transmission. There are many techniques proposed for program partitioning and granularity control <ref> [CL95, GY93a, SK92, WF89] </ref> and partitioned programs can be represented in a format of coarse-grain task graphs. Finding a mapping of weighted 1 coarse-grain task graphs onto target architectures with the shortest parallel time is important for studying the impact of partitioning and generating efficient code.
Reference: [CR92] <author> Y-C Chung and S. Ranka, </author> <title> Applications and performance analysis of a compile-time optimization application for list scheduling algorithms on distributed memory multiprocessors, </title> <booktitle> Supercomputing 92, </booktitle> <pages> pp. 512-521. </pages>
Reference-contexts: There exists task dependence within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [YFGS]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [CR92, ELA94, PLW94, PAM93, SK89, WF93, WG88, YG92] </ref>. The main optimization is to minimize parallel time by balancing load, reducing communication overhead, and overlapping communication with computation. <p> We will verify this in our experiments. On the other hand, the theorem indicates that program partitioning that produces ITGs should make g (G) not too small. This is consistent to the previous results <ref> [GY93a, CR92] </ref>. 5 Scheduling for acyclic ITGs If an ITG does not have a dependence cycle, then the ITG itself is a DAG. The following algorithm is used to handle such a graph.
Reference: [Ch89] <author> P. Chretienne, </author> <title> Task Scheduling over Distributed Memory Machines, </title> <booktitle> Proc. of Inter. Workshop on Parallel and Distributed Algorithms, </booktitle> <publisher> (North Holland, Ed.), </publisher> <year> 1989. </year>
Reference-contexts: A trade-off between parallelization and communication locality must be addressed. Because of this reason, the problem of scheduling with communication is much harder than that 5 without communication <ref> [Ch89, SK89] </ref> in terms of complexity. For example, optimal scheduling of a DAG without communication in an unbounded number of processors is straightforward while the problem becomes NP-complete if inter-processor communication is not zero. Secondly, there exists task parallelism within each iteration and loop parallelism across iterations.
Reference: [Ch93] <author> P. Chretienne, </author> <title> Cyclic scheduling with communication delays: a polynomial special case. </title> <month> Dec </month> <year> 1993. </year> <type> Tech Report, </type> <institution> LITP. </institution>
Reference-contexts: What distinguishes our work with that of others is the incorporation of both communication and computation costs in ITG scheduling. The optimal solution for a special class of ITGs has been 2 studied in <ref> [Ch93] </ref>. Our heuristic algorithm is for general ITGs. The proposed algorithm contains the following parts: graph characterization, unfolding, graph transformation, DAG scheduling and the ITG schedule transformation. The optimal loop unfolding factor is derived so that the scheduling performance can be guaranteed compared to the optimal solution.
Reference: [Dun91] <author> T. H. Dunigan, </author> <title> Performance of the INTEL iPSC/860 and nCUBE 6400 Hypercube, </title> <institution> ORNL/TM-11790, Oak Ridge National Lab., TN, </institution> <year> 1991. </year>
Reference-contexts: The cost is denoted as c x;y , which is usually estimated as communication latency + transmission speed fi size of message <ref> [Dun91] </ref>. The scheduling problem for an ITG is to assign the instances of tasks to the given p processors and determine the execution order of tasks within each processor. <p> Our algorithms have not considered the overhead of memory accessing and effect of caching and [WF93] discussed an approach for addressing this issue. The scheduling algorithms we present have not considered the processor distance. It has been shown (e.g. <ref> [Dun91] </ref>) in the modern processor architectures, the distance factor does not affect the cost of communication significantly because of wormhole routing technology. However, this routing scheme requires the exclusive use of channels, and there exists communication contention if network traffic is heavy.
Reference: [DY95] <author> P. Diniz and T. Yang. </author> <title> Efficient Parallelization of Relaxation Iterative Methods for Solving Banded Linear Systems on Multiprocessors, </title> <institution> TRCS94-15, Dept. of CS, UCSB. </institution>
Reference-contexts: We analyze the performance of run-time execution of static schedules when the estimation of task weights at static-time is not accurate. We conduct several experiments to verify our approach. We also investigate the applications of our techniques for scientific computing such as SOR <ref> [DY95, FYG95] </ref>. This paper is organized as follows. Section 2 gives the problem definition and assumptions. Section 3 discusses the possible approaches for mapping ITGs. Section 4 presents the scheduling algorithm and the optimality analysis in details for the cyclic ITGs. <p> Our approach based on software pipelining is essentially to overlap computation and communication of several SOR iterations. We conduct the experiments in using the SOR method for solving banded matrices to compare the multi-coloring approach with our approach <ref> [DY95] </ref>. Table 2 shows the megaflops obtained in nCUBE-2 and Intel Paragon machines (vector units are not used). The left part shows the megaflop performance for fixed matrix size n and p = 64. The right part is for a fixed matrix band width.
Reference: [D92] <author> I. S. Duff, R. G. Grimes and J. G. Lewis, </author> <title> Users' Guide for the Harwell-Boeing Sparse Matrix Collection, </title> <publisher> TR-PA-92-86. </publisher>
Reference-contexts: Some of the edges in this graph are marked distance 1. The unmarked edges have distance 0. We have tested the performance of sparse SOR ITGs. The test matrices are the part of the Harwell-Boeing Test Suites <ref> [D92] </ref>. We use matrix BCSSTK14 arising from structural analysis for the roof of Omni Coliseum at Atlanta and matrix BCSSTK15 for Module of an offshore platform. The simulated performance is shown in the right part of Fig.15.
Reference: [ELA94] <author> H. El-Rewini, T. G. Lewis and H. H. Ali, </author> <title> Task Scheduling in Parallel and Distributed Systems, </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: There exists task dependence within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [YFGS]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [CR92, ELA94, PLW94, PAM93, SK89, WF93, WG88, YG92] </ref>. The main optimization is to minimize parallel time by balancing load, reducing communication overhead, and overlapping communication with computation. <p> Our work has been motivated by the above research but takes into consideration the characteristics of asynchronous parallelism and the impact of communication. We will examine how task graph and loop scheduling techniques can be combined together for mapping iterative task computation on message-passing architectures. <ref> [ELA94] </ref> presented an approach which uses a DAG scheduling algorithm for one iteration of the computation and computes the optimal unfolding for a special case. We will show how to derive the near-optimal unfolding factor for general ITGs. Load balancing and communication minimization are the important aspects of optimization.
Reference: [FYG95] <author> C. Fu, T. Yang, and A. Gerasoulis, </author> <title> Integrating software pipelining and graph scheduling for iterative scientific computing, </title> <booktitle> To appear in Lecture Notes in Computer Science, Proc. of Irregular '95, </booktitle> <address> Lyon, France, </address> <month> Sept </month> <year> 1995. </year>
Reference-contexts: We analyze the performance of run-time execution of static schedules when the estimation of task weights at static-time is not accurate. We conduct several experiments to verify our approach. We also investigate the applications of our techniques for scientific computing such as SOR <ref> [DY95, FYG95] </ref>. This paper is organized as follows. Section 2 gives the problem definition and assumptions. Section 3 discusses the possible approaches for mapping ITGs. Section 4 presents the scheduling algorithm and the optimality analysis in details for the cyclic ITGs.
Reference: [GR89] <author> H. Gabow and R. Tarjan, </author> <title> Faster scaling algorithms for network problems, </title> <journal> SIAM J. Computing, </journal> <month> Oct </month> <year> 1989. </year>
Reference-contexts: These inequalities can be solved in a complexity of O ( p * ) using the shortest path algorithm <ref> [GR89] </ref> where constant * is the desired accuracy in finding the minimum value fi such that fi fl (G) fi fi fl (G) + *. For example, for Fig.3 (a), * is chosen as 0.5 and the value of fi is computed as 40. Granularity.
Reference: [GS92] <author> F. Gasperoni and U. </author> <title> Schweigelshohn Scheduling Loops on Parallel Processors: A simple algorithm with close to optimum performance. </title> <booktitle> Proc. of CONPAR 92 , pp. </booktitle> <pages> 613-624. </pages>
Reference-contexts: The solution to such a problem is also useful for run-time compilation with the inspector/executor approach [SCMB]. Mapping multiple loops are studied in [RA93] for exploring fine grain parallelism. Software pipelining <ref> [AN88, GS92, L88, RE68, VGN92] </ref> is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques [PM91, N88] have also been developed to allow a compiler to explore more parallelism. <p> In Fig. 3 (d), task instances in different iterations are executed at the same time. iterations. * The idea of exploring parallelism within and across iterations has been proposed in the context of instruction-level software pipelining [AN88, L88]. In <ref> [GS92] </ref>, a near-optimal scheduling algorithm for pipelining with no communication delay is proposed. <p> This transformation was first proposed in <ref> [GS92] </ref> for mapping graphs when communication is zero. In Section 4.6, we will show such a transformation is still valid for our case by carefully designing the ITG schedule in Section 4.5. The dependence represented by these deleted edges will be satisfied in the final schedule constructed in Section 4.5. <p> The proof is based on the analysis in <ref> [GS92] </ref>. The main difference is that we consider communication delay and also we add a proof on resource constraint satisfaction. Theorem 2 The algorithm correctly produces a schedule for an ITG. Proof: Let H i = DIV (ff i ; fi f ).
Reference: [GY93a] <author> A. Gerasoulis and T. Yang, </author> <title> On the Granularity and Clustering of Directed Acyclic Task Graphs, </title> <journal> IEEE Trans. on Parallel and Distributed Systems., </journal> <volume> Vol. 4, no. 6, </volume> <month> June </month> <year> 1993, </year> <pages> pp 686-701. </pages>
Reference-contexts: 1 Introduction Parallel programming on distributed memory machines requires coarse grain program partitioning since there is a large startup overhead in message transmission. There are many techniques proposed for program partitioning and granularity control <ref> [CL95, GY93a, SK92, WF89] </ref> and partitioned programs can be represented in a format of coarse-grain task graphs. Finding a mapping of weighted 1 coarse-grain task graphs onto target architectures with the shortest parallel time is important for studying the impact of partitioning and generating efficient code. <p> In the task computation model [SK89, KB88], exploring data locality means to localize data communication between tasks by assigning them in the same processor and then tasks could exchange data through a local memory to avoid high-cost inter-processor communication <ref> [GY93a, SK89] </ref>. The performance of scheduling is also greatly affected by the granularity of task graphs. [GY93a] presents a quantitative analysis on the impact of granularity over the choice of optimal DAG scheduling strategies. <p> The performance of scheduling is also greatly affected by the granularity of task graphs. <ref> [GY93a] </ref> presents a quantitative analysis on the impact of granularity over the choice of optimal DAG scheduling strategies. What distinguishes our work with that of others is the incorporation of both communication and computation costs in ITG scheduling. <p> We will verify this in our experiments. On the other hand, the theorem indicates that program partitioning that produces ITGs should make g (G) not too small. This is consistent to the previous results <ref> [GY93a, CR92] </ref>. 5 Scheduling for acyclic ITGs If an ITG does not have a dependence cycle, then the ITG itself is a DAG. The following algorithm is used to handle such a graph.
Reference: [H89] <author> J. J. Hwang, Y. C. Chow, F. D. Anger, and C. Y. Lee, </author> <title> Scheduling precedence graphs in systems with interprocessor communication times, </title> <journal> SIAM J. Comput., </journal> <pages> pp. 244-257, </pages> <year> 1989. </year>
Reference-contexts: X t i (p 1 + p=g (R)) p fl CP (R)(1 1=p + 1=g (R)): Since p fl P T (R) is equal to the summation of working and idle areas, thus we have P T (R) (1 1=p + 1=g (R))CP (R) + Seq (R)=p: The ETF algorithm <ref> [H89] </ref> has a similar performance bound with a complexity O (pv 2 ) while our algorithm has a lower complexity O ((v + e) log v). 4.5 Schedule Transformation The DAG scheduling produces processor assignment P roc (i) and the starting time fl i for each task T i in K
Reference: [L88] <author> M. Lam, </author> <title> Software pipelining: an effective scheduling technique for VLIW machines, </title> <booktitle> ACM Conf. on Programming Language Design and Implementation, </booktitle> <year> 1988, </year> <pages> 318-328. </pages>
Reference-contexts: The solution to such a problem is also useful for run-time compilation with the inspector/executor approach [SCMB]. Mapping multiple loops are studied in [RA93] for exploring fine grain parallelism. Software pipelining <ref> [AN88, GS92, L88, RE68, VGN92] </ref> is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques [PM91, N88] have also been developed to allow a compiler to explore more parallelism. <p> In Fig. 3 (d), task instances in different iterations are executed at the same time. iterations. * The idea of exploring parallelism within and across iterations has been proposed in the context of instruction-level software pipelining <ref> [AN88, L88] </ref>. In [GS92], a near-optimal scheduling algorithm for pipelining with no communication delay is proposed.
Reference: [KB88] <author> S.J. Kim and J.C. Browne, </author> <title> A General Approach to Mapping of Parallel Computation upon Multiprocessor Architectures, </title> <booktitle> Proc. of Inter. Conf. on Parallel Processing, 1988, V3, </booktitle> <pages> 1-8. 31 </pages>
Reference-contexts: We will show how to derive the near-optimal unfolding factor for general ITGs. Load balancing and communication minimization are the important aspects of optimization. It is known that data locality must be explored for reducing unnecessary communication. In the task computation model <ref> [SK89, KB88] </ref>, exploring data locality means to localize data communication between tasks by assigning them in the same processor and then tasks could exchange data through a local memory to avoid high-cost inter-processor communication [GY93a, SK89].
Reference: [MG89] <author> C. McCreary and H. Gill, </author> <title> Automatic Determination of Grain Size for Efficient Parallel Processing, </title> <journal> Communications of ACM, </journal> <volume> vol. 32, </volume> <pages> pp. 1073-1078, </pages> <month> Sept., </month> <year> 1989. </year>
Reference: [N88] <author> A. Nicolau, </author> <title> Loop quantization: A generalized loop unwinding techniques, </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 5(1988), </volume> <pages> 568-586. </pages>
Reference-contexts: Mapping multiple loops are studied in [RA93] for exploring fine grain parallelism. Software pipelining [AN88, GS92, L88, RE68, VGN92] is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques <ref> [PM91, N88] </ref> have also been developed to allow a compiler to explore more parallelism. Our work has been motivated by the above research but takes into consideration the characteristics of asynchronous parallelism and the impact of communication. <p> The parallel time is 290. 1 Our results can be extended for inexact dependence values such as `+' and `*' used in the literature [PKK93, P88, WF89] Gantt chart of a schedule when N = 3 for (b). Loop unrolling or graph unfolding techniques <ref> [PM91, N88] </ref> have been found useful to explore more parallelism. Let f be the unfolding factor. When a graph is unfolded f times, the number of tasks increases by a factor of f and the number of iterations for the new ITG is bN=f c. <p> In [GS92], a near-optimal scheduling algorithm for pipelining with no communication delay is proposed. We need to extend this 6 result to incorporate communication optimization with load balancing since communication is a major overhead in a message-passing machine. * As we mentioned above, graph unfolding techniques <ref> [PM91, N88] </ref> have been also developed. Unfolding can increase the number of tasks within each iteration and thus more parallelism could be explored. Our goal is to only examine tasks from few iterations (limited by the unfolding factor f ) but explore parallelism of the entire iteration space.
Reference: [PLW94] <author> M. A. Palis, J.-C. Liou; Wei, D.S.L. </author> <title> A greedy task clustering heuristic that is provably good. </title> <booktitle> Proceedings of the 1994 International Symposium on Parallel Architectures, Algorithms and Networks (ISPAN). IEEE 1994. p. </booktitle> <pages> 398-405. </pages>
Reference-contexts: There exists task dependence within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [YFGS]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [CR92, ELA94, PLW94, PAM93, SK89, WF93, WG88, YG92] </ref>. The main optimization is to minimize parallel time by balancing load, reducing communication overhead, and overlapping communication with computation.
Reference: [PM91] <author> K. K. Parhi and D. G. Messerschmitt, </author> <title> Static rate-optimal scheduling of iterative dataflow programs via optimum unfolding, </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40:2, </volume> <year> 1991, </year> <pages> pp. 178-195. </pages>
Reference-contexts: Mapping multiple loops are studied in [RA93] for exploring fine grain parallelism. Software pipelining [AN88, GS92, L88, RE68, VGN92] is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques <ref> [PM91, N88] </ref> have also been developed to allow a compiler to explore more parallelism. Our work has been motivated by the above research but takes into consideration the characteristics of asynchronous parallelism and the impact of communication. <p> The parallel time is 290. 1 Our results can be extended for inexact dependence values such as `+' and `*' used in the literature [PKK93, P88, WF89] Gantt chart of a schedule when N = 3 for (b). Loop unrolling or graph unfolding techniques <ref> [PM91, N88] </ref> have been found useful to explore more parallelism. Let f be the unfolding factor. When a graph is unfolded f times, the number of tasks increases by a factor of f and the number of iterations for the new ITG is bN=f c. <p> In [GS92], a near-optimal scheduling algorithm for pipelining with no communication delay is proposed. We need to extend this 6 result to incorporate communication optimization with load balancing since communication is a major overhead in a message-passing machine. * As we mentioned above, graph unfolding techniques <ref> [PM91, N88] </ref> have been also developed. Unfolding can increase the number of tasks within each iteration and thus more parallelism could be explored. Our goal is to only examine tasks from few iterations (limited by the unfolding factor f ) but explore parallelism of the entire iteration space. <p> This graph is called G f . The new graph needs to be executed only bN=f c iterations. We use the unfolding algorithm proposed in <ref> [PM91] </ref> to construct G f . The new graph G f contains f fl v nodes such that each node T i in G has f nodes in G f : T i;1 ; T i;2 ; ; T i;f . <p> For example, unfolding the ITG in Fig. 3 (a) with f = 2 results in an ITG shown in Fig. 3 (b). Notice the unfolded graph has the following properties <ref> [PM91] </ref>: fi fl (G f ) = f fi fl (G). 4.3 Graph transformation We transform G f into a DAG so that the DAG scheduling technique could be applied. Define DIV (x; y) as the largest integer that is less than or equal to x=y, i.e. bx=yc.
Reference: [PAM93] <author> S. S. Pande, D. P. Agrawal and J. Mauney, </author> <title> A scalable scheduling scheme for functional parallelism on distributed memory multiprocessor systems. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <month> April </month> <year> 1995, </year> <note> vol.6, (no.4):388-99. </note>
Reference-contexts: There exists task dependence within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [YFGS]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [CR92, ELA94, PLW94, PAM93, SK89, WF93, WG88, YG92] </ref>. The main optimization is to minimize parallel time by balancing load, reducing communication overhead, and overlapping communication with computation.
Reference: [P88] <author> C. D. Polychronopoulos, </author> <title> Parallel Programming and Compilers, </title> <publisher> Kluwer Academic, </publisher> <year> 1988. </year>
Reference-contexts: Using DAG scheduling algorithms for ITGs is not feasible since the number of iterations may be too large or may not even be known at compile-time. Loop parallelism can be uncovered by transformation methods <ref> [P88, SK92, WF89] </ref> and various loop scheduling techniques have been proposed. Self-scheduling (e.g. [P88]) is a dynamic method for DOALL parallelism when task weights are not predictable at static time. <p> Using DAG scheduling algorithms for ITGs is not feasible since the number of iterations may be too large or may not even be known at compile-time. Loop parallelism can be uncovered by transformation methods [P88, SK92, WF89] and various loop scheduling techniques have been proposed. Self-scheduling (e.g. <ref> [P88] </ref>) is a dynamic method for DOALL parallelism when task weights are not predictable at static time. Compared to this work, we are interested in computation in which there exist dependencies between tasks; task weights are predictable statically and do not change significantly at run-time. <p> The expanded graph for Fig. 3 (b) is in (c). A schedule for this graph when N = 3 is depicted in Fig. 3 (d). The parallel time is 290. 1 Our results can be extended for inexact dependence values such as `+' and `*' used in the literature <ref> [PKK93, P88, WF89] </ref> Gantt chart of a schedule when N = 3 for (b). Loop unrolling or graph unfolding techniques [PM91, N88] have been found useful to explore more parallelism. Let f be the unfolding factor.
Reference: [PKK93] <author> K. Psarris, X. Kong, and D. Klappholz. </author> <title> The Direction Vector I Test. </title> <journal> IEEE Trans. on Parallel and Disributed Systems, </journal> <volume> Vol. 4, No. 11, </volume> <month> November </month> <year> 1993, </year> <pages> pp. 1280-1290. </pages>
Reference-contexts: The expanded graph for Fig. 3 (b) is in (c). A schedule for this graph when N = 3 is depicted in Fig. 3 (d). The parallel time is 290. 1 Our results can be extended for inexact dependence values such as `+' and `*' used in the literature <ref> [PKK93, P88, WF89] </ref> Gantt chart of a schedule when N = 3 for (b). Loop unrolling or graph unfolding techniques [PM91, N88] have been found useful to explore more parallelism. Let f be the unfolding factor.
Reference: [RA93] <author> J. Ramanujam, </author> <title> Multiple dimension software pipelining. </title> <booktitle> Proc. of IPPS 94. </booktitle>
Reference-contexts: The solution to such a problem is also useful for run-time compilation with the inspector/executor approach [SCMB]. Mapping multiple loops are studied in <ref> [RA93] </ref> for exploring fine grain parallelism. Software pipelining [AN88, GS92, L88, RE68, VGN92] is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques [PM91, N88] have also been developed to allow a compiler to explore more parallelism.
Reference: [RE68] <author> R. Reiter, </author> <title> Scheduling parallel computations, </title> <journal> Journal of ACM, </journal> <month> Oct </month> <year> 1968, </year> <pages> pp. 590-599. </pages>
Reference-contexts: The solution to such a problem is also useful for run-time compilation with the inspector/executor approach [SCMB]. Mapping multiple loops are studied in [RA93] for exploring fine grain parallelism. Software pipelining <ref> [AN88, GS92, L88, RE68, VGN92] </ref> is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques [PM91, N88] have also been developed to allow a compiler to explore more parallelism. <p> We need to derive a small unfolding factor since the complexity of scheduling could be too high when the unfolding factor is large. We will use the concept of integer-periodic scheduling <ref> [RE68] </ref> as in instruction-level software pipelin-ing and structure a schedule as follows: all instances of task T i are assigned to the same processor. <p> This is the well-known optimal rate (the smallest iteration interval) of the pipelining when the communication cost is zero and there are a sufficient number of processors <ref> [RE68] </ref>.
Reference: [SCMB] <author> Saltz, J., Crowley, K., Mirchandaney, R. and Barryman,H., </author> <title> Run-time scheduling and execution of loops on message passing machines, </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> Vol. 8, </volume> <year> 1990, </year> <pages> pp. 303-312. </pages>
Reference-contexts: Compared to this work, we are interested in computation in which there exist dependencies between tasks; task weights are predictable statically and do not change significantly at run-time. The solution to such a problem is also useful for run-time compilation with the inspector/executor approach <ref> [SCMB] </ref>. Mapping multiple loops are studied in [RA93] for exploring fine grain parallelism. Software pipelining [AN88, GS92, L88, RE68, VGN92] is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures.
Reference: [SK89] <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors, </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: There exists task dependence within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [YFGS]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [CR92, ELA94, PLW94, PAM93, SK89, WF93, WG88, YG92] </ref>. The main optimization is to minimize parallel time by balancing load, reducing communication overhead, and overlapping communication with computation. <p> We will show how to derive the near-optimal unfolding factor for general ITGs. Load balancing and communication minimization are the important aspects of optimization. It is known that data locality must be explored for reducing unnecessary communication. In the task computation model <ref> [SK89, KB88] </ref>, exploring data locality means to localize data communication between tasks by assigning them in the same processor and then tasks could exchange data through a local memory to avoid high-cost inter-processor communication [GY93a, SK89]. <p> In the task computation model [SK89, KB88], exploring data locality means to localize data communication between tasks by assigning them in the same processor and then tasks could exchange data through a local memory to avoid high-cost inter-processor communication <ref> [GY93a, SK89] </ref>. The performance of scheduling is also greatly affected by the granularity of task graphs. [GY93a] presents a quantitative analysis on the impact of granularity over the choice of optimal DAG scheduling strategies. <p> Their dependence structure is expressed as a directed acyclic graph (DAG). A task is an indivisible unit of computation which reads its input data, it performs operations and then produces new data <ref> [SK89, WG88] </ref>. The scheduling problem is to find the mapping of tasks to processors and assign the execution starting times to those tasks. Fig. 1 shows an example of task parallelism with five tasks and their schedule on 2 processors. <p> A trade-off between parallelization and communication locality must be addressed. Because of this reason, the problem of scheduling with communication is much harder than that 5 without communication <ref> [Ch89, SK89] </ref> in terms of complexity. For example, optimal scheduling of a DAG without communication in an unbounded number of processors is straightforward while the problem becomes NP-complete if inter-processor communication is not zero. Secondly, there exists task parallelism within each iteration and loop parallelism across iterations.
Reference: [SK92] <author> V. Sarkar and R. Thekkath, </author> <title> A general framework for iteration-reordering loop transformations, </title> <booktitle> ACM Conf. on Programming Language Design and Implementation, </booktitle> <year> 1992, </year> <pages> 175-187. </pages>
Reference-contexts: 1 Introduction Parallel programming on distributed memory machines requires coarse grain program partitioning since there is a large startup overhead in message transmission. There are many techniques proposed for program partitioning and granularity control <ref> [CL95, GY93a, SK92, WF89] </ref> and partitioned programs can be represented in a format of coarse-grain task graphs. Finding a mapping of weighted 1 coarse-grain task graphs onto target architectures with the shortest parallel time is important for studying the impact of partitioning and generating efficient code. <p> Using DAG scheduling algorithms for ITGs is not feasible since the number of iterations may be too large or may not even be known at compile-time. Loop parallelism can be uncovered by transformation methods <ref> [P88, SK92, WF89] </ref> and various loop scheduling techniques have been proposed. Self-scheduling (e.g. [P88]) is a dynamic method for DOALL parallelism when task weights are not predictable at static time.
Reference: [VGN92] <author> V. H. Van Dongen, G. R. Gao and Q. </author> <title> Ning A polynomial time method for optimal software pipelining. </title> <booktitle> Proc. of CONPAR 92, </booktitle> <pages> pp. 613-624. </pages>
Reference-contexts: The solution to such a problem is also useful for run-time compilation with the inspector/executor approach [SCMB]. Mapping multiple loops are studied in [RA93] for exploring fine grain parallelism. Software pipelining <ref> [AN88, GS92, L88, RE68, VGN92] </ref> is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques [PM91, N88] have also been developed to allow a compiler to explore more parallelism.
Reference: [WF89] <author> M. Wolfe, </author> <title> Optimizing Supercompilers for Supercomputers, </title> <publisher> MIT Press, </publisher> <address> Boston, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Parallel programming on distributed memory machines requires coarse grain program partitioning since there is a large startup overhead in message transmission. There are many techniques proposed for program partitioning and granularity control <ref> [CL95, GY93a, SK92, WF89] </ref> and partitioned programs can be represented in a format of coarse-grain task graphs. Finding a mapping of weighted 1 coarse-grain task graphs onto target architectures with the shortest parallel time is important for studying the impact of partitioning and generating efficient code. <p> Using DAG scheduling algorithms for ITGs is not feasible since the number of iterations may be too large or may not even be known at compile-time. Loop parallelism can be uncovered by transformation methods <ref> [P88, SK92, WF89] </ref> and various loop scheduling techniques have been proposed. Self-scheduling (e.g. [P88]) is a dynamic method for DOALL parallelism when task weights are not predictable at static time. <p> The expanded graph for Fig. 3 (b) is in (c). A schedule for this graph when N = 3 is depicted in Fig. 3 (d). The parallel time is 290. 1 Our results can be extended for inexact dependence values such as `+' and `*' used in the literature <ref> [PKK93, P88, WF89] </ref> Gantt chart of a schedule when N = 3 for (b). Loop unrolling or graph unfolding techniques [PM91, N88] have been found useful to explore more parallelism. Let f be the unfolding factor.
Reference: [WF93] <author> R. Wolski and J. Feo, </author> <title> Program Partitioning for NUMA Multiprocessor Computer Systems, </title> <type> Tech. Report, </type> <institution> Lawrence Livermore Nat. Lab., </institution> <year> 1992. </year> <editor> J. </editor> <booktitle> of Parallel and Distributed Computing, </booktitle> <year> 1993. </year>
Reference-contexts: There exists task dependence within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [YFGS]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [CR92, ELA94, PLW94, PAM93, SK89, WF93, WG88, YG92] </ref>. The main optimization is to minimize parallel time by balancing load, reducing communication overhead, and overlapping communication with computation. <p> Currently we are implementing a run-time support system for efficiently executing graph schedules. [CFH95] proposed a hierarchical tiling framework and our work is useful and can be extended for assisting performance prediction. Our algorithms have not considered the overhead of memory accessing and effect of caching and <ref> [WF93] </ref> discussed an approach for addressing this issue. The scheduling algorithms we present have not considered the processor distance. It has been shown (e.g. [Dun91]) in the modern processor architectures, the distance factor does not affect the cost of communication significantly because of wormhole routing technology.
Reference: [WG88] <author> M. Y. Wu and D. Gajski, Hypertool: </author> <title> A programming aid for message-passing systems, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 1, no. 3, pp.330-343, </volume> <year> 1990. </year>
Reference-contexts: There exists task dependence within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [YFGS]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [CR92, ELA94, PLW94, PAM93, SK89, WF93, WG88, YG92] </ref>. The main optimization is to minimize parallel time by balancing load, reducing communication overhead, and overlapping communication with computation. <p> Their dependence structure is expressed as a directed acyclic graph (DAG). A task is an indivisible unit of computation which reads its input data, it performs operations and then produces new data <ref> [SK89, WG88] </ref>. The scheduling problem is to find the mapping of tasks to processors and assign the execution starting times to those tasks. Fig. 1 shows an example of task parallelism with five tasks and their schedule on 2 processors.
Reference: [YG94] <author> T. Yang and A. Gerasoulis. </author> <title> DSC: Scheduling parallel tasks on an unbounded number of processors, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 5, No. 9, </volume> <pages> 951-967, </pages> <year> 1994. </year>
Reference-contexts: The result of the DAG scheduling is used for constructing the ITG schedule in Section 4.5. The first algorithm involves the following multiple stages in scheduling: 1) It clusters tasks with intensive communication using the DSC algorithm <ref> [YG94] </ref>. 2) It merges clusters to p processors using a load balancing algorithm. 3) It reorders tasks within each processor to overlap computation with communication. <p> However in practice, we find that f is small, thus f is not included in the complexity term. 10 In the one-stage approach, we use the idea of the DSC algorithm <ref> [YG94] </ref> but limit the number of clusters to p. The algorithm first computes the blevel value for each task where blevel (T x ) is the length of the longest path from this task T x to an exit node. <p> If a tie occurs, we will pick the latter one. The tlevel value of the selected task will be the latest starting time fl x of this task unless there are no processors available. The minimization procedure is also used in DSC ( <ref> [YG94] </ref>, page 958). The idea is to further reduce the starting time of the selected task by merging one or more parents of this examined task to one processor. In this way the unnecessary communication could be saved. same processor which leads to the minimum starting time of this task.
Reference: [YG93] <author> T. Yang and A. Gerasoulis. </author> <title> List scheduling with and without communication. </title> <booktitle> Parallel Computing, V. 19 (1993) pp. </booktitle> <pages> 1321-1344. </pages>
Reference: [YG92] <author> T. Yang and A. Gerasoulis, </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message-Passing Multiprocessors, </title> <booktitle> Proc. of 6th ACM Inter. Confer. on Supercomputing, </booktitle> <address> Washington D.C., </address> <year> 1992, </year> <pages> pp. 428-437. </pages>
Reference-contexts: There exists task dependence within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [YFGS]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [CR92, ELA94, PLW94, PAM93, SK89, WF93, WG88, YG92] </ref>. The main optimization is to minimize parallel time by balancing load, reducing communication overhead, and overlapping communication with computation. <p> The starting time fl i and the processor assignment P roc (i) of each task T i are provided. In mapping the kernel DAG, we use two algorithms: one is a multi-stage algorithm designed for the PYRROS system <ref> [YG92] </ref>, another is a one-stage algorithm. We will choose the smaller one between two solutions produced by these algorithms. The result of the DAG scheduling is used for constructing the ITG schedule in Section 4.5. <p> When executing each individual task, the task receives the data items it needs, performs computation, and then sends out data produced to its children (using aggregate multicasting if possible). The style is similar to the one used in the PYRROS system <ref> [YG92] </ref>. Next we examine the performance of this code structure and provide an analysis on performance variation between predicted time and actual run-time performance. 21 Let o max = max T x 2T asks (j) o x . o min = min T x 2T asks (j) o x . <p> However, this routing scheme requires the exclusive use of channels, and there exists communication contention if network traffic is heavy. Assigning communication-intensive tasks close to each other in terms of processor distances would reduce the chance of contention. The communication contention is hard to model. In <ref> [YG92] </ref>, the processor distance information is incorporated in computing communication volume after virtual processor assignments of tasks in a DAG are determined. Then physical mapping can be adjusted [B90]. We are investigating a similar method for mapping ITGs.
Reference: [YFGS] <author> T. Yang, C. Fu, A. Gerasoulis and V. Sarkar, </author> <title> Mapping iterative task graphs on distributed-memory machines. </title> <booktitle> To appear in Proc. of 24th Inter. Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1995. </year> <month> 32 </month>
Reference-contexts: An iterative task graph (ITG) represents a sequence of task computations where each iteration consists of the execution of a set of tasks. There exists task dependence within an iteration and across iterations. Iterative task computations involve both task and loop parallelism <ref> [YFGS] </ref>. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. [CR92, ELA94, PLW94, PAM93, SK89, WF93, WG88, YG92]. The main optimization is to minimize parallel time by balancing load, reducing communication overhead, and overlapping communication with computation.
References-found: 40

