URL: http://nl.ijs.si/telri-wg4/wg5rep1.ps.gz
Refering-URL: http://nl.ijs.si/telri-wg4/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Keyword: Corpus Tool Identification Tomaz Erjavec Language Speech Group  Keywords: public domain tools, language engineering, corpus lingusitcs, standards  
Date: August 12, 1996  
Address: Ljubljana, Slovenia  
Affiliation: Intelligent Systems Dept. Jozef Stefan Institute  
Abstract: TELRI WG5 Report: Abstract This report gives an overview and pointers to public domain and to freely available software that can be used for corpus creation and exploitation. The focus is on tools that are available via the World Wide Web for the Unix platform. First are presented some SGML tools, then tools for legacy data up-conversion. This is followed by tools for linguistic annotation, in particular part-of-speech taggers and aligners for parallel corpora. Finaly, some corpus exploitation tools, i.e. concordancers are presented. Discussed is the relation of tools to standards, in particular SGML, and the benefits and disadvantages of using public domain tools. Given is an overview of a number of generic string processing and corpus conversion tools of statistically based annotations systems and computational linguistic software. Some on-going initiatives on production, standardisation and availability of language tools are mentioned and a number of Web sites, related to the discussed topics are listed. The hypertext version of this report can be found at the URL http://nl.ijs.si/telri-wg5/wg5rep1. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Brian W. Kernighan, and Peter J. Weinberger. </author> <title> The Awk Programming Language. </title> <publisher> Addison Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1988. </year> <note> 33 http://www.lpl.univ-aix.fr/projects/multext/LSD/LSD1.html 34 http://www.lpl.univ-aix.fr/projects/multext/ 35 http://nl.ijs.si/ME/ 36 http://www.ilc.pi.cnr.it/EAGLES/home.html 37 http://www.ltg.hcrc.ed.ac.uk/projects/helpdesk/ 13 </note>
Reference-contexts: The general purpose programming language which is currently (and probably for a while to come) the de facto standard is ANSI C and it's object oriented extension C++. Unix also offers a number of languages that are particularly suited for string processing, such as sed [10] and awk <ref> [1] </ref>; particular mention deserves Perl [21], which is suitable as much for writing short, throw-away programs as for complex conversion tasks.
Reference: [2] <author> Evan L. Antworth. PC-KIMMO: </author> <title> a Two-level Processor for Morpho--logical Analysis. </title> <booktitle> Number 16 in Occasional Publications in Academic Computing. Summer Institute in Linguistics, </booktitle> <address> Dallas, Texas, </address> <year> 1990. </year>
Reference-contexts: For morphological analysis and synthesis, by far the most widely used and investigated is Koskenniemi's finite-state two-level model. It is primarily meant to deal with spelling changes at or near morpheme boundaries. The best known implementation is probably PC-KIMMO 28 <ref> [2] </ref>, although a number of other implementations also exist. For general lexical structuring including but not limited to morphological dependencies, a simple, yet powerful and efficient language is DATR [13]. DATR is a lexical knowledge representation language in which it is possible to define networks allowing multiple default inheritance.
Reference: [3] <author> Eric Brill. </author> <title> A simple rule-based part of speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing, ACL, </booktitle> <address> Trento, Italy, </address> <year> 1992. </year>
Reference-contexts: The best known is Brill's rule based tagger 20 <ref> [3, 4] </ref>. In the training phase, this tagger makes an initial hypothesis about the correct tags. In an iterative fashion it then betters its performance with regard to the training corpus by postulating context dependent tag rewrite rules.
Reference: [4] <author> Eric Brill. </author> <title> Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. </title> <journal> Computational Linguistics, </journal> <volume> 21(4) </volume> <pages> 543-565, </pages> <year> 1995. </year>
Reference-contexts: The best known is Brill's rule based tagger 20 <ref> [3, 4] </ref>. In the training phase, this tagger makes an initial hypothesis about the correct tags. In an iterative fashion it then betters its performance with regard to the training corpus by postulating context dependent tag rewrite rules.
Reference: [5] <author> Bob Carpenter and Gerald Penn. </author> <note> ALE user's guide version 2.0. Laboratory for computational linguistics technical report, </note> <institution> Carnegie Mellon University, Pittsburgh, </institution> <year> 1994. </year>
Reference-contexts: There are a number of such systems available, pointers to which can be, again, found at the DFKI Web page. Here we will mention only three of the better known ones. The Attribute Logic Engine, ALE 31 <ref> [5] </ref> is written in Prolog and incorporates a chart parser and lexical rules. It is optimised for speed of processing which, however, makes it less than ideal for a grammar development environment.
Reference: [6] <author> Eugene Charniak. </author> <title> Statistical Language Learning. Language and Computers 12. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The most widely implemented approach to stochastic tagging is the one that uses Hidden Markov Models (HMM). The theory behind this approach is 12 http://www.falch.no/people/pepper/sgmltool/ 13 http://www.ltg.hcrc.ed.ac.uk/ 14 http://www.ltg.hcrc.ed.ac.uk/software/lt nsl.html 7 explained e.g. in <ref> [6] </ref>. Such taggers have the putative advantage that they be trained on an untagged text, using the so called Baum-Welch algorithm. However, experience shows (cf. [11]), that the results obtained with such training are low, and that training on even a small pre-tagged corpus gives better results.
Reference: [7] <author> Oliver Christ. </author> <title> A modular and flexible architecture for an integrated corpus query system. </title> <booktitle> In Proceedings of COMPLEX '94: 3rd conference on Computational Lexicography and Text Research, </booktitle> <address> Budapest, Hungary, </address> <year> 1994. </year> <note> CMP-LG archive id 9408005. </note>
Reference-contexts: It seems that the systems listed below are available free or charge, but in all cases they can be obtained only by getting in touch with the developers. Stuttgart's Institute fur Maschinelle Sprachverarbeitung (IMS) developed the Corpus Query System cqp/Xkwic 22 <ref> [7] </ref>. The corpus query processor cqp is a command-language based query interpreter, which can be used independently or by Xkwic, which is a X-windows graphical user interface. The cqp system supports a restricted set of annotations and parallel corpora, but not SGML syntax.
Reference: [8] <author> D. Cutting, J. Kupiec, J. Pedersen, and P. Sibun. </author> <title> A practical part-of-speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing, </booktitle> <pages> pages 133-140, </pages> <address> Trento, Italy, </address> <year> 1992. </year>
Reference-contexts: The taggers that have source code and usually accompanying documentation available via the Internet are the following: 15 * The Xerox tagger 16 is described in <ref> [8] </ref> and implemented in Common Lisp.
Reference: [9] <author> Jochen Dorre and Andreas Eisele. </author> <title> A comprehensive unification-based grammar formalism. </title> <type> Technical Report Deliverable DYANA R3.1.B, </type> <institution> Centre for Cognitive Science, Edinburgh, </institution> <year> 1991. </year>
Reference-contexts: The Attribute Logic Engine, ALE 31 [5] is written in Prolog and incorporates a chart parser and lexical rules. It is optimised for speed of processing which, however, makes it less than ideal for a grammar development environment. IMS offers two systems 32 : Comprehensive Unification Grammar (CUF) <ref> [9] </ref> written in Prolog, and Typed Feature Formalism (TFS) [22] in Lisp. Especially CUF offers a very powerful grammar development environment. For a detailed comparison between ALE, CUF and TFS see also [18].
Reference: [10] <author> Dale Doucherty. sed & awk. O'Reilly & Associates, Sebastopo, Califor-nia, </author> <year> 1991. </year>
Reference-contexts: The general purpose programming language which is currently (and probably for a while to come) the de facto standard is ANSI C and it's object oriented extension C++. Unix also offers a number of languages that are particularly suited for string processing, such as sed <ref> [10] </ref> and awk [1]; particular mention deserves Perl [21], which is suitable as much for writing short, throw-away programs as for complex conversion tasks.
Reference: [11] <author> D. Elworthy. </author> <title> Does baum-welch reestimation help taggers? In Proceedings of the ACL SIGDAT workshop From Text to Tags: Issues in Multilingual Language Analysis, </title> <address> Dublin, </address> <year> 1995. </year> <month> forthcoming. </month>
Reference-contexts: The theory behind this approach is 12 http://www.falch.no/people/pepper/sgmltool/ 13 http://www.ltg.hcrc.ed.ac.uk/ 14 http://www.ltg.hcrc.ed.ac.uk/software/lt nsl.html 7 explained e.g. in [6]. Such taggers have the putative advantage that they be trained on an untagged text, using the so called Baum-Welch algorithm. However, experience shows (cf. <ref> [11] </ref>), that the results obtained with such training are low, and that training on even a small pre-tagged corpus gives better results.
Reference: [12] <author> Tomaz Erjavec. </author> <title> Public domain generic tools: an overview. </title> <booktitle> In Proceedings of the First European TELRI Seminar: Language Resources for Language Technology, </booktitle> <pages> pages 37-48, </pages> <year> 1996. </year> <month> 15-16 September </month> <year> 1995, </year> <note> Tihany, Hungary. 14 </note>
Reference-contexts: 1 Introduction This report is an extended version of the paper presented at the first Euro-pean TELRI Seminar: "Language Resources for Language Technology" <ref> [12] </ref>. The report gives an introduction to language engineering software, especially as it relates to computerised textual corpora. The focus of the paper is on language engineering tools, i.e. relatively small and independent pieces of software, meant for a particular, usually low-level task.
Reference: [13] <author> Roger Evans and Gerald Gazdar. </author> <title> The DATR papers. </title> <institution> Cognitive Science Research Paper CSRP-139, University of Sussex, </institution> <address> Brighton, </address> <year> 1990. </year>
Reference-contexts: The best known implementation is probably PC-KIMMO 28 [2], although a number of other implementations also exist. For general lexical structuring including but not limited to morphological dependencies, a simple, yet powerful and efficient language is DATR <ref> [13] </ref>. DATR is a lexical knowledge representation language in which it is possible to define networks allowing multiple default inheritance. The original Sussex version 29 , is publicly available and is implemented in Prolog.
Reference: [14] <author> William Gale and Ken W. Church. </author> <title> A program for aligning sentences in bilingual corpora. </title> <journal> Computational Linguistics, </journal> <volume> 19(1) </volume> <pages> 75-102, </pages> <year> 1993. </year>
Reference-contexts: Such an alignment is complicated by the fact that certain sentences of the translation might correspond to more (or less) than one sentence of the original. An interesting and available tool for such aligning is the Gale and Church aligner, <ref> [14] </ref>. It's advantage is that is needs no linguistic resources, but makes use of the simple insight that a text and its translation will have roughly the same number of characters. In spite of this simple mechanism, it purportedly produces relatively good results.
Reference: [15] <author> Charles F. Goldfarb. </author> <title> The SGML Handbook. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1990. </year>
Reference-contexts: In addition, the growing acceptance of certain standards that enable the exchange and platform independence of corpora encodings have also had a important impact on corpora availability and reuse. In particular, the Text Encoding Initiative guidelines, TEI 3 [20, 17], which adopt the ISO standard SGML 4 <ref> [15] </ref> as their markup (meta)language are a significant contribution to the standardisation effort in this area. This ease of availability and adoption of standards is important not only for corpora themselves, but also for software that helps in producing and, to a lesser extent, utilising these corpora.
Reference: [16] <author> Ralph E. Griswold and Madge T. Griswold. </author> <title> The Icon Programming Language. </title> <publisher> Prentice Hall, </publisher> <address> New Jersey, </address> <note> second edition, </note> <year> 1990. </year>
Reference-contexts: Another general purpose programming language which is in the public domain and particularly suited to the manipulation of character strings is Icon <ref> [16] </ref>. It was developed at the University of Arizona, and was extensively used in the British National Corpus project. The basic SGML tool is the validating parser that checks for syntactic well-formedness of SGML documents and reports errors in case the document is not well-formed.
Reference: [17] <author> Nancy Ide and Jean Veronis, </author> <title> editors. The Text Encoding Initiative: Background and Context. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, </address> <year> 1995. </year>
Reference-contexts: In addition, the growing acceptance of certain standards that enable the exchange and platform independence of corpora encodings have also had a important impact on corpora availability and reuse. In particular, the Text Encoding Initiative guidelines, TEI 3 <ref> [20, 17] </ref>, which adopt the ISO standard SGML 4 [15] as their markup (meta)language are a significant contribution to the standardisation effort in this area.
Reference: [18] <editor> Suresh Manandhar. CUF in context. In Jochen Dorre, editor, </editor> <title> Computational Aspects of Constraint-Based Linguistics Description. </title> <institution> ILLC/Department of Philosophy, University of Amsterdam, </institution> <year> 1993. </year>
Reference-contexts: IMS offers two systems 32 : Comprehensive Unification Grammar (CUF) [9] written in Prolog, and Typed Feature Formalism (TFS) [22] in Lisp. Especially CUF offers a very powerful grammar development environment. For a detailed comparison between ALE, CUF and TFS see also <ref> [18] </ref>.
Reference: [19] <author> Stuart Shieber, Hans Uszkoreit, J. Robinson, and M. Tyson. </author> <title> The formalism and implementation of PATR-II. </title> <booktitle> In Research on Interactive Acquisition and Use of Knowledge, </booktitle> <pages> pages 39-79. </pages> <institution> AI Center, SRI International, Menlo Park, Cal., </institution> <year> 1983. </year>
Reference-contexts: Apart from DCG, the best known unification-based context-free parser is the PATR system <ref> [19] </ref>. A version of PATR called PC-PATR is avaliable from the Summer Institute of Linguistics 30 . More recent unification-based systems have replaced untyped feature structures of PATR with typed ones, thus conferring the benefits of type checking and type inheritance to their grammars.
Reference: [20] <author> C. M. Sperberg-McQueen and Lou Burnard, </author> <title> editors. Guidelines for Electronic Text Encoding and Interchange. </title> <publisher> Chicago and Oxford, </publisher> <year> 1994. </year>
Reference-contexts: In addition, the growing acceptance of certain standards that enable the exchange and platform independence of corpora encodings have also had a important impact on corpora availability and reuse. In particular, the Text Encoding Initiative guidelines, TEI 3 <ref> [20, 17] </ref>, which adopt the ISO standard SGML 4 [15] as their markup (meta)language are a significant contribution to the standardisation effort in this area.
Reference: [21] <author> Larry Wall and Randal L. Schwartz. </author> <title> Programming Perl. </title> <publisher> O'Reilly & Associates, </publisher> <address> Sebastopol, California, </address> <year> 1991. </year>
Reference-contexts: Unix also offers a number of languages that are particularly suited for string processing, such as sed [10] and awk [1]; particular mention deserves Perl <ref> [21] </ref>, which is suitable as much for writing short, throw-away programs as for complex conversion tasks.
Reference: [22] <author> Remi Zajac. </author> <title> Inheritance and constraint-based grammar formalisms. </title> <journal> Computational Linguistics, </journal> <volume> 18(2), </volume> <year> 1992. </year> <month> 15 </month>
Reference-contexts: It is optimised for speed of processing which, however, makes it less than ideal for a grammar development environment. IMS offers two systems 32 : Comprehensive Unification Grammar (CUF) [9] written in Prolog, and Typed Feature Formalism (TFS) <ref> [22] </ref> in Lisp. Especially CUF offers a very powerful grammar development environment. For a detailed comparison between ALE, CUF and TFS see also [18].
References-found: 22

