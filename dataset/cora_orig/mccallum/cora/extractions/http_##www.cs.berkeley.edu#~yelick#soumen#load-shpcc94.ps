URL: http://www.cs.berkeley.edu/~yelick/soumen/load-shpcc94.ps
Refering-URL: http://www.cs.berkeley.edu/~yelick/papers.html
Root-URL: 
Email: fsoumen,ranade,yelickg@cs.berkeley.edu  
Title: In the 1994 IEEE Scalable High Performance Computing Conference, Knoxville, TN. Randomized Load Balancing for
Author: Soumen Chakrabarti Abhiram Ranade Katherine Yelick 
Address: Berkeley CA 94720  
Affiliation: Computer Science Division, University of California,  
Abstract: In this paper, we study the performance of a randomized algorithm for balancing load across a multiprocessor executing a dynamic irregular task tree. Specifically, we show that the time taken to explore a task tree is likely to be within a small constant factor of an inherent lower bound for the tree instance. Our model permits arbitrary task times and overlap between computation and load balance, and thus extends earlier work which assumed fixed cost tasks and used a bulk synchronous style in which the system alternated between distinct computing and load balancing steps. Our analysis is supported by experiments with application codes, demonstrating that the efficiency is high enough to make this method practical. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bruno Buchberger. </author> <title> Grobner basis: an algorithmic method in polynomial ideal theory. </title> <editor> In N. K. Bose, editor, </editor> <booktitle> Multidimensional Systems Theory, chapter 6, </booktitle> <pages> pages 184-232. </pages> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1985. </year>
Reference-contexts: Further, let s j have been dequeued from queue q j and executed by processor q j . Then there exists a set of vertices V e H and a partition 1 ; : : : ; h (s) of the interval <ref> [1; T ] </ref> such that * V " s 1 ; : : : ; s h (s) = ;. * Each v 2 V is generated during the time interval [1; t]. <p> of vertices V e H and a partition 1 ; : : : ; h (s) of the interval [1; T ] such that * V " s 1 ; : : : ; s h (s) = ;. * Each v 2 V is generated during the time interval <ref> [1; t] </ref>. Further, each node in V arrives into q j during interval j , for some j, 1 j h (s). * v2V t (v) t hT S. Proof. We only sketch the proof, the details being similar to a similar proof in [8]. The proof is constructive. <p> We then continue the construction from t 0 1 in an analogous manner. Reasoning as in lemma 4.1, we can see that the processing times of nodes in V must cover all of <ref> [1; t] </ref>, except for the time spent in processing nodes s 1 ; : : : ; s h (s) (which is at most S) and the time spent in processing nodes outside e H (which is at most hT ). <p> = (q 1 ; : : : ; q h (s) ) of task queues, identified by the processor on which the queue is located. * A sequence of individually contiguous but mu tually disjoint intervals ( 1 ; : : : ; h (s) ) that partition the interval <ref> [1; t] </ref>. 669 * A set of nodes V e H. fi 2 Definition 4.2 A delay sequence (s; Q; ; V ) of length t occurs in an execution if * The tree expansion takes time at least t. * s 2 e H is a node whose expansion finished <p> The vertices in V must be selected so that their execution times sum to at least t hT S. This part of our argument is more complicated than the one of [8] because the expansion time of a node can have arbitrary values in the range <ref> [1; T ] </ref>. We first describe the counting argument assuming that t (v) can only take on one of the fi (lg T ) values in 1; 2; 4; : : :; 2 dlg Te . Later, in x4.2.2 we will show how this assumption can be discarded. <p> This does not necessarily balance the number of eigenvalues on either side, so the tree can potentially be quite unbalanced. The first histogram in Figure 2 shows the distribution of tasks times. 5.1.2 The Grobner Basis Algorithm Buchberger's Grobner basis algorithm <ref> [1] </ref> starts with a set G of multivariate symbolic polynomials and repeatedly checks if there is any pair f; g 2 G such that a certain scaled sum of f and g is not represented in G.
Reference: [2] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> Im plementing an irregular application on a distributed memory multiprocessor. </title> <booktitle> In ACM SIG-PLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 169-178, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Second, using task graphs generated from application codes, we estimate the constant factor. The randomized strategy we consider has already been used in applications like a symbolic polynomial equation solver and an eigenvalue program and has been found to give good speedups <ref> [2, 4] </ref>. These results represent a step towards bridging the gap between the theory and practice of dynamic load balancing algorithms. Tree structured task graphs arise frequently in paradigms such as divide and conquer, backtrack search, and heuristic search using branch and bound. <p> Our results will still be applicable, however, if the smallest task time is comparable to or greater than network communication time for a task. Our experiments suggest that some applications like Grobner bases computation <ref> [2] </ref>, and the eigenvalue problem [4] fit this paradigm, either because the volume of data associated with a task is small, or because caching and data replication are feasible. For the applications we consider, transmitting the task takes about 4-6s, whereas the smallest task takes thousands of microseconds to execute. <p> For applications with tasks that take much more time expanding than transmitting, actual performance corroborates the above results that predict that speedups are scalable, to a small constant factor. 5.1 The Applications There are applications for which the theory developed makes reasonable performance predictions. The Grobner basis problem <ref> [2] </ref> and the bisection eigen value problem [4] are two examples. They can be abstracted to tree-structured computations, in which locality has little influence on performance because replication is used. 5.1.1 The Bisection Eigenvalue Algorithm A symmetric tridiagonal N fi N real matrix is known to have N real eigenvalues. <p> Solving the task involves executing the body of the while-loop above, which can take extremely diverse task times (i.e., T can be hundreds to thousands in practice). Even though the augmentation of G looks inherently sequential, the tasks in the tree can actually run with significant concurrency <ref> [2] </ref>. G is replicated across processors, and maintaining consistency is inexpensive since augmentations are rare in practice. Thus, locality is not a primary factor in performance. 5.2 Measurements Randomly placing tasks give good empirical performance for many applications. <p> For our example, the tree has W = 11053339s, n = 142, h = 11, T = 174860s ffi simulation is worse than actual runs because they did not work with the same tree; it is hard to control what tree is expanded as the algorithm is highly indeterminate <ref> [2] </ref>. 5.2.2 Bisection Eigenvalue Algorithm Depending on the input, dynamic load balancing may or may not be necessary for the bisection algorithm. If the eigenvalues are distributed uniformly on the number line, static load balancing usually does quite well.
Reference: [3] <author> David Culler, Richard Karp, David Patterson, Abhijit Sahay, Klaus Schauser, Eunice San-tos, Ramesh Sumbramonian, and Thorsten von Eicken. </author> <title> Logp: Towards a realistic model of parallel computation. </title> <booktitle> In ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <year> 1993. </year>
Reference-contexts: Thus the analysis would be accurate for PRAMs, but it would also be acceptable for networks that are effectively modeled as being completely connected (for example, the LogP model <ref> [3] </ref>), provided transmitting a task has a negligible cost compared to executing it. To study the effect of network latency on efficiency, we fixed the number of processors P and varied network latency L.
Reference: [4] <author> Inderjit Dhillon and James Demmel. </author> <title> Private communication., </title> <month> March </month> <year> 1994. </year>
Reference-contexts: Second, using task graphs generated from application codes, we estimate the constant factor. The randomized strategy we consider has already been used in applications like a symbolic polynomial equation solver and an eigenvalue program and has been found to give good speedups <ref> [2, 4] </ref>. These results represent a step towards bridging the gap between the theory and practice of dynamic load balancing algorithms. Tree structured task graphs arise frequently in paradigms such as divide and conquer, backtrack search, and heuristic search using branch and bound. <p> Our results will still be applicable, however, if the smallest task time is comparable to or greater than network communication time for a task. Our experiments suggest that some applications like Grobner bases computation [2], and the eigenvalue problem <ref> [4] </ref> fit this paradigm, either because the volume of data associated with a task is small, or because caching and data replication are feasible. For the applications we consider, transmitting the task takes about 4-6s, whereas the smallest task takes thousands of microseconds to execute. <p> The Grobner basis problem [2] and the bisection eigen value problem <ref> [4] </ref> are two examples. They can be abstracted to tree-structured computations, in which locality has little influence on performance because replication is used. 5.1.1 The Bisection Eigenvalue Algorithm A symmetric tridiagonal N fi N real matrix is known to have N real eigenvalues.
Reference: [5] <author> Richard M. Karp and Yanjun Zhang. </author> <title> A ran domized parallel branch-and-bound procedure. </title> <journal> JACM, </journal> <volume> 40 </volume> <pages> 765-789, </pages> <year> 1993. </year> <note> Preliminary version in ACM STOC 1988, pp290-300. </note>
Reference-contexts: A sequential algorithm maintains a pool of generated but unexpanded tasks, and repeatedly selects the most promising task (based on some measure) and solves it, possibly generating more tasks that are added to the pool. For the parallel case, the following strategy has been proposed and analyzed <ref> [5, 8] </ref>: every processor maintains its own local pool, and when a processor needs work, it selects the best task from its local pool fl Supported in part by ARPA under contract DABT63-92-C-0026, by NSF (numbers CCR-9210260 and CDA-8722788), and by Lawrence Livermore National Laboratory. <p> The sequential paradigm for such problems is backtrack search. Nodes in the task graph are partial configurations, leaves are solution configurations or "dead ends", and the goal is to expand the tree so as to enumerate all solution leaves. 3.3 Branch and Bound Graph A branch and bound graph <ref> [5] </ref> is a tree structured graph in which each task v has an associated cost c (v), with the requirements that c (v) &gt; c (Parent (v)) and all costs are distinct. <p> the running time, which sets our goal for the analysis of the randomized model described in x3.5. 4.1 Centralized Priority Queue For exhaustive search problems with H = e H, the running time is fi (W=P + S), analogous to the fi (n=P + h) bound for unit task times <ref> [5] </ref>. We omit the proof of this fact, and consider the more interesting case of branch and bound trees. Lemma 4.1 With an ideal shared priority queue, the execution time t is given by t = fi W Proof. First consider the lower bound.
Reference: [6] <author> Steven Lucco. </author> <title> A dynamic scheduling method for irregular parallel programs. </title> <booktitle> In ACM SIGPLAN Symposium on Programming Language Design and Implementation, </booktitle> <pages> pages 200-211, </pages> <year> 1992. </year>
Reference-contexts: A sampling algorithm monitors task statistics to use in scheduling decisions. An oblivious algorithm does scheduling without such statistics. Lucco and Polychronopoulos, among others, have addressed the problem of dynamically scheduling forall loops in which the execution time of the loop iterations are unpredictable <ref> [6, 7] </ref>. However, there are no dependencies between tasks in the analysis, and the task pool only shrinks after initialization; there is no dynamic task creation. Lucco's sampling algorithm, taper, is a probabilistic version of guided self-scheduling proposed by Polychronopoulos [7].
Reference: [7] <author> C. D. Polychronopoulos. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: A sampling algorithm monitors task statistics to use in scheduling decisions. An oblivious algorithm does scheduling without such statistics. Lucco and Polychronopoulos, among others, have addressed the problem of dynamically scheduling forall loops in which the execution time of the loop iterations are unpredictable <ref> [6, 7] </ref>. However, there are no dependencies between tasks in the analysis, and the task pool only shrinks after initialization; there is no dynamic task creation. Lucco's sampling algorithm, taper, is a probabilistic version of guided self-scheduling proposed by Polychronopoulos [7]. <p> However, there are no dependencies between tasks in the analysis, and the task pool only shrinks after initialization; there is no dynamic task creation. Lucco's sampling algorithm, taper, is a probabilistic version of guided self-scheduling proposed by Polychronopoulos <ref> [7] </ref>. Taper 666 collects tasks into clusters, large at first to reduce overhead, small later for even finishing time. However, dependencies and data locality are two important aspects that are not addressed.
Reference: [8] <author> Abhiram Ranade. </author> <title> A simpler analysis of the Karp-Zhang parallel branch-and-bound method. </title> <type> Technical Report UCB/CSD 90/586, </type> <institution> University of California, Berkeley, </institution> <address> CA 94720, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: A sequential algorithm maintains a pool of generated but unexpanded tasks, and repeatedly selects the most promising task (based on some measure) and solves it, possibly generating more tasks that are added to the pool. For the parallel case, the following strategy has been proposed and analyzed <ref> [5, 8] </ref>: every processor maintains its own local pool, and when a processor needs work, it selects the best task from its local pool fl Supported in part by ARPA under contract DABT63-92-C-0026, by NSF (numbers CCR-9210260 and CDA-8722788), and by Lawrence Livermore National Laboratory. <p> The rest of the analysis deals with estimating the likelihood that any set V of nodes interferes (in the above sense) with the execution of any node s. This is formalized using the notion of a delay sequence, adapted from <ref> [8] </ref>. We show that the probability of having a long execution, in which the computation is significantly delayed by interference from V , is small. Lemma 4.2 Suppose the expansion of some node s 2 e H finishes at time t. <p> Further, each node in V arrives into q j during interval j , for some j, 1 j h (s). * v2V t (v) t hT S. Proof. We only sketch the proof, the details being similar to a similar proof in <ref> [8] </ref>. The proof is constructive. The idea is to work backwards from the time t and consider the earliest time instant t 0 &lt; t such that during the interval [t 0 ; t] only nodes in e H are expanded in q h (s) . <p> The hardest to count is the number of choices for V . The vertices in V must be selected so that their execution times sum to at least t hT S. This part of our argument is more complicated than the one of <ref> [8] </ref> because the expansion time of a node can have arbitrary values in the range [1; T ].
Reference: [9] <author> David S. Watkins. </author> <title> Fundamentals of Matrix Computations. </title> <publisher> John Wiley and Sons, </publisher> <year> 1991. </year>
Reference-contexts: This primitive can be used to successively subdivide the real line and locate all eigenvalues to arbitrary precision <ref> [9] </ref>. The kernel is shown below. Parameters lVal and rVal represent the current endpoints of the current range and Cnt is the number of eigenvalues in that range.
Reference: [10] <author> I.-C. Wu and H. T. Kung. </author> <title> Communication complexity for parallel divide-and-conquer. </title> <booktitle> In 32nd IEEE FOCS, </booktitle> <pages> pages 151-162, </pages> <year> 1991. </year> <month> 673 </month>
Reference-contexts: Another problem is to extend the results to account for the effects of locality. For unit time tasks in a finite tree, with the restriction that the task tree is also the communication graph, Wu and Kung <ref> [10] </ref> have described a method based on controlled global information. 7 Conclusion Dynamic scheduling algorithms are designed to ensure load balance when the computational load is not known in advance.
References-found: 10

