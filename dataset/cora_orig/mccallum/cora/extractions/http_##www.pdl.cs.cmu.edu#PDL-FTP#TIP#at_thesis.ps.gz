URL: http://www.pdl.cs.cmu.edu/PDL-FTP/TIP/at_thesis.ps.gz
Refering-URL: http://www.pdl.cs.cmu.edu/Publications/publications.html
Root-URL: 
Title: Practical and Theoretical Issues in Prefetching and Caching  
Author: Andrew Tomkins Avrim Blum, Garth Gibson, Daniel D. Sleator, Richard J. Lipton, 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy. Thesis Committee: Merrick Furst, Computer Science Department (chair)  
Note: c 1997 Andrew Tomkins  
Address: Pittsburgh, PA  
Affiliation: Computer Science Department Carnegie Mellon University  Computer Science Department  Computer Science Department  Computer Science Department  Princeton University Computer Science Department  
Date: October 7, 1997  
Pubnum: CMU-CS-97-181  
Abstract: This research is sponsored by the Department of the Navy, Office of Naval Research under Contract No. N00174-96-0002, and the Defense Advanced Research Projects Agency (DARPA), Army Research Office under Contract No. DABT63-93-C-0054. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Department of Defense or the United States Government. 
Abstract-found: 1
Intro-found: 1
Reference: [ADU71] <author> Alfred V. Aho, Peter J. Denning, and Jeffrey D. Ullman. </author> <title> Principles of optimal page replacement. </title> <journal> Journal of the ACM, </journal> <volume> 18(1) </volume> <pages> 80-93, </pages> <month> January </month> <year> 1971. </year> <title> (p 39) </title>
Reference-contexts: Aho, Denning and Ullman <ref> [ADU71] </ref>, and later Coffman and Denning [EGCD73], study page replacement under particular probabilistic models of future page arrivals.
Reference: [Bar96] <author> Yair Bartal. </author> <title> Probabalistic approximation of metric spaces and its algorithmic applications. </title> <booktitle> In Proceedings of the 37th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 183-193, </pages> <month> October, </month> <year> 1996. </year> <title> (p 149) </title>
Reference-contexts: We also give an (log k) lower bound on the competitive ratio of any such algorithm, extending and simplifying results of [KRR91]. All work described in this chapter was joint with Avrim Blum and Merrick Furst. The techniques described here, combined with a recent result of Yair Bartal's <ref> [Bar96] </ref>, led (with substantial modifications and improvements) to a much more general result, solving the problem described here not just for weighted cache spaces, but for any metric space (with an increase from O (log 2 k) to O (log 6 k) in the competitive ratio).
Reference: [BBBT96] <author> Yair Bartal, Avrim Blum, Carl Burch, and Andrew Tomkins. </author> <title> A polylog(n)- competitive algorithm for metrical task systems. </title> <note> In submitted, </note> <year> 1996. </year> <pages> (pp 23, 137, 147, 149, 179) </pages>
Reference-contexts: As a final note, the results for weighted caching are no longer the most general known. Since this work was done, results of Bartal, Blum, Burch and myself <ref> [BBBT96] </ref> give an O (log k)-competitive algorithm for arbitrary (k + 1)-point spaces that also gives polylog competitive ratio for k + polylog (k)-point spaces. 3 For server problems, the optimal off-line cost with free time is 0. 24 Introduction Part I Systems Chapter 2 Systems Introduction That's a fine idea <p> I will describe details of the collaboration in each chapter, but in general, Avrim Blum contributed to all the topics described here, Merrick Furst contributed to the results of Chapter 9, and the results of Chapter 8 were later extended (substantially) by Yair Bartal, Avrim Blum, Carl Burch and myself <ref> [BBBT96] </ref>, improving the proofs and presentation in this document. 7.1 Online Problems Online algorithms must process a sequence of requests, making decisions about early requests before seeing later requests. Informally, we face on-line problems routinely. For 138 Theory Overview instance, when driving on the highway, we must choose a lane. <p> Irani and Seiden [IS95] give an H n +O ( p log n)-competitive algorithm, matching the lower bound to within lower-order terms. We give an O (log 2 n)-competitive algorithm for any weighted-cache task system. Finally, later work by Blum, Bartal, Burch and myself <ref> [BBBT96] </ref> gives an O (log 6 n)-competitive algorithm for any metric space. Fiat and Ricklin [FR94] study a very different problem with a similar flavor, the weighted-server problem. <p> That work was joint between Yair Bartal, Avrim Blum, and Carl Burch, and myself, and is described in <ref> [BBBT96] </ref>. The work described here benefited from the presentation worked out in [BBBT96], and in particular from Carl Burch's suggestion of a potential function for weighted cache spaces. 8.1 Definitions and Preliminaries An algorithm for the k-server problem on a (k + 1)-point metric space can be viewed as controlling the <p> That work was joint between Yair Bartal, Avrim Blum, and Carl Burch, and myself, and is described in <ref> [BBBT96] </ref>. The work described here benefited from the presentation worked out in [BBBT96], and in particular from Carl Burch's suggestion of a potential function for weighted cache spaces. 8.1 Definitions and Preliminaries An algorithm for the k-server problem on a (k + 1)-point metric space can be viewed as controlling the position of a hole (the point without a server) which moves whenever <p> A complete summary of the results of this chapter appears in Section 9.1. 10.2.1 Future Theory Work Results of Bartal, Blum, Burch and myself <ref> [BBBT96] </ref> give a polylog-competitive algorithm for metrical task systems on any metric space, extending our results for weighted-cache spaces with a slight increase in the competitive factor (from O (log 2 k) to O (log 6 k)). This essentially closes the door on extensions to other metrical task system spaces. <p> However, there is an interesting orthogonal direction for extensions: weighted caching with multiple holes. Our results apply to metrical task systems on weighted-cache spaces, or, according to Theorem 4, the k-server problem for (k + 1)-point weighted-cache spaces. The results of <ref> [BBBT96] </ref> can be applied to generate an algorithm with polylog competitive factor for spaces with polylog (n) holes. Beyond this, (even, say, for a space of k + k t 2k points) the question is open.
Reference: [BBK + 90] <author> S. Ben-David, A. Borodin, R. Karp, G. Tardos, and A. Wigderson. </author> <title> On the power of randomization in online algorithms. </title> <booktitle> In ACM Symposium on Theory of Computing, </booktitle> <pages> pages 379-386, </pages> <year> 1990. </year> <title> (p 139) </title>
Reference-contexts: So we view the adversary as knowing the distribution of the algorithm's responses, but not the actual responses themselves. It is common to study randomized algorithms in the oblivious adversary model because the benefit randomization provides is limited against an adaptive adversary. Ben-David et al. <ref> [BBK + 90] </ref> showed that a c-competitive randomized algorithm against an adaptive adversary implies a c-competitive deterministic algorithm.
Reference: [BC91] <author> J. L. Baer and T. F. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> Supercomputing '91, 1991. (p 38) </booktitle>
Reference-contexts: Smith [Smi78] showed that large-block prefetching to cache may be ineffective, and while prefetching is potentially effective, small details of the implementation can have a significant effect on the overall performance. Baer and Chen <ref> [BC91] </ref> describe a hardware scheme for pre-loading a cache in the presence of sequential accesses to memory.
Reference: [Bel66] <author> L.A. Belady. </author> <title> A study of replacement algorithms for virtual storage computers. </title> <journal> IBM Systems Journal, </journal> <volume> 5 </volume> <pages> 78-101, </pages> <year> 1966. </year> <title> (p 39) </title>
Reference-contexts: and while the advantages can be substantial, the margins are tight the more complex reasoning about prefetches that is routine in disk prefetching work is not feasible in this domain. 2.3 Related Work 39 2.3.6 Theoretical Treatments The first theoretical treatment of caching was a study by Belady in 1966 <ref> [Bel66] </ref>, which introduced the problem and studied many algorithms, including the MIN algorithm, which makes optimal replacement decisions when future accesses are known. Aho, Denning and Ullman [ADU71], and later Coffman and Denning [EGCD73], study page replacement under particular probabilistic models of future page arrivals.
Reference: [BGV95] <author> Rakesh Barve, Edward F. Grove, and Jeffrey Scott Vitter. </author> <title> Application-controlled paging for a shared cache. </title> <booktitle> In 36th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 204-213, </pages> <address> Milwaukee, Wisconsin, </address> <month> October 23-25 </month> <year> 1995. </year> <journal> IEEE. </journal> <pages> (pp 39, 180) </pages>
Reference-contexts: In the case of multiple processes that give hints and compete for cache resources, Barve, Grove and Vitter <ref> [BGV95] </ref> describe a competitive approach to cache management. If the hints are perfect and only the interleaving is unknown, they show an algorithm with a competitive factor logarithmic in the number of processes. <p> First, tiptoe in the presence of multiple processes may be given complete and correct disclosures for each, but it will not know the interleaving of the requests. In fact, this exact model has been studied under competitive analysis by Barve, Grove and Vitter et al. <ref> [BGV95] </ref>. Furthermore, even for a single process, there are again online elements of the problem.
Reference: [BKRS92] <author> A. Blum, H.J. Karloff, Y. Rabani, and M. Saks. </author> <title> A decomposition theorem and bounds for randomized server problems. </title> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 197-207, </pages> <year> 1992. </year> <pages> (pp 23, </pages> <booktitle> 143, </booktitle> <volume> 145, 147, 149, 151, 164, 167) 190 BIBLIOGRAPHY </volume>
Reference-contexts: We then show a number of related results, exploring the model. Unlike the stan dard on-line model for which there exists a general ( q log k= log log k) lower bound for any space <ref> [BKRS92] </ref>, Section 9.4 shows that in the free-time model there exist metric spaces with constant competitive ratio. Section 9.5 then considers algorithms that are given possibly erroneous hints to guide their activity during free time. Next, Section 9.6 considers bounded rather than infinite free time for certain metric spaces. <p> The primary result of Chapter 8 is an O (log 2 k)-competitive randomized algorithm for weighted caching on (k+1)-point spaces (also known Cat-and-Mouse problems or Pursuit-Evasion games <ref> [BKRS92] </ref>). We also give an (log k) lower bound on the competitive ratio of any such algorithm for every fixed weighted-cache space, extending and simplifying results of [KRR91]. <p> the (log k) lower bound for arbitrary weighted cache spaces mentioned above generalizes to algorithms with free time. 3 For server problems, the optimal off-line cost with free time is 0. 7.6 Related Work 145 Unlike the standard on-line model for which there exists a general ( q lower bound <ref> [BKRS92] </ref>, we show that there exist metric spaces in which one can achieve a constant competitive ratio in the free-time model. (Interestingly, these are exactly the types of spaces proven to have an (log k) lower bound in the standard model by [BKRS92].) In addition, we show that even with free <p> there exists a general ( q lower bound <ref> [BKRS92] </ref>, we show that there exist metric spaces in which one can achieve a constant competitive ratio in the free-time model. (Interestingly, these are exactly the types of spaces proven to have an (log k) lower bound in the standard model by [BKRS92].) In addition, we show that even with free time, there is an (k) lower bound on the competitive ratio for any deterministic algorithm and any space, and thus, without randomization, free time helps by at most a constant factor. <p> We show an (log k) lower bound for any weighted cache space. A weaker bound of (log log k) due to Karloff, Rabani and Ravid [KRR91], subsequently improved to q by Blum, Karloff, Rabani and Saks <ref> [BKRS92] </ref>, applies to every metric space. Irani and Seiden give an en=(e 1)-competitive randomized MTS algorithm [IS95] for any metric space. Early results on specific spaces have focused largely on the uniform space: Borodin, Linial and Saks [BLS92] give a 2H n upper bound and an H n lower bound. <p> This chapter gives an O (log 2 k)-competitive randomized algorithm for weighted caching on (k + 1)-point spaces (also known Cat-and-Mouse problems or Pursuit-Evasion games <ref> [BKRS92] </ref>). We also give an (log k) lower bound on the competitive ratio of any such algorithm, extending and simplifying results of [KRR91]. All work described in this chapter was joint with Avrim Blum and Merrick Furst. <p> The algorithm must balance these two costs so the adversary will have no obvious winning strategy. This is the same high-level idea used in <ref> [BKRS92] </ref> for "sufficiently unbalanced" spaces. One difficulty in this case, however, is that because distances are growing only by factors of 2 (as opposed to factors of p (i) for a sufficiently large polynomial p as required by [BKRS92]) we need to be particularly careful about our inductive assumptions and the <p> This is the same high-level idea used in <ref> [BKRS92] </ref> for "sufficiently unbalanced" spaces. One difficulty in this case, however, is that because distances are growing only by factors of 2 (as opposed to factors of p (i) for a sufficiently large polynomial p as required by [BKRS92]) we need to be particularly careful about our inductive assumptions and the way in which the algorithm for an i-point space interacts with the algorithm for a larger containing space. 3 Let Alg i denote the algorithm for the space of points f0; 1; : : : ; ig. <p> We also show that the (log k) lower bound for arbitrary weighted cache spaces mentioned above generalizes to algorithms with free time. Unlike the standard on-line model for which there exists a general ( q lower bound <ref> [BKRS92] </ref>, we show that there exist metric spaces in which one can achieve a constant competitive ratio in the free-time model. (Interestingly, these are exactly the types of spaces proven to have an (log k) lower bound in the standard model by [BKRS92].) In addition, we show that even with free <p> there exists a general ( q lower bound <ref> [BKRS92] </ref>, we show that there exist metric spaces in which one can achieve a constant competitive ratio in the free-time model. (Interestingly, these are exactly the types of spaces proven to have an (log k) lower bound in the standard model by [BKRS92].) In addition, we show that even with free time, there is an (k) lower bound on the competitive ratio for any deterministic algorithm and any space, and thus, without randomization, free time helps by at most a constant factor. <p> The base case k = 1 is clear, so assume inductively that there is a c (k+1)=2 -competitive algorithm for the (k + 1)=2-point space. The general algorithm is essentially the same as that in <ref> [BKRS92] </ref>, with appropriate use of free time. As noted in [BKRS92], we may assume that on any sequence of requests causing the off-line cost of the (k + 1)=2 point space to increase by s, the on-line cost is at most c (k+1)=2 (s + d internal ), where d internal <p> The base case k = 1 is clear, so assume inductively that there is a c (k+1)=2 -competitive algorithm for the (k + 1)=2-point space. The general algorithm is essentially the same as that in <ref> [BKRS92] </ref>, with appropriate use of free time. As noted in [BKRS92], we may assume that on any sequence of requests causing the off-line cost of the (k + 1)=2 point space to increase by s, the on-line cost is at most c (k+1)=2 (s + d internal ), where d internal is the diameter of that space.
Reference: [BLS87] <author> A. Borodin, N. Linial, and M. Saks. </author> <title> An optimal online algorithm for metrical task systems. </title> <booktitle> In 19th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 373-382, </pages> <year> 1987. </year> <title> (p 141) </title>
Reference-contexts: How long should the process spin before it should be swapped out? This is simply the continuous version of the ski-buying problem. 7.3.2 Metrical Task Systems Metrical task systems were initially described by Borodin, Linial and Saks <ref> [BLS87] </ref>. An MTS is presented as a graph of k + 1 points, called states, and a metric on the states. At any point, the algorithm must be in exactly one state. A request i is a vector of k + 1 values, (1) (k+1) i .
Reference: [BLS92] <author> A. Borodin, N. Linial, and M. Saks. </author> <title> An optimal online algorithm for metrical task systems. </title> <journal> JACM, </journal> <volume> 39(4) </volume> <pages> 745-763, </pages> <year> 1992. </year> <pages> (pp 146, 147) </pages>
Reference-contexts: Two types of lower bounds are known for randomized MTS algorithms. For certain specific metric spaces such as the uniform space studied by Borodin, Linial and Saks <ref> [BLS92] </ref> and the super-increasing space of Karloff, Rabani and Ravid [KRR91] there are 7.6 Related Work 147 (log k) lower bounds on the competitive ratio of any online algorithm. We show an (log k) lower bound for any weighted cache space. <p> Irani and Seiden give an en=(e 1)-competitive randomized MTS algorithm [IS95] for any metric space. Early results on specific spaces have focused largely on the uniform space: Borodin, Linial and Saks <ref> [BLS92] </ref> give a 2H n upper bound and an H n lower bound. Irani and Seiden [IS95] give an H n +O ( p log n)-competitive algorithm, matching the lower bound to within lower-order terms. We give an O (log 2 n)-competitive algorithm for any weighted-cache task system.
Reference: [Cao96] <author> Pei Cao. </author> <title> Application-Controlled File Caching and Prefetching. </title> <type> PhD thesis, </type> <institution> Princeton University, </institution> <year> 1996. </year> <pages> (pp 28, </pages> <booktitle> 29, </booktitle> <volume> 32, 33, 59, </volume> <pages> 65) </pages>
Reference-contexts: The first is cache management advice of the form, "Cache file foo.data using the MRU cache replacement scheme." This approach has been implemented by Cao et al. <ref> [Cao96, CFL94b] </ref>. The second approach, known as disclosure, has been taken by Patterson et al., and also by Cao et al.. A disclosure is a hint about future accesses given in the language of the existing I/O interface. <p> Another system presented by Pei Cao and collaborators <ref> [Cao96] </ref> has shown a reduction of up to 50% in overall execution time on a different suite of applications, also modified to give hints to an operating system designed to accept them. In this case, the system had a single disk and the improvement was due largely to cache management. <p> I show how to extend cost-benefit allocation to incorporate forestall's adaptive single-process prefetching algorithm, resulting in a new algorithm called tiptoe, or TIP with Temporal Overload Estimators. The next system I consider is lru-sp, due to Cao et al. <ref> [CFL94a, Cao96] </ref>, an extension of traditional LRU replacement. <p> My work is also closely related, and in some cases collaborative with, work of Pei Cao, Tracy Kimbrel, Anna Karlin and others on lru-sp, aggressive and related algorithms <ref> [KTP + 96, Cao96, CFKL95b, CFKL95a, CFL94a, CFL94b] </ref>. Upcoming chapters describe these systems in detail, so I will not include a discussion of them here. At a high level, the work in this thesis represents an approach to providing and using more detailed access information in the I/O subsystem. <p> Before this work two systems had been implemented: the TIP2 system of Patterson, Gibson et al. [PGG + 95] and the lru-sp/aggressive system of Cao et al. [CFKL95b] (this algorithm was embedded in a filesystem called ACFS, application-controlled file system, described in Pei Cao's thesis <ref> [Cao96] </ref>). Recall that the SPACE problem (Standalone Prefetching And Cache Eviction) involves managing prefetching and cache eviction decisions for a single process with full knowledge of future accesses and a fixed set of resources. <p> Lru-sp was developed by Cao, Felten, and Li <ref> [CFL94a, CFL94b, Cao96] </ref>; its goal is to adapt traditional LRU allocation, which has been successful due to performance and fairness qualities, to incorporate prefetching and informed cache management. Lru-sp uses a global LRU queue to partition cache buffers among the competing processes.
Reference: [CB92] <author> Tien-Fu Chen and Jean-Loup Baer. </author> <title> Reducing memory latency via non-blocking and prefetching caches. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 51-61, </pages> <month> October </month> <year> 1992. </year> <title> (p 38) </title>
Reference-contexts: Rogers and Li [RL92] describe a simple hardware modification and compiler support to allow speculative loading of data into cache, which results in significant reductions in average memory latency for a series of benchmarks. Chen and Baer <ref> [CB92] </ref> suggest a combination of hardware-based cache prefetching and hardware support for non-blocking caches. Tullsen and Eggers [TE93] showed that some architectures (in this case, a high-latency bus-based multiprocessor) are not well-suited for prefetching.
Reference: [CD85] <author> H.T. Chou and D.J. DeWitt. </author> <title> An evaluation of buffer management strategies for relational database systems. </title> <booktitle> In Proceedings of the 11th International Conference on Very Large Data Bases, </booktitle> <pages> pages 127-141, </pages> <year> 1985. </year> <title> (p 36) </title>
Reference-contexts: The query locality set model (QLSM) of Chou and Dewitt <ref> [CD85] </ref> also specifies eviction policies within a particular process. 8 For instance, if data is being read cyclically then MRU is the most appropriate policy.
Reference: [CDKK85] <author> H.-T. Chou, David J. Dewitt, Randy H. Katz, and Anthony C. Klug. </author> <title> Design and implementation of the wisconsin storage system, </title> <month> January </month> <year> 1985. </year> <title> (p 36) </title>
Reference-contexts: Stonebraker goes on to argue that some form of "advice" should be passed from the database management system to the cache manager. Chou et al. <ref> [CDKK85] </ref> give a general interface that allows application programs to mark individual pages with a priority representing the application's view of the importance of the page.
Reference: [CFKL95a] <author> P. Cao, E.W. Felten, A. Karlin, and K. Li. </author> <title> Implementation and performance of integrated application-controlled caching, prefetching and disk scheduling. </title> <type> Technical Report TR-CS95-493, </type> <institution> Princeton University, </institution> <year> 1995. </year> <title> (p 33) </title>
Reference-contexts: My work is also closely related, and in some cases collaborative with, work of Pei Cao, Tracy Kimbrel, Anna Karlin and others on lru-sp, aggressive and related algorithms <ref> [KTP + 96, Cao96, CFKL95b, CFKL95a, CFL94a, CFL94b] </ref>. Upcoming chapters describe these systems in detail, so I will not include a discussion of them here. At a high level, the work in this thesis represents an approach to providing and using more detailed access information in the I/O subsystem.
Reference: [CFKL95b] <author> P. Cao, E.W. Felten, A. Karlin, and K. Li. </author> <title> A study of integrated prefetching and caching strategies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS, </booktitle> <month> May, </month> <year> 1995. </year> <pages> (pp 18, </pages> <booktitle> 20, </booktitle> <volume> 30, 33, 59, 63, 82, 99, 131, </volume> <pages> 181) </pages>
Reference-contexts: To address this problem, existing systems for Informed Prefetching and Caching, in which the application provides "hints" to the system about upcoming accesses, submit requests in parallel to make use of the large aggregate bandwidth of the disk array <ref> [PGG + 95, CFL94b, CFKL95b, PG94] </ref>. Figure 1.1 shows the average reduction in stall time provided by the TIP2 system of Patterson, Gibson et al. [PGG + 95], relative to Digital's OSF/1 operating system. <p> Chapter 4 describes the four algorithms I compare. The first is the TIP2 algorithm of Patterson et al., mentioned above. Second is another existing system from the literature: lru-sp/aggressive, by Cao et al. <ref> [CFKL95b, CFL94a] </ref>. These two systems take different approaches to prefetching within a single stream of hints. <p> This conservative approach may fail to prefetch deeply enough into the request stream when disk parallelism is limited or when the I/O workload is highly unbalanced across the disks of an array. The second algorithm we consider, the aggressive algorithm of Cao et al. <ref> [CFKL95b] </ref>, prefetches deeply without regard to disk load. <p> My work is also closely related, and in some cases collaborative with, work of Pei Cao, Tracy Kimbrel, Anna Karlin and others on lru-sp, aggressive and related algorithms <ref> [KTP + 96, Cao96, CFKL95b, CFKL95a, CFL94a, CFL94b] </ref>. Upcoming chapters describe these systems in detail, so I will not include a discussion of them here. At a high level, the work in this thesis represents an approach to providing and using more detailed access information in the I/O subsystem. <p> Before this work two systems had been implemented: the TIP2 system of Patterson, Gibson et al. [PGG + 95] and the lru-sp/aggressive system of Cao et al. <ref> [CFKL95b] </ref> (this algorithm was embedded in a filesystem called ACFS, application-controlled file system, described in Pei Cao's thesis [Cao96]). <p> T driver + T disk y1 1 &lt; y ^ P T driver y ^ P y &gt; ^ P 4.2 lru-sp/aggressive The lru-sp/aggressive system maintains an explicit division between the SPACE algorithm for single-process prefetching (aggressive) and the allocation algorithm (lru sp). 4.2.1 aggressive Cao, Felten, Karlin and Li <ref> [CFKL95b] </ref> present the aggressive algorithm for single-process prefetching and caching. Like TIP2, aggressive is designed with a specific 64 Algorithms system model in mind. <p> Lru-sp/aggressive, tiptoe and lru-sp/forestall issue requests in groups of up to 16, a technique that Cao has called batching <ref> [CFKL95b] </ref>. In my implementation the technique works as follows. Prefetching is only initiated on idle disks. When a system decides to prefetch on a particular disk, it continues to submit I/O's until the batch is full or the algorithm no longer chooses to prefetch. <p> When a prefetching algorithm fetches data and evicts blocks that will be needed again, it may attempt to select blocks for eviction so as to increase disk locality when the blocks are read back in. This topic is discussed in more detail in Section 5.3. Briefly, Cao et al. <ref> [CFKL95b] </ref> describe a mechanism they call "batching" in which the prefetching algorithm waits for the disk to go idle and then submits up to B requests, where the batchsize B is a parameter of the algorithm (16 in my implementation and Cao's). Lru-sp/aggressive, tiptoe and lru-sp/forestall all adopt this scheme. <p> As mentioned above, a full treatment of this topic requires a theoretical model of nonconstant disk service time so a treatment within tiptoe is beyond my scope; nonetheless, the phenomenon arises in simulations. As described in Section 4.5, Cao et al in their presentation of aggressive <ref> [CFKL95b] </ref> describe a mechanism they call "batching" in which the prefetching algorithm waits for the disk to go idle and then submits up to B requests, where the batchsize B is a parameter of the algorithm. Lru-sp, tiptoe and lru-sp/forestall all adopt this scheme in my implementation. <p> As described in Section 4.1.1, a more appropriate metric is the increase in I/O service time (i.e., the increase in stall, plus the increase in other overhead incurred by the I/O subsystem). A model to address this difficulty using the competitive framework is given by Cao, Karlin et al. <ref> [CFKL95b] </ref>, and by Kimbrel and Karlin in the multi-disk case [KK96b]. Their model counts I/O stall time rather than simply I/O time, but still does not take into account other components of I/O service time (e.g., T driver ).
Reference: [CFL94a] <author> P. Cao, E.W. Felten, and K. Li. </author> <title> Application-controlled file caching policies. </title> <booktitle> In 1994 Usenix Summer Technical Conference, </booktitle> <pages> pages 171-182, </pages> <month> June, </month> <year> 1994. </year> <pages> (pp 20, 32, 33, 65) </pages>
Reference-contexts: Chapter 4 describes the four algorithms I compare. The first is the TIP2 algorithm of Patterson et al., mentioned above. Second is another existing system from the literature: lru-sp/aggressive, by Cao et al. <ref> [CFKL95b, CFL94a] </ref>. These two systems take different approaches to prefetching within a single stream of hints. <p> I show how to extend cost-benefit allocation to incorporate forestall's adaptive single-process prefetching algorithm, resulting in a new algorithm called tiptoe, or TIP with Temporal Overload Estimators. The next system I consider is lru-sp, due to Cao et al. <ref> [CFL94a, Cao96] </ref>, an extension of traditional LRU replacement. <p> My work is also closely related, and in some cases collaborative with, work of Pei Cao, Tracy Kimbrel, Anna Karlin and others on lru-sp, aggressive and related algorithms <ref> [KTP + 96, Cao96, CFKL95b, CFKL95a, CFL94a, CFL94b] </ref>. Upcoming chapters describe these systems in detail, so I will not include a discussion of them here. At a high level, the work in this thesis represents an approach to providing and using more detailed access information in the I/O subsystem. <p> Lru-sp was developed by Cao, Felten, and Li <ref> [CFL94a, CFL94b, Cao96] </ref>; its goal is to adapt traditional LRU allocation, which has been successful due to performance and fairness qualities, to incorporate prefetching and informed cache management. Lru-sp uses a global LRU queue to partition cache buffers among the competing processes.
Reference: [CFL94b] <author> P. Cao, E.W. Felten, and K. Li. </author> <title> Implementation and performance of application-controlled file caching. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <address> Monterey, CA, </address> <pages> pages 165-178, </pages> <month> November, </month> <year> 1994. </year> <pages> (pp 18, 28, 33, 65) </pages>
Reference-contexts: To address this problem, existing systems for Informed Prefetching and Caching, in which the application provides "hints" to the system about upcoming accesses, submit requests in parallel to make use of the large aggregate bandwidth of the disk array <ref> [PGG + 95, CFL94b, CFKL95b, PG94] </ref>. Figure 1.1 shows the average reduction in stall time provided by the TIP2 system of Patterson, Gibson et al. [PGG + 95], relative to Digital's OSF/1 operating system. <p> The first is cache management advice of the form, "Cache file foo.data using the MRU cache replacement scheme." This approach has been implemented by Cao et al. <ref> [Cao96, CFL94b] </ref>. The second approach, known as disclosure, has been taken by Patterson et al., and also by Cao et al.. A disclosure is a hint about future accesses given in the language of the existing I/O interface. <p> My work is also closely related, and in some cases collaborative with, work of Pei Cao, Tracy Kimbrel, Anna Karlin and others on lru-sp, aggressive and related algorithms <ref> [KTP + 96, Cao96, CFKL95b, CFKL95a, CFL94a, CFL94b] </ref>. Upcoming chapters describe these systems in detail, so I will not include a discussion of them here. At a high level, the work in this thesis represents an approach to providing and using more detailed access information in the I/O subsystem. <p> Lru-sp was developed by Cao, Felten, and Li <ref> [CFL94a, CFL94b, Cao96] </ref>; its goal is to adapt traditional LRU allocation, which has been successful due to performance and fairness qualities, to incorporate prefetching and informed cache management. Lru-sp uses a global LRU queue to partition cache buffers among the competing processes.
Reference: [CKPV90] <author> M. Chrobak, H. Karloff, T. Payne, and S. Vishwanathan. </author> <title> New results on server problems. </title> <booktitle> In First Annual ACM-SIAM Symposium On Discrete Algorithms, </booktitle> <pages> pages 290-300, </pages> <year> 1990. </year> <pages> (pp 146, 172, 182) BIBLIOGRAPHY 191 </pages>
Reference-contexts: The problem was first described by Manasse, McGeoch and Sleator [MMS88a] as an example of an asymmetric server problem. In [RS89] Raghavan and Snir presented their harmonic algorithm, an O (k)-competitive randomized algorithm and the first competitive algorithm for the problem. Subsequently Chrobak, Karloff, Payne and Vishwanathan <ref> [CKPV90] </ref> gave an O (k)-competitive deterministic algorithm called the BALANCE algorithm, and presented some hardness results involving asymmetric metric spaces. Young [You91] studied the problem in the context of approximative primal-dual algorithms, and generalized the result to adversaries with fewer than k servers. <p> The distance between two points is simply the arc length of the unique path connecting the points in the tree. Chrobak, Karloff, Payne and Vishwanathan <ref> [CKPV90] </ref> give a k-competitive deterministic algorithm called double coverage for scheduling on the line, as described in Section 7.3.1. Karlin, Manasse, McGeoch and Owicki [KMMO94] consider the spin-block problem described in Section 7.3.1. <p> Typically competitive analysis is applied to sequences that are computer generated at a small timescale; for instance, deciding what to cache [MMS90, FKM + 91], moving disk heads to serve requests for disk blocks <ref> [CKPV90] </ref>, or deciding whether a process should spin or block [KMMO94]. Here, we consider the problem of interacting with a user. In our model, a user presents a request which the algorithm must service, waits for some amount of time, then presents the next request. <p> It still seems 182 Conclusion that there is substantial promise for this approach, whether the algorithms are the types of randomized algorithms described here or their simpler deterministic counterparts such as the BALANCE algorithm described by Chrobak et al. <ref> [CKPV90] </ref>. However, within the scope of this thesis, it has not been possible to perform such an experiment.
Reference: [CKV93] <author> K. Curewitz, P. Krishnan, and J.S. Vitter. </author> <title> Practical prefetching via data compression. </title> <booktitle> In Proceedings of the 1993 ACM Conference on Management of Data (SIGMOD), </booktitle> <pages> pages 257-266, </pages> <month> May, </month> <year> 1993. </year> <title> (p 35) </title>
Reference-contexts: Palmer and Zdonik [PZ91] present Fido, a cache manager that employs an associative memory technique to predict future accesses in a particular isolated context. They showed some early simulation results suggesting that predictive prefetching within the database domain is a promising approach. More recently, Curewitz, Krishnan and Vitter <ref> [CKV93] </ref> present a theoretically-grounded approach to predictive prefetching based on the observation that a succinct representation of a sequence of requests must capture some internal structure within the sequence. The representation should therefore be useful for predicting future elements of the sequence.
Reference: [CL91] <author> Marek Chrobak and Lawrence Larmore. </author> <title> An optimal on-line algorithm for k servers on trees. </title> <journal> SIAM J. Computing, </journal> <volume> 20(1) </volume> <pages> 144-148, </pages> <year> 1991. </year> <title> (p 146) </title>
Reference-contexts: We give an O (log 2 k)-competitive algorithm for (k + 1)-point spaces. There are a number of other metric spaces for which deterministic algorithms have been studied. Chrobak and Larmore <ref> [CL91] </ref> present a k-competitive deterministic algorithm for any metric space that is a tree, meaning a planar embedding of a free tree. The distance between two points is simply the arc length of the unique path connecting the points in the tree.
Reference: [CP90] <author> Peter M. Chen and David A. Patterson. </author> <title> Maximizing performance in a striped disk array. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 322-331. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1990. </year> <title> (p 54) </title>
Reference-contexts: All read system calls are for an entire block; 2922 blocks are read, and no block is read more than once. 1 hint 2923 consume 7193 3.5 The Simulator The simulator is built on top of the Berkeley RaidSim simulator <ref> [Lee89, CP90, LK91] </ref>, as modified by Mark Holland [Hol94] at CMU. RaidSim can simulate various flavors of RAID disk arrays using an internal geometry-aware disk simulator to determine disk access times.
Reference: [CR93] <author> C. Chen and N. Roussopoulos. </author> <title> Adaptive database buffer allocation using query feedback. </title> <booktitle> In Proc. of the 19th VLDB Conference, </booktitle> <address> Dublin, Ireland, </address> <year> 1993. </year> <title> (p 37) </title>
Reference-contexts: In [FNS91] the same authors generalized the conditions in which a query may be admitted for processing. In all these approaches, however, queries that are not sequential or looping are considered to be random, and their estimates are created accordingly. Chen and Roussopoulos <ref> [CR93] </ref> extend the marginal gains of other queries based on a sampling of their re-use under the LRU policy.
Reference: [CR96] <author> Andrew Choi and Manfred Ruschitzka. </author> <title> Optimal management of dynamic buffer caches. Performance Evaluation, </title> <booktitle> 26 </booktitle> <pages> 239-262, </pages> <year> 1996. </year> <title> (p 39) </title>
Reference-contexts: In the presence of variable-sized buffers there have been a number of extensions to this work, beginning with Prieve and Fabry's VMIN algorithm [PF76], and more recently, work to derive practical versions of VMIN, such as Choi and Ruschitzka's work on SETVMIN <ref> [CR96] </ref>. In the case of multiple processes that give hints and compete for cache resources, Barve, Grove and Vitter [BGV95] describe a competitive approach to cache management.
Reference: [CY89] <author> D. W. Cornell and P. S. Yu. </author> <title> Integration of buffer management and query optimization in relational database environment. </title> <booktitle> In Proc. of the 15th Int. Conf. on Very Large Data Bases, </booktitle> <pages> pages 247-255, </pages> <address> Amsterdam, </address> <month> August </month> <year> 1989. </year> <title> (p 36) </title>
Reference-contexts: Around the same time, the original hot set model was extended by its proponents to include different access patterns, so that all operations supported by IBM's System R database test-bed could be represented in the model [SS86]. Cornell and Yu <ref> [CY89] </ref> integrate query optimization with buffer cache management. They phrase the problem as a 0-1 integer linear program, which they approximate to choose query plans based on buffer restrictions. Later, in [YC91], the same authors present a different global minimization technique based on simulated annealing for the same problem.
Reference: [Den68] <author> Peter J. Denning. </author> <title> The working set model for program behavior. </title> <journal> Communications of the ACM, </journal> <volume> 11(5) </volume> <pages> 323-333, </pages> <month> May </month> <year> 1968. </year> <title> (p 36) </title>
Reference-contexts: Within a database buffer manager, which has more complete information about the types of queries being performed, the most successful early approach to management is the hot set buffer pool management model of Sacco and Schkolnick [SS82], an extension of the working set model of Denning <ref> [Den68] </ref>.
Reference: [DWAP94] <author> M. Dahlin, R. Wang, T. Anderson, and D. Patterson. </author> <title> Cooperative caching: Using remote client memory to improve file system performance. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 267-280, </pages> <month> Nov </month> <year> 1994. </year> <title> (p 115) </title>
Reference-contexts: This type of re-use is typically an aggregate over a number of background processes executing simultaneously. I consider each type of re-use in turn. 6.3.1 Traditional Background Load Dahlin <ref> [DWAP94] </ref> records the activity of an Auspex file server supporting 231 client machines over a one-week period at the University of California at Berkeley. 1 The traces contain around 6.6 million requests and transfer 8.1 Gbyte of read data.
Reference: [EGCD73] <author> Jr. Edward G. Coffman and Peter J. Denning. </author> <title> Operating Systems Theory. </title> <publisher> Prentice Hall, </publisher> <year> 1973. </year> <title> (p 39) </title>
Reference-contexts: Aho, Denning and Ullman [ADU71], and later Coffman and Denning <ref> [EGCD73] </ref>, study page replacement under particular probabilistic models of future page arrivals.
Reference: [FKM + 91] <author> A. Fiat, R.M. Karp, M. Luby L. A. McGeoch, D.D. Sleator, and N.E. Young. </author> <title> Competitive paging algorithms. </title> <journal> Journal of Algorithms, </journal> <volume> 12 </volume> <pages> 685-699, </pages> <year> 1991. </year> <pages> (pp 145, </pages> <booktitle> 157, </booktitle> <volume> 159, 169, 172, </volume> <pages> 179) </pages>
Reference-contexts: The uniform space, described above in Section 7.3.1, was initially studied by Sleator and Tarjan [ST85], who showed that LRU and FIFO are k-competitive. Randomized approaches for the uniform space began with Fiat et al.'s marking algorithm <ref> [FKM + 91] </ref>, a 2H k -competitive algorithm where H k ~ = ln k is the k th harmonic number. They also show a log k lower bound on the competitive ratio of any randomized algorithm for the 146 Theory Overview uniform space. <p> Our algorithm will make requests to Swhenever certain conditions are met in M , and will then move the hole between levels whenever Smoves its hole between points. Our algorithm will also use the Marking Algorithm of <ref> [FKM + 91] </ref>; for completeness I define the algorithm here. The k servers begin on marked points 1 : : : k. Whenever a point is requested, it is marked. Whenever k + 1 points are marked, all marks except the most recent one are erased. <p> Next we note that if the hole is in L i then the corresponding hole of Smust be at point i, so when the phase of L i completes, Swill pay at least d i . The analysis of the Marking Algorithm from <ref> [FKM + 91] </ref> makes it clear that the expected cost to the algorithm over the course of a phase is bounded by the cost to move H k servers, with no amortization (the proof of [FKM + 91] uses amortization in the lower bound on the cost to the o*ine algorithm, <p> The analysis of the Marking Algorithm from <ref> [FKM + 91] </ref> makes it clear that the expected cost to the algorithm over the course of a phase is bounded by the cost to move H k servers, with no amortization (the proof of [FKM + 91] uses amortization in the lower bound on the cost to the o*ine algorithm, but we require only a bound on the online algorithm's cost here). <p> The off-line algorithm will always keep its hole away from point 0, and the hints provide no additional information to the algorithm. Thus, the analysis of <ref> [FKM + 91] </ref> (or Theorem 8) shows an (log k) lower bound on the performance of the algorithm. Now we consider the adversarial request sequence model. We modify the marking algorithm of [FKM + 91] to take advantage of hints. <p> Thus, the analysis of <ref> [FKM + 91] </ref> (or Theorem 8) shows an (log k) lower bound on the performance of the algorithm. Now we consider the adversarial request sequence model. We modify the marking algorithm of [FKM + 91] to take advantage of hints. Recall that at each request to an unmarked vertex, the marking algorithm first marks the vertex and then if no server is present, moves a random unmarked server (chosen uniformly) to the vertex. <p> This completes the theorem. 172 Free Time 9.7 Free Time For Command Processing In this section we incorporate free time into a simple model of command processing. Typically competitive analysis is applied to sequences that are computer generated at a small timescale; for instance, deciding what to cache <ref> [MMS90, FKM + 91] </ref>, moving disk heads to serve requests for disk blocks [CKPV90], or deciding whether a process should spin or block [KMMO94]. Here, we consider the problem of interacting with a user. <p> Unfortunately, as new weights are added the competitive ratio increases geometrically so this does not give a good bound for more general weighted caching spaces. The Marking Algorithm of <ref> [FKM + 91] </ref> is shown to be competitive for unweighted caching with an arbitrary number of holes, but the techniques of that paper do not appear to generalize in a straightforward manner.
Reference: [FNS91] <author> Christos Faloutsos, Raymond Ng, and Timos Sellis. </author> <title> Predictive load control for flexibile buffer allocation. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Data Bases, </booktitle> <pages> pages 265-274, </pages> <year> 1991. </year> <title> (p 37) 192 BIBLIOGRAPHY </title>
Reference-contexts: For instance, in a looping reference over N items, adding one buffer would decrease the number of cache misses by approximately 1 every N accesses. Using these estimates, they propose an algorithm MG-x-y that makes allocation decisions based on runtime buffer availability. In <ref> [FNS91] </ref> the same authors generalized the conditions in which a query may be admitted for processing. In all these approaches, however, queries that are not sequential or looping are considered to be random, and their estimates are created accordingly.
Reference: [FO71] <author> R. J. Feiertag and E. I. Organisk. </author> <title> The Multics Input/Output system. </title> <booktitle> In Proc. of the 3rd Symp. on Operating System Principles, </booktitle> <pages> pages 35-41, </pages> <year> 1971. </year> <title> (p 34) </title>
Reference-contexts: The technique successfully hid more than half the I/O latency is seven of eight NAS parallel benchmarks. Predictive Disk Prefetching The most common incarnation of predictive prefetching is sequential readahead, ranging from one-block lookahead as described by Smith [Smi85] and implemented in Multics <ref> [FO71] </ref> and Unix [MJLF84] to more aggressive versions such as Digital's OSF 1 operating system, which will prefetch up to sixty-four 8KByte blocks in advance. Similarly, there have been efforts to provide higher throughput by reading larger objects, as in [MK91].
Reference: [FR94] <author> Amos Fiat and Moty Ricklin. </author> <title> Competitive algorithms for the weighted server problem. </title> <journal> Theoretical Computer Science, </journal> <volume> 130 </volume> <pages> 85-99, </pages> <year> 1994. </year> <title> (p 147) </title>
Reference-contexts: We give an O (log 2 n)-competitive algorithm for any weighted-cache task system. Finally, later work by Blum, Bartal, Burch and myself [BBBT96] gives an O (log 6 n)-competitive algorithm for any metric space. Fiat and Ricklin <ref> [FR94] </ref> study a very different problem with a similar flavor, the weighted-server problem. In their model the weights apply to the servers rather than to the nodes of the graph, and the cost of moving a server across a distance is scaled by the server's weight.
Reference: [GA93] <author> James Griffioen and Randy Appleton. </author> <title> Automatic prefetching in a WAN. </title> <booktitle> In Proc. of the IEEE Workshop on Advances in Parallel and Distributed Systems, </booktitle> <pages> pages 8-12, </pages> <month> October </month> <year> 1993. </year> <title> (p 34) </title>
Reference-contexts: The primary advantage of this type of predictive prefetching versus informed prefetching is that applications do not need to be modified: the system makes predictions about future accesses (usually based on past accesses) and prefetches accordingly. Griffioen and Appleton have presented a series of results of this flavor. Initially <ref> [GA93] </ref> they considered wide-area file systems, in which latencies are extremely high and prefetching at the file level affords the opportunity for large savings. They later applied the same approach to general high-latency storage [GA94, GA95].
Reference: [GA94] <author> J. Griffioen and R. Appleton. </author> <title> Reducing file system latency using a predictive approach. </title> <booktitle> In USENIX Summer 1994 Technical Conference, </booktitle> <pages> pages 197-208, </pages> <month> June, </month> <year> 1994. </year> <title> (p 34) </title>
Reference-contexts: Griffioen and Appleton have presented a series of results of this flavor. Initially [GA93] they considered wide-area file systems, in which latencies are extremely high and prefetching at the file level affords the opportunity for large savings. They later applied the same approach to general high-latency storage <ref> [GA94, GA95] </ref>. Prediction of future file accesses is performed by a data structure called the probability graph, in which files are nodes, and edges 2.3 Related Work 35 represent sequential (or nearly sequential) accesses or invocations.
Reference: [GA95] <author> J. Griffioen and R. Appleton. </author> <title> Performance measurements of automatic prefetching. </title> <booktitle> In Proc. of the ISCA International Conference on Parallel and Distributed Computing Systems, </booktitle> <month> September </month> <year> 1995. </year> <title> (p 34) </title>
Reference-contexts: Griffioen and Appleton have presented a series of results of this flavor. Initially [GA93] they considered wide-area file systems, in which latencies are extremely high and prefetching at the file level affords the opportunity for large savings. They later applied the same approach to general high-latency storage <ref> [GA94, GA95] </ref>. Prediction of future file accesses is performed by a data structure called the probability graph, in which files are nodes, and edges 2.3 Related Work 35 represent sequential (or nearly sequential) accesses or invocations.
Reference: [GA96] <author> J. Griffioen and R. Appleton. </author> <title> The design, implementation, and evaluation of a predictive caching file system. </title> <type> Technical Report CS-264-96, </type> <institution> Kentucky University, </institution> <month> June </month> <year> 1996. </year> <title> (p 35) </title>
Reference-contexts: The graph is used to predict future file accesses, and the predictions are then used to initiate prefetching. They showed that cache miss rates can be reduced by up to 40% for some workloads. In <ref> [GA96] </ref> the same authors give an implementation of the system under SunOS. In the domain of predictive prefetching for mobile computing, Kuenning et al. [KPR94] analyzed trace data and suggested that the necessary cache contents could be generated automatically using predictive approaches.
Reference: [GJ91] <author> A.S. Grimshaw and E.C. Loyot Jr. </author> <title> ELFS: Object-oriented extensible file systems. </title> <type> Technical Report Computer Science Technical Report No. </type> <institution> TR-91-14, University of Virginia, </institution> <year> 1991. </year> <title> (p 34) </title>
Reference-contexts: It is possible, for instance, to load a web page then specify a dynamic set containing all links on the page. The server could then prefetch the links in parallel. 34 Systems Introduction Grimshaw and Loyot <ref> [GJ91] </ref> present ELFS, an extensible object-oriented file system with build-in support for prefetching and cache management. Objects can specify appropriate prefetching and caching behaviors based on specific knowledge about access patterns and re-use.
Reference: [HC92] <author> Kieran Harty and David R. Cheriton. </author> <title> Application-controlled physical memory using external page-cache management. </title> <booktitle> In The Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 187-197, </pages> <month> October </month> <year> 1992. </year> <title> (p 38) </title>
Reference-contexts: Based on this conclusion, he advocates prepaging advice given by the programmer or the compiler to the system. Several researchers have incorporated user-level paging managers into existing operating systems. McNamee and Armstrong [MA90] added user-level paging control to Mach; Harty and Cheriton <ref> [HC92] </ref> extended the V++ kernel in the same way.
Reference: [Hol94] <author> Mark Holland. </author> <title> On-Line Data Reconstruction in Redundant Disk Arrays. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1994. </year> <pages> (pp 54, 56) </pages>
Reference-contexts: All read system calls are for an entire block; 2922 blocks are read, and no block is read more than once. 1 hint 2923 consume 7193 3.5 The Simulator The simulator is built on top of the Berkeley RaidSim simulator [Lee89, CP90, LK91], as modified by Mark Holland <ref> [Hol94] </ref> at CMU. RaidSim can simulate various flavors of RAID disk arrays using an internal geometry-aware disk simulator to determine disk access times. <p> There were several options for generating an accurate disk model. The first was to use the existing model from RaidSim [Lee89], which had been modified substantially by Mark Holland <ref> [Hol94] </ref>. Using this model was unattractive because the existing code did not include an on-disk readahead cache, and work of Ruemmler and Wilkes [RW94] has shown that disk caching can have a significant effect on model accuracy.
Reference: [HP96] <author> J. Hennessy and D. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year> <title> (p 115) </title>
Reference-contexts: A single process performing a cyclic access pattern is difficult to cache for if fewer buffers are dedicated to caching than the size of the working set then there will be no re-use. On the other hand, re-use according to the traditional LRU hit-rate curve <ref> [HP96] </ref> is more forgiving | the allocator may dedicate a small fraction of the working set, but still realize a large fraction of potential cache hits. This type of re-use is typically an aggregate over a number of background processes executing simultaneously.
Reference: [IS95] <author> S. Irani and S. Seiden. </author> <title> Randomized algorithms for metrical task systems. </title> <booktitle> In Workshop on Algorithms and Data Structures, 1995. (p 147) </booktitle>
Reference-contexts: A weaker bound of (log log k) due to Karloff, Rabani and Ravid [KRR91], subsequently improved to q by Blum, Karloff, Rabani and Saks [BKRS92], applies to every metric space. Irani and Seiden give an en=(e 1)-competitive randomized MTS algorithm <ref> [IS95] </ref> for any metric space. Early results on specific spaces have focused largely on the uniform space: Borodin, Linial and Saks [BLS92] give a 2H n upper bound and an H n lower bound. Irani and Seiden [IS95] give an H n +O ( p log n)-competitive algorithm, matching the lower <p> Irani and Seiden give an en=(e 1)-competitive randomized MTS algorithm <ref> [IS95] </ref> for any metric space. Early results on specific spaces have focused largely on the uniform space: Borodin, Linial and Saks [BLS92] give a 2H n upper bound and an H n lower bound. Irani and Seiden [IS95] give an H n +O ( p log n)-competitive algorithm, matching the lower bound to within lower-order terms. We give an O (log 2 n)-competitive algorithm for any weighted-cache task system.
Reference: [KE90] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Prefetching in file systems for MIMD multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(2) </volume> <pages> 218-230, </pages> <month> April </month> <year> 1990. </year> <title> (p 37) BIBLIOGRAPHY 193 </title>
Reference-contexts: Kotz and Ellis <ref> [KE90] </ref> consider prefetching with perfect knowledge of future accesses, and show that the technique often improves the cache hit ratio and reduces overall execution time. However in some situations prefetching actually hurts performance.
Reference: [KE91] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Practical prefetching techniques for parallel file systems. </title> <booktitle> In First Intl. Conf. on Parallel and Distributed Information Systems, </booktitle> <pages> pages 182-189, </pages> <address> Miami Beach, Florida, </address> <month> December 4-6 </month> <year> 1991. </year> <title> (p 37) </title>
Reference-contexts: Kotz and Ellis [KE90] consider prefetching with perfect knowledge of future accesses, and show that the technique often improves the cache hit ratio and reduces overall execution time. However in some situations prefetching actually hurts performance. In <ref> [KE91, KE93] </ref> the same authors extend their results to prefetch predictively rather than assuming a priori knowledge of future requests. They develop simple recognizers of traditional access patterns and develop prefetching policies that are specific to the patterns. They report that, in general, prefetching is beneficial in their setting.
Reference: [KE93] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Practical prefetching techniques for multiprocessor file systems. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1) </volume> <pages> 33-51, </pages> <month> January, </month> <year> 1993. </year> <title> (p 37) </title>
Reference-contexts: Kotz and Ellis [KE90] consider prefetching with perfect knowledge of future accesses, and show that the technique often improves the cache hit ratio and reduces overall execution time. However in some situations prefetching actually hurts performance. In <ref> [KE91, KE93] </ref> the same authors extend their results to prefetch predictively rather than assuming a priori knowledge of future requests. They develop simple recognizers of traditional access patterns and develop prefetching policies that are specific to the patterns. They report that, in general, prefetching is beneficial in their setting.
Reference: [KK96a] <author> T. Kimbrel and A. Karlin. </author> <title> Integrated parallel prefetching and caching. </title> <type> Technical Report UW-CSE-96-01-10, </type> <institution> University of Washington, </institution> <year> 1996. </year> <title> (p 31) </title>
Reference-contexts: Al 4 Before developing forestall, our collaboration also considered a fourth algorithm called reverse-aggressive, due to Kimbrel and Karlin, which is guaranteed to be within a small constant factor of optimal on every sequence for a particular system model <ref> [KK96a] </ref>. This is the only algorithm for which such a guarantee is known. We found that reverse-aggressive performs well in all situations, but is difficult and expensive to implement, while forestall performs as well as reverse-aggressive and is both simpler and cheaper to implement.
Reference: [KK96b] <author> T. Kimbrel and A. Karlin. </author> <title> Near-optimal parallel prefetching and caching. </title> <booktitle> In Proceedings of the 1996 IEEE Symposium on Foundations of Computer Science, </booktitle> <month> October, </month> <year> 1996. </year> <pages> (pp 64, 181) </pages>
Reference-contexts: Forestall, described below, matched reverse-aggressive's performance and is much simpler to implement; therefore, I simulate forestall and present it in detail. For completeness, I give a brief description of reverse-aggressive| see Kimbrel and Karlin <ref> [KK96b] </ref> for details. Briefly, reverse-aggressive constructs a prefetching schedule for the reversed sequence that replaces at most one block on each disk in parallel as follows: Whenever a disk is free, determine the block B not needed for the longest time on that disk. <p> A model to address this difficulty using the competitive framework is given by Cao, Karlin et al. [CFKL95b], and by Kimbrel and Karlin in the multi-disk case <ref> [KK96b] </ref>. Their model counts I/O stall time rather than simply I/O time, but still does not take into account other components of I/O service time (e.g., T driver ). It is not known whether their results can be extended to a model with positive T driver .
Reference: [KLVA93] <author> Keith Krueger, David Loftesness, Amin Vahdat, and Tom Anderson. </author> <title> Tools for the development of application-specific virtual memory management. </title> <booktitle> In OOPSLA 1993 Conference Proceedings, </booktitle> <pages> pages 48-64, </pages> <month> October </month> <year> 1993. </year> <title> (p 38) </title>
Reference-contexts: Several researchers have incorporated user-level paging managers into existing operating systems. McNamee and Armstrong [MA90] added user-level paging control to Mach; Harty and Cheriton [HC92] extended the V++ kernel in the same way. Finally, Krueger et al. <ref> [KLVA93] </ref> present a general toolkit and set of profiling tools to provide user-level control of paging. 2.3.5 Prefetching From Main Memory There is a large body of work on prefetching from main memory, rather than to main memory.
Reference: [KMMO94] <author> A.R. Karlin, M.S. Manasse, L.A. McGeoch, and S. Owicki. </author> <title> Competitive randomized algorithms for nonuniform problems. </title> <journal> Algorithmica, </journal> <volume> 11 </volume> <pages> 542-571, </pages> <year> 1994. </year> <pages> (pp 146, 151, 172) </pages>
Reference-contexts: The distance between two points is simply the arc length of the unique path connecting the points in the tree. Chrobak, Karloff, Payne and Vishwanathan [CKPV90] give a k-competitive deterministic algorithm called double coverage for scheduling on the line, as described in Section 7.3.1. Karlin, Manasse, McGeoch and Owicki <ref> [KMMO94] </ref> consider the spin-block problem described in Section 7.3.1. They give a randomized algorithm for the problem with competitive ratio approaching e=(e 1) ~ = 1:58, an improvement upon the traditional deterministic algorithm which has (optimal deterministic) competitive ratio 2. <p> The algorithm begins with its hole in the (i 1)-point space. As the adversary continues to request points from that space, the algorithm slowly moves its probability mass (the probability of the hole being located at any given point) towards point i, using probabilities based on the algorithm of <ref> [KMMO94] </ref> for 3-point spaces. Finally, when point i is requested, all the probability mass moves back to the (i 1)-point space and the process is repeated. To counter such an algorithm, the adversary may adopt one of two strategies. <p> Typically competitive analysis is applied to sequences that are computer generated at a small timescale; for instance, deciding what to cache [MMS90, FKM + 91], moving disk heads to serve requests for disk blocks [CKPV90], or deciding whether a process should spin or block <ref> [KMMO94] </ref>. Here, we consider the problem of interacting with a user. In our model, a user presents a request which the algorithm must service, waits for some amount of time, then presents the next request. The amount of free time between requests is limited but unknown to the algorithm.
Reference: [KMRS88] <author> A.R. Karlin, M.S. Manasse, L. Rudolph, and D.D. Sleator. </author> <title> Competitive snoopy caching. </title> <journal> Algorithmica, </journal> <volume> 3(1) </volume> <pages> 79-119, </pages> <year> 1988. </year> <title> (p 138) </title>
Reference-contexts: formally presents the k-server problems and the Metrical Task Systems, two common classes of online problems that refine the definition above; all the results in this part of the thesis concern these two classes. 7.2 Competitive Analysis: A Metric for Online Prob lems In this section I describe Competitive Analysis <ref> [ST85, KMRS88] </ref>, a metric developed by Sleator and Tarjan twelve years ago that has become successful for analyzing online problems. Let OPT (), the optimal o*ine cost of sequence , be the lowest cost attainable by any algorithm on sequence , including algorithms that know the entire sequence in advance. <p> These definitions are due to Karlin, Manasse, Rudolph and Sleator <ref> [KMRS88] </ref>. In other work, the competitive ratio 1 This is sometimes referred to as weak competitiveness, distinguished from strong competitiveness in which c = 0. 7.3 Sub-Classes of Online Problems 139 is sometimes referred to as the competitive factor , and is related to the regret ratio of theoretical finance.
Reference: [Kor90] <author> Kim Korner. </author> <title> Intelligent caching for remote file service. </title> <booktitle> In Proceedings of the 10th Intl. Conf. on Distributed Computing Systems, </booktitle> <pages> pages 220-226, </pages> <year> 1990. </year> <title> (p 35) </title>
Reference-contexts: Later work by Lei and Duchamp [LD97] gives an implementation of such a prefetching scheme, here based on access trees. Their results show a reduction in cache miss rate of up to 47%, and a reduction in application latency of up to 40%. Korner <ref> [Kor90] </ref> presents a predictive technique based on file extenders that is used primarily to perform informed cache management, although the work also includes a prefetching component. An example rule used in the cache manager might be, if a file ends with ".out" then don't cache the file.
Reference: [Kot94] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proc. of the 1st USENIX Symp. on Operating Systems Design and Implementation, </booktitle> <address> Mon-terey, CA, </address> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year> <title> (p 37) </title>
Reference-contexts: They develop simple recognizers of traditional access patterns and develop prefetching policies that are specific to the patterns. They report that, in general, prefetching is beneficial in their setting. Kotz <ref> [Kot94] </ref> goes on to argue for Disk-Directed I/O , an informed prefetching technique in which high-level structure is passed to the I/O subsystems.
Reference: [KPR94] <author> G. Kuenning, G. Popek, and P. Reiher. </author> <title> An analysis of trace data for predictive file caching in mobile computing. </title> <booktitle> In Proc. of the Summer 1994 USENIX conference, </booktitle> <month> June, </month> <year> 1994. </year> <title> (p 35) </title>
Reference-contexts: They showed that cache miss rates can be reduced by up to 40% for some workloads. In [GA96] the same authors give an implementation of the system under SunOS. In the domain of predictive prefetching for mobile computing, Kuenning et al. <ref> [KPR94] </ref> analyzed trace data and suggested that the necessary cache contents could be generated automatically using predictive approaches.
Reference: [KRR91] <author> H.J. Karloff, Y. Rabani, and Y. Ravid. </author> <title> Lower bounds for randomized k-server algorithms. </title> <booktitle> In Proceedings of the 23rd Annual ACM Symposium on Theory of Computing, 1991. </booktitle> <volume> (pp 143, 146, 147, 149, 150) 194 BIBLIOGRAPHY </volume>
Reference-contexts: We also give an (log k) lower bound on the competitive ratio of any such algorithm for every fixed weighted-cache space, extending and simplifying results of <ref> [KRR91] </ref>. <p> Two types of lower bounds are known for randomized MTS algorithms. For certain specific metric spaces such as the uniform space studied by Borodin, Linial and Saks [BLS92] and the super-increasing space of Karloff, Rabani and Ravid <ref> [KRR91] </ref> there are 7.6 Related Work 147 (log k) lower bounds on the competitive ratio of any online algorithm. We show an (log k) lower bound for any weighted cache space. A weaker bound of (log log k) due to Karloff, Rabani and Ravid [KRR91], subsequently improved to q by Blum, <p> space of Karloff, Rabani and Ravid <ref> [KRR91] </ref> there are 7.6 Related Work 147 (log k) lower bounds on the competitive ratio of any online algorithm. We show an (log k) lower bound for any weighted cache space. A weaker bound of (log log k) due to Karloff, Rabani and Ravid [KRR91], subsequently improved to q by Blum, Karloff, Rabani and Saks [BKRS92], applies to every metric space. Irani and Seiden give an en=(e 1)-competitive randomized MTS algorithm [IS95] for any metric space. <p> This chapter gives an O (log 2 k)-competitive randomized algorithm for weighted caching on (k + 1)-point spaces (also known Cat-and-Mouse problems or Pursuit-Evasion games [BKRS92]). We also give an (log k) lower bound on the competitive ratio of any such algorithm, extending and simplifying results of <ref> [KRR91] </ref>. All work described in this chapter was joint with Avrim Blum and Merrick Furst. <p> Consider the smallest positive i 0 such that r i 0 log c 2 k. For all i i 0 1 This value is sometimes called the work function. 2 This definition is somewhat different from the super-increasing space used by <ref> [KRR91] </ref> in which distances increase at an even faster rate. 8.2 The Super-Increasing Algorithm 151 we must have r i r i+1 r i + O (1=k). Therefore, the final ratio, r k , can be no larger than O (log k).
Reference: [KS92] <author> James J. Kistler and M. Satyanarayanan. </author> <title> Disconnected operation in the coda file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 1-25, </pages> <month> February </month> <year> 1992. </year> <title> (p 34) </title>
Reference-contexts: A number of researchers in mobile computing have considered the problem of prefetch-ing data to allow disconnected operation. Kistler and Satyanarayanan of the CMU Coda project <ref> [KS92] </ref> are concerned with providing an environment for a completely disconnected user. They take the informed approach that user-level tools can allow an appropriate cache state to be specified by hand, and this data can then be fetched or kept resident when disconnected operation is expected.
Reference: [KTP + 96] <author> T. Kimbrel, A. Tomkins, R.H. Patterson, B. Bershad, P. Cao, E.W. Felten, G. Gibson, A. Karlin, and K. Li. </author> <title> A trace-driven comparison of algorithms for parallel prefetching and caching. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 19-34, </pages> <year> 1996. </year> <pages> (pp 20, </pages> <booktitle> 30, </booktitle> <volume> 33, 36, 59, 66, 70, 87, 132, </volume> <pages> 133) </pages>
Reference-contexts: Second is another existing system from the literature: lru-sp/aggressive, by Cao et al. [CFKL95b, CFL94a]. These two systems take different approaches to prefetching within a single stream of hints. Tracy Kimbrel and myself with a number of collaborators compared the two internal prefetching algorithms on single hinted streams <ref> [KTP + 96] </ref>, and found that a hybrid of the two approaches called forestall performed better than either. Therefore, I also consider each of these two systems augmented to include forestall-based prefetching. The extension of lru-sp/ aggressive is straightforward and results in lru-sp/forestall. <p> This study focuses on the case of a single process prefetching and caching its data; the results appear in <ref> [KTP + 96] </ref>. We draw several conclusions. To summarize, TIP2 will sometimes allow the disk to idle in situations that would benefit from more aggressive prefetching. Lru-sp/aggressive on the other hand can sometimes prefetch too aggressively. <p> Chapter 5 addresses the SPACE problem: prefetching and caching within a single stream. These results were derived in a recent collaborative study with Kimbrel, Patter-son, Cao, Gibson, Karlin, Bershad, Felten, and Li <ref> [KTP + 96] </ref>, which analyzes prefetching and caching algorithms in the context of a single process disclosing all its accesses at startup. In this domain, all resources are dedicated to the single hinted stream, so the allocation problem does not arise, and only the SPACE problem need be solved. <p> My work is also closely related, and in some cases collaborative with, work of Pei Cao, Tracy Kimbrel, Anna Karlin and others on lru-sp, aggressive and related algorithms <ref> [KTP + 96, Cao96, CFKL95b, CFKL95a, CFL94a, CFL94b] </ref>. Upcoming chapters describe these systems in detail, so I will not include a discussion of them here. At a high level, the work in this thesis represents an approach to providing and using more detailed access information in the I/O subsystem. <p> Work on TIP2 [PGG + 95] addressed both parts of the problem but was more strongly focused on allocation of resources among multiple processes. Once the system was built, more recent work <ref> [KTP + 96, TPG97] </ref> focused more specifically on cache management within a process, and on integrating the two problems. 2.3 Related Work 37 dynamic algorithm that takes re-use into account. <p> TIP2 and lru-sp/aggressive take different approaches to both problems. I begin by describing these two systems. Next, I focus on the SPACE problem, comparing the approaches taken by these two systems, and describing collaborative work with Kimbrel, Karlin, Patterson, Gibson, Cao, Bershad, Felten and Li <ref> [KTP + 96] </ref>. TIP2 is limited in how deeply it will prefetch ahead in to the request stream, while lru-sp/aggressive prefetches as deeply as resources allow. <p> This work was joint between the Parallel Data Lab (Patterson, Gibson and myself), and several other researchers at Washington and elsewhere (Kimbrel, Karlin, Cao and others) <ref> [KTP + 96] </ref>. Consider the TIP2 system described above running in a single-process fully-hinted domain. As there are no unhinted accesses, the value of maintaining a large LRU cache will be negligible so the entire buffer cache will be dedicated to hinted caching. <p> integer such that r i is the first reference to an uncached block and X jijdisk (j)=d Incore (j) T disk &gt; i T app : (4.2) This specification of a constrained disk is very similar to that used by the forestall algorithm as presented in our earlier collaborative study <ref> [KTP + 96] </ref>, but it differs in two details. <p> This work began as a joint study <ref> [KTP + 96] </ref> between myself, Hugo Patterson and Garth Gibson at the Parallel Data Lab at CMU; Tracy Kimbrel, Anna Karlin and Brian Bershad at the University of Washington; Pei Cao at the University of Wisconsin; and Kai Li and Ed Felten at Princeton University. <p> Furthermore, experiment 9 shows that queue depth can interact with stripe unit size to give non-intuitive results | sometimes adding disks can actually decrease the average I/O time. Lesson 7: Leaving a constrained disk idle leads to additional stall. This effect was documented in <ref> [KTP + 96] </ref> for the single-process case. We discuss it in Figure 4.3, and mention it here because it arises in the two-process case as well. <p> This effect alone is not responsible for all the difference between the two algorithms in these experiments; Lesson 3 is the primary contributor to the disparity. Lesson 8: Submitting an I/O requires T driver computational overhead. This effect was also documented in the single-process case in <ref> [KTP + 96] </ref>. We discuss it in Figure 4.4, and it appears in experiments 1, 2, 5, 8 and 9 on larger array sizes. In experiment 1, for instance, lru-sp/aggressive on ten disks incurs 52% more driver overhead than tiptoe.
Reference: [KTR94] <author> David Kotz, Song Bac Toh, and Sriram Radhakrishnan. </author> <title> A detailed simulation model of the hp 97650 disk drive. </title> <type> Technical Report PCS-TR94-220, </type> <institution> Dartmouth University, </institution> <month> July 18, </month> <year> 1994. </year> <pages> (pp 42, 56) </pages>
Reference-contexts: Most experiments are performed using CSCAN scheduling, following [SCO90]. The disk simulator, described in Section 3.6, simulates the HP97560 disk drive <ref> [RW94, KTR94] </ref>; it models head position, and computes seek, rotate and transfer latencies in the usual manner. It also includes a non-segmented on-disk cache, and provides a simple model of SCSI bus overhead. The simulator has been validated against the simulator of Kotz et al. [KTR94]. <p> It also includes a non-segmented on-disk cache, and provides a simple model of SCSI bus overhead. The simulator has been validated against the simulator of Kotz et al. <ref> [KTR94] </ref>. Since the traces capture disk blocks, the simulator correctly models file layout on the disk. 3.1.2 Processor Scheduling The simulator has an embedded round-robin CPU scheduling algorithm. <p> Also, the existing model did not have CSCAN queueing, which is commonly used and has been shown effective on modern drives [SCO90]. The next option was to incorporate an off-the-shelf public-domain disk model. In particular the model of Kotz et al. <ref> [KTR94] </ref> is widely used. I decided not to take this approach because incorporating a new disk model into RaidSim, and matching the thread interfaces, seemed like a significant project. Instead, I implemented readahead and CSCAN queueing, and then validated the resulting system again the Kotz model.
Reference: [LD97] <author> Hui Lei and Dan Duchamp. </author> <title> An analytical approach to file prefetching. </title> <booktitle> In 1997 USENIX Annual Technical Conference, </booktitle> <address> Anaheim CA, </address> <month> January </month> <year> 1997. </year> <title> (p 35) </title>
Reference-contexts: Tait and Duchamp [TD91] give a predictive mechanism called the working graph; they show that predictive prefetching using this structure performs better than LRU and has small overhead, for the domain of low-bandwidth connected mobile computing. Later work by Lei and Duchamp <ref> [LD97] </ref> gives an implementation of such a prefetching scheme, here based on access trees. Their results show a reduction in cache miss rate of up to 47%, and a reduction in application latency of up to 40%.
Reference: [Lee89] <author> Edward K. Lee. </author> <title> The performance of parity placements in disk arrays. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(6) </volume> <pages> 651-664, </pages> <month> June </month> <year> 1989. </year> <pages> (pp 32, 54, 56) </pages>
Reference-contexts: They also capture context switches, allowing an accurate measurement of inter-access process compute time. I developed a disk-accurate, trace-driven simulator based on the RaidSim simulator <ref> [Lee89] </ref>, which generated the results given in Chapters 5 and 6. Details about the simulator, the applications, and the traces are given in Chapter 3. I describe three primary results. First, in the single-process domain forestall outperforms both conservative and aggressive non-adaptive techniques. <p> All read system calls are for an entire block; 2922 blocks are read, and no block is read more than once. 1 hint 2923 consume 7193 3.5 The Simulator The simulator is built on top of the Berkeley RaidSim simulator <ref> [Lee89, CP90, LK91] </ref>, as modified by Mark Holland [Hol94] at CMU. RaidSim can simulate various flavors of RAID disk arrays using an internal geometry-aware disk simulator to determine disk access times. <p> There were several options for generating an accurate disk model. The first was to use the existing model from RaidSim <ref> [Lee89] </ref>, which had been modified substantially by Mark Holland [Hol94]. Using this model was unattractive because the existing code did not include an on-disk readahead cache, and work of Ruemmler and Wilkes [RW94] has shown that disk caching can have a significant effect on model accuracy.
Reference: [LHR90] <author> Kai-Fu Lee, Hsiao-Wuen Hon, and Raj Reddy. </author> <title> An overview of the SPHINX speech recognition system. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, (USA), </journal> <volume> 38(1) </volume> <pages> 35-45, </pages> <month> Jan, </month> <year> 1990. </year> <title> (p 46) </title>
Reference-contexts: Postgres then discloses these pre-computed block addresses to TIP. In the second pass, Postgres rereads the outer relation without hints but skips the index lookup and instead directly reads the hinted inner-relation tuples whose addresses are stored in the array. 3.2.5 Sphinx Sphinx <ref> [LHR90] </ref> is a high-quality, speaker-independent, continuous-voice, speech-recognition system. In our experiments, Sphinx recognizes an 18-second recording commonly used in Sphinx regression testing. Sphinx represents acoustics with Hidden Markov Models and uses a Viterbi beam search to prune unpromising word combinations from its current Viterbi search graph.
Reference: [LK91] <author> Edward K. Lee and Randy H. Katz. </author> <title> Performance consequences of parity placement in disk arrays. </title> <booktitle> In ASPLOS4, </booktitle> <pages> pages 190-199. </pages> <publisher> ACM, </publisher> <year> 1991. </year> <title> (p 54) </title>
Reference-contexts: All read system calls are for an entire block; 2922 blocks are read, and no block is read more than once. 1 hint 2923 consume 7193 3.5 The Simulator The simulator is built on top of the Berkeley RaidSim simulator <ref> [Lee89, CP90, LK91] </ref>, as modified by Mark Holland [Hol94] at CMU. RaidSim can simulate various flavors of RAID disk arrays using an internal geometry-aware disk simulator to determine disk access times.
Reference: [MA90] <author> Dylan McNamee and Katherine Armstrong. </author> <title> Extending the Mach external pager interface to accomodate user-level page replacement policies. </title> <booktitle> In Proc. of the USENIX Association Mach Workshop, </booktitle> <pages> pages 17-29, </pages> <year> 1990. </year> <title> (p 38) </title>
Reference-contexts: Based on this conclusion, he advocates prepaging advice given by the programmer or the compiler to the system. Several researchers have incorporated user-level paging managers into existing operating systems. McNamee and Armstrong <ref> [MA90] </ref> added user-level paging control to Mach; Harty and Cheriton [HC92] extended the V++ kernel in the same way.
Reference: [MDK96] <author> T. Mowry, A. Demke, and O. Krieger. </author> <title> Automatic compiler-inserted I/O prefetching for out-of-core applications. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <year> 1996. </year> <pages> (pp 18, 29, 34) </pages>
Reference-contexts: Patterson, Gibson et al. argued for the feasibility of hints by modifying a wide range of I/O-intensive applications to provide hints. Mowry et al. showed that for some workloads, the compiler can be augmented to provide hints automatically <ref> [MDK96] </ref>. So there are strong indications that applications can be written or compiled to provide hints, and there is evidence from the work cited above that hints can provide a dramatic reduction in application stall time. <p> If the software has already been written but the source code is available, recent work by Mowry, Demke and Kreiger has shown that compilers can induce some programs to disclose their future accesses automatically <ref> [MDK96] </ref>, especially in the realm of scientific computing. Section 2.3 describes these results in more detail. Finally, although no research has yet been published about this approach, speculative execution offers the opportunity to discover future reads that are not too strongly dependent on data that has not yet arrived. <p> They take the informed approach that user-level tools can allow an appropriate cache state to be specified by hand, and this data can then be fetched or kept resident when disconnected operation is expected. Finally, Mowry, Demke and Krieger <ref> [MDK96] </ref> present a compiler to produce executables that automatically generate prefetch requests for data that will be accessed in the near future. The compiler is designed for scientific codes, and performs static analysis to uncover many common access patterns, including complex strided accesses.
Reference: [MJLF84] <author> M. K. McKusick, W. J. Joy, S. J. Le*er, and R. S. Fabry. </author> <title> A fast file system for Unix. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 181-197, </pages> <month> August </month> <year> 1984. </year> <title> (p 34) </title>
Reference-contexts: The technique successfully hid more than half the I/O latency is seven of eight NAS parallel benchmarks. Predictive Disk Prefetching The most common incarnation of predictive prefetching is sequential readahead, ranging from one-block lookahead as described by Smith [Smi85] and implemented in Multics [FO71] and Unix <ref> [MJLF84] </ref> to more aggressive versions such as Digital's OSF 1 operating system, which will prefetch up to sixty-four 8KByte blocks in advance. Similarly, there have been efforts to provide higher throughput by reading larger objects, as in [MK91].
Reference: [MK91] <author> L. W. McVoy and S. R. Kleiman. </author> <title> Extent-like performance from a UNIX file system. </title> <booktitle> In Proc. of 1991 Winter USENIX, </booktitle> <pages> pages 33-43, </pages> <year> 1991. </year> <title> (p 34) BIBLIOGRAPHY 195 </title>
Reference-contexts: Similarly, there have been efforts to provide higher throughput by reading larger objects, as in <ref> [MK91] </ref>. But recently several researchers have addressed predictive prefetching more speculatively, prefetching entire files before they are referenced, or predicting accesses across files.
Reference: [MLG92] <author> Todd C. Mowry, Monica S. Lam, and Anoop Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In The Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year> <title> (p 38) </title>
Reference-contexts: Chen and Baer [CB92] suggest a combination of hardware-based cache prefetching and hardware support for non-blocking caches. Tullsen and Eggers [TE93] showed that some architectures (in this case, a high-latency bus-based multiprocessor) are not well-suited for prefetching. Mowry, Lam and Gupta <ref> [MLG92] </ref>, on the other hand, showed that careful software-controlled compiler-directed prefetching for scientific codes provided substantial improvements in performance, reducing execution time of some benchmarks by a factor of 2.
Reference: [MMS88a] <author> M.S. Manasse, L.A. McGeoch, and D.D. Sleator. </author> <title> Competitive algorithms for on-line problems. </title> <booktitle> In Proceedings of the 20th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 322-333, </pages> <year> 1988. </year> <pages> (pp 22, 146) </pages>
Reference-contexts: The cost of servicing a sequence of requests is the sum of the costs of servicing each request. Weighted caching has been studied in its own right since online algorithms were first introduced in <ref> [MMS88a] </ref>. We came to the problem, however, because it was central to an extension we were considering to the traditional online model. In the standard model, an algorithm is asked to process a request sequence. <p> There are several results known for weighted caching, described in Section 7.4, a natural extension of the standard caching problem that corresponds to the k-server problem on the uniform space. The problem was first described by Manasse, McGeoch and Sleator <ref> [MMS88a] </ref> as an example of an asymmetric server problem. In [RS89] Raghavan and Snir presented their harmonic algorithm, an O (k)-competitive randomized algorithm and the first competitive algorithm for the problem.
Reference: [MMS88b] <author> M.S. Manasse, L.A. McGeoch, and D.D. Sleator. </author> <title> Competitive algorithms for server problems. </title> <type> Technical Report CMU-CS-88-197, </type> <institution> Carnegie Mellon University, </institution> <year> 1988. </year> <pages> (pp 140, 145, 146) </pages>
Reference-contexts: work in this thesis addresses two such classes: the k-server problems (or simply server problems) and the metrical task systems. 2 For competitive algorithms, Murphy's law is a correct and complete representation of reality. 140 Theory Overview 7.3.1 k-Server Problems The k-server problems were introduced by Manasse, McGeoch and Sleator <ref> [MMS88b, MMS90] </ref>. A server problem is defined by an n-vertex graph with a distance d on the vertices. There are k servers inhabiting vertices of the graph. A request is simply a vertex, and represents the requirement that a server be moved to that vertex. <p> Finally, we present an algorithm that a computer might use to pre-process potential future commands while waiting for a user to type. The algorithm takes into account the relative durations of possible future instructions. 7.6 Related Work The general k-server problem was first presented by Manasse, McGeoch and Sleator <ref> [MMS88b] </ref>, who also show a lower bound of k on the competitive ratio of any deterministic server algorithm. The uniform space, described above in Section 7.3.1, was initially studied by Sleator and Tarjan [ST85], who showed that LRU and FIFO are k-competitive.
Reference: [MMS90] <author> M.S. Manasse, L.A. McGeoch, and D.D. Sleator. </author> <title> Competitive algorithms for server problems. </title> <journal> Journal of Algorithms, </journal> <volume> 11 </volume> <pages> 208-230, </pages> <year> 1990. </year> <pages> (pp 140, 166, 170, 172) </pages>
Reference-contexts: work in this thesis addresses two such classes: the k-server problems (or simply server problems) and the metrical task systems. 2 For competitive algorithms, Murphy's law is a correct and complete representation of reality. 140 Theory Overview 7.3.1 k-Server Problems The k-server problems were introduced by Manasse, McGeoch and Sleator <ref> [MMS88b, MMS90] </ref>. A server problem is defined by an n-vertex graph with a distance d on the vertices. There are k servers inhabiting vertices of the graph. A request is simply a vertex, and represents the requirement that a server be moved to that vertex. <p> In fact, there exists a (natural) metric space in which one can achieve a deterministic ratio of (k + 1)=2 with free time, in contrast to the lower bound of k <ref> [MMS90] </ref> in the standard model. Theorem 12 For the metric space of (k + 1) equally spaced points on the line, there is a deterministic k-server algorithm with free time that achieves competitive ratio (k + 1)=2. <p> We restrict our attention to algorithms that are lazy in the sense that they move the hole only when the adversary hits the hole; we do not know whether a loss of generality is involved in this claim (no loss of generality is involved for the traditional model | see <ref> [MMS90] </ref>). All algorithms given in this paper are lazy, or can be converted to lazy algorithms in a straightforward manner. Finally, we restrict our attention to metric spaces whose points are nodes of an unweighted graph, and whose distances are given by the shortest path between the points. <p> This completes the theorem. 172 Free Time 9.7 Free Time For Command Processing In this section we incorporate free time into a simple model of command processing. Typically competitive analysis is applied to sequences that are computer generated at a small timescale; for instance, deciding what to cache <ref> [MMS90, FKM + 91] </ref>, moving disk heads to serve requests for disk blocks [CKPV90], or deciding whether a process should spin or block [KMMO94]. Here, we consider the problem of interacting with a user.
Reference: [Nat89] <institution> National Center for Supercomputing Applications. XDataSlice for the X window system. Technical Report http://www.nsca.uiuc.edu/, University of Illinois at Urbana-Champaign, </institution> <year> 1989. </year> <title> (p 44) </title>
Reference-contexts: Annotating this code to give hints was straightforward: at the start of each iteration, Davidson discloses the whole-file sequential read anticipated in the next iteration. 3.2.2 Xds XDataSlice (Xds) is an interactive scientific visualization tool developed at the National Center for Supercomputer Applications at the University of Illinois <ref> [Nat89] </ref>. Among other features, Xds lets scientists view arbitrary planar slices through their 3-dimensional data with a false color mapping. The datasets may originate from a broad range of applications such as airflow simulations, pollution modeling, or magnetic resonance imaging, and tend to be very large.
Reference: [NFS91] <author> Raymond Ng, Christos Faloutsos, and Timos Sellis. </author> <title> Flexible buffer allocation based on marginal gains. </title> <booktitle> In Proc. of the 1991 ACM Conf. on Management of Data (SIGMOD), </booktitle> <pages> pages 387-396, </pages> <year> 1991. </year> <pages> (pp 37, 60) </pages>
Reference-contexts: Thus, a page that has been accessed once but is not re-used is given small value. Another line of research combines dynamic and static information. Most closely related to the work described in this thesis is the marginal gains approach of Ng, Faloutsos and Sellis <ref> [NFS91] </ref> (in fact, their approach inspired the cost-benefit approach of TIP2). They define the marginal gain of using s buffers to service a particular reference as the difference between the expected number of faults with s buffers versus s 1 buffers. <p> SPACE algorithm has been referred to elsewhere as fixed-horizon, since prefetches are submitted only for missing blocks within a fixed "prefetch horizon." TIP2's approach to the allocation problem, cost-benefit analysis, is a powerful and general tool that has been successful in other domains as well | Ng, Faloutsos and Sellis <ref> [NFS91] </ref> apply the technique to database cache management. To serve multiple processes, each disclosing an arbitrary fraction of its accesses, the buffer cache manager must cache two distinct types of data.
Reference: [OOW93] <author> Elizabeth J. O'Neil, Patrick E. O'Neil, and Gerhard Weikum. </author> <title> The LRU-K page replacement algorithm for database disk buffering. </title> <booktitle> In Proc. of the 1993 ACM SIGMOD Conference on Management of Data, </booktitle> <pages> pages 297-306, </pages> <month> May </month> <year> 1993. </year> <title> (p 36) </title>
Reference-contexts: Alternatively, traditional buffer replacement under LRU is entirely dynamic in the sense that decisions are made solely on the basis of the buffer cache state, and are performed while the query executes. There have been other purely dynamic approaches. O'Neil et al. <ref> [OOW93] </ref> extend the LRU algorithm to LRU-k, a 7 The earlier working set model makes similar estimates based on the number of buffers a process touched, without reference to data re-use | thus processes that moved quickly through data, without re-use, would be given inappropriately large allocations. <p> The same effect arises in general file system cache management, and is in fact on the most significant effects differentiating the algorithms studied in this thesis. O'Neil et al. <ref> [OOW93] </ref> describe another approach to this problem, discussed below. 8 The same general line of research led to the work in this thesis. Work on TIP2 [PGG + 95] addressed both parts of the problem but was more strongly focused on allocation of resources among multiple processes.
Reference: [PF76] <author> B.G. </author> <title> Prieve and R.S. Fabry. VMIN | an optimal variable-space page replacement algorithm. </title> <journal> Communications of the ACM, </journal> <volume> 19(5) </volume> <pages> 295-297, </pages> <year> 1976. </year> <title> (p 39) </title>
Reference-contexts: Aho, Denning and Ullman [ADU71], and later Coffman and Denning [EGCD73], study page replacement under particular probabilistic models of future page arrivals. In the presence of variable-sized buffers there have been a number of extensions to this work, beginning with Prieve and Fabry's VMIN algorithm <ref> [PF76] </ref>, and more recently, work to derive practical versions of VMIN, such as Choi and Ruschitzka's work on SETVMIN [CR96]. In the case of multiple processes that give hints and compete for cache resources, Barve, Grove and Vitter [BGV95] describe a competitive approach to cache management.
Reference: [PG94] <author> R. Hugo Patterson and Garth Gibson. </author> <title> Exposing I/O concurrency with informed prefetching. </title> <booktitle> In Proceedings of the Third International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 7-16, </pages> <month> September </month> <year> 1994. </year> <note> Unpublished version in lab. (pp 18, 45) </note>
Reference-contexts: To address this problem, existing systems for Informed Prefetching and Caching, in which the application provides "hints" to the system about upcoming accesses, submit requests in parallel to make use of the large aggregate bandwidth of the disk array <ref> [PGG + 95, CFL94b, CFKL95b, PG94] </ref>. Figure 1.1 shows the average reduction in stall time provided by the TIP2 system of Patterson, Gibson et al. [PGG + 95], relative to Digital's OSF/1 operating system. <p> In the second half of the split loop, Xds reads the cached pixel mappings, reads the corresponding data from the cached blocks, and applies the false coloring <ref> [PG94] </ref>. Our test dataset consists of 512 3 32-bit floating point values requiring 512 MByte of disk storage. The dataset is organized into 8 KByte blocks of 16x16x8 data points, and is stored on the disk in Z-major order.
Reference: [PGG + 95] <author> R. Hugo Patterson, Garth A. Gibson, Eka Ginting, Daniel Stodolsky, and Jim Zelenka. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 79-95, </pages> <month> December, </month> <year> 1995. </year> <pages> (pp 18, </pages> <booktitle> 33, </booktitle> <volume> 36, 44, 59, 60, 80) 196 BIBLIOGRAPHY </volume>
Reference-contexts: To address this problem, existing systems for Informed Prefetching and Caching, in which the application provides "hints" to the system about upcoming accesses, submit requests in parallel to make use of the large aggregate bandwidth of the disk array <ref> [PGG + 95, CFL94b, CFKL95b, PG94] </ref>. Figure 1.1 shows the average reduction in stall time provided by the TIP2 system of Patterson, Gibson et al. [PGG + 95], relative to Digital's OSF/1 operating system. <p> Figure 1.1 shows the average reduction in stall time provided by the TIP2 system of Patterson, Gibson et al. <ref> [PGG + 95] </ref>, relative to Digital's OSF/1 operating system. On a large array with applications modified to provide the necessary hints, stall is reduced by approximately a factor of 6 to 17% of its original value under OSF/1. <p> time on average across a series of hinted two-process experiments by 12% relative to lru-sp/aggressive, 10% relative to lru-sp/forestall and 3% relative to TIP2. 2.3 Related Work The work in this dissertation is a direct extension of work by Hugo Patterson, Garth Gibson and their collaborators on the TIP2 system <ref> [PGG + 95] </ref>. My work is also closely related, and in some cases collaborative with, work of Pei Cao, Tracy Kimbrel, Anna Karlin and others on lru-sp, aggressive and related algorithms [KTP + 96, Cao96, CFKL95b, CFKL95a, CFL94a, CFL94b]. <p> O'Neil et al. [OOW93] describe another approach to this problem, discussed below. 8 The same general line of research led to the work in this thesis. Work on TIP2 <ref> [PGG + 95] </ref> addressed both parts of the problem but was more strongly focused on allocation of resources among multiple processes. <p> The particular applications were chosen to represent a broad range of problem domains. The changes made to these applications to allow them to generate hints (i.e., all the work described in Section 3.2) were done by Hugo Patterson 44 Simulation Environment and the other authors of <ref> [PGG + 95] </ref>. The text of this section is largely drawn from that paper. 3.2.1 Davidson The Multi-Configuration Hartree-Fock, MCHF, is a suite of computational-physics programs which we obtained from Vanderbilt University, where they are used for atomic-physics calculations. <p> Before this work two systems had been implemented: the TIP2 system of Patterson, Gibson et al. <ref> [PGG + 95] </ref> and the lru-sp/aggressive system of Cao et al. [CFKL95b] (this algorithm was embedded in a filesystem called ACFS, application-controlled file system, described in Pei Cao's thesis [Cao96]). <p> Extending lru-sp/aggressive, yielding lru-sp/forestall, is more straightforward. I present these two new algorithms and then discuss implementation details. 60 Algorithms 4.1 TIP2 The TIP2 system of Patterson, Gibson et al. <ref> [PGG + 95] </ref> presents an integrated approach to the SPACE problem and the allocation problem. I begin with a two-paragraph summary of the approach, and then give details. In the single-process model, TIP2 prefetches conservatively due to its system model, which assumes large disk arrays and no disk queueing. <p> I perform 80 Algorithms this profiling using the same "segmenting" technique as Patterson et al., described in <ref> [PGG + 95] </ref>. However we implement aging differently. Every 1000 requests Patterson et al. recompute a new LRU hit rate profile, and compute a weighted average of this new profile with the existing profile.
Reference: [PGK88] <author> David A. Patterson, Garth Gibson, and Randy H. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In International Conference on Management of Data (SIGMOD), </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year> <title> (p 18) </title>
Reference-contexts: The results in this part of the thesis benefit from collaboration with Garth Gibson, Hugo Patterson, and a number of other colleagues. I give precise acknowledgments as I describe each piece of collaborative work. 1.1.1 The Big Picture Storage Parallelism in the form of disk arrays <ref> [PGK88] </ref> has been advocated as a means to address the increasing gap between processor speed and I/O bandwidth [SGM86] (the so-called "I/O bottleneck" [PGK88, Smi85]) . But many workloads consist of streams of computation interspersed with synchronous I/O calls. <p> I give precise acknowledgments as I describe each piece of collaborative work. 1.1.1 The Big Picture Storage Parallelism in the form of disk arrays [PGK88] has been advocated as a means to address the increasing gap between processor speed and I/O bandwidth [SGM86] (the so-called "I/O bottleneck" <ref> [PGK88, Smi85] </ref>) . But many workloads consist of streams of computation interspersed with synchronous I/O calls. The program blocks for I/O from disk 1, computes again when the data arrives, and then blocks for I/O from disk 2, never benefiting from the parallelism of the disk array.
Reference: [PK94] <author> Christos Papadimitriou and Elias Koutsoupias. </author> <title> The work function algorithm is competitive. </title> <booktitle> In STOC 94, </booktitle> <pages> pages 507-511, </pages> <month> May, </month> <year> 1994. </year> <pages> (pp 146, 166) </pages>
Reference-contexts: They give a randomized algorithm for the problem with competitive ratio approaching e=(e 1) ~ = 1:58, an improvement upon the traditional deterministic algorithm which has (optimal deterministic) competitive ratio 2. In the deterministic setting, Papadimitriou and Koutsoupias <ref> [PK94] </ref> proved a longstanding conjecture that the work-function algorithm has competitive ratio polynomial in k for every metric space. In fact, they showed a competitive ratio of 2k 1, within a factor of 2 of optimal for every space. <p> Specifically, we show that for any metric space on at least (k+1) points, no deterministic server algorithm with free time can be better than (k + 1)=2-competitive. In light of results of <ref> [PK94] </ref>, who show that the Work Function algorithm for the standard k-server problem is 2k 1-competitive, free time provides at best a small improvement.
Reference: [PZ91] <author> Mark Palmer and Stanley B. Zdonik. </author> <title> FIDO: A cache that learns to fetch. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Data Bases, </booktitle> <pages> pages 255-264, </pages> <month> September, </month> <year> 1991. </year> <title> (p 35) </title>
Reference-contexts: An example rule used in the cache manager might be, if a file ends with ".out" then don't cache the file. In simulations the approach is shown to improve performance by up to 340%. There have also been approaches to predictive prefetching in the database community. Palmer and Zdonik <ref> [PZ91] </ref> present Fido, a cache manager that employs an associative memory technique to predict future accesses in a particular isolated context. They showed some early simulation results suggesting that predictive prefetching within the database domain is a promising approach.
Reference: [RL92] <author> Anne Rogers and Kai Li. </author> <title> Software support for speculative loads. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 38-50, </pages> <month> October </month> <year> 1992. </year> <title> (p 38) </title>
Reference-contexts: Baer and Chen [BC91] describe a hardware scheme for pre-loading a cache in the presence of sequential accesses to memory. Rogers and Li <ref> [RL92] </ref> describe a simple hardware modification and compiler support to allow speculative loading of data into cache, which results in significant reductions in average memory latency for a series of benchmarks. Chen and Baer [CB92] suggest a combination of hardware-based cache prefetching and hardware support for non-blocking caches.
Reference: [RS89] <author> P. Raghavan and M. Snir. </author> <title> Memory versus randomization in online algorithms. </title> <booktitle> In Proc. ICALP, 1989. (p 146) </booktitle>
Reference-contexts: The problem was first described by Manasse, McGeoch and Sleator [MMS88a] as an example of an asymmetric server problem. In <ref> [RS89] </ref> Raghavan and Snir presented their harmonic algorithm, an O (k)-competitive randomized algorithm and the first competitive algorithm for the problem. Subsequently Chrobak, Karloff, Payne and Vishwanathan [CKPV90] gave an O (k)-competitive deterministic algorithm called the BALANCE algorithm, and presented some hardness results involving asymmetric metric spaces.
Reference: [RW94] <author> Chris Ruemmler and John Wilkes. </author> <title> An introduction to disk drive modelling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <month> March, </month> <year> 1994. </year> <pages> (pp 42, 54, 56) </pages>
Reference-contexts: Most experiments are performed using CSCAN scheduling, following [SCO90]. The disk simulator, described in Section 3.6, simulates the HP97560 disk drive <ref> [RW94, KTR94] </ref>; it models head position, and computes seek, rotate and transfer latencies in the usual manner. It also includes a non-segmented on-disk cache, and provides a simple model of SCSI bus overhead. The simulator has been validated against the simulator of Kotz et al. [KTR94]. <p> In the experiments described below, data is striped over an array of disks (from 1-10 disks), with no parity and a stripe unit of eight 8 KByte blocks (64 KByte). The disk simulator, described below in Section 3.6, models the HP97560 disk drive <ref> [RW94] </ref>. The simulator also supports various forms of disk queueing; in our experiments all issued accesses are CSCAN sorted in the disk queues. RaidSim was modified to include a buffer cache module layered on top of the disk array, and modules for all the algorithms. <p> The first was to use the existing model from RaidSim [Lee89], which had been modified substantially by Mark Holland [Hol94]. Using this model was unattractive because the existing code did not include an on-disk readahead cache, and work of Ruemmler and Wilkes <ref> [RW94] </ref> has shown that disk caching can have a significant effect on model accuracy. In fact, their model is accurate to within a demerit of 5.7% (see below for the definition of this measure), but without caching the demerit grows to 112%.
Reference: [SAC + 79] <author> P. Griffiths Selinger, M. M. Astrahan, D. D. Chamberlin, R. A. Lorie, and T. G. Price. </author> <title> Access path selection in a relational database management system. </title> <booktitle> In Proc. of the 1979 ACM SIGMOD, </booktitle> <pages> pages 23-34, </pages> <address> Boston, MA, </address> <year> 1979. </year> <title> (p 35) </title>
Reference-contexts: Selinger et al. <ref> [SAC + 79] </ref>, 36 Systems Introduction for instance, use access knowledge within the DBMS, employing a simple model of page re-use to estimate the cost of various alternative access paths and join orders, taking into account both I/O and CPU costs.
Reference: [SCO90] <author> Margo Seltzer, Peter Chen, and John Ousterhout. </author> <title> Disk scheduling revisited. </title> <booktitle> In Proc. of USENIX Winter 1990 Technical Conference, </booktitle> <pages> pages 313-324, </pages> <year> 1990. </year> <pages> (pp 42, 56) </pages>
Reference-contexts: The simulator is described in detail in Section 3.5, but I give a high-level description here to summarize which effects are captured and which are not. 3.1.1 Disks and Disk Drivers The simulator implements a number of disk scheduling disciplines. Most experiments are performed using CSCAN scheduling, following <ref> [SCO90] </ref>. The disk simulator, described in Section 3.6, simulates the HP97560 disk drive [RW94, KTR94]; it models head position, and computes seek, rotate and transfer latencies in the usual manner. It also includes a non-segmented on-disk cache, and provides a simple model of SCSI bus overhead. <p> In fact, their model is accurate to within a demerit of 5.7% (see below for the definition of this measure), but without caching the demerit grows to 112%. Also, the existing model did not have CSCAN queueing, which is commonly used and has been shown effective on modern drives <ref> [SCO90] </ref>. The next option was to incorporate an off-the-shelf public-domain disk model. In particular the model of Kotz et al. [KTR94] is widely used. I decided not to take this approach because incorporating a new disk model into RaidSim, and matching the thread interfaces, seemed like a significant project.
Reference: [SF94] <author> A. Stathopoulos and C. F. Fischer. </author> <title> A Davidson program for finding a few selected extreme eigenpairs of a large, sparse, real, symmetric matrix. </title> <journal> Computer Physics Communications, </journal> <volume> 79 </volume> <pages> 268-290, </pages> <year> 1994. </year> <title> (p 44) </title>
Reference-contexts: The text of this section is largely drawn from that paper. 3.2.1 Davidson The Multi-Configuration Hartree-Fock, MCHF, is a suite of computational-physics programs which we obtained from Vanderbilt University, where they are used for atomic-physics calculations. Davidson <ref> [SF94] </ref> is an element of the suite that computes, by successive refinement, the extreme eigenvalue-eigenvector pairs of a large, sparse, real, symmetric matrix stored on disk. In our test, the size of this matrix is 16.3 MByte.
Reference: [SGM86] <author> K. Salem and H. Garcia-Molina. </author> <title> Disk striping. </title> <booktitle> In Proc. of the 2nd IEEE Int. Conf. on Data Engineering, 1986. (p 18) </booktitle>
Reference-contexts: I give precise acknowledgments as I describe each piece of collaborative work. 1.1.1 The Big Picture Storage Parallelism in the form of disk arrays [PGK88] has been advocated as a means to address the increasing gap between processor speed and I/O bandwidth <ref> [SGM86] </ref> (the so-called "I/O bottleneck" [PGK88, Smi85]) . But many workloads consist of streams of computation interspersed with synchronous I/O calls.
Reference: [Smi78] <author> Alan J. Smith. </author> <title> Sequential program prefetching in memory hierarchies. </title> <journal> IEEE Computer, </journal> <volume> 11(12) </volume> <pages> 7-21, </pages> <month> December </month> <year> 1978. </year> <title> (p 38) </title>
Reference-contexts: Finally, Krueger et al. [KLVA93] present a general toolkit and set of profiling tools to provide user-level control of paging. 2.3.5 Prefetching From Main Memory There is a large body of work on prefetching from main memory, rather than to main memory. Smith <ref> [Smi78] </ref> showed that large-block prefetching to cache may be ineffective, and while prefetching is potentially effective, small details of the implementation can have a significant effect on the overall performance. Baer and Chen [BC91] describe a hardware scheme for pre-loading a cache in the presence of sequential accesses to memory.
Reference: [Smi85] <author> A.J. Smith. </author> <title> Disk cache | miss ratio analysis and design considerations. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 161-203, </pages> <month> August </month> <year> 1985. </year> <pages> (pp 18, 34) BIBLIOGRAPHY 197 </pages>
Reference-contexts: I give precise acknowledgments as I describe each piece of collaborative work. 1.1.1 The Big Picture Storage Parallelism in the form of disk arrays [PGK88] has been advocated as a means to address the increasing gap between processor speed and I/O bandwidth [SGM86] (the so-called "I/O bottleneck" <ref> [PGK88, Smi85] </ref>) . But many workloads consist of streams of computation interspersed with synchronous I/O calls. The program blocks for I/O from disk 1, computes again when the data arrives, and then blocks for I/O from disk 2, never benefiting from the parallelism of the disk array. <p> The technique successfully hid more than half the I/O latency is seven of eight NAS parallel benchmarks. Predictive Disk Prefetching The most common incarnation of predictive prefetching is sequential readahead, ranging from one-block lookahead as described by Smith <ref> [Smi85] </ref> and implemented in Multics [FO71] and Unix [MJLF84] to more aggressive versions such as Digital's OSF 1 operating system, which will prefetch up to sixty-four 8KByte blocks in advance. Similarly, there have been efforts to provide higher throughput by reading larger objects, as in [MK91].
Reference: [SR86] <author> M. Stonebraker and L.A. Rowe. </author> <title> The design of POSTGRES. </title> <booktitle> In Proceedings of the ACM SIGMOD 1986 International Conference on Management of Data, </booktitle> <address> Washington, DC, </address> <pages> pages 28-30, </pages> <year> 1986. </year> <title> (p 45) </title>
Reference-contexts: Our test links the 562 object files of the TIP-1 kernel, an earlier version of the TIP2 kernel described here. These object files comprise approximately 64 MByte, and produce an 8.8 MByte kernel. 3.2.4 Postgres1 and Postgres2 Postgres version 4.2 <ref> [SR86, SRH90] </ref> is an extensible object-oriented relational database system from the University of California at Berkeley. In our test, Postgres executes a join of two relations.
Reference: [SRH90] <author> M. Stonebraker, L.A. Rowe, and M. Horohama. </author> <title> The implementation of POSTGRES. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 125-142, </pages> <month> March, </month> <year> 1990. </year> <title> (p 45) </title>
Reference-contexts: Our test links the 562 object files of the TIP-1 kernel, an earlier version of the TIP2 kernel described here. These object files comprise approximately 64 MByte, and produce an 8.8 MByte kernel. 3.2.4 Postgres1 and Postgres2 Postgres version 4.2 <ref> [SR86, SRH90] </ref> is an extensible object-oriented relational database system from the University of California at Berkeley. In our test, Postgres executes a join of two relations.
Reference: [SS82] <author> G. M. Sacco and M. Schkolnick. </author> <title> A mechanism for managing the buffer pool in a relational database using the hot set model. </title> <booktitle> In Proc. of the 8th Int. Conf. on Very Large Data Bases, </booktitle> <pages> pages 257-262, </pages> <month> September </month> <year> 1982. </year> <title> (p 36) </title>
Reference-contexts: Within a database buffer manager, which has more complete information about the types of queries being performed, the most successful early approach to management is the hot set buffer pool management model of Sacco and Schkolnick <ref> [SS82] </ref>, an extension of the working set model of Denning [Den68].
Reference: [SS86] <author> G. M. Sacco and M. Schkolnick. </author> <title> Buffer management in relational database systems. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 11(4) </volume> <pages> 474-498, </pages> <month> December </month> <year> 1986. </year> <title> (p 36) </title>
Reference-contexts: Around the same time, the original hot set model was extended by its proponents to include different access patterns, so that all operations supported by IBM's System R database test-bed could be represented in the model <ref> [SS86] </ref>. Cornell and Yu [CY89] integrate query optimization with buffer cache management. They phrase the problem as a 0-1 integer linear program, which they approximate to choose query plans based on buffer restrictions.
Reference: [SS95] <author> D. Steere and M. Satyanarayanan. </author> <title> Using Dynamic Sets to overcome high I/O latencies during search. </title> <booktitle> In Proc. of the 5th Workshop on Hot Topics in Operating Systems, </booktitle> <address> Orcas Island, WA, </address> <pages> pages 136-140, </pages> <month> May 4-5, </month> <year> 1995. </year> <title> (p 33) </title>
Reference-contexts: Informed Prefetching This thesis is concerned with informed prefetching, in which the application discloses the data it will require. Though most related work addresses either cache management or predictive prefetching, there are a few other non-predictive approaches to prefetching. Steere and Satyanarayanan <ref> [SS95] </ref> present a general abstraction called dynamic sets to allow parallel prefetching of objects that will be required in the near future. It is possible, for instance, to load a web page then specify a dynamic set containing all links on the page.
Reference: [ST85] <author> D. D. Sleator and R. E. Tarjan. </author> <title> Amortized efficiency of list update and paging rules. </title> <journal> Communications of the ACM, </journal> <volume> 28(2) </volume> <pages> 202-208, </pages> <year> 1985. </year> <pages> (pp 138, 145) </pages>
Reference-contexts: formally presents the k-server problems and the Metrical Task Systems, two common classes of online problems that refine the definition above; all the results in this part of the thesis concern these two classes. 7.2 Competitive Analysis: A Metric for Online Prob lems In this section I describe Competitive Analysis <ref> [ST85, KMRS88] </ref>, a metric developed by Sleator and Tarjan twelve years ago that has become successful for analyzing online problems. Let OPT (), the optimal o*ine cost of sequence , be the lowest cost attainable by any algorithm on sequence , including algorithms that know the entire sequence in advance. <p> The uniform space, described above in Section 7.3.1, was initially studied by Sleator and Tarjan <ref> [ST85] </ref>, who showed that LRU and FIFO are k-competitive. Randomized approaches for the uniform space began with Fiat et al.'s marking algorithm [FKM + 91], a 2H k -competitive algorithm where H k ~ = ln k is the k th harmonic number.
Reference: [Sto81] <author> Michael Stonebraker. </author> <title> Operating system support for database management. </title> <journal> Communications of the ACM, </journal> <volume> 24(7) </volume> <pages> 412-418, </pages> <month> July </month> <year> 1981. </year> <title> (p 35) </title>
Reference-contexts: The representation should therefore be useful for predicting future elements of the sequence. They formalize this notion of "prefetching via data compression," and give predictors that perform well compared to Fido's associative memory approach. 2.3.2 Database Cache Management Stonebraker <ref> [Sto81] </ref> points out that databases often have access to detailed information about access patterns, relative to general-purpose file systems, and should be able to incorporate this information into resource management decisions.
Reference: [TD91] <author> C. Tait and D. Duchamp. </author> <title> Detection and exploitation of file working sets. </title> <booktitle> In Proc. Eleventh Intl. Conf. on Distributed Computing Systems, </booktitle> <pages> pages 2-9, </pages> <publisher> IEEE, </publisher> <year> 1991. </year> <title> (p 35) </title>
Reference-contexts: In [GA96] the same authors give an implementation of the system under SunOS. In the domain of predictive prefetching for mobile computing, Kuenning et al. [KPR94] analyzed trace data and suggested that the necessary cache contents could be generated automatically using predictive approaches. Tait and Duchamp <ref> [TD91] </ref> give a predictive mechanism called the working graph; they show that predictive prefetching using this structure performs better than LRU and has small overhead, for the domain of low-bandwidth connected mobile computing.
Reference: [TE93] <author> Dean M. Tullsen and Susan J. Eggers. </author> <title> Limitations of cache prefetching on a bus-based multiprocessor. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 278-288, </pages> <month> May </month> <year> 1993. </year> <title> (p 38) </title>
Reference-contexts: Chen and Baer [CB92] suggest a combination of hardware-based cache prefetching and hardware support for non-blocking caches. Tullsen and Eggers <ref> [TE93] </ref> showed that some architectures (in this case, a high-latency bus-based multiprocessor) are not well-suited for prefetching.
Reference: [TPG97] <author> A. Tomkins, R.H. Patterson, and G. Gibson. </author> <title> Informed multi-process prefetching and caching. </title> <booktitle> In Proceedings of the ACM SIGMETRICS, </booktitle> <pages> pages 100-114, </pages> <year> 1997. </year> <pages> (pp 20, 36, 101) </pages>
Reference-contexts: Tiptoe and lru-sp/forestall, which use the hybrid algorithm forestall, combine the benefits of fixed-horizon and aggressive by dynamically estimating upcoming load based on whatever hints are present and deciding how aggressively to prefetch. Finally, in Chapter 6 I describe a study performed with Hugo Patterson and Garth Gibson <ref> [TPG97] </ref> that considers prefetching and caching in a multiprogramming environment. <p> Work on TIP2 [PGG + 95] addressed both parts of the problem but was more strongly focused on allocation of resources among multiple processes. Once the system was built, more recent work <ref> [KTP + 96, TPG97] </ref> focused more specifically on cache management within a process, and on integrating the two problems. 2.3 Related Work 37 dynamic algorithm that takes re-use into account. <p> The work described here is an extension of work presented at Sigmetrics 97 in collaboration with Hugo Patterson and Garth Gibson <ref> [TPG97] </ref>. I begin with a discussion of metrics for the multi-process case in Section 6.1. Next, in Section 6.2 I study pairs of the applications described in Chapter 3 running simultaneously.
Reference: [Tri79] <author> K.S. Trivedi. </author> <title> An analysis of prepaging. </title> <journal> Computing, </journal> <volume> 22 </volume> <pages> 191-210, </pages> <year> 1979. </year> <title> (p 38) 198 BIBLIOGRAPHY </title>
Reference-contexts: For instance, a 38 Systems Introduction multi-threaded application can inform the system that a series of interleaved sequential reads is beginning. 2.3.4 Prefetching and Virtual Memory Both prefetching and informed cache management have been incorporated into virtual memory systems. Trivedi <ref> [Tri79] </ref> studied prefetching of virtual memory pages, referred to as prepaging. He argued that predictive techniques are not sufficient to prefetch data across phase transition boundaries, in which the behavior of the user changes dramatically (e.g., when a user switches from word processing to compilation).
Reference: [WLAG93] <author> Robert Wahbe, Steven Lucco, Thomas E. Anderson, and Susan L. Graham. </author> <title> Efficient software-based fault isolation. </title> <booktitle> In Proc. of the 1993 ACM SIGOPS, </booktitle> <pages> pages 203-216, </pages> <year> 1993. </year> <title> (p 29) </title>
Reference-contexts: When a program stalls for I/O and the CPU would otherwise be idle, the system may continue to execute the program speculatively in a "sandbox" <ref> [WLAG93] </ref> so as to discover future accesses without corrupting existing state. This approach offers the additional advantage that source code is not required. Thus, there is evidence to suggest that compilers or programmers can provide disclosures, and there are preliminary approaches to generating disclosures transparently.
Reference: [WM92] <author> S. Wu and U. Manber. </author> <title> AGREP | a fast approximate pattern-matching tool. </title> <booktitle> In Proc. of the 1992 Winter USENIX Conference, </booktitle> <address> San Francisco, CA, </address> <pages> pages 20-24, </pages> <month> Jan, </month> <year> 1992. </year> <title> (p 47) </title>
Reference-contexts: However, there is a high variance on the number of pairs and triples consulted and fetched, so storage parallelism is appropriate. 3.2.6 Agrep Agrep, a variant of grep, was written by Wu and Manber at the University of Arizona <ref> [WM92] </ref>. It is a full-text pattern matching program that performs approximate matches. Invoked in its simplest form, it opens the files specified on its command line one at a time, in argument order, and reads each sequentially.
Reference: [YC91] <author> Philip S. Yu and Douglas W. Cornell. </author> <title> Optimal buffer allocation in a multi-query environment, </title> <booktitle> 1991. (p 36) </booktitle>
Reference-contexts: Cornell and Yu [CY89] integrate query optimization with buffer cache management. They phrase the problem as a 0-1 integer linear program, which they approximate to choose query plans based on buffer restrictions. Later, in <ref> [YC91] </ref>, the same authors present a different global minimization technique based on simulated annealing for the same problem.

References-found: 100

