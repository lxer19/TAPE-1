URL: http://www.isle.org/~langley/papers/diet.ecml97.ps
Refering-URL: http://www.isle.org/~langley/pubs.html
Root-URL: 
Email: ronnyk@sgi.com, langley@cs.stanford.edu, yygirl@cs.stanford.edu  
Phone: 1  2  3  
Title: The Utility of Feature Weighting in Nearest-Neighbor Algorithms  
Author: Ron Kohavi and Pat Langley and Yeogirl Yun 
Address: 2011 N. Shoreline Blvd., Mountain View, CA 94043  Stanford, CA 94305  Stanford, CA 94305  
Affiliation: Silicon Graphics, Inc.,  Robotics Laboratory, Stanford University,  Electrical Engineering Dept., Stanford University,  
Abstract: Nearest-neighbor algorithms are known to depend heavily on their distance metric. In this paper, we investigate the use of a weighted Euclidean metric in which the weight for each feature comes from a small set of options. We describe Diet, an algorithm that directs search through a space of discrete weights using cross-validation error as its evaluation function. Although a large set of possible weights can reduce the learner's bias, it can also lead to increased variance and overfitting. Our empirical study shows that, for many data sets, there is an advantage to weighting features, but that increasing the number of possible weights beyond two (zero and one) has very little benefit and sometimes degrades performance.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> David W. Aha. </author> <title> Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 36(1) </volume> <pages> 267-287, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction In recent years, instance-based methods <ref> [5, 13, 1] </ref> have emerged as a promising approach to machine learning, with researchers reporting excellent results on many real-world induction tasks. <p> For instance, Cost and Salzberg [3] use differences in probability distributions across classes to modify the distance metric for nominal attributes, whereas Daelemans, Gillis, and Durieux [4] and Wettschereck [13] use mutual information to compute coefficients on numeric attributes. Aha <ref> [1] </ref> reports an incremental scheme that alters feature weights depending on their distance and predicted class. All of these researchers report improvement over the simple version of nearest neighbor that gives attributes equal weight. In this paper we explore a different approach to determining feature weights for nearest-neighbor classification. <p> 35 19 Vehicle 200 646 18 4 LED24 200 3000 24 10 Breast 100 599 10 2 C Hypothyroid 200 2963 25 2 Mushroom 100 8024 22 2 Vote 100 335 16 2 LED7 200 3000 7 10 also ran Quinlan's C4.5 [12], a well-known decision-tree algorithm, and Aha's IB4 <ref> [1] </ref>, an instance-based algorithm which incorporates a different weighting schemes that assigns weights to features incrementally.
Reference: 2. <author> R. E. Bellman. </author> <title> Adaptive Control Processes : A Guided Tour. </title> <publisher> Princeton University Press, </publisher> <address> NJ, </address> <year> 1961. </year>
Reference-contexts: One drawback of such methods is that, when combined with a naive distance metric that weights attributes equally, they can suffer from the "curse of dimensionality," in which the number of cases needed to maintain a given error rate grows rapidly with the number of features <ref> [2] </ref>. This observation has led some researchers to augment nearest-neighbor methods with techniques for determining distinct weights for each feature.
Reference: 3. <author> Scott Cost and Steven Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10(1) </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference-contexts: This observation has led some researchers to augment nearest-neighbor methods with techniques for determining distinct weights for each feature. For instance, Cost and Salzberg <ref> [3] </ref> use differences in probability distributions across classes to modify the distance metric for nominal attributes, whereas Daelemans, Gillis, and Durieux [4] and Wettschereck [13] use mutual information to compute coefficients on numeric attributes.
Reference: 4. <author> W. Daelemans, S. Gillis, and G. Durieux. </author> <title> The acquisition of stress: A data-oriented approach. </title> <journal> Computational Linguistics, </journal> <volume> 20, </volume> <year> 1994. </year>
Reference-contexts: This observation has led some researchers to augment nearest-neighbor methods with techniques for determining distinct weights for each feature. For instance, Cost and Salzberg [3] use differences in probability distributions across classes to modify the distance metric for nominal attributes, whereas Daelemans, Gillis, and Durieux <ref> [4] </ref> and Wettschereck [13] use mutual information to compute coefficients on numeric attributes. Aha [1] reports an incremental scheme that alters feature weights depending on their distance and predicted class. All of these researchers report improvement over the simple version of nearest neighbor that gives attributes equal weight.
Reference: 5. <author> Belur V. Dasarathy. </author> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction In recent years, instance-based methods <ref> [5, 13, 1] </ref> have emerged as a promising approach to machine learning, with researchers reporting excellent results on many real-world induction tasks.
Reference: 6. <author> Stuart Geman, Eli Bienenstock, and Renee Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-48, </pages> <year> 1992. </year>
Reference-contexts: Given a target concept, the average error of an algorithm for different data sets of size m can be decomposed into a squared bias component and a variance component <ref> [6] </ref>. The squared bias measures how closely, for a random instance, the average prediction of the learning algorithm matches the target value for that instance.
Reference: 7. <author> George John, Ron Kohavi, and Karl Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 121-129. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1994. </year>
Reference-contexts: Rather than considering a continuous weight space, we restrict weights to a small, finite set. This reduces representational power and hence increases bias, but it should also lower the variance and thus reduce chances of overfitting. We report on Diet, a system that incorporates a wrapper method <ref> [7] </ref> to search the weight space using cross-validation error from the nearest neighbor algorithm. Experimental studies suggest that, on many natural data sets, restricting the set of weights to only two alternatives|which is equivalent to feature subset selection|gives the best results.
Reference: 8. <author> J. D Kelly and L. Davis. </author> <title> A hybrid genetic algorithm for classification. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 645-650. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: In future work, we should repeat our experiments with training sets of different sizes, to determine whether more weights prove useful at later points in the learning curve. We should also run comparative studies between Diet and other approaches to feature weighting. For example, Kelly and Davis <ref> [8] </ref> report using a genetic algorithm to search the space of feature weights, and Lowe [10] presents an alternative scheme that employs conjugate gradient descent through the weight space.
Reference: 9. <author> Ron Kohavi and David H. Wolpert. </author> <title> Bias plus variance decomposition for zero-one loss functions. </title> <editor> In Lorenza Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1996. </year> <note> Available at http://robotics.stanford.edu/users/ronnyk. </note>
Reference-contexts: For classification problems, researchers have recently proposed different decompositions of bias and variance. We used the decomposition proposed by Ko-havi and Wolpert <ref> [9] </ref>, which has the desirable property that it is equivalent to the standard squared error loss (except for a leading constant of one-half) if one views the label as a vector of zero-one indicator variables and computes squared error loss on the vector. on which the error varied widely across different
Reference: 10. <author> D. G. Lowe. </author> <title> Similarity metric learning for a variable-kernel classifier. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 72-85, </pages> <year> 1995. </year>
Reference-contexts: We should also run comparative studies between Diet and other approaches to feature weighting. For example, Kelly and Davis [8] report using a genetic algorithm to search the space of feature weights, and Lowe <ref> [10] </ref> presents an alternative scheme that employs conjugate gradient descent through the weight space. Such comparisons will help determine the relative benefits of the two key ideas behind Diet: restricting the number of weights used in nearest-neighbor classification and using a wrapper method to search the space of such weights.
Reference: 11. <author> Christopher J. Merz and Patrick M. Murphy. </author> <title> UCI repository of machine learning databases. </title> <note> At http://www.ics.uci.edu/~mlearn/MLRepository.html, 1996. </note>
Reference-contexts: To ensure relevance, we also wanted to study Diet's behavior on natural domains. To determine candidates on which feature weighting would likely yield improvements, we inspected learning curves (which plot error against the number of training cases) for domains from the UCI repository <ref> [11] </ref> and identified six data sets (Anneal, Chess, Segment, Soybean-Large, Vehicle, and LED24) in which nearest neighbor did not reach asymptote early in the curve. This suggested there was room for improvement on these data sets.
Reference: 12. <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: Segment 300 2010 19 7 Soybean-Large 100 583 35 19 Vehicle 200 646 18 4 LED24 200 3000 24 10 Breast 100 599 10 2 C Hypothyroid 200 2963 25 2 Mushroom 100 8024 22 2 Vote 100 335 16 2 LED7 200 3000 7 10 also ran Quinlan's C4.5 <ref> [12] </ref>, a well-known decision-tree algorithm, and Aha's IB4 [1], an instance-based algorithm which incorporates a different weighting schemes that assigns weights to features incrementally.
Reference: 13. <author> Dietrich Wettschereck. </author> <title> A Study of Distance-Based Machine Learning Algorithms. </title> <type> PhD thesis, </type> <institution> Oregon State University, </institution> <year> 1994. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: 1 Introduction In recent years, instance-based methods <ref> [5, 13, 1] </ref> have emerged as a promising approach to machine learning, with researchers reporting excellent results on many real-world induction tasks. <p> This observation has led some researchers to augment nearest-neighbor methods with techniques for determining distinct weights for each feature. For instance, Cost and Salzberg [3] use differences in probability distributions across classes to modify the distance metric for nominal attributes, whereas Daelemans, Gillis, and Durieux [4] and Wettschereck <ref> [13] </ref> use mutual information to compute coefficients on numeric attributes. Aha [1] reports an incremental scheme that alters feature weights depending on their distance and predicted class. All of these researchers report improvement over the simple version of nearest neighbor that gives attributes equal weight.
References-found: 13

