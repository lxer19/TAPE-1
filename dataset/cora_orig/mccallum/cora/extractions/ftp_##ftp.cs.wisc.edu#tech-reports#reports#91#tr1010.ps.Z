URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/91/tr1010.ps.Z
Refering-URL: http://www.cs.wisc.edu/~arch/uwarch/tech_reports/tech_reports.html
Root-URL: 
Title: The Impact of Pipelined Channels on k-ary n-cube Networks  
Author: Steven L. Scott and James R. Goodman 
Address: 1210 W. Dayton Street Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin Madison  
Note: Submitted to IEEE Transactions on Parallel and Distributed Systems  This work was supported by a fellowship from the Fannie and John Hertz Foundation and by National Science Foundation grant #CCR-892766.  
Abstract: In a pipelined-channel interconnection network, multiple bits may be simultaneously in flight on a single wire. This allows the cycle time of the network to be independent of the wire lengths, significantly affecting the network design tradeoffs. This paper investigates the design and performance of pipelined-channel k-ary n-cube networks, with particular emphasis on the choice of dimensionality and radix. Networks are investigated under the constant link width, constant node size and constant bisection constraints. We find that the optimal dimensionality of pipelined-channel networks is higher than that of non-pipelined-channel networks, with the difference being greater under looser wiring constraints. Their radix should remain roughly constant as network size is grown, decreasing slightly for some unidirectional tori and increasing slightly for some bi-directional meshes. Pipelined-channel networks are shown to provide lower latency and higher bandwidth than their non-pipelined-channel counterparts, especially for high dimensional networks. The paper also investigates the effects of switching overhead and message lengths, indicating where results agree with and differ from previous results obtained for non-pipelined-channel networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Abraham and K. Padmanabhan, </author> <title> Performance of the Direct Binary n-Cube Network for Multiprocessors, </title> <journal> IEEE Transactions on Computers 38(7), </journal> <month> July </month> <year> 1989, </year> <pages> 775-785. </pages>
Reference: [2] <author> V. S. Adve and M. K. Vernon, </author> <title> Performance Analysis of Multiprocessor Mesh Interconnection Networks with Wormhole Routing, </title> <type> Computer Sciences Technical Report #1001, </type> <institution> University of Wisconsin-Madison, Madison, WI 53706, </institution> <month> February </month> <year> 1991. </year>
Reference: [3] <author> A. Agarwal, </author> <title> Limits on Interconnection Network Performance, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> October </month> <year> 1991, </year> <pages> 398-412. </pages>
Reference-contexts: Since the optimal dimensionality remains fixed, the optimal radix of the network increases significantly for large systems. Agarwal <ref> [3] </ref> extended Dally's analysis in two important ways. First, he included switching time in the latency equations, which was missing in Dally's study. Second, he considered a weaker wiring constraint: constant node size. <p> Model and Assumptions The first assumption made in this work is that we are dealing with uni-directional links. There are two reasons for doing this. First, previous analysis <ref> [3, 7] </ref> has focused on uni-directional networks. Second, pipelined channels are naturally uni-directional. While the results are qualitatively similar for bi-directional networks, some of the details change (see Section 2.6). 4 Recall that a k-ary n-cube network consists of N =k n nodes. <p> 1 hh J N 1/3 hhhhh M O 1 hh J N 1/3 hhhhh M O if k &gt;2 Since the maximum wire delay is suffered over all links, including the short ones, the total delay due to wire transmission is increased as the dimensionality of a network is increased <ref> [3, 7] </ref>. In a pipelined-channel network, the number of cycles spent traversing a given wire is determined by that wire's length only. Thus T wire avg is a function of the mean wire length rather than the maximum wire length. <p> The constant link width constraint may be realistic for sufficiently small systems, depending upon the technology used to implement the network. Intra-node data paths, for example, may dictate the link width and the pin limitations may not be restrictive. Agarwal <ref> [3] </ref> considers all three constraints, with the emphasis on the constant node size constraint. 2.6. Bi-directional Networks Pipelined-channels are naturally uni-directional, and the analysis thus far has assumed uni-directional networks. This section briefly discusses bi-directional networks. Although routing is simpler in uni-directional networks, there are some disadvantages to their use. <p> Section 4.1 investigates the effect of varying the ratio of switching to transmission time, S. Section 4.2 investigates the effect of varying the packet length. The results differ somewhat from results obtained previously for non-pipelined-channel networks <ref> [3] </ref>. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Link Wires Wires Cycle Decode Roundtrip Maximum Width per accross Time Delay Unloaded Throughput Dim Radix Node Bisection Increase Cycles Latency iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii 2 64 32 128 4096 2.00 1 556.0 0.426 4 8 32 256 32768 3.00 1 246.0 2.556 12 2 32 768 131072 5.00 1 250.0 10.735 <p> When S is large, higher dimensional networks will be prefered due to their smaller number of hops. Therefore, decreasing S will decrease the optimal dimensionality by increasing the impact of longer wire lengths <ref> [3] </ref>. As switching speeds continue to increase, the value of S in real systems will become smaller. This will cause wire transmission delay to become dominant, and the unloaded latency in pipelined-channel networks will become less sensitive to dimensionality. <p> Effect of Packet Lengths the packet length, L, is varied from 64 to 1024 bits. Parts (a), (b) and (c) assume the constant link width, constant node size and constant bisection constraints, respectively. In non-pipelined-channel networks, longer packet lengths always decrease the optimal dimensionality <ref> [3] </ref>. This is because the number of flits, P, becomes dominant over the number of network hops (see Equation (5)), making the reduction in number of hops in high-dimensional networks less important relative to the increased wire delay.
Reference: [4] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith, </author> <title> The Tera Computer System, </title> <booktitle> Proc. 1990 International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1990. </year> <month> 29 </month>
Reference-contexts: The performance of these networks has been the subject of more recent attention [1-3, 7]. Direct networks are gaining in popularity and have been employed in many recent existing or proposed machines, including the Connection Machine [11], Intel iPSC, Cosmic Cube [22], Tera supercomputer <ref> [4] </ref>, CMU-Intel iWarp [5], and Stanford DASH multiprocessor [17]. The most commonly used direct networks are variants of the k-ary n-cube [26]. The k-ary n-cube consists of N =k n nodes, arranged in n dimensions, with k nodes per dimension (n is the dimensionality, k is the radix).
Reference: [5] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H. T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peterson, J. Susman, J. Sutton, J. Urbanski, and J. Webb, </author> <title> Supporting Systolic and Memory Communication in iWarp, </title> <booktitle> Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 70-81. </pages>
Reference-contexts: The performance of these networks has been the subject of more recent attention [1-3, 7]. Direct networks are gaining in popularity and have been employed in many recent existing or proposed machines, including the Connection Machine [11], Intel iPSC, Cosmic Cube [22], Tera supercomputer [4], CMU-Intel iWarp <ref> [5] </ref>, and Stanford DASH multiprocessor [17]. The most commonly used direct networks are variants of the k-ary n-cube [26]. The k-ary n-cube consists of N =k n nodes, arranged in n dimensions, with k nodes per dimension (n is the dimensionality, k is the radix). <p> Pipelined channels are not widely used in multiprocessor interconnects, however. Limited pipelined channels (allowing a small, fixed number of bits on a wire) are used in the CMU-Intel iWarp <ref> [5] </ref>, various Cray Research machines [25] and the Thinking Machine CM5 [28]. A higher degree of pipelining is achievable using the Caltech Slack chip [23], which is designed to allow tightly-coupled Mosaic channels to communicate efficiently over long cables.
Reference: [6] <author> W. J. Dally, </author> <title> Deadlock Free Message Routing in Multiprocessor Interconnection Networks, </title> <journal> IEEE Transactions on Computers C-36(5), </journal> <month> May </month> <year> 1987, </year> <pages> 547-553. </pages>
Reference-contexts: This decouples link throughput from link latency, and fundamentally changes the network design tradeoffs. The pipelined channel routing protocol described in Section 2.1 also provides some of the same benefits as virtual channels <ref> [6, 8] </ref>. By guaranteeing that buffer congestion occurs only when entering/exiting a dimension, 3 deadlock free routing is provided for uni-directional k-ary n-cubes. In addition, sustained throughput is able to come close to maximum throughput, especially when k is large (see [20] and Section 4 of this paper).
Reference: [7] <author> W. J. Dally, </author> <title> Performance Analysis of k-ary n-cube Interconnection Networks, </title> <journal> IEEE Transactions on Computers 39(6), </journal> <month> June </month> <year> 1990, </year> <pages> 775-785. </pages>
Reference-contexts: The performance of these networks has been extensively analyzed in the literature [9, 14, 16, 18, 29]. Direct networks incorporate the processing elements within the network itself, allowing for direct communication between processors [21]. The performance of these networks has been the subject of more recent attention <ref> [1-3, 7] </ref>. Direct networks are gaining in popularity and have been employed in many recent existing or proposed machines, including the Connection Machine [11], Intel iPSC, Cosmic Cube [22], Tera supercomputer [4], CMU-Intel iWarp [5], and Stanford DASH multiprocessor [17]. <p> Under these assumptions, binary hypercubes appear very attractive, delivering the lowest latency and traffic per link of any k-ary n-cube configuration. These assumptions are unrealistic, however. Dally has investigated network performance while taking wire delay into account and applying the constant bisection constraint <ref> [7] </ref>. This constraint holds the number of wires crossing the bisection of a network constant as the dimensionality is varied, which causes the link width to decrease as dimensionality is increased. The constraint is motivated by wiring density limitations in VLSI, but may also hold for multi-chip or multi-board implementations. <p> In addition, sustained throughput is able to come close to maximum throughput, especially when k is large (see [20] and Section 4 of this paper). Conventional networks using flit-level flow control typically provide half or less of their maximum throughput, due to coupled resource allocation <ref> [7] </ref>. Note, however, that the switches described in Section 2.1 require buffers able to contain an entire packet. Pipelined channels have long been used in wide area networks and local area networks, since the physical delays involved compel their use. <p> Model and Assumptions The first assumption made in this work is that we are dealing with uni-directional links. There are two reasons for doing this. First, previous analysis <ref> [3, 7] </ref> has focused on uni-directional networks. Second, pipelined channels are naturally uni-directional. While the results are qualitatively similar for bi-directional networks, some of the details change (see Section 2.6). 4 Recall that a k-ary n-cube network consists of N =k n nodes. <p> Wire Length Wire lengths depend upon network dimensionality and layout. We assume that the network is being implemented in three physical dimensions (this differs from <ref> [7] </ref>, where two dimensions are assumed). We also make the simplifying assumption that interprocessor spacing is equal in all dimensions. The long wraparound links in a torus (see Figure 1 (a)) can be avoided by folding the network as shown in Figure 1 (b). <p> 1 hh J N 1/3 hhhhh M O 1 hh J N 1/3 hhhhh M O if k &gt;2 Since the maximum wire delay is suffered over all links, including the short ones, the total delay due to wire transmission is increased as the dimensionality of a network is increased <ref> [3, 7] </ref>. In a pipelined-channel network, the number of cycles spent traversing a given wire is determined by that wire's length only. Thus T wire avg is a function of the mean wire length rather than the maximum wire length. <p> The number of wires across the bisection is B = 2Wk n -1 , so link width is given by (10) W const_bisec = k I L 2N M O The constant bisection constraint was used by Dally <ref> [7] </ref> in order to reflect the limited wiring area of a network implemented on a single VLSI substrate. A multiprocessor implemented across multiple boards may also be bisection constrained, or may be node-size constrained due to pin limitations off-chip and off-board.
Reference: [8] <author> W. J. Dally, </author> <title> Virtual-Channel Flow Control, </title> <journal> IEEE Transactions on Parallel and Distributed Systems 3(2), </journal> <month> March </month> <year> 1992, </year> <pages> 194-205. </pages>
Reference-contexts: This decouples link throughput from link latency, and fundamentally changes the network design tradeoffs. The pipelined channel routing protocol described in Section 2.1 also provides some of the same benefits as virtual channels <ref> [6, 8] </ref>. By guaranteeing that buffer congestion occurs only when entering/exiting a dimension, 3 deadlock free routing is provided for uni-directional k-ary n-cubes. In addition, sustained throughput is able to come close to maximum throughput, especially when k is large (see [20] and Section 4 of this paper).
Reference: [9] <author> D. M. Dias and J. R. </author> <title> Jump, The Performance of Multistage Interconnection Networks for Multiprocessors, </title> <journal> IEEE Transactions on Computers C-32(12), </journal> <month> December </month> <year> 1983, </year> <pages> 1091-1098. </pages>
Reference-contexts: Indirect networks, such as the omega network [15], connect processors and memories through multiple intermediate stages of switching elements. The performance of these networks has been extensively analyzed in the literature <ref> [9, 14, 16, 18, 29] </ref>. Direct networks incorporate the processing elements within the network itself, allowing for direct communication between processors [21]. The performance of these networks has been the subject of more recent attention [1-3, 7].
Reference: [10] <author> T. Feng, </author> <title> A Survey of Interconnection Networks, </title> <booktitle> Computer, </booktitle> <month> December, </month> <year> 1981, </year> <pages> 12-27. </pages>
Reference-contexts: 1. INTRODUCTION The interconnection network used in a multiprocessor or multicomputer can have a major impact on system performance and cost. Understanding the tradeoffs in network design thus becomes a critical requirement for building cost-effective, high-performance systems. A wide variety of interconnection networks have been proposed (see <ref> [10] </ref> or [24] for a summary), each of which can be classified as either direct or indirect. Indirect networks, such as the omega network [15], connect processors and memories through multiple intermediate stages of switching elements.
Reference: [11] <author> W. D. Hillis, </author> <title> The Connection Machine, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: The performance of these networks has been the subject of more recent attention [1-3, 7]. Direct networks are gaining in popularity and have been employed in many recent existing or proposed machines, including the Connection Machine <ref> [11] </ref>, Intel iPSC, Cosmic Cube [22], Tera supercomputer [4], CMU-Intel iWarp [5], and Stanford DASH multiprocessor [17]. The most commonly used direct networks are variants of the k-ary n-cube [26].
Reference: [12] <institution> IEEE, IEEE Std 1596-1992 (Scalable Coherent Interface), Prepublication copies available from IEEE Service Center, </institution> <address> Piscataway, N.J. </address> <note> To purchase a copy, call 1-800-678-4333., </note> <year> 1992. </year>
Reference-contexts: A higher degree of pipelining is achievable using the Caltech Slack chip [23], which is designed to allow tightly-coupled Mosaic channels to communicate efficiently over long cables. The IEEE Scalable Coherent Interface (SCI) <ref> [12] </ref>, on which we base our model, allows an arbitrary amount of pipelining. The remainder of this paper presents a performance study of pipelined-channel k-ary n-cube networks, with particular emphasis on how the design tradeoffs differ from those of non-pipelined-channel networks. <p> A dimension is skipped if the packet source and destination have the same ordinate in that dimension. A low-level protocol handles the transmission around each ring, independent of the other dimensions and any higher-level, end-to-end protocol. The ring interfaces are based loosely on the IEEE SCI logical layer protocol <ref> [12, 20] </ref>. The links are W bits wide, so a packet of L bits is decomposed into P flits, with P given by (1) P = J L hhh J Routing is similar to virtual cut through [13]. <p> In this case, of course, the switching delay would be greater for packets changing to other than the next highest dimension. Low-level routing in each dimension is modelled after the logical layer of the SCI protocol <ref> [12] </ref>. When a packet is removed from a ring (either switched to the processor or to a different dimension), an echo packet is routed the remainder of the way around the ring to acknowledge the receipt of the packet on this ring. <p> Ring acknowledgements avoid handshaking latency altogether and retain performance as transmission delays increase. In addition, they can be used to provide fault tolerance by checking a CRC (cyclic redundancy check) at the end of the packet <ref> [12] </ref>. Unlike the input queue, the ring buffer is guaranteed to be able to accept a flit on every cycle. An output queue may only initiate a packet transmission on the output link if the ring buffer has enough free space to hold a packet of equal length. <p> The latency used to determine the optimal network configuration is the sum of two packet transmission latencies: one with a 16 byte packet and one with an 80 byte packet. These correspond to the address and data packet sizes in SCI <ref> [12] </ref>. To compute the optimal dimensionality, we have treated all discrete quantities as continuous. Thus, wire and decode delay cycles as well as the dimensionality and radix of the networks can be fractional.
Reference: [13] <author> P. Kermani and L. Kleinrock, </author> <title> Virtual Cut-Through: A New Computer Communication Switching Technique, </title> <booktitle> Computer Networks 3, </booktitle> <year> 1979, </year> <pages> 267-286. </pages>
Reference-contexts: The links are W bits wide, so a packet of L bits is decomposed into P flits, with P given by (1) P = J L hhh J Routing is similar to virtual cut through <ref> [13] </ref>. On being placed in the input queue by the CPU, a packet is switched to the output queue for the appropriate dimension and gated onto the output link (requiring T switch cycles).
Reference: [14] <author> C. P. Kruskal and M. Snir, </author> <title> The Performance of Multistage Interconnection Networks for Multiprocessors, </title> <journal> IEEE Transactions on Computers C-32(12), </journal> <month> December </month> <year> 1983, </year> <pages> 1091-1098. </pages>
Reference-contexts: Indirect networks, such as the omega network [15], connect processors and memories through multiple intermediate stages of switching elements. The performance of these networks has been extensively analyzed in the literature <ref> [9, 14, 16, 18, 29] </ref>. Direct networks incorporate the processing elements within the network itself, allowing for direct communication between processors [21]. The performance of these networks has been the subject of more recent attention [1-3, 7].
Reference: [15] <author> D. H. Lawrie, </author> <title> Access and Alignment of Data in an Array Processor, </title> <journal> IEEE Transactions on Computers C-24(12), </journal> <month> December </month> <year> 1975, </year> <pages> 1145-1155. </pages>
Reference-contexts: A wide variety of interconnection networks have been proposed (see [10] or [24] for a summary), each of which can be classified as either direct or indirect. Indirect networks, such as the omega network <ref> [15] </ref>, connect processors and memories through multiple intermediate stages of switching elements. The performance of these networks has been extensively analyzed in the literature [9, 14, 16, 18, 29]. Direct networks incorporate the processing elements within the network itself, allowing for direct communication between processors [21].
Reference: [16] <author> M. Lee and C. Wu, </author> <title> Performance Analysis of Circuit Switching Baseline Interconnection Networks, </title> <booktitle> Proc. 11th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1984, </year> <pages> 82-90. </pages>
Reference-contexts: Indirect networks, such as the omega network [15], connect processors and memories through multiple intermediate stages of switching elements. The performance of these networks has been extensively analyzed in the literature <ref> [9, 14, 16, 18, 29] </ref>. Direct networks incorporate the processing elements within the network itself, allowing for direct communication between processors [21]. The performance of these networks has been the subject of more recent attention [1-3, 7].
Reference: [17] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam, </author> <title> Design of the Stanford DASH Multiprocessor, </title> <type> CSL Technical Report 89-403, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Direct networks are gaining in popularity and have been employed in many recent existing or proposed machines, including the Connection Machine [11], Intel iPSC, Cosmic Cube [22], Tera supercomputer [4], CMU-Intel iWarp [5], and Stanford DASH multiprocessor <ref> [17] </ref>. The most commonly used direct networks are variants of the k-ary n-cube [26]. The k-ary n-cube consists of N =k n nodes, arranged in n dimensions, with k nodes per dimension (n is the dimensionality, k is the radix). Figure 1 illustrates several different k-ary n-cubes.
Reference: [18] <author> J. H. Patel, </author> <title> Performance of Processor-Memory Interconnections for Multiprocessors, </title> <journal> IEEE Transactions on Computers C-30(10), </journal> <month> October </month> <year> 1981, </year> <pages> 771-780. </pages>
Reference-contexts: Indirect networks, such as the omega network [15], connect processors and memories through multiple intermediate stages of switching elements. The performance of these networks has been extensively analyzed in the literature <ref> [9, 14, 16, 18, 29] </ref>. Direct networks incorporate the processing elements within the network itself, allowing for direct communication between processors [21]. The performance of these networks has been the subject of more recent attention [1-3, 7].
Reference: [19] <author> S. L. Scott, </author> <title> A Cache Coherence Mechanism for Scalable, Shared Memory Multiprocessors, </title> <booktitle> Proc. 1991 International Symposium on Shared Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: The analysis thus far has considered only unloaded latency. When bandwidth is considered, the argument for keeping the radix small becomes more compelling. Under uniform traffic, the rate of traffic across a link is proportional to the radix of the network <ref> [19] </ref>. Thus, if the network is grown by increasing the radix, the rate of traffic per link will also increase. Although this will be offset by larger link widths when wiring is constrained, the total throughput per processor still argues for lower radix.
Reference: [20] <author> S. L. Scott, J. R. Goodman, and M. K. Vernon, </author> <title> Performance of the SCI Ring, </title> <booktitle> To appear in the 19th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: By guaranteeing that buffer congestion occurs only when entering/exiting a dimension, 3 deadlock free routing is provided for uni-directional k-ary n-cubes. In addition, sustained throughput is able to come close to maximum throughput, especially when k is large (see <ref> [20] </ref> and Section 4 of this paper). Conventional networks using flit-level flow control typically provide half or less of their maximum throughput, due to coupled resource allocation [7]. Note, however, that the switches described in Section 2.1 require buffers able to contain an entire packet. <p> A dimension is skipped if the packet source and destination have the same ordinate in that dimension. A low-level protocol handles the transmission around each ring, independent of the other dimensions and any higher-level, end-to-end protocol. The ring interfaces are based loosely on the IEEE SCI logical layer protocol <ref> [12, 20] </ref>. The links are W bits wide, so a packet of L bits is decomposed into P flits, with P given by (1) P = J L hhh J Routing is similar to virtual cut through [13]. <p> In this way the ring buffer cannot fill up before the packet has been completely drained from the output queue. The SCI protocol, on which this is based, is discussed in more detail by Scott, Goodman and Vernon <ref> [20] </ref>. 2.2. Latency in a Pipelined-Channel Network The unloaded latency can be derived by simply accounting for the delays described in Section 2.1.
Reference: [21] <author> C. L. Seitz, </author> <title> Concurrent VLSI Architectures, </title> <journal> IEEE Transactions on Computers 33(12), </journal> <month> December </month> <year> 1984, </year> <pages> 775-785. </pages>
Reference-contexts: The performance of these networks has been extensively analyzed in the literature [9, 14, 16, 18, 29]. Direct networks incorporate the processing elements within the network itself, allowing for direct communication between processors <ref> [21] </ref>. The performance of these networks has been the subject of more recent attention [1-3, 7].
Reference: [22] <author> C. L. Seitz, </author> <title> The Cosmic Cube, </title> <journal> Communications of the ACM 28(1), </journal> <month> January </month> <year> 1985, </year> <pages> 22-33. </pages>
Reference-contexts: The performance of these networks has been the subject of more recent attention [1-3, 7]. Direct networks are gaining in popularity and have been employed in many recent existing or proposed machines, including the Connection Machine [11], Intel iPSC, Cosmic Cube <ref> [22] </ref>, Tera supercomputer [4], CMU-Intel iWarp [5], and Stanford DASH multiprocessor [17]. The most commonly used direct networks are variants of the k-ary n-cube [26].
Reference: [23] <author> C. L. Seitz, </author> <type> Personal correspondence. </type> <month> April </month> <year> 1992. </year>
Reference-contexts: Limited pipelined channels (allowing a small, fixed number of bits on a wire) are used in the CMU-Intel iWarp [5], various Cray Research machines [25] and the Thinking Machine CM5 [28]. A higher degree of pipelining is achievable using the Caltech Slack chip <ref> [23] </ref>, which is designed to allow tightly-coupled Mosaic channels to communicate efficiently over long cables. The IEEE Scalable Coherent Interface (SCI) [12], on which we base our model, allows an arbitrary amount of pipelining.
Reference: [24] <author> H. J. Siegel, </author> <title> Interconnection Networks for SIMD Machines, </title> <booktitle> Computer 12(6), </booktitle> <month> June, </month> <year> 1979, </year> <pages> 57-65. 30 </pages>
Reference-contexts: 1. INTRODUCTION The interconnection network used in a multiprocessor or multicomputer can have a major impact on system performance and cost. Understanding the tradeoffs in network design thus becomes a critical requirement for building cost-effective, high-performance systems. A wide variety of interconnection networks have been proposed (see [10] or <ref> [24] </ref> for a summary), each of which can be classified as either direct or indirect. Indirect networks, such as the omega network [15], connect processors and memories through multiple intermediate stages of switching elements.
Reference: [25] <author> J. E. Smith, </author> <type> Personal correspondence. </type> <month> April </month> <year> 1992. </year>
Reference-contexts: Pipelined channels are not widely used in multiprocessor interconnects, however. Limited pipelined channels (allowing a small, fixed number of bits on a wire) are used in the CMU-Intel iWarp [5], various Cray Research machines <ref> [25] </ref> and the Thinking Machine CM5 [28]. A higher degree of pipelining is achievable using the Caltech Slack chip [23], which is designed to allow tightly-coupled Mosaic channels to communicate efficiently over long cables.
Reference: [26] <author> H. Sullivan and T. R. Bashkow, </author> <title> A Large Scale, Homogeneous, Fully Distributed Parallel Machine, </title> <booktitle> Proc. 4th Annual International Symposium on Computer Architecture, </booktitle> <month> March </month> <year> 1977, </year> <pages> 105-117. </pages>
Reference-contexts: The most commonly used direct networks are variants of the k-ary n-cube <ref> [26] </ref>. The k-ary n-cube consists of N =k n nodes, arranged in n dimensions, with k nodes per dimension (n is the dimensionality, k is the radix). Figure 1 illustrates several different k-ary n-cubes.
Reference: [27] <author> A. S. Tannenbaum, </author> <title> Computer Networks, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1988. </year>
Reference-contexts: Pipelined channels have long been used in wide area networks and local area networks, since the physical delays involved compel their use. There is an abundance of research in the literature regarding transmission protocols, reliability, flow control, routing, performance and many other issues regarding these networks <ref> [27] </ref>. Pipelined channels are not widely used in multiprocessor interconnects, however. Limited pipelined channels (allowing a small, fixed number of bits on a wire) are used in the CMU-Intel iWarp [5], various Cray Research machines [25] and the Thinking Machine CM5 [28].
Reference: [28] <author> TMC, </author> <title> CM5 Reference Manual, Thinking Machines, </title> <publisher> Inc., </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Pipelined channels are not widely used in multiprocessor interconnects, however. Limited pipelined channels (allowing a small, fixed number of bits on a wire) are used in the CMU-Intel iWarp [5], various Cray Research machines [25] and the Thinking Machine CM5 <ref> [28] </ref>. A higher degree of pipelining is achievable using the Caltech Slack chip [23], which is designed to allow tightly-coupled Mosaic channels to communicate efficiently over long cables. The IEEE Scalable Coherent Interface (SCI) [12], on which we base our model, allows an arbitrary amount of pipelining.
Reference: [29] <author> H. Yoon, K. Y. Lee, and M. T. Liu, </author> <title> Performance Analysis and Comparison of Packet Switching Interconnection Networks, </title> <booktitle> Proc. 1987 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1987, </year> <pages> 542-545. 31 </pages>
Reference-contexts: Indirect networks, such as the omega network [15], connect processors and memories through multiple intermediate stages of switching elements. The performance of these networks has been extensively analyzed in the literature <ref> [9, 14, 16, 18, 29] </ref>. Direct networks incorporate the processing elements within the network itself, allowing for direct communication between processors [21]. The performance of these networks has been the subject of more recent attention [1-3, 7].
References-found: 29

