URL: http://www.cs.mu.oz.au/tr_db/mu_93_28.ps.gz
Refering-URL: http://www.cs.mu.oz.au/tr_db/TR.html
Root-URL: 
Title: Static Compression for Dynamic Texts  
Author: Alistair Moffat Neil Sharman Justin Zobel 
Abstract: Two problems arise when semi-static word-based compression methods are applied to large texts, such as those stored in information retrieval systems. First, the space required for the model during decoding can become very large. Second, the need to handle document insertions means that the collection must be periodically recompressed if compression efficiency is to be maintained. Here we show that with careful management the impact of both of these drawbacks can be minimised. Experiments with a word-based model and over 500 Mb of text show that compression rates can be retained even in the face of severe memory limitations on the decoder, and in the face of significant expansion in the size of the text itself.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T.C. Bell, A. Moffat, C.G. Nevill-Manning, I.H. Witten, and J. Zobel. </author> <title> Data compression in full-text retrieval systems. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 44 </volume> <pages> 508-531, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: It is particularly appropriate for compressing full-text document collections, an application in which very large quantities of text are stored, but individual documents must be independently decodable <ref> [1] </ref>. In this case a semi-static approach that makes a preliminary pass over the text before compressing it in a second pass is appropriate, and when the word-based model is coupled with static Huffman coding, it yields the three necessary properties: good compression, fast decoding, and random access [10].
Reference: [2] <author> J. Bentley, D. Sleator, R. Tarjan, and V. Wei. </author> <title> A locally adaptive data compression scheme. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 320-330, </pages> <month> April </month> <year> 1986. </year>
Reference-contexts: 1 Word-Based Compression The use of a word-based zero-order compression model to represent English text has been considered by several authors <ref> [2, 6, 7, 15] </ref>. It is particularly appropriate for compressing full-text document collections, an application in which very large quantities of text are stored, but individual documents must be independently decodable [1]. <p> Method B3 was selected as a basis for further experiments, for two reasons: first, because it gave the best compression of the approaches described so far; and second because, in contrast to the binary code of method B2, it offers codewords of varying length. Bentley et al. <ref> [2] </ref> describe a "Move-to-Front" (MTF) heuristic for assigning codes to symbols. We considered the use of this strategy for reorganising the list of words in the auxiliary lexicon, but discarded it because of the relatively high cost of tracking list positions.
Reference: [3] <author> G.V. Cormack and R.N. Horspool. </author> <title> Data compression using dynamic Markov modelling. </title> <journal> Computer Journal, </journal> <volume> 30 </volume> <pages> 541-550, </pages> <month> December </month> <year> 1987. </year> <month> 15 </month>
Reference-contexts: 30 Zero-order character-based, semi static, Huffman coding cacm 61.2 6 5 Zero-order character-based, adap tive, arithmetic coding [17] compress 43.2 25 55 LZ78 implementation [12] gzip-f 38.6 15 95 LZ77 method, fast option gzip-b 36.8 8 100 LZ77 method, best option dmc 33.2 1 0.7 Variable-order bit-based, adaptive, arithmetic coding <ref> [3] </ref> ppmc 30.3 4 4 Third-order character-based, adap tive, arithmetic coding [8] huffword 28.4 10 65 Zero-order word-based, semi-static, Huffman coding [10] Table 4: Compression and throughput on wsj be adapted to cope well both with dynamic environments, and with situations in which decode-time memory is limited.
Reference: [4] <author> P. Elias. </author> <title> Universal codeword sets and representations of the integers. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-21:194-203, </volume> <month> March </month> <year> 1975. </year>
Reference-contexts: Now locations in the auxiliary lexicon must be coded, and to complete the scheme some method for doing this is required. Method B1 One possibility is to use Elias's C ffi code <ref> [4] </ref> for positive integers. Each word in the auxiliary lexicon is assigned a code based upon its ordinal number: the first, a 1-bit code; the second and third, 4-bit codes; and, in general, the n'th a code of approximately 1 + 2 log log n + log n bits.
Reference: [5] <author> D. Harman. </author> <title> Overview of the first text retrieval conference. </title> <booktitle> In Proc. First Text REtrieval Conference (TREC-1), </booktitle> <pages> pages 1-20, </pages> <address> Gaithersburg, Maryland, </address> <month> November </month> <year> 1992. </year> <institution> National Institute of Standards. </institution>
Reference-contexts: As part of the international trec information retrieval experiment we have been dealing with several corpora of English text, each of approximately 500 Mb <ref> [5] </ref>. One of the collections is several years of articles from the Wall Street Journal , and this 508.15 Mb wsj database uses 289,101 distinct words totalling 2,159,044 characters; and 8,912 distinct non-words requiring in total 77,882 bytes.
Reference: [6] <author> R.N. Horspool and G.V. Cormack. </author> <title> Constructing word-based text compression algorithms. </title> <booktitle> In Storer and Cohn [11], </booktitle> <pages> pages 62-81. </pages>
Reference-contexts: 1 Word-Based Compression The use of a word-based zero-order compression model to represent English text has been considered by several authors <ref> [2, 6, 7, 15] </ref>. It is particularly appropriate for compressing full-text document collections, an application in which very large quantities of text are stored, but individual documents must be independently decodable [1].
Reference: [7] <author> A. Moffat. </author> <title> Word based text compression. </title> <journal> Software|Practice and Experience, </journal> <volume> 19 </volume> <pages> 185-198, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: 1 Word-Based Compression The use of a word-based zero-order compression model to represent English text has been considered by several authors <ref> [2, 6, 7, 15] </ref>. It is particularly appropriate for compressing full-text document collections, an application in which very large quantities of text are stored, but individual documents must be independently decodable [1].
Reference: [8] <author> A. Moffat. </author> <title> Implementing the PPM data compression scheme. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 38 </volume> <pages> 1917-1921, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Zero-order character-based, adap tive, arithmetic coding [17] compress 43.2 25 55 LZ78 implementation [12] gzip-f 38.6 15 95 LZ77 method, fast option gzip-b 36.8 8 100 LZ77 method, best option dmc 33.2 1 0.7 Variable-order bit-based, adaptive, arithmetic coding [3] ppmc 30.3 4 4 Third-order character-based, adap tive, arithmetic coding <ref> [8] </ref> huffword 28.4 10 65 Zero-order word-based, semi-static, Huffman coding [10] Table 4: Compression and throughput on wsj be adapted to cope well both with dynamic environments, and with situations in which decode-time memory is limited.
Reference: [9] <author> A. Moffat, N. Sharman, I.H. Witten, </author> <title> and T.C. Bell. An empirical evaluation of coding methods for multi-symbol alphabets. Information Processing & Management. </title> <note> To appear. </note>
Reference-contexts: The latter can, however, be assumed to be similar to the character probabilities already observed in the text available, provided only that every symbol is allocated a code whether it has occurred or not. The escape probability is harder to estimate, but based upon previous experiments <ref> [9] </ref>, method XC of Witten and Bell [14] provides a good approximation. This technique assigns the escape symbol a "frequency" of t 1 , where t 1 is the number of symbols that have occurred once. <p> One way this flexibility could be exploited is to make the Huffman code for the lexicon words adaptive. But the resultant complex juggling of codewords substantially slows decoding <ref> [9] </ref>, and one of the virtues of the word-based scheme is lost. On the other hand the words in the auxiliary lexicon of Method B are coded using very simple codes, and there is also some advantage to be gained by reorganising those codewords on the fly.
Reference: [10] <author> A. Moffat and J. Zobel. </author> <title> Coding for compression in full-text retrieval systems. </title> <booktitle> In Storer and Cohn [11], </booktitle> <pages> pages 72-81. </pages>
Reference-contexts: In this case a semi-static approach that makes a preliminary pass over the text before compressing it in a second pass is appropriate, and when the word-based model is coupled with static Huffman coding, it yields the three necessary properties: good compression, fast decoding, and random access <ref> [10] </ref>. There are, however, two drawbacks to the use of this combination. First, a great deal of decode-time memory space might be required to store the tokens of the compression alphabet. <p> These experiments were run on a Sun SPARC 10 Model 512, which is experimentally about 2.2 times faster than a SPARCstation 2. The last row shows the huffword method that we have assumed throughout this paper: a zero-order word-based model using canonical Huffman coding <ref> [10] </ref>. Although it is relatively slow during compression, it provide fast decompression, and compression efficiency unmatched by any of the other methods that were tested. Witten et al. [16] describe further details of this implementation. 6 Summary Word-based text compression models have been advocated by many researchers. <p> 55 LZ78 implementation [12] gzip-f 38.6 15 95 LZ77 method, fast option gzip-b 36.8 8 100 LZ77 method, best option dmc 33.2 1 0.7 Variable-order bit-based, adaptive, arithmetic coding [3] ppmc 30.3 4 4 Third-order character-based, adap tive, arithmetic coding [8] huffword 28.4 10 65 Zero-order word-based, semi-static, Huffman coding <ref> [10] </ref> Table 4: Compression and throughput on wsj be adapted to cope well both with dynamic environments, and with situations in which decode-time memory is limited.
Reference: [11] <editor> J.A. Storer and M. Cohn, editors. </editor> <booktitle> Proc. IEEE Data Compression Conference, </booktitle> <address> Snowbird, Utah, March 1992. </address> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California. </address>
Reference: [12] <author> T.A. Welch. </author> <title> A technique for high performance data compression. </title> <journal> IEEE Computer, </journal> <volume> 17 </volume> <pages> 8-20, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: model can 14 Method Compression Encoding Decoding Description (% of input) (Mb/min) (Mb/min) lzrw1 63.5 70 190 LZ77 method, tuned for speed [13] pack 61.8 60 30 Zero-order character-based, semi static, Huffman coding cacm 61.2 6 5 Zero-order character-based, adap tive, arithmetic coding [17] compress 43.2 25 55 LZ78 implementation <ref> [12] </ref> gzip-f 38.6 15 95 LZ77 method, fast option gzip-b 36.8 8 100 LZ77 method, best option dmc 33.2 1 0.7 Variable-order bit-based, adaptive, arithmetic coding [3] ppmc 30.3 4 4 Third-order character-based, adap tive, arithmetic coding [8] huffword 28.4 10 65 Zero-order word-based, semi-static, Huffman coding [10] Table 4: Compression
Reference: [13] <author> R.N. Williams. </author> <title> An extremely fast Ziv-Lempel data compression algorithm. </title> <booktitle> In Proc. IEEE Data Compression Conference, </booktitle> <pages> pages 362-371, </pages> <address> Snowbird, Utah, April 1991. </address> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California. </address>
Reference-contexts: It has been shown that the word-based model can 14 Method Compression Encoding Decoding Description (% of input) (Mb/min) (Mb/min) lzrw1 63.5 70 190 LZ77 method, tuned for speed <ref> [13] </ref> pack 61.8 60 30 Zero-order character-based, semi static, Huffman coding cacm 61.2 6 5 Zero-order character-based, adap tive, arithmetic coding [17] compress 43.2 25 55 LZ78 implementation [12] gzip-f 38.6 15 95 LZ77 method, fast option gzip-b 36.8 8 100 LZ77 method, best option dmc 33.2 1 0.7 Variable-order bit-based,
Reference: [14] <author> I.H. Witten and T.C. Bell. </author> <title> The zero frequency problem: Estimating the probabilities of novel events in adaptive text compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37 </volume> <pages> 1085-1094, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The escape probability is harder to estimate, but based upon previous experiments [9], method XC of Witten and Bell <ref> [14] </ref> provides a good approximation. This technique assigns the escape symbol a "frequency" of t 1 , where t 1 is the number of symbols that have occurred once. That is, the number of symbols of frequency zero is approximated by the number of symbols of frequency one.
Reference: [15] <author> I.H. Witten, </author> <title> T.C. Bell, and C.G. Nevill. Indexing and compressing full-text databases for CD-ROM. </title> <journal> Journal of Information Science, </journal> <volume> 17 </volume> <pages> 265-271, </pages> <year> 1992. </year>
Reference-contexts: 1 Word-Based Compression The use of a word-based zero-order compression model to represent English text has been considered by several authors <ref> [2, 6, 7, 15] </ref>. It is particularly appropriate for compressing full-text document collections, an application in which very large quantities of text are stored, but individual documents must be independently decodable [1].
Reference: [16] <author> I.H. Witten, A. Moffat, </author> <title> and T.C. Bell. Managing Gigabytes: Compressing and indexing large collections of documents and images. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York. </address> <publisher> Forthcoming. </publisher>
Reference-contexts: Although it is relatively slow during compression, it provide fast decompression, and compression efficiency unmatched by any of the other methods that were tested. Witten et al. <ref> [16] </ref> describe further details of this implementation. 6 Summary Word-based text compression models have been advocated by many researchers. Here we have explored the particular needs of large information retrieval systems, in which hundreds of megabytes of data are stored, retrieval is non-sequential, and new text is continually being appended.
Reference: [17] <author> I.H. Witten, R. Neal, and J.G. Cleary. </author> <title> Arithmetic coding for data compression. </title> <journal> Communications of the ACM, </journal> <volume> 30 </volume> <pages> 520-541, </pages> <month> June </month> <year> 1987. </year> <month> 16 </month>
Reference-contexts: It has been shown that the word-based model can 14 Method Compression Encoding Decoding Description (% of input) (Mb/min) (Mb/min) lzrw1 63.5 70 190 LZ77 method, tuned for speed [13] pack 61.8 60 30 Zero-order character-based, semi static, Huffman coding cacm 61.2 6 5 Zero-order character-based, adap tive, arithmetic coding <ref> [17] </ref> compress 43.2 25 55 LZ78 implementation [12] gzip-f 38.6 15 95 LZ77 method, fast option gzip-b 36.8 8 100 LZ77 method, best option dmc 33.2 1 0.7 Variable-order bit-based, adaptive, arithmetic coding [3] ppmc 30.3 4 4 Third-order character-based, adap tive, arithmetic coding [8] huffword 28.4 10 65 Zero-order word-based,
References-found: 17

