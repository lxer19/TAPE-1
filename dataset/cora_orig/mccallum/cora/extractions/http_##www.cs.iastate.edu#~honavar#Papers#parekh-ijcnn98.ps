URL: http://www.cs.iastate.edu/~honavar/Papers/parekh-ijcnn98.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/honavar.html
Root-URL: 
Email: fparekhjhonavarg@cs.iastate.edu  
Title: Constructive Theory Refinement in Knowledge Based Neural Networks  
Author: Rajesh Parekh Vasant Honavar 
Address: 226 Atanasoff Hall  Ames IA 50011. U.S.A.  
Affiliation: Artificial Intelligence Research Group Department of Computer Science  Iowa State University  
Abstract: Knowledge based artificial neural networks offer an approach for connectionist theory refinement. We present an algorithm for refining and extending the domain theory incorporated in a knowledge based neural network using constructive neural network learning algorithms. The initial domain theory comprising of propositional rules is translated into a knowledge based network of threshold logic units (threshold neuron). The domain theory is modified by dynamically adding neurons to the existing network. A constructive neural network learning algorithm is used to add and train these additional neurons using a sequence of labeled examples. We propose a novel hybrid constructive learning algorithm based on the Tiling and Pyramid constructive learning algorithms that allows knowledge based neural network to handle patterns with continuous valued attributes. Results of experiments on two non-trivial tasks (the ribosome binding site prediction and the financial advisor) show that our algorithm compares favorably with other algorithms for connectionist theory refinement both in terms of generalization accuracy and network size. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Mitchell, </author> <title> Machine Learning, </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: Smaller networks tend to generalize better under otherwise similar scenarios <ref> [1] </ref>. B. Financial Advisor Dataset As described earlier the financial advisor dataset comprises of 5500 patterns that are generated at random to satisfy the rules mentioned in Table I. Incomplete domain knowledge was modeled by pruning certain rules and their antecedents from the original rule base (as described in [11]).
Reference: [2] <author> P. Langley, </author> <title> Elements of Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference: [3] <author> V. </author> <title> Honavar, </title> <booktitle> "Machine learning: Principles and applications," in Encyclopedia of Electrical and Electronics Engineering, </booktitle> <editor> J. Web-ster, Ed. </editor> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1998, </year> <note> To appear. </note>
Reference: [4] <author> S. Gallant, </author> <title> Neural Network Learning and Expert Systems, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: It is possi-ble to use any of the constructive neural network learning algorithms described in [7] for this purpose. However, this paper focuses on a hybrid algorithm that combines the features of the multi-category versions [7], [6], [9] of the Tiling [13] and the Pyramid <ref> [4] </ref> algorithms. The Pyramid algorithm successively adds new layers of threshold neurons to the network. Each newly added layer becomes the network's new output layer. The neurons of the new layer are connected to the input layer and all previously added layers of the network. <p> The convergence of the Pyramid algorithm is proved by demonstrating that the total classification error in the training set decreases with each added layer. The Pyramid algorithm as originally proposed by Gallant <ref> [4] </ref> assumes the patterns to be binary or bipolar. The extension of the Pyramid algorithm to handle real valued attributes requires a pre-processing of the pattern set. The individual patterns could be normalized or projected onto a parabolic surface, or discretized to guarantee convergence [7]. <p> KBANN uses a rules-to-network algorithm to construct an AND-OR graph representation of the initial domain knowledge and translates this graph to an appropriate neural network topology. KBANN then uses the standard backpropagation learning algorithm <ref> [4] </ref> to refine the domain knowledge. The approaches described by Fu [27] and Katz [28] are similar to the KBANN algorithm. The KBANN learning algorithm is demonstrated to perform better than several other machine learning algorithms on domains such as promoter and splice-junction datasets [12], [26]. <p> It uses the hyperplane detection from examples (HDE) algorithm [30] to construct the hidden layer. Each hidden neuron corresponds to a hyperplane. Fletcher and Obradovic's algorithm maps these hyperplanes to a set of threshold neurons and then then trains the output neuron using the pocket algorithm <ref> [4] </ref>. Our approach is similar to the one taken by Fletcher and Obradovic. Instead of constructing a single hidden layer Tiling-Pyramid builds a network of one or more hidden layers (if necessary) on top of the initial network representing the domain theory (see Fig. 1).
Reference: [5] <author> V. Honavar and L. Uhr, </author> <title> "Generative learning structures and processes for connectionist networks," </title> <journal> Information Sciences, </journal> <volume> vol. 70, </volume> <pages> pp. 75-108, </pages> <year> 1993. </year>
Reference: [6] <author> V. Honavar, </author> <title> "Structural learning," in Encyclopedia of Electrical and Electronics Engineering, </title> <editor> J. Webster, Ed. </editor> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1998, </year> <note> To appear. </note>
Reference-contexts: Embedding the Financial Advisor Domain Theory in a Neural Network. B. A Hybrid Constructive Learning Algorithm The proposed approach to data-driven, knowledge-guided theory refinement uses a constructive neural network learning algorithm <ref> [6] </ref> to augment the domain theory that is encoded by the initial network topology. It is possi-ble to use any of the constructive neural network learning algorithms described in [7] for this purpose. However, this paper focuses on a hybrid algorithm that combines the features of the multi-category versions [7], [6], <p> <ref> [6] </ref> to augment the domain theory that is encoded by the initial network topology. It is possi-ble to use any of the constructive neural network learning algorithms described in [7] for this purpose. However, this paper focuses on a hybrid algorithm that combines the features of the multi-category versions [7], [6], [9] of the Tiling [13] and the Pyramid [4] algorithms. The Pyramid algorithm successively adds new layers of threshold neurons to the network. Each newly added layer becomes the network's new output layer.
Reference: [7] <author> R. G. Parekh, J. Yang, and V. G. Honavar, </author> <title> "Constructive neural network learning algorithms for multi-category real-valued pattern classification," </title> <type> Tech. Rep. </type> <institution> ISU-CS-TR97-06, Department of Computer Science, Iowa State University, </institution> <year> 1997, </year> <note> (Submitted). </note>
Reference-contexts: A Hybrid Constructive Learning Algorithm The proposed approach to data-driven, knowledge-guided theory refinement uses a constructive neural network learning algorithm [6] to augment the domain theory that is encoded by the initial network topology. It is possi-ble to use any of the constructive neural network learning algorithms described in <ref> [7] </ref> for this purpose. However, this paper focuses on a hybrid algorithm that combines the features of the multi-category versions [7], [6], [9] of the Tiling [13] and the Pyramid [4] algorithms. The Pyramid algorithm successively adds new layers of threshold neurons to the network. <p> It is possi-ble to use any of the constructive neural network learning algorithms described in <ref> [7] </ref> for this purpose. However, this paper focuses on a hybrid algorithm that combines the features of the multi-category versions [7], [6], [9] of the Tiling [13] and the Pyramid [4] algorithms. The Pyramid algorithm successively adds new layers of threshold neurons to the network. Each newly added layer becomes the network's new output layer. <p> The extension of the Pyramid algorithm to handle real valued attributes requires a pre-processing of the pattern set. The individual patterns could be normalized or projected onto a parabolic surface, or discretized to guarantee convergence <ref> [7] </ref>. The algorithm proposed in this paper relies on discretiza-tion of continuous valued attributes. Discretization (or quantization) involves a mapping of the pattern space R N to an equivalent bipolar (or binary) representation f1; 1g N (typically, N 0 &gt; N ). <p> The interested reader is referred to [15] for a survey of several approaches to quantization. We use an adaptive quantization algorithm that dynamically constructs an appropriate sized layer of threshold neurons to perform vector quantization which is based on the MTiling algorithm <ref> [7] </ref>, [9]. MTiling, is a multicategory version of the Tiling algorithm [13]. It trains a group of M master neurons (where M is the number of output classes) using a perceptron style learning algorithm. <p> Furthermore, the hybrid approach used in Tiling-Pyramid lends itself to the use of other network construction algorithms in place of the Pyramid algorithm. Since the performance of different constructive learning algorithms often differs quite significantly for different datasets <ref> [7] </ref>, this flexibility is likely to be of use in practice. Experimental results demonstrate that the performance of the Tiling-Pyramid algorithm compares favorably with those reported by Fletcher and Obradovic on the financial advisor rule base in terms of both the generalization accuracy and the network size.
Reference: [8] <author> J. Yang, R. Parekh, and V. Honavar, </author> <title> "DistAl: An inter-pattern distance-based constructive learning algorithm," </title> <type> Tech. Rep. </type> <institution> ISU-CS-TR 97-05, Iowa State University, </institution> <year> 1997. </year>
Reference: [9] <author> J. Yang, R. Parekh, and V. Honavar, </author> <title> "MTiling a constructive neural network learning algorithm for multi-category pattern classification," </title> <booktitle> in Proceedings of the World Congress on Neural Networks'96, </booktitle> <address> San Diego, </address> <year> 1996, </year> <pages> pp. 182-187. </pages>
Reference-contexts: It is possi-ble to use any of the constructive neural network learning algorithms described in [7] for this purpose. However, this paper focuses on a hybrid algorithm that combines the features of the multi-category versions [7], [6], <ref> [9] </ref> of the Tiling [13] and the Pyramid [4] algorithms. The Pyramid algorithm successively adds new layers of threshold neurons to the network. Each newly added layer becomes the network's new output layer. <p> The interested reader is referred to [15] for a survey of several approaches to quantization. We use an adaptive quantization algorithm that dynamically constructs an appropriate sized layer of threshold neurons to perform vector quantization which is based on the MTiling algorithm [7], <ref> [9] </ref>. MTiling, is a multicategory version of the Tiling algorithm [13]. It trains a group of M master neurons (where M is the number of output classes) using a perceptron style learning algorithm.
Reference: [10] <author> G. F. Luger and W. A. Stubblefield, </author> <booktitle> Artificial Intelligence and the Design of Expert Systems, </booktitle> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1989. </year>
Reference-contexts: Note that this threshold neuron implements the propositional rule "if a 4b &gt; 6 then c". Using the approach outlined above, the initial neural network topology corresponding to the simple financial advisor rule base (due to <ref> [10] </ref>) of Table I is shown in Fig. 5. Each threshold neuron in the network computes a bipolar hardlimiting function (i.e., the threshold neuron's outputs Fig. 3. Neural Network Implementation of Knowledge Rules. Fig. 4.
Reference: [11] <author> J. Fletcher and Z. Obradovic, </author> <title> "Combining prior symbolic knowledge and constructive neural network learning," </title> <journal> Connection Science, </journal> <volume> vol. 5, no. 3,4, </volume> <pages> pp. 365-375, </pages> <year> 1993. </year>
Reference-contexts: II. Constructive Theory Refinement Using a Knowledge-Based Neural Network A. Embedding the Domain Theory in the Neural Network We use a symbolic knowledge encoding procedure to translate a domain theory in the form of propositional rules into a network of threshold neurons using rules-to-networks algorithm of Towell and Shavlik <ref> [11] </ref>, [12]. The procedure involves rewriting the rules into a format that highlights the hierarchical structure of the domain theory. In particular the disjuncts are expressed as a set of rules that each have only one antecedent. <p> The ribosome binding set's domain theory contains 17 rules. Additionally, there are 1877 labeled training examples. The promoter dataset contains a set of 31 rules and 936 labeled training examples. The financial advisor rule base is shown in Table I. Following the procedure used by Fletcher and Obradovic <ref> [11] </ref>, a set of 5500 labeled examples (500 for training and 5000 for testing) are randomly generated using the financial advisor rule base. We used the Tiling-Pyramid constructive learning algorithm described in section II to augment the initial domain knowledge. The hybrid network was trained using the thermal perceptron [17]. <p> B. Financial Advisor Dataset As described earlier the financial advisor dataset comprises of 5500 patterns that are generated at random to satisfy the rules mentioned in Table I. Incomplete domain knowledge was modeled by pruning certain rules and their antecedents from the original rule base (as described in <ref> [11] </ref>). For example, if sav adeq was selected as the pruning point, then the rules for sav adeq, dep sav adeq, and assets hi are eliminated from the rule base. In other words rules 2, 3, 6, and 7 are pruned. <p> Furthermore, rule 1 is modified to read "if (inc adeq) then invest stocks". The initial network is constructed from this modified rule base and is then augmented using constructive learning. Our experiments follow those performed by Fletcher and Obradovic <ref> [11] </ref>. In Table IV we summarize the average generalization (on the 5000 test patterns) and the average network size over 25 runs for 6 different pruning points. The generalization accuracy of the corresponding network prior to the theory refinement (i.e., based on rules alone) is also reported. <p> In Table V we show the results for the experiments with the HDE algorithm that were reported by Fletcher and Obradovic 3 <ref> [11] </ref>. <p> Fletcher and Obradovic's algorithm starts with an initial network representing the domain theory and modifies this theory by training a single hidden layer of threshold neurons using the labeled training data <ref> [11] </ref>. It uses the hyperplane detection from examples (HDE) algorithm [30] to construct the hidden layer. Each hidden neuron corresponds to a hyperplane. Fletcher and Obradovic's algorithm maps these hyperplanes to a set of threshold neurons and then then trains the output neuron using the pocket algorithm [4].
Reference: [12] <author> G. G. Towell, J. W. Shavlik, and M. O. Noordwier, </author> <title> "Refinement of approximate domain theories by knowledge-based neural networks," </title> <booktitle> in Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA, </address> <year> 1990, </year> <pages> pp. 861-866. </pages>
Reference-contexts: Constructive Theory Refinement Using a Knowledge-Based Neural Network A. Embedding the Domain Theory in the Neural Network We use a symbolic knowledge encoding procedure to translate a domain theory in the form of propositional rules into a network of threshold neurons using rules-to-networks algorithm of Towell and Shavlik [11], <ref> [12] </ref>. The procedure involves rewriting the rules into a format that highlights the hierarchical structure of the domain theory. In particular the disjuncts are expressed as a set of rules that each have only one antecedent. <p> Some symbolic theory refinement algorithms use Inductive Logic Programming (ILP) based techniques [23]. Examples of such systems which use first order predicate logic for knowledge representation include FOCL [24] and FORTE [25]. Towell and Shavlik proposed the KBANN (knowledge based artificial neural network) learning algorithm for connectionist theory refinement <ref> [12] </ref>, [26]. KBANN uses a rules-to-network algorithm to construct an AND-OR graph representation of the initial domain knowledge and translates this graph to an appropriate neural network topology. KBANN then uses the standard backpropagation learning algorithm [4] to refine the domain knowledge. <p> The approaches described by Fu [27] and Katz [28] are similar to the KBANN algorithm. The KBANN learning algorithm is demonstrated to perform better than several other machine learning algorithms on domains such as promoter and splice-junction datasets <ref> [12] </ref>, [26]. However, KBANN is limited by the fact that it does not modify the network topology. The TopGen [29] and REGENT [18] algorithms were designed to add new neurons to the KBANN network thereby extending the realm of network topologies considered by KBANN.
Reference: [13] <author> M. Mezard and J. Nadal, </author> <title> "Learning feed-forward networks: The tiling algorithm," </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> vol. 22, </volume> <pages> pp. 2191-2203, </pages> <year> 1989. </year>
Reference-contexts: It is possi-ble to use any of the constructive neural network learning algorithms described in [7] for this purpose. However, this paper focuses on a hybrid algorithm that combines the features of the multi-category versions [7], [6], [9] of the Tiling <ref> [13] </ref> and the Pyramid [4] algorithms. The Pyramid algorithm successively adds new layers of threshold neurons to the network. Each newly added layer becomes the network's new output layer. The neurons of the new layer are connected to the input layer and all previously added layers of the network. <p> We use an adaptive quantization algorithm that dynamically constructs an appropriate sized layer of threshold neurons to perform vector quantization which is based on the MTiling algorithm [7], [9]. MTiling, is a multicategory version of the Tiling algorithm <ref> [13] </ref>. It trains a group of M master neurons (where M is the number of output classes) using a perceptron style learning algorithm.
Reference: [14] <author> T Kohonen, </author> <title> Self-Organization and Associative Memory, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1989. </year>
Reference-contexts: The algorithm proposed in this paper relies on discretiza-tion of continuous valued attributes. Discretization (or quantization) involves a mapping of the pattern space R N to an equivalent bipolar (or binary) representation f1; 1g N (typically, N 0 &gt; N ). Quantization algorithms e.g., the Learning Vector Quantizer (LVQ) <ref> [14] </ref>, are often used in conjunction with supervised machine learning algorithms. The interested reader is referred to [15] for a survey of several approaches to quantization.
Reference: [15] <author> J. Dougherty, R. Kohavi, and M. Sahami, </author> <title> "Supervised and unsupervised discretization of continuous features," </title> <booktitle> in Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> San Fransisco, CA, </address> <year> 1995, </year> <pages> pp. 194-202. </pages>
Reference-contexts: Quantization algorithms e.g., the Learning Vector Quantizer (LVQ) [14], are often used in conjunction with supervised machine learning algorithms. The interested reader is referred to <ref> [15] </ref> for a survey of several approaches to quantization. We use an adaptive quantization algorithm that dynamically constructs an appropriate sized layer of threshold neurons to perform vector quantization which is based on the MTiling algorithm [7], [9]. MTiling, is a multicategory version of the Tiling algorithm [13].
Reference: [16] <author> R. Parekh, </author> <title> Constructive learning: Inducing grammars and neural networks, </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, Iowa State University, Ames, IA, </institution> <year> 1998. </year>
Reference-contexts: This choice was motivated by the results of experiments <ref> [16] </ref> that compared the Tiling-Pyramid and Tiling-Cascade learning algorithms with MPyramid, MCascade, and MTiling. The experiments showed that Tiling-Pyramid and Tiling-Cascade significantly outperformed MPyramid and MCascade algo rithms on many real-world datasets. Fig. 6. Block Diagram of a Hybrid Constructive Network. III.
Reference: [17] <author> M. Frean, </author> <title> "A thermal perceptron learning rule," </title> <journal> Neural Computation, </journal> <volume> vol. 4, </volume> <pages> pp. 946-957, </pages> <year> 1992. </year>
Reference-contexts: We used the Tiling-Pyramid constructive learning algorithm described in section II to augment the initial domain knowledge. The hybrid network was trained using the thermal perceptron <ref> [17] </ref>. Each threshold neuron was trained for 500 epochs with the initial weights chosen randomly between 1 and 1, the learning rate held constant at 1 and the initial temperature T 0 set to 1.0.
Reference: [18] <author> D. W. Opitz and J. W. Shavlik, </author> <title> "Connectionist theory refinement: Genetically searching the space of network topologies," </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> vol. 6, </volume> <pages> pp. 177-209, </pages> <year> 1997. </year>
Reference-contexts: The generalization performance of the refined domain theory (represented by the trained neural network) is significantly better than that of the original set of rules. Furthermore, our approach compares favorably with TopGen and REGENT <ref> [18] </ref> on both the datasets. In terms of generalization accuracy Tiling-Pyramid performs slightly worse than TopGen and REGENT on the ribosome dataset and slightly better on the promoter dataset. Our approach trains a single network as against a pool networks evaluated by TopGen and REGENT. <p> When compared with the training times for TopGen and REGENT (which are reported to be several days of CPU time <ref> [18] </ref>) we see that Tiling-Pyramid offers a significant advantage over TopGen and REGENT. (However, we must note that TopGen and REGENT appear to have been designed to exploit available computing resources and come up with hypotheses that have good generalization performance. <p> The KBANN learning algorithm is demonstrated to perform better than several other machine learning algorithms on domains such as promoter and splice-junction datasets [12], [26]. However, KBANN is limited by the fact that it does not modify the network topology. The TopGen [29] and REGENT <ref> [18] </ref> algorithms were designed to add new neurons to the KBANN network thereby extending the realm of network topologies considered by KBANN. <p> TopGen and REGENT reportedly outperform KBANN in terms of generalization performance on several problems <ref> [18] </ref>, [29]. The generalization performance of Tiling-Pyramid compares very favorably with that of TopGen and REGENT on both the ribosome binding site and promoter datasets. The hybrid Tiling-Pyramid algorithm generates significantly smaller networks as compared to both TopGen and REGENT.
Reference: [19] <author> A. Ginsberg, </author> <title> "Theory reduction, theory revision, </title> <booktitle> and retransla-tion," in Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA, </address> <year> 1990, </year> <pages> pp. 777-782, </pages> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Discussion Theory refinement systems can be broadly classified into two categories: symbolic and neural network based approaches. Some symbolic theory refinement algorithms use decision tree induction for theory revision. Examples of such systems include RTLS <ref> [19] </ref>, EITHER [20], PTR [21], and TGCI [22]. Some symbolic theory refinement algorithms use Inductive Logic Programming (ILP) based techniques [23]. Examples of such systems which use first order predicate logic for knowledge representation include FOCL [24] and FORTE [25].
Reference: [20] <author> D. Ourston and R. J. Mooney, </author> <title> "Theory refinement: Combining analytical and empirical methods," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 66, </volume> <pages> pp. 273-310, </pages> <year> 1994. </year>
Reference-contexts: Discussion Theory refinement systems can be broadly classified into two categories: symbolic and neural network based approaches. Some symbolic theory refinement algorithms use decision tree induction for theory revision. Examples of such systems include RTLS [19], EITHER <ref> [20] </ref>, PTR [21], and TGCI [22]. Some symbolic theory refinement algorithms use Inductive Logic Programming (ILP) based techniques [23]. Examples of such systems which use first order predicate logic for knowledge representation include FOCL [24] and FORTE [25].
Reference: [21] <author> M. Kopel, R. Feldman, and A. Serge, </author> <title> "Bias-driven revision of logical domain theories," </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> vol. 1, </volume> <pages> pp. 159-208, </pages> <year> 1994. </year>
Reference-contexts: Discussion Theory refinement systems can be broadly classified into two categories: symbolic and neural network based approaches. Some symbolic theory refinement algorithms use decision tree induction for theory revision. Examples of such systems include RTLS [19], EITHER [20], PTR <ref> [21] </ref>, and TGCI [22]. Some symbolic theory refinement algorithms use Inductive Logic Programming (ILP) based techniques [23]. Examples of such systems which use first order predicate logic for knowledge representation include FOCL [24] and FORTE [25].
Reference: [22] <author> S. Donoho and L. Rendell, </author> <title> "Representing and restructuring domain theories: A constructive induction approach," </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> vol. 2, </volume> <pages> pp. 411-446, </pages> <year> 1995. </year>
Reference-contexts: Discussion Theory refinement systems can be broadly classified into two categories: symbolic and neural network based approaches. Some symbolic theory refinement algorithms use decision tree induction for theory revision. Examples of such systems include RTLS [19], EITHER [20], PTR [21], and TGCI <ref> [22] </ref>. Some symbolic theory refinement algorithms use Inductive Logic Programming (ILP) based techniques [23]. Examples of such systems which use first order predicate logic for knowledge representation include FOCL [24] and FORTE [25].
Reference: [23] <author> S. Muggleton, </author> <title> Inductive Logic Programming, </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1992. </year>
Reference-contexts: Some symbolic theory refinement algorithms use decision tree induction for theory revision. Examples of such systems include RTLS [19], EITHER [20], PTR [21], and TGCI [22]. Some symbolic theory refinement algorithms use Inductive Logic Programming (ILP) based techniques <ref> [23] </ref>. Examples of such systems which use first order predicate logic for knowledge representation include FOCL [24] and FORTE [25]. Towell and Shavlik proposed the KBANN (knowledge based artificial neural network) learning algorithm for connectionist theory refinement [12], [26].
Reference: [24] <author> M. Pazzani and D. Kibler, </author> <title> "The utility of knowledge in inductive learning," </title> <journal> Machine Learning, </journal> <volume> vol. 9, </volume> <pages> pp. 57-94, </pages> <year> 1992. </year>
Reference-contexts: Examples of such systems include RTLS [19], EITHER [20], PTR [21], and TGCI [22]. Some symbolic theory refinement algorithms use Inductive Logic Programming (ILP) based techniques [23]. Examples of such systems which use first order predicate logic for knowledge representation include FOCL <ref> [24] </ref> and FORTE [25]. Towell and Shavlik proposed the KBANN (knowledge based artificial neural network) learning algorithm for connectionist theory refinement [12], [26]. KBANN uses a rules-to-network algorithm to construct an AND-OR graph representation of the initial domain knowledge and translates this graph to an appropriate neural network topology.
Reference: [25] <author> B. Richards and R. Mooney, </author> <title> "Automated refinement of first-order horn-clause domain theories," </title> <journal> Machine Learning, </journal> <volume> vol. 19, </volume> <pages> pp. 95-131, </pages> <year> 1995. </year>
Reference-contexts: Examples of such systems include RTLS [19], EITHER [20], PTR [21], and TGCI [22]. Some symbolic theory refinement algorithms use Inductive Logic Programming (ILP) based techniques [23]. Examples of such systems which use first order predicate logic for knowledge representation include FOCL [24] and FORTE <ref> [25] </ref>. Towell and Shavlik proposed the KBANN (knowledge based artificial neural network) learning algorithm for connectionist theory refinement [12], [26]. KBANN uses a rules-to-network algorithm to construct an AND-OR graph representation of the initial domain knowledge and translates this graph to an appropriate neural network topology.
Reference: [26] <author> G. Towell and J. Shavlik, </author> <title> "Knowledge-based artificial neural networks," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 70, no. </volume> <pages> 1-2, pp. 119-165, </pages> <year> 1994. </year>
Reference-contexts: Examples of such systems which use first order predicate logic for knowledge representation include FOCL [24] and FORTE [25]. Towell and Shavlik proposed the KBANN (knowledge based artificial neural network) learning algorithm for connectionist theory refinement [12], <ref> [26] </ref>. KBANN uses a rules-to-network algorithm to construct an AND-OR graph representation of the initial domain knowledge and translates this graph to an appropriate neural network topology. KBANN then uses the standard backpropagation learning algorithm [4] to refine the domain knowledge. <p> The approaches described by Fu [27] and Katz [28] are similar to the KBANN algorithm. The KBANN learning algorithm is demonstrated to perform better than several other machine learning algorithms on domains such as promoter and splice-junction datasets [12], <ref> [26] </ref>. However, KBANN is limited by the fact that it does not modify the network topology. The TopGen [29] and REGENT [18] algorithms were designed to add new neurons to the KBANN network thereby extending the realm of network topologies considered by KBANN.
Reference: [27] <author> L. M. Fu, </author> <title> "Integration of neural heuristics into knowledge-based inference," </title> <journal> Connection Science, </journal> <volume> vol. 1, </volume> <pages> pp. 325-340, </pages> <year> 1989. </year>
Reference-contexts: KBANN uses a rules-to-network algorithm to construct an AND-OR graph representation of the initial domain knowledge and translates this graph to an appropriate neural network topology. KBANN then uses the standard backpropagation learning algorithm [4] to refine the domain knowledge. The approaches described by Fu <ref> [27] </ref> and Katz [28] are similar to the KBANN algorithm. The KBANN learning algorithm is demonstrated to perform better than several other machine learning algorithms on domains such as promoter and splice-junction datasets [12], [26]. However, KBANN is limited by the fact that it does not modify the network topology.
Reference: [28] <author> B. F. Katz, "Ebl and sbl: </author> <title> A neural network synthesis," </title> <booktitle> in Proceedings of the Eleventh Annual Conference of the Cognitive Science Society, </booktitle> <year> 1989, </year> <pages> pp. 683-689. </pages>
Reference-contexts: KBANN uses a rules-to-network algorithm to construct an AND-OR graph representation of the initial domain knowledge and translates this graph to an appropriate neural network topology. KBANN then uses the standard backpropagation learning algorithm [4] to refine the domain knowledge. The approaches described by Fu [27] and Katz <ref> [28] </ref> are similar to the KBANN algorithm. The KBANN learning algorithm is demonstrated to perform better than several other machine learning algorithms on domains such as promoter and splice-junction datasets [12], [26]. However, KBANN is limited by the fact that it does not modify the network topology.
Reference: [29] <author> D. W. Opitz and J. W. Shavlik, </author> <title> "Dynamically adding symbolically meaningful nodes to knowledge-based neural networks," </title> <journal> Knowledge-Based Systems, </journal> <volume> vol. 8, no. 6, </volume> <pages> pp. 301-311, </pages> <year> 1995. </year>
Reference-contexts: The KBANN learning algorithm is demonstrated to perform better than several other machine learning algorithms on domains such as promoter and splice-junction datasets [12], [26]. However, KBANN is limited by the fact that it does not modify the network topology. The TopGen <ref> [29] </ref> and REGENT [18] algorithms were designed to add new neurons to the KBANN network thereby extending the realm of network topologies considered by KBANN. <p> TopGen and REGENT reportedly outperform KBANN in terms of generalization performance on several problems [18], <ref> [29] </ref>. The generalization performance of Tiling-Pyramid compares very favorably with that of TopGen and REGENT on both the ribosome binding site and promoter datasets. The hybrid Tiling-Pyramid algorithm generates significantly smaller networks as compared to both TopGen and REGENT.
Reference: [30] <author> E. Baum and K. Lang, </author> <title> "Constructing hidden units using examples and queries," </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <volume> vol. 3, </volume> <editor> R. Lippmann, J. Moody, and D. Touretzky, Eds., </editor> <address> San Mateo, CA, </address> <year> 1991, </year> <pages> pp. 904-910, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Fletcher and Obradovic's algorithm starts with an initial network representing the domain theory and modifies this theory by training a single hidden layer of threshold neurons using the labeled training data [11]. It uses the hyperplane detection from examples (HDE) algorithm <ref> [30] </ref> to construct the hidden layer. Each hidden neuron corresponds to a hyperplane. Fletcher and Obradovic's algorithm maps these hyperplanes to a set of threshold neurons and then then trains the output neuron using the pocket algorithm [4]. Our approach is similar to the one taken by Fletcher and Obradovic.
Reference: [31] <author> G. Towell and J. Shavlik, </author> <title> "Extracting rules from knowledge-based neural networks," </title> <journal> Machine Learning, </journal> <volume> vol. 13, </volume> <pages> pp. 71-101, </pages> <year> 1993. </year>
Reference-contexts: Extraction of rules from the trained neural networks is of significant interest in datamining and knowledge discovery applications <ref> [31] </ref>, [32], [33]. We have not yet explored rule extraction from Tiling-Pyramid networks. We conjecture that such networks lend themselves well to knowledge extraction. In particular, the use threshold neurons permits more straightforward rule extraction than in the case of sigmoid neurons that are typically used in backpropagation type algorithms.
Reference: [32] <author> L. M. Fu, </author> <title> "Knowledge based connectionism for refining domain theories," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 23, no. 1, </volume> <year> 1993. </year>
Reference-contexts: Extraction of rules from the trained neural networks is of significant interest in datamining and knowledge discovery applications [31], <ref> [32] </ref>, [33]. We have not yet explored rule extraction from Tiling-Pyramid networks. We conjecture that such networks lend themselves well to knowledge extraction. In particular, the use threshold neurons permits more straightforward rule extraction than in the case of sigmoid neurons that are typically used in backpropagation type algorithms.
Reference: [33] <author> M. Craven, </author> <title> Extracting Comprehensible Models from Trained Neural Networks, </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, University of Wisconsin, Madison, WI, </institution> <year> 1996. </year>
Reference-contexts: Extraction of rules from the trained neural networks is of significant interest in datamining and knowledge discovery applications [31], [32], <ref> [33] </ref>. We have not yet explored rule extraction from Tiling-Pyramid networks. We conjecture that such networks lend themselves well to knowledge extraction. In particular, the use threshold neurons permits more straightforward rule extraction than in the case of sigmoid neurons that are typically used in backpropagation type algorithms.
References-found: 33

