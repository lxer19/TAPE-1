URL: ftp://ftp.cse.ucsc.edu/pub/rna/cpm94.ps.Z
Refering-URL: http://www.cse.ucsc.edu/research/compbio/scfg.html
Root-URL: http://www.cse.ucsc.edu
Email: Email: haussler@cse.ucsc.edu  
Title: Recent Methods for RNA Modeling Using Stochastic Context-Free Grammars  
Author: Yasubumi Sakakibara Michael Brown Richard Hughey I. Saira Mian Kimmen Sjolander Rebecca C. Underwood David Haussler 
Address: Santa Cruz, CA 95064, USA  
Affiliation: 1 Computer and Information Sciences 2 Computer Engineering 3 Sinsheimer Laboratories University of California,  
Abstract: Stochastic context-free grammars (SCFGs) can be applied to the problems of folding, aligning and modeling families of homologous RNA sequences. SCFGs capture the sequences' common primary and secondary structure and generalize the hidden Markov models (HMMs) used in related work on protein and DNA. This paper discusses our new algorithm, Tree-Grammar EM, for deducing SCFG parameters automatically from unaligned, unfolded training sequences. Tree-Grammar EM, a generalization of the HMM forward-backward algorithm, is based on tree grammars and is faster than the previously proposed inside-outside SCFG training algorithm. Independently, Sean Eddy and Richard Durbin have introduced a trainable "covariance model" (CM) to perform similar tasks. We compare and contrast our methods with theirs. Tools for analyzing RNA will become increasingly important as in vitro evolution and selection techniques produce greater numbers of synthesized RNA families to supplement those related by phylogeny. Recent efforts have applied stochastic context-free grammars (SCFGs) to the problems of statistical modeling, multiple alignment, discrimination and prediction of the secondary structure of RNA families. Our approach in applying SCFGs to modeling RNA is highly related to our work on modeling protein families and domains with HMMs [HKMS93, KBM + 94]. In RNA, the nucleotides adenine (A), cytosine (C), guanine (G) and uracil (U) interact to form characteristic secondary-structure motifs such as helices, loops and bulges [Sae84, WPT89]. Intramolecular AU and G-C Watson-Crick pairs as well as G-U and, more rarely, G-A base pairs constitute the so-called biological palindromes in the genome. When RNA sequences are aligned, both primary ? Y. Sakakibara's current address is ISIS, Fujitsu Labs Ltd., 140, Miyamoto, Numazu, Shizuoka 410-03, Japan. We thank Anders Krogh, Harry Noller and Bryn Weiser for discussions and assistance, and Michael Waterman and David Searls for discussions. This work was supported by NSF grants CDA-9115268 and IRI-9123692 and NIH grant number GM17129. This material is based upon work supported under a National Science Foundation Graduate Research Fellowship. 
Abstract-found: 1
Intro-found: 1
Reference: [AU72] <author> A. V. Aho and J. D. Ullman. </author> <title> The Theory of Parsing, Translation and Compiling, Vol. I: Parsing. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1972. </year>
Reference-contexts: Efficiently computing Prob (s j G) presents a problem because the number of possible parse trees for s is exponential in the length of the sequence. However, a dynamic programming technique analogous to the Cocke-Younger-Kasami (CYK) or Early parsing methods <ref> [AU72] </ref> for non-stochastic CFGs can complete this task in polynomial time (proportional to the cube of the length of sequence s).
Reference: [Bak79] <author> J. K. Baker. </author> <title> Trainable grammars for speech recognition. </title> <booktitle> Speech Commu--nication Papers for the 97th Meeting of the Acoustical Society of America, </booktitle> <pages> pages 547-550, </pages> <year> 1979. </year>
Reference-contexts: It is a novel generalization of the forward-backward algorithm commonly used to train HMMs. Our algorithm, Tree-Grammar EM, is based on tree grammars [TW68] and is more efficient than the inside-outside algorithm [LY90], a computationally expensive generalization of the forward-backward algorithm developed to train SCFGs <ref> [Bak79] </ref>. Full details are described elsewhere [SBH + 93]; here we present a summary. 1 Stochastic Context-Free Grammar Methods Specifying a probability for each production in a grammar yields a stochastic grammar. A stochastic grammar assigns a probability to each string it derives. <p> A stochastic grammar assigns a probability to each string it derives. Stochastic regular grammars are equivalent to HMMs and suggest an interesting generalization from HMMs to SCFGs <ref> [Bak79] </ref>. <p> Use grammar G new and the CYK-like parsing algorithm to parse the input sequences, producing a new set of trees T new . g Fig. 4. Pseudocode for the Tree-Grammar EM training algorithm. The inside-outside algorithm <ref> [LY90, Bak79] </ref> is an expectation maximization (EM) algorithm that calculates maximum likelihood estimates of an SCFG's parameters based on training data. However, it requires the grammar to be in Chomsky normal form, which is possible but inconvenient for modeling RNA (and requires more nonterminals). <p> This consists of iterating the following two-step procedure until the model parameters stabilize: First, they align each sequence to the model using the same alignment algorithm that we use, a Viterbi approximation to the inside-outside algorithm <ref> [LY90, Bak79] </ref>. Second, they apply the Viterbi approximation of EM to maximize a Bayesian posterior-probability estimate [ED94]. In this way, training consists of optimizing the alignment scores of the input training sequences. Eddy and Durbin use a variant of their alignment algorithm to perform database searches as well. 1.
Reference: [BHJ + 91] <author> J. W. Brown, E. S. Haas, B. D. James, D. A. Hunt, J. S. Liu, and N. R. </author> <title> Pace. Phylogenetic analysis and evolution of RNase P RNA in proteobac-teria. </title> <journal> Journal of Bacteriology, </journal> <volume> 173 </volume> <pages> 3855-3863, </pages> <year> 1991. </year>
Reference-contexts: They may prove useful in maintaining, updating and revising existing multiple sequence alignments. In addition, a grammar itself may be a valuable tool for representing an RNA family or domain such as group I introns [MW90, MECS90], group II introns [MUO89], RNAse P RNA <ref> [BHJ + 91, TE93] </ref>, small nuclear RNAs [GP88] and 7S RNA (signal recognition particle RNA) [Zwi89]. The main difficulties in applying this work to other families of RNA will be the development of appropriate initial grammars and the computational cost of parsing longer sequences.
Reference: [BHK + 93a] <author> M. P. Brown, R. Hughey, A. Krogh, I. S. Mian, K. Sjolander, and D. Haussler. </author> <title> Dirichlet mixture priors for HMMs. </title> <note> In preparation, </note> <year> 1993. </year>
Reference-contexts: It appears that our basic grammar training algorithm, which is quite different from theirs, may be somewhat faster. Further, our custom-designed grammars and greater emphasis on learned, as opposed to constructed, Bayesian prior probability densities <ref> [BHK + 93a] </ref> may allow us to train with fewer training sequences.
Reference: [BHK + 93b] <author> M. P. Brown, R. Hughey, A. Krogh, I. S. Mian, K. Sjolander, and D. Haussler. </author> <title> Using Dirichlet mixture priors to derive hidden Markov models for protein families. </title> <editor> In L. Hunter, D. Searls, and J. Shavlik, editors, </editor> <booktitle> Proc. of First Int. Conf. on Intelligent Systems for Molecular Biology, </booktitle> <pages> pages 47-55, </pages> <address> Menlo Park, CA, July 1993. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: One solution is to use regularization to control the effective number of free parameters. We regularize our grammars by taking a Bayesian approach to the parameter estimation problem, similar to our approach with protein HMMs <ref> [KBM + 94, BHK + 93b] </ref>. Before training the grammars, we construct a prior probability density for each of their "important" parameter sets. This prior density takes the form of a Dirichlet distribution [SD89]. <p> We similarly use the alignment to calculate a four-parameter Dirichlet prior for nucleotide distributions in loop region positions. (We present further details elsewhere <ref> [BHK + 93b] </ref>.) These parameters constitute our regularizer. We add them as "pseudocounts" during each re-estimation step of Tree-Grammar EM (step 3b in Figure 4). Thus, at each iteration, TG Reestimator computes mean posterior estimates of the model parameters rather than maximum likelihood estimates.
Reference: [ED94] <author> S. R. Eddy and R. </author> <title> Durbin. RNA sequence analysis using covariance models. </title> <journal> Submitted to Nucleic Acids Research, </journal> <year> 1994. </year>
Reference-contexts: In this work, we use formal language theory to describe a means to generalize HMMs to model most RNA interactions. We compare our method to Eddy and Durbin's use of "covariance models" (CMs) to model RNA <ref> [ED94] </ref>. CMs are equivalent to SCFGs, but Eddy and Durbin employ different algorithms for training and producing multiple alignments. As in the elegant work of Searls [Sea92], we view the character strings representing DNA, RNA and protein as sentences derived from a formal grammar. <p> Results using this approach were reported in our previous work [SBU + 93]. Also, Eddy and Durbin report recent results in which nearly all aspects of the grammar are determined solely from the training sequences <ref> [ED94] </ref>. <p> As discussed in their work <ref> [ED94] </ref>, covariance models (CMs) are describable as SCFGs. <p> Second, they apply the Viterbi approximation of EM to maximize a Bayesian posterior-probability estimate <ref> [ED94] </ref>. In this way, training consists of optimizing the alignment scores of the input training sequences. Eddy and Durbin use a variant of their alignment algorithm to perform database searches as well. 1. Start with an initial (possibly random) alignment denoted A 0 . <p> It is currently not clear which approach will be best. The former problem might be solved by the development of effective methods for learning the grammar itself from training sequences. The work of Eddy and Durbin is an important step in this direction <ref> [ED94] </ref>. Their method relies on correlations between columns in a multiple alignment [GPH + 92, Lap92, KB93, Wat89, WOW + 90, San85, Wat88] to discover the es sential base-pairing structure in an RNA family.
Reference: [ER91] <author> J. Engelfriet and G. Rozenberg. </author> <title> Graph grammars based on node rewriting: An introduction to NLC graph grammars. </title> <editor> In E. Ehrig, H.J. Kreowski, and G. Rozenberg, editors, </editor> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> volume 532, </volume> <pages> pages 12-23. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Modeling pseudoknots and higher-order structure would require still more general methods. One possibility would be to consider stochastic graph grammars (see the introductory survey by Engelfriet and Rozenberg <ref> [ER91] </ref>) in hopes of obtaining a more general model of the interactions present in the molecule beyond the primary structure.
Reference: [Fu82] <author> K. S. Fu. </author> <title> Syntactic pattern recognition and applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1982. </year>
Reference-contexts: In this way, TG Reestimator can be used even when precise biological knowledge of the base pairing is not available. TG Reestimator constitutes one part of the entire training procedure, Tree-Grammar EM. The Tree-Grammar EM procedure is based on the theory of stochastic tree grammars <ref> [TW68, Fu82] </ref>. Tree grammars are used to derive labeled trees instead of strings. Labeled trees can be used to represent the secondary structure of RNA easily [SZ90] (see Figure 2). A tree grammar for RNA denotes both the primary sequence and the secondary structure of each molecule. <p> This progression is similar to the path taken by the late King Sun Fu and colleagues in their development of the field of syntactic pattern recognition <ref> [Fu82] </ref>. Modeling pseudoknots and higher-order structure would require still more general methods. One possibility would be to consider stochastic graph grammars (see the introductory survey by Engelfriet and Rozenberg [ER91]) in hopes of obtaining a more general model of the interactions present in the molecule beyond the primary structure.
Reference: [FW75] <author> G. E. Fox and C. R Woese. </author> <title> 5S RNA secondary structure. </title> <journal> Nature, </journal> <volume> 256 </volume> <pages> 505-507, </pages> <year> 1975. </year>
Reference-contexts: Two principal methods have been established for predicting RNA secondary structure (i.e., which nucleotides are base-paired). The first technique, phylo-genetic analysis of homologous RNA molecules <ref> [FW75, WGGN83] </ref>, ascertains structural features that are conserved during evolution. The second technique employs thermodynamics to compare the free energy changes predicted for formation of possible secondary structure and relies on finding the structure with the lowest free energy [TUL71, TSF88, Gou87].
Reference: [Gou87] <author> M. Gouy. </author> <title> Secondary structure prediction of RNA. </title> <editor> In M. J. Bishop and C. R. Rawlings, editors, </editor> <title> Nucleic acid and protein sequence analysis, </title> <booktitle> a practical approach, </booktitle> <pages> pages 259-284. </pages> <publisher> IRL Press, Oxford, </publisher> <address> England, </address> <year> 1987. </year>
Reference-contexts: The first technique, phylo-genetic analysis of homologous RNA molecules [FW75, WGGN83], ascertains structural features that are conserved during evolution. The second technique employs thermodynamics to compare the free energy changes predicted for formation of possible secondary structure and relies on finding the structure with the lowest free energy <ref> [TUL71, TSF88, Gou87] </ref>. Though in principle HMMs could also be used to model RNA, the standard HMM approach treats all positions as having independent distributions and is unable to model the interactions between positions.
Reference: [GP88] <author> C. Guthrie and B. Patterson. Spliceosomal snRNAs. </author> <booktitle> Annual Review of Genetics, </booktitle> <volume> 22 </volume> <pages> 387-419, </pages> <year> 1988. </year>
Reference-contexts: In addition, a grammar itself may be a valuable tool for representing an RNA family or domain such as group I introns [MW90, MECS90], group II introns [MUO89], RNAse P RNA [BHJ + 91, TE93], small nuclear RNAs <ref> [GP88] </ref> and 7S RNA (signal recognition particle RNA) [Zwi89]. The main difficulties in applying this work to other families of RNA will be the development of appropriate initial grammars and the computational cost of parsing longer sequences.
Reference: [GPH + 92] <author> R. R. Gutell, A. Power, G. Z. Hertz, E. J. Putz, and G. D. Stormo. </author> <title> Identifying constraints on the higher-order structure of RNA: continued development and application of comparative sequence analysis methods. </title> <journal> Nucleic Acids Research, </journal> <volume> 20 </volume> <pages> 5785-5795, </pages> <year> 1992. </year>
Reference-contexts: structure (essentially, to choose an SCFG's productions), they use the standard Nussinov/Zuker dynamic-programming algorithm for RNA folding [NPGK78, Zuk89], with the difference that the cost function being optimized is a function of the "mutual information" of two columns in the input multiple alignment (based on Gutell et al.'s MIXY procedure <ref> [GPH + 92] </ref>), rather than of the number of base pairs or the thermodynamic stacking energy. Once a model structure exists, they train the model's parameters (production probabilities) using the Viterbi approximation of EM. <p> The former problem might be solved by the development of effective methods for learning the grammar itself from training sequences. The work of Eddy and Durbin is an important step in this direction [ED94]. Their method relies on correlations between columns in a multiple alignment <ref> [GPH + 92, Lap92, KB93, Wat89, WOW + 90, San85, Wat88] </ref> to discover the es sential base-pairing structure in an RNA family.
Reference: [HKMS93] <author> D. Haussler, A. Krogh, I. S. Mian, and K. Sjolander. </author> <title> Protein modeling using hidden Markov models: Analysis of globins. </title> <booktitle> In Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <volume> volume 1, </volume> <pages> pages 792-802, </pages> <address> Los Alamitos, CA, 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Recent efforts have applied stochastic context-free grammars (SCFGs) to the problems of statistical modeling, multiple alignment, discrimination and prediction of the secondary structure of RNA families. Our approach in applying SCFGs to modeling RNA is highly related to our work on modeling protein families and domains with HMMs <ref> [HKMS93, KBM + 94] </ref>. In RNA, the nucleotides adenine (A), cytosine (C), guanine (G) and uracil (U) interact to form characteristic secondary-structure motifs such as helices, loops and bulges [Sae84, WPT89]. <p> SCFGs are a generalization of HMMs. The three main types of nontermi--nals in an SCFG correspond to each of the primary states in an HMM: match, insert and skip <ref> [HKMS93, KBM + 94] </ref>. The match nonterminals in a grammar correspond to important structural positions in an RNA molecule. Insert non-terminals also generate nucleotides but with different distributions. These are used to model loops by inserting nucleotides between important (match) positions.
Reference: [KB93] <author> T. Klinger and D. Brutlag. </author> <title> Detection of correlations in tRNA sequences with structural implications. </title> <editor> In Lawrence Hunter, David Searls, and Jude Shavlik, editors, </editor> <booktitle> First International Conference on Intelligent Systems for Molecular Biology, </booktitle> <address> Menlo Park, 1993. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The former problem might be solved by the development of effective methods for learning the grammar itself from training sequences. The work of Eddy and Durbin is an important step in this direction [ED94]. Their method relies on correlations between columns in a multiple alignment <ref> [GPH + 92, Lap92, KB93, Wat89, WOW + 90, San85, Wat88] </ref> to discover the es sential base-pairing structure in an RNA family.
Reference: [KBM + 94] <author> A. Krogh, M. Brown, I. S. Mian, K. Sjolander, and D. Haussler. </author> <title> Hidden Markov models in computational biology: Applications to protein modeling. </title> <journal> Journal of Molecular Biology, </journal> <volume> 235 </volume> <pages> 1501-1531, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: Recent efforts have applied stochastic context-free grammars (SCFGs) to the problems of statistical modeling, multiple alignment, discrimination and prediction of the secondary structure of RNA families. Our approach in applying SCFGs to modeling RNA is highly related to our work on modeling protein families and domains with HMMs <ref> [HKMS93, KBM + 94] </ref>. In RNA, the nucleotides adenine (A), cytosine (C), guanine (G) and uracil (U) interact to form characteristic secondary-structure motifs such as helices, loops and bulges [Sae84, WPT89]. <p> A stochastic grammar assigns a probability to each string it derives. Stochastic regular grammars are equivalent to HMMs and suggest an interesting generalization from HMMs to SCFGs [Bak79]. In this work, we explore stochastic models for tRNA sequences using a stochastic context-free grammar that is similar to our HMMs <ref> [KBM + 94] </ref> but incorporates base-pairing information. 1.1 Context-free grammars for RNA A grammar is principally a set of productions (rewrite rules) that is used to generate a set of strings, a language. The productions are applied iteratively to generate a string in a process called derivation. <p> SCFGs are a generalization of HMMs. The three main types of nontermi--nals in an SCFG correspond to each of the primary states in an HMM: match, insert and skip <ref> [HKMS93, KBM + 94] </ref>. The match nonterminals in a grammar correspond to important structural positions in an RNA molecule. Insert non-terminals also generate nucleotides but with different distributions. These are used to model loops by inserting nucleotides between important (match) positions. <p> One solution is to use regularization to control the effective number of free parameters. We regularize our grammars by taking a Bayesian approach to the parameter estimation problem, similar to our approach with protein HMMs <ref> [KBM + 94, BHK + 93b] </ref>. Before training the grammars, we construct a prior probability density for each of their "important" parameter sets. This prior density takes the form of a Dirichlet distribution [SD89]. <p> We regularize loops with very large uniform pseudo-counts over the four possible nucleotides so that their probability distributions will be fixed at uniform values rather than estimated from the training data. This is equivalent to the regularization we used for the insert states of HMMs <ref> [KBM + 94] </ref>. <p> This further reduces the number of parameters to be estimated, helping to avoid overfitting. 1.6 The initial grammar In the initial grammar, we model a loop that is typically l nucleotides in length by an HMM model with l match states as described in our previous protein work <ref> [KBM + 94] </ref>, except that the four-letter nucleic-acids alphabet replaces the twenty-letter amino-acids alphabet. Nucleotide distributions in such a loop are defined by probabilities of l match-nonterminals' productions. Longer or shorter loops can be derived using special nonterminals and productions that allow position-specific insertions and deletions.
Reference: [Lap92] <author> Allan Lapedes. </author> <title> Private communication, </title> <year> 1992. </year>
Reference-contexts: The former problem might be solved by the development of effective methods for learning the grammar itself from training sequences. The work of Eddy and Durbin is an important step in this direction [ED94]. Their method relies on correlations between columns in a multiple alignment <ref> [GPH + 92, Lap92, KB93, Wat89, WOW + 90, San85, Wat88] </ref> to discover the es sential base-pairing structure in an RNA family.
Reference: [LOM + 93] <author> N. Larsen, G. J. Olsen, B. L. Maidak, M. J. McCaughey, R. Overbeek, T. J. Macke, T. L. Marsh, and C. R. Woese. </author> <title> The ribosomal database project. </title> <journal> Nucleic Acids Research, </journal> <volume> 21 </volume> <pages> 3021-3023, </pages> <year> 1993. </year>
Reference-contexts: For instance, Watson-Crick pairs are more frequently observed than other base pairs. To calculate precise prior information about base-pair probabilities, we obtain the 16 parameters of a Dirichlet density over possible base-paired position distributions from a large alignment of 16S rRNA sequences <ref> [LOM + 93] </ref>. We similarly use the alignment to calculate a four-parameter Dirichlet prior for nucleotide distributions in loop region positions. (We present further details elsewhere [BHK + 93b].) These parameters constitute our regularizer.
Reference: [LS94] <author> R. H. Lathrop and T. F. Smith. </author> <title> A branch-and-bound algorithm for optimal protein threading with pairwise (contact potential) amino acid in teractions. </title> <booktitle> In Proceedings of the 27th Hawaii International Conference on System Sciences, </booktitle> <address> Los Alamitos, CA, 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The main difficulties in applying this work to other families of RNA will be the development of appropriate initial grammars and the computational cost of parsing longer sequences. The latter problem can only be solved by the development of fundamentally different parsing methods, perhaps relying more on branch-and-bound methods <ref> [LS94] </ref> or heuristics. It is currently not clear which approach will be best. The former problem might be solved by the development of effective methods for learning the grammar itself from training sequences. The work of Eddy and Durbin is an important step in this direction [ED94].
Reference: [LY90] <author> K. Lari and S. J. Young. </author> <title> The estimation of stochastic context-free grammars using the inside-outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4 </volume> <pages> 35-56, </pages> <year> 1990. </year>
Reference-contexts: We have designed an algorithm that deduces grammar parameters automatically from a set of unaligned primary sequences. It is a novel generalization of the forward-backward algorithm commonly used to train HMMs. Our algorithm, Tree-Grammar EM, is based on tree grammars [TW68] and is more efficient than the inside-outside algorithm <ref> [LY90] </ref>, a computationally expensive generalization of the forward-backward algorithm developed to train SCFGs [Bak79]. Full details are described elsewhere [SBH + 93]; here we present a summary. 1 Stochastic Context-Free Grammar Methods Specifying a probability for each production in a grammar yields a stochastic grammar. <p> Use grammar G new and the CYK-like parsing algorithm to parse the input sequences, producing a new set of trees T new . g Fig. 4. Pseudocode for the Tree-Grammar EM training algorithm. The inside-outside algorithm <ref> [LY90, Bak79] </ref> is an expectation maximization (EM) algorithm that calculates maximum likelihood estimates of an SCFG's parameters based on training data. However, it requires the grammar to be in Chomsky normal form, which is possible but inconvenient for modeling RNA (and requires more nonterminals). <p> This consists of iterating the following two-step procedure until the model parameters stabilize: First, they align each sequence to the model using the same alignment algorithm that we use, a Viterbi approximation to the inside-outside algorithm <ref> [LY90, Bak79] </ref>. Second, they apply the Viterbi approximation of EM to maximize a Bayesian posterior-probability estimate [ED94]. In this way, training consists of optimizing the alignment scores of the input training sequences. Eddy and Durbin use a variant of their alignment algorithm to perform database searches as well. 1.
Reference: [MECS90] <author> F. Michel, A. D. Ellington, S. Couture, and J. W. Szostak. </author> <title> Phylogenetic and genetic evidence for base-triples in the catalytic domain of group I introns. </title> <journal> Nature, </journal> <volume> 347 </volume> <pages> 578-580, </pages> <year> 1990. </year>
Reference-contexts: They may prove useful in maintaining, updating and revising existing multiple sequence alignments. In addition, a grammar itself may be a valuable tool for representing an RNA family or domain such as group I introns <ref> [MW90, MECS90] </ref>, group II introns [MUO89], RNAse P RNA [BHJ + 91, TE93], small nuclear RNAs [GP88] and 7S RNA (signal recognition particle RNA) [Zwi89].
Reference: [MUO89] <author> F. Michel, K. Umesono, and H. Ozeki. </author> <title> Comparative and functional anatomy of group II catalytic introns-a review. </title> <journal> Gene, </journal> <volume> 82 </volume> <pages> 5-30, </pages> <year> 1989. </year>
Reference-contexts: They may prove useful in maintaining, updating and revising existing multiple sequence alignments. In addition, a grammar itself may be a valuable tool for representing an RNA family or domain such as group I introns [MW90, MECS90], group II introns <ref> [MUO89] </ref>, RNAse P RNA [BHJ + 91, TE93], small nuclear RNAs [GP88] and 7S RNA (signal recognition particle RNA) [Zwi89]. The main difficulties in applying this work to other families of RNA will be the development of appropriate initial grammars and the computational cost of parsing longer sequences.
Reference: [MW90] <author> F. Michel and E. Westhof. </author> <title> Modelling of the three-dimensional architecture of group I catalytic introns based on comparative sequence analysis. </title> <journal> Journal of Molecular Biology, </journal> <volume> 216 </volume> <pages> 585-610, </pages> <year> 1990. </year>
Reference-contexts: They may prove useful in maintaining, updating and revising existing multiple sequence alignments. In addition, a grammar itself may be a valuable tool for representing an RNA family or domain such as group I introns <ref> [MW90, MECS90] </ref>, group II introns [MUO89], RNAse P RNA [BHJ + 91, TE93], small nuclear RNAs [GP88] and 7S RNA (signal recognition particle RNA) [Zwi89].
Reference: [NPGK78] <author> R. Nussinov, G. Pieczenik, J. R. Griggs, and D. J. Kleitman. </author> <title> Algorithms for loop matchings. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 35 </volume> <pages> 68-82, </pages> <year> 1978. </year>
Reference-contexts: As discussed in their work [ED94], covariance models (CMs) are describable as SCFGs. To deduce a covariance model's structure (essentially, to choose an SCFG's productions), they use the standard Nussinov/Zuker dynamic-programming algorithm for RNA folding <ref> [NPGK78, Zuk89] </ref>, with the difference that the cost function being optimized is a function of the "mutual information" of two columns in the input multiple alignment (based on Gutell et al.'s MIXY procedure [GPH + 92]), rather than of the number of base pairs or the thermodynamic stacking energy.
Reference: [Rab89] <author> L. R. Rabiner. </author> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <journal> Proc IEEE, </journal> <volume> 77(2) </volume> <pages> 257-286, </pages> <year> 1989. </year>
Reference-contexts: To obtain the most likely parse tree for the sequence s, we calculate max parse trees d Prob (S 0 d The dynamic-programming procedure to do this resembles the Viterbi algorithm for HMMs <ref> [Rab89] </ref>.
Reference: [Sae84] <author> W. Saenger. </author> <title> Principles of nucleic acid structure. Springer Advanced Texts in Chemistry. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: In RNA, the nucleotides adenine (A), cytosine (C), guanine (G) and uracil (U) interact to form characteristic secondary-structure motifs such as helices, loops and bulges <ref> [Sae84, WPT89] </ref>. Intramolecular AU and G-C Watson-Crick pairs as well as G-U and, more rarely, G-A base pairs constitute the so-called biological palindromes in the genome. When RNA sequences are aligned, both primary ? Y. Sakakibara's current address is ISIS, Fujitsu Labs Ltd., 140, Miyamoto, Numazu, Shizuoka 410-03, Japan.
Reference: [Sak92] <author> Y. Sakakibara. </author> <title> Efficient learning of context-free grammars from positive structural examples. </title> <journal> Information and Computation, </journal> <volume> 97 </volume> <pages> 23-60, </pages> <year> 1992. </year>
Reference-contexts: The root, 1 $, and the internal node, 3 $, represent AU and G-C base pairs, respectively. To avoid unnecessary complexity, we describe this new algorithm in terms of CFGs instead of tree grammars <ref> [TW68, Sak92] </ref>. A tree is a rooted, directed, connected acyclic finite graph in which the direct successors of any node are linearly ordered from left to right. The predecessor of a node is called the parent ; the successor, a child; and a child of the parent, a sibling.
Reference: [San85] <author> D. Sankoff. </author> <title> Simultaneous solution of the RNA folding, alignment and pro-tosequence problems. </title> <journal> SIAM J. Appl. Math., </journal> <volume> 45 </volume> <pages> 810-825, </pages> <year> 1985. </year>
Reference-contexts: To enhance the alignment of some sequences with respect to others, spaces may need to be inserted in them.) Elucidation of common folding patterns among multiple sequences may indicate the pertinent regions to be aligned and vice versa <ref> [San85] </ref>. Two principal methods have been established for predicting RNA secondary structure (i.e., which nucleotides are base-paired). The first technique, phylo-genetic analysis of homologous RNA molecules [FW75, WGGN83], ascertains structural features that are conserved during evolution. <p> The former problem might be solved by the development of effective methods for learning the grammar itself from training sequences. The work of Eddy and Durbin is an important step in this direction [ED94]. Their method relies on correlations between columns in a multiple alignment <ref> [GPH + 92, Lap92, KB93, Wat89, WOW + 90, San85, Wat88] </ref> to discover the es sential base-pairing structure in an RNA family.
Reference: [SBH + 93] <author> Y. Sakakibara, M. Brown, R. Hughey, I. S. Mian, K. Sjolander, R. Underwood, and D. Haussler. </author> <title> The application of stochastic context-free grammars to folding, aligning and modeling homologous RNA sequences. </title> <note> Submitted for publication, </note> <year> 1993. </year>
Reference-contexts: Our algorithm, Tree-Grammar EM, is based on tree grammars [TW68] and is more efficient than the inside-outside algorithm [LY90], a computationally expensive generalization of the forward-backward algorithm developed to train SCFGs [Bak79]. Full details are described elsewhere <ref> [SBH + 93] </ref>; here we present a summary. 1 Stochastic Context-Free Grammar Methods Specifying a probability for each production in a grammar yields a stochastic grammar. A stochastic grammar assigns a probability to each string it derives. <p> Use model C new and the Viterbi inside-outside aligning algorithm to produce a new multiple alignment A new . Set A curr = A new . g Fig. 6. Pseudocode for the Eddy and Durbin training algorithm. 2 Discussion In a recent paper <ref> [SBH + 93] </ref>, we present our results in detail and compare these with the results of Sean Eddy and Richard Durbin's covariance models. <p> The results show that all our grammars, except one trained on zero sequences, can perfectly discriminate nonmitochondrial tRNA sequences from non-tRNA sequences, and that our multiple alignments are nearly identical to the published tRNA alignments (full details are given elsewhere <ref> [SBH + 93] </ref>). Similarly, Eddy and Durbin's covariance models performed very well in database searches and produced multiple alignments nearly identical to the published. Both methods achieve local optima, rather than global, but the resulting models seem adequate for tRNA.
Reference: [SBM + 94] <author> Y. Sakakibara, M. Brown, I. S. Mian, R. Underwood, and D. Haussler. </author> <title> Stochastic context-free grammars for modeling RNA. </title> <booktitle> In Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <address> Los Alamitos, CA, 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Durbin and Eddy have done the latter modifications in their tRNA experiments and report good results in searching the GenBank structural RNA database and 2.2 Mb of C. elegans ge-nomic sequence for tRNAs, even without using special intron models. In our earlier work <ref> [SBM + 94] </ref>, we reported some very preliminary results on modifying tRNA grammars to accommodate introns. We are currently planning to do further work in this direction.
Reference: [SBU + 93] <author> Y. Sakakibara, M. Brown, R. Underwood, I. S. Mian, and D. Haussler. </author> <title> Stochastic context-free grammars for modeling RNA. </title> <type> Technical Report UCSC-CRL-93-16, </type> <institution> UC Santa Cruz, Computer and Information Sciences Dept., </institution> <address> Santa Cruz, CA 95064, </address> <year> 1993. </year>
Reference-contexts: Results using this approach were reported in our previous work <ref> [SBU + 93] </ref>. Also, Eddy and Durbin report recent results in which nearly all aspects of the grammar are determined solely from the training sequences [ED94]. <p> The TG Reestimator algorithm iteratively finds the best parse for each molecule in the training set and then readjusts the production probabilities to maximize the probability of these parses. The new algorithm also tends to converge faster because each training example is much more informative <ref> [SBU + 93] </ref>. Fig. 5. The folded RNA sequence (AA (GUC)U) can be represented as a tree t (left), which can be broken into two parts such as t=3 (middle) and tn3 (right). The root, 1 $, and the internal node, 3 $, represent AU and G-C base pairs, respectively.
Reference: [SD89] <author> T. J. Santner and D. E. Duffy. </author> <title> The Statistical Analysis of Discrete Data. </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Before training the grammars, we construct a prior probability density for each of their "important" parameter sets. This prior density takes the form of a Dirichlet distribution <ref> [SD89] </ref>. The "important" productions are of two forms: S ! aSb and S ! aS, where terminal symbols a; b 2 fA; C; G; Ug. S ! aSb productions, which generate base pairs, come in groups of 16, corresponding to all possible pairs of terminal symbols.
Reference: [SD93] <author> D. B. Searls and S. Dong. </author> <title> A syntactic pattern recognition system for DNA sequences. </title> <booktitle> In Proc. 2nd Int. Conf. on Bioinformatics, Supercomputing and complex genome analysis. World Scientific, </booktitle> <year> 1993. </year> <note> In press. </note>
Reference-contexts: Recent work provides an effective method for building a stochastic context-free grammar (SCFG) to model a family of RNA sequences. Some analogs of stochastic grammars and training methods do appear in Searls' most recent work in the form of costs and other trainable parameters used during parsing <ref> [Sea93a, Sea93b, SD93] </ref>, but we believe that our integrated probabilistic framework may prove to be a simpler and more effective approach. We have designed an algorithm that deduces grammar parameters automatically from a set of unaligned primary sequences.
Reference: [Sea92] <author> David B. </author> <title> Searls. The linguistics of DNA. </title> <journal> American Scientist, </journal> <volume> 80 </volume> <pages> 579-591, </pages> <month> November-December </month> <year> 1992. </year>
Reference-contexts: We compare our method to Eddy and Durbin's use of "covariance models" (CMs) to model RNA [ED94]. CMs are equivalent to SCFGs, but Eddy and Durbin employ different algorithms for training and producing multiple alignments. As in the elegant work of Searls <ref> [Sea92] </ref>, we view the character strings representing DNA, RNA and protein as sentences derived from a formal grammar. <p> Using productions of this type, a CFG can specify the language of biological palindromes. Searls' original work <ref> [Sea92] </ref> argues the benefits of using CFGs as models for RNA folding, but does not discuss stochastic grammars or methods for creating the grammar from training sequences. Recent work provides an effective method for building a stochastic context-free grammar (SCFG) to model a family of RNA sequences.
Reference: [Sea93a] <author> D. B. </author> <title> Searls. The computational linguistics of biological sequences. </title> <booktitle> In Artificial Intelligence and Molecular Biology, chapter 2, </booktitle> <pages> pages 47-120. </pages> <publisher> AAAI Press, </publisher> <year> 1993. </year>
Reference-contexts: Recent work provides an effective method for building a stochastic context-free grammar (SCFG) to model a family of RNA sequences. Some analogs of stochastic grammars and training methods do appear in Searls' most recent work in the form of costs and other trainable parameters used during parsing <ref> [Sea93a, Sea93b, SD93] </ref>, but we believe that our integrated probabilistic framework may prove to be a simpler and more effective approach. We have designed an algorithm that deduces grammar parameters automatically from a set of unaligned primary sequences.
Reference: [Sea93b] <author> D. B. </author> <title> Searls. String variable grammar: a logic grammar formalism for DNA sequences, </title> <note> 1993. Unpublished. </note>
Reference-contexts: Recent work provides an effective method for building a stochastic context-free grammar (SCFG) to model a family of RNA sequences. Some analogs of stochastic grammars and training methods do appear in Searls' most recent work in the form of costs and other trainable parameters used during parsing <ref> [Sea93a, Sea93b, SD93] </ref>, but we believe that our integrated probabilistic framework may prove to be a simpler and more effective approach. We have designed an algorithm that deduces grammar parameters automatically from a set of unaligned primary sequences.
Reference: [SZ90] <author> B. A. Shapiro and K. Zhang. </author> <title> Comparing multiple RNA secondary structures using tree comparisons. </title> <journal> CABIOS, </journal> <volume> 6(4) </volume> <pages> 309-318, </pages> <year> 1990. </year>
Reference-contexts: The Tree-Grammar EM procedure is based on the theory of stochastic tree grammars [TW68, Fu82]. Tree grammars are used to derive labeled trees instead of strings. Labeled trees can be used to represent the secondary structure of RNA easily <ref> [SZ90] </ref> (see Figure 2). A tree grammar for RNA denotes both the primary sequence and the secondary structure of each molecule.
Reference: [TE93] <author> A. J. Tranguch and D. R. Engelke. </author> <title> Comparative structural analysis of nuclear RNase P RNAs from yeast. </title> <journal> Journal of Biological Chemistry, </journal> <volume> 268 </volume> <pages> 14045-1455, </pages> <year> 1993. </year>
Reference-contexts: They may prove useful in maintaining, updating and revising existing multiple sequence alignments. In addition, a grammar itself may be a valuable tool for representing an RNA family or domain such as group I introns [MW90, MECS90], group II introns [MUO89], RNAse P RNA <ref> [BHJ + 91, TE93] </ref>, small nuclear RNAs [GP88] and 7S RNA (signal recognition particle RNA) [Zwi89]. The main difficulties in applying this work to other families of RNA will be the development of appropriate initial grammars and the computational cost of parsing longer sequences.
Reference: [TSF88] <author> D. H. Turner, N. Sugimoto, and S. M. Freier. </author> <title> RNA structure prediction. </title> <journal> Annual Review of Biophysics and Biophysical Chemistry, </journal> <volume> 17 </volume> <pages> 167-192, </pages> <year> 1988. </year>
Reference-contexts: The first technique, phylo-genetic analysis of homologous RNA molecules [FW75, WGGN83], ascertains structural features that are conserved during evolution. The second technique employs thermodynamics to compare the free energy changes predicted for formation of possible secondary structure and relies on finding the structure with the lowest free energy <ref> [TUL71, TSF88, Gou87] </ref>. Though in principle HMMs could also be used to model RNA, the standard HMM approach treats all positions as having independent distributions and is unable to model the interactions between positions.
Reference: [TUL71] <author> I. Tinoco Jr., O. C. Uhlenbeck, and M. D. Levine. </author> <title> Estimation of secondary structure in ribonucleic acids. </title> <journal> Nature, </journal> <volume> 230 </volume> <pages> 363-367, </pages> <year> 1971. </year>
Reference-contexts: The first technique, phylo-genetic analysis of homologous RNA molecules [FW75, WGGN83], ascertains structural features that are conserved during evolution. The second technique employs thermodynamics to compare the free energy changes predicted for formation of possible secondary structure and relies on finding the structure with the lowest free energy <ref> [TUL71, TSF88, Gou87] </ref>. Though in principle HMMs could also be used to model RNA, the standard HMM approach treats all positions as having independent distributions and is unable to model the interactions between positions.
Reference: [TW68] <author> J. W. Thatcher and J. B. Wright. </author> <title> Generalized finite automata theory with an application to a decision problem of second-order logic. </title> <journal> Mathematical Systems Theory, </journal> <volume> 2 </volume> <pages> 57-81, </pages> <year> 1968. </year>
Reference-contexts: We have designed an algorithm that deduces grammar parameters automatically from a set of unaligned primary sequences. It is a novel generalization of the forward-backward algorithm commonly used to train HMMs. Our algorithm, Tree-Grammar EM, is based on tree grammars <ref> [TW68] </ref> and is more efficient than the inside-outside algorithm [LY90], a computationally expensive generalization of the forward-backward algorithm developed to train SCFGs [Bak79]. <p> In this way, TG Reestimator can be used even when precise biological knowledge of the base pairing is not available. TG Reestimator constitutes one part of the entire training procedure, Tree-Grammar EM. The Tree-Grammar EM procedure is based on the theory of stochastic tree grammars <ref> [TW68, Fu82] </ref>. Tree grammars are used to derive labeled trees instead of strings. Labeled trees can be used to represent the secondary structure of RNA easily [SZ90] (see Figure 2). A tree grammar for RNA denotes both the primary sequence and the secondary structure of each molecule. <p> The root, 1 $, and the internal node, 3 $, represent AU and G-C base pairs, respectively. To avoid unnecessary complexity, we describe this new algorithm in terms of CFGs instead of tree grammars <ref> [TW68, Sak92] </ref>. A tree is a rooted, directed, connected acyclic finite graph in which the direct successors of any node are linearly ordered from left to right. The predecessor of a node is called the parent ; the successor, a child; and a child of the parent, a sibling.
Reference: [Wat88] <author> M. S. Waterman. </author> <title> Computer analysis of nucleic acid sequences. </title> <booktitle> Methods in Enzymology, </booktitle> <volume> 164 </volume> <pages> 765-792, </pages> <year> 1988. </year>
Reference-contexts: The former problem might be solved by the development of effective methods for learning the grammar itself from training sequences. The work of Eddy and Durbin is an important step in this direction [ED94]. Their method relies on correlations between columns in a multiple alignment <ref> [GPH + 92, Lap92, KB93, Wat89, WOW + 90, San85, Wat88] </ref> to discover the es sential base-pairing structure in an RNA family.
Reference: [Wat89] <author> M. S. Waterman. </author> <title> Consensus methods for folding single-stranded nucleic acids. </title> <editor> In M. S. Waterman, editor, </editor> <title> Mathematical Methods for DNA Sequences, chapter 8. </title> <publisher> CRC Press, </publisher> <year> 1989. </year>
Reference-contexts: The former problem might be solved by the development of effective methods for learning the grammar itself from training sequences. The work of Eddy and Durbin is an important step in this direction [ED94]. Their method relies on correlations between columns in a multiple alignment <ref> [GPH + 92, Lap92, KB93, Wat89, WOW + 90, San85, Wat88] </ref> to discover the es sential base-pairing structure in an RNA family. <p> Their method relies on correlations between columns in a multiple alignment [GPH + 92, Lap92, KB93, Wat89, WOW + 90, San85, Wat88] to discover the es sential base-pairing structure in an RNA family. Another approach would be to use a method like that proposed by Waterman <ref> [Wat89] </ref> to find helices in a rough initial multiple alignment, use these helices to design a simple initial grammar in a semi-automated fashion using our high-level RNA grammar specification language, then use the grammar to obtain a better multiple alignment, and iterate this process until a suitable result is obtained.
Reference: [WGGN83] <author> C. R. Woese, R. R. Gutell, R. Gupta, and H. F. Noller. </author> <title> Detailed analysis of the higher-order structure of 16S-like ribosomal ribonucleic acids. </title> <journal> Microbiology Reviews, </journal> <volume> 47(4) </volume> <pages> 621-669, </pages> <year> 1983. </year>
Reference-contexts: Two principal methods have been established for predicting RNA secondary structure (i.e., which nucleotides are base-paired). The first technique, phylo-genetic analysis of homologous RNA molecules <ref> [FW75, WGGN83] </ref>, ascertains structural features that are conserved during evolution. The second technique employs thermodynamics to compare the free energy changes predicted for formation of possible secondary structure and relies on finding the structure with the lowest free energy [TUL71, TSF88, Gou87].
Reference: [WOW + 90] <author> S. Winker, R. Overbeek, C.R. Woese, G.J. Olsen, and N. Pfluger. </author> <title> Structure detection through automated covariance search. </title> <booktitle> Computer Applications in the Biosciences, </booktitle> <volume> 6 </volume> <pages> 365-371, </pages> <year> 1990. </year>
Reference-contexts: The former problem might be solved by the development of effective methods for learning the grammar itself from training sequences. The work of Eddy and Durbin is an important step in this direction [ED94]. Their method relies on correlations between columns in a multiple alignment <ref> [GPH + 92, Lap92, KB93, Wat89, WOW + 90, San85, Wat88] </ref> to discover the es sential base-pairing structure in an RNA family.
Reference: [WPT89] <author> J. R. Wyatt, J. D. Puglisi, and I. Tinoco Jr. </author> <title> RNA folding: pseudoknots, loops and bulges. </title> <journal> BioEssays, </journal> <volume> 11(4) </volume> <pages> 100-106, </pages> <year> 1989. </year>
Reference-contexts: In RNA, the nucleotides adenine (A), cytosine (C), guanine (G) and uracil (U) interact to form characteristic secondary-structure motifs such as helices, loops and bulges <ref> [Sae84, WPT89] </ref>. Intramolecular AU and G-C Watson-Crick pairs as well as G-U and, more rarely, G-A base pairs constitute the so-called biological palindromes in the genome. When RNA sequences are aligned, both primary ? Y. Sakakibara's current address is ISIS, Fujitsu Labs Ltd., 140, Miyamoto, Numazu, Shizuoka 410-03, Japan.
Reference: [Zuk89] <author> M. Zuker. </author> <title> On finding all suboptimal foldings of an RNA molecule. </title> <journal> Science, </journal> <volume> 244 </volume> <pages> 48-52, </pages> <year> 1989. </year>
Reference-contexts: As discussed in their work [ED94], covariance models (CMs) are describable as SCFGs. To deduce a covariance model's structure (essentially, to choose an SCFG's productions), they use the standard Nussinov/Zuker dynamic-programming algorithm for RNA folding <ref> [NPGK78, Zuk89] </ref>, with the difference that the cost function being optimized is a function of the "mutual information" of two columns in the input multiple alignment (based on Gutell et al.'s MIXY procedure [GPH + 92]), rather than of the number of base pairs or the thermodynamic stacking energy.
Reference: [Zwi89] <author> C. Zwieb. </author> <title> Structure and function of signal recognition particle RNA. </title> <journal> Progress in Nucleic Acid Research and Molecular Biology, </journal> <volume> 37 </volume> <pages> 207-234, </pages> <year> 1989. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: In addition, a grammar itself may be a valuable tool for representing an RNA family or domain such as group I introns [MW90, MECS90], group II introns [MUO89], RNAse P RNA [BHJ + 91, TE93], small nuclear RNAs [GP88] and 7S RNA (signal recognition particle RNA) <ref> [Zwi89] </ref>. The main difficulties in applying this work to other families of RNA will be the development of appropriate initial grammars and the computational cost of parsing longer sequences.
References-found: 47

