URL: ftp://ftp.idsia.ch/pub/juergen/everything.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00391.html
Root-URL: 
Title: A Computer Scientist's View of Life, the Universe, and Everything  
Author: Jurgen Schmidhuber 
Keyword: Preliminaries  
Address: Corso Elvezia 36, CH-6900-Lugano, Switzerland  
Affiliation: IDSIA,  
Note: In C. Freksa, ed., Foundations of Computer Science: Potential Theory- Cognition Lecture Notes in Computer Science, Springer, 1997.  k U 2 where U 1  
Email: juergen@idsia.ch  
Web: http://www.idsia.ch/~juergen  
Abstract: Is the universe computable? If so, it may be much cheaper in terms of information requirements to compute all computable universes instead of just ours. I apply basic concepts of Kolmogorov complexity theory to the set of possible universes, and chat about perceived and true randomness, life, generalization, and learning in a given universe. Assumptions. A long time ago, the Great Programmer wrote a program that runs all possible universes on His Big Computer. "Possible" means "computable": (1) Each universe evolves on a discrete time scale. (2) Any universe's state at a given time is describable by a finite number of bits. One of the many universes is ours, despite some who evolved in it and claim it is incomputable. Computable universes. Let T M denote an arbitrary universal Turing machine with unidirectional output tape. T M 's input and output symbols are "0", "1", and "," (comma). T M 's possible input programs can be ordered alphabetically: "" (empty program), "0", "1", ",", "00", "01", "0,", "10", "11", "1,", ",0", ",1", ",,", "000", etc. Let A k denote T M 's k-th program in this list. Its output will be a finite or infinite string over the alphabet f "0","1",","g. This sequence of bitstrings separated by commas will be interpreted as the evolution E k of universe U k . If E k includes at least one comma, then let U l k represents U k 's state at the l-th time step of E k (k; l 2 f1; 2; : : : ; g). E k is represented by the sequence U 1 k corresponds to U k 's big bang. Different algorithms may compute the same universe. Some universes are finite (those whose programs cease producing outputs at some point), others are not. I don't know about ours. TM not important. The choice of the Turing machine is not important. This is due to the compiler theorem: for each universal Turing machine C there exists a constant prefix C 2 f "0","1",","g fl such that for all possible programs p, C's output in response to program C p is identical to T M 's output in response to p. The prefix C is the compiler that compiles programs for T M into equivalent programs for C. k denote the l-th (possibly empty) bitstring before the l-th comma. U l
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> G.J. Chaitin. </author> <title> Algorithmic Information Theory. </title> <publisher> Cambridge University Press, </publisher> <address> Cam-bridge, </address> <year> 1987. </year>
Reference-contexts: 1. Shortest algorithm cannot be found. It can be shown that there is no algorithm that can generate the shortest program for computing arbitrary given data on a given computer <ref> [2, 9, 1] </ref>. In particular, our physicists cannot expect to find the most compact description of our universe. 2. Additional problems of the Heisenberg type. Heisenberg tells us that we cannot even observe the precise, current state of a single electron, let alone our universe. <p> Programs are "self-delimiting" <ref> [3, 1] </ref> | once U halts due to computations based on the randomly chosen symbols (the program) on its input tape, there won't be any additional program symbols. <p> Assuming that recognition implies relating observations to previous knowledge, both L k 's and the observer's life will have to share mutual algorithmic information <ref> [1] </ref>: there will be a comparatively short algorithm computing L k 's from the observer's life, and vice versa. Of course, creatures living in a given universe don't have to have any idea of the symbol strings by which they are represented. Possible limitations of the Great Programmer.
Reference: 2. <author> A.N. </author> <title> Kolmogorov. Three approaches to the quantitative definition of information. </title> <journal> Problems of Information Transmission, </journal> <volume> 1 </volume> <pages> 1-11, </pages> <year> 1965. </year>
Reference-contexts: 1. Shortest algorithm cannot be found. It can be shown that there is no algorithm that can generate the shortest program for computing arbitrary given data on a given computer <ref> [2, 9, 1] </ref>. In particular, our physicists cannot expect to find the most compact description of our universe. 2. Additional problems of the Heisenberg type. Heisenberg tells us that we cannot even observe the precise, current state of a single electron, let alone our universe.
Reference: 3. <author> L. A. Levin. </author> <title> Laws of information (nongrowth) and aspects of the foundation of probability theory. </title> <journal> Problems of Information Transmission, </journal> <volume> 10(3) </volume> <pages> 206-210, </pages> <year> 1974. </year>
Reference-contexts: Programs are "self-delimiting" <ref> [3, 1] </ref> | once U halts due to computations based on the randomly chosen symbols (the program) on its input tape, there won't be any additional program symbols. <p> The probability of a string is dominated by the probabilities of its shortest programs. This is known as the "coding theorem" <ref> [3] </ref>. Similar coding theorems exist for the case of non-halting programs which cease requesting additional input symbols at a certain point.
Reference: 4. <author> L. A. Levin. </author> <title> Randomness conservation inequalities: Information and independence in mathematical theories. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 15-37, </pages> <year> 1984. </year>
Reference-contexts: Unlike with previous learning paradigms, the entire life is considered for performance evaluations. Experiments in [7, 6] show the paradigm's practical feasibility. For instance, in [7] A includes an extension of Levin search <ref> [4] </ref> for generating the PMPs. Philosophy Life after death. Members of certain religious sects expect resurrection of the dead in a paradise where lions and lambs cuddle each other. There is a possible continuation of our world where they will be right.
Reference: 5. <author> J. Schmidhuber. </author> <title> Discovering neural nets with low Kolmogorov complexity and high generalization capability. </title> <booktitle> Neural Networks, </booktitle> <year> 1997. </year> <note> In press. </note>
Reference: 6. <author> J. Schmidhuber, J. Zhao, and N. Schraudolph. </author> <title> Reinforcement learning with self-modifying policies. </title> <editor> In S. Thrun and L. Pratt, editors, </editor> <title> Learning to learn. </title> <publisher> Kluwer, </publisher> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: Since the success of a policy modification recursively depends on the success of later modifications for which it is setting the stage, the framework provides a basis for "learning how to learn". Unlike with previous learning paradigms, the entire life is considered for performance evaluations. Experiments in <ref> [7, 6] </ref> show the paradigm's practical feasibility. For instance, in [7] A includes an extension of Levin search [4] for generating the PMPs. Philosophy Life after death. Members of certain religious sects expect resurrection of the dead in a paradise where lions and lambs cuddle each other.
Reference: 7. <author> J. Schmidhuber, J. Zhao, and M. Wiering. </author> <title> Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement. </title> <journal> Machine Learning, </journal> <volume> 26, </volume> <year> 1997. </year> <note> In press. </note>
Reference-contexts: Since the success of a policy modification recursively depends on the success of later modifications for which it is setting the stage, the framework provides a basis for "learning how to learn". Unlike with previous learning paradigms, the entire life is considered for performance evaluations. Experiments in <ref> [7, 6] </ref> show the paradigm's practical feasibility. For instance, in [7] A includes an extension of Levin search [4] for generating the PMPs. Philosophy Life after death. Members of certain religious sects expect resurrection of the dead in a paradise where lions and lambs cuddle each other. <p> Unlike with previous learning paradigms, the entire life is considered for performance evaluations. Experiments in [7, 6] show the paradigm's practical feasibility. For instance, in <ref> [7] </ref> A includes an extension of Levin search [4] for generating the PMPs. Philosophy Life after death. Members of certain religious sects expect resurrection of the dead in a paradise where lions and lambs cuddle each other. There is a possible continuation of our world where they will be right.
Reference: 8. <author> C. E. Shannon. </author> <title> A mathematical theory of communication (parts I and II). </title> <journal> Bell System Technical Journal, </journal> <volume> XXVII:379-423, </volume> <year> 1948. </year>
Reference: 9. <author> R.J. Solomonoff. </author> <title> A formal theory of inductive inference. Part I. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22, </pages> <year> 1964. </year>
Reference-contexts: 1. Shortest algorithm cannot be found. It can be shown that there is no algorithm that can generate the shortest program for computing arbitrary given data on a given computer <ref> [2, 9, 1] </ref>. In particular, our physicists cannot expect to find the most compact description of our universe. 2. Additional problems of the Heisenberg type. Heisenberg tells us that we cannot even observe the precise, current state of a single electron, let alone our universe.
Reference: 10. <author> D. H. Wolpert. </author> <title> The lack of a priori distinctions between learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 8(7) </volume> <pages> 1341-1390, </pages> <year> 1996. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: In other words, the mutual algorithmic information between history and future will be zero. As a consequence, in most universes (those that can be computed by long algorithms only), successful generalization from previous experience is not possible. Neither is inductive transfer. This simple insight is related to results in <ref> [10] </ref>. Learning. Given the above, since learning means to use previous experience to improve future performance, learning is possible only in the few regular universes (no learning without compressibility). On the other hand, regularity by itself is not sufficient to allow for learning.
References-found: 10

