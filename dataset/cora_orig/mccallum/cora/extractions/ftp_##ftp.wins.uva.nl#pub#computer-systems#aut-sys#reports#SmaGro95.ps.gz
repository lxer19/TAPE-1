URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/SmaGro95.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Phone: phone +31 20 5257463, fax +31 20 5257490  
Title: Approximation with neural networks: Between local and global approximation  
Author: Patrick van der Smagt and Frans Groen 
Web: URL http://www.fwi.uva.nl/research/neuro/  
Address: Kruislaan 403, NL-1098 SJ Amsterdam  
Affiliation: Department of Computer Systems, University of Amsterdam  
Abstract: We investigate neural network based approximation methods. These methods depend on the locality of the basis functions. After discussing local and global basis functions, we propose a a multi-resolution hierarchical method. The various resolutions are stored at various levels in a tree. At the root of the tree, a global approximation is kept; the leafs store the learning samples themselves. Intermediate nodes store intermediate representations. In order to find an optimal partitioning of the input space, self-organising maps (SOM's) are used. The proposed method has implementational problems reminiscent of those encountered in many-particle simulations. We will investigate the parallel implementation of this method, using parallel hierarchical meth ods for many-particle simulations as a starting point.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Barnes and P. Hut, </author> <title> "A hierarchical O(N log N ) force calculation algorithm," </title> <journal> Nature, </journal> <volume> vol. 324, </volume> <pages> pp. 446-449, </pages> <year> 1986. </year>
Reference-contexts: The method described above results in a partitioning of the input space similar to the method described by Barnes and Hut <ref> [1] </ref>. In our case, however, the input space is not divided along static (predefined) boundaries, but along the regions of influence of the neurons in each cluster. Using the hierarchical decomposition, we have to calculate the interaction between a learning sample and a set of clusters.
Reference: [2] <author> D. Cubanski and D. Cyganski, </author> <title> "Multivariate classification through adaptive Delaunay-based C 0 spline approximation," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 17, </volume> <pages> pp. 55-66, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Partitioning strategy The optimality of the resulting approximation depends on the partitioning of the input space. Although a partitioning along static boundaries leads to reasonable results [6], boundaries may be very awkwardly chosen. There are several classic methods for sample partitioning. For instance, Delaunay tessellation <ref> [2] </ref> is known to optimally partition the input space, yet costs O (N 2m 1 ), i.e., almost quadratic in the number of samples for high-dimensional functions. This means that the method is prohibitive for many-sample problems. We therefore choose for an input partitioning using self-organising maps.
Reference: [3] <author> R. Hecht-Nielsen, </author> <title> "Counterpropagation networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 1, </volume> <pages> pp. 131-139, </pages> <year> 1988. </year>
Reference-contexts: In neural network learning, the local approach is used by radial basis and Counterpropagation <ref> [3] </ref> neural networks, CMAC networks [8], and hierarchical networks [6]. Orthogonal? A final method of creating non-interfering basis functions is by making them or Fig. 1: A one-dimensional function approximated by global, local, and nested-network basis functions. thogonal. <p> This means that the method is prohibitive for many-sample problems. We therefore choose for an input partitioning using self-organising maps. This approach is reminiscent of the Counterpropagation network <ref> [3] </ref>, which uses a self-organising map for determining the optimal placing of radial basis functions. In our case, however, the self-organising map is used to partition the input space in a Voronoi tessellation, following the distribution of the learning samples. The hierarchical partitioning method works as follows.
Reference: [4] <author> T. M. Heskes, </author> <title> Learning Processes in Neural Networks. </title> <type> PhD thesis, </type> <institution> Katholieke Universiteit Ni-jmegen, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Although gradient-based learning is, in general, faster [10], stochastic learning methods are useful for avoiding local minima. Stochastic learning is widely used for training self-organising maps, where the problem of local minima is serious. In this case the stochasticity results from the distribution of the learning samples <ref> [4] </ref> which are used to train the network. However, in both cases convergence to a global optimum cannot be guaranteed in high-dimensional function approximation. Furthermore, in general learning is a very costly operation. 3.
Reference: [5] <author> T. Hesselroth, K. Sarkar, P. van der Smagt, and K. Schulten, </author> <title> "Neural network control of a pneumatic robot arm," </title> <journal> IEEE Tr. on Systems, Man, and Cybernetics, </journal> <volume> vol. 24, </volume> <pages> pp. 28-38, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: k indicates the winning neuron corresponding with input sample x and ff is a learning rate, defined as ff i;k = *(t)e jikj 2 =2 (t) 2 : (9) Learning rules (6) and (7) were previously proposed in [9], while the quadratic term in ff is taken in accordance with <ref> [5] </ref>. We take for * and the normal time-dependency *(t) = * 0 * f t=t max 0 : With each neuron is associated its (Voronoi-) region of influence, indicated by V d;i which is a sub-space of the SOM's input space &lt; m .
Reference: [6] <author> A. Jansen, P. van der Smagt, and F. C. A. Groen, </author> <title> "Nested networks for robot control," in Neural Network Applications, </title> <editor> (A. F. </editor> <address> Murray, </address> <publisher> ed.), </publisher> <pages> pp. 221-239, </pages> <address> Dordrecht: </address> <publisher> Kluwer Acad. Publ., </publisher> <year> 1995. </year>
Reference-contexts: In neural network learning, the local approach is used by radial basis and Counterpropagation [3] neural networks, CMAC networks [8], and hierarchical networks <ref> [6] </ref>. Orthogonal? A final method of creating non-interfering basis functions is by making them or Fig. 1: A one-dimensional function approximated by global, local, and nested-network basis functions. thogonal. <p> Furthermore, in general learning is a very costly operation. 3. Multi-resolution representations with the Nested Network In order to attempt to solve the above problems, we have introduced a split-and-merge algorithm in neural network function approximation <ref> [6] </ref>. Starting with a global approximation, the network is incrementally refined by splitting those parts of the input space where the approximation is currently too imprecise. <p> Figure 1 depicts three types of approximation: global, with local kernels (SOM-based), or by splitting up the input space. 3.1. Partitioning strategy The optimality of the resulting approximation depends on the partitioning of the input space. Although a partitioning along static boundaries leads to reasonable results <ref> [6] </ref>, boundaries may be very awkwardly chosen. There are several classic methods for sample partitioning. For instance, Delaunay tessellation [2] is known to optimally partition the input space, yet costs O (N 2m 1 ), i.e., almost quadratic in the number of samples for high-dimensional functions. <p> The method can be well used to approximate functions of high dimensionality, and has a good basis for adapting to changing circumstances (i.e., a change in the underlying function F ) by comparing the approximation at various levels in the tree <ref> [6] </ref>. Acknowledgments. The authors are grateful for many helpful discussions with Ben Krose and Peter Sloot.
Reference: [7] <author> T. M. Martinetz, S. G. Berkovich, and K. J. Schul-ten, </author> <title> ""Neural-Gas" networ for vector quantization and its application to time-series prediction," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 4, no. 4, </volume> <pages> pp. 558-569, </pages> <year> 1993. </year>
Reference-contexts: Successful results have been reported in various applications such as control [9], interpretation of data obtained from physics experiments, or prediction of time-series such as financial data <ref> [7] </ref>; classification problems are also an example of function approximation. However, a major point of research remains how to create accurate approx-imators which learn fast and are able to adapt to changing environments.
Reference: [8] <author> W. T. Miller III, </author> <title> "Real-time application of neural networks for sensor-based control of robots with vision," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 19, </volume> <pages> pp. 825-831, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: In neural network learning, the local approach is used by radial basis and Counterpropagation [3] neural networks, CMAC networks <ref> [8] </ref>, and hierarchical networks [6]. Orthogonal? A final method of creating non-interfering basis functions is by making them or Fig. 1: A one-dimensional function approximated by global, local, and nested-network basis functions. thogonal.
Reference: [9] <author> H. Ritter and K. Schulten, </author> <title> "Topology conserving mappings for learning motor tasks," in Neural Networks of Computing, </title> <editor> (J. S. Denker, </editor> <publisher> ed.), </publisher> <pages> pp. 376-380, </pages> <booktitle> AIP Conference Proceedings 151, </booktitle> <year> 1986. </year>
Reference-contexts: 1. Introduction The use of neural networks for the approximation of functions of high dimensionality from randomly distributed learning samples has long been established. Successful results have been reported in various applications such as control <ref> [9] </ref>, interpretation of data obtained from physics experiments, or prediction of time-series such as financial data [7]; classification problems are also an example of function approximation. However, a major point of research remains how to create accurate approx-imators which learn fast and are able to adapt to changing environments. <p> (6) b d;i b d;i + ff i;k x w d;i where k indicates the winning neuron corresponding with input sample x and ff is a learning rate, defined as ff i;k = *(t)e jikj 2 =2 (t) 2 : (9) Learning rules (6) and (7) were previously proposed in <ref> [9] </ref>, while the quadratic term in ff is taken in accordance with [5].
Reference: [10] <author> P. van der Smagt, </author> <title> "Minimisation methods for training feed-forward networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 7, no. 1, </volume> <pages> pp. 1-11, </pages> <year> 1994. </year>
Reference-contexts: We have thus found that, except in some specific cases, we have to revert to global basis functions and solve the complex parameter estimation problem. 2.4. Learning In order to tackle that problem, we distinguish gradient-based learning and stochastic learning. Although gradient-based learning is, in general, faster <ref> [10] </ref>, stochastic learning methods are useful for avoiding local minima. Stochastic learning is widely used for training self-organising maps, where the problem of local minima is serious. In this case the stochasticity results from the distribution of the learning samples [4] which are used to train the network.
Reference: [11] <author> P. van der Smagt and B. Krose, </author> <title> "Using many-particle decomposition to get a parallel self-organising map," </title> <booktitle> in Proc. 1995 Conf. Computer Science in the Netherlands, </booktitle> <year> 1995. </year> <note> In press. </note>
Reference-contexts: Subsequently, d is incremented, and steps (6)-(11) are repeated for the neurons at depth d (i.e., all new neurons). Computational cost. Due to the partitioning of the neurons in disjoint clusters, the neighbourhood learning rule can be applied per cluster instead of per neuron. As we have shown elsewhere <ref> [11] </ref> the computational cost thus goes down from O (N M ) for a network with M neurons, to O (N log M ). Parallel implementation using many-particle systems. The hierarchical decomposition of the input space using self-organising maps is a good basis for parallel implementation.
References-found: 11

