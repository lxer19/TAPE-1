URL: http://www.math.tau.ac.il/~gales/papers/nips95.ps.gz
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Title: Implementation Issues in the Fourier Transform Algorithm  
Author: Yishay Mansour Sigal Sahar 
Address: Tel-Aviv, ISRAEL  
Affiliation: Computer Science Dept. Tel-Aviv University  
Abstract: The Fourier transform of boolean functions has come to play an important role in proving many important learnability results. We aim to demonstrate that the Fourier transform techniques are also a useful and practical algorithm in addition to being a powerful theoretical tool. We describe the more prominent changes we have introduced to the algorithm, ones that were crucial and without which the performance of the algorithm would severely deteriorate. One of the benefits we present is the confidence level for each prediction which measures the likelihood the prediction is correct.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Mihir Bellare. </author> <title> A technique for upper bounding the spectral norm with applications to learning. </title> <booktitle> In 5 th Annual Workshop on Computational Learning T heory, </booktitle> <pages> pages 62-70, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Originally the procedure was used to learn decision trees [5], and in [8, 2, 4] it was used to learn polynomial size DNF. The FT technique applies naturally to the uniform distribution, though some of the learnability results were extended to product distribution <ref> [1, 3] </ref>. fl e-mail: mansour@cs.tau.ac.il y e-mail: gales@cs.tau.ac.il A great advantage of the FT algorithm is that it does not make any assumptions on the function it is learning. We can apply it to any function and hope to obtain "large" Fourier coefficients.
Reference: [2] <author> Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich. </author> <title> Weakly learning DNF and characterizing statistical query learning using fourier analysis. </title> <booktitle> In T he 26 th Annual ACM Symposium on T heory of Computing, </booktitle> <pages> pages 253 - 262, </pages> <year> 1994. </year>
Reference-contexts: The work of [5] developed a very powerful algorithmic procedure: given a function and a threshold parameter it finds in polynomial time all the Fourier coefficients of the function larger than the threshold. Originally the procedure was used to learn decision trees [5], and in <ref> [8, 2, 4] </ref> it was used to learn polynomial size DNF.
Reference: [3] <author> Merrick L. Furst, Jeffrey C. Jackson, and Sean W. Smith. </author> <title> Improved learning of AC 0 functions. </title> <booktitle> In 4 th Annual Workshop on Computational Learning T heory, </booktitle> <pages> pages 317-325, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Originally the procedure was used to learn decision trees [5], and in [8, 2, 4] it was used to learn polynomial size DNF. The FT technique applies naturally to the uniform distribution, though some of the learnability results were extended to product distribution <ref> [1, 3] </ref>. fl e-mail: mansour@cs.tau.ac.il y e-mail: gales@cs.tau.ac.il A great advantage of the FT algorithm is that it does not make any assumptions on the function it is learning. We can apply it to any function and hope to obtain "large" Fourier coefficients.
Reference: [4] <author> J. Jackson. </author> <title> An efficient membership-query algorithm for learning DNF with respect to the uniform distribution. </title> <booktitle> In Annual Symposium on Switching and Automata Theory, </booktitle> <pages> pages 42 - 53, </pages> <year> 1994. </year>
Reference-contexts: The work of [5] developed a very powerful algorithmic procedure: given a function and a threshold parameter it finds in polynomial time all the Fourier coefficients of the function larger than the threshold. Originally the procedure was used to learn decision trees [5], and in <ref> [8, 2, 4] </ref> it was used to learn polynomial size DNF.
Reference: [5] <author> E. Kushilevitz and Y. Mansour. </author> <title> Learning decision trees using the fourier spectrum. </title> <journal> SIAM Journal on Computing 22(6): </journal> <pages> 1331-1348, </pages> <year> 1993. </year>
Reference-contexts: The first connection between the Fourier representation and learnability of boolean functions was established in [6] where the class AC 0 was learned (using its FT representation) in O (n polylog (n) ) time. The work of <ref> [5] </ref> developed a very powerful algorithmic procedure: given a function and a threshold parameter it finds in polynomial time all the Fourier coefficients of the function larger than the threshold. Originally the procedure was used to learn decision trees [5], and in [8, 2, 4] it was used to learn polynomial <p> The work of <ref> [5] </ref> developed a very powerful algorithmic procedure: given a function and a threshold parameter it finds in polynomial time all the Fourier coefficients of the function larger than the threshold. Originally the procedure was used to learn decision trees [5], and in [8, 2, 4] it was used to learn polynomial size DNF.
Reference: [6] <author> N. Linial, Y. Mansour, and N. Nisan. </author> <title> Constant depth circuits, fourier transform and learnability. </title> <journal> JACM 40(3) </journal> <pages> 607-620, </pages> <year> 1993. </year>
Reference-contexts: It has been used mainly to demonstrate the learnability of various classes of functions with respect to the uniform distribution. The first connection between the Fourier representation and learnability of boolean functions was established in <ref> [6] </ref> where the class AC 0 was learned (using its FT representation) in O (n polylog (n) ) time.
Reference: [7] <author> Y. Mansour. </author> <title> Learning Boolean Functions via the Fourier Transform. Advances in Neural Computation, edited by V.P. </title> <editor> Roychodhury and K-Y. Siu and A. Orlitsky, </editor> <publisher> Kluwer Academic Pub. </publisher> <year> 1994. </year> <note> Can be accessed via ftp://ftp.math.tau.ac.il/pub/mansour/PAPERS/LEARNING/fourier-survey.ps.Z. </note>
Reference-contexts: We end with our conclusions in Section 5. 2 FOURIER TRANSFORM (FT) THEORY In this section we briefly introduce the FT theory and algorithm. its connection to learning and the algorithm that finds the large coefficients. A comprehensive survey of the theoretical results and proofs can be found in <ref> [7] </ref>. We consider boolean functions of n variables: f : f0; 1g n ! f1; 1g. <p> Again, for more details see <ref> [7] </ref>. 3 EXPERIMENTS We implemented the FT algorithm (Section 2.2) and went forth to run a series of experiments. The parameters of each experiment include the target function, , m 1 and m 2 . We briefly introduce the parameters here and defer the detailed discussion.
Reference: [8] <author> Yishay Mansour. </author> <title> An o(n log log n ) learning algorihm for DNF under the uniform distribution. J: </title> <journal> of Computer and System Science, </journal> <volume> 50(3) </volume> <pages> 543-550, </pages> <year> 1995. </year>
Reference-contexts: The work of [5] developed a very powerful algorithmic procedure: given a function and a threshold parameter it finds in polynomial time all the Fourier coefficients of the function larger than the threshold. Originally the procedure was used to learn decision trees [5], and in <ref> [8, 2, 4] </ref> it was used to learn polynomial size DNF.
References-found: 8

