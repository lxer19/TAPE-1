URL: http://www.cs.cmu.edu/afs/cs/project/theo-11/www/wwkb/ms-ie.ps.gz
Refering-URL: http://www.cs.cmu.edu/~WebKB/
Root-URL: 
Email: dayne@cs.cmu.edu  
Phone: Phone  
Title: Title: Multistrategy Learning for Information Extraction  
Degree: Author: Dayne Freitag  
Keyword: Multistrategy learning, information extraction, classification, inductive logic programming  
Note: Email address of contact author:  number of contact author: 412-268-3608 Multiple submission statement: Not applicable  
Address: 5000 Forbes Avenue Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Information extraction (IE) is the problem of filling out pre-defined structured summaries from text documents. We are interested in performing IE in non-traditional domains, where much of the text is often ungrammatical, such as electronic bulletin board posts and Web pages. We suggest that the best approach is one that takes into account many different kinds of information, and argue for the suitability of a multistrategy approach. We describe learners for IE drawn from three separate machine learning paradigms: rote memorization, "bag-of-words" text classification, and relational rule induction. By building regression models mapping from learner confidence to probability of correctness and combining probabilities appropriately, it is possible to improve extraction accuracy over that achieved by any individual learner. We describe three different multistrategy approaches. Experiments on two IE domains, a collection of electronic seminar announcements from a university computer science department and a set of newswire articles describing corporate acquisitions from the Reuters collection, demonstrate the effectiveness of all three approaches. 
Abstract-found: 1
Intro-found: 1
Reference: [CL96] <author> J. Cowie and W. Lehnert. </author> <title> Information extraction. </title> <journal> Communications of the ACM, </journal> <volume> 39(1), </volume> <year> 1996. </year>
Reference-contexts: In hypertext, it can support directed and efficient automatic navigation. It can serve as a source of high-quality features for document categorization. And the output of an IE system can be viewed as a kind of succinct and directed summarization. Although traditional IE <ref> [CL96] </ref> concentrates on domains consisting of grammatical prose, we are interested in extracting information from "messy" text, such as Web pages, email, and finger plan files. Our goal is the development of machine learning methods for such domains.
Reference: [CM97] <author> M. E. Califf and R. J. Mooney. </author> <title> Relational learning of pattern-match rules for information extraction. </title> <booktitle> In Working Papers of ACL-97 Workshop on Natural Language Learning, </booktitle> <year> 1997. </year>
Reference-contexts: To perform well, these methods must be prepared to exploit non-linguistic information, such as stock phrases, document formatting, meta-textual structure (e.g., in HTML), and term frequency statistics. Several learning IE systems have been proposed which are also targeted at such domains [Sod97] <ref> [CM97] </ref> [Kus97]. These all take a single approach or attack a particular aspect of the problem of learning for IE. The best possible machine learning system, however, will be able to exploit any or all such sources of information. This suggests a multistrategy approach. <p> Symbolic learning algorithms from the "covering" family form hypotheses that match such data spaces well. Previous research has shown the effectiveness of such methods for the IE problem [Sod96] <ref> [CM97] </ref>. Our relational learner, called SRV, is a variant of FOIL [Qui90]. Its example space consists of all text fragments from the training document collection as long (in number of tokens) as the smallest field instance but no longer than the largest.
Reference: [CS93] <author> P.K. Chan and S.J. Stolfo. </author> <title> Experiments on multistrategy learning by meta-learning. </title> <booktitle> In Proceedings of the Second International Conference on Information and Knowledge Management (CIKM 93), </booktitle> <pages> pages 314-323, </pages> <year> 1993. </year>
Reference-contexts: All constituent learners are inductive, each designed to solve the IE problem individually. The problem we address is how the predictions of these learners, which are treated as black boxes, can be profitably combined. This approach has been called "meta-learning" in the literature <ref> [CS93] </ref>. In this paper, we introduce three machine learning algorithms for IE, each drawn from a different paradigm and each suitable for particular kinds of IE problems. Next, we describe three ways of combining the basic learners, all variations of the meta-learning idea.
Reference: [Dom96] <author> P. Domingos. </author> <title> Unifying instance-based and rule-based induction. </title> <journal> Machine Learning, </journal> <volume> 24(2) </volume> <pages> 141-168, </pages> <year> 1996. </year>
Reference-contexts: The bulk of emphasis in past research in this area has been on systems which combine analytical and empirical techniques. Our work, however, is an example of what has been called "empirical multistrategy learning" <ref> [Dom96] </ref>. All constituent learners are inductive, each designed to solve the IE problem individually. The problem we address is how the predictions of these learners, which are treated as black boxes, can be profitably combined. This approach has been called "meta-learning" in the literature [CS93].
Reference: [Fre97] <author> D. Freitag. </author> <title> Using grammatical inference to improve precision in information extraction. In Notes of the ICML-97 Workshop on Automata Induction, Grammatical Inference, and Language Acquisition, </title> <year> 1997. </year>
Reference-contexts: We also gather statistics on a fixed number context terms on either side of training field instances. All of the resulting constituent probabilities are combined with the prior as terms in a large product that is Bayes's output for a fragment. The algorithm is described in greater detail elsewhere <ref> [Fre97] </ref>. 2.3 Relational Learning Both Bayes and Rote are hobbled by their inability to take into account anything but simple term frequency statistics. It may be the case, however, that the information needed to perform information extraction comes in other forms.
Reference: [Kus97] <author> N. Kushmerick. </author> <title> Wrapper Induction for Information Extraction. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1997. </year> <note> Tech Report UW-CSE-97-11-04. </note>
Reference-contexts: To perform well, these methods must be prepared to exploit non-linguistic information, such as stock phrases, document formatting, meta-textual structure (e.g., in HTML), and term frequency statistics. Several learning IE systems have been proposed which are also targeted at such domains [Sod97] [CM97] <ref> [Kus97] </ref>. These all take a single approach or attack a particular aspect of the problem of learning for IE. The best possible machine learning system, however, will be able to exploit any or all such sources of information. This suggests a multistrategy approach.
Reference: [Lew92] <author> D. Lewis. </author> <title> Representation and Learning in Information Retrieval. </title> <type> PhD thesis, </type> <institution> Univ. of Massachusetts, </institution> <year> 1992. </year> <type> CS Tech. Report 91-93. </type>
Reference-contexts: These announcements are tagged for four fields: speaker, location, stime (start time), and etime (end time). The other domain, a collection of 600 newswire articles on corporate acquisitions from the Reuters data set <ref> [Lew92] </ref>, defines nine fields.
Reference: [Mar61] <author> M.E. Maron. </author> <title> Automatic indexing: An experimental inquiry. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 8 </volume> <pages> 404-417, </pages> <year> 1961. </year>
Reference-contexts: Although it is hard to exploit contextual regularities by memorizing, statistical approaches are well suited for this. We base our bag-of-words learner, which we call Bayes, on the Naive Bayes algorithm, as used in document classification and elsewhere (originally in <ref> [Mar61] </ref>). Each fragment of text in a document (of appropriate size) is regarded as a competing hypothesis. Given a document, we want to find the most likely hypothesis (the fragment most likely to be a field instance).
Reference: [MT94] <editor> R.S. Michalski and G. Tecuci, editors. </editor> <title> Machine Learning: A Multistrategy Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: This suggests a multistrategy approach. Multistrategy learning is an attempt to devise systems which, by employing multiple constituent learners, which are typically drawn from diverse paradigms, achieve performance superior to any single learner <ref> [MT94] </ref>. The bulk of emphasis in past research in this area has been on systems which combine analytical and empirical techniques. Our work, however, is an example of what has been called "empirical multistrategy learning" [Dom96]. All constituent learners are inductive, each designed to solve the IE problem individually.
Reference: [Qui90] <author> J. R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: Symbolic learning algorithms from the "covering" family form hypotheses that match such data spaces well. Previous research has shown the effectiveness of such methods for the IE problem [Sod96] [CM97]. Our relational learner, called SRV, is a variant of FOIL <ref> [Qui90] </ref>. Its example space consists of all text fragments from the training document collection as long (in number of tokens) as the smallest field instance but no longer than the largest. A negative example is any fragment that is not tagged as a field instance.
Reference: [Sod96] <author> S. Soderland. </author> <title> Learning Text Analysis Rules for Domain-specific Natural Language Processing. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1996. </year> <type> CS Tech. Report 96-087. </type>
Reference-contexts: Symbolic learning algorithms from the "covering" family form hypotheses that match such data spaces well. Previous research has shown the effectiveness of such methods for the IE problem <ref> [Sod96] </ref> [CM97]. Our relational learner, called SRV, is a variant of FOIL [Qui90]. Its example space consists of all text fragments from the training document collection as long (in number of tokens) as the smallest field instance but no longer than the largest.
Reference: [Sod97] <author> S. Soderland. </author> <title> Learning to extract text-based information from the world wide web. </title> <booktitle> In Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining, </booktitle> <year> 1997. </year>
Reference-contexts: To perform well, these methods must be prepared to exploit non-linguistic information, such as stock phrases, document formatting, meta-textual structure (e.g., in HTML), and term frequency statistics. Several learning IE systems have been proposed which are also targeted at such domains <ref> [Sod97] </ref> [CM97] [Kus97]. These all take a single approach or attack a particular aspect of the problem of learning for IE. The best possible machine learning system, however, will be able to exploit any or all such sources of information. This suggests a multistrategy approach.
References-found: 12

