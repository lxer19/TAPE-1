URL: http://www.cs.princeton.edu/~dubnicki/papers/adjust_caches.ps
Refering-URL: http://www.cs.princeton.edu/~dubnicki/pub.html
Root-URL: http://www.cs.princeton.edu
Title: Adjustable Block Size Coherent Caches  
Author: Czarek Dubnicki Thomas J. LeBlanc 
Address: Rochester, New York 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract: of sharing and locality exhibited by the program and the cache block size. Large cache blocks exploit processor and spatial locality, but may cause unnecessary cache invalidations due to false sharing. Small cache blocks can reduce the number of cache invalidations, but increase the number of bus or network transactions required to load data into the cache. In this paper we describe a cache organization that dynamically adjusts the cache block size according to recently observed reference behavior. Cache blocks are split across cache lines when false sharing occurs, and merged back into a single cache line to exploit spatial locality. To evaluate this cache organization, we simulate a scalable multiprocessor with coherent caches, using a suite of memory reference traces to model program behavior. We show that for every fixed block size, some program suffers a 33% increase in the average waiting time per reference, and a factor of 2 increase in the average number of words transferred per reference, when compared against the performance of an adjustable block size cache. In the few cases where adjust ing the block size does not provide superior performance, it comes within 7% of the best fixed block size alternative. We conclude that an adjustable block size cache offers significantly better performance than every fixed block size cache, especially when there is variability in the granularity of sharing exhibited by applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B.N. Bershad, E.D. Lazowska, and H.M. Levy. </author> <title> Presto: A system for object-oriented parallel programming. </title> <journal> Software Practice and Experience, </journal> <volume> 18(8), </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: S-cholesky performs parallel factorization of a sparse matrix; S-mp3d solves a problem in fluid flow simulation. Both programs, which were written for a bus-based multiprocessor, exhibit fine-grain shar ing and poor processor locality. * Presto applications Four programs were implemented in Presto <ref> [1] </ref>. P-life is a parallel implementation of Con-way's game of life, p-qsort implements parallel quick-sort, p-matmult is a matrix multiplication program, and p-gauss is a Gaussian elimination program. Like the SPLASH programs, these applications were written for a bus-based multiprocessor, and exhibit fine-grain sharing and poor processor locality.
Reference: [2] <author> W.J. Bolosky, M.L. Scott, R.P. Fitzgerald, R.J. Fowler, and A.L. Cox. </author> <title> NUMA policies and their relation to memory architecture. </title> <booktitle> Proc. 4th ASPLOS, </booktitle> <pages> pages 212-221, </pages> <year> 1991. </year>
Reference-contexts: In addition to our work, these traces have been used by others to study memory management policies and the performance benefits of disabling coherence to minimize short-term false sharing <ref> [2] </ref>. The programs in our trace suite fall into three different classes: * C-threads applications The majority of the programs were implemented using the C-threads package on a non-uniform memory access (NUMA) multiprocessor. Sorbyr and sorbyc implement red-black successive over-relaxation.
Reference: [3] <author> L. Bomans, D. Roose, and R. Hempel. </author> <title> The Ar-gonne/GMD macros in FORTRAN for portable parallel programming and their implementation on the intel iPSC/2. </title> <journal> Journal of Parallel Computing, </journal> <volume> 15 </volume> <pages> 119-132, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Gauss is a Gaussian elimination program. All of these programs exhibit coarse-grain parallelism and good processor locality. * SPLASH applications Two programs were taken from the SPLASH suite [14]. These programs were written in C using the Argonne macros for parallel con structs <ref> [3] </ref>. S-cholesky performs parallel factorization of a sparse matrix; S-mp3d solves a problem in fluid flow simulation. Both programs, which were written for a bus-based multiprocessor, exhibit fine-grain shar ing and poor processor locality. * Presto applications Four programs were implemented in Presto [1].
Reference: [4] <author> L.M. Censier and P. Feautier. </author> <title> A new solution to coherence problems in multicache systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 27 </volume> <pages> 1112-1118, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: Each processing node contains a processor, a processor cache, and a memory module. Data coherence is maintained in hardware using a directory protocol <ref> [4] </ref>. There are four levels in the memory hierarchy. The first level consists of the local processor caches. Local memory is the second level. The third level includes the caches of all other nodes. The fourth level consists of all nonlocal memory modules.
Reference: [5] <author> D.R. Cheriton, H.A. Goosen, and P. Machanick. </author> <title> Restructuring a parallel simulation to improve cache behavior in a shared-memory mutiprocessor: A first experience. </title> <booktitle> In Intl. Symp. on Shared Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: One general technique to achieve such a match is to structure the application based on knowledge of the cache organization, allocating separate objects to sepa rate cache lines [15], grouping objects according to sharing patterns <ref> [5; 9] </ref>, and avoiding fine-grain sharing [8]. Using a variety of program restructuring techniques, Cheriton et al [5] were able to achieve an order of magnitude improvement in the cache miss ratio for shared data in large (128 bytes) cache lines. <p> Using a variety of program restructuring techniques, Cheriton et al <ref> [5] </ref> were able to achieve an order of magnitude improvement in the cache miss ratio for shared data in large (128 bytes) cache lines.
Reference: [6] <author> D.R. Cheriton, A. Gupta, P.D. Boyle, and H.A. Goosen. </author> <title> The VMP multiprocessor: Initial experience, refinements, and performance evaluation. </title> <booktitle> In 15 International Symposium on Computer Architecture, </booktitle> <pages> pages 410-421, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Simulation results of three parallel applications on the VMP multiprocessor also show that no single block size is best for all programs <ref> [6] </ref>. The best block size for the three programs considered varied between 32 and 132 bytes.
Reference: [7] <author> S.J. Eggers and R.H. Katz. </author> <title> A characterization of sharing in parallel programs and its application to coherency protocol evaluation. </title> <booktitle> In 15 International Symposium on Computer Architecture, </booktitle> <pages> pages 373-383, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Several studies have shown that the performance of coherent caches depends on the relationship between the granularity of sharing and locality exhibited by a program and the cache block size (the amount of data stored in a single cache line) <ref> [7; 8; 11] </ref>. Large cache blocks improve application performance when there is substantial processor and spatial locality (i.e. all words allocated to one cache line are accessed by one processor in some window of time). <p> The mismatch between block size and program sharing patterns has been studied both in the context of bus-based machines and scalable multiprocessors. Eggers and Katz <ref> [7; 8] </ref> examined the effect of sharing on cache and bus performance using traces from four application programs. They showed that miss ratios for some parallel programs increase when the cache line size increases, while for other programs the miss ratio decreases with an increase in line size.
Reference: [8] <author> S.J. Eggers and R.H. Katz. </author> <title> The effect of sharing on the cache and bus performance of parallel programs. </title> <booktitle> In Proc. 3rd ASPLOS, </booktitle> <pages> pages 257-270, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Several studies have shown that the performance of coherent caches depends on the relationship between the granularity of sharing and locality exhibited by a program and the cache block size (the amount of data stored in a single cache line) <ref> [7; 8; 11] </ref>. Large cache blocks improve application performance when there is substantial processor and spatial locality (i.e. all words allocated to one cache line are accessed by one processor in some window of time). <p> However, if cache blocks are too large, then false sharing [15] is introduced (wherein two data items not being shared happen to reside in the same cache line), which also increases the number of invalidations <ref> [8] </ref>. Since the granularity of sharing can vary widely among programs, the likelihood of a mismatch between the block size and the object sizes used by some programs is high. <p> The mismatch between block size and program sharing patterns has been studied both in the context of bus-based machines and scalable multiprocessors. Eggers and Katz <ref> [7; 8] </ref> examined the effect of sharing on cache and bus performance using traces from four application programs. They showed that miss ratios for some parallel programs increase when the cache line size increases, while for other programs the miss ratio decreases with an increase in line size. <p> One general technique to achieve such a match is to structure the application based on knowledge of the cache organization, allocating separate objects to sepa rate cache lines [15], grouping objects according to sharing patterns [5; 9], and avoiding fine-grain sharing <ref> [8] </ref>. Using a variety of program restructuring techniques, Cheriton et al [5] were able to achieve an order of magnitude improvement in the cache miss ratio for shared data in large (128 bytes) cache lines.
Reference: [9] <author> S.J. Eggers and Jeremiassen T.E. </author> <title> Eliminating false sharing. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> De-cember </month> <year> 1990. </year>
Reference-contexts: One general technique to achieve such a match is to structure the application based on knowledge of the cache organization, allocating separate objects to sepa rate cache lines [15], grouping objects according to sharing patterns <ref> [5; 9] </ref>, and avoiding fine-grain sharing [8]. Using a variety of program restructuring techniques, Cheriton et al [5] were able to achieve an order of magnitude improvement in the cache miss ratio for shared data in large (128 bytes) cache lines.
Reference: [10] <author> A. Gupta, J. Hennessy, K. Gharachorloo, Todd Mowry, and W.D. Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In 18 International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The incremental cost of sending an additional word in a message is A. This parameter is equal to 10 in our simulations. These parameter values are representative of the costs we would expect to incur in a scalable multiprocessor. As in the DASH multiprocessor <ref> [10] </ref>, a cache fill from a remote node can be expensive.
Reference: [11] <author> A. Gupta and W.D. Weber. </author> <title> Analysis of cache invalidation patterns in shared-memory multiprocessors. </title> <booktitle> In Cache and Interconnect Architectures in Multiprocessors, </booktitle> <pages> pages 83-108. </pages> <publisher> Kulwer Academic Publishers, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Several studies have shown that the performance of coherent caches depends on the relationship between the granularity of sharing and locality exhibited by a program and the cache block size (the amount of data stored in a single cache line) <ref> [7; 8; 11] </ref>. Large cache blocks improve application performance when there is substantial processor and spatial locality (i.e. all words allocated to one cache line are accessed by one processor in some window of time). <p> cache blocks are 0 This research was supported by the National Science Foundation under grant CDA-8822724. smaller than the data objects used on a per-processor basis, then accessing a single object can generate references to multiple cache lines, increasing the number of invalidations and decreasing the beneficial effects of prefetching <ref> [11] </ref>. However, if cache blocks are too large, then false sharing [15] is introduced (wherein two data items not being shared happen to reside in the same cache line), which also increases the number of invalidations [8]. <p> Simulation results of three parallel applications on the VMP multiprocessor also show that no single block size is best for all programs [6]. The best block size for the three programs considered varied between 32 and 132 bytes. Gupta and Weber <ref> [11] </ref> examined the effect of cache block size on the number and size of invalidations in a multiprocessor system with a directory-based cache coherency protocol. 1 With four-byte blocks, most shared writes generated one invalidation.
Reference: [12] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In 17 International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In addition, each tag cache entry contains a field for the size of the data block. Both processor caches and tag caches are fully associative. 3.2 Coherency Protocol Data coherence is maintained using a variant of the write-invalidate, ownership protocol for the DASH multiprocessor <ref> [12] </ref>. Under this protocol, each data block is assigned to a memory module. The node containing the memory module to which a given data block is assigned is called the home node for the data block.
Reference: [13] <author> B.W. O'Krafka and R.A. </author> <title> Newton. An empirical evaluation of two memory-efficient directory methods. </title> <booktitle> In 17 International Symposium on Computer Architecture, </booktitle> <pages> pages 138-147, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The first level consists of the local processor caches. Local memory is the second level. The third level includes the caches of all other nodes. The fourth level consists of all nonlocal memory modules. Each memory module contains random access memory and a tag cache <ref> [13] </ref>, which is used to implement a dis tributed directory. Each tag cache is indexed by block address. A tag cache line maintains a bit vector of nodes that currently have a copy of the data block indexed by the tag cache line.
Reference: [14] <author> J.P. Singh, W-D. Weber, and A. Gupta. </author> <title> Splash: Stan-ford parallel applications for shared-memory. </title> <type> Technical report, </type> <institution> Department of Computer Science, Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Gauss is a Gaussian elimination program. All of these programs exhibit coarse-grain parallelism and good processor locality. * SPLASH applications Two programs were taken from the SPLASH suite <ref> [14] </ref>. These programs were written in C using the Argonne macros for parallel con structs [3]. S-cholesky performs parallel factorization of a sparse matrix; S-mp3d solves a problem in fluid flow simulation.
Reference: [15] <author> J. Torrellas, M.S. Lam, and J.L. Hennessy. </author> <title> Shared data placement optimizations to reduce multiprocessor cache miss rates. </title> <booktitle> In Proc. 1990 ICPP Volume II, </booktitle> <pages> pages 266-270, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: However, if cache blocks are too large, then false sharing <ref> [15] </ref> is introduced (wherein two data items not being shared happen to reside in the same cache line), which also increases the number of invalidations [8]. <p> One general technique to achieve such a match is to structure the application based on knowledge of the cache organization, allocating separate objects to sepa rate cache lines <ref> [15] </ref>, grouping objects according to sharing patterns [5; 9], and avoiding fine-grain sharing [8]. Using a variety of program restructuring techniques, Cheriton et al [5] were able to achieve an order of magnitude improvement in the cache miss ratio for shared data in large (128 bytes) cache lines.
References-found: 15

