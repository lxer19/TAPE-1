URL: ftp://ftp.dcs.exeter.ac.uk/pub/parallel/models/parcom.ps.Z
Refering-URL: http://www.dcs.exeter.ac.uk/reports/reports.html
Root-URL: 
Title: A Parallelizing Compiler and Run Time System based on Optimistic Execution  
Author: Adam Back and Stephen J. Turner 
Keyword: parallel programming, parallelizing compiler, data dependence, optimistic execution, Time Warp, virtual time.  
Address: Road Exeter EX4 4PT England  
Affiliation: Parallel Systems Research Group Department of Computer Science University of Exeter Prince of Wales  
Abstract: Most parallelizing compilers to date have been based broadly on compile-time data flow analysis. This reliance on static information imposes an overly restrictive view of the parallelism available in a program. The static approach cannot in general provide exact data dependence analysis and must therefore adopt the worst case view: if it is possible for a data dependency to occur, then it must be assumed always to occur. This means that sections of code will be treated as sequential which might otherwise have been parallelized. The use of optimistic execution techniques is seen as a new direction in par-allelization technology. Optimistic methods such as the "Time Warp" mechanism have been successfully used in parallel discrete event simulation. In this paper, we discuss the use of optimistic execution as a way of parallelizing programs written in a general purpose programming language. We present the design and implementation of a compiler and run time system which uses optimistic techniques to execute "sequential" C ++ programs on a multicomputer architecture. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Back and S. Turner. </author> <title> Time-stamp generation for optimistic parallel computing. </title> <booktitle> In Proceedings of the 28th Annual Simulation Symposium, Phoenix, Arizona, </booktitle> <pages> pages 144-153. </pages> <publisher> IEEE Press, </publisher> <month> April </month> <year> 1995. </year>
Reference-contexts: Only if both the fractional values and the length of time-stamps are equal are the time-stamps considered equal, that is, trailing zeros become significant in the ordering, so that 0:10 6= 0:1. A full discussion of the time-stamp allocation mechanism can be found in <ref> [1] </ref>. 11 6 Run Time System 6.1 Server Objects and Roll-Back A server object is a remote procedure call (RPC) server for an object. <p> Message (11) requests that i be incremented. If1::eval1 requests the value of v [i] in message (13), the value of i is chosen to be 1 in the diagram so the value of v <ref> [1] </ref> is asked for. The request for the value of v [1] specifies that the result be sent to if1:eval2, this allows the if1 server object to resume the logic of the if statement when the value of v [i] has been obtained. <p> Message (11) requests that i be incremented. If1::eval1 requests the value of v [i] in message (13), the value of i is chosen to be 1 in the diagram so the value of v <ref> [1] </ref> is asked for. The request for the value of v [1] specifies that the result be sent to if1:eval2, this allows the if1 server object to resume the logic of the if statement when the value of v [i] has been obtained. It may serve other RPC requests before receiving the value of v [i].
Reference: [2] <author> S. Bellenot. </author> <title> Global Virtual Time algorithms. </title> <booktitle> In Proc. SCS Distributed Simulation Conference, </booktitle> <pages> pages 122-127, </pages> <year> 1990. </year>
Reference-contexts: The progress of the simulation is measured by global virtual time <ref> [2] </ref> or GVT (see figure 1), which is the minimum of the logical clocks (and time-stamps of messages in transit). No event with a time-stamp smaller than GVT will ever be rolled back, so storage used by such events (e.g. saved states and messages) can be discarded.
Reference: [3] <author> F. Bodin, P. Beckman, D. Gannon, J. Gotwals, S. Narayana, S. Srinivas, and B. Win-nicka. Sage++: </author> <title> An object-oriented toolkit and class library for building Fortran and C++ restructuring tools. Object Oriented Numerics, </title> <year> 1994. </year>
Reference-contexts: The asynchronous nature of the remote procedure calls, and the program transformations needed to enable this are an important part of the design, and are what creates the parallelism in the execution. Our compiler currently makes use of source-to-source transformations using the sage++ transformation tool <ref> [3] </ref>. Remote procedure call messages are time-stamped, in such a way that if the program were executed in increasing time-stamp order, an execution semantically indistinguishable from the sequential execution is obtained, just as executing a simulation using the "Time Warp" mechanism gives exactly the same results as a sequential simulation.
Reference: [4] <author> J. Boyle, R. Butler, T. Disz, B. Glickfeld, E. Lusk, R. Overbeek, J. Patterson, and R. Stevens. </author> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart, and Winston, </publisher> <year> 1987. </year>
Reference-contexts: Few would disagree that parallel programming is more difficult than sequential: there is the extra dimension of dealing with many parallel processes and their synchronization, and the ensuing dead-lock, live-lock, and communication problems. In many cases, the parallel programming options available are communications libraries (for example, PVM [11], p4 <ref> [4] </ref> or MPI [8]) which support a message-passing style of programming, or extensions to existing languages such as in High Performance Fortran [7]. For this reason automatic parallelization is an important area. <p> Messages are time-stamped in accordance with the requirements of the optimistic execution scheme. The message passing system was built on top of the p4 message passing library <ref> [4] </ref>. The p4 model was chosen as a portability layer [18], to ensure that the system could be ported to a wide range of parallel computers. A description of the p4 library is given in [5].
Reference: [5] <author> R. Butler and E. Lusk. </author> <title> User's Guide to the p4 Parallel Programming System. </title> <type> Technical report, </type> <institution> ANL-92/17, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: The message passing system was built on top of the p4 message passing library [4]. The p4 model was chosen as a portability layer [18], to ensure that the system could be ported to a wide range of parallel computers. A description of the p4 library is given in <ref> [5] </ref>. It has the functionality of tagged messages, where messages are buffered at the receiving process. Messages may therefore be received out of order, the call to receive a message has as a parameter the tag type to receive.
Reference: [6] <author> K.M. Chandy and J. Misra. </author> <title> Distributed Simulation: A case study in design and verification of distributed programs. </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> SE5(5):440-452, </volume> <year> 1979. </year>
Reference-contexts: In PDES [10], there are broadly two approaches taken to ensuring that causality (the principle that events in the future cannot affect events in the past) is preserved in the simulation: conservative, and optimistic. The conservative approach <ref> [6] </ref> strictly avoids the possibility of any causality violation ever occurring.
Reference: [7] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <type> Technical report, </type> <institution> Rice University, Houston, Texas, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: In many cases, the parallel programming options available are communications libraries (for example, PVM [11], p4 [4] or MPI [8]) which support a message-passing style of programming, or extensions to existing languages such as in High Performance Fortran <ref> [7] </ref>. For this reason automatic parallelization is an important area. The motivation for using optimistic execution techniques in parallelizing code stems from the restrictions associated with compilers based on static data dependence analysis alone.
Reference: [8] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, Tennessee, </institution> <month> May </month> <year> 1994. </year> <month> 19 </month>
Reference-contexts: In many cases, the parallel programming options available are communications libraries (for example, PVM [11], p4 [4] or MPI <ref> [8] </ref>) which support a message-passing style of programming, or extensions to existing languages such as in High Performance Fortran [7]. For this reason automatic parallelization is an important area.
Reference: [9] <author> R.M. Fujimoto. </author> <title> The virtual time machine. </title> <booktitle> SPAA (Symposium on Parallel Algorithms and Architectures), </booktitle> <pages> pages 199-208, </pages> <year> 1989. </year>
Reference-contexts: A special case of lazy re-evaluation which is particularly significant involves the handling of read-only methods. Here, it is known that the state will not be affected and a straggler message for a read-only method may be always be processed without re-executing the rolled back events <ref> [9] </ref>. 6.4 Memory Management The run time system also contains a specialised set of memory management routines, which have the property of being able to undo memory management operations.
Reference: [10] <author> R.M. Fujimoto. </author> <title> Parallel discrete event simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Thus the programmer need not even be aware that there is a parallel implementation, the effect should be transparent. The optimistic approach adopted is based on a technique used successfully in the field of parallel discrete event simulation (PDES). In PDES <ref> [10] </ref>, there are broadly two approaches taken to ensuring that causality (the principle that events in the future cannot affect events in the past) is preserved in the simulation: conservative, and optimistic. The conservative approach [6] strictly avoids the possibility of any causality violation ever occurring. <p> The simulation moves forwards in simulated time by jumping from the time-stamp of one event to the next. This is in contrast to time-driven simulation methods where time moves forwards uniformly. In parallel discrete event simulation <ref> [10] </ref> the physical system is modelled by a set of logical processes which correspond to interacting objects in the physical system. The interactions between the physical objects are modelled by the exchange of time-stamped event messages. Each process has a logical clock which denotes the simulated time of that object. <p> After the roll-back, execution resumes from that point in time along new execution paths. An event that causes roll-back may require the mechanism to perform two actions: restoring the state of the object and cancelling all intermediate side effects by "unsending" previously sent messages <ref> [10] </ref>. The first action is accomplished by regularly saving the object's state, and restoring an old state on roll-back. "Unsending" a previously sent message is accomplished by sending an anti-message that annihilates the original when it reaches its destination. <p> The uniqueness of time-stamps is a feature of our time-stamp allocation scheme, but is not generally the case with optimistic PDES systems. 6.3 Optimizations One optimization of this scheme is lazy cancellation <ref> [10] </ref>. This optimization stems from the fact that sometimes, even though a message arrives late, no difference in sent messages results. If this turns out to be the case, the anti-messages may have caused unnecessary roll-backs. <p> Another optimization to the optimistic execution scheme is lazy re-evaluation <ref> [10] </ref>: this says that even when an object has been rolled back to take account of a straggler message, it is not certain that the object's state will be affected.
Reference: [11] <author> A. Gest, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM3 User's Guide and Reference Manual. </title> <type> Technical report, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, Tennessee 37831, ORNL/TM-12187., </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Few would disagree that parallel programming is more difficult than sequential: there is the extra dimension of dealing with many parallel processes and their synchronization, and the ensuing dead-lock, live-lock, and communication problems. In many cases, the parallel programming options available are communications libraries (for example, PVM <ref> [11] </ref>, p4 [4] or MPI [8]) which support a message-passing style of programming, or extensions to existing languages such as in High Performance Fortran [7]. For this reason automatic parallelization is an important area.
Reference: [12] <author> D.R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Optimistic simulations are more aggressive than conservative ones, and are thus able, in general, to exploit more parallelism. The most widely known optimistic execution technique is the "Time Warp" mechanism based on the virtual time paradigm <ref> [12] </ref> and this mechanism forms the basis of our system. Optimistic execution techniques can be used effectively on distributed memory architectures [13, 19]. Parallelism is achieved by distributing the objects over the different nodes of a parallel multicomputer, and events are scheduled on remote objects by sending asynchronous messages.
Reference: [13] <author> JPL. </author> <title> Time Warp Operating System User's Manual. </title> <institution> Jet Propulsion Laboratory, </institution> <year> 1991. </year>
Reference-contexts: The most widely known optimistic execution technique is the "Time Warp" mechanism based on the virtual time paradigm [12] and this mechanism forms the basis of our system. Optimistic execution techniques can be used effectively on distributed memory architectures <ref> [13, 19] </ref>. Parallelism is achieved by distributing the objects over the different nodes of a parallel multicomputer, and events are scheduled on remote objects by sending asynchronous messages.
Reference: [14] <author> L. Lamport. </author> <title> The parallel execution of DO loops. </title> <journal> Communications of the A.C.M., </journal> <volume> 17(2) </volume> <pages> 83-93, </pages> <month> Feb </month> <year> 1974. </year>
Reference-contexts: Consequently, few parallelizing compilers try, or are very successful at, predicting program execution outside of a certain few narrowly defined constructs. Early vectorizing FORTRAN compilers were amongst the first parallelizing compilers. The techniques used were mostly centered around parallelizing "DO loops" <ref> [14] </ref>. This technique works well for the narrow range of applications where the loops have independence between different iterations, or where transformations can be performed to produce usable independence. For example, parallelizing matrix operations is an approach which works well on vector processor based parallel computers.
Reference: [15] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: In many ways, the virtual time paradigm is the inverse of Lamport's algorithm <ref> [15] </ref>: we assume that every event is labelled with a clock value from some totally ordered time-scale in a manner consistent with causality. A partial ordering may then be obtained which allows a fast parallel execution.
Reference: [16] <author> A. </author> <title> Palaniswamy and P.A. Wilsey. An analytical comparison of periodic checkpointing and incremental state saving. </title> <booktitle> In Proc. 7th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 127-134, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Implemented simplisticly, state-saving can be expensive. Care must be taken to ensure that a workable balance is achieved and that the cost of the state-saving mechanism is not too high compared to the work done between state saving. There are two basic approaches to state-saving: periodic and incremental <ref> [16] </ref>: 1. Periodic State-saving is when the state of the object is saved at regular time-intervals. A complete snapshot of the object's state is saved. As the state is only saved periodically, there will be points in logical time for which no state is known.
Reference: [17] <author> R. Ronngren. </author> <title> Performance Issues in Discrete Event Simulation. </title> <institution> Royal Institute of Technology, Stockholm, </institution> <year> 1995. </year>
Reference-contexts: If a causality violation is detected, an anti-message can be sent by just sending the logged time-stamps of the messages to cancel. This is an optimization proposed by Ronngren <ref> [17] </ref>: only the time-stamp is required, as this is sufficient for cancellation where time-stamps are unique. The uniqueness of time-stamps is a feature of our time-stamp allocation scheme, but is not generally the case with optimistic PDES systems. 6.3 Optimizations One optimization of this scheme is lazy cancellation [10].
Reference: [18] <author> S.J. Turner and A. </author> <title> Back. A T9000 implementation of the p4 parallel programming model. </title> <booktitle> In Proceedings of the North American Transputer User Group, </booktitle> <year> 1994. </year>
Reference-contexts: Messages are time-stamped in accordance with the requirements of the optimistic execution scheme. The message passing system was built on top of the p4 message passing library [4]. The p4 model was chosen as a portability layer <ref> [18] </ref>, to ensure that the system could be ported to a wide range of parallel computers. A description of the p4 library is given in [5]. It has the functionality of tagged messages, where messages are buffered at the receiving process.
Reference: [19] <author> S.J Turner, M. Damitio, and S. Trivett. </author> <title> Distributed simulation with a transputer version of the time warp operating system. </title> <booktitle> In Proceedings of Transputers '94, </booktitle> <pages> pages 39-54. </pages> <publisher> IOS Press, </publisher> <year> 1994. </year>
Reference-contexts: The most widely known optimistic execution technique is the "Time Warp" mechanism based on the virtual time paradigm [12] and this mechanism forms the basis of our system. Optimistic execution techniques can be used effectively on distributed memory architectures <ref> [13, 19] </ref>. Parallelism is achieved by distributing the objects over the different nodes of a parallel multicomputer, and events are scheduled on remote objects by sending asynchronous messages.
Reference: [20] <author> L.G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Comm. A.C.M., </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: The message passing system is required to transport the messages between server objects. To facilitate parallel slackness <ref> [20] </ref>, many objects are placed on each processor, so the message passing system must also be able to handle messages between processes on the same processor. Messages are time-stamped in accordance with the requirements of the optimistic execution scheme.
References-found: 20

