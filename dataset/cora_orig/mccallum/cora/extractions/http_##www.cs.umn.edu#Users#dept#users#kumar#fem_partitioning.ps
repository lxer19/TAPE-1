URL: http://www.cs.umn.edu/Users/dept/users/kumar/fem_partitioning.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: kumar@cs:umn:edu  
Phone: Tel: 612-624-8023  
Title: Scalability Analysis of Partitioning Strategies for Finite Element Graphs.  
Author: Grama Y. Ananth and Vipin Kumar Vipin Kumar Ananth Y. Grama ananth@cs:umn:edu 
Note: Corresponding Author:  Presenting Author:  
Date: April 3, 1992  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science, University of Minnesota  
Abstract: Issues of partitioning Finite Element Graphs are central to parallel formulations of the Finite Element Method. Due to the nature of the problem, optimal partitioning schemes should conform to three basic criteria: equal load on all processors; locality of communication; and maximum computation to communication ratio associated with each partition. Many techniques have been presented in literature which achieve these to different degrees. This paper presents scalability analysis of three partitioning strategies, namely, striped partitioning, binary decomposition, and scattered decomposition. The analysis is performed using the Isoefficiency metric, which helps in predicting performance of these schemes on a range of processors and architectures. fl This work was supported by Army Research Office grant # 28408-MA-SDI to the University of Minnesota and by the Army High Performance Computing Research Center at the University of Minnesota. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Aykanat, F. Ozguner, F. Ercal, and P. Sadayappan. </author> <title> Iterative algorithms for solution of large sparse systems of linear equations on hypercubes. </title> <journal> In IEEE Transactions on computers, </journal> <volume> Vol 37, pp.1554 - 1568, </volume> <month> December </month> <year> 1988. </year>
Reference-contexts: The mapping problem in its optimal form is known to be NP complete even for simple models of computation and communication costs [7]. Hence, a number of heuristic approaches have been presented to derive reasonable suboptimal partitions in a reasonable amount of time <ref> [1, 16, 19, 25, 18] </ref>. All of these try to balance the various tradeoffs mentioned. Most of these schemes have been evaluated only on specific parallel computers for certain problems, and from the available experimental results, it is difficult to ascertain relative merits of these schemes.
Reference: [2] <author> M. Behr, T.E. Tezduyar, and H. Higuchi. </author> <title> Wake interference behind two flat plates normal to the flow. </title> <type> Technical report, Tech Report 91-21, </type> <institution> Army High Performance Computing Reseaarch Center, University of Minnesota, </institution> <year> 1991. </year>
Reference-contexts: We present results from the simulations of various partitioning strategies. Figure 2 shows the structure of the meshes used in these simulations. This mesh is used in actual fluid flow computation <ref> [2] </ref>. This mesh was scaled uniformly to generate meshes with elements ranging from 2000 elements to 300,000 elements. Simulations were run for up to 1024 processors connected in a hypercube.
Reference: [3] <author> M.J. Berger and S.H. Bokhari. </author> <title> Partitioning strategy for nonuniform problems on multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(5):570-580, </volume> <year> 1987. </year>
Reference-contexts: However, this is done at the cost of increased communication overheads in intercluster communication for adjoining clusters that do not lie on the same processor. Detailed discussion on this technique is presented in [22, 4, 18]. Binary Decomposition Berger and Bokhari <ref> [3] </ref> propose the use of binary decomposition to partition irregular domains. In this technique, the domain is partitioned into two parts recursively so that the number of elements in each part is identical. This technique ensures a very good load balance.
Reference: [4] <author> D.M.Nichol and J.H.Saltz. </author> <title> An analysis of scatter decomposition. </title> <journal> IEEE Transactions on Computers, </journal> <month> Nov </month> <year> 1990. </year>
Reference-contexts: However, this is done at the cost of increased communication overheads in intercluster communication for adjoining clusters that do not lie on the same processor. Detailed discussion on this technique is presented in <ref> [22, 4, 18] </ref>. Binary Decomposition Berger and Bokhari [3] propose the use of binary decomposition to partition irregular domains. In this technique, the domain is partitioned into two parts recursively so that the number of elements in each part is identical. This technique ensures a very good load balance. <p> In the analysis for Scattered Decomposition, we use results from various related analyses. We briefly review these results first. 5.3.1 Related Analyses. There have been various attempts to characterize the performance of scattered decomposition in the context of various problems. Nichol and Saltz <ref> [4] </ref> use a probabilistic model of workload in one dimension to formally explain the working of scattered decomposition. Assuming correlation for workload to be a convex function of distance, they show that increasing the degree of scattered decomposition does not increase the common processor workload variance.
Reference: [5] <author> Vipin Kumar et. al. </author> <title> Sacalability analysis of load-balancing schemes on parallel computers. </title> <type> Technical report, </type> <institution> Computer Science Department, University of Minnesota, </institution> <note> 1989 (working paper). </note> <author> [6] et.al G. Fox. </author> <title> Solving Problems on Concurrent Computers. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: In particular, it has helped determine optimal load balancing techniques used in tree search based algorithms on various architectures [8, 11, 12, 15]. In this paper, we perform scalability analysis, using the Isoefficiency metric <ref> [13, 5, 14] </ref>, of three partitioning algorithms, namely, striped partitioning, binary decomposition, and scattered decomposition. This helps us determine the relative performance of these schemes over a range of processors, and the effect of communication related parameters on the performance of these schemes.
Reference: [7] <author> M. Garey and D.S. Johnson. </author> <title> Computers and Intractability. </title> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: Optimizing one of these criterion leads to a deterioration with respect to one or more of the other criteria. The mapping problem in its optimal form is known to be NP complete even for simple models of computation and communication costs <ref> [7] </ref>. Hence, a number of heuristic approaches have been presented to derive reasonable suboptimal partitions in a reasonable amount of time [1, 16, 19, 25, 18]. All of these try to balance the various tradeoffs mentioned.
Reference: [8] <author> Ananth Grama, Vipin Kumar, and V. Nageshwara Rao. </author> <title> Experimental evaluation of load balancing techniques for the hypercube. </title> <booktitle> In Proceedings of the Parallel Computing 91 Conference, </booktitle> <year> 1991. </year>
Reference-contexts: Hence, any conclusions drawn on a set of experimental results may become invalid by changes in any one of the above parameters. Scalability analysis is very useful in extrapolating these conclusions <ref> [8, 20, 23, 24] </ref>. The isoefficiency metric has been found to be quite useful in characterizing the scalability of a number of algorithms [9, 14, 17, 21, 23, 24]. <p> The isoefficiency metric has been found to be quite useful in characterizing the scalability of a number of algorithms [9, 14, 17, 21, 23, 24]. In particular, it has helped determine optimal load balancing techniques used in tree search based algorithms on various architectures <ref> [8, 11, 12, 15] </ref>. In this paper, we perform scalability analysis, using the Isoefficiency metric [13, 5, 14], of three partitioning algorithms, namely, striped partitioning, binary decomposition, and scattered decomposition.
Reference: [9] <author> Anshul Gupta and Vipin Kumar. </author> <title> On the scalability of FFT on parallel computers. </title> <booktitle> In Proceedings of the Frontiers 90 Conference on Massively Parallel Computation, </booktitle> <month> October </month> <year> 1990. </year> <note> An extended version of the paper is available as a technical report from the Department of Computer Science and Army High Performance Computing Research Center, </note> <institution> University of Minnesota. </institution> <month> 21 </month>
Reference-contexts: Scalability analysis is very useful in extrapolating these conclusions [8, 20, 23, 24]. The isoefficiency metric has been found to be quite useful in characterizing the scalability of a number of algorithms <ref> [9, 14, 17, 21, 23, 24] </ref>. In particular, it has helped determine optimal load balancing techniques used in tree search based algorithms on various architectures [8, 11, 12, 15].
Reference: [10] <author> C. P. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11, NO. 10 </volume> <pages> 1001-1016, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: Improved load balance is thus achieved at the expense of increased communication overheads. Kruskal and Weiss <ref> [10] </ref> analyze the problem of allocating independent subtasks to processors. Though this work does not directly apply to the problem of analyzing scattered decomposition, we will show how this can be used to gain an estimate of the overhead incurred in balancing load using this technique.
Reference: [11] <author> Vipin Kumar, Ananth Grama, and V. Nageshwara Rao. </author> <title> Scalable load balancing techniques for parallel computers. </title> <type> Technical report, Tech Report 91-55, </type> <institution> Computer Science Department, University of Minnesota, </institution> <year> 1991. </year>
Reference-contexts: The isoefficiency metric has been found to be quite useful in characterizing the scalability of a number of algorithms [9, 14, 17, 21, 23, 24]. In particular, it has helped determine optimal load balancing techniques used in tree search based algorithms on various architectures <ref> [8, 11, 12, 15] </ref>. In this paper, we perform scalability analysis, using the Isoefficiency metric [13, 5, 14], of three partitioning algorithms, namely, striped partitioning, binary decomposition, and scattered decomposition.
Reference: [12] <author> Vipin Kumar and V. Nageshwara Rao. </author> <title> Parallel depth-first search, part II: Analysis. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16 </volume> (6):501-519, 1987. 
Reference-contexts: The isoefficiency metric has been found to be quite useful in characterizing the scalability of a number of algorithms [9, 14, 17, 21, 23, 24]. In particular, it has helped determine optimal load balancing techniques used in tree search based algorithms on various architectures <ref> [8, 11, 12, 15] </ref>. In this paper, we perform scalability analysis, using the Isoefficiency metric [13, 5, 14], of three partitioning algorithms, namely, striped partitioning, binary decomposition, and scattered decomposition. <p> If a parallel algorithm is used to solve a given problem instance of a fixed size (i.e., T e ), then the efficiency decreases as P increases. This property is true of all parallel algorithms. For a large class of parallel algorithms (e.g., parallel DFS <ref> [12] </ref>, parallel shortest path algorithms the following additional property is also true: * For any given number P of processors, the efficiency of the parallel algorithm goes up mono tonically (i.e., it never goes down, and approaches a constant e, such that 0 &lt; e 1) if it is used to
Reference: [13] <author> Vipin Kumar and V. Nageshwara Rao. </author> <title> Load balancing on the hypercube architecture. </title> <booktitle> In Proceedings of the 1989 Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <pages> pages 603-608, </pages> <year> 1989. </year>
Reference-contexts: In particular, it has helped determine optimal load balancing techniques used in tree search based algorithms on various architectures [8, 11, 12, 15]. In this paper, we perform scalability analysis, using the Isoefficiency metric <ref> [13, 5, 14] </ref>, of three partitioning algorithms, namely, striped partitioning, binary decomposition, and scattered decomposition. This helps us determine the relative performance of these schemes over a range of processors, and the effect of communication related parameters on the performance of these schemes.
Reference: [14] <author> Vipin Kumar and Vineet Singh. </author> <title> Scalability of Parallel Algorithms for the All-Pairs Shortest Path Problem: A Summary of Results. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1990. </year> <note> Extended version available as a technical report from the department of computer science, </note> <institution> University of Minnesota, Minneapolis, </institution> <note> MN 55455 and as MCC TR ACT-OODS-058-90. </note>
Reference-contexts: Scalability analysis is very useful in extrapolating these conclusions [8, 20, 23, 24]. The isoefficiency metric has been found to be quite useful in characterizing the scalability of a number of algorithms <ref> [9, 14, 17, 21, 23, 24] </ref>. In particular, it has helped determine optimal load balancing techniques used in tree search based algorithms on various architectures [8, 11, 12, 15]. <p> In particular, it has helped determine optimal load balancing techniques used in tree search based algorithms on various architectures [8, 11, 12, 15]. In this paper, we perform scalability analysis, using the Isoefficiency metric <ref> [13, 5, 14] </ref>, of three partitioning algorithms, namely, striped partitioning, binary decomposition, and scattered decomposition. This helps us determine the relative performance of these schemes over a range of processors, and the effect of communication related parameters on the performance of these schemes.
Reference: [15] <author> V. Nageshwara Rao and Vipin Kumar. </author> <title> Parallel depth-first search, part I: Implementation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16 </volume> (6):479-499, 1987. 
Reference-contexts: The isoefficiency metric has been found to be quite useful in characterizing the scalability of a number of algorithms [9, 14, 17, 21, 23, 24]. In particular, it has helped determine optimal load balancing techniques used in tree search based algorithms on various architectures <ref> [8, 11, 12, 15] </ref>. In this paper, we perform scalability analysis, using the Isoefficiency metric [13, 5, 14], of three partitioning algorithms, namely, striped partitioning, binary decomposition, and scattered decomposition.
Reference: [16] <author> J. W. Parker, T. Cwik, R. Ferraro, P. Liewer, P. Lyster, and J. Patterson. </author> <title> Helmholtz finite elements performance on mark iii and intel ipsc/860 hypercubes. </title> <booktitle> In Proceedings of 6th DMCC conference, </booktitle> <address> Portland, Oregon, </address> <year> 1991. </year>
Reference-contexts: The mapping problem in its optimal form is known to be NP complete even for simple models of computation and communication costs [7]. Hence, a number of heuristic approaches have been presented to derive reasonable suboptimal partitions in a reasonable amount of time <ref> [1, 16, 19, 25, 18] </ref>. All of these try to balance the various tradeoffs mentioned. Most of these schemes have been evaluated only on specific parallel computers for certain problems, and from the available experimental results, it is difficult to ascertain relative merits of these schemes.
Reference: [17] <author> S. Ranka and S. Sahni. </author> <title> Hypercube Algorithms for Image Processing and Pattern Recognition. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Scalability analysis is very useful in extrapolating these conclusions [8, 20, 23, 24]. The isoefficiency metric has been found to be quite useful in characterizing the scalability of a number of algorithms <ref> [9, 14, 17, 21, 23, 24] </ref>. In particular, it has helped determine optimal load balancing techniques used in tree search based algorithms on various architectures [8, 11, 12, 15].
Reference: [18] <author> R.Morrison and S.Otto. </author> <title> The scattered decomposition for finite elements. </title> <journal> Journal of Scientific Computing., </journal> <volume> 2. No.1, </volume> <month> March 87. </month>
Reference-contexts: The mapping problem in its optimal form is known to be NP complete even for simple models of computation and communication costs [7]. Hence, a number of heuristic approaches have been presented to derive reasonable suboptimal partitions in a reasonable amount of time <ref> [1, 16, 19, 25, 18] </ref>. All of these try to balance the various tradeoffs mentioned. Most of these schemes have been evaluated only on specific parallel computers for certain problems, and from the available experimental results, it is difficult to ascertain relative merits of these schemes. <p> This technique thus optimizes criteria 2 and 3 at the expense of 1. Scattered Decomposition Scattered Decomposition (also referred to as modular mapping) is a technique which has been applied extensively to decompose highly irregular domains <ref> [6, 22, 18] </ref>. This method tries to balance load by partitioning the domain into a large number of rectangular clusters ( r ) such that r &gt;> P (number of processors). Each processor is now responsible for processing a disjoint set of r=P such clusters. <p> However, this is done at the cost of increased communication overheads in intercluster communication for adjoining clusters that do not lie on the same processor. Detailed discussion on this technique is presented in <ref> [22, 4, 18] </ref>. Binary Decomposition Berger and Bokhari [3] propose the use of binary decomposition to partition irregular domains. In this technique, the domain is partitioned into two parts recursively so that the number of elements in each part is identical. This technique ensures a very good load balance.
Reference: [19] <author> P. Sadayappan and F. Ercal. </author> <title> Mapping of finite element graphs onto processor meshes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36:1408-1424, </volume> <year> 1987. </year>
Reference-contexts: The mapping problem in its optimal form is known to be NP complete even for simple models of computation and communication costs [7]. Hence, a number of heuristic approaches have been presented to derive reasonable suboptimal partitions in a reasonable amount of time <ref> [1, 16, 19, 25, 18] </ref>. All of these try to balance the various tradeoffs mentioned. Most of these schemes have been evaluated only on specific parallel computers for certain problems, and from the available experimental results, it is difficult to ascertain relative merits of these schemes. <p> These can be controlled to a certain extent by alternating the orientation of the partitioning cut. This technique optimizes criterion 3 and allows a tradeoff between criteria 1 and 2. There have been several other techniques that have also been proposed in literature. Sadayappan et al. <ref> [19] </ref> propose a 2-D decomposition scheme with boundary refinement. This technique works in two steps, first the mesh is partitioned into pieces which might have unequal load in terms of nodes, but the communication involved is close to optimal.
Reference: [20] <author> Vineet Singh, Vipin Kumar, Gul Agha, and Chris Tomlinson. </author> <title> Scalability of parallel sorting on mesh multicomputers. </title> <type> Technical Report ACT-SPA-298-90, </type> <institution> Microelectronics and Computer Technology Corp., Austin,TX, </institution> <year> 1990. </year> <note> Also available as a technical report (number TR 90-45) from the department of computer science, </note> <institution> University of Minnesota, </institution> <address> Minneapolis, MN 55455. </address>
Reference-contexts: Hence, any conclusions drawn on a set of experimental results may become invalid by changes in any one of the above parameters. Scalability analysis is very useful in extrapolating these conclusions <ref> [8, 20, 23, 24] </ref>. The isoefficiency metric has been found to be quite useful in characterizing the scalability of a number of algorithms [9, 14, 17, 21, 23, 24].
Reference: [21] <author> Vineet Singh, Vipin Kumar, Gul Agha, and Chris Tomlinson. </author> <title> Scalability of parallel sorting on mesh multicomputers. </title> <booktitle> In Proceedings of the Fifth International Parallel Processing Symposium, </booktitle> <month> 22 March </month> <year> 1991. </year> <note> Extended version available as a technical report (number TR 90-45) from the department of computer science, </note> <institution> University of Minnesota, Minneapolis, </institution> <note> MN 554 55, and as TR ACT-SPA-298-90 from MCC, Austin, Texas. </note>
Reference-contexts: Scalability analysis is very useful in extrapolating these conclusions [8, 20, 23, 24]. The isoefficiency metric has been found to be quite useful in characterizing the scalability of a number of algorithms <ref> [9, 14, 17, 21, 23, 24] </ref>. In particular, it has helped determine optimal load balancing techniques used in tree search based algorithms on various architectures [8, 11, 12, 15].
Reference: [22] <author> Winifred Williams. </author> <title> Load balancing on hypercubes : A preliminary look. </title> <booktitle> In Hypercubes 87., </booktitle> <year> 1987. </year>
Reference-contexts: This technique thus optimizes criteria 2 and 3 at the expense of 1. Scattered Decomposition Scattered Decomposition (also referred to as modular mapping) is a technique which has been applied extensively to decompose highly irregular domains <ref> [6, 22, 18] </ref>. This method tries to balance load by partitioning the domain into a large number of rectangular clusters ( r ) such that r &gt;> P (number of processors). Each processor is now responsible for processing a disjoint set of r=P such clusters. <p> However, this is done at the cost of increased communication overheads in intercluster communication for adjoining clusters that do not lie on the same processor. Detailed discussion on this technique is presented in <ref> [22, 4, 18] </ref>. Binary Decomposition Berger and Bokhari [3] propose the use of binary decomposition to partition irregular domains. In this technique, the domain is partitioned into two parts recursively so that the number of elements in each part is identical. This technique ensures a very good load balance.
Reference: [23] <author> Jinwoon Woo and Sartaj Sahni. </author> <title> Computing biconnected components on a hypercube. </title> <type> Technical Report TR 89-7, </type> <institution> Computer Science Department, University of Minnesota, </institution> <address> Minneapolis, MN 55455, </address> <month> February </month> <year> 1989. </year>
Reference-contexts: Hence, any conclusions drawn on a set of experimental results may become invalid by changes in any one of the above parameters. Scalability analysis is very useful in extrapolating these conclusions <ref> [8, 20, 23, 24] </ref>. The isoefficiency metric has been found to be quite useful in characterizing the scalability of a number of algorithms [9, 14, 17, 21, 23, 24]. <p> Scalability analysis is very useful in extrapolating these conclusions [8, 20, 23, 24]. The isoefficiency metric has been found to be quite useful in characterizing the scalability of a number of algorithms <ref> [9, 14, 17, 21, 23, 24] </ref>. In particular, it has helped determine optimal load balancing techniques used in tree search based algorithms on various architectures [8, 11, 12, 15].
Reference: [24] <author> Jinwoon Woo and Sartaj Sahni. </author> <title> Hypercube computing : connected components. </title> <type> Technical Report TR 88-50, </type> <institution> Computer Science Department, University of Minnesota, </institution> <address> Minneapolis, MN 55455, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Hence, any conclusions drawn on a set of experimental results may become invalid by changes in any one of the above parameters. Scalability analysis is very useful in extrapolating these conclusions <ref> [8, 20, 23, 24] </ref>. The isoefficiency metric has been found to be quite useful in characterizing the scalability of a number of algorithms [9, 14, 17, 21, 23, 24]. <p> Scalability analysis is very useful in extrapolating these conclusions [8, 20, 23, 24]. The isoefficiency metric has been found to be quite useful in characterizing the scalability of a number of algorithms <ref> [9, 14, 17, 21, 23, 24] </ref>. In particular, it has helped determine optimal load balancing techniques used in tree search based algorithms on various architectures [8, 11, 12, 15].
Reference: [25] <author> Sanjay Ranka. Yeh-Ching Chung. </author> <title> Mapping finite element graphs on hypercubes. </title> <booktitle> In Proceedings of Frontiers of Massively Parallel Computations, </booktitle> <year> 1990. </year> <month> 23 </month>
Reference-contexts: The mapping problem in its optimal form is known to be NP complete even for simple models of computation and communication costs [7]. Hence, a number of heuristic approaches have been presented to derive reasonable suboptimal partitions in a reasonable amount of time <ref> [1, 16, 19, 25, 18] </ref>. All of these try to balance the various tradeoffs mentioned. Most of these schemes have been evaluated only on specific parallel computers for certain problems, and from the available experimental results, it is difficult to ascertain relative merits of these schemes. <p> In the second phase, the boundaries are refined in such a way that the load on each processor is also equal, though in doing so, we might be causing extra communication overhead. Chung and Ranka <ref> [25] </ref> propose two other schemes - 2-way striping and Greedy assignment. 2-way striping also works in two phases where the first 5 phase optimizes communication and the second phase tries to achieve load balance using specific load transfer heuristics.
References-found: 24

