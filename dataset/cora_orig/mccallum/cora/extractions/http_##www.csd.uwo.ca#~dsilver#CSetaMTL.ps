URL: http://www.csd.uwo.ca/~dsilver/CSetaMTL.ps
Refering-URL: http://www.csd.uwo.ca/~dsilver/
Root-URL: 
Phone: (519)473-6168  
Title: The Parallel Transfer of Task Knowledge Using Dynamic Learning Rates Based on a Measure of
Author: Daniel L. Silver and Robert E. Mercer 
Date: April 17, 1996  
Web: (dsilver, mercer)csd.uwo.ca  
Address: Ontario London, Ontario, Canada N6A 3K7  
Affiliation: Department of Computer Science, The University of Western  
Abstract-found: 0
Intro-found: 1
Reference: [AM95] <author> Yaser S. Abu-Mostafa, </author> <title> Hints, Neural Computation, </title> <journal> Massachusetts Institute of Technology, </journal> <volume> Vol. 7, </volume> <pages> pp. 639-671, </pages> <year> 1995. </year>
Reference-contexts: In contrast to representational transfer is a form we define as functional. Functional transfer does not involve the explicit assignment of prior task representation to a new task, rather it employs the use of implicit pressures from supplemental training examples <ref> [Sudd90, AM95] </ref>, the parallel learning of related tasks constrained to use a common internal representation [Caru93, Caru95, Baxt95a], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b].
Reference: [Baxt95a] <author> Jonathan Baxter, </author> <title> Learning internal representations, </title> <booktitle> Proceedings of the Eighth International Conference on Computational Learning Theory, </booktitle> <publisher> (to appear) ACM Press, </publisher> <address> Santa Cruz, CA, </address> <year> 1995. </year>
Reference-contexts: Functional transfer does not involve the explicit assignment of prior task representation to a new task, rather it employs the use of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation <ref> [Caru93, Caru95, Baxt95a] </ref>, or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b]. <p> This form of transfer has its greatest value from the perspective of increased generalization performance. Certain methods of functional transfer have also been found to reduce training time. Chief among these methods is the parallel Multiple Task Learning (MTL) paradigm explored recently by Caruana and Baxter <ref> [Caru95, Baxt95a] </ref>. During our research into representational methods of transfer, a fundamental problem | multiple representations (different sets of weight values) for any single task | has frustrated efforts aimed at consolidating task knowledge and selecting analogous source knowledge for a new task.
Reference: [Baxt95b] <author> Jonathan Baxter, </author> <title> Learning Internal Representations, </title> <type> Phd Thesis, </type> <institution> Department of Mathematics and Staistics, The Flinders University of South Australia, Australia, </institution> <year> 1995. </year>
Reference-contexts: Kehoe proposes a multi-layer network model which is composed of two associative mapping functions: a task domain common component and a task specific component. The common component remains available for use in subsequent learning. This concept has recently been formalized by Baxter <ref> [Baxt95b] </ref> as parallel learning and demonstrated by Caruana [Caru95] by a method called Multiple Task Learning, or MTL. An MTL network uses a feed-forward multi-layer network with an output for each task to be learned (see Figure 1). <p> Subsequently, the back-propagated error signal from any output node k is considered to be of equal value to all others. At the point of lowest training error, the MTL network does its best to average the error across all of the output nodes. Baxter <ref> [Baxt95b] </ref> has proven that the number of examples required for learning any one task using an MTL network decreases as a function of the total number of tasks being learned in 4 parallel.
Reference: [Caru93] <author> Richard A. Caruana, </author> <title> Multitask Learning: A Knowledge-Based Source of Inductive Bias, </title> <booktitle> Proceedings of the tenth international conference on machine learning, </booktitle> <publisher> University of Massachusetts, </publisher> <pages> pp. 41-48, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Functional transfer does not involve the explicit assignment of prior task representation to a new task, rather it employs the use of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation <ref> [Caru93, Caru95, Baxt95a] </ref>, or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b].
Reference: [Caru95] <author> Richard A. Caruana, </author> <title> Learning many related tasks at the same time with backpropagation, </title> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 7, pp. 657-664, </pages> <address> San Mateo, CA, </address> <year> 1995. </year>
Reference-contexts: Functional transfer does not involve the explicit assignment of prior task representation to a new task, rather it employs the use of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation <ref> [Caru93, Caru95, Baxt95a] </ref>, or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b]. <p> This form of transfer has its greatest value from the perspective of increased generalization performance. Certain methods of functional transfer have also been found to reduce training time. Chief among these methods is the parallel Multiple Task Learning (MTL) paradigm explored recently by Caruana and Baxter <ref> [Caru95, Baxt95a] </ref>. During our research into representational methods of transfer, a fundamental problem | multiple representations (different sets of weight values) for any single task | has frustrated efforts aimed at consolidating task knowledge and selecting analogous source knowledge for a new task. <p> The common component remains available for use in subsequent learning. This concept has recently been formalized by Baxter [Baxt95b] as parallel learning and demonstrated by Caruana <ref> [Caru95] </ref> by a method called Multiple Task Learning, or MTL. An MTL network uses a feed-forward multi-layer network with an output for each task to be learned (see Figure 1). Training examples contain a set of input attributes as well as a target output for each task.
Reference: [Elli65] <author> H. Ellis, </author> <title> Transfer of Learning, </title> <publisher> MacMillan, </publisher> <address> New York, NY, </address> <year> 1965. </year>
Reference-contexts: The transfer of task knowledge can be considered a major aspect of the problem of learning to learn <ref> [Elli65] </ref> and has close ties to analogical reasoning [Hall89]. Previous work [Silv95] describes a method of sequentially learning a set of related tasks using an architecture of feed-forward networks which we call a Consolidation System.
Reference: [Fahl90] <author> S.E. Fahlman and C. Lebiere, </author> <booktitle> The cascade-correlation learning architecture, Advances in Neural Information Processing Systems 2, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 2, pp. 524-532, </pages> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: In general, we define this form of transfer as being representational since it involves the direct or indirect assignment of known task representation to a new task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Prat93a, Prat94, Shar92, Shav90, Towe90, Fahl90, Sing92, Ring93] </ref>. This form of transfer often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional.
Reference: [Hall89] <author> Rogers P. Hall, </author> <title> Computational approaches to analogical reasoning: A comparative analysis, </title> <journal> Arificial Intelligence, Elseivier Sience Publishers B.V., </journal> <volume> Vol. 39, </volume> <pages> pp. 39-120, </pages> <publisher> North-Holland, </publisher> <year> 1989. </year>
Reference-contexts: The transfer of task knowledge can be considered a major aspect of the problem of learning to learn [Elli65] and has close ties to analogical reasoning <ref> [Hall89] </ref>. Previous work [Silv95] describes a method of sequentially learning a set of related tasks using an architecture of feed-forward networks which we call a Consolidation System.
Reference: [Inc95] <author> The MathWorks Inc, </author> <title> The Student Edition of MATLAB, Version 4, Users Guide, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1995. </year>
Reference-contexts: The learning algorithms we use treat the unknown examples as having no error and subsequently have no effect on learning 3 . The Learning System. Our back-propagation of error system, developed using MATLAB 4.2 <ref> [Inc95] </ref>, can simulate a single task learning (STL) method, a multi-task learning (MTL) method, or the MTL method. In MTL or MTL mode the system is currently capable of parallel learning up to 3 tasks.
Reference: [Jaco88] <author> R.A. Jacobs, </author> <title> Increased rates of convergence through learning rate adaptation, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 295-307, </pages> <year> 1988. </year>
Reference-contexts: It has been used for various purposes by other authors such as <ref> [Jaco88, Vogl88, Naik92] </ref>. 6 The success of MTL rests on a judicious choice for the function k = f (; R k ) and the ability to measure the relatedness of the parallel tasks to the primary task.
Reference: [Keho88] <author> E. James Kehoe, </author> <title> A layered network model of associative learning: Learning to learn and configuration, </title> <journal> Psychological Review, </journal> <volume> Vol. 95, No. 4, </volume> <pages> pp. 411-433, </pages> <year> 1988. </year> <month> 13 </month>
Reference-contexts: We conclude with a discussion of the results and ideas for future work in this area. 1 We formally define a simple measure of relatedness in the following sections. 3 2 Background Kehoe points out in <ref> [Keho88] </ref> that psychological studies of human and animal learning suggest that besides the development of a specific discriminate function which satisfies the task at hand, there is the acquisition of general knowledge of the structural relationship between input attributes.
Reference: [Mitc80] <author> Tom. M. Mitchell, </author> <title> The need for biases in learning generalizations, </title> <booktitle> Readings in Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 184-191, </pages> <address> San Mateo, CA, </address> <year> 1980. </year>
Reference-contexts: This process is a form of knowledge-based inductive bias referred to in the literature as the transfer of knowledge from one or more source tasks to a target or primary task <ref> [Mitc80, Prat93b] </ref>. The transfer of task knowledge can be considered a major aspect of the problem of learning to learn [Elli65] and has close ties to analogical reasoning [Hall89].
Reference: [Mitc93] <author> Tom Mitchell and Sebastian Thrun, </author> <title> Explanation based neural network learning for robot control, </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 5, pp. 287-294, </pages> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation [Caru93, Caru95, Baxt95a], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance.
Reference: [Naik92] <author> D. K. Naik, R. J. Mammone, and A. Agarwal, </author> <title> Meta-Neural Network approach to learning by learning, </title> <journal> Intelligence Engineering Systems through Artificial Neural Networks, ASME Press, </journal> <volume> Vol. 2, </volume> <pages> pp. 245-252, </pages> <year> 1992. </year>
Reference-contexts: of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation [Caru93, Caru95, Baxt95a], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance. <p> It has been used for various purposes by other authors such as <ref> [Jaco88, Vogl88, Naik92] </ref>. 6 The success of MTL rests on a judicious choice for the function k = f (; R k ) and the ability to measure the relatedness of the parallel tasks to the primary task.
Reference: [Naik93] <author> D.K. Naik and Richard J. Mammone, </author> <title> Learning by learning in neural networks, Artificial Neural Networks for Speech and Vision; ed: </title> <editor> Richard J. Mammone, </editor> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1993. </year>
Reference-contexts: of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation [Caru93, Caru95, Baxt95a], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance.
Reference: [Prat93a] <author> Lorien Y. Pratt, </author> <title> Discriminability-Based transfer between neural networks, </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 5, pp. 204-211, </pages> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: In general, we define this form of transfer as being representational since it involves the direct or indirect assignment of known task representation to a new task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Prat93a, Prat94, Shar92, Shav90, Towe90, Fahl90, Sing92, Ring93] </ref>. This form of transfer often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional. <p> In particular we believe it will be important to take into consideration more subtle characteristics of the backpropagation learning algorithm such as the impact of high magnitude weight values <ref> [Prat93a] </ref> or the energy involved in moving from one point in weight space to another. Acknowledgements: This research was supported by the Department of Nuclear Medicine, Vic-toria Hospital, London, Ontario as part of on-going research efforts into methods of automated diagnosis.
Reference: [Prat93b] <author> Lorien Y. Pratt, </author> <title> Transferring previously learned back-propagation neural networks to new learning tasks, </title> <type> PhD Thesis, </type> <institution> Department of Computer Science, Rutgers University, </institution> <address> New Brunswick, NJ, </address> <year> 1993. </year>
Reference-contexts: This process is a form of knowledge-based inductive bias referred to in the literature as the transfer of knowledge from one or more source tasks to a target or primary task <ref> [Mitc80, Prat93b] </ref>. The transfer of task knowledge can be considered a major aspect of the problem of learning to learn [Elli65] and has close ties to analogical reasoning [Hall89].
Reference: [Prat94] <author> Lorien Y. Pratt, </author> <title> Experiments on the transfer of knowledge between neural networks, </title> <editor> In S. Hanson, G. Drastal, and R. Rivest, editors, </editor> <title> Computational Learning Theory and Natural Learning Systems, Constraints and Prospects, </title> <publisher> MIT Press, </publisher> <pages> pp. 523-560, </pages> <address> Cambridge, Mass., </address> <year> 1994. </year>
Reference-contexts: In general, we define this form of transfer as being representational since it involves the direct or indirect assignment of known task representation to a new task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Prat93a, Prat94, Shar92, Shav90, Towe90, Fahl90, Sing92, Ring93] </ref>. This form of transfer often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional.
Reference: [Ring93] <author> Mark Ring, </author> <title> Learning sequential tasks by incrementally adding higher orders, </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 5, pp. 155-122, </pages> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: In general, we define this form of transfer as being representational since it involves the direct or indirect assignment of known task representation to a new task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Prat93a, Prat94, Shar92, Shav90, Towe90, Fahl90, Sing92, Ring93] </ref>. This form of transfer often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional.
Reference: [Shar92] <author> Noel E. Sharkey and Amanda J.C. Sharkey, </author> <title> Adaptive generalization and the transfer of knowledge, </title> <institution> Working paper Center for Connection Science, University of Exeter, pp. n.sharkey@dcs.shef.ac.uk, UK, </institution> <year> 1992. </year>
Reference-contexts: In general, we define this form of transfer as being representational since it involves the direct or indirect assignment of known task representation to a new task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Prat93a, Prat94, Shar92, Shav90, Towe90, Fahl90, Sing92, Ring93] </ref>. This form of transfer often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional.
Reference: [Shav90] <author> Jude W. Shavlik and Geoffrey G. Towell, </author> <title> An appraoch to combining explanation-based and neural learning algorithms, </title> <booktitle> Readings in Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 828-839, </pages> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: In general, we define this form of transfer as being representational since it involves the direct or indirect assignment of known task representation to a new task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Prat93a, Prat94, Shar92, Shav90, Towe90, Fahl90, Sing92, Ring93] </ref>. This form of transfer often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional.
Reference: [Silv95] <author> Daniel L. Silver and Robert E. Mercer, </author> <title> Toward a model of consolidation: The retention and transfer of neural net task knowledge, </title> <booktitle> Proceedings of the INNS World Congress on Neural Networks, </booktitle> <editor> Lawrence Erlbaun Assosciates, </editor> <volume> Vol. III, </volume> <pages> pp. 164-169, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: The transfer of task knowledge can be considered a major aspect of the problem of learning to learn [Elli65] and has close ties to analogical reasoning [Hall89]. Previous work <ref> [Silv95] </ref> describes a method of sequentially learning a set of related tasks using an architecture of feed-forward networks which we call a Consolidation System.
Reference: [Sing92] <author> Satinder P. Singh, </author> <title> Transfer of learning by composing solutions for elemental sequential tasks, </title> <booktitle> Machine Learning, </booktitle> <year> 1992. </year>
Reference-contexts: In general, we define this form of transfer as being representational since it involves the direct or indirect assignment of known task representation to a new task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Prat93a, Prat94, Shar92, Shav90, Towe90, Fahl90, Sing92, Ring93] </ref>. This form of transfer often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional.
Reference: [Sudd90] <author> Steven Suddarth and Y Kergoisien, </author> <title> Rule injection hints as a means of improving network performance and learning time, </title> <booktitle> Proceedings of the EURASIP workshop on Neural Networks, </booktitle> <year> 1990. </year>
Reference-contexts: In contrast to representational transfer is a form we define as functional. Functional transfer does not involve the explicit assignment of prior task representation to a new task, rather it employs the use of implicit pressures from supplemental training examples <ref> [Sudd90, AM95] </ref>, the parallel learning of related tasks constrained to use a common internal representation [Caru93, Caru95, Baxt95a], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b].
Reference: [Thru93] <author> Sebastian Thrun and Tom M.Mitchell, </author> <title> Lifelong Robot Learning, </title> <type> Technical Report IAI-TR-93-7, </type> <institution> Institute for Informatics III, University of Bonn, Bonn, Germany, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation [Caru93, Caru95, Baxt95a], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance.
Reference: [Thru94a] <author> Sebastian Thrun, </author> <title> A Lifelong Learning Perspective for Mobile Robot Control, </title> <booktitle> Proceedings of the IEEE Conference on Intelligent Robots and Systems, IEEE, </booktitle> <month> September 12-16, </month> <year> 1994. </year>
Reference-contexts: of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation [Caru93, Caru95, Baxt95a], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance.
Reference: [Thru94b] <author> Sebastian Thrun and Tom M.Mitchell, </author> <title> Learning one more thing, </title> <type> Technical Report CMU--CS-94-184, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1994. </year>
Reference-contexts: of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation [Caru93, Caru95, Baxt95a], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance.
Reference: [Towe90] <author> Geoffrey G. Towell, Jude W. Shavlik, and Michiel O. Noordewier, </author> <title> Refinement of approximate domain theories by knowledge-based neural networks, </title> <booktitle> Proceedings Eigth National Conference on Artificial Intelligence (AAAI-90), AAAI Press/MIT Press, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 861-866, </pages> <address> Menlo Park, CA, </address> <year> 1990. </year>
Reference-contexts: In general, we define this form of transfer as being representational since it involves the direct or indirect assignment of known task representation to a new task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Prat93a, Prat94, Shar92, Shav90, Towe90, Fahl90, Sing92, Ring93] </ref>. This form of transfer often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional.

References-found: 28

