URL: http://www.cs.berkeley.edu/~lazzaro/biblio/decoder.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~lazzaro/biblio/bib.html
Root-URL: 
Email: lazzaro@cs.berkeley.edu, johnw@cs.berkeley.edu  rpl@sst.ll.mit.edu  
Title: A Micropower Analog VLSI HMM State Decoder for Wordspotting  
Author: John Lazzaro and John Wawrzynek Richard Lippmann 
Address: Berkeley, CA 94720-1776  Room S4-121, 244 Wood Street Lexington, MA 02173-0073  
Affiliation: CS Division, UC Berkeley  MIT Lincoln Laboratory  
Abstract: We describe the implementation of a hidden Markov model state decoding system, a component for a wordspotting speech recognition system. The key specification for this state decoder design is microwatt power dissipation; this requirement led to a continuous-time, analog circuit implementation. We describe the tradeoffs inherent in the choice of an analog design and explain the mapping of the discrete-time state decoding algorithm into the continuous domain. We characterize the operation of a 10-word (81 state) state decoder test chip.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bourlard, H. and Morgan, N. </author> <year> (1994). </year> <title> Connectionist speech recognition : a hybrid approach. </title> <address> Boston: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Analog circuit implementations of state decoding have previously been used in data storage applications (Matthews and Spencer, 1993). 2. HIDDEN MARKOV MODEL STATE DECODING Hidden Markov models (HMMs) are used in the most successful modern speech recognition systems <ref> (Bourlard and Morgan, 1994) </ref>. An HMM speech recognition system consists of a probabilistic state machine, and a method for tracing the state transitions of the machine for a given input speech waveform. the presence of keywords ("Yes," "No") in conversational speech. <p> We estimate the probability b i (n) that the signal in frame n was produced by state i, using static pattern recognition techniques. Static pattern recognition of short speech segments is a difficult task: state-of-the-art approaches provide about 60% accuracy <ref> (Bourlard and Morgan, 1994) </ref>. To improve the accuracy of these local estimates, we need to integrate information over the entire word. We do this by creating a set of state variables for the machine, called likelihoods, that are incrementally updated at every frame.
Reference: <author> Coggins, R., Jabri, M., Flower, B., and Pickard, S. </author> <year> (1995). </year> <title> "A low-power network for on-line diagnosis of heart patients," </title> <journal> IEEE Micro, </journal> <volume> vol 15, no. 3, </volume> <pages> pp. 8-25. </pages>
Reference-contexts: Power consumption is a requirement that sometimes dictates an analog design. For example, implantable medical devices require signal processing systems that operate continuously for many years from a small battery. A maximum power consumption specification of a few microwatts is not uncommon in these devices <ref> (Coggins et al., 1995) </ref>. For many of these micropower systems, an analog implementation may be the only realistic way to meet the power specification. In this paper, we describe an analog implementation of a common signal processing block in pattern recognition systems: a hidden Markov model state decoder. <p> Several generations of mi-cropower audio pre-processing systems (Watts et al., 1992; van Shaik et al., 1996) have been implemented and these designs are good candidates for the "Feature Detection" block in Figure 2. Micropower implementations of multi-layer perceptrons <ref> (Coggins et al., 1995) </ref>, radial basis function networks (Delbruck, 1991), or Gaussian mixture models are suitable for the "Probability Generation" block. The design of individual signal processing blocks is perhaps the easiest challenge of this implementation project.
Reference: <author> Delbruck, T. </author> <year> (1991). </year> <title> "Bump circuits for computing similarity and dissimilarity of analog voltages," </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks (IJCNN-91-Seattle), </booktitle> <volume> vol 1, </volume> <pages> pp. 475-479. </pages>
Reference-contexts: The filler state input probability current is constant. The probability currents of all states of the other word models are set to the minimum value, essentially removing these word models from the system. The chip includes circuits for generating test patterns of input probability currents. A radial-basis function circuit <ref> (Delbruck, 1991) </ref>, shown in Figure 8a, is the core element in the test generation system; the circuit shown generates the output current I u sech 2 ( (V p V n ) ) + I l (13) as a function of its differential voltage input. <p> Several generations of mi-cropower audio pre-processing systems (Watts et al., 1992; van Shaik et al., 1996) have been implemented and these designs are good candidates for the "Feature Detection" block in Figure 2. Micropower implementations of multi-layer perceptrons (Coggins et al., 1995), radial basis function networks <ref> (Delbruck, 1991) </ref>, or Gaussian mixture models are suitable for the "Probability Generation" block. The design of individual signal processing blocks is perhaps the easiest challenge of this implementation project. Harder tasks include the interfaces between these blocks, and the compensation of the entire system to temperature and power-supply variations.
Reference: <author> Lippmann, R. P., Chang, E. I., and Jankowski, C. R. </author> <year> (1994). </year> <title> "Wordspotter training using figure-of-merit back-propagation," </title> <booktitle> Proceedings International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 389-392. </pages>
Reference-contexts: This type of recognition where a small set of words is detected in unconstrained speech is called wordspotting <ref> (Lippmann et al.,, 1994) </ref>. Wordspotting can provide simple-to-use voice control of systems with little user instruction or training. As the speaker pronounces the word "Yes" the state machine sequences through states 1-10; while pronouncing the word "No" the machine sequences through 11-20. <p> In an electrical sense, this pattern generation system provides a realistic proxy for the "Probability Generation" system (Figure 2) of a speech recognition system, since radial basis functions are often used in the final processing stage of speech classifiers (e.g. <ref> (Lippmann et al.,, 1994) </ref>). As detailed in Section 3.3, the test chip uses a simple circuit implementation for the continuous-time delay, that has a limited range of linearity as quantified in Equations 11a and 11b.
Reference: <author> Lippmann, R. P. and Gold, B. </author> <year> (1987). </year> <title> "Neural-net classifiers useful for speech recognition," </title> <booktitle> Proceedings of the International Conference on Neural Networks (ICNN), </booktitle> <volume> vol. IV, </volume> <pages> pp 417-425. </pages>
Reference-contexts: Section 4 describes a test chip using this decoder. Section 5 shows experimental data from this chip. The state-decoder design is a practical implementation of the Viterbi Net architecture originally described in <ref> (Lippmann and Gold, 1987) </ref>. Analog circuit implementations of state decoding have previously been used in data storage applications (Matthews and Spencer, 1993). 2. HIDDEN MARKOV MODEL STATE DECODING Hidden Markov models (HMMs) are used in the most successful modern speech recognition systems (Bourlard and Morgan, 1994).
Reference: <author> Matthews, T. W. and Spencer, R. R. </author> <year> (1993). </year> <title> "An integrated analog CMOS Viterbi detector for digital magnetic recording," </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 28, no. 12, </volume> <pages> pp. 1294-302. </pages>
Reference-contexts: Section 5 shows experimental data from this chip. The state-decoder design is a practical implementation of the Viterbi Net architecture originally described in (Lippmann and Gold, 1987). Analog circuit implementations of state decoding have previously been used in data storage applications <ref> (Matthews and Spencer, 1993) </ref>. 2. HIDDEN MARKOV MODEL STATE DECODING Hidden Markov models (HMMs) are used in the most successful modern speech recognition systems (Bourlard and Morgan, 1994).
Reference: <author> Oguey, H. and Aebischer, D. </author> <year> (1996). </year> <title> "CMOS current reference without resistance" Proceedings European Solid State Circuits Conference pp. 104-107. </title> <editor> van Schaik, A., Fragniere, E., and Vittoz, E. </editor> <year> (1996). </year> <title> "Improved silicon cochlea using compatible lateral bipolar transistors," </title> <booktitle> in Advances in Neural Information Processing Systems 8, </booktitle> <editor> D. Tourestzky, M. C. Mozer, and M. E. Hasselmo, Eds, </editor> <address> Cambridge, Mass: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Several analog nonidealities affect the continuous-time state decoding architecture. One issue surrounds the tuning of the conductance parameter G to meet the condition t =(C t =G) = 1. This condition requires G to be compensated for ambient temperature and power-supply variations. Recent advances in micropower compensation circuits <ref> (Oguey and Aebischer, 1996) </ref> could be leveraged to solve this problem within the power consumption specifications of the system. A second issue involves the voltage range of linearity of the transconductance amplifier in the delay circuit.
Reference: <author> Watts, L., Kerns, D. A., Lyon, R. F., and Mead, C. </author> <year> (1992). </year> <title> "Improved implementation of the silicon cochlea," </title> <journal> IEEE Journal Solid State Circuits, </journal> <volume> vol 27, no. 3, </volume> <pages> pp. 692-700. </pages>
References-found: 8

