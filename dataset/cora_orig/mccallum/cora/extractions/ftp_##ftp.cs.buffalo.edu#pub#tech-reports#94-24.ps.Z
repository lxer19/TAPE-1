URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/94-24.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Title: Linear Speed-Up, Information Vicinity, and Finite-State Machines  
Author: Kenneth W. Regan 
Date: May 1994  
Affiliation: State University of New York at Buffalo  
Abstract: Connections are shown between two properties of a machine model: linear speed-up and polynomial vicinity. In the context of the author's Block Move (BM) model, these relate to: "How long does it take to simulate a finite transducer S on a given input z?" This question is related to the century-old problem of finding economical representations for finite groups. Under some cost measures for computing S(z), the BM enjoys the linear speed-up property, but under more-realistic measures, and subject to a reasonable but unproved hypothesis, it has the antithetical property of a constant-factor time hierarchy.
Abstract-found: 1
Intro-found: 1
Reference: [AKS83] <author> M. Ajtai, J. Komlos, and E. Szemeredi. </author> <title> An O(n log n) sorting network. </title> <booktitle> In Proc. 15th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 1-9, </pages> <year> 1983. </year>
Reference-contexts: We suspect that the AKS sorting network <ref> [AKS83] </ref> can be simulated efficiently on the BM in 1 -time O (N log 2 N ), which would cut the extra term to O (log N ), but have not yet checked this.
Reference: [Con90] <author> M. Conner. </author> <title> Sequential machines realized by group representations. </title> <journal> Info. Comp., </journal> <volume> 85 </volume> <pages> 183-201, </pages> <year> 1990. </year>
Reference-contexts: If the time for composing two mappings were linear in N log N , or even proportional to jjN log N , then the time for the off-line simulation would be O (jSj I n). Via the Krohn-Rhodes decomposition of DGSMs into "permutation" and "reset" machines (see <ref> [Con90] </ref>), we need only worry about the case where the mappings g c generate a group acting on a set of size N . <p> So we ask, Is there a representation for such a permutation group that allows elements to be multiplied in linear time? We note that the matrix methods proposed in <ref> [Con90] </ref> actually take quadratic time in worst case. Our intended use for a good bound T (S; z) on the time for the off-line simulation also depends at this moment on the following: 6 Weak Efficiency Hypothesis.
Reference: [CR73] <author> S. Cook and R. Reckhow. </author> <title> Time bounded random access machines. </title> <journal> J. Comp. Sys. Sci., </journal> <volume> 7 </volume> <pages> 354-375, </pages> <year> 1973. </year> <month> 7 </month>
Reference-contexts: This definition does not try to take account of everything, but suffices for this paper. All of the popular RAM models, including the log-cost RAM of Cook and Reckhow <ref> [CR73] </ref> and the polynomially compact RAM of Grandjean and Robson [GR91], have exponential vicinity. So do the pointer models of Schonhage [Sch80] and Jones [Jon93], and the Turing machines with tree-structured tapes considered by Huhne [Huh93].
Reference: [EL94] <author> S. Even and A. Litman. </author> <title> On the capabilities of systolic systems. </title> <journal> Math. Sys. Thy., </journal> <volume> 27 </volume> <pages> 3-28, </pages> <year> 1994. </year>
Reference-contexts: These are analogous to clock-cycle and lag for systolic automata as described by Even and Litman <ref> [EL94] </ref>. 3 Results Under the parameters d (a) = a 1=d , the BM has vicinity O (t d ), simply because any access outside the first t d tape cells incurs a charge (a) &gt; t. <p> Can it be closed? What happens with log jj in place of jj? In the larger sense, we are asking: Is there a more-efficient way to simulate a finite transduction offline than by composing state mappings? We inquire whether the systolic approach of <ref> [EL94] </ref> or the BDD-approach of [SW93] can yield more-efficient simulations, at least on the BM. As noted above, a positive answer would have a derivative effect on computational group theory.
Reference: [FS92] <author> Y. Feldman and E. Shapiro. </author> <title> Spatial machines: A more-realistic approach to parallel computation. </title> <journal> Comm. ACM, </journal> <volume> 35 </volume> <pages> 60-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: However, it connects to a concrete and important general property of machine models. Feldman and Shapiro <ref> [FS92] </ref> informally define the vicinity of a machine to be the maximum number of data bits on which the evolution of the computation over the next t time units can possibly depend.
Reference: [GR91] <author> E. Grandjean and J. Robson. </author> <title> RAM with compact memory: a robust and realistic model of computation. </title> <booktitle> In Proc. </booktitle> <volume> CSL '90, LNCS 533, </volume> <pages> pages 195-233, </pages> <year> 1991. </year>
Reference-contexts: This definition does not try to take account of everything, but suffices for this paper. All of the popular RAM models, including the log-cost RAM of Cook and Reckhow [CR73] and the polynomially compact RAM of Grandjean and Robson <ref> [GR91] </ref>, have exponential vicinity. So do the pointer models of Schonhage [Sch80] and Jones [Jon93], and the Turing machines with tree-structured tapes considered by Huhne [Huh93].
Reference: [Hen66] <author> F. Hennie. </author> <title> On-line Turing machine computation. </title> <journal> IEEE Trans. Electr. Comp., </journal> <volume> EC-15:35-44, </volume> <year> 1966. </year>
Reference-contexts: So by hypothesis of , there exists E &gt; 1 such that for all D; K &gt; 1 there exist infinitely-many n such that (n=D) &gt; (1=E)(n) + K. We design a P along the lines of the "Storage-Retrieval Problem" in <ref> [Hen66, Huh93] </ref> such that P cannot be sped up on-line by a factor of E under . Besides its argument x on its main tape, P is given a list of integers i 0 ; : : : ; i l on an auxiliary tape.
Reference: [HS65] <author> J. Hartmanis and R. Stearns. </author> <title> On the computational complexity of algorithms. </title> <journal> Transactions of the AMS, </journal> <volume> 117 </volume> <pages> 285-306, </pages> <year> 1965. </year>
Reference-contexts: 1 Speed-Up and Vicinity Hartmanis and Stearns <ref> [HS65] </ref> proved that the standard multitape Turing machine (TM) model enjoys the following property, for which we give a general statement: Definition 1.1.
Reference: [HU79] <author> J. Hopcroft and J. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1979. </year>
Reference-contexts: Formally S is a deterministic generalized sequential machine (DGSM), as defined in <ref> [HU79] </ref>. If z is the string formed by the characters in cells a 1 : : : b 1 , then S (z) is written into the block a 2 : : : b 2 beginning at a 2 .
Reference: [Huh93] <author> M. Huhne. </author> <title> Linear speed-up does not hold for Turing machines with tree storages. </title> <journal> Inf. Proc. Lett., </journal> <volume> 47 </volume> <pages> 313-318, </pages> <year> 1993. </year>
Reference-contexts: It also makes M 0 simulate M on-line, meaning that (after the initial step) every d*te moves of M 0 simulate t moves of M. Huhne <ref> [Huh93] </ref> gives a formal definition of on-line speedup. fl Author's address: Department of Computer Science, University at Buffalo, 226 Bell Hall, Buffalo NY 14260-2000, USA. E-mail: regan@cs.buffalo.edu; tel. (716) 645-3189. <p> All of the popular RAM models, including the log-cost RAM of Cook and Reckhow [CR73] and the polynomially compact RAM of Grandjean and Robson [GR91], have exponential vicinity. So do the pointer models of Schonhage [Sch80] and Jones [Jon93], and the Turing machines with tree-structured tapes considered by Huhne <ref> [Huh93] </ref>. Turing machines with k tapes have u (t) = k (2t + 1), and those with d-dimensional tapes have u (t) = fi (t d ). <p> The proof that linear speedup does not hold on-line when allows super-polynomial vicinity generalizes and refines the argument of Huhne <ref> [Huh93] </ref>. <p> For smaller functions, we show by refining the argument in <ref> [Huh93] </ref> that not only does the BM under lack polynomial vicinity, but also linear speed-up, regardless of how time (S; z) is defined. 3 Theorem 3.1 Suppose that for all ffi &gt; 0, (n) = o (n ffi ). <p> So by hypothesis of , there exists E &gt; 1 such that for all D; K &gt; 1 there exist infinitely-many n such that (n=D) &gt; (1=E)(n) + K. We design a P along the lines of the "Storage-Retrieval Problem" in <ref> [Hen66, Huh93] </ref> such that P cannot be sped up on-line by a factor of E under . Besides its argument x on its main tape, P is given a list of integers i 0 ; : : : ; i l on an auxiliary tape.
Reference: [Jon93] <author> N. Jones. </author> <title> Constant factors do matter. </title> <booktitle> In Proc. 25th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 602-611, </pages> <year> 1993. </year>
Reference-contexts: All of the popular RAM models, including the log-cost RAM of Cook and Reckhow [CR73] and the polynomially compact RAM of Grandjean and Robson [GR91], have exponential vicinity. So do the pointer models of Schonhage [Sch80] and Jones <ref> [Jon93] </ref>, and the Turing machines with tree-structured tapes considered by Huhne [Huh93]. Turing machines with k tapes have u (t) = k (2t + 1), and those with d-dimensional tapes have u (t) = fi (t d ). <p> other constant in the simulation independent of M . (The lag term is similarly absorbed.) The existence of C 0 follows by analyzing the tape-reduction theorem of [Reg94b] for d and the choice of T (S; z), and the consequence of such a "constant-factor time hierarchy" was observed by Jones <ref> [Jon93] </ref>. In conclusion, we note the large gap between norm (S) = N for which linear speedup holds, and norm (S) = N log 3 N jj (with l constant) for which it appears not to hold.
Reference: [Reg94a] <author> K. Regan. </author> <title> Linear-time algorithms in memory hierarchies, </title> <note> 1994. This proceedings. </note>
Reference-contexts: This paper addresses these concerns in the case of the Block Move (BM) model of <ref> [Reg94b, Reg94a] </ref>, which has useful generality because it comes with a parameter : N ! N called a memory access cost function that calibrates its vicinity. The proof that linear speedup does not hold on-line when allows super-polynomial vicinity generalizes and refines the argument of Huhne [Huh93]. <p> An analogous result for straight-line programs is proved in <ref> [Reg94a] </ref>, but we have not proved the hypothesis for machines, which seems needed for the following: Theorem 3.4 Let time (S; z) in block moves be defined as T (S; z) from Lemma 3.3, and suppose the "weak efficiency hypothesis" holds.
Reference: [Reg94b] <author> K. Regan. </author> <title> Linear time and memory efficient computation, </title> <note> 1994. Revision of UB-CS-TR 92-28, accepted to SIAM J. Comput. </note>
Reference-contexts: This paper addresses these concerns in the case of the Block Move (BM) model of <ref> [Reg94b, Reg94a] </ref>, which has useful generality because it comes with a parameter : N ! N called a memory access cost function that calibrates its vicinity. The proof that linear speedup does not hold on-line when allows super-polynomial vicinity generalizes and refines the argument of Huhne [Huh93]. <p> Specific machine forms may be found in <ref> [Reg94b] </ref>, along with results that for the functions d (a) = a 1=d , the forms all simulate each other up to linear d -time. <p> make the term c (M)R (n) less than t (n) as n ! 1, because the choice of T (S; z) makes every other constant in the simulation independent of M . (The lag term is similarly absorbed.) The existence of C 0 follows by analyzing the tape-reduction theorem of <ref> [Reg94b] </ref> for d and the choice of T (S; z), and the consequence of such a "constant-factor time hierarchy" was observed by Jones [Jon93].
Reference: [Sch80] <author> A. Schonhage. </author> <title> Storage modification machines. </title> <journal> SIAM J. Comput., </journal> <volume> 9 </volume> <pages> 490-508, </pages> <year> 1980. </year>
Reference-contexts: All of the popular RAM models, including the log-cost RAM of Cook and Reckhow [CR73] and the polynomially compact RAM of Grandjean and Robson [GR91], have exponential vicinity. So do the pointer models of Schonhage <ref> [Sch80] </ref> and Jones [Jon93], and the Turing machines with tree-structured tapes considered by Huhne [Huh93]. Turing machines with k tapes have u (t) = k (2t + 1), and those with d-dimensional tapes have u (t) = fi (t d ).
Reference: [SW93] <author> D. Sieling and I. Wegener. </author> <title> Reduction of OBDDs in linear time. </title> <journal> Inf. Proc. Lett., </journal> <volume> 48 </volume> <pages> 139-144, </pages> <year> 1993. </year> <month> 8 </month>
Reference-contexts: Can it be closed? What happens with log jj in place of jj? In the larger sense, we are asking: Is there a more-efficient way to simulate a finite transduction offline than by composing state mappings? We inquire whether the systolic approach of [EL94] or the BDD-approach of <ref> [SW93] </ref> can yield more-efficient simulations, at least on the BM. As noted above, a positive answer would have a derivative effect on computational group theory.
References-found: 15

