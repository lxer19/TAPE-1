URL: ftp://ftp.cag.lcs.mit.edu/pub/raw/documents/hipc98.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/~barua/papers/index.html
Root-URL: 
Email: fbarua,walt,saman,agarwalg@lcs.mit.edu  
Title: Memory Bank Disambiguation using Modulo Unrolling for Raw Machines  
Author: Rajeev Barua, Walter Lee, Saman Amarasinghe, Anant Agarwal 
Note: This research is funded in part by DARPA contract DABT63-96-C-0036. Proceedings of the Fifth Int'l Conference of High-Performance Computing (HIPC), December, 1998. Also MIT-LCS-TR-759.  
Date: July 8, 1998  
Web: http://cag-www.lcs.mit.edu/raw  
Address: Cambridge, MA 02139, U.S.A.  
Affiliation: M.I.T. Laboratory for Computer Science  
Abstract: The Raw approach of replicated processor tiles interconnected with a fast static mesh network provides a simple, scalable design that maximizes the resources available in next generation processor technology. In Raw architectures, scalable access to memory is provided by distributing the memory across processor tiles. Management of the memory can be performed by well known techniques which generate the requisite communication code on distributed address-space architectures. On the other hand, the fast, static network provides the compiler with a simple interface to optimize such communication. This paper addresses this novel problem of statically determining the exact communication required for each memory reference. We introduce a code transformation called modulo unrolling following which memory-bank disambiguation of certain classes of accesses becomes possible. Modulo unrolling transformations ensure that every instance of these memory references will access the memory bank of a single processor tile known at compile-time. Such memory references can then be accessed on the static network. We show that this can be achieved by using a relatively small unroll factor. For dense matrix scientific applications we were able to access all the array references on the static network, thus achieving good speedup on the RAW processor. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Kranz, and V. Natarajan. </author> <title> Automatic Partitioning of Parallel Loops for Cache-Coherent Multiprocessors. </title> <booktitle> In 22nd International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year> <journal> IEEE. Also in IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol 6, </volume> <pages> pp 943-961, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: First, accesses statically promoted to different processors by the method in Section 5 are always different. Second, even among accesses going to the same processor, accesses belonging to the same uniformly generated set 2 <ref> [1] </ref> differing by a non-zero constant must also be different. Finally, accesses to different un-aliased arrays are always different. 2 Two affine array accesses are said to be in the same uniformly generated set if they access the same array, and their index expressions differ by at most a constant.
Reference: [2] <author> S. Amarasinghe. </author> <title> Parallelizing Compiler Techniques Based on Linear Inequalities. </title> <type> In Ph.D Thesis, </type> <institution> Stanford University. </institution> <note> Also appears as Techical Report CSL-TR-97-714, </note> <month> Jan </month> <year> 1997. </year>
Reference-contexts: Strip mining While this last observation may be used to generate code directly, we automate this process by noting that strip mining the last dimension is performed in <ref> [2] </ref> for a different purpose. We use the software developed for [2] to strip mine the last dimension by N and strength reduce the divide operations. <p> Strip mining While this last observation may be used to generate code directly, we automate this process by noting that strip mining the last dimension is performed in <ref> [2] </ref> for a different purpose. We use the software developed for [2] to strip mine the last dimension by N and strength reduce the divide operations. Strip mining replaces the last dimension by itself divided by N , and it adds a new dimension at the end with index being the original last dimension index mod N .
Reference: [3] <author> S. Amarasinghe, J. Anderson, C. Wilson, S. Liao, B. Murphy, R. French, and M. Lam. </author> <title> Multiprocessors from a Software Perspective. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 5261, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: The size of the datasets in these benchmarks is intentionally made to be small to feature the low communication overhead of Raw. Traditional multiprocessors, with their high overheads, would be unable to attain speedup for such datasets <ref> [3] </ref>. Most of the speedup attained can be attributed to the exploitation of ILP, but unrolling plays a beneficiary role as well. In RAWCC , unrolling speeds up a program by exposing scalar optimizations across loop 14 iterations.
Reference: [4] <author> J. Babb, M. Frank, V. Lee, E. Waingold, R. Barua, M. Taylor, J. Kim, S. Devabhaktuni, and A. Agarwal. </author> <title> The raw benchmark suite: Computation structures for general purpose computing. </title> <booktitle> In IEEE Symposium on Field-Programmable Custom Computing Machines, </booktitle> <address> Napa Valley, CA, </address> <month> Apr. </month> <year> 1997. </year>
Reference-contexts: Experiments are performed on the Raw simulator, which simulates the Raw prototype described in Section 2. A description of parameters of the Raw prototype, including instruction and communication latencies can be found in [6]. The benchmarks we select include programs from the Raw benchmark suite <ref> [4] </ref>, program kernels from the nasa7 benchmark of Spec92, tomcatv of Spec92, and the kernel basic block which accounts for 50% of the run-time in fpppp of Spec92.
Reference: [5] <author> J. R. Ellis. Bulldog: </author> <title> A Compiler for VLIW Architectures. </title> <type> In Ph.D Thesis, </type> <institution> Yale University, </institution> <year> 1985. </year> <month> 16 </month>
Reference-contexts: We present performance results of this compiler. We have developed strategies to efficiently compile dynamic accesses on Raw, but these are beyond the scope of this paper. 2 Static promotion for Raw is related to the concept of memory-bank disambiguation <ref> [5] </ref> for distributed bank architectures, such as certain VLIWs. Memory-bank disambiguation is an analysis concept which aims to predict at compile-time the memory bank accessed by a reference. Static promotion of a reference is possible exactly when it can be memory bank disambiguated. <p> For a detailed comparison to other architectures, see [9]. Section 1 points out that static promotion is related to memory bank disambiguation, and it lists their differences. Memory bank disambiguation was a term used by Ellis in the Bulldog Compiler <ref> [5] </ref> for a point-to-point VLIW model. For such VLIWs, he shows that successful disambiguation means that an access can be executed through a fast front door to a memory bank, while an unsuccessful access must be sent over a slower back door. <p> The lack of point-to-point VLIWs seems to explain the dearth of work on memory bank disambiguation for compiling for VLIWs. The modulo unrolling scheme we propose is a descendant of a simple technique presented by Ellis <ref> [5] </ref>. He observes that unrolling can sometimes help disambiguate accesses, but he does not attempt to formalize the observation into a theory or algorithm.
Reference: [6] <author> W. Lee, R. Barua, D. Srikrishna, J. Babb, V. Sarkar, S. Amarasinghe, and A. Agarwal. </author> <title> Space-Time Scheduling of Instruction-Level Parallelism on a Raw Machine. </title> <booktitle> In Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> San Jose, California, </address> <month> October </month> <year> 1998. </year>
Reference-contexts: These passes transform input programs into a form that the space-time scheduler, described in <ref> [6] </ref>, can schedule. The front-end includes the focus of this paper, namely the mechanisms for static promotion and the tile identification of promoted accesses. In addition, the front-end performs dependence analysis and optimizations, dynamic access optimizations and code generation, and other memory and control optimizations. <p> After unrolling, each access refers to locations from only one processor. onto messages between tiles on the Raw static network. The mapping is done in a manner that maximizes parallelism, data locality, and load balance while guaranteeing deadlock-free behavior. Details are described in <ref> [6] </ref>. 5 Static Promotion using Modulo Unrolling In this section we show how static promotion can be done for certain classes of array accesses. As a motivating example, consider the code in Figure 3 (a). <p> Experiments are performed on the Raw simulator, which simulates the Raw prototype described in Section 2. A description of parameters of the Raw prototype, including instruction and communication latencies can be found in <ref> [6] </ref>. The benchmarks we select include programs from the Raw benchmark suite [4], program kernels from the nasa7 benchmark of Spec92, tomcatv of Spec92, and the kernel basic block which accounts for 50% of the run-time in fpppp of Spec92.
Reference: [7] <author> G. Lowney et al. </author> <title> The Multiflow Trace Scheduling Compiler. </title> <booktitle> In Journal of Supercomputing, </booktitle> <pages> pages 51142, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: However, most VLIWs today use global buses for communication, not a point-to-point network. VLIW machines of various degrees of scalability have been proposed, ranging from completely centralized machines to machines with distributed functional units, register files, and memory <ref> [7] </ref>. The lack of point-to-point VLIWs seems to explain the dearth of work on memory bank disambiguation for compiling for VLIWs. The modulo unrolling scheme we propose is a descendant of a simple technique presented by Ellis [5]. <p> A different type of memory disambiguation is relevant on the more typical VLIW machines such as the Multiflow Trace <ref> [7] </ref>. These machines use global buses rather than a point-to-point network. Relative memory disambiguation [7] aims to discover if two memory access are necessarily different, though not necessarily known. Successful disambiguation implies that accesses can be executed in parallel. <p> A different type of memory disambiguation is relevant on the more typical VLIW machines such as the Multiflow Trace <ref> [7] </ref>. These machines use global buses rather than a point-to-point network. Relative memory disambiguation [7] aims to discover if two memory access are necessarily different, though not necessarily known. Successful disambiguation implies that accesses can be executed in parallel.
Reference: [8] <author> M. D. Smith. </author> <title> Extending suif for machine-dependent optimizations. </title> <booktitle> In Proceedings of the First SUIF Compiler Workshop, </booktitle> <pages> pages 1425, </pages> <address> Stanford, CA, </address> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Speedup compares the run-time of the RAWCC -compiled code versus the run-time of the code generated by the Machsuif MIPS compiler. suif <ref> [8] </ref> targeted for an R2000. Table 2 shows the speedups attained by the benchmarks for Raw machines of various sizes. Note that these speedups do not measure the advantage Raw is attaining over modern architectures due to a faster clock.
Reference: [9] <author> E. Waingold, M. Taylor, V. Sarkar, W. Lee, V. Lee, J. Kim, M. Frank, P. Finch, S. Devabhaktuni, R. Barua, J. Babb, S. Amarasinghe, and A. Agarwal. </author> <title> Baring It All to Software: The RAW Machine. </title> <booktitle> IEEE Computer, </booktitle> <month> September </month> <year> 1997. </year> <note> Also as MIT-LCS-TR-709. </note>
Reference-contexts: Run-time cost for this complexity is paid even when exact compile-time prediction of memory locations accessed is possible. Multiprocessors provide truly distributed resources, but they incur very high communication costs, restricting them to exploiting coarse-grained parallelism only. The Raw machine <ref> [9] </ref> aims to provide truly distributed resources at communication costs low enough to exploit ILP. It distributes the register files, memories and ALUs into identical tiles, and it provides a fast static compiler-routed network organized as a two-dimensional mesh for inter-tile communication. <p> Section 5 describes the static promotion strategy for arrays in more detail. Section 6 describes some optimizations to increase ILP in the context of scientific codes. Section 7 presents some experimental results. Section 8 describes related work, while Section 9 concludes. 2 Raw Architecture The Raw architecture <ref> [9] </ref> is motivated by the desire to maximize the performance per silicon area of a machine. The design is kept simple to maximize the amount of processing resources that can fit on a chip and to enable a fast clock. <p> Due to space limitations, we do not present related work on the architectural aspects of Raw. For a detailed comparison to other architectures, see <ref> [9] </ref>. Section 1 points out that static promotion is related to memory bank disambiguation, and it lists their differences. Memory bank disambiguation was a term used by Ellis in the Bulldog Compiler [5] for a point-to-point VLIW model.
Reference: [10] <author> R. Wilson et al. </author> <title> SUIF: A Parallelizing and Optimizing Research Compiler. </title> <journal> SIGPLAN Notices, </journal> <volume> 29(12):3137, </volume> <month> December </month> <year> 1994. </year> <month> 17 </month>
References-found: 10

