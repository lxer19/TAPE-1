URL: http://bugle.cs.uiuc.edu/Papers/CICWhite.ps.Z
Refering-URL: http://bugle.cs.uiuc.edu/Projects/Test/
Root-URL: http://www.cs.uiuc.edu
Title: The Dearth of Usable Software Development Tools for High-Performance Parallel Computing Systems  
Author: Daniel A. Reed Ann H. Hayes Margaret L. Simmons 
Address: Urbana, Illinois 61801  Los Alamos, New Mexico 87545  
Affiliation: Department of Computer Science University of Illinois  Computing, Information, and Communications Division Los Alamos National Laboratory  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Messina, P., and Sterling, T., Eds. </author> <title> Pasadena Workshop on System Software and Tools for High-Performance Computing Environments. </title> <publisher> SIAM, </publisher> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: Unfortunately, the software infrastructure for massively parallel systems has not kept pace with the rapid evolution of new parallel architectures. Despite recognition of the critical importance of robust, flexible, and efficient software tools at national HPCC meetings, including those in Pasadena <ref> [1, 2] </ref> and Pittsburgh [8], little has changed in the past five years. We believe there are several fundamental reasons for the current dilemma, and they all relate to the lack of incentives for tool development and testing. <p> The critical importance of robust, flexible, easy-to-use, and efficient software tools has been emphasized repeatedly at national HPCC meetings, including those in Pasadena <ref> [1, 2] </ref> and Pittsburgh [8]. Despite this recognition, little has changed in the past five years. <p> However, as described below, we believe a more rigorous, broad-based approach is required that co-locates a formal testing and evaluation facility with one or more national groups of experienced computational scientists. These views are not simply our own. They mirror those expressed at the Pasadena <ref> [1, 2] </ref> and Pittsburgh [8] meetings. In addition, they reflect a broad consensus developed at a series of five workshops on performance and debugging tools we have hosted over the past seven years [3, 7, 4, 5, 6].
Reference: [2] <editor> Messina, P., and Sterling, T., Eds. </editor> <booktitle> Second Pasadena Workshop on System Software and Tools for HPC Environments. </booktitle> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: Unfortunately, the software infrastructure for massively parallel systems has not kept pace with the rapid evolution of new parallel architectures. Despite recognition of the critical importance of robust, flexible, and efficient software tools at national HPCC meetings, including those in Pasadena <ref> [1, 2] </ref> and Pittsburgh [8], little has changed in the past five years. We believe there are several fundamental reasons for the current dilemma, and they all relate to the lack of incentives for tool development and testing. <p> The critical importance of robust, flexible, easy-to-use, and efficient software tools has been emphasized repeatedly at national HPCC meetings, including those in Pasadena <ref> [1, 2] </ref> and Pittsburgh [8]. Despite this recognition, little has changed in the past five years. <p> However, as described below, we believe a more rigorous, broad-based approach is required that co-locates a formal testing and evaluation facility with one or more national groups of experienced computational scientists. These views are not simply our own. They mirror those expressed at the Pasadena <ref> [1, 2] </ref> and Pittsburgh [8] meetings. In addition, they reflect a broad consensus developed at a series of five workshops on performance and debugging tools we have hosted over the past seven years [3, 7, 4, 5, 6].
Reference: [3] <author> Simmons, M., Koskela, R., and Bucher, I., Eds. </author> <title> Instrumentation for Future Parallel Computing Systems. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: These views are not simply our own. They mirror those expressed at the Pasadena [1, 2] and Pittsburgh [8] meetings. In addition, they reflect a broad consensus developed at a series of five workshops on performance and debugging tools we have hosted over the past seven years <ref> [3, 7, 4, 5, 6] </ref>. These workshops have brought together vendors, academic tool researchers, and application scientists to discuss tool research and experiences with performance and debugging tools for massively parallel systems.
Reference: [4] <author> Simmons, M. L., Hayes, A. H., and Reed, D. A. </author> <booktitle> Santa Fe Workshop on Parallel Computer Systems, </booktitle> <address> Oct. 1991, Santa Fe, NM. </address>
Reference-contexts: These views are not simply our own. They mirror those expressed at the Pasadena [1, 2] and Pittsburgh [8] meetings. In addition, they reflect a broad consensus developed at a series of five workshops on performance and debugging tools we have hosted over the past seven years <ref> [3, 7, 4, 5, 6] </ref>. These workshops have brought together vendors, academic tool researchers, and application scientists to discuss tool research and experiences with performance and debugging tools for massively parallel systems.
Reference: [5] <author> Simmons, M. L., Hayes, A. H., and Reed, D. A. </author> <title> Keystone Workshop on Software Tools for Parallel Computing Systems: A Dialogue Between Users and Developers, </title> <month> Apr. </month> <year> 1993, </year> <title> Keystone, </title> <publisher> CO. </publisher>
Reference-contexts: Despite this recognition, little has changed in the past five years. Substantial research money is directed toward study and development of software tools, but as one speaker at the Keystone workshop on performance tools <ref> [5] </ref> asked, "Why don't users 2 use the tools that tool developers develop?" We believe the fundamental reasons for this dilemma relate to the lack of incentives for tool development and testing. First, the massively parallel systems market is small, and the real cost of commercial tool development is high. <p> These views are not simply our own. They mirror those expressed at the Pasadena [1, 2] and Pittsburgh [8] meetings. In addition, they reflect a broad consensus developed at a series of five workshops on performance and debugging tools we have hosted over the past seven years <ref> [3, 7, 4, 5, 6] </ref>. These workshops have brought together vendors, academic tool researchers, and application scientists to discuss tool research and experiences with performance and debugging tools for massively parallel systems.
Reference: [6] <author> Simmons, M. L., Hayes, A. H., Reed, D. A., and Brown, J., Eds. </author> <title> Debugging and Performance Tuning for Parallel Computing Systems. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: These views are not simply our own. They mirror those expressed at the Pasadena [1, 2] and Pittsburgh [8] meetings. In addition, they reflect a broad consensus developed at a series of five workshops on performance and debugging tools we have hosted over the past seven years <ref> [3, 7, 4, 5, 6] </ref>. These workshops have brought together vendors, academic tool researchers, and application scientists to discuss tool research and experiences with performance and debugging tools for massively parallel systems.
Reference: [7] <author> Simmons, M. L., and Koskela, R., Eds. </author> <title> Parallel Computing Systems: Performance Instrumentation and Visualization. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1990. </year>
Reference-contexts: These views are not simply our own. They mirror those expressed at the Pasadena [1, 2] and Pittsburgh [8] meetings. In addition, they reflect a broad consensus developed at a series of five workshops on performance and debugging tools we have hosted over the past seven years <ref> [3, 7, 4, 5, 6] </ref>. These workshops have brought together vendors, academic tool researchers, and application scientists to discuss tool research and experiences with performance and debugging tools for massively parallel systems.
Reference: [8] <author> Stevens, R. </author> <booktitle> Workshop and Conference on Grand Challenge Applications and Software Technology, </booktitle> <address> May 1993, Pittsburgh, PA. </address> <month> 6 </month>
Reference-contexts: Unfortunately, the software infrastructure for massively parallel systems has not kept pace with the rapid evolution of new parallel architectures. Despite recognition of the critical importance of robust, flexible, and efficient software tools at national HPCC meetings, including those in Pasadena [1, 2] and Pittsburgh <ref> [8] </ref>, little has changed in the past five years. We believe there are several fundamental reasons for the current dilemma, and they all relate to the lack of incentives for tool development and testing. <p> The critical importance of robust, flexible, easy-to-use, and efficient software tools has been emphasized repeatedly at national HPCC meetings, including those in Pasadena [1, 2] and Pittsburgh <ref> [8] </ref>. Despite this recognition, little has changed in the past five years. <p> However, as described below, we believe a more rigorous, broad-based approach is required that co-locates a formal testing and evaluation facility with one or more national groups of experienced computational scientists. These views are not simply our own. They mirror those expressed at the Pasadena [1, 2] and Pittsburgh <ref> [8] </ref> meetings. In addition, they reflect a broad consensus developed at a series of five workshops on performance and debugging tools we have hosted over the past seven years [3, 7, 4, 5, 6].
References-found: 8

