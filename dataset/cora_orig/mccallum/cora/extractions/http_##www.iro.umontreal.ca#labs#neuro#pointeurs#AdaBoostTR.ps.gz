URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/AdaBoostTR.ps.gz
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/other.html
Root-URL: http://www.iro.umontreal.ca
Email: fschwenk,bengioyg@iro.umontreal.ca  
Title: Adaptive Boosting of Neural Networks for Character Recognition  
Author: Holger Schwenk Yoshua Bengio 
Keyword: AdaBoost, learning algorithms, artificial neural networks, bounds on generalization error, margin distributions, autoencoders, handwritten character recognition.  
Date: May 29, 1997  
Address: Montr eal, Montreal, Qc H3C-3J7, Canada  
Affiliation: Dept. Informatique et Recherche Op erationnelle Universit e de  
Abstract: Technical Report #1072, D epartement d'Informatique et Recherche Op erationnelle, Universit e de Montr eal Abstract Boosting is a general method for improving the performance of any learning algorithm that consistently generates classifiers which need to perform only slightly better than random guessing. A recently proposed and very promising boosting algorithm is AdaBoost [5]. It has been applied with great success to several benchmark machine learning problems using rather simple learning algorithms [4], in particular decision trees [1, 2, 6]. In this paper we use AdaBoost to improve the performances of neural networks applied to character recognition tasks. We compare training methods based on sampling the training set and weighting the cost function. Our system achieves about 1.4% error on a data base of online handwritten digits from more than 200 writers. Adaptive boosting of a multi-layer network achieved 2% error on the UCI Letters offline characters data set. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman. </author> <title> Bias, variance, and Arcing classifiers. </title> <type> Technical Report 460, </type> <institution> Statistics Departement, University of California at Berkeley, </institution> <year> 1996. </year>
Reference-contexts: 1 Introduction AdaBoost [4, 5] (for Adaptive Boosting) constructs a composite classifier by sequentially training classifiers, while putting more and more emphasis on certain patterns. AdaBoost has been applied to rather weak learning algorithms (with low capacity) [4] and to decision trees <ref> [1, 2, 6] </ref>, and not yet, until now, to the best of our knowledge, to artificial neural networks. These experiments displayed rather intriguing generalization properties, such as continued decrease in generalization error after training error reaches zero. <p> These experiments displayed rather intriguing generalization properties, such as continued decrease in generalization error after training error reaches zero. Previous workers also disagree on the reasons for the impressive generalization performance displayed by AdaBoost on a large array of tasks. One issue raised by Breiman <ref> [1] </ref> and the authors of AdaBoost [4] is whether some of this effect is due to a reduction in variance similar to the one obtained from the Bagging algorithm. <p> In doing so, we also compare three different versions of AdaBoost: (R) training each hypothesis with a fixed training set obtained by resampling with replacement (with probabilities D t ) from the original training set (as in <ref> [1] </ref>), (E) training by resampling after each epoch a new training set from the original training set, and (W) training by directly weighting the cost 1 function (here the squared error) of the neural network. Note that the second version is a better approximation of the weighted cost function. <p> There is also a multi-class version, called pseudoloss-AdaBoost, that can be used when the classifier computes confidence scores for each class [4, 5]. The result of training the t th classifier is now a hypothesis h t : X fi Y ! <ref> [0; 1] </ref>. Furthermore we use a distribution over the set of all miss-labels: B = f (i; y) : i 2 f1; :::; N g; y 6= y i g. Therefore jBj = N (k 1). <p> Train neural network with respect to distribution D t and obtain hypothesis h t : X fi Y ! <ref> [0; 1] </ref> 2. calculate the pseudo-loss of h t : * t = 2 (i;y)2B 3. set fi t = * t =(1 * t ) 4. update distribution D t D t+1 (i) = Z t fi 2 ((1+h t (x i ;y i )h t (x i ;y)) where <p> Figure 3 gives an idea of the great variety of writing styles of this database. We only applied a simple preprocessing: the characters were resampled to 11 points, centered and size normalized to a (x,y)-coordinate sequence in <ref> [1; 1] </ref> 22 . Since the Diabolo classifier is invariant to small transformations we don't need to extract further features. <p> model of the different writing style within one class, this seems to be evidence that the example emphasizing of AdaBoost was able to assign them automatically to different machines. 6 The surprising effect of continuously decreasing generalization error even after training error reaches zero has already been observed by others <ref> [1, 2, 4, 6] </ref>. <p> the recently proven theorem of Schapire et al. [7]: the bound on the generalization error (equation 2) depends only on the margin distribution and on the VC-dimension of the basic learning machine (one Diabolo classifier or MLP respectively), not on the number of machines combined by AdaBoost. of x 2 <ref> [1; 1] </ref>. It is clearly visible that AdaBoost increases the number of examples with high margin: with 100 machines all examples have a margin higher than 0.5. This improvement is achieved with all versions of AdaBoost (R, E or W). <p> The behavior of AdaBoost for neural networks confirms previous observations on other learning algorithms <ref> [1, 2, 4, 6, 7] </ref>, such as the continued generalization improvement after zero training error has been reached, and the associated improvement in the margin distribution. In general, AdaBoost appears to help most the weaker models (such as the 10-hidden units networks in our experiments). <p> Furthermore these results add credence to the view of Freund and Schapire that the improvement in generalization error brought by Ada-Boost is mainly due to the emphasizing (that increases the margin), rather than to a variance reduction due to the randomization of the resampling process (as was suggested in <ref> [1] </ref>). 8
Reference: [2] <author> H. Drucker and C. Cortes, </author> <title> Boosting decision trees. </title> <booktitle> In NIPS*8, </booktitle> <pages> pages 479-485, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction AdaBoost [4, 5] (for Adaptive Boosting) constructs a composite classifier by sequentially training classifiers, while putting more and more emphasis on certain patterns. AdaBoost has been applied to rather weak learning algorithms (with low capacity) [4] and to decision trees <ref> [1, 2, 6] </ref>, and not yet, until now, to the best of our knowledge, to artificial neural networks. These experiments displayed rather intriguing generalization properties, such as continued decrease in generalization error after training error reaches zero. <p> model of the different writing style within one class, this seems to be evidence that the example emphasizing of AdaBoost was able to assign them automatically to different machines. 6 The surprising effect of continuously decreasing generalization error even after training error reaches zero has already been observed by others <ref> [1, 2, 4, 6] </ref>. <p> The behavior of AdaBoost for neural networks confirms previous observations on other learning algorithms <ref> [1, 2, 4, 6, 7] </ref>, such as the continued generalization improvement after zero training error has been reached, and the associated improvement in the margin distribution. In general, AdaBoost appears to help most the weaker models (such as the 10-hidden units networks in our experiments).
Reference: [3] <author> Feng. C., Sutherland, A., King, R., Muggleton, S., & Henery, R. </author> <title> Comparison of machine learning classifiers to statistics and neural networks. </title> <booktitle> In Proceedings of the Fourth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 41-52, </pages> <year> 1993. </year>
Reference-contexts: The best performance reported in STATLOG <ref> [3] </ref> is 6.4%.
Reference: [4] <author> Y. Freund and R.E. Schapire. </author> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Machine Learning: Proceedings of Thirteenth International Conference, </booktitle> <pages> pages 148-156, </pages> <year> 1996. </year> <month> 1 </month>
Reference-contexts: 1 Introduction AdaBoost <ref> [4, 5] </ref> (for Adaptive Boosting) constructs a composite classifier by sequentially training classifiers, while putting more and more emphasis on certain patterns. <p> 1 Introduction AdaBoost [4, 5] (for Adaptive Boosting) constructs a composite classifier by sequentially training classifiers, while putting more and more emphasis on certain patterns. AdaBoost has been applied to rather weak learning algorithms (with low capacity) <ref> [4] </ref> and to decision trees [1, 2, 6], and not yet, until now, to the best of our knowledge, to artificial neural networks. These experiments displayed rather intriguing generalization properties, such as continued decrease in generalization error after training error reaches zero. <p> Previous workers also disagree on the reasons for the impressive generalization performance displayed by AdaBoost on a large array of tasks. One issue raised by Breiman [1] and the authors of AdaBoost <ref> [4] </ref> is whether some of this effect is due to a reduction in variance similar to the one obtained from the Bagging algorithm. In this paper we explore the application of AdaBoost to Diabolo (auto-associative) networks and multi-layer neural networks (MLPs) on two character recognition tasks. <p> It converges (learns the training set) if each classifier yields a weighted error that is less than 50%, i.e., better than chance in the 2-class case. There is also a multi-class version, called pseudoloss-AdaBoost, that can be used when the classifier computes confidence scores for each class <ref> [4, 5] </ref>. The result of training the t th classifier is now a hypothesis h t : X fi Y ! [0; 1]. <p> The final decision f is obtained by adding together the weighted confidence scores of all machines. Figure 1 right summarizes the algorithm. For more details we refer the reader to the references <ref> [4, 5] </ref>. This multi-class boosting algorithm converges if each classifier yields a pseudoloss that is less than 50%, i.e., better than any constant hypothesis. <p> model of the different writing style within one class, this seems to be evidence that the example emphasizing of AdaBoost was able to assign them automatically to different machines. 6 The surprising effect of continuously decreasing generalization error even after training error reaches zero has already been observed by others <ref> [1, 2, 4, 6] </ref>. <p> Each input feature was normalized according to its mean and variance on the training set. The plain and boosted networks are compared to the 1-nearest neighbor classifier as well as to plain and boosted C4.5 decision trees (results from <ref> [4] </ref>). Table 3: Error rates on the UCI Letters data set nearest fully connected MLP C4.5 (results from [4]) neighbor alone adaboosted y alone adaboosted z train: - 3.4% 0.0% unknown 0.0% test: 4.8% 6.2% 2.0% 13.8% 3.3% y using 20 machines z using &gt; 100 machines The results obtained with <p> The plain and boosted networks are compared to the 1-nearest neighbor classifier as well as to plain and boosted C4.5 decision trees (results from <ref> [4] </ref>). Table 3: Error rates on the UCI Letters data set nearest fully connected MLP C4.5 (results from [4]) neighbor alone adaboosted y alone adaboosted z train: - 3.4% 0.0% unknown 0.0% test: 4.8% 6.2% 2.0% 13.8% 3.3% y using 20 machines z using &gt; 100 machines The results obtained with the boosted network are extremely good (2% error) and are the best that the authors know to be <p> The best performance reported in STATLOG [3] is 6.4%. Adaboosted decision trees are reported to achieve 3.3% <ref> [4, 7] </ref>, but it seems that many trees are necessary (more than 100) while we need only 20 MLPs (see table 3 and figure 6). 5 Conclusion As demonstrated in two character recognition applications, AdaBoost can significantly improve neural classifiers such as multi-layer networks and Diabolo networks. <p> The behavior of AdaBoost for neural networks confirms previous observations on other learning algorithms <ref> [1, 2, 4, 6, 7] </ref>, such as the continued generalization improvement after zero training error has been reached, and the associated improvement in the margin distribution. In general, AdaBoost appears to help most the weaker models (such as the 10-hidden units networks in our experiments).
Reference: [5] <author> Y. Freund and R.E. Schapire. </author> <title> A decision theoretic generalization of on-line learning and an application to boosting. </title> <journal> Journal of Computer and System Science, </journal> <note> to appear. 1 </note>
Reference-contexts: 1 Introduction AdaBoost <ref> [4, 5] </ref> (for Adaptive Boosting) constructs a composite classifier by sequentially training classifiers, while putting more and more emphasis on certain patterns. <p> It converges (learns the training set) if each classifier yields a weighted error that is less than 50%, i.e., better than chance in the 2-class case. There is also a multi-class version, called pseudoloss-AdaBoost, that can be used when the classifier computes confidence scores for each class <ref> [4, 5] </ref>. The result of training the t th classifier is now a hypothesis h t : X fi Y ! [0; 1]. <p> P t (i) may be used for resampling using the above described version R or E. Freund and Schapire define the pseudoloss of a learning machine as <ref> [5] </ref>: * t = 2 (i;y)2B It is minimized if the confidence scores in the correct labels are 1.0 and the confidence scores of all the wrong labels are 0.0. The final decision f is obtained by adding together the weighted confidence scores of all machines. <p> The final decision f is obtained by adding together the weighted confidence scores of all machines. Figure 1 right summarizes the algorithm. For more details we refer the reader to the references <ref> [4, 5] </ref>. This multi-class boosting algorithm converges if each classifier yields a pseudoloss that is less than 50%, i.e., better than any constant hypothesis. <p> AdaBoost has very interesting theoretical properties, in particular it can be shown that the error of the composite classifier on the training data decreases exponentially fast to zero <ref> [5] </ref>. More importantly, however, bounds on the generalization error of such a system have been formulated [7]. These are based on a notion of margin of classification, defined as the difference between the score of the correct class and the strongest score of a wrong class.
Reference: [6] <author> J.R. Quinlan. Bagging, </author> <title> Boosting and C4.5. </title> <booktitle> In 14th National Conference on Artificial Intelligence, </booktitle> <year> 1996. </year>
Reference-contexts: 1 Introduction AdaBoost [4, 5] (for Adaptive Boosting) constructs a composite classifier by sequentially training classifiers, while putting more and more emphasis on certain patterns. AdaBoost has been applied to rather weak learning algorithms (with low capacity) [4] and to decision trees <ref> [1, 2, 6] </ref>, and not yet, until now, to the best of our knowledge, to artificial neural networks. These experiments displayed rather intriguing generalization properties, such as continued decrease in generalization error after training error reaches zero. <p> model of the different writing style within one class, this seems to be evidence that the example emphasizing of AdaBoost was able to assign them automatically to different machines. 6 The surprising effect of continuously decreasing generalization error even after training error reaches zero has already been observed by others <ref> [1, 2, 4, 6] </ref>. <p> The behavior of AdaBoost for neural networks confirms previous observations on other learning algorithms <ref> [1, 2, 4, 6, 7] </ref>, such as the continued generalization improvement after zero training error has been reached, and the associated improvement in the margin distribution. In general, AdaBoost appears to help most the weaker models (such as the 10-hidden units networks in our experiments).
Reference: [7] <author> R.E. Schapire, Y. Freund, P. Bartlett, </author> <title> and W.S. Leee. Boosting the margin: A new explanation for the effectiveness of voting methods. In Machines That Learn - Snowbird, </title> <booktitle> 1997. </booktitle> <pages> 1 </pages>
Reference-contexts: AdaBoost has very interesting theoretical properties, in particular it can be shown that the error of the composite classifier on the training data decreases exponentially fast to zero [5]. More importantly, however, bounds on the generalization error of such a system have been formulated <ref> [7] </ref>. These are based on a notion of margin of classification, defined as the difference between the score of the correct class and the strongest score of a wrong class. <p> In the case in which there are just two possible labels f1; +1g, this is yf (x), where f is the composite classifier and y the correct label. Obviously, the classification is correct if the margin is positive. We now state the theorem bounding the generalization error of Adaboost <ref> [7] </ref> (and other classifiers obtained by 2 Input: sequence of N examples (x 1 ; y 1 ); : : : ; (x N ; y N ) with labels y i 2 Y = f1; : : : ; kg Init: D 1 (i) = 1=N for all i Init: <p> The distribution of the margins however plays an important role. It can be shown that the AdaBoost algorithm is especially well suited to the task of maximizing the number of training examples with large margin <ref> [7] </ref>. 3 The Diabolo Classifier Normally, neural networks used for classification are trained to map an input vector to an output vector that encodes directly the classes, usually by the so called 1-out-of-N encoding. <p> This seems to contradict Occam's razor, but it may be explained by the recently proven theorem of Schapire et al. <ref> [7] </ref>: the bound on the generalization error (equation 2) depends only on the margin distribution and on the VC-dimension of the basic learning machine (one Diabolo classifier or MLP respectively), not on the number of machines combined by AdaBoost. of x 2 [1; 1]. <p> The best performance reported in STATLOG [3] is 6.4%. Adaboosted decision trees are reported to achieve 3.3% <ref> [4, 7] </ref>, but it seems that many trees are necessary (more than 100) while we need only 20 MLPs (see table 3 and figure 6). 5 Conclusion As demonstrated in two character recognition applications, AdaBoost can significantly improve neural classifiers such as multi-layer networks and Diabolo networks. <p> The behavior of AdaBoost for neural networks confirms previous observations on other learning algorithms <ref> [1, 2, 4, 6, 7] </ref>, such as the continued generalization improvement after zero training error has been reached, and the associated improvement in the margin distribution. In general, AdaBoost appears to help most the weaker models (such as the 10-hidden units networks in our experiments).
Reference: [8] <author> H. Schwenk and M. Milgram. </author> <title> Transformation invariant autoassociation with application to handwritten character recognition. </title> <booktitle> NIPS*7, </booktitle> <pages> pages 991-998. </pages> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Furthermore, one has to calculate only one distance measure for each class to recognize. This allows to incorporate knowledge by a domain specific distance measure at a very low computational cost. In previous work <ref> [8] </ref>, we have shown that the well-known tangent-distance [11] can be used in the objective function of the autoencoders. <p> Interestingly, this intuitive algorithm has gained a theoretical justification by the recent theorem of equation 2, since this discriminant learning algorithm decreases the number of examples with low margin. This Diabolo classifier has achieved state-of-the-art results in handwritten OCR <ref> [8, 9] </ref>. Recently, we have also extended the idea of a transformation invariant distance measure to on-line character recognition [10]. One autoencoder alone, however, can not learn efficiently the model of a character if it is written in many different stroke orders and directions.
Reference: [9] <author> H. Schwenk and M. Milgram. </author> <title> Learning discriminant tangent models for handwritten character recognition. </title> <booktitle> In ICANN*96, </booktitle> <pages> pages 585-590. </pages> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: This allows to incorporate knowledge by a domain specific distance measure at a very low computational cost. In previous work [8], we have shown that the well-known tangent-distance [11] can be used in the objective function of the autoencoders. Furthermore, we used a discriminant learning algorithm <ref> [9] </ref>: the network weights are updated so that the reconstruction distance is minimized for the network of the desired class and maximized for the closest incorrect one, like in LVQ2. <p> Interestingly, this intuitive algorithm has gained a theoretical justification by the recent theorem of equation 2, since this discriminant learning algorithm decreases the number of examples with low margin. This Diabolo classifier has achieved state-of-the-art results in handwritten OCR <ref> [8, 9] </ref>. Recently, we have also extended the idea of a transformation invariant distance measure to on-line character recognition [10]. One autoencoder alone, however, can not learn efficiently the model of a character if it is written in many different stroke orders and directions.
Reference: [10] <author> H. Schwenk and M. Milgram. </author> <title> Constraint tangent distance for online character recognition. </title> <booktitle> In International Conference on Pattern Recognition, </booktitle> <pages> pages D 520-524, </pages> <year> 1996. </year>
Reference-contexts: This Diabolo classifier has achieved state-of-the-art results in handwritten OCR [8, 9]. Recently, we have also extended the idea of a transformation invariant distance measure to on-line character recognition <ref> [10] </ref>. One autoencoder alone, however, can not learn efficiently the model of a character if it is written in many different stroke orders and directions. The architecture can be extended by using several autoencoders per class, each one specializing on a particular writing style (subclass). <p> Classical clustering algorithms would probably tend to ignore subclasses with very few examples since they aren't responsible for much of the error, but this may result in poor generalization behavior. Therefore, in previous work we have manually assigned the subclass labels <ref> [10] </ref>. Of course, this is not a generally satisfactory approach, and certainly infeasible when the training set is large. <p> Both data sets have a pre-defined training and test set. The Diabolo classifier was only applied to the online data set (since it takes advantage of the structure of the input features). The online data set was collected at Paris 6 University <ref> [10] </ref>. It is writer-independent (different writers in training and test sets) and there are 203 writers, 1200 training examples and 830 test examples. Each writer gave only one example per class. Therefore, there are many different writing styles, with very different frequencies.
Reference: [11] <author> P. Simard, Y. Le Cun, and J. Denker. </author> <title> Efficient pattern recognition using a new transformation distance. </title> <booktitle> NIPS*5, </booktitle> <pages> pages 50-58. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year> <note> 1 electronically available at htttp://www.research.att.com/yoav 9 </note>
Reference-contexts: Furthermore, one has to calculate only one distance measure for each class to recognize. This allows to incorporate knowledge by a domain specific distance measure at a very low computational cost. In previous work [8], we have shown that the well-known tangent-distance <ref> [11] </ref> can be used in the objective function of the autoencoders. Furthermore, we used a discriminant learning algorithm [9]: the network weights are updated so that the reconstruction distance is minimized for the network of the desired class and maximized for the closest incorrect one, like in LVQ2.
References-found: 11

