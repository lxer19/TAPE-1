URL: ftp://ftp.speech.sri.com/pub/people/julia/papers/chi95.ps.Z
Refering-URL: http://www.speech.sri.com/people/julia/ix.html
Root-URL: 
Email: cheyer@ai.sri.com pcohen@cse.ogi.edu*  julia@sig.enst.fr cfaure@sig.enst.fr  
Title: Developing Multimodal Systems using an Agent Architecture  
Author: Luc JULIA Adam CHEYER Claudie FAURE and Philip R. COHEN 
Keyword: Agent Architecture, Multimodal Interface.  
Address: 333 Ravenswood Ave Menlo Park, CA 94025 USA  46, rue Barrault 75634 PARIS Cedex 13 FRANCE  
Affiliation: A.I.Center SRI International  ENST dpt SIG CNRS URA 820  
Abstract: In this paper, we present a new agent-based architecture that facilitates the creation of synergistic multimodal systems. This architecture, called MMAAR for Micro/Macro Agent ARchitecture, is the product of combining the best features of two existing agentstyle approaches to developing multimodal systems. A prototype of a distributed application integrating handwriting, gesture and speech recognition for a map-based task is described. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> COHEN, P.R., CHEYER, A., WANG, M. and BAEG, </author> <title> S.C. An Open Agent Architecture. </title> <booktitle> In Proc. </booktitle> <address> AAAI94 - SA (Stanford), </address> <pages> pp. 1-8. </pages>
Reference-contexts: P.R. Cohen is currently at the Oregon Graduate Institute of Science and Technology in Portland. A part of this research was supported by a contract from the Electronics and Telecommunications Research Institute (Korea). The architecture for OAA <ref> [1] </ref>, based loosely on Schwartzs FLiPSiDE system, uses a blackboardstyle concept to provide content-based message routing, global data management, and process coordination for a collection of agent clients connected to the blackboard server.
Reference: 2. <author> FAURE, C. and JULIA, L. </author> <title> An Agent-Based Architecture for a Multimodal Interface. </title> <booktitle> In Proc. AAAI94 - IM4S (Stanford), </booktitle> <pages> pp. 82-86. </pages>
Reference-contexts: Architecture of TAPAGE The TAPAGE system provides a true synergistic multimodal interface, where users can combine modalities simultaneously and freely. To capture signals emitted during a users interaction, TAPAGE integrates a set of modality agents, each responsible for a very specialized kind of signal <ref> [2] </ref>. The modality agents are connected to an interpret agent who is responsible for combining the inputs across all modalities to form a valid command for the application. Commands in TAPAGE are represented by a verb and associated argument slots.
References-found: 2

