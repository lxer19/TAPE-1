URL: http://nathan.gmd.de/persons/dietrich.wettschereck/pubs/ecml94.ps
Refering-URL: http://nathan.gmd.de/persons/dietrich.wettschereck/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: wettscd@cs.orst.edu  
Title: A Hybrid Nearest-Neighbor and Nearest-Hyperrectangle Algorithm  
Author: Dietrich Wettschereck 
Note: To appear in the Proceedings of the 7th European Conference on Machine Learning  
Address: Hall 303  Corvallis, OR 97331-3202 USA  
Affiliation: Dearborn  Department of Computer Science Oregon State University  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> Aha, D.W.: </author> <title> A Study of Instance-Based Algorithms for Supervised Learning Tasks. </title> <type> Technical Report, </type> <institution> University of California, </institution> <address> Irvine (1990) </address>
Reference-contexts: If the example is equidistant to several hyperrectangles, the smallest of these is taken to be the "nearest" hyperrectan-gle. In our implementation of NGE, we first make a pass over the training examples and normalize the values of each feature into the interval [0,1] (linear normalization <ref> [1] </ref>). Features of values in the test set are normalized by the same scaling factors (but note that they may fall outside the [0,1] range). Aside from this scaling pass, the algorithm is entirely incremental. The original NGE algorithm was designed for continuous features only. <p> Better classification accuracy can often be achieved by using more than the first nearest neighbor to classify a query. The number k of neighbors to be considered is usually determined via leave-one-out cross-validation [13]. Aha <ref> [1] </ref> describes several space-efficient variations of nearest-neighbor algorithms. 1.3 Experimental Methods and Test Domains To measure the performance of the NGE and nearest neighbor algorithms, we employed the training set/test set methodology. <p> The decision boundaries in Tasks A and C are rectangular, while in Task B the boundary is diagonal. The data sets for the other eight domains were obtained from the UC-Irvine repository <ref> [1, 6] </ref> of machine learning databases. Table 1 describes some of the characteristics of the domains used. <p> Table 1. Domain characteristics (modified from Aha <ref> (1990) </ref>). B = Boolean, C = Continuous, N = Nominal.
Reference: 2. <author> Carpenter, G.A., Grossberg, S., Markuzon, N., Reynolds, J.h., Rosen, </author> <title> D.B.: Fuzzy ARTMAP: A Neural Network Architecture for Incremental Supervised Learning of Analog Multidimensional Maps. </title> <booktitle> IEEE Transactions on Neural Networks 3 (1992) 698-713 </booktitle>
Reference: 3. <author> Dasarathy, </author> <title> B.V.: Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. </title> <publisher> IEEE Computer Society Press (1991) </publisher>
Reference-contexts: These prototypes can be used to justify or explain decisions made by the classifier. The NGE algorithm, as described in this paper, differs from Nearest Neighbor methods, including those with reduced exemplar sets <ref> [3, Chapter 6] </ref>, through its rectangular bias and its ability to distinguish queries that fall inside of some hyperrectangles from those that are not covered by any hyperrectangle. It differs from Parzen Windows [9] in that hyperrectangles can be of varying sizes and edge lengths may be unequal. <p> On the other hand, the metric may heavily influence the number and shape of hyperrectangles constructed. 1.2 The Nearest Neighbor Algorithm One of the most venerable algorithms in machine learning is the nearest neighbor algorithm (NN, see <ref> [3] </ref> for a survey of the literature). The entire training set is stored in memory. To classify a new example, the Euclidean distance (possibly weighted) is computed between the example and each stored training example, and the new example is assigned the class of the nearest neighboring example.
Reference: 4. <author> Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, K., Sandhu, S., Guppy, K., Lee, S., Froelicher, V.: </author> <title> Rapid searches for complex patterns in biological molecules. </title> <journal> American Journal of Cardiology 64 (1989) 304-310 </journal>
Reference-contexts: Table 1 describes some of the characteristics of the domains used. There are a few important points to note: (a) the Waveform-40 domain is identical to the Waveform-21 domain with the addition of 19 irrelevant features (having random values), (b) the Cleveland database <ref> [4] </ref> contains some missing features, and (c) many input features in the Hungarian database [4] and the Voting Record database are missing. Table 1. Domain characteristics (modified from Aha (1990)). B = Boolean, C = Continuous, N = Nominal. <p> There are a few important points to note: (a) the Waveform-40 domain is identical to the Waveform-21 domain with the addition of 19 irrelevant features (having random values), (b) the Cleveland database <ref> [4] </ref> contains some missing features, and (c) many input features in the Hungarian database [4] and the Voting Record database are missing. Table 1. Domain characteristics (modified from Aha (1990)). B = Boolean, C = Continuous, N = Nominal.
Reference: 5. <author> Friedman, J.H., Bentley J.L., Finkel, R.A.: </author> <title> An Algorithm for Finding Best Matches in Logarithmic Expected Time. </title> <journal> ACM Transactions on Mathematical Software. </journal> <month> 3 </month> <year> (1977) </year> <month> 209-226 </month>
Reference-contexts: Wave-40 3.10.1 (1%) 14.4% Cleveland 23.00.8 (11%) 35.8% Hungarian 25.30.5 (12%) 49.8% Voting 12.30.7 (4%) 78.0% Letter recogn. 663.52.7 (4%) 68.3% fl All other test examples were classified by kNN 2 Formula (1) assumes that retrieval of training data is not conducted more efficiently with methods such as k-d trees <ref> [5] </ref> or box-trees [7]. In domains with many relevant features, neither k-d trees nor box-trees provide significant speedups over serial search.
Reference: 6. <author> Murphy, P.M., Aha, D.W.: </author> <title> UCI Repository of machine learning databases [Machine-readable data repository]. </title> <type> Technical Report, </type> <institution> University of California, </institution> <address> Irvine (1991) </address>
Reference-contexts: The decision boundaries in Tasks A and C are rectangular, while in Task B the boundary is diagonal. The data sets for the other eight domains were obtained from the UC-Irvine repository <ref> [1, 6] </ref> of machine learning databases. Table 1 describes some of the characteristics of the domains used.
Reference: 7. <author> Omohundro, </author> <title> S.M.: Five Balltree Construction Algorithms. </title> <type> Technical Report, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA (1989) </address>
Reference-contexts: 14.4% Cleveland 23.00.8 (11%) 35.8% Hungarian 25.30.5 (12%) 49.8% Voting 12.30.7 (4%) 78.0% Letter recogn. 663.52.7 (4%) 68.3% fl All other test examples were classified by kNN 2 Formula (1) assumes that retrieval of training data is not conducted more efficiently with methods such as k-d trees [5] or box-trees <ref> [7] </ref>. In domains with many relevant features, neither k-d trees nor box-trees provide significant speedups over serial search.
Reference: 8. <author> Omohundro, </author> <title> S.M.: Best-First Model Merging for Dynamic Learning and Recognition. </title> <booktitle> Neural Information Processing Systems 4 San Mateo California: </booktitle> <publisher> Morgan Kaufmann Publishers, INC. </publisher> <year> (1992) </year> <month> 958-965 </month>
Reference-contexts: Future work will introduce noise tolerance into the BNGE algorithm by introducing a mechanism for accepting merges of hyperrectangles even if examples of other classes would be covered. A conceivable approach would be Omohundro's bottom-up model merging approach <ref> [8] </ref>, for example. The KBNGE algorithm exhibits classification accuracies comparable to the best known accuracies, it is fast in training and testing time, and it is easy to use. We believe KBNGE is an important tool to include in the set of commonly used machine learning algorithms.
Reference: 9. <author> Parzen, E.: </author> <title> An estimation of a probability density function and mode. </title> <journal> Ann. Math. Stat. </journal> <month> 33 </month> <year> (1962) </year> <month> 1065-1076 </month>
Reference-contexts: It differs from Parzen Windows <ref> [9] </ref> in that hyperrectangles can be of varying sizes and edge lengths may be unequal. Salzberg [10] achieved promising classification results with NGE in three domains.
Reference: 10. <author> Salzberg, S.: </author> <title> A Nearest Hyperrectangle Learning Method. </title> <note> Machine Learning 6 (1991) 277-309 </note>
Reference-contexts: 1 Introduction Salzberg <ref> [10] </ref> describes a family of learning algorithms based on nested generalized exemplars (NGE). In NGE, an exemplar is a single training example and a generalized exemplar is an axis-parallel hyperrectangle that may cover several training examples. These hyperrectangles may overlap or nest. <p> It differs from Parzen Windows [9] in that hyperrectangles can be of varying sizes and edge lengths may be unequal. Salzberg <ref> [10] </ref> achieved promising classification results with NGE in three domains. However, Wettschereck & Dietterich [14] have reported that when tested in 11 additional domains, NGE does not perform as well as the nearest neighbor (NN) algorithm in 6 out of the 11.
Reference: 11. <author> Schaffer, C.: </author> <title> Overfitting Avoidance as Bias. </title> <note> Machine Learning 10 (1993) 153-178 </note>
Reference-contexts: It is important to note that the main purpose of the pruning technique described here is to find a more compact BNGE classifier with somewhat similar classification accuracy. Since this approach is a modification of BNGE's bias, it may also suffer from the same problems as other pruning techniques <ref> [11] </ref> with respect to classification accuracy. However, pruning never increases the amount of storage required by BNGE. BNGE BNGE p1 BNGE BNGE p2 BNGE BNGE p5 BNGE P erformance relativ e BNGE without pruning Fig. 2.
Reference: 12. <author> Simpson, P.K.: </author> <title> Fuzzy min-max neural networks: 1. Classification. </title> <booktitle> IEEE Transactions on Neural Networks 3 (1992) 776-786 </booktitle>
Reference: 13. <author> Weiss, </author> <title> S.M., Kulikowski, C.A.: Computer Systems that learn. </title> <address> San Mateo Califor-nia: </address> <publisher> Morgan Kaufmann Publishers, INC. </publisher> <year> (1991) </year>
Reference-contexts: Better classification accuracy can often be achieved by using more than the first nearest neighbor to classify a query. The number k of neighbors to be considered is usually determined via leave-one-out cross-validation <ref> [13] </ref>. Aha [1] describes several space-efficient variations of nearest-neighbor algorithms. 1.3 Experimental Methods and Test Domains To measure the performance of the NGE and nearest neighbor algorithms, we employed the training set/test set methodology. <p> In particular, the votes on adoption of the budget and the physician fee freeze were most informative, and 11 of the 16 features were irrelevant. 3 A Hybrid Algorithm - KBNGE algorithm (kNN, k determined via leave-one-out cross-validation <ref> [13] </ref>) in eleven domains. Shown are relative performance differences between kNN and the other algorithms compared. An asterisk appears when the difference is statistically significant. BNGE (without pruning) outperforms the first Nearest Neighbor algorithm (NN) in 3 domains and is outperformed by NN in 4 other domains. <p> The value of x differs from domain to domain (see Table 3) and must be determined empirically. The justification for the formula is that kNN has to compare each query to all training 1 Once again, k values were determined via leave-one-out cross-validation <ref> [13] </ref>.
Reference: 14. <author> Wettschereck, D., Dietterich, T.G.: </author> <title> An Experimental Comparison of the Nearest--Neighbor and Nearest-Hyperrectangles Algorithms. </title> <note> Machine Learning (to appear) </note>
Reference-contexts: It differs from Parzen Windows [9] in that hyperrectangles can be of varying sizes and edge lengths may be unequal. Salzberg [10] achieved promising classification results with NGE in three domains. However, Wettschereck & Dietterich <ref> [14] </ref> have reported that when tested in 11 additional domains, NGE does not perform as well as the nearest neighbor (NN) algorithm in 6 out of the 11. Wettschereck & Dietterich [14] point out some weaknesses of NGE and suggest ways to improve its performance. <p> Salzberg [10] achieved promising classification results with NGE in three domains. However, Wettschereck & Dietterich <ref> [14] </ref> have reported that when tested in 11 additional domains, NGE does not perform as well as the nearest neighbor (NN) algorithm in 6 out of the 11. Wettschereck & Dietterich [14] point out some weaknesses of NGE and suggest ways to improve its performance. The single most successful improvement in predictive accuracy can be achieved by elimination of overlapping hyperrectangles. <p> A performance difference was considered significant when the p-value was smaller than 0.05. Eleven domains of varying size and complexity were used to compare the performance of NGE to nearest neighbor. The first three data sets are two dimensional data sets especially constructed in Wettschereck & Dietterich <ref> [14] </ref> to evaluate NGE. The decision boundaries in Tasks A and C are rectangular, while in Task B the boundary is diagonal. The data sets for the other eight domains were obtained from the UC-Irvine repository [1, 6] of machine learning databases. <p> In domains where they do provide speedups, KBNGE could also be acceler ated by storing the hyperrectangles in a box tree. 4 Conclusions and Discussion A batch version of NGE without overlapping hyperrectangles, called BNGE, was introduced in Wettschereck & Dietterich <ref> [14] </ref> and shown to significantly outperform NGE in most domains tested. A simple pruning technique, which significantly reduces the amount of storage required by NGE and BNGE, is introduced in this paper.
Reference: 15. <author> Wettschereck, D.: </author> <title> A Study of Distance-Based and Local Machine Learning Algorithms. </title> <type> Ph.D. Thesis. </type> <institution> Oregon State University, </institution> <note> OR (to appear) </note>
Reference-contexts: i H lower;f i E f i if H j lower;f i &gt; E f i 0 otherwise else d f i (E; H j ) = 0 otherwise Choice of the distance metric can significantly influence the performance of any distance-based machine learning algorithm in domains with continuous features <ref> [15] </ref>. Euclidean distance (L 2 -norm) is used in this paper for NGE. Note that the decision whether a query is inside or outside of a hyperrectangle is independent of the metric.
References-found: 15

