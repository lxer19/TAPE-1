URL: ftp://ftp.comlab.ox.ac.uk/pub/Packages/ILP/Papers/slp.ps
Refering-URL: http://gruffle.comlab.ox.ac.uk/oucl/groups/machlearn/mlg_pub.html
Root-URL: 
Title: Stochastic Logic Programs  
Author: Stephen Muggleton 
Address: Parks Road, Oxford, OX1 3QD, United Kingdom.  
Affiliation: Oxford University Computing Laboratory,  
Abstract: One way to represent a machine learning algorithm's bias over the hypothesis and instance space is as a pair of probability distributions. This approach has been taken both within Bayesian learning schemes and the framework of U-learnability. However, it is not obvious how an Inductive Logic Programming (ILP) system should best be provided with a probability distribution. This paper extends the results of a previous paper by the author which introduced stochastic logic programs as a means of providing a structured definition of such a probability distribution. Stochastic logic programs are a generalisation of stochastic grammars. A stochastic logic program consists of a set of labelled clauses p : C where p is from the interval [0; 1] and C is a range-restricted definite clause. A stochastic logic program P has a distributional semantics, that is one which assigns a probability distribution to the atoms of each predicate in the Herbrand base of the clauses in P . These probabilities are assigned to atoms according to an SLD-resolution strategy which employs a stochastic selection rule. It is shown that the probabilities can be computed directly for fail-free logic programs and by normalisation for arbitrary logic programs. The stochastic proof strategy can be used to provide three distinct functions: 1) a method of sampling from the Herbrand base which can be used to provide selected targets or example sets for ILP experiments, 2) a measure of the information content of examples or hypotheses; this can be used to guide the search in an ILP system and 3) a simple method for conditioning a given stochastic logic program on samples of data. Functions 1) and 3) are used to measure the generality of hypotheses in the ILP system Progol4.2. This supports an implementation of a Bayesian technique for learning from positive examples only. fl This paper is an extension of a paper with the same title which appeared in [12] 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. </author> <title> Boole. The Laws of Thought. </title> <publisher> MacMillan & Co., </publisher> <address> London, </address> <month> 1854. </month>
Reference-contexts: 1 Introduction The integration of logic and probability theory has been the subject of many studies <ref> [1, 3, 9, 5, 6, 17, 20] </ref>. It has also been investigated within logic programming [16, 19]. Recently the motivation for many such studies has been the development of formalisms for rule-based expert systems which employ uncertain reasoning. By contrast, the motivation for the present paper comes from machine learning. <p> Q is a set of states. is an alphabet of symbols. q 0 is the initial state and F Q (F = fq 2 g in Figure 1) is the set of final states. ffi : (Q n F ) fi ! Q fi <ref> [0; 1] </ref> is a stochastic transition function which associates probabilities with labelled transitions between states. The sum of probabilities associated with transitions from any state q 2 (Q n F ) is 1. In the following represents the empty string. <p> The sum of probabilities associated with transitions from any state q 2 (Q n F ) is 1. In the following represents the empty string. The transition function ffi fl : (Q n F ) fi fl ! Q fi <ref> [0; 1] </ref> is defined as follows. ffi fl (q; ) = hq; 1i. ffi fl (q; au) = hq au ; p a p u i if and only if ffi (q; a) = hq a ; p a i and ffi fl (q a ; u) = hq au ; <p> C is said to be range-restricted if and only if every variable in the head of C is found in the body of C. A stochastic clause is a pair p : C where p is in the interval <ref> [0; 1] </ref> and C is a range-restricted clause. A set of stochastic clauses P is called a stochastic logic program if and only if for each predicate symbol q in P the probability labels for all clauses with q in the head sum to 1. Example 6 Coin example. <p> Figure 6 shows a recursive SLP P which describes a polynomial distribution over the natural numbers expressed in reverse binary form. Numbers are constructed by first choosing the length of 8 1:0 : natp (N ) nate (U ); bin (U; N ) 0:5 : bin (0; <ref> [1] </ref>) 0:5 : bin (s (U ); [CjN ]) coin (C); bin (U; N ) the binary representation and then filling out the binary expression by repeated tossing of a fair coin (see Figure 4). <p> The predicate random clause/3 and its sub-predicate choose/6 are shown in predicate label/2, which is also implemented as a primitive in CProgol4.2 (see above). Note that since P1 is simply a ratio, it is immaterial whether the labels are themselves in the interval <ref> [0; 1] </ref> or simply arbitrary positive reals. CPro-gol4.2 by default assigns all clauses a label value of 1.
Reference: [2] <author> W. Buntine. </author> <title> A Theory of Learning Classification Rules. </title> <type> PhD thesis, </type> <institution> School of Computing Science, University of Technology, </institution> <address> Sydney, </address> <year> 1990. </year>
Reference-contexts: One way to represent a machine learning algorithm's bias over the hypothesis and instance space is as a pair of probability distributions. This approach has been taken in various ways within the frameworks of PAC-learnability [21], Bayesian learning <ref> [2, 7] </ref> and U-learnability [11, 14]. However, it is not obvious how a machine learning algorithm, in particular an Inductive Logic Programming (ILP) system [10, 15] should best be provided with a probability distribution over a set of logical formulae.
Reference: [3] <author> R. Carnap. </author> <title> The logical foundations of probability. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1962. </year>
Reference-contexts: 1 Introduction The integration of logic and probability theory has been the subject of many studies <ref> [1, 3, 9, 5, 6, 17, 20] </ref>. It has also been investigated within logic programming [16, 19]. Recently the motivation for many such studies has been the development of formalisms for rule-based expert systems which employ uncertain reasoning. By contrast, the motivation for the present paper comes from machine learning.
Reference: [4] <author> W.F. Clocksin and C.S. Mellish. </author> <title> Programming in Prolog. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: P r (jG) = 0:5, P r (abjG) = 0:25, P r (aabbjG) = 0:125. 5 0:5 : S ! 0:5 : coin (0) 0:5 : coin (1) 4 Stochastic logic programs Every context-free grammar can be expressed as a definite clause grammar <ref> [4] </ref>. For this reason the generalisation of stochastic context-free grammars to stochastic logic programs (SLPs) is reasonably straightforward. First, a definite clause C is defined in the standard way as having the following form.
Reference: [5] <author> A.P. Dempster. </author> <title> A generalisation of bayesian inference. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 30 </volume> <pages> 205-247, </pages> <year> 1968. </year>
Reference-contexts: 1 Introduction The integration of logic and probability theory has been the subject of many studies <ref> [1, 3, 9, 5, 6, 17, 20] </ref>. It has also been investigated within logic programming [16, 19]. Recently the motivation for many such studies has been the development of formalisms for rule-based expert systems which employ uncertain reasoning. By contrast, the motivation for the present paper comes from machine learning.
Reference: [6] <author> R. Fagin and J. Halpern. </author> <title> Uncertainty, belief and probability. </title> <booktitle> In Proceedings of IJCAI-89, </booktitle> <address> San Mateo, CA, 1988. </address> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: 1 Introduction The integration of logic and probability theory has been the subject of many studies <ref> [1, 3, 9, 5, 6, 17, 20] </ref>. It has also been investigated within logic programming [16, 19]. Recently the motivation for many such studies has been the development of formalisms for rule-based expert systems which employ uncertain reasoning. By contrast, the motivation for the present paper comes from machine learning.
Reference: [7] <author> D. Haussler, M Kearns, and R. Shapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <booktitle> In COLT-91: Proceedings of the 4th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 61-74, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kauffmann. </publisher>
Reference-contexts: One way to represent a machine learning algorithm's bias over the hypothesis and instance space is as a pair of probability distributions. This approach has been taken in various ways within the frameworks of PAC-learnability [21], Bayesian learning <ref> [2, 7] </ref> and U-learnability [11, 14]. However, it is not obvious how a machine learning algorithm, in particular an Inductive Logic Programming (ILP) system [10, 15] should best be provided with a probability distribution over a set of logical formulae.
Reference: [8] <author> K. Lari and S. J. Young. </author> <title> The estimation of stochastic context-free grammars using the inside-outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4 </volume> <pages> 35-56, </pages> <year> 1990. </year>
Reference-contexts: This paper proposes stochastic logic programs (SLPs) as a means of providing a structured definition of such a probability distribution. SLPs are a generalisation of stochastic grammars <ref> [8] </ref>. Although they have a distributional semantics, SLPs' relationship to Probabilistic Logic Programs [16] and BS-programs [19] is unclear. This paper is organised as follows. In Section 2 the formal framework for U-learnability is introduced. Stochastic grammars are then defined and described in Section 3. <p> The process terminates once the string contains no non-terminals. The probability of the generated string is the product of the labels of rewrite rules used. 3.3 Stochastic context-free grammars Stochastic context-free grammars <ref> [8] </ref> can be treated in the same way as the labelled productions of the last section.
Reference: [9] <author> J. </author> <title> Lukasiewicz. Logical foundations of probability theory. </title> <editor> In L. Berkowski, editor, </editor> <booktitle> Selected works of Jan Lukasiewicz. </booktitle> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1970. </year>
Reference-contexts: 1 Introduction The integration of logic and probability theory has been the subject of many studies <ref> [1, 3, 9, 5, 6, 17, 20] </ref>. It has also been investigated within logic programming [16, 19]. Recently the motivation for many such studies has been the development of formalisms for rule-based expert systems which employ uncertain reasoning. By contrast, the motivation for the present paper comes from machine learning.
Reference: [10] <author> S. Muggleton. </author> <title> Inductive logic programming. </title> <journal> New Generation Computing, </journal> <volume> 8(4) </volume> <pages> 295-318, </pages> <year> 1991. </year>
Reference-contexts: This approach has been taken in various ways within the frameworks of PAC-learnability [21], Bayesian learning [2, 7] and U-learnability [11, 14]. However, it is not obvious how a machine learning algorithm, in particular an Inductive Logic Programming (ILP) system <ref> [10, 15] </ref> should best be provided with a probability distribution over a set of logical formulae. This paper proposes stochastic logic programs (SLPs) as a means of providing a structured definition of such a probability distribution. SLPs are a generalisation of stochastic grammars [8].
Reference: [11] <author> S. Muggleton. </author> <title> Bayesian inductive logic programming. </title> <editor> In W. Cohen and H. Hirsh, editors, </editor> <booktitle> Proceedings of the Eleventh International Machine Learning Conference, </booktitle> <pages> pages 371-379, </pages> <address> San Mateo, CA, 1994. </address> <publisher> Morgan-Kaufmann. </publisher>
Reference-contexts: One way to represent a machine learning algorithm's bias over the hypothesis and instance space is as a pair of probability distributions. This approach has been taken in various ways within the frameworks of PAC-learnability [21], Bayesian learning [2, 7] and U-learnability <ref> [11, 14] </ref>. However, it is not obvious how a machine learning algorithm, in particular an Inductive Logic Programming (ILP) system [10, 15] should best be provided with a probability distribution over a set of logical formulae. <p> Section 5 describes a Prolog implementation of stochastic logic programs. A discussion of research issues and applications of stochastic logic programs concludes the paper in Section 6. 2 U-learnability The following is a variant of the U-learnability framework presented in <ref> [11, 14] </ref>. The teacher starts by choosing distributions D H and D X from the family of distributions D H and D X over concept descriptions H (wffs with associated bounds for time taken to test entailment) and instances X (ground wffs) respectively.
Reference: [12] <author> S. Muggleton. </author> <title> Stochastic logic programs. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> Advances in Inductive Logic Programming. </booktitle> <publisher> IOS Press/Ohmsha, </publisher> <year> 1995. </year>
Reference: [13] <author> S. Muggleton. </author> <title> Learning from positive data. </title> <booktitle> In Proceedings of the Sixth Inductive Logic Programming Workshop, </booktitle> <address> Stockholm University, </address> <year> 1996. </year>
Reference-contexts: Since SLPs seem a simple and natural extension of logic programs it is hoped that they might find further applications within logic programming. Possible areas for application include robotics, planning and natural language. SLPs have been applied in the problem of learning from positive examples only <ref> [13] </ref>. This required the implementation of the following function which defines the generality of an hypothesis. g (H) = x2H The generality is thus the sum of the probability of all instances of hypothesis H. Clearly such a sum can be infinite.
Reference: [14] <author> S. </author> <title> Muggleton and C.D. Page. A learnability model for universal representations. </title> <type> Technical Report PRG-TR-3-94, </type> <institution> Oxford University Computing Laboratory, Oxford, </institution> <year> 1994. </year>
Reference-contexts: One way to represent a machine learning algorithm's bias over the hypothesis and instance space is as a pair of probability distributions. This approach has been taken in various ways within the frameworks of PAC-learnability [21], Bayesian learning [2, 7] and U-learnability <ref> [11, 14] </ref>. However, it is not obvious how a machine learning algorithm, in particular an Inductive Logic Programming (ILP) system [10, 15] should best be provided with a probability distribution over a set of logical formulae. <p> Section 5 describes a Prolog implementation of stochastic logic programs. A discussion of research issues and applications of stochastic logic programs concludes the paper in Section 6. 2 U-learnability The following is a variant of the U-learnability framework presented in <ref> [11, 14] </ref>. The teacher starts by choosing distributions D H and D X from the family of distributions D H and D X over concept descriptions H (wffs with associated bounds for time taken to test entailment) and instances X (ground wffs) respectively. <p> if and only if there exists a Turing machine learner L such that for any choice of ffi and * (0 &lt; ffi; * 1) with probability at least (1 ffi) in any of the sessions m is less than a fixed polynomial function of 1 ffi and 1 In <ref> [14] </ref> positive results were given for the U-learnability of time-bounded logic programs. <p> The distributions described in <ref> [14] </ref> include both those that decay exponentially over the length of formulae and those that decay polynomially. SLPs can easily be used to describe an exponential decay distribution over the natural numbers as follows. Example 11 Exponential distribution.
Reference: [15] <author> S. Muggleton and L. De Raedt. </author> <title> Inductive logic programming: Theory and methods. </title> <journal> Journal of Logic Programming, </journal> <volume> 19,20:629-679, </volume> <year> 1994. </year>
Reference-contexts: This approach has been taken in various ways within the frameworks of PAC-learnability [21], Bayesian learning [2, 7] and U-learnability [11, 14]. However, it is not obvious how a machine learning algorithm, in particular an Inductive Logic Programming (ILP) system <ref> [10, 15] </ref> should best be provided with a probability distribution over a set of logical formulae. This paper proposes stochastic logic programs (SLPs) as a means of providing a structured definition of such a probability distribution. SLPs are a generalisation of stochastic grammars [8].
Reference: [16] <author> R. Ng and V.S. Subrahmanian. </author> <title> Probabilistic logic programming. </title> <journal> Information and Computation, </journal> <volume> 101(2) </volume> <pages> 150-201, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction The integration of logic and probability theory has been the subject of many studies [1, 3, 9, 5, 6, 17, 20]. It has also been investigated within logic programming <ref> [16, 19] </ref>. Recently the motivation for many such studies has been the development of formalisms for rule-based expert systems which employ uncertain reasoning. By contrast, the motivation for the present paper comes from machine learning. <p> This paper proposes stochastic logic programs (SLPs) as a means of providing a structured definition of such a probability distribution. SLPs are a generalisation of stochastic grammars [8]. Although they have a distributional semantics, SLPs' relationship to Probabilistic Logic Programs <ref> [16] </ref> and BS-programs [19] is unclear. This paper is organised as follows. In Section 2 the formal framework for U-learnability is introduced. Stochastic grammars are then defined and described in Section 3. Section 4 introduces stochastic logic programs as a generalisation of stochastic grammars.
Reference: [17] <author> N. Nilsson. </author> <title> Probabilistic logic. </title> <journal> Artificial Intelligence Journal, </journal> <volume> 28 </volume> <pages> 71-87, </pages> <year> 1986. </year>
Reference-contexts: 1 Introduction The integration of logic and probability theory has been the subject of many studies <ref> [1, 3, 9, 5, 6, 17, 20] </ref>. It has also been investigated within logic programming [16, 19]. Recently the motivation for many such studies has been the development of formalisms for rule-based expert systems which employ uncertain reasoning. By contrast, the motivation for the present paper comes from machine learning.
Reference: [18] <author> L.R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-286, </pages> <year> 1989. </year>
Reference-contexts: Clearly both D H and D X are functions over countably infinite domains. Stochastic grammars provide one approach to defining a probability distribution over a countably infinite set. 3 Stochastic grammars 3.1 Stochastic automata Stochastic automata, otherwise called Hidden Markov Models <ref> [18] </ref>, have found many applications in speech recognition. An example is shown in Figure 1. Stochastic automata are defined by a 5-tuple A = hQ; ; q 0 ; F; ffii.
Reference: [19] <author> T. Sato. </author> <title> A statistical learning method for logic programs with distributional semantics. </title> <editor> In L. Sterling, editor, </editor> <booktitle> Proceedings of the Twelth International conference on logic programming, </booktitle> <pages> pages 715-729, </pages> <address> Cambridge, Mas-sachusetts, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: 1 Introduction The integration of logic and probability theory has been the subject of many studies [1, 3, 9, 5, 6, 17, 20]. It has also been investigated within logic programming <ref> [16, 19] </ref>. Recently the motivation for many such studies has been the development of formalisms for rule-based expert systems which employ uncertain reasoning. By contrast, the motivation for the present paper comes from machine learning. <p> This paper proposes stochastic logic programs (SLPs) as a means of providing a structured definition of such a probability distribution. SLPs are a generalisation of stochastic grammars [8]. Although they have a distributional semantics, SLPs' relationship to Probabilistic Logic Programs [16] and BS-programs <ref> [19] </ref> is unclear. This paper is organised as follows. In Section 2 the formal framework for U-learnability is introduced. Stochastic grammars are then defined and described in Section 3. Section 4 introduces stochastic logic programs as a generalisation of stochastic grammars. <p> The probability of the derivation of a is the product of the probability of the choices in the refutation. As with stochastic context-free grammars, the probability of a is then the sum of the probabilities of the derivations of a. This stochastic SLD-strategy corresponds to a distributional semantics <ref> [19] </ref> for P . That is, each atom a in the success set of n (P ) is assigned a non-zero probability (due to the completeness of SLD-derivation).
Reference: [20] <author> G. Shafer. </author> <title> A mathematical theory of evidence. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1976. </year>
Reference-contexts: 1 Introduction The integration of logic and probability theory has been the subject of many studies <ref> [1, 3, 9, 5, 6, 17, 20] </ref>. It has also been investigated within logic programming [16, 19]. Recently the motivation for many such studies has been the development of formalisms for rule-based expert systems which employ uncertain reasoning. By contrast, the motivation for the present paper comes from machine learning.
Reference: [21] <author> L.G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: By contrast, the motivation for the present paper comes from machine learning. One way to represent a machine learning algorithm's bias over the hypothesis and instance space is as a pair of probability distributions. This approach has been taken in various ways within the frameworks of PAC-learnability <ref> [21] </ref>, Bayesian learning [2, 7] and U-learnability [11, 14]. However, it is not obvious how a machine learning algorithm, in particular an Inductive Logic Programming (ILP) system [10, 15] should best be provided with a probability distribution over a set of logical formulae.
References-found: 21

