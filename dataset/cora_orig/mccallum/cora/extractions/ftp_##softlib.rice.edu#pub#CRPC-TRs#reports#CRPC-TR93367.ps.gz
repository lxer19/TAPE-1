URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93367.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Runtime Support and Compilation Methods for User-Specified Data Distributions  
Author: Ravi Ponnusamy yz Joel Saltz Alok Choudhary Yuan-Shin Hwang Geoffrey Fox 
Address: College Park, MD 20742 Syracuse, NY 13244  
Affiliation: UMIACS and Computer Science Dept. Northeast Parallel Architectures Center University of Maryland Syracuse University  
Abstract: This paper describes two new ideas by which an HPF compiler can deal with irregular computations effectively. The first mechanism invokes a user-specified parallel mapping procedure via a set of compiler directives. The directives allow use of program arrays to describe graph connectivity, spatial location of array elements and computational load. The second mechanism is a simple conservative method that in many cases enables a compiler to recognize that it is possible to reuse previously computed information from inspectors (e.g. communication schedules, loop iteration partitions, information that associates off-processor data copies with on-processor buffer locations). We present performance results for these mechanisms from a Fortran 90D compiler implementation. fl This work was sponsored in part by ARPA (NAG-1-1485), NSF (ASC 9213821) and ONR (SC292-1-22913). Also supported by NASA Contract No. NAS1-19480 while author Saltz was in residence at ICASE, NASA Langley Research Center, Hampton, Virginia. Author Choudhary was also supported by NSF Young Investigator award (CCR-9357840). The content of the information does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Baden. </author> <title> Programming abstractions for dynamically partitioning and coordinating localized scientific calculations running on multiprocessors. </title> <journal> SIAM J. Sci. and Stat. Computation., </journal> <volume> 12(1), </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: Baden <ref> [1] </ref> has developed a programming environment targeted towards particle computations. This programming environment provides facilities that support dynamic load balancing.
Reference: [2] <author> S.T. Barnard and H. Simon. </author> <title> A fast multilevel implementation of recursive spectral bisection for partitioning unstructured problems. </title> <type> Technical Report RNR-92-033, </type> <institution> NAS Systems Division, NASA Ames Research Center, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Only a modest effort was made to produce an efficient parallel implementation of the partitioner and we believe that the performance and the execution time of the partitioner can be tremendously improved by using a multilevel version of the partitioner <ref> [2, 21] </ref>. We partitioned the GeoCoL graph into a number of subgraphs equal to the number of processors employed. It should be noted that any parallelized partitioner could be used. The graph generation time depicts the time required to generate GeoCoL graph.
Reference: [3] <author> M.J. Berger and S. H. Bokhari. </author> <title> A partitioning strategy for nonuniform problems on multiprocessors. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(5):570-580, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: These two data distribution schemes are often called BLOCK and CYCLIC data distributions [14], respectively. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [37, 41, 29, 27, 3, 20] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution. <p> For instance, a user might choose a partitioner that is based on coordinates <ref> [3] </ref> to partition data. A coordinate bisection partitioner decomposes data using the spatial location of vertices in the mesh. If the user chooses a graph based partitioner, such as the spectral partitioner [37], the connectivity of the mesh could be used to decompose the data. <p> Consider instead a distribution based on the spatial locations of atoms. Figure 4 depicts a distribution of atoms to processors carried out using a coordinate bisection partitioner <ref> [3] </ref>. When we compare Figure 3 with Figure 4, we see that the later figure has a much smaller amounts of surface area between the portions of the molecule associated with each processor. <p> In the absence of such a criterion, a compiler will choose a loop iteration partitioning scheme; e.g., partitioning 17 Table 2: Common Partitioning Heuristics Partitioner Reference Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection [37] p p p Coordinate Bisection <ref> [3] </ref> p p Hierarchical Subbox [11] p p Decomposition Simulated Annealing [29] p p p Neural Network [29] p p p Genetic Algorithms [29] p p p Inertial Bisection [33] p p Kernighan - Lin [24] loops so as to minimize non-local distributed array references. <p> There are many partitioning heuristics methods available based on physical phenomena and proximity <ref> [37, 3, 41, 20] </ref>. Table 2 lists some of the commonly used heuristics and the type of information they use for partitioning. Most data partitioners make use of undirected connectivity graphs and spatial information. Currently these partitioners must be coupled to user programs manually. <p> In such cases, each mesh point is associated with a location in space. We can assign each graph vertex a set of coordinates that describe its spatial location. These 18 spatial locations can be used to partition data structures <ref> [3, 33] </ref>. Vertices may also be assigned weights to represent estimated computational costs. In order to accurately estimate the computational costs, we need information on how work will be partitioned. <p> To map arrays we employed two different kinds of parallel partitioners: (1) geometry based partitioners (coordinate bisection <ref> [3] </ref> and inertial bisection [33]), and (2) a connectivity based partitioner (recursive spectral 26 Table 4: Unstructured Mesh Template - 53K Mesh - 32 Processors Recursive Coordinate Bisection Block Partition (Time Hand Compiler: Compiler Hand Compiler in Secs) Coded No Schedule Schedule Coded Schedule Reuse Reuse Reuse Partitioner 1.3 1.3 1.3
Reference: [4] <author> Harry Berryman, Joel Saltz, and Jeffrey Scroggs. </author> <title> Execution time support for adaptive scientific algorithms on distributed memory machines. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(3) </volume> <pages> 159-178, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Future work will include exploration of this optimization. 4 Coupling Partitioners In irregular problems, it is often desirable to allocate computational work to processors by assigning all computations that involve a given loop iteration to a single processor <ref> [4] </ref>. Consequently, we partition both distributed arrays and loop iterations using a two-phase approach (Figure 9). In the first phase, termed the "data partitioning" phase, distributed arrays are partitioned. In the second phase, called "workload partitioning", loop iterations are partitioned using the information from the first phase.
Reference: [5] <author> Zeki Bozkus, Alok Choudhary, Geoffrey Fox, Tomasz Haupt, Sanjay Ranka, and Min-You Wu. </author> <title> Compiling Fortran 90D/HPF for distributed memory MIMD computers. </title> <note> To appear in the Journal of Parallel and Distributed Computing, </note> <month> March </month> <year> 1993. </year>
Reference-contexts: In the Vienna Fortran [43] language definition a user can specify a customized distribution function. The runtime support and compiler transformation strategies described here can also be applied to Vienna Fortran. We have implemented our ideas using the Syracuse Fortran 90D/HPF compiler <ref> [5] </ref>. <p> Baden [1] has developed a programming environment targeted towards particle computations. This programming environment provides facilities that support dynamic load balancing. There are a variety of compiler projects targeted at distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse <ref> [15, 5] </ref> and the Vienna Fortran compiler project [43] at the University of Vienna are two examples. The Jade project at Stanford [26], the DINO project at Colorado [35], Kathy Yelick's work [7] at Berkeley, and the CODE project at UT, Austin provide parallel programming environments.
Reference: [6] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus. Charmm: </author> <title> A program for macromolecular energy, minimization, and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4:187, </volume> <year> 1983. </year>
Reference-contexts: We call these problem irregular concurrent problems [10]. Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers [31, 38, 16], molecular dynamics codes (CHARMM, AMBER, GROMOS, etc.) <ref> [6] </ref>, diagonal or polynomial preconditioned iterative linear solvers [39], and time dependent flame modeling codes [34]. In this paper, we focus on the runtime support, the language extensions and the compiler support required to provide efficient data and work load distribution. <p> We would partition mesh edges so that 1) we obtain a good load balance and 2) computations mostly employ locally stored data. Other unstructured problems have analogous indirectly accessed arrays. For instance, consider the nonbonded force calculation in the molecular dynamics code CHARMM <ref> [6] </ref>. Figure 5 depicts the non-bonded force calculation loop. Force components associated with each atom are stored as Fortran arrays. The outer loop L1 sweeps over all atoms; in this discussion, we assume that L1 is a parallel loop. <p> These performance measurements are for a loop over edges from 3-D unstructured Euler solver [31] for both 10K and 53K mesh points, and for an electrostatic force calculation loop in a molecular dynamics code for a 648 atom water simulation <ref> [6] </ref>. The functionality of these loops is equivalent to the loop L1 in Figure 11. Table 3 presents the performance results of the compiler generated code with and without the schedule reuse technique.
Reference: [7] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> Implementing an irregular application on a distributed memory multiprocessor. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <month> May </month> <year> 1993. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 28, No. 7. </volume> <pages> 31 </pages>
Reference-contexts: The Jade project at Stanford [26], the DINO project at Colorado [35], Kathy Yelick's work <ref> [7] </ref> at Berkeley, and the CODE project at UT, Austin provide parallel programming environments. Runtime compilation methods have been employed in four compiler projects: the Fortran D project [22], the Kali project [25], Marina Chen's work at Yale [28] and the PARTI project [32, 42, 36].
Reference: [8] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Vienna Fortran, Fortran D and HPF provide a rich set of data decomposition specifications. A definition of such language extensions may be found in Fox et al [15], Loveman et al [14], and Chapman et al <ref> [8] </ref>, [9]. Fortran D and HPF require that users explicitly define how data is to be distributed. Vienna Fortran allows users to write procedures to generate user defined distributions.
Reference: [9] <author> Barbara Chapman, Piyush Mehrotra, and Hans Zima. </author> <title> Programming in Vienna Fortran. </title> <type> Technical Report 92-9, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: Vienna Fortran, Fortran D and HPF provide a rich set of data decomposition specifications. A definition of such language extensions may be found in Fox et al [15], Loveman et al [14], and Chapman et al [8], <ref> [9] </ref>. Fortran D and HPF require that users explicitly define how data is to be distributed. Vienna Fortran allows users to write procedures to generate user defined distributions.
Reference: [10] <author> A. Choudhary, G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, S. Ranka, and J. Saltz. </author> <title> Software support for irregular and loosely synchronous problems. </title> <booktitle> Computing Systems in Engineering, 3(1-4):43-52, 1992. Papers presented at the Symposium on High-Performance Computing for Flight Vehicles, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: In this class of problems, once runtime information is available, data access patterns are known before each computational phase. We call these problem irregular concurrent problems <ref> [10] </ref>. Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers [31, 38, 16], molecular dynamics codes (CHARMM, AMBER, GROMOS, etc.) [6], diagonal or polynomial preconditioned iterative linear solvers [39], and time dependent flame modeling codes [34].
Reference: [11] <author> T. W. Clark, R. v. Hanxleden, J. A. McCammon, and L. R. Scott. </author> <title> Parallelization strategies for a molecular dynamics program. In Intel Supercomputer University Partners Conference, </title> <address> Timberline Lodge, Mt. Hood, OR, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: In the absence of such a criterion, a compiler will choose a loop iteration partitioning scheme; e.g., partitioning 17 Table 2: Common Partitioning Heuristics Partitioner Reference Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection [37] p p p Coordinate Bisection [3] p p Hierarchical Subbox <ref> [11] </ref> p p Decomposition Simulated Annealing [29] p p p Neural Network [29] p p p Genetic Algorithms [29] p p p Inertial Bisection [33] p p Kernighan - Lin [24] loops so as to minimize non-local distributed array references. <p> For instance, we find that it is sometimes important to take estimated computational costs into account when carrying out coordinate or inertial bisection for problems where computational costs vary greatly from node to node. Other partitioners make use of both geometrical and connectivity information <ref> [11] </ref>. Since the data structure that stores information on which data partitioning is to be based can represent Geometrical, Connectivity and/or Load information, we call this the GeoCoL data structure.
Reference: [12] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed memory compiler methods for irregular problems - data copy reuse and runtime partitioning. In Compilers and Runtime Software for Scalable Multiprocessors, </title> <editor> J. Saltz and P. Mehrotra Editors, </editor> <address> Amsterdam, The Netherlands, 1992. </address> <publisher> Elsevier. </publisher>
Reference-contexts: The gather on each processor fetches all the necessary x references that reside off-processor. The scatter add calls accumulates the off-processor y values. A detailed description of the functionality of these procedures is given in Das et al <ref> [12] </ref>. 2.4 Overview of CHAOS We have developed efficient runtime support to deal with problems that consist of a sequence of clearly demarcated concurrent computational phases. The project is called CHAOS; the runtime support is called the CHAOS library. <p> In earlier work, we have outlined a strategy (but did not attempt a compiler implementation) that would make it possible for compilers to generate compiler embedded connectivity based partitioners directly from marked loops <ref> [12] </ref>. The approach described here requires more input from the user and less compiler support. 8 Conclusions We have described work that demonstrates two new mechanisms for dealing effectively with irregular computations. The first mechanism invokes a user-specified parallel mapping procedure using a set of compiler directives.
Reference: [13] <author> R. Das and J. H. Saltz. </author> <title> Program slicing techniques for compiling irregular problems. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland , OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The compile time analysis needed to reuse inspector communication schedules is touched upon in <ref> [18, 13] </ref>. We propose a conservative method that in many cases allows us to reuse the results from inspectors.
Reference: [14] <author> D. Loveman (Ed.). </author> <title> Draft High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Another example would be to assign consecutively indexed array elements to processors in a round-robin fashion. These two data distribution schemes are often called BLOCK and CYCLIC data distributions <ref> [14] </ref>, respectively. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements [37, 41, 29, 27, 3, 20]. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. <p> Vienna Fortran, Fortran D and HPF provide a rich set of data decomposition specifications. A definition of such language extensions may be found in Fox et al [15], Loveman et al <ref> [14] </ref>, and Chapman et al [8], [9]. Fortran D and HPF require that users explicitly define how data is to be distributed. Vienna Fortran allows users to write procedures to generate user defined distributions.
Reference: [15] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <institution> Department of Computer Science Rice COMP TR90-141, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: We present methods and a prototype implementation that make it possible for compilers to efficiently handle irregular problems coded using a set of language extensions closely related to Fortran D <ref> [15] </ref>, Vienna Fortran [43] and High-Performance Fortran (HPF). <p> Vienna Fortran, Fortran D and HPF provide a rich set of data decomposition specifications. A definition of such language extensions may be found in Fox et al <ref> [15] </ref>, Loveman et al [14], and Chapman et al [8], [9]. Fortran D and HPF require that users explicitly define how data is to be distributed. Vienna Fortran allows users to write procedures to generate user defined distributions. <p> Baden [1] has developed a programming environment targeted towards particle computations. This programming environment provides facilities that support dynamic load balancing. There are a variety of compiler projects targeted at distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse <ref> [15, 5] </ref> and the Vienna Fortran compiler project [43] at the University of Vienna are two examples. The Jade project at Stanford [26], the DINO project at Colorado [35], Kathy Yelick's work [7] at Berkeley, and the CODE project at UT, Austin provide parallel programming environments.
Reference: [16] <author> S. Hammond and T. Barth. </author> <title> An optimal massively parallel Euler solver for unstructured grids. </title> <journal> AIAA Journal, </journal> <note> AIAA Paper 91-0441, </note> <month> January </month> <year> 1991. </year>
Reference-contexts: In this class of problems, once runtime information is available, data access patterns are known before each computational phase. We call these problem irregular concurrent problems [10]. Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers <ref> [31, 38, 16] </ref>, molecular dynamics codes (CHARMM, AMBER, GROMOS, etc.) [6], diagonal or polynomial preconditioned iterative linear solvers [39], and time dependent flame modeling codes [34].
Reference: [17] <author> R. v. Hanxleden. </author> <title> Compiler support for machine independent parallelization of irregular problems. </title> <type> Technical Report CRPC-TR92301-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> November </month> <year> 1992. </year> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR92301-S. </note>
Reference-contexts: The GEOMETRY construct is closely related to the geometrical partitioning or value based decomposition directives proposed by von Hanxleden <ref> [17] </ref>. Similarly, a GeoCoL data structure that specifies only vertex weights can be constructed using the keyword LOAD as follows. <p> For all problems, the performance of the compiler generated code is within 15% of that of the hand coded version. 7 Related Work Research has been carried out by von Hanxleden <ref> [17] </ref> on compiler-linked partitioners that decompose arrays based on distributed array element values; these are called value based decompositions. Our GEOMETRY construct can be viewed as a particular type of value based decomposition. Several researchers have developed programming environments that are targeted toward particular classes of irregular or adaptive problems.
Reference: [18] <author> R. v. Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler analysis for irregular problems in Fortran D. </title> <booktitle> In Proceedings of the 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: The compile time analysis needed to reuse inspector communication schedules is touched upon in <ref> [18, 13] </ref>. We propose a conservative method that in many cases allows us to reuse the results from inspectors.
Reference: [19] <author> R. v. Hanxleden, K. Kennedy, and J. Saltz. </author> <title> Value-based distributions in fortran d | a preliminary report. </title> <type> Technical Report CRPC-TR93365-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: To our knowledge, the implementation described in this paper was the first distributed memory compiler to provide this kind of support. User specified partitioning has recently been implemented in the D System Fortran 77D compiler <ref> [19] </ref>; the CHAOS runtime support described in this paper has been employed in this implementation. In the Vienna Fortran [43] language definition a user can specify a customized distribution function. The runtime support and compiler transformation strategies described here can also be applied to Vienna Fortran.
Reference: [20] <author> R. v. Hanxleden and L. R. Scott. </author> <title> Load balancing on message passing architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 312-324, </pages> <year> 1991. </year>
Reference-contexts: These two data distribution schemes are often called BLOCK and CYCLIC data distributions [14], respectively. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [37, 41, 29, 27, 3, 20] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution. <p> There are many partitioning heuristics methods available based on physical phenomena and proximity <ref> [37, 3, 41, 20] </ref>. Table 2 lists some of the commonly used heuristics and the type of information they use for partitioning. Most data partitioners make use of undirected connectivity graphs and spatial information. Currently these partitioners must be coupled to user programs manually.
Reference: [21] <author> B. Hendrickson and R. Leland. </author> <title> An improved spectral graph partitioning algorithm for mapping parallel computations. </title> <type> Technical Report SAND 92-1460, </type> <institution> Sandia National Laboratory, </institution> <address> Albuquerque, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: Only a modest effort was made to produce an efficient parallel implementation of the partitioner and we believe that the performance and the execution time of the partitioner can be tremendously improved by using a multilevel version of the partitioner <ref> [2, 21] </ref>. We partitioned the GeoCoL graph into a number of subgraphs equal to the number of processors employed. It should be noted that any parallelized partitioner could be used. The graph generation time depicts the time required to generate GeoCoL graph.
Reference: [22] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. In Compilers and Runtime Software for Scalable Multiprocessors, </title> <editor> J. Saltz and P. Mehrotra Editors, </editor> <address> Amsterdam, The Netherlands, </address> <note> To appear 1991. Elsevier. </note>
Reference-contexts: The Jade project at Stanford [26], the DINO project at Colorado [35], Kathy Yelick's work [7] at Berkeley, and the CODE project at UT, Austin provide parallel programming environments. Runtime compilation methods have been employed in four compiler projects: the Fortran D project <ref> [22] </ref>, the Kali project [25], Marina Chen's work at Yale [28] and the PARTI project [32, 42, 36]. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [25] and the ARF compiler was the first compiler to support irregularly distributed arrays [42].
Reference: [23] <author> A. Jameson, T. J. Baker, and N. P. Weatherhill. </author> <title> Calculation of inviscid transonic flow over a complete aircraft. </title> <type> AIAA paper 86-0103, </type> <month> January </month> <year> 1986. </year>
Reference-contexts: In that code, computational costs vary dynamically and cannot be estimated until runtime. 2.1.1 Codes with Indirectly Accessed Arrays The first application code is an unstructured Euler solver used to study the flow of air over an airfoil <ref> [31, 38, 23] </ref>. Complex aerodynamic shapes require high resolution meshes and, consequently, large numbers of mesh points. A mesh vertex is an abstraction represented by Fortran array data structures. Physical values (e.g. velocity, pressure) are associated with each mesh vertex.
Reference: [24] <author> B.W. Kernighan and S. Lin. </author> <title> An efficient heuristic procedure for partitioning graphs. </title> <journal> Bell System Technical Journal, </journal> <volume> 49(2) </volume> <pages> 291-307, </pages> <month> February </month> <year> 1970. </year>
Reference-contexts: Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection [37] p p p Coordinate Bisection [3] p p Hierarchical Subbox [11] p p Decomposition Simulated Annealing [29] p p p Neural Network [29] p p p Genetic Algorithms [29] p p p Inertial Bisection [33] p p Kernighan - Lin <ref> [24] </ref> loops so as to minimize non-local distributed array references. <p> Data partitioners can make use of different kinds of program information. Some partitioners operate on data structures that represent undirected graphs <ref> [37, 24, 29] </ref>. Graph vertices represent array indices, graph edges represent dependencies. Consider the example loop L1 in Figure 11. The graph vertices represent the N elements of arrays x and y.
Reference: [25] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory architectures. </title> <booktitle> In 2nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 177-186. </pages> <publisher> ACM, </publisher> <month> March </month> <year> 1990. </year>
Reference-contexts: The Jade project at Stanford [26], the DINO project at Colorado [35], Kathy Yelick's work [7] at Berkeley, and the CODE project at UT, Austin provide parallel programming environments. Runtime compilation methods have been employed in four compiler projects: the Fortran D project [22], the Kali project <ref> [25] </ref>, Marina Chen's work at Yale [28] and the PARTI project [32, 42, 36]. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [25] and the ARF compiler was the first compiler to support irregularly distributed arrays [42]. <p> Runtime compilation methods have been employed in four compiler projects: the Fortran D project [22], the Kali project <ref> [25] </ref>, Marina Chen's work at Yale [28] and the PARTI project [32, 42, 36]. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [25] and the ARF compiler was the first compiler to support irregularly distributed arrays [42]. In earlier work, we have outlined a strategy (but did not attempt a compiler implementation) that would make it possible for compilers to generate compiler embedded connectivity based partitioners directly from marked loops [12].
Reference: [26] <author> Monica Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The cache performance and optimizations of block algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 63-74. </pages> <publisher> ACM Press, </publisher> <month> April </month> <year> 1991. </year> <month> 32 </month>
Reference-contexts: There are a variety of compiler projects targeted at distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse [15, 5] and the Vienna Fortran compiler project [43] at the University of Vienna are two examples. The Jade project at Stanford <ref> [26] </ref>, the DINO project at Colorado [35], Kathy Yelick's work [7] at Berkeley, and the CODE project at UT, Austin provide parallel programming environments.
Reference: [27] <author> W. E. Leland. </author> <title> Load-balancing heuristics and process behavior. </title> <booktitle> In Proceedings of Performance 86 and ACM SIG--METRICS 86, </booktitle> <pages> pages 54-69, </pages> <year> 1986. </year>
Reference-contexts: These two data distribution schemes are often called BLOCK and CYCLIC data distributions [14], respectively. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [37, 41, 29, 27, 3, 20] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution.
Reference: [28] <author> L. C. Lu and M.C. Chen. </author> <title> Parallelizing loops with indirect array references or pointers. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Runtime compilation methods have been employed in four compiler projects: the Fortran D project [22], the Kali project [25], Marina Chen's work at Yale <ref> [28] </ref> and the PARTI project [32, 42, 36]. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [25] and the ARF compiler was the first compiler to support irregularly distributed arrays [42].
Reference: [29] <author> N. Mansour. </author> <title> Physical optimization algorithms for mapping data to distributed-memory multiprocessors. </title> <type> Technical report, Ph.D. Dissertation, </type> <institution> School of Computer Science,Syracuse University, </institution> <year> 1992. </year>
Reference-contexts: These two data distribution schemes are often called BLOCK and CYCLIC data distributions [14], respectively. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [37, 41, 29, 27, 3, 20] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution. <p> of such a criterion, a compiler will choose a loop iteration partitioning scheme; e.g., partitioning 17 Table 2: Common Partitioning Heuristics Partitioner Reference Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection [37] p p p Coordinate Bisection [3] p p Hierarchical Subbox [11] p p Decomposition Simulated Annealing <ref> [29] </ref> p p p Neural Network [29] p p p Genetic Algorithms [29] p p p Inertial Bisection [33] p p Kernighan - Lin [24] loops so as to minimize non-local distributed array references. <p> will choose a loop iteration partitioning scheme; e.g., partitioning 17 Table 2: Common Partitioning Heuristics Partitioner Reference Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection [37] p p p Coordinate Bisection [3] p p Hierarchical Subbox [11] p p Decomposition Simulated Annealing <ref> [29] </ref> p p p Neural Network [29] p p p Genetic Algorithms [29] p p p Inertial Bisection [33] p p Kernighan - Lin [24] loops so as to minimize non-local distributed array references. <p> scheme; e.g., partitioning 17 Table 2: Common Partitioning Heuristics Partitioner Reference Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection [37] p p p Coordinate Bisection [3] p p Hierarchical Subbox [11] p p Decomposition Simulated Annealing <ref> [29] </ref> p p p Neural Network [29] p p p Genetic Algorithms [29] p p p Inertial Bisection [33] p p Kernighan - Lin [24] loops so as to minimize non-local distributed array references. <p> Data partitioners can make use of different kinds of program information. Some partitioners operate on data structures that represent undirected graphs <ref> [37, 24, 29] </ref>. Graph vertices represent array indices, graph edges represent dependencies. Consider the example loop L1 in Figure 11. The graph vertices represent the N elements of arrays x and y.
Reference: [30] <author> D. J. Mavriplis. </author> <title> Adaptive mesh generation for viscous flows using delaunay triangulation. </title> <journal> Journal of Computational Physics, </journal> <volume> 90(2) </volume> <pages> 271-291, </pages> <year> 1990. </year>
Reference-contexts: Since meshes are typically associated with physical objects, a spatial location can often be associated with each mesh point. The spatial location of the mesh points and the connectivity of the vertices is determined by the mesh generation strategy <ref> [40, 30] </ref>. Figure 2 depicts a mesh generated by such a process. This is an unstructured mesh representation of a three dimensional aircraft wing.
Reference: [31] <author> D. J. Mavriplis. </author> <title> Three dimensional unstructured multigrid for the Euler equations, paper 91-1549cp. </title> <booktitle> In AIAA 10th Computational Fluid Dynamics Conference, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: In this class of problems, once runtime information is available, data access patterns are known before each computational phase. We call these problem irregular concurrent problems [10]. Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers <ref> [31, 38, 16] </ref>, molecular dynamics codes (CHARMM, AMBER, GROMOS, etc.) [6], diagonal or polynomial preconditioned iterative linear solvers [39], and time dependent flame modeling codes [34]. <p> In that code, computational costs vary dynamically and cannot be estimated until runtime. 2.1.1 Codes with Indirectly Accessed Arrays The first application code is an unstructured Euler solver used to study the flow of air over an airfoil <ref> [31, 38, 23] </ref>. Complex aerodynamic shapes require high resolution meshes and, consequently, large numbers of mesh points. A mesh vertex is an abstraction represented by Fortran array data structures. Physical values (e.g. velocity, pressure) are associated with each mesh vertex. <p> These performance measurements are for a loop over edges from 3-D unstructured Euler solver <ref> [31] </ref> for both 10K and 53K mesh points, and for an electrostatic force calculation loop in a molecular dynamics code for a 648 atom water simulation [6]. The functionality of these loops is equivalent to the loop L1 in Figure 11.
Reference: [32] <author> R. Mirchandaney, J. H. Saltz, R. M. Smith, D. M. Nicol, and Kay Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the 1988 ACM International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: For example, the values of indirection arrays edge1 and edge2 of loop L2 in pre-process the data references of distributed arrays. On each processor, we pre-compute which data need to be exchanged. The result of this pre-processing is a communication schedule <ref> [32] </ref>. Each processor uses communication schedules to exchange required data before and after executing a loop. The same schedules can be used repeatedly, as long as the data reference patterns remain unchanged. In are not modified within L1, it is possible to reuse communication schedules for L2. <p> The project is called CHAOS; the runtime support is called the CHAOS library. The CHAOS library is a superset of the PARTI library <ref> [32, 42, 36] </ref>. Solving concurrent irregular problems on distributed memory machines using our runtime support involves five major steps (Figure 9). The first three steps in the figure concern mapping data and computations onto processors. <p> Runtime compilation methods have been employed in four compiler projects: the Fortran D project [22], the Kali project [25], Marina Chen's work at Yale [28] and the PARTI project <ref> [32, 42, 36] </ref>. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [25] and the ARF compiler was the first compiler to support irregularly distributed arrays [42].
Reference: [33] <author> B. Nour-Omid, A. Raefsky, and G. Lyzenga. </author> <title> Solving finite element equations on concurrent computers. </title> <booktitle> In Proc. of Symposium on Parallel Computations and theis Impact on Mechanics, </booktitle> <address> Boston, </address> <month> December </month> <year> 1987. </year>
Reference-contexts: Common Partitioning Heuristics Partitioner Reference Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection [37] p p p Coordinate Bisection [3] p p Hierarchical Subbox [11] p p Decomposition Simulated Annealing [29] p p p Neural Network [29] p p p Genetic Algorithms [29] p p p Inertial Bisection <ref> [33] </ref> p p Kernighan - Lin [24] loops so as to minimize non-local distributed array references. <p> In such cases, each mesh point is associated with a location in space. We can assign each graph vertex a set of coordinates that describe its spatial location. These 18 spatial locations can be used to partition data structures <ref> [3, 33] </ref>. Vertices may also be assigned weights to represent estimated computational costs. In order to accurately estimate the computational costs, we need information on how work will be partitioned. <p> To map arrays we employed two different kinds of parallel partitioners: (1) geometry based partitioners (coordinate bisection [3] and inertial bisection <ref> [33] </ref>), and (2) a connectivity based partitioner (recursive spectral 26 Table 4: Unstructured Mesh Template - 53K Mesh - 32 Processors Recursive Coordinate Bisection Block Partition (Time Hand Compiler: Compiler Hand Compiler in Secs) Coded No Schedule Schedule Coded Schedule Reuse Reuse Reuse Partitioner 1.3 1.3 1.3 0.0 0.0 Inspector, remap
Reference: [34] <author> G. Patnaik, K.J. Laskey, K. Kailasanath, E.S. Oran, and T. V. Brun. </author> <title> FLIC A detailed, two-dimensional flame model. </title> <type> NRL Report 6555, </type> <institution> Naval Research Laboratory, </institution> <address> Washington, DC, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: We call these problem irregular concurrent problems [10]. Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers [31, 38, 16], molecular dynamics codes (CHARMM, AMBER, GROMOS, etc.) [6], diagonal or polynomial preconditioned iterative linear solvers [39], and time dependent flame modeling codes <ref> [34] </ref>. In this paper, we focus on the runtime support, the language extensions and the compiler support required to provide efficient data and work load distribution.
Reference: [35] <author> Matthew Rosing, Robert B. Schnabel, and Robert P. Weaver. </author> <title> The DINO parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 30-42, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: There are a variety of compiler projects targeted at distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse [15, 5] and the Vienna Fortran compiler project [43] at the University of Vienna are two examples. The Jade project at Stanford [26], the DINO project at Colorado <ref> [35] </ref>, Kathy Yelick's work [7] at Berkeley, and the CODE project at UT, Austin provide parallel programming environments. Runtime compilation methods have been employed in four compiler projects: the Fortran D project [22], the Kali project [25], Marina Chen's work at Yale [28] and the PARTI project [32, 42, 36].
Reference: [36] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Runtime compilation for multiprocessors. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(6) </volume> <pages> 573-592, </pages> <year> 1991. </year>
Reference-contexts: The project is called CHAOS; the runtime support is called the CHAOS library. The CHAOS library is a superset of the PARTI library <ref> [32, 42, 36] </ref>. Solving concurrent irregular problems on distributed memory machines using our runtime support involves five major steps (Figure 9). The first three steps in the figure concern mapping data and computations onto processors. <p> Finally, in Phase E, information from the earlier phases is used to carry out the computation and com munication. CHAOS and PARTI procedures have been used in a variety of applications, including sparse matrix linear solvers, adaptive computational fluid dynamics codes, molecular dynamics codes and a prototype compiler <ref> [36] </ref> aimed at distributed memory multiprocessors. 2.5 Overview of Existing Language Support While our data decomposition directives are presented in the context of Fortran D, the same optimizations and analogous language extensions could be used for a wide range of languages and compilers such as Vienna Fortran, pC++, and HPF. <p> Runtime compilation methods have been employed in four compiler projects: the Fortran D project [22], the Kali project [25], Marina Chen's work at Yale [28] and the PARTI project <ref> [32, 42, 36] </ref>. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [25] and the ARF compiler was the first compiler to support irregularly distributed arrays [42].
Reference: [37] <author> H. Simon. </author> <title> Partitioning of unstructured mesh problems for parallel processing. </title> <booktitle> In Proceedings of the Conference on Parallel Methods on Large Scale Structural Analysis and Physics Applications. </booktitle> <publisher> Pergamon Press, </publisher> <year> 1991. </year>
Reference-contexts: These two data distribution schemes are often called BLOCK and CYCLIC data distributions [14], respectively. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [37, 41, 29, 27, 3, 20] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution. <p> For instance, a user might choose a partitioner that is based on coordinates [3] to partition data. A coordinate bisection partitioner decomposes data using the spatial location of vertices in the mesh. If the user chooses a graph based partitioner, such as the spectral partitioner <ref> [37] </ref>, the connectivity of the mesh could be used to decompose the data. The next step in parallelizing this application involves assigning equal amount of work to processors. An Euler solver consists of a sequence of loops that sweep over a mesh. <p> In the absence of such a criterion, a compiler will choose a loop iteration partitioning scheme; e.g., partitioning 17 Table 2: Common Partitioning Heuristics Partitioner Reference Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection <ref> [37] </ref> p p p Coordinate Bisection [3] p p Hierarchical Subbox [11] p p Decomposition Simulated Annealing [29] p p p Neural Network [29] p p p Genetic Algorithms [29] p p p Inertial Bisection [33] p p Kernighan - Lin [24] loops so as to minimize non-local distributed array references. <p> There are many partitioning heuristics methods available based on physical phenomena and proximity <ref> [37, 3, 41, 20] </ref>. Table 2 lists some of the commonly used heuristics and the type of information they use for partitioning. Most data partitioners make use of undirected connectivity graphs and spatial information. Currently these partitioners must be coupled to user programs manually. <p> Data partitioners can make use of different kinds of program information. Some partitioners operate on data structures that represent undirected graphs <ref> [37, 24, 29] </ref>. Graph vertices represent array indices, graph edges represent dependencies. Consider the example loop L1 in Figure 11. The graph vertices represent the N elements of arrays x and y. <p> - 53K Mesh - 32 Processors Inertial Bisection Spectral Bisection (Time Hand Compiler Hand Compiler: in Secs) Coded Schedule Coded Schedule Reuse Reuse Graph Generation - 1.8 2.1 Partitioner 1.4 1.4 226 227 Inspector, remap 3.1 3.3 3.1 3.2 Executor 14.7 16.1 12.5 13.4 Total 19.2 20.8 243 246 bisection <ref> [37] </ref>). The performance of the compiler embedded mapper and a hand parallelized version are shown in Tables 4 and 5. <p> In Table 5, Partitioner for Spectral Bisection depicts the time needed to partition the GeoCoL graph data structure using a parallelized version of Simon's single level spectral partitioner <ref> [37] </ref>. Only a modest effort was made to produce an efficient parallel implementation of the partitioner and we believe that the performance and the execution time of the partitioner can be tremendously improved by using a multilevel version of the partitioner [2, 21].
Reference: [38] <author> V. Venkatakrishnan, H. D. Simon, and T. J. Barth. </author> <title> A MIMD implementation of a parallel Euler solver for unstructured grids, </title> <note> submitted to Journal of Supercomputing. Report RNR-91-024, </note> <institution> NAS Systems Division, NASA Ames Research Center, </institution> <month> Sept </month> <year> 1991. </year>
Reference-contexts: In this class of problems, once runtime information is available, data access patterns are known before each computational phase. We call these problem irregular concurrent problems [10]. Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers <ref> [31, 38, 16] </ref>, molecular dynamics codes (CHARMM, AMBER, GROMOS, etc.) [6], diagonal or polynomial preconditioned iterative linear solvers [39], and time dependent flame modeling codes [34]. <p> In that code, computational costs vary dynamically and cannot be estimated until runtime. 2.1.1 Codes with Indirectly Accessed Arrays The first application code is an unstructured Euler solver used to study the flow of air over an airfoil <ref> [31, 38, 23] </ref>. Complex aerodynamic shapes require high resolution meshes and, consequently, large numbers of mesh points. A mesh vertex is an abstraction represented by Fortran array data structures. Physical values (e.g. velocity, pressure) are associated with each mesh vertex.
Reference: [39] <author> P. Venkatkrishnan, J. Saltz, and D. Mavriplis. </author> <title> Parallel preconditioned iterative methods for the compressible navier stokes equations. </title> <booktitle> In 12th International Conference on Numerical Methods in Fluid Dynamics, </booktitle> <address> Oxford, England, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: We call these problem irregular concurrent problems [10]. Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers [31, 38, 16], molecular dynamics codes (CHARMM, AMBER, GROMOS, etc.) [6], diagonal or polynomial preconditioned iterative linear solvers <ref> [39] </ref>, and time dependent flame modeling codes [34]. In this paper, we focus on the runtime support, the language extensions and the compiler support required to provide efficient data and work load distribution.
Reference: [40] <author> N. P. Weatherill. </author> <title> The generation of unstructured grids using dirichlet tessalations. </title> <type> Report MAE 1715, </type> <institution> Princeton, </institution> <month> July </month> <year> 1985. </year>
Reference-contexts: Since meshes are typically associated with physical objects, a spatial location can often be associated with each mesh point. The spatial location of the mesh points and the connectivity of the vertices is determined by the mesh generation strategy <ref> [40, 30] </ref>. Figure 2 depicts a mesh generated by such a process. This is an unstructured mesh representation of a three dimensional aircraft wing.
Reference: [41] <author> R. Williams. </author> <title> Performance of dynamic load balancing algorithms for unstructured mesh calculations. </title> <journal> Concurrency, Practice and Experience, </journal> <volume> 3(5) </volume> <pages> 457-482, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: These two data distribution schemes are often called BLOCK and CYCLIC data distributions [14], respectively. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [37, 41, 29, 27, 3, 20] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution. <p> There are many partitioning heuristics methods available based on physical phenomena and proximity <ref> [37, 3, 41, 20] </ref>. Table 2 lists some of the commonly used heuristics and the type of information they use for partitioning. Most data partitioners make use of undirected connectivity graphs and spatial information. Currently these partitioners must be coupled to user programs manually. <p> Our GEOMETRY construct can be viewed as a particular type of value based decomposition. Several researchers have developed programming environments that are targeted toward particular classes of irregular or adaptive problems. Williams <ref> [41] </ref> 29 Table 9: Performance of Compiler-linked Partitioners Total 10K Mesh 53k Mesh 648 Atoms 1024x32 Grid Time (in Processors Processors Processors Processors Secs) 8 16 32 64 4 8 16 16 32 Hand coded 8.8 7.0 18.5 14.9 14.3 8.5 7.0 23.5 15.2 Compiler 10.1 8.5 19.8 17.0 15.2 9.7
Reference: [42] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 26-30, </pages> <year> 1991. </year>
Reference-contexts: The project is called CHAOS; the runtime support is called the CHAOS library. The CHAOS library is a superset of the PARTI library <ref> [32, 42, 36] </ref>. Solving concurrent irregular problems on distributed memory machines using our runtime support involves five major steps (Figure 9). The first three steps in the figure concern mapping data and computations onto processors. <p> Runtime compilation methods have been employed in four compiler projects: the Fortran D project [22], the Kali project [25], Marina Chen's work at Yale [28] and the PARTI project <ref> [32, 42, 36] </ref>. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [25] and the ARF compiler was the first compiler to support irregularly distributed arrays [42]. <p> The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [25] and the ARF compiler was the first compiler to support irregularly distributed arrays <ref> [42] </ref>. In earlier work, we have outlined a strategy (but did not attempt a compiler implementation) that would make it possible for compilers to generate compiler embedded connectivity based partitioners directly from marked loops [12].
Reference: [43] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald. </author> <title> Vienna Fortran a language specification. </title> <type> Report ACPC-TR92-4, </type> <institution> Austrian Center for Parallel Computation, University of Vienna, Vienna, Austria, </institution> <year> 1992. </year> <month> 33 </month>
Reference-contexts: We present methods and a prototype implementation that make it possible for compilers to efficiently handle irregular problems coded using a set of language extensions closely related to Fortran D [15], Vienna Fortran <ref> [43] </ref> and High-Performance Fortran (HPF). <p> User specified partitioning has recently been implemented in the D System Fortran 77D compiler [19]; the CHAOS runtime support described in this paper has been employed in this implementation. In the Vienna Fortran <ref> [43] </ref> language definition a user can specify a customized distribution function. The runtime support and compiler transformation strategies described here can also be applied to Vienna Fortran. We have implemented our ideas using the Syracuse Fortran 90D/HPF compiler [5]. <p> Once we have the new distribution provided by the partitioner, we redistribute the arrays based on it. A communication schedule is built and used to redistribute the arrays from the default to the new distribution. Vienna Fortran <ref> [43] </ref> provides support for the user to specify a function for distributing data. <p> This programming environment provides facilities that support dynamic load balancing. There are a variety of compiler projects targeted at distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse [15, 5] and the Vienna Fortran compiler project <ref> [43] </ref> at the University of Vienna are two examples. The Jade project at Stanford [26], the DINO project at Colorado [35], Kathy Yelick's work [7] at Berkeley, and the CODE project at UT, Austin provide parallel programming environments.
References-found: 43

