URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/94/tr1217.ps
Refering-URL: http://www.cs.wisc.edu/math-prog/tech-reports/
Root-URL: 
Title: Alternating Direction Splittings For Block-Angular Parallel Optimization  
Author: Renato De Leone Robert R. Meyer Spyridon Kontogiorgis 
Abstract: We develop and compare three decomposition algorithms derived from the method of alternating directions. They may be viewed as block Gauss-Seidel variants of augmented Lagrangian approaches that take advantage of block-angular structure. From a parallel computation viewpoint, they are ideally suited to a data parallel environment. Numerical results for large-scale multicommodity flow problems are presented to demonstrate the effectiveness of these decomposition algorithms on the Thinking Machines CM-5 parallel supercomputer relative to the widely-used serial optimization package MINOS 5.4 . 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.I. Ali and J.L. Kennington. </author> <title> MNETGEN program documentation. </title> <type> Technical Report IEOR 77003, </type> <institution> Dept. of Industrial Engineering and Operations Research, Southern Methodist University, Dallas, TX, </institution> <year> 1977. </year>
Reference-contexts: We will reserve superscript t to denote vectors and matrices generated at step t = 0; 1; 2; : : : of an iterative process. A collection of vectors x with indices in a finite set f1; : : : ; Kg will be represented by x <ref> [1] </ref> ; x [2] ; : : : ; x [K] . In case it is clear from the context, we sometimes let x represent the concatenation of the vectors x [1] ; x [2] ; : : : ; x [K] . <p> vectors x with indices in a finite set f1; : : : ; Kg will be represented by x <ref> [1] </ref> ; x [2] ; : : : ; x [K] . In case it is clear from the context, we sometimes let x represent the concatenation of the vectors x [1] ; x [2] ; : : : ; x [K] . For scalar ff, we define ff + := maxfff; 0g. For vectors x and y, minfx; yg and maxfx; yg are to be taken component-wise. Similarly, for a vector x, x + is defined component-wise. <p> The problem, denoted by (CBA), is min f <ref> [1] </ref> (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] i = 1; <p> The problem, denoted by (CBA), is min f <ref> [1] </ref> (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] i = 1; : : <p> The problem, denoted by (CBA), is min f <ref> [1] </ref> (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] i = 1; : : : ; K: We take each component function in the objective f [i] to be finite-valued, convex and continuous. <p> The problem, denoted by (CBA), is min f <ref> [1] </ref> (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] i = 1; : : : ; K: We take each component function in the objective f [i] to be finite-valued, convex and continuous. <p> The problem, denoted by (CBA), is min f <ref> [1] </ref> (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] i = 1; : : : ; K: We take each component function in the objective f [i] to be finite-valued, convex and continuous. <p> Let also G 2 y <ref> [1] </ref> ; : : : ; y [K] ; d [1] ; : : : ; d [K] := &gt; &gt; &gt; &gt; &lt; 0 if i=1 +1 otherwise (11) Both functions G 1 and G 2 are closed, convex and also proper, because of our assumption that (CBA) is solvable. <p> Let also G 2 y <ref> [1] </ref> ; : : : ; y [K] ; d [1] ; : : : ; d [K] := &gt; &gt; &gt; &gt; &lt; 0 if i=1 +1 otherwise (11) Both functions G 1 and G 2 are closed, convex and also proper, because of our assumption that (CBA) is solvable. Problem (CBA) is equivalent to min G 1 x [1] <p> <ref> [1] </ref> ; : : : ; d [K] := &gt; &gt; &gt; &gt; &lt; 0 if i=1 +1 otherwise (11) Both functions G 1 and G 2 are closed, convex and also proper, because of our assumption that (CBA) is solvable. Problem (CBA) is equivalent to min G 1 x [1] ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + G 2 y [1] ; : : : ; y [K] ; d [1] ; : : : ; d [K] subject to x [i] = y [i] ) which <p> Problem (CBA) is equivalent to min G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + G 2 y [1] ; : : : ; y [K] ; d [1] ; : : : ; d [K] subject to x [i] = y [i] ) which is in the form of problem (4), with the correspondences x <p> Problem (CBA) is equivalent to min G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + G 2 y [1] ; : : : ; y [K] ; d [1] ; : : : ; d [K] subject to x [i] = y [i] ) which is in the form of problem (4), with the correspondences x [x [1] : : : x [K] ; ~ d [1] : : <p> Problem (CBA) is equivalent to min G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + G 2 y [1] ; : : : ; y [K] ; d [1] ; : : : ; d [K] subject to x [i] = y [i] ) which is in the form of problem (4), with the correspondences x [x [1] : : : x [K] ; ~ d [1] : : : ~ d [K] ]; A I ; (13) We <p> : : : ; ~ d [K] + G 2 y <ref> [1] </ref> ; : : : ; y [K] ; d [1] ; : : : ; d [K] subject to x [i] = y [i] ) which is in the form of problem (4), with the correspondences x [x [1] : : : x [K] ; ~ d [1] : : : ~ d [K] ]; A I ; (13) We associate a multiplier vector q [i] with each constraint x [i] = y [i] , and a multiplier vector p [i] with each constraint ~ d [i] = d <p> 2 y <ref> [1] </ref> ; : : : ; y [K] ; d [1] ; : : : ; d [K] subject to x [i] = y [i] ) which is in the form of problem (4), with the correspondences x [x [1] : : : x [K] ; ~ d [1] : : : ~ d [K] ]; A I ; (13) We associate a multiplier vector q [i] with each constraint x [i] = y [i] , and a multiplier vector p [i] with each constraint ~ d [i] = d [i] . <p> We will use a shorthand notation, and write x t+1 for the concatenation of the vectors x t+1 t+1 [K] , and similarly for y t+1 etc. We will also write f (x) for i=1 f [i] (x [i] ). We also define fl t x := diag x <ref> [1] </ref> ; : : : ; fl t and let the diagonal matrix fl t K consist of K copies of fl t d placed along the diagonal. <p> Then, the iterates are such that: (i) q t+1 = 0, t 0. (iii) i=1 [i] = d, t 0 <ref> [1] </ref> = p t+1 [K] , t 0 We will now derive some explicit formulas for the update of the p and d vectors that are equivalent to those in (20) and (17). One of these will use dual information from the first subproblem. <p> We add proximal terms only for the vectors D [i] x [i] that reflect the consumption of the shared resource. The trade-off is that we cannot guarantee convergence of the full set of primal variables, although the objective value converges to the optimal objective. We define G 1 x <ref> [1] </ref> ; : : : ; x [K] := i=1 with the extended objective function h [i] (x [i] ) as in (3), and G 2 d [1] ; : : : ; d [K] := &gt; &gt; &gt; &gt; &lt; 0 if i=1 +1 otherwise (30) Both functions G 1 <p> We define G 1 x <ref> [1] </ref> ; : : : ; x [K] := i=1 with the extended objective function h [i] (x [i] ) as in (3), and G 2 d [1] ; : : : ; d [K] := &gt; &gt; &gt; &gt; &lt; 0 if i=1 +1 otherwise (30) Both functions G 1 and G 2 are closed, convex and also proper, because of our assumption that (CBA) is solvable. 10 The splitting matrix D is block-diagonal, D := diag <p> Problem (CBA) is equiva lent to min G 1 x <ref> [1] </ref> ; : : : ; x [K] + G 2 d [1] ; : : : ; d [K] subject to D [i] x [i] = d [i] ; i = 1; : : : ; K (31) which is in the form of problem (4), with the correspondences x <p> Problem (CBA) is equiva lent to min G 1 x <ref> [1] </ref> ; : : : ; x [K] + G 2 d [1] ; : : : ; d [K] subject to D [i] x [i] = d [i] ; i = 1; : : : ; K (31) which is in the form of problem (4), with the correspondences x [x [1] : : : x [K] ]; A D; (32) If <p> : : : ; x [K] + G 2 d <ref> [1] </ref> ; : : : ; d [K] subject to D [i] x [i] = d [i] ; i = 1; : : : ; K (31) which is in the form of problem (4), with the correspondences x [x [1] : : : x [K] ]; A D; (32) If certain variables in block i do not appear in the coupling constraints, then the corresponding columns of D [i] are zeros, and in this case the matrix D is not of full column rank. <p> Then the D [i] x [i] terms for (RP) behave like d t+ 1 [i] terms for this (ARP). In more detail: we derive an (ARP) splitting by defining a function G 1 as G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] := &gt; &gt; &gt; &gt; &lt; K X f [i] (x [i] ) + ( j B [i] )(x [i] ) if D [i] x [i] = ~ d [i] ; <p> Then the D [i] x [i] terms for (RP) behave like d t+ 1 [i] terms for this (ARP). In more detail: we derive an (ARP) splitting by defining a function G 1 as G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] := &gt; &gt; &gt; &gt; &lt; K X f [i] (x [i] ) + ( j B [i] )(x [i] ) if D [i] x [i] = ~ d [i] ; 8i +1 otherwise and by defining a function G 2 of <p> := &gt; &gt; &gt; &gt; &lt; K X f [i] (x [i] ) + ( j B [i] )(x [i] ) if D [i] x [i] = ~ d [i] ; 8i +1 otherwise and by defining a function G 2 of the d [i] variables only G 2 d <ref> [1] </ref> ; : : : ; d [K] := &gt; &gt; &gt; &gt; &lt; 0 if i=1 +1 otherwise (43) Then, an equivalent formulation of the (CBA) problem is min G 1 x [1] ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ <p> otherwise and by defining a function G 2 of the d [i] variables only G 2 d <ref> [1] </ref> ; : : : ; d [K] := &gt; &gt; &gt; &gt; &lt; 0 if i=1 +1 otherwise (43) Then, an equivalent formulation of the (CBA) problem is min G 1 x [1] ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + G 2 d [1] ; : : : ; d [K] subject to d [i] = ~ d [i] ; i = 1; : : : ; K (44) in <p> [i] variables only G 2 d <ref> [1] </ref> ; : : : ; d [K] := &gt; &gt; &gt; &gt; &lt; 0 if i=1 +1 otherwise (43) Then, an equivalent formulation of the (CBA) problem is min G 1 x [1] ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + G 2 d [1] ; : : : ; d [K] subject to d [i] = ~ d [i] ; i = 1; : : : ; K (44) in which the x [i] variables are not included in the explicit <p> [K] := &gt; &gt; &gt; &gt; &lt; 0 if i=1 +1 otherwise (43) Then, an equivalent formulation of the (CBA) problem is min G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + G 2 d [1] ; : : : ; d [K] subject to d [i] = ~ d [i] ; i = 1; : : : ; K (44) in which the x [i] variables are not included in the explicit constraints. 13 In terms of correspondences with problem (4), this splitting has x <p> : : : ; d [K] subject to d [i] = ~ d [i] ; i = 1; : : : ; K (44) in which the x [i] variables are not included in the explicit constraints. 13 In terms of correspondences with problem (4), this splitting has x [x <ref> [1] </ref> : : : x [K] ; ~ d [1] : : : ~ d [K] ]; A [0 I ]; (45) The splitting matrix A has not full column rank now; moreover, multipliers and primal proximal terms are associated only with the ~ d variables. <p> [i] = ~ d [i] ; i = 1; : : : ; K (44) in which the x [i] variables are not included in the explicit constraints. 13 In terms of correspondences with problem (4), this splitting has x [x <ref> [1] </ref> : : : x [K] ; ~ d [1] : : : ~ d [K] ]; A [0 I ]; (45) The splitting matrix A has not full column rank now; moreover, multipliers and primal proximal terms are associated only with the ~ d variables. <p> In this scheme we add proximal terms only for the activity vectors x [i] . We define, as in the (RP) splitting, G 1 x <ref> [1] </ref> ; : : : ; x [K] := i=1 with the extended objective function h [i] (x [i] ) as in (3). The difference with the (RP) splitting lies in the definition of G 2 . We take here G 2 y [1] ; : : : ; y [K] <p> in the (RP) splitting, G 1 x <ref> [1] </ref> ; : : : ; x [K] := i=1 with the extended objective function h [i] (x [i] ) as in (3). The difference with the (RP) splitting lies in the definition of G 2 . We take here G 2 y [1] ; : : : ; y [K] := &gt; &gt; &gt; &gt; &lt; 0 if i=1 +1 otherwise (50) Both functions G 1 and G 2 are closed, convex and also proper, because of our assumption that (CBA) is solvable. <p> We can write problem (CBA) as min G 1 x <ref> [1] </ref> ; : : : ; x [K] + G 2 y [1] ; : : : ; y [K] subject to x [i] = y [i] ; i = 1; : : : ; K (51) which is in the form of problem (4), with the correspondences x [x [1] <p> We can write problem (CBA) as min G 1 x <ref> [1] </ref> ; : : : ; x [K] + G 2 y [1] ; : : : ; y [K] subject to x [i] = y [i] ; i = 1; : : : ; K (51) which is in the form of problem (4), with the correspondences x [x [1] : : : x [K] ]; A I ; (52) A tentative <p> <ref> [1] </ref> ; : : : ; x [K] + G 2 y [1] ; : : : ; y [K] subject to x [i] = y [i] ; i = 1; : : : ; K (51) which is in the form of problem (4), with the correspondences x [x [1] : : : x [K] ]; A I ; (52) A tentative Lagrange multiplier vector p [i] is associated with each block of constraints x [i] = y [i] , i = 1; : : : ; K. <p> We let each block i maintain its own diagonal positive penalty matrix fl t [i] . We define fl t := diag <ref> [1] </ref> ; : : : ; fl t We now present the resulting ADI method using shorthand notation. <p> We are mainly interested in comparing the relative performance of the three splittings in terms of number of iterations and total computational time to achieve a prescribed solution accuracy. To assess performance we used MNETGEN <ref> [1] </ref> to generate three hundred random multicommodity network problems (MC), one hundred with linear objectives and two hundred with quadratic objective functions. For (MC) the constraint matrices A [i] and D [i] have a special structure.
Reference: [2] <author> A.A. Assad. </author> <title> Multicommodity network flows: A survey. </title> <journal> Networks, </journal> <volume> 8 </volume> <pages> 37-91, </pages> <year> 1978. </year>
Reference-contexts: A collection of vectors x with indices in a finite set f1; : : : ; Kg will be represented by x [1] ; x <ref> [2] </ref> ; : : : ; x [K] . In case it is clear from the context, we sometimes let x represent the concatenation of the vectors x [1] ; x [2] ; : : : ; x [K] . For scalar ff, we define ff + := maxfff; 0g. <p> indices in a finite set f1; : : : ; Kg will be represented by x [1] ; x <ref> [2] </ref> ; : : : ; x [K] . In case it is clear from the context, we sometimes let x represent the concatenation of the vectors x [1] ; x [2] ; : : : ; x [K] . For scalar ff, we define ff + := maxfff; 0g. For vectors x and y, minfx; yg and maxfx; yg are to be taken component-wise. Similarly, for a vector x, x + is defined component-wise. <p> The problem, denoted by (CBA), is min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] i = 1; : : : ; K: We <p> The problem, denoted by (CBA), is min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] i = 1; : : : ; K: We take each <p> The problem, denoted by (CBA), is min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] i = 1; : : : ; K: We take each component function in the objective f [i] to be finite-valued, convex and continuous. <p> The problem, denoted by (CBA), is min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] i = 1; : : : ; K: We take each component function in the objective f [i] to be finite-valued, convex and continuous. <p> The problem, denoted by (CBA), is min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] i = 1; : : : ; K: We take each component function in the objective f [i] to be finite-valued, convex and continuous. <p> If some components of the activity are allowed to grow without restriction, we take the corresponding components of u [i] to be +1. Two particular classes of optimization problems that give rise to (CBA) and (LQ) problems are: multi-commodity network flow, which occurs in scheduling and transportation models <ref> [2] </ref>, and scenario analysis, which arises in stochastic optimization [19], for financial planning under uncertainty over a multi-period horizon. <p> We call such a choice a canonical initialization. Definition 4.5 An initialization d 0 ; q 0 ; p 0 for the ADI scheme for (ARP) is CANONICAL if q 0 = 0; p 0 <ref> [2] </ref> = : : : = p 0 K X d 0 8 4.1.2 The algorithm simplified We now make use of the results of the previous section and of canonical initialization in order to present a simpler version of the iterative step: we pass only three vectors from an iteration <p> Lemma 4.8 (Invariants of the (RP) splitting) Let the ADI scheme (33)-(35) be started from any p 0 , d 0 and any diagonal positive matrix fl 0 . Then, for t 0, (i) p t+1 <ref> [2] </ref> = : : : = p t+1 (ii) i=1 [i] d We can have these properties established one iteration earlier, by a canonical initialization. Definition 4.9 An initialization p 0 , d 0 for the ADI scheme for (RP) is CANONICAL if p 0 [2] = : : : = <p> t 0, (i) p t+1 <ref> [2] </ref> = : : : = p t+1 (ii) i=1 [i] d We can have these properties established one iteration earlier, by a canonical initialization. Definition 4.9 An initialization p 0 , d 0 for the ADI scheme for (RP) is CANONICAL if p 0 [2] = : : : = p 0 K X d 0 4.2.2 The algorithm simplified By making use of the results of the previous section and of canonical initialization we now present a simpler version of the algorithm. <p> We can have these properties established one iteration earlier, by a canonical initialization. Definition 4.12 An initialization p 0 , y 0 for the ADI scheme for (AP) is CANONICAL if p 0 <ref> [2] </ref> = : : : = p 0 T 0 ; for 0 0 and i=1 [i] d 4.3.2 The algorithm simplified For this splitting, it is the vectors, rather than the p multipliers, that carry the essential global information for the updates.
Reference: [3] <author> D.P. Bertsekas. </author> <title> Multiplier methods: A survey. </title> <journal> Automatica, </journal> <volume> 12 </volume> <pages> 133-145, </pages> <year> 1976. </year>
Reference-contexts: The augmented Lagrangian function is computationally more stable than the pure Lagrangian <ref> [3] </ref>.
Reference: [4] <author> D.P. Bertsekas. </author> <title> Constrained Optimization and Lagrange Multiplier Methods. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference: [5] <author> D.P. Bertsekas and J.N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall Inc., </publisher> <year> 1989. </year>
Reference-contexts: In [20] the following theorem is proved, by employing saddle-point arguments similar to those in [18, chapter 3], or <ref> [5, chapter 3] </ref>. Theorem 3.1 (Convergence of ADI) Assume that the original problem (4) and the ADI subproblems (7) and (8) are solvable. Let f (x t ; p t ; z t )g be some sequence of iterates produced by the ADI algorithm (7)-(9).
Reference: [6] <author> R.W. Cottle, J.-S. Pang, and R.E. Stone. </author> <title> The Linear Complementarity Problem. </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Then, by a standard theorem, ([6, chapter 3]) the LCP is solvable. In the general case, it can be solved by either a pivotal method, such as Lemke's, or an iterative one, such as matrix-splitting schemes (see <ref> [6, chapters 4 and 5] </ref> for a good coverage of such methods.) However, having to solve a non-trivial LCP per iteration is likely to increase the duration of the non-parallelizable join phase of the algorithm.
Reference: [7] <author> G.B. Dantzig and P. Wolfe. </author> <title> Decomposition principle for linear programs. </title> <journal> Operations Research, </journal> <volume> 8 </volume> <pages> 101-111, </pages> <year> 1960. </year>
Reference-contexts: Numerical results are presented for the solution of multicommodity flow problems with these three techniques on the TMC CM-5 parallel supercomputer. As with many other decomposition methods (Dantzig-Wolfe decomposition <ref> [7] </ref>, the Schultz-Meyer shifted barrier method [28], the Parallel Variable Distribution [13] and Parallel Constraint Distribution [12] algorithms of Ferris and Mangasarian, and the Diagonal Quadratic Approximation method of Mulvey and Ruszczy nski [24]) these procedures use the fork-join protocol for the global organization of the computation.
Reference: [8] <author> R. De Leone, R.R. Meyer, S. Kontogiorgis, A. Zakarian, and G. Zakeri. </author> <title> Coordination methods in coarse-grained decomposition. </title> <note> SIAM Journal on Optimization, 1994. to appear. </note>
Reference-contexts: This is in contrast to decomposition methods that involve solving a complex optimization problem in the coordination phase, a potential serial bottleneck (for further discussion see <ref> [8] </ref>). This article is organized as follows: section 2 introduces the convex block-angular problem and section 3 introduces the method of Alternating Directions (ADI). In section 4 we derive three highly-parallel algorithms that result from specializing the ADI method to the block-angular problem.
Reference: [9] <author> J. Douglas and H.H. Rachford Jr. </author> <title> On the numerical solution of heat conduction problems in two- and three-space variables. </title> <journal> Transactions of the American Mathematical Society, </journal> <volume> 82 </volume> <pages> 421-439, </pages> <year> 1956. </year>
Reference-contexts: In numerical analysis the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in <ref> [9] </ref> by these authors. This correspondence is discussed in detail in [17], [18], [21] and [10]. See also the monograph by Marchuk [23] for a discussion of splitting methods in the solution of pde's.
Reference: [10] <author> J. Eckstein. </author> <title> Splitting methods for monotone operators with applications to parallel optimization. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Civil Engineering, </institution> <year> 1989. </year>
Reference-contexts: The ADI method has been studied extensively and sufficient conditions for convergence have been obtained by either using the theory of maximal monotone operators (Lions and Mercier [21], Gabay [17], Eckstein and Bertsekas <ref> [10] </ref>, [11]), or the theory of Karush-Kuhn-Tucker saddle-points of Lagrangian functions (Glowinski, Fortin and Le Tallec [14], [18]). In numerical analysis the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [9] by these authors. <p> In numerical analysis the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [9] by these authors. This correspondence is discussed in detail in [17], [18], [21] and <ref> [10] </ref>. See also the monograph by Marchuk [23] for a discussion of splitting methods in the solution of pde's. <p> In case a single fixed penalty parameter is used, it is reported in [16] that the number of iterations can significantly increase if the penalty is chosen too small or too large. Similar computational results are reported in [14] and <ref> [10] </ref>. The solution of the subproblems was obtained using MINOS 5.4 ([25],[26]). <p> The resulting algorithm converges under more stringent assumptions, and is also less robust numerically, as Fortin and Glowinski point out in [14], citing experience in a variety of numerical analysis applications. The following theorem is from Eckstein <ref> [10] </ref>. Theorem 6.2 (Peaceman-Rachford method for convex optimization) Let the convex problem described in theorem 6.1 have a Karush-Kuhn-Tucker point.
Reference: [11] <author> J. Eckstein and D.P. Bertsekas. </author> <title> On the Douglas-Rachford splitting method and the proximal point method for maximal monotone operators. </title> <journal> Mathematical Programming, Series A, </journal> <volume> 55(3) </volume> <pages> 293-318, </pages> <year> 1992. </year> <month> 25 </month>
Reference-contexts: The ADI method has been studied extensively and sufficient conditions for convergence have been obtained by either using the theory of maximal monotone operators (Lions and Mercier [21], Gabay [17], Eckstein and Bertsekas [10], <ref> [11] </ref>), or the theory of Karush-Kuhn-Tucker saddle-points of Lagrangian functions (Glowinski, Fortin and Le Tallec [14], [18]). In numerical analysis the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [9] by these authors. <p> For the finite-dimensional case, A is assumed to have full column rank ([10], <ref> [11] </ref>) or G 1 is assumed to have compact level sets ([5, chapter 3]); for the dual algorithm in [16], both primal and dual problems are assumed to be feasible and the solution set of the primal is assumed to be bounded. 4 For the finite-dimensional setting, we have been able <p> In the first variant the subproblems are solved inexactly and a relaxation parameter ! is added. The following theorem is from Eckstein and Bertsekas <ref> [11] </ref>. Theorem 6.1 (Over-relaxation plus inexact minimization) Consider the finite dimensional problem min G 1 (x) + G 2 (z) subject to Ax = z with A a full-column rank matrix and G 1 , G 2 closed, proper, convex, extended-real-valued functions.
Reference: [12] <author> M.C. Ferris and O.L. Mangasarian. </author> <title> Parallel constraint distribution. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(4) </volume> <pages> 487-500, </pages> <year> 1991. </year>
Reference-contexts: Numerical results are presented for the solution of multicommodity flow problems with these three techniques on the TMC CM-5 parallel supercomputer. As with many other decomposition methods (Dantzig-Wolfe decomposition [7], the Schultz-Meyer shifted barrier method [28], the Parallel Variable Distribution [13] and Parallel Constraint Distribution <ref> [12] </ref> algorithms of Ferris and Mangasarian, and the Diagonal Quadratic Approximation method of Mulvey and Ruszczy nski [24]) these procedures use the fork-join protocol for the global organization of the computation.
Reference: [13] <author> M.C. Ferris and O.L. Mangasarian. </author> <title> Parallel variable distribution. </title> <type> Technical Report 1175, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1993. </year>
Reference-contexts: Numerical results are presented for the solution of multicommodity flow problems with these three techniques on the TMC CM-5 parallel supercomputer. As with many other decomposition methods (Dantzig-Wolfe decomposition [7], the Schultz-Meyer shifted barrier method [28], the Parallel Variable Distribution <ref> [13] </ref> and Parallel Constraint Distribution [12] algorithms of Ferris and Mangasarian, and the Diagonal Quadratic Approximation method of Mulvey and Ruszczy nski [24]) these procedures use the fork-join protocol for the global organization of the computation.
Reference: [14] <author> M. Fortin and R. Glowinski. </author> <title> On decomposition-coordination methods using an Augmented Lagrangian. </title> <editor> In M. Fortin and R. Glowinski, editors, </editor> <title> Augmented Lagrangian methods: Applications to the numerical solution of boundary-valued problems. </title> <publisher> North-Holland, </publisher> <year> 1983. </year>
Reference-contexts: The ADI method has been studied extensively and sufficient conditions for convergence have been obtained by either using the theory of maximal monotone operators (Lions and Mercier [21], Gabay [17], Eckstein and Bertsekas [10], [11]), or the theory of Karush-Kuhn-Tucker saddle-points of Lagrangian functions (Glowinski, Fortin and Le Tallec <ref> [14] </ref>, [18]). In numerical analysis the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [9] by these authors. This correspondence is discussed in detail in [17], [18], [21] and [10]. <p> In case a single fixed penalty parameter is used, it is reported in [16] that the number of iterations can significantly increase if the penalty is chosen too small or too large. Similar computational results are reported in <ref> [14] </ref> and [10]. The solution of the subproblems was obtained using MINOS 5.4 ([25],[26]). <p> The intention is to incorporate in the modified objective of the second subproblem the most recent information about the violation of the constraints. The resulting algorithm converges under more stringent assumptions, and is also less robust numerically, as Fortin and Glowinski point out in <ref> [14] </ref>, citing experience in a variety of numerical analysis applications. The following theorem is from Eckstein [10]. Theorem 6.2 (Peaceman-Rachford method for convex optimization) Let the convex problem described in theorem 6.1 have a Karush-Kuhn-Tucker point.
Reference: [15] <author> M. Frank and F. Wolfe. </author> <title> An algorithm for quadratic programming. </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 3 </volume> <pages> 95-110, </pages> <year> 1956. </year>
Reference-contexts: For problem (LQ), if we assume that the feasible set is nonempty, we can guarantee the existence of minimizers if each f [i] is bounded from below on B [i] , by the Frank-Wolfe theorem <ref> [15] </ref>.
Reference: [16] <author> M. Fukushima. </author> <title> Application of the alternating direction method of multipliers to separable convex programming problems. </title> <journal> Computational Optimization and Applications, </journal> <volume> 1 </volume> <pages> 93-111, </pages> <year> 1992. </year>
Reference-contexts: The scalar penalty factor &gt; 0 remains fixed throughout the iterations. The method works towards achieving optimality in both the primal and the dual space, by taking alternating steps. See also Fukushima <ref> [16] </ref> for an application of the method to the dual, rather than the primal problem (4). The rate of convergence is linear [17]. In order to reduce the number of iterations to optimality, we need to have the penalty term vary per iteration. <p> For the finite-dimensional case, A is assumed to have full column rank ([10], [11]) or G 1 is assumed to have compact level sets ([5, chapter 3]); for the dual algorithm in <ref> [16] </ref>, both primal and dual problems are assumed to be feasible and the solution set of the primal is assumed to be bounded. 4 For the finite-dimensional setting, we have been able to relax these assumptions, and require only existence of Karush-Kuhn-Tucker points for problems (4), (7) and (8). <p> In our code all penalty parameters remain within positive bounds, and are never decreased. In case a single fixed penalty parameter is used, it is reported in <ref> [16] </ref> that the number of iterations can significantly increase if the penalty is chosen too small or too large. Similar computational results are reported in [14] and [10]. The solution of the subproblems was obtained using MINOS 5.4 ([25],[26]).
Reference: [17] <author> D. Gabay. </author> <title> Applications of the method of multipliers to variational inequalities. </title> <editor> In M. Fortin and R. Glowinski, editors, </editor> <title> Augmented Lagrangian methods: Applications to the numerical solution of boundary-valued problems. </title> <publisher> North-Holland, </publisher> <year> 1983. </year>
Reference-contexts: The ADI method has been studied extensively and sufficient conditions for convergence have been obtained by either using the theory of maximal monotone operators (Lions and Mercier [21], Gabay <ref> [17] </ref>, Eckstein and Bertsekas [10], [11]), or the theory of Karush-Kuhn-Tucker saddle-points of Lagrangian functions (Glowinski, Fortin and Le Tallec [14], [18]). In numerical analysis the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [9] by these authors. <p> In numerical analysis the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [9] by these authors. This correspondence is discussed in detail in <ref> [17] </ref>, [18], [21] and [10]. See also the monograph by Marchuk [23] for a discussion of splitting methods in the solution of pde's. <p> The method works towards achieving optimality in both the primal and the dual space, by taking alternating steps. See also Fukushima [16] for an application of the method to the dual, rather than the primal problem (4). The rate of convergence is linear <ref> [17] </ref>. In order to reduce the number of iterations to optimality, we need to have the penalty term vary per iteration. An even better approach would be to use a separate value of for each linear constraint, also varying per iteration.
Reference: [18] <author> R. Glowinski and P. Le Tallec. </author> <title> Augmented Lagrangian and Operator-Splitting Methods in Nonlinear Mechanics. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1989. </year>
Reference-contexts: The ADI method has been studied extensively and sufficient conditions for convergence have been obtained by either using the theory of maximal monotone operators (Lions and Mercier [21], Gabay [17], Eckstein and Bertsekas [10], [11]), or the theory of Karush-Kuhn-Tucker saddle-points of Lagrangian functions (Glowinski, Fortin and Le Tallec [14], <ref> [18] </ref>). In numerical analysis the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [9] by these authors. This correspondence is discussed in detail in [17], [18], [21] and [10]. <p> Bertsekas [10], [11]), or the theory of Karush-Kuhn-Tucker saddle-points of Lagrangian functions (Glowinski, Fortin and Le Tallec [14], <ref> [18] </ref>). In numerical analysis the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [9] by these authors. This correspondence is discussed in detail in [17], [18], [21] and [10]. See also the monograph by Marchuk [23] for a discussion of splitting methods in the solution of pde's. <p> In [20] the following theorem is proved, by employing saddle-point arguments similar to those in <ref> [18, chapter 3] </ref>, or [5, chapter 3]. Theorem 3.1 (Convergence of ADI) Assume that the original problem (4) and the ADI subproblems (7) and (8) are solvable. Let f (x t ; p t ; z t )g be some sequence of iterates produced by the ADI algorithm (7)-(9). <p> In the existing literature convergence of the ADI iterates to an optimal point is proved under relatively strong assumptions on the problem: strict convexity and differentiability of G 1 and growth of G 1 faster than linear at infinity ([14], <ref> [18] </ref>).
Reference: [19] <author> P. </author> <title> Kall. Stochastic Linear Programming. </title> <publisher> Springer Verlag, </publisher> <year> 1976. </year>
Reference-contexts: Two particular classes of optimization problems that give rise to (CBA) and (LQ) problems are: multi-commodity network flow, which occurs in scheduling and transportation models [2], and scenario analysis, which arises in stochastic optimization <ref> [19] </ref>, for financial planning under uncertainty over a multi-period horizon.
Reference: [20] <author> S. Kontogiorgis. </author> <title> Alternating Direction Methods for the Parallel Solution of Large-scale Block-structured Optimization Problems. </title> <type> PhD thesis, </type> <institution> University of Wisconsin - Madison, Department of Computer Sciences, </institution> <note> in preparation, </note> <year> 1994. </year>
Reference-contexts: The proofs of the lemmas and theorems presented in this article are given in <ref> [20] </ref>. 2 The convex block-angular (CBA) problem Our object of study is a convex minimization problem with linear constraints having a specific block structure in both the objective function and the constraint matrix. <p> In <ref> [20] </ref> the following theorem is proved, by employing saddle-point arguments similar to those in [18, chapter 3], or [5, chapter 3]. Theorem 3.1 (Convergence of ADI) Assume that the original problem (4) and the ADI subproblems (7) and (8) are solvable. <p> Then, fx t g and fp t g converge to solutions of the primal and dual problem, respectively. 24 In <ref> [20] </ref> relaxation and Peaceman-Rachford iterative schemes for the (ARP) and (AP) splittings for (CBA) are derived. The main theoretical result is that the update formulas for these variants and the standard ADI (Douglas-Rachford) method are a family parameterized by a single scalar.
Reference: [21] <author> P.L. Lions and B. Mercier. </author> <title> Splitting algorithms for the sum of two nonlinear operators. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16(6) </volume> <pages> 964-979, </pages> <year> 1979. </year>
Reference-contexts: 1 Introduction Three decomposition algorithms are derived from the method of alternating directions <ref> [21] </ref> for block-angular optimization. The three methods will be categorized below according to the type of proximal terms that they contain. <p> The ADI method has been studied extensively and sufficient conditions for convergence have been obtained by either using the theory of maximal monotone operators (Lions and Mercier <ref> [21] </ref>, Gabay [17], Eckstein and Bertsekas [10], [11]), or the theory of Karush-Kuhn-Tucker saddle-points of Lagrangian functions (Glowinski, Fortin and Le Tallec [14], [18]). In numerical analysis the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [9] by these authors. <p> In numerical analysis the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [9] by these authors. This correspondence is discussed in detail in [17], [18], <ref> [21] </ref> and [10]. See also the monograph by Marchuk [23] for a discussion of splitting methods in the solution of pde's.
Reference: [22] <author> O.L. Mangasarian. </author> <title> Nonlinear Programming. </title> <publisher> McGraw-Hill, </publisher> <year> 1969. </year>
Reference-contexts: Since the constraints are linear, the reverse convex constraint qualification is satisfied <ref> [22] </ref>, and thus each minimizer of (4) corresponds to a saddle-point of the Lagrangian.
Reference: [23] <author> G.I. </author> <title> Marchuk. Splitting and alternating direction methods. </title> <editor> In P.G. Ciarlet and J.L. Lions, editors, </editor> <title> Handbook of Numerical Analysis. </title> <publisher> North-Holland, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: In numerical analysis the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [9] by these authors. This correspondence is discussed in detail in [17], [18], [21] and [10]. See also the monograph by Marchuk <ref> [23] </ref> for a discussion of splitting methods in the solution of pde's.
Reference: [24] <author> J.M. Mulvey and A. Ruszczynski. </author> <title> A diagonal quadratic approximation method for large scale linear programs. </title> <journal> Operations Research Letters, </journal> <volume> 12 </volume> <pages> 205-215, </pages> <year> 1992. </year>
Reference-contexts: As with many other decomposition methods (Dantzig-Wolfe decomposition [7], the Schultz-Meyer shifted barrier method [28], the Parallel Variable Distribution [13] and Parallel Constraint Distribution [12] algorithms of Ferris and Mangasarian, and the Diagonal Quadratic Approximation method of Mulvey and Ruszczy nski <ref> [24] </ref>) these procedures use the fork-join protocol for the global organization of the computation. In this computational model, there is a data parallel phase in which each processing element (PE) is assigned part of the current data and solves an optimization problem based on this data.
Reference: [25] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.1 user's guide. </title> <type> Technical Report SOL 83.20R, </type> <institution> Stanford University, </institution> <year> 1987. </year>
Reference: [26] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.4 release notes, appendix to MINOS 5.1 user's guide. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference: [27] <author> D.W. Peaceman and H.H. Rachford Jr. </author> <title> The numerical solution of parabolic and elliptic differential equations. </title> <journal> SIAM Journal, </journal> <volume> 3 </volume> <pages> 28-42, </pages> <year> 1955. </year>
Reference-contexts: If the dual has no optimal solution, then at least one of the sequences fx t g, fp t g is unbounded. In the second variant, called the Peaceman-Rachford method after <ref> [27] </ref>, an additional multiplier is intro duced and updated between the solution of the subproblems. The intention is to incorporate in the modified objective of the second subproblem the most recent information about the violation of the constraints.
Reference: [28] <author> G.L. Schultz and R.R. Meyer. </author> <title> An interior point method for block angular optimization. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(4) </volume> <pages> 583-602, </pages> <year> 1991. </year>
Reference-contexts: Numerical results are presented for the solution of multicommodity flow problems with these three techniques on the TMC CM-5 parallel supercomputer. As with many other decomposition methods (Dantzig-Wolfe decomposition [7], the Schultz-Meyer shifted barrier method <ref> [28] </ref>, the Parallel Variable Distribution [13] and Parallel Constraint Distribution [12] algorithms of Ferris and Mangasarian, and the Diagonal Quadratic Approximation method of Mulvey and Ruszczy nski [24]) these procedures use the fork-join protocol for the global organization of the computation.
Reference: [29] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <booktitle> The Connection Machine CM-5 Technical Summary, </booktitle> <year> 1991. </year>
Reference-contexts: Then, the sequence of ADI iterates for the (AP) splitting converges to an optimal primal solution for (CBA). 17 5 Computational results We have implemented the three ADI algorithms on the Connection Machine CM-5 <ref> [29] </ref> at the Computer Sciences Department of the University of Wisconsin-Madison, which consists of two 32-PE partitions of 33-MHz SPARC processors connected in a fat-tree topology. The machine uses the CMOST Version 7.3 Beta.2.7 operating system.
References-found: 29

