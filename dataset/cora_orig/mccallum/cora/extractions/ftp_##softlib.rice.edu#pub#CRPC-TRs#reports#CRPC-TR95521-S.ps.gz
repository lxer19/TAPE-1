URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95521-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: [seema,ken,johnmc,sethi]@rice.edu  
Title: Compilation Techniques for Block-Cyclic Distributions  
Author: Seema Hiranandani Ken Kennedy John Mellor-Crummey Ajay Sethi 
Address: Houston, TX 77251-1892  
Affiliation: Department of Computer Science Rice University  
Abstract: Compilers for data-parallel languages such as Fortran D and High-Performance Fortran use data alignment and distribution specifications as the basis for translating programs for execution on MIMD distributed-memory machines. This paper describes techniques for generating efficient code for programs that use block-cyclic distributions. These techniques can be applied to programs with symbolic loop bounds, symbolic array dimensions, and loops with non-unit strides. We present algorithms for computing the data elements that need to be communicated among processors both for loops with unit and non-unit strides, a linear-time algorithm for computing the memory access sequence for loops with non-unit strides, and experimental results for a hand-compiled test case using block-cyclic distributions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V.S. Adve, C. Koelbel, and J.Mellor-Crummey. </author> <title> Performance analysis of data parallel programs. </title> <note> Submitted to Supercomputing '94, </note> <month> April </month> <year> 1994. </year>
Reference-contexts: By broadcasting the first pivot as soon as possible, other processors can perform elimination steps while the other processor computes the pivots for the rest of the columns. This optimization would maximize the communication and computation overlap. A similar optimization was discussed by Adve, et. al. <ref> [1] </ref> in the context of cyclic distributions. To evaluate the potential benefits of block-cyclic distributions for data parallel codes, it is clear that further work is necessary.
Reference: [2] <author> F. Bodin, E.D. Granston, and T. Montaut. </author> <title> Experiences reducing false sharing in shared virtual memory systems (under preparation). </title> <type> Technical report, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <year> 1994. </year>
Reference-contexts: If these quantities are symbolic variables, then the formulae would require run-time evaluation. In experiments that measured the overhead of performing similar calculations at run time in shared virtual memory systems, the overhead of these calculations was found to be insignificant <ref> [2] </ref>.
Reference: [3] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: A owned by processor 0 elements of B needed by processor 0 (b) Elements of B owned by processor 1 (c) Elements which processor 1 needs to send to processor 0 (a) " (b) on run-time resolution to explicitly calculate the ownership and communication for each reference at run time <ref> [3, 20, 23] </ref>. <p> Each processor only computes values of data it owns <ref> [3, 20, 23] </ref>. The left-hand side (lhs) of each assignment statement in a loop nest is used to calculate the set of loop iterations that cause a processor to assign to local data.
Reference: [4] <author> S. Chatterjee, J. Gilbert, F. Long, R. Schreiber, and S. Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <booktitle> In Proceedings of the Fourth ACM SIG-PLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Cyclic and block-cyclic distributions are useful for writing efficient and load-balanced dense matrix algorithms on distributed-memory machines [5]. Previous research has focused on compilation strategies for handling block and cyclic distributions efficiently [19, 17]. Block-cyclic distributions have only been studied in detail recently. Chatterjee et. al. <ref> [4] </ref> present a general solution for generating local addresses and communication sets for data-parallel programs with block-cyclic distributions. Stichnoth et. al. [22] look at the problem of generating communication sets for block-cyclically distributed arrays. Gupta et. al. [7] compute the communication and local index sets using virtual processor approach. <p> The access stride for a given array reference on a processor is distance in local memory between each access. For example, the stride for array reference A (i; j) on processor 0 is (11, 3, 3, 3). The key insight, as noted in <ref> [4] </ref>, is that the offset of an element determines the offset of the next element on the same processor. Since the offsets range between 0 and (block-size-1), by pigeon hole principle, at least two of the first (block-size+1) local memory locations on any particular processor must have the same offset. <p> As soon as an already filled array element is encountered, the algorithm stops. Therefore, the while loop (together with the inner for loop) iterates at most b times and hence it is an O (b) algorithm. Therefore, as compared to the method suggested in <ref> [4] </ref>, not only is our approach conceptually more intuitive, the algorithms given above are an O (b). 5.2 Send and Recv Sets Once we have computed the memory access sequence for an array access, the computation of the send and receive sets is comparatively easier. <p> In general, in case of communication required because of shifts, we can find both the processors that need to send the data and the location of the array element within the owner processor <ref> [4] </ref> . Note that, in case of shifts, a processor communicates with at most two processors. Suppose that element A (i) is located on processor p with offset o. We want to find the processor and local memory location of A (i-d).
Reference: [5] <author> J. Dongarra, R. van de Geijn, and D. Walker. </author> <title> A look at scalable dense linear algebra libraries. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <pages> pages 372-379, </pages> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: A more precise definition of these distributions is given in section 3. Cyclic and block-cyclic distributions are useful for writing efficient and load-balanced dense matrix algorithms on distributed-memory machines <ref> [5] </ref>. Previous research has focused on compilation strategies for handling block and cyclic distributions efficiently [19, 17]. Block-cyclic distributions have only been studied in detail recently. Chatterjee et. al. [4] present a general solution for generating local addresses and communication sets for data-parallel programs with block-cyclic distributions.
Reference: [6] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: For RSDs representing array elements contiguous to the local array section, the compiler reserves storage using overlaps created by extending the local array bounds <ref> [6] </ref>. Otherwise, temporary buffers or hash tables are used for storing non-local data. 7) Generate code The compiler uses the results of previous stages to generate a SPMD message-passing program for nodes of a MIMD distributed-memory machine.
Reference: [7] <author> S.K.S. Gupta, S.D. Kaushik, S. Mufti, S. Sharma, C.-H. Huang, and P. Sadayappan. </author> <title> On compiling array expressions for efficient execution on distributed-memory machines. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 301-305, </pages> <year> 1993. </year>
Reference-contexts: Block-cyclic distributions have only been studied in detail recently. Chatterjee et. al. [4] present a general solution for generating local addresses and communication sets for data-parallel programs with block-cyclic distributions. Stichnoth et. al. [22] look at the problem of generating communication sets for block-cyclically distributed arrays. Gupta et. al. <ref> [7] </ref> compute the communication and local index sets using virtual processor approach.
Reference: [8] <author> S.K.S. Gupta, S.D. Kaushik, C.-H. Huang, and P. Sadayap-pan. </author> <title> On compiling array expressions for efficient execution on distributed-memory machines. </title> <type> Technical report OSU-CISRC-4/94-TR19, </type> <institution> Department of Computer and Information Science, Ohio State University, Columbus, OH, </institution> <month> March </month> <year> 1994. </year>
Reference: [9] <author> R. v. Hanxleden. </author> <title> Handling irregular problems with fortran d a preliminary report. </title> <type> Technical Report CRPC-TR93339-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: For more complicated patterns, for example in case of stride changes, there does not exist any simple lookup technique for generating the communication sets because the pattern of destination processors can have period longer than the block size b. In such cases, we resort to the inspector-executor model <ref> [21, 9] </ref> for irregular loops. 6 Example and Experimental Results To explore the effects of block-cyclic data distributions, we experimented with the DGEFA subroutine from Linpack. DGEFA is a key subroutine which performs Gaussian elimination with partial pivoting.
Reference: [10] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of inter-procedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Regular section descriptors (RSDs) are built for the sections of data to be communicated. RSDs compactly represent rectangular array sections and their higher dimension analogs <ref> [10] </ref>. 6) Manage storage The compiler identifies the extent and type of non-local data accesses represented by RSDs to calculate the storage required for non-local data.
Reference: [11] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Data parallel languages such as High-Performance Fortran (HPF) <ref> [11, 18] </ref> and Fortran D [15] have attracted considerable attention as promising languages for writing portable parallel programs. These languages support an abstract model of parallel programming in which users annotate a single-threaded program with data alignment and distribution directives.
Reference: [12] <author> S. Hiranandani, K. Kennedy, J. Mellor-Crummey, and A. Sethi. </author> <title> Advanced compilation techniques for fortran d. </title> <type> Technical Report CRPC-TR93338, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Statement S 1 corresponds to the computation of the pivot while statement S 2 corresponds to row elimination with column indexing. Performing partition and communication analysis (refer to <ref> [12] </ref> for details) yields the following sets for statement S 2 : image set A (p) = [1 : n][pfl8+1 : n : 8 : P fl8] iter set A (p) = [1 : n][lb : n : 8 : P fl8][k+1 : n] index set A (p) = [k + <p> Each non-local reference is classified as resulting in Single Send/Receive, Shift, Broadcast, Gather, All-to-All, Inspector/Executor or Run-time Resolution type of communication. The details of the algorithm can be found in <ref> [12] </ref>. 4.4 Send and Receive Sets In order to compute the send and receive sets (send p set and recv p set, respectively), the compiler needs to find out, in the most general case, the intersection of two block-cyclically distributed arrays.
Reference: [13] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Below, we briefly review the sequence of steps performed by the Rice Fortran 77D compiler; details of the compilation process are described elsewhere <ref> [13, 14] </ref>. 1) Analyze program The compiler performs scalar dataflow analysis, symbolic analysis, and dependence testing to determine the type and level of all data dependencies [16]. 2) Partition data The compiler determines the decomposition of each array and uses alignment and distribution statements to calculate the array section owned by
Reference: [14] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: Below, we briefly review the sequence of steps performed by the Rice Fortran 77D compiler; details of the compilation process are described elsewhere <ref> [13, 14] </ref>. 1) Analyze program The compiler performs scalar dataflow analysis, symbolic analysis, and dependence testing to determine the type and level of all data dependencies [16]. 2) Partition data The compiler determines the decomposition of each array and uses alignment and distribution statements to calculate the array section owned by <p> We use P to denote the number of processors, numbered 0 through P 1. For the following canonical loop nest, do ~ i = ~ l to ~u by ~s enddo we define the following sets; formal definitions of these sets are presented elsewhere <ref> [14] </ref>. * image set B (p) is the set of indices of array B that are owned by processor p. * iter set A (p) is the set of loop iterations that cause ref erence A to access data owned by processor p. * index set B (p) is the set
Reference: [15] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Data parallel languages such as High-Performance Fortran (HPF) [11, 18] and Fortran D <ref> [15] </ref> have attracted considerable attention as promising languages for writing portable parallel programs. These languages support an abstract model of parallel programming in which users annotate a single-threaded program with data alignment and distribution directives.
Reference: [16] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Below, we briefly review the sequence of steps performed by the Rice Fortran 77D compiler; details of the compilation process are described elsewhere [13, 14]. 1) Analyze program The compiler performs scalar dataflow analysis, symbolic analysis, and dependence testing to determine the type and level of all data dependencies <ref> [16] </ref>. 2) Partition data The compiler determines the decomposition of each array and uses alignment and distribution statements to calculate the array section owned by each processor. 3) Partition computation The compiler uses the "owner computes" rule to partition the computation among the processors.
Reference: [17] <author> C. Koelbel. </author> <title> Compile-time generation of regular communications patterns. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 101-110, </pages> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: A more precise definition of these distributions is given in section 3. Cyclic and block-cyclic distributions are useful for writing efficient and load-balanced dense matrix algorithms on distributed-memory machines [5]. Previous research has focused on compilation strategies for handling block and cyclic distributions efficiently <ref> [19, 17] </ref>. Block-cyclic distributions have only been studied in detail recently. Chatterjee et. al. [4] present a general solution for generating local addresses and communication sets for data-parallel programs with block-cyclic distributions. Stichnoth et. al. [22] look at the problem of generating communication sets for block-cyclically distributed arrays.
Reference: [18] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Data parallel languages such as High-Performance Fortran (HPF) <ref> [11, 18] </ref> and Fortran D [15] have attracted considerable attention as promising languages for writing portable parallel programs. These languages support an abstract model of parallel programming in which users annotate a single-threaded program with data alignment and distribution directives.
Reference: [19] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space programs for distributed execution. </title> <type> ICASE Report 90-70, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: A more precise definition of these distributions is given in section 3. Cyclic and block-cyclic distributions are useful for writing efficient and load-balanced dense matrix algorithms on distributed-memory machines [5]. Previous research has focused on compilation strategies for handling block and cyclic distributions efficiently <ref> [19, 17] </ref>. Block-cyclic distributions have only been studied in detail recently. Chatterjee et. al. [4] present a general solution for generating local addresses and communication sets for data-parallel programs with block-cyclic distributions. Stichnoth et. al. [22] look at the problem of generating communication sets for block-cyclically distributed arrays.
Reference: [20] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: A owned by processor 0 elements of B needed by processor 0 (b) Elements of B owned by processor 1 (c) Elements which processor 1 needs to send to processor 0 (a) " (b) on run-time resolution to explicitly calculate the ownership and communication for each reference at run time <ref> [3, 20, 23] </ref>. <p> Each processor only computes values of data it owns <ref> [3, 20, 23] </ref>. The left-hand side (lhs) of each assignment statement in a loop nest is used to calculate the set of loop iterations that cause a processor to assign to local data.
Reference: [21] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(4) </volume> <pages> 303-312, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: For more complicated patterns, for example in case of stride changes, there does not exist any simple lookup technique for generating the communication sets because the pattern of destination processors can have period longer than the block size b. In such cases, we resort to the inspector-executor model <ref> [21, 9] </ref> for irregular loops. 6 Example and Experimental Results To explore the effects of block-cyclic data distributions, we experimented with the DGEFA subroutine from Linpack. DGEFA is a key subroutine which performs Gaussian elimination with partial pivoting.
Reference: [22] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating communication for array statements: Design, implementation, and evaluation. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Previous research has focused on compilation strategies for handling block and cyclic distributions efficiently [19, 17]. Block-cyclic distributions have only been studied in detail recently. Chatterjee et. al. [4] present a general solution for generating local addresses and communication sets for data-parallel programs with block-cyclic distributions. Stichnoth et. al. <ref> [22] </ref> look at the problem of generating communication sets for block-cyclically distributed arrays. Gupta et. al. [7] compute the communication and local index sets using virtual processor approach. <p> However, as the example in Stichnoth et. al. <ref> [22] </ref> treat the block-cyclic sets as a union of disjoint cyclic sets. Since the cyclic sets are closed under intersection, the intersection of the two block-cyclic sets can be determined by intersecting all possible pairs of the cyclic sets. <p> Hence the time required to compute the intersection is O (LCM (Block-size (A), Block-size (B))*P) which is better than the time complexity of the method suggested in <ref> [22] </ref>. Note that both the sender and receiver compute the intersection using the global indices. To pack and unpack the message, both the processors need to translate the global indices to local indices.
Reference: [23] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: A owned by processor 0 elements of B needed by processor 0 (b) Elements of B owned by processor 1 (c) Elements which processor 1 needs to send to processor 0 (a) " (b) on run-time resolution to explicitly calculate the ownership and communication for each reference at run time <ref> [3, 20, 23] </ref>. <p> Each processor only computes values of data it owns <ref> [3, 20, 23] </ref>. The left-hand side (lhs) of each assignment statement in a loop nest is used to calculate the set of loop iterations that cause a processor to assign to local data.
References-found: 23

