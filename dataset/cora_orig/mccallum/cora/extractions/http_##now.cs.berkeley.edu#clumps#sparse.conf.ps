URL: http://now.cs.berkeley.edu/clumps/sparse.conf.ps
Refering-URL: http://now.cs.berkeley.edu/clumps/index.html
Root-URL: 
Title: Improving Memory-System Performance of Sparse Matrix-Vector Multiplication  
Author: Sivan Toledo 
Abstract: Sparse matrix-vector multiplication is an important kernel that often runs inefficiently on superscalar RISC processors. This paper describes techniques that increase instruction-level parallelism and improve performance. The techniques include reordering to reduce cache misses originally due to Das et al., blocking to reduce load instructions, and prefetching to prevent multiple load-store units from stalling simultaneously. The techniques improve performance from about 40 Mflops (on a well-ordered matrix) to over 100 Mflops on a 266 Mflops machine.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. C. Agarwal, F. G. Gustavson, and M. Zubair, </author> <title> A high performance algorithm using pre-processing for sparse matrix-vector multiplication, </title> <booktitle> in Proceedings of Supercomputing '92, </booktitle> <month> Nov. </month> <year> 1992, </year> <pages> pp. 32-41. </pages>
Reference-contexts: The floating-point units are therefore underutilized. We present below a blocking technique that reduces the number of load instructions. Blocking in sparse matrix-vector multiplication was used in somewhat different forms in <ref> [1, 3] </ref>. Another inessential but still important factor that limits the performance of the code in Figure 1 is the fact that the column index colind (jp) must be converted from an integer index to a byte offset from the beginning of x. <p> Other researchers have proposed algorithms that attempt to find larger (and hence fewer) dense blocks and/or blocks that are not completely dense. The full paper contains detailed comparisons between our blocking strategy and those of Agarwal, Gustavson and Zubair <ref> [1] </ref> and of Balay, Gropp, McInnes and Smith [3]. Prefetching in Irregular Loops. <p> We have explored it further and found that the Cuthill-McKee yields excellent results 8 on a variety of matrices. Two other techniques, representing nonzeros in small dense blocks and prefetching to allow cache hits-under-miss processing are novel (others have proposed to represent nonzeros in larger blocks <ref> [1, 3] </ref>). They, too, improve performance significantly on many matrices. On an IBM workstation, the combined effect of the four techniques can improve performance from about 15 Mflops to over 95 Mflops depending on the size and sparseness of the matrix.
Reference: [2] <author> R. C. Agarwal, F. G. Gustavson, and M. Zubair, </author> <title> Improving performance of linear algebra algorithms for dense matrices using algorithmic prefetch, </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38 (1994), </volume> <pages> pp. 265-275. </pages>
Reference-contexts: Prefetching in Irregular Loops. Traditionally, prefetching was considered to be a technique for hiding latency, in the sense that prefetching can prevent memory access latency from degrading performance, as long as memory bandwidth is sufficient to keep the processing units busy (See <ref> [2] </ref> or [8], for example). In many codes, for example dense matrix multiplication, the ratio of floating-point to load instructions is high. This high ratio allows the algorithm to hide the latency of cache misses by prefetching cache lines early. <p> To put our results in perspective, we note that on this machine the dense matrix-vector-multiplication subroutine (DGEMV) in IBM's Engineering and Scientific Subroutine Library, which uses both blocking and prefetching <ref> [2] </ref>, achieves performance of about 170 Mflops when applied to large matrices. 5 Table 1 The characteristics of the test-suite matrices.
Reference: [3] <author> S. Balay, W. Gropp, L. C. McInnes, and B. Smith, </author> <title> PETSc 2.0 users manual, </title> <type> Tech. Rep. </type> <institution> ANL-95/11, Revision 2.0.15, Argonne National Laboratory, </institution> <year> 1996. </year>
Reference-contexts: The floating-point units are therefore underutilized. We present below a blocking technique that reduces the number of load instructions. Blocking in sparse matrix-vector multiplication was used in somewhat different forms in <ref> [1, 3] </ref>. Another inessential but still important factor that limits the performance of the code in Figure 1 is the fact that the column index colind (jp) must be converted from an integer index to a byte offset from the beginning of x. <p> Other researchers have proposed algorithms that attempt to find larger (and hence fewer) dense blocks and/or blocks that are not completely dense. The full paper contains detailed comparisons between our blocking strategy and those of Agarwal, Gustavson and Zubair [1] and of Balay, Gropp, McInnes and Smith <ref> [3] </ref>. Prefetching in Irregular Loops. Traditionally, prefetching was considered to be a technique for hiding latency, in the sense that prefetching can prevent memory access latency from degrading performance, as long as memory bandwidth is sufficient to keep the processing units busy (See [2] or [8], for example). <p> We have explored it further and found that the Cuthill-McKee yields excellent results 8 on a variety of matrices. Two other techniques, representing nonzeros in small dense blocks and prefetching to allow cache hits-under-miss processing are novel (others have proposed to represent nonzeros in larger blocks <ref> [1, 3] </ref>). They, too, improve performance significantly on many matrices. On an IBM workstation, the combined effect of the four techniques can improve performance from about 15 Mflops to over 95 Mflops depending on the size and sparseness of the matrix.
Reference: [4] <author> D. A. Burgess and M. B. Giles, </author> <title> Renumbering unstructured grids to improve the performance of codes on hierarchical memory machines, </title> <type> Tech. Rep. 95/06, </type> <institution> Numerical Analysis Group, Oxford University Computing Laboratory, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: One technique that can reduce the number of cache misses is to reorder the matrix to reduce the number of cache misses on x. This technique was proposed by Das et al. [5], analyzed in certain cases by Temam and Jalby [9] and further investigated by Burgess and Giles <ref> [4] </ref>. We study this technique further in this paper, and we also show that the effectiveness of the new techniques that we propose depends on it. A second factor that limits performance is the tendency of multiple load/store functional units to miss on the same cache line. <p> Temam and Jalby [9] analyzed the number of cache misses as a function of the bandwidth for certain cache configurations. The technique was investigated further by Burgess 3 and Giles <ref> [4] </ref>, who extended it to other unstructured-grid computations. Burgess and Giles studied experimentally several reordering strategies, including reverse Cuthill-McKee and a greedy blocking method. The found that reordering improved performance relative to a random ordering, but they did not find a sensitivity to the particular ordering method used. <p> One of the techniques, precomputing addresses for indirect addressing, is trivial but important. The technique of reordering the matrix to reduce its bandwidth and hence reduce cache misses has been proposed by Das et al. [5] and investigated further in two other papers <ref> [4, 9] </ref>. We have explored it further and found that the Cuthill-McKee yields excellent results 8 on a variety of matrices. Two other techniques, representing nonzeros in small dense blocks and prefetching to allow cache hits-under-miss processing are novel (others have proposed to represent nonzeros in larger blocks [1, 3]).
Reference: [5] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy, </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives, </title> <journal> AIAA Journal, </journal> <volume> 32 (1994), </volume> <pages> pp. 489-496. </pages>
Reference-contexts: One technique that can reduce the number of cache misses is to reorder the matrix to reduce the number of cache misses on x. This technique was proposed by Das et al. <ref> [5] </ref>, analyzed in certain cases by Temam and Jalby [9] and further investigated by Burgess and Giles [4]. We study this technique further in this paper, and we also show that the effectiveness of the new techniques that we propose depends on it. <p> Reducing Cache Misses through Bandwidth Reduction. Das et al. <ref> [5] </ref> proposed to reorder sparse matrices using a bandwidth-reducing technique in order to reduce the number of cache misses that accesses to x generate. Temam and Jalby [9] analyzed the number of cache misses as a function of the bandwidth for certain cache configurations. <p> One of the techniques, precomputing addresses for indirect addressing, is trivial but important. The technique of reordering the matrix to reduce its bandwidth and hence reduce cache misses has been proposed by Das et al. <ref> [5] </ref> and investigated further in two other papers [4, 9]. We have explored it further and found that the Cuthill-McKee yields excellent results 8 on a variety of matrices.
Reference: [6] <author> I. S. Duff and G. Meurant, </author> <title> The effect of ordering on preconditioned conjugate gradient, </title> <journal> BIT, </journal> <volume> 29 (1989), </volume> <pages> pp. 635-657. </pages>
Reference-contexts: Reordering sparse matrices using the Cuthill-McKee ordering has another benefit in sparse iterative solvers. When a conjugate gradient solver uses an incomplete Cholesky preconditioner, the ordering of the matrix effects the convergence rate. Duff and Meurant <ref> [6] </ref> compared the convergence rate of incomplete-Cholesky-preconditioned conjugate gradient with 17 different orderings on 4 model problems. In their tests the Cuthill-McKee and the reverse Cuthill-McKee resulted convergence rates that were best or close to best.
Reference: [7] <author> A. Gupta, WGPP: </author> <title> Watson graph partitioning (and sparse matrix ordering) package, </title> <type> Tech. Rep. </type> <institution> RC20453, IBM T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The five orderings are random ordering, a nested-dissection-type ordering (denoted by WGPP), reverse Cuthill-McKee (RCM), Cuthill-McKee (CM), and the original ordering of the matrix as stored in the matrix collection. The WGPP ordering code was written by Anshul Gupta <ref> [7] </ref>. The Effect of Blocking and Prefetching. Figure 2 shows the performance of the ten codes.
Reference: [8] <author> A. K. Porterfield, </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications, </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Prefetching in Irregular Loops. Traditionally, prefetching was considered to be a technique for hiding latency, in the sense that prefetching can prevent memory access latency from degrading performance, as long as memory bandwidth is sufficient to keep the processing units busy (See [2] or <ref> [8] </ref>, for example). In many codes, for example dense matrix multiplication, the ratio of floating-point to load instructions is high. This high ratio allows the algorithm to hide the latency of cache misses by prefetching cache lines early.
Reference: [9] <author> O. Temam and W. Jalby, </author> <title> Characterizing the behavior of sparse algorithms on caches, </title> <booktitle> in Proceedings of Supercomputing '92, </booktitle> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: One technique that can reduce the number of cache misses is to reorder the matrix to reduce the number of cache misses on x. This technique was proposed by Das et al. [5], analyzed in certain cases by Temam and Jalby <ref> [9] </ref> and further investigated by Burgess and Giles [4]. We study this technique further in this paper, and we also show that the effectiveness of the new techniques that we propose depends on it. <p> Reducing Cache Misses through Bandwidth Reduction. Das et al. [5] proposed to reorder sparse matrices using a bandwidth-reducing technique in order to reduce the number of cache misses that accesses to x generate. Temam and Jalby <ref> [9] </ref> analyzed the number of cache misses as a function of the bandwidth for certain cache configurations. The technique was investigated further by Burgess 3 and Giles [4], who extended it to other unstructured-grid computations. <p> One of the techniques, precomputing addresses for indirect addressing, is trivial but important. The technique of reordering the matrix to reduce its bandwidth and hence reduce cache misses has been proposed by Das et al. [5] and investigated further in two other papers <ref> [4, 9] </ref>. We have explored it further and found that the Cuthill-McKee yields excellent results 8 on a variety of matrices. Two other techniques, representing nonzeros in small dense blocks and prefetching to allow cache hits-under-miss processing are novel (others have proposed to represent nonzeros in larger blocks [1, 3]).
References-found: 9

