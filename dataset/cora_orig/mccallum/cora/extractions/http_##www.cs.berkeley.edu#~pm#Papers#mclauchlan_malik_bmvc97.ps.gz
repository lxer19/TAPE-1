URL: http://www.cs.berkeley.edu/~pm/Papers/mclauchlan_malik_bmvc97.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~pm/Abstracts/abstracts.html
Root-URL: 
Title: Vision for Longitudinal Vehicle Control  
Abstract: An important component of the drive towards intelligent vehicles is the ability to maintain a fixed distance from a lead vehicle using feedback provided by range sensors. We are investigating the possibility of using stereo vision to provide the range information, in conjunction with a scanning laser radar sensor. The vision algorithms build on fixation and reconstruction algorithms designed for active vision systems, and combine stereo and motion cues. We shall present preliminary results comparing the quality of range measurements provided by a vision system with the laser radar system, using data obtained off-line. Later we will implement the tracker in real time on a network of C40 DSPs, and combine the laser and vision sensing in a cooperative manner. Note to reviewers: a longer version of this paper has been submitted to the IEEE Intelligent Transportation Systems conference in Boston. At BMVC we shall present new results from the real-time system currently under development. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Beymer, P.F. McLauchlan, B. Coifman, and J. Malik. </author> <title> A real-time computer vision system for measuring traffic parameters. </title> <note> Accepted for CVPR'97, </note> <year> 1997. </year>
Reference-contexts: Redundancy is a vital issue here, because vision is a massively redundant sensor, and approaches which negate this aspect are likely to be discarded in the long term. Previous experience of vehicle tracking without explicit models <ref> [1, 10] </ref> leads us to believe that this approach is valid. Because we reconstruct the geometry of the lead vehicle, the algorithms generalize naturally to different vehicle types.
Reference: [2] <author> A. Blake, R. Curwen, and A. Zisserman. </author> <title> A framework for spatiotemporal control in the tracking of visual contours. </title> <journal> International Journal of Computer Vision, </journal> <volume> 11(2) </volume> <pages> 127-146, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Model-based trackers locate known features in the scene and use them to update the pose of the object. Fixed geometric models [8] and deformable "snakes" <ref> [2] </ref> have been tried. 1 2. "Blob" trackers which define an object of interest as one which may be seg-mented from the rest of the scene using a "figure/ground" type of algorithm, such as detection of motion against a stationary background. This approach is very common [11, 10, 14]. 3.
Reference: [3] <author> K. J. Bradshaw, P. F. McLauchlan, I. D. Reid, and D. W. Murray. </author> <title> Saccade and pursuit on an active head/eye platform. </title> <editor> In J. Illingworth, editor, </editor> <booktitle> Proc. 4th British Machine Vision Conf., Guildford, </booktitle> <pages> pages 35-44. </pages> <publisher> BMVA Press, </publisher> <year> 1993. </year>
Reference-contexts: This approach is very common [11, 10, 14]. 3. Direct image velocity measurement trackers that compute velocity using image flow, correlation or similar methods <ref> [4, 19, 3] </ref>. Here we assume the usual correlation scheme whereby the correlation template is updated over time. The template may be kept fixed, in which case the correlation tracker returns position feedback rather than velocity feedback. <p> In our highway scenario we can expect the apparent motion of the background to be large, so we cannot use methods in class 2. Velocity-based trackers (class 3) have had some success in motion tracking <ref> [19, 3] </ref>, but they have a basic problem that restricts their usefulness. In order to track an object over an extended time, it is necessary to compute the position of the object, which in this context may only be computed by integrating the velocity over time. <p> Since there are inevitable errors in the computed velocities, these errors will tend to accumulate over time. Thus we can expect the computed position to drift over time, unless a separate process corrects for the drift by re-centering the object using a figure/ground separation algorithm, as was done in <ref> [3] </ref>. Thus we are drawn to feature tracking algorithms, which have the capability to allow the position of an object to be accurately estimated over an extended time.
Reference: [4] <author> P.J. Burt, J.R. Bergen, R. Hingorani, R. Kolczynski, W. A. Lee, A. Leung, J. Lubin, and H. Shvaytser. </author> <title> Object tracking with a moving camera. </title> <booktitle> In Proc. IEEE Workshop on Visual Motion, </booktitle> <pages> pages 2-12, </pages> <year> 1989. </year>
Reference-contexts: This approach is very common [11, 10, 14]. 3. Direct image velocity measurement trackers that compute velocity using image flow, correlation or similar methods <ref> [4, 19, 3] </ref>. Here we assume the usual correlation scheme whereby the correlation template is updated over time. The template may be kept fixed, in which case the correlation tracker returns position feedback rather than velocity feedback.
Reference: [5] <editor> E.D. Dickmanns and B.D. Mysliwetz. </editor> <title> Recursive 3-d road and relative ego-state recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(2) </volume> <pages> 199-213, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: While the context of highways and vehicles is clearly very structured, we avoid using direct scene models in the low-level tracking algorithms, and this distinguishes our work from that of Dickmanns's group <ref> [5] </ref>, for instance. Thus we have rejected model-based trackers (class 1). We draw on the large amount of work on scene reconstruction from multiple images in unstructured scenes, in particular the work on robust motion segmentation [28], and affine reconstruction [26].
Reference: [6] <author> S.M. Fairley, I.D. Reid, and D.W. Murray. </author> <title> Transfer of fixation for an active stereo platform via affine structure recovery. </title> <booktitle> In Proc. 5th Int'l Conf. on Computer Vision, Boston, </booktitle> <pages> pages 1100-1105. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: The motion parameters were then used to transfer a chosen fixation point from the previous image (s) to the latest. This algorithm was extended to stereo cameras in <ref> [6] </ref>. While we have a simpler problem, with a fixed camera and a 2D scene representation, similar principles apply. <p> The VSDF is a general algorithm for visual reconstruction that deals naturally with fragmentary data and combines data from multiple images in a statistically near-optimal manner. In comparison with the previous system <ref> [6] </ref> we have improved the robustness of the feature tracker to the extent that outliers are much less likely to occur, because the set of feature matches is forced to be globally consistent [27, 12]. <p> We thus employ the Tomasi/Kanade algorithm on the first stere-opair of our image sequence, and thereafter use the simplest version of the VSDF as presented in [16]. Details may be found in [13]. 5 Affine transfer The theory of affine transfer presented in <ref> [20, 21, 6] </ref> is a method of choosing a fixation point on a tracked object, so that image-plane errors of the fixation point from the centre of the image (s) may be fed back to the motors controlling the camera (s).
Reference: [7] <author> M.A. Fischler and R.C. Bolles. </author> <title> Random sample concensus: A paradigm for model fitting with applications to image analysis and automated cartography. </title> <journal> Comm. ACM, </journal> <volume> 24(6) </volume> <pages> 381-395, </pages> <year> 1981. </year>
Reference-contexts: We follow [27] and apply the RANSAC algorithm of Fischler & Bolles <ref> [7] </ref> to compute a large subset of feature matches consistent with a single set of 2D transformation parameters. Details of the matching algorithm may be found in [13].
Reference: [8] <author> C. Harris. </author> <title> Tracking with rigid models. </title> <editor> In A. Blake and A. Yuille, editors, </editor> <title> Active Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: Model-based trackers locate known features in the scene and use them to update the pose of the object. Fixed geometric models <ref> [8] </ref> and deformable "snakes" [2] have been tried. 1 2. "Blob" trackers which define an object of interest as one which may be seg-mented from the rest of the scene using a "figure/ground" type of algorithm, such as detection of motion against a stationary background.
Reference: [9] <author> H. Kikuchi, M. Ishiyama, and T Nakajima. </author> <title> Development of laser radar for radar brake system. </title> <booktitle> In Proc. AVEC'94, </booktitle> <pages> pages 385-389, </pages> <year> 1994. </year>
Reference-contexts: Steering control will be driven by a combination of lane tracking and magnetometers detecting magnetic "nails" in the road [25]. In our system vision can potentially provide higher bandwidth (30/60Hz) output than is available from the laser radar system <ref> [9] </ref>. In this paper we compare the outputs of the two sensors, and consider some fundamental questions concerning the use of vision in this context: 1. <p> The NTSC video from the cameras was recorded on Hi-8 Camcorders (Sony CCD-TRV81), and digitized. The range data from Honda's laser radar system, described in <ref> [9] </ref>, was logged on a PC. The main problem was synchronising the video and range data after it had been recorded. This we achieved using time code generation equipment (Horita TRG-50PC, WG-50 and VG-50).
Reference: [10] <author> D. Koller, J. Weber, and J. Malik. </author> <title> Robust multiple car tracking with occlusion reasoning. </title> <booktitle> In Proc. 3rd European Conf. on Computer Vision, Stockholm, </booktitle> <volume> volume 1, </volume> <pages> pages 189-196, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: This approach is very common <ref> [11, 10, 14] </ref>. 3. Direct image velocity measurement trackers that compute velocity using image flow, correlation or similar methods [4, 19, 3]. Here we assume the usual correlation scheme whereby the correlation template is updated over time. <p> Redundancy is a vital issue here, because vision is a massively redundant sensor, and approaches which negate this aspect are likely to be discarded in the long term. Previous experience of vehicle tracking without explicit models <ref> [1, 10] </ref> leads us to believe that this approach is valid. Because we reconstruct the geometry of the lead vehicle, the algorithms generalize naturally to different vehicle types.
Reference: [11] <author> J.M Letang, V. Rebuffel, and P. Bouthemy. </author> <title> Motion detection robust to perturbations: a statistical regularization and temporal integration framework. </title> <booktitle> In Proc. 4th Int'l Conf. on Computer Vision, </booktitle> <address> Berlin, </address> <pages> pages 21-30, </pages> <address> Los Alamitos, CA, 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: This approach is very common <ref> [11, 10, 14] </ref>. 3. Direct image velocity measurement trackers that compute velocity using image flow, correlation or similar methods [4, 19, 3]. Here we assume the usual correlation scheme whereby the correlation template is updated over time.
Reference: [12] <author> Q.-T. Luong and O.D. Faugeras. </author> <title> A stability analysis of the fundamental matrix. </title> <booktitle> In Proc. 3rd European Conf. on Computer Vision, Stockholm, </booktitle> <pages> pages 577-588, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: In comparison with the previous system [6] we have improved the robustness of the feature tracker to the extent that outliers are much less likely to occur, because the set of feature matches is forced to be globally consistent <ref> [27, 12] </ref>. By integrating the VSDF reconstruction algorithm we now have a stable method of transferring the fixation point over a large sequence of images, and the fixation point transfer mechanism is greatly simplified. <p> The alternative that has recently been proposed is to select a set of feature matches that are globally consistent, in the sense of satisfying the rigidity constraint. Matching of point features between a stereopair of images while enforcing rigidity in general involves computing the fundamental matrix <ref> [27, 12] </ref>. In the case of an affine camera and 2D shape, we can instead compute the 2D affine transformation between the two images. can be written as: z l = M lr z r + t lr (2) where M lr and t lr define the transformation.
Reference: [13] <author> P. F. McLauchlan and J. Malik. </author> <title> Vision for longitudinal vehicle control. Submitted to Intelligent Vehicles Conference, </title> <address> Boston, </address> <year> 1997. </year>
Reference-contexts: We thus employ the Tomasi/Kanade algorithm on the first stere-opair of our image sequence, and thereafter use the simplest version of the VSDF as presented in [16]. Details may be found in <ref> [13] </ref>. 5 Affine transfer The theory of affine transfer presented in [20, 21, 6] is a method of choosing a fixation point on a tracked object, so that image-plane errors of the fixation point from the centre of the image (s) may be fed back to the motors controlling the camera <p> We follow [27] and apply the RANSAC algorithm of Fischler & Bolles [7] to compute a large subset of feature matches consistent with a single set of 2D transformation parameters. Details of the matching algorithm may be found in <ref> [13] </ref>. Temporal matching proceeds in the same way as stereo matching, except that we match the previous structure estimates X i to the new image, rather than the previous feature set.
Reference: [14] <author> P. F. McLauchlan, I. D. Reid, and D. W. Murray. </author> <title> Coarse motion for saccade control. </title> <editor> In D. Hogg and R. Boyle, editors, </editor> <booktitle> Proc. 3rd British Machine Vision Conf., Leeds, </booktitle> <pages> pages 357-366. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1992. </year> <month> 10 </month>
Reference-contexts: This approach is very common <ref> [11, 10, 14] </ref>. 3. Direct image velocity measurement trackers that compute velocity using image flow, correlation or similar methods [4, 19, 3]. Here we assume the usual correlation scheme whereby the correlation template is updated over time.
Reference: [15] <author> P. F. McLauchlan, I. D. Reid, and D. W. Murray. </author> <title> Recursive affine structure and motion from image sequences. </title> <booktitle> In Proc. 3rd European Conf. on Computer Vision, Stockholm, </booktitle> <volume> volume 1, </volume> <pages> pages 217-224, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: effective range measurements? Our work draws together recent developments in algorithms for visual tracking, specifically: * The fixation point transfer algorithm of Reid & Murray [21]. * A simplified version of the robust feature matching method of Torr et al. [27]. * The recursive VSDF 3D reconstruction algorithm described in <ref> [15, 16] </ref>. Our approach is to combine shape reconstruction (2D planar reconstruction rather than the usual 3D) from stereo/motion with motion estimation, using recently developed robust and efficient feature matching methods. <p> As we have previously demonstrated <ref> [15, 16] </ref>, the variable state dimension filter algorithm achieves virtually the same accuracy, but has the advantages of being recursive, not requiring complete data, and allowing new features to be added to the reconstruction as they appear and discarded features to be removed.
Reference: [16] <author> P.F. McLauchlan and D.W. Murray. </author> <title> A unifying framework for structure and motion recovery from image sequences. </title> <booktitle> In Proc. 5th Int'l Conf. on Computer Vision, </booktitle> <address> Boston, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: effective range measurements? Our work draws together recent developments in algorithms for visual tracking, specifically: * The fixation point transfer algorithm of Reid & Murray [21]. * A simplified version of the robust feature matching method of Torr et al. [27]. * The recursive VSDF 3D reconstruction algorithm described in <ref> [15, 16] </ref>. Our approach is to combine shape reconstruction (2D planar reconstruction rather than the usual 3D) from stereo/motion with motion estimation, using recently developed robust and efficient feature matching methods. <p> Feature tracking, which track point and line features (typically) over an image sequence [29, 22, 24], sometimes in conjunction with rigidity constraints to impose global consistency <ref> [18, 16] </ref>. While the context of highways and vehicles is clearly very structured, we avoid using direct scene models in the low-level tracking algorithms, and this distinguishes our work from that of Dickmanns's group [5], for instance. Thus we have rejected model-based trackers (class 1). <p> We improve on the algorithm of Reid & Murray in that in their version of the algorithm, the 3D structure computed at each time step is discarded. We have argued in <ref> [16] </ref> that maintaining scene structure expicitly within a reconstruction algorithm stabilises the computation of motion over time, so a logical extension of the fixation point transfer algorithm is to recursively update the structure of the tracked object, and employ the improved motion estimates to perform fixation point transfer. <p> This is the method we have implemented. The reconstruction technique detailed below may be seen as a version of the Variable State Dimension Filter (VSDF) algorithm <ref> [16] </ref>, specialized to 2D affine scene reconstruction. The VSDF is a general algorithm for visual reconstruction that deals naturally with fragmentary data and combines data from multiple images in a statistically near-optimal manner. <p> As we have previously demonstrated <ref> [15, 16] </ref>, the variable state dimension filter algorithm achieves virtually the same accuracy, but has the advantages of being recursive, not requiring complete data, and allowing new features to be added to the reconstruction as they appear and discarded features to be removed. <p> We thus employ the Tomasi/Kanade algorithm on the first stere-opair of our image sequence, and thereafter use the simplest version of the VSDF as presented in <ref> [16] </ref>.
Reference: [17] <author> J. L. Mundy and A. P. Zisserman, </author> <title> editors. Geometric Invariance in Computer Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: We shall assume that the viewed vehicle is a planar object projected affinely (the most general type of parallel projection <ref> [17] </ref>) onto the image plane. We shall not assume any knowledge of the camera calibration for the purpose of tracking the vehicle. Thus we have possibly the simplest available imaging model. However we believe that this model is adequate in the scenario we are considering.
Reference: [18] <author> P.A.Beardsley, A.Zisserman, and D.W.Murray. </author> <title> Sequential updating of projective and affine structure from motion. </title> <journal> International Journal of Computer Vision, </journal> <volume> 23(3), </volume> <year> 1997. </year>
Reference-contexts: Feature tracking, which track point and line features (typically) over an image sequence [29, 22, 24], sometimes in conjunction with rigidity constraints to impose global consistency <ref> [18, 16] </ref>. While the context of highways and vehicles is clearly very structured, we avoid using direct scene models in the low-level tracking algorithms, and this distinguishes our work from that of Dickmanns's group [5], for instance. Thus we have rejected model-based trackers (class 1).
Reference: [19] <author> K. Pahlavan, T. Uhlin, and J.-O. Eklundh. </author> <title> Dynamic fixation. </title> <booktitle> In Proc. 4th Int'l Conf. on Computer Vision, </booktitle> <address> Berlin, </address> <pages> pages 412-419, </pages> <address> Los Alamitos, CA, 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: This approach is very common [11, 10, 14]. 3. Direct image velocity measurement trackers that compute velocity using image flow, correlation or similar methods <ref> [4, 19, 3] </ref>. Here we assume the usual correlation scheme whereby the correlation template is updated over time. The template may be kept fixed, in which case the correlation tracker returns position feedback rather than velocity feedback. <p> In our highway scenario we can expect the apparent motion of the background to be large, so we cannot use methods in class 2. Velocity-based trackers (class 3) have had some success in motion tracking <ref> [19, 3] </ref>, but they have a basic problem that restricts their usefulness. In order to track an object over an extended time, it is necessary to compute the position of the object, which in this context may only be computed by integrating the velocity over time.
Reference: [20] <author> I. D. Reid and D. W. Murray. </author> <title> Tracking foveated corner clusters using affine structure. </title> <booktitle> In Proc. 4th Int'l Conf. on Computer Vision, </booktitle> <address> Berlin, </address> <pages> pages 76-83, </pages> <address> Los Alamitos, CA, 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: We thus employ the Tomasi/Kanade algorithm on the first stere-opair of our image sequence, and thereafter use the simplest version of the VSDF as presented in [16]. Details may be found in [13]. 5 Affine transfer The theory of affine transfer presented in <ref> [20, 21, 6] </ref> is a method of choosing a fixation point on a tracked object, so that image-plane errors of the fixation point from the centre of the image (s) may be fed back to the motors controlling the camera (s).
Reference: [21] <author> I. D. Reid and D. W. Murray. </author> <title> Active tracking of foveated feature clusters using affine structure. </title> <journal> International Journal of Computer Vision, </journal> <volume> 18(1) </volume> <pages> 1-20, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: How can we combine motion and stereo cues to get the most effective range measurements? Our work draws together recent developments in algorithms for visual tracking, specifically: * The fixation point transfer algorithm of Reid & Murray <ref> [21] </ref>. * A simplified version of the robust feature matching method of Torr et al. [27]. * The recursive VSDF 3D reconstruction algorithm described in [15, 16]. <p> We now turn to those problems. 3 Fixation and Scene Reconstruction In <ref> [21] </ref> a fixation technique was described that allows a single "fixation" point to be chosen from a cluster of tracked features, in a way that is robust to losing track of individual features, while allowing the same object point to be fixated over time. <p> The fixation point was used to drive the (motorized) camera to maintain the view direction at a desired location on the object. The other important idea in <ref> [21] </ref> is that reconstructing the shape of a tracked object can be built into the fixation process in a cooperative way, rather than being a separate, higher-level process. <p> We thus employ the Tomasi/Kanade algorithm on the first stere-opair of our image sequence, and thereafter use the simplest version of the VSDF as presented in [16]. Details may be found in [13]. 5 Affine transfer The theory of affine transfer presented in <ref> [20, 21, 6] </ref> is a method of choosing a fixation point on a tracked object, so that image-plane errors of the fixation point from the centre of the image (s) may be fed back to the motors controlling the camera (s).
Reference: [22] <author> L.S. Shapiro, A. Zisserman, and J.M. Brady. </author> <title> 3D motion recovery via affine epipolar geometry. </title> <journal> International Journal of Computer Vision, </journal> <volume> 16 </volume> <pages> 147-182, </pages> <year> 1995. </year>
Reference-contexts: This can be remedied by correlating only small image regions and allowing image deformations, but then the algorithm takes on the character of a feature tracker [23]. 4. Feature tracking, which track point and line features (typically) over an image sequence <ref> [29, 22, 24] </ref>, sometimes in conjunction with rigidity constraints to impose global consistency [18, 16]. While the context of highways and vehicles is clearly very structured, we avoid using direct scene models in the low-level tracking algorithms, and this distinguishes our work from that of Dickmanns's group [5], for instance.
Reference: [23] <author> J. Shi and C. Tomasi. </author> <title> Good features to track. </title> <booktitle> In Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 593-600, </pages> <year> 1994. </year>
Reference-contexts: In this case however a correlation algorithm will become very sensitive to slight change of viewpoint. This can be remedied by correlating only small image regions and allowing image deformations, but then the algorithm takes on the character of a feature tracker <ref> [23] </ref>. 4. Feature tracking, which track point and line features (typically) over an image sequence [29, 22, 24], sometimes in conjunction with rigidity constraints to impose global consistency [18, 16].
Reference: [24] <author> S. M. Smith. ASSET-2: </author> <title> Real-Time Motion Segmentation and Shape Tracking. </title> <booktitle> In Proc. 5th Int'l Conf. on Computer Vision, Boston, </booktitle> <pages> pages 237-244, </pages> <year> 1995. </year>
Reference-contexts: This can be remedied by correlating only small image regions and allowing image deformations, but then the algorithm takes on the character of a feature tracker [23]. 4. Feature tracking, which track point and line features (typically) over an image sequence <ref> [29, 22, 24] </ref>, sometimes in conjunction with rigidity constraints to impose global consistency [18, 16]. While the context of highways and vehicles is clearly very structured, we avoid using direct scene models in the low-level tracking algorithms, and this distinguishes our work from that of Dickmanns's group [5], for instance.
Reference: [25] <author> C.J. Taylor, J. Malik, and J. Weber. </author> <title> A real-time approach to stereopsis and lane-finding. </title> <booktitle> In Proc. IEEE Intelligent Vehicles Symposium, </booktitle> <year> 1996. </year>
Reference-contexts: Steering control will be driven by a combination of lane tracking and magnetometers detecting magnetic "nails" in the road <ref> [25] </ref>. In our system vision can potentially provide higher bandwidth (30/60Hz) output than is available from the laser radar system [9]. In this paper we compare the outputs of the two sensors, and consider some fundamental questions concerning the use of vision in this context: 1.
Reference: [26] <author> C. Tomasi and T. Kanade. </author> <title> Shape and motion from image streams under orthography: A factorization approach. </title> <journal> International Journal of Computer Vision, </journal> <volume> 9(2) </volume> <pages> 137-154, </pages> <year> 1992. </year>
Reference-contexts: Thus we have rejected model-based trackers (class 1). We draw on the large amount of work on scene reconstruction from multiple images in unstructured scenes, in particular the work on robust motion segmentation [28], and affine reconstruction <ref> [26] </ref>. These approaches are able to take advantage of the redundant information in images, because they latch onto whatever features are available, whereas model-based methods are restricted to the features associated with the chosen model. <p> The motion and shape of the object was recovered up to a 2D/3D affine transformation from each set of two/three consecutive images, using the measurement matrix factorization algorithm described in <ref> [26] </ref>. The motion parameters were then used to transfer a chosen fixation point from the previous image (s) to the latest. This algorithm was extended to stereo cameras in [6]. While we have a simpler problem, with a fixed camera and a 2D scene representation, similar principles apply. <p> Stereo 2D affine projection is illustrated in figure 1. Given complete data for n features over k images, Tomasi & Kanade's algorithm <ref> [26] </ref> achieves the optimal solution for a 3D affine reconstruction. It also generalizes simply to the 2D problem by taking the biggest two singular vectors instead of the biggest three.
Reference: [27] <author> P.H.S. Torr, P.A. Beardsley, and D.W. Murray. </author> <title> Robust vision. </title> <editor> In E. Hancock, editor, </editor> <booktitle> Proc. 5th British Machine Vision Conf., </booktitle> <address> York, </address> <pages> pages 145-154. </pages> <publisher> BMVA Press, </publisher> <year> 1994. </year>
Reference-contexts: we combine motion and stereo cues to get the most effective range measurements? Our work draws together recent developments in algorithms for visual tracking, specifically: * The fixation point transfer algorithm of Reid & Murray [21]. * A simplified version of the robust feature matching method of Torr et al. <ref> [27] </ref>. * The recursive VSDF 3D reconstruction algorithm described in [15, 16]. Our approach is to combine shape reconstruction (2D planar reconstruction rather than the usual 3D) from stereo/motion with motion estimation, using recently developed robust and efficient feature matching methods. <p> In comparison with the previous system [6] we have improved the robustness of the feature tracker to the extent that outliers are much less likely to occur, because the set of feature matches is forced to be globally consistent <ref> [27, 12] </ref>. By integrating the VSDF reconstruction algorithm we now have a stable method of transferring the fixation point over a large sequence of images, and the fixation point transfer mechanism is greatly simplified. <p> The alternative that has recently been proposed is to select a set of feature matches that are globally consistent, in the sense of satisfying the rigidity constraint. Matching of point features between a stereopair of images while enforcing rigidity in general involves computing the fundamental matrix <ref> [27, 12] </ref>. In the case of an affine camera and 2D shape, we can instead compute the 2D affine transformation between the two images. can be written as: z l = M lr z r + t lr (2) where M lr and t lr define the transformation. <p> In the case of an affine camera and 2D shape, we can instead compute the 2D affine transformation between the two images. can be written as: z l = M lr z r + t lr (2) where M lr and t lr define the transformation. We follow <ref> [27] </ref> and apply the RANSAC algorithm of Fischler & Bolles [7] to compute a large subset of feature matches consistent with a single set of 2D transformation parameters. Details of the matching algorithm may be found in [13].
Reference: [28] <author> P.H.S. Torr, A. Zisserman, and S.J. Maybank. </author> <title> Robust detection of degenerate configurations for the fundamental matrix. </title> <booktitle> In Proc. 5th Int'l Conf. on Computer Vision, Boston, </booktitle> <pages> pages 1037-1042, </pages> <year> 1995. </year>
Reference-contexts: Thus we have rejected model-based trackers (class 1). We draw on the large amount of work on scene reconstruction from multiple images in unstructured scenes, in particular the work on robust motion segmentation <ref> [28] </ref>, and affine reconstruction [26]. These approaches are able to take advantage of the redundant information in images, because they latch onto whatever features are available, whereas model-based methods are restricted to the features associated with the chosen model.
Reference: [29] <author> Z. Zhang and O. Faugeras. </author> <title> 3D Dynamic Scene Analysis. </title> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <month> 11 </month>
Reference-contexts: This can be remedied by correlating only small image regions and allowing image deformations, but then the algorithm takes on the character of a feature tracker [23]. 4. Feature tracking, which track point and line features (typically) over an image sequence <ref> [29, 22, 24] </ref>, sometimes in conjunction with rigidity constraints to impose global consistency [18, 16]. While the context of highways and vehicles is clearly very structured, we avoid using direct scene models in the low-level tracking algorithms, and this distinguishes our work from that of Dickmanns's group [5], for instance.
References-found: 29

