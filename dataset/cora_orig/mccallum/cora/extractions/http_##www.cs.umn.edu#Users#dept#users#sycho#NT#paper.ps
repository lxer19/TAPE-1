URL: http://www.cs.umn.edu/Users/dept/users/sycho/NT/paper.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/sycho/NT/
Root-URL: http://www.cs.umn.edu
Email: E-mail: sycho@cs.umn.edu  
Title: Decoupling Local Variable Accesses in a Wide-Issue Superscalar Processor  
Author: Sangyeun Cho, Pen-Chung Yew, and Gyungho Lee 
Keyword: cache bandwidth, instruction level parallelism, run-time stack, data stream partitioning  
Address: Minneapolis, MN 55455 San Antonio, TX 78249  
Affiliation: Dept. of Comp. Sci. and Eng. Division of Engineering University of Minnesota University of Texas at San Antonio  
Abstract: Providing adequate data bandwidth is extremely important for a wide-issue superscalar processor to achieve its full performance potential. Adding a large number of ports to a data cache, however, becomes increasingly inefficient, and can add to the hardware complexity significantly. This paper takes an alternative or complementary approach for providing more data bandwidth, called the data-decoupled architecture. The approach, with help from a compiler or support from hardware, partitions the memory stream into multiple independent streams early in the processor pipeline, and feeds each stream to a separate memory access queue and cache. Under this model, the paper studies decoupling memory accesses to program's local variables that are allocated on the run-time stack. Using a set of integer programs from the SPEC 95 benchmark suite, it is shown that local variable accesses constitute a large portion of all the memory references, while their reference space typically is very small, averaging around 7 words per function. To service local variable accesses quickly, two optimizations, fast data forwarding and access combining are proposed and studied. Some of the important design parameters, such as the cache size, the number of cache ports, and the degree of access combining, are studied based on simulations. The potential performance of the proposed scheme is measured using various configurations, and it is concluded that the scheme is a viable alternative to building a single multi-ported data cache. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Bergner, P. Dahl, D. Engebretsen, and M. O'Keefe. </author> <title> Spill Code Minimization via Interference Region Spilling, </title> <booktitle> Proc. of the 1997 ACM SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. 287 295. </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: For example, spill codes can produce a significant number of memory references at run time, as many as 20% of all the executed instructions, when 16 registers are available to a register allocator that uses a graph-coloring algorithm <ref> [1] </ref>. <p> Chow and Hennessy [6] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics <ref> [4, 2, 6, 1] </ref> try to efficiently assign a set of hard registers to the live ranges. Increasing the number of registers or using a sophisticated register allocation scheme will cut down the number of memory references in the first category above.
Reference: [2] <author> P. Briggs, K. D. Cooper, K. Kennedy, and L. Torczon. </author> <title> Coloring Heuristics for Register Allocation, </title> <booktitle> Proc. of the 1989 ACM SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. 275 284. </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Chow and Hennessy [6] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics <ref> [4, 2, 6, 1] </ref> try to efficiently assign a set of hard registers to the live ranges. Increasing the number of registers or using a sophisticated register allocation scheme will cut down the number of memory references in the first category above.
Reference: [3] <author> D. Burger and T. M. Austin. </author> <title> The SimpleScalar Tool Set, </title> <note> Version 2.0, Computer Sciences Department Technical Report, No. 1342, </note> <institution> Univ. of Wisconsin, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: Loops which previously cannot benefit from the unrolling optimization or software pipelining due to high register pressure, can use such program optimizations based on a new cost model. 3 Experimental Setup 3.1 Simulator and machine model We use an extended version of the sim-outorder simulator in the SimpleScalar toolset <ref> [3] </ref>. The machine model used in the experiments is a superscalar processor that supports out-of-order issue and execution, based on the Register Update Unit (RUU) [22]. The RUU scheme uses a reorder buffer (ROB) to automatically perform register renaming and hold the results of pending instructions.
Reference: [4] <author> G. J. Chaitin. </author> <title> Register Allocation and Spilling via Graph Coloring, </title> <booktitle> Proc. of the 1982 ACM SIGPLAN Symp. on Compiler Construction, </booktitle> <pages> pp. 98 105, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: Chow and Hennessy [6] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics <ref> [4, 2, 6, 1] </ref> try to efficiently assign a set of hard registers to the live ranges. Increasing the number of registers or using a sophisticated register allocation scheme will cut down the number of memory references in the first category above.
Reference: [5] <author> S. Cho, J.-Y. Tsai, Y. Song, B. Zheng, S. J. Schwinn, X. Wang, Q. Zhao, Z. Li, D. J. Lilja, and P.-C. Yew. </author> <title> High-Level Information An Approach for Integrating Front-End and Back-End Compilers, </title> <booktitle> Proc. of the 1998 Int'l Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1998, </year> <note> to appear. </note>
Reference-contexts: For example, data-independent memory streams in a program region, such as two disjoint arrays can be decoupled. How to exploit this type of decoupling efficiently is not trivial. Annotating independent memory access instructions with high-level compiler analysis information <ref> [5] </ref> may facilitate fast non-speculative dynamic memory disambiguation in a wide-issue processor [15]. Compiler and architectural considerations for efficient memory handling remain as a very important part of designing a balanced, cost-effective processor.
Reference: [6] <author> F. C. Chow and J. L. Hennessy. </author> <title> The Priority-Based Coloring Approach to Register Allocation, </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 12:4, </volume> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: Among current microprocessors, Sun UltraSparc employs a special register file structure called register window to reduce the cost of a procedure call/return [27]. Chow and Hennessy <ref> [6] </ref> categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics [4, 2, 6, 1] try to efficiently assign a set of hard registers to the live ranges. <p> Chow and Hennessy [6] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics <ref> [4, 2, 6, 1] </ref> try to efficiently assign a set of hard registers to the live ranges. Increasing the number of registers or using a sophisticated register allocation scheme will cut down the number of memory references in the first category above.
Reference: [7] <author> D. Ditzel and R. McLellan. </author> <title> Register Allocation for Free: The C Machine Stack Cache, </title> <booktitle> Proc. of the Symp. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 48 56, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: To service the local variable accesses quickly, we study two optimizations fast data forwarding and access combining. Although there have been efforts to optimize the local variable accesses with hardware support in the past <ref> [7, 10, 24] </ref>, little work has been done to study their performance impact in the context of a wide-issue superscalar processor with a multi-ported data cache. <p> Note that a locally declared variable whose storage class is static is not local in our definition. 4 99% percentile are shown to ease reading. Ditzel and McLellan <ref> [7] </ref> report similar results. that declares them is called, and automatically deallocated the space when the function exits. Although not visible to a programmer, compilers generate additional local variables for saving/restoring registers, passing arguments, and register spilling [13]. <p> The result suggests that the separate cache for the local variables need not be large to obtain a high hit rate; The cache size to capture ten outstanding function frames is only 70 words on average. In fact, this has been the motivation for some previous work <ref> [7, 10, 24, 27] </ref>. The high frequency of local variable accesses and their spatial locality motivate us to consider decoupling and servicing the local variable accesses separately. <p> Moreover, identifying local variables is relatively easy for a compiler. 2.2.2 Architectural support To facilitate local variable accesses, a specialized hardware organization to simulate the run-time stack may appear attractive <ref> [7, 10, 24] </ref>. However, we use a more general design composed of a cache termed the local variable cache (LVC) within the framework of the data-decoupled architecture. This approach has two advantages; First, the LVC is a conventional cache, and can leverage the most efficient current design. <p> Alternatively, the processor can assume those accesses indexed by $sp (or the frame pointer, $fp) as local variable accesses <ref> [7] </ref>. There are, however, local variable accesses not indexed by $sp (or $fp); For example, when the address of a local variable is taken, either to index through the data structure, or pass it as a parameter to a procedure, $sp is not used. <p> Figure 10 indicates that in an aggressive out-of-order superscalar processor, the performance improvement due to function inlining is limited. 17 5 Related Work The idea of optimizing local variables on run-time stack is not new. Ditzel and McLellan <ref> [7] </ref> studied a transparent data buffer as a close mapping of the run-time stack, called the stack cache. The stack cache is effectively a large register file to simulate the run-time stack that replaces the general register file.
Reference: [8] <author> J. Edmondson et al. </author> <title> Internal Organization of the Alpha 21164, a 300-MHz, 64-Bit, Quad-Issue, CMOS RISC Microprocessor, </title> <journal> Digital Technical Journal, </journal> <volume> Volume 7, Number 1, </volume> <year> 1995. </year>
Reference-contexts: Except for the very expensive ideal multi-porting, these techniques have been incorporated in recent superscalar processors. For example, Digital's 21264 provides a two-ported data cache by running the cache twice as fast as the processor clock [11], Digital's 21164, the predecessor of 21264, uses a replicated data cache <ref> [8] </ref>, and the MIPS R10000 implements a two-way interleaved data cache [31]. Each design, however, is either costly to implement, and/or has significant drawbacks. The time-division multiplexing does not scale beyond a certain number of ports (seemingly two ports).
Reference: [9] <author> J. Emer and D. Clark. </author> <title> A Characterization of Processor Performance in the VAX-11/780, </title> <booktitle> Proc. of the 11th Int'l Symp. on Computer Architecture, </booktitle> <month> June </month> <year> 1984. </year>
Reference-contexts: Although not visible to a programmer, compilers generate additional local variables for saving/restoring registers, passing arguments, and register spilling [13]. Memory accesses to these variables, typically indexed by stack pointer ($sp), can constitute a large fraction of overall memory references <ref> [9] </ref>. For example, spill codes can produce a significant number of memory references at run time, as many as 20% of all the executed instructions, when 16 registers are available to a register allocator that uses a graph-coloring algorithm [1]. <p> These studies aimed primarily to reduce the impact of a procedure call/return on the processor performance, motivated by an observation that programs written in a high-level language tend to have many procedure calls and returns <ref> [9] </ref>, and that a function call is the most costly source language statement [19].
Reference: [10] <author> M. J. Flynn and L. W. Hoevel. </author> <title> Execution Architecture: The DELtran Experiment, </title> <journal> IEEE Trans. on Computers, C-32(2): </journal> <volume> 156 175, </volume> <month> Feb. </month> <year> 1983. </year> <month> 21 </month>
Reference-contexts: To service the local variable accesses quickly, we study two optimizations fast data forwarding and access combining. Although there have been efforts to optimize the local variable accesses with hardware support in the past <ref> [7, 10, 24] </ref>, little work has been done to study their performance impact in the context of a wide-issue superscalar processor with a multi-ported data cache. <p> The result suggests that the separate cache for the local variables need not be large to obtain a high hit rate; The cache size to capture ten outstanding function frames is only 70 words on average. In fact, this has been the motivation for some previous work <ref> [7, 10, 24, 27] </ref>. The high frequency of local variable accesses and their spatial locality motivate us to consider decoupling and servicing the local variable accesses separately. <p> Moreover, identifying local variables is relatively easy for a compiler. 2.2.2 Architectural support To facilitate local variable accesses, a specialized hardware organization to simulate the run-time stack may appear attractive <ref> [7, 10, 24] </ref>. However, we use a more general design composed of a cache termed the local variable cache (LVC) within the framework of the data-decoupled architecture. This approach has two advantages; First, the LVC is a conventional cache, and can leverage the most efficient current design. <p> Ditzel and McLellan [7] studied a transparent data buffer as a close mapping of the run-time stack, called the stack cache. The stack cache is effectively a large register file to simulate the run-time stack that replaces the general register file. The contour buffer proposed by Flynn and Hoevel <ref> [10] </ref> in their Directly Executed Languages model, is a programmer-addressable buffer that is used in conjunction with the run-time stack in memory.
Reference: [11] <author> L. Gwennap. </author> <title> Digital 21264 Sets New Standard, </title> <type> Microprocessor Report, Volume 10, </type> <note> Issue 14, </note> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Except for the very expensive ideal multi-porting, these techniques have been incorporated in recent superscalar processors. For example, Digital's 21264 provides a two-ported data cache by running the cache twice as fast as the processor clock <ref> [11] </ref>, Digital's 21164, the predecessor of 21264, uses a replicated data cache [8], and the MIPS R10000 implements a two-way interleaved data cache [31]. Each design, however, is either costly to implement, and/or has significant drawbacks. <p> The ROB and the LSQ effectively form the instruction window of the processor. The primary on-chip data cache that is 32 KB, 2-way set-associative, and multi-ported, has 2-cycle hit time, as in some of the recent machines <ref> [31, 11] </ref>. The 512 KB secondary data cache, either on-chip or off-chip, has a 12-cycle hit latency. Data caches are lock-up free.
Reference: [12] <author> M. D. Hill. </author> <title> A Case for Direct-Mapped Caches, </title> <journal> IEEE Computer, pp. </journal> <volume> 25 40, </volume> <month> Dec. </month> <year> 1988. </year>
Reference-contexts: We prefer this design to a 4 KB LVC or a set-associative LVC that may achieve even lower miss rates, because a small direct-mapped cache is likely to have an access time advantage when a fast clock is used <ref> [12] </ref>. The additional caching space provided by a 2 KB LVC resulted in slight decrease in traffic to the L2 cache for all the programs except 126.gcc, which experienced a slight increase.
Reference: [13] <author> A. I. Holub. </author> <title> Compiler Design in C, </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: Ditzel and McLellan [7] report similar results. that declares them is called, and automatically deallocated the space when the function exits. Although not visible to a programmer, compilers generate additional local variables for saving/restoring registers, passing arguments, and register spilling <ref> [13] </ref>. Memory accesses to these variables, typically indexed by stack pointer ($sp), can constitute a large fraction of overall memory references [9].
Reference: [14] <author> M. H. Lipasti and J. P. Shen. </author> <title> Superspeculative Microarchitecture for Beyond AD 2000, </title> <journal> IEEE Computer, pp. </journal> <volume> 59 66, </volume> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: For example, for a processor to sustain ten instructions per cycle (IPC), the memory subsystem should provide average bandwidth of four references per cycle, or more, to prevent excessive queuing delays, assuming that about 40% of all instructions are loads and stores <ref> [14] </ref>. A straightforward approach for increasing memory bandwidth is to implement a multi-ported data cache [23]. There are a number of techniques to provide multiple cache ports: ideal multi-porting, time-division multiplexing, replicating the cache, and interleaving [20]. <p> and future work are summarized in Section 6. 2 Data-Decoupled Architecture 2.1 Concept To extract and exploit more parallelism, a future superscalar processor may establish a wide instruction window that consists of a large number of reservation stations, from which instructions are steered to a set of pipelined functional units <ref> [18, 14] </ref>. Building such a processor, unfortunately, poses many 2 and caches. great challenges; Especially, the hardware complexity of the logic that identifies and issues ready instructions from a large pool of reservation stations becomes an increasingly severe impediment to a faster clock rate [17]. <p> The data-decoupled architecture has two fundamental operating issues: memory stream partitioning and load balancing. First, decoded memory access instructions should be partitioned into independent streams before they enter the instruction windows. Either run-time or compile-time information on per-reference access behavior is needed. When a type of run-time speculation <ref> [14, 28, 16] </ref> is used for the classification, verification and recovery actions are required on mispredictions. On the other hand, extracting classification information at compile time can simplify the hardware design, while putting more burden on the compiler. <p> There are dynamic techniques to decouple a portion of data references and service them using sepate, specialized functional units. Lipasti introduced a notion called load stream partitioning in 18 his Superflow processor model <ref> [14] </ref>, which simply partitions loads into multiple streams based on their run-time behavior, and sends them to disjoint functional units for processing. The functional units used include a constant verification unit, a queue for load/store folding, a stream buffer/prefetch engine, and a conventional data cache.
Reference: [15] <author> A. Moshovos, S. E. Breach, T. N. Vijaykumar, and G. S. Sohi. </author> <title> Dynamic Speculation and Synchronization of Data Dependences, </title> <booktitle> Proc. of the 24th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 181 193, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Such reduction in hardware complexity can lead to a shorter clock cycle time [17]. Second, dividing a large data stream into multiple smaller streams opens up more opportunities for optimizing each with specialized techniques. For instance, various speculative techniques on data dependence and forwarding <ref> [15, 16, 28] </ref> can be tailored to each stream for higher efficiency. This paper investigates the potential of decoupling local variable accesses in the data-decoupled architecture framework, using a small cache called the local variable cache (LVC) and an instruction queue called the local variable access queue (LVAQ). <p> How to exploit this type of decoupling efficiently is not trivial. Annotating independent memory access instructions with high-level compiler analysis information [5] may facilitate fast non-speculative dynamic memory disambiguation in a wide-issue processor <ref> [15] </ref>. Compiler and architectural considerations for efficient memory handling remain as a very important part of designing a balanced, cost-effective processor. We expect that the decouple-and-conquer approach to the memory bandwidth and/or latency problem, as proposed in this paper, will be of greater significance as more aggressive wide-issue processors emerge.
Reference: [16] <author> A. Moshovos and G. S. Sohi. </author> <title> Streamlining Inter-operation Memory Communication via Data Dependence Prediction, </title> <booktitle> Proc. of the 30th Annual Int'l Symp. on Microarchitecture, </booktitle> <pages> pp. 235 245, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: Such reduction in hardware complexity can lead to a shorter clock cycle time [17]. Second, dividing a large data stream into multiple smaller streams opens up more opportunities for optimizing each with specialized techniques. For instance, various speculative techniques on data dependence and forwarding <ref> [15, 16, 28] </ref> can be tailored to each stream for higher efficiency. This paper investigates the potential of decoupling local variable accesses in the data-decoupled architecture framework, using a small cache called the local variable cache (LVC) and an instruction queue called the local variable access queue (LVAQ). <p> The data-decoupled architecture has two fundamental operating issues: memory stream partitioning and load balancing. First, decoded memory access instructions should be partitioned into independent streams before they enter the instruction windows. Either run-time or compile-time information on per-reference access behavior is needed. When a type of run-time speculation <ref> [14, 28, 16] </ref> is used for the classification, verification and recovery actions are required on mispredictions. On the other hand, extracting classification information at compile time can simplify the hardware design, while putting more burden on the compiler. <p> The functional units used include a constant verification unit, a queue for load/store folding, a stream buffer/prefetch engine, and a conventional data cache. Techniques to detect dependent memory access instructions and explicitly synchronize and forward data between them have been proposed <ref> [28, 16] </ref>. They provide a dynamic technique to detect a producer operation and a consumer operation within the instruction window, and try to forward the data in a special buffer before the effective addresses are calculated, without accessing the cache memory.
Reference: [17] <author> S. Parlacharla, N. P. Jouppi, and J. E. Smith. </author> <title> Complexity-Effective Superscalar Processors, </title> <booktitle> Proc. of the 24th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 206 218, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: More importantly, the network and the control logic for orchestrating memory accesses between a large number of reservation stations and cache ports become simpler. Such reduction in hardware complexity can lead to a shorter clock cycle time <ref> [17] </ref>. Second, dividing a large data stream into multiple smaller streams opens up more opportunities for optimizing each with specialized techniques. For instance, various speculative techniques on data dependence and forwarding [15, 16, 28] can be tailored to each stream for higher efficiency. <p> Building such a processor, unfortunately, poses many 2 and caches. great challenges; Especially, the hardware complexity of the logic that identifies and issues ready instructions from a large pool of reservation stations becomes an increasingly severe impediment to a faster clock rate <ref> [17] </ref>. The situation is exacerbated when there are multiple functional units of the same type, such as identical integer ALUs, because extra cycles may be needed for arbitration. <p> However, since it is increasingly difficult to add a port to a cache (beyond two ports) and build a wide instruction window without victimizing clock cycle time, achieving a comparable performance with simpler hardware is a valid goal <ref> [17] </ref>.
Reference: [18] <author> Y. N. Patt, S. J. Patel, D. H. Friendly, and J. Stark. </author> <title> One Billion Transistors, One Uniprocessor, One Chip, </title> <journal> IEEE Computer, pp. </journal> <volume> 51 57, </volume> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: and future work are summarized in Section 6. 2 Data-Decoupled Architecture 2.1 Concept To extract and exploit more parallelism, a future superscalar processor may establish a wide instruction window that consists of a large number of reservation stations, from which instructions are steered to a set of pipelined functional units <ref> [18, 14] </ref>. Building such a processor, unfortunately, poses many 2 and caches. great challenges; Especially, the hardware complexity of the logic that identifies and issues ready instructions from a large pool of reservation stations becomes an increasingly severe impediment to a faster clock rate [17].
Reference: [19] <author> D. A. Patterson and C. H. Sequin. </author> <title> A VLSI RISC, </title> <journal> IEEE Computer, pp. </journal> <volume> 8 21, </volume> <month> Sept. </month> <year> 1982. </year>
Reference-contexts: These studies aimed primarily to reduce the impact of a procedure call/return on the processor performance, motivated by an observation that programs written in a high-level language tend to have many procedure calls and returns [9], and that a function call is the most costly source language statement <ref> [19] </ref>. Unlike the previous approaches, the technique proposed in this paper does not require processor intervention or complex algorithms to manage the buffer, which were mandated in the previous techniques to deal with buffer overflow/underflow and context switches.
Reference: [20] <author> J. A. Rivers, G. S. Tyson, E. S. Davidson, and T. M. Austin. </author> <title> On High-Bandwidth Data Cache Design for Multi-Issue Processors, </title> <booktitle> Proc. of the 30th Annual Int'l Symp. on Microarchitecture, </booktitle> <pages> pp. 46 56, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: A straightforward approach for increasing memory bandwidth is to implement a multi-ported data cache [23]. There are a number of techniques to provide multiple cache ports: ideal multi-porting, time-division multiplexing, replicating the cache, and interleaving <ref> [20] </ref>. Except for the very expensive ideal multi-porting, these techniques have been incorporated in recent superscalar processors. <p> Designing an effective multi-ported cache has been a continuing topic of active research <ref> [23, 29, 30, 20] </ref>. These studies have focused on increasing the efficiency of cache ports by adding a small buffer, or understanding tradeoffs of various strategies in terms of the cost and performance under specific processor models. The data-decoupled architecture is orthogonal to the data cache design techniques.
Reference: [21] <author> R. W. Scheifler. </author> <title> An Analysis of Inline Substitution for a Structured Programming Language, </title> <journal> Communications of the ACM, </journal> <volume> Volume 20, Number 9, </volume> <month> Sept. </month> <year> 1977. </year>
Reference-contexts: Taking into account the hardware complexity of the access combining, the two-way combining is a reasonable choice for use. 4.3 Impact of function inlining Function inlining <ref> [21] </ref> replaces a function call with the function body, removing the function call/return costs and providing enlarged and specialized functions to the code optimizers. This subsection studies 15 the impact of the function inlining on the proposed technique. Figure 10 presents the results.
Reference: [22] <author> G. S. Sohi. </author> <title> Instruction Issue Logic for High-Performance, Interruptible, Multiple Functional Unit, Pipelined Computers, </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39(3):349 359, </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: The machine model used in the experiments is a superscalar processor that supports out-of-order issue and execution, based on the Register Update Unit (RUU) <ref> [22] </ref>. The RUU scheme uses a reorder buffer (ROB) to automatically perform register renaming and hold the results of pending instructions. In each cycle, the ROB retires completed instructions in program order to the architected register file. The processor's memory system employs a load/store queue (LSQ).
Reference: [23] <author> G. S. Sohi and M. Franklin. </author> <title> High-Bandwidth Data Memory Systems for Superscalar Processors, </title> <booktitle> Proc. of the Fourth Int'l Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 53 62, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Furthermore, the ability to provide the execution core with adequate (cache) memory bandwidth becomes extremely critical for the next generations of wide-issue processors <ref> [23, 29] </ref>. For example, for a processor to sustain ten instructions per cycle (IPC), the memory subsystem should provide average bandwidth of four references per cycle, or more, to prevent excessive queuing delays, assuming that about 40% of all instructions are loads and stores [14]. <p> A straightforward approach for increasing memory bandwidth is to implement a multi-ported data cache <ref> [23] </ref>. There are a number of techniques to provide multiple cache ports: ideal multi-porting, time-division multiplexing, replicating the cache, and interleaving [20]. Except for the very expensive ideal multi-porting, these techniques have been incorporated in recent superscalar processors. <p> Designing an effective multi-ported cache has been a continuing topic of active research <ref> [23, 29, 30, 20] </ref>. These studies have focused on increasing the efficiency of cache ports by adding a small buffer, or understanding tradeoffs of various strategies in terms of the cost and performance under specific processor models. The data-decoupled architecture is orthogonal to the data cache design techniques.
Reference: [24] <author> T. J. Stanley and R. G. Wedig. </author> <title> A Performance Analysis of Automatically Managed Top of Stack Buffers, </title> <booktitle> Proc. of the 14th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 272 281, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: To service the local variable accesses quickly, we study two optimizations fast data forwarding and access combining. Although there have been efforts to optimize the local variable accesses with hardware support in the past <ref> [7, 10, 24] </ref>, little work has been done to study their performance impact in the context of a wide-issue superscalar processor with a multi-ported data cache. <p> The result suggests that the separate cache for the local variables need not be large to obtain a high hit rate; The cache size to capture ten outstanding function frames is only 70 words on average. In fact, this has been the motivation for some previous work <ref> [7, 10, 24, 27] </ref>. The high frequency of local variable accesses and their spatial locality motivate us to consider decoupling and servicing the local variable accesses separately. <p> Moreover, identifying local variables is relatively easy for a compiler. 2.2.2 Architectural support To facilitate local variable accesses, a specialized hardware organization to simulate the run-time stack may appear attractive <ref> [7, 10, 24] </ref>. However, we use a more general design composed of a cache termed the local variable cache (LVC) within the framework of the data-decoupled architecture. This approach has two advantages; First, the LVC is a conventional cache, and can leverage the most efficient current design. <p> The contour buffer proposed by Flynn and Hoevel [10] in their Directly Executed Languages model, is a programmer-addressable buffer that is used in conjunction with the run-time stack in memory. Stanley and Wedig <ref> [24] </ref> proposed three buffer management algorithms for a Top of Stack (TOS) buffer, which is a register file designed to cache the top elements of the stack.
Reference: [25] <institution> The Standard Performance Evaluation Corporation, </institution> <note> http://www.specbench.org. </note>
Reference-contexts: attempts are restricted by the current technological trends; Aggressive ILP optimizations often increase register pressure, which could introduce spill codes, and ultra-fast processor clocks and considerations for instruction set architecture (ISA) compatibility, as exemplified by the Intel's x86 architecture, may disallow increasing the size of a register file. integer programs <ref> [25] </ref>. 3 A large fraction of memory references are to local variables, with an average of 30% of loads and 47% of stores in the programs studied. Over 60% of loads and 80% of stores are local variable accesses in 147.vortex. <p> Important parameters of the base machine model are summarized in Table 1. 3.2 Benchmark programs We use eight integer programs from the SPEC 95 benchmark suite <ref> [25] </ref>, whose characteristics are summarized in Table 2.
Reference: [26] <author> Y. Tamir and C. H. Sequin. </author> <title> Strategies for Managing the Register File in RISC, </title> <journal> IEEE Trans. on Computers, C-32(11): </journal> <volume> 977 989, </volume> <month> Nov. </month> <year> 1983. </year>
Reference-contexts: The major reason why a small LVC of 2 KB or 4 KB achieves a high hit rate is that function frames tend to be very small as shown in Figure 3, and most of the program execution has a relative call depth of four or five routines <ref> [26] </ref>. The line size of the LVC had negligible impact on the hit rate when the LVC size is larger than 2 KB. The hit rate of an LVC would be relatively insensitive to the input data, because the function frames are generally determined at compile time.
Reference: [27] <author> M. Tremblay, B. Joy, and K. Shin. </author> <title> A Three Dimensional Register File for Superscalar Processors, </title> <booktitle> Proc. of the 28th Annual Hawaii Int'l Conf. on Systems Sciences, </booktitle> <publisher> IEEE CS Press, </publisher> <year> 1995. </year>
Reference-contexts: The result suggests that the separate cache for the local variables need not be large to obtain a high hit rate; The cache size to capture ten outstanding function frames is only 70 words on average. In fact, this has been the motivation for some previous work <ref> [7, 10, 24, 27] </ref>. The high frequency of local variable accesses and their spatial locality motivate us to consider decoupling and servicing the local variable accesses separately. <p> Among current microprocessors, Sun UltraSparc employs a special register file structure called register window to reduce the cost of a procedure call/return <ref> [27] </ref>. Chow and Hennessy [6] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference.
Reference: [28] <author> G. Tyson and T. M. Austin. </author> <title> Improving the Accuracy and Performance of Memory Communi--cation Through Renaming, </title> <booktitle> Proc. of the 30th Annual Int'l Symp. on Microarchitecture, </booktitle> <pages> pp. 218 227, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: Such reduction in hardware complexity can lead to a shorter clock cycle time [17]. Second, dividing a large data stream into multiple smaller streams opens up more opportunities for optimizing each with specialized techniques. For instance, various speculative techniques on data dependence and forwarding <ref> [15, 16, 28] </ref> can be tailored to each stream for higher efficiency. This paper investigates the potential of decoupling local variable accesses in the data-decoupled architecture framework, using a small cache called the local variable cache (LVC) and an instruction queue called the local variable access queue (LVAQ). <p> The data-decoupled architecture has two fundamental operating issues: memory stream partitioning and load balancing. First, decoded memory access instructions should be partitioned into independent streams before they enter the instruction windows. Either run-time or compile-time information on per-reference access behavior is needed. When a type of run-time speculation <ref> [14, 28, 16] </ref> is used for the classification, verification and recovery actions are required on mispredictions. On the other hand, extracting classification information at compile time can simplify the hardware design, while putting more burden on the compiler. <p> The functional units used include a constant verification unit, a queue for load/store folding, a stream buffer/prefetch engine, and a conventional data cache. Techniques to detect dependent memory access instructions and explicitly synchronize and forward data between them have been proposed <ref> [28, 16] </ref>. They provide a dynamic technique to detect a producer operation and a consumer operation within the instruction window, and try to forward the data in a special buffer before the effective addresses are calculated, without accessing the cache memory.
Reference: [29] <author> K. M. Wilson, K. Olukotun, and M. Rosenblum. </author> <title> Increasing Cache Port Efficiency for Dynamic Superscalar Microprocessors, </title> <booktitle> Proc. of the 23th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 147 157, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Furthermore, the ability to provide the execution core with adequate (cache) memory bandwidth becomes extremely critical for the next generations of wide-issue processors <ref> [23, 29] </ref>. For example, for a processor to sustain ten instructions per cycle (IPC), the memory subsystem should provide average bandwidth of four references per cycle, or more, to prevent excessive queuing delays, assuming that about 40% of all instructions are loads and stores [14]. <p> Designing an effective multi-ported cache has been a continuing topic of active research <ref> [23, 29, 30, 20] </ref>. These studies have focused on increasing the efficiency of cache ports by adding a small buffer, or understanding tradeoffs of various strategies in terms of the cost and performance under specific processor models. The data-decoupled architecture is orthogonal to the data cache design techniques.
Reference: [30] <author> K. M. Wilson and K. Olukotun. </author> <title> Designing High Bandwidth On-Chip Caches, </title> <booktitle> Proc. of the 24th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 121 132, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Designing an effective multi-ported cache has been a continuing topic of active research <ref> [23, 29, 30, 20] </ref>. These studies have focused on increasing the efficiency of cache ports by adding a small buffer, or understanding tradeoffs of various strategies in terms of the cost and performance under specific processor models. The data-decoupled architecture is orthogonal to the data cache design techniques.
Reference: [31] <author> K. C. Yeager. </author> <title> The MIPS R10000 Superscalar Microprocessor, </title> <journal> IEEE Micro, </journal> <volume> Volume 16, Number 2, </volume> <pages> pp. 28 40, </pages> <month> April </month> <year> 1996. </year> <pages> 23 24 </pages>
Reference-contexts: For example, Digital's 21264 provides a two-ported data cache by running the cache twice as fast as the processor clock [11], Digital's 21164, the predecessor of 21264, uses a replicated data cache [8], and the MIPS R10000 implements a two-way interleaved data cache <ref> [31] </ref>. Each design, however, is either costly to implement, and/or has significant drawbacks. The time-division multiplexing does not scale beyond a certain number of ports (seemingly two ports). The replication approach broadcasts a store to each replicated cache for data coherence, and requires doubled silicon area. <p> The MIPS R10000 processor, for example, partitions the window into an integer queue, a floating-point queue, and an address queue, based on the instruction type <ref> [31] </ref>. The data-decoupled architecture further partitions the instruction window for data memory accesses, and provides a separate cache for each partitioned window. An example of a pipelined, two-way data-decoupled architecture is depicted in Figure 1. <p> Varing number of ports. L2 data cache 4-way set-assoc. 512 KB. 12-cycle access time. Memory 50-cycle access time. Fully interleaved. Instruction cache Perfect I-cache with 1 cycle latency. Branch prediction Perfect branch prediction. Inst. latencies Same as those of MIPS R10000 <ref> [31] </ref>. Table 1: The base machine model. Decode and commit widths are the same as the issue width. which are derived from the MIPS R10000 implementation [31]. The ROB and the LSQ effectively form the instruction window of the processor. <p> Fully interleaved. Instruction cache Perfect I-cache with 1 cycle latency. Branch prediction Perfect branch prediction. Inst. latencies Same as those of MIPS R10000 <ref> [31] </ref>. Table 1: The base machine model. Decode and commit widths are the same as the issue width. which are derived from the MIPS R10000 implementation [31]. The ROB and the LSQ effectively form the instruction window of the processor. The primary on-chip data cache that is 32 KB, 2-way set-associative, and multi-ported, has 2-cycle hit time, as in some of the recent machines [31, 11]. <p> The ROB and the LSQ effectively form the instruction window of the processor. The primary on-chip data cache that is 32 KB, 2-way set-associative, and multi-ported, has 2-cycle hit time, as in some of the recent machines <ref> [31, 11] </ref>. The 512 KB secondary data cache, either on-chip or off-chip, has a 12-cycle hit latency. Data caches are lock-up free.
References-found: 31

