URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/95nips-ds.ps.gz
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Email: bdasgupt@daisy.uwaterloo.ca  sontag@control.rutgers.edu  
Title: Sample Complexity for Learning Recurrent Perceptron Mappings  
Author: Bhaskar Dasgupta Eduardo D. Sontag 
Address: Waterloo, Ontario N2L 3G1 CANADA  New Brunswick, NJ 08903 USA  
Affiliation: Department of Computer Science University of Waterloo  Department of Mathematics Rutgers University  
Abstract: Recurrent perceptron classifiers generalize the classical perceptron model. They take into account those correlations and dependences among input coordinates which arise from linear digital filtering. This paper provides tight bounds on sample complexity associated to the fitting of such models to experimental data.
Abstract-found: 1
Intro-found: 1
Reference: <author> A.D. Back and A.C. Tsoi, </author> <title> FIR and IIR synapses, a new neural network architecture for time-series modeling, </title> <booktitle> Neural Computation, 3 (1991), </booktitle> <pages> pp. 375-385. </pages>
Reference: <author> A.D. Back and A.C. Tsoi, </author> <title> A comparison of discrete-time operator models for nonlinear system identification, </title> <booktitle> Advances in Neural Information Processing Systems (NIPS'94), </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995, </year> <note> to appear. </note>
Reference: <author> A.M. Baksho, S. Dasgupta, J.S. Garnett, and C.R. Johnson, </author> <title> On the similarity of conditions for an open-eye channel and for signed filtered error adaptive filter stability, </title> <booktitle> Proc. IEEE Conf. Decision and Control, </booktitle> <address> Brighton, UK, Dec. 1991, </address> <publisher> IEEE Publications, </publisher> <year> 1991, </year> <pages> pp. 1786-1787. </pages>
Reference-contexts: Various dynamical system models for classification appear from instance when learning finite automata and languages (Giles et. al., 1990) and in signal processing as a channel equalization problem (at least in the simplest 2-level case) when modeling linear channels transmitting digital data from a quantized source, e.g. <ref> (Baksho et. al., 1991) </ref> and (Pulford et. al., 1991). When dealing with linear dynamical classifiers, the inner product ~c:v represents a convolution by a separating vector ~c that is the impulse-response of a recursive digital filter of some order n t k.
Reference: <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth, </author> <title> Learnability and the Vapnik-Chervonenkis dimension, </title> <journal> J. of the ACM, </journal> <volume> 36 (1989), </volume> <pages> pp. 929-965. </pages>
Reference: <author> D.F. Delchamps, </author> <title> Extracting State Information from a Quantized Output Record, </title> <journal> Systems and Control Letters, </journal> <volume> 13 (1989), </volume> <pages> pp. 365-372. </pages>
Reference: <author> R.O. Duda and P.E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: 1 Introduction One of the most popular approaches to binary pattern classification, underlying many statistical techniques, is based on perceptrons or linear discriminants; see for instance the classical reference <ref> (Duda and Hart, 1973) </ref>. In this context, one is interested in classifying k-dimensional input patterns v = (v 1 ; : : : ; v k ) into two disjoint classes A + and A .
Reference: <author> C.E. Giles, G.Z. Sun, H.H. Chen, Y.C. Lee, and D. Chen, </author> <title> Higher order recurrent networks and grammatical inference, </title> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <editor> D.S. Touretzky, ed., </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: The existence of such a dynamical system reflects the fact that the signals of interest exhibit context dependence and correlations, and this prior information can help in narrowing down the search for a classifier. Various dynamical system models for classification appear from instance when learning finite automata and languages <ref> (Giles et. al., 1990) </ref> and in signal processing as a channel equalization problem (at least in the simplest 2-level case) when modeling linear channels transmitting digital data from a quantized source, e.g. (Baksho et. al., 1991) and (Pulford et. al., 1991).
Reference: <author> P. Goldberg and M. Jerrum, </author> <title> Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers, </title> <journal> Mach Learning, </journal> <volume> 18, </volume> <year> (1995): </year> <pages> 131-148. </pages>
Reference-contexts: result vc (F) blog (vc (F fl ))c.) The proof of Theorem 2 is omitted due to space limitations. 4 Proof of Main Result We recall the following result; it was proved, using Milnor-Warren bounds on the number of connected components of semi-algebraic sets, by Goldberg and Jerrum: Fact 4.1 <ref> (Goldberg and Jerrum, 1995) </ref> Assume given a function F : fl fi X ! f1; 1g and the associated class of functions F := fF (; ) : X! f1; 1g j 2 flg.
Reference: <author> D. Haussler, </author> <title> Decision theoretic generalizations of the PAC model for neural nets and other learning applications, </title> <journal> Information and Computation, </journal> <volume> 100, </volume> <year> (1992): </year> <pages> 78-150. </pages>
Reference: <author> R. Koplon and E.D. Sontag, </author> <title> Linear systems with sign-observations, </title> <journal> SIAM J. Control and Optimization, </journal> <volume> 31(1993): 1245 - 1266. </volume>
Reference: <author> W. Maass, </author> <booktitle> Perspectives of current research about the complexity of learning in neural nets, in Theoretical Advances in Neural Computation and Learning , V.P. </booktitle>
Reference: <editor> Roychowdhury, K.Y. Siu, and A. Orlitsky, eds., </editor> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1994, </year> <pages> pp. 295-336. </pages>
Reference: <author> G.W. Pulford, R.A. Kennedy, and B.D.O. Anderson, </author> <title> Neural network structure for emulating decision feedback equalizers, </title> <booktitle> Proc. Int. Conf. Acoustics, Speech, and Signal Processing, </booktitle> <address> Toronto, Canada, </address> <month> May </month> <year> 1991, </year> <pages> pp. 1517-1520. </pages>
Reference-contexts: system models for classification appear from instance when learning finite automata and languages (Giles et. al., 1990) and in signal processing as a channel equalization problem (at least in the simplest 2-level case) when modeling linear channels transmitting digital data from a quantized source, e.g. (Baksho et. al., 1991) and <ref> (Pulford et. al., 1991) </ref>. When dealing with linear dynamical classifiers, the inner product ~c:v represents a convolution by a separating vector ~c that is the impulse-response of a recursive digital filter of some order n t k.
Reference: <author> E.D. Sontag, </author> <title> Neural networks for control, in Essays on Control: Perspectives in the Theory and its Applications (H.L. </title> <editor> Trentelman and J.C. Willems, eds.), </editor> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1993, </year> <pages> pp. </pages> <month> 339-380. </month> <title> Gy orgy Tur an, </title> <booktitle> Computational Learning Theory and Neural Networks:A Survey of Selected Topics, in Theoretical Advances in Neural Computation and Learning, </booktitle> <editor> V.P. Roychowdhury, K.Y. Siu,and A. Orlitsky, eds., </editor> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1994, </year> <pages> pp. 243-293. </pages>
Reference: <author> L.G. </author> <title> Valiant A theory of the learnable, </title> <journal> Comm. ACM, </journal> <volume> 27, </volume> <year> 1984, </year> <pages> pp. 1134-1142. </pages>
Reference: <author> V.N.Vapnik, </author> <title> Estimation of Dependencies Based on Empirical Data, </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1982. </year>
References-found: 16

