URL: http://www.cs.duke.edu/~mlittman/docs/nc97-gmdp.ps
Refering-URL: 
Root-URL: 
Email: szepes@sol.cc.u-szeged.hu  mlittman@cs.duke.edu  
Title: A Unified Analysis of Value-Function-Based Reinforcement-Learning Algorithms  
Author: Csaba Szepesvari Michael L. Littman 
Date: December 3, 1997  
Address: Szeged 6720, Aradi vrt tere 1. Hungary  Durham, NC 27708-0129  
Affiliation: Research Group on Artificial Intelligence "Jozsef Attila" University  Department of Computer Science Duke University  
Abstract: Reinforcement learning is the problem of generating optimal behavior in a sequential decision-making environment given the opportunity of interacting with it. Many algorithms for solving reinforcement-learning problems work by computing improved estimates of the optimal value function. We extend prior analyses of reinforcement-learning algorithms and present a powerful new theorem that can provide a unified analysis of value-function-based reinforcement-learning algorithms. The usefulness of the theorem lies in how it allows the asynchronous convergence of a complex reinforcement-learning algorithm to be proven by verifying that a simpler synchronous algorithm converges. We illustrate the application of the theorem by analyzing the convergence of Q-learning, model-based reinforcement learning, Q-learning with multi-state updates, Q-learning for Markov games, and risk-sensitive reinforcement learning. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G.; Bradtke, S. J.; and Singh, S. P. </author> <year> 1995. </year> <title> Learning to act using real-time dynamic programming. </title> <booktitle> Artificial Intelligence 72(1) </booktitle> <pages> 81-138. </pages> <note> 59 Barto, </note> <author> A. G.; Sutton, R. S.; and Watkins, C. J. C. H. </author> <year> 1989. </year> <title> Learning and sequential decision making. </title> <type> Technical Report 89-95, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA. </address> <note> Also published in Learning and Computational Neuroscience: Foundations of Adaptive Networks, </note> <editor> Michael Gabriel and John Moore, editors. </editor> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: In this way, we will be able to prove the convergence of a wide range of reinforcement-learning algorithms all at once. For example, we will get a convergence proof for Q-learning (Section 3.1), adaptive real-time dynamic programming <ref> (Barto, Bradtke, & Singh 1995) </ref> (the iteration v t+1 = T (p t ; c t )v t outlined earlier), model-based reinforcement learning (Section 3.2), Q-learning with multi-state updates (Section 3.3), Q-learning for Markov games (Section 3.4), risk-sensitive reinforcement learning (Section 3.5), and many other related algorithms. 4 2 Fixed-Point Convergence <p> This can be accomplished easily, however, by setting p t (x; a; y) = 0 if no transition from x to y under a has been observed. In this framework, the adaptive real-time dynamic-programming algorithm <ref> (Barto, Bradtke, & Singh 1995) </ref> takes the form V t+1 (x) = min a2A t (c t (x; a; ) + flV t ()) ; if x 2 t t V t (x); otherwise, (24) where c t (x; a; y) is the estimated cost-function and t t is the set
Reference: <author> Bertsekas, D. P., and Casta~non, D. A. </author> <year> 1989. </year> <title> Adaptive aggregation methods for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control 34(6) </journal> <pages> 589-598. </pages>
Reference: <author> Bertsekas, D. P., and Tsitsiklis, J. N. </author> <year> 1989. </year> <title> Parallel and Distributed Computation: Numerical Methods. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Bertsekas, D. P., and Tsitsiklis, J. N. </author> <year> 1996. </year> <title> Neuro-Dynamic Programming. </title> <address> Belmont, MA: </address> <publisher> Athena Scientific. </publisher>
Reference: <author> Gordon, G. J. </author> <year> 1995. </year> <title> Stable function approximation in dynamic programming. </title>
Reference: <editor> In Prieditis, A., and Russell, S., eds., </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 261-268. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Gullapalli, V., and Barto, A. G. </author> <year> 1994. </year> <title> Convergence of indirect adaptive asynchronous value iteration algorithms. </title> <editor> In Cowan, J. D.; Tesauro, G.; and Alspec-tor, J., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> 695-702. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Heger, M. </author> <year> 1994. </year> <title> Consideration of risk in reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> 105-111. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As a consequence of this, model-based methods can be used to find optimal policies in mdps, alternating Markov games, Markov games (Littman 1994), risk-sensitive models <ref> (Heger 1994) </ref>, and exploration-sensitive (i.e., sarsa) models (John 1994; Rummery & Niranjan 1994).
Reference: <author> Jaakkola, T.; Jordan, M. I.; and Singh, S. P. </author> <year> 1994. </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <booktitle> Neural Computation 6(6) </booktitle> <pages> 1185-1201. </pages>
Reference: <author> John, G. H. </author> <year> 1994. </year> <title> When the best move isn't optimal: Q-learning with exploration. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 1464. </pages>
Reference: <author> Konda, V., and Borkar, V. </author> <year> 1997. </year> <title> Learning algorithms for Markov decision processes. </title> <note> Submitted. </note>
Reference-contexts: However, if the actions that are tried depend on the estimated Q t values, then there does not seem to be any simple way to ensure the convergence of Q t unless randomized policies are used during learning whose rate of change is slower than that of the estimation process <ref> (Konda & Borkar 1997) </ref>.
Reference: <author> Korf, R. E. </author> <year> 1990. </year> <title> Real-time heuristic search. </title> <booktitle> Artificial Intelligence 42 </booktitle> <pages> 189-211. </pages>
Reference: <author> Kushner, H., and Clark, D. </author> <year> 1978. </year> <title> Stochastic approximation methods for constrained and unconstrained systems. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York. </address>
Reference: <author> Littman, M. L., and Szepesvari, C. </author> <year> 1996. </year> <title> A generalized reinforcement-learning model: Convergence and applications. </title> <editor> In Saitta, L., ed., </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> 310-318. </pages> <note> 60 Littman, </note> <author> M. L. </author> <year> 1994. </year> <title> Markov games as a framework for multi-agent rein-forcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> 157-163. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Interestingly, although this process is not of the relaxation form, still Theorem 3 implies their convergence for a wide variety of models and methods. In order to capture this generality, let us introduce a class of generalized mdps. In generalized mdps <ref> (Szepesvari & Littman 1996) </ref>, the cost-propagation operator Q takes the special form (QV )(x; a) = y2X (c (x; a; y) + flV (y)) : Here, L (x;a) f () might take the form P y2X p (x; a; y)f (y), which corresponds to the case of expected total-discounted cost criterion, <p> Theorem 12 implies the convergence of Q-learning for alternating Markov games because min and max are both non-expansions <ref> (Littman 1996) </ref>. Markov games are a generalization of both mdps and alternating Markov games in which the two players simultaneously choose actions at each step in the process (Owen 1982; Littman 1994).
Reference: <author> Littman, M. L. </author> <year> 1996. </year> <title> Algorithms for Sequential Decision Making. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, Brown University. </institution> <note> Also Technical Report CS-96-09. </note>
Reference-contexts: Interestingly, although this process is not of the relaxation form, still Theorem 3 implies their convergence for a wide variety of models and methods. In order to capture this generality, let us introduce a class of generalized mdps. In generalized mdps <ref> (Szepesvari & Littman 1996) </ref>, the cost-propagation operator Q takes the special form (QV )(x; a) = y2X (c (x; a; y) + flV (y)) : Here, L (x;a) f () might take the form P y2X p (x; a; y)f (y), which corresponds to the case of expected total-discounted cost criterion, <p> Theorem 12 implies the convergence of Q-learning for alternating Markov games because min and max are both non-expansions <ref> (Littman 1996) </ref>. Markov games are a generalization of both mdps and alternating Markov games in which the two players simultaneously choose actions at each step in the process (Owen 1982; Littman 1994).
Reference: <author> Ljung, L. </author> <year> 1977. </year> <title> Analysis of recursive stochastic algorithms. </title> <journal> IEEE Trans. Automat. Control 22 </journal> <pages> 551-575. </pages>
Reference: <author> Mahadevan, S. </author> <year> 1996. </year> <title> Average reward reinforcement learning: Foundations, algorithms, and empirical results. </title> <booktitle> Machine Learning </booktitle> 22(1/2/3):159-196. 
Reference: <author> Moore, A. W., and Atkeson, C. G. </author> <year> 1993. </year> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <booktitle> Machine Learning 13 </booktitle> <pages> 103-130. </pages>
Reference-contexts: learning rule (the estimation part) works as intended. 3.2 Model-Based Reinforcement Learning Q-learning shows that optimal value functions can be estimated without ever explicitly learning c and p; however, building estimates of c and p can make more efficient use of experience at the expense of additional storage and computation <ref> (Moore & Atkeson 1993) </ref>. The parameters of c and p can be learned from experience by keeping statistics for each state-action pair on the expected cost and the proportion of transitions to each next state.
Reference: <author> Owen, G. </author> <year> 1982. </year> <title> Game Theory: Second edition. </title> <address> Orlando, Florida: </address> <publisher> Academic Press. </publisher>
Reference-contexts: The sets X , A, and B are finite. Optimal policies are in equilibrium, meaning that neither player has any incentive to deviate from its policy as long as its opponent adopts its policy. In 41 every Markov game, there is a pair of optimal policies that are stationary <ref> (Owen 1982) </ref>. Unlike mdps and alternating Markov games, the optimal policies are sometimes stochastic; there are Markov games in which no deterministic policy is optimal (the classic playground game of "rock, paper, scissors" is of this type).
Reference: <author> Polya, G., and Szeg-o, G. </author> <year> 1972. </year> <title> Problems and Theorems in Analysis I. </title> <publisher> Springer-Verlag Berlin Heidelberg. </publisher>
Reference: <author> Puterman, M. L. </author> <year> 1994. </year> <title> Markov Decision Processes|Discrete Stochastic Dynamic Programming. </title> <address> New York, NY: </address> <publisher> John Wiley & Sons, Inc. </publisher>
Reference: <author> Ribeiro, C. </author> <year> 1995. </year> <title> Attentional mechanisms as a strategy for generalisation in the Q-learning algorithm. </title> <booktitle> In Proceedings of ICANN'95, </booktitle> <volume> volume 1, </volume> <pages> 455-460. </pages>
Reference: <author> Robbins, H., and Monro, S. </author> <year> 1951. </year> <title> A stochastic approximation method. </title> <journal> Annals of Mathematical Statistics 22 </journal> <pages> 400-407. </pages>
Reference-contexts: For example, the theorem makes the convergence of Q-learning a consequence of the classical Robbins-Monro theory <ref> (Robbins & Monro 1951) </ref>. Conditions 1, 2, and 3 are standard for this type of result; the first two are Lipschitz conditions on the two parameters of the operator sequence T = (T 0 ; T 1 ; : : :) and Condition 3 is a learning-rate condition.
Reference: <author> Robbins, H., and Siegmund, D. </author> <year> 1971. </year> <title> A convergence theorem for non-negative almost supermartingales and some applications. </title> <editor> In Rustagi, J., ed., </editor> <title> Optimizing Methods in Statistics. </title> <address> New York: </address> <publisher> Academic Press. </publisher> <pages> 235-257. </pages>
Reference: <author> Rummery, G. A., and Niranjan, M. </author> <year> 1994. </year> <title> On-line Q-learning using connectionist systems. </title> <type> Technical Report CUED/F-INFENG/TR 166, </type> <institution> Cambridge University Engineering Department. </institution>
Reference: <author> Schwartz, A. </author> <year> 1993. </year> <title> A reinforcement learning method for maximizing undis-counted rewards. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 298-305. </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schweitzer, P. J. </author> <year> 1984. </year> <title> Aggregation methods for large Markov chains. </title> <editor> In Iazola, G.; Coutois, P. J.; and Hordijk, A., eds., </editor> <title> Mathematical Computer Performance and Reliability. </title> <publisher> Amsterdam, Holland: Elsevier. </publisher> <pages> 275-302. </pages>
Reference: <author> Singh, S. P., and Sutton, R. S. </author> <year> 1996. </year> <title> Reinforcement learning with replacing eligibility traces. </title> <booktitle> Machine Learning </booktitle> 22(1/2/3):123-158. 
Reference: <author> Singh, S.; Jaakkola, T.; Littman, M. L.; and Szepesvari, C. </author> <year> 1997. </year> <title> Convergence results for single-step on-policy reinforcement-learning algorithms. </title> <note> Submitted for review. 61 Singh, </note> <author> S.; Jaakkola, T.; and Jordan, M. </author> <year> 1995. </year> <title> Reinforcement learning with soft state aggregation. </title> <editor> In Tesauro, G.; Touretzky, D. S.; and Leen, T. K., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> 361-368. </pages> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference: <author> Szepesvari, C., and Littman, M. L. </author> <year> 1996. </year> <title> Generalized Markov decision processes: Dynamic-programming and reinforcement-learning algorithms. </title> <type> Technical Report CS-96-11, </type> <institution> Brown University, Providence, RI. Szepesvari, m. </institution> <year> 1997. </year> <title> On the asymptotic convergence rate of Q-learning. </title> <booktitle> In Proc. of Neural Information Processing Systems. </booktitle> <pages> accepted. </pages>
Reference-contexts: Interestingly, although this process is not of the relaxation form, still Theorem 3 implies their convergence for a wide variety of models and methods. In order to capture this generality, let us introduce a class of generalized mdps. In generalized mdps <ref> (Szepesvari & Littman 1996) </ref>, the cost-propagation operator Q takes the special form (QV )(x; a) = y2X (c (x; a; y) + flV (y)) : Here, L (x;a) f () might take the form P y2X p (x; a; y)f (y), which corresponds to the case of expected total-discounted cost criterion,
Reference: <author> Tsitsiklis, J. N. </author> <year> 1994. </year> <title> Asynchronous stochastic approximation and Q-learning. </title> <booktitle> Machine Learning 16(3) </booktitle> <pages> 185-202. </pages>
Reference: <author> Vrieze, O. J., and Tijs, S. H. </author> <year> 1982. </year> <title> Fictitious play applied to sequences of games and discounted stochastic games. </title> <journal> International Journal of Game Theory 11(2) </journal> <pages> 71-85. </pages>
Reference: <author> Watkins, C. J. C. H., and Dayan, P. </author> <year> 1992. </year> <title> Q-learning. </title> <booktitle> Machine Learning 8(3) </booktitle> <pages> 279-292. </pages>
Reference-contexts: The approximation of Q fl comes then from the "optimistic" (in the sense of Bertsekas & Tsitsiklis (1996)) replacement of Q in the above iteration by Q t . The corre sponding process, called Q-learning <ref> (Watkins & Dayan 1992) </ref>, is ^ Q t+1 = T t ( ^ Q t ; ^ Q t ): (4) Note that the convergence of the iteration defined in Equation (2) follows trivially from the law of large numbers since, for any fixed pair (x; a), the values 3 Q
Reference: <author> Watkins, C. J. C. H. </author> <year> 1989. </year> <title> Learning from Delayed Rewards. </title> <type> Ph.D. Dissertation, </type> <institution> King's College, </institution> <address> Cambridge, UK. </address> <month> 62 </month>
Reference-contexts: In a model-free approach, such as Q-learning <ref> (Watkins 1989) </ref>, for example, the decision maker directly estimates v fl without ever estimating p or c. We describe an abstract version of Q-learning next, as it provides a framework and vocabulary for sum 2 marizing the majority of our results.
References-found: 34

