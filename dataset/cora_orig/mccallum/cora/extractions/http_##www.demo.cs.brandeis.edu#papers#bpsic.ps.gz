URL: http://www.demo.cs.brandeis.edu/papers/bpsic.ps.gz
Refering-URL: http://www.demo.cs.brandeis.edu/papers/long.html
Root-URL: http://www.cs.brandeis.edu
Email: kolen-j@cis.ohio-state.edu, pollack@cis.ohio-state.edu  
Title: Back Propagation is Sensitive to Initial Conditions  
Author: John F. Kolen Jordan B. Pollack 
Note: TR 90-JK-BPSIC  
Address: Columbus, Ohio 43210, USA  
Affiliation: Laboratory for Artificial Intelligence Research Computer and Information Science Department The Ohio State University  
Abstract: This paper explores the effect of initial weight selection on feed-forward networks learning simple functions with the back-propagation technique. We first demonstrate, through the use of Monte Carlo techniques, that the magnitude of the initial condition vector (in weight space) is a very significant parameter in convergence time variability. In order to further understand this result, additional deterministic experiments were performed. The results of these experiments demonstrate the extreme sensitivity of back propagation to initial weight configuration. 
Abstract-found: 1
Intro-found: 1
Reference: <author> K. L. Babcock and R. M. Westervelt. </author> <year> 1987. </year> <title> Dynamics of simple electronic neural networks, </title> <journal> Physica, 28D:305-316. </journal>
Reference: <author> M. Barnsley. </author> <year> 1988. </year> <title> Fractals Everywhere, </title> <publisher> Academic Press, </publisher> <address> San Diego, CA. </address> <year> 1988. </year>
Reference-contexts: Note the triangles appearing in Figures 19, 21, 23 and the mosaic in Figure 25 corresponding to the area which did not converge in 200 iterations in Figure 24. The triangular boundaries are similar to fractal structures generated under iterated function systems <ref> (Barnsley 1988) </ref>: in this case, the iterated function is the back propagation learning method. We propose that these fractal-like boundaries arise in back-propagation due to the existence of multiple solutions (attractors), the non-zero learning parameters, and the non-linear deterministic nature of the gradient descent approach.
Reference: <author> A. Blum and R. Rivest. </author> <year> 1988. </year> <title> Training a 3-node Neural Network is NP-Complete. </title> <booktitle> Proceedings of IEEE Conference on Neural Information Processing Systems, </booktitle> <address> Denver, Colorado, </address> <year> 1988. </year>
Reference: <author> M. Y. Choi and B. A. Huberman. </author> <year> 1983. </year> <title> Dynamic behavior of nonlinear networks, </title> <journal> Physical Review A, </journal> <volume> 28 </volume> <pages> 1204-1206. </pages>
Reference: <author> J. J. Hopfield. </author> <year> 1982. </year> <title> Neural Networks and Physical Systems with Emergent Collective Computational Abilities. </title> <booktitle> Proceedings US National Academy of Science 79 </booktitle> <pages> 2554-2558. </pages>
Reference: <author> B. Derrida and R. Meir. </author> <year> 1988. </year> <title> Chaotic behavior of a layered neural network. </title> <journal> Physical Review A, </journal> <volume> 38 </volume> <pages> 3116-3119. </pages>
Reference: <author> D. D'Humieres, M. R. Beasley, B. A. Huberman, and A. Libchaber. </author> <year> 1982. </year> <title> Chaotic States and Routes to Chaos in the Forced Pendulum. </title> <journal> Physical Review A, </journal> <volume> 26 </volume> <pages> 3483-96. </pages>
Reference: <author> B. A. Huberman and T. Hogg. </author> <year> 1987. </year> <journal> Phase Transitions and Artificial Intelligence. Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 155-172. </pages>
Reference: <author> S. Judd. </author> <year> 1988. </year> <title> Learning in Networks is Hard. </title> <journal> Journal of Complexity 4 </journal> <pages> 177-192. </pages>
Reference: <author> KOLEN & POLLACK J. Kolen. </author> <year> 1988. </year> <title> Faster Learning Through a Probabilistic Approximation Algorithm. </title> <booktitle> Proceedings of the Second IEEE International Conference on Neural Networks, </booktitle> <address> San Diego, </address> <publisher> Cal-ifornia, </publisher> <pages> pp. </pages> <month> I:449-454. </month>
Reference: <author> J. Kolen and A. Goel. </author> <title> To appear. Learning in Parallel Distributed Processing Networks: Computational Complexity and Information Content. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics. </journal>
Reference: <author> K. E. Kurten and J. W. Clark. </author> <year> 1986. </year> <title> Chaos in Neural Networks. </title> <journal> Physics Letters, </journal> <volume> 114A, </volume> <pages> 413-418. </pages>
Reference: <author> R. P. Lippman and B. Gold. </author> <year> 1987. </year> <title> Neural Classifiers Useful for Speech Recognition. </title> <booktitle> In 1st International Conference on Neural Networks ,IEEE, </booktitle> <address> IV:417-426. </address>
Reference-contexts: As the number of solutions available to the gradient descent method increases, the more complicated the non-local interactions between them. This explains the puzzling result that several researchers have noted, that as more hidden units are added, instead of speeding up, back-propagation slows down <ref> (e.g. Lippman and Gold, 1987) </ref>. Rather than a hill-climbing metaphor with local peaks to get stuck on, we should instead think of a many-body metaphor: The existence of many bodies does not imply that a particle will take a simple path to land on one.
Reference: <author> M. L. Minsky and S. A. Papert. </author> <year> 1988. </year> <title> Perceptrons. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> J. B. Pollack. </author> <year> 1989. </year> <title> Implications of Recursive Auto Associative Memories. </title> <booktitle> In Advances in Neural Information Processing Systems. </booktitle> <publisher> (ed. D. </publisher> <pages> Touretzky) pp 527-536, </pages> <publisher> Morgan Kaufman, </publisher> <address> San Mateo. </address>
Reference-contexts: Finally, we view this result as strong impetus to discover how to exploit the information-creative aspects of non-linear dynamical systems for future models of cognition <ref> (Pollack 1989) </ref>. Acknowledgments This work was supported by Office of Naval Research grant number N00014-89-J1200. Substantial free use of over 200 Sun workstations was generously provided by our department.
Reference: <author> U. Riedal, R. Kuhn, and J. L. van Hemmen.1988. </author> <title> Temporal sequences and chaos in neural nets, </title> <journal> Physical Review A, </journal> <volume> 38 </volume> <pages> 1105-1108. </pages>
Reference: <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <year> 1986. </year> <title> Learning Representation by Back-Propagating Errors. </title> <booktitle> Nature 323 </booktitle> <pages> 533-536. </pages>
Reference-contexts: Introduction Back Propagation <ref> (Rumelhart, Hinton, & Williams, 1986) </ref> is the network training method of choice for many neural network projects, and for good reason.

References-found: 17

