URL: http://arti.vub.ac.be/www/steels/autsys.ps
Refering-URL: http://www.cs.brandeis.edu/~zippy/alife-library.html
Root-URL: 
Email: E-mail: steels@arti.vub.ac.be  
Title: When are robots intelligent autonomous agents?  
Author: Luc Steels 
Keyword: intelligence, self-organisation, representation, complex dynamical systems.  
Address: Pleinlaan 2, B-1050 Brussels, Belgium  
Affiliation: Artificial Intelligence Laboratory Vrije Universiteit Brussel  
Abstract: The paper explores a biologically inspired definition of intelligent autonomous agents. Intelligence is related to whether behavior of a system contributes to its self-maintenance. Behavior becomes more intelligent (or copes with more ecological pressures) when it is capable to create and use representations. The notion of representation should not be restricted to formal expressions with a truth-theoretic semantics. The dynamics at various levels of intelligent sys tems plays an essential role in forming representations.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Arkin, R. </author> <title> (1989) Motor Schema based mobile robot navigation. </title> <journal> Int. Journal of Robotics Research. </journal> <volume> Vol 8, 4 p. </volume> <pages> 92-112. </pages>
Reference: 2. <author> Brooks, R. </author> <year> 1991. </year> <title> Intelligence without reason, </title> <booktitle> IJCAI-91, </booktitle> <address> Sydney, Australia, </address> <pages> pp 569-595. </pages>
Reference: 3. <author> Brooks, R. </author> <year> 1991. </year> <title> Intelligence without representation, </title> <journal> AI Journal. </journal>
Reference-contexts: Although it seems obvious that the ability to handle representations is the most distinguishing characteristic of intelligent systems, this has lately become a controversial point. Autonomous agents researchers have been arguing `against representations'. For example, Brooks <ref> [3] </ref> has claimed that intelligence can be realised without representations. Others have argued that non-representational control systems like the Watt governor are adequate models of cognition [35].
Reference: 4. <author> Clancey, W.J. </author> <title> (1993) Situated Action: A Neuropsychological Interpretation. </title> <booktitle> Cognitive Science 17(1), </booktitle> <pages> 87-116. </pages>
Reference-contexts: The problem is that it seems possible (given enough technological effort) to build highly complex programs which are indistinguishable in performance from human intelligence for a specific area, but these programs do not capture the evolution, nor the embedded (contextual) nature of intelligence <ref> [4] </ref>. As a consequence 'intelligent' programs are often qualified as being no longer intelligent as soon as the person inspecting the program figures out how the problem has been solved. <p> Autonomous agents researchers have been arguing `against representations'. For example, Brooks [3] has claimed that intelligence can be realised without representations. Others have argued that non-representational control systems like the Watt governor are adequate models of cognition [35]. Researchers in situated cognition <ref> [4] </ref>, [27] and in 'constructivist' cognitive science [19] have argued that representations do not play the important role that is traditionally assigned to them. Researchers in neural networks in general reject `symbolic representations' in favor of sub-symbolic or non-symbolic processing [31].
Reference: 5. <author> Cliff, D., I. Harvey, and P. Husbands. </author> <booktitle> (1993) Explorations in evolutionary robotics. Adaptive Behavior 2(1), </booktitle> <pages> 71-104. </pages>
Reference: 6. <author> Cohen, J. and I. </author> <title> Stewart (1994) The Collapse of Chaos. </title>
Reference-contexts: At the moment there is however a strong opposing tendency in the basic sciences to take a wholistic point of view <ref> [6] </ref>. This means that it is now understood that there are properties at each level which cannot be reduced to the level below, but follow from the dynamics at that level, and from interactions (resonances) between the dynamics of the different levels ([25]).
Reference: 7. <author> Dawkins, R. </author> <title> (1976) The Selfish Gene. </title> <publisher> Oxford University Press. Oxford. </publisher>
Reference-contexts: The goal of this paper is to present a definition of intelligent autonomous agents. The definition has taken its inspiration from biology (in particular [22], <ref> [7] </ref>) and is different from traditional definitions currently used in AI, such as the definition based on the Turing test. Our definition is quite tough and no robots can at this point be said to be intelligent or autonomous. 2 Agents Let me start with the notion of an agent.
Reference: 8. <author> Engels, C. and G. </author> <title> Schoener (1994) Dynamic fields endow behavior-based robots with representations. </title> <booktitle> Robotics and Autonomous Systems. </booktitle> <year> 1994. </year>
Reference: 9. <author> Floreano, D. and F. </author> <title> Mondada (1994) Automatic Creation of an Autonomous Agent: Genetic Evolution of a Neural-Network Driven Robot. </title> <editor> In: Cliff, D. et.al. </editor> <booktitle> (1994) From animals to animats 3. Proceedings of the 3d International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press. </publisher> <address> Cambridge Ma. p. </address> <pages> 421-430. </pages>
Reference-contexts: For artificial robotic agents, the building plans, the design principles, and the initial structures of one type of agent when it starts its operation correspond to a kind of genetic level. Several researchers have begun to make this level explicit and perform experiments in genetic evolution <ref> [9] </ref>, [?]. The structural level. This is the level of the components and processes making up the individual agents: cells, cell assemblies, organs, etc. Each of these components has its own defense mechanisms, renewal mechanisms, and adaptive processes.
Reference: 10. <author> Hayes, P. K. Ford, and N. </author> <title> Agnew (1994) On Babies and Bathwater. A Cautionary Tale. </title> <journal> AI Magazine. </journal> <volume> 15 (4), </volume> <pages> 15-26. </pages>
Reference-contexts: Researchers in neural networks in general reject `symbolic representations' in favor of sub-symbolic or non-symbolic processing [31]. All this is resulting in a strong debate of representationalists vs. non-representationalists <ref> [10] </ref>. Let me attempt to clarify the issues. In classical AI, physical structures acting as representations are usually called symbols and the processes operating over them are called symbol processing operations.
Reference: 11. <author> Genesereth, M. and S. Ketchpel. </author> <title> (1994) Software Agents. </title> <journal> Comm. of the ACM. </journal> <volume> 37(7). </volume> <pages> p. 48-53. </pages>
Reference-contexts: In our view, the term agent is used inappropriately for software agents when they do not have any self-interest whatsoever (as is for example the case in [17]), or when the notion of agent is restricted to a unit that is capable to engage in particular communications <ref> [11] </ref>. The drive towards self-maintenance is found in biology at many different levels and equivalent levels can be defined for robotic agents: The genetic level. This is the level which maintains the survivability of the species.
Reference: 12. <author> Genesereth, M. and N. </author> <booktitle> Nilsson (1987) Logical Foundations of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Pub. </publisher> <address> Los Altos. </address>
Reference: 13. <author> Haken, H. </author> <year> (1983). </year> <title> Advanced synergetics: instability hierarchies of self-organisating systems and devices, </title> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference: 14. <author> Jaeger, H. </author> <title> (1994) Dynamic Symbol Systems Ph.D. </title> <type> thesis. </type> <institution> Faculty of Technology. Bielefeld. </institution>
Reference-contexts: The operations that can be performed to obtain predictive power must be truth-preserving. These restrictions on representations are obviously too narrow. States in dynamical systems <ref> [14] </ref> may also behave as representations. Representations should not be restricted to those amenable to formal semantics nor should processing be restricted to logically justified inferences.
Reference: 15. <author> Kaneko, K. </author> <title> (1994) Relevance of dynamic clustering to biological networks. </title> <journal> Physica D 75, </journal> <pages> 55-73. </pages>
Reference: 16. <author> Kiss, G. </author> <title> (1993) Autonomous Agents, AI and Chaos Theory. </title> <editor> In: Meyer, J.A., et.al. (eds.) </editor> <booktitle> From Animals to Animats 2 Proceedings of the Second Int. Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press, Cambridge. </publisher> <pages> pp. 518-524. </pages>
Reference: 17. <author> Maes, P. </author> <title> (1994) Agents that Reduce Work and Information Overload. </title> <journal> Comm. of the ACM 37(7). </journal> <volume> p. </volume> <pages> 30-40. </pages>
Reference-contexts: Without a biological perspective it is however difficult to distinguish between agents and other types of machines or software systems. In our view, the term agent is used inappropriately for software agents when they do not have any self-interest whatsoever (as is for example the case in <ref> [17] </ref>), or when the notion of agent is restricted to a unit that is capable to engage in particular communications [11]. The drive towards self-maintenance is found in biology at many different levels and equivalent levels can be defined for robotic agents: The genetic level.
Reference: 18. <author> Mataric, M. </author> <note> (1994) Learning to Behave Socially. In: </note> <author> Cliff, D. </author> <title> et.al. </title> <booktitle> (1994) From animals to animats 3. Proceedings of the 3d International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press. </publisher> <address> Cambridge Ma. p. </address> <pages> 453-462. </pages>
Reference-contexts: This may include defense mechanisms, social differentiation according to the needs of the group, etc. In the case of artificial systems, the group level becomes relevant when there are groups of robotic agents which have to cooperate in order to survive within a particular ecosystem and accomplish tasks together <ref> [18] </ref>. For a long time, the natural sciences have made progress by reducing the complexity at one level by looking at the underlying components. Behavior at a particular level is explained by clarifying the behavior of the components at the next level down.
Reference: 19. <author> Maturana, H.R. and F.J. </author> <title> Varela (1987) The Tree of Knowledge: The Bio--logical roots of Human Understanding. </title> <publisher> Shamhala Press, </publisher> <address> Boston. </address>
Reference-contexts: Autonomous agents researchers have been arguing `against representations'. For example, Brooks [3] has claimed that intelligence can be realised without representations. Others have argued that non-representational control systems like the Watt governor are adequate models of cognition [35]. Researchers in situated cognition [4], [27] and in 'constructivist' cognitive science <ref> [19] </ref> have argued that representations do not play the important role that is traditionally assigned to them. Researchers in neural networks in general reject `symbolic representations' in favor of sub-symbolic or non-symbolic processing [31]. All this is resulting in a strong debate of representationalists vs. non-representationalists [10].
Reference: 20. <author> McFarland, D. </author> <title> (1990) Animal behaviour. </title> <publisher> Oxford University Press, Oxford. </publisher>
Reference: 21. <author> McFarland, D. and T. </author> <booktitle> Boesser (1994) Intelligent Behavior in Animals and Robots. </booktitle> <publisher> MIT Press/Bradford Books, </publisher> <address> Cambridge Ma. </address>
Reference: 22. <author> McFarland, D. </author> <title> (1994) Towards Robot Cooperation. </title> <booktitle> Proceedings of the Simulation of Adaptive Behavior Conference. </booktitle> <address> Brighton. </address> <publisher> MIT Press. </publisher>
Reference-contexts: The goal of this paper is to present a definition of intelligent autonomous agents. The definition has taken its inspiration from biology (in particular <ref> [22] </ref>, [7]) and is different from traditional definitions currently used in AI, such as the definition based on the Turing test. <p> To be self-controlling, the agent must have relevant self-knowledge and motivation, since they are the prerequisites of a controller. In other words, an autonomous agent must know what to do to exercise control, and must want to exercise control in one way and not in another way." <ref> [22] </ref>, p.4.
Reference: 23. <author> McFarland, D., E. Spier, and P. </author> <month> Stuer </month> <year> (1994) </year>
Reference: 24. <author> Newell,A. </author> <year> (1981). </year> <title> The knowledge level, </title> <journal> Journal of Artificial Intelligence, </journal> <volume> vol 18, no 1, </volume> <pages> pp 87-127. </pages>
Reference-contexts: To find a firmer foundation it seems necessary to look for a definition of intelligence which is not related to subjective judgement. The second set of definitions is in terms of knowledge and intensionality. For example, Newell has worked out the notion of a knowledge level description <ref> [24] </ref>. Such a description can be made of a system if its behavior is most coherently described in terms of the possession of knowledge and the application of this knowledge (principle of rationality).
Reference: 25. <author> Nicolis, G. and I. </author> <title> Prigogine (1985) Exploring Complexity. </title> <type> Piper, </type> <institution> Munchen. </institution>
Reference: 26. <author> Penrose, R. </author> <title> (1990) The Emperor's New Mind. </title> <publisher> Oxford University Press. Oxford. </publisher>
Reference-contexts: There are still other definitions, which however are not used within AI itself. For example, several authors, most notably Roger Penrose, claim that intelligence is intimately tied up with consciousness and self-consciousness <ref> [26] </ref>. This in turn is defined as the capability to intuit mathematical truths or perform esthetic judgements.
Reference: 27. <author> Pfeifer, R. and P. </author> <title> Verschure (1992) Distributed Adaptive Control: A Paradigm for Designing Autonomous Agents. </title> <editor> In: Varela, F.J. and P. Bourgine (eds.) </editor> <booktitle> (1992) Toward a Practice of Autonomous Systems. Proceedings of the First European Conference on Artificial Life. </booktitle> <publisher> MIT Press/Bradford Books, </publisher> <address> Cambridge Ma. p. </address> <pages> 21-30. </pages>
Reference-contexts: Autonomous agents researchers have been arguing `against representations'. For example, Brooks [3] has claimed that intelligence can be realised without representations. Others have argued that non-representational control systems like the Watt governor are adequate models of cognition [35]. Researchers in situated cognition [4], <ref> [27] </ref> and in 'constructivist' cognitive science [19] have argued that representations do not play the important role that is traditionally assigned to them. Researchers in neural networks in general reject `symbolic representations' in favor of sub-symbolic or non-symbolic processing [31].
Reference: 28. <author> Schoner, G. and M. </author> <title> Dose (1993) A dynamical systems approach to task-level system integration used to plan and control autonomous vehicle motion, </title> <journal> Journal of Robotics and Autonomous Systems, </journal> <volume> vol 10, </volume> <pages> pp 253-267. </pages>
Reference: 29. <editor> Simon, H. </editor> <booktitle> (1969) The Sciences of the Artificial. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge Ma. </address>
Reference: 30. <author> Smithers, T. </author> <title> (1994) Are autonomous agents information processing systems? In: </title> <editor> Steels, L. and R. Brooks, (Eds.), </editor> <title> The `artificial life' route to `artifical intelligence': building situated embodied agents, </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> New Haven. </address>
Reference: 31. <author> Smolensky, P. </author> <booktitle> (1986) Information Processing in Dynamical Systems. Foundations of Harmony Theory. In: </booktitle> <editor> Rumelhart, D.E., J.L. McClelland, (eds.) </editor> <booktitle> Parallel Distributed Processing. Explorations in the Microstructure of Cognition. </booktitle> <volume> Vol 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge Ma. </address> <pages> pp. 194-281. </pages>
Reference-contexts: Researchers in situated cognition [4], [27] and in 'constructivist' cognitive science [19] have argued that representations do not play the important role that is traditionally assigned to them. Researchers in neural networks in general reject `symbolic representations' in favor of sub-symbolic or non-symbolic processing <ref> [31] </ref>. All this is resulting in a strong debate of representationalists vs. non-representationalists [10]. Let me attempt to clarify the issues. In classical AI, physical structures acting as representations are usually called symbols and the processes operating over them are called symbol processing operations.
Reference: 32. <editor> Steels, L. </editor> <booktitle> (1994a) The artificial life roots of artificial intelligence. Artificial Life Journal, Vol 1,1. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference: 33. <author> Steels, L. </author> <title> (1994b) A case study in the behavior-oriented design of autonomous agents. </title> <booktitle> Proceedings of the Simulation of Adaptive Behavior Conference. </booktitle> <address> Brighton. </address> <publisher> MIT Press. </publisher> <address> Cambridge. </address>
Reference: 34. <author> Trautteur, G. (ed.) </author> <title> (1994) Approaches to consciousness. </title> <publisher> Kluwer Academic Publishing. Amsterdam. </publisher>
Reference-contexts: This in turn is defined as the capability to intuit mathematical truths or perform esthetic judgements. The topic of consciousness is so far not at the center of discussion in AI and no claims have ever been made that artificial intelligence systems exhibit consciousness (although see the discussion in <ref> [34] </ref>). Whether this means, as Penrose suggests, that consciousness falls outside the scope of artificial systems, is another matter. In any case it seems that the coupling of intelligence with consciousness unnecessarily restricts the scope of intelligent systems. These various definitions are all to some extent controversial.
Reference: 35. <author> Van Gelder, T. </author> <title> (1992) What might cognition be if not computation. </title> <institution> Dept of Cognitive Science. Indiana University, Bloomington. </institution> <type> Technical report. </type>
Reference-contexts: Autonomous agents researchers have been arguing `against representations'. For example, Brooks [3] has claimed that intelligence can be realised without representations. Others have argued that non-representational control systems like the Watt governor are adequate models of cognition <ref> [35] </ref>. Researchers in situated cognition [4], [27] and in 'constructivist' cognitive science [19] have argued that representations do not play the important role that is traditionally assigned to them. Researchers in neural networks in general reject `symbolic representations' in favor of sub-symbolic or non-symbolic processing [31].
Reference: 36. <author> Winston, P. </author> <booktitle> (1992) Artificial Intelligence. </booktitle> <publisher> Addison-Wesley Pub. </publisher> <month> Cy. </month> <title> Reading Ma. This article was processed using the L a T E X macro package with LLNCS style </title>
References-found: 36

