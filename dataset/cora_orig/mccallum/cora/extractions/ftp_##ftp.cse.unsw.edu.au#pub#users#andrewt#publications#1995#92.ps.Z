URL: ftp://ftp.cse.unsw.edu.au/pub/users/andrewt/publications/1995/92.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/publications/1995/SCSE_publications.html
Root-URL: 
Email: linda@cse.unsw.edu.au  charlesw@cse.unsw.edu.au  
Phone: tel +61-2-385-3979  
Title: Comparison of Two Methods for Increasing the Training Set Size for Neural Networks  
Author: Linda Milne Charles Willock 
Date: November 28, 1995  
Address: Sydney NSW 2052  
Affiliation: Computer Science and Engineering University of New South Wales  
Abstract: The use of artificial neural networks with geographical data is often constrained by the very small number of training data points which can be obtained. Using multi-spectral and parametric data from the Nullica state forest in NSW, Australia, we look at addition of noise, and resampling, as methods of increasing the number and quality of the training set to get the most out of the data. Resampling the data appears to offer potential as a method of 'generalising' the neural network without the accuracy trade-off of added noise.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H Bischof, W Schneider, and AJ Pinz. </author> <title> Multispectral classification of landsat-images using neural networks. </title> <journal> IEEE Transactions on Geoscience and Remote Sensing, </journal> <volume> 30(3):482 5 490, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: The classifier can then be used to estimate species populations for the entire forest. Neural networks are used extensively for pattern recognition problems, and are being adopted by the remote sensing and GIS communities (some examples include <ref> [1, 2, 3, 4] </ref>). Unfortunately neural networks require large amounts of training data to perform well.
Reference: [2] <author> T Yoshida and S Omatu. </author> <title> Neural network approach to land cover mapping. </title> <journal> IEEE Transactions on Geoscience and Remote Sensing, </journal> <volume> 32(5) </volume> <pages> 1103-1109, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: The classifier can then be used to estimate species populations for the entire forest. Neural networks are used extensively for pattern recognition problems, and are being adopted by the remote sensing and GIS communities (some examples include <ref> [1, 2, 3, 4] </ref>). Unfortunately neural networks require large amounts of training data to perform well.
Reference: [3] <author> AK Skidmore, W Brinkhof, and J Delaney. </author> <title> Using neural networks to analyse spatial data. </title> <booktitle> In Proceedings of the 7th Australasian Remote Sensing Conference, </booktitle> <pages> pages 235-246, </pages> <year> 1994. </year>
Reference-contexts: The classifier can then be used to estimate species populations for the entire forest. Neural networks are used extensively for pattern recognition problems, and are being adopted by the remote sensing and GIS communities (some examples include <ref> [1, 2, 3, 4] </ref>). Unfortunately neural networks require large amounts of training data to perform well.
Reference: [4] <author> DZ Sui. </author> <title> Recent applications of neural networks for spatial data handling. </title> <journal> Canadian Journal of Remote Sensing, </journal> <volume> 20(4) </volume> <pages> 368-380, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: The classifier can then be used to estimate species populations for the entire forest. Neural networks are used extensively for pattern recognition problems, and are being adopted by the remote sensing and GIS communities (some examples include <ref> [1, 2, 3, 4] </ref>). Unfortunately neural networks require large amounts of training data to perform well. <p> Neural networks also provide a powerful tool for analysing multi-source data <ref> [4] </ref>, and so it is worth developing techniques for their use with small data sets. One well known method for improving the generalisation capability of a neural network is to augment the training data set with clones of the training data points to which noise has been added [5].
Reference: [5] <author> J Sietsma and RJF Dow. </author> <title> Creating artificial neural networks that generalise. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 67-79, </pages> <year> 1991. </year>
Reference-contexts: One well known method for improving the generalisation capability of a neural network is to augment the training data set with clones of the training data points to which noise has been added <ref> [5] </ref>. Thus, we can increase the training set size by adding any number of noisy cases to the original data. However, if the data is already noisy it may in fact decrease the overall accuracy obtained. Another possible method for improving the generalisation is to resample the data points. <p> For each of the methods the performance figures obtained from the five networks were averaged to determine an overall performance figure. The first method for increasing the training set size was to add noise to each of the input feature values <ref> [5] </ref>. For each of the normalised input vectors in the original training set three additional noisy vectors were added, giving a training set size of 760 vectors. This process of adding uniformly distributed noise was carried out to generate two training sets in addition to the original data.
Reference: [6] <author> JA Richards. </author> <title> Remote Sensing Digital Image Analysis : An Introduction. </title> <publisher> Springer-Verlag, 2nd ed. </publisher> <address> edition, </address> <year> 1993. </year> <month> 6 </month>
Reference-contexts: Another possible method for improving the generalisation is to resample the data points. For each gridpoint in the data we can consider the value of each recorded parameter as defining a surface. Using cubic convolution interpolation <ref> [6] </ref> we can resample this surface to a smaller gridsize and, in the process, provide additional sample points each slightly different from the original value. Resampling has the advantage of increasing the number of data points while potentially decreasing the noise in the dataset. <p> The first set had a noise range -0.01 to 0.01 (the 1% noise set), the second, a noise range -0.1 to 0.1 (the 10% noise set). The target output values remained unchanged. The final training set was obtained using cubic convolution interpolation resampling <ref> [6] </ref>. Each attribute for the entire study area was resampled using a 15m x 15m grid. The four pixels corresponding to the original 30m x 30m pixel for the surveyed plot data were then extracted, again giving a training set size of 760 vectors.
References-found: 6

