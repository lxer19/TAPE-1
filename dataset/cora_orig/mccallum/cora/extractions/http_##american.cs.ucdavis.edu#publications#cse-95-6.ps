URL: http://american.cs.ucdavis.edu/publications/cse-95-6.ps
Refering-URL: http://www.cs.ucdavis.edu/research/tech-reports/1995.html
Root-URL: http://www.cs.ucdavis.edu
Title: d d A New Approach to Cache Management  
Date: 8, 1995  
Note: August  This work was supported by National Science Foundation Grants CCR-8706722 and CCR-90-11535. 1  
Abstract: Gary Tyson, Matthew Farrens, John Matthews and Andrew Pleszkun Technical Report No. CSE-95-6 Computer Science Department University of California, Davis, CA 95616 tel: (916) 752-7002, fax: (916) 752-4767 -tyson,farrens,matthewj-@cs.ucdavis.edu arp@tosca.colorado.edu Abstract As processor performance continues to improve, more emphasis must be placed on the performance of the memory system. The caches employed in current processor designs are very similar to those described in early cache studies. In this paper, a detailed characterization of cache behavior for individual load instructions is given. A novel approach to modifying cache behavior to improve overall performance of the cache and the memory system is also described. This approach can improve aspects of memory performance by as much as 60 percent on existing executables. hhhhhhhhhhhhhhhhhhhhhhhh
Abstract-found: 1
Intro-found: 1
Reference: [ASWR93] <author> S. G. Abraham, R. A. Sugumar, D. Windheiser, B. R. Rau and R. Gupta, </author> <title> ``Predictability of Load/Store Instruction Latencies'', </title> <booktitle> Proceedings of the 26th Annual International Symposium on Microarchitecture, </booktitle> <address> Austin, Texas (December 1-3, </address> <year> 1993), </year> <pages> pp. 139-152. </pages>
Reference-contexts: By keeping these recently replaced lines in close proximity to the data cache, subsequent references to them will not experience the full main memory miss penalty. Among the most intriguing software approaches to reducing the miss penalty is a study by Abraham, et. al. <ref> [ASWR93] </ref> in which they observe that a very small number of load instructions are responsible for causing a disproportionate percentage of cache misses. <p> Instead of concentrating on miss penalty, one could potentially reduce the miss rate of the data cache by simply not caching those data references that would lead to a poor miss rate. 3. Deciding What to Cache As Abraham, et el. <ref> [ASWR93] </ref> point out, a large percentage of the data misses are caused by a very small number of instructions. <p> Thus, a cache lookup for an item is unaffected by whether it is marked C/NA or not only the allocation on a miss is affected. We looked at both static (similar to <ref> [ASWR93] </ref>) and dynamic approaches to identifying and marking these C/NA instructions. 4.1. Static Method We began by modeling a simple strategy in which all load instructions that do not meet a threshold for cache hit rate are marked C/NA. <p> Furthermore, the 75% threshold also relates to the memory bandwidth requirements for a cache line replacement (32 bytes) and a 64-bit load reference (8 bytes), and is the same value settled on by <ref> [ASWR93] </ref>. 4.1.1. Cache Hit Rate Table 4 shows the cache hit rate after the poorest performing instructions were marked C/NA.
Reference: [CaGr94] <author> B. Calder and D. Grunwald, </author> <title> ``Fast and Accurate Instruction Fetch and Branch Prediction'', </title> <booktitle> Proceedings of the 21th Annual Symposium on Computer Architecture, </booktitle> <address> Chicago, Illinois (April 18-21, </address> <year> 1994), </year> <pages> pp. 2-11. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>. Briefly, dynamic branch prediction strategies collect run-time information about branch behavior to predict whether a branch will be taken in the future. Typically, these strategies associate several bits of information with a branch instruction.
Reference: [CaPo] <author> D. Callahan and A. Porterfield, </author> <title> ``Data Cache Performance and Supercomputer Applications'', </title> <booktitle> Proceedings of Supercomputing '90, </booktitle> <pages> pp. 564-572. </pages>
Reference-contexts: If data items are prefetched during idle data cache cycles, references to a prefetched item will find it already in the cache and thus will not cause a miss and will not experience the associated miss penalty. A number of studies have investigated various forms of prefetching <ref> [CaPo, ChBa95, KlLe91] </ref>, using either compiler-based or hardware-based techniques to determine which data items to prefetch. An example of hardware-based prefetching is the work by Chen and Baer [ChBa95].
Reference: [ChBa95] <author> T. Chen and J. Baer, </author> <title> ``Effective Hardware Based Data Prefetching for High-Performance Processors'', </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 44, no. </volume> <month> 5 (May </month> <year> 1995), </year> <pages> pp. 609-623. </pages>
Reference-contexts: If data items are prefetched during idle data cache cycles, references to a prefetched item will find it already in the cache and thus will not cause a miss and will not experience the associated miss penalty. A number of studies have investigated various forms of prefetching <ref> [CaPo, ChBa95, KlLe91] </ref>, using either compiler-based or hardware-based techniques to determine which data items to prefetch. An example of hardware-based prefetching is the work by Chen and Baer [ChBa95]. <p> A number of studies have investigated various forms of prefetching [CaPo, ChBa95, KlLe91], using either compiler-based or hardware-based techniques to determine which data items to prefetch. An example of hardware-based prefetching is the work by Chen and Baer <ref> [ChBa95] </ref>. In this paper the authors propose keeping a history of the strides of data references, and using that information to make predictions as to what should be prefetched. A somewhat different hardware approach to reducing the miss penalty is put forth by [Joup90].
Reference: [FiFr92] <author> J. A. Fisher and S. M. Freudenberger, </author> <title> ``Predicting Conditional Branch Directions from Previous Runs of a Program'', </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, MA (October 12-15, </address> <year> 1992), </year> <pages> pp. 85-95. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>. Briefly, dynamic branch prediction strategies collect run-time information about branch behavior to predict whether a branch will be taken in the future. Typically, these strategies associate several bits of information with a branch instruction.
Reference: [HePa90] <author> J. Hennessy and D. Patterson, </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, California, (1990). - 22 - d d </address>
Reference-contexts: In this section we will briefly summarize the aspects of cache behavior relevant to this study. The average access time for a memory reference is determined by the hit rate in the cache, the corresponding miss rate and the miss penalty. From <ref> [HePa90] </ref>: Memory stall clock cycles = Reads x Read miss rate x Read Miss Penalty + Writes x Write miss rate x Write Miss Penalty This equation shows that in order to minimize the average access time, the hit rate should be maximized (thereby minimizing the miss rate) while simultaneously minimizing <p> Cache Misses (Miss Rate) Cache misses can be categorized into three types of misses: Compulsory, Capacity and Conflict <ref> [HePa90] </ref>. 1 Compulsory misses are those misses that are initially experienced when a cache is being filled (often called cold start misses), and are very difficult to eliminate. A capacity miss occurs in a cache when more active data items exist than the cache can encompass.
Reference: [Joup90] <author> N. Jouppi, </author> <title> ``Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers'', </title> <booktitle> Proceedings of the Seventeenth Annual International Symposium on Computer Architecture, </booktitle> <volume> vol. 18, no. </volume> <month> 2 (May </month> <year> 1990), </year> <pages> pp. 364-373. </pages>
Reference-contexts: In this paper the authors propose keeping a history of the strides of data references, and using that information to make predictions as to what should be prefetched. A somewhat different hardware approach to reducing the miss penalty is put forth by <ref> [Joup90] </ref>. The author makes the observation that a cache or a degree of associativity that is too small will lead to a substantial number of conflict (or capacity) misses, and that there is a good chance that the line that is selected for replacement will be needed again soon.
Reference: [KlLe91] <author> A. C. Klaiber and H. M. Levy, </author> <title> ``An Architecture for Software-Controlled Data Prefetching'', </title> <booktitle> Proceedings of the Eighteenth Annual International Symposium on Computer Architecture, </booktitle> <address> Toronto, Canada (May 27-30, </address> <year> 1991), </year> <pages> pp. 43-53. </pages>
Reference-contexts: If data items are prefetched during idle data cache cycles, references to a prefetched item will find it already in the cache and thus will not cause a miss and will not experience the associated miss penalty. A number of studies have investigated various forms of prefetching <ref> [CaPo, ChBa95, KlLe91] </ref>, using either compiler-based or hardware-based techniques to determine which data items to prefetch. An example of hardware-based prefetching is the work by Chen and Baer [ChBa95].
Reference: [MuQF91] <author> J. M. Mulder, N. T. Quach and M. J. Flynn, </author> <title> ``An Area Model for On-Chip Memories and its Application'', </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 26, no. </volume> <month> 2 (February </month> <year> 1991), </year> <pages> pp. 98-105. </pages>
Reference-contexts: Several studies have investigated the effect of cache access time with respect to cache size and cache associativity <ref> [MuQF91, WaRP92] </ref>. These studies carefully parameterized a hardware model of the components of the cache (such as data array, tag array, compare logic, bus delays, etc.) and found that going from a direct mapped to 2-way associative cache substantially increases the access time to the cache.
Reference: [PaS92] <author> S. Pan, K. So and J. T. Rahmeh, </author> <title> ``Improving the Accuracy of Dynamic Branch Prediction Using Branch Correlation'', </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, MA (October 12-15, </address> <year> 1992), </year> <pages> pp. 76-84. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>. Briefly, dynamic branch prediction strategies collect run-time information about branch behavior to predict whether a branch will be taken in the future. Typically, these strategies associate several bits of information with a branch instruction.
Reference: [Smit81] <author> J. E. Smith, </author> <title> ``A Study of Branch Prediction Strategies'', </title> <booktitle> Proceedings of the Eighth Annual International Symposium on Computer Architecture, </booktitle> <address> Minneapolis, Minnesota (May 1981), </address> <pages> pp. 135-148. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>. Briefly, dynamic branch prediction strategies collect run-time information about branch behavior to predict whether a branch will be taken in the future. Typically, these strategies associate several bits of information with a branch instruction.
Reference: [SrWa94] <author> A. Srivastava and D. W. Wall, </author> <title> ``Atom: A system for building custonized program analysis tools'', </title> <booktitle> Proceedings of the ACM SIGPLAN Notices 1994 Conference on Programming Languages and Implementations(June 1994), </booktitle> <pages> pp. 196-205. </pages>
Reference-contexts: Since they did not look at an extensive set of benchmark programs, we began by performing some experiments similar to theirs (although more extensive) in which we measured the miss rate associated with individual load and store instructions. - 5 - d d Using the ATOM program trace facilities <ref> [SrWa94] </ref> and the SPEC92 suite of benchmarks, such statistics were relatively straight-forward to gather. We instrumented each program in the SPEC 92 suite in order to track the data cache hit rate associated with each unique data address.
Reference: [WaRP92] <author> T. Wada, S. Rajan and S. A. Przybylski, </author> <title> ``An Analytical Access Time Model for On-Chip Cache Memories'', </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 27, no. </volume> <month> 8 (August </month> <year> 1992), </year> <pages> pp. 1147-1156. </pages>
Reference-contexts: Several studies have investigated the effect of cache access time with respect to cache size and cache associativity <ref> [MuQF91, WaRP92] </ref>. These studies carefully parameterized a hardware model of the components of the cache (such as data array, tag array, compare logic, bus delays, etc.) and found that going from a direct mapped to 2-way associative cache substantially increases the access time to the cache.
Reference: [YeP91] <author> T. Yeh and Y. Patt, </author> <title> ``Two-Level Adaptive Training Branch Prediction'', </title> <booktitle> Proceedings of the 24th Annual International Symposium on Microarchitecture, </booktitle> <address> Albuquerque, New Mexico (November 18-20, </address> <year> 1991), </year> <pages> pp. 51-61. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>. Briefly, dynamic branch prediction strategies collect run-time information about branch behavior to predict whether a branch will be taken in the future. Typically, these strategies associate several bits of information with a branch instruction.
Reference: [YeP92] <author> T. Yeh and Y. Patt, </author> <title> ``Alternative Implementations of Two-Level Adaptive Training Branch Prediction'', </title> <booktitle> Proceedings of the Nineteenth Annual International Symposium on Computer Architecture, </booktitle> <address> Queensland, Australia (May 19-21, </address> <year> 1992), </year> <pages> pp. 124-134. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>. Briefly, dynamic branch prediction strategies collect run-time information about branch behavior to predict whether a branch will be taken in the future. Typically, these strategies associate several bits of information with a branch instruction.
Reference: [YeP93] <author> T. Yeh and Y. Patt, </author> <title> ``A Comparison of Dynamic Branch Predictors that use Two Levels of Branch History'', </title> <booktitle> Proceedings of the Twentieth Annual International Symposium on Computer Architecture, </booktitle> <address> San Diego, CA (May 16-19, </address> <year> 1993), </year> <pages> pp. 257-266. </pages> <address> d d </address>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>. Briefly, dynamic branch prediction strategies collect run-time information about branch behavior to predict whether a branch will be taken in the future. Typically, these strategies associate several bits of information with a branch instruction.
References-found: 16

