URL: http://www.cs.princeton.edu/shrimp/Papers/icpp96NX.ps
Refering-URL: http://www.cs.princeton.edu/shrimp/html/papers_stack_14.html
Root-URL: http://www.cs.princeton.edu
Email: falpert|dubnicki|felten|lig@cs.princeton.edu  
Title: DESIGN AND IMPLEMENTATION OF NX MESSAGE PASSING USING SHRIMP VIRTUAL MEMORY MAPPED COMMUNICATION  
Author: Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Kai Li 
Address: Princeton, New Jersey 08544-2009  
Affiliation: Department of Computer Science Princeton University  
Abstract: This paper describes the design, implementation and performance of the NX message-passing interface on the Shrimp multicomputer. Unlike traditional methods, our implementation, exploiting Shrimp's virtual memory-mapped communication facility, performs buffer management at user level without using a special message-passing processor, and requires no CPU intervention upon message arrival in the common cases. For a four-byte message, our implementation achieves a user-to-user latency of 12 microseconds, about factor of four smaller than that on the Intel Paragon. For large messages, our implementation quickly approaches the bandwidth limit imposed by the Shrimp hardware. 
Abstract-found: 1
Intro-found: 1
Reference: [1] . <author> MPI-FM version 1.0. </author> <title> http://www-csag.cs.uiuc.edu/projects/comm. [2] . MPICH-A Portable Implementation of MPI. </title> <address> http://www.mcs.anl.gov/home/lusk/mpich. </address>
Reference-contexts: This method works only for applications that have fixed communication patterns and requires a compiler to detect and generate application specific message-passing libraries. MPI-FM <ref> [1] </ref> is a high-performance implementation of the MPI message passing interface, based on a port of the portable MPICH [2] layer to the minimal Illiois Fast Messages [15] library on the Myrinet [7] network. MPI-FM achieves a latency of 25 sec and a peak bandwidth of 17.5 MB/s.
Reference: [3] <institution> BCPR Services Inc. </institution> <note> EISA Specification, Version 3.12, </note> <year> 1992. </year>
Reference-contexts: Each node of the Shrimp system is a 60 MHz Pentium-based Xpress PC system [13]. The Xpress PC consists of a Pentium CPU with a second-level cache connected to DRAM memory modules and an EISA <ref> [3] </ref> I/O bus adapter via the Xpress memory bus. The caches snoop DMA transactions and automatically invalidate corresponding cache lines, keeping consistent with all main memory updates, including incoming DMA. <p> We conducted our experiments on a four-node Shrimp multicomputer prototype. Each node of the Shrimp system is a 60 MHz Pentium-based Xpress PC system [18, 14, 13], with 32 MB DRAM and a 256 kB second-level cache. The nodes use an EISA <ref> [3] </ref> I/O bus. The operating system on each node is Linux with a thin layer of software to provide virtual memory-mapped communication [10]. Results To measure latency and bandwidth for each message size, we performed 10; 000 message-passing round trips, "bouncing" between two Shrimp nodes.
Reference: [4] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Mul-ticomputers. </title> <booktitle> In Proc. of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: This paper describes the design and implementation of the NX message-passing interface on the Shrimp multicomputer. Unlike traditional NX implementations, ours operates entirely at user level. It takes advantage of Shrimp's virtual memory-mapped communication facility <ref> [4, 6] </ref> which provides a basic mechanism to allow a process to transfer data directly from its virtual memory into the memory of a remote process with full protection. <p> The challenge in implementing a message-passing interface such as NX at user level is to exploit fully the virtual memory-mapped communication mechanism, while maintaining protection constraints and NX semantics. More complete descriptions of Shrimp can be found elsewhere <ref> [4, 5, 6] </ref>. 4 IMPLEMENTING NX MESSAGE PASSING ON VMMC Shrimp provides us the opportunity to design an NX implementation that avoids interrupts in the common case. This offers improved performance, but leads to several new design challenges.
Reference: [5] <author> Matthias Blumrich, Cezary Dubnick, Edward Felten, and Kai Li. </author> <title> Protected, user-level dma for the shrimp network interface. </title> <booktitle> In IEEE 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Virtual memory-mapped communication supports two data transfer modes, deliberate update and automatic update. In deliberate update, the sending process explicitly requests the transfer of data from the sender's virtual address space to a previously imported receive buffer. The Shrimp network interface has a user-level DMA mechanism <ref> [5] </ref> which allows a user program to initiate a deliberate update data transfer a An exporter can restrict possible importers, verifying such restrictions when an import is attempted. in two user-level instructions. Automatic update transfers data without explicit processor involvement on either the sender-side or the receiver-side. <p> The challenge in implementing a message-passing interface such as NX at user level is to exploit fully the virtual memory-mapped communication mechanism, while maintaining protection constraints and NX semantics. More complete descriptions of Shrimp can be found elsewhere <ref> [4, 5, 6] </ref>. 4 IMPLEMENTING NX MESSAGE PASSING ON VMMC Shrimp provides us the opportunity to design an NX implementation that avoids interrupts in the common case. This offers improved performance, but leads to several new design challenges. <p> The Shrimp hardware combines these writes into a single network packet, since the writes are to consecutive memory addresses. The deliberate update version is slower for two reasons. First, although the user-level DMA mechanism <ref> [5] </ref> requires only two user-level instructions to initiate a deliberate update DMA send, these instructions are a read and a write to the EISA I/O space, which require about half a microsecond each. Second, a separate deliberate update operation is required for each contiguous region that is sent.
Reference: [6] <author> Matthias Blumrich, Cezary Dubnicki, Edward Felten, Kai Li, and Malena Mesarina. </author> <title> Virtual-memory-mapped network interfaces. </title> <journal> IEEE MICRO, </journal> <volume> 15(1) </volume> <pages> 21-28, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: This paper describes the design and implementation of the NX message-passing interface on the Shrimp multicomputer. Unlike traditional NX implementations, ours operates entirely at user level. It takes advantage of Shrimp's virtual memory-mapped communication facility <ref> [4, 6] </ref> which provides a basic mechanism to allow a process to transfer data directly from its virtual memory into the memory of a remote process with full protection. <p> The challenge in implementing a message-passing interface such as NX at user level is to exploit fully the virtual memory-mapped communication mechanism, while maintaining protection constraints and NX semantics. More complete descriptions of Shrimp can be found elsewhere <ref> [4, 5, 6] </ref>. 4 IMPLEMENTING NX MESSAGE PASSING ON VMMC Shrimp provides us the opportunity to design an NX implementation that avoids interrupts in the common case. This offers improved performance, but leads to several new design challenges.
Reference: [7] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. Myrinet: </author> <title> A gigabit-per-second local area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: MPI-FM [1] is a high-performance implementation of the MPI message passing interface, based on a port of the portable MPICH [2] layer to the minimal Illiois Fast Messages [15] library on the Myrinet <ref> [7] </ref> network. MPI-FM achieves a latency of 25 sec and a peak bandwidth of 17.5 MB/s.
Reference: [8] <author> William J. Dally and Charles L. Seitz. </author> <title> The torus routing chip. </title> <journal> Distributed Computing, </journal> <volume> 1 </volume> <pages> 187-196, </pages> <year> 1986. </year>
Reference-contexts: The caches snoop DMA transactions and automatically invalidate corresponding cache lines, keeping consistent with all main memory updates, including incoming DMA. The interconnect is an Intel Paragon routing backplane, a two-dimensional mesh of Intel iMRC routers [19], essentially faster and wider versions of the Caltech Mesh Routing Chip <ref> [8] </ref>. The backplane supports deadlock-free, oblivious wormhole routing [9] and preserves the order of messages from each sender to each receiver. Virtual Memory Mapped Communication The Shrimp hardware and software together support the virtual memory mapped communication (VMMC) model [10].
Reference: [9] <author> William J. Dally and Charles L. Seitz. </author> <title> Deadlock-free message routing in multiprocessor interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(5):547-553, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: The interconnect is an Intel Paragon routing backplane, a two-dimensional mesh of Intel iMRC routers [19], essentially faster and wider versions of the Caltech Mesh Routing Chip [8]. The backplane supports deadlock-free, oblivious wormhole routing <ref> [9] </ref> and preserves the order of messages from each sender to each receiver. Virtual Memory Mapped Communication The Shrimp hardware and software together support the virtual memory mapped communication (VMMC) model [10]. In VMMC, a process wanting to receive messages exports a region of its virtual memory.
Reference: [10] <author> Cezary Dubnicki, Liviu Iftode, Edward W. Felten, and Kai Li. </author> <title> Software support for virtual memory-mapped communication. </title> <note> to appear in 96' IPPS, </note> <month> May </month> <year> 1996. </year>
Reference-contexts: The backplane supports deadlock-free, oblivious wormhole routing [9] and preserves the order of messages from each sender to each receiver. Virtual Memory Mapped Communication The Shrimp hardware and software together support the virtual memory mapped communication (VMMC) model <ref> [10] </ref>. In VMMC, a process wanting to receive messages exports a region of its virtual memory. A remote process can then import the region a , setting up an export-import mapping. Setting up such a mapping requires system calls by both processes. <p> The nodes use an EISA [3] I/O bus. The operating system on each node is Linux with a thin layer of software to provide virtual memory-mapped communication <ref> [10] </ref>. Results To measure latency and bandwidth for each message size, we performed 10; 000 message-passing round trips, "bouncing" between two Shrimp nodes. We used the Pentium cycle counter to measure the elapsed time, and then divided by 20; 000 to derive the latency and bandwidth results. <p> Figure 2 and Figure 3 show those results. As we can see, the one-way latency for a zero-byte message is 10 sec. The minimum communication latency presently achievable on the raw Shrimp hardware and system software is 4.8 sec <ref> [10] </ref>. (The main contribution to this latency is time needed for arbitration of the EISA bus). Thus, the software overhead of our NX implementation is about 5 sec. This is an order-of-magnitude overhead reduction compared to the Intel Paragon implementation.
Reference: [11] <author> Edward W Felten. </author> <title> Protocol Compilation: High-Performance Communication for Parallel Programs. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: In addition, buffer management is still done at kernel level. Another way to improve NX message-passing performance is to take advantage of application-specific communication patterns to perform buffer management at compile time instead of run time to reduce the overhead of message passing <ref> [11] </ref>. This method works only for applications that have fixed communication patterns and requires a compiler to detect and generate application specific message-passing libraries.
Reference: [12] <author> Mark Homewood and Moray McLaren. </author> <title> Meiko CS-2 interconnect elan elite design. </title> <booktitle> In Proceedings of Hot Interconnects '93 Symposium, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: The software overhead of NX message-passing primitives in our implementation is an order-of-magnitute smaller than that of commercial systems such as CM-5, Intel Paragon and Meiko CS-2 <ref> [12] </ref>. We have not provided detailed comparision between our implementation and the traditional approaches. Doing so would require careful instrumentation to analyze the differences. We plan to conduct such studies in the near future.
Reference: [13] <author> Intel Corporation. </author> <title> Express Platforms Technical Product Summary: System Overview, </title> <month> April </month> <year> 1993. </year>
Reference-contexts: Each node of the Shrimp system is a 60 MHz Pentium-based Xpress PC system <ref> [13] </ref>. The Xpress PC consists of a Pentium CPU with a second-level cache connected to DRAM memory modules and an EISA [3] I/O bus adapter via the Xpress memory bus. <p> Such experiments allow us to determine the best message size at which to switch protocols for optimal message-passing performance. We conducted our experiments on a four-node Shrimp multicomputer prototype. Each node of the Shrimp system is a 60 MHz Pentium-based Xpress PC system <ref> [18, 14, 13] </ref>, with 32 MB DRAM and a 256 kB second-level cache. The nodes use an EISA [3] I/O bus. The operating system on each node is Linux with a thin layer of software to provide virtual memory-mapped communication [10].
Reference: [14] <author> Intel Corporation. </author> <title> Pentium Processor Data Book, </title> <year> 1993. </year>
Reference-contexts: Such experiments allow us to determine the best message size at which to switch protocols for optimal message-passing performance. We conducted our experiments on a four-node Shrimp multicomputer prototype. Each node of the Shrimp system is a 60 MHz Pentium-based Xpress PC system <ref> [18, 14, 13] </ref>, with 32 MB DRAM and a 256 kB second-level cache. The nodes use an EISA [3] I/O bus. The operating system on each node is Linux with a thin layer of software to provide virtual memory-mapped communication [10].
Reference: [15] <author> S. Pakin, M Lauria, and A. Chien. </author> <title> High performance messaging on workstations. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: MPI-FM [1] is a high-performance implementation of the MPI message passing interface, based on a port of the portable MPICH [2] layer to the minimal Illiois Fast Messages <ref> [15] </ref> library on the Myrinet [7] network. MPI-FM achieves a latency of 25 sec and a peak bandwidth of 17.5 MB/s.
Reference: [16] <author> Paul Pierce. </author> <title> The NX/2 operating system. </title> <booktitle> In Proceedings of 3rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 384-390, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: 1 INTRODUCTION NX has been a common message-passing interface on multicomputers for about a decade <ref> [16] </ref>. Its implementations, however, require substantial software overhead. For example, one-way user-to-user latency for a four-byte message on the Intel Paragon is about 50 microseconds, of which only about a microsecond is in hardware [17]. <p> If our Shrimp nodes used the PCI bus, we would see much higher peak bandwidths. 7 RELATED WORK NX and similar libraries have traditionally been implemented in two ways. The first is to make communication a service of the operating system kernel, as on the iPSC/860 implementation of NX <ref> [16] </ref>. In this approach, arrival of a packet at a processor triggers and interrupt, which causes a kernel handler to run. The kernel handler examines the message header and then dispatches the message to memory.
Reference: [17] <author> Paul Pierce. </author> <title> The Paragon implementation of the NX message passing interface. </title> <booktitle> In Proceedings of SHPCC 94, </booktitle> <year> 1994. </year>
Reference-contexts: 1 INTRODUCTION NX has been a common message-passing interface on multicomputers for about a decade [16]. Its implementations, however, require substantial software overhead. For example, one-way user-to-user latency for a four-byte message on the Intel Paragon is about 50 microseconds, of which only about a microsecond is in hardware <ref> [17] </ref>. Over 98% of the overhead is in software to achieve protected communication, buffer management, translation between virtual and physical addresses, DMA setup, message receiving, dispatching and delivery. It is clear that reducing the software overhead is the key to achieving low-latency message passing. <p> The drawback of this method is that system calls and interrupts are expensive, and that kernel-level memory management can be complicated. The second approach is to use a special processor to handle communication, as in the Paragon implementation of NX <ref> [17] </ref>. The communication processor runs specially written messaging code. It constantly polls the network interface to retrieve incoming packets without requiring interrupts. Application processes communicate with the communication processor by writing commands into a queue implemented in shared memory; no system calls are necessary.
Reference: [18] <author> Avtar Saini. </author> <title> An overview of the Intel Pentium processor. </title> <booktitle> In Compcon Spring '93, </booktitle> <pages> pages 60-62, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Such experiments allow us to determine the best message size at which to switch protocols for optimal message-passing performance. We conducted our experiments on a four-node Shrimp multicomputer prototype. Each node of the Shrimp system is a 60 MHz Pentium-based Xpress PC system <ref> [18, 14, 13] </ref>, with 32 MB DRAM and a 256 kB second-level cache. The nodes use an EISA [3] I/O bus. The operating system on each node is Linux with a thin layer of software to provide virtual memory-mapped communication [10].
Reference: [19] <author> Roger Traylor and Dave Dunning. </author> <title> Routing chip set for Intel Paragon parallel supercomputer. </title> <booktitle> In Proceedings of Hot Chips '92 Symposium, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: The caches snoop DMA transactions and automatically invalidate corresponding cache lines, keeping consistent with all main memory updates, including incoming DMA. The interconnect is an Intel Paragon routing backplane, a two-dimensional mesh of Intel iMRC routers <ref> [19] </ref>, essentially faster and wider versions of the Caltech Mesh Routing Chip [8]. The backplane supports deadlock-free, oblivious wormhole routing [9] and preserves the order of messages from each sender to each receiver.
References-found: 18

