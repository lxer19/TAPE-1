URL: http://www.cse.ucsc.edu/~tmk/final2.ps
Refering-URL: http://www.cse.ucsc.edu/~tmk/
Root-URL: http://www.cse.ucsc.edu
Title: Predicting File System Actions from Prior Events  
Author: Thomas M. Kroeger and Darrell D. E. Long zx 
Address: Santa Cruz  
Affiliation: Department of Computer Information Sciences University of California,  
Abstract: We have adapted a multi-order context modeling technique used in the data compression method Prediction by Partial Match (PPM) to track sequences of file access events. From this model, we are able to determine file system accesses that have a high probability of occurring as the next event. By prefetching the data for these events, we have transformed an LRU cache into a predictive cache that in our simulations averages 15% more cache hits than LRU. In fact, on average our four-megabyte predictive cache has a higher cache hit rate than a 90 megabyte LRU cache. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. G. Baker, J. H. Hartman, M. D. Kupfer, K. W. Shirriff, and J. K. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> Proceedings of 13th Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212. </pages> <publisher> ACM, </publisher> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: the cache for more than just the next event, as long as the event occurs before its data is removed from the cache we still avoid a cache miss. 3 3 Simulation To simulate the workload of a system, we used file open events from the Sprite file system traces <ref> [1] </ref>. We chose to consider whole file caching for three reasons.
Reference: [2] <author> T. C. Bell, J. G. Cleary, and I. H. Witten. </author> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year> <month> 9 </month>
Reference-contexts: Our model tracks previous file system events through a finite multi-order context modeling technique adapted from the data compression technique Prediction by Partial Match (PPM) <ref> [2] </ref>. This model uses a trie [9] to store sequences of previous file system events and the number of times they have occurred. Our selector examines the most recently seen sequences and the counts of the events that have followed them to determine likely next events. <p> On the other hand, if we only consider the first order context, t, then h is not so unlikely. Techniques that track multiple contexts of varying orders are termed Multi-Order Context Models <ref> [2] </ref>. To prevent the model from quickly growing beyond available resources, most implementations of a multi order context model limit the highest order tracked to some finite number (m), hence the term Finite Multi-Order Context Model. <p> Once we have updated each context in our set of current contexts, we have a new state that describes our file system. Figure 1 extends an example from Bell <ref> [2] </ref> to illustrate how this trie would develop when given the sequence of 1 A trie is commonly used as as a efficient data structure to hold a dictionary of words. It is based on a tree in which each node contains a specific character. <p> Additionally they suggest that such methods could have great success within a variety of other applications such as hyper-text. Our work adapts PPM in a different manner. We avoid the use of vine pointers <ref> [2, 10] </ref> and instead keep an array of the current contexts. Their method of selection for prefetching (choosing the n most probable items, where n is a parameter of their method) differs from the threshold based method we use.
Reference: [3] <author> Pei Cao, Edward W. Felten, Anna R. Karlin, and Kai Li. </author> <title> A Study of Integrated Prefetching and Caching Strategies. </title> <booktitle> Proceedings of the 1995 SIG-METRICS. ACM, </booktitle> <year> 1995. </year>
Reference-contexts: Finally, such an application specific method would not be able to make use of any relationships that exist across applications (e.g. between make and cc). Cao et al. <ref> [3] </ref> have approached this problem from a unique perspective, by examining what characteristics an off-line prefetching technique would require to be successful. 5 Future Work The following items are intended as areas of future exploration: Trie memory requirements Our current implementation was designed as a proof of concept without concern for
Reference: [4] <author> K. M. Curewitz, P. Krishnan, and J. S. Vitter. </author> <title> Practical Prefetching via Data Compression. </title> <journal> SIG-MOD Record, </journal> <volume> 22(2) </volume> <pages> 257-266. </pages> <publisher> ACM, </publisher> <month> Jun. </month> <year> 1993. </year>
Reference-contexts: The use of data compression modeling techniques for prefetch-ing in operating systems was first investigated Vitter, Krishnan and Curewitz <ref> [19, 4] </ref>. It was their work that inspired us to adapt PPM's modeling techniques, to track file system events instead of characters of an alphabet. <p> They prove that such techniques converge to an optimal online algorithm. They go on to test this work for memory access patterns <ref> [4] </ref> in an object oriented database and a CAD system. They deal with the large model size by paging portions of the model to secondary memory, and show that this can be done with negligible effect on performance. <p> Both Griffioen [5] and Curewitz <ref> [4] </ref> successfully ad 7 dress this issue. We hope to expand on their work, in conjunction with data compression techniques. We expect that our predictive cache will see significantly improved efficiency after we implement a fixed limit on the number of children each node can have. <p> Such a forward-looking prediction would enable us to avoid cache misses for the sequence BAC as well as BCA. Modifications to the prefetching algorithm - Curewitz's <ref> [4] </ref> approach to prefetching is to select the n most likely items (where n is a parameter of the model).
Reference: [5] <author> J. Griffioen and R. Appleton. </author> <title> Reducing File System Latency Using a Predictive Approach. </title> <booktitle> Proceedings of USENIX Summer Technical Conference, </booktitle> <pages> pages 197-207. </pages> <publisher> USENIX, </publisher> <month> Jun. </month> <year> 1994. </year>
Reference-contexts: Both Griffioen <ref> [5] </ref> and Curewitz [4] successfully ad 7 dress this issue. We hope to expand on their work, in conjunction with data compression techniques. We expect that our predictive cache will see significantly improved efficiency after we implement a fixed limit on the number of children each node can have.
Reference: [6] <author> J. Griffioen and R. Appleton. </author> <title> Performance Measurements of Automatic Prefetching. </title> <booktitle> Parallel and Distributed Computing Systems, </booktitle> <pages> pages 165-170. </pages> <publisher> IEEE, </publisher> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: Figure 2 shows how our hit ratio varied as the threshold settings ranged from a probability of 0.001 to 0.25. From this graph we can see that a setting in the region of 0.05 to 0.1 will offer the best performance. From Griffioen and Appleton's work <ref> [6] </ref> and our earlier work, we expected this setting to be quite low. Even so, it is surprising that such an aggressive prefetch threshold produced the best results. To explain this, we first consider that each trace is comprised of over 10,000 distinct files. <p> Lastly, the problem domain we examine (file systems access patterns) differs from that which they have worked under (virtual memory access patterns). 6 Within the domain of file systems, Griffioen and Appleton <ref> [6] </ref> have worked to developed a predictive model that for each file accumulates frequency counts of all files that are accessed in a window after the first. These frequency counts are then used to drive a prefetching cache.
Reference: [7] <author> J. H. Howard, M. L. Kazar, S. G. Menees, D. A. Nichols, M. Satyanarayanan, R. N. Sidebotham, and M. J. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81. </pages> <publisher> ACM, </publisher> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: Whole file caching has been used effectively in several distributed file systems <ref> [7, 8, 18] </ref>. In a mobile environment the possibility of temporary disconnection and the availability of local storage make whole file caching the best option. We split the file system traces into eight 24 hour periods lettered A through H.
Reference: [8] <author> J. J. Kistler and M. Satyanarayanan. </author> <title> Disconnected Operation in the Coda File System. </title> <booktitle> Proceedings of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pages 213-25. </pages> <publisher> ACM, </publisher> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Whole file caching has been used effectively in several distributed file systems <ref> [7, 8, 18] </ref>. In a mobile environment the possibility of temporary disconnection and the availability of local storage make whole file caching the best option. We split the file system traces into eight 24 hour periods lettered A through H.
Reference: [9] <author> D. E. Knuth. </author> <title> Sorting and Searching, </title> <booktitle> volume 3 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: Our model tracks previous file system events through a finite multi-order context modeling technique adapted from the data compression technique Prediction by Partial Match (PPM) [2]. This model uses a trie <ref> [9] </ref> to store sequences of previous file system events and the number of times they have occurred. Our selector examines the most recently seen sequences and the counts of the events that have followed them to determine likely next events. <p> The nature of a context model, where one set of contexts is built from the previous set, makes it well suited for a trie 1 <ref> [9] </ref>, where the children of each node indicate the events that have followed the sequence represented by that node. A resulting property of this trie is that the frequency count for each current context is equal to the sum of its children's counts plus one 2 .
Reference: [10] <author> P. Krishnan. </author> <title> Online Prediction Algorithms for Databases and Operating Systems. </title> <type> PhD thesis. </type> <institution> Brown University, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Additionally they suggest that such methods could have great success within a variety of other applications such as hyper-text. Our work adapts PPM in a different manner. We avoid the use of vine pointers <ref> [2, 10] </ref> and instead keep an array of the current contexts. Their method of selection for prefetching (choosing the n most probable items, where n is a parameter of their method) differs from the threshold based method we use.
Reference: [11] <author> G. Kuenning. </author> <title> The Design of the Seer Predictive Caching System. </title> <booktitle> Workshop on Mobile Computing Systems and Applications, </booktitle> <pages> pages 37-43. </pages> <publisher> IEEE, </publisher> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: Their work has concluded that such a predictive caching system has promise to be effective for a wide variety of environments. Kuenning has extended this work <ref> [11] </ref>, developing the concept of a Semantic Distance, and using this to determine groupings of files that should be kept on local disks for mobile computers.
Reference: [12] <author> G. Kuenning, G. J. Popek, and P. Reiher. </author> <title> An Analysis of Trace Data for Predictive File Caching in Mobile Computing. </title> <booktitle> Proceedings of USENIX Summer Technical Conference, </booktitle> <pages> pages 291-303. </pages> <publisher> USENIX, </publisher> <year> 1994. </year>
Reference-contexts: Their prediction model differs from ours in that they look at more than just the next event. Additionally, they only consider a first order model. Nevertheless, the method of prefetch selection based on a probability threshold was first presented in their work. Kuenning, Popek, and Reiher <ref> [12] </ref> have done extensive work analyzing the behavior of file system requests for various mobile environments with the intent of developing a prefetching system that would predict needed files and cache them locally.
Reference: [13] <author> G. G. Langdon and J. J. Rissanen. </author> <title> A Doubly-Adaptive File Compression Algorithm. </title> <journal> IEEE Transactions on Communications, COM-31(11):1253-1255, Nov. </journal> <volume> 83. </volume>
Reference-contexts: Additionally, in our initial implementation we have made no effort to efficiently use memory in our model. We intend to expand on methods used successfully in the compression technique DAFC <ref> [13] </ref>, to limit the 5 order, threshold settings 0.001, 0.01, 0.025, 0.05, 0.075, 0.1 and 0.25).
Reference: [14] <author> J. Ousterhout. </author> <booktitle> Why Aren't Operating Systems Getting Faster as Fast as Hardware? Proceedings of USENIX Summer Technical Conference, </booktitle> <pages> pages 247-56. </pages> <publisher> USENIX, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction With the rapid increase of processor speeds, file system latency is a critical issue in computer system performance <ref> [14] </ref>. Standard Least Recently Used (LRU) based caching techniques offer some assistance, but by ignoring any relationships that exist between file system events, they fail to make full use of the available information. We will show that many of the events in a file system are closely related.
Reference: [15] <author> H. Patterson, G. Gibson, E. Ginting, D. Stodol-sky, and J. Zelenka. </author> <title> Transparent Informed Pre-fecting. </title> <booktitle> Proceedings of 13th Symposium on Operating Systems Principles. ACM, </booktitle> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Kuenning has extended this work [11], developing the concept of a Semantic Distance, and using this to determine groupings of files that should be kept on local disks for mobile computers. Patterson, et al. <ref> [16, 15] </ref>, have modified a compiler and file system to implement a method called Transparent Informed Prefetching where applications inform the file system which files to prefetch. While this method can offer significant improvements in throughput, it is dependent on the applications ability to know its future actions.
Reference: [16] <author> H. Patterson, G. Gibson, and M. </author> <note> Satyanarayanan. </note>
Reference-contexts: Kuenning has extended this work [11], developing the concept of a Semantic Distance, and using this to determine groupings of files that should be kept on local disks for mobile computers. Patterson, et al. <ref> [16, 15] </ref>, have modified a compiler and file system to implement a method called Transparent Informed Prefetching where applications inform the file system which files to prefetch. While this method can offer significant improvements in throughput, it is dependent on the applications ability to know its future actions.
References-found: 16

