URL: http://www.cs.rpi.edu/~kaploww/update.ps
Refering-URL: http://www.cs.rpi.edu/~kaploww/research.html
Root-URL: http://www.cs.rpi.edu
Title: Impact of Memory Hierarchy on Program Partitioning and Scheduling  
Author: Wesley K. Kaplow William A. Maniatty, and Boleslaw K. Szymanski 
Address: Troy, N.Y. 12180-3590, USA  
Affiliation: Department of Computer Science Rensselaer Polytechnic Institute,  
Abstract: In this paper we present a method for determining the cache performance of the loop nests in a program. The cache-miss data are produced by simulating the loop nest execution on an architecturally parameterized cache simulator. We show that the cache-miss rates are highly non-linear with respect to the ranges of the loops, and correlate well with the performance of the loop nests on actual target machines. The cache-miss ratio is used to guide program optimizations such as loop interchange and iteration-space blocking. It can also be used to provide an estimate for the runtime of a program. Both applications are important in scheduling programs for parallel execution. Presented here are examples of program optimization for several popular processors, such as the IBM 9076 SP1, the SuperSPARC, and the Intel i860. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A Static Performance Estimator to Guide data Partitioning Descisions. </title> <booktitle> In Third ACM SIGPLAN Symposium on Principles and Practice on Parallel Programming, Williams-burg, VA, </booktitle> <pages> pages 213-223. </pages> <publisher> ACM, </publisher> <address> NY, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Gerasoulis and Yang [6] estimate the execution cost of nodes in a task graph by the number of arithmetic operations and communication operations and the time required per operation on a target. Balasundaram, et. al <ref> [1] </ref> use a training-set method to determine the cost of elementary operations on a target and then use a linear model to determine the cost of a section of code. 1.4 Organization of the paper In this paper we present a simulation based approach that can be used with a class <p> This selection requires the estimation of the execution time for the subrange assigned to each processor and communication time for necessary messages. For example in <ref> [1] </ref>, it can be seen that the most efficient partitioning scheme is dependent on the domain size. Our sample scientific numerical problem is the simple but commonly used Jacobi method for solving partial differential equations. <p> Our next effort is to integrate static performance estimates of these optimized loop constructs based on a training-set-like method <ref> [1] </ref>, adjusted for cache effects. Once this is done we will have an accurate execution time function that describes the loop. Additionally we will integrate it with a method to determine the cost of the communication requirement of the loop.
Reference: [2] <author> M. J. Clement and M. J. Quinn. </author> <title> Analytical Performance Prediction on Multicomputers. </title> <booktitle> In Proc. ACM Supercomputing, </booktitle> <address> Seattle, WA, </address> <pages> pages 886-894. </pages> <publisher> ACM, </publisher> <address> NY, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Several researchers evaluate the performance of program code sections using linear models in which the estimated time is equal to the execution frequency times Nest the cost of each operation. This cost is not adjusted for memory hierarchy effects. A performance analysis methodology given in Clement et al., <ref> [2] </ref> uses a linear model to determine program execution time with the additional consideration that all variables that are shared between processors are not cached and have a larger access time than local variables.
Reference: [3] <author> T. Fahringer. </author> <title> Automatic Cache Performance Prediction in a Parallelizing Compiler. </title> <booktitle> In Proceeding of AICA 1993, </booktitle> <address> Lecce/Italy, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Moreover, his method is applicable only to loops with constant data dependency vectors. Ferrante et al.[7] determine an upper bound for the number of distinct cache lines referenced in a program using a detailed analysis of the data dependency of array references and index expressions in loop nests. Fahringer <ref> [3] </ref> develops the notion of array access classes that are created by grouping together array references that exhibit the same spatial and temporal reuse. He applies this method successfully to loop interchange and loop distribution optimizations. <p> When the code is blocked, the reference pattern generated by the blocked traversal will not follow a traversal of a contiguously allocated Block Sizes array. The static prediction methods, such as proposed by Wolf and Lam [9] or Fahringer <ref> [3] </ref> do not take this into consideration, and therefore they are not able to choose the correct blocking factor for a loop (in particular, the method in [9] relies on use of copy optimization in which each block is copied to an auxiliary array).
Reference: [4] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for Cache and Local Memory Management by Global Program Transformation . Journal of Parallel and Distributed Computing, </title> <month> October </month> <year> 1988. </year>
Reference-contexts: Since the computational complexity remains constant, this is caused by a decrease in cache effectiveness. Iteration Loop nests are prime prospects for code optimization. Many loop restructuring optimizations influence cache performance. Examples are loop interchange, fusion, distribution, iteration-space blocking, and skewing (c.f., <ref> [15, 9, 4] </ref>) which can dramatically improve the performance of loops and therefore programs (c.f., [8, 11]). For example, Figure 1 shows the application of an iteration-space blocking optimization, which can improve performance by preserving the locality of reference independently of problem size.
Reference: [5] <author> A. Gerasoulis and T. Yang. </author> <title> Comparison of Clustering Heuristics for Scheduling Directed Acyclic Graphs on Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> (16):276-291, 1992. 
Reference: [6] <author> A. Gerasoulis and T. Yang. </author> <title> On the Granularity and Clustering of Directed Acyclic Task Graphs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 686-701, </pages> <year> 1993. </year>
Reference-contexts: A performance analysis methodology given in Clement et al., [2] uses a linear model to determine program execution time with the additional consideration that all variables that are shared between processors are not cached and have a larger access time than local variables. Gerasoulis and Yang <ref> [6] </ref> estimate the execution cost of nodes in a task graph by the number of arithmetic operations and communication operations and the time required per operation on a target.
Reference: [7] <author> J. Ferrante, V. Sarkar, W. Thrash. </author> <title> On Estimating and Enhancing Cache Effectiveness. </title> <booktitle> In Languages and Compilers for Parallel Computing, Fourth Internation Workshop, </booktitle> <address> Santa Clara, CA. </address> <publisher> Springer-Verlag, </publisher> <address> NY, </address> <month> August </month> <year> 1991. </year>
Reference: [8] <author> K. Kennedy and K. S. McKinley. </author> <title> Optimizing for Parallelism and Data Locality. </title> <booktitle> In International Conference on Supercomputing 1992, </booktitle> <address> Washington, D.C., </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Iteration Loop nests are prime prospects for code optimization. Many loop restructuring optimizations influence cache performance. Examples are loop interchange, fusion, distribution, iteration-space blocking, and skewing (c.f., [15, 9, 4]) which can dramatically improve the performance of loops and therefore programs (c.f., <ref> [8, 11] </ref>). For example, Figure 1 shows the application of an iteration-space blocking optimization, which can improve performance by preserving the locality of reference independently of problem size.
Reference: [9] <author> M. S. Lam, E. E. Rothberg, and M. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In Proc. ACM ASPLOS, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 63-74. </pages> <publisher> ACM, </publisher> <address> NY, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Since the computational complexity remains constant, this is caused by a decrease in cache effectiveness. Iteration Loop nests are prime prospects for code optimization. Many loop restructuring optimizations influence cache performance. Examples are loop interchange, fusion, distribution, iteration-space blocking, and skewing (c.f., <ref> [15, 9, 4] </ref>) which can dramatically improve the performance of loops and therefore programs (c.f., [8, 11]). For example, Figure 1 shows the application of an iteration-space blocking optimization, which can improve performance by preserving the locality of reference independently of problem size. <p> Fahringer [3] develops the notion of array access classes that are created by grouping together array references that exhibit the same spatial and temporal reuse. He applies this method successfully to loop interchange and loop distribution optimizations. Wolf and Lam <ref> [9] </ref> develop a cache model based on the number of loops carrying reuse. They classify cache misses into two groups: cross interference, which is the interference between two different variables, and self interference, caused by references to the same array. <p> Too large a factor causes a large increase in self-interference misses in the cache (the addresses currently in cache are used again in the following rows of the current block but are removed from cache by references to the subsequent elements in the current row) <ref> [9] </ref>. We can use the cache simulation to determine the effective blocking factor. <p> When the code is blocked, the reference pattern generated by the blocked traversal will not follow a traversal of a contiguously allocated Block Sizes array. The static prediction methods, such as proposed by Wolf and Lam <ref> [9] </ref> or Fahringer [3] do not take this into consideration, and therefore they are not able to choose the correct blocking factor for a loop (in particular, the method in [9] relies on use of copy optimization in which each block is copied to an auxiliary array). <p> The static prediction methods, such as proposed by Wolf and Lam <ref> [9] </ref> or Fahringer [3] do not take this into consideration, and therefore they are not able to choose the correct blocking factor for a loop (in particular, the method in [9] relies on use of copy optimization in which each block is copied to an auxiliary array). Our approach is to determine the optimum blocking factor for in-place execution.
Reference: [10] <author> C. Ozturan, B. Sinharoy, and B.K. Szymanski. </author> <title> Compiler Technology for Parallel Scientific Computation. </title> <note> to appear in Scientific Programming, </note> <year> 1994. </year>
Reference-contexts: This will result in a fully characterized program components that can be used for both compiler optimization decisions and machine architecture evaluations. This research is part of an ongoing effort to provide an architecturally based parallel program optimization tools built around the EPL <ref> [10] </ref> system.
Reference: [11] <author> A. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, Texas, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Iteration Loop nests are prime prospects for code optimization. Many loop restructuring optimizations influence cache performance. Examples are loop interchange, fusion, distribution, iteration-space blocking, and skewing (c.f., [15, 9, 4]) which can dramatically improve the performance of loops and therefore programs (c.f., <ref> [8, 11] </ref>). For example, Figure 1 shows the application of an iteration-space blocking optimization, which can improve performance by preserving the locality of reference independently of problem size. <p> Static techniques use program analysis to estimate the number of cache misses generated by a program fragment. Most of the recent work has focused, as we do, on the loop nests of a program. Porterfield <ref> [11] </ref> estimates the number of cache lines referenced by a loop, but considers only caches with unit line size. Moreover, his method is applicable only to loops with constant data dependency vectors. <p> By loop nest, we understand a perfectly nested loop structure as defined in <ref> [11] </ref>. We assume a Single Program Multiple Data (SPMD) program in which the same loop nest is executed on every processor, with each processor evaluating a subrange of the loops in the nest.
Reference: [12] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <booktitle> Research monographs in parallel and distributed computing. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA., </address> <year> 1989. </year>
Reference: [13] <author> H. S. Stone. </author> <title> High-Performance Computer Architecture. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: There are both hardware and software methods for capturing the reference traces. Once obtained, the they can be fed into an architectural simulator of the cache. The greatest drawback to this approach is that to be effective, traces have to be millions of references long <ref> [13] </ref>. Another problem, especially relevant to scientific numerical codes, is that the identity of the program components and structure that generated the address trace is lost, and therefore it is difficult to make conclusions about how to modify the source code to improve performance. <p> Each processor contains an execution unit and a memory hierarchy that includes at least one level of cache memory. As defined in <ref> [13] </ref>, a cache is the first level of memory closest to the processor. It generally has access times that are commensurate with the instruction cycle time of the processor, and is therefore several times faster than main memory access time.
Reference: [14] <author> O. Teman, C. Fricker, and W. Jalby. </author> <title> Impact of Cache Interferences on Usual Numerical Dense Loop Nests. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(8), </volume> <month> August </month> <year> 1993. </year>
Reference: [15] <author> M. E. Wolf and M. S. Lam. </author> <title> A Data Locality Optimizing Algorithm. </title> <booktitle> In Proc. ACM SIGPLAN, </booktitle> <address> Toronto, Canada. </address> <publisher> ACM, </publisher> <address> NY, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Since the computational complexity remains constant, this is caused by a decrease in cache effectiveness. Iteration Loop nests are prime prospects for code optimization. Many loop restructuring optimizations influence cache performance. Examples are loop interchange, fusion, distribution, iteration-space blocking, and skewing (c.f., <ref> [15, 9, 4] </ref>) which can dramatically improve the performance of loops and therefore programs (c.f., [8, 11]). For example, Figure 1 shows the application of an iteration-space blocking optimization, which can improve performance by preserving the locality of reference independently of problem size. <p> Correspondingly, the simulated cache hit-rates follow roughly the same pattern, where both the original and interchanged loops have a distinct fall-off at the loop range of approximately 500. Target Results for the SP1 Iteration-space blocking. One of the most important optimizations for loop nests is blocking or tiling <ref> [15] </ref>. This kind of optimization is used to increase the locality of data references during the loop execution to increase probability that these references will be already located in cache.
References-found: 15

