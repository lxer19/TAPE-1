URL: http://www.media.mit.edu/~barons/papers/AuthoringAndTranscriptionTools-AVIOS91.ps
Refering-URL: http://www.media.mit.edu/~barons/AronsAnnotatedBibliography.html
Root-URL: http://www.media.mit.edu
Title: Authoring and Transcription Tools for Speech-Based Hypermedia Systems  
Author: Barry Arons 
Keyword: Speech-Only Hypermedia  
Address: 20 Ames Street, E15-353 Cambridge MA, 02139  
Affiliation: MIT Media Laboratory  
Email: E-mail:  
Phone: Phone: +1 617-253-2245  
Web: barons@media-lab.mit.edu  
Abstract: Authoring is usually one of the most difficult parts in the design and implementation of hypertext and hypermedia systems. This problem is exacerbated if the data to be presented by the system is speech, rather than text or graphics, because of the slow and serial nature of speech. This paper provides an overview of speech-only hypermedia, discusses the difficulties associated with authoring databases for such a system, and explores a variety of techniques to assist in the authoring process. Since the introduction of Hypercard for the Macintosh, the ideas behind hypertext systems have become commonplace. The addition of graphics, audio, still images, or video to such systems is helping to create a wealth of new hypermedia applications, but few of these systems take advantage of voice input or output. To create an end-user application, the raw source material must be assembled and structured as part of authoring process. For example, the authoring of a videodisc-based hypermedia system involves selecting appropriate video segments, scripting branch points, and creating graphics [7]. These systems generally use mouse interfaces to traverse through screen-based environments. The authoring ideas discussed in this paper were developed for an experimental hyperspeech system that explores ideas of creating and navigating in a speech-only hypermedia framework (see [1] for a thorough discussion of the project). The system uses speech recognition to maneuver in a database of digitally recorded speech segments; synthetic speech is used for control information and user feedback. In this research prototype, recorded audio interviews on the future of the human interface were segmented by topic, with hypertext-style links added to connect logically related comments and ideas. The user speaks commands to hear supporting or opposing views, comments from a different speaker, or control the state of the system. This project investigates techniques for presenting speech as data, allowing a user to navigate by voice through a database of recorded speech without any visual cues. The ideas being developed can be applied to create a generalized form of interaction with unstructured speech data. Applications for such a technology include the use of recorded speech, rather than text, as a brainstorming tool or personal memory aid. A hyperspeech system would allow a user to create, organize, sort, and filter audio notes under circumstances where a traditional graphical interface would not be practical (e.g., while driving) or appropriate (e.g., for someone who is visually impaired). Speech interfaces are particularly attractive for handheld computers without keyboards or large displays. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Arons. Hyperspeech: </author> <title> Navigating in speech-only hypermedia. </title> <booktitle> In Hypertext '91, </booktitle> <pages> pages 133-146. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference: [2] <author> B. Arons. </author> <title> A review of the cocktail party effect. </title> <journal> Journal of the American Voice I/O Society, </journal> <volume> 12 </volume> <pages> 35-50, </pages> <month> July </month> <year> 1992. </year>
Reference: [3] <author> J. Conklin. </author> <title> Hypertext: an introduction and survey. </title> <journal> IEEE Computer, </journal> <volume> 20(9) </volume> <pages> 17-41, </pages> <month> September </month> <year> 1987. </year>
Reference: [4] <author> W. Foulke and T. G. Sticht. </author> <title> Review of research on the intelligibility and comprehension of accelerated speech. </title> <journal> Psychological Bulletin, </journal> <volume> 72 </volume> <pages> 50-62, </pages> <year> 1969. </year>
Reference: [5] <author> W. W. Gaver. </author> <title> Auditory icons: Using sound in computer interfaces. </title> <journal> Human-Computer Interaction, </journal> <volume> 2 </volume> <pages> 167-177, </pages> <year> 1989. </year>
Reference: [6] <author> A. Hu. </author> <title> Automatic emphasis detection in fluent speech with transcription. Unpublished M.I.T. </title> <type> Bachelor's thesis, </type> <month> May </month> <year> 1987. </year>
Reference-contexts: However, there are a variety of techniques that can be employed to completely automate the hyperspeech authoring process. If an accurate transcript is available, it is possible to automatically correlate the text with syllabic units detected in the recording <ref> [6, 9] </ref> 3 . For a hyperspeech database, this type of tool would allow the hypermedia author to segment the transcripts in a text-based editor, and then create the audio file correspondences as an automated post-process.
Reference: [7] <author> J. S. Huntley and S. Alessi. </author> <title> Videodisc authoring tools: Evaluating products and a process. </title> <booktitle> Optical Information Systems, </booktitle> <pages> pages 259-281, </pages> <year> 1987. </year>
Reference: [8] <author> N. Maxemchuk. </author> <title> An experimental speech storage and editing facility. </title> <journal> Bell System Technical Journal, </journal> <volume> 59(8) </volume> <pages> 1383-1395, </pages> <month> October </month> <year> 1980. </year>
Reference: [9] <author> P. Mermelstein. </author> <title> Automatic segmentation of speech into syllabic units. </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> 58(4) </volume> <pages> 880-883, </pages> <month> October </month> <year> 1975. </year>
Reference-contexts: However, there are a variety of techniques that can be employed to completely automate the hyperspeech authoring process. If an accurate transcript is available, it is possible to automatically correlate the text with syllabic units detected in the recording <ref> [6, 9] </ref> 3 . For a hyperspeech database, this type of tool would allow the hypermedia author to segment the transcripts in a text-based editor, and then create the audio file correspondences as an automated post-process.
Reference: [10] <author> M. J. Muller and J. E. Daniel. </author> <title> Toward a definition of voice documents. </title> <booktitle> In Proceedings of COIS '90, </booktitle> <year> 1990. </year>
Reference: [11] <author> K. M. Pitman. CREF: </author> <title> An editing facility for managing structured text. A. I. </title> <type> Memo 829, </type> <institution> Massachusetts Institute of Technology, </institution> <month> February </month> <year> 1985. </year>
Reference: [12] <author> A. Richaume, F. Steenkeste, P. Lecocq, and Y. Moschetto. </author> <title> Intelligibility and comprehension of French normal, accelerated, and compressed speech. </title> <booktitle> In IEEE Engineering in Medicine and Biology Society 10th Annual International Conference, </booktitle> <pages> pages 1531-1532, </pages> <year> 1988. </year>
Reference: [13] <author> T. A. Salthouse. </author> <title> The skill of typing. </title> <publisher> Scientific American, </publisher> <pages> pages 128-135, </pages> <month> February </month> <year> 1984. </year>
Reference: [14] <author> C. Schmandt and B. Arons. </author> <title> A conversational telephone messaging system. </title> <journal> IEEE Transactions on Consumer Electronics, </journal> <volume> CE-30(3):xxi-xxiv, </volume> <month> August </month> <year> 1984. </year>
Reference-contexts: However, if the process that gathers the speech data asks very specific questions, it is possible to automatically segment and organize recorded messages by semantic content <ref> [14] </ref>. If the questions are properly structured (and the interviewees are cooperative), the bulk of the nodes in the hyperspeech database can be automatically generated. This technique is particularly powerful for hyperspeech authoring, as it not only creates the content of the database, but can link the nodes as well.
Reference: [15] <author> C. Schmandt and B. Arons. </author> <title> Desktop audio. Unix Review, </title> <month> October </month> <year> 1989. </year>
Reference: [16] <author> T. G. Aguirre Smith and N. C. Pincever. </author> <title> Parsing movies in context. </title> <booktitle> In Proceedings of the Summer 1991 USENIX Conference, </booktitle> <pages> pages 157-168. </pages> <publisher> Usenix, </publisher> <year> 1991. </year>
Reference-contexts: Through continued work in the area of managing and navigating within audio-only databases, it may someday be practical to author such databases through a voice interface, or completely automate this part of the authoring task. 3 Related ideas for exploring the content of movies sound tracks are described in <ref> [16] </ref>. Acknowledgements Chris Schmandt and Lisa Stifelman assisted in the editing of this paper. Mike Hawley and Walter Bender provided useful technical information and insight. This work was funded by Apple Computer and Sun Microsystems.
References-found: 16

