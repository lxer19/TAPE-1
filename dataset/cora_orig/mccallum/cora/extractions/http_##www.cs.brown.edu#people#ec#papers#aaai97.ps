URL: http://www.cs.brown.edu/people/ec/papers/aaai97.ps
Refering-URL: http://www.cs.brown.edu/people/ec/
Root-URL: 
Email: ec@cs.brown.edu  
Title: Statistical Parsing with a Context-free Grammar and Word Statistics  
Author: Eugene Charniak 
Affiliation: Department of Computer Science, Brown University  
Abstract: We describe a parsing system based upon a language model for English that is, in turn, based upon assigning probabilities to possible parses for a sentence. This model is used in a parsing system by finding the parse for the sentence with the highest probability. This system outperforms previous schemes. As this is the third in a series of parsers by different authors that are similar enough to invite detailed comparisons but different enough to give rise to different levels of performance, we also report on some experiments designed to identify what aspects of these systems best explain their relative performance. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Caraballo, S. and Charniak, E. </author> <title> Figures of merit for best-first probabilistic chart parsing. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing. </booktitle> <year> 1996, </year> <pages> 127-132. </pages>
Reference-contexts: We parse a new (test) sentence s by first obtaining a set of parses using relatively standard context-free chart-parsing technology. No attempt is made to find all possible parses for s. Rather, techniques described in <ref> [1] </ref> are used to select constituents that promise to contribute to the most probable parses, where parse probability is measured according to the simple probabilistic context-free grammar distribution p (r j t).
Reference: 2. <author> Charniak, E. </author> <title> Expected-Frequency Interpolation. </title> <institution> Department of Computer Science, Brown University, </institution> <type> Technical Report CS96-37, </type> <year> 1996. </year>
Reference-contexts: given the amount of training data used, of how often one would expect the particular concurrence of events, e.g., given the amount of training data used, how many times we should see "profits" as the head of an np under an s headed by "rose." Our method is described in <ref> [2] </ref> and is not discussed further here. The other aspect of Equation 2 that is not standard deleted interpolation is the term ^p (s j c h ; t; l).
Reference: 3. <author> Charniak, E. </author> <title> Tree-bank grammars. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <year> 1996, </year> <pages> 1031-1036. </pages>
Reference-contexts: The Algorithm We now consider in more detail how the probability model just described is turned into a parser. Before parsing we train the parser using the pre-parsed training corpus. First we read a context-free grammar (a tree-bank grammar) off the corpus, as described in <ref> [3] </ref>. We then collect the statistics used to compute the empirically observed probability distributions needed for Equations 2 and 3. We parse a new (test) sentence s by first obtaining a set of parses using relatively standard context-free chart-parsing technology. <p> Balanced against this, however, is the comparative lack of coverage of the tree-bank grammar we use. The standard assumption about tree-bank grammars is that they lack coverage because many uncommon grammar rules are not encountered in the particular corpus used to create the grammar. As noted in <ref> [3] </ref>, this problem is not as bad as people expect, and the tests therein showed that lack of coverage was not a significant problem. However, in [3] parsing is done using only tag sequence information, which, as the PCFG results in Figure 2 show, is a poor system. <p> As noted in <ref> [3] </ref>, this problem is not as bad as people expect, and the tests therein showed that lack of coverage was not a significant problem. However, in [3] parsing is done using only tag sequence information, which, as the PCFG results in Figure 2 show, is a poor system. We estimate that lack of coverage due to the use of a tree-bank grammar lowers performance somewhere between .5% and 1% in both precision and recall.
Reference: 4. <author> Collins, M. J. </author> <title> A new statistical parser based on bigram lexical dependencies. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the ACL. </booktitle> <year> 1996. </year>
Reference-contexts: The ability to do this is the reason we first compute a set of parses and only later apply the full probability model to them. sciously fitting the model to that test sample. This arrangement was chosen because it is exactly what was used in <ref> [4] </ref> and [5]. The next section compares our results to theirs. After training we parsed the testing corpus using five versions of our system. In each case the program pulled out the most probable parse according to the probability model under consideration. <p> We give results according to seven figures of merit: LR (labeled recall | the number of correct nonterminal labeled constituents divided by the number of such constituents in the tree-bank version) LR2 (LR, but using the slightly idiosyncratic definition of correctness used in <ref> [4] </ref>), LP (labeled precision | the number of correct non-terminal labeled constituents divided by the number of such constituents produced by the parser), LP2 (LP, but using the definition of correctness from [4]), CB (the average number of cross-brackets per sentence), 0CB (percentage of sentences with zero cross-brackets), and 2CB (percentage <p> such constituents in the tree-bank version) LR2 (LR, but using the slightly idiosyncratic definition of correctness used in <ref> [4] </ref>), LP (labeled precision | the number of correct non-terminal labeled constituents divided by the number of such constituents produced by the parser), LP2 (LP, but using the definition of correctness from [4]), CB (the average number of cross-brackets per sentence), 0CB (percentage of sentences with zero cross-brackets), and 2CB (percentage of sentences with 2 cross-brackets). <p> In particular, we designed our experiments to conform exactly to those performed on two previous statistical parsing systems that also used the Penn Wall Street Journal Treebank to train parsers, those of Magerman [5] and Collins <ref> [4] </ref>. Thus our training and testing data are exactly the same sets of sentences used in this previous work, as are the testing measurements. In this section we describe these earlier parsers, and then describe experiments designed to shed light on the performance differences among the three systems.
Reference: 5. <author> Magerman, D. M. </author> <title> Statistical decision-tree models for parsing. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics. </booktitle> <year> 1995, </year> <pages> 276-283. </pages>
Reference-contexts: The ability to do this is the reason we first compute a set of parses and only later apply the full probability model to them. sciously fitting the model to that test sample. This arrangement was chosen because it is exactly what was used in [4] and <ref> [5] </ref>. The next section compares our results to theirs. After training we parsed the testing corpus using five versions of our system. In each case the program pulled out the most probable parse according to the probability model under consideration. <p> In particular, we designed our experiments to conform exactly to those performed on two previous statistical parsing systems that also used the Penn Wall Street Journal Treebank to train parsers, those of Magerman <ref> [5] </ref> and Collins [4]. Thus our training and testing data are exactly the same sets of sentences used in this previous work, as are the testing measurements.
Reference: 6. <author> Marcus, M. P., Santorini, B. and Marcinkiewicz, M. A. </author> <title> Building a large annotated corpus of English: the Penn treebank. </title> <booktitle> Computational Linguistics 19 (1993), </booktitle> <pages> 313-330. </pages>
Reference-contexts: Naturally there are also drawbacks. Creating the requisite training corpus, or tree-bank, is a Herculean task, so there are not many to choose from. (In this paper we use the Penn Wall Street Journal Treebank <ref> [6] </ref>.) Thus the variety of parse types generated by such systems is limited. At the same time, the dearth of training corpora has at least one positive effect.
Reference: 7. <author> Pereira, F., Tishby, N. and Lee, L. </author> <title> Distributional clustering of English words. </title> <booktitle> In Proceedings of the Association for Computational Linguistics. ACL, </booktitle> <year> 1993. </year>
Reference-contexts: We do not describe the clustering method here except to note that it uses a scheme something like that in <ref> [7] </ref>.
References-found: 7

