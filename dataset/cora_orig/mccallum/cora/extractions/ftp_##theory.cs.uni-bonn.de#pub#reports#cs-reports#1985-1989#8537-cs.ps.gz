URL: ftp://theory.cs.uni-bonn.de/pub/reports/cs-reports/1985-1989/8537-cs.ps.gz
Refering-URL: http://cs.uni-bonn.de/info5/publications/CS-1985-1989-en.html
Root-URL: http://cs.uni-bonn.de
Title: VC Dimension and Learnability of Sparse Polynomials and Rational Functions  
Author: Marek Karpinski Thorsten Werther 
Note: Supported in part by Leibniz Center for Research in Computer Science, by the DFG Grant KA 673/2-1, and by the SERC Grant GR-E 68297.  
Affiliation: Department of Computer Science, University of Bonn, and International Computer Science Institute, Berke-ley, California.  Department of Computer Science, University of Bonn, and International Computer Science Institute, Berke ley, California.  
Abstract: We prove upper and lower bounds on the VC dimension of sparse univariate polynomials over reals, and apply these results to prove uniform learnability of sparse polynomials and rational functions. As another application we solve an open problem of Vapnik ([Vap-nik 82]) on uniform approximation of the general regression functions, a central problem of computational statistics (cf. [Vapnik 82]), p. 256). 
Abstract-found: 1
Intro-found: 1
Reference: [BT 88] <author> Ben-Or, M., Tiwari, P., </author> <title> A Deterministic Algorithm for Sparse Multivariate Polynomial Interpolation, </title> <booktitle> Proc. 20 th ACM STOC (1988), </booktitle> <pages> pp. 301-309. </pages>
Reference-contexts: These bounds are based on the Vapnik-Chervonenkis (VC) dimension of a class C. For the corresponding problem of interpolation of polynomials over fields of characteristic zero cf. [GK 87], <ref> [BT 88] </ref> and over finite fileds cf. [GKS 88]. Definition 1. For a concept class C on X and for S ae X , let C (S) be the set of subsets T of S such that T = S " c for some concept c in C.
Reference: [BEHW 87] <author> Blumer, A., Ehrenfeucht, A., Hausler, D., Warmuth, M., </author> <title> Learnability and the Vapnik-Chervonenkis Dimension, </title> <institution> UC Santa Cruz, </institution> <type> Tech. Rep. </type> <institution> UCSC-CRL-87-20, </institution> <year> 1987. </year>
Reference-contexts: The goal of the learning algorithm is to compute a good uniform approximation of the target concept, with high probability. Upper and lower bounds on the sample complexity for learning various concept classes have been given in [VC 71], <ref> [BEHW 87] </ref>, [Fl 89]. These bounds are based on the Vapnik-Chervonenkis (VC) dimension of a class C. For the corresponding problem of interpolation of polynomials over fields of characteristic zero cf. [GK 87], [BT 88] and over finite fileds cf. [GKS 88]. Definition 1. <p> This paper is organized as follows. Section 2 gives lower and upper bounds on the VC dimension of sparse polynomials, proving that the class of sparse polynomials is uniformly learnable (cf. <ref> [BEHW 87] </ref>). Furthermore, upper and lower bounds on the sample complexity for learning sparse polynomials can be derived ([Fl 89], [Ha 89]). The more complete discussion of underlying algorithms and their analysis will be given in a full version of this paper.
Reference: [BT 89] <author> Borodin, A., Tiwari, P., </author> <title> On the Decidability of Sparse Univariate Polynomial Interpolation, </title> <type> IBM Research Report RC 14923 (#66763), </type> <month> Sep. </month> <year> 1989. </year>
Reference: [Fl 89] <author> Floyd, S., </author> <title> Space-Bounded Learning and the Vapnik-Chervonenkis Dimension, </title> <type> Manuscript, </type> <institution> International Computer Science Institute, Berkeley, </institution> <year> 1989. </year>
Reference-contexts: The goal of the learning algorithm is to compute a good uniform approximation of the target concept, with high probability. Upper and lower bounds on the sample complexity for learning various concept classes have been given in [VC 71], [BEHW 87], <ref> [Fl 89] </ref>. These bounds are based on the Vapnik-Chervonenkis (VC) dimension of a class C. For the corresponding problem of interpolation of polynomials over fields of characteristic zero cf. [GK 87], [BT 88] and over finite fileds cf. [GKS 88]. Definition 1.
Reference: [GK 87] <author> Grigoriev, D.Yu., Karpinski, M., </author> <title> The Matching Problem for Bipartite Graphs with Polynomially Bounded Permannets Is in N C, </title> <booktitle> Proc. 28 th IEEE FOCS (1987), </booktitle> <pages> pp. 166-172. </pages>
Reference-contexts: These bounds are based on the Vapnik-Chervonenkis (VC) dimension of a class C. For the corresponding problem of interpolation of polynomials over fields of characteristic zero cf. <ref> [GK 87] </ref>, [BT 88] and over finite fileds cf. [GKS 88]. Definition 1. For a concept class C on X and for S ae X , let C (S) be the set of subsets T of S such that T = S " c for some concept c in C.
Reference: [GKS 88] <author> Grigoriev, D.Yu., Karpinski, M., Singer, M., </author> <title> Fast Parallel Algorithms for Sparse Multivariate Polynomial Interpolation over Finite Fields, </title> <institution> University of Bonn, </institution> <note> Research Report No. 8523-CS, </note> <year> 1988. </year>
Reference-contexts: These bounds are based on the Vapnik-Chervonenkis (VC) dimension of a class C. For the corresponding problem of interpolation of polynomials over fields of characteristic zero cf. [GK 87], [BT 88] and over finite fileds cf. <ref> [GKS 88] </ref>. Definition 1. For a concept class C on X and for S ae X , let C (S) be the set of subsets T of S such that T = S " c for some concept c in C.
Reference: [Ha 89] <author> Hausler, D., </author> <title> Generalizing the PAC Model: Sample Size Bounds From Metric Dimension-based Uniform Convergence Results, </title> <booktitle> Proc. 30 th IEEE FOCS (1989), </booktitle> <pages> pp. 40-45. </pages>
Reference-contexts: We denote the VC dimension of the set P t in this context by VC (P t ). Several generalizations of the standard PAC-model have been considered (cf. <ref> [Ha 89] </ref>, [Vap 89]) in order to deal with real-valued functions instead with the indicator functions implied by the class P t as above. We will use the notion proposed in [Ha 89]. <p> Several generalizations of the standard PAC-model have been considered (cf. <ref> [Ha 89] </ref>, [Vap 89]) in order to deal with real-valued functions instead with the indicator functions implied by the class P t as above. We will use the notion proposed in [Ha 89]. Here for each f 2 P t , an indicator function I (f ) is defined by I (f )(x; y; *) = 1 if jf (x) yj * 0 otherwise ; where * is any positive real number. <p> Section 2 gives lower and upper bounds on the VC dimension of sparse polynomials, proving that the class of sparse polynomials is uniformly learnable (cf. [BEHW 87]). Furthermore, upper and lower bounds on the sample complexity for learning sparse polynomials can be derived ([Fl 89], <ref> [Ha 89] </ref>). The more complete discussion of underlying algorithms and their analysis will be given in a full version of this paper. Section 3 generalizes these bounds for the *-VC dimension of t-sparse polynomials.
Reference: [KV 89] <author> Kearns, M., Valiant, L.G., </author> <title> Cryptographic Limitations on Learning Boolean For--mulae and Finite Automata, </title> <booktitle> Proc. 21 st ACM STOC (1989), </booktitle> <pages> pp. 433-444. </pages>
Reference: [Po 84] <author> Pollard, D., </author> <title> Convergence of Stochastic Processes, </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: The most important case of this problem is the approximation of polynomial regression (cf. [Vap 82], pp. 254-258). For underlying definitions and terminology see <ref> [Po 84] </ref>, [Vap 82]. The classical scheme of approximating polynomial regression, which involves the determination of the true degree n of regression and the expansion in a system of n orthogonal polynomials of degree 1; 2; : : :; n, can be successfully implemented only when large samples are used.
Reference: [Val 84] <author> Valiant, L.G., </author> <title> A Theory of the Learnable, </title> <journal> Comm. ACM, </journal> <volume> 27(11), </volume> <year> 1984, </year> <pages> pp. 1134-1142. </pages>
Reference-contexts: 1 Introduction The paper studies the problem of computational identification (learnability) of sparse real polynomials and rational functions. In <ref> [Val 84] </ref>, Valiant introduced a model of learning concepts from examples taken from an unknown distribution. In this model, a concept c from a class C is a subset of an instance space X. Let C be a class of concepts from X .
Reference: [Vap 82] <author> Vapnik, V.N., </author> <title> Estimation of Dependences Based on Empirical Data, </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: In this situation we consider f0; 1g-valued indicator functions (<ref> [Vap 82] </ref>). Hence, in the context of the Problem of Pattern Recognition [Vap 82], we define examples (x; y) from the instance space X = (IR; IR) to be labeled positive if the point (x; y) lies 'above' the t-sparse polynomial f (f 2 P t the unknown target concept), and vice versa, i.e. &lt; (x; y); + &gt; () y f (x) <p> The most important case of this problem is the approximation of polynomial regression (cf. <ref> [Vap 82] </ref>, pp. 254-258). For underlying definitions and terminology see [Po 84], [Vap 82]. <p> The most important case of this problem is the approximation of polynomial regression (cf. <ref> [Vap 82] </ref>, pp. 254-258). For underlying definitions and terminology see [Po 84], [Vap 82]. The classical scheme of approximating polynomial regression, which involves the determination of the true degree n of regression and the expansion in a system of n orthogonal polynomials of degree 1; 2; : : :; n, can be successfully implemented only when large samples are used. <p> Then, by Lemma 10 there are two polynomials with at least d=12 intersections and with Lemma 6 we conclude that d 12 (4t 1) + 3 = 48t 9. 2 Now we state our main theorem, solving the problem of Vapnik <ref> [Vap 82] </ref>. Theorem 12. The polynomial regression can be approximated uniformly by sparse polynomials for small samples. 2 Acknowledgements We thank Manuel Blum, Allan Borodin, Sally Floyd, Les Valiant and Manfred Warmuth for the number of interesting conversations.
Reference: [Vap 89] <author> Vapnik, V.N., </author> <title> Inductive Principles of the Search for Empirical Dependences (Methods Based on Weak Convergence of Probability Measures), </title> <booktitle> Proc. of the 2 nd Workshop on Computational Learning Theory, </booktitle> <year> 1989. </year>
Reference-contexts: We denote the VC dimension of the set P t in this context by VC (P t ). Several generalizations of the standard PAC-model have been considered (cf. [Ha 89], <ref> [Vap 89] </ref>) in order to deal with real-valued functions instead with the indicator functions implied by the class P t as above. We will use the notion proposed in [Ha 89].
Reference: [VC 71] <author> Vapnik, V.N., Chervonenkis, </author> <title> A.Y., On the Uniform Convergence of Relative Frequencies of Events and their Probabilities, </title> <journal> Th. Prob. and its Appl., </journal> <volume> 16(2), </volume> <year> 1971, </year> <pages> pp. 264-280. </pages>
Reference-contexts: The goal of the learning algorithm is to compute a good uniform approximation of the target concept, with high probability. Upper and lower bounds on the sample complexity for learning various concept classes have been given in <ref> [VC 71] </ref>, [BEHW 87], [Fl 89]. These bounds are based on the Vapnik-Chervonenkis (VC) dimension of a class C. For the corresponding problem of interpolation of polynomials over fields of characteristic zero cf. [GK 87], [BT 88] and over finite fileds cf. [GKS 88]. Definition 1.
References-found: 13

