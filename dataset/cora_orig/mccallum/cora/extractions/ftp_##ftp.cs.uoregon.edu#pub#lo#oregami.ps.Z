URL: ftp://ftp.cs.uoregon.edu/pub/lo/oregami.ps.Z
Refering-URL: http://www.cs.uoregon.edu/research/DistributedComputing/archive.html
Root-URL: http://www.cs.uoregon.edu
Title: OREGAMI: Tools for Mapping Parallel Computations to Parallel Architectures  
Author: Virginia M. Lo Sanjay Rajopadhye Samik Gupta, David Keldsen, Moataz A. Mohamed Bill Nitzberg Jan Arne Telle, and Xiaoxiong Zhong 
Keyword: mapping, routing, embedding, task assignment, regular parallel computations, parallel programming environments  
Address: 97403-1202  
Affiliation: Dept. of Computer and Information Science University of Oregon Eugene, Oregon  
Note: Appears in International Journal of Parallel Programming, Volume 20, Number  This research is sponsored by the NSF grant MIP91-08528 and the Oregon Advanced Computing Institute. Partially supported by NSF grant CCR-8808532 Partially supported by NSF grant MIP-8802454  
Email: lo@cs.uoregon.edu  
Phone: 503-686-4408  
Date: 3, June 1991.  
Abstract: The OREGAMI project involves the design, implementation, and testing of algorithms for mapping parallel computations to message-passing parallel architectures. OREGAMI addresses the mapping problem by exploiting regularity and by allowing the user to guide and evaluate mapping decisions made by OREGAMI's efficient combinatorial mapping algorithms. OREGAMI's approach to mapping is based on a new graph theoretic model of parallel computation called the Temporal Communication Graph. The OREGAMI software tools includes three components: (1) LaRCS is a graph description language which allows the user to describe regularity in the communication topology as well as the temporal communication behavior (the pattern of message-passing over time). (2) MAPPER is our library of mapping algorithms which utilize information provided by LaRCS to perform contraction, embedding, and routing. (3) METRICS is an interactive graphics tool for display and analysis of mappings. This paper gives an overview of the OREGAMI project, the software tools, and OREGAMI's mapping algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. B. Akers and B. Krishnamurthy. </author> <title> A group-theoretic model for symmetric interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-38(4):555-566, </volume> <month> April </month> <year> 1989. </year>
Reference-contexts: We note that this approach to contraction will be especially useful for data parallel algorithms which are inherently node symmetric. In addition, many interesting interconnection networks, such as the butterfly, hypercube, cube-connected cycles, are themselves based on Cayley graphs that have an underlying group structure <ref> [1] </ref>. Hopefully, this will also be an aid in the embedding and routing steps of the mapping. Example: We will use the 8-node perfect broadcast algorithm to illustrate the operation of Algorithm Cayley-Contract. (See Figure 8).
Reference: [2] <author> D. A. Bailey and J. E. Cuny. </author> <title> Graph grammar based specification of interconnection structures for massively parallel computation. </title> <booktitle> In Proceedings of the Third International Workshop on Graph Grammars, </booktitle> <pages> pages 73-85, </pages> <year> 1987. </year>
Reference-contexts: The LaRCS language bears similarities to a number of graph description languages or configuration languages which have been developed for a variety of purposes in the area of parallel and distributed computing. These include formal approaches such as edge grammars [5] and graph grammars [20], <ref> [2] </ref>, as well as more practical languages such as GDL [4], CONIC [28], GARP [20], and ParaGraph [3]. Of these, LaRCS, edge grammars, and GDL were designed specifically to address the mapping problem. LaRCS is unique in its ability to describe the temporal behavior of parallel computations.
Reference: [3] <author> D. A. Bailey and J. E. </author> <title> Cuny. </title> <booktitle> Visual extensions to parallel programming languages, </booktitle> <pages> pages 17-36. </pages> <publisher> MIT Press, </publisher> <month> August </month> <year> 1989. </year>
Reference-contexts: These include formal approaches such as edge grammars [5] and graph grammars [20], [2], as well as more practical languages such as GDL [4], CONIC [28], GARP [20], and ParaGraph <ref> [3] </ref>. Of these, LaRCS, edge grammars, and GDL were designed specifically to address the mapping problem. LaRCS is unique in its ability to describe the temporal behavior of parallel computations. <p> Examples of such systems include Prep-P [4, 6], Poker [38, 40], ORCA [39, 16], the Parallel Programming Environments Project <ref> [3] </ref>, and TIPS [44]. OREGAMI and Prep-P focus on the mapping problem. The other projects address the broader issues of program design and development. 7 3 OREGAMI System Overview This section describes the OREGAMI software tools and traces the use of these tools using the n-body algorithm as an example.
Reference: [4] <author> F. Berman. </author> <title> Experience with an automatic solution to the mapping problem. </title> <booktitle> In The Characteristics of Parallel Algorithms, </booktitle> <pages> pages 307-334. </pages> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: The LaRCS description of the n-body algorithm and further details about LaRCS are described in the next section. 2.2 Related Research As an integrated set of tools for mapping, OREGAMI is similar in many respects to Francine Berman's Prep-P System <ref> [4] </ref> [6]. Both Prep-P and OREGAMI provide a graph description language for describing the communication structure of the parallel computation. Prep-P's GDL language is based on the static task graph model of parallel computation and is embedded within the system's parallel programming language XX. <p> These include formal approaches such as edge grammars [5] and graph grammars [20], [2], as well as more practical languages such as GDL <ref> [4] </ref>, CONIC [28], GARP [20], and ParaGraph [3]. Of these, LaRCS, edge grammars, and GDL were designed specifically to address the mapping problem. LaRCS is unique in its ability to describe the temporal behavior of parallel computations. <p> A primitive form of phase expressions was introduced by [29]. &gt;From the viewpoint of support environments for parallel programming, OREGAMI belongs to the family of systems which take a process-oriented view of parallel computation based on explicit message-passing. Examples of such systems include Prep-P <ref> [4, 6] </ref>, Poker [38, 40], ORCA [39, 16], the Parallel Programming Environments Project [3], and TIPS [44]. OREGAMI and Prep-P focus on the mapping problem.
Reference: [5] <author> F. Berman and L. Snyder. </author> <title> On mapping parallel algorithms into parallel architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 4(5) </volume> <pages> 439-458, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: A large body of theoretical work on graph embeddings has yielded 1:1 mappings tailored for specific regular graphs; some of the more recent results are given in [32]. Many of these algorithms have or will be included in the OREGAMI library. Edge grammars <ref> [5] </ref> use formal language techniques to address the contraction of families of task graphs. Stone and Bokhari [41], [8] use a variety of graph theoretic algorithms to address task assignment for structures including trees, chains, and arbitrary task graphs. <p> The LaRCS language bears similarities to a number of graph description languages or configuration languages which have been developed for a variety of purposes in the area of parallel and distributed computing. These include formal approaches such as edge grammars <ref> [5] </ref> and graph grammars [20], [2], as well as more practical languages such as GDL [4], CONIC [28], GARP [20], and ParaGraph [3]. Of these, LaRCS, edge grammars, and GDL were designed specifically to address the mapping problem.
Reference: [6] <author> F. Berman and B. Stramm. Prep-p: </author> <title> Evolution and overview. </title> <type> Technical Report CS89-158, </type> <institution> Dept. of Computer Science, University of California at San Diego, </institution> <year> 1989. </year>
Reference-contexts: The TCG model is compatible with both of these existing models. Thus algorithms for static task assignment such as [8] <ref> [6] </ref> [23], [35] and scheduling algorithms for precedence-constrained graphs such as [30] [9] can be applied to the TCG model. <p> The LaRCS description of the n-body algorithm and further details about LaRCS are described in the next section. 2.2 Related Research As an integrated set of tools for mapping, OREGAMI is similar in many respects to Francine Berman's Prep-P System [4] <ref> [6] </ref>. Both Prep-P and OREGAMI provide a graph description language for describing the communication structure of the parallel computation. Prep-P's GDL language is based on the static task graph model of parallel computation and is embedded within the system's parallel programming language XX. <p> A primitive form of phase expressions was introduced by [29]. &gt;From the viewpoint of support environments for parallel programming, OREGAMI belongs to the family of systems which take a process-oriented view of parallel computation based on explicit message-passing. Examples of such systems include Prep-P <ref> [4, 6] </ref>, Poker [38, 40], ORCA [39, 16], the Parallel Programming Environments Project [3], and TIPS [44]. OREGAMI and Prep-P focus on the mapping problem.
Reference: [7] <author> B.P. Bianchini and J.P. Shen. </author> <title> Interprocessor traffic scheduling algorithm for multiprocessor networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4):396-409, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: Work in the area of systolic arrays has yielded elegant mapping techniques for computations whose data dependencies can be expressed as affine recurrences [31] [12]. Related work in the area of application-dependent routing includes [19] and <ref> [7] </ref>. Our mapping algorithms build on these foundations and utilize techniques from group theory, graph theory, coding theory, and linear algebra. Other approaches to the mapping problem include search algorithms, linear programming, and clustering algorithms. These latter techniques typically do not exploit the regularity of the task graph.
Reference: [8] <author> S. H. Bokhari. </author> <title> Assignment Problems in Parallel and Distributed Computing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1987. </year>
Reference-contexts: The TCG model is compatible with both of these existing models. Thus algorithms for static task assignment such as <ref> [8] </ref> [6] [23], [35] and scheduling algorithms for precedence-constrained graphs such as [30] [9] can be applied to the TCG model. <p> Many of these algorithms have or will be included in the OREGAMI library. Edge grammars [5] use formal language techniques to address the contraction of families of task graphs. Stone and Bokhari [41], <ref> [8] </ref> use a variety of graph theoretic algorithms to address task assignment for structures including trees, chains, and arbitrary task graphs. Work in the area of systolic arrays has yielded elegant mapping techniques for computations whose data dependencies can be expressed as affine recurrences [31] [12].
Reference: [9] <author> J.C. Browne. </author> <title> Framework for formulation and analysis of parallel computation structures. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 1-9, </pages> <year> 1986. </year>
Reference-contexts: The TCG model is compatible with both of these existing models. Thus algorithms for static task assignment such as [8] [6] [23], [35] and scheduling algorithms for precedence-constrained graphs such as [30] <ref> [9] </ref> can be applied to the TCG model. The contribution of the TCG is that it augments these two models with the ability to explicitly capture regularity, allowing the development of specialized mapping and scheduling algorithms to exploit this regularity.
Reference: [10] <author> J.C. Browne. </author> <title> Code: A unified approach to parallel programming. </title> <journal> IEEE Software, </journal> <volume> 6(4) </volume> <pages> 10-19, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: As discussed in the introduction, OREGAMI is currently a front-end mapping tool and only generates symbolic mapping directives. OREGAMI is also related to a number of mapping and scheduling systems that utilize the classic DAG model of parallel computation such as CODE/ROPE <ref> [10] </ref>, TaskGrapher [14], Polychronopoulos [30] and Sakar [36]. These systems differ from OREGAMI, Prep-P, and TMAP because they are designed for the purpose of parallelizing sequential code.
Reference: [11] <author> R. H. Campbell and A. N. Habermann. </author> <title> The Specification of Process Synchronization by Path Expressions, </title> <booktitle> volume 16, </booktitle> <pages> pages 89-102. </pages> <publisher> Springer-Verlag, </publisher> <year> 1974. </year>
Reference-contexts: Identification of these synchrony sets can be used to refine the routing algorithm and to produce local scheduling directives for each processor that ensure synchronous execution of the tasks in each set. The scheduling directives can be expressed in a notation similar to path expressions <ref> [11] </ref> that specify the allowable ways to multiplex the tasks assigned to a given processor. Testing: Our experiments thus far have focused on testing the performance of individual mapping algorithms.
Reference: [12] <author> Marina C. Chen. </author> <title> A design methodology for synthesizing parallel algorithms and architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 3(6) </volume> <pages> 461-491, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Stone and Bokhari [41], [8] use a variety of graph theoretic algorithms to address task assignment for structures including trees, chains, and arbitrary task graphs. Work in the area of systolic arrays has yielded elegant mapping techniques for computations whose data dependencies can be expressed as affine recurrences [31] <ref> [12] </ref>. Related work in the area of application-dependent routing includes [19] and [7]. Our mapping algorithms build on these foundations and utilize techniques from group theory, graph theory, coding theory, and linear algebra. Other approaches to the mapping problem include search algorithms, linear programming, and clustering algorithms.
Reference: [13] <author> M. H. Coffin. </author> <title> Par: An approach to architecture-independent parallel programming. </title> <type> Technical Report TR90-28, </type> <institution> Dept. of Computer Science, University of Arizona, </institution> <month> August </month> <year> 1990. </year> <month> 33 </month>
Reference-contexts: OREGAMI is designed for use as a front-end mapping tool in conjunction with parallel programming languages that support explicit message-passing, such as OCCAM, C*, Dino [33], Par <ref> [13] </ref>, and C and Fortran with communication extensions. The underlying architecture is assumed to consist of homogeneous processors connected by some regular network topology, with current focus on the hypercube, mesh, and deBruijn topologies. Routing technologies supported by OREGAMI include store-and-forward, virtual cut-through, and wormhole routing.
Reference: [14] <author> H. El-Rewini and T.G. Lewis. </author> <title> Scheduling parallel program tasks onto arbitrary target machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 138-153, </pages> <year> 1990. </year>
Reference-contexts: As discussed in the introduction, OREGAMI is currently a front-end mapping tool and only generates symbolic mapping directives. OREGAMI is also related to a number of mapping and scheduling systems that utilize the classic DAG model of parallel computation such as CODE/ROPE [10], TaskGrapher <ref> [14] </ref>, Polychronopoulos [30] and Sakar [36]. These systems differ from OREGAMI, Prep-P, and TMAP because they are designed for the purpose of parallelizing sequential code.
Reference: [15] <author> M. Fellows. </author> <title> Problem corner. </title> <journal> Contemporary Mathematics, </journal> <volume> 89 </volume> <pages> 187-188, </pages> <year> 1989. </year>
Reference-contexts: No polynomial-time algorithm is known that recognizes Cayley graphs based on an adjacency matrix representation <ref> [15] </ref>. However, with the aid of the LaRCS communication phases, for a task graph T with n nodes and m edges, in time O (nm), our algorithm either reports that T does not satisfy the criteria or produces a contraction as described above.
Reference: [16] <author> W. G. Griswold, G. A. Harrison, D. Notkin, and L. Snyder. </author> <title> Port ensembles: a communication abstraction for nonshared memory parallel programming. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, University of Washington, </institution> <year> 1989. </year>
Reference-contexts: Examples of such systems include Prep-P [4, 6], Poker [38, 40], ORCA <ref> [39, 16] </ref>, the Parallel Programming Environments Project [3], and TIPS [44]. OREGAMI and Prep-P focus on the mapping problem.
Reference: [17] <author> Y. Han and R. Finkel. </author> <title> An optimal scheme for disseminating information. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 198-203, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: For example, many parallel algorithms use a specific tree topology to aggregate results when a variety of alternate communication topologies will suffice (any spanning tree or the perfect broadcast ring of <ref> [17] </ref>). We would like to automatically select the aggregate topology that is `compatible' with the topologies of other phases in the 31 computation. Finally, we will continue to add to the library of `canned' mappings for nameable task graphs.
Reference: [18] <author> S. L. Johnsson. </author> <title> Communication in network architectures. In VLSI and Parallel Computation, page page 290. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: Each contracted edge represents an identical number of messages, thus the contraction is perfectly load balanced 20 Class Mapping Technique Complexity Performance I binomial tree Gray code O (1) no contention to hypercube labeling avg. dilation 1 <ref> [18] </ref> [25] I binomial tree Gray code O (1) no contention to mesh reflection avg. dilation 1.2 [25] I binomial tree combinatorial/ O (1) no contention to deBruijn shift register avg. dilation 2 [48] sequences I ring to enumeration of O (1) gives precise no. deBruijn necklaces/shift necklaces [34] register sequences
Reference: [19] <author> D.D. Kandlur and K.G. Shin. </author> <title> Traffic routing for multi-computer networks with virtual cut-through capability,. </title> <booktitle> In Preceedings of the 10th International Conference on Distributed Computer Systems,, </booktitle> <pages> pages 398-405, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Work in the area of systolic arrays has yielded elegant mapping techniques for computations whose data dependencies can be expressed as affine recurrences [31] [12]. Related work in the area of application-dependent routing includes <ref> [19] </ref> and [7]. Our mapping algorithms build on these foundations and utilize techniques from group theory, graph theory, coding theory, and linear algebra. Other approaches to the mapping problem include search algorithms, linear programming, and clustering algorithms. These latter techniques typically do not exploit the regularity of the task graph.
Reference: [20] <author> Simon M. Kaplan and Gail E. Kaiser. Garp: </author> <title> Graph abstractions for concurrent programming. </title> <editor> In H. Ganzinger, editor, </editor> <booktitle> European Symposium on Programming, volume 300 of Lecture Notes in Computer Science, </booktitle> <pages> pages 191-205, </pages> <address> Heidelberg, March 1988. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The LaRCS language bears similarities to a number of graph description languages or configuration languages which have been developed for a variety of purposes in the area of parallel and distributed computing. These include formal approaches such as edge grammars [5] and graph grammars <ref> [20] </ref>, [2], as well as more practical languages such as GDL [4], CONIC [28], GARP [20], and ParaGraph [3]. Of these, LaRCS, edge grammars, and GDL were designed specifically to address the mapping problem. LaRCS is unique in its ability to describe the temporal behavior of parallel computations. <p> These include formal approaches such as edge grammars [5] and graph grammars <ref> [20] </ref>, [2], as well as more practical languages such as GDL [4], CONIC [28], GARP [20], and ParaGraph [3]. Of these, LaRCS, edge grammars, and GDL were designed specifically to address the mapping problem. LaRCS is unique in its ability to describe the temporal behavior of parallel computations.
Reference: [21] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: Weights associated with the nodes and edges can be used to represent computation and communication costs, respectively. Note that the TCG also can be viewed as a graph theoretic representation of Lamport's process-time diagrams <ref> [21] </ref> augmented with weights and colors. The coloring of the Lamport process-time graph is described in [24] and involves the identification of logically synchronous communication and computation phases as described in the next section. for the Cosmic Cube [37].
Reference: [22] <author> V. M. Lo. </author> <title> Algorithms for static task assignment and symmetric contraction in distributed computing systems. </title> <booktitle> In Proceedings IEEE 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 239-244, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: no. deBruijn necklaces/shift necklaces [34] register sequences II algorithms expressible recurrences O (1) optimal as affine recurrences theory [31] (work in progress) II Cayley/node symm. group theory O (V E) optimal contraction (Cayley graphs) load balancing [27] III arbitrary maximum weight O (EVlogV) minimizes IPC contraction matching algorithm subject to <ref> [22] </ref> load balancing; III store-and-forward maximal matching O (HDM 3 ) minimizes routing algorithm contention [26] III wormhole routing iteratively O (jEjM 2 W 2 d) minimize reduce contention; contention; [46] deadlock-free III wormhole routing shortest path O (M (nN + log M )) minimize algorithm contention; [47] deadlock-free III ecube <p> The data used included four types of task graphs, with the best performance exhibited by graphs characterized by regular communication topologies (90.0% of the contractions found were optimal). Details of Algorithm MWM-Contraction, proofs of optimality, and simulation results can be found in <ref> [22] </ref>.
Reference: [23] <author> V. M. Lo. </author> <title> Heuristic algorithms for task assignment in distributed systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-37(11):1384-1397, </volume> <month> November </month> <year> 1988. </year>
Reference-contexts: The TCG model is compatible with both of these existing models. Thus algorithms for static task assignment such as [8] [6] <ref> [23] </ref>, [35] and scheduling algorithms for precedence-constrained graphs such as [30] [9] can be applied to the TCG model.
Reference: [24] <author> V. M. Lo. </author> <title> Temporal communication graphs: Lamport's process-time graphs augmented for the purpose of mapping and scheduling. </title> <type> Technical Report CIS-TR-92-05, </type> <institution> University of Oregon, </institution> <year> 1992. </year> <note> Submitted to Journal of Parallel and Distributed Computing. </note>
Reference-contexts: A formal definition of the TCG is given in <ref> [24] </ref> which also defines the formal semantics of LaRCS in terms of the TCG. Consider each individual process comprising the parallel computation. <p> Note that the TCG also can be viewed as a graph theoretic representation of Lamport's process-time diagrams [21] augmented with weights and colors. The coloring of the Lamport process-time graph is described in <ref> [24] </ref> and involves the identification of logically synchronous communication and computation phases as described in the next section. for the Cosmic Cube [37]. This algorithm will be used as an example later in the paper to illustrate the use of the OREGAMI mapping tools. <p> The TCG is also capable of modeling computations of arbitrary granularity, characterized by irregular and asynchronous communication. We note that the TCG does not model nondeterministic computations and dynamically spawned tasks. More information about the TCG model and its use in parallel programming environments can be found in <ref> [24] </ref>. 4 The n-body problem requires determining the equilibrium of n bodies in space (where n is odd) under the action of a gravitational or electrostatic, etc. field. <p> Only expressions derived by a finite application of these rules are phase expressions. A precise definition of the semantics of phase expressions is given in <ref> [24] </ref>. 12 3.1.1 The Benefits of LaRCS for Mapping LaRCS plays a critical role in OREGAMI by (1) providing information about the regular structure of the parallel computation through the comphase declarations and the phase expression; and (2) by serving as an efficient representation of families of regular computation graphs whose
Reference: [25] <author> V. M. Lo, S. Rajopadhye, S. Gupta, D. Keldsen, M. A. Mohamed, and J. Telle. </author> <title> Mapping divide-and-conquer algorithms to parallel architectures. </title> <booktitle> In Proceedings IEEE 1990 International Conference on Parallel Processing, </booktitle> <pages> pages III:128-135, </pages> <month> August </month> <year> 1990. </year> <note> Also available as University of Oregon Technical Report CIS-TR-89-19. 34 </note>
Reference-contexts: These mappings have constant time complexity because they are precomputed. They are proven to be optimal with respect to contention and we prove bounds on the average dilation (see Table II.) Space limitations prevents us from discussing the mappings in this paper; a formal treatment can be found in <ref> [25] </ref> and [48]. 4.2 Contraction of Cayley Node Symmetric Task Graphs We have developed a contraction algorithm for a subset of node symmetric task graphs called Cayley graphs which is made feasible by the information contained in the LaRCS comphase declarations. <p> Each contracted edge represents an identical number of messages, thus the contraction is perfectly load balanced 20 Class Mapping Technique Complexity Performance I binomial tree Gray code O (1) no contention to hypercube labeling avg. dilation 1 [18] <ref> [25] </ref> I binomial tree Gray code O (1) no contention to mesh reflection avg. dilation 1.2 [25] I binomial tree combinatorial/ O (1) no contention to deBruijn shift register avg. dilation 2 [48] sequences I ring to enumeration of O (1) gives precise no. deBruijn necklaces/shift necklaces [34] register sequences II <p> an identical number of messages, thus the contraction is perfectly load balanced 20 Class Mapping Technique Complexity Performance I binomial tree Gray code O (1) no contention to hypercube labeling avg. dilation 1 [18] <ref> [25] </ref> I binomial tree Gray code O (1) no contention to mesh reflection avg. dilation 1.2 [25] I binomial tree combinatorial/ O (1) no contention to deBruijn shift register avg. dilation 2 [48] sequences I ring to enumeration of O (1) gives precise no. deBruijn necklaces/shift necklaces [34] register sequences II algorithms expressible recurrences O (1) optimal as affine recurrences theory [31] (work in progress) II Cayley/node
Reference: [26] <author> V. M. Lo, S. Rajopadhye, S. Gupta, D. Keldsen, M. A. Mohamed, and J. Telle. OREGAMI: </author> <title> Software tools for mapping parallel algorithms to parallel architectures. </title> <booktitle> In Proceedings 1990 International Conference on Parallel Processing, </booktitle> <pages> pages II:88-92, </pages> <month> August </month> <year> 1990. </year> <note> Updated version available as University of Oregon Technical Report CIS-TR-89-18a and will appear in the International Journal of Parallel Programming. </note>
Reference-contexts: affine recurrences theory [31] (work in progress) II Cayley/node symm. group theory O (V E) optimal contraction (Cayley graphs) load balancing [27] III arbitrary maximum weight O (EVlogV) minimizes IPC contraction matching algorithm subject to [22] load balancing; III store-and-forward maximal matching O (HDM 3 ) minimizes routing algorithm contention <ref> [26] </ref> III wormhole routing iteratively O (jEjM 2 W 2 d) minimize reduce contention; contention; [46] deadlock-free III wormhole routing shortest path O (M (nN + log M )) minimize algorithm contention; [47] deadlock-free III ecube routing Gray code O (1) deadlock-free III X-Y routing x then y O (1) deadlock-free
Reference: [27] <author> V. M. Lo, S. Rajopadhye, M. A. Mohamed, S. Gupta, B. Nitzberg, J. A. Telle, and X. X. Zhong. LaRCS: </author> <title> A language for describing parallel computations for the purpose of mapping. </title> <type> Technical Report CIS-TR-90-16a, </type> <institution> University of Oregon, </institution> <year> 1990. </year> <note> Updated version to appear in IEEE Transactions on Parallel and Distributed Systems. </note>
Reference-contexts: avg. dilation 2 [48] sequences I ring to enumeration of O (1) gives precise no. deBruijn necklaces/shift necklaces [34] register sequences II algorithms expressible recurrences O (1) optimal as affine recurrences theory [31] (work in progress) II Cayley/node symm. group theory O (V E) optimal contraction (Cayley graphs) load balancing <ref> [27] </ref> III arbitrary maximum weight O (EVlogV) minimizes IPC contraction matching algorithm subject to [22] load balancing; III store-and-forward maximal matching O (HDM 3 ) minimizes routing algorithm contention [26] III wormhole routing iteratively O (jEjM 2 W 2 d) minimize reduce contention; contention; [46] deadlock-free III wormhole routing shortest path
Reference: [28] <author> J. Magee, J. Kramer, and M. Sloman. </author> <title> Constructing distributed systems in conic. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-15(6):663-675, </volume> <month> June </month> <year> 1989. </year>
Reference-contexts: These include formal approaches such as edge grammars [5] and graph grammars [20], [2], as well as more practical languages such as GDL [4], CONIC <ref> [28] </ref>, GARP [20], and ParaGraph [3]. Of these, LaRCS, edge grammars, and GDL were designed specifically to address the mapping problem. LaRCS is unique in its ability to describe the temporal behavior of parallel computations.
Reference: [29] <author> P. A. Nelson and L. Snyder. </author> <title> Programming paradigms for nonshared memory parallel computers. </title> <booktitle> In The Characteristics of Parallel Algorithms, </booktitle> <pages> pages 3-20. </pages> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Of these, LaRCS, edge grammars, and GDL were designed specifically to address the mapping problem. LaRCS is unique in its ability to describe the temporal behavior of parallel computations. A primitive form of phase expressions was introduced by <ref> [29] </ref>. &gt;From the viewpoint of support environments for parallel programming, OREGAMI belongs to the family of systems which take a process-oriented view of parallel computation based on explicit message-passing.
Reference: [30] <author> C. D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: As discussed earlier, the TCG can be seen as a hybrid of the two predominant models of parallel computation: the static task graph model of Stone [41], and the precedence-constrained (DAG) model <ref> [30] </ref> used in multiprocessor scheduling and in the parallelization of sequential code. Task assignment and scheduling research utilizing these two models has more or less followed disjoint paths over the past two decades, in that techniques and algorithms developed for one model have not been applicable to the other. <p> The TCG model is compatible with both of these existing models. Thus algorithms for static task assignment such as [8] [6] [23], [35] and scheduling algorithms for precedence-constrained graphs such as <ref> [30] </ref> [9] can be applied to the TCG model. The contribution of the TCG is that it augments these two models with the ability to explicitly capture regularity, allowing the development of specialized mapping and scheduling algorithms to exploit this regularity. <p> As discussed in the introduction, OREGAMI is currently a front-end mapping tool and only generates symbolic mapping directives. OREGAMI is also related to a number of mapping and scheduling systems that utilize the classic DAG model of parallel computation such as CODE/ROPE [10], TaskGrapher [14], Polychronopoulos <ref> [30] </ref> and Sakar [36]. These systems differ from OREGAMI, Prep-P, and TMAP because they are designed for the purpose of parallelizing sequential code.
Reference: [31] <author> Sanjay V. Rajopadhye and Richard M. Fujimoto. </author> <title> Synthesizing systolic arrays from recurrence equations. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 163-189, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Stone and Bokhari [41], [8] use a variety of graph theoretic algorithms to address task assignment for structures including trees, chains, and arbitrary task graphs. Work in the area of systolic arrays has yielded elegant mapping techniques for computations whose data dependencies can be expressed as affine recurrences <ref> [31] </ref> [12]. Related work in the area of application-dependent routing includes [19] and [7]. Our mapping algorithms build on these foundations and utilize techniques from group theory, graph theory, coding theory, and linear algebra. Other approaches to the mapping problem include search algorithms, linear programming, and clustering algorithms. <p> mesh reflection avg. dilation 1.2 [25] I binomial tree combinatorial/ O (1) no contention to deBruijn shift register avg. dilation 2 [48] sequences I ring to enumeration of O (1) gives precise no. deBruijn necklaces/shift necklaces [34] register sequences II algorithms expressible recurrences O (1) optimal as affine recurrences theory <ref> [31] </ref> (work in progress) II Cayley/node symm. group theory O (V E) optimal contraction (Cayley graphs) load balancing [27] III arbitrary maximum weight O (EVlogV) minimizes IPC contraction matching algorithm subject to [22] load balancing; III store-and-forward maximal matching O (HDM 3 ) minimizes routing algorithm contention [26] III wormhole routing
Reference: [32] <author> A. L. Rosenberg. </author> <title> Graph embeddings 1988: Recent breakthroughs new directions. </title> <type> Technical Report 88-28, </type> <institution> University of Massachusetts at Amherst, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: A large body of theoretical work on graph embeddings has yielded 1:1 mappings tailored for specific regular graphs; some of the more recent results are given in <ref> [32] </ref>. Many of these algorithms have or will be included in the OREGAMI library. Edge grammars [5] use formal language techniques to address the contraction of families of task graphs.
Reference: [33] <author> M. Rosing, R. B. Schnabel, and R.P. Weaver. </author> <title> The dino parallel programming language. </title> <type> Technical Report CU-CS-457-90, </type> <institution> Dept. of Computer Science, University of Colorado at Boulder, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: OREGAMI is designed for use as a front-end mapping tool in conjunction with parallel programming languages that support explicit message-passing, such as OCCAM, C*, Dino <ref> [33] </ref>, Par [13], and C and Fortran with communication extensions. The underlying architecture is assumed to consist of homogeneous processors connected by some regular network topology, with current focus on the hypercube, mesh, and deBruijn topologies. Routing technologies supported by OREGAMI include store-and-forward, virtual cut-through, and wormhole routing.
Reference: [34] <author> R. Rowley and B. Bose. </author> <title> On necklaces in shu*e-exchange and de bruijn networks. </title> <booktitle> In Proceedings International Conference on Parallel Processing, </booktitle> <pages> pages I:347-350, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: dilation 1 [18] [25] I binomial tree Gray code O (1) no contention to mesh reflection avg. dilation 1.2 [25] I binomial tree combinatorial/ O (1) no contention to deBruijn shift register avg. dilation 2 [48] sequences I ring to enumeration of O (1) gives precise no. deBruijn necklaces/shift necklaces <ref> [34] </ref> register sequences II algorithms expressible recurrences O (1) optimal as affine recurrences theory [31] (work in progress) II Cayley/node symm. group theory O (V E) optimal contraction (Cayley graphs) load balancing [27] III arbitrary maximum weight O (EVlogV) minimizes IPC contraction matching algorithm subject to [22] load balancing; III store-and-forward
Reference: [35] <author> P. Sadayappan, F. Ercal, and J. Ramanujam. </author> <title> Clustering partitioning approaches to mapping parallel programs onto a hypercube. </title> <journal> Parallel Computing, </journal> <volume> 13 </volume> <pages> 1-16, </pages> <year> 1990. </year>
Reference-contexts: The TCG model is compatible with both of these existing models. Thus algorithms for static task assignment such as [8] [6] [23], <ref> [35] </ref> and scheduling algorithms for precedence-constrained graphs such as [30] [9] can be applied to the TCG model. The contribution of the TCG is that it augments these two models with the ability to explicitly capture regularity, allowing the development of specialized mapping and scheduling algorithms to exploit this regularity.
Reference: [36] <author> V. Sakar. </author> <title> Partitioning and scheduling parallel programs for execution on multiprocessors. </title> <type> Technical report, Ph.d. Thesis, </type> <institution> Dept. of Computer Science, Stanford University, </institution> <year> 1987. </year>
Reference-contexts: As discussed in the introduction, OREGAMI is currently a front-end mapping tool and only generates symbolic mapping directives. OREGAMI is also related to a number of mapping and scheduling systems that utilize the classic DAG model of parallel computation such as CODE/ROPE [10], TaskGrapher [14], Polychronopoulos [30] and Sakar <ref> [36] </ref>. These systems differ from OREGAMI, Prep-P, and TMAP because they are designed for the purpose of parallelizing sequential code. The OREGAMI mapping algorithms take the approach of many researchers who have attacked the mapping problem by exploiting the regularity found in the computation graph and/or the interconnection network.
Reference: [37] <author> C. L. Seitz. </author> <title> The cosmic cube. </title> <journal> Communications of the ACM, </journal> <volume> 28(1) </volume> <pages> 22-33, </pages> <month> January </month> <year> 1985. </year> <month> 35 </month>
Reference-contexts: The coloring of the Lamport process-time graph is described in [24] and involves the identification of logically synchronous communication and computation phases as described in the next section. for the Cosmic Cube <ref> [37] </ref>. This algorithm will be used as an example later in the paper to illustrate the use of the OREGAMI mapping tools.
Reference: [38] <author> L. Snyder. </author> <title> Introduction to the configurable, highly parallel computer. </title> <booktitle> Computer, </booktitle> <address> 15(1):47--56, </address> <month> January </month> <year> 1982. </year>
Reference-contexts: The LaRCS specification is independent of any specific parallel programming language and is coded separately by the programmer. Prep-P is a fully integrated system: Prep-P mappings are targeted for the CHiP reconfigurable parallel architecture <ref> [38] </ref> and Prep-P currently generates code that runs on the CHiP simulator known 2 In reality, when the program executes, the timing of logically synchronous activities may not be synchronous with respect to real time, due to effects such as the hardware characteristics of the execution environment and the multiplexing of <p> A primitive form of phase expressions was introduced by [29]. &gt;From the viewpoint of support environments for parallel programming, OREGAMI belongs to the family of systems which take a process-oriented view of parallel computation based on explicit message-passing. Examples of such systems include Prep-P [4, 6], Poker <ref> [38, 40] </ref>, ORCA [39, 16], the Parallel Programming Environments Project [3], and TIPS [44]. OREGAMI and Prep-P focus on the mapping problem.
Reference: [39] <author> L. Snyder. </author> <booktitle> The XYZ abstraction levels of Poker-like languages, </booktitle> <pages> pages 470-489. </pages> <publisher> MIT Press, </publisher> <month> August </month> <year> 1989. </year>
Reference-contexts: Examples of such systems include Prep-P [4, 6], Poker [38, 40], ORCA <ref> [39, 16] </ref>, the Parallel Programming Environments Project [3], and TIPS [44]. OREGAMI and Prep-P focus on the mapping problem.
Reference: [40] <author> L. Snyder and D. Socha. </author> <title> Poker on the cosmic cube: the first retargetable parallel programming language and environment. </title> <booktitle> In Proceedings 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 628-635, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: A primitive form of phase expressions was introduced by [29]. &gt;From the viewpoint of support environments for parallel programming, OREGAMI belongs to the family of systems which take a process-oriented view of parallel computation based on explicit message-passing. Examples of such systems include Prep-P [4, 6], Poker <ref> [38, 40] </ref>, ORCA [39, 16], the Parallel Programming Environments Project [3], and TIPS [44]. OREGAMI and Prep-P focus on the mapping problem.
Reference: [41] <author> H. S. Stone. </author> <title> Multiprocessor scheduling with the aid of network flow algorithms. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-3(1):85-93, </volume> <month> January </month> <year> 1977. </year>
Reference-contexts: Both the TCG and its forerunner, the classic static task graph of Stone <ref> [41] </ref>, are designed for systems in which the programmer designs his or her program as a set of parallel processes that communicate through explicit message passing. The identity of all of the processes is known at compile time, and all processes exist throughout the lifetime of the parallel computation. <p> This algorithm will be used as an example later in the paper to illustrate the use of the OREGAMI mapping tools. As discussed earlier, the TCG can be seen as a hybrid of the two predominant models of parallel computation: the static task graph model of Stone <ref> [41] </ref>, and the precedence-constrained (DAG) model [30] used in multiprocessor scheduling and in the parallelization of sequential code. <p> Many of these algorithms have or will be included in the OREGAMI library. Edge grammars [5] use formal language techniques to address the contraction of families of task graphs. Stone and Bokhari <ref> [41] </ref>, [8] use a variety of graph theoretic algorithms to address task assignment for structures including trees, chains, and arbitrary task graphs. Work in the area of systolic arrays has yielded elegant mapping techniques for computations whose data dependencies can be expressed as affine recurrences [31] [12].
Reference: [42] <author> J. Vuillemin. </author> <title> A data structure for manipulating priority queues. </title> <journal> Communications of the ACM, </journal> <volume> 21(4) </volume> <pages> 309-315, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: A formal treatment of these algorithms and extensive performance evaluation is found in the referenced papers. 4.1 Canned mapping of binomial tree divide and conquer algorithms Our contribution to the library of canned mappings is a set of mappings of the binomial tree <ref> [42] </ref> to the mesh and the deBruijn networks. As stated earlier, the input to OREGAMI for these canned mappings is simply the name of the computation graph and the architecture, plus parameters to instantiate the sizes of the graphs.
Reference: [43] <author> C. L. Seitz W. J. Dally. </author> <title> Deadlock-free message routing in multiprocessor interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36(5) </volume> <pages> 547-553, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Algorithms WORM1-Route and WORM2-Route are described in [46] and [47]. Algorithm MM/SF-Route was tested for the hypercube by comparing its performance with random routing and the ecube routing algorithm <ref> [43] </ref>. The performance metric used in these experiments was maximum contention, i.e. maximum number of communication edges assigned to a single link. The experiments were performed on parallel computations from the OREGAMI test suite.
Reference: [44] <author> A. Wagner, S. Chanson, N. Goldstein, J. Jiang, H. Larsen, and H. Sreekantaswamy. </author> <title> Tips: Transputer-based interactive parallelizing system. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, University of British Columbia, </institution> <year> 1990. </year>
Reference-contexts: OREGAMI also is related to the TMAP mapping tool, a component of the TIPS transputer-based interactive parallelizing system <ref> [44] </ref>. TMAP is an adaptation of Prep-P for the transputer architecture. As discussed in the introduction, OREGAMI is currently a front-end mapping tool and only generates symbolic mapping directives. <p> Examples of such systems include Prep-P [4, 6], Poker [38, 40], ORCA [39, 16], the Parallel Programming Environments Project [3], and TIPS <ref> [44] </ref>. OREGAMI and Prep-P focus on the mapping problem. The other projects address the broader issues of program design and development. 7 3 OREGAMI System Overview This section describes the OREGAMI software tools and traces the use of these tools using the n-body algorithm as an example.
Reference: [45] <author> H. Wielandt. </author> <title> Finite Permutation Groups. </title> <publisher> Academic Press, </publisher> <year> 1964. </year>
Reference-contexts: The algorithm is rooted in group theory <ref> [45] </ref> and yields a symmetric contraction in which there are an identical number of nodes per cluster and with each cluster having exactly one incoming and one outgoing 'contracted edge' for each communication phase.
Reference: [46] <author> X. X. Zhong and V. M. Lo. </author> <title> Application specific deadlock free wormhole routing on multi-computers. </title> <type> Technical Report CIS-TR-92-03, </type> <institution> University of Oregon, </institution> <month> June </month> <year> 1992. </year> <note> To appear in PARLE 92. </note>
Reference-contexts: optimal contraction (Cayley graphs) load balancing [27] III arbitrary maximum weight O (EVlogV) minimizes IPC contraction matching algorithm subject to [22] load balancing; III store-and-forward maximal matching O (HDM 3 ) minimizes routing algorithm contention [26] III wormhole routing iteratively O (jEjM 2 W 2 d) minimize reduce contention; contention; <ref> [46] </ref> deadlock-free III wormhole routing shortest path O (M (nN + log M )) minimize algorithm contention; [47] deadlock-free III ecube routing Gray code O (1) deadlock-free III X-Y routing x then y O (1) deadlock-free Table II: OREGAMI mapping algorithms 21 8 fi 8 mesh (c) Mapping of 16 node <p> However, we note that this algorithm can be used for systems which utilize wormwhole routing schemes by including deadlock avoidance techniques such as virtual 28 network partitioning. Algorithms WORM1-Route and WORM2-Route are described in <ref> [46] </ref> and [47]. Algorithm MM/SF-Route was tested for the hypercube by comparing its performance with random routing and the ecube routing algorithm [43]. The performance metric used in these experiments was maximum contention, i.e. maximum number of communication edges assigned to a single link.
Reference: [47] <author> X. X. Zhong and V. M. Lo. </author> <title> An efficient heuristic for application specific routing on mesh connected multiprocessors. </title> <type> Technical Report CIS-TR-92-04, </type> <institution> University of Oregon, </institution> <year> 1992. </year> <note> To appear in 1992 International Conference on Parallel Processing. </note>
Reference-contexts: algorithm subject to [22] load balancing; III store-and-forward maximal matching O (HDM 3 ) minimizes routing algorithm contention [26] III wormhole routing iteratively O (jEjM 2 W 2 d) minimize reduce contention; contention; [46] deadlock-free III wormhole routing shortest path O (M (nN + log M )) minimize algorithm contention; <ref> [47] </ref> deadlock-free III ecube routing Gray code O (1) deadlock-free III X-Y routing x then y O (1) deadlock-free Table II: OREGAMI mapping algorithms 21 8 fi 8 mesh (c) Mapping of 16 node binomial tree to 16 node deBruijn 22 with respect to both computation and communication. <p> However, we note that this algorithm can be used for systems which utilize wormwhole routing schemes by including deadlock avoidance techniques such as virtual 28 network partitioning. Algorithms WORM1-Route and WORM2-Route are described in [46] and <ref> [47] </ref>. Algorithm MM/SF-Route was tested for the hypercube by comparing its performance with random routing and the ecube routing algorithm [43]. The performance metric used in these experiments was maximum contention, i.e. maximum number of communication edges assigned to a single link.
Reference: [48] <author> X. X. Zhong, S. Rajopadhye, and V. M. Lo. </author> <title> Parallel implementation of divide-and-conquer algorithms on binary de bruijn networks. </title> <type> Technical Report CIS-TR-91-21, </type> <institution> University of Oregon, </institution> <year> 1991. </year> <note> To appear in 6th International Parallel Processing Symposium. 36 </note>
Reference-contexts: They are proven to be optimal with respect to contention and we prove bounds on the average dilation (see Table II.) Space limitations prevents us from discussing the mappings in this paper; a formal treatment can be found in [25] and <ref> [48] </ref>. 4.2 Contraction of Cayley Node Symmetric Task Graphs We have developed a contraction algorithm for a subset of node symmetric task graphs called Cayley graphs which is made feasible by the information contained in the LaRCS comphase declarations. <p> Complexity Performance I binomial tree Gray code O (1) no contention to hypercube labeling avg. dilation 1 [18] [25] I binomial tree Gray code O (1) no contention to mesh reflection avg. dilation 1.2 [25] I binomial tree combinatorial/ O (1) no contention to deBruijn shift register avg. dilation 2 <ref> [48] </ref> sequences I ring to enumeration of O (1) gives precise no. deBruijn necklaces/shift necklaces [34] register sequences II algorithms expressible recurrences O (1) optimal as affine recurrences theory [31] (work in progress) II Cayley/node symm. group theory O (V E) optimal contraction (Cayley graphs) load balancing [27] III arbitrary maximum
References-found: 48

