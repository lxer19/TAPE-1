URL: http://www.hds.utc.fr/~scanu/regul.ps
Refering-URL: http://www.hds.utc.fr/~scanu/RNA.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: TABLE DES MATI ERES 1 Apprentissage et approximation les techniques de regularisation 3 1.1 Introduction
Author: de risque .. Les espace de Holder .. Theoreme de Stone .. Le 
Note: 21 1.5 Reglage du parametre 23 1.5.1 Determination d'un critere 23 1.5.2 Les mesures empiriques 24 1.5.3 Les mesures theoriques 25 1.6 Etude des performances assymptotique de l'estimation 27 1.6.1 Notion  28 1.8.3 Regularisation et physique statistique 29 1.8.4 Regularisation et discriminateur de Bayes 29 1.8.5 Regularisation et complexite la regularisation stochastique 30 1.9 La regularisation et optimisation 30 1.9.1 Les differents stabilisateurs 31 1.9.2 Regularisation et diminution de la capacite des modeles 32 1.9.3 Regularisation et introduction de bruit 32 1.10 Conclusion 34  
Abstract-found: 0
Intro-found: 0
Reference: [Atteia, 1992] <author> Atteia, M. </author> <year> (1992). </year> <title> Hilbertian kernels and spline functions. </title> <publisher> North-holland. </publisher>
Reference-contexts: Voir par exemple <ref> [Laurent, 1972, Morosov, 1984, Atteia, 1992] </ref> : Probleme d'interpolation soient X et Y deux ensembles. soit S ` = f (x i ; y i ) j x i 2 X; y i 2 Y i = 1; `g un `-echantillon, soit F l'ensemble des fonctions de X vers Y (souvent
Reference: [Besse, 1989] <author> Besse, P. </author> <year> (1989). </year> <title> Approximation spline et optimalite en analyse en composantes principales. </title> <type> PhD thesis, </type> <institution> Universite de Toulouse III, Toulouse, France. </institution>
Reference-contexts: Mais certaines questions restent ouvertes : * Comment obtenir une interpolation discontinue ? Question abordee dans <ref> [Besse, 1989] </ref> * Quelles conditions prendre aux bornes ? Comment modeliser le fait que le domaine de x soit associe a une densite de probabilite IP (x) inconnue a priori. <p> 1 Multiquadratique Hardy G (x) = j=1 j g (x j ) Additives [Girosi et al., 1994] e G (!) = j=1 eg (! j ) Tensorielles [Girosi et al., 1994] d Y e jx j j = e jxj L 1 On trouvera un tableau d'exemple de splines dans <ref> [Besse, 1989] </ref>. 1.3.4 Identification des coefficients Reste a identifier les coefficients c i . <p> D'autres ouvrages, consacres a l'etude d'une technique particuliere, seront cites dans le texte. C'est une fois de plus dans la revue des application des splines aux statistiques <ref> [Besse, 1989] </ref> que la plus part des idees presentees ci-dessous ont ete trouvees. 1.5.1 Determination d'un critere Le reglage du parametre de regularisation est considere comme la phase de selection du modele le plus adequat. <p> f (x i ) 2 Cette ecriture a debouche sur une technique de minimisation du risque empirique pondere par w i [Wahba, 1990] : GCV () = ` i=1 2 avec les ponderations : w i = 1 s ii () ` tr (I S ) (1:53) avec differentes justifications <ref> [Besse, 1989] </ref> * intuitives * heurostiques * theoriques Maximum de vraissemblance : le point de vue bayesien Minimum assymptotique la comparaison entre le cout empirique et l'esperence du cout. Enfin, l'ideal serait de pouvoir disposer d'une methode locale d'ajustement du parametre . <p> Sans vouloir entrer dans les details nous pouvons renvoyer le lecteur interesse au livre de [Hardle, 1990]p 177. La Regularisation 27 1.6 Etude des performances assymptotique de l'estimation 1.6.1 Notion de risque Le plan de cette section est due a [Hardle, 1990, Juditsky et al., 1994] et a <ref> [Besse, 1989] </ref> 1.6.2 Theoreme de Stone [?] 1.6.3 Les espace de Holder Conclusion :il faut reduire l'espace les ondelettes de Bessof 1.6.4 Le cas des perceptrons multi-couches : Le theoreme de Baron [?] Dans [Besse, 1989] les auteurs se demandaient si il existait un estimateur non-parametrique dont le taux de convergence <p> Le plan de cette section est due a [Hardle, 1990, Juditsky et al., 1994] et a <ref> [Besse, 1989] </ref> 1.6.2 Theoreme de Stone [?] 1.6.3 Les espace de Holder Conclusion :il faut reduire l'espace les ondelettes de Bessof 1.6.4 Le cas des perceptrons multi-couches : Le theoreme de Baron [?] Dans [Besse, 1989] les auteurs se demandaient si il existait un estimateur non-parametrique dont le taux de convergence serait inferieur a 1 ` 1.7 Reduction du nombre des parametres : Les fonctions de base radiales generalisees. 1.7.1 Extension aux centres mobiles : F.B.R.G. <p> D'une maniere plus generale, c'est cet article qui a propose d'utiliser la theorie de la regularisation pour resoudre des problemes d'apprentissage. Mais la methode de regularisation etait deja exposee dans le livre de Vapnik [Vapnik, 1982]. Pour plus de details les livres de [Wahba, 1990] sur les splines, de <ref> [Besse, 1989] </ref> pour les applications des splines en statistique et de [Morosov, 1984] sur les problemes mal poses sont d'excellentes monographies. 36 Reseaux de Neurones Artificiels
Reference: [Besse and Thomas, 1989] <author> Besse, P. and Thomas, C. </author> <year> (1989). </year> <title> Le lissage par fonctions splines en statistique : revue bibliographique. </title> <journal> Statistique et analyse des donnees, </journal> <volume> 14(1) </volume> <pages> 55-83. </pages>
Reference-contexts: Nous donnerons la solution de cette equation plus loin. Remarque : Dans <ref> [Besse and Thomas, 1989] </ref> la situation est resumee de la maniere suivante : on recherche f opt a priori dans un espace de Hilbert D, solution du probleme de minimisation suivant : min 1 kAf yk 2 + 2 L (1:7) L etant lui m^eme un espace de Hilbert.
Reference: [Bosq and Lecoutre, 1987] <author> Bosq, D. and Lecoutre, J.-P. </author> <year> (1987). </year> <institution> Thiorie de l'estimation fonction-nelle. Economica. </institution>
Reference-contexts: par estimer la densite condi tionnelle IP (Y jX = x) par bp (Y jX = x), puis ensuite estimer m (x) par bm (x) en integrant : bm (x) = Y 2 Mais pas la seule : on peut aussi definir la mediane conditionnelle ou la fonction frontiere, voir <ref> [Bosq and Lecoutre, 1987] </ref>. La Regularisation 19 Mais l'estimation de la densite conditionnelle n'est pas facile. En revanche cette densite peut se calculer en appliquant le theoreme de bayes : IP (Y jX = x) = IP (x) L'equation 1.19 peut alors se reecrire [Bosq and Lecoutre, 1987] : m (x) <p> conditionnelle ou la fonction frontiere, voir <ref> [Bosq and Lecoutre, 1987] </ref>. La Regularisation 19 Mais l'estimation de la densite conditionnelle n'est pas facile. En revanche cette densite peut se calculer en appliquant le theoreme de bayes : IP (Y jX = x) = IP (x) L'equation 1.19 peut alors se reecrire [Bosq and Lecoutre, 1987] : m (x) = Y IP (x; y) dy = Y y IP (x; y) dy (1:22) Le probleme revient a estimer les densites IP (x; y) et IP (x) a partir des donnees empiriques. 1.4.2 Estimation d'une densite Nous allons maintenant decrire une technique d'estimation d'une
Reference: [Boucheron, 1992] <author> Boucheron, S. </author> <year> (1992). </year> <institution> Theorie de l'apprentissage. Langue, Raisonnement, Cal-cul. Hermes, Paris. </institution>
Reference-contexts: Depassant cette crainte legitime de reductionnisme, la theorie calculatoire de l'apprentissage assimile a priori l'inference inductive a un calcul <ref> [Boucheron, 1992] </ref>. A partir de cette hypothese, cette theorie de l'apprentissage (inductif) a pour but l'etude de la determination d'une fonction a partir de donnees empiriques, et ce principalement du point de vue de la complexite, complexite algorithmique et complexite en echantillon.
Reference: [Breizis, 1988] <author> Breizis, H. </author> <year> (1988). </year> <institution> Analyse fonctionnelle. Masson, Paris. </institution>
Reference-contexts: C'est la formalisation d'un a priori indispensable pour la formulation d'une solution. Le probleme revient a definir un critere. Il est possible de montrer que si ce critere defini une semi-norme (coercive et convexe) alors il sera bien un stabilisateur <ref> [Breizis, 1988] </ref>. C'est bien le cas des mesures de regularite suivantes : Exemple : Differents types de stabilisateurs Les mesures suivantes de la regularite d'une fonction f sont des stabilisateurs. <p> La Regularisation 11 Puisque par hypothese D est un espace de Hilbert, on a donc l'existence et l'unicite de l'operateur adjoint <ref> [Breizis, 1988] </ref>, c'est a dire d'un operateur P fl de L ! D 0 tel que : (P f; P ) L = hP fl P f; i D 0 ;D Les operateurs A et P etant bornes sur D, cela nous permet d'ecrire que H est G-differentiable et de donner
Reference: [Broomhead and Lowe, 1988] <author> Broomhead, D. and Lowe, D. </author> <year> (1988). </year> <title> Multivariable functional interpolation and adaptive network. </title> <journal> Complex Systems, </journal> <volume> 2 </volume> <pages> 321-355. </pages>
Reference-contexts: Il n'est pas indispensable de faire concider les centres t i et les donnes x i . On peut essayer d'optimiser aussi la position (et le nombre) des centres. Pour reduire la complexite en nombre de parametres et par consequence la complexite calculatoire Broomhead et Lowe <ref> [Broomhead and Lowe, 1988] </ref> ont propose de passer par une etape intermediaire qui consiste a extraire des representants de l'ensemble d'apprentissage t i appeles "centres", qui ne concident pas necessairement avec les points de donnees.
Reference: [Chauvin, 1990] <author> Chauvin, Y. </author> <year> (1990). </year> <title> Dynamic behavior of constrained back-propagation networks. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 643-649. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Le but est de penalise non seulement les connections importantes mais aussi les connections trop faibles. Les zones "favorisees" sont autour des valeurs 1 et -1. <ref> [Chauvin, 1990] </ref> a propose : ` X ky i f (x i ; W )k 2 + 1 j=1 j j jNj X a 2 1 + a 2 ou jN j designe le nombre d'unites du reseaux et a j l'activation interne de l'unite j.
Reference: [Cohen, 1992] <author> Cohen, I. </author> <year> (1992). </year> <title> Modeles deformables 2-D et 3-D : application a la segmentation d'images medicales. </title> <type> PhD thesis, </type> <institution> Universite de Paris IX Dauphine, Paris, France. </institution>
Reference-contexts: Que se passe t-il en dehors de ce domaine ? 5. Trouver de nouveaux algorithmes d'optimisation plus efficaces, plus stables. Eventuellement m^eme plus rapides, l'analogue de la FFT pour la transformee de Fourier. Il y a beaucoup d'applications potentielles comme en segmentation par contours actifs <ref> [Cohen, 1992] </ref>. Reste aussi a discuter les relations entre la regularisation et la capacite des classes de fonctions.
Reference: [Delahaye, 1994] <author> Delahaye, J.-P. </author> <year> (1994). </year> <title> Informations, </title> <editor> complexite et hasard. </editor> <publisher> Hermes. </publisher>
Reference-contexts: Le probleme consiste a formaliser ce qui est "apprenable par induction", donc a partir d'exemples, et ce qui ne l'est pas. Schematiquement, la theorie de la complexite de l'information propose des idees seduisantes nous permettant d'esquisser une reponse (pour plus de details sur ces questions, voir <ref> [Delahaye, 1994] </ref>). En se pla~cant dans le cadre des suites binaires ont peut formuler le raisonnement suivant. Les suites qui ne sont pas apprenables sont des suites aleatoires. Les suites aleatoires sont incompressibles (c'est la definition de Martin Lof, voir [Delahaye, 1994], section 2.2 et suivantes). <p> une reponse (pour plus de details sur ces questions, voir <ref> [Delahaye, 1994] </ref>). En se pla~cant dans le cadre des suites binaires ont peut formuler le raisonnement suivant. Les suites qui ne sont pas apprenables sont des suites aleatoires. Les suites aleatoires sont incompressibles (c'est la definition de Martin Lof, voir [Delahaye, 1994], section 2.2 et suivantes). Tout ce qui est compressible est apprenable, le mode de compression permettant de definir un biais d'induction c'est-a-dire un a priori ([Boucheron, 1992] p. 99).
Reference: [Denux et al., 1991] <author> Denux, T., Lengelle, R., and Canu, S. </author> <year> (1991). </year> <title> Initialization of weights in a feedforward neural network using prototypes. </title> <editor> In Kohonen, T., Makisara, K., Simula, O., and Kangas, J., editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <pages> pages 623-628. </pages> <publisher> North-Holland, Amsterdam. </publisher>
Reference-contexts: Chacune de ces fonctions possede certains avantages qui dependent de la distribution des donnees, sauf peut-^etre la multiquadratique. 1.8.2 Les fonctions de base radiale et les perceptrons multi couche C'est la methode des prototypes <ref> [Denux et al., 1991, Maruyama et al., 1992] </ref> La fonction de transfert calcule par un P.M.C. peut s'ecrire de la maniere suivante : f (x) = i=1 ou ' designe la fonction d'activation du PMC, monotone et bornee. <p> Dans ce cas nous avons rajoute une dimension a l'espace des entrees. Cette dimension supplementaire permet de normaliser les donnees. Cette remarque deja presente dans les reseaux RCE a ete utilise dans <ref> [Denux et al., 1991] </ref> pour initialiser un P.M.C.
Reference: [Dontchev and Zolezzi, 1993] <author> Dontchev, A. L. and Zolezzi, T. </author> <year> (1993). </year> <title> Well-posed optimization problems. </title> <booktitle> Number 1543 in Lecture notes in mathematics. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: Ce critere de selection, encore appele stabilisateur, pour transformer un probleme mal pose en un probleme bien pose, devra posseder les quatre proprietes suivantes : 1 Ce theoreme est donne en annexe, mais pour plus de detail on pourra se reporter aux ouvrages specialises <ref> [Tikhonov and Arsenin, 1977, Morosov, 1984, Dontchev and Zolezzi, 1993] </ref>.
Reference: [Duchon, 1976] <author> Duchon, J. </author> <year> (1976). </year> <title> interpolation des fonctions de deux variables suivant le principe de la flexion des plaques minces. </title> <journal> R.A.I.R.O., </journal> <volume> 10(12) </volume> <pages> 5-12. </pages>
Reference-contexts: Ce n'est donc pas un terme stabilisateur. 1.3.3 Les splines multi-dimensionelles La prise en compte d'un espace multi-dimensionel a toujours pose des problemes difficile. En deux dimensions il a fallu attandre les travaux de <ref> [Duchon, 1976] </ref>, dont l'application aux statisatiques est discute dans [?]. Pour prendre en compte cet aspect multi-dimensionel, certains auteurs [Girosi et al., 1994] ont propose une autre maniere d'ecrire l'equation de regularisation . <p> @ 2 f kP f k 2 = i=0 ff i = 2i (x) 2 2 Gaussienne Yuille et Grzywacz ff i = 1 1 cosh (x) Multi-dimensionnelles x = (x 1 ; x 2 ; :::; x d ) 0 Radiales m &gt; d 2 r = kxk Splines <ref> [Duchon, 1976] </ref> e G (!) = 1 k!k 2m r 2md d impair multivariables r 2md log (r) d pair Plaques minces Hardy et Desmareis (r 2 + c 2 ) m ; m &lt; 1 Multiquadratique Hardy G (x) = j=1 j g (x j ) Additives [Girosi et al.,
Reference: [Efron, 1979] <author> Efron, B. </author> <year> (1979). </year> <title> Bootstrap methods : another look at the jackknife. </title> <journal> Ann. Statist., </journal> <volume> 7 </volume> <pages> 1-26. </pages>
Reference-contexts: La Regularisation 25 La technique du reechantilonage (Bootsrtaping dans la litterature anglo-saxonne ou Cyrano dans [?]) a ete introduite dans <ref> [Efron, 1979] </ref>. Comme le jackknife, le reechantillonnage peut permet-tre d'estimer le biais d'un estimateur. Nous allons l'appliquer au co^ut empirique vu comme un estimateur de l'erreur de prediction. Une etude detaillee de l'application du reechantillonnage a l'estimation de l'erreur de prediction est donnee dans [Efron and Tibshirani, 1993] p 247.
Reference: [Efron and Tibshirani, 1993] <author> Efron, B. and Tibshirani, R. J. </author> <year> (1993). </year> <title> An Introduction to the Bootstrap. Number 57 in Monographs on statistics and applied Probability. </title> <publisher> Chapman & all, </publisher> <address> New York. </address>
Reference-contexts: La methode du "Jackknife" (aussi appele Eustache dans [?]), proposee par [?], a ete la premiere methode de correction des biais base sur l'utilisation de la puissance de calcul de l'ordinateur. Le chapitre 11 de <ref> [Efron and Tibshirani, 1993] </ref> est consacre a une etude detaillee de cette technique. Cette methode consiste a diminuer le biais de l'estimateur empirique du co^ut en eliminant le premier terme du developpement de Taylor du biais. <p> Comme le jackknife, le reechantillonnage peut permet-tre d'estimer le biais d'un estimateur. Nous allons l'appliquer au co^ut empirique vu comme un estimateur de l'erreur de prediction. Une etude detaillee de l'application du reechantillonnage a l'estimation de l'erreur de prediction est donnee dans <ref> [Efron and Tibshirani, 1993] </ref> p 247. Le principe du reechantillonnage consiste a construire b echantillons en tirant ` exemples parmi les donnees disponibles avec remise. On constitue alors b nouveaux `-echantillons dont le j-ieme est note f (x (j) (j) i ) i=1;` g. <p> ` X ` X (y j f i (x j )) 2 Le reechantillonnage BS () = J emp + 1 b X ` X 2 (j) (j) 2 Tableau 2 : Differentes methodes d'estimation de l'erreur de prediction Reste a savoir quand utiliser quelle methode et pour quel resultats. <ref> [Efron and Tibshirani, 1993] </ref> donne quelques indications a ce sujet. La validation croise donne un estimateur sans biais, mais avec une grande variabilite, surtout lorsque ` est petit.
Reference: [Folland, 1992] <author> Folland, G. B. </author> <year> (1992). </year> <title> Fourier analysis and its applications. </title> <publisher> Wadsworth & Brooks/Cole. </publisher> <address> 37 38 Reseaux de Neurones Artificiels </address>
Reference: [Girard, 1988] <author> Girard, D. </author> <year> (1988). </year> <title> Detection de discontinuites dans un signal ou une image par inf-convolution spline et validation croisee : un algoritme rapide non parametr 'e. </title> <type> Technical Report 702-J-M, </type> <institution> IMAG, Grenoble. </institution>
Reference-contexts: Comment formuler le probleme pour que la solution soit discontinue <ref> [Girard, 1988] </ref> ? Comment prendre en compte la distribution des entrees pour ajuster l'approximation ? * Si l'on connait la structure de la fonction d'interpolation, il reste a en identifier les parametres. La regularisation propose une solution a ce dernier probleme que nous decrirons dans la section 1.3.4.
Reference: [Girosi et al., 1994] <author> Girosi, F., Jones, M., and Poggio, T. </author> <year> (1994). </year> <title> Prior, stabilizers and basis functions : from regularization to radial, tensor and additive splines. </title> <type> Technical Report A.I. Memo 1430 - C.B.C.L. Paper 75, </type> <institution> Massachusetts institute of technology, </institution> <note> Artificial intelligence Laboratory ftp publications.ai.mit.edu /ai-publications1993/AIM-140.ps.Z. </note>
Reference-contexts: de f de la maniere suivante : (f ) = IR d e G (!) ou e G (!) est une fonction positive tendant vers zero lorsque k!k tend vers 1, de sorte que 1 soit un filtre passe haut permettant de controler la densite spectrale de la fonction f <ref> [Thomas-Agnan, 1990, Girosi et al., 1994] </ref>. En d'autres termes, pour que la fonctionnelle (f) ait un sens il faut que f soit une fonction a bande limite. Les splines definies a l'aide de contraintes sur leur transformee de Fourier sont aussi appelees ff spline. <p> En deux dimensions il a fallu attandre les travaux de [Duchon, 1976], dont l'application aux statisatiques est discute dans [?]. Pour prendre en compte cet aspect multi-dimensionel, certains auteurs <ref> [Girosi et al., 1994] </ref> ont propose une autre maniere d'ecrire l'equation de regularisation . <p> kxk Splines [Duchon, 1976] e G (!) = 1 k!k 2m r 2md d impair multivariables r 2md log (r) d pair Plaques minces Hardy et Desmareis (r 2 + c 2 ) m ; m &lt; 1 Multiquadratique Hardy G (x) = j=1 j g (x j ) Additives <ref> [Girosi et al., 1994] </ref> e G (!) = j=1 eg (! j ) Tensorielles [Girosi et al., 1994] d Y e jx j j = e jxj L 1 On trouvera un tableau d'exemple de splines dans [Besse, 1989]. 1.3.4 Identification des coefficients Reste a identifier les coefficients c i . <p> impair multivariables r 2md log (r) d pair Plaques minces Hardy et Desmareis (r 2 + c 2 ) m ; m &lt; 1 Multiquadratique Hardy G (x) = j=1 j g (x j ) Additives <ref> [Girosi et al., 1994] </ref> e G (!) = j=1 eg (! j ) Tensorielles [Girosi et al., 1994] d Y e jx j j = e jxj L 1 On trouvera un tableau d'exemple de splines dans [Besse, 1989]. 1.3.4 Identification des coefficients Reste a identifier les coefficients c i .
Reference: [Hanson and Burr, 1988] <author> Hanson, S. and Burr, D. </author> <year> (1988). </year> <title> Minkowski-r back propagation: Learning connectionist models with non-euclidian error signals. </title> <editor> In D.Z., A., editor, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <pages> pages 348-357. </pages> <institution> American Institute of Physics, </institution> <address> New York. </address>
Reference-contexts: a une solution estimee w 0 , on a pour le terme regularisant P : P = kw w 0 k 2 (1:59) Si l'estimee w 0 est bonne, on ne modifie pas ou peu la solution, sinon, on peut iterer avec comme nouvelle estimee, la solution du probleme regularise. <ref> [Hanson and Burr, 1988] </ref> ont propose de minimiser : ` X ky i f (x i ; W )k 2 + j=1 j ou jW j denote le nombre de parametres a optimiser.
Reference: [Hardle, 1990] <author> Hardle, W. </author> <year> (1990). </year> <title> Applied nonparametric regression, </title> <booktitle> volume 19 of Economic Society Monographs. </booktitle> <publisher> Cambridge University Press, </publisher> <address> New York. </address>
Reference-contexts: Les fonctions R i (x) sont les noyaux equivalents (rien a voir avec les noyaux de Green) si l'on considere l'approximation spline comme une methode de noyau. Pour une methode de noyau, la forme generale de l'approximation est donnee par la formule de Nadaraya-Watson (1.39). Le livre de <ref> [Hardle, 1990] </ref> presente une etude detaillee de ce type de modeles, incluant la "regression spline", sous le nom de "procedures de moyennage local" (local averaging methods). Le formalisme choisi rejoint celui que nous avons adapte equation 1.43 avec differents formes pour les fonctions de ponderation R i (x). <p> Soit utiliser les donnees disponibles pour construire un estimateur, soit construire theoriquement une fonction representant l'ecart entre ce qu'on a et ce qu'on voudrait avoir. Ce probleme a fait l'objet d'un chapitre dans la plus part des livres consacres aux methodes "flex-ibles" de regression <ref> [Hardle, 1990, Hastie and Tibshirani, 1990, Wahba, 1990] </ref>. D'autres ouvrages, consacres a l'etude d'une technique particuliere, seront cites dans le texte. <p> Il ne devra pas ^etre inferieur a 50, et le plus grand possible etant donnees les contraintes de calcul. Soulignons qu'en appliquant localement cette technique <ref> [Hardle, 1990] </ref> p. 177 propose une technique d'amelioration locale du lissage. Rappelons que ces techniques sont avant tout des methodes d'estimation de l'erreur de prediction a partir de ccdonnees empiriques, et que c'est en quelque sorte les detourner que de les utiliser pour identifier la complexite du modele. <p> D'ou l'idee d'etudier theoriquement le comportement du cout empirique pour proposer des modifications permettant de construre un estimateur de l'esperence du cout relativement plus facile a evaluer. Dans <ref> [Hardle, 1990] </ref> p 166, cinq criteres sont presentes dans le cadre de la regression : 26 Reseaux de Neurones Artificiels * La validation croise generalisee * Le critere d'information d'Akaike (AIC) * FPE (Finite prediction error) * Le selecteur de modele de Shibata * Le selecteur de largeur de bande de <p> En effet, il n'y a aucune raison pour que la complexite d'une fonction soit constante sur tout son domaine de definition. Il s'agit avant tout d'une propriete locale. Sans vouloir entrer dans les details nous pouvons renvoyer le lecteur interesse au livre de <ref> [Hardle, 1990] </ref>p 177. <p> Il s'agit avant tout d'une propriete locale. Sans vouloir entrer dans les details nous pouvons renvoyer le lecteur interesse au livre de [Hardle, 1990]p 177. La Regularisation 27 1.6 Etude des performances assymptotique de l'estimation 1.6.1 Notion de risque Le plan de cette section est due a <ref> [Hardle, 1990, Juditsky et al., 1994] </ref> et a [Besse, 1989] 1.6.2 Theoreme de Stone [?] 1.6.3 Les espace de Holder Conclusion :il faut reduire l'espace les ondelettes de Bessof 1.6.4 Le cas des perceptrons multi-couches : Le theoreme de Baron [?] Dans [Besse, 1989] les auteurs se demandaient si il existait
Reference: [Hastie and Tibshirani, 1990] <author> Hastie, T. and Tibshirani, R. </author> <year> (1990). </year> <title> Generalized Additive Models, </title> <booktitle> volume 43 of Monographs on Statistics and Applied Probability. </booktitle> <publisher> Chapman & Hall, </publisher> <address> Redwood City. </address>
Reference-contexts: Les splines additives Une autre maniere de traiter les composantes du vecteur d'entree separement en posant : G (x) = j=1 ou les j sont des parametres fixe a l'avance. Le modele resultant est celui des MARS (multivariate adaptive regression splines) simplifie <ref> [Hastie and Tibshirani, 1990] </ref>, c'est-a-dire que la fonction a identifier a la forme suivante : f (x) = j=1 ou les fonctions f j sont des approximation splines monodimensionelles appliquees a la j-ieme composante du vecteur d'entree. <p> La matrice S est la matrice qui permet de transformer les valeurs mesurees en valeurs estimees aux points de mesure. Les proprietes des valeurs propres de la matrice S nous renseignent alors sur la qualite du lissage effectue <ref> [Hastie and Tibshirani, 1990] </ref>. On peut aussi en deduire le nombre de parametres effectifs du modele. En effet le modele spline, ainsi que d'autres modeles pouvant se mettre sous la forme 1.44, admettent un grand nombre de parametres, autant que de donnees. <p> Par analogie avec les modeles parametriques cette quantite va nous permettre d'estimer le nombre de parametres effectifs du modele en posant : p eff = T r (S S t En effet, <ref> [Hastie and Tibshirani, 1990] </ref> precisent que dans le cas des moindres carres, la matrice S est idempotente et on peut ecrire T r (S S t ) = T r (S ) = rang (S ), ce qui est egal au nombre de predicteurs independants du model. <p> Soit utiliser les donnees disponibles pour construire un estimateur, soit construire theoriquement une fonction representant l'ecart entre ce qu'on a et ce qu'on voudrait avoir. Ce probleme a fait l'objet d'un chapitre dans la plus part des livres consacres aux methodes "flex-ibles" de regression <ref> [Hardle, 1990, Hastie and Tibshirani, 1990, Wahba, 1990] </ref>. D'autres ouvrages, consacres a l'etude d'une technique particuliere, seront cites dans le texte. <p> Cette procedure est donc relativement co^uteuse en temps de calcul. Elle trouve sa justification justifie que par le fait que l'esperance de l'erreur de validation croisee est une approximation de l'erreur de prediction <ref> [Hastie and Tibshirani, 1990] </ref>. IE (V C ()) ` i=1 mais surtout parce que la minimisation de ces deux criteres donne le m^eme resultat. A partir de cet estimateur il est possible d'enconstruire d'autres sur le m^eme principe mais en utilisant plusieurs exemples pour le test ("leave many out").
Reference: [Hinton, 1989] <author> Hinton, G. </author> <year> (1989). </year> <title> Deterministic boltzmann learning performs steepest descent in weight space. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 143-150. </pages>
Reference: [Hoerl and Kennard, 1970] <author> Hoerl, A. and Kennard, R. </author> <year> (1970). </year> <title> Ridge regression : biaised estimation for nonorthogonal problems. </title>
Reference-contexts: = (G + I) 1 y (1:17) et donc comme f (x) = G (x)c avec G (x) le vecteur ligne de composantes G (x x i ), on a : f (x) = G (x)(G + I) 1 y (1:18) Ce resultat n'est pas sans rappeler celui preconise par <ref> [Hoerl and Kennard, 1970] </ref> sous le nom de "ridge regression". Dans ce cadre, on se propose de rechercher une solution au probleme pour laquelle la somme des carres des coefficients est bornee.
Reference: [Juditsky et al., 1994] <author> Juditsky, A., Zhang, Q., delyon, B., Glorennec, P.-Y., and Benveniste, A. </author> <year> (1994). </year> <title> Wavelett in identification. </title> <type> Technical Report 849, </type> <institution> IRISA, </institution> <note> ftp ftp.irisa.fr /pub/techreports/1994/PI-849.ps.Z. </note>
Reference-contexts: Dans ce cadre les reseaux de neurones artificiels du type perceptron multi-couche (P.M.C.) seront vus comme une technique d'interpolation ou d'approximation efficace dans certains cas, no-tamment par exemple lorsque la dimension de l'espace des entrees est importante <ref> [Juditsky et al., 1994] </ref>. Il parait alors legitime de se demander quel a priori est lie a cette classe d'approximateur. C'est le but de ce travail. <p> Il s'agit avant tout d'une propriete locale. Sans vouloir entrer dans les details nous pouvons renvoyer le lecteur interesse au livre de [Hardle, 1990]p 177. La Regularisation 27 1.6 Etude des performances assymptotique de l'estimation 1.6.1 Notion de risque Le plan de cette section est due a <ref> [Hardle, 1990, Juditsky et al., 1994] </ref> et a [Besse, 1989] 1.6.2 Theoreme de Stone [?] 1.6.3 Les espace de Holder Conclusion :il faut reduire l'espace les ondelettes de Bessof 1.6.4 Le cas des perceptrons multi-couches : Le theoreme de Baron [?] Dans [Besse, 1989] les auteurs se demandaient si il existait
Reference: [Laurent, 1972] <author> Laurent, P.-J. </author> <year> (1972). </year> <title> Approximation et Optimisation. </title> <publisher> Hermann, </publisher> <address> Paris. </address>
Reference-contexts: Voir par exemple <ref> [Laurent, 1972, Morosov, 1984, Atteia, 1992] </ref> : Probleme d'interpolation soient X et Y deux ensembles. soit S ` = f (x i ; y i ) j x i 2 X; y i 2 Y i = 1; `g un `-echantillon, soit F l'ensemble des fonctions de X vers Y (souvent
Reference: [Maruyama et al., 1992] <author> Maruyama, M., Poggio, T., and Girosi, F. </author> <year> (1992). </year> <title> A connection between grbf and mlp. </title> <type> Technical Report 1291, </type> <institution> M.I.T. AI Laboratory, </institution> <address> Cambridge, MA. </address>
Reference-contexts: Chacune de ces fonctions possede certains avantages qui dependent de la distribution des donnees, sauf peut-^etre la multiquadratique. 1.8.2 Les fonctions de base radiale et les perceptrons multi couche C'est la methode des prototypes <ref> [Denux et al., 1991, Maruyama et al., 1992] </ref> La fonction de transfert calcule par un P.M.C. peut s'ecrire de la maniere suivante : f (x) = i=1 ou ' designe la fonction d'activation du PMC, monotone et bornee.
Reference: [Matsuoka, 1992] <author> Matsuoka, K. </author> <year> (1992). </year> <title> Noise injection into inputs in back-propagation learning. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 22(3) </volume> <pages> 436-440. </pages>
Reference-contexts: Neal dans [?] mentionne des resultats satisfaisants sous reserve d'un choix judicieux du parametre de regularisation. 1.9.2 Regularisation et diminution de la capacite des modeles 1.9.3 Regularisation et introduction de bruit <ref> [Matsuoka, 1992] </ref> a etudie l'influence de l'adjonction d'un bruit en entree sur la fonction de transfert d'un reseau.
Reference: [Moody, 1992] <author> Moody, J. E. </author> <year> (1992). </year> <title> The effective number of parameters : an analysis of generalization and regularization in nonlinear learning systems. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 847-854, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Les auteurs preferent la notion de "degres de libertes" a celle de "parametre effectifs" defendue par d'autres <ref> [Moody, 1992] </ref>. Cette question du nombre des degres de libertes de l'approximation fait l'objet de la section 3.5 de leur ouvrage. Son utilite est liee au probleme de la determination de la valeur du parametre de lissage .
Reference: [Morosov, 1984] <author> Morosov, V. </author> <year> (1984). </year> <title> Methods for solving incorrectly posed problems. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: Voir par exemple <ref> [Laurent, 1972, Morosov, 1984, Atteia, 1992] </ref> : Probleme d'interpolation soient X et Y deux ensembles. soit S ` = f (x i ; y i ) j x i 2 X; y i 2 Y i = 1; `g un `-echantillon, soit F l'ensemble des fonctions de X vers Y (souvent <p> Ce critere de selection, encore appele stabilisateur, pour transformer un probleme mal pose en un probleme bien pose, devra posseder les quatre proprietes suivantes : 1 Ce theoreme est donne en annexe, mais pour plus de detail on pourra se reporter aux ouvrages specialises <ref> [Tikhonov and Arsenin, 1977, Morosov, 1984, Dontchev and Zolezzi, 1993] </ref>. <p> Ces relations avec d'autres approches seront abordees dans la section 5. Le reglage du parametre de regularisation est delicat dans la pratique. C'est m^eme le probleme central de la technique de regularisation d'apres <ref> [Morosov, 1984] </ref>. Dans la suite nous allons choisir d'utiliser un critere quadratique comme mesure d'adequation locale. Il est a remarquer que si ce critere est le plus utilise il n'est pas le seul a pouvoir l'^etre. De nombreux autres criteres ont ete proposes. Certains sont rappeles dans l'annexe ??. <p> On a construit H (f ) pour qu'elle soit convexe et coercive. Reste a resoudre le probleme de minimisation. Le theoreme suivant donne la forme generale de la solution. Theoreme <ref> [Morosov, 1984] </ref> Supposons que D soit un espace vectoriel tel que D soit un espace de Hilbert, ou D designe la fermeture de D. <p> Mais la methode de regularisation etait deja exposee dans le livre de Vapnik [Vapnik, 1982]. Pour plus de details les livres de [Wahba, 1990] sur les splines, de [Besse, 1989] pour les applications des splines en statistique et de <ref> [Morosov, 1984] </ref> sur les problemes mal poses sont d'excellentes monographies. 36 Reseaux de Neurones Artificiels
Reference: [Murray and Edwards, 1993] <author> Murray, A. and Edwards, P. </author> <year> (1993). </year> <title> Synaptic weight noise during mlp learning enhances fault-tolerance, generalisation and learning trajectory. </title> <editor> In Hanson, S., Cowan, J., and Lee Giles, C., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 491-498. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: N.B : Le developpement de Taylor utilise dans l'article est illicite, et donc les resultats exposes dans ce paragraphe sont incorrects. Ce type d'etude est aussi developpe dans <ref> [Murray and Edwards, 1993] </ref> ou les auteurs font une hypothese sur l'independance des bruits sur chaque connexion. Nous allons maintenant faire le calcul. Dans un probleme d'approximation, on peut considerer les points constituent l'ensemble d'apprentissage comme bruites.
Reference: [Poggio and Girosi, 1989] <author> Poggio, T. and Girosi, F. </author> <year> (1989). </year> <title> A theory of networks for approximation and learning. </title> <type> Technical Report 1140, </type> <institution> M.I.T. AI Laboratory, </institution> <address> Cambridge, MA. </address>
Reference-contexts: Nous allons voir que les donnees seules ne suffisent pas, mais qu'un a priori sur la nature du modele est necessaire a la construction d'une solution au probleme d'apprentissage. Comme le soulignent <ref> [Poggio and Girosi, 1989] </ref>, encore faut-il que le probleme ait un sens : comment retrouver une moitie d'annuaire a partir de l'autre moitie ! Ce point fondamental a ete discute par de nombreux auteurs. <p> peuvent s'ecrire comme la combinaison lineaire de fonctions de base. 12 Reseaux de Neurones Artificiels 1.3.2 Resolution de l'equation regularisee Le probleme est le suivant : comment choisir P pour que f , c'est a dire la solution recherchee ait de bonnes proprietes ? Une solution possible est donnee dans <ref> [Poggio and Girosi, 1989] </ref> Theoreme : Si P fl P est un operateur differentiel a coefficients constants, la fonction f opt 2 D qui minimise : H (f) = 2 i=1 kP fk 2 a la forme suivante : f opt (x) = i=1 ou G denote une fonction de Green <p> Il reste que les hypotheses sur la forme du bruit sur les donnees sont assez severes. 1.10 Conclusion <ref> [Poggio and Girosi, 1989] </ref> traitent le probleme a l'envers : ils posent d'abord le probleme d'interpolation et recherchent l'ensemble des fonctions interpollantes ayant une certaine forme (une forme radiale bien sur c'est a dire la forme h (kx x i k) ). <p> Qu'ils soient aussi remercies pour leur patience. Merci aussi a Stephane Boucheron pour ses remarques pertinentes et son soutient sans faille. Notes bibliographiques L'article de base ayant servi a la redaction de ce chapitre est <ref> [Poggio and Girosi, 1989] </ref>. C'est le premier article qui a fait le lien entre les reseaux de neurones artificiels et la regularisation. D'une maniere plus generale, c'est cet article qui a propose d'utiliser la theorie de la regularisation pour resoudre des problemes d'apprentissage.
Reference: [Powell, 1987] <author> Powell, M. </author> <year> (1987). </year> <title> Radial basis functions for multivariable interpolation : a review. In Masson, </title> <editor> J. and Cox, M., editors, </editor> <title> Algorithms for approximation. </title> <publisher> Clarendon Press, Oxford,. </publisher>
Reference-contexts: Les polynomes constituent une composante globale de la solution puisque leur derive d'ordre m est nulle et qu'ils appartiennent donc ainsi au noyau de l'operateur P . Soulignons que <ref> [Powell, 1987] </ref> a ete le premier a implementer ce resultat sous la forme d'un perceptron multi-couches (P.M.C.) dont les fonctions d'activation etaient des fonctions de base radiales, des gaussiennes centrees en x i .
Reference: [Rudin, 1991] <author> Rudin, W. </author> <year> (1991). </year> <title> Functional analysis. </title> <booktitle> International Series in Pure and applied mathematics. </booktitle> <address> Mc Graww-Hill. La Regularisation 39 </address>
Reference: [Saitoh, 1988] <author> Saitoh, S. </author> <year> (1988). </year> <title> Theory of reproducing kernels and its application. </title> <editor> J. Willey, </editor> <address> New York. </address>
Reference: [Sejnowski and Rosenberg, 1987] <author> Sejnowski, T. and Rosenberg, C. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce english text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168. </pages>
Reference-contexts: 1.7.2 Approximation et equation differentielle voici la reference [?] 1.8 Relations avec les autres methodes 1.8.1 les F.B.R.G. et les t^aches de classification Ces fonctions ont toutes ete employees pour resoudre des problemes, en particulier [?], a utiliser les F.B.R. pour resoudre le probleme de la lecture mieux que NetTalk <ref> [Sejnowski and Rosenberg, 1987] </ref>.
Reference: [Seung et al., 1991] <author> Seung, H., Sompolinsky, H., and Tishby, N. </author> <year> (1991). </year> <title> Learning curves in large neural networks. </title> <booktitle> In Proceedings of the Annual Workshop on Computational Learning Theory 1991, </booktitle> <pages> pages 112-127, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: penalisation il a ete montre qu'il est possible d'eliminer cette composant supplementaire (donc de de-normaliser les entrees) sans diminuer de maniere significatives les performances. 1.8.3 Regularisation et physique statistique En physique statistique on retrouve une equation analogue a la regularisation 1.3 ou est propor tionnel a l'inverse d'une temperatureT : <ref> [Seung et al., 1991] </ref> F = `J emp T S ou F designe l'energie libre quantite a minimiser, S l'entropie de la solution mesurant un "gain d'information" et T un reel appele la temperature. 1.8.4 Regularisation et discriminateur de Bayes Le probleme de reconstruction d'une hypersurface peut ^etre reformule comme un
Reference: [Silverman, 1984] <author> Silverman, B. </author> <year> (1984). </year> <title> Spline smoothing: the equivalent variable kernel method. </title> ???, <booktitle> 12(3) </booktitle> <pages> 898-916. </pages>
Reference-contexts: et 1.39 auraient la m^eme forme si l'on pouvait trouver une fonction K verifiant : R i (x) = b ) i=1 K ( xx i Dans le cas des D m splines, une approximation de la fonction de ponderation R i (x) de l'equation 1.43 a ete donne par <ref> [Silverman, 1984] </ref> sous la forme d'un "noyau". Ce noyau est appele le "noyau equivalant" au lissage spline.
Reference: [Silverman, 1986] <author> Silverman, B. </author> <year> (1986). </year> <title> Density Estimation for Statistical and Data Analysis, </title> <booktitle> volume 26 of Monographs on Statistics and Applied Probability. </booktitle> <publisher> Chapman & Hall, London. </publisher>
Reference-contexts: Ce probleme de l'estimation d'une densite a partir de donnees empirique est en soit un probleme difficile qui a donne lieu a de nombreuses etudes <ref> [Silverman, 1986] </ref>. Nous allons presenter ici une de ces techniques, basee sur la regularisation.
Reference: [Thomas-Agnan, 1990] <author> Thomas-Agnan, C. </author> <year> (1990). </year> <title> A family of splines for non-parametric regression and their relationships with kriging. </title> <journal> statistics, </journal> <volume> 21(4) </volume> <pages> 533-548. </pages>
Reference-contexts: de f de la maniere suivante : (f ) = IR d e G (!) ou e G (!) est une fonction positive tendant vers zero lorsque k!k tend vers 1, de sorte que 1 soit un filtre passe haut permettant de controler la densite spectrale de la fonction f <ref> [Thomas-Agnan, 1990, Girosi et al., 1994] </ref>. En d'autres termes, pour que la fonctionnelle (f) ait un sens il faut que f soit une fonction a bande limite. Les splines definies a l'aide de contraintes sur leur transformee de Fourier sont aussi appelees ff spline. <p> C'est ainsi que sont definies les ff-splines <ref> [Thomas-Agnan, 1990] </ref>.
Reference: [Tikhonov and Arsenin, 1977] <author> Tikhonov, A. N. and Arsenin, V. Y. </author> <year> (1977). </year> <title> Solution of ill-posed problems. </title> <editor> W. H. Wilson, </editor> <address> Washington, D. C. </address>
Reference-contexts: Si l'on change de mesure, ce qui est instable deviendra mal conditionne, et les difficultes persisteront. Pour resoudre les problemes mal poses, <ref> [Tikhonov and Arsenin, 1977] </ref> ont propose une methode permettant de transformer les problemes mal poses en problemes bien poses. Pour ce faire, ils ont commence par donner une condition suffisante pour qu'un probleme soit bien pose. <p> Ce critere de selection, encore appele stabilisateur, pour transformer un probleme mal pose en un probleme bien pose, devra posseder les quatre proprietes suivantes : 1 Ce theoreme est donne en annexe, mais pour plus de detail on pourra se reporter aux ouvrages specialises <ref> [Tikhonov and Arsenin, 1977, Morosov, 1984, Dontchev and Zolezzi, 1993] </ref>. <p> definition de l'operateur P , sous la contrainte d Y (Af; y) " Deux remarques s'imposent apres la definition de ce probleme : * Tikhonov a montre que sous l'hypothese de quasi monotonie du stabilisateur (la convex-ite n'est pas necessaire), la solution du probleme d'optimisation s'obtient en saturant la contrainte <ref> [Tikhonov and Arsenin, 1977] </ref> . On pourra considerer la contrainte d'egalite - d Y (Af; y) = ". * Lorsque " tend vers 0, la solution du probleme tend vers une interpolation "reguliere". " represente donc la quantite de "bruit" que l'on accepte dans les donnees. <p> L'approximation regularisee de l'echantillon S ` est la fonction f opt qui realise le minimum de la fonctionnelle H (f ) definie par : H (f ) = 2 i=1 kP f k 2 H (f) est baptise la fonctionnelle lissante par <ref> [Tikhonov and Arsenin, 1977] </ref>. La Regularisation 9 L'equation 1.3 peut ^etre interprete de la maniere suivante : * Le premier terme est plutot local. Il mesure l'adequation de la fonction recherchee aux don nees.
Reference: [Vapnik, 1982] <author> Vapnik, V. </author> <year> (1982). </year> <title> Estimation of Dependances Based on Empirical Data. </title> <booktitle> Springer Series in Statistics. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: Il est clair que le second terme represente la connaissance a priori que l'on a sur le phenomene. 1.8.5 Regularisation et complexite : la regularisation stochastique Vapnik <ref> [Vapnik, 1982] </ref> va utiliser la borne superieure de l'esperance du co^ut pour construire un algorithme : La minimisation du risque structurel. 1.9 La regularisation et optimisation Les algorithmes d'optimisation utilises pour identifier les parametres des modeles connexionistes sont souvent bases sur la minimisation d'un fonction co^ut empirique J emp . <p> D'une maniere plus generale, c'est cet article qui a propose d'utiliser la theorie de la regularisation pour resoudre des problemes d'apprentissage. Mais la methode de regularisation etait deja exposee dans le livre de Vapnik <ref> [Vapnik, 1982] </ref>. Pour plus de details les livres de [Wahba, 1990] sur les splines, de [Besse, 1989] pour les applications des splines en statistique et de [Morosov, 1984] sur les problemes mal poses sont d'excellentes monographies. 36 Reseaux de Neurones Artificiels
Reference: [Vapnik, 1983] <author> Vapnik, V. N. </author> <year> (1983). </year> <title> Estimation of dependences based on empirical data. </title> <booktitle> Springer series in statistics. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: En revanche, il est toujours possible de comparer des fonctions de repartitions et d'en deduire les densitees. La densite f (x) que l'on recherche est alors solution de probleme de minimisation suivant <ref> [Vapnik, 1983] </ref> : 1 jjF F ` jj 2 = 2 Z L 2 (1.26) 1 jj (H fl f) F ` jj 2 Le probleme de minimisation de J (f) est un probleme mal pose au sens de Tikhonov.
Reference: [Wahba, 1990] <author> Wahba, G. </author> <year> (1990). </year> <title> Spline Models for observational data. </title> <booktitle> Number 59 in regional conference Series in applied mathematics. </booktitle> <publisher> siam, </publisher> <address> philadelphia, Pennsylvania. </address>
Reference-contexts: Les splines definies a l'aide de contraintes sur leur transformee de Fourier sont aussi appelees ff spline. Remarque : A travers cette definition on a associe l'operateur P au stabilisateur. L'operateur P est note ainsi car il est possible de le considerer comme un projecteur <ref> [Wahba, 1990] </ref>. Pour l'operateur P , de nombreux choix sont possibles. <p> Recapitulatif Les principales solutions trouvees dans la litterature sont resumees dans le tableau suivant : La Regularisation 17 Caracteristique Noyau Nom Auteur (s) Mono-dimensionnelles D m = @ m : + splines polynomiales Reinsch, <ref> [Wahba, 1990] </ref> D m -splines P fl P f = f @ 2 f kP f k 2 = i=0 ff i = 2i (x) 2 2 Gaussienne Yuille et Grzywacz ff i = 1 1 cosh (x) Multi-dimensionnelles x = (x 1 ; x 2 ; :::; x d ) <p> Soit utiliser les donnees disponibles pour construire un estimateur, soit construire theoriquement une fonction representant l'ecart entre ce qu'on a et ce qu'on voudrait avoir. Ce probleme a fait l'objet d'un chapitre dans la plus part des livres consacres aux methodes "flex-ibles" de regression <ref> [Hardle, 1990, Hastie and Tibshirani, 1990, Wahba, 1990] </ref>. D'autres ouvrages, consacres a l'etude d'une technique particuliere, seront cites dans le texte. <p> peut ^etre reecrit en utilisant les termes diagonal s ii () de la matrice de lissage S ([Wahba, 1990, ?] theoreme 3.1) CV () = ` i=1 y i f (x i ) 2 Cette ecriture a debouche sur une technique de minimisation du risque empirique pondere par w i <ref> [Wahba, 1990] </ref> : GCV () = ` i=1 2 avec les ponderations : w i = 1 s ii () ` tr (I S ) (1:53) avec differentes justifications [Besse, 1989] * intuitives * heurostiques * theoriques Maximum de vraissemblance : le point de vue bayesien Minimum assymptotique la comparaison entre <p> D'une maniere plus generale, c'est cet article qui a propose d'utiliser la theorie de la regularisation pour resoudre des problemes d'apprentissage. Mais la methode de regularisation etait deja exposee dans le livre de Vapnik [Vapnik, 1982]. Pour plus de details les livres de <ref> [Wahba, 1990] </ref> sur les splines, de [Besse, 1989] pour les applications des splines en statistique et de [Morosov, 1984] sur les problemes mal poses sont d'excellentes monographies. 36 Reseaux de Neurones Artificiels
Reference: [Weigend et al., 1991] <author> Weigend, A., Rumelhart, D., and Huberman, B. </author> <year> (1991). </year> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In Lippman, R., Moody, J., and Touretzky, D., editors, </editor> <booktitle> Neural Information Processing 3, </booktitle> <pages> pages 875-882. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: L'idee est de penaliser les connections trop importante pour couper (w = 0) celles qui ne seraient pas utiles. <ref> [Weigend et al., 1991] </ref> eux ont utilise : ` X ky i f (x i ; W )k 2 + j=1 j j en ajoutant une heuristique pour diminuer au cours de l'optimisation. Le but est de penalise non seulement les connections importantes mais aussi les connections trop faibles.
Reference: [Zuily, 1986] <author> Zuily, C. </author> <year> (1986). </year> <title> Distributions et equations aux derivees partielles. Collection meth-odes. </title> <publisher> Hermann, </publisher> <address> Paris. </address>
Reference-contexts: Soulignons que [Powell, 1987] a ete le premier a implementer ce resultat sous la forme d'un perceptron multi-couches (P.M.C.) dont les fonctions d'activation etaient des fonctions de base radiales, des gaussiennes centrees en x i . Demonstration : la demonstration de ce theoreme est donne en exercice dans <ref> [Zuily, 1986] </ref> lorsque P*P est un operateur differentiel a coefficients constants. C'est une application du theoreme de Malgrange-Ehrenpreisis sur l'existence et la forme des solutions des equations differentielles.
References-found: 45

