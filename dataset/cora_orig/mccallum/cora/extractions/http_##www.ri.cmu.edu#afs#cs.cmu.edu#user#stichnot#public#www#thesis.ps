URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/stichnot/public/www/thesis.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/stichnot/public/www/publications.html
Root-URL: 
Title: Generating Code for High-Level Operations through Code Composition  
Author: James M. Stichnoth Thomas Gross, chair David R. O'Hallaron Jaspal Subhlok Joel Saltz, James M. Stichnoth 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy Thesis Committee:  
Note: Copyright c 1997  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Computer Science Department Carnegie Mellon University  University of Maryland  
Date: August 1997  
Pubnum: CMU-CS-97-165  
Abstract: Effort sponsored by the Defense Advanced Research Projects Agency and Rome Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-96-1-0287. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Defense Advanced Research Projects Agency, Rome Laboratory or the U.S. Government. Distribution Statement A. Approved for public release; distribution is unlimited. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Adams. </author> <title> CRAY T3D system architecture overview. </title> <type> Technical report, </type> <institution> Cray Research Inc., </institution> <note> Sep-tember 1993. Available as http://www.cray.com/PUBLIC/product-info/mpp/T3D Architecture Over/T3D.overview.html. </note>
Reference-contexts: not confused. * Each processor must allocate all necessary communication buffers at the beginning and deallocate them at the end. * The processors must interleave the sends and receives as necessary to prevent communication dead lock. * On distributed memory machines with shared memory support, such as the Cray T3D <ref> [1, 9] </ref> and T3E [49], the algorithm might need to be modified so that the communication set loop writes directly to remote memory rather than going through communication buffers [56]. <p> The direct deposit model takes this idea one step further. On machines with fine-grained remote store capabilities, like the Cray T3D <ref> [1, 9] </ref> and T3E [49], as well as on true shared-memory machines, the direct deposit model allows the sender to compute the destination addresses and perform remote stores for each element, rather than using communication buffers and explicit communication operations. <p> Unfortunately, a limitation of the T3D prevents the full implementation of direct deposit for the RIACS algorithm (as well as the other related table-generation algorithms). The DTB Annex <ref> [1] </ref> is a 32-entry table that allows a T3D node to send data to up to 32 nodes at once.
Reference: [2] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Thus the inner loop of the computation set generation can perform the + and fl computation. Note that each temporary requires as much memory as the left-hand side array. To reduce the amount of temporary space needed, we can use standard compiler techniques <ref> [2] </ref> to reorder the computation to minimize temporaries. We can also use the right-hand side array A as a temporary, since it is overwritten by the computation. * Restriction 3: All array subscripts are subscript triplets. If there are any scalar subscripts, two steps are required. <p> I provide a complete list with descriptions in Appendix A. 4.5 Global optimization framework 4.5.1 Standard global optimizations Catacomb implements several standard global optimizations <ref> [2, 17] </ref>: * Constant folding and other expression simplifications. Every time a new expression is created, Cat acomb tries to fold constant expressions and exploit algebraic identities to simplify the resulting ex pressions. * Constant/copy propagation. * Dead assignment elimination. * Unreachable code elimination.
Reference: [3] <author> J.R. Allen, K. Kennedy, C. Porterfield, and J. Warren. </author> <title> Conversion of control dependence to data dependence. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 177189. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1983. </year>
Reference-contexts: See Figure 4.10. The transformations of statements inside loops and if statements are similar to the technique of if-conversion, originally proposed by Allen, Kennedy, Porterfield, and Warren <ref> [3] </ref>. If there is a sequence 58 CHAPTER 4. CATACOMB to data flow analysis. of statements in a branch of an if statement, if-conversion transforms the construct into a sequence of statements, each of which is guarded by a separate if statement.
Reference: [4] <institution> American National Standards Institute. Fortran 90, </institution> <month> May </month> <year> 1991. </year> <title> X3J3 internal document S8.118. </title>
Reference-contexts: The subscript triplet notation `: h: s, also used in Fortran 90 <ref> [4] </ref>, describes a sequence of array indices starting with `, with stride s, and having an upper bound h. If s is negative, h is instead a lower bound. If the stride parameter is omitted, it defaults to 1.
Reference: [5] <author> C. Ancourt, F. Coelho, F. Irigoin, and R. Keryell. </author> <title> A linear algebra framework for static HPF code distribution. </title> <type> Technical Report A-278-CRI, </type> <institution> Centre de Recherche en Informatique, Ecole Nationale Superieure des Mines de Paris, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: Most of the array assignment techniques are not powerful enough to handle this kind of construct. 2.3. OTHER ALGORITHMS 17 2.3.6 Ecole des Mines algorithm Another linear algebraic algorithm is the Ecole des Mines algorithm <ref> [5] </ref>. Like the IBM algorithm, it uses matrix transformations to compute part of the access pattern at compile time. However, it uses a much larger set of transformations, allowing it to reduce the number of loops required, to improve the communication patterns, and to improve memory access patterns.
Reference: [6] <author> J. Auslander, M. Philipose, C. Chambers, S. Eggers, and B. Bershad. </author> <title> Fast, effective dynamic compilation. </title> <booktitle> In Proceedings of the ACM SIGPLAN'96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 149159, </pages> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: It deals with the performance problem by adding dynamic compilation to the runtime system, including such optimizations as partial evaluation and register allocation. Note that all code fragments that are dynamically compiled are explicitly annotated by the programmer. A different approach is taken at the University of Washington <ref> [6] </ref>. Using a combination of automatic methods and programmer annotations, their system for runtime code generation dynamically creates and optimizes code based on runtime constants.
Reference: [7] <author> H. Bao, J. Bielak, O. Ghattas, D.R. O'Hallaron, L.F. Kallivokas, J.R. Shewchuk, and J. Xu. </author> <title> Earthquake ground motion modeling on parallel computers. </title> <booktitle> In Supercomputing '96, </booktitle> <address> Pittsburgh, Pennsylvania, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: In this case, the entire loop itself serves as a complex high-level operation. Section 6.1 describes the problem of irregular data transfer in more detail. * Archimedes. Archimedes [53] is the language and compiler portion of the Quake project <ref> [7] </ref> at Carnegie Mellon, which is focused on predicting ground motion during earthquakes. Archimedes includes a domain-specific language and compiler for compiling and executing unstructured finite element simulations on parallel computers. <p> Once the compiler interface is designed, the composition system can effectively perform the analysis, creating a new set of inspector/executor loop nests that it returns to the compiler. 6.2 Archimedes The Quake project <ref> [7] </ref> at Carnegie Mellon focuses on predicting the ground motion during large earthquakes. At its heart is Archimedes [53], a system for executing unstructured finite element simulations on parallel computers. Figure 6.3 shows the structure of Archimedes. An Archimedes problem consists of two inputs.
Reference: [8] <author> R. Barrett, M. Berry, T.F. Chan, J. Demmel, J. Donato, J Dongarra, V. Eijkhout, R. Pozo, C. Romine, and H. Van der Vorst. </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, 2nd Edition. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Pennsylvania, </address> <year> 1994. </year>
Reference-contexts: At a higher level, Barrett et al. <ref> [8] </ref> use the concept of templates in a numerical computation context. In their system, templates are designed and written in a high-level language to handle specific features of iterative solvers for linear systems. For example, the programmer could write different libraries to solve 105 106 CHAPTER 7.
Reference: [9] <author> R. Barriuso and Knies A. </author> <title> SHMEM user's guide for C. </title> <type> Technical report, </type> <institution> Cray Research Inc., </institution> <month> June 20 </month> <year> 1994. </year> <note> Revision 2.1. </note>
Reference-contexts: not confused. * Each processor must allocate all necessary communication buffers at the beginning and deallocate them at the end. * The processors must interleave the sends and receives as necessary to prevent communication dead lock. * On distributed memory machines with shared memory support, such as the Cray T3D <ref> [1, 9] </ref> and T3E [49], the algorithm might need to be modified so that the communication set loop writes directly to remote memory rather than going through communication buffers [56]. <p> The direct deposit model takes this idea one step further. On machines with fine-grained remote store capabilities, like the Cray T3D <ref> [1, 9] </ref> and T3E [49], as well as on true shared-memory machines, the direct deposit model allows the sender to compute the destination addresses and perform remote stores for each element, rather than using communication buffers and explicit communication operations.
Reference: [10] <author> G.E. Blelloch, S. Chatterjee, J.C. Hardwick, J. Sipelstein, and M. Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1):414, </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: Because of the irregular distribution of the unstructured mesh's data structures, executing these high-level operations in parallel requires complex code. In addition, the irregular distributions result in complex communication requirements, similar to above. Archimedes is described in more detail in Section 6.2. * NESL. NESL <ref> [10] </ref> is a nested data parallel language that operates on distributed segmented vectors. Each vector operation is considered a high-level operation because it operates on all elements of the vector at once.
Reference: [11] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, S. Ranka, and M.-Y. Wu. </author> <title> Compiling Fortran 90D/HPF for distributed memory MIMD computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1):1526, </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: After making these decisions, it is up to the programmer to convert each loop or loop nest into the inspector and the executor, making use of the routines in the CHAOS library. More recently, several parallelizing compilers <ref> [74, 11, 45, 75] </ref> analyze the sequential loops in a program and automatically produce parallel loops with calls to the appropriate CHAOS routines. For many kinds of simple sequential loops, this translation is fairly straightforward.
Reference: [12] <author> T. Brandes. </author> <note> ADAPTOR programmer's guide version 4.0. Available as ftp://ftp.gmd.de/GMD/ adaptor/docs/pguide.ps via http://www.gmd.de/SCAI/lab/adaptor/adaptor home.html, April 1996. 113 114 BIBLIOGRAPHY </note>
Reference-contexts: This compile-time analysis was not extended to the more general block-cyclic distribution. Instead, the block-cyclic distribution was handled using runtime analysis techniques. ADAPTOR The ADAPTOR system <ref> [12] </ref> is a freely available HPF compilation system. It translates data parallel Fortran programs enhanced with array assignment statements and parallel loops into SPMD message-passing Fortran code. One drawback is that it only supports block and cyclic distributions, and not the more general block-cyclic distribution.
Reference: [13] <author> S. Chatterjee, J. Gilbert, F.J.E. Long, R. Schreiber, and S.-H. Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 149158, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: However, their analysis can be easily extended to handle computation set generation, including the runtime test to decide which of two representations leads to the more favorable loop nest. 2.3.2 RIACS algorithm The RIACS algorithm <ref> [13, 14] </ref> is the first algorithm to be developed in the class of table-driven algorithms. The table-driven algorithms make use of the fact that the access pattern of the communication set repeats at some level.
Reference: [14] <author> S. Chatterjee, J. Gilbert, F.J.E. Long, R. Schreiber, and S.-H. Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 26(1):7284, </volume> <month> April </month> <year> 1995. </year>
Reference-contexts: The two-level mapping adds complexity to the analysis because in general, such an array no longer has a true block-cyclic distribution. Several of the array assignment algorithms discussed later in this chapter deal also with the two-level mapping <ref> [14, 33, 68] </ref>, but these extensions are beyond the scope of my thesis. <p> However, their analysis can be easily extended to handle computation set generation, including the runtime test to decide which of two representations leads to the more favorable loop nest. 2.3.2 RIACS algorithm The RIACS algorithm <ref> [13, 14] </ref> is the first algorithm to be developed in the class of table-driven algorithms. The table-driven algorithms make use of the fact that the access pattern of the communication set repeats at some level.
Reference: [15] <author> S. Chatterjee, J. Gilbert, R. Schreiber, and S.-H. Teng. </author> <title> Automatic array alignment in data-parallel programs. </title> <booktitle> In Proceedings of the Twentieth ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1628, </pages> <address> Charleston, South Carolina, </address> <month> January </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: Several of the array assignment algorithms discussed later in this chapter deal also with the two-level mapping [14, 33, 68], but these extensions are beyond the scope of my thesis. Also beyond its scope are algorithms for determining the best alignments and distributions for the arrays in a program <ref> [15, 16, 73] </ref>; the discussion in this chapter assumes that distribution decisions have been made in advance, either automatically by the compiler or manually by the user. 2.2 The CMU algorithm In previous work [54, 55] I derived an algorithm for determining and iterating across both communication sets and local computation
Reference: [16] <author> S. Chatterjee, J.R. Gilbert, L. Oliker, R. Schreiber, and T. Sheffler. </author> <title> Algorithms for automatic alignment of arrays. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 38(2):145157, </volume> <month> November </month> <year> 1996. </year>
Reference-contexts: Several of the array assignment algorithms discussed later in this chapter deal also with the two-level mapping [14, 33, 68], but these extensions are beyond the scope of my thesis. Also beyond its scope are algorithms for determining the best alignments and distributions for the arrays in a program <ref> [15, 16, 73] </ref>; the discussion in this chapter assumes that distribution decisions have been made in advance, either automatically by the compiler or manually by the user. 2.2 The CMU algorithm In previous work [54, 55] I derived an algorithm for determining and iterating across both communication sets and local computation
Reference: [17] <author> F. Chow. </author> <title> A Portable Machine-Independent Global Optimizer Design and Measurements. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1984. </year>
Reference-contexts: I provide a complete list with descriptions in Appendix A. 4.5 Global optimization framework 4.5.1 Standard global optimizations Catacomb implements several standard global optimizations <ref> [2, 17] </ref>: * Constant folding and other expression simplifications. Every time a new expression is created, Cat acomb tries to fold constant expressions and exploit algebraic identities to simplify the resulting ex pressions. * Constant/copy propagation. * Dead assignment elimination. * Unreachable code elimination.
Reference: [18] <author> C. Consel and O. Danvy. </author> <title> Tutorial notes on partial evaluation. </title> <booktitle> In Proceedings of the Twentieth ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 493501, </pages> <address> Charleston, South Carolina, </address> <month> January </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: However, even at such a high level, this kind of specialized system fits well within the code composition framework I describe. 7.2 Partial evaluation 7.2.1 The relationship of code composition Like most optimizing compilation systems, Catacomb and the concept of code composition are related to the field of partial evaluation <ref> [31, 18] </ref>. The goal of a system for partial evaluation is to attempt to perform as much of the computation as possible in advance (e.g., at compile time), yielding a more efficient program.
Reference: [19] <author> T.H. Cormen, C.E. Leiserson, and R.L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Let x, y, and g be integers such that xs i + ys j = g = gcd (s i ; s j ). These integers are computed using Euclid's extended GCD algorithm [35, Volume 2]. A theorem of modular linear equations <ref> [19, Chapter 33] </ref> tells us that a solution exists if and only if gj (r j r i ) [that is, g evenly divides (r j r i )], and that one possible value for m is m = (r j r i )x=g. (If g does not evenly divide (r <p> For nontrivial queries, there are generally a number of different ways in which the query can be processed. For example, algebraic manipulation of a query may lead to an equivalent query that requires less intermediate data (similar to the classic dynamic programming problem of optimal parenthesization of matrix multiplication <ref> [19, Chapter 16] </ref>). An important part of any database system is the query optimizer, which performs several functions to try to minimize the amount of work required to process the query. There are many ways in which a query optimizer can reduce the amount of work.
Reference: [20] <author> Intel Corporation. </author> <title> Paragon User's Guide. </title> <publisher> Intel Corporation, </publisher> <address> Portland, Oregon, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: The specifics of these implementational details vary depending on the communication architecture model used (e.g., PVM [25, 63], MPI [39], or Intel NX <ref> [20] </ref>). <p> Thus only the CMU algorithm is amenable to the empty message suppression optimization. * Manage communication details. Different communication architectures have different semantics for achieving the best communication performance. For MPI [39] and NX <ref> [20] </ref>, the most efficient communication ordering is: 1. Post all receives in advance, using MPI Irecv or irecv. 2. Synchronize all processors, using MPI Barrier or gsync. 3. Execute all packs and sends, using MPI Send or csend for sending. 4.
Reference: [21] <author> R. Das, J. Saltz, and R. von Hanxleden. </author> <title> Slicing analysis and indirect accesses to distributed arrays. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, volume 768 of Lecture Notes in Computer Science, </booktitle> <pages> pages 152168, </pages> <address> Portland, Oregon, August 1993. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: However, the translation becomes more complex as more levels of indirection in the array references are added. For compiling irregular programs that use multiple levels of indirection in distributed arrays, Das, Saltz, and von Hanxleden use a technique called slicing analysis <ref> [21] </ref>. The idea behind slicing analysis is that for each loop containing a distributed array reference with multiple levels of indirection, that loop can be rewritten as several loops, each of which contains only a single level of indirection.
Reference: [22] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3):462478, </volume> <month> September </month> <year> 1994. </year>
Reference-contexts: Because the translation table is large (approximately the same size as the corresponding distributed array), the table itself needs to be distributed. CHAOS supports a paged translation table <ref> [22] </ref>, meaning that the translation table itself has a block-cyclic distribution. CHAOS also allows partial replication of the translation table, similar to Fx's data distribution with overlapped alignment described in Section 5.1.3.
Reference: [23] <author> P. Dinda and D. O'Hallaron. </author> <title> Fast message assembly using compact address relations. </title> <booktitle> In Proceedings of the International Conference on Measurement and Modeling of Computer Systems (ACM SIGMET-RICS'96), </booktitle> <pages> pages 4756, </pages> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: This happens particularly with the harder-to-analyze block-cyclic data distribution (as opposed to the simpler block and cyclic distributions). If, during the course of the program, the same array assignment executes many times, it may be advantageous to encode the communication pattern as a compact address relation <ref> [23] </ref>. This address relation is a 5.1. STRUCTURE 71 representation, possibly compressed, of the access pattern of the communication set generation on the sender and/or receiver. Figure 5.3 shows three examples of address relations.
Reference: [24] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification version 1.0, </title> <month> May </month> <year> 1993. </year>
Reference-contexts: As such, it is important to have a feel for the characteristics of this class of operations. The following are a few examples of problem domains containing complex high-level operations. * The array assignment statement. High Performance Fortran (HPF) <ref> [24] </ref> uses the array assignment statement to perform a bulk transfer of array data, and to perform data parallel computation. <p> Chapter 2 The Array Assignment Statement A strong motivating example for the ideas in this dissertation is the array assignment statement. In brief, the array assignment statement, which is a key component of High Performance Fortran (HPF) <ref> [24] </ref>, effects a parallel transfer of a sequence of elements from a source array into a destination array.
Reference: [25] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM: Parallel Virtual Machine. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1994. </year> <note> Available as http://www.netlib.org/ pvm3/book/pvm-book.html. </note>
Reference-contexts: The specifics of these implementational details vary depending on the communication architecture model used (e.g., PVM <ref> [25, 63] </ref>, MPI [39], or Intel NX [20]). <p> The barrier synchronization ensures that all receives are posted before any sends can start, and it provides a fence between messages from two array assignments. The disadvantage of this approach is that a receive buffer must be allocated for every potential sender, thus increasing the memory requirements. Unfortunately, PVM <ref> [25, 63] </ref> does not allow receive buffers to be posted in advance. Furthermore, the implementation, rather than the PVM specification, determines how much buffer space is available to temporarily store messages that have already arrived but for which no receive has been issued.
Reference: [26] <author> T. Gross, D. O'Hallaron, and J. Subhlok. </author> <title> Task parallelism in a High Performance Fortran framework. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <address> 2(3):1626, </address> <month> Fall </month> <year> 1994. </year>
Reference-contexts: The Fx compiler supports a model of data parallel tasks. The programmer specifies the granularity of the tasks and the data dependences between the tasks, and Fx automatically determines the mapping of tasks to processors <ref> [26, 62] </ref>. Fx also takes into consideration memory, bandwidth, and latency requirements in determining the task mapping [60]. Data transfer in such a task parallel program can be divided into three categories: intratask data transfer, split/join data transfer, and intertask data transfer.
Reference: [27] <author> S.K.S. Gupta, S.D. Kaushik, C.-H. Huang, and P. Sadayappan. </author> <title> On compiling array expressions for efficient execution on distributed-memory machines. </title> <type> Technical Report OSE-CISRC-4/94-TR19, </type> <institution> The Ohio State University, </institution> <month> April </month> <year> 1994. </year> <note> BIBLIOGRAPHY 115 </note>
Reference-contexts: OTHER ALGORITHMS 13 2.3 Other algorithms In this section I present an overview of other array assignment statement algorithms. I continue to refer to each algorithm by the institution at which it was developed. 2.3.1 OSU algorithm The OSU algorithm <ref> [27, 28, 29] </ref>, developed by Gupta, Kaushik, Huang, and Sadayappan, is another set-theoretic algorithm, based on unions and intersections of regular sections.
Reference: [28] <author> S.K.S. Gupta, S.D. Kaushik, C.-H. Huang, and P. Sadayappan. </author> <title> Compiling array expressions for efficient execution on distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 32(2):155172, </volume> <month> February </month> <year> 1996. </year>
Reference-contexts: OTHER ALGORITHMS 13 2.3 Other algorithms In this section I present an overview of other array assignment statement algorithms. I continue to refer to each algorithm by the institution at which it was developed. 2.3.1 OSU algorithm The OSU algorithm <ref> [27, 28, 29] </ref>, developed by Gupta, Kaushik, Huang, and Sadayappan, is another set-theoretic algorithm, based on unions and intersections of regular sections. <p> Each formulation results in a 3-deep loop nest that generates the same set, but in a different order, and with different looping overhead penalties. Gupta et al. <ref> [28] </ref> also developed a simple runtime test to decide which of these four representations leads to the most favorable loop nest. However, this test does not consider the effect of the looping structure on the memory performance.
Reference: [29] <author> S.K.S. Gupta, S.D. Kaushik, S. Mufti, S. Sharma, C.-H. Huang, and P. Sadayappan. </author> <title> On compiling array expressions for efficient execution on distributed-memory machines. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing 1993, </booktitle> <pages> pages 301305, </pages> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: OTHER ALGORITHMS 13 2.3 Other algorithms In this section I present an overview of other array assignment statement algorithms. I continue to refer to each algorithm by the institution at which it was developed. 2.3.1 OSU algorithm The OSU algorithm <ref> [27, 28, 29] </ref>, developed by Gupta, Kaushik, Huang, and Sadayappan, is another set-theoretic algorithm, based on unions and intersections of regular sections.
Reference: [30] <author> S. Hinrichs, C. Kosak, D. O'Hallaron, T. Stricker, and R. </author> <title> Take. An architecture for optimal all-to-all personalized communication. </title> <booktitle> In Proceedings of the ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 310319, </pages> <address> Cape May, New Jersey, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: However, there are some architectures for which a carefully optimized communication schedule can allow an exchange of all buffers at once without congestion <ref> [30] </ref>; the architecture component can target such a system if desired. Deposit: To use the deposit model or the direct deposit model, the communication set algorithm must be able to compute destination addresses at the same time that it computes source addresses.
Reference: [31] <author> N.D. Jones, C.K. Gomard, and P. Sestoft. </author> <title> Partial Evaluation and Automatic Program Generation. </title> <booktitle> International Series in Computer Science. </booktitle> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: However, even at such a high level, this kind of specialized system fits well within the code composition framework I describe. 7.2 Partial evaluation 7.2.1 The relationship of code composition Like most optimizing compilation systems, Catacomb and the concept of code composition are related to the field of partial evaluation <ref> [31, 18] </ref>. The goal of a system for partial evaluation is to attempt to perform as much of the computation as possible in advance (e.g., at compile time), yielding a more efficient program.
Reference: [32] <author> S. Kaushik. </author> <title> Implementation of the OSU algorithm, </title> <note> 1995. Code available from ftp://ftp.cis. ohio-state.edu/pub/hpce/compiler/Source/arraysect/t3d/. </note>
Reference-contexts: Most of the communication set algorithms, with the notable exception of the OSU algorithm, support this 72 CHAPTER 5. CATACOMB AND THE ARRAY ASSIGNMENT model. (The authors' implementation of the OSU algorithm <ref> [32] </ref>, which I used as a basis for the OSU code templates, gives no easy way to modify it to add this support.) If the algorithm does not support the deposit model, the architecture component must use the standard model instead.
Reference: [33] <author> S.D. Kaushik, C.-H. Huang, and P. Sadayappan. </author> <title> Compiling array statements for efficient execution on distributed-memory machines: Two-level mappings. </title> <booktitle> In Proceeding of the Eighth Workshop on Languages and Compilers for Parallel Computing, volume 1033 of Lecture Notes in Computer Science, pages 209223, </booktitle> <address> Columbus, Ohio, August 1995. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: The two-level mapping adds complexity to the analysis because in general, such an array no longer has a true block-cyclic distribution. Several of the array assignment algorithms discussed later in this chapter deal also with the two-level mapping <ref> [14, 33, 68] </ref>, but these extensions are beyond the scope of my thesis.
Reference: [34] <author> K. Kennedy, N. Nedeljkovic, and A. Sethi. </author> <title> A linear-time algorithm for computing the memory access sequence in data-parallel programs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 102111, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: This increases the amount of buffer space required (the CMU and OSU algorithms require only a single send buffer), and prevents overlap of communication and computation of the sets. 2.3.3 Rice algorithm The Rice algorithm <ref> [34] </ref>, also in the class of table-driven algorithms, is a modification of the RIACS algorithm. It improves the RIACS algorithm in two ways. First, it includes an algorithmic improvement in the complexity of the table generation phase.
Reference: [35] <author> D.E. Knuth. </author> <booktitle> The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1981. </year>
Reference-contexts: This equation tells us that ms i r j r i (mod s j ). Let x, y, and g be integers such that xs i + ys j = g = gcd (s i ; s j ). These integers are computed using Euclid's extended GCD algorithm <ref> [35, Volume 2] </ref>. <p> As an example, consider the template shown in Figure 4.8. This template is designed to compute the greatest common divisor (gcd) of two positive integers, using Euclid's gcd algorithm <ref> [35, Volume 2] </ref>. The basic idea of the template is to test whether the two inputs are both compile-time constants, and if they are, compute their gcd at compile time, otherwise produce code that 4.6. INTERPRETATION OF CONTROL CONSTRUCTS 55 as arguments to control functions.
Reference: [36] <author> C. Koelbel. </author> <title> Compiling Programs for Nonshared Memory Machines. </title> <type> PhD thesis, </type> <institution> Purdue University, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: In this section I give an overview of some of this previous work. 18 CHAPTER 2. THE ARRAY ASSIGNMENT STATEMENT Kali The Kali language and implementation <ref> [36] </ref> was one of the first systems to address the issue of array assignment statements in the presence of block-cyclic distributions. Although Kali did not treat the array assignment statement directly, it included substantial compile-time analysis of FORALL loops with linear subscript functions.
Reference: [37] <author> H. Korth and A. Silberschatz. </author> <title> Database System Concepts. </title> <booktitle> Advanced Computer Science Series. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1986. </year>
Reference-contexts: The size of the implementation is indicative of the complexity of the high-level operations. * Query optimization. An example outside the domain of parallel languages is query optimization in relational databases <ref> [37] </ref>. A query is a high-level operation because it is syntactically compact, yet 1.2. HIGH-LEVEL CODE COMPOSITION 3 operates on a large amount of data. <p> and to have it specialized at compile time to produce the correct version to be executed at run time. 6.3 Precompilation and query optimization in relational database systems An example outside the domain of parallel languages in which code composition is beneficial is precompi-lation and query optimization in relational databases <ref> [37] </ref>. Database application programs are written in a host language, with embedded commands to the database manager written in a data manipulation language or query language. Clearly, the host language syntax is in general quite different from the query language syntax.
Reference: [38] <author> T. MacDonald, D. Pase, and A. Meltzer. </author> <title> Addressing in Cray Research's MPP Fortran. </title> <booktitle> In Proceedings of the Third Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 161172, </pages> <note> Austrian Center for Parallel Computation, </note> <month> July </month> <year> 1992. </year>
Reference-contexts: As I discuss in Section 3.1.2, this decision adversely affects both the generality of the implementation and the potential for optimization based on compile-time knowledge. Runtime resolution The Cray MPP Fortran programming model <ref> [38, 42] </ref> supports array assignment statements with block-cyclic array distributions. However, their execution strategy for the array assignment is an extremely simplistic method known as runtime resolution, which does not scale up as the number of processors increases. To understand its problems, consider the communication set generation algorithm.
Reference: [39] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <note> 1995. Available as http://www.mcs.anl.gov/mpi/mpi-report-1.1/mpi-report.html. </note>
Reference-contexts: The specifics of these implementational details vary depending on the communication architecture model used (e.g., PVM [25, 63], MPI <ref> [39] </ref>, or Intel NX [20]). <p> Thus only the CMU algorithm is amenable to the empty message suppression optimization. * Manage communication details. Different communication architectures have different semantics for achieving the best communication performance. For MPI <ref> [39] </ref> and NX [20], the most efficient communication ordering is: 1. Post all receives in advance, using MPI Irecv or irecv. 2. Synchronize all processors, using MPI Barrier or gsync. 3. Execute all packs and sends, using MPI Send or csend for sending. 4.
Reference: [40] <author> S. Midkiff. </author> <title> Local iteration set computation for block-cyclic distributions. </title> <type> Technical Report RC-19910, </type> <institution> IBM T.J. Watson Research Center, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: As such, like the other table generation algorithms, the LSU table is large and relatively expensive to generate in the presence of large block sizes. 2.3.5 IBM algorithm Midkiff describes the IBM algorithm <ref> [40] </ref>, one in a class of linear algebraic algorithms. It essentially uses symbolic Gaussian elimination at compile time to compute the intersection of a regular section (as specified by a subscript triplet) and a block-cyclic set.
Reference: [41] <author> William Morris, </author> <title> editor. The American Heritage Dictionary of the English Language. </title> <publisher> Houghton Mifflin, </publisher> <address> Boston, Massachusetts, </address> <year> 1982. </year>
Reference-contexts: I chose the term composition for two reasons. One definition of the word composition is A putting together of parts or elements to form a whole. <ref> [41] </ref> This definition reflects the process of building up a customized sequence of code at compile time, to be executed at run time. There is also a mathematical sense of the word, as in function composition.
Reference: [42] <author> D.M. Pase, T. MacDonald, and A. Meltzer. </author> <title> MPP Fortran Programming Model. </title> <institution> Cray Research, Inc., Eagan, Minnesota, </institution> <year> 1993. </year> <note> 116 BIBLIOGRAPHY </note>
Reference-contexts: As I discuss in Section 3.1.2, this decision adversely affects both the generality of the implementation and the potential for optimization based on compile-time knowledge. Runtime resolution The Cray MPP Fortran programming model <ref> [38, 42] </ref> supports array assignment statements with block-cyclic array distributions. However, their execution strategy for the array assignment is an extremely simplistic method known as runtime resolution, which does not scale up as the number of processors increases. To understand its problems, consider the communication set generation algorithm. <p> In this case, the Fortran compiler would generate a call to foo, but Catacomb would need to produce the function foo . * Catacomb produces function headers in both ANSI C style and the older K&R style, to support older C compilers. * The Fortran compiler on the Cray T3D <ref> [42] </ref> produces external symbols and external symbol references in all uppercase. Catacomb provides an option to support this. 4.7. COMPILER INTERFACE 65 4.7.3 Extending the symbol data structure Catacomb is expected to be used in conjunction with domain-specific compilers.
Reference: [43] <author> M. Poletto, D. Engler, and M.F. Kaashoek. tcc: </author> <title> A system for fast, flexible, and high-level dynamic code generation. </title> <booktitle> In Proceedings of the ACM SIGPLAN'97 Conference on Programming Language Design and Implementation, page to appear, </booktitle> <address> Las Vegas, New Mexico, </address> <month> June </month> <year> 1997. </year> <note> ACM. </note>
Reference-contexts: Eval is understandably absent from most compiler-based languages (e.g., C), due both to the problem of implementing an interpreter or compiler in the runtime system, and to the typically large performance difference between compiled and interpreted code. The tcc compiler for the `C language <ref> [43] </ref> takes on this challenge by augmenting the C language with the backquote and eval constructs, similar to Lisp. It deals with the performance problem by adding dynamic compilation to the runtime system, including such optimizations as partial evaluation and register allocation.
Reference: [44] <author> S. Pollack and T. Sterling. </author> <title> A Guide to PL/I and Structured Programming (Third edition). </title> <publisher> Holt, Rine-hart, and Winston, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: A consequence that many C programmers may be familiar with is the inability to perform preprocessor operations like #if sizeof (int)==4. Because the preprocessor executes before the compiler, the preprocessor has no way of evaluating sizeof expressions. PL/I <ref> [44] </ref> offers a more powerful preprocessor. However, it also is incapable of a single-phase execution model, and neither it nor the C preprocessor is equipped to perform structural queries on general expressions, a feature critical to code composition.
Reference: [45] <author> R. Ponnusamy, Y.-S. Hwang, R. Das, J. Saltz, A. Choudhary, and G. Fox. </author> <title> Supporting irregular distributions on FORTRAN 90D/HPF compilers. </title> <type> Technical Report CS-TR-3268, </type> <institution> Department of Computer Science, University of Maryland, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: After making these decisions, it is up to the programmer to convert each loop or loop nest into the inspector and the executor, making use of the routines in the CHAOS library. More recently, several parallelizing compilers <ref> [74, 11, 45, 75] </ref> analyze the sequential loops in a program and automatically produce parallel loops with calls to the appropriate CHAOS routines. For many kinds of simple sequential loops, this translation is fairly straightforward.
Reference: [46] <author> D.J. Salomon. </author> <title> Using partial evaluation in support of portability, reusability, and maintainability. </title> <editor> In Tibor Gyim othy, editor, </editor> <booktitle> Proceeding of the Sixth International Conference on Compiler Construction, volume 1060 of Lecture Notes in Computer Science, pages 208222, </booktitle> <address> Link oping, Sweden, April 1996. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: While this is an interesting way to gain compile-time control over the structure of an expression, in practice the specifications end up being overly complex and unreadable. There are other macro extensions to C (e.g., Safer C <ref> [46] </ref> and Programmable Syntax Macros [72]) that offer many of the same benefits as Catacomb. Safer C allows the programmer to annotate code execution at one of 5 times: compile time, link time, load time, frame activation time, and (of course) run time.
Reference: [47] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Multiprocessors and run-time compilation. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(6):573592, </volume> <month> December </month> <year> 1991. </year>
Reference-contexts: CHAOS also allows partial replication of the translation table, similar to Fx's data distribution with overlapped alignment described in Section 5.1.3. Although PARTI/CHAOS was designed to be used as the target of a parallelizing compiler <ref> [47] </ref>, it took some time for compiler support to catch up with the library development. As such, the application programmer had to use the routines manually to transform a sequential program into a parallel version.
Reference: [48] <author> J. Saltz, R. Ponnusamy, S.D. Sharma, B. Moon, Y.-S. Hwang, M. Uyasl, and R. Das. </author> <title> A manual for the CHAOS runtime library. </title> <type> Technical Report CS-TR-3437, </type> <institution> Department of Computer Science, University of Maryland, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: DATA TRANSFER IN IRREGULAR APPLICATIONS 99 To date, the most effective means of executing irregular computations is through the use of the PARTI or CHAOS runtime libraries <ref> [64, 48, 50] </ref>, developed by Saltz et al. These libraries contain sophisticated routines that help the programmer or compiler translate a sequential program into a parallel program that uses the inspector/executor approach.
Reference: [49] <author> S.L. Scott. </author> <title> Synchronization and communication in the T3E multiprocessor. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2636, </pages> <address> Cambridge, Massachusetts, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: processor must allocate all necessary communication buffers at the beginning and deallocate them at the end. * The processors must interleave the sends and receives as necessary to prevent communication dead lock. * On distributed memory machines with shared memory support, such as the Cray T3D [1, 9] and T3E <ref> [49] </ref>, the algorithm might need to be modified so that the communication set loop writes directly to remote memory rather than going through communication buffers [56]. The specifics of these implementational details vary depending on the communication architecture model used (e.g., PVM [25, 63], MPI [39], or Intel NX [20]). <p> The direct deposit model takes this idea one step further. On machines with fine-grained remote store capabilities, like the Cray T3D [1, 9] and T3E <ref> [49] </ref>, as well as on true shared-memory machines, the direct deposit model allows the sender to compute the destination addresses and perform remote stores for each element, rather than using communication buffers and explicit communication operations.
Reference: [50] <author> S.D. Sharma, R. Ponnusamy, B. Moon, Y.-S. Hwang, R. Das, and J. Saltz. </author> <title> Run-time and compile-time support for adaptive irregular problems. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 97106, </pages> <address> Washington, DC, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: DATA TRANSFER IN IRREGULAR APPLICATIONS 99 To date, the most effective means of executing irregular computations is through the use of the PARTI or CHAOS runtime libraries <ref> [64, 48, 50] </ref>, developed by Saltz et al. These libraries contain sophisticated routines that help the programmer or compiler translate a sequential program into a parallel program that uses the inspector/executor approach.
Reference: [51] <author> J.R. Shewchuk. </author> <title> Triangle: Engineering a 2D quality mesh generator and delaunay triangulator. </title> <booktitle> In First Workshop on Applied Computational Geometry, </booktitle> <pages> pages 124133, </pages> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: The other input is a program written by an engineer in a high-level language that describes the numerical solution to a partial differential equation over the input problem domain. The input problem domain is first given to Triangle <ref> [51] </ref> (a two-dimensional Delaunay triangulation mesh generator) or Pyramid [52] (a three-dimensional Delaunay-based mesh generator) to create a triangular 102 CHAPTER 6. OTHER CODE COMPOSITION DOMAINS or tetrahedral finite element mesh. Next, a component called Slice partitions the mesh into one sub-mesh per processor, using a geometric partitioning algorithm [65].
Reference: [52] <author> J.R. Shewchuk. </author> <title> Delaunay Refinement Mesh Generation. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: The other input is a program written by an engineer in a high-level language that describes the numerical solution to a partial differential equation over the input problem domain. The input problem domain is first given to Triangle [51] (a two-dimensional Delaunay triangulation mesh generator) or Pyramid <ref> [52] </ref> (a three-dimensional Delaunay-based mesh generator) to create a triangular 102 CHAPTER 6. OTHER CODE COMPOSITION DOMAINS or tetrahedral finite element mesh. Next, a component called Slice partitions the mesh into one sub-mesh per processor, using a geometric partitioning algorithm [65].
Reference: [53] <author> J.R. Shewchuk and O. Ghattas. </author> <title> A compiler for parallel finite element methods with domain-decomposed unstructured meshes. </title> <editor> In David E. Keyes and Jinchao Xu, editors, </editor> <booktitle> Proceedings of the Seventh International Conference on Domain Decomposition Methods in Scientific and Engineering Computing, volume 180 of Contemporary Mathematics, </booktitle> <pages> pages 445450. </pages> <publisher> American Mathematical Society, </publisher> <month> October </month> <year> 1993. </year>
Reference-contexts: In this case, the entire loop itself serves as a complex high-level operation. Section 6.1 describes the problem of irregular data transfer in more detail. * Archimedes. Archimedes <ref> [53] </ref> is the language and compiler portion of the Quake project [7] at Carnegie Mellon, which is focused on predicting ground motion during earthquakes. Archimedes includes a domain-specific language and compiler for compiling and executing unstructured finite element simulations on parallel computers. <p> At its heart is Archimedes <ref> [53] </ref>, a system for executing unstructured finite element simulations on parallel computers. Figure 6.3 shows the structure of Archimedes. An Archimedes problem consists of two inputs.
Reference: [54] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating communication for array statements: Design, implementation, and evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1):150159, </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: scope are algorithms for determining the best alignments and distributions for the arrays in a program [15, 16, 73]; the discussion in this chapter assumes that distribution decisions have been made in advance, either automatically by the compiler or manually by the user. 2.2 The CMU algorithm In previous work <ref> [54, 55] </ref> I derived an algorithm for determining and iterating across both communication sets and local computation sets for the array assignment statement in which the arrays have block-cyclic distributions.
Reference: [55] <author> J.M. Stichnoth. </author> <title> Efficient compilation of array statements for private memory systems. </title> <type> Technical Report CMU-CS-93-109, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year> <note> BIBLIOGRAPHY 117 </note>
Reference-contexts: scope are algorithms for determining the best alignments and distributions for the arrays in a program [15, 16, 73]; the discussion in this chapter assumes that distribution decisions have been made in advance, either automatically by the compiler or manually by the user. 2.2 The CMU algorithm In previous work <ref> [54, 55] </ref> I derived an algorithm for determining and iterating across both communication sets and local computation sets for the array assignment statement in which the arrays have block-cyclic distributions. <p> The relaxed approach only needs to compute the exact lower bound after all intersections have been computed in the four-parameter regular set representation. 2.2.4 Communication set Through a tedious set of algebraic manipulations <ref> [55] </ref>, I derived an algorithm for generating the communication set for the sending processor p. This algorithm generates the union of regular set intersections, including the application of the M ap function, specified in Figure 2.7. Figure 2.8 gives the pseudo-code for this computation. <p> With one left-hand side array and one right-hand side array, there are nine pack/send algorithms and nine receive/unpack algorithms to manage. For the computation set generation, there are three algorithms to manage. Refer to the original technical report <ref> [55] </ref> for the complete specifications of the additional algorithms. 2.3. OTHER ALGORITHMS 13 2.3 Other algorithms In this section I present an overview of other array assignment statement algorithms.
Reference: [56] <author> T. Stricker. </author> <title> Direct Deposit: A Communication Architecture for Parallel and Distributed Programs. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: as necessary to prevent communication dead lock. * On distributed memory machines with shared memory support, such as the Cray T3D [1, 9] and T3E [49], the algorithm might need to be modified so that the communication set loop writes directly to remote memory rather than going through communication buffers <ref> [56] </ref>. The specifics of these implementational details vary depending on the communication architecture model used (e.g., PVM [25, 63], MPI [39], or Intel NX [20]). <p> Address-data blocks provide some compression when the sequence of addresses consists of long sub-sequences with a fixed stride. A modified form of address-data blocks can be used when the length of each fixed-stride block is the same. model <ref> [56, 59] </ref> of communication. In the standard model of communication, the sending processor executes a communication set algorithm to pack elements from the source array into a communication buffer, and then sends the buffer to the receiving processor.
Reference: [57] <author> T. Stricker and T. Gross. </author> <title> Optimizing memory system performance for communication in parallel computers. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <address> Portofino, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The extra benefit here is the decrease in buffering and memory bandwidth due to the direct deposit model not using explicit communication buffers <ref> [57, 58] </ref>. The second desirable capability is to support the saving and reuse of communication patterns. For some array assignments in conjunction with some array assignment algorithms, the loop overhead in the communication set generation is large in comparison to the actual data accesses.
Reference: [58] <author> T. Stricker and T. Gross. </author> <title> Global address space, non-uniform bandwidth: A memory system performance characterization of parallel systems. </title> <booktitle> In Proceedings of the Third International Symposium on High-Performance Computer Architecture, </booktitle> <address> San Antonio, Texas, </address> <month> February </month> <year> 1997. </year>
Reference-contexts: The extra benefit here is the decrease in buffering and memory bandwidth due to the direct deposit model not using explicit communication buffers <ref> [57, 58] </ref>. The second desirable capability is to support the saving and reuse of communication patterns. For some array assignments in conjunction with some array assignment algorithms, the loop overhead in the communication set generation is large in comparison to the actual data accesses.
Reference: [59] <author> T. Stricker, J. Stichnoth, D. O'Hallaron, S. Hinrichs, and T. Gross. </author> <title> Decoupling synchronization and data transfer in message passing systems of parallel computers. </title> <booktitle> In Proceedings of the Ninth International Conference on Supercomputing, </booktitle> <pages> pages 110, </pages> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: Address-data blocks provide some compression when the sequence of addresses consists of long sub-sequences with a fixed stride. A modified form of address-data blocks can be used when the length of each fixed-stride block is the same. model <ref> [56, 59] </ref> of communication. In the standard model of communication, the sending processor executes a communication set algorithm to pack elements from the source array into a communication buffer, and then sends the buffer to the receiving processor.
Reference: [60] <author> J. Subhlok, D. O'Hallaron, T. Gross, P. Dinda, and J. Webb. </author> <title> Communication and memory requirements as the basis for mapping task and data parallel programs. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 330339, </pages> <address> Washington, DC, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: The programmer specifies the granularity of the tasks and the data dependences between the tasks, and Fx automatically determines the mapping of tasks to processors [26, 62]. Fx also takes into consideration memory, bandwidth, and latency requirements in determining the task mapping <ref> [60] </ref>. Data transfer in such a task parallel program can be divided into three categories: intratask data transfer, split/join data transfer, and intertask data transfer. Split/join data transfer happens when a task splits itself into several smaller subtasks, or when the subtasks complete and rejoin the parent task.
Reference: [61] <author> J. Subhlok, J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Exploiting task and data parallelism on a multicomputer. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 1322, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In Section 2.1, I describe the array assignment statement in greater detail, including the standard HPF block-cyclic data distribution. In Section 2.2, I describe my solution, referred to as the CMU algorithm, which I implemented in the Fx parallelizing compiler <ref> [61] </ref>. In Section 2.3, I compare and contrast other approaches to the array assignment statement problem. <p> To address runtime performance issues, the systems include special-case code for the more common occurrences. To address the generality issue, the systems also include extra code for, e.g., different numbers of dimensions, or else they simply ignore all but the canonical case. Only one system, the Fx compiler <ref> [61] </ref>, uses a full custom code generation approach. The improved efficiency and generality come at a cost: in the original version of Fx, the custom code generation component consisted of over 15,000 lines of C code in the compiler, and implemented only the CMU algorithm. <p> This chapter describes the design, development, and performance of the Catacomb implementation of the array assignment. This framework was implemented in the context of Fx parallelizing Fortran compiler <ref> [61] </ref>. To distinguish the implementation of the array assignment from other possible uses of Catacomb, I refer to it in this chapter as Catacomb/Fx. <p> A particular use of the finalization template is to produce code to deallocate memory allocated during the table generation phase. 5.1.2 Supporting task parallelism There exist a variety of ways to decompose parallelism into data parallel methods and task parallel methods <ref> [61] </ref>, as shown in Figure 5.5. At one end of the spectrum, data parallel programs are typically expressed 5.1. STRUCTURE 75 as a single thread of control operating on data sets that are distributed across all processors.
Reference: [62] <author> J. Subhlok and G. Vondran. </author> <title> Optimal mapping of sequences of data parallel tasks. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 134143, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The Fx compiler supports a model of data parallel tasks. The programmer specifies the granularity of the tasks and the data dependences between the tasks, and Fx automatically determines the mapping of tasks to processors <ref> [26, 62] </ref>. Fx also takes into consideration memory, bandwidth, and latency requirements in determining the task mapping [60]. Data transfer in such a task parallel program can be divided into three categories: intratask data transfer, split/join data transfer, and intertask data transfer.
Reference: [63] <author> V.S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4):315339, </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: The specifics of these implementational details vary depending on the communication architecture model used (e.g., PVM <ref> [25, 63] </ref>, MPI [39], or Intel NX [20]). <p> The barrier synchronization ensures that all receives are posted before any sends can start, and it provides a fence between messages from two array assignments. The disadvantage of this approach is that a receive buffer must be allocated for every potential sender, thus increasing the memory requirements. Unfortunately, PVM <ref> [25, 63] </ref> does not allow receive buffers to be posted in advance. Furthermore, the implementation, rather than the PVM specification, determines how much buffer space is available to temporarily store messages that have already arrived but for which no receive has been issued.
Reference: [64] <author> A. Sussman, G. Agrawal, and J. Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives, revision 4.1. </title> <type> Technical Report CS-TR-3070.1, </type> <institution> University of Maryland, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: DATA TRANSFER IN IRREGULAR APPLICATIONS 99 To date, the most effective means of executing irregular computations is through the use of the PARTI or CHAOS runtime libraries <ref> [64, 48, 50] </ref>, developed by Saltz et al. These libraries contain sophisticated routines that help the programmer or compiler translate a sequential program into a parallel program that uses the inspector/executor approach.
Reference: [65] <author> S.-H. Teng. </author> <title> Points, Spheres, and Separators: A Unified Geometric Approach to Graph Partitioning. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: OTHER CODE COMPOSITION DOMAINS or tetrahedral finite element mesh. Next, a component called Slice partitions the mesh into one sub-mesh per processor, using a geometric partitioning algorithm <ref> [65] </ref>. Slice also tries to optimize placement and routing of the sub-meshes onto the processors. Finally, a tool called Parcel compiles the irregular communication patterns into the resulting executable. In addition, there is a compiler component called Author, which presents the engineer/programmer with a language nearly identical to C.
Reference: [66] <author> R. Thakur, A. Choudhary, and G. Fox. </author> <title> Runtime array redistribution in HPF programs. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <pages> pages 309316, </pages> <address> Knoxville, Tennessee, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Furthermore, it also addresses the issue of memory allocation and local memory mapping for two-level mappings. 2.3.7 Syracuse algorithm The Syracuse algorithm, developed by Thakur, Choudhary, and Fox <ref> [66] </ref>, is not a full array assignment statement algorithm. It only handles redistributions of whole arrays (rather than array sections) that are distributed over the same number of processors. The formulation does not fit into any of the above categories of algorithms. <p> This task is tedious even for small values of d and k, and severely hampers the maintainability of the routines. Because of the complexity of dealing with multidimensional arrays, the Syracuse algorithm <ref> [66] </ref> actually advocates a separate communication phase for each dimension of the array. For example, to change an array distribution from (block,block) to (cyclic,cyclic), the algorithm first attacks one dimension by changing to, e.g., (block,cyclic), and then going from there to (cyclic,cyclic). <p> Finally, in Section 5.2.4, I consider the effects of the optimizations I discussed previously in Sections 4.5 and 4.6.5. 5.2.1 Evaluation of the Syracuse approach Section 2.3.7 describes the Syracuse algorithm <ref> [66] </ref> for redistributing arrays. The Syracuse algorithm is not a general array assignment algorithm, as it only allows redistributions of entire arrays, rather than arbitrary array sections (i.e., the subscript triplets have fixed values). <p> The authors correctly observe that extending the special cases of the canonical algorithm to a multidimensional algorithm causes an exponential blowup in the amount of code to write: This method requires different algorithms for different numbers of dimensions and different types of redistributions and these algorithms cannot be optimized much. <ref> [66] </ref> To avoid the exponential blowup in the direct approach, optimized special cases can be applied to at most one array dimension; the rest of the dimensions must be handled using the potentially inefficient general case. <p> I present data below indicating that the indirect approach does inflict the expected performance penalty, and that the technique of code composition achieves generality without sacrificing efficiency or maintainability. The original Syracuse paper <ref> [66] </ref> presents performance data for redistributing a 1024 fi 1024 element array from a (block,block) distribution to a (cyclic,cyclic) distribution. Their experiment was performed on an Intel Paragon, with the number of processors varying from 2 to 32.
Reference: [67] <author> A. Thirumalai and J. Ramanujam. </author> <title> Fast address sequence generation for data-parallel programs using integer lattices. </title> <booktitle> In Proceedings of the Eighth Workshop on Languages and Compilers for Parallel Computing, volume 1033 of Lecture Notes in Computer Science, pages 191208, </booktitle> <address> Columbus, Ohio, August 1995. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: Like the RIACS algorithm, the table in the Rice algorithm becomes relatively large and expensive to generate as the block size becomes large. 2.3.4 LSU algorithm The LSU algorithm <ref> [67, 68] </ref> is also a modification of the RIACS algorithm. Like the Rice algorithm, it reduces the table generation complexity from O (b log b) to O (b). It does not, however, attempt to augment the table with destination address information like the Rice algorithm does.
Reference: [68] <author> A. Thirumalai and J. Ramanujam. </author> <title> Efficient computation of address sequences in data-parallel programs using closed forms for basis vectors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 38(2):188 203, </volume> <month> November </month> <year> 1996. </year>
Reference-contexts: The two-level mapping adds complexity to the analysis because in general, such an array no longer has a true block-cyclic distribution. Several of the array assignment algorithms discussed later in this chapter deal also with the two-level mapping <ref> [14, 33, 68] </ref>, but these extensions are beyond the scope of my thesis. <p> Like the RIACS algorithm, the table in the Rice algorithm becomes relatively large and expensive to generate as the block size becomes large. 2.3.4 LSU algorithm The LSU algorithm <ref> [67, 68] </ref> is also a modification of the RIACS algorithm. Like the Rice algorithm, it reduces the table generation complexity from O (b log b) to O (b). It does not, however, attempt to augment the table with destination address information like the Rice algorithm does.
Reference: [69] <author> T. Veldhuizen. </author> <title> Expression templates. C++ Report, </title> <address> 7(5):2631, </address> <month> June </month> <year> 1995. </year> <note> 118 BIBLIOGRAPHY </note>
Reference-contexts: The C++ template system provides a simple way to generate new functions and methods, tailored to a specific data type. Veldhuizen <ref> [69] </ref> has developed a mechanism called expression templates, which allows the template system to compose code in more complex ways, based on the structure of input expressions.
Reference: [70] <author> C. Verbrugge, P. Co, and L. Hendren. </author> <title> Generalized constant propagation: A study in C. </title> <editor> In Tibor Gyim othy, editor, </editor> <booktitle> Proceeding of the Sixth International Conference on Compiler Construction, volume 1060 of Lecture Notes in Computer Science, </booktitle> <pages> pages 7490, </pages> <address> Link oping, Sweden, April 1996. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: All of these optimizations are based on bounds analysis. Bounds analysis, which is similar to the technique of generalized constant propagation <ref> [70] </ref>, is based on the following observation. Sometimes, even 48 CHAPTER 4. CATACOMB though the compiler cannot determine a specific value for a variable or expression, it can determine that it must belong to a range of values. Consider an example from the domain of the distributed array assignment statement.
Reference: [71] <author> L. Wang, J. Stichnoth, and S. Chatterjee. </author> <title> Runtime performance of parallel array assignment: An empirical study. </title> <booktitle> In Proceedings of Supercomputing '96, </booktitle> <address> Pittsburgh, Pennsylvania, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: Following the nomenclature introduced by Wang, Stichnoth, and Chatterjee <ref> [71] </ref>, I name this algorithm, as well as the algorithms described in Section 2.3, after their places of origin; hence the CMU algorithm here. <p> It does not, however, attempt to augment the table with destination address information like the Rice algorithm does. In practice, the LSU algorithm significantly outperforms both the RIACS and the Rice algorithms for generating the table <ref> [71] </ref>. Note that 16 CHAPTER 2. THE ARRAY ASSIGNMENT STATEMENT the LSU algorithm generates the same table as the RIACS algorithm, so that using the table to iterate through the array is exactly the same; only the overhead of table generation changes. <p> The compiler writer may wish to incorporate and explore the performance of several different array assignment statement algorithms. This approach is reasonable even for a production HPF-like compiler, because in practice, no single algorithm outperforms all other algorithms under all circumstances <ref> [71] </ref>. The simplest way to incorporate a new algorithm into the compiler is to directly use the author's own implementation, and the most easily exchangeable form of an implementation is a runtime library routine. <p> I must stress that it is not the purpose of these experiments to compare the performance of the array assignment algorithms head to head. Wang, Stichnoth, and Chatterjee compared these algorithms elsewhere <ref> [71] </ref>, using Catacomb/Fx to generate the code, and found that for each algorithm, there is a class of array assignment statements for which that algorithm outperforms the others.
Reference: [72] <author> D. Weise and R. </author> <title> Crew. Programmable syntax macros. </title> <booktitle> In Proceedings of the ACM SIGPLAN'93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 156165, </pages> <address> Albuquerque, New Mexico, </address> <month> June </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: While this is an interesting way to gain compile-time control over the structure of an expression, in practice the specifications end up being overly complex and unreadable. There are other macro extensions to C (e.g., Safer C [46] and Programmable Syntax Macros <ref> [72] </ref>) that offer many of the same benefits as Catacomb. Safer C allows the programmer to annotate code execution at one of 5 times: compile time, link time, load time, frame activation time, and (of course) run time.
Reference: [73] <author> S. Wholey. </author> <title> Automatic Data Mapping for Distributed-Memory Parallel Computers. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Several of the array assignment algorithms discussed later in this chapter deal also with the two-level mapping [14, 33, 68], but these extensions are beyond the scope of my thesis. Also beyond its scope are algorithms for determining the best alignments and distributions for the arrays in a program <ref> [15, 16, 73] </ref>; the discussion in this chapter assumes that distribution decisions have been made in advance, either automatically by the compiler or manually by the user. 2.2 The CMU algorithm In previous work [54, 55] I derived an algorithm for determining and iterating across both communication sets and local computation
Reference: [74] <author> J. Wu, R. Das, J. Saltz, H. Berryman, and S. Hiranandani. </author> <title> Distributed memory compiler design for sparse problems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(6):737754, </volume> <month> June </month> <year> 1995. </year>
Reference-contexts: After making these decisions, it is up to the programmer to convert each loop or loop nest into the inspector and the executor, making use of the routines in the CHAOS library. More recently, several parallelizing compilers <ref> [74, 11, 45, 75] </ref> analyze the sequential loops in a program and automatically produce parallel loops with calls to the appropriate CHAOS routines. For many kinds of simple sequential loops, this translation is fairly straightforward.
Reference: [75] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrota, and A. Schwald. </author> <title> Vienna Fortran a language specification version 1.1. </title> <type> Technical Report ACPC/TR 92-4, </type> <institution> Austrian Center for Parallel Computation, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: After making these decisions, it is up to the programmer to convert each loop or loop nest into the inspector and the executor, making use of the routines in the CHAOS library. More recently, several parallelizing compilers <ref> [74, 11, 45, 75] </ref> analyze the sequential loops in a program and automatically produce parallel loops with calls to the appropriate CHAOS routines. For many kinds of simple sequential loops, this translation is fairly straightforward.
References-found: 75

