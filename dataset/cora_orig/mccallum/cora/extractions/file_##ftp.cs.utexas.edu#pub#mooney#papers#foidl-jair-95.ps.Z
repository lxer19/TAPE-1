URL: file://ftp.cs.utexas.edu/pub/mooney/papers/foidl-jair-95.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/foidl.html
Root-URL: 
Title: Induction of First-Order Decision Lists:  
Note: Journal of Artificial Intelligence Research 3 (1995) 1-24 Submitted 1/95; published 6/95  
Abstract: Results on Learning the Past Tense of English Verbs Abstract This paper presents a method for inducing logic programs from examples that learns a new class of concepts called first-order decision lists, defined as ordered lists of clauses each ending in a cut. The method, called Foidl, is based on Foil (Quinlan, 1990) but employs intensional background knowledge and avoids the need for explicit negative examples. It is particularly useful for problems that involve rules with specific exceptions, such as learning the past-tense of English verbs, a task widely studied in the context of the symbolic/connectionist debate. Foidl is able to learn concise, accurate programs for this problem from significantly fewer examples than previous methods (both connectionist and symbolic).
Abstract-found: 1
Intro-found: 1
Reference: <author> Bain, M. </author> <year> (1992). </year> <title> Experiments in non-monotonic first-order induction. </title> <editor> In Muggleton, S. (Ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 423-435. </pages> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> Bain, M., & Muggleton, S. </author> <year> (1992). </year> <title> Non-monotonic learning. </title> <editor> In Muggleton, S. (Ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 145-162. </pages> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> Bergadano, F. </author> <year> (1993). </year> <title> An interactive system to learn functional logic programs. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial intelligence, </booktitle> <pages> pp. </pages> <address> 1044-1049 Chambery, France. </address>
Reference-contexts: All rights reserved. Mooney & Califf 2. Explicit negative examples are frequently unavailable and an adequate set of negative examples computed using a closed-world assumption is infinite or intractably large. 3. Concise representation of many concepts requires the use of clause-ordering and/or cuts <ref> (Bergadano, Gunetti, & Trinchero, 1993) </ref>. This paper presents a new ILP method called Foidl (First-Order Induction of Decision Lists) which helps overcome each of these limitations by incorporating the following properties: 1. Background knowledge is represented intensionally as a logic program. 2. <p> However, output completeness permits more flexibility by allowing some arguments to be specified as inputs and only counting as negative examples those extra outputs generated for specific inputs in the training set. Flip <ref> (Bergadano, 1993) </ref> provides a method for learning functional programs without negative examples by making an assumption equivalent to output completeness for the functional case. Output completeness is more general in that it permits learning non-functional programs as well.
Reference: <author> Bergadano, F., Gunetti, D., & Trinchero, U. </author> <year> (1993). </year> <title> The difficulties of learning logic programs with cut. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 91-107. </pages>
Reference-contexts: All rights reserved. Mooney & Califf 2. Explicit negative examples are frequently unavailable and an adequate set of negative examples computed using a closed-world assumption is infinite or intractably large. 3. Concise representation of many concepts requires the use of clause-ordering and/or cuts <ref> (Bergadano, Gunetti, & Trinchero, 1993) </ref>. This paper presents a new ILP method called Foidl (First-Order Induction of Decision Lists) which helps overcome each of these limitations by incorporating the following properties: 1. Background knowledge is represented intensionally as a logic program. 2. <p> However, output completeness permits more flexibility by allowing some arguments to be specified as inputs and only counting as negative examples those extra outputs generated for specific inputs in the training set. Flip <ref> (Bergadano, 1993) </ref> provides a method for learning functional programs without negative examples by making an assumption equivalent to output completeness for the functional case. Output completeness is more general in that it permits learning non-functional programs as well.
Reference: <author> Califf, M. E. </author> <year> (1994). </year> <title> Learning the past tense of English verbs: An inductive logic programming approach. </title> <type> Unpublished project report. </type>
Reference-contexts: Although ILP methods seem more appropriate for this problem, our initial attempts to apply Foil and Golem to past-tense learning gave very disappointing results <ref> (Califf, 1994) </ref>. Below, we discuss how the three problems listed in the introduction contribute to the difficulty of applying current ILP methods to this problem.
Reference: <author> Cameron-Jones, R. M., & Quinlan, J. R. </author> <year> (1994). </year> <title> Efficient top-down induction of logic programs. </title> <journal> SIGART Bulletin, </journal> <volume> 5 (1), </volume> <pages> 33-42. </pages>
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-284. </pages>
Reference-contexts: In the original algorithm of Rivest (1987) and in CN2 <ref> (Clark & Niblett, 1989) </ref>, rules are learned in the order they appear in the final decision list (i.e., new rules are appended to the end of the list as they are learned).
Reference: <author> Cohen, W. W. </author> <year> (1994). </year> <title> Pac-learning nondeterminate clauses. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 676-681 Seattle, WA. </address> <note> 21 Mooney & Califf Cohen, </note> <author> W. </author> <year> (1992). </year> <title> Compiling prior knowledge into an explicit bias. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 102-110 Aberdeen, Scotland. </address>
Reference: <author> Cotrell, G., & Plunkett, K. </author> <year> (1991). </year> <title> Learning the past tense in a recurrent network: Acquiring the mapping from meaning to sounds. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. </pages> <address> 328-333 Chicago, IL. </address>
Reference-contexts: The results on this approach are significantly better than normal SPA but still inferior to Foidl's results. Also, this approach still requires a fixed-sized input window which prevents it from handling arbitrary-length irregular verbs. Recurrent neural networks could also be used to avoid word-length restrictions <ref> (Cotrell & Plunkett, 1991) </ref>, although it appears that no one has yet applied them to the standard present-tense to past-tense mapping problem. However, we believe the difficulty of training recurrent networks and their relatively poor ability to maintain state information arbitrarily long would limit their performance on this task.
Reference: <author> De Raedt, L., & Bruynooghe, M. </author> <year> (1993). </year> <title> A theory of clausal discovery. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial intelligence, </booktitle> <pages> pp. </pages> <address> 1058-1063 Chambery, France. </address>
Reference-contexts: The non-monotonic semantics used to eliminate the need for negative examples in Claudien <ref> (De Raedt & Bruynooghe, 1993) </ref> has the same effect as an output completeness assumption in the case where all arguments of the target relation are outputs.
Reference: <author> Dzeroski, S., Muggleton, S., & Russell, S. </author> <year> (1992). </year> <booktitle> Pac-learnability of determinate logic programs.. In Proceedings of the 1992 Workshop on Computational Learning Theory Pittsburgh, </booktitle> <address> PA. </address>
Reference: <author> Lachter, J., & Bever, T. </author> <year> (1988). </year> <title> The relation between linguistic structure and associative theories of language learning: A constructive critique of some connectionist learning models. </title> <editor> In Pinker, S., & Mehler, J. (Eds.), </editor> <booktitle> Connections and Symbols, </booktitle> <pages> pp. 195-247. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <editor> Lavrac, N., & Dzeroski, S. (Eds.). </editor> <year> (1994). </year> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood. </publisher>
Reference-contexts: The use of intensional background knowledge is the least distinguishing feature since a number of other ILP systems also incorporate this aspect. Focl (Pazzani & Kibler, 1992), mFoil <ref> (Lavrac & Dzeroski, 1994) </ref>, Grendel (Cohen, 1992), Forte (Richards & Mooney, 1995), and Chillin (Zelle & Mooney, 1994a) all use intensional background to some degree in the context of a Foil-like algorithm.
Reference: <author> Ling, C. X. </author> <year> (1994). </year> <title> Learning the past tense of English verbs: The symbolic pattern associ-ator vs. connectionist models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 209-229. </pages>
Reference: <author> Ling, C. X. </author> <year> (1995). </year> <type> Personal communication. </type>
Reference-contexts: However, humans can obviously produce the correct past tense of arbitrarily-long novel words, which Foidl can easily model while fixed-length feature-based representations clearly cannot. Ling also developed a version of SPA that eliminates position dependence and fixed word-length <ref> (Ling, 1995) </ref> by using a sliding window like that used in NETtalk (Sejnowski & Rosenberg, 1987).
Reference: <author> Ling, C. X., & Marinov, M. </author> <year> (1993). </year> <title> Answering the connectionist challenge: A symbolic model of learning the past tense of English verbs. </title> <journal> Cognition, </journal> <volume> 49 (3), </volume> <pages> 235-290. </pages>
Reference-contexts: Its ability to model human results on generating the past tense of novel psuedo-verbs (e.g., spling ! splang) could also be examined and compared to SPA <ref> (Ling & Marinov, 1993) </ref> and connectionist methods. Although first-order decision lists represent a fairly general class of programs, currently our only convincing experimental results are on the past-tense problem.
Reference: <author> MacWhinney, B., & Leinbach, J. </author> <year> (1991). </year> <title> Implementations are not conceptualizations: Revising the verb model. </title> <journal> Cognition, </journal> <volume> 40, </volume> <pages> 291-296. </pages>
Reference: <author> Muggleton, S., & Buntine, W. </author> <year> (1988). </year> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 339-352 Ann Arbor, MI. </address>
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1990). </year> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Learning Theory Tokyo, </booktitle> <address> Japan. </address> <publisher> Ohmsha. </publisher>
Reference-contexts: Explicit negative examples of the target predicate are available. 3. The target program is expressed in "pure" Prolog where clause-order is irrelevant and procedural operators such as cut (!) are disallowed. The currently most well-known and successful ILP systems, Golem <ref> (Muggleton & Feng, 1990) </ref> and Foil (Quinlan, 1990), both make all three of these assumptions. However, each of these assumptions brings significant limitations since: 1. An adequate extensional representation of background knowledge is frequently infinite or intractably large. c fl1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.
Reference: <author> Muggleton, S., King, R., & Sternberg, M. </author> <year> (1992). </year> <title> Protein secondary structure prediction using logic-based machine learning. </title> <journal> Protein Engineering, </journal> <volume> 5 (7), </volume> <pages> 647-657. </pages>
Reference-contexts: ILP methods have successfully induced small programs for sorting and list manipulation (Shapiro, 1983; Sammut & Banerji, 1986; Muggleton & Buntine, 1988; Quinlan & Cameron-Jones, 1993) as well as produced encouraging results on important applications such as predicting protein secondary structure <ref> (Muggleton, King, & Sternberg, 1992) </ref> and automating the construction of natural-language parsers (Zelle & Mooney, 1994b). However, current ILP techniques make important assumptions that restrict their application. Below are three common assumptions: 1. Background knowledge is provided in extensional form as a set of ground literals. 2.
Reference: <author> Muggleton, S. H. (Ed.). </author> <year> (1992). </year> <title> Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference-contexts: ILP methods have successfully induced small programs for sorting and list manipulation (Shapiro, 1983; Sammut & Banerji, 1986; Muggleton & Buntine, 1988; Quinlan & Cameron-Jones, 1993) as well as produced encouraging results on important applications such as predicting protein secondary structure <ref> (Muggleton, King, & Sternberg, 1992) </ref> and automating the construction of natural-language parsers (Zelle & Mooney, 1994b). However, current ILP techniques make important assumptions that restrict their application. Below are three common assumptions: 1. Background knowledge is provided in extensional form as a set of ground literals. 2.
Reference: <author> Norvig, P. </author> <year> (1992). </year> <title> Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <month> CA. </month> <title> 22 Induction of First-Order Decision Lists: Learning English Past Tense Pazzani, </title> <editor> M., & Kibler, D. </editor> <year> (1992). </year> <title> The utility of background knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 57-94. </pages>
Reference-contexts: Unlike the current Prolog version, the Common Lisp version supports learning recursive clauses 4 and output-completeness for non-functional target predicates. However, the Common Lisp version is significantly slower since it relies on an un-optimized Prolog interpreter and compiler written in Lisp <ref> (from Norvig, 1992) </ref>. Consequently, all of the presented results are from the Prolog version running on a Sun SPARCstation 2. 5 4.
Reference: <author> Pinker, S., & Prince, A. </author> <year> (1988). </year> <title> On language and connectionism: Analysis of a parallel distributed model of language acquisition. </title> <editor> In Pinker, S., & Mehler, J. (Eds.), </editor> <booktitle> Connections and Symbols, </booktitle> <pages> pp. 73-193. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo,CA. </address>
Reference-contexts: Ling and Marinov (1993) and Ling (1994) criticize all of the current connectionist models of past-tense acquisition for heavily-engineered representations and poor experimental methodology. They present more systematic results on a system called SPA (Symbolic Pattern Associator) which uses a slightly modified version of C4.5 <ref> (Quinlan, 1993) </ref> to build a 4 Induction of First-Order Decision Lists: Learning English Past Tense forest of decision trees that maps a fixed-length input pattern to a fixed-length output pattern.
Reference: <author> Quinlan, J. R. </author> <year> (1994). </year> <title> Past tenses of verbs and first-order learning. </title> <editor> In Zhang, C., Debenham, J., & Lukose, D. (Eds.), </editor> <booktitle> Proceedings of the Seventh Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 13-20 Singapore. </address> <publisher> World Scientific. </publisher>
Reference: <author> Quinlan, J. R., & Cameron-Jones, R. M. </author> <year> (1993). </year> <title> FOIL: A midterm report. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pp. 3-20 Vienna. </pages>
Reference-contexts: Ling and Marinov (1993) and Ling (1994) criticize all of the current connectionist models of past-tense acquisition for heavily-engineered representations and poor experimental methodology. They present more systematic results on a system called SPA (Symbolic Pattern Associator) which uses a slightly modified version of C4.5 <ref> (Quinlan, 1993) </ref> to build a 4 Induction of First-Order Decision Lists: Learning English Past Tense forest of decision trees that maps a fixed-length input pattern to a fixed-length output pattern.
Reference: <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 (3), </volume> <pages> 239-266. </pages>
Reference-contexts: Explicit negative examples of the target predicate are available. 3. The target program is expressed in "pure" Prolog where clause-order is irrelevant and procedural operators such as cut (!) are disallowed. The currently most well-known and successful ILP systems, Golem (Muggleton & Feng, 1990) and Foil <ref> (Quinlan, 1990) </ref>, both make all three of these assumptions. However, each of these assumptions brings significant limitations since: 1. An adequate extensional representation of background knowledge is frequently infinite or intractably large. c fl1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. Mooney & Califf 2.
Reference: <author> Richards, B. L., & Mooney, R. J. </author> <year> (1995). </year> <title> Automated refinement of first-order Horn-clause domain theories. Machine Learning, </title> <publisher> in press. </publisher>
Reference-contexts: The use of intensional background knowledge is the least distinguishing feature since a number of other ILP systems also incorporate this aspect. Focl (Pazzani & Kibler, 1992), mFoil (Lavrac & Dzeroski, 1994), Grendel (Cohen, 1992), Forte <ref> (Richards & Mooney, 1995) </ref>, and Chillin (Zelle & Mooney, 1994a) all use intensional background to some degree in the context of a Foil-like algorithm.
Reference: <author> Rivest, R. L. </author> . <year> (1987). </year> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2 (3), </volume> <pages> 229-246. </pages>
Reference-contexts: When answering an output query, the cuts simply eliminate all but the first answer produced when trying the clauses in order. Therefore, this representation is similar to propositional decision lists <ref> (Rivest, 1987) </ref>, which are ordered lists of pairs (rules) of the form (t i ; c i ) where the test t i is a conjunction of features and c i is a category label and an example is assigned to the category of the first pair whose test it satisfies. <p> Theoretical results on the learnability of restricted classes of first-order decision lists is another interesting area for research. Given the results on the PAC-learnability of propositional decision lists <ref> (Rivest, 1987) </ref> and restricted classes of ILP problems (Dzeroski, Muggle-ton, & Russell, 1992; Cohen, 1994), an appropriately restricted class of first-order decision lists should be PAC-learnable. 7.
Reference: <author> Rumelhart, D. E., & McClelland, J. </author> <year> (1986). </year> <title> On learning the past tense of English verbs. </title> <editor> In Rumelhart, D. E., & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 216-271. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Sammut, C., & Banerji, R. B. </author> <year> (1986). </year> <title> Learning concepts by asking questions. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), </editor> <booktitle> Machine Learning: An AI Approach, </booktitle> <volume> Vol. II, </volume> <pages> pp. 167-191. </pages> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Sejnowski, T. J., & Rosenberg, C. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168. </pages>
Reference-contexts: Ling also developed a version of SPA that eliminates position dependence and fixed word-length (Ling, 1995) by using a sliding window like that used in NETtalk <ref> (Sejnowski & Rosenberg, 1987) </ref>. A large window is used which includes 15 letters on either side of the current position (padded with blanks if necessary) in order to always include the entire word for all the examples in the corpus.
Reference: <author> Shapiro, E. </author> <year> (1983). </year> <title> Algorithmic Program Debugging. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Shavlik, J. W., Mooney, R. J., & Towell, G. G. </author> <year> (1991). </year> <title> Symbolic and neural learning algorithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 111-143. </pages>
Reference-contexts: The training times for the various systems considered in this research are difficult to compare. Ling does not provide timing results, though we can probably assume based on research comparing symbolic and neural learning algorithms <ref> (Shavlik, Mooney, & Towell, 1991) </ref> that SPA runs fairly quickly since it is based on C4.5 and that backpropagation took considerably longer. Our tests with Foil and Foidl are not directly comparable because they were run on different architectures. The Foil runs were done on a Sparc 5.
Reference: <author> Stahl, I., Tausend, B., & Wirth, R. </author> <year> (1993). </year> <title> Two methods for improving inductive logic programming systems. </title> <booktitle> In Machine Learning: ECML-93, </booktitle> <pages> pp. 41-55 Vienna. </pages>
Reference: <author> Webb, G. I., & Brkic, N. </author> <year> (1993). </year> <title> Learning decision lists by prepending inferred rules. </title> <booktitle> In Proceedings of the Australian Workshop on Machine Learning and Hybrid Systems, </booktitle> <pages> pp. </pages> <address> 6-10 Melbourne, Australia. </address>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1994a). </year> <title> Combining top-down and bottom-up methods in inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning New Brunswick, </booktitle> <address> NJ. </address> <note> 23 Mooney & Califf Zelle, </note> <author> J. M., & Mooney, R. J. </author> <year> (1994b). </year> <title> Inducing deterministic Prolog parsers from treebanks: A machine learning approach. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 748-753 Seattle, WA. </address> <month> 24 </month>
Reference-contexts: The use of intensional background knowledge is the least distinguishing feature since a number of other ILP systems also incorporate this aspect. Focl (Pazzani & Kibler, 1992), mFoil (Lavrac & Dzeroski, 1994), Grendel (Cohen, 1992), Forte (Richards & Mooney, 1995), and Chillin <ref> (Zelle & Mooney, 1994a) </ref> all use intensional background to some degree in the context of a Foil-like algorithm.
References-found: 37

