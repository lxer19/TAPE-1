URL: ftp://gaia.cs.umass.edu/pub/Sale95:Performance.ps.Z
Refering-URL: http://www-net.cs.umass.edu/papers/papers.html
Root-URL: 
Email: (salehi,kurose,towsley)@cs.umass.edu  
Title: The Performance Impact of Scheduling for Cache Affinity in Parallel Network Processing  
Author: James D. Salehi, James F. Kurose, and Don Towsley 
Date: August 1995.  
Address: (HPDC-4), Pentagon City, VA,  Amherst MA 01003, U.S.A.  
Affiliation: Computing  Department of Computer Science University of Massachusetts,  
Note: Fourth IEEE International Symposium on High-Performance Distributed  
Abstract: We explore processor-cache affinity scheduling of parallel network protocol processing, in a setting in which protocol processing executes on a shared-memory multiprocessor concurrently with a general workload of non-protocol activity. We find affinity-based scheduling can significantly reduce the communication delay associated with protocol processing, enabling the host to support a greater number of concurrent streams and to provide higher maximum throughput to individual streams. In addition, we compare the performance of two parallelization alternatives, Locking and Independent Protocol Stacks (IPS), with very different caching behaviors. We find that IPS (which maximizes cache affinity) delivers much lower message latency and significantly higher message throughput capacity, yet exhibits less robust response to intra-stream burstiness and limited intra-stream scalability. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, M. Horowitz and J. Hennessy. </author> <title> An Analytical Cache Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(2) </volume> <pages> 184-215, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: The authors in [22] perform a detailed validation of this expression on segments of a 200 million reference trace of a multiprogrammed IBM/370 MVS workload, consisting of a representative workload of user applications and operating system activity. (In addition, they show the expression to be consistent with data given in <ref> [1, 23] </ref>). We use the specific parameters derived by the authors for this workload (W =2.19827, a=0.033233, b=0.827457, log d=-0.13025) to model the non-protocol activity in our system.
Reference: [2] <author> T. Anderson, E. Lazowska and H. Levy. </author> <title> The Performance Implications of Thread Management Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: Let m denote the average memory reference rate of the intervening processing (we assume m is stationary), R = x=m denote the average number of references issued during execution time 7 Note that while the load-balancing and synchronization benefits of per-processor thread pools has been explored <ref> [2] </ref>, the cache affinity benefits of such organization have not previously been evaluated. protocol footprint x, and u (R; L) denote the footprint functionthe number of unique memory lines referenced at the processor in R references for a cache line size L bytes.
Reference: [3] <author> M. Bjorkman and P. Gunningberg. </author> <title> Locking Effects in Multiprocessor Implementations of Protocols. </title> <booktitle> Proceedings of ACM SIGCOMM Conference, p. </booktitle> <pages> 74-83, </pages> <month> Sep. </month> <year> 1993. </year>
Reference-contexts: Previous studies have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) [4, 6, 12, 24, 27]. In this paper, we explore affinity-based scheduling of parallel networking, an area of research which has recently generated considerable interest (e.g., <ref> [3, 11, 13, 19, 21] </ref>). <p> We consider protocol parallelization paradigms in which each message, during the course of its processing, visits a single processor and executes within the context of a single thread. This captures the packet-level and connection-level parallelism found in several multiprocessor protocol implementations <ref> [3, 13, 16, 21] </ref>; a related form of parallelism is found in the STREAMS implementations in several commercial operating systems (e.g., [19]). We do not consider functional or layer parallelisms, since they incur high synchronization overheads on RISC-based shared-memory machines [20]. We present two sets of results. <p> Packet-level and connection-level parallelisms enable concurrency at higher levels of granularity. 1 lel applications requiring low-latency communication, such as those performing multiprocessor IPC or RPC in a distributed environment. In the context of parallel network processing, in which it is well-known that software synchronization can impose a large overhead <ref> [3, 13, 19] </ref>, our work establishes the importance of accounting for caching effects as well. Second, we implement and compare the performance of two approaches toward enabling protocol concurrency.
Reference: [4] <author> M. Devarakonda and A. Mukherjee. </author> <title> Issues in Implementation of Cache-Affinity Scheduling. </title> <booktitle> Proceedings of the Winter USENIX Conference, p. </booktitle> <pages> 345-357, </pages> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: Previous studies have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) <ref> [4, 6, 12, 24, 27] </ref>. In this paper, we explore affinity-based scheduling of parallel networking, an area of research which has recently generated considerable interest (e.g., [3, 11, 13, 19, 21]). <p> No previous study has examined the technique in the context of parallel network processing. Four studies consider scheduling at the process level <ref> [4, 6, 24, 27] </ref>, while one considers the finer granularity of loop scheduling [12]. Vaswani and Zajorian [27] show experimentally that affinity-based scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> The authors use a variety of queueing-theoretic techniques and employ the analytic cache model developed by Thiebaut and Stone [25]. In <ref> [4] </ref>, Devarakonda and Mukherjee explore implementation issues in affinity-based scheduling, both in-kernel and within a user-level thread scheduler, on an 8-processor Encore Multimax running Mach 2.5. A schedulable task is defined to have affinity strictly for the processor it most recently visited. <p> Second, as pointed out in [10], at its most influential (i.e., for 1-byte packets), TCP-specific processing only accounts for around 15% of overall packet execution time (for the DEC Ultrix 4.2a TCP/IP/FDDI in-kernel protocol stack studied in that work). Our conclusions contrast with those of several earlier studies <ref> [4, 6, 27] </ref> and support the contention [24, 12] that there are platforms and common workloads for which affinity-based scheduling is worthwhile. We hope to have demonstrated techniques and a methodology which will facilitate further research in this area.
Reference: [5] <author> R. Gusella. </author> <title> A Measurement Study of Diskless Workstation Traffic on an Ethernet IEEE Transactions on Communications, </title> <booktitle> 38(9) </booktitle> <pages> 1557-1568, </pages> <month> Sep. </month> <year> 1990. </year>
Reference-contexts: Although copying and checksumming (which scale with packet size) are expensive, the reason the non-data touching overheads predominate is that typically in real environments most packets are small (e.g., <ref> [5, 10] </ref>) 6 . We do point out how Figures 10 and 11 (below) can be interpreted to evaluate the impact of data-touching operations on affinity-based scheduling under Locking and IPS, respectively.
Reference: [6] <author> A. Gupta, A. Tucker and S. Urushibara. </author> <title> The Impact of Operating System Scheduling Policies And Synchronization Methods on the Performance of Parallel Applications. </title> <booktitle> Proceedings of the ACM SIGMETRICS Conference, p. </booktitle> <pages> 120-132, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Previous studies have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) <ref> [4, 6, 12, 24, 27] </ref>. In this paper, we explore affinity-based scheduling of parallel networking, an area of research which has recently generated considerable interest (e.g., [3, 11, 13, 19, 21]). <p> No previous study has examined the technique in the context of parallel network processing. Four studies consider scheduling at the process level <ref> [4, 6, 24, 27] </ref>, while one considers the finer granularity of loop scheduling [12]. Vaswani and Zajorian [27] show experimentally that affinity-based scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> Although experi mental measurements do not support advocacy of in-kernel affinity scheduling, within the user-level thread scheduler the authors find affinity scheduling yields a 12% reduction in execution time for one of the two real applications. In a trace-driven simulation study, Gupta, Tucker and Urushibara <ref> [6] </ref> consider in-kernel affinity-based scheduling of parallel applications on a shared memory platform. Their approach is to simulate a multiprocessor system by interleaving the process execution traces obtained from a set of parallel scientific applications. <p> Second, as pointed out in [10], at its most influential (i.e., for 1-byte packets), TCP-specific processing only accounts for around 15% of overall packet execution time (for the DEC Ultrix 4.2a TCP/IP/FDDI in-kernel protocol stack studied in that work). Our conclusions contrast with those of several earlier studies <ref> [4, 6, 27] </ref> and support the contention [24, 12] that there are platforms and common workloads for which affinity-based scheduling is worthwhile. We hope to have demonstrated techniques and a methodology which will facilitate further research in this area.
Reference: [7] <author> M. Hill and A. Smith. </author> <title> Evaluating Associativity in CPU Caches, </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1612-1630, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: This results in only a small change to F (x) under the assumption that the reference stream is split approximately equally between the two caches. Experimental measure 11 ments support this assumption; see for example Table 1 of <ref> [7] </ref>.) F (x) has been computed for the 100-MHz clock rate of MIPS R4400, assuming an average of 5 clock cycles per memory reference (m = 5). Note that the protocol footprint is flushed much more slowly from L2 than from L1, reflecting its much larger size.
Reference: [8] <author> N. Hutchinson and L. Peterson. </author> <title> The x-Kernel: An Architecture for Implementing Network Protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: We establish our results using experimental measurements in conjunction with simulation and analytic techniques. We begin with an unparallelized version of the x-kernel protocol framework <ref> [8, 15] </ref> running in user-space on an 8-processor MIPS R4400-based SGI Challenge XL. We parallelize the receive-side fast-path of the x-kernel's UDP/IP/FDDI protocol stack, and conduct a set of multiprocessor experiments designed to measure packet execution times under specific conditions of cache state.
Reference: [9] <author> R. Jain and S. Routhier. </author> <title> Packet Trains: Measurements and a New Model for Computer Network Traffic, </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 4(6) </volume> <pages> 986-995, </pages> <month> Sep. </month> <year> 1986. </year>
Reference-contexts: We are currently pursuing several extensions to this work [18], including i) evaluating affinity-based scheduling of send-side UDP/IP/FDDI processing; ii) examining the performance of affinity-based scheduling as a function of stream burstiness and source locality, as captured by the Packet-Train model of <ref> [9] </ref>; iii) exploring under IPS the impact of varying the number of independent stacks; and iv) incorporating into our results the overhead of copying uncached packet data. Another compelling problem is to examine affinity-based scheduling of TCP/IP processing, as TCP accounts for a large fraction of wide-area network traffic.
Reference: [10] <author> J. Kay and J. Pasquale. </author> <title> The Importance of Non-Data Touching Processing Overheads in TCP/IP. </title> <booktitle> Proceedings of ACM SIGCOMM Conference, p. </booktitle> <pages> 259-268, </pages> <month> Sep. </month> <year> 1993. </year>
Reference-contexts: We present results for packet processing without data-touching operations (e.g., copying, software checksumming), motivated by the fact that in many real environments packet processing time is dominated by non-data touching operations with generally fixed per-packet overheads <ref> [10] </ref>. Although copying and checksumming (which scale with packet size) are expensive, the reason the non-data touching overheads predominate is that typically in real environments most packets are small (e.g., [5, 10]) 6 . <p> Although copying and checksumming (which scale with packet size) are expensive, the reason the non-data touching overheads predominate is that typically in real environments most packets are small (e.g., <ref> [5, 10] </ref>) 6 . We do point out how Figures 10 and 11 (below) can be interpreted to evaluate the impact of data-touching operations on affinity-based scheduling under Locking and IPS, respectively. <p> Although TCP is a far more complex protocol than UDP, our results are likely to hold directly for TCP, for two reasons. First, the breakdowns of overall processing time overheads for TCP and UDP packets are very similar (compare for example graphs 3a and 3b in <ref> [10] </ref>). Second, as pointed out in [10], at its most influential (i.e., for 1-byte packets), TCP-specific processing only accounts for around 15% of overall packet execution time (for the DEC Ultrix 4.2a TCP/IP/FDDI in-kernel protocol stack studied in that work). <p> First, the breakdowns of overall processing time overheads for TCP and UDP packets are very similar (compare for example graphs 3a and 3b in <ref> [10] </ref>). Second, as pointed out in [10], at its most influential (i.e., for 1-byte packets), TCP-specific processing only accounts for around 15% of overall packet execution time (for the DEC Ultrix 4.2a TCP/IP/FDDI in-kernel protocol stack studied in that work).
Reference: [11] <author> T. La Porta and M. Schwartz. </author> <title> Performance Analysis of MSP: Feature-Rich High-Speed Transport Protocol. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(6) </volume> <pages> 740-753, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Previous studies have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) [4, 6, 12, 24, 27]. In this paper, we explore affinity-based scheduling of parallel networking, an area of research which has recently generated considerable interest (e.g., <ref> [3, 11, 13, 19, 21] </ref>).
Reference: [12] <author> E. Markatos and T. LeBlanc. </author> <title> Using Processor Affinity in Loop Scheduling on Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: Previous studies have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) <ref> [4, 6, 12, 24, 27] </ref>. In this paper, we explore affinity-based scheduling of parallel networking, an area of research which has recently generated considerable interest (e.g., [3, 11, 13, 19, 21]). <p> No previous study has examined the technique in the context of parallel network processing. Four studies consider scheduling at the process level [4, 6, 24, 27], while one considers the finer granularity of loop scheduling <ref> [12] </ref>. Vaswani and Zajorian [27] show experimentally that affinity-based scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> Simulation results show that affinity-based scheduling yields a small but consistently positive impact across all applications, increasing processor utilization by an overall average of about 3%. Markatos and LeBlanc consider affinity-based scheduling of non-nested, completely parallelizable loops on several modern shared-memory multiprocessor platforms <ref> [12] </ref>. Previous work in loop scheduling has focused on load balancing of loops among processors and the overheads introduced by processor synchronization. <p> Our conclusions contrast with those of several earlier studies [4, 6, 27] and support the contention <ref> [24, 12] </ref> that there are platforms and common workloads for which affinity-based scheduling is worthwhile. We hope to have demonstrated techniques and a methodology which will facilitate further research in this area.
Reference: [13] <author> E. Nahum, D. Yates, J. Kurose and D. Towsley. </author> <title> Performance Issues in Parallelized Network Protocols. </title> <booktitle> Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation, p. </booktitle> <pages> 125-137, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Previous studies have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) [4, 6, 12, 24, 27]. In this paper, we explore affinity-based scheduling of parallel networking, an area of research which has recently generated considerable interest (e.g., <ref> [3, 11, 13, 19, 21] </ref>). <p> We consider protocol parallelization paradigms in which each message, during the course of its processing, visits a single processor and executes within the context of a single thread. This captures the packet-level and connection-level parallelism found in several multiprocessor protocol implementations <ref> [3, 13, 16, 21] </ref>; a related form of parallelism is found in the STREAMS implementations in several commercial operating systems (e.g., [19]). We do not consider functional or layer parallelisms, since they incur high synchronization overheads on RISC-based shared-memory machines [20]. We present two sets of results. <p> Packet-level and connection-level parallelisms enable concurrency at higher levels of granularity. 1 lel applications requiring low-latency communication, such as those performing multiprocessor IPC or RPC in a distributed environment. In the context of parallel network processing, in which it is well-known that software synchronization can impose a large overhead <ref> [3, 13, 19] </ref>, our work establishes the importance of accounting for caching effects as well. Second, we implement and compare the performance of two approaches toward enabling protocol concurrency. <p> We began with an un-parallelized x-kernel (version 3.2) running in user-space above the native IRIX 5.2 operating system; a simulated device driver emulates the protocol functionality associ ated with managing an FDDI network interface attachment. We developed in-memory drivers (a technique also used in <ref> [13, 21] </ref>), since the Challenge's eight 100MHz R4400 processors are together much faster than the single FDDI network attachment on our machine. Data is not received from the actual FDDI network. <p> The upper bound on the reduction (as given by the V =0 curves) is around 40-50%. These graphs can be interpreted to illustrate the impact data-touching operations on the benefits of affinity-based scheduling. For example, checksumming on our platform can be performed at a rate of 32 bytes/s <ref> [13] </ref>. Consider the worst case (from the perspective of affinity-based scheduling) of a stream transmitting the largest possible FDDI packets, each with 4432 bytes of data. The fixed overhead would be 139s per packet.
Reference: [14] <author> William Nowicki, </author> <title> Silicon Graphics Inc. </title> <type> Personal communication, </type> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: For example, the SGI implementation of the NFS server takes advantage of the fact that most SGI network interfaces support checksumming in firmware <ref> [14] </ref>. By DMA'ing unfragmented data directly to the network interface, the server avoids bringing the data into the CPU cache. protocol receive time tends to t cold; cold =284.3s).
Reference: [15] <author> S. O'Malley and L. Peterson. </author> <title> A Dynamic Network Architecture. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(2) </volume> <pages> 110-143, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: We establish our results using experimental measurements in conjunction with simulation and analytic techniques. We begin with an unparallelized version of the x-kernel protocol framework <ref> [8, 15] </ref> running in user-space on an 8-processor MIPS R4400-based SGI Challenge XL. We parallelize the receive-side fast-path of the x-kernel's UDP/IP/FDDI protocol stack, and conduct a set of multiprocessor experiments designed to measure packet execution times under specific conditions of cache state.
Reference: [16] <author> D. Presotto. </author> <title> Multiprocessor STREAMS for Plan 9. United Kingdom UNIX User's Group, </title> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: We consider protocol parallelization paradigms in which each message, during the course of its processing, visits a single processor and executes within the context of a single thread. This captures the packet-level and connection-level parallelism found in several multiprocessor protocol implementations <ref> [3, 13, 16, 21] </ref>; a related form of parallelism is found in the STREAMS implementations in several commercial operating systems (e.g., [19]). We do not consider functional or layer parallelisms, since they incur high synchronization overheads on RISC-based shared-memory machines [20]. We present two sets of results.
Reference: [17] <author> J. Salehi, J. Kurose and D. Towsley. </author> <title> Scheduling for Cache Affinity in Parallelized Communication Protocols. </title> <type> TR UM-CS-1994-075, </type> <institution> U. Massachusetts, </institution> <month> Oct. </month> <year> 1994. </year> <note> (available via ftp from gaia.cs.umass.edu in pub/Sale94:Scheduling.ps.Z) </note>
Reference-contexts: We show instead how to parameterize the analytic model with experimental timing measurements. Finally, in the design of the experiments formulated to yield these measurements, we illustrate an experimental method for isolating the indi 2 These observations lead us to propose in <ref> [17] </ref> a hybrid approach for a specific class of streams, which offers the best overall performance yielding high message throughput, high intra-stream scalability, and robustness in the presence of bursty arrivals. vidual components of affinity-based overhead. This paper is organized as follows. <p> This paper is organized as follows. We begin by presenting the problem formulation in Section 2. In Section 3 we summarize the salient aspects of the multiprocessor simulation model and the analytic model of message execution time; extensive details are provided in <ref> [17] </ref>. Section 4 discusses the implementation-based experiments performed to measure the parameters needed by the analytic model. Performance results are presented in Section 5. <p> In this section, we summarize the salient aspects of our approach; extensive details are provided in <ref> [17] </ref>. We begin by developing a multiprocessor simulation model that closely follows the behavior of Figure 1.
Reference: [18] <author> J. Salehi, J. Kurose and D. Towsley. </author> <title> Further Results in Affinity-Based Scheduling of Parallel Networking. </title> <type> TR UM-CS-1995-046, </type> <institution> U. Massachusetts, </institution> <month> May </month> <year> 1995. </year> <note> (available via ftp from gaia.cs.umass.edu in pub/Sale95:Further.ps.Z) </note>
Reference-contexts: We do point out how Figures 10 and 11 (below) can be interpreted to evaluate the impact of data-touching operations on affinity-based scheduling under Locking and IPS, respectively. Further, in <ref> [18] </ref> we explicitly incorporate into our results the overhead of copying packet data. 5.1 Affinity scheduling under Locking Figures 6 and 7 explore the performance of affinity-based scheduling under Locking, plotting mean packet delay as a function of packet arrival rate. <p> Under Locking, processors should be managed MRUexcept under high arrival rate, when Wired-Streams scheduling performs better. Under IPS, independent stacks should be wired to processorsexcept under low arrival rate, when MRU processor scheduling performs better. We are currently pursuing several extensions to this work <ref> [18] </ref>, including i) evaluating affinity-based scheduling of send-side UDP/IP/FDDI processing; ii) examining the performance of affinity-based scheduling as a function of stream burstiness and source locality, as captured by the Packet-Train model of [9]; iii) exploring under IPS the impact of varying the number of independent stacks; and iv) incorporating into
Reference: [19] <author> S. Saxena, J. Peacock, F. Yang, V. Verma and M. Krish-nan. </author> <title> Pitfalls in Multithreading SVR4 STREAMS and Other Weightless Processes. </title> <booktitle> Proceedings of the Winter USENIX Conference, p. </booktitle> <pages> 85-96, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Previous studies have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) [4, 6, 12, 24, 27]. In this paper, we explore affinity-based scheduling of parallel networking, an area of research which has recently generated considerable interest (e.g., <ref> [3, 11, 13, 19, 21] </ref>). <p> This captures the packet-level and connection-level parallelism found in several multiprocessor protocol implementations [3, 13, 16, 21]; a related form of parallelism is found in the STREAMS implementations in several commercial operating systems (e.g., <ref> [19] </ref>). We do not consider functional or layer parallelisms, since they incur high synchronization overheads on RISC-based shared-memory machines [20]. We present two sets of results. First, we propose affinity-based scheduling policies for the resources involved in parallel networking, and evaluate their performance. <p> Packet-level and connection-level parallelisms enable concurrency at higher levels of granularity. 1 lel applications requiring low-latency communication, such as those performing multiprocessor IPC or RPC in a distributed environment. In the context of parallel network processing, in which it is well-known that software synchronization can impose a large overhead <ref> [3, 13, 19] </ref>, our work establishes the importance of accounting for caching effects as well. Second, we implement and compare the performance of two approaches toward enabling protocol concurrency.
Reference: [20] <author> D. Schmidt and T. Suda. </author> <title> Measuring the Impact of Alternative Parallel Process Architectures on Communication Subsystem Performance. </title> <booktitle> Proceedings of the 4 th International Workshop on Protocols for High-Speed Networks, </booktitle> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: We do not consider functional or layer parallelisms, since they incur high synchronization overheads on RISC-based shared-memory machines <ref> [20] </ref>. We present two sets of results. First, we propose affinity-based scheduling policies for the resources involved in parallel networking, and evaluate their performance.
Reference: [21] <author> D. Schmidt and T. Suda. </author> <title> Measuring the Performance of Parallel Message-based Process Architectures. </title> <booktitle> Proceedings of the IEEE INFOCOM Conference, p. </booktitle> <pages> 624-633, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Previous studies have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) [4, 6, 12, 24, 27]. In this paper, we explore affinity-based scheduling of parallel networking, an area of research which has recently generated considerable interest (e.g., <ref> [3, 11, 13, 19, 21] </ref>). <p> We consider protocol parallelization paradigms in which each message, during the course of its processing, visits a single processor and executes within the context of a single thread. This captures the packet-level and connection-level parallelism found in several multiprocessor protocol implementations <ref> [3, 13, 16, 21] </ref>; a related form of parallelism is found in the STREAMS implementations in several commercial operating systems (e.g., [19]). We do not consider functional or layer parallelisms, since they incur high synchronization overheads on RISC-based shared-memory machines [20]. We present two sets of results. <p> We began with an un-parallelized x-kernel (version 3.2) running in user-space above the native IRIX 5.2 operating system; a simulated device driver emulates the protocol functionality associ ated with managing an FDDI network interface attachment. We developed in-memory drivers (a technique also used in <ref> [13, 21] </ref>), since the Challenge's eight 100MHz R4400 processors are together much faster than the single FDDI network attachment on our machine. Data is not received from the actual FDDI network.
Reference: [22] <author> J. Singh, H. Stone and D. Thiebaut. </author> <title> A Model of Workloads and its Use in Miss-Rate Prediction for Fully Associative Caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7) </volume> <pages> 811-825, </pages> <month> Jul. </month> <year> 1992. </year>
Reference-contexts: These measurements are then used to parameterize the analytic component of a simulation model of multiprocessor protocol processing, under various affinity-based scheduling policies. The analytic model, which combines established analytic results from other researchers <ref> [22, 25] </ref>, accounts for the impact of general non-protocol activity on packet execution time as well as the specific cache architecture and organization of our experimental platform. <p> We begin by developing a multiprocessor simulation model that closely follows the behavior of Figure 1. To capture the displacement of the cached protocol footprint by the non-protocol workload, we develop an analytic model of packet execution time (combining established analytic results from <ref> [22, 25] </ref>) that reflects the specific cache architecture and organization of our SGI Challenge. We then conduct a set of multiprocessor experiments, designed to measure packet execution times under specific conditions of cache state, and parameterize the analytic model with the experimentally-measured values. <p> To capture this displacement, the model first computes (using analytic results from <ref> [22, 25] </ref>) the fractions F 1 (x i ) and F 2 (x i ) of the footprint that have been flushed from L1 and L2, respectively, given that non-protocol processing has executed for time x i since P i last executed protocol code. Details are provided in the appendix. <p> A Appendix To derive F 1 (x) and F 2 (x) (the fractions of the protocol footprint displaced from the L1 and L2 caches by intervening non-protocol processing having executed for duration x since the processor last executed protocol code), we rely on the work of Singh, Stone and Thiebaut <ref> [22] </ref>. <p> In <ref> [22] </ref>, the authors show that this function is closely modeled by an expression of the form u (R; L) = W L a R b d log L log R (2) where the constants W , a, b, and d relate to working set size, spatial locality, temporal locality, and interactions <p> The authors in <ref> [22] </ref> perform a detailed validation of this expression on segments of a 200 million reference trace of a multiprogrammed IBM/370 MVS workload, consisting of a representative workload of user applications and operating system activity. (In addition, they show the expression to be consistent with data given in [1, 23]).
Reference: [23] <author> A. Smith. </author> <title> Line (Block) Size Choice for CPU Cache Memories. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(9):1063-1075, </volume> <month> Sep. </month> <year> 1987. </year>
Reference-contexts: The authors in [22] perform a detailed validation of this expression on segments of a 200 million reference trace of a multiprogrammed IBM/370 MVS workload, consisting of a representative workload of user applications and operating system activity. (In addition, they show the expression to be consistent with data given in <ref> [1, 23] </ref>). We use the specific parameters derived by the authors for this workload (W =2.19827, a=0.033233, b=0.827457, log d=-0.13025) to model the non-protocol activity in our system.
Reference: [24] <author> M. Squillante and E. Lazowska. </author> <title> Using Processor Cache Affinity Information in Shared-Memory Multiprocessor Scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-143, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Previous studies have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) <ref> [4, 6, 12, 24, 27] </ref>. In this paper, we explore affinity-based scheduling of parallel networking, an area of research which has recently generated considerable interest (e.g., [3, 11, 13, 19, 21]). <p> This allows us to evaluate the marginal contributions of the individual policies, and assess their relative performance gains. Second, in our modeling of non-protocol activity, we present extensions to an existing analytic model of the execution time of an affinity-scheduled task in a multitasking environment <ref> [25, 24] </ref>. The primary contribution here is to relax the requirement of having identified the task's footprintthe set of cache lines currently referenced by the executing task. In practice, it can be hard to determine the footprint, especially for large, multi-threaded, multiprocessor applications. <p> In addition, the approachwhich is based on timing measurementsdoes not require identifying the footprint of the task being affinity scheduled (as is assumed, e.g., in the SGI Challenge <ref> [24] </ref>), and thus avoids the difficulties inherent in capturing memory traces from large parallel applications. 3.1 Multiprocessor simulation model Consider first the simulation of Locking, under which there are N processors and N protocol threads. <p> The impact of the non-protocol workload is captured by scaling these bounds by the fraction of the protocol footprint found at each corresponding layer in the cache hierarchy. 5 Task execution time as the linear interpolation of the maximum reload transient is also the approach taken in <ref> [24] </ref>. In the authors' formulation, the inherent computing demand of a task is denoted D, the average time to reload the entire footprint is C, and the fraction of the footprint displaced is R. The task execution time is modeled as D + RC. <p> No previous study has examined the technique in the context of parallel network processing. Four studies consider scheduling at the process level <ref> [4, 6, 24, 27] </ref>, while one considers the finer granularity of loop scheduling [12]. Vaswani and Zajorian [27] show experimentally that affinity-based scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> The explanation lies in the fact that among these applications, the upper bound on the time to completely reload the processor cache (about 1-2ms) is small in comparison to the processor reallocation interval (about 200-500ms). Squillante and Lazowska <ref> [24] </ref> conduct a modeling study designed to gain insight into the general class of scheduling policies which consider the state of processor caches. The study examines the performance of a range of in-kernel affinity-based scheduling policies on a multiprocessor system running multiple independent single-threaded processes. <p> Our conclusions contrast with those of several earlier studies [4, 6, 27] and support the contention <ref> [24, 12] </ref> that there are platforms and common workloads for which affinity-based scheduling is worthwhile. We hope to have demonstrated techniques and a methodology which will facilitate further research in this area. <p> Let F (x) denote the fraction of the cache which is flushed by the u (R; L) references. (We re-introduce the time duration x for clarity). We compute F (x) by assuming the references map independently into cache sets, an assumption also made in <ref> [24, 25] </ref> in similar context. Let the random variable X denote the number of references that map to a randomly chosen set. X has binomial distribution with parameters n = u (R; L) and p = 1=S.
Reference: [25] <author> D. Thiebaut and H. Stone. </author> <title> Footprints in the Cache. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(4) </volume> <pages> 305-329, </pages> <month> Nov. </month> <year> 1987. </year>
Reference-contexts: These measurements are then used to parameterize the analytic component of a simulation model of multiprocessor protocol processing, under various affinity-based scheduling policies. The analytic model, which combines established analytic results from other researchers <ref> [22, 25] </ref>, accounts for the impact of general non-protocol activity on packet execution time as well as the specific cache architecture and organization of our experimental platform. <p> This allows us to evaluate the marginal contributions of the individual policies, and assess their relative performance gains. Second, in our modeling of non-protocol activity, we present extensions to an existing analytic model of the execution time of an affinity-scheduled task in a multitasking environment <ref> [25, 24] </ref>. The primary contribution here is to relax the requirement of having identified the task's footprintthe set of cache lines currently referenced by the executing task. In practice, it can be hard to determine the footprint, especially for large, multi-threaded, multiprocessor applications. <p> We begin by developing a multiprocessor simulation model that closely follows the behavior of Figure 1. To capture the displacement of the cached protocol footprint by the non-protocol workload, we develop an analytic model of packet execution time (combining established analytic results from <ref> [22, 25] </ref>) that reflects the specific cache architecture and organization of our SGI Challenge. We then conduct a set of multiprocessor experiments, designed to measure packet execution times under specific conditions of cache state, and parameterize the analytic model with the experimentally-measured values. <p> To capture this displacement, the model first computes (using analytic results from <ref> [22, 25] </ref>) the fractions F 1 (x i ) and F 2 (x i ) of the footprint that have been flushed from L1 and L2, respectively, given that non-protocol processing has executed for time x i since P i last executed protocol code. Details are provided in the appendix. <p> This work motivated our own by demonstrating that when the cache reload time is large with respect to the task's inherent computing demands, affinity scheduling can have a significant impact. The authors use a variety of queueing-theoretic techniques and employ the analytic cache model developed by Thiebaut and Stone <ref> [25] </ref>. In [4], Devarakonda and Mukherjee explore implementation issues in affinity-based scheduling, both in-kernel and within a user-level thread scheduler, on an 8-processor Encore Multimax running Mach 2.5. A schedulable task is defined to have affinity strictly for the processor it most recently visited. <p> Let F (x) denote the fraction of the cache which is flushed by the u (R; L) references. (We re-introduce the time duration x for clarity). We compute F (x) by assuming the references map independently into cache sets, an assumption also made in <ref> [24, 25] </ref> in similar context. Let the random variable X denote the number of references that map to a randomly chosen set. X has binomial distribution with parameters n = u (R; L) and p = 1=S.
Reference: [26] <author> D. Thiebaut. </author> <title> On the Fractal Dimension of Computer Programs and its Application to the Prediction of the Cache Miss Ratio. </title> <journal> IEEE Transactionson Computers, </journal> <volume> 38(7) </volume> <pages> 1012-1026, </pages> <month> Jul. </month> <year> 1989. </year>
Reference-contexts: R (2) where the constants W , a, b, and d relate to working set size, spatial locality, temporal locality, and interactions between spatial and temporal locality, respectively, of the intervening processing. (It had been previously shown that u (R; L) is a power function of R for fixed L <ref> [26] </ref>).
Reference: [27] <author> R. Vaswani and J. Zahorjan. </author> <title> The Implications of Cache Affinity on Processor Scheduling for Multiprogrammed, Shared Memory Multiprocessors. </title> <booktitle> Proceedings of the Thirteenth Symposium on Operating Systems Principles, p. </booktitle> <pages> 26-40, </pages> <month> Oct. </month> <year> 1991. </year> <month> 12 </month>
Reference-contexts: Previous studies have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) <ref> [4, 6, 12, 24, 27] </ref>. In this paper, we explore affinity-based scheduling of parallel networking, an area of research which has recently generated considerable interest (e.g., [3, 11, 13, 19, 21]). <p> No previous study has examined the technique in the context of parallel network processing. Four studies consider scheduling at the process level <ref> [4, 6, 24, 27] </ref>, while one considers the finer granularity of loop scheduling [12]. Vaswani and Zajorian [27] show experimentally that affinity-based scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> No previous study has examined the technique in the context of parallel network processing. Four studies consider scheduling at the process level [4, 6, 24, 27], while one considers the finer granularity of loop scheduling [12]. Vaswani and Zajorian <ref> [27] </ref> show experimentally that affinity-based scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. The measured reduction in task response time enabled by affinity-based scheduling does not exceed 1%. <p> Second, as pointed out in [10], at its most influential (i.e., for 1-byte packets), TCP-specific processing only accounts for around 15% of overall packet execution time (for the DEC Ultrix 4.2a TCP/IP/FDDI in-kernel protocol stack studied in that work). Our conclusions contrast with those of several earlier studies <ref> [4, 6, 27] </ref> and support the contention [24, 12] that there are platforms and common workloads for which affinity-based scheduling is worthwhile. We hope to have demonstrated techniques and a methodology which will facilitate further research in this area.
References-found: 27

