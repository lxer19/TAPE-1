URL: http://ki.cs.tu-berlin.de/~scheffer/papers/ijcai97.ps
Refering-URL: http://ki.cs.tu-berlin.de/~scheffer/publications.html
Root-URL: 
Email: scheffer@cs.tu-berlin.de  
Title: Unbiased Assessment of Learning Algorithms  
Author: Tobias Scheffer and Ralf Herbrich 
Address: FR 5-8 Franklinstr. 28/29, 10587 Berlin, Germany  
Affiliation: Technische Universitat Berlin, Artificial Intelligence Group,  
Note: Proc. IJCAI, 97.  
Abstract: In order to rank the performance of machine learning algorithms, many researchers conduct experiments on benchmark data sets. Since most learning algorithms have domain-specific parameters, it is a popular custom to adapt these parameters to obtain a minimal error rate on the test set. The same rate is then used to rank the algorithm, which causes an optimistic bias. We quantify this bias, showing, in particular, that an algorithm with more parameters will probably be ranked higher than an equally good algorithm with fewer parameters. We demonstrate this result, showing the number of parameters and trials required in order to pretend to outperform C4.5 or FOIL, respectively, for various benchmark problems. We then describe out how unbiased ranking experiments should be conducted. 
Abstract-found: 1
Intro-found: 1
Reference: [ Cat91 ] <author> J. Catlett. </author> <title> Megainduction: machine learning on very large data bases. </title> <type> PhD thesis, </type> <institution> University of Sydney, </institution> <year> 1991. </year>
Reference-contexts: Parameters of learning algorithms are undesirable since the more parameters an algorithm has the less robust the algorithm is, and the harder it is to obtain a satisfactory result for a new problem. Buchanan, paraphrased by <ref> [ Cat91 ] </ref> , called this the China syndrome: Some learning algorithms have so many parameters that the only person who can make the program run is with high probability currently in China.
Reference: [ DM94 ] <author> B. Dolsak and S. Muggleton. </author> <title> The application of inductive logic programming to finite-element mesh design. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 453-472. </pages> <publisher> Academic Press, </publisher> <year> 1994. </year>
Reference-contexts: By iteratively filling an array that is indexed n and k with P ( P n i=1 X i = k) the formula can be evaluated quickly. 3.1 Affected benchmark problems FEM mesh design: <ref> [ DM94 ] </ref> , a relational problem popular in inductive logic programming. It is explicitly split into five learning problems. There are 277 samples and 13 classes and the entropy is H = 2:87. FOIL [ Qui90 ] achieves an accuracy of 21% (59 hits).
Reference: [ Efr79 ] <author> B. Efron. </author> <title> Bootstrap methods: another look at the jacknife. </title> <journal> Annals of Statistics, </journal> <volume> 7(1) </volume> <pages> 1-26, </pages> <year> 1979. </year>
Reference-contexts: If the data set is too large, the accuracy is usually estimated on a test set that was not used for learning (one-shot training and test), which causes a slight pessimistic bias. For model selection purposes, a .632 bootstrap <ref> [ Efr79 ] </ref> may be preferable. Bootstrap experiments are conducted by re-sampling a number of training sets of size n from an original data set of size n by randomly drawing samples with replacement.
Reference: [ Efr83 ] <author> B. Efron. </author> <title> Estimating the error rate of a prediction rule. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 68(382) </volume> <pages> 316-330, </pages> <year> 1983. </year>
Reference-contexts: On the average, (1 1=e)m :632m distinct samples will appear in the training set, and the averaged accuracies on the remaining test sets provide an optimistically biased estimate. The variance is claimed to be lower in many cases than the variance of cross validation <ref> [ Efr83 ] </ref> , which is important when choosing an optimal model.
Reference: [ KJ97 ] <author> R. Kohavi and G. John. </author> <title> Wrappers for feature sub set selection. </title> <journal> J. AI, </journal> <note> Special Issue on Relevance, 1997. to appear. </note>
Reference-contexts: Hence, the assumption that the test sets are not used for learning, which is essential to the result that n-fold cross validation is bias-free, is violated. Many authors are aware of that problem and properly separate model selection from accuracy estimation, e.g., <ref> [ KJ97 ] </ref> , but a majority of authors seem to consider the resulting distortion of the results negligible. One of many examples is the StatLog project [ MST94 ] , where it has not been taken into account by all contributing partners.
Reference: [ Koh95 ] <author> R. Kohavi. </author> <title> Wrappers for performance enhancement and oblivious decision graphs. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1995. </year>
Reference-contexts: 1 Introduction Estimating the accuracy of a classifier is a topic that has experienced much attention in the ML community. One of the main results is that N -fold cross validation provides a bias-free [ Sto74 ] though not variance-free <ref> [ Zha92; Koh95 ] </ref> , estimate of the true accuracy. n-fold cross validation means that n classifiers are learned from ((n 1)=n)ths of the available data, and tested on the remaining (1=n)th of the training set.
Reference: [ MST94 ] <author> D. Michie, D. J. Spiegelhalter, and C. C. Taylor. </author> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: Many papers propose new or modified machine learning algorithms, with claims such as "my new algorithm B is way better than algorithm A", or "extension X improves algorithm A a lot" typically supported by ranking experiments on a well known set of benchmark problems. There is also a book <ref> [ MST94 ] </ref> , resulting from the Euro-pean StatLog project, that is dedicated to the comparison of learning algorithms for benchmark problems. Virtually any learning algorithm possesses a number of parameters (e.g., learning rates, number of learning steps, pruning thresholds, etc.). <p> Many authors are aware of that problem and properly separate model selection from accuracy estimation, e.g., [ KJ97 ] , but a majority of authors seem to consider the resulting distortion of the results negligible. One of many examples is the StatLog project <ref> [ MST94 ] </ref> , where it has not been taken into account by all contributing partners. A slight bias would not be dramatic, if all learning algorithms would be effected equally, such that ranking results would still remain valid. <p> Land-sat satellite images: This data set contains 4435 training and 2000 test instances. The default error rate is .231. Based on <ref> [ MST94 ] </ref> , C4.5 is ranked 10th (error .150, i.e.,1700 hits on the test set). To be ranked 9th it would have to outperform Bay-tree (error .147) for which C4.5 needs only z = 6 extra hits on the test set. <p> Using c = 16 samples (parameter channel of 40 bits) and 96 experiments, we still have a 2% chance of being over-ranked. DNA: This data set, also described in <ref> [ MST94 ] </ref> , possesses 2000 training and 1186 test instances. C4.5 is ranked 10th (1096 hits). To be ranked 9th it would have to outperform INDCart, requiring z = 4 extra hits. <p> In this experimental setting, diabetes is "safe". Heart disease: In this data set <ref> [ MST94 ] </ref> there are 270 samples, 2 classes, H = :991.
Reference: [ Qui90 ] <author> J. R. Quinlan. </author> <title> Learning logical definitions from releations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: It is explicitly split into five learning problems. There are 277 samples and 13 classes and the entropy is H = 2:87. FOIL <ref> [ Qui90 ] </ref> achieves an accuracy of 21% (59 hits). To achieve 26% accuracy with a probability of 99%, FOIL needs c = 5 class labels, while to achieve 31% with a probability of 93% FOIL would need c = 8 class labels.
Reference: [ Qui92 ] <author> J. R. Quinlan. </author> <title> C4.5 Programs for Machine Learn ing. </title> <publisher> Morgan Kaufmann Publisher, </publisher> <year> 1992. </year>
Reference-contexts: worst case, but while the algorithm given above essentially performs a gradient search in parameter space, the faster algorithm behaves unlike we would expect a parameter optimizer to behave, so the first result should be somewhat closer to the behavior of a "real" learning algorithm. classifier h, learned by C4.5 <ref> [ Qui92 ] </ref> say, which classifies p h objects of the test set (of size m) correctly. For the first c test instances we repeat the class labels that we were told by the parameter optimizer, rather than using h, we classify the remaining m c instances using h.
Reference: [ Ren70 ] <author> A. Renyi. </author> <title> Probability Theory. </title> <publisher> North-Holland, </publisher> <year> 1970. </year>
Reference-contexts: However, we can split the probability of a sum of random numbers considering every possible combination that yields this sum <ref> [ Ren70 ] </ref> : P (X + Y = z) = P j P (X = j)P (Y = z j).
Reference: [ Sto74 ] <author> M. Stone. </author> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B 36 </volume> <pages> 111-147, </pages> <year> 1974. </year>
Reference-contexts: 1 Introduction Estimating the accuracy of a classifier is a topic that has experienced much attention in the ML community. One of the main results is that N -fold cross validation provides a bias-free <ref> [ Sto74 ] </ref> though not variance-free [ Zha92; Koh95 ] , estimate of the true accuracy. n-fold cross validation means that n classifiers are learned from ((n 1)=n)ths of the available data, and tested on the remaining (1=n)th of the training set.
Reference: [ Zha92 ] <author> J. Zhang. </author> <title> On the distributional properties of model selection criteria. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 87(419) </volume> <pages> 732-737, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Estimating the accuracy of a classifier is a topic that has experienced much attention in the ML community. One of the main results is that N -fold cross validation provides a bias-free [ Sto74 ] though not variance-free <ref> [ Zha92; Koh95 ] </ref> , estimate of the true accuracy. n-fold cross validation means that n classifiers are learned from ((n 1)=n)ths of the available data, and tested on the remaining (1=n)th of the training set.
References-found: 12

