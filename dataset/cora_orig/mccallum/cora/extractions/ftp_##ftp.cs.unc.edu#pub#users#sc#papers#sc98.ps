URL: ftp://ftp.cs.unc.edu/pub/users/sc/papers/sc98.ps
Refering-URL: http://www.cs.unc.edu/~sc/research/papers.html
Root-URL: http://www.cs.unc.edu
Email: Email: mithuna@cs.duke.edu  Email: sc@cs.unc.edu  Email: alvy@cs.duke.edu  
Phone: Phone: 919 660 6587. FAX: 919 660 6519.  Phone: 919 962 1766. FAX: 919 962 1799.  Phone: 919 660 6551. FAX: 919 660 6519.  
Title: Tuning Strassen's Matrix Multiplication for Memory Efficiency  
Author: Mithuna Thottethodi Siddhartha Chatterjee Alvin R. Lebeck 
Address: Durham, NC 27708-0129.  Chapel Hill, NC 27599-3175.  Durham, NC 27708-0129.  
Affiliation: Department of Computer Science Duke University  Department of Computer Science The University of North Carolina  Department of Computer Science Duke University  
Abstract: Strassen's algorithm for matrix multiplication gains its lower arithmetic complexity at the expense of reduced locality of reference, which makes it challenging to implement the algorithm efficiently on a modern machine with a hierarchical memory system. We report on an implementation of this algorithm that uses several unconventional techniques to make the algorithm memory-friendly. First, the algorithm internally uses a non-standard array layout known as Mor-ton order that is based on a quad-tree decomposition of the matrix. Second, we dynamically select the recursion truncation point to minimize padding without affecting the performance of the algorithm, which we can do by virtue of the cache behavior of the Morton ordering. Each technique is critical for performance, and their combination as done in our code multiplies their effectiveness. Performance comparisons of our implementation with that of competing implementations show that our implementation often outperforms the alternative techniques (up to 25%). However, we also observe wide variability across platforms and across matrix sizes, indicating that at this time, no single implementation is a clear choice for all platforms or matrix sizes. We also note that the time required to convert matrices to/from Morton order is a noticeable amount of execution time (5% to 15%). Eliminating this overhead further reduces our execution time. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. K. Abdali and D. S. Wise. </author> <title> Experiments with quadtree representation of matrices. </title> <editor> In P. Gi-anni, editor, </editor> <booktitle> Symbolic and Algebraic Computation, volume 358 of Lecture Notes in Computer Science, </booktitle> <pages> pages 96-108. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: His scheme for space savings is not directly applicable for two reasons: we cannot assume that the input matrices can be overwritten, and his scheme requires several copying operations that reduce performance. 5.2 Hierarchical schemes for matrix storage Wise and his coauthors <ref> [1, 24] </ref> have investigated the algorithmic advantages of quad-tree representations of matrices. Morton ordering has also appeared in the parallel computing literature, where is has been used for load balancing of irregular problems [20].
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. D. Croz, A. Greenbaum, S. Ham-marling, A. McKenney, S. Oustrouchov, and D. Sorenson. </author> <title> LAPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <note> second edition, </note> <year> 1995. </year>
Reference-contexts: Our future work includes investigating techniques to further improve the performance and stability of Strassen's algorithm, while minimizing code complexity. We also plan to examine the effects of rectangular input matrices. Our implementation supports the same interface as Level 3 BLAS dgemm routine <ref> [2] </ref>, we plan to examine its performance for a variety of input parameters.
Reference: [3] <author> D. H. Bailey. </author> <title> Extra high speed matrix multiplication on the Cray-2. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 9(3) </volume> <pages> 603-607, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction The central role of matrix multiplication as a building block in numerical codes has generated a significant amount of research into techniques for improving the performance of this basic operation. Several of these efforts <ref> [3, 6, 12, 13, 14, 19] </ref> focus on algorithms whose arithmetic complexity fl This work supported in part by DARPA Grant DABT63-98-1-0001, NSF Grants CDA-97-2637 and CDA-95-12356, Duke University, and an equipment donation through Intel Corporation's Technology for Education 2000 Program. Chatterjee is partially supported by NSF CAREER Award CCR-95-01979. <p> to eliminate these conflict misses. 5 Related Work We discuss three separate areas of related work: implementations of Strassen-type algorithms, hierarchical schemes for matrix storage, and compiler technology for improving the cache behavior of loop nests. 5.1 Other implementations Previous implementations of Strassen's algorithm include Bailey's implementation for the CRAY-2 <ref> [3] </ref>, the DGEMMW implementation by Douglas et al. [6], and the DGEFMM implementation by Huss-Lederman et al.[13]. Bailey, coding in Fortran, used a static two-level unfolding of the recursion by code duplication. Douglas et al. introduced the dynamic overlap method for handling odd-sized dimensions.
Reference: [4] <author> S. Coleman and K. S. McKinley. </author> <title> Tile size selection using cache organization and data layout. </title> <booktitle> In Proceedings of SIGPLAN'95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 279-290, </pages> <address> La Jolla, CA, </address> <month> jun </month> <year> 1995. </year>
Reference-contexts: We do not carry the recursion to the level of single matrix elements as they do, but truncate the recursion when we reach tile sizes that fit in the upper levels of the memory hierarchy. 5.3 Cache behavior Several authors <ref> [17, 15, 4, 21] </ref> discuss loop transformations such as tiling that attempt to reduce the number of cache misses incurred by a loop nest and thus improve its performance.
Reference: [5] <author> J. J. Dongarra, J. D. Croz, S. Hammarling, and I. Duff. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: concerning these fast matrix multiplication algorithms, as they are covered elsewhere [10]. 2.1 Interface In order to stay consistent with previous work in this area and to permit meaningful comparisons, our implementation of Winograd's variant follows the same calling conventions as the dgemm subroutine in the Level 3 BLAS library <ref> [5] </ref>.
Reference: [6] <author> C. Douglas, M. Heroux, G. Slishman, and R. M. Smith. GEMMW: </author> <title> a portable level 3 BLAS Winograd variant of Strassen's matrix-matrix multiply algorithm. </title> <journal> Journal of Computational Physics, </journal> <pages> 110(1-10), </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction The central role of matrix multiplication as a building block in numerical codes has generated a significant amount of research into techniques for improving the performance of this basic operation. Several of these efforts <ref> [3, 6, 12, 13, 14, 19] </ref> focus on algorithms whose arithmetic complexity fl This work supported in part by DARPA Grant DABT63-98-1-0001, NSF Grants CDA-97-2637 and CDA-95-12356, Duke University, and an equipment donation through Intel Corporation's Technology for Education 2000 Program. Chatterjee is partially supported by NSF CAREER Award CCR-95-01979. <p> We measured execution times of our implementation (MODGEMM) and two alternative implementations, DGEFMM uses dynamic peeling [13] and DGEMMW uses dynamic overlap <ref> [6] </ref>, on both a DEC Alpha and a SUN UltraSPARC II. Our results show wide variability in the performance of all three implementations. On the Alpha, our implementation (MODGEMM) ranges from 30% slower to 20% faster than DGEFMM for matrix sizes from 150 to 1024. <p> D + fi fl C, if such post-processing is necessary. 4 Results This section compares the performance of our implementation (MODGEMM) of Strassen's algorithm to a previous implementation that uses dynamic peeling (DGEFMM) [13] (we use the author's original code), and to a previous implementation that uses dynamic overlap (DGEMMW) <ref> [6] </ref>. We measure the execution time of the various implementations on a 500 MHz DEC Alpha Miata and a 300 MHz Sun Ultra 60. <p> We discuss three separate areas of related work: implementations of Strassen-type algorithms, hierarchical schemes for matrix storage, and compiler technology for improving the cache behavior of loop nests. 5.1 Other implementations Previous implementations of Strassen's algorithm include Bailey's implementation for the CRAY-2 [3], the DGEMMW implementation by Douglas et al. <ref> [6] </ref>, and the DGEFMM implementation by Huss-Lederman et al.[13]. Bailey, coding in Fortran, used a static two-level unfolding of the recursion by code duplication. Douglas et al. introduced the dynamic overlap method for handling odd-sized dimensions. Huss-Lederman et al. introduced the dynamic peeling method. <p> called Morton order; by converting from standard layouts (e.g., column-major) to internal Morton layout at the interface level; and by exploiting dynamic selection of the recursion truncation point to minimize padding. 13 We compare our implementation to two alternative implementations that use dynamic peeling (DGEFMM) [13] and dynamic overlap (DGEMMW) <ref> [6] </ref>. Execution time measurements on a DEC Alpha and a SUN UltraSPARC II reveal wide variability in the performance of all three implementations. On the Alpha, our implementation (MODGEMM) ranges from 30% slower to 20% faster than DGEFMM for matrix sizes from 150 to 1024.
Reference: [7] <author> P. C. Fischer and R. L. Probert. </author> <title> Efficient procedures for using matrix algorithms. </title> <booktitle> In Automata, Languages and Programming, number 14 in Lecture Notes in Computer Science, </booktitle> <pages> pages 413-427. </pages> <publisher> Springer-Verlag, </publisher> <year> 1974. </year>
Reference-contexts: B 22 ) P 3 = A 11 * (B 12 B 22 ) P 5 = (A 11 + A 12 ) * B 22 P 7 = (A 12 A 22 ) * (B 21 + B 22 ) In this paper, we discuss and implement Winograd's variant <ref> [7] </ref> of Strassen's algorithm, which uses seven matrix multiplications and 15 matrix additions. It is well-known that this is the minimum number of multiplications and additions possible for any recursive matrix multiplication algorithm based on division into quadrants. The division of the matrices into quadrants follows equation (1).
Reference: [8] <author> J. D. Frens and D. S. Wise. </author> <title> Auto-blocking matrix-multiplication or tracking BLAS3 performance with source code. </title> <booktitle> In PPoPP97, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Second, to achieve performance stability as T varies, it is also important to have the tile contiguous in memory, thus avoiding self-interference misses [17]. Given the hierarchical nature of the algorithm (the decomposition is by quadrants within quadrants within : : :), hierarchical layouts such as Morton ordering <ref> [8] </ref> naturally suggest themselves for storing the matrices. An operational definition of Morton ordering is as follows. Divide the original matrix into four quadrants, and lay out these quadrants in memory in the order NW, NE, SW, SE. <p> Morton ordering has also appeared in the parallel computing literature, where is has been used for load balancing of irregular problems [20]. Most recently, Frens and Wise <ref> [8] </ref> discuss an implementation of a recursive O (n 3 ) matrix multiplication algorithm using hierarchical matrix layout, in which they sequence the recursive calls in an unusual manner to get better reuse in cache.
Reference: [9] <author> S. Ghosh, M. Martonosi, and S. Malik. </author> <title> Cache miss equations: An analytical representation of cache misses. </title> <booktitle> In Proceedings of International Conference on Supercomputing, </booktitle> <pages> pages 317-324, </pages> <address> Vienna, Austria, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: Our top-level conversion between the column-major layout at the interface level and the Morton ordering used internally can be viewed as a logical extension of this proposal. Ghosh et al. <ref> [9] </ref> present an analytical representation of cache misses for perfect loop nests, which they use to guide selected code optimization problems.
Reference: [10] <author> N. J. Higham. </author> <title> Accuracy and Stability of Numerical Algorithms. </title> <publisher> SIAM, </publisher> <address> Philadephia, </address> <year> 1996. </year> <month> 14 </month>
Reference-contexts: We do not discuss in this paper numerical issues concerning these fast matrix multiplication algorithms, as they are covered elsewhere <ref> [10] </ref>. 2.1 Interface In order to stay consistent with previous work in this area and to permit meaningful comparisons, our implementation of Winograd's variant follows the same calling conventions as the dgemm subroutine in the Level 3 BLAS library [5].
Reference: [11] <author> M. D. Hill and A. J. Smith. </author> <title> Evaluating associativity in CPU caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-38(12):1612-1630, </volume> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: A primary condition for performance is to choose a tile size T such that the tiles fit into the first-level cache, thus avoiding capacity misses <ref> [11] </ref>. This is easily achieved using tile sizes in the range shown in Figure 2. Second, to achieve performance stability as T varies, it is also important to have the tile contiguous in memory, thus avoiding self-interference misses [17].
Reference: [12] <author> C. H. Huang, J. R. Johnson, and R. W. Johnson. </author> <title> A tensor product formulation of Strassen's matrix multiplication algorithm. </title> <journal> Applied Mathematics Letters, </journal> <volume> 3(3) </volume> <pages> 67-71, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction The central role of matrix multiplication as a building block in numerical codes has generated a significant amount of research into techniques for improving the performance of this basic operation. Several of these efforts <ref> [3, 6, 12, 13, 14, 19] </ref> focus on algorithms whose arithmetic complexity fl This work supported in part by DARPA Grant DABT63-98-1-0001, NSF Grants CDA-97-2637 and CDA-95-12356, Duke University, and an equipment donation through Intel Corporation's Technology for Education 2000 Program. Chatterjee is partially supported by NSF CAREER Award CCR-95-01979.
Reference: [13] <author> S. Huss-Lederman, E. M. Jacobson, J. R. Johnson, A. Tsao, and T. Turnbull. </author> <title> Implementation of Strassen's algorithm for matrix multiplication. </title> <booktitle> In Proceedings of Supercomputing '96, </booktitle> <year> 1996. </year> <note> http://www.supercomp.org/sc96. </note>
Reference-contexts: 1 Introduction The central role of matrix multiplication as a building block in numerical codes has generated a significant amount of research into techniques for improving the performance of this basic operation. Several of these efforts <ref> [3, 6, 12, 13, 14, 19] </ref> focus on algorithms whose arithmetic complexity fl This work supported in part by DARPA Grant DABT63-98-1-0001, NSF Grants CDA-97-2637 and CDA-95-12356, Duke University, and an equipment donation through Intel Corporation's Technology for Education 2000 Program. Chatterjee is partially supported by NSF CAREER Award CCR-95-01979. <p> This overhead is generally limited by stopping the recursion early and performing a conventional matrix multiplication on submatrices that are below the recursion truncation point <ref> [13] </ref>. Second, the division step must efficiently handle odd-sized matrices. <p> We measured execution times of our implementation (MODGEMM) and two alternative implementations, DGEFMM uses dynamic peeling <ref> [13] </ref> and DGEMMW uses dynamic overlap [6], on both a DEC Alpha and a SUN UltraSPARC II. Our results show wide variability in the performance of all three implementations. <p> We now review these implementation issues, identify possible solution strategies, and justify our specific choices. 3.1 Recursion truncation point The seven products can be computed by recursively invoking Strassen's algorithm on smaller sub-problems, and switching to the conventional algorithm at some matrix size T (called the recursion truncation point <ref> [13] </ref>) at which Strassen's construction is no longer advantageous. If one were to estimate running time by counting arithmetic operations, the recursion truncation point would be around 16. However, the empirically observed value of this parameter is at least an order of magnitude higher. <p> These interference effects can be mitigated using non-standard data layouts, as we discuss further in Section 3.3. * A second solution, dynamic peeling, peels off the extra row or column at each level, and separately adds their contributions to the overall solution in a later fix-up computation <ref> [13] </ref>. This eliminates the need for extra padding, but reduces the portion of the matrix to which Strassen's algorithm applies, thus reducing the potential benefits of the recursive strategy. <p> We then post-process to compute C ff fl D + fi fl C, if such post-processing is necessary. 4 Results This section compares the performance of our implementation (MODGEMM) of Strassen's algorithm to a previous implementation that uses dynamic peeling (DGEFMM) <ref> [13] </ref> (we use the author's original code), and to a previous implementation that uses dynamic overlap (DGEMMW) [6]. We measure the execution time of the various implementations on a 500 MHz DEC Alpha Miata and a 300 MHz Sun Ultra 60. <p> using a non-standard array layout called Morton order; by converting from standard layouts (e.g., column-major) to internal Morton layout at the interface level; and by exploiting dynamic selection of the recursion truncation point to minimize padding. 13 We compare our implementation to two alternative implementations that use dynamic peeling (DGEFMM) <ref> [13] </ref> and dynamic overlap (DGEMMW) [6]. Execution time measurements on a DEC Alpha and a SUN UltraSPARC II reveal wide variability in the performance of all three implementations. On the Alpha, our implementation (MODGEMM) ranges from 30% slower to 20% faster than DGEFMM for matrix sizes from 150 to 1024.
Reference: [14] <author> IBM Corporation. </author> <title> IBM engineering and scientific subroutine library guide and reference, 1992. Order number: </title> <publisher> SC23-0526. </publisher>
Reference-contexts: 1 Introduction The central role of matrix multiplication as a building block in numerical codes has generated a significant amount of research into techniques for improving the performance of this basic operation. Several of these efforts <ref> [3, 6, 12, 13, 14, 19] </ref> focus on algorithms whose arithmetic complexity fl This work supported in part by DARPA Grant DABT63-98-1-0001, NSF Grants CDA-97-2637 and CDA-95-12356, Duke University, and an equipment donation through Intel Corporation's Technology for Education 2000 Program. Chatterjee is partially supported by NSF CAREER Award CCR-95-01979.
Reference: [15] <author> I. Kodukula, N. Ahmed, and K. Pingali. </author> <title> Data-centric multi-level blocking. </title> <booktitle> In PLDI97, </booktitle> <year> 1997. </year>
Reference-contexts: We do not carry the recursion to the level of single matrix elements as they do, but truncate the recursion when we reach tile sizes that fit in the upper levels of the memory hierarchy. 5.3 Cache behavior Several authors <ref> [17, 15, 4, 21] </ref> discuss loop transformations such as tiling that attempt to reduce the number of cache misses incurred by a loop nest and thus improve its performance.
Reference: [16] <author> A. Kreczmar. </author> <title> On memory requirements of Strassen's algorithms. </title> <editor> In A. Mazurkiewicz, editor, </editor> <booktitle> Mathematical Foundations of Computer Science 1976, number 45 in Lecture Notes in Computer Science, </booktitle> <pages> pages 404-407. </pages> <publisher> Springer-Verlag, </publisher> <year> 1976. </year>
Reference-contexts: In some cases (such as on the CRAYs), this issue did not arise because the memory system was not cache-based. Section 4 gives extensive performance comparisons of our implementation vs. DGEFMM and DGEMMW. 12 Kreczmar <ref> [16] </ref> proposes an elegant memory-efficient version of Strassen's algorithm based on overwriting one of the input arguments.
Reference: [17] <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In ASPLOS4, </booktitle> <pages> pages 63-74, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This is easily achieved using tile sizes in the range shown in Figure 2. Second, to achieve performance stability as T varies, it is also important to have the tile contiguous in memory, thus avoiding self-interference misses <ref> [17] </ref>. Given the hierarchical nature of the algorithm (the decomposition is by quadrants within quadrants within : : :), hierarchical layouts such as Morton ordering [8] naturally suggest themselves for storing the matrices. An operational definition of Morton ordering is as follows. <p> We do not carry the recursion to the level of single matrix elements as they do, but truncate the recursion when we reach tile sizes that fit in the upper levels of the memory hierarchy. 5.3 Cache behavior Several authors <ref> [17, 15, 4, 21] </ref> discuss loop transformations such as tiling that attempt to reduce the number of cache misses incurred by a loop nest and thus improve its performance. <p> While these loop transformations are not specific to matrix multiplication, the conventional three-loop algorithm for matrix multiplication falls into the category of codes that they can handle. Lam, Rothberg, and Wolf <ref> [17] </ref> investigated and modeled the influence of cache interference on the performance of tiled programs. They emphasized the importance of having tiles be contiguous in memory to avoid self-interference misses, and proposed data copying to satisfy this condition.
Reference: [18] <author> A. R. Lebeck and D. A. Wood. </author> <title> Cache profiling and the SPEC benchmarks: A case study. </title> <journal> IEEE COMPUTER, </journal> <volume> 27(10) </volume> <pages> 15-26, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The first observation from this graph is that MODGEMM miss ratios (6% to 2%) are lower than DGEFMM (8%), which matches our expectations. The second observation is the unexpected dramatic drop in MODGEMM's miss ratio at a matrix size of 513. Preliminary investigations using CProf <ref> [18] </ref> reveal that this drop is due to a reduction in conflict misses. 11 To understand this phenomenon, consider that for matrix sizes of 505 to 512 the padded matrix size is 512 and the recursion truncation point is at tile size 32.
Reference: [19] <author> V. P. Pauca, X. Sun, S. Chatterjee, and A. R. Lebeck. </author> <title> Architecture-efficient Strassen's matrix multiplication: A case study of divide-and-conquer algorithms. </title> <booktitle> In International Linear Algebra Society(ILAS) Symposium on Algorithms for Control, Signals and Image Processing, </booktitle> <year> 1997. </year>
Reference-contexts: 1 Introduction The central role of matrix multiplication as a building block in numerical codes has generated a significant amount of research into techniques for improving the performance of this basic operation. Several of these efforts <ref> [3, 6, 12, 13, 14, 19] </ref> focus on algorithms whose arithmetic complexity fl This work supported in part by DARPA Grant DABT63-98-1-0001, NSF Grants CDA-97-2637 and CDA-95-12356, Duke University, and an equipment donation through Intel Corporation's Technology for Education 2000 Program. Chatterjee is partially supported by NSF CAREER Award CCR-95-01979.
Reference: [20] <author> J. R. Pilkington and S. B. Baden. </author> <title> Dynamic partitioning of non-uniform structured workloads with spacefilling curves. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 7(3) </volume> <pages> 288-300, </pages> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: Morton ordering has also appeared in the parallel computing literature, where is has been used for load balancing of irregular problems <ref> [20] </ref>. Most recently, Frens and Wise [8] discuss an implementation of a recursive O (n 3 ) matrix multiplication algorithm using hierarchical matrix layout, in which they sequence the recursive calls in an unusual manner to get better reuse in cache.
Reference: [21] <author> G. Rivera and C.-W. Tseng. </author> <title> Data transformations for eliminating conflict misses. </title> <booktitle> In Proceedings of SIGPLAN'98 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 38-49, </pages> <address> Montreal, Canada, </address> <month> jun </month> <year> 1998. </year>
Reference-contexts: We do not carry the recursion to the level of single matrix elements as they do, but truncate the recursion when we reach tile sizes that fit in the upper levels of the memory hierarchy. 5.3 Cache behavior Several authors <ref> [17, 15, 4, 21] </ref> discuss loop transformations such as tiling that attempt to reduce the number of cache misses incurred by a loop nest and thus improve its performance.
Reference: [22] <author> A. Srivastava and A. Eustace. </author> <title> Atom a system for building customized program analysis tools. </title> <booktitle> In PLDI94, </booktitle> <pages> pages 196-205, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Furthermore, without Mor-ton conversion, MODGEMM is very competitive with DGEMMW, and outperforms it for many matrix sizes. 4.2 Cache Effects Our initial efforts to gain further insight into the performance variability of our implementation begins with analysis of its cache behavior. Here, we present preliminary results. We used ATOM <ref> [22] </ref> to perform cache simulations of a 16KB direct-mapped cache with 32 byte blocks of both the DGEFMM, and MODGEMM implementations. Figure 9 shows the miss ratios of each implementation for matrix sizes ranging from 500 to 523.
Reference: [23] <author> V. Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Numerische Mathematik, </journal> <volume> 13 </volume> <pages> 354-356, </pages> <year> 1969. </year>
Reference-contexts: Chatterjee is partially supported by NSF CAREER Award CCR-95-01979. Lebeck is partially supported by NSF CAREER Award MIP-97-02547. 1 is O (n log 2 7 ) instead of the conventional O (n 3 ) algorithm. Strassen's algorithm <ref> [23] </ref> for matrix multiplication and its variants are the most practical of such algorithms, and are classic examples of theoretically high-performance algorithms that are challenging to implement efficiently on modern high-end computers with deep memory hierarchies. Strassen's algorithm achieves its lower complexity using a divide-and-conquer approach. <p> Section 3 discusses the implementation issues that affect memory efficiency, and our solutions to these issues. Section 4 presents performance results for our code. Section 5 cites related work and compares our techniques to them. Section 6 presents conclusions and future work. 2 Background Strassen's original algorithm <ref> [23] </ref> is usually described in the following divide-and-conquer form. Let A and B be two n fi n matrices, where n is an even integer. Partition the two input matrices A 2 and B and the result matrix C into quadrants as follows.

References-found: 23

