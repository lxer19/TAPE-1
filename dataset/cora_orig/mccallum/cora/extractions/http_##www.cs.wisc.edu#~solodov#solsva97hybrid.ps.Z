URL: http://www.cs.wisc.edu/~solodov/solsva97hybrid.ps.Z
Refering-URL: http://www.cs.wisc.edu/~solodov/solodov.html
Root-URL: http://www.cs.wisc.edu
Title: Journal of Convex Analysis (accepted for publication) A HYBRID PROJECTION-PROXIMAL POINT ALGORITHM  
Author: M. V. Solodov and B. F. Svaiter 
Keyword: Key words. Maximal monotone operators, proximal point methods, projection methods.  
Address: Janeiro, RJ 22460-320, Brazil.  
Affiliation: Instituto de Matematica Pura e Aplicada, Estrada Dona Castorina 110, Jardim Bot^anico, Rio de  
Note: AMS subject classifications. 90C25, 49J45, 49M45. Research of the first author is supported by CNPq Grant 300734/95-6 and by PRONEX-Optimization, research of the second author is supported by CNPq Grant 301200/93-9(RN) and by PRONEX-Optimization.  
Email: Email solodov@impa.br and benar@impa.br  
Date: January 27, 1997 (Revised August 24, 1998)  
Abstract: We propose a modification of the classical proximal point algorithm for finding zeroes of a maximal monotone operator in a Hilbert space. In particular, an approximate proximal point iteration is used to construct a hyperplane which strictly separates the current iterate from the solution set of the problem. This step is then followed by a projection of the current iterate onto the separating hyperplane. All information required for this projection operation is readily available at the end of the approximate proximal step, and therefore this projection entails no additional computational cost. The new algorithm allows significant relaxation of tolerance requirements imposed on the solution of proximal point subproblems, which yields a more practical framework. Weak global convergence and local linear rate of convergence are established under suitable assumptions. Additionally, presented analysis yields an alternative proof of convergence for the exact proximal point method, which allows a nice geometric interpretation, and is somewhat more intuitive than the classical proof. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.F. Bonnans, J.C. Gilbert, C. Lemarechal, and C. Sagastizabal. </author> <title> A family of variable metric proximal point methods. </title> <journal> Mathematical Programming, </journal> <volume> 68 </volume> <pages> 15-47, </pages> <year> 1995. </year>
Reference-contexts: It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient. Developments aimed at speeding up convergence of proximal and related methods can be found in <ref> [1, 4, 9, 10, 13, 17, 2, 3] </ref>. Most of the work just cited is focused on the variable metric approach and other ways of incorporating second order information 1 to achieve faster convergence. <p> It is therefore worthwhile to develop new algorithms which admit less stringent requirements on solving the subproblems. There have been some advances in this direction in the context of optimization problems (for example, <ref> [1, 25] </ref>). In the general case of solving operator equations, the situation seems to be more complicated, and we are not aware of any previous work in this direction. In this paper, we propose one such algorithm.
Reference: [2] <author> J.V. Burke and Maijian Qian. </author> <title> The variable metric proximal point algorithm, I: Basic convergence theory, 1996. </title> <institution> Department of Mathematics, University of Washington, </institution> <address> Seattle, WA. </address>
Reference-contexts: Similar criteria were used, for example, in <ref> [2] </ref>. Proximal point methods have been studied extensively (e.g. [14, 18, 19, 15, 12, 11, 6, 7, 5], see [8] for a survey). It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient. <p> It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient. Developments aimed at speeding up convergence of proximal and related methods can be found in <ref> [1, 4, 9, 10, 13, 17, 2, 3] </ref>. Most of the work just cited is focused on the variable metric approach and other ways of incorporating second order information 1 to achieve faster convergence. <p> Most of the work just cited is focused on the variable metric approach and other ways of incorporating second order information 1 to achieve faster convergence. As it has been remarked in [19] and <ref> [2] </ref>, for a proximal point method to be practical, it is also important that it should work with approximate solutions of subproblems. It is therefore worthwhile to develop new algorithms which admit less stringent requirements on solving the subproblems. <p> For 6= 0 however, Algorithm 1.1 is different from other inexact proximal-based methods in the literature. 3 We point out that our tolerance condition (1.7) is less restrictive than the corresponding tolerance requirements in <ref> [19, 2] </ref> given by (1.4) or (1.5), which are standard in the literature. For one thing, the right-hand-side of (1.7) involves the largest of two quantities, kv i k and i ky i x i k.
Reference: [3] <author> J.V. Burke and Maijian Qian. </author> <title> The variable metric proximal point algorithm, II: Quasi-Newton updating, 1996. </title> <institution> Department of Mathematics, University of Washington, </institution> <address> Seattle, WA. </address>
Reference-contexts: It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient. Developments aimed at speeding up convergence of proximal and related methods can be found in <ref> [1, 4, 9, 10, 13, 17, 2, 3] </ref>. Most of the work just cited is focused on the variable metric approach and other ways of incorporating second order information 1 to achieve faster convergence.
Reference: [4] <author> X. Chen and M. Fukushima. </author> <title> Proximal quasi-Newton methods for nondifferentiable convex optimization. </title> <journal> Mathematical Programming. </journal> <note> To appear. 12 </note>
Reference-contexts: It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient. Developments aimed at speeding up convergence of proximal and related methods can be found in <ref> [1, 4, 9, 10, 13, 17, 2, 3] </ref>. Most of the work just cited is focused on the variable metric approach and other ways of incorporating second order information 1 to achieve faster convergence.
Reference: [5] <author> J. Eckstein and D.P. Bertsekas. </author> <title> On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators. </title> <journal> Mathematical Programming, </journal> <volume> 55 </volume> <pages> 293-318, </pages> <year> 1992. </year>
Reference-contexts: Similar criteria were used, for example, in [2]. Proximal point methods have been studied extensively (e.g. <ref> [14, 18, 19, 15, 12, 11, 6, 7, 5] </ref>, see [8] for a survey). It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient.
Reference: [6] <author> M.C. Ferris. </author> <title> Finite termination of the proximal point algorithm. </title> <journal> Mathematical Programming, </journal> <volume> 50 </volume> <pages> 359-366, </pages> <year> 1991. </year>
Reference-contexts: Similar criteria were used, for example, in [2]. Proximal point methods have been studied extensively (e.g. <ref> [14, 18, 19, 15, 12, 11, 6, 7, 5] </ref>, see [8] for a survey). It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient.
Reference: [7] <author> O. Guler. </author> <title> New proximal point algorithms for convex minimization. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2 </volume> <pages> 649-664, </pages> <year> 1992. </year>
Reference-contexts: Similar criteria were used, for example, in [2]. Proximal point methods have been studied extensively (e.g. <ref> [14, 18, 19, 15, 12, 11, 6, 7, 5] </ref>, see [8] for a survey). It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient.
Reference: [8] <author> B. Lemaire. </author> <title> The proximal algorithm. </title> <editor> In J.P. Penot, editor, </editor> <booktitle> International Series of Numerical Mathematics, </booktitle> <pages> pages 73-87. </pages> <publisher> Birkhauser, </publisher> <address> Basel, </address> <year> 1989. </year>
Reference-contexts: Similar criteria were used, for example, in [2]. Proximal point methods have been studied extensively (e.g. [14, 18, 19, 15, 12, 11, 6, 7, 5], see <ref> [8] </ref> for a survey). It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient. Developments aimed at speeding up convergence of proximal and related methods can be found in [1, 4, 9, 10, 13, 17, 2, 3].
Reference: [9] <editor> C. Lemarechal and C. Sagastizabal. </editor> <title> An approach to variable metric bundle methods. </title> <editor> In J. Henry and J-P. Yvon, editors, </editor> <booktitle> Lecture notes in Control and Information Sciences No. 197, System Modelling and Optimization, </booktitle> <pages> pages 144-162. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1994. </year>
Reference-contexts: It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient. Developments aimed at speeding up convergence of proximal and related methods can be found in <ref> [1, 4, 9, 10, 13, 17, 2, 3] </ref>. Most of the work just cited is focused on the variable metric approach and other ways of incorporating second order information 1 to achieve faster convergence.
Reference: [10] <editor> C. Lemarechal and C. Sagastizabal. </editor> <title> Variable metric bundle methods : From conceptual to implementable forms. </title> <journal> Mathematical Programming, </journal> <volume> 76 </volume> <pages> 393-410, </pages> <year> 1997. </year>
Reference-contexts: It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient. Developments aimed at speeding up convergence of proximal and related methods can be found in <ref> [1, 4, 9, 10, 13, 17, 2, 3] </ref>. Most of the work just cited is focused on the variable metric approach and other ways of incorporating second order information 1 to achieve faster convergence.
Reference: [11] <author> F.J. Luque. </author> <title> Asymptotic convergence analysis of the proximal point algorithm. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 22 </volume> <pages> 277-293, </pages> <year> 1984. </year>
Reference-contexts: Similar criteria were used, for example, in [2]. Proximal point methods have been studied extensively (e.g. <ref> [14, 18, 19, 15, 12, 11, 6, 7, 5] </ref>, see [8] for a survey). It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient.
Reference: [12] <editor> B. Martinet. Regularisation d'inequations variationelles par approximations successives. Revue Fran~caise d'Informatique et de Recherche Operationelle, </editor> <volume> 4 </volume> <pages> 154-159, </pages> <year> 1970. </year>
Reference-contexts: Similar criteria were used, for example, in [2]. Proximal point methods have been studied extensively (e.g. <ref> [14, 18, 19, 15, 12, 11, 6, 7, 5] </ref>, see [8] for a survey). It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient.
Reference: [13] <author> R. Mi*in. </author> <title> A quasi-second-order proximal bundle algorithm. </title> <journal> Mathematical Programming, </journal> <volume> 73 </volume> <pages> 51-72, </pages> <year> 1996. </year>
Reference-contexts: It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient. Developments aimed at speeding up convergence of proximal and related methods can be found in <ref> [1, 4, 9, 10, 13, 17, 2, 3] </ref>. Most of the work just cited is focused on the variable metric approach and other ways of incorporating second order information 1 to achieve faster convergence.
Reference: [14] <author> J.-J. Moreau. </author> <title> Proximite et dualite dans un espace Hilbertien. </title> <journal> Bulletin de la Societe Mathematique de France, </journal> <volume> 93 </volume> <pages> 273-299, </pages> <year> 1965. </year>
Reference-contexts: Similar criteria were used, for example, in [2]. Proximal point methods have been studied extensively (e.g. <ref> [14, 18, 19, 15, 12, 11, 6, 7, 5] </ref>, see [8] for a survey). It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient.
Reference: [15] <author> G.B. Passty. </author> <title> Weak convergence theorems for nonexpansive mappings in Banach spaces. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 67 </volume> <pages> 274-276, </pages> <year> 1979. </year>
Reference-contexts: Similar criteria were used, for example, in [2]. Proximal point methods have been studied extensively (e.g. <ref> [14, 18, 19, 15, 12, 11, 6, 7, 5] </ref>, see [8] for a survey). It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient.
Reference: [16] <author> B.T. Polyak. </author> <title> Introduction to Optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York, </address> <year> 1987. </year> <month> 13 </month>
Reference-contexts: Moreover, z = P H [x] is also the projection of x onto the halfspace fs 2 H j hv; s yi 0g. Since x belongs to this halfspace, it follows from the basic properties of the projection operator (see, for example, <ref> [16, p.121] </ref>) that hx z; z xi 0.
Reference: [17] <author> L. Qi and X. Chen. </author> <title> A preconditioning proximal Newton method for nondifferentiable convex optimization. </title> <type> Technical Report AMR 95/20, </type> <institution> Department of Applied Mathematics, University of New South Wales, </institution> <address> Sydney, SW, Australia, </address> <year> 1995. </year>
Reference-contexts: It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient. Developments aimed at speeding up convergence of proximal and related methods can be found in <ref> [1, 4, 9, 10, 13, 17, 2, 3] </ref>. Most of the work just cited is focused on the variable metric approach and other ways of incorporating second order information 1 to achieve faster convergence.
Reference: [18] <author> R.T. Rockafellar. </author> <title> Augmented Lagrangians and applications of the proximal point algorithm in convex programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 1 </volume> <pages> 97-116, </pages> <year> 1976. </year>
Reference-contexts: Similar criteria were used, for example, in [2]. Proximal point methods have been studied extensively (e.g. <ref> [14, 18, 19, 15, 12, 11, 6, 7, 5] </ref>, see [8] for a survey). It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient.
Reference: [19] <author> R.T. Rockafellar. </author> <title> Monotone operators and the proximal point algorithm. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 14 </volume> <pages> 877-898, </pages> <year> 1976. </year>
Reference-contexts: In <ref> [19] </ref>, the following approximation criteria for the solution of proximal point subproblems (1.2) were used : k" i k i i ; i=0 and 1 X i &lt; 1: (1.5) The first criterion was needed to establish global convergence of the proximal point algorithm, while the second was required for local <p> the solution of proximal point subproblems (1.2) were used : k" i k i i ; i=0 and 1 X i &lt; 1: (1.5) The first criterion was needed to establish global convergence of the proximal point algorithm, while the second was required for local linear rate of convergence result <ref> [19, Theorems 1 and 2] </ref> (it was also assumed that the sequence f i g is uniformly bounded from above). Similar criteria were used, for example, in [2]. Proximal point methods have been studied extensively (e.g. [14, 18, 19, 15, 12, 11, 6, 7, 5], see [8] for a survey). <p> Similar criteria were used, for example, in [2]. Proximal point methods have been studied extensively (e.g. <ref> [14, 18, 19, 15, 12, 11, 6, 7, 5] </ref>, see [8] for a survey). It has long been realized however that in many applications, proximal point methods in the classical form are not very efficient. <p> Most of the work just cited is focused on the variable metric approach and other ways of incorporating second order information 1 to achieve faster convergence. As it has been remarked in <ref> [19] </ref> and [2], for a proximal point method to be practical, it is also important that it should work with approximate solutions of subproblems. It is therefore worthwhile to develop new algorithms which admit less stringent requirements on solving the subproblems. <p> For 6= 0 however, Algorithm 1.1 is different from other inexact proximal-based methods in the literature. 3 We point out that our tolerance condition (1.7) is less restrictive than the corresponding tolerance requirements in <ref> [19, 2] </ref> given by (1.4) or (1.5), which are standard in the literature. For one thing, the right-hand-side of (1.7) involves the largest of two quantities, kv i k and i ky i x i k. <p> more intuitive, as it allows for a nice and simple geometric interpretation using separation arguments. 9 We next show that the iterates converge at a linear rate to the solution of the problem, provided T 1 is Lipschitz continuous at zero (note that, in that case, the solution is unique) <ref> [19] </ref>.
Reference: [20] <author> M.V. Solodov and B.F. Svaiter. </author> <title> A new projection method for variational inequality problems. </title> <journal> SIAM Journal on Control and Optimization. </journal> <note> To appear. </note>
Reference-contexts: By separation arguments, it can be shown that the new iterate is closer to the solution set than the previous one, which essentially entails global convergence of the algorithm. A similar idea has also been used in <ref> [20] </ref> to devise new projection methods for solving variational inequality problems with a continuous (pseudo)monotone operator.
Reference: [21] <author> M.V. Solodov and B.F. Svaiter. </author> <title> A globally convergent inexact Newton method for systems of monotone equations. </title> <editor> In M. Fukushima and L. Qi, editors, </editor> <title> Reformulation - Nonsmooth, Piecewise Smooth, </title> <booktitle> Semismooth and Smoothing Methods, </booktitle> <pages> pages 355-369. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1998. </year>
Reference-contexts: The general algorithm presented here has already proved to be useful for devising truly globally convergent (i.e. the whole sequence of iterates is globally convergent to a solution without any regularity assumptions) and locally superlinearly convergent inexact Newton methods for solving systems of monotone equations <ref> [21] </ref> and monotone nonlinear complementarity problems [23]. An extention to inexact Bregman-function-based proximal algorithms is given in [22]. Using the presented approach in conjunction with variable metric proximal point methods and decomposition (operator splitting) algorithms can be an interesting subject for future research.
Reference: [22] <author> M.V. Solodov and B.F. Svaiter. </author> <title> An inexact hybrid generalized proximal point algorithm and some new results on the theory of Bregman functions, </title> <note> 1998. Mathematics of Operations Research, submitted. </note>
Reference-contexts: An extention to inexact Bregman-function-based proximal algorithms is given in <ref> [22] </ref>. Using the presented approach in conjunction with variable metric proximal point methods and decomposition (operator splitting) algorithms can be an interesting subject for future research.
Reference: [23] <author> M.V. Solodov and B.F. Svaiter. </author> <title> A truly globally convergent Newton-type method for the monotone nonlinear complementarity problem, </title> <note> 1998. SIAM Journal on Optimization, submitted. </note>
Reference-contexts: general algorithm presented here has already proved to be useful for devising truly globally convergent (i.e. the whole sequence of iterates is globally convergent to a solution without any regularity assumptions) and locally superlinearly convergent inexact Newton methods for solving systems of monotone equations [21] and monotone nonlinear complementarity problems <ref> [23] </ref>. An extention to inexact Bregman-function-based proximal algorithms is given in [22]. Using the presented approach in conjunction with variable metric proximal point methods and decomposition (operator splitting) algorithms can be an interesting subject for future research.
Reference: [24] <author> P. Tseng. </author> <title> On linear convergence of iterative methods for the variational inequality problem. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 60 </volume> <pages> 237-252, </pages> <year> 1995. </year>
Reference-contexts: Remark 2.2 In the finite-dimensional case, it is possible to relax the hypothesis of Theorem 2.2. In particular, if i &lt; 1, then the following condition <ref> [24] </ref> is sufficient for linear convergence of the algorithm : ky P S [y]k Lkvk for all y 2 T 1 (v); kvk ffi; where P S [] denotes the projection map onto the solution set S (which need not be a singleton).
Reference: [25] <author> C. Zhu. </author> <title> Asymptotic convergence analysis of some inexact proximal point algorithms for minimization. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 6 </volume> <pages> 626-637, </pages> <year> 1996. </year> <month> 14 </month>
Reference-contexts: It is therefore worthwhile to develop new algorithms which admit less stringent requirements on solving the subproblems. There have been some advances in this direction in the context of optimization problems (for example, <ref> [1, 25] </ref>). In the general case of solving operator equations, the situation seems to be more complicated, and we are not aware of any previous work in this direction. In this paper, we propose one such algorithm.
References-found: 25

