URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR396.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Title: SOLVING LINEAR INEQUALITIES IN A LEAST SQUARES SENSE  
Author: R. BRAMLEY AND B. WINNICKA 
Abstract: In 1980, Han [5] described a finitely terminating algorithm for solving a system Ax b of linear inequalities in a least squares sense. The algorithm requires a singular value decomposition of a submatrix of A on each iteration, making it impractical for all but the smallest problems. This paper shows that a modification of Han's algorithm allows the iterates to be computed using QR factorization with column pivoting, which significantly reduces the computational cost and allows efficient updating/downdating techniques to be used. The effectiveness of this modification is demonstrated, implementation details are given, and the behaviour of the algorithm discussed. Theoretical and numerical results are shown from the application of the algorithm for linear separability problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Bennett and O. Mangasarian, </author> <title> Robust linear programming discrimination of two linearly inseparable sets, Optimization Methods and Software, </title> <booktitle> 1 (1992), </booktitle> <pages> pp. 23-34. </pages>
Reference-contexts: Maximum Iterations Required for m fi n Problems 18 Fig. 8. Maximum Iterations Required for m fi n Problems 19 note that the use of the constant 1 is arbitrary, since we may scale the vector (w; fl) without changing the placement or orientation of the resulting hyperplane. In <ref> [1] </ref>, Bennett and Mangasarian formulate this problem in terms of the L 1 norm: miminize 1 m X i w + fl + 1 + 1 k X j w fl + 1 + and then solve it as the linear programming problem: minimize over (w; fl; y; z) y T <p> Note that it is easy to check for the condition in (23) before beginning computations, and perturbing any entry of the matrix G will prevent the trivial solution from oc-curing. This should be contrasted with the L 1 norm minimizing method of <ref> [1] </ref>, where only the objective function for the linear program need be scaled. However, in that formulation it is only guaranteed that nontrivial solutions exist to the linear program, but it is not assured that a linear programming program will find such a nontrivial solution. <p> Each data point has 9 components, corresponding to experimental measurements. The second data set consists of 297 points, 137 from set A (corresponding to a negative diagnosis) and 160 from set B (corresponding to a positive diagnosis). We discarded data samples that had missing measurements. As in <ref> [1] </ref>, the data was divided randomly into a training group consisting of 2/3 of the data points, and a testing group consisting of the remaining 1/3 of the data. <p> For the second data set, 5 or 6 iterations were needed each time and an average of 95% of the flops were spent in finding the search direction. Table 2 shows the results both with and without the secondary minimization on fl, along with similar results from <ref> [1] </ref>. The results are not strictly comparable since that work used 566 data points from the Wisconsin Cancer Database and 197 data points from the Cleveland Heart Disease Database, but the comparison suggest that the inequality least squares method provides a solution similar to that of a linear programming method.
Reference: [2] <author> R. D. et al, </author> <title> International application of a new probability algorithm for the diagnosis of coronary artery disease, </title> <journal> American Journal of Cardiology, </journal> <volume> 64 (1989), </volume> <pages> pp. 304-310. </pages>
Reference-contexts: Performance on Test Databases. We have also run tests on the Wis-consin Breast Cancer Database and the Cleveland Heart Disease Database <ref> [2] </ref>. Both data sets are available from the University of California-Irvine Repository Of Machine Learning Databases and Domain Theories 2 . The first data set consists of 551 points, 346 from set A (corresponding to benign tumors) and 205 from set B (corresponding to malignant tumors).
Reference: [3] <author> G. Golub and C. V. Loan, </author> <title> Matrix Computations, </title> <publisher> John Hopkins University Press, </publisher> <address> Baltimore, 2 ed., </address> <year> 1989. </year>
Reference-contexts: The observations may be inconsistent, and in this case a solution is sought that minimizes the norm of the residuals. More information about linear least squares problems and solution techniques can be found in <ref> [7, 13, 3] </ref>. <p> The vector G y f is the minimum norm, least squares solution to the problem of minimizing jjGy f jj, and can be computed by a QR factorization when G is full-rank, or by the singular value decomposition (SVD) otherwise. See Chapter 5 of <ref> [3] </ref> for details on computing these factorizations. Second, let I f1; 2; : : : ; mg be an index set. Then A I is the submatrix of A consisting of rows with indices in I.
Reference: [4] <author> G. Golub and V. Pereysa, </author> <title> Differentiation of pseudoinverse, separable nonlinear least squares problems and other tales, in Generalized Inverses and Applications, </title> <editor> M. Nashed, ed., </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976, </year> <pages> pp. 303-323. </pages>
Reference-contexts: This is critical since, with one exception, the proof of finite termination in [5] involves only the quantity A I d, not d. Before addressing that exception, we recall a result of Golub and Pereya <ref> [4] </ref>: Theorem 3.2. Let G be an m fi n matrix, and let GP = Q ^ R be the QR with column pivoting factorization of G.
Reference: [5] <author> S.-P. Han, </author> <title> Least-squares solution of linear inequalities, </title> <type> Tech. Rep. </type> <institution> TR-2141, Mathematics Research Center, University of Wisconsin-Madison, </institution> <year> 1980. </year>
Reference-contexts: As we will show, the analogue of an active set for the algorithm described in this paper is automatically determined without difficult decisions of when to drop a constraint. The only algorithm specifically designed for solving linear inequalities in a least squares sense was developed by S.-P. Han <ref> [5] </ref>. That algorithm requires finding the minimum norm least squares (equality) solution to systems A I x = b I , where A I is a submatrix of A consisting of rows of A. <p> Section 2 of this paper defines notation and reviews some basic properties of least squares solutions for linear inequalities, most of which can be found in <ref> [5] </ref>. Section 3 states Han's algorithm and shows the minor change in the convergence proof that allows QR with column pivoting to be used. Section 4 discusses implementation details and demonstrates the robustness of the modified algorithm. <p> Basics of systems of linear inequalities. This Section summarizes some fundamental properties of linear inequalities from Han's technical report, and proofs of the results can be found in <ref> [5] </ref>. Let A 2 &lt; mfin be an arbitrary real matrix, and let b 2 &lt; m a given vector. No relation is assumed between m and n, and the matrix A can be rank-deficient, ill-conditioned, or even the zero matrix. <p> Note that the vector (I A y I A I )y is in the null space of A I , so that A I d = A I d svd . This is critical since, with one exception, the proof of finite termination in <ref> [5] </ref> involves only the quantity A I d, not d. Before addressing that exception, we recall a result of Golub and Pereya [4]: Theorem 3.2. Let G be an m fi n matrix, and let GP = Q ^ R be the QR with column pivoting factorization of G. <p> Since QR factorization with column pivoting takes O (mn 2 ) work in the dense case, this would provide a polynomial upper bound on the algorithm. A linear programming problem can be stated as a system of inequalities <ref> [5] </ref>, so this algorithm would be another polynomial method for linear programming. Furthermore, the subproblems generated by the inequality least squares algorithm have condition numbers no worse than that of the original data, since at each step a decomposition is performed on a submatrix of A.
Reference: [6] <author> S. Katznelson, </author> <title> An algorithm for solving nonlinear resistor networks, </title> <journal> Bell System Technical Journal, </journal> <volume> 44 (1965), </volume> <pages> pp. 1605-1620. 29 </pages>
Reference-contexts: It is worthwhile to compare those results with related ones. As early as 1965, Katznelson <ref> [6] </ref> established finite termination of an algorithm for solving piecewise linear systems of equations that arise in circuits.
Reference: [7] <author> C. L. Lawson and R. J. Hanson, </author> <title> Solving Least Squares Problems, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1974. </year>
Reference-contexts: The observations may be inconsistent, and in this case a solution is sought that minimizes the norm of the residuals. More information about linear least squares problems and solution techniques can be found in <ref> [7, 13, 3] </ref>.
Reference: [8] <author> W. Li and J. Swetits, </author> <title> A Newton method for solving convex quadratic programs, </title> <journal> SIAM Journal on Optimization, </journal> <volume> 3 (1993), </volume> <pages> pp. 466-488. </pages>
Reference-contexts: It is worthwhile to compare those results with related ones. As early as 1965, Katznelson [6] established finite termination of an algorithm for solving piecewise linear systems of equations that arise in circuits. More recently Li and Swetits <ref> [8] </ref> have established finite termination for solving systems of the form (x) = Q T [(Ax + b) + + (Cx + d)] = 0 (c.f. the gradient of f (x)).
Reference: [9] <author> O. L. Mangasarian, </author> <title> Nonlinear Programming, </title> <publisher> McGraw-Hill Book Co., </publisher> <address> New York, </address> <year> 1969. </year>
Reference-contexts: System (2) is interpreted componentwise, so we want a T i x b i for all i = 1; : : : ; m. Note that this notation differs from that used by Mangasarian <ref> [9] </ref>, where means at least one component satisfies the inequality strictly, that is, a T k x &lt; b k for some k. Possibly no such x exists.
Reference: [10] <author> J. M. Ortega and W. C. Rheinboldt, </author> <title> Iterative Solution of Nonlinear Equations in Several Variables, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Initially we drop the superscripts k, and consider one step of the algorithm. Since rf is continuous and globally Lipschitz with a Lipschitz constant of kAk 2 , <ref> [10, Theorem 8.3.1, p. 254] </ref> gives f (x + d) f (x) + d T rf (x) + 2 2 As a function of , the right hand side of the above bound has a minimum at ^ = kAk 2 kdk 2 :(11) Substituting this value of in (10) and
Reference: [11] <author> R. Penrose, </author> <title> A generalized inverse for matrices, </title> <journal> Proc. Cambridge Phil. Soc., </journal> <volume> 51 (1955), </volume> <pages> pp. 406-413. </pages>
Reference-contexts: Again the proof is straightforward, using the relation rf (x) = A T (Ax fl b) + . 3 3. Modification of Han's algorithm. Two notations are needed for the state-ment of Han's algorithm. First, G y denotes the pseudo-inverse of the matrix G <ref> [11] </ref>. In practice, all that is needed is the action of G y on a vector f , not the linear operator itself in explicit form.
Reference: [12] <author> G. Stewart, </author> <title> An iterative method for solving linear inequalities, </title> <type> Tech. Rep. </type> <institution> TR-1833, University of Maryland Computer Science Department, </institution> <year> 1987. </year>
Reference-contexts: Futhermore when the system is not consistent, linear programming can identify that case, but does not provide any kind of an "optimal" solution. Other methods fl Work supported by NSF grant CCR-9120105 1 developed for solving linear inequalities include an unusual algorithm by Stewart <ref> [12] </ref>, which defines a function that diverges in a direction that converges to a solution of the inequalities; if no solution exists, the function converges to a unique minimum.
Reference: [13] <author> G. W. Stewart, </author> <title> Introduction to Matrix Computations, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year> <month> 30 </month>
Reference-contexts: The observations may be inconsistent, and in this case a solution is sought that minimizes the norm of the residuals. More information about linear least squares problems and solution techniques can be found in <ref> [7, 13, 3] </ref>.
References-found: 13

