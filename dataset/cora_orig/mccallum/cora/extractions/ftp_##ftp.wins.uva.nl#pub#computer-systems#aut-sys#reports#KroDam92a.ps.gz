URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/KroDam92a.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: krose@fwi.uva.nl  
Title: Adaptive state space quantisation: adding and removing neurons  
Author: Ben J.A. Krose and Joris W.M. van Dam 
Address: Amsterdam Kruislaan 403, NL-1098 SJ Amsterdam, The Netherlands  
Affiliation: Faculty of Mathematics and Computer Science, University of  
Abstract: This paper describes a self-learning control system for a mobile robot. Based on local sensor data, a robot is taught to avoid collisions with obstacles. The only feedback to the control system is a binary-valued external reinforcement signal, which indicates whether or not a collision has occured. A reinforcement learning scheme is used to find a correct mapping from input (sensor) space to output (steering signal) space. An adaptive quantisation scheme is introduced, through which the discrete division of input space is built up from scratch by the system itself. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Barto, A.G., R.S. Sutton en C.W. Anderson, </author> <title> "Neuronlike adaptive elements that can solve difficult learning control problems", </title> <journal> IEEE Trans. on Systems, Man and Cybernetics 13 (1983), </journal> <pages> 834-846 </pages>
Reference-contexts: We use a reinforcement learning strategy which is very similar to the Barto <ref> [1] </ref> approach. The state space is discretised into N regions such that x i = 1 if ~s falls in region i and x i = 0 otherwise. An external reinforcement signal r = 1 is generated upon a collision of the vehicle with an obstacle. <p> It is defined, that a neuron i can be removed, if its nearest neighbour j has a comparable weight vector ( ~w j ~w i ), giving a comparable stochastic function for the selection of steering signals (equ. <ref> [1] </ref>). After removal of neuron i, most of the inputs ~s, that previously fell into region i, will now fall into region j. For these inputs, comparable actions will be selected. 3 Results Experiments were carried out in simulation, using a dynamical model of the vehicle's behaviour.
Reference: [2] <author> Sutton, </author> <title> R.S. "Integrated architectures for learning, planning and reacting based on approximating dynamic programming", </title> <booktitle> Proc. of the Seventh Int. Conf. on Machine Learning, </booktitle> <year> 1990 </year>
Reference-contexts: Comparing evaluations at successive time steps in ^r (t) gives useful means of evaluating the system's performance. Similar to <ref> [2] </ref> the function u = F (~x) is a stochastic function where for a state x j a steering signal u (t) is selected according to a Boltzmann distribution: P (u i j x j ) = P (2) where w ij is weight of the connection between state x j <p> A negative value of ^r (t) punishes the steering signals selected at previous time steps by reducing the probability of its selection P (u i jx j ). A positive value for ^r increases this probability. This procedure for updating the weights is adapted from <ref> [2] </ref>. A problem with reinforcement learning is the a-priori selection of the resolution and region boundaries of the state space. Instead of a fixed quantisation of state space, an adaptive vector quantisation scheme can be used. <p> Note that the origin in state space is representative of a situation, where not a single object is detected by the robot's range sensors. By keeping the values ~w 0 at ~ 0, random behaviour is implemented (see equation <ref> [2] </ref>). The system operates in two distinct modes: normal mode and collision mode. In normal mode, the system attempts to avoid collisions following the behavioural patterns implemented in the neurons present at that time.
Reference: [3] <author> Kohonen, T. </author> <title> "Self-Organization and Associative Memory". </title> <publisher> Springer Verlag, </publisher> <year> 1984. </year>
Reference: [4] <author> Krose, B.J.A. and J.W.M. van Dam, </author> <title> "Learning to avoid collision: a reinforcement learning paradigm for mobile robot navigation" IFAC/IFIP/IMACS International Symposium on Artificial Intellig ence in Real-Time Control, </title> <address> Delft, </address> <year> 1992. </year>
Reference-contexts: These results are superior to the results obtained when a rigid discretisation of state space was defined a-priori, and are comparable to those obtained with a system in which a self-organising clustering algorithm (Kohonen-net) was used to perform the discretisation of state space (see <ref> [4] </ref>). However, in the adaptive state space quantisation scheme introduced in this paper, the average number of neurons was about 30% less than the number of neurons in the Kohonen net.
References-found: 4

