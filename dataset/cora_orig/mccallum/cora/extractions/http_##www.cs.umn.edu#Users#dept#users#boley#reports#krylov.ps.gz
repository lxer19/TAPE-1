URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/krylov.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/
Root-URL: http://www.cs.umn.edu
Title: Krylov Space Methods on State-Space Control Models  
Author: Daniel L. Boley 
Address: Minneapolis, Minnesota  
Affiliation: Department of Computer Science University of Minnesota  
Date: 13:733-758, 1994  
Note: CSSP  
Abstract: We give an overview of various Lanczos/Krylov space methods and how they are being used for solving certain problems in Control Systems Theory based on state-space models. The matrix methods used are based on Krylov sequences and are closely related to modern iterative methods for standard matrix problems such as sets of linear equations and eigenvalue calculations. We show how these methods can be applied to problems in Control Theory such as controllability, observability and model reduction. All the methods are based on the use of state-space models, which may be very sparse and of high dimensionality. For example, we show how one may compute an approximate solution to a Lyapunov equation arising from discrete-time linear dynamic system with a large sparse system matrix by the use of the Arnoldi Algorithm, and so obtain an approximate Grammian matrix. This has applications in model reduction. The close relation between the matrix Lanczos algorithm and the algebraic structure of linear control systems is also explored. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. Adamjan, D. Arov, and M. Krein, </author> <title> Analytic properties of Schmidt pairs for a Hankel operator and the generalized Schur-Takagi problem, </title> <journal> Mat. USSR Sbornik, </journal> <volume> 15 (1971), </volume> <pages> pp. 31-73. </pages>
Reference-contexts: Many methods operating directly on the coefficients in the expansions of the form (37) (or more general MacClaurin expansions) exist, and many are equivalent to the Lanczos Process [11, 13, 32, 31, 54, and refs. therein], and many use related algorithms on related Hankel matrices, though based on different theory <ref> [1, 36, 66] </ref>.
Reference: [2] <author> U. M. Al-Saggaf and G. F. Franklin, </author> <title> An error bound for a discrete reduced order model of a linear multivariable system, </title> <journal> IEEE Trans. Auto. Contr., </journal> <month> AC-32 </month> <year> (1987), </year> <pages> pp. 815-819. </pages>
Reference-contexts: These small gain states can be truncated away without disturbing the system by much <ref> [2, 22, 27] </ref>. The Lanczos-type algorithms can be used to compute approximate balanced realizations for systems of very large dimensionality, whereas most other methods are restricted to systems of only modest dimensionality. <p> Thus the balanced realization can be computed using the same prescription as for the continuous time case [38]. A similar error bound for truncated balanced realizations was obtained in <ref> [2] </ref>. It is difficult to solve the Lyapunov equations for very large sparse systems. The Arnoldi Algorithm may be used to compute approximate solutions to such Lyapunov equations (27) (28) (32) (33), which will satisfy certain Galerkin conditions. We will describe these conditions for the discrete time case.
Reference: [3] <author> A. Antoulas, </author> <title> New results on the algebraic theory of linear systems: The solution of the cover problems, </title> <journal> Lin. Alg & Appl., </journal> <volume> 50 (1983), </volume> <pages> pp. 1-45. </pages>
Reference-contexts: Several authors have also looked at the MIMO realization problem (see e.g. [67] and refs. therein, such as <ref> [3, 14, 46, 70] </ref>. Many of the algorithms involved are recursive algorithms which are intimately related to the Clustered Lanczos Algorithm (see e.g. [11, 32, 54]).
Reference: [4] <author> W. E. </author> <title> Arnoldi, The principle of minimized iterations in the solution of the matrix eigenvalue problem, </title> <journal> Quart. Appl. Math., </journal> <volume> 9 (1951), </volume> <pages> pp. 17-29. </pages>
Reference-contexts: The Lanczos Algorithm [47] is an example of a method that generates bases for Krylov subspaces starting with a given vector. The Arnoldi Algorithm <ref> [4] </ref> can be thought of as a "one-sided" method, which generates one sequence of vectors that span the reachable space. 2.1 Arnoldi Algorithm The first algorithm we will describe is the Arnoldi Algorithm [4, 9, 69], which is a recursive way to generate an orthonormal basis for the Krylov space generated <p> The Arnoldi Algorithm [4] can be thought of as a "one-sided" method, which generates one sequence of vectors that span the reachable space. 2.1 Arnoldi Algorithm The first algorithm we will describe is the Arnoldi Algorithm <ref> [4, 9, 69] </ref>, which is a recursive way to generate an orthonormal basis for the Krylov space generated by a given matrix A and vectors X. It will be seen that it is also a way to reduce a given matrix to block upper Hessenberg form H.
Reference: [5] <author> A. W. Bojanczyk, L. W. Ewerbring, F. T. Luk, and P. Van Dooren, </author> <title> An accurate product SVD algorithm, </title> <booktitle> Signal Processing, 25 (1991), </booktitle> <pages> pp. 189-201. </pages>
Reference-contexts: Effective methods for solving (27), (28) directly for the Cholesky factors without forming W c , W o and for finding the SVD of a matrix product without forming the product were given in [37] and <ref> [39, 5] </ref>, respectively, thus enhancing the numerical accuracy of the results.
Reference: [6] <author> D. L. Boley, </author> <title> Computing the Kalman decomposition, an optimal method, </title> <journal> IEEE Trans. Auto. Contr, </journal> <month> AC-29 </month> <year> (1984), </year> <pages> pp. 51-53. </pages> <note> [Correction in AC-36 (1991), p. 1341]. </note>
Reference-contexts: spaces S c and S o as the orthogonal complements of S c and S o , respectively, and we use orthonormal bases of resulting intersection spaces in the transformation T , then the resulting T is the best conditioned (in the 2-norm) of all transformations yielding the Kalman Decomposition <ref> [6] </ref>. As shown in [9] it is evident that the Arnoldi Algorithm starting with A and B computes an orthonormal basis for S c .
Reference: [7] <author> D. L. Boley, R. P. Brent, G. H. Golub, and F. T. Luk, </author> <title> Error correction via the Lanczos process, </title> <journal> SIAM J. Matrix Anal., </journal> <year> (1992), </year> <pages> pp. 312-332. </pages>
Reference-contexts: The close connection between the modified Non-symmetric Lanczos Algorithm and orthogonal polynomials with respect to indefinite inner products is discussed in <ref> [7, 8, 28] </ref>. Recently, in [10, 54] the close relation was observed independently between the Lanczos Algorithm and the controllability- observability structure of dynamical systems. All the above papers address the Lanczos algorithm with single starting vectors.
Reference: [8] <author> D. L. Boley, S. Elhay, G. H. Golub, and M. H. Gutknecht, </author> <title> Nonsymmetric Lanczos and finding orthogonal polynomials associated with indefinite weights, Numerical Algorithms, </title> <booktitle> 1 (1991), </booktitle> <pages> pp. 21-44. </pages>
Reference-contexts: The close connection between the modified Non-symmetric Lanczos Algorithm and orthogonal polynomials with respect to indefinite inner products is discussed in <ref> [7, 8, 28] </ref>. Recently, in [10, 54] the close relation was observed independently between the Lanczos Algorithm and the controllability- observability structure of dynamical systems. All the above papers address the Lanczos algorithm with single starting vectors. <p> In order to handle the possibility of a breakdown, the method must be modified. Originally a limited recovery method was proposed in [55], but a full recovery method was not proposed until more recently in <ref> [8, 54] </ref>. Numerical implementations have been described in [23, 24]. The modification is based on the idea of generating the individual vectors to satisfy (6), but grouping the generated vectors into clusters, and enforcing the bi-orthogonality condition only between different clusters. <p> The resulting method is the following, taken from <ref> [8] </ref>, but using the modified Gram-Schmidt algorithm as before: Clustered Nonsymmetric Lanczos Algorithm [8]. 0. Start with matrix A, two vectors x 0 ; y 0 1. Initialize clusters X 0 = ( x 0 ) ; Y 0 = ( y 0 ). 2. <p> The resulting method is the following, taken from <ref> [8] </ref>, but using the modified Gram-Schmidt algorithm as before: Clustered Nonsymmetric Lanczos Algorithm [8]. 0. Start with matrix A, two vectors x 0 ; y 0 1. Initialize clusters X 0 = ( x 0 ) ; Y 0 = ( y 0 ). 2. Initialize cluster counter p = 0 and marker k 0 = 0. 3.
Reference: [9] <author> D. L. Boley and G. H. Golub, </author> <title> The Lanczos algorithm and controllability, </title> <journal> Systems & Control Letters, </journal> <volume> 4 (1984), </volume> <pages> pp. </pages> <month> 317-324. </month> <title> [10] , The nonsymmetric Lanczos algorithm and controllability, </title> <journal> Systems & Control Letters, </journal> <volume> 16 (1991), </volume> <pages> pp. 97-105. </pages>
Reference-contexts: The Arnoldi Algorithm [4] can be thought of as a "one-sided" method, which generates one sequence of vectors that span the reachable space. 2.1 Arnoldi Algorithm The first algorithm we will describe is the Arnoldi Algorithm <ref> [4, 9, 69] </ref>, which is a recursive way to generate an orthonormal basis for the Krylov space generated by a given matrix A and vectors X. It will be seen that it is also a way to reduce a given matrix to block upper Hessenberg form H. <p> Unless the starting vector is deficient in certain eigendirections [normally an unusual circumstance], r max = n. The following description is taken from <ref> [9] </ref>, suitably modified to use modified Gram-Schmidt Orthogonalization [30, p218] as suggested in [60]. Block Arnoldi Algorithm [9]. 0. Start with n fi n matrix A and n fi p matrix X. fGenerate orthonormal vectors X and block upper Hessenberg matrix H g 1. <p> Unless the starting vector is deficient in certain eigendirections [normally an unusual circumstance], r max = n. The following description is taken from <ref> [9] </ref>, suitably modified to use modified Gram-Schmidt Orthogonalization [30, p218] as suggested in [60]. Block Arnoldi Algorithm [9]. 0. Start with n fi n matrix A and n fi p matrix X. fGenerate orthonormal vectors X and block upper Hessenberg matrix H g 1. Factor X 0 R = QR factorization of X. fnormalizationg 2. <p> As shown in <ref> [9] </ref> it is evident that the Arnoldi Algorithm starting with A and B computes an orthonormal basis for S c .
Reference: [11] <author> D. L. Boley, T. J. Lee, and F. T. Luk, </author> <title> The Lanczos algorithm and Hankel matrix factorization, </title> <journal> Lin. Alg. & Appl., </journal> <volume> 172 (1992), </volume> <pages> pp. 109-133. </pages>
Reference-contexts: Several authors have also looked at the MIMO realization problem (see e.g. [67] and refs. therein, such as [3, 14, 46, 70]. Many of the algorithms involved are recursive algorithms which are intimately related to the Clustered Lanczos Algorithm (see e.g. <ref> [11, 32, 54] </ref>). We describe two approaches for the MIMO case proposed recently for constructing such a smaller state-space model, both based on the use of a large state-space model. <p> Many methods operating directly on the coefficients in the expansions of the form (37) (or more general MacClaurin expansions) exist, and many are equivalent to the Lanczos Process <ref> [11, 13, 32, 31, 54, and refs. therein] </ref>, and many use related algorithms on related Hankel matrices, though based on different theory [1, 36, 66].
Reference: [12] <author> S. P. Boyd and C. H. Barratt, </author> <title> Linear Controller Design, </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: We have illustrated the use of the Lanczos algorithms on problems of Controllability, Observability, and Model Reduction. The grammians are useful also for other computations in Optimal Control and H 1 Theory (e.g. finding the H 2 , H 1 , and Hankel norms of plants <ref> [12, and refs. therein] </ref>), hence the methods of this paper can be used to compute approximate solutions to those problems.
Reference: [13] <author> C. Brezinski, </author> <title> Pade-Type Approximation and General Orthogonal Polynomials, </title> <publisher> Birkhauser Basel, </publisher> <year> 1980. </year>
Reference-contexts: Many methods operating directly on the coefficients in the expansions of the form (37) (or more general MacClaurin expansions) exist, and many are equivalent to the Lanczos Process <ref> [11, 13, 32, 31, 54, and refs. therein] </ref>, and many use related algorithms on related Hankel matrices, though based on different theory [1, 36, 66].
Reference: [14] <author> A. Bultheel, </author> <title> Recursive algorithms for the matrix Pade problem, </title> <journal> Math. Comp., </journal> <volume> 35 (1980), </volume> <pages> pp. </pages> <month> 875--892. </month>
Reference-contexts: Several authors have also looked at the MIMO realization problem (see e.g. [67] and refs. therein, such as <ref> [3, 14, 46, 70] </ref>. Many of the algorithms involved are recursive algorithms which are intimately related to the Clustered Lanczos Algorithm (see e.g. [11, 32, 54]).
Reference: [15] <author> R. R. Craig Jr. and A. L. Hale, </author> <title> Block-Krylov component synthesis method for structural model reduction, </title> <journal> J. Guid., Contr., & Dyn., </journal> <volume> 11 (1988), </volume> <pages> pp. 562-570. </pages>
Reference-contexts: p + q = s + t, then the system (42) obtained by the oblique projection satisfies (CT )(L T AT ) i (L T B) = CA i B for i = p s 1; ; +q + t + 1: This approach has been exploited very successfully in <ref> [15, 64, 65] </ref> to applications in large flexible space structures. One problem with this method is that the block Nonsymmetric Lanczos Algorithm may break down, if the matrix in step 7 of the algorithm is singular. In practice, this has not been a problem in the applications in [65].
Reference: [16] <author> J. Cullum, W. Kerner, and R. Willoughby, </author> <title> A generalized nonsymmetric Lanczos procedure, </title> <journal> Computer Physics Communications, </journal> <volume> 53 (1989), </volume> <pages> pp. 19-48. </pages>
Reference-contexts: This algorithm is particularly suited for large sparse matrix problems. A block Lanczos analog has been studied and analyzed in [29, 17, 53]. However, until recently, the nonsymmetric Lanczos Algorithm has received much less attention. Some recent computational experience with this algorithm can be found in <ref> [16] </ref>. Besides some numerical stability problems, the method suffered from the possibility of an incurable breakdown from which the only way to "recover" was to restart the whole process from the beginning with different starting vectors [69, p388ff].
Reference: [17] <author> J. Cullum and R. Willoughby, </author> <title> Lanczos Algorithms for Large Symmetric Eigenvalue Computations, vol. I, Theory, </title> <publisher> Birkhauser Boston, </publisher> <year> 1985. </year>
Reference-contexts: The idea was to reduce a general matrix to tridiag-onal form, from which the eigenvalues could be easily determined. For symmetric matrices, the Lanczos Algorithm has been studied extensively <ref> [17, 53] </ref>. In that case, the convergence of the algorithm, when used to compute eigenvalues, has been extensively analyzed in [43, 52, 57, 61], [69, p270ff]. This algorithm is particularly suited for large sparse matrix problems. A block Lanczos analog has been studied and analyzed in [29, 17, 53]. <p> In that case, the convergence of the algorithm, when used to compute eigenvalues, has been extensively analyzed in [43, 52, 57, 61], [69, p270ff]. This algorithm is particularly suited for large sparse matrix problems. A block Lanczos analog has been studied and analyzed in <ref> [29, 17, 53] </ref>. However, until recently, the nonsymmetric Lanczos Algorithm has received much less attention. Some recent computational experience with this algorithm can be found in [16].
Reference: [18] <author> L. S. de Jong, </author> <title> Numerical aspects of recursive realization algorithms, </title> <journal> SIAM J. Contr. & Opt., </journal> <volume> 16 (1978), </volume> <pages> pp. 646-659. </pages>
Reference-contexts: In the SISO case, such a reduced order realization may be found via computation of a transfer function b F (s), expressed as a rational function of polynomials (see e.g. [25, Ch. 15 x10], <ref> [18, 32, 42] </ref>). Several authors have also looked at the MIMO realization problem (see e.g. [67] and refs. therein, such as [3, 14, 46, 70]. Many of the algorithms involved are recursive algorithms which are intimately related to the Clustered Lanczos Algorithm (see e.g. [11, 32, 54]).
Reference: [19] <author> C. A. Desoer, </author> <title> A Second Course in Linear Systems, </title> <publisher> Van Nostrand Reinhold, </publisher> <year> 1970. </year>
Reference-contexts: A classical algebraic characterization of these spaces is given by Theorem 2 <ref> [19] </ref>. S c j K (A; B; 1); S o j nullspace n o - 9 - where S ? denotes the orthogonal complement of the set S.
Reference: [20] <author> P. V. Dooren, </author> <title> Numerical linear algebra techniques for large scale matrix problems in systems and control, </title> <booktitle> in Proc. 31st Conf. on Dec. & Contr, </booktitle> <address> Tuscon AZ, </address> <month> Dec. </month> <year> 1992. </year> <note> session TM6, to appear. [21] , Upcoming numerical linear algebra issues in systems and control theory, TR 1007, </note> <institution> Univ. of Minn. IMA, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Many of the techniques are mentioned in [30]. It is only recently that the "technology" developed for large sparse linear algebra problems has been applied to control problems of large dimensionality <ref> [20] </ref>. Most of the methods are based on the recursive generation of Krylov spaces. These methods are based on the idea of projecting the problem onto ever expanding subspaces generated by the matrices occurring in the problem itself.
Reference: [22] <author> D. F. Enns, </author> <title> Model reduction with balanced realizations: An error bound and a frequency weighted generalization, </title> <booktitle> in Proc. 23rd IEEE Conf. </booktitle> <address> Dec. Contr., </address> <year> 1984, </year> <pages> pp. 127-132. </pages>
Reference-contexts: These small gain states can be truncated away without disturbing the system by much <ref> [2, 22, 27] </ref>. The Lanczos-type algorithms can be used to compute approximate balanced realizations for systems of very large dimensionality, whereas most other methods are restricted to systems of only modest dimensionality. <p> Then it has been shown (e.g. <ref> [22] </ref>) that kG (s) G 1 (s)k 1 2 r+1 where the norm k k 1 on functions analytic everywhere except at some poles in the left half plane is defined by ! where j! varies over the entire imaginary axis.
Reference: [23] <author> R. W. Freund, M. H. Gutknecht, and N. M. Nachtigal, </author> <title> An implementation of the look-ahead Lanczos algorithm for non-hermitian matrices, part I, math. numerical analysis report 90-10, </title> <publisher> M.I.T., </publisher> <year> 1990. </year>
Reference-contexts: More recently, several modifications allowing the Lanczos process to continue after such breakdowns have been proposed in [35, 34, 55], and a numerical implementation has been developed in <ref> [23, 24] </ref>. The close connection between the modified Non-symmetric Lanczos Algorithm and orthogonal polynomials with respect to indefinite inner products is discussed in [7, 8, 28]. Recently, in [10, 54] the close relation was observed independently between the Lanczos Algorithm and the controllability- observability structure of dynamical systems. <p> In order to handle the possibility of a breakdown, the method must be modified. Originally a limited recovery method was proposed in [55], but a full recovery method was not proposed until more recently in [8, 54]. Numerical implementations have been described in <ref> [23, 24] </ref>. The modification is based on the idea of generating the individual vectors to satisfy (6), but grouping the generated vectors into clusters, and enforcing the bi-orthogonality condition only between different clusters. <p> In this case, the last block D p will no longer be exactly zero. The block tridiagonal structure of H; G will remain, but will have to be especially enforced via complete biorthogonalization. A detailed numerical implementation that addresses most of these issues can be found in <ref> [23, 24] </ref>. We note that also for this algorithm, the matrices of generated vectors X k ; Y k satisfy (6) for every k.
Reference: [24] <author> R. W. Freund and N. M. Nachtigal, </author> <title> An implementation of the look-ahead Lanczos algorithm for non-hermitian matrices, part II, math. numerical analysis report 90-11, </title> <publisher> M.I.T., </publisher> <year> 1990. </year>
Reference-contexts: More recently, several modifications allowing the Lanczos process to continue after such breakdowns have been proposed in [35, 34, 55], and a numerical implementation has been developed in <ref> [23, 24] </ref>. The close connection between the modified Non-symmetric Lanczos Algorithm and orthogonal polynomials with respect to indefinite inner products is discussed in [7, 8, 28]. Recently, in [10, 54] the close relation was observed independently between the Lanczos Algorithm and the controllability- observability structure of dynamical systems. <p> In order to handle the possibility of a breakdown, the method must be modified. Originally a limited recovery method was proposed in [55], but a full recovery method was not proposed until more recently in [8, 54]. Numerical implementations have been described in <ref> [23, 24] </ref>. The modification is based on the idea of generating the individual vectors to satisfy (6), but grouping the generated vectors into clusters, and enforcing the bi-orthogonality condition only between different clusters. <p> In this case, the last block D p will no longer be exactly zero. The block tridiagonal structure of H; G will remain, but will have to be especially enforced via complete biorthogonalization. A detailed numerical implementation that addresses most of these issues can be found in <ref> [23, 24] </ref>. We note that also for this algorithm, the matrices of generated vectors X k ; Y k satisfy (6) for every k.
Reference: [25] <author> F. R. Gantmacher, </author> <title> Theory of Matrices, </title> <journal> vol. </journal> <volume> 2, </volume> <publisher> Chelsea, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: By the Corollary in <ref> [25, p. 206] </ref>, the leading r fi r submatrix of this infinite Hankel matrix must be nonsingular. Hence k fl r, and D r , the leading r fi r submatrix of D k fl , must also be nonsingular. <p> In the SISO case, such a reduced order realization may be found via computation of a transfer function b F (s), expressed as a rational function of polynomials (see e.g. <ref> [25, Ch. 15 x10] </ref>, [18, 32, 42]). Several authors have also looked at the MIMO realization problem (see e.g. [67] and refs. therein, such as [3, 14, 46, 70].
Reference: [26] <author> E. G. Gilbert, </author> <title> Controllability and observability in multivariable control systems, </title> <journal> SIAM J. Contr., </journal> <volume> 1 (1963), </volume> <pages> pp. 128-151. </pages>
Reference-contexts: of S c and S o , respectively, as well as their mutual intersections: S co = S c S o ; S co = S c S o ; S co = S c S o ; S co = S c S o : (21) The Kalman Decomposition <ref> [26, 41] </ref> of (18), (19) is obtained by applying a similarity transformation T = ( T co ; T co ; T co ; T co ) where each T ab is a basis for the corresponding S ab .
Reference: [27] <author> K. Glover, </author> <title> All optimal Hankel norm approximations of linear multivariable systems, and their L 1 error bounds, </title> <journal> Intl. J. Contr., </journal> <volume> 39 (1984), </volume> <pages> pp. 1115-1193. </pages>
Reference-contexts: These small gain states can be truncated away without disturbing the system by much <ref> [2, 22, 27] </ref>. The Lanczos-type algorithms can be used to compute approximate balanced realizations for systems of very large dimensionality, whereas most other methods are restricted to systems of only modest dimensionality.
Reference: [28] <author> G. H. Golub and M. H. Gutknecht, </author> <title> Modified moments for indefinite weight functions, </title> <journal> Numer. Math., </journal> <volume> 57 (1990), </volume> <pages> pp. 607-624. </pages>
Reference-contexts: The close connection between the modified Non-symmetric Lanczos Algorithm and orthogonal polynomials with respect to indefinite inner products is discussed in <ref> [7, 8, 28] </ref>. Recently, in [10, 54] the close relation was observed independently between the Lanczos Algorithm and the controllability- observability structure of dynamical systems. All the above papers address the Lanczos algorithm with single starting vectors.
Reference: [29] <author> G. H. Golub and R. Underwood, </author> <title> The block Lanczos method for computing eigenvalues, in Mathematical Software III, </title> <editor> J. Rice, ed., </editor> <publisher> Acad. Press, </publisher> <year> 1977, </year> <pages> pp. 364-377. </pages>
Reference-contexts: In that case, the convergence of the algorithm, when used to compute eigenvalues, has been extensively analyzed in [43, 52, 57, 61], [69, p270ff]. This algorithm is particularly suited for large sparse matrix problems. A block Lanczos analog has been studied and analyzed in <ref> [29, 17, 53] </ref>. However, until recently, the nonsymmetric Lanczos Algorithm has received much less attention. Some recent computational experience with this algorithm can be found in [16].
Reference: [30] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins Univ. Press, </publisher> <editor> 2nd ed., </editor> <year> 1989. </year>
Reference-contexts: Many of the techniques are mentioned in <ref> [30] </ref>. It is only recently that the "technology" developed for large sparse linear algebra problems has been applied to control problems of large dimensionality [20]. Most of the methods are based on the recursive generation of Krylov spaces. <p> Unless the starting vector is deficient in certain eigendirections [normally an unusual circumstance], r max = n. The following description is taken from [9], suitably modified to use modified Gram-Schmidt Orthogonalization <ref> [30, p218] </ref> as suggested in [60]. Block Arnoldi Algorithm [9]. 0. Start with n fi n matrix A and n fi p matrix X. fGenerate orthonormal vectors X and block upper Hessenberg matrix H g 1. Factor X 0 R = QR factorization of X. fnormalizationg 2. <p> The method used can be either based on a Gram-Schmidt process or by applying a series of Givens rotations or Householder transformations <ref> [30] </ref>. With this formulation, the diagonal blocks H kk will be square of progressively smaller (or non-increasing) dimensions as k increases, and the off-diagonal blocks will be rectangular. It is useful to explain the steps individually, since the same process is used in all the Krylov space algorithms. <p> The purpose of steps 4-6 is to orthogonalize the result against all the previous vectors generated. The process used is a modified Gram-Schmidt orthogonalization, which is mathematically equivalent to the ordinary Gram-Schmidt process, but behaves better numerically <ref> [30] </ref>. Numerical experience [60, 58] has also shown that it is useful to repeat the steps 4-6 to ensure that the orthogonality condition is satisfied to the precision of the computer. <p> However, numerical experience has shown that it is often necessary to reorthogonalize the vectors against all the previous vectors when computing in approximate floating point arithmetic (see the extensive literature reported in <ref> [30, Ch. 9] </ref>). <p> Then the individual spaces (21) may be obtained by numerically computing bases for the intersections and orthogonal complements. One method to compute the intersection and orthogonal complements of two spaces S 1 , S 2 given two respective orthonormal bases T 1 , T 2 , is described in <ref> [30, x12.4.4] </ref>. Briefly, one computes the singular value decomposition (SVD) of T T 1 T 2 (see the next section for further discussion on the SVD of a matrix product).
Reference: [31] <author> W. B. Gragg, </author> <title> The Pade table and its relation to certain algorithms in numerical analysis, </title> <journal> SIAM Rev., </journal> <volume> 14 (1972), </volume> <pages> pp. 1-62. - 20 </pages> - 
Reference-contexts: Many methods operating directly on the coefficients in the expansions of the form (37) (or more general MacClaurin expansions) exist, and many are equivalent to the Lanczos Process <ref> [11, 13, 32, 31, 54, and refs. therein] </ref>, and many use related algorithms on related Hankel matrices, though based on different theory [1, 36, 66].
Reference: [32] <author> W. B. Gragg and A. Lindquist, </author> <title> On the partial realization problem, </title> <journal> Lin. Alg. & Appl., </journal> <volume> 50 (1985), </volume> <pages> pp. 277-319. </pages>
Reference-contexts: In the SISO case, such a reduced order realization may be found via computation of a transfer function b F (s), expressed as a rational function of polynomials (see e.g. [25, Ch. 15 x10], <ref> [18, 32, 42] </ref>). Several authors have also looked at the MIMO realization problem (see e.g. [67] and refs. therein, such as [3, 14, 46, 70]. Many of the algorithms involved are recursive algorithms which are intimately related to the Clustered Lanczos Algorithm (see e.g. [11, 32, 54]). <p> Several authors have also looked at the MIMO realization problem (see e.g. [67] and refs. therein, such as [3, 14, 46, 70]. Many of the algorithms involved are recursive algorithms which are intimately related to the Clustered Lanczos Algorithm (see e.g. <ref> [11, 32, 54] </ref>). We describe two approaches for the MIMO case proposed recently for constructing such a smaller state-space model, both based on the use of a large state-space model. <p> Many methods operating directly on the coefficients in the expansions of the form (37) (or more general MacClaurin expansions) exist, and many are equivalent to the Lanczos Process <ref> [11, 13, 32, 31, 54, and refs. therein] </ref>, and many use related algorithms on related Hankel matrices, though based on different theory [1, 36, 66].
Reference: [33] <author> G. Gu, P. P. Khargonekar, and E. B. Lee, </author> <title> Approximation of infinite-dimensional systems, </title> <journal> IEEE Trans. Auto. Contr., </journal> <month> AC-34 </month> <year> (1989), </year> <pages> pp. 610-618. </pages>
Reference-contexts: The systems (18) and (19) are state-space realizations of the transfer function (39). In applications, F (s) may be either the usual case of a finite dimensional system of large order or an infinite dimensional system <ref> [33] </ref>. In either case, the model reduction problem is "solved" by constructing (realizing) a lower order model of the form (18) such that b C b A i b B = F i for i = 0; ; k, for a given k. <p> We describe two approaches for the MIMO case proposed recently for constructing such a smaller state-space model, both based on the use of a large state-space model. The first approach was proposed in <ref> [33] </ref> as a method for finding a finite dimensional approximation to an infinite dimensional system, in the case that the transfer function F (s) is given, but not the expansion (37) nor a state-space realization. In this case, [33] proposed applying the discrete Fourier transform to the sequence F (1); F <p> The first approach was proposed in <ref> [33] </ref> as a method for finding a finite dimensional approximation to an infinite dimensional system, in the case that the transfer function F (s) is given, but not the expansion (37) nor a state-space realization. In this case, [33] proposed applying the discrete Fourier transform to the sequence F (1); F (!); F (! 2 ); ; F (! N1 ) for some N &gt; k, where ! is an N -th root of unity to obtain approximations to the first N terms in the series (37). <p> choose the triple b A; b B; b C to be: b A = B B 0 . . . . . . 1 C A 0 B @ 0 0 C C ; b C = ( F 0 F 1 F k ) We refer the reader to <ref> [33] </ref> for the detailed derivations, including how they apply further model reduction techniques to the resulting finite dimensional model. In particular, they use the balanced realization as described in the previous section.
Reference: [34] <author> M. H. Gutknecht, </author> <title> The unsymmetric Lanczos algorithms and their relations to Pade approximation, continued fractions, the qd algorithm, biconjugate gradient squared algorithms, and fast Hankel solvers. </title> <type> preprint, </type> <year> 1990. </year> <title> [35] , A completed theory of the unsymmetric Lanczos process and related algorithms, part I, </title> <journal> SIAM J. Mat. Anal., </journal> <volume> 13 (1992), </volume> <pages> pp. 594-639. </pages>
Reference-contexts: More recently, several modifications allowing the Lanczos process to continue after such breakdowns have been proposed in <ref> [35, 34, 55] </ref>, and a numerical implementation has been developed in [23, 24]. The close connection between the modified Non-symmetric Lanczos Algorithm and orthogonal polynomials with respect to indefinite inner products is discussed in [7, 8, 28].
Reference: [36] <author> M. H. Gutknecht and L. N. Trefethen, </author> <title> Real polynomial Chebyshev approximation by the Caratheodory-Fejer method, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 19 (1982), </volume> <pages> pp. 358-371. </pages>
Reference-contexts: Many methods operating directly on the coefficients in the expansions of the form (37) (or more general MacClaurin expansions) exist, and many are equivalent to the Lanczos Process [11, 13, 32, 31, 54, and refs. therein], and many use related algorithms on related Hankel matrices, though based on different theory <ref> [1, 36, 66] </ref>.
Reference: [37] <author> S. J. Hammarling, </author> <title> Numerical solution of stable, non-negative definite Lyapunov equations, </title> <journal> IMA J. Numer. Anal., </journal> <volume> 2 (1982), </volume> <pages> pp. </pages> <month> 303-323. </month> <title> [38] , Numerical solution of the discrete-time convergent, non-negative definite Lyapunov equation, </title> <journal> Systems & Control Letters, </journal> <volume> 17 (1991), </volume> <pages> pp. 137-139. </pages>
Reference-contexts: Effective methods for solving (27), (28) directly for the Cholesky factors without forming W c , W o and for finding the SVD of a matrix product without forming the product were given in <ref> [37] </ref> and [39, 5], respectively, thus enhancing the numerical accuracy of the results.
Reference: [39] <author> M. T. Heath, A. J. Laub, C. C. Paige, and R. C. Ward, </author> <title> Computing the SVD of a product of two matrices, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 7 (1986), </volume> <pages> pp. 1147-1159. </pages>
Reference-contexts: Effective methods for solving (27), (28) directly for the Cholesky factors without forming W c , W o and for finding the SVD of a matrix product without forming the product were given in [37] and <ref> [39, 5] </ref>, respectively, thus enhancing the numerical accuracy of the results.
Reference: [40] <author> J. Hickin and N. K. Sinha, </author> <title> Model reduction for linear multivariable systems, </title> <journal> IEEE Trans. Auto. Contr., </journal> <month> AC-25 </month> <year> (1980), </year> <pages> pp. 1121-1127. </pages>
Reference-contexts: The parameters F i are called the high frequency moments [68] or Markov Parameters <ref> [40, 62] </ref>.
Reference: [41] <author> R. E. </author> <title> Kalman, Mathematical description of linear systems, </title> <journal> SIAM J. Contr., </journal> <volume> 1 (1963), </volume> <pages> pp. </pages> <month> 152-192. </month> <title> [42] , On partial realizations, transfer functions and canonical forms, </title> <booktitle> Acta Polytech. Scand., MA31 (1979), </booktitle> <pages> pp. 9-32. </pages> <address> Helsinki. </address>
Reference-contexts: of S c and S o , respectively, as well as their mutual intersections: S co = S c S o ; S co = S c S o ; S co = S c S o ; S co = S c S o : (21) The Kalman Decomposition <ref> [26, 41] </ref> of (18), (19) is obtained by applying a similarity transformation T = ( T co ; T co ; T co ; T co ) where each T ab is a basis for the corresponding S ab .
Reference: [43] <author> S. Kaniel, </author> <title> Estimates for some computational techniques in linear algebra, </title> <journal> Math. Comp., </journal> <volume> 20 (1966), </volume> <pages> pp. 369-378. </pages>
Reference-contexts: The idea was to reduce a general matrix to tridiag-onal form, from which the eigenvalues could be easily determined. For symmetric matrices, the Lanczos Algorithm has been studied extensively [17, 53]. In that case, the convergence of the algorithm, when used to compute eigenvalues, has been extensively analyzed in <ref> [43, 52, 57, 61] </ref>, [69, p270ff]. This algorithm is particularly suited for large sparse matrix problems. A block Lanczos analog has been studied and analyzed in [29, 17, 53]. However, until recently, the nonsymmetric Lanczos Algorithm has received much less attention.
Reference: [44] <author> H. M. Kim and R. R. Craig Jr., </author> <title> Structural dynamics analysis using an unsymmetric block Lanczos algorithm, International Journal for Numerical Methods in Engineering, </title> <booktitle> 26 (1988), </booktitle> <pages> pp. </pages> <month> 2305-2318. </month> <title> [45] , Computational enhancement of an unsymmetric block Lanczos algorithm, International Journal for Numerical Methods in Engineering, </title> <booktitle> 30 (1990), </booktitle> <pages> pp. 1083-1089. </pages>
Reference-contexts: Recently, in [10, 54] the close relation was observed independently between the Lanczos Algorithm and the controllability- observability structure of dynamical systems. All the above papers address the Lanczos algorithm with single starting vectors. Recently, a block nonsymmetric Lanczos algorithm was proposed in <ref> [44, 45] </ref>, which is capable of starting with several starting vectors, analogous to the block Arnoldi algorithm, but so far the breakdown situation has not been addressed for the block case. <p> The following is the form of the algorithm taken from <ref> [44, 45] </ref>, but using the modified Gram-Schmidt bi-orthogonalization process. Block Nonsymmetric Lanczos Algorithm [44, 45]. 0. Start with matrix A and two sets of vectors X; Y such that Y T X is nonsingular 1. <p> The following is the form of the algorithm taken from <ref> [44, 45] </ref>, but using the modified Gram-Schmidt bi-orthogonalization process. Block Nonsymmetric Lanczos Algorithm [44, 45]. 0. Start with matrix A and two sets of vectors X; Y such that Y T X is nonsingular 1. Compute factorizations X 0 B 0 = X and Y 0 0 = Y . 2. <p> To derive what they are, we write I = D k = Y T k (Y k ) T X k B 1 Among all the possible solutions to this equation, <ref> [44] </ref> has proposed using the particular choice T k B k = LU Decomposition of (Y (1) (1) and they have further suggested the use of pivoting to enhance numerical stability [45]. <p> In particular, they use the balanced realization as described in the previous section. The second approach that we describe was proposed by <ref> [44, 45] </ref>, and is based on the theory in [68].
Reference: [46] <author> S. Y. Kung, </author> <title> A new identification and model reduction algorithm via singular value decompositions, </title> <booktitle> in Proc. 12-th Asilomar Conf. Circ., Syst. Comp., </booktitle> <month> November </month> <year> 1984, </year> <pages> pp. 705-714. </pages>
Reference-contexts: Several authors have also looked at the MIMO realization problem (see e.g. [67] and refs. therein, such as <ref> [3, 14, 46, 70] </ref>. Many of the algorithms involved are recursive algorithms which are intimately related to the Clustered Lanczos Algorithm (see e.g. [11, 32, 54]).
Reference: [47] <author> C. </author> <title> Lanczos, An iteration method for the solution of the eigenvalue problem linear differential and integral operators, </title> <institution> J. Res. Natl. Bur. Stand., </institution> <month> 45 </month> <year> (1950), </year> <pages> pp. 255-282. </pages>
Reference-contexts: This latter matrix will be in reduced form, such as tridiagonal, and represents the projection of the original matrix operator A onto the Krylov Space, typically of smaller dimension. The Lanczos Algorithm was originally proposed by Lanczos <ref> [47] </ref> as a method for the computation of eigenvalues of symmetric and nonsymmetric matrices. The idea was to reduce a general matrix to tridiag-onal form, from which the eigenvalues could be easily determined. For symmetric matrices, the Lanczos Algorithm has been studied extensively [17, 53]. <p> Recently, a block nonsymmetric Lanczos algorithm was proposed in [44, 45], which is capable of starting with several starting vectors, analogous to the block Arnoldi algorithm, but so far the breakdown situation has not been addressed for the block case. The Lanczos Algorithm <ref> [47] </ref> is an example of a method that generates bases for Krylov subspaces starting with a given vector. <p> The resulting algorithm is the Lanczos Algorithm, and indeed it is historically the first recursive Krylov sequence method proposed <ref> [47] </ref>. 2.2 Nonsymmetric Lanczos Algorithm If one starts with the symmetric Lanczos Algorithm and relaxes the condition that the generated vectors be orthonormal, but maintains the condition that the generated matrix of coefficients be tridiagonal, one obtains the nonsymmetric Lanczos algorithm, capable of reducing most any matrix to tridiagonal form.
Reference: [48] <author> A. J. Laub, </author> <title> On computing "balancing" transformations, </title> <booktitle> in Proc. 1980 JACC, </booktitle> <address> San Francisco, </address> <year> 1980, </year> <pages> pp. </pages> <month> FA8-E. </month> <title> [49] , Numerical linear algebra aspects of control design computations, </title> <journal> IEEE Trans. Auto. Contr., </journal> <month> AC-30 </month> <year> (1985), </year> <pages> pp. 97-108. </pages>
Reference-contexts: 1 B; b C = CT (30) then the grammians will be transformed by contragredient (or congruence) transformations into c W c = T 1 W c T T ; c W o = T T W o T: (31) The balanced realization may be computed by the following prescription <ref> [48, 50] </ref>. Define L c L T c , L o L T the Cholesky factorizations of W c , W o , respectively.
Reference: [50] <author> A. J. Laub, M. T. Heath, C. C. Paige, and R. C. Ward, </author> <title> Computation of system balancing transformations and other applications of simultaneous diagonalization algorithms, </title> <journal> IEEE Trans. Auto. Contr., </journal> <month> AC-32 </month> <year> (1987), </year> <pages> pp. 115-122. </pages>
Reference-contexts: 1 B; b C = CT (30) then the grammians will be transformed by contragredient (or congruence) transformations into c W c = T 1 W c T T ; c W o = T T W o T: (31) The balanced realization may be computed by the following prescription <ref> [48, 50] </ref>. Define L c L T c , L o L T the Cholesky factorizations of W c , W o , respectively. <p> Then the particular choice T = L c V 1=2 will reduce the grammians to the same diagonal form c W c = c W o = <ref> [50] </ref>. The resulting system (29) with this particular choice for T is the balanced realization for the system (18) [51].
Reference: [51] <author> B. C. Moore, </author> <title> Principal component analysis in linear systems: Controllability, observability, and model reduction, </title> <journal> IEEE Trans. Auto. Contr., </journal> <volume> AC-26 (1981), </volume> <pages> pp. 17-31. </pages>
Reference-contexts: of coefficients from the Clustered nonsymmetric Lanczos algorithm, D 0 is the leading diagonal block of D r , and e 1 = ( 1 0 0 ) T denotes the initial coordinate unit vector of appropriate dimensions. - 11 - 4 Model Reduction via Balanced Realization The balanced realization <ref> [51] </ref> is a method for balancing the gains between inputs and states with those between states and outputs, and isolating the states with small gains. These small gain states can be truncated away without disturbing the system by much [2, 22, 27]. <p> Then the particular choice T = L c V 1=2 will reduce the grammians to the same diagonal form c W c = c W o = [50]. The resulting system (29) with this particular choice for T is the balanced realization for the system (18) <ref> [51] </ref>. Effective methods for solving (27), (28) directly for the Cholesky factors without forming W c , W o and for finding the SVD of a matrix product without forming the product were given in [37] and [39, 5], respectively, thus enhancing the numerical accuracy of the results.
Reference: [52] <author> C. C. Paige, </author> <title> The Computation of Eigenvalues and Eigenvectors of Very Large Sparse Matrices, </title> <type> PhD thesis, </type> <institution> Univ. of London, </institution> <year> 1971. </year>
Reference-contexts: The idea was to reduce a general matrix to tridiag-onal form, from which the eigenvalues could be easily determined. For symmetric matrices, the Lanczos Algorithm has been studied extensively [17, 53]. In that case, the convergence of the algorithm, when used to compute eigenvalues, has been extensively analyzed in <ref> [43, 52, 57, 61] </ref>, [69, p270ff]. This algorithm is particularly suited for large sparse matrix problems. A block Lanczos analog has been studied and analyzed in [29, 17, 53]. However, until recently, the nonsymmetric Lanczos Algorithm has received much less attention.
Reference: [53] <author> B. N. Parlett, </author> <title> The Symmetric Eigenvalue Problem, </title> <publisher> Prentice Hall, </publisher> <year> 1980. </year> <title> [54] , Reduction to tridiagonal form and minimal realizations, </title> <journal> SIAM J. Matr. Anal., </journal> <volume> 13 (1992), </volume> <pages> pp. 567-593. </pages>
Reference-contexts: The idea was to reduce a general matrix to tridiag-onal form, from which the eigenvalues could be easily determined. For symmetric matrices, the Lanczos Algorithm has been studied extensively <ref> [17, 53] </ref>. In that case, the convergence of the algorithm, when used to compute eigenvalues, has been extensively analyzed in [43, 52, 57, 61], [69, p270ff]. This algorithm is particularly suited for large sparse matrix problems. A block Lanczos analog has been studied and analyzed in [29, 17, 53]. <p> In that case, the convergence of the algorithm, when used to compute eigenvalues, has been extensively analyzed in [43, 52, 57, 61], [69, p270ff]. This algorithm is particularly suited for large sparse matrix problems. A block Lanczos analog has been studied and analyzed in <ref> [29, 17, 53] </ref>. However, until recently, the nonsymmetric Lanczos Algorithm has received much less attention. Some recent computational experience with this algorithm can be found in [16].
Reference: [55] <author> B. N. Parlett, D. R. Taylor, and Z. A. Liu, </author> <title> A look-ahead Lanczos algorithm for unsymmetric matrices, </title> <journal> Math. Comp., </journal> <volume> 44 (1985), </volume> <pages> pp. 105-124. </pages>
Reference-contexts: More recently, several modifications allowing the Lanczos process to continue after such breakdowns have been proposed in <ref> [35, 34, 55] </ref>, and a numerical implementation has been developed in [23, 24]. The close connection between the modified Non-symmetric Lanczos Algorithm and orthogonal polynomials with respect to indefinite inner products is discussed in [7, 8, 28]. <p> Thus with either choice, the computation of these coefficients in step 4 can be cut in half. In order to handle the possibility of a breakdown, the method must be modified. Originally a limited recovery method was proposed in <ref> [55] </ref>, but a full recovery method was not proposed until more recently in [8, 54]. Numerical implementations have been described in [23, 24].
Reference: [56] <author> L. Reichel and D. Y. Hu, </author> <title> Krylov subspace methods for the Sylvester equation, </title> <journal> Lin. Alg. & Appl., </journal> <volume> 172 (1992), </volume> <pages> pp. 283-314. </pages>
Reference-contexts: The continuous time case is analogous and is treated in [59]. In addition, one can obtain an error bound for the discrete time solution. Similar techniques have been proposed for the more general Sylvester equation AX XB = C in <ref> [56] </ref>, in which the approximate solutions can be recursively generated as the Arnoldi process advances. We show how the Arnoldi Algorithm yields an approximate solution to (32), then we show that it satisfies the Galerkin property and an error bound. <p> recursive nature of the Arnoldi Algorithm to use the approximate solution obtained after each step of the Arnoldi Algorithm to generate subsequent approximate solutions rapidly, but for the sake of brevity and clarity we do not describe that process here; this is the basis of the methods in, for example, <ref> [56, 60] </ref>. This solution satisfies the following theorem, proved in [59] for the continuous time case: Theorem 4. Define the inner product on n fi n matrices as hX; Y i j tr (X T Y ).
Reference: [57] <author> Y. Saad, </author> <title> On the rates of convergence of the Lanczos and the block Lanczos methods, </title> <journal> SIAM J. Num. Anal., </journal> <volume> 17 (1980), </volume> <pages> pp. </pages> <month> 687-706. </month> <title> [58] , Krylov subspace methods for solving large unsymmetric linear systems, </title> <journal> Math. Comp., </journal> <volume> 37 (1981), </volume> <pages> pp. </pages> <month> 105-126. </month> <title> [59] , Numerical solution of large Lyapunov equations, in Signal Processing, Scattering and Operator Theory, and Numerical Methods. </title> <booktitle> Proc. Intl. Symp. MTNS-89, </booktitle> <volume> vol. 3, </volume> <publisher> Birkhauser, </publisher> <year> 1990, </year> <pages> pp. 503-511. </pages>
Reference-contexts: The idea was to reduce a general matrix to tridiag-onal form, from which the eigenvalues could be easily determined. For symmetric matrices, the Lanczos Algorithm has been studied extensively [17, 53]. In that case, the convergence of the algorithm, when used to compute eigenvalues, has been extensively analyzed in <ref> [43, 52, 57, 61] </ref>, [69, p270ff]. This algorithm is particularly suited for large sparse matrix problems. A block Lanczos analog has been studied and analyzed in [29, 17, 53]. However, until recently, the nonsymmetric Lanczos Algorithm has received much less attention.
Reference: [60] <author> Y. Saad and M. H. Schultz, </author> <title> GMRES: A generalized minimal residual algorithm for solving un-symmetric linear systems, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7 (1986), </volume> <pages> pp. 856-869. </pages>
Reference-contexts: Unless the starting vector is deficient in certain eigendirections [normally an unusual circumstance], r max = n. The following description is taken from [9], suitably modified to use modified Gram-Schmidt Orthogonalization [30, p218] as suggested in <ref> [60] </ref>. Block Arnoldi Algorithm [9]. 0. Start with n fi n matrix A and n fi p matrix X. fGenerate orthonormal vectors X and block upper Hessenberg matrix H g 1. Factor X 0 R = QR factorization of X. fnormalizationg 2. <p> The purpose of steps 4-6 is to orthogonalize the result against all the previous vectors generated. The process used is a modified Gram-Schmidt orthogonalization, which is mathematically equivalent to the ordinary Gram-Schmidt process, but behaves better numerically [30]. Numerical experience <ref> [60, 58] </ref> has also shown that it is useful to repeat the steps 4-6 to ensure that the orthogonality condition is satisfied to the precision of the computer. <p> recursive nature of the Arnoldi Algorithm to use the approximate solution obtained after each step of the Arnoldi Algorithm to generate subsequent approximate solutions rapidly, but for the sake of brevity and clarity we do not describe that process here; this is the basis of the methods in, for example, <ref> [56, 60] </ref>. This solution satisfies the following theorem, proved in [59] for the continuous time case: Theorem 4. Define the inner product on n fi n matrices as hX; Y i j tr (X T Y ).
Reference: [61] <author> D. Scott, </author> <title> Analysis of the symmetric Lanczos process, </title> <institution> electronic res. lab. report UCB/ERL M78/40, Univ. of Calif., Berkeley, </institution> <year> 1978. </year>
Reference-contexts: The idea was to reduce a general matrix to tridiag-onal form, from which the eigenvalues could be easily determined. For symmetric matrices, the Lanczos Algorithm has been studied extensively [17, 53]. In that case, the convergence of the algorithm, when used to compute eigenvalues, has been extensively analyzed in <ref> [43, 52, 57, 61] </ref>, [69, p270ff]. This algorithm is particularly suited for large sparse matrix problems. A block Lanczos analog has been studied and analyzed in [29, 17, 53]. However, until recently, the nonsymmetric Lanczos Algorithm has received much less attention.
Reference: [62] <author> L. S. Shieh and Y. J. Wei, </author> <title> A mixed method for multivariable system reduction, </title> <journal> IEEE Trans. Auto. Contr., </journal> <month> AC-20 </month> <year> (1975), </year> <pages> pp. 429-432. </pages>
Reference-contexts: The parameters F i are called the high frequency moments [68] or Markov Parameters <ref> [40, 62] </ref>.
Reference: [63] <author> G. W. Stewart and J. G. Sun, </author> <title> Matrix Perturbation Theory, </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: Proof: One can find a consistent matrix norm such that kAk fl ffi <ref> [63] </ref>. Since all norms on a finite dimensional space are equivalent, one can find a constant such that kM k 2 kM k fl for all matrices M .
Reference: [64] <author> T. J. Su, </author> <title> A Decentralized Linear Quadratic Control Design Method for Flexible Structures, </title> <type> PhD thesis, </type> <institution> Univ. of Texas, Austin, </institution> <year> 1989. </year>
Reference-contexts: In order to create models matching both the high frequency moments and the low frequency moments, <ref> [64] </ref> propose combining the vectors generated from the Block Lanczos method using A, B, C T with those from the Lanczos method using A 1 , A 1 B, C T . <p> p + q = s + t, then the system (42) obtained by the oblique projection satisfies (CT )(L T AT ) i (L T B) = CA i B for i = p s 1; ; +q + t + 1: This approach has been exploited very successfully in <ref> [15, 64, 65] </ref> to applications in large flexible space structures. One problem with this method is that the block Nonsymmetric Lanczos Algorithm may break down, if the matrix in step 7 of the algorithm is singular. In practice, this has not been a problem in the applications in [65].
Reference: [65] <author> T. J. Su and R. R. Craig Jr., </author> <title> Model reduction and control of flexible structures using Krylov vectors, </title> <journal> J. Guid., Contr., & Dyn., </journal> <volume> 14 (1991), </volume> <pages> pp. 260-267. </pages>
Reference-contexts: p + q = s + t, then the system (42) obtained by the oblique projection satisfies (CT )(L T AT ) i (L T B) = CA i B for i = p s 1; ; +q + t + 1: This approach has been exploited very successfully in <ref> [15, 64, 65] </ref> to applications in large flexible space structures. One problem with this method is that the block Nonsymmetric Lanczos Algorithm may break down, if the matrix in step 7 of the algorithm is singular. In practice, this has not been a problem in the applications in [65]. <p> One problem with this method is that the block Nonsymmetric Lanczos Algorithm may break down, if the matrix in step 7 of the algorithm is singular. In practice, this has not been a problem in the applications in <ref> [65] </ref>. We note that in this use of the algorithm, we do not compute the spaces K (A; B; 1) or K (A T ; C T ; 1). Rather, we choose a k max and run the algorithm only that many steps.
Reference: [66] <author> L. N. Trefethen, </author> <title> Rational Chebyshev approximation on the unit disk, </title> <journal> Numer. Math., </journal> <volume> 37 (1981), </volume> <pages> pp. 297-320. </pages>
Reference-contexts: Many methods operating directly on the coefficients in the expansions of the form (37) (or more general MacClaurin expansions) exist, and many are equivalent to the Lanczos Process [11, 13, 32, 31, 54, and refs. therein], and many use related algorithms on related Hankel matrices, though based on different theory <ref> [1, 36, 66] </ref>.
Reference: [67] <author> P. Van Dooren, </author> <title> Numerical aspects of system and control problems, </title> <journal> Journal A, </journal> <volume> 30 (1987), </volume> <pages> pp. 25-32. </pages> <address> Brussels. </address> - <month> 22 </month> - 
Reference-contexts: In the SISO case, such a reduced order realization may be found via computation of a transfer function b F (s), expressed as a rational function of polynomials (see e.g. [25, Ch. 15 x10], [18, 32, 42]). Several authors have also looked at the MIMO realization problem (see e.g. <ref> [67] </ref> and refs. therein, such as [3, 14, 46, 70]. Many of the algorithms involved are recursive algorithms which are intimately related to the Clustered Lanczos Algorithm (see e.g. [11, 32, 54]).
Reference: [68] <author> C. D. Villemagne and R. E. Skelton, </author> <title> Model reduction using a projection formulation, </title> <journal> Intl. J. Contr., </journal> <volume> 46 (1987), </volume> <pages> pp. 2141-2169. </pages>
Reference-contexts: The parameters F i are called the high frequency moments <ref> [68] </ref> or Markov Parameters [40, 62]. <p> In particular, they use the balanced realization as described in the previous section. The second approach that we describe was proposed by [44, 45], and is based on the theory in <ref> [68] </ref>. <p> Nonsymmetric Lanczos Algorithm started with A, B, C using scaling (17), and given the reduced order model defined by (40) (41), then b C b A i b B = CA i B for i = 0; ; 2 (k 1) To prove this, we use a lemma: Lemma 9 <ref> [68] </ref>. b A i b B = Y T k A i B and b (A T ) i b C T = X T Proof of Lemma: The properties (15), (16) are identical to (4), so the proof is analogous to that for Lemma 6. <p> In order to obtain a model that also exhibits similar steady state behavior, it is necessary to compute a model in which the low frequency moments b C b A i1 b B, i = 0; ; p, match those for the original system <ref> [68] </ref>. <p> projection _ z = L T AT z + L T Bu; y = CT z; (42) where the columns of matrices L and T include bases for the spaces K (A T ; A T C T ; p) and K (A 1 ; A 1 B; p), respectively <ref> [68] </ref>, where M T j (M T ) 1 j (M 1 ) T . <p> This is based on the following theorem, whose proof is analogous to that of Theorem 8, which is itself a special case of the following. Theorem 10 <ref> [68] </ref>.
Reference: [69] <author> J. H. Wilkinson, </author> <title> The Algebraic Eigenvalue Problem, </title> <publisher> Clarendon Press, </publisher> <year> 1965. </year>
Reference-contexts: For symmetric matrices, the Lanczos Algorithm has been studied extensively [17, 53]. In that case, the convergence of the algorithm, when used to compute eigenvalues, has been extensively analyzed in [43, 52, 57, 61], <ref> [69, p270ff] </ref>. This algorithm is particularly suited for large sparse matrix problems. A block Lanczos analog has been studied and analyzed in [29, 17, 53]. However, until recently, the nonsymmetric Lanczos Algorithm has received much less attention. Some recent computational experience with this algorithm can be found in [16]. <p> Some recent computational experience with this algorithm can be found in [16]. Besides some numerical stability problems, the method suffered from the possibility of an incurable breakdown from which the only way to "recover" was to restart the whole process from the beginning with different starting vectors <ref> [69, p388ff] </ref>. More recently, several modifications allowing the Lanczos process to continue after such breakdowns have been proposed in [35, 34, 55], and a numerical implementation has been developed in [23, 24]. <p> The Arnoldi Algorithm [4] can be thought of as a "one-sided" method, which generates one sequence of vectors that span the reachable space. 2.1 Arnoldi Algorithm The first algorithm we will describe is the Arnoldi Algorithm <ref> [4, 9, 69] </ref>, which is a recursive way to generate an orthonormal basis for the Krylov space generated by a given matrix A and vectors X. It will be seen that it is also a way to reduce a given matrix to block upper Hessenberg form H. <p> Identity (8) implies that the resulting matrices H; G will actually take on the above tridiagonal form. The resulting algorithm is then, as taken from <ref> [69, pp388f] </ref>, but using a modified Gram-Schmidt bi-orthogonalization process analogous the ordinary process used above in the Arnoldi Algorithm. Simple Nonsymmetric Lanczos Algorithm [69]. 0. Start with matrix A and two vectors x 0 ; y 0 such that y T 0 x 0 = d 0 6= 0 1. <p> Identity (8) implies that the resulting matrices H; G will actually take on the above tridiagonal form. The resulting algorithm is then, as taken from [69, pp388f], but using a modified Gram-Schmidt bi-orthogonalization process analogous the ordinary process used above in the Arnoldi Algorithm. Simple Nonsymmetric Lanczos Algorithm <ref> [69] </ref>. 0. Start with matrix A and two vectors x 0 ; y 0 such that y T 0 x 0 = d 0 6= 0 1. For k = 1; 2; , while x k 6= 0 and/or y k 6= 0 2.
Reference: [70] <author> H. P. Zeiger and A. J. McEwen, </author> <title> Approximate linear realizations of given dimensions via Ho's algorithm, </title> <journal> IEEE Trans. Auto. Contr., </journal> <note> AC-19 (1974), </note> <editor> p. </editor> <volume> 153. </volume> - <pages> 23 </pages> - 
Reference-contexts: Several authors have also looked at the MIMO realization problem (see e.g. [67] and refs. therein, such as <ref> [3, 14, 46, 70] </ref>. Many of the algorithms involved are recursive algorithms which are intimately related to the Clustered Lanczos Algorithm (see e.g. [11, 32, 54]).
References-found: 60

