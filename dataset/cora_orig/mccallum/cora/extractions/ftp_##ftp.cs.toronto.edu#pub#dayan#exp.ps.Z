URL: ftp://ftp.cs.toronto.edu/pub/dayan/exp.ps.Z
Refering-URL: http://www.ai.mit.edu/people/cohn/SAL95/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Exploration Bonuses and Dual Control  
Author: Peter Dayan Terrence J Sejnowski 
Keyword: Running head: Exploration bonuses Key words: reinforcement learning, dynamic programming, exploration bonus certainty equivalence  
Note: It is applied to two-dimensional mazes with moveable barriers.  
Address: E25-201, MIT PO Box 85800 Cambridge, MA 02139 San Diego, CA 92186-5800  La Jolla, CA 92093  
Affiliation: Center for Biological and Computational Learning Howard Hughes Medical Institute Department of Brain and Cognitive Sciences The Salk Institute  Department of Biology University of California at San Diego  
Abstract: Finding the Bayesian balance between exploration and exploitation in adaptive optimal control is in general intractable. This paper shows how to compute suboptimal estimates based on a certainty equivalence approximation arising from a form of dual control. This systematizes and extends existing uses of exploration bonuses in reinforcement learning (Sutton, 1990). The approach has two components: a statistical model of uncertainty in the world and a way of turning this into exploratory behaviour. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, AG, Sutton, RS & Watkins, </author> <month> CJCH </month> <year> (1989). </year> <title> Learning and sequential decision making. </title> <editor> In M Gabriel & J Moore, editors, </editor> <booktitle> Learning and Computational Neuroscience: Foundations of Adaptive Networks. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press, Bradford Books. </publisher>
Reference: <author> Bertsekas, D & Shreve, </author> <title> SE (1978). Stochastic Optimal Control: The Discrete Time Case. </title> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference: <author> Cohn, </author> <title> DA (1994). Neural network exploration using optimal experiment design. </title> <editor> In JD Cowan, G Tesauro & J Allspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, 6. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 679-686. </pages>
Reference-contexts: It is this uncertainty that should drive exploration. Optimal exploration is just optimal experimental design (Fedorov, 1972) applied to sequential decision problems <ref> (Cohn, 1994) </ref>. The agent's initial uncertainty about the transition or reward structure of the world should drive the initial experimentation, and, if the world can change stochastically over time, then this further source of uncertainty should drive continuing exploration.
Reference: <author> Dersin, PL, Athans, M & Kendrick, </author> <title> DA (1981). Some properties of the dual adaptive stochastic control algorithm. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 26, </volume> <pages> 1001-1008. </pages>
Reference: <author> Dreyfus, </author> <title> SE (1965). Dynamic Programming and the Calculus of Variations. </title> <address> New York, NY: </address> <publisher> Academic Press. </publisher> <month> Fedorov, </month> <title> V (1972). Theory of Optimal Experiments. </title> <address> New York: </address> <publisher> Academic Press. </publisher> <address> 18 Fe'ldbaum, AA (1965). </address> <booktitle> Optimal Control Systems. </booktitle> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference-contexts: This is only a practical rather than a theoretical difference. The current algorithm was inspired by a technique called open loop feedback (or receding horizon) control <ref> (Dreyfus, 1965) </ref>, which uses current uncertainty to determine current control. Completely open loop or feedforward control in this context would calculate optimally all the moves the agent would make from its current position to the goal, allowing for the uncertainty about the efficacies.
Reference: <author> Howard, </author> <title> RA (1960). Dynamic Programming and Markov Processes. </title> <address> New York, NY: </address> <publisher> Technology Press & Wiley. </publisher>
Reference: <author> Kumar, </author> <title> PR (1985). A survey of some results in stochastic adaptive control. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 23, </volume> <pages> 329-380. </pages> <month> Lovejoy, </month> <title> WS (1991). A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28, </volume> <pages> 47-66. </pages>
Reference: <author> Meier, L, </author> <month> IIIrd </month> <year> (1965). </year> <title> Combined optimal control and estimation. </title> <booktitle> Proceedings of the Third Annual Allerton Conference on Circuit and System Theory. </booktitle>
Reference: <author> Monahan, </author> <title> GE (1982). A survey of partially observable Markov decision processes: Theory, models and algorithms. </title> <journal> Management Science, </journal> <volume> 28, </volume> <pages> 1-16. </pages> <note> Moore, </note> <author> AW & Atkeson, </author> <title> CG (1993). Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 103-130. </pages> <editor> Moore, </editor> <title> AW & Atkeson CG (1994). The Parti-Game algorithm. </title> <editor> In G Tesauro, JD Cowan & J Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, 6. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Performing DP to solve a POMDP is highly computationally expensive <ref> (Monahan, 1982) </ref>, and we therefore make a form of certainty equivalence approximation.
Reference: <author> Peng, J & Williams, </author> <title> RJ (1992). Efficient search control in DYNA. </title> <institution> College of Computer Science, Northeastern University. Rishel, </institution> <month> RW </month> <year> (1970). </year> <title> Necessary and sufficient dynamic programming conditions for continuous time stochastic optimal control. </title> <journal> SIAM Journal of Control, </journal> <volume> 8, </volume> <pages> 559-571. </pages>
Reference: <author> Sato, M, Abe, K & Takeda, </author> <title> H (1982). Learning control of finite Markov chains with unknown transition probabilities. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 27, </volume> <pages> 502-505. Schmidhuber, </pages> <month> JH </month> <year> (1991). </year> <title> Adaptive Confidence and Adaptive Curiosity. </title> <type> TR FKI-149-91, </type> <institution> Tech-nische Universit at M unchen, Germany. </institution>
Reference-contexts: Note that the algorithm itself has no free parameters although the model of change in the world may. It is standard to allow the agent to perform synchronous DP in its model of the world for indirect approaches to solving Markov decision problems under conditions of partial information <ref> (Sato et al, 1982) </ref>. However, this can itself be computationally ruinous in large problems. Having defined the goal of the dynamic programming, approximate but faster methods such as reinforcement learning can be used.
Reference: <author> Simon, </author> <title> HA (1955). A behavioral model of rational choice. </title> <journal> Quarterly Journal of Economics, </journal> <volume> 69, </volume> <pages> 99-118. </pages>
Reference: <author> Simon, </author> <title> HA (1956). Rational choice and the structure of the environment. </title> <journal> Psychological Review, </journal> <volume> 63, </volume> <pages> 129-138. </pages> <month> Striebel, </month> <title> CT (1965). Sufficient statistics in the optimal control of stochastic systems. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 12, </volume> <pages> 576-592. </pages> <editor> Sutton, </editor> <title> RS (1990). Integrated architectures for learning, planning, and reacting based 19 on approximating dynamic programming. </title> <booktitle> Machine Learning: Proceedings of the Seventh International Conference, </booktitle> <pages> 216-224. </pages>
Reference: <author> Thrun, </author> <title> SB (1992). The role of exploration in learning control. In DA White & DA Sofge, editors, Handbook of Intelligent Control: Neural, Fuzzy and Adaptive Approaches. </title> <address> New York, NY: </address> <publisher> Van Nostrand Reinhold. </publisher> <editor> Thrun, SB & M oller, </editor> <title> K (1992). Active exploration in dynamic environments. </title> <editor> In JE Moody, SJ Hanson & RP Lippmann, </editor> <booktitle> editors Advances in Neural Information Processing Systems, </booktitle> <volume> 4, </volume> <pages> 531-538. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The balance between exploration and exploitation is one of the major issues in learning control, and systematic methods for addressing it are sorely required. In reinforcement learning, various approaches to addressing the tradeoff between exploration and exploitation have been studied empirically <ref> (see Thrun, 1992, for a review) </ref>. One popular technique is to be optimistic in parts of the state-space that have never been explored, or, if the environment can change over time, have not been explored recently (Sutton, 1990; Moore & Atkeson, 1993; Christiansen, Mason & Mitchell, 1991).
Reference: <author> Tse, E & Bar-Shalom, </author> <title> Y (1973). An actively adaptive control for linear systems with random parameters via the dual control approach. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 18, </volume> <pages> 109-117. </pages>
Reference: <author> Tse, E, Bar-Shalom, Y & Meier, L, </author> <month> IIIrd </month> <year> (1973). </year> <title> Wide-sense adaptive dual control for nonlinear stochastic systems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 18, </volume> <pages> 98-108. Watkins, </pages> <month> CJCH </month> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD Thesis. </type> <institution> University of Cam-bridge, </institution> <address> England. </address> <month> 20 </month>
References-found: 16

