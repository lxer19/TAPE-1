URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-364.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Title: "Correlation and Interpolation Networks for Real-time Expression Analysis/Synthesis",  Task-specific Gesture Analysis in Real-Time using Interpolated Views  
Author: Trevor J. Darrell, Irfan A. Essa, Alex P. Pentland 
Date: Revised, December, 1995  
Affiliation: MIT Media Lab  
Note: (Submitted, IEEE Trans. PAMI; Portions previously appeared, "Space-Time Gestures", IEEE Proc. CVPR '93, and in  Proc. NIPS '94)  
Abstract: M.I.T. Media Laboratory Perceptual Computing Group Technical Report No. 364. Abstract Hand and face gestures are modeled using an appearance-based approach in which patterns are represented as a vector of similarity scores to a set of view models defined in space and time. These view models are learned from examples using unsupervised clustering techniques. A supervised learning paradigm is used to interpolate view scores into a task-dependent coordinate system appropriate for recognition and control tasks. We apply this analysis to the problem of context-specific gesture interpolation and recognition, and demonstrate real-time systems which perform these tasks.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bellman, R. E., </author> <title> Dynamic Programming. </title> <publisher> Princeton, </publisher> <address> NJ: </address> <publisher> Princeton Univ. Press, </publisher> <year> 1957. </year> <month> 16 </month>
Reference-contexts: To find the similarity between two sequences, we again use a normalized correlation metric, but after using the Dynamic Time Warping (DTW) method to temporally align the two sequences, thus allowing the time-course of a gesture to vary. The DTW method involves the use of dynamic programming techniques <ref> [1] </ref> to solve an elastic pattern matching task, and was originally developed to solve the time alignment problem in the speech and signal processing literature [15]. Note that DTW is a simplification of Hidden Markov Modeling (HMM); they are equivalent for the relatively simple (non-branching) sequences we will be considering. <p> To temporally align an observed sequence of spatial view model score vectors, R = fr [0]; r <ref> [1] </ref>; :::; r [T ]g , with a temporal view model of spatial scores T p = fs p [0]; s p [1]; :::; s p [T 0 p ]g, where the number of vectors in each sequence are not necessarily equal, we consider a grid for each view model whose <p> To temporally align an observed sequence of spatial view model score vectors, R = fr [0]; r <ref> [1] </ref>; :::; r [T ]g , with a temporal view model of spatial scores T p = fs p [0]; s p [1]; :::; s p [T 0 p ]g, where the number of vectors in each sequence are not necessarily equal, we consider a grid for each view model whose horizontal axis is associated with R and whose vertical axis is associated with T p . <p> The RBF was defined with exemplars of the form (y (i) ; g (i) ), where g (i) are the spatio-temporal appearance vectors in the training set, y (i) is set to <ref> [1; 0; 0] </ref> T if the i-th gesture in the training set is a "hello" gesture, [0; 1; 0] T if "good-bye", and [0; 0; 1] T if it is a conflictor or no gesture ("other"). <p> The RBF was defined with exemplars of the form (y (i) ; g (i) ), where g (i) are the spatio-temporal appearance vectors in the training set, y (i) is set to [1; 0; 0] T if the i-th gesture in the training set is a "hello" gesture, <ref> [0; 1; 0] </ref> T if "good-bye", and [0; 0; 1] T if it is a conflictor or no gesture ("other"). We classified all of the gestures in the test set, defining the predicted class based on the index of the largest interpolated score in the result vector. <p> exemplars of the form (y (i) ; g (i) ), where g (i) are the spatio-temporal appearance vectors in the training set, y (i) is set to [1; 0; 0] T if the i-th gesture in the training set is a "hello" gesture, [0; 1; 0] T if "good-bye", and <ref> [0; 0; 1] </ref> T if it is a conflictor or no gesture ("other"). We classified all of the gestures in the test set, defining the predicted class based on the index of the largest interpolated score in the result vector.
Reference: [2] <author> Beymer, </author> <title> D.J., "Face recognition under varying pose", </title> <booktitle> Proc. IEEE Conf. Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 756-761, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: We present a system which tracks facial expressions in real-time without mechanical actuators or make-up, using our interpolated appearance-based vision methods. This approach follows in the tradition of others who have explored techniques for visual analysis of facial expression <ref> [20, 16, 2, 7, 10] </ref>. In our method spatial view outputs are interpolated to control the motor states of a 3-D computer graphics face, using the face model employed in Essa and Pentland [7]. The result vector ^y is defined to be the motor state of the animated face.
Reference: [3] <author> Breuel, T., </author> <title> "View-based Recognition", </title> <booktitle> IAPR Workshop on Machine Vision Applications, </booktitle> <address> Tokyo, Japan, </address> <year> 1992. </year>
Reference-contexts: Recognition using views was analyzed by Breuel, who established that there are reasonable bounds on the number of views needed for a given error rate <ref> [3] </ref>. However, the view-based models used in these approaches rely on a feature-based representation of an image, in which a "view" is the list of vertex locations of semantically relevant features. Unfortunately, the automatic extraction of these features remains a difficult problem.
Reference: [4] <author> Cipolla, R., Okamotot, Y., and Kuno, Y., </author> <title> "Qualitative visual interpretation of 3D hand gestures using motion parallax", </title> <booktitle> IAPR Workshop on Machine Vision Applications, </booktitle> <address> Tokyo, Japan, </address> <year> 1992. </year>
Reference-contexts: Hands and faces are complex 1 3D articulated structures, whose kinematics and dynamics are difficult to model with full realism. Consequently, instead of performing model-based reconstruction and attempting to extract explicit 3D model parameters (for example see <ref> [4, 8, 9] </ref>), we use a direct approach which represents the object performing the gesture with a vector of similarity scores to a set of 2-D views.
Reference: [5] <author> Darrell, T. and Pentland, A., </author> <booktitle> "Space-Time Gestures" Proceedings IEEE CVPR-93, </booktitle> <address> New York, </address> <year> 1993. </year>
Reference-contexts: run of the rotating box example); the time required to exhaustively search all models in this case was on the order of 200-300ms. (At this resolution our system can store up to 100 view models in memory accessible by the searching hardware.) Using the predictive search pruning mechanism described in <ref> [5] </ref>, which exploits temporal correlation in the observed view scores, we were able to reduce the processing time for this example to under 100ms.
Reference: [6] <author> Darrell, T., and Pentland, A., </author> <title> "Attention-driven Expression and Gesture Analysis in an Interactive Environment," </title> <booktitle> Int'l Workshop on Face and Gesture Recognition, </booktitle> <address> Zurich, Switzerland, </address> <month> June 26-28., </month> <year> 1995. </year>
Reference: [7] <author> Essa, I., and Pentland, A. P., </author> <title> "A vision system for observing and extracting facial action parameters", </title> <booktitle> In Proc. IEEE Conf. Computer Vision and Pattern Recognition, </booktitle> <year> 1994. </year>
Reference-contexts: We present a system which tracks facial expressions in real-time without mechanical actuators or make-up, using our interpolated appearance-based vision methods. This approach follows in the tradition of others who have explored techniques for visual analysis of facial expression <ref> [20, 16, 2, 7, 10] </ref>. In our method spatial view outputs are interpolated to control the motor states of a 3-D computer graphics face, using the face model employed in Essa and Pentland [7]. The result vector ^y is defined to be the motor state of the animated face. <p> In our method spatial view outputs are interpolated to control the motor states of a 3-D computer graphics face, using the face model employed in Essa and Pentland <ref> [7] </ref>. The result vector ^y is defined to be the motor state of the animated face. Training examples are acquired by setting the model face to generate a particular expression, asking the user to mimic the expression, and recording the pair of vision scores and muscle parameters.
Reference: [8] <author> Fukumoto, M., Mase, K., and Suenaga, Y., </author> <title> "Real-Time Detection of Pointing Actions for a Glove-Free Interface", </title> <booktitle> IAPR Workshop on Machine Vision Applications. </booktitle> <address> Tokyo, Japan, </address> <year> 1992. </year>
Reference-contexts: Hands and faces are complex 1 3D articulated structures, whose kinematics and dynamics are difficult to model with full realism. Consequently, instead of performing model-based reconstruction and attempting to extract explicit 3D model parameters (for example see <ref> [4, 8, 9] </ref>), we use a direct approach which represents the object performing the gesture with a vector of similarity scores to a set of 2-D views.
Reference: [9] <author> Ishibuchi, K., Takemura, H., and Kishino, F., </author> <title> "Real-Time Hand Shape Recognition using Pipe-line Image Processor", </title> <booktitle> IEEE Workshop on Robot and Human Communication, </booktitle> <pages> pp. 111-116, </pages> <year> 1992. </year>
Reference-contexts: Hands and faces are complex 1 3D articulated structures, whose kinematics and dynamics are difficult to model with full realism. Consequently, instead of performing model-based reconstruction and attempting to extract explicit 3D model parameters (for example see <ref> [4, 8, 9] </ref>), we use a direct approach which represents the object performing the gesture with a vector of similarity scores to a set of 2-D views.
Reference: [10] <author> Mase, K., </author> <title> "Recognition of facial expressions for optical flow", </title> <journal> IEICE Transactions, Special Issue on Computer Vision and its Applications, </journal> <volume> E 74(10), </volume> <year> 1991. </year>
Reference-contexts: We present a system which tracks facial expressions in real-time without mechanical actuators or make-up, using our interpolated appearance-based vision methods. This approach follows in the tradition of others who have explored techniques for visual analysis of facial expression <ref> [20, 16, 2, 7, 10] </ref>. In our method spatial view outputs are interpolated to control the motor states of a 3-D computer graphics face, using the face model employed in Essa and Pentland [7]. The result vector ^y is defined to be the motor state of the animated face.
Reference: [11] <author> Moghaddam, B., and Pentland, A., </author> <title> "Probabilistic Visual Learning for Object Detection", </title> <booktitle> Proceedings of the International Conference on Computer Vision 1995 (ICCV'95), </booktitle> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: However the choice of normalized correlation was only due to hardware restrictions; with other hardware we could add a preprocessing step (such as edge extraction) and apply the method unchanged (see <ref> [11] </ref> for such an example). In practice, we have simply controlled the illumination, for instance, near-band IR illumination from LEDs is visible to standard CCD cameras but invisible to humans. Such IR illumination has the additional advantage that humans have little skin pigmentation at these wavelengths.
Reference: [12] <author> Murase, H.,and Nayar, S. K., </author> <title> "Learning and Recognition of 3D Objects from Appearance", </title> <booktitle> Proc. IEEE Qualitative Vision Workshop, </booktitle> <address> New York City, </address> <pages> pp. 39-49, </pages> <year> 1993. </year>
Reference-contexts: More closely related to our approach is the work of Turk and Pentland [17], who used combinations of low-order eigenvectors to describe a space of target appearances. In this way they were able to detect and recognize human faces. Murase and Nayar <ref> [12] </ref> later generalized this appearance-based approach to accurately recognize a set of industrial objects and determine their pose.
Reference: [13] <author> Poggio, T., and Girosi, F., </author> <title> "A theory of networks for approximation and learning", </title> <institution> MIT AI Lab TR-1140, </institution> <year> 1989. </year>
Reference-contexts: Interpolation is done using a supervised learning paradigm, based on a set of training examples which define the desired result for a particular set of view model outputs. Using the Radial Basis Function (RBF) method presented in <ref> [13] </ref>, we compute a result vector ^y to be a weighted sum of radial functions centered at an exemplar value: ^y (g) = i=1 3 Plans for selling cars that have cameras pointed at the driver, for drowsiness detection, have already been announced by some manufacturers. 9 where c = F
Reference: [14] <author> Poggio, T., and Edelman, S., </author> <title> "A Network that Learns to Recognize Three Dimensional Objects", </title> <journal> Nature, </journal> <volume> Vol. 343, No. 6255, </volume> <pages> pp. 263-266, </pages> <year> 1990. </year>
Reference-contexts: This approach is related to the idea of view-based representation, as advocated by Ullman [18] and Poggio <ref> [14] </ref>, for representing 3-D objects by interpolating between a small set of 2-D views. Recognition using views was analyzed by Breuel, who established that there are reasonable bounds on the number of views needed for a given error rate [3].
Reference: [15] <author> Sakoe, H., and Chiba, S., </author> <title> "Dynamic Programming optimization for spoken word recognition", </title> <journal> IEEE Trans. ASSP, </journal> <volume> Vol. 26, </volume> <pages> pp. 623-625, </pages> <year> 1980. </year>
Reference-contexts: The DTW method involves the use of dynamic programming techniques [1] to solve an elastic pattern matching task, and was originally developed to solve the time alignment problem in the speech and signal processing literature <ref> [15] </ref>. Note that DTW is a simplification of Hidden Markov Modeling (HMM); they are equivalent for the relatively simple (non-branching) sequences we will be considering.
Reference: [16] <author> Terzopoulus, D., and Waters, K., </author> <title> "Analysis and synthesis of facial image sequences using physical and anatomical models", </title> <journal> IEEE Trans. PAMI, </journal> <volume> 15(6) </volume> <pages> 569-579, </pages> <year> 1993. </year>
Reference-contexts: We present a system which tracks facial expressions in real-time without mechanical actuators or make-up, using our interpolated appearance-based vision methods. This approach follows in the tradition of others who have explored techniques for visual analysis of facial expression <ref> [20, 16, 2, 7, 10] </ref>. In our method spatial view outputs are interpolated to control the motor states of a 3-D computer graphics face, using the face model employed in Essa and Pentland [7]. The result vector ^y is defined to be the motor state of the animated face.
Reference: [17] <author> Turk, M., and Pentland, A. P., </author> <title> "Eigenfaces for Recognition", </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> vol. 3, </volume> <pages> pp. 71-89, </pages> <year> 1991. </year>
Reference-contexts: Unfortunately, the automatic extraction of these features remains a difficult problem. More closely related to our approach is the work of Turk and Pentland <ref> [17] </ref>, who used combinations of low-order eigenvectors to describe a space of target appearances. In this way they were able to detect and recognize human faces. Murase and Nayar [12] later generalized this appearance-based approach to accurately recognize a set of industrial objects and determine their pose.
Reference: [18] <author> Ullman, S., and Basri, R., </author> <title> "Recognition by Linear Combinations of Models," </title> <journal> IEEE PAMI, </journal> <volume> Vol. 13, No. 10, </volume> <pages> pp. 992-1007, </pages> <year> 1991. </year> <month> 17 </month>
Reference-contexts: This approach is related to the idea of view-based representation, as advocated by Ullman <ref> [18] </ref> and Poggio [14], for representing 3-D objects by interpolating between a small set of 2-D views. Recognition using views was analyzed by Breuel, who established that there are reasonable bounds on the number of views needed for a given error rate [3].
Reference: [19] <author> K. Waters and D. Terzopoulos. </author> <title> "Modeling and animating faces using scanned data", </title> <journal> The Journal of Visualization and Computer Animation, </journal> <volume> 2 </volume> <pages> 123-128, </pages> <year> 1991. </year>
Reference: [20] <author> L. Williams. </author> <title> "Performance-driven facial animation", </title> <booktitle> ACM SIGGRAPH Conference Proceedings, </booktitle> <volume> 24(4) </volume> <pages> 235-242, </pages> <year> 1990. </year>
Reference-contexts: We present a system which tracks facial expressions in real-time without mechanical actuators or make-up, using our interpolated appearance-based vision methods. This approach follows in the tradition of others who have explored techniques for visual analysis of facial expression <ref> [20, 16, 2, 7, 10] </ref>. In our method spatial view outputs are interpolated to control the motor states of a 3-D computer graphics face, using the face model employed in Essa and Pentland [7]. The result vector ^y is defined to be the motor state of the animated face.
Reference: [21] <author> Y. Yacoob and L. Davis, </author> <title> "Computing spatio-temporal representations of human faces", </title> <booktitle> Proc. IEEE Conf. Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 70-75, </pages> <address> Seattle, WA, </address> <year> 1994. </year> <month> 18 </month>
References-found: 21

