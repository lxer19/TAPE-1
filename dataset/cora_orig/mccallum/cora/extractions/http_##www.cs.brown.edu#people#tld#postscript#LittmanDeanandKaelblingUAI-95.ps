URL: http://www.cs.brown.edu/people/tld/postscript/LittmanDeanandKaelblingUAI-95.ps
Refering-URL: http://www.cs.brown.edu/people/tld/
Root-URL: 
Email: Email: fmlittman,tld,lpkg@cs.brown.edu  
Title: On the Complexity of Solving Markov Decision Problems  
Author: Michael L. Littman, Thomas L. Dean, Leslie Pack Kaelbling 
Address: 115 Waterman Street Providence, RI 02912, USA  
Affiliation: Department of Computer Science Brown University  
Abstract: Markov decision problems (MDPs) provide the foundations for a number of problems of interest to AI researchers studying automated planning and reinforcement learning. In this paper, we summarize results regarding the complexity of solving MDPs and the running time of MDP solution algorithms. We argue that, although MDPs can be solved efficiently in theory, more study is needed to reveal practical algorithms for solving large problems quickly. To encourage future research, we sketch some alternative methods of analysis that rely on the struc ture of MDPs.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Bradtke, S. J., and Singh, S. P. </author> <year> (1995). </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72(1) </volume> <pages> 81-138. </pages>
Reference: <author> Bellman, R. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press. </publisher>
Reference-contexts: A fundamental result in the theory of MDPs is that there exists a stationary policy that dominates or has equal total cost to every other policy <ref> (Bellman, 1957) </ref>. Such a policy is termed an optimal policy and the total cost it attaches to each state is said to be the optimal total cost for that state.
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, </publisher> <address> En-glewood Cliffs, N.J. </address>
Reference-contexts: The total-cost functions so computed are guaranteed to converge in the limit to the optimal total-cost function. In addition, the policy associated with the successive total-cost functions will converge to the optimal policy in a finite number of iterations <ref> (Bertsekas, 1987) </ref>, and in practice this convergence can be quite rapid. The basic value-iteration algorithm is described as follows: 1. For each i 2 S , initialize E 0 ( fl ji). 2. Set n to be 1. 3.
Reference: <author> Bertsekas, D. P. and Casta~non, D. A. </author> <year> (1989). </year> <title> Adaptive aggregation for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 34(6) </volume> <pages> 589-598. </pages>
Reference: <author> Bland, R. G. </author> <year> (1977). </year> <title> New finite pivoting rules for the simplex method. </title> <journal> Mathematics of Operations Research, </journal> <volume> 2 </volume> <pages> 103-107. </pages>
Reference: <author> Boutilier, C., Dean, T., and Hanks, S. </author> <year> (1995a). </year> <title> Planning under uncertainty: Structural assumptions and computational leverage. </title> <booktitle> Submitted to the International Joint Conference on Artificial Intelligence. </booktitle>
Reference: <author> Boutilier, C., Dearden, R., and Goldszmidt, M. </author> <year> (1995b). </year> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the 1995 International Joint Conference on Artificial Intelligence. </booktitle>
Reference: <author> Coppersmith, D. and Winograd, S. </author> <year> (1987). </year> <title> Matrix multiplication via arithmetic progressions. </title> <booktitle> In Proceedings of 19th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 1-6. </pages>
Reference-contexts: We are interested in finding a polynomial upper bound or in 2 In theory, value determination can probably be done somewhat faster, since it primarily requires inverting a N fi N matrix, which can be done in O (N 2:376 ) steps <ref> (Coppersmith and Winograd, 1987) </ref>. solution to the family of MDPs illustrated here (af ter (Melekopoglou and Condon, 1990)). showing that no such upper bound exists (i.e., that the number of iterations can be exponential in the worst case).
Reference: <author> Dagum, P., Karp, R., Luby, M., and Ross, S. M. </author> <year> (1995). </year> <title> An optimal stopping rule for Monte Carlo estimation. </title> <note> Submitted to FOCS-95. </note>
Reference: <author> Dagum, P. and Luby, M. </author> <year> (1993). </year> <title> Approximating probabilistic inference in bayesian belief networks is NP-hard. </title> <journal> Artificial Intelligence, </journal> <volume> 60 </volume> <pages> 141-153. </pages>
Reference-contexts: Fully-polynomial randomized approximation schemes (FPRAS) such as this are generally designed for problems that cannot be computed exactly in polynomial time (e.g., <ref> (Dagum and Luby, 1993) </ref>), but researchers are now developing iterative algorithms with tight probabilistic performance bounds that provide reliable estimates (e.g., the Dagum et al. (1995) optimal stopping rule for Monte Carlo estimation).
Reference: <author> Dantzig, G. </author> <year> (1963). </year> <title> Linear Programming and Extensions. </title> <publisher> Princeton University Press. </publisher>
Reference: <author> Dantzig, G. and Wolfe, P. </author> <year> (1960). </year> <title> Decomposition principle for dynamic programs. </title> <journal> Operations Research, </journal> <volume> 8(1) </volume> <pages> 101-111. </pages>
Reference: <author> Dean, T., Kaelbling, L., Kirman, J., and Nicholson, A. </author> <year> (1993). </year> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings AAAI-93, </booktitle> <pages> pages 574-579. </pages> <publisher> AAAI. </publisher>
Reference: <author> Dean, T., Kaelbling, L., Kirman, J., and Nicholson, A. </author> <year> (1995a). </year> <title> Planning under time constraints in stochastic domains. </title> <note> To appear in Artificial Intelligence. </note>
Reference-contexts: This value could be used to help characterize hard and easy MDPs. Some work has already been done to characterize MDPs with respect to their computational properties, including experimental comparisons that illustrate that there are plenty of easy problems mixed in with extraordinarily hard ones <ref> (Dean et al., 1995a) </ref>, and categorization schemes that attempt to relate measurable attributes of MDPs such as the amount of uncertainty in actions to the type of solution method that works best (Kirman, 1994).
Reference: <author> Dean, T., Kanazawa, K., Koller, D., and Russell, S. </author> <year> (1995b). </year> <title> Decision theoretic planning: Part I. </title> <note> Submitted to the Journal for Artificial Intelligence Research. </note>
Reference-contexts: Markov decision problems (MDPs) provide the theoretical foundations for decision-theoretic planning, reinforcement learning, and other sequential decision-making tasks of interest to researchers and practitioners in artificial intelligence and operations research <ref> (Dean et al., 1995b) </ref>. MDPs employ dynamical models based on well-understood stochastic processes and performance criteria based on established theory in operations research, economics, combinatorial optimization, and the social sciences (Puterman, 1994). It would seem that MDPs exhibit special structure that might be exploited to expedite their solution.
Reference: <author> Dean, T. and Lin, S.-H. </author> <year> (1995). </year> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> In Proceedings of the 1995 International Joint Conference on Artificial Intelligence. </booktitle>
Reference: <author> Denardo, E. </author> <year> (1982). </year> <title> Dynamic Programming: Models and Applications. </title> <publisher> Prentice-Hall, Inc. </publisher>
Reference: <author> D'Epenoux, F. </author> <year> (1963). </year> <title> A probabilistic production and inventory problem. </title> <journal> Management Science, </journal> <volume> 10 </volume> <pages> 98-108. </pages>
Reference: <author> Derman, C. </author> <year> (1970). </year> <title> Finite State Markovian Decision Processes. </title> <publisher> Cambridge University Press, </publisher> <address> New York. </address>
Reference-contexts: In the expected discounted cumulative cost criterion, the cost of a trajectory is the sum over all t of fl t times the instantaneous cost at time t, where 0 &lt; fl &lt; 1 is the discount rate and t indicates the stage. 1 Under reasonable assumptions <ref> (Derman, 1970) </ref>, the expected cost to target and expected discounted cumulative cost criteria give rise 1 When fl is considered part of the input, it is assumed to be encodable in B bits. to equivalent computational problems.
Reference: <author> Howard, R. A. </author> <year> (1960). </year> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachusetts. </address>
Reference: <author> Jerrum, M. and Sinclair, A. </author> <year> (1988). </year> <title> Conductance and the rapid mixing property for Markov chains: the approximation of the permanent resolved (preliminary version). </title> <booktitle> In ACM Symposium on Theoretical Computing, </booktitle> <pages> pages 235-244. </pages>
Reference: <author> Kaelbling, L. P. </author> <year> (1993). </year> <title> Hierarchical learning in stochastic domains: A preliminary report. </title> <booktitle> In Proceedings Tenth International Conference on Machine Learning, page 1993. </booktitle>
Reference: <author> Kalai, G. </author> <year> (1992). </year> <title> A subexponential randomized simplex algorithm. </title> <booktitle> In Proceedings of 24th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 475-482. </pages>
Reference: <author> Karmarkar, N. </author> <year> (1984). </year> <title> A new polynomial time algorithm for linear programming. </title> <journal> Combinatorica, </journal> <volume> 4(4) </volume> <pages> 373-395. </pages>
Reference: <author> Khachian, L. G. </author> <year> (1979). </year> <title> A polynomial algorithm for linear programming. </title> <journal> Soviet Math Dokl., </journal> <volume> 20 </volume> <pages> 191-194. </pages>
Reference: <author> Kirman, J. </author> <year> (1994). </year> <title> Predicting Real-Time Planner Performance by Domain Characterization. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University. </institution>
Reference-contexts: including experimental comparisons that illustrate that there are plenty of easy problems mixed in with extraordinarily hard ones (Dean et al., 1995a), and categorization schemes that attempt to relate measurable attributes of MDPs such as the amount of uncertainty in actions to the type of solution method that works best <ref> (Kirman, 1994) </ref>. One thing not considered by any of the algorithms mentioned above is that, in practice, the initial state is often known.
Reference: <author> Klee, V. and Minty, G. J. </author> <year> (1972). </year> <title> How good is the simplex algorithm? In Shisha, </title> <editor> O., editor, </editor> <title> Inequalities, </title> <booktitle> III, </booktitle> <pages> pages 159-175. </pages> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Kushner, H. J. and Chen, C.-H. </author> <year> (1974). </year> <title> Decomposition of systems governed by Markov chains. </title> <journal> IEEE Transactions on Automatic Control, AC-19(5):501-507. </journal>
Reference: <author> Leighton, T., Makedon, F., Plotkin, S., Stein, C., Tar-dos, E., and Tragoudas, S. </author> <year> (1991). </year> <title> Fast approximation algorithms for multicommodity flow problems. </title> <booktitle> In Proceedings of 23rd Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 101-111. </pages>
Reference-contexts: Unfortunately, the degree of the polynomial is nontrivial and the methods that are guaranteed to achieve such polynomial-time performance do not make any significant use of the structure of MDPs. Furthermore, as with the multi-commodity flow problem <ref> (Leighton et al., 1991) </ref>, the existence of a linear programming solution does not preclude the need for more efficient algorithms, even if it means finding only approximately optimal solutions. This section sketches some directions that could be pursued to find improved algorithms for MDPs.
Reference: <author> Melekopoglou, M. and Condon, A. </author> <year> (1990). </year> <title> On the complexity of the policy iteration algorithm for stochastic games. </title> <type> Technical Report CS-TR-90-941, </type> <institution> Computer Sciences Department, University of Wiscon-sin Madison. </institution> <note> To appear in the ORSA Journal on Computing. </note>
Reference-contexts: polynomial upper bound or in 2 In theory, value determination can probably be done somewhat faster, since it primarily requires inverting a N fi N matrix, which can be done in O (N 2:376 ) steps (Coppersmith and Winograd, 1987). solution to the family of MDPs illustrated here (af ter <ref> (Melekopoglou and Condon, 1990) </ref>). showing that no such upper bound exists (i.e., that the number of iterations can be exponential in the worst case).
Reference: <author> Moore, A. W. and Atkeson, C. G. </author> <year> (1993). </year> <title> Memory-based reinforcement learning: Efficient computation with prioritized sweeping. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 263-270, </pages> <address> San Mateo, California. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moore, A. W. and Atkeson, C. G. </author> <year> (1995). </year> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state spaces. </title> <note> To appear in Machine Learning. </note>
Reference: <author> Papadimitriou, C. H. and Tsitsiklis, J. N. </author> <year> (1987). </year> <title> The complexity of Markov chain decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3) </volume> <pages> 441-450. </pages>
Reference: <author> Peng, J. and Williams, R. J. </author> <year> (1993). </year> <title> Efficient learning and planning within the dyna framework. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 1(4) </volume> <pages> 437-454. </pages>
Reference: <author> Plotkin, S. A., Shmoys, D. B., and Tardos, E. </author> <year> (1991). </year> <title> Fast approximation algorithms for fractional packing and covering problems. </title> <booktitle> In 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 495-504. </pages>
Reference-contexts: For example, approximation algorithms have been designed for solving linear programs. One is designed for finding *- optimal solutions to a certain class of linear programs which includes the primal linear program given in Section 4.1 <ref> (Plotkin et al., 1991) </ref>. Although this particular scheme is unlikely to yield practical implementations (it is most useful for solving linear programs with exponentially many constraints) the application of approximate linear-programming approaches to MDPs is worth more study.
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Processes. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: MDPs employ dynamical models based on well-understood stochastic processes and performance criteria based on established theory in operations research, economics, combinatorial optimization, and the social sciences <ref> (Puterman, 1994) </ref>. It would seem that MDPs exhibit special structure that might be exploited to expedite their solution. <p> Return . Step 2b is the value determination phase and Step 2c is the policy improvement phase. Since there are only M N distinct policies, and each new policy dominates the previous one <ref> (Puterman, 1994) </ref>, it is obvious that policy iteration terminates in at most an exponential number of steps. <p> Show that each iteration results in an improve-ment of a factor of at least fl in the distance between the estimated and optimal total-cost func tions. This is the standard "contraction mapping" result for discounted MDPs <ref> (Puterman, 1994) </ref>. 3. Give an expression for the distance between estimated and optimal total-cost functions after n iterations. Show how this gives a bound on the number of iterations required for an *-optimal policy.
Reference: <author> Puterman, M. L. and Shin, M. C. </author> <year> (1978). </year> <title> Modified policy iteration algorithms for discounted Markov decision processes. </title> <journal> Management Science, </journal> <volume> 24 </volume> <pages> 1127-1137. </pages>
Reference: <author> Schrijver, A. </author> <year> (1986). </year> <title> Theory of linear and integer programming. </title> <publisher> Wiley-Interscience. </publisher>
Reference-contexts: A standard result in the theory of linear programming is that the solution to such a linear program can be written as rational numbers where each component is represented using a number of bits polynomial in the size of the system and B, B fl <ref> (Schrijver, 1986) </ref>. This means that if we can find a policy that is * = 1=2 B fl +1 -optimal, the policy must be optimal. 5. Substitute this value of * into the bound to get a bound on the number of iterations needed for an exact answer.
Reference: <author> Schweitzer, P. J. </author> <year> (1984). </year> <title> Aggregation methods for large Markov chains. </title> <editor> In Iazola, G., Coutois, P. J., and Hordijk, A., editors, </editor> <booktitle> Mathematical Computer Performance and Reliability, </booktitle> <pages> pages 275-302. </pages> <publisher> Elsevier, Ams-terdam, Holland. </publisher>
Reference-contexts: Structure in the underlying dynamics should allow us to aggregate states and decompose problems into weakly-coupled subproblems, thereby simplifying computation. Aggregation has long been an active topic of research in operations research and optimal control <ref> (Schweitzer, 1984) </ref>. In particular, Bertsekas and Casta~non (1989) describe adaptive aggregation techniques that might be very important for large, structured state spaces, and Kushner and Chen (1974) describe how to use Dantzig-Wolfe LP decomposition techniques (1960) for solving large MDPs.
Reference: <author> Tseng, P. </author> <year> (1990). </year> <title> Solving H-horizon, stationary Markov decision problems in time proportional to log(H). </title> <journal> Operations Research Letters, </journal> <volume> 9(5) </volume> <pages> 287-297. </pages>
Reference: <author> Williams, R. J. and Baird, L. C. I. </author> <year> (1993). </year> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-13, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA. </address>
Reference-contexts: The Bellman residual at step n is defined to be max fi fi By examining the Bellman residual during value itera tion and stopping when it gets below some threshold, * 0 = *(1 fl)=(2fl), we can guarantee that the resulting policy will be *-optimal <ref> (Williams and Baird, 1993) </ref>. The running time for each iteration is O (M N 2 ), thus, once again, the method is polynomial if and only if the total number of iterations required is polynomial.
References-found: 41

