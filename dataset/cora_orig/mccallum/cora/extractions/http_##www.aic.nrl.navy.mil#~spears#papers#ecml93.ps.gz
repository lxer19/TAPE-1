URL: http://www.aic.nrl.navy.mil/~spears/papers/ecml93.ps.gz
Refering-URL: http://www.cs.gmu.edu:80/research/gag/pubs.html
Root-URL: 
Title: An Overview of Evolutionary Computation  
Author: William M. Spears Kenneth A. De Jong Thomas Ba ck David B. Fogel Hugo de Garis 
Abstract: Evolutionary computation uses computational models of evolution - ary processes as key elements in the design and implementation of computer-based problem solving systems. In this paper we provide an overview of evolutionary computation, and describe several evolutionary algorithms that are currently of interest. Important similarities and di fferences are noted, which lead to a discussion of important issues that need to be resolved, and items for future research.
Abstract-found: 1
Intro-found: 1
Reference: <author> Atmar, W. </author> <title> (1992) The philosophical errors that plague both evolutionary theory and simulated evolutionary programming. </title> <booktitle> Proceedings of the First Annual Conference on Evolutionary Programming, </booktitle> <pages> 27-34. </pages> <address> San Diego, CA: </address> <publisher> Evolutionary Programming Society. </publisher>
Reference: <author> Ba ck, T., Hoffmeister, F., & Schwefel, H.-P. </author> <title> (1991) A survey of evolution strategies. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, 2 9. </booktitle> <address> La Jolla, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <editor> Ba ck, T., & Schwefel, H.-P. </editor> <title> (1993) An overview of evolutionary algorithms for parameter optimization. </title> <note> Submitted to the Journal of Evolutionary Computation. </note>
Reference: <editor> Belew, R. K., & Booker, L. B. (eds.) </editor> <booktitle> (1991) Proceedings of the Fourth International Conference on Genetic Algorithms. </booktitle> <address> La Jolla, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Booker, L. B. </author> <title> (1992) Recombination distributions for genetic algorithms. </title> <booktitle> Proceed--ings of the Foundations of Genetic Algorithms Workshop. </booktitle> <address> Vail, </address> <publisher> CO: Morgan Kaufmann. </publisher>
Reference: <author> Box, G. E. P. </author> <title> (1957) Evolutionary operation: a method of increasing industrial productivity. </title> <journal> Applied Statistics, </journal> <volume> Vol. 6, </volume> <pages> 81-101. </pages>
Reference: <author> Davis, L. </author> <title> (1989) Adapting operator probabilities in genetic algorithms. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> 60-69. </pages> <address> La Jolla, CA: </address> <publisher> Morgan Kaufmann. </publisher> <editor> de Garis, H. </editor> <title> (1990) Genetic programming: modular evolution for darwin machines. </title> <booktitle> Proceedings of the 1990 International Joint Conference on Neural Networks, </booktitle> <pages> 194-197. </pages> <address> Washington, DC: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: These approaches in turn have inspired the development of additional evolutionary algorithms such as "classifier systems" (Holland, 1986), the LS systems (Smith, 1983), "adaptive operator" systems <ref> (Davis, 1989) </ref>, GENITOR (Whitley, 1989), SAMUEL (Grefenstette, 1989), "genetic programming" (de Garis, 1990; Koza, 1991), "messy GAs" (Goldberg, 1991), and the CHC approach (Eshelman, 1991). We will not attempt to survey this broad range of activities here. <p> A variety of features are typically added to EAs in this context to improve both the speed and the precision of the results. Interested readers should review Davis' work on real-valued representations and adaptive operators <ref> (Davis, 1989) </ref>, Whitley's GENITOR system incorporating ranking and "steady state" mechanisms (Whitley, 1989), Goldberg's "messy GAs", that involve adaptive representations (Goldberg, 1991), and Eshelman's high-powered CHC algorithm (Eshelman, 1991). A second active area of application of EAs is in the design of robust rule learn - ing systems. <p> We can also see this as a more genotypic level of representation, since the individual is in some sense encoded in the bit string. Recently, however, the GA community has investigated more phenotypic representations, including vectors of real values <ref> (Davis, 1989) </ref>, ordered lists (Whitley et al., 1989), neural networks (Harp et. al, 1991), and Lisp expressions (Koza, 1991). For each of these representations, special mutation and recombination operators are introduced. The EP and ES communities are similar in this regard.
Reference: <author> De Jong, K. A. </author> <title> (1975) An analysis of the behavior of a class of genetic adaptive systems. </title> <type> Doctoral Thesis, </type> <institution> Department of Computer and Communication Sciences. University of Michigan, </institution> <address> Ann Arbor. </address>
Reference: <author> De Jong, K. & Spears, W. </author> <title> (1991) Learning concept classification rules using genetic algorithms. </title> <booktitle> Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 651-656. </pages> <address> Sydney, Australia: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Although deceptiveness is one possibil - ity, other measures of problem difficulty are needed. One possibility is fitness correlation, which appears to be a measure of EA-Hardness that places less emphasis on optimality <ref> (Manderick et al., 1991) </ref>. Fitness correlation measures the correlation between the fitness of children and their parents. Manderick et al. found a strong relationship between GA performance and the strength of the correlations. Similarly, Lipsitch (1991) has also proposed examining fitness correlations. Another possibility is problem modality.
Reference: <author> De Jong, K. A. </author> <booktitle> (1992) Are genetic algorithms function optimizers? Proceedings of the Second International Conference on Parallel Problem Solving from Nature. </booktitle>
Reference-contexts: In a GA mutation flips bits with some small probability, and is often considered to be a background operator. Recombination, on the other hand, is emphasized as the primary search operator. GAs are often used as optimizers, although some researchers emphasize its general adaptive capabilities <ref> (De Jong, 1992) </ref>. procedure GA; - t = 0; initialize population P (t); evaluate P (t); until (done) - t = t + 1; parent_selection P (t); recombine P (t) mutate P (t); evaluate P (t); survive P (t); - Fig. 4.
Reference: <editor> Eshelman, L. J., & Schaffer, J. D. </editor> <title> (1991) Preventing premature convergence in genetic algorithms by preventing incest. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> 115-122. </pages> <address> La Jolla, CA: </address> <publisher> Morgan Kauf - mann. </publisher>
Reference-contexts: These approaches in turn have inspired the development of additional evolutionary algorithms such as "classifier systems" (Holland, 1986), the LS systems (Smith, 1983), "adaptive operator" systems (Davis, 1989), GENITOR (Whitley, 1989), SAMUEL (Grefenstette, 1989), "genetic programming" (de Garis, 1990; Koza, 1991), "messy GAs" (Goldberg, 1991), and the CHC approach <ref> (Eshelman, 1991) </ref>. We will not attempt to survey this broad range of activities here. <p> Interested readers should review Davis' work on real-valued representations and adaptive operators (Davis, 1989), Whitley's GENITOR system incorporating ranking and "steady state" mechanisms (Whitley, 1989), Goldberg's "messy GAs", that involve adaptive representations (Goldberg, 1991), and Eshelman's high-powered CHC algorithm <ref> (Eshelman, 1991) </ref>. A second active area of application of EAs is in the design of robust rule learn - ing systems. Holland's (1986) classifier systems were some of the early examples, followed by the LS systems of Smith (1983).
Reference: <author> Fogel, L. J., Owens, A. J., & Walsh, M. J. </author> <title> (1966) Artificial Intelligence Through Simulated Evolution. </title> <address> New York: </address> <publisher> Wiley Publishing. </publisher>
Reference-contexts: For the sake of brevity we will not concentrate on this early work but will discuss in some detail three methodologies that have emerged in the last few decades: "evolutionary programming" <ref> (Fogel et al., 1966) </ref>, "evolution strategies" (Rechenberg, 1973), and "genetic algorithms" (Holland, 1975). Although similar at the highest level, each of these varieties implements an evolutionary algorithm in a different manner.
Reference: <author> Fogel, D. B. </author> <title> (1992) An analysis of evolutionary programming. </title> <booktitle> Proceedings of the First Annual Conference on Evolutionary Programming, </booktitle> <pages> 43-51. </pages> <address> La Jolla, CA: </address> <publisher> Evolutionary Programming Society. </publisher>
Reference: <editor> Fogel, D. B., & Atmar, J. W. (eds.) </editor> <booktitle> (1992) Proceedings of the First Annual Conference on Evolutionary Programming. </booktitle> <address> La Jolla, CA: </address> <booktitle> Evolutionary Programming Society Fraser, </booktitle> <editor> A. S. </editor> <title> (1957) Simulation of genetic systems by automatic digital computers. </title> <journal> Australian Journal of Biological Science, </journal> <volume> 10, </volume> <pages> 484-491. </pages>
Reference: <author> Fujiko, C., & Dickinson, J. </author> <title> (1987) Using the genetic algorithm to generate LISP source code to solve the prisoner's dilemma. </title> <booktitle> Proceedings of the Second International Conference on Genetic Algorithms, </booktitle> <pages> 236-240. </pages> <address> Cambridge, MA: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference: <author> Goldberg, D. E. </author> <title> (1989a) Sizing populations for serial and parallel genetic algorithms. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> 70-79. </pages> <address> Fairfax, VA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A small population has less carrying capacity, which is usually adequate for simple problems. Larger populations, with larger carrying capacities, are often better for more difficult problems. Although some work has attempted to characterize good population sizes <ref> (Goldberg, 1989a) </ref>, more theory is needed. In lieu of theory, then, perhaps the evolutionary algorithm can adapt both selection pressure and the population size dynamically, as it solves problems. 3.6 Performance Measures, EA-Hardness, and Evolvability Of course, one can not refer to adaptation without having a performance goal in mind.
Reference: <author> Goldberg, D. E. </author> <title> (1989b) Genetic Algorithms in Search, Optimization & Machine Learning. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Although this is a quite reasonable definition, difficult problems are often constructed by taking advantage of the EA in such a way that selection deliberately leads the search away from the optimum. Such problems are called deceptive <ref> (Goldberg, 1989b) </ref>. From a function optimization point of view, the problem is indeed deceptive. However, the EA may nonetheless maximize accumulated payoff. Should we call a deceptive problem EA-Hard? The answer obviously depends on our goals.
Reference: <author> Goldberg, D. E., Deb, K., & Korb, B. </author> <title> (1991) Don't worry, be messy. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> 24-30. </pages> <address> La Jolla, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These approaches in turn have inspired the development of additional evolutionary algorithms such as "classifier systems" (Holland, 1986), the LS systems (Smith, 1983), "adaptive operator" systems (Davis, 1989), GENITOR (Whitley, 1989), SAMUEL (Grefenstette, 1989), "genetic programming" (de Garis, 1990; Koza, 1991), "messy GAs" <ref> (Goldberg, 1991) </ref>, and the CHC approach (Eshelman, 1991). We will not attempt to survey this broad range of activities here. <p> Interested readers should review Davis' work on real-valued representations and adaptive operators (Davis, 1989), Whitley's GENITOR system incorporating ranking and "steady state" mechanisms (Whitley, 1989), Goldberg's "messy GAs", that involve adaptive representations <ref> (Goldberg, 1991) </ref>, and Eshelman's high-powered CHC algorithm (Eshelman, 1991). A second active area of application of EAs is in the design of robust rule learn - ing systems. Holland's (1986) classifier systems were some of the early examples, followed by the LS systems of Smith (1983). <p> Although much has been done experimentally, very little has been said theoret - ically that helps one choose good representations, nor that explains what it means to have a good representation. Also, very little has been done in the way of adaptive representations, with the exception of messy GAs <ref> (Goldberg, 1991) </ref>, Argot (Shaefer, 1987), the dynamic parameter encoding (DPE) scheme of Schraudolph and Belew (1992), and the Delta coding of Whitley et al. (1991). Messy GAs, Argot, DPE, and Delta coding all attempt to manipulate the granularity of the representation, thus focusing search at the appropriate level.
Reference: <author> Grefenstette, J. G., and Baker, J. E. </author> <title> (1989) How genetic algorithms work: a critical look at implicit parallelism. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> 20-27. </pages> <address> Fairfax, VA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These approaches in turn have inspired the development of additional evolutionary algorithms such as "classifier systems" (Holland, 1986), the LS systems (Smith, 1983), "adaptive operator" systems (Davis, 1989), GENITOR (Whitley, 1989), SAMUEL <ref> (Grefenstette, 1989) </ref>, "genetic programming" (de Garis, 1990; Koza, 1991), "messy GAs" (Goldberg, 1991), and the CHC approach (Eshelman, 1991). We will not attempt to survey this broad range of activities here.
Reference: <author> Grefenstette, John J. </author> <title> (1989) A system for learning control strategies with genetic algorithms. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> 183-190. </pages> <address> Fairfax, VA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These approaches in turn have inspired the development of additional evolutionary algorithms such as "classifier systems" (Holland, 1986), the LS systems (Smith, 1983), "adaptive operator" systems (Davis, 1989), GENITOR (Whitley, 1989), SAMUEL <ref> (Grefenstette, 1989) </ref>, "genetic programming" (de Garis, 1990; Koza, 1991), "messy GAs" (Goldberg, 1991), and the CHC approach (Eshelman, 1991). We will not attempt to survey this broad range of activities here.
Reference: <author> Grefenstette, J. G. </author> <title> (1992) Deception considered harmful. </title> <booktitle> Proceedings of the Founda - tions of Genetic Algorithms Workshop. </booktitle> <address> Vail, </address> <publisher> CO: Morgan Kaufmann. </publisher>
Reference: <author> Harp, S. A., Samad, T., & Guha, A. </author> <title> (1991) Towards the genetic synthesis of neural networks. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> 360-369. </pages> <address> La Jolla, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We can also see this as a more genotypic level of representation, since the individual is in some sense encoded in the bit string. Recently, however, the GA community has investigated more phenotypic representations, including vectors of real values (Davis, 1989), ordered lists (Whitley et al., 1989), neural networks <ref> (Harp et. al, 1991) </ref>, and Lisp expressions (Koza, 1991). For each of these representations, special mutation and recombination operators are introduced. The EP and ES communities are similar in this regard.
Reference: <author> Holland, J. H. </author> <booktitle> (1975) Adaptation in Natural and Artifi cial Systems. </booktitle> <address> Ann Arbor, Michigan: </address> <publisher> The University of Michigan Press. </publisher>
Reference-contexts: For the sake of brevity we will not concentrate on this early work but will discuss in some detail three methodologies that have emerged in the last few decades: "evolutionary programming" (Fogel et al., 1966), "evolution strategies" (Rechenberg, 1973), and "genetic algorithms" <ref> (Holland, 1975) </ref>. Although similar at the highest level, each of these varieties implements an evolutionary algorithm in a different manner.
Reference: <author> Holland, J. </author> <title> (1986) Escaping brittleness: The possibilities of general-purpose learn - ing algorithms applied to parallel rule-based systems. </title> <editor> In R. Michalski, J. Car-bonell, T. Mitchell (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Los Altos: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We will highlight the important differences (and similarities) in the following sections, by examining some of the variety represented by the current family of evolutionary algorithms. These approaches in turn have inspired the development of additional evolutionary algorithms such as "classifier systems" <ref> (Holland, 1986) </ref>, the LS systems (Smith, 1983), "adaptive operator" systems (Davis, 1989), GENITOR (Whitley, 1989), SAMUEL (Grefenstette, 1989), "genetic programming" (de Garis, 1990; Koza, 1991), "messy GAs" (Goldberg, 1991), and the CHC approach (Eshelman, 1991). We will not attempt to survey this broad range of activities here.
Reference: <author> Janikow, C. </author> <title> (1991) Inductive learning of decision rules from attribute-based exam - ples: A knowledge-intensive genetic algorithm approach. </title> <institution> TR91-030, The University of North Carolina at Chapel Hill, Dept. of Computer Science, Chapel Hill, NC. </institution>
Reference: <author> Koza, J. R. </author> <title> (1991) Evolving a computer program to generate random numbers using the genetic programming paradigm. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> 37-44. </pages> <address> La Jolla, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Recently, however, the GA community has investigated more phenotypic representations, including vectors of real values (Davis, 1989), ordered lists (Whitley et al., 1989), neural networks (Harp et. al, 1991), and Lisp expressions <ref> (Koza, 1991) </ref>. For each of these representations, special mutation and recombination operators are introduced. The EP and ES communities are similar in this regard.
Reference: <author> Lipsitch, M. </author> <title> (1991) Adaptation on rugged landscapes generated by iterated local interactions of neighboring genes. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> 128-135. </pages> <address> La Jolla, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Manderick, B., de Weger, M., & Spiessens, P. </author> <title> (1991) The genetic algorithm and the structure of the fitness landscape. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> 143-149. </pages> <address> La Jolla, CA: </address> <publisher> Morgan Kaufmann. </publisher> <editor> Ma nner, R., & Manderick, B. </editor> <booktitle> (1992) Proceedings of the Second International Conference on Parallel Problem Solving from Nature, </booktitle> <address> Amsterdam: </address> <note> North Hol land. </note> <author> Mu hlenbein, H., & Schlierkamp-Voosen, D. </author> <title> (1993) The distributed breeder genetic algorithm. </title> <note> Submitted to the Journal of Evolutionary Computation. </note>
Reference-contexts: Although deceptiveness is one possibil - ity, other measures of problem difficulty are needed. One possibility is fitness correlation, which appears to be a measure of EA-Hardness that places less emphasis on optimality <ref> (Manderick et al., 1991) </ref>. Fitness correlation measures the correlation between the fitness of children and their parents. Manderick et al. found a strong relationship between GA performance and the strength of the correlations. Similarly, Lipsitch (1991) has also proposed examining fitness correlations. Another possibility is problem modality.
Reference: <author> Rechenberg, I. </author> <title> (1973) Evolutionsstrategie: Optimierung Technischer Systeme nach Prinzipien der Biologischen Evolution. </title> <publisher> Frommann-Holzboog, Stuttgart. </publisher>
Reference-contexts: For the sake of brevity we will not concentrate on this early work but will discuss in some detail three methodologies that have emerged in the last few decades: "evolutionary programming" (Fogel et al., 1966), "evolution strategies" <ref> (Rechenberg, 1973) </ref>, and "genetic algorithms" (Holland, 1975). Although similar at the highest level, each of these varieties implements an evolutionary algorithm in a different manner.
Reference: <editor> Schaffer, J. D., Eshelman, L. J. </editor> <title> (1991) On crossover as an evolutionarily viable stra tegy. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> 61-68. </pages> <address> La Jolla, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schaffer, J. D. & Morishima, A. </author> <title> (1987) An adaptive crossover distribution mechan - isms for genetic algorithms. </title> <booktitle> Proceedings of the Second International Conference on Genetic Algorithms, </booktitle> <pages> 36-40. </pages> <address> Cambridge, MA: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference: <author> Schraudolph, N. N., & Belew, R. K. </author> <title> (1992) Dynamic parameter encoding for genetic algorithms. </title> <journal> Machine Learning Journal, </journal> <volume> Volume 9, Number 1, </volume> <pages> 9-22. </pages>
Reference: <editor> Schwefel, H.-P. </editor> <booktitle> (1981) Numerical Optimization of Computer Models. </booktitle> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Shaefer, C. G. </author> <title> (1987) The ARGOT strategy: adaptive representation genetic optim izer technique. </title> <booktitle> Proceedings of the Second International Conference on Genetic Algorithms, </booktitle> <pages> 50-58. </pages> <address> Cambridge, MA: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: Also, very little has been done in the way of adaptive representations, with the exception of messy GAs (Goldberg, 1991), Argot <ref> (Shaefer, 1987) </ref>, the dynamic parameter encoding (DPE) scheme of Schraudolph and Belew (1992), and the Delta coding of Whitley et al. (1991). Messy GAs, Argot, DPE, and Delta coding all attempt to manipulate the granularity of the representation, thus focusing search at the appropriate level.
Reference: <author> Smith, S. </author> <title> (1983) Flexible learning of problem solving heuristics through adaptive search. </title> <booktitle> Proceedings of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 422-425. </pages> <address> Karlsruche, Germany: </address> <publisher> William Kaufmann. </publisher>
Reference-contexts: We will highlight the important differences (and similarities) in the following sections, by examining some of the variety represented by the current family of evolutionary algorithms. These approaches in turn have inspired the development of additional evolutionary algorithms such as "classifier systems" (Holland, 1986), the LS systems <ref> (Smith, 1983) </ref>, "adaptive operator" systems (Davis, 1989), GENITOR (Whitley, 1989), SAMUEL (Grefenstette, 1989), "genetic programming" (de Garis, 1990; Koza, 1991), "messy GAs" (Goldberg, 1991), and the CHC approach (Eshelman, 1991). We will not attempt to survey this broad range of activities here.
Reference: <author> Spears, W. M., and De Jong, K. </author> <title> A (1991) On the virtues of uniform crossover. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> 230-236. </pages> <address> La Jolla, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Spears, W. M. </author> <title> (1992a) Crossover or mutation? Proceedings of the Foundations of Genetic Algorithms Workshop, </title> <address> Vail, Colorado: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Spears, W. M. </author> <title> (1992b) Adapting crossover in a genetic algorithm. </title> <institution> Naval Research Laboratory AI Center Report AIC-92-025. </institution> <address> Washington, DC 20375 USA. </address>
Reference: <author> Vose, M. D., & Liepins, G. E. </author> <title> (1991) Schema disruption. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> 237-242. </pages> <address> La Jolla, CA: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Whitley, D. </author> <title> (1989) The GENITOR algorithm and selection pressure: why rank-based allocation of reproductive trials is best. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> 116-121. </pages> <address> Fairfax, VA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These approaches in turn have inspired the development of additional evolutionary algorithms such as "classifier systems" (Holland, 1986), the LS systems (Smith, 1983), "adaptive operator" systems (Davis, 1989), GENITOR <ref> (Whitley, 1989) </ref>, SAMUEL (Grefenstette, 1989), "genetic programming" (de Garis, 1990; Koza, 1991), "messy GAs" (Goldberg, 1991), and the CHC approach (Eshelman, 1991). We will not attempt to survey this broad range of activities here. <p> A variety of features are typically added to EAs in this context to improve both the speed and the precision of the results. Interested readers should review Davis' work on real-valued representations and adaptive operators (Davis, 1989), Whitley's GENITOR system incorporating ranking and "steady state" mechanisms <ref> (Whitley, 1989) </ref>, Goldberg's "messy GAs", that involve adaptive representations (Goldberg, 1991), and Eshelman's high-powered CHC algorithm (Eshelman, 1991). A second active area of application of EAs is in the design of robust rule learn - ing systems. <p> We can also see this as a more genotypic level of representation, since the individual is in some sense encoded in the bit string. Recently, however, the GA community has investigated more phenotypic representations, including vectors of real values (Davis, 1989), ordered lists <ref> (Whitley et al., 1989) </ref>, neural networks (Harp et. al, 1991), and Lisp expressions (Koza, 1991). For each of these representations, special mutation and recombination operators are introduced. The EP and ES communities are similar in this regard.
Reference: <author> Whitley, D., Starkweather, T., & Fuquay, D. </author> <title> (1989) Scheduling problems and travel - ing salesmen: the genetic edge recombination operator. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> 133-140. </pages> <address> Fairfax, VA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These approaches in turn have inspired the development of additional evolutionary algorithms such as "classifier systems" (Holland, 1986), the LS systems (Smith, 1983), "adaptive operator" systems (Davis, 1989), GENITOR <ref> (Whitley, 1989) </ref>, SAMUEL (Grefenstette, 1989), "genetic programming" (de Garis, 1990; Koza, 1991), "messy GAs" (Goldberg, 1991), and the CHC approach (Eshelman, 1991). We will not attempt to survey this broad range of activities here. <p> A variety of features are typically added to EAs in this context to improve both the speed and the precision of the results. Interested readers should review Davis' work on real-valued representations and adaptive operators (Davis, 1989), Whitley's GENITOR system incorporating ranking and "steady state" mechanisms <ref> (Whitley, 1989) </ref>, Goldberg's "messy GAs", that involve adaptive representations (Goldberg, 1991), and Eshelman's high-powered CHC algorithm (Eshelman, 1991). A second active area of application of EAs is in the design of robust rule learn - ing systems. <p> We can also see this as a more genotypic level of representation, since the individual is in some sense encoded in the bit string. Recently, however, the GA community has investigated more phenotypic representations, including vectors of real values (Davis, 1989), ordered lists <ref> (Whitley et al., 1989) </ref>, neural networks (Harp et. al, 1991), and Lisp expressions (Koza, 1991). For each of these representations, special mutation and recombination operators are introduced. The EP and ES communities are similar in this regard.
Reference: <author> Whitley, D., Mathias, K., & Fitzhorn, P. </author> <title> (1991) Delta coding: an iterative search strategy for genetic algorithms. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> 77-84. </pages> <address> La Jolla, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <editor> Whitley, D. (ed.) </editor> <booktitle> (1992) Proceedings of the Foundations of Genetic Algorithms Workshop. </booktitle> <address> Vail, </address> <publisher> CO: Morgan Kaufmann. </publisher>
References-found: 43

