URL: http://www.cs.huji.ac.il/~feit/parsched/p-96-9.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~feit/parsched/parsched96.html
Root-URL: http://www.cs.huji.ac.il
Email: mirong@cs.wisc.edu  
Title: Managing Checkpoints for Parallel Programs  
Author: Jim Pruyne and Miron Livny 
Address: fpruyne,  
Affiliation: Department of Computer Sciences University of Wisconsin-Madison  
Abstract: Checkpointing is a valuable tool for any scheduling system to have. With the ability to checkpoint, schedulers are not locked into a single allocation of resources to jobs, but instead can stop running jobs, and re-allocate resources with out sacrificing any completed computations. Checkpointing techniques are not new, but they have not been widely available on parallel platforms. We have implemented CoCheck, a system for check-pointing message passing parallel programs. Parallel programs tend to be large in terms of their aggregate memory utilization, so the size of their checkpoint is also large. Because of this, checkpoints must be handled carefully to avoid overloading the system when checkpoints take place. Today's distributed file systems do not handle this situation well. We therefore propose the use of checkpoint servers which are specifically designed to move checkpoints from the checkpointing process, across the interconnection network, and on to stable storage. A scheduling system can utilize numerous checkpoint servers in any configuration in order to provide good checkpointing performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. J. Litzkow, M. Livny, and M. W. </author> <title> Mutka, "Condor: A hunter of idle workstations," </title> <booktitle> in Proceedings of the 8th International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 104-111, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: One common use for check-pointing is to provide fault tolerance. Checkpoint-ing also allows the scheduler to re-allocate resources among both running and queued jobs without sacrificing any computations already performed. For example, Condor's <ref> [1] </ref> ability to checkpoint sequential programs has allowed it to effectively utilize the idle time of privately owned workstations for long running jobs.
Reference: [2] <author> M. Squillante, </author> <title> "On the benefits and limitations of dynamic partitioning in parallel computer systems," in Job Scheduling Strategies for Parallel Processing (D. </title> <editor> G. Feitelson and L. Rudolph, eds.), </editor> <volume> vol. </volume> <booktitle> 949 of Lecture notes in Compter Science, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: In this way, the time already invested in the job will be preserved. Another use for checkpointing in a parallel system is to perform dynamic partitioning which has been shown <ref> [2] </ref> to be more effective than static methods of scheduling parallel programs. In a dynamic partitioning scheme, the number of resources allocated to a job is changed while the job is running based on changes in load on the overall system.
Reference: [3] <author> K. M. Chandy and L. Lamport, </author> <title> "Distributed snapshots: Determining global states of distributed systems," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 3, </volume> <pages> pp. 63-75, </pages> <month> Feb. </month> <year> 1985. </year>
Reference-contexts: It is therefore important to perform checkpoint and restart operations as quickly as possible. Techniques for checkpointing of parallel and distributed programs have been understood for quite some time. For example, Chandy and Lamport proposed a "distributed snapshot" protocol in 1985 <ref> [3] </ref>. Thus far, however, implementations of these techniques for real parallel systems have been rare. Building on the theory and the experience gained doing checkpointing for single process jobs in Condor, we have developed a system for checkpointing message 1 passing parallel programs called CoCheck (for Consis--tent Checkpointing).
Reference: [4] <author> G. Stellner and J. Pruyne, </author> <title> "Resource management and checkpointing for PVM," </title> <booktitle> in Proceedings of the 2nd European Users' Group Meeting, </booktitle> <pages> pp. 131-136, </pages> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: Section 3 provides discussion of alternatives which led to the development of the checkpoint server, and how it may be used by a scheduler. This is followed by some practical experience with the overall system and some conclusions and thoughts on future work. 2 CoCheck CoCheck <ref> [4] </ref> is a freely available system for creating checkpoints of parallel programs which communicate via a Message Passing Environment (MPE). It has been developed via a collaboration between researchers at the Technical University of Munich and ourselves. The implementation available today works with PVM [5] on workstation clusters.
Reference: [5] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam, </author> <title> PVM: Parallel Virtual Machine A Users' Guid and Tutorial for Networked Parallel Computing. </title> <address> Cambridge, MA.: </address> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: It has been developed via a collaboration between researchers at the Technical University of Munich and ourselves. The implementation available today works with PVM <ref> [5] </ref> on workstation clusters. Work is ongoing in Munich to extend CoCheck to support MPI [6]. We started with a number of important design goals when developing CoCheck. The first goal of CoCheck was to remain portable.
Reference: [6] <author> G. Stellner, </author> <title> "CoCheck: Checkpointing and process migration for MPI," </title> <booktitle> in Proceedings of the International Parallel Processing Symposium, IEEE, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: It has been developed via a collaboration between researchers at the Technical University of Munich and ourselves. The implementation available today works with PVM [5] on workstation clusters. Work is ongoing in Munich to extend CoCheck to support MPI <ref> [6] </ref>. We started with a number of important design goals when developing CoCheck. The first goal of CoCheck was to remain portable. That is, although the first implementation was done on top of PVM, the concepts used in CoCheck should be applicable to any MPE.
Reference: [7] <author> M. J. Litzkow and M. Solomon, </author> <title> "Supporting checkpointing and process migration outside the Unix kernel," </title> <booktitle> in Proceedings of the Winter Usenix Conference, </booktitle> <address> (San Francisco, CA), </address> <year> 1992. </year>
Reference-contexts: The overlay library also implements the protocol to capture the network state which is described below. Single process checkpointing libraries have existed for quite some time. CoCheck utilizes the checkpoint-ing library which was developed as part of Condor <ref> [7, 8] </ref> which, among others, provides this functionality without any modifications to the operating system on which it runs.
Reference: [8] <author> T. Tannenbaum and M. Litzkow, </author> <title> "The Condor distributed processing system," </title> <journal> Dr. Dobb's Journal, </journal> <pages> pp. 40-48, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The overlay library also implements the protocol to capture the network state which is described below. Single process checkpointing libraries have existed for quite some time. CoCheck utilizes the checkpoint-ing library which was developed as part of Condor <ref> [7, 8] </ref> which, among others, provides this functionality without any modifications to the operating system on which it runs.
Reference: [9] <author> J. Pruyne and M. Livny, </author> <title> "Providing resource management services to parallel applications," </title> <booktitle> in Proceedings of the Second Workshop on Environments and Tools for Parallel Scientific Computing (J. </booktitle> <editor> Dongarra and B. Tourancheau, eds.), </editor> <booktitle> SIAM Proceedings Series, </booktitle> <pages> pp. 152-161, </pages> <publisher> SIAM, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: The final component of CoCheck, the resource manager process is the coordinator for the entire system. The RM process provided with CoCheck is an extension to the external RM process first designed for use with PVM <ref> [9] </ref>. This process receives requests for checkpointing services, and initiates the CoCheck protocol between itself and the overlay library of each the application processes to perform these services.
Reference: [10] <author> J. Pruyne and M. Livny, </author> <title> "Parallel processing on dynamic resources with CARMI," in Job Scheduling Strategies for Parallel Processing (D. </title> <editor> G. Fei-telson and L. Rudolph, eds.), </editor> <volume> vol. </volume> <booktitle> 949 of Lecture notes in Compter Science, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: The checkpoint and restart functions are asynchronous remote procedure calls against the resource manager process. As with CARMI <ref> [10] </ref>, they immediately return an integer request identifier. In this way, a process requesting a checkpoint need not block while the CoCheck protocol is running, and while the individual checkpoints are being stored. Processes may, though, include themselves in any of the set of processes defined by GeneralCkpt (). <p> The Computer Sciences department at the University of Wisconsin has around 200 desktop workstations most of which are available to the Condor resource management system for executing long running sequential applications. Each of these workstations is also available to users via CARMI <ref> [10] </ref> the resource management and parallel programming interface to Condor. Condor has always supported checkpointing of sequential applications, and CoCheck has recently been integrated with CARMI to provide checkpointing services to parallel applications. Because checkpoints in this environment are triggered by owners returning to their workstations, they occur relatively frequently.
Reference: [11] <author> R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, and B. Lyon, </author> <title> "Design and implementation of the Sun network file system," </title> <booktitle> in Proceedings of the Summer Usenix Conference, </booktitle> <pages> pp. 119-130, </pages> <year> 1985. </year>
Reference-contexts: Time spent checkpointing is time when useful computation is not taking place. The simplest, and perhaps most desirable method of storing a checkpoint of a parallel program is to simply use an existing distributed file system. Examples of these include the Network File System (NFS) <ref> [11] </ref> and the Andrew File System (AFS) [12]. Using these systems for storing checkpoints is quite attractive because it allows them to be stored in the same way as other files. The problem of where and how data is stored is handled by the file system.
Reference: [12] <author> J. H. Howard, M. L. Kazar, S. G. Menees, D. A. Nichols, M. Satyanarayanan, R. N. Sidebotham, and M. J. West, </author> <title> "Scale and performance in a distributed file system," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 6, </volume> <pages> pp. 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The simplest, and perhaps most desirable method of storing a checkpoint of a parallel program is to simply use an existing distributed file system. Examples of these include the Network File System (NFS) [11] and the Andrew File System (AFS) <ref> [12] </ref>. Using these systems for storing checkpoints is quite attractive because it allows them to be stored in the same way as other files. The problem of where and how data is stored is handled by the file system.
Reference: [13] <author> J. Gerner, </author> <title> "Input/output on the IBM SP2-an overview." </title> <address> http://www.tc.cornell.edu/ SmartNodes/Newsletters/IO.series/intro.html. </address> <month> 9 </month>
Reference-contexts: Practice has shown that AFS is not adequate for parallel systems. For example, the Cornell Theory Center recommends that AFS not be used when data transfers become large <ref> [13] </ref>. The caching scheme used by AFS is particularly poor at writing results such as checkpoint files.
References-found: 13

