URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/96/tr1310.ps.Z
Refering-URL: http://www.cs.wisc.edu/~arch/uwarch/tech_reports/tech_reports.html
Root-URL: 
Title: Trace Cache: a Low Latency Approach to High Bandwidth Instruction Fetching  
Author: Eric Rotenberg Steve Bennett Jim Smith 
Date: April 11, 1996  
Abstract: Superscalar processors require sufficient instruction fetch bandwidth to feed their highly parallel execution cores. Fetch bandwidth is determined by a number of factors, namely instruction cache hit rate, branch prediction accuracy, and taken branches in the instruction stream. Taken branches introduce the problem of noncontiguous instruction fetching: the dynamic instruction sequence exists in the cache, but the instructions are not in contiguous cache locations. This report considers the problem of fetching noncontiguous blocks of instructions in a single cycle. We propose the trace cache, a special instruction cache that captures dynamic instruction sequences. Each line in the trace cache stores a dynamic code sequence, which may contain one or more taken branches. Dynamic sequences are built up as the program executes. If a predicted dynamic sequence exists in the trace cache, it can be fed directly to the decoders. We investigate other methods for fetching noncontiguous instruction sequences in a single cycle. The Branch Address Cache [2] and Collapsing Buffer [1] achieve high bandwidth by feeding multiple noncontiguous fetch addresses to an interleaved cache and performing complex alignment on the instructions as they come out of the cache. Inevitably, this approach lengthens the critical path through the instruction fetch unit. Extra stages in the fetch pipeline increase branch mispredict recovery time, decreasing overall performance. Our approach moves complexity due to noncontiguous instruction fetching off the critical path and onto the fill side of the trace cache. We compare the performance of the trace cache against other fetch designs. We first consider simple instruction fetching mechanisms that predict only one branch at a time or fetch only up to the first taken branch. We also consider more aggressive methods that are able to fetch beyond multiple taken branches. For integer benchmarks, the trace cache improves performance on average by 34% over the fetch unit limited to one basic block per cycle, and 17% over the fetch unit limited to multiple contiguous basic blocks. The corresponding improvements for floating point benchmarks are 16% and 9%. Further, the trace cache consistently performs better than the other high bandwidth fetch mechanisms studied even if single-cycle fetch latency is assumed across all mechanisms. Simulations with more realistic latencies for the other high bandwidth approaches, based on pipeline stages before and after the instruction cache, show that the trace cache clearly outperforms other approaches: on average, 20% and 10% better than the next highest performer for integer and floating point benchmarks, respectively. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Conte, et al. </author> , <title> "Optimization of Instruction Fetch Mechanisms for High Issue Rates," </title> <booktitle> Proceedings of the International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Their approach hides multiple individual branch predictions within a single prediction; e.g. rather than make 2 branch predictions, make 1 prediction that selects from among 4 paths. This enables the use of more accurate two-level predictors. Another hardware scheme proposed by Conte, Mills, Menezes, and Patel <ref> [1] </ref> uses two passes through an interleaved branch target buffer. Each pass through the branch target buffer produces a fetch address, allowing two nonadjacent cache lines to be fetched. In addition, the interleaved branch target buffer enables detection of any number of branches in a cache line. <p> We then show how the core fetch unit is augmented with the trace cache. 2.1 Core Fetch Unit The core fetch unit is implemented using established hardware schemes. It is called interleaved sequential in <ref> [1] </ref>. Fetching up to the first predicted taken branch each cycle can be done using the combination of an accurate multiple branch predictor [2], an interleaved branch target buffer (BTB) [1][5], a return address stack (RAS) [13], and a 2-way interleaved instruction cache [1][7]. Refer to Figure 3. <p> The BTB must be n-way interleaved, where n is the number of instructions in a cache line. This is so that all instructions within a cache line can be checked for branches in parallel <ref> [1] </ref>. The BTB can detect other types of control transfer instructions as well. If a jump is detected, the jump address may be predicted. (Jump target predictions are not considered in this paper, however.) Return addresses can almost always be obtained with no penalty by using a call/return stack. <p> The alignment network must (1) 13 interchange the cache lines from numerous banks (with more than two banks, the permutations grow quickly), and (2) collapse the basic blocks together, eliminating unused intervening instructions. Though not discussed in [2], logic like the collapsing buffer <ref> [1] </ref> discussed in the next section will be needed to do this. 14 3.2 Collapsing Buffer The instruction fetch mechanism proposed by Conte, Mills, Menezes, Patel [1] is illustrated in buffer (BTB), (3) a multiple branch predictor, (4) special logic after the BTB, and (5) an interchange and alignment network featuring <p> Though not discussed in [2], logic like the collapsing buffer <ref> [1] </ref> discussed in the next section will be needed to do this. 14 3.2 Collapsing Buffer The instruction fetch mechanism proposed by Conte, Mills, Menezes, Patel [1] is illustrated in buffer (BTB), (3) a multiple branch predictor, (4) special logic after the BTB, and (5) an interchange and alignment network featuring a collapsing buffer. The hardware is similar to the core fetch unit of the trace cache (described in Section 3), but has two important distinctions. <p> Because the BTB is used in both the first and second stages, a new fetch cycle cannot be initiated until the third stage unless the BTB is dual-ported and the BTB logic is duplicated. The collapsing buffer takes only a single stage if implemented as a bus-based crossbar <ref> [1] </ref>. <p> One might expect that under the single-cycle fetch latency assumption, the three approaches would perform similarly. However, TC enjoys a noticeable lead over CB. This is most likely because the original collapsing buffer was not designed to handle backward taken intrablock branches <ref> [1] </ref>, whereas the TC can handle any arbitrary trace. The BAC performs worst of the three, and in some cases even performs below SEQ.3 particularly in floating point. There are two explanations for this behavior: * Instruction cache bank conflicts are the primary performance loss for BAC.
Reference: [2] <author> T-Y Yeh, D. Marr and Y. Patt, </author> <title> "Increasing the Instruction Fetch Rate via Multiple Branch Prediction and a Branch Address Cache," </title> <booktitle> Proceedings of the 7th ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: All of these attempt to fetch multiple, possibly noncontiguous basic blocks each cycle from the instruction cache. First, Yeh, Marr, and Patt <ref> [2] </ref> consider a fetch mechanism that provides high bandwidth by predicting multiple branch target addresses every cycle. The method features a Branch Address 4 Cache, a natural extension of the branch target buffer [5]. <p> It is called interleaved sequential in [1]. Fetching up to the first predicted taken branch each cycle can be done using the combination of an accurate multiple branch predictor <ref> [2] </ref>, an interleaved branch target buffer (BTB) [1][5], a return address stack (RAS) [13], and a 2-way interleaved instruction cache [1][7]. Refer to Figure 3. The core fetch unit is designed to fetch as many contiguous instructions possible, up to a maximum instruction limit and a maximum branch limit. <p> This predictor was chosen for its accuracy and because it is more easily extended to multiple branch predictions than other predictors which require address information <ref> [2] </ref>[4]. It is relatively straightforward to extend the single correlated branch predictor to multiple predictions each cycle, as proposed in [2]. An actual hardware implementation is shown in Figure 4. BTB logic combines the BTB hit information with the branch predictions to produce the next fetch address, and to generate trailing zeroes in the valid instruction bit vectors (if there is a predicted taken branch). <p> The analysis compares these mechanisms against the trace cache, with latency being the key point for comparison. 3.1 Branch Address Cache The branch address cache fetch mechanism proposed by Yeh, Marr, and Patt <ref> [2] </ref> is shown in Figure 7. There are four primary components: (1) a branch address cache (BAC), (2) a multiple branch predictor, (3) an interleaved instruction cache, and (4) an interchange and alignment network. <p> The alignment network must (1) 13 interchange the cache lines from numerous banks (with more than two banks, the permutations grow quickly), and (2) collapse the basic blocks together, eliminating unused intervening instructions. Though not discussed in <ref> [2] </ref>, logic like the collapsing buffer [1] discussed in the next section will be needed to do this. 14 3.2 Collapsing Buffer The instruction fetch mechanism proposed by Conte, Mills, Menezes, Patel [1] is illustrated in buffer (BTB), (3) a multiple branch predictor, (4) special logic after the BTB, and (5) <p> The BTB is a particularly important resource because it gets most of the base performance. A rough area estimate for the BAC is about 60 kB (464 bits per entry <ref> [2] </ref>). The results are split into two sets. The first set assumes all fetch units have a latency of 1 cycle, in order to demonstrate each mechanism's ability to deliver bandwidth performance.
Reference: [3] <author> M. Franklin and M. Smotherman, </author> <title> "A Fill-Unit Approach to Multiple Instruction Issue," </title> <booktitle> Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1994. </year>
Reference: [4] <author> S. Dutta and M. Franklin, </author> <title> "Control Flow Prediction with Tree-Like Subgraphs for Superscalar Processors," </title> <note> to appear Micro-28, </note> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: Similarly, a hit in the branch address cache combined with multiple branch predictions produces the starting addresses of the next several basic blocks. These addresses are fed into a highly interleaved instruction cache to fetch multiple basic blocks in a single cycle. A second study by Franklin and Dutta <ref> [4] </ref> uses a similar approach to the branch address cache (providing multiple branch targets), but with a new method for predicting multiple branches in a single cycle. <p> The fill unit, proposed by Melvin, Shebanow and Patt [17], caches RISC-like instructions which are derived from a CISC instruction stream. This predecoding eased the problem of supporting a complex instruction set such as VAX on the HPS restricted dataflow engine. Franklin and Smotherman <ref> [4] </ref> extended the fill unit's role to dynamically assemble VLIW-like instruction words from a RISC instruction stream, which are then stored in a shadow cache.
Reference: [5] <author> J. Lee and A. J. Smith, </author> <title> "Branch Prediction Strategies and Branch Target Buffer Design," </title> <booktitle> IEEE Computer, </booktitle> <month> January </month> <year> 1984. </year>
Reference-contexts: First, Yeh, Marr, and Patt [2] consider a fetch mechanism that provides high bandwidth by predicting multiple branch target addresses every cycle. The method features a Branch Address 4 Cache, a natural extension of the branch target buffer <ref> [5] </ref>. With a branch target buffer, a single branch prediction and a BTB hit produces the starting address of the next basic block. Similarly, a hit in the branch address cache combined with multiple branch predictions produces the starting addresses of the next several basic blocks.
Reference: [6] <author> J. Losq, </author> <title> "Generalized History Table for Branch Prediction," </title> <journal> IBM Technical Disclosure Bulletin, </journal> <month> June </month> <year> 1982. </year>
Reference: [7] <author> G. F. Grohoski, </author> <title> "Machine Organization of the IBM RISC System/6000 processor," </title> <journal> IBM Journal of Research and Development, </journal> <month> January </month> <year> 1990. </year>
Reference-contexts: a fetch limit of 16 instructions and 3 branches is used throughout. 7 The cache is interleaved so that 2 consecutive cache lines can be accessed; this allows fetching sequential code that spans a cache line boundary, always guaranteeing a full cache line or up to the first taken branch <ref> [7] </ref>. This scheme requires minimal complexity for aligning instructions: (1) logic to swap the order of the two cache lines (interchange switch), (2) a left-shifter to align the instructions into a 16-wide instruction latch, and (3) logic to mask off unused instructions.
Reference: [8] <author> S-T Pan, K. So, and J. T. Rahmeh, </author> <title> "Improving the Accuracy of Dynamic Branch Prediction Using Branch Correlation," </title> <booktitle> Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1992. </year>
Reference: [9] <author> T-Y Yeh and Y. N. Patt, </author> <title> "Alternative Implementations of Two-Level Adaptive Branch Prediction," </title> <booktitle> Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference: [10] <author> T-Y Yeh, </author> <title> "Two-level Adaptive Branch Prediction and Instruction Fetch Mechanisms for High-Performance Superscalar Processors," </title> <type> PhD Thesis, </type> <institution> Department of Electrical Engineering and Computer Science, University of Michigan, </institution> <year> 1993. </year>
Reference-contexts: While storing counters with each branch achieves multiple branch prediction trivially, branch prediction accuracy is limited. Branch prediction is fundamental to ILP, and should have precedence over other factors. For high branch prediction accuracy, we use a 4kB GAg (14) correlated branch predictor <ref> [10] </ref>. The 14 bit global branch history register indexes into a single pattern history table. This predictor was chosen for its accuracy and because it is more easily extended to multiple branch predictions than other predictors which require address information [2][4].
Reference: [11] <author> T-Y Yeh and Y. N. Patt, </author> <title> "A Comprehensive Instruction Fetch Mechanism for a Processor Supporting Speculative Execution," </title> . 
Reference: [12] <author> E. Hao, P. Chang and Y. Patt, </author> <title> "The Effect of Speculatively Updating Branch History on Branch Prediction Accuracy, Revisited," </title> <booktitle> Proceedings of the 27th annual International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1994. </year>
Reference: [13] <author> D. Kaeli and P. Emma, </author> <title> "Branch History Table Prediction of Moving Target Branches Due to Subroutine Returns," </title> <booktitle> Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: It is called interleaved sequential in [1]. Fetching up to the first predicted taken branch each cycle can be done using the combination of an accurate multiple branch predictor [2], an interleaved branch target buffer (BTB) [1][5], a return address stack (RAS) <ref> [13] </ref>, and a 2-way interleaved instruction cache [1][7]. Refer to Figure 3. The core fetch unit is designed to fetch as many contiguous instructions possible, up to a maximum instruction limit and a maximum branch limit.
Reference: [14] <author> D. Wall, </author> <title> "Limits of Instruction-Level Parallelism," </title> <booktitle> International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991. </year>
Reference: [15] <author> L. Gwennap, </author> <title> "MIPS R10000 Uses Decoupled Architecture," </title> <type> Microprocessor Report, </type> <month> October </month> <year> 1994. </year>
Reference-contexts: Table 2: Operation execution latencies. Note that for loads and stores, data cache misses are not simulated. Floating point latencies for the most common operations are similar to the MIPS R10000 <ref> [15] </ref>. error. For example, cache structures do not see the pollution effects caused by fetching and executing instructions down the wrong path.
Reference: [16] <author> J. Larus, </author> <title> "Efficient Program Tracing," </title> <booktitle> IEEE Computer, </booktitle> <month> May </month> <year> 1993. </year> <month> 30 </month>
Reference-contexts: The benchmarks were compiled on a Sun SPARCstation 10/30 using "gcc -O4 -static -fschedule-insns -fschedule-insns2" for integer benchmarks and "f77 -O4 -Bstatic -fast -cg89" for floating point benchmarks. SPARC instruction traces were generated using the Quick Profiler and Tracer (QPT) <ref> [16] </ref> and then fed into the trace-driven processor simulator. Table 3 shows inputs for each of the benchmarks. Benchmarks were simulated for 100 million instructions. The foremost problem with trace-driven simulation is that incorrect speculative execution cannot be simulated, since traces represent only the correct path of execution.
Reference: [17] <author> S. Melvin, M. Shebanow and Y. Patt, </author> <title> "Hardware Support for Large Atomic Units in Dy--namically Scheduled Machines," </title> <booktitle> Proceedings of the 21st Annual International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1988. </year>
Reference-contexts: The work also proposes compiler techniques to reduce the frequency of taken branches. Two previously proposed hardware structures are similar to the trace cache but exist in different applications. The fill unit, proposed by Melvin, Shebanow and Patt <ref> [17] </ref>, caches RISC-like instructions which are derived from a CISC instruction stream. This predecoding eased the problem of supporting a complex instruction set such as VAX on the HPS restricted dataflow engine.
Reference: [18] <author> J. E. Smith, </author> <title> "A Study of Branch Prediction Strategies," </title> <booktitle> Proceedings of the 8th Symposium on computer Architecture, </booktitle> <month> May </month> <year> 1981. </year>
Reference: [19] <author> J. E. Smith and G. S. Sohi, </author> <title> "The Microarchitecture of Superscalar Processors," </title> <booktitle> Proceedings IEEE, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: The instruction execution engine is the "consumer" which removes instructions from the buffer and executes them, subject to data dependence and resource constraints. Control dependences (branches and jumps) provide a feedback mechanism between the producer and consumer. Processors having this organization employ aggressive techniques to exploit instruction-level parallelism <ref> [19] </ref>. Wide dispatch and issue paths place an upper bound on peak instruction throughput. Large issue buffers are used to maintain a window of instructions necessary for detecting parallelism, and a large pool of physical registers provides destinations for all of the in-flight instructions issued from the window.
Reference: [20] <author> N. Jouppi, </author> <title> "Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers," </title> <booktitle> Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year> <month> 31 </month>
Reference-contexts: cache hit rates, the design could use a small buffer to store recent traces; a trace in this buffer is only committed to the trace cache after one or more hits to that trace. * victim trace cache: An alternative to judicious trace selection is to use a victim cache <ref> [20] </ref>. A victim trace cache may keep valuable traces from being permanently displaced by useless traces. 12 3 Other High Bandwidth Fetch Mechanisms In this section we analyze the organization of two previously proposed fetch mechanisms aimed at fetching and aligning multiple noncontiguous basic blocks each cycle.
References-found: 20

