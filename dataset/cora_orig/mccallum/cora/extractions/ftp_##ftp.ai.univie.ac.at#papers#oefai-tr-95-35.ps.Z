URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-95-35.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/biblio_ora?sort_by_author=yes&tailor=1&loc=0&format=ml/ml&keyword=Publications&keyword=WWW_ML&relop=/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: stefan@ai.univie.ac.at  
Title: Structural Regression Trees  
Author: Stefan Kramer 
Keyword: Inductive Learning, Inductive Logic Programming, Regression  
Address: Schottengasse 3, A-1010 Vienna, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: In many real-world domains the task of machine learning algorithms is to learn a theory predicting numerical values. In particular several standard test domains used in Inductive Logic Programming (ILP) are concerned with predicting numerical values from examples and relational and mostly non-determinate background knowledge. However, so far no ILP algorithm except one can predict numbers and cope with non-determinate background knowledge. (The only exception is a covering algorithm called FORS.) In this paper we present Structural Regression Trees (SRT), a new algorithm which can be applied to the above class of problems by integrating the statistical method of regression trees into ILP. SRT constructs a tree containing a literal (an atomic formula or its negation) or a conjunction of literals in each node, and assigns a numerical value to each leaf. SRT provides more comprehensible results than purely statistical methods, and can be applied to a class of problems most other ILP systems cannot handle. Experiments in several real-world domains demonstrate that the approach is competitive with existing methods, indicating that the advantages are not at the expense of predictive accuracy. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Bisson, </author> <title> `Learning in FOL with a similarity measure', </title> <booktitle> in Proc. Tenth National Conference on Artificial Intelligence (AAAI-92), </booktitle> <year> (1992). </year>
Reference-contexts: This way of detecting and handling outliers adds an instance-based aspect to SRT. However, it is just an additional possibility, and can be turned off by means of the minimum similarity parameter. 2 <ref> [1] </ref> defined a similarity measure for first-order logic, but it measures the similarity of two tuples in a relation, not of two "relational structures". 6 4 Experimental Results A common step in pharmaceutical development is forming a quantitative structure-activity relationship (QSAR) that relates the structure of a compound to its biological
Reference: [2] <author> H. Bostrom, </author> <title> `Covering vs. Divide-and-Conquer for Top-Down Induction of logic programs', </title> <booktitle> in Proc. Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pp. 1194-1200, </pages> <address> San Mateo, CA, (1995). </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: We therefore think that a more generic solution would make the application of the method easier. One of the current limitations of the approach is that only constants are 6 For a good discussion of tree-based vs. covering algorithms in ILP we have to refer to <ref> [2] </ref>. For a comparison of tree induction and rule induction in propositional regression see [30]. 10 assigned to the leaves, not linear models as in [14] and [21].
Reference: [3] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J Stone, </author> <title> Classification and Regression Trees, </title> <booktitle> The Wadsworth Statistics/Probability Series, Wadsworth International Group, </booktitle> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: Thirdly, regression models do not allow for easy utilization of domain knowledge. The only way to include knowledge is to "engineer" features, and to map these symbolic features to real 1 valued features. In order to solve some of these problems, regression tree methods (CART <ref> [3] </ref>, RETIS [14], M5 [21]) have been developed. Regression trees are supposed to be more comprehensible than traditional regression models. Furthermore, regression trees by definition partition the instance space, so the features may be of different importance for different parts of the space. <p> SRT can be viewed as integrating the statistical method of regression trees <ref> [3] </ref> into ILP. SRT can be applied to a class of problems no ILP systems except FORS can handle, and provides more comprehensible results than purely statistical methods. The main difference between SRT and FORS is that it is a tree-based and not a covering algorithm. <p> We are planning to extend the algorithm such that parts of the tree of one iteration can be reused in the next iteration. We also plan to compare our way of coverage-based prepruning and tree selection by MDL with more traditional pruning methods a la CART <ref> [3] </ref>. Besides, we addressed the problem of non-determinate literals. We adopted and generalized solutions for this problem, but they involve the tiresome task of writing a new specification of admissible literals and conjunctions for each domain.
Reference: [4] <author> P. Cheeseman, </author> <title> `On finding the most probable model', in Computational Models of Discovery and Theory Formation, </title> <editor> eds., J. Shrager and P. Langley, </editor> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> (1990). </year>
Reference-contexts: principle [24], and will be discussed in the next section. 4 3.2 Tree Selection by MDL The MDL principle tries to measure both the simplicity and the accuracy of a particular theory in a common currency, namely in terms of the number of bits needed for encoding theory and data. <ref> [4] </ref> defines the message length of a theory (called model in his article) as: Total message length = Message length to describe the model + Message length to describe the data, given the model.
Reference: [5] <author> W.W. Cohen, </author> <title> `Grammatically biased learning: Learning logic programs using an explicit antecedent description language', </title> <journal> Artificial Intelligence, </journal> <volume> 68(2), </volume> <year> (1994). </year>
Reference-contexts: Furthermore, the user can constrain the set of possible literals depending 5 on the body of the clause so far. The conditions on the body are arbitrary Prolog clauses, and therefore the user has even more possibilities to define a language than by Antecedent Description Grammars (ADGs) <ref> [5] </ref>. To further reduce the number of possibilities, the set of literals and conjunctions is also constrained by modes, types of variables, and variable symmetries. 3.4 Outlier Detection by Analogy Test instances that are outliers strongly deteriorate the average performance of learned regression models.
Reference: [6] <author> B. Dolsak, I. Bratko, and A. Jezernik, </author> <title> `Finite element mesh design: An engineering domain for ILP application', </title> <booktitle> in Proceedings of the Fourth International Workshop on Inductive Logic Programming (ILP-94), GMD-Studien Nr. </booktitle> <volume> 237, </volume> <pages> pp. 305-320, </pages> <year> (1994). </year>
Reference-contexts: 1 Introduction Many real-world machine learning domains involve the prediction of a numerical value. In particular several test domains used in Inductive Logic Programming (ILP) (including the Mesh data sets <ref> [6] </ref> and the problem of learning quantitative structure-activity relations (QSAR) [12] [13]) are concerned with the prediction of numerical values from examples and relational background knowledge. <p> Furthermore, we performed experiments in the domain of finite element mesh design (for details see <ref> [6] </ref>), where the background knowledge is non-determinate. Table 5 shows the results of SRT for the mesh dataset together with the results of FOSSIL [11] and results of other methods that were directly taken from [15].
Reference: [7] <author> S. Dzeroski, </author> <title> Numerical Constraints and Learnability in Inductive Logic Programming, </title> <type> Ph.D. dissertation, </type> <institution> University of Ljubljana, Ljubljana, Slovenija, </institution> <year> 1995. </year>
Reference-contexts: This kind of learning problem is called Relational Regression in <ref> [7] </ref>, and can be formulated in the "normal" ILP framework (i.e., it is not part of the non-monotonic ILP framework which includes the closed-world assumption). Nevertheless, Relational Regression differs from other ILP learning tasks in that there are no negative examples.
Reference: [8] <author> S. Dzeroski and I. Bratko, </author> <title> `Handling noise in Inductive Logic Programming', </title> <booktitle> in Proceedings of the International Workshop on Inductive Logic Programming, </booktitle> <address> Tokyo, Japan, </address> <year> (1992). </year> <month> 11 </month>
Reference-contexts: Table 5 shows the results of SRT for the mesh dataset together with the results of FOSSIL [11] and results of other methods that were directly taken from [15]. SRT performs better than FOIL [20] and mFOIL <ref> [8] </ref>, but worse than the other methods. However, statistical analysis shows that only the differences between FOIL and the other algorithms are significant. 5 Struct.
Reference: [9] <author> S. Dzeroski and B. Kompare, </author> <year> 1995. </year> <type> Personal Communication. </type>
Reference-contexts: Summing up, the experiments showed that also in this domain SRT is competitive, although the differences between SRT and the rest are not statistically significant. Finally, we applied SRT to a domain where we are trying to predict the half-rate of surface water aerobic aqueous biodegradation in hours <ref> [9] </ref>. To simplify the learning task, we discretized this quantity and mapped it to f1; 2; 3; 4g. The background knowledge is non-determinate, and except for the molecular weight there are no "global" features available. The dataset contains 62 chemicals, and we performed 6-fold cross-validation in our tests.
Reference: [10] <author> S. Dzeroski, L. Todoroski, and T. Urbancic, </author> <title> `Handling real numbers in inductive logic programming: A step towards better behavioural clones', in Machine Learning: </title> <editor> ECML-95, eds., N. Lavrac and S. </editor> <booktitle> Wrobel, </booktitle> <pages> pp. 283-286, </pages> <address> Berlin Heidelberg New York, (1995). </address> <publisher> Springer. </publisher>
Reference-contexts: The other approach, a combination of DINUS [16] and RETIS [14], transforms the learning problem into a propositional language, and subsequently applies a regression tree algorithm to the transformed problem <ref> [10] </ref>. The transformation, however, does not work for non-determinate background knowledge, which is a strict limitation of the approach. In this paper we present Structural Regression Trees (SRT), a new algorithm for predicting numerical values from examples and relational background knowledge.
Reference: [11] <author> J. Furnkranz, `Fossil: </author> <title> A robust relational learner', in Machine Learning: </title> <editor> ECML-94, eds., F. Bergadano and L. </editor> <publisher> De Raedt, </publisher> <pages> pp. 122-137, </pages> <address> Berlin Heidel-berg New York, (1994). </address> <publisher> Springer. </publisher>
Reference-contexts: Furthermore, we performed experiments in the domain of finite element mesh design (for details see [6]), where the background knowledge is non-determinate. Table 5 shows the results of SRT for the mesh dataset together with the results of FOSSIL <ref> [11] </ref> and results of other methods that were directly taken from [15]. SRT performs better than FOIL [20] and mFOIL [8], but worse than the other methods. However, statistical analysis shows that only the differences between FOIL and the other algorithms are significant. 5 Struct.
Reference: [12] <author> J.D. Hirst, R.D. King, and M.J.E. Sternberg, </author> <title> `Quantitative structure-activity relationships by neural networks and inductive logic programming. the inhibition of dihydrofolate reductase by pyrimidines', </title> <journal> Journal of Computer-Aided Molecular Design, </journal> <volume> 8, </volume> <pages> 405-420, </pages> <year> (1994). </year>
Reference-contexts: 1 Introduction Many real-world machine learning domains involve the prediction of a numerical value. In particular several test domains used in Inductive Logic Programming (ILP) (including the Mesh data sets [6] and the problem of learning quantitative structure-activity relations (QSAR) <ref> [12] </ref> [13]) are concerned with the prediction of numerical values from examples and relational background knowledge. <p> Two QSAR domains, namely the inhibition of Es-cherichia coli dihydrofolate reductase (DHFR) by pyrimidines <ref> [12] </ref> and by tri-azines [13] have been used to test SRT. The pyrimidine dataset consists of 2198 background facts and 55 instances (compounds), which are partitioned into 5 cross-validation sets. For the tri-azines, the background knowledge are 2933 facts, and 186 instances (compounds) are used to perform 6-fold cross validation. <p> Hirst et al. made comprehensive comparisons of several methods in these domains, but they concluded there is no statistically significant difference between these methods. For the experiments we set minimum similarity = 0:75. Table 3 shows the results of the methods compared in <ref> [12] </ref> and in [13], and the results of SRT. The table summarizes the test set performances in both domains as measured by the Spearman rank correlation coefficients.
Reference: [13] <author> J.D. Hirst, R.D. King, and M.J.E. Sternberg, </author> <title> `Quantitative structure-activity relationships by neural networks and inductive logic programming: The inhibition of dihydrofolate reductase by triazines', </title> <journal> Journal of Computer-Aided Molecular Design, </journal> <volume> 8, </volume> <pages> 421-432, </pages> <year> (1994). </year>
Reference-contexts: 1 Introduction Many real-world machine learning domains involve the prediction of a numerical value. In particular several test domains used in Inductive Logic Programming (ILP) (including the Mesh data sets [6] and the problem of learning quantitative structure-activity relations (QSAR) [12] <ref> [13] </ref>) are concerned with the prediction of numerical values from examples and relational background knowledge. This kind of learning problem is called Relational Regression in [7], and can be formulated in the "normal" ILP framework (i.e., it is not part of the non-monotonic ILP framework which includes the closed-world assumption). <p> Two QSAR domains, namely the inhibition of Es-cherichia coli dihydrofolate reductase (DHFR) by pyrimidines [12] and by tri-azines <ref> [13] </ref> have been used to test SRT. The pyrimidine dataset consists of 2198 background facts and 55 instances (compounds), which are partitioned into 5 cross-validation sets. For the tri-azines, the background knowledge are 2933 facts, and 186 instances (compounds) are used to perform 6-fold cross validation. <p> Hirst et al. made comprehensive comparisons of several methods in these domains, but they concluded there is no statistically significant difference between these methods. For the experiments we set minimum similarity = 0:75. Table 3 shows the results of the methods compared in [12] and in <ref> [13] </ref>, and the results of SRT. The table summarizes the test set performances in both domains as measured by the Spearman rank correlation coefficients.
Reference: [14] <author> A. Karalic, </author> <title> `Employing linear regression in regression tree leaves', </title> <booktitle> in Proc. Tenth European Conference on Artificial Intelligence (ECAI-92), </booktitle> <editor> ed., B. </editor> <publisher> Neumann, </publisher> <pages> pp. 440-441, </pages> <address> Chichester, UK, (1992). </address> <publisher> Wiley. </publisher>
Reference-contexts: Nevertheless, Relational Regression differs from other ILP learning tasks in that there are no negative examples. So far, two methods have been applied to this problem: FORS [15] builds a first-order theory by a covering algorithm. The other approach, a combination of DINUS [16] and RETIS <ref> [14] </ref>, transforms the learning problem into a propositional language, and subsequently applies a regression tree algorithm to the transformed problem [10]. The transformation, however, does not work for non-determinate background knowledge, which is a strict limitation of the approach. <p> Thirdly, regression models do not allow for easy utilization of domain knowledge. The only way to include knowledge is to "engineer" features, and to map these symbolic features to real 1 valued features. In order to solve some of these problems, regression tree methods (CART [3], RETIS <ref> [14] </ref>, M5 [21]) have been developed. Regression trees are supposed to be more comprehensible than traditional regression models. Furthermore, regression trees by definition partition the instance space, so the features may be of different importance for different parts of the space. <p> For a comparison of tree induction and rule induction in propositional regression see [30]. 10 assigned to the leaves, not linear models as in <ref> [14] </ref> and [21]. Since it could help to build more accurate models, one of the next steps will be to assign linear regression models to the leaves. Acknowledgements I would like to thank Johannes Furnkranz, Bernhard Pfahringer and Gerhard Widmer for valuable discussions.
Reference: [15] <author> A. Karalic, </author> <title> First Order Regression, </title> <type> Ph.D. dissertation, </type> <institution> University of Ljubl-jana, Ljubljana, Slovenija, </institution> <year> 1995. </year>
Reference-contexts: Nevertheless, Relational Regression differs from other ILP learning tasks in that there are no negative examples. So far, two methods have been applied to this problem: FORS <ref> [15] </ref> builds a first-order theory by a covering algorithm. The other approach, a combination of DINUS [16] and RETIS [14], transforms the learning problem into a propositional language, and subsequently applies a regression tree algorithm to the transformed problem [10]. <p> Table 5 shows the results of SRT for the mesh dataset together with the results of FOSSIL [11] and results of other methods that were directly taken from <ref> [15] </ref>. SRT performs better than FOIL [20] and mFOIL [8], but worse than the other methods. However, statistical analysis shows that only the differences between FOIL and the other algorithms are significant. 5 Struct. <p> This domain involves non-determinate background knowledge, too. In Table 6 we compiled results from <ref> [15] </ref> and [27], and filled the result of SRT.
Reference: [16] <author> N. Lavrac and S. Dzeroski, </author> <title> Inductive Logic Programming, </title> <publisher> Ellis Horwood, </publisher> <address> Chichester, UK, </address> <year> 1994. </year>
Reference-contexts: Nevertheless, Relational Regression differs from other ILP learning tasks in that there are no negative examples. So far, two methods have been applied to this problem: FORS [15] builds a first-order theory by a covering algorithm. The other approach, a combination of DINUS <ref> [16] </ref> and RETIS [14], transforms the learning problem into a propositional language, and subsequently applies a regression tree algorithm to the transformed problem [10]. The transformation, however, does not work for non-determinate background knowledge, which is a strict limitation of the approach.
Reference: [17] <author> M. </author> <title> Manago, `Knowledge-intensive induction', </title> <booktitle> in Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <editor> ed., </editor> <booktitle> A.M. Segre, </booktitle> <pages> pp. 151-155. </pages> <publisher> Morgan Kaufman, </publisher> <year> (1989). </year>
Reference-contexts: The regression tree resulting from the growing phase is usually bigger than a classification tree, since it takes more nodes to achieve pure leaves. Manago's KATE <ref> [17] </ref> learns decision trees from examples represented in a frame-based language that is equivalent to first-order predicate calculus. KATE makes extensive use of a given hierarchy and heuristics to generate the branch tests. To our knowledge, KATE was the first system to induce first-order theories in a divide-and-conquer fashion.
Reference: [18] <author> S. Muggleton and C. Feng, </author> <title> `Efficient induction of logic programs', in Inductive Logic Programming, </title> <editor> ed., S. Muggleton, </editor> <address> 281-298, </address> <publisher> Academic Press, </publisher> <address> London, U.K., </address> <year> (1992). </year>
Reference-contexts: The only reason why Hirst et al. use the Spearman rank correlation coefficient instead of, say, the average error is to compare GOLEM <ref> [18] </ref> (which cannot predict numbers) with other methods. 3 For the pyrimidines, SRT performs better than other methods, but the improvement is not statistically significant.
Reference: [19] <author> B. Pfahringer and S. Kramer, </author> <title> `Compression-based evaluation of partial determinations', </title> <booktitle> in Proceedings of the First International Conference on Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press, </publisher> <year> (1995). </year>
Reference-contexts: For a single node, we encode the choice from all possible literals, so that the encoding considers predicates as well as all possible variabilizations of the predicates. We chose MDL instead of cross-validation, since it is computationally less expensive, and it can be used for pruning in search <ref> [19] </ref>. However, we are planning to compare both methods for model selection in the future. 3.3 Non-Determinate Background Knowledge Literals are non-determinate if they introduce new variables that can be bound in several alternative ways.
Reference: [20] <author> J.R. Quinlan, </author> <title> `Learning logical definitions from relations', </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266, </pages> <year> (1990). </year>
Reference-contexts: Table 5 shows the results of SRT for the mesh dataset together with the results of FOSSIL [11] and results of other methods that were directly taken from [15]. SRT performs better than FOIL <ref> [20] </ref> and mFOIL [8], but worse than the other methods. However, statistical analysis shows that only the differences between FOIL and the other algorithms are significant. 5 Struct.
Reference: [21] <author> J.R. Quinlan, </author> <title> `Learning with continuous classes', </title> <booktitle> in Proceedings AI'92, </booktitle> <publisher> ed., Sterling Adams, </publisher> <pages> pp. 343-348, </pages> <address> Singapore, (1992). </address> <publisher> World Scientific. </publisher>
Reference-contexts: Thirdly, regression models do not allow for easy utilization of domain knowledge. The only way to include knowledge is to "engineer" features, and to map these symbolic features to real 1 valued features. In order to solve some of these problems, regression tree methods (CART [3], RETIS [14], M5 <ref> [21] </ref>) have been developed. Regression trees are supposed to be more comprehensible than traditional regression models. Furthermore, regression trees by definition partition the instance space, so the features may be of different importance for different parts of the space. <p> For a comparison of tree induction and rule induction in propositional regression see [30]. 10 assigned to the leaves, not linear models as in [14] and <ref> [21] </ref>. Since it could help to build more accurate models, one of the next steps will be to assign linear regression models to the leaves. Acknowledgements I would like to thank Johannes Furnkranz, Bernhard Pfahringer and Gerhard Widmer for valuable discussions.
Reference: [22] <author> J.R. Quinlan, C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: In such a way, we try to avoid overfitting the data in the presence of noise. For the construction of a single tree, SRT uses the same method as used for the usual top-down induction of decision trees <ref> [22] </ref>. The algorithm recursively builds a binary tree, selecting a literal or a conjunction of literals (as defined by user-defined schemata [26]) in each node of the tree until a stopping criterion is 2 fulfilled.
Reference: [23] <author> J.R. Quinlan, </author> <title> `A case study in machine learning', </title> <booktitle> in Proceedings ACSC-16 Sixteenth Australian Computer Science Conference, </booktitle> <year> (1993). </year>
Reference-contexts: For the triazine dataset, SRT performs quite well, but again the differences are not statistically significant. 4 Since the Spearman rank correlation coefficient does not measure the quantitative error of a prediction, we included several other measures as proposed by Quinlan <ref> [23] </ref>. Clearly, these measures have disadvantages, too, but they represent interesting aspects of how well a theory works for a given test set. Unfortunately, we do not yet have a full comparison with other methods that are capable of predicting numbers.
Reference: [24] <author> J. Rissanen, </author> <title> `Modeling by shortest data description', </title> <journal> Automatica, </journal> <volume> 14, </volume> <pages> 465-471, </pages> <year> (1978). </year>
Reference-contexts: More precisely, SRT generates a series of increasingly complex trees, and subsequently returns one of the generated trees according to a preference criterion. SRT's preference criterion is based on minimum description length (MDL) principle <ref> [24] </ref>. In such a way, we try to avoid overfitting the data in the presence of noise. For the construction of a single tree, SRT uses the same method as used for the usual top-down induction of decision trees [22]. <p> So we choose this value as the next minimum coverage. Finally, SRT returns the one tree from this series that obtains the best compression of the data. The compression measure is based on the minimum description length (MDL) principle <ref> [24] </ref>, and will be discussed in the next section. 4 3.2 Tree Selection by MDL The MDL principle tries to measure both the simplicity and the accuracy of a particular theory in a common currency, namely in terms of the number of bits needed for encoding theory and data. [4] defines
Reference: [25] <author> J. Rissanen, </author> <title> `Stochastic complexity and modeling', </title> <journal> The Annals of Statistics, </journal> <volume> 14(3), </volume> <pages> 1080-1100, </pages> <year> (1986). </year>
Reference-contexts: In our coding scheme, we turn them into integers by multiplication and rounding. The factor is the minimum integer that still allows to discern the values in the training data after rounding. Subsequently, the integers are encoded by the universal prior of integers (UPI) <ref> [25] </ref> | in this way the coding length of the numbers roughly corresponds to their magnitude. The encoding of the tree is simply the encoding of the choices made (for the respective literals) as the tree is built.
Reference: [26] <author> G. Silverstein and M.J. Pazzani, </author> <title> `Relational cliches: Constraining constructive induction during relational learning', </title> <booktitle> in Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <editor> eds., L.A. Birnbaum and G.C. </editor> <publisher> Collins, </publisher> <pages> pp. 203-207, </pages> <address> San Mateo, CA, (1991). </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For the construction of a single tree, SRT uses the same method as used for the usual top-down induction of decision trees [22]. The algorithm recursively builds a binary tree, selecting a literal or a conjunction of literals (as defined by user-defined schemata <ref> [26] </ref>) in each node of the tree until a stopping criterion is 2 fulfilled. With each selected literal or conjunction, the examples covered by a node are further partitioned, depending on the success or failure of the literal (s) on the example. <p> In SRT, the user has to specify which literal (s) may be used to extend a clause. Firstly, the user can define conjunctions of literals that are used for a limited look-ahead. (These user-defined schemata are similar to relational cliches <ref> [26] </ref>). Furthermore, the user can constrain the set of possible literals depending 5 on the body of the clause so far. The conditions on the body are arbitrary Prolog clauses, and therefore the user has even more possibilities to define a language than by Antecedent Description Grammars (ADGs) [5].
Reference: [27] <author> A. Srinivasan, S. Muggleton, and R.D. King, </author> <title> `Comparing the use of background knowledge by Inductive Logic Programming systems', </title> <booktitle> in Proceedings of the 5th International Workshop on Inductive Logic Programming (ILP-95), </booktitle> <pages> pp. 199-230. </pages> <institution> Katholieke Universiteit Leuven, </institution> <year> (1995). </year>
Reference-contexts: (For FOIL/mFOIL the paired t-test is highly significant, even with Bonferroni adjustment.) For FOIL/FOSSIL, only the t-test shows significance. 8 We also applied SRT to the biological problem of learning to predict the mutagenic activity of a chemical, i.e., if it is harmful to DNA. (For details see [28] and <ref> [27] </ref>). This domain involves non-determinate background knowledge, too. In Table 6 we compiled results from [15] and [27], and filled the result of SRT. <p> shows significance. 8 We also applied SRT to the biological problem of learning to predict the mutagenic activity of a chemical, i.e., if it is harmful to DNA. (For details see [28] and <ref> [27] </ref>). This domain involves non-determinate background knowledge, too. In Table 6 we compiled results from [15] and [27], and filled the result of SRT.
Reference: [28] <author> A. Srinivasan, S. Muggleton, R.D. King, and M. Sternberg, `Mutagenesis: </author> <title> ILP experiments in a non-determinate biological domain', </title> <booktitle> in Proceedings of the Fourth International Workshop on Inductive Logic Programming (ILP-94), GMD-Studien Nr. </booktitle> <volume> 237, </volume> <pages> pp. 217-232, </pages> <year> (1994). </year>
Reference-contexts: is significant. (For FOIL/mFOIL the paired t-test is highly significant, even with Bonferroni adjustment.) For FOIL/FOSSIL, only the t-test shows significance. 8 We also applied SRT to the biological problem of learning to predict the mutagenic activity of a chemical, i.e., if it is harmful to DNA. (For details see <ref> [28] </ref> and [27]). This domain involves non-determinate background knowledge, too. In Table 6 we compiled results from [15] and [27], and filled the result of SRT.
Reference: [29] <author> L. Watanabe and L. Rendell, </author> <title> `Learning structural decision trees from examples', </title> <booktitle> in Proc. Twelfth International Joint Conference on Artificial Intelligence (IJCAI-91), </booktitle> <pages> pp. 770-776, </pages> <address> San Mateo, CA, (1991). </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: KATE makes extensive use of a given hierarchy and heuristics to generate the branch tests. To our knowledge, KATE was the first system to induce first-order theories in a divide-and-conquer fashion. Watanabe and Rendell <ref> [29] </ref> also investigated the use of divide-and-conquer for learning first-order theories.
Reference: [30] <author> S.M. Weiss and N. Indurkhya, </author> <title> `Rule-based machine learning methods for functional prediction', </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3, </volume> <pages> 383-403, </pages> <year> (1995). </year>
Reference-contexts: One of the current limitations of the approach is that only constants are 6 For a good discussion of tree-based vs. covering algorithms in ILP we have to refer to [2]. For a comparison of tree induction and rule induction in propositional regression see <ref> [30] </ref>. 10 assigned to the leaves, not linear models as in [14] and [21]. Since it could help to build more accurate models, one of the next steps will be to assign linear regression models to the leaves.
References-found: 30

