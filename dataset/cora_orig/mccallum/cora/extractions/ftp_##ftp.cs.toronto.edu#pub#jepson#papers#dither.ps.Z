URL: ftp://ftp.cs.toronto.edu/pub/jepson/papers/dither.ps.Z
Refering-URL: http://www.cs.toronto.edu/vis/publications/TechReports.html
Root-URL: 
Title: Linear Subspace Methods for Recovering Translational Direction  
Author: Allan D. Jepson David J. Heeger 
Affiliation: University of Toronto NASA-Ames Research Center  
Abstract: The image motion field for an observer moving through a static environment depends on the observer's translational and rotational velocities along with the distances to surface points. Given such a motion field as input we have recently introduced sub-space methods for the recovery of the observer's motion and the depth structure of the scene. This class of methods involve splitting the equations describing the motion field into separate equations for the observer's translational direction, the rotational velocity, and the relative depths. The resulting equations can then be solved successively, beginning with the equations for the translational direction. Here we concentrate on this first step. In earlier work, a linear method was shown to provide a biased estimate of the translational direction. We discuss the source of this bias and show how it can be effectively removed. The consequence is that the observer's velocity and the relative depths to points in the scene can all be recovered by successively solving three linear problems. This paper is the University of Toronto, Department of Computer Science, Technical Report: RBCV-TR-92-40, April 1992. Correspondence should be sent to A. Jepson, Department of Com puter Science, University of Toronto, 6 Kings College Road, Toronto, Ontario M5S 1A4. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.L. Barron, </author> <title> D.J. Fleet, S.S. Beauchemin, T.A. Burkitt, Performance of optical flow techniques, </title> <institution> TR299, Dept. of Computer Science, University of Western Ontario, </institution> <note> 1992 (see also CVPR 1992). </note>
Reference-contexts: These error results were also obtained from simulated scenes, and therefore should be taken as a lower bound on the sorts of errors we can expect in practice (see also <ref> [1, 3] </ref>). The optical flow data we use is generated from the depth map for the computer generated scene shown in Figure 3.1. The range of projected distances along the optical axis (i.e. the range of values in the "Z-buffer") is roughly a factor of two for this scene.
Reference: [2] <author> R.C. Bolles, H.H. Baker, and D.H. Marimont, </author> <title> Epipoloar-plane image analysis: An approach to determine structure from motion, </title> <journal> Int. J. of Comp. </journal> <note> Vis., 1 (1987), pp.7-55. </note>
Reference-contexts: Finally, given estimates of both the translational direction and rotational velocities of the camera, several methods are available for obtaining reliable information about the (relative) distances to various scene points (see <ref> [2, 9] </ref>). Unlike many previously proposed algorithms, our approach to motion analysis applies to the general case of arbitrary motion with respect to an arbitrary scene. There is no assumption of smooth or planar surfaces.
Reference: [3] <author> D.J. </author> <title> Fleet, Measurement of Image Velocity , Kluwer, </title> <address> Boston (1992). </address>
Reference-contexts: These error results were also obtained from simulated scenes, and therefore should be taken as a lower bound on the sorts of errors we can expect in practice (see also <ref> [1, 3] </ref>). The optical flow data we use is generated from the depth map for the computer generated scene shown in Figure 3.1. The range of projected distances along the optical axis (i.e. the range of values in the "Z-buffer") is roughly a factor of two for this scene. <p> As discussed above, there are several additional difficulties we might expect to be confronted with given real data. The two primary concerns are: 1) optical flow measurements are typically sparse; and 2) given an environment with occlusion boundaries the optical flow measurements will typically have numerous outliers (see <ref> [3] </ref>). There are two possible approaches to dealing with sparse data. In particular, the data 15 values might be interpolated on a uniform grid.
Reference: [4] <author> D.J. Fleet and A.D. Jepson, </author> <title> Computation of component image velocity from local phase information, </title> <booktitle> Int. J. of Comp. Vision, 5:1 (1990), </booktitle> <pages> pp. 77-104. </pages>
Reference-contexts: Due to the ease in which we can obtain well controlled test data, we consider only computer generated data in this paper. Clearly for the results on such sequences to generalize to real sequences we need to model the noise properties of optical flow measurement techniques. Fleet and Jepson <ref> [4] </ref>, for example, report optical flow measurements with roughly Gaussianly distributed errors having a magnitude about 5% of the length of the optical flow vector.
Reference: [5] <author> D.J. Heeger and A.D. Jepson, </author> <title> Simple method for computing 3D motion and depth, </title> <booktitle> Proc. ICCV 1990, </booktitle> <address> Osaka, Japan, pp.96-100. </address>
Reference-contexts: Here we pursue subspace methods which have been recently introduced for solving this problem (see <ref> [5, 6] </ref>). The general approach involves splitting the problem into three subproblems, each of which can be solved in the following order. First, we obtain constraints which involve only the translational direction, ~ T , of the camera. <p> Unlike many previously proposed algorithms, our approach to motion analysis applies to the general case of arbitrary motion with respect to an arbitrary scene. There is no assumption of smooth or planar surfaces. The results in our previous work <ref> [5, 6] </ref> demonstrate that our approach can be stable with respect to random errors in the flow field measurements, and that it performs quite favorably when compared with other proposed approaches. It is simple to compute and it is highly parallel, not requiring iteration and not requiring an initial guess. <p> With this notation, the constraints on ~ T take the simple form ~ i ( ~ T ) ~ O = 0; for i = 1; . . . ; K 3; (1:1) where "" denotes the usual vector inner-product. In previous work <ref> [5, 6] </ref> we show how to compute the constraint vectors ~ i , and show that these vectors are typically nonlinear functions of ~ T . <p> We proposed that the nonlinear problem (1.1) can be solved for ~ T simply by sampling the constraints on a mesh distributed over a hemisphere of possible orientations for ~ T , and then seeking points of least square error <ref> [5, 6] </ref>. In [7] we introduced an alternative method for finding the camera's translational direc tion, which avoids sampling ~ T -space at many points, and results in a linear system for the translational direction. <p> However, they require initial guesses and iterations, or alternatively, we might consider directly sampling the unit sphere of possible translational directions on a mesh to locate the minimum. The latter approach is similar to our earlier non-linear approach for finding the translational direction <ref> [5] </ref>, and its use would largely eliminate the benefit of having linear constraints on the translational direction. We would therefore like a quadratic objective function which effectively implements the one above. At first glance this seems like an impossibility. <p> For the purposes of this paper we simply demonstrate the existing algorithm on an optical flow field computed for the (synthetic) Yosemite image sequence. This sequence has been used with our non-linear algorithm, which produced an error in the translational direction of about 4 degrees <ref> [5] </ref>. Here we use a different optical flow field, namely one provided by Eero Simoncelli generated using the method described in [10]. The field has been interpolated and sampled on a regular grid, so the convolution method can be directly applied. <p> As mentioned in Section 2, this invariance is important for the convolution form of our algorithm. A couple of other methods can be understood from this formulation. In particular, the original nonlinear method described in <ref> [5, 6] </ref> is equivalent to choosing the coefficients ~c such that they annihilate the samples of all polynomials of the same general form ~ T [~x fi [~x fi ~ ]].
Reference: [6] <author> D.J. Heeger and A.D. Jepson, </author> <title> Subspace methods for recovering rigid motion I: Algorithm and implementation, </title> <journal> Int. J. of Comp. Vision, </journal> <note> V.7, N.2 (1992), pp.95-117. </note>
Reference-contexts: Here we pursue subspace methods which have been recently introduced for solving this problem (see <ref> [5, 6] </ref>). The general approach involves splitting the problem into three subproblems, each of which can be solved in the following order. First, we obtain constraints which involve only the translational direction, ~ T , of the camera. <p> Unlike many previously proposed algorithms, our approach to motion analysis applies to the general case of arbitrary motion with respect to an arbitrary scene. There is no assumption of smooth or planar surfaces. The results in our previous work <ref> [5, 6] </ref> demonstrate that our approach can be stable with respect to random errors in the flow field measurements, and that it performs quite favorably when compared with other proposed approaches. It is simple to compute and it is highly parallel, not requiring iteration and not requiring an initial guess. <p> It is simple to compute and it is highly parallel, not requiring iteration and not requiring an initial guess. For a brief review of the existing literature see <ref> [6] </ref>. In this paper we are primarily concerned with simplifying the first step of the subspace methods in which the direction of translation is estimated independently of the rotational velocity and depths. <p> With this notation, the constraints on ~ T take the simple form ~ i ( ~ T ) ~ O = 0; for i = 1; . . . ; K 3; (1:1) where "" denotes the usual vector inner-product. In previous work <ref> [5, 6] </ref> we show how to compute the constraint vectors ~ i , and show that these vectors are typically nonlinear functions of ~ T . <p> We proposed that the nonlinear problem (1.1) can be solved for ~ T simply by sampling the constraints on a mesh distributed over a hemisphere of possible orientations for ~ T , and then seeking points of least square error <ref> [5, 6] </ref>. In [7] we introduced an alternative method for finding the camera's translational direc tion, which avoids sampling ~ T -space at many points, and results in a linear system for the translational direction. <p> Using the rigid motion equation (2.1), and the perspective projection equation ~x (t) = X 3 (t) we find that the image velocity is given by (see <ref> [6] </ref>) ~u (~x) = p (~x)A (~x) ~ T + B (~x) ~ : (A:1) Here p (~x) = 1=X 3 is inverse depth, ~ T is the translational velocity and ~ is the rotational velocity. <p> As mentioned in Section 2, this invariance is important for the convolution form of our algorithm. A couple of other methods can be understood from this formulation. In particular, the original nonlinear method described in <ref> [5, 6] </ref> is equivalent to choosing the coefficients ~c such that they annihilate the samples of all polynomials of the same general form ~ T [~x fi [~x fi ~ ]].
Reference: [7] <author> A.D. </author> <title> Jepson and D.J. Heeger, A fast subspace algorithm for recovering rigid motion, </title> <booktitle> Proc. IEEE Workshop on Visual Motion, Princeton (1991), </booktitle> <pages> pp. 124-131. </pages>
Reference-contexts: We proposed that the nonlinear problem (1.1) can be solved for ~ T simply by sampling the constraints on a mesh distributed over a hemisphere of possible orientations for ~ T , and then seeking points of least square error [5, 6]. In <ref> [7] </ref> we introduced an alternative method for finding the camera's translational direc tion, which avoids sampling ~ T -space at many points, and results in a linear system for the translational direction. <p> Fortunately, we can show that for general scenes containing a rich depth structure this linear approach still does provide a robust estimate <ref> [7] </ref>. However our preliminary experiments indicated that a straight forward implementation of the linear approach provides a biased estimate of the translational direction. This bias was observed to be more severe for images having smaller angular extents. <p> As mentioned in the introduction, given this direction we are left with linear problems for each of the rotational velocity and the inverse relative depths. Here we consider only this first step, namely the computation of the translational direction. For the algorithm proposed in <ref> [7] </ref>, the optical flow data at each point is first expanded into the 3-vector ~q (~x k ) = Q (~x k )~u (~x k ); (2:2) where Q (~x) = B 0 1 x 2 x 1 C Notice that this preprocessing step is local; each image velocity is transformed <p> (deg) 7.0 12 19 24 26 Error T 1 (deg) 0:3 0:1 0:2 0:1 0:08 0:04 0:03 0:02 0:01 0:01 Error T 2 (deg) 6:5 0:6 11:4 0:4 18:6 0:2 23:6 0:1 25:5 0:01 Table 3.1: Results from the basic algorithm by the length of ~x fi ~ T (see <ref> [7] </ref>). (No value was computed around the border of the image where the convolution mask would have extended beyond the edge of the image.) As we discussed in [7], the translation constraint vector ~t (~x) has a large amplitude only where there is significant nonplanar variation in the depth structure. <p> 18:6 0:2 23:6 0:1 25:5 0:01 Table 3.1: Results from the basic algorithm by the length of ~x fi ~ T (see <ref> [7] </ref>). (No value was computed around the border of the image where the convolution mask would have extended beyond the edge of the image.) As we discussed in [7], the translation constraint vector ~t (~x) has a large amplitude only where there is significant nonplanar variation in the depth structure. The amplitude response could clearly be useful for other operations such as image segmentation. <p> One other property of the constraint vectors is needed to understand the distribution shown in Figure 4.1, namely that the exact ~t vectors tend to be nearly perpendicular to the mean sampling direction. In particular, the exact constraint lines tend to pass close to the sampling direction <ref> [7] </ref>. Since the covariance of ~t is small in this direction, the noise does not perturb the position of the constraint lines very much near the sampling direction.
Reference: [8] <author> N. da Vitoria Lobo and J.K. Tsotsos, </author> <title> Using collinear points to compute egomotion and detect nonrigidity, </title> <booktitle> IEEE CVPR Proc., Hawaii (1991), </booktitle> <pages> pp. 344-350. </pages>
Reference-contexts: A second variant is due to da Vitorio Lobo and Tsotsos <ref> [8] </ref>. They consider only sampling patterns for which the sample points all lie on a line in the image plane passing through the projection of the true translational direction.
Reference: [9] <author> L. Matthies, R. Szeliski, and T. Kanade, </author> <title> Kalman filter-based algorithms for estimating depth from image sequences, </title> <journal> Int. J. of Comp. </journal> <note> Vis., 3 (1989), pp.209-238. </note>
Reference-contexts: Finally, given estimates of both the translational direction and rotational velocities of the camera, several methods are available for obtaining reliable information about the (relative) distances to various scene points (see <ref> [2, 9] </ref>). Unlike many previously proposed algorithms, our approach to motion analysis applies to the general case of arbitrary motion with respect to an arbitrary scene. There is no assumption of smooth or planar surfaces.
Reference: [10] <author> E.P. Simoncelli, </author> <title> E.H. Adelson, and D.J. Heeger, Probability distributions of optical flow, </title> <booktitle> IEEE CVPR Proc., Hawaii (1991), </booktitle> <pages> pp. 310-315. </pages>
Reference-contexts: This sequence has been used with our non-linear algorithm, which produced an error in the translational direction of about 4 degrees [5]. Here we use a different optical flow field, namely one provided by Eero Simoncelli generated using the method described in <ref> [10] </ref>. The field has been interpolated and sampled on a regular grid, so the convolution method can be directly applied. Due to errors in the interpolant near the borders of the image and near the horizon, we found it necessary to crop the flow field.
Reference: [11] <author> M.E. Spetsakis, </author> <title> Models of Statistical Visual Motion Estimation. </title> <type> Tech. Rep. </type> <institution> CS-91-06, Dept. of Computer Science, York University, </institution> <address> North York, Ontario (1991). </address>
Reference-contexts: This bias was observed to be more severe for images having smaller angular extents. In this paper we analize the cause of this bias, and discuss a simple method for its removal. In related work Spetsakis <ref> [11] </ref> discusses the bias observed in a completely different method for structure from motion, and comments on the possibility of the biases having a common cause.
Reference: [12] <author> M.E. Spetsakis and J. Aloimonos, </author> <title> Optimal computing of structure from motion using point correspondences in two frames, </title> <booktitle> Proc. of IEEE ICCV, Florida (1988), </booktitle> <pages> pp. 449-453, 684. </pages>
Reference-contexts: Thus we can view this approach as a maximum likelihood method, as has been suggested in <ref> [12] </ref>. There are, of course, techniques for finding locally optimal values of non-quadratic objective functions. However, they require initial guesses and iterations, or alternatively, we might consider directly sampling the unit sphere of possible translational directions on a mesh to locate the minimum.
Reference: [13] <author> C. Tomasi and T. Kanade, </author> <title> Factoring image sequences into shape and motion, </title> <booktitle> Proc. IEEE Workshop on Visual Motion, Princeton (1991), </booktitle> <pages> pp. 21-28. 20 </pages>
Reference-contexts: For the angular extent of 5 degrees we are approaching, or within, the domain of applicability of techniques which rely on an orthographic approximation, such as the factoring approach discussed in <ref> [13] </ref>. At this extreme we find that the eigenvalues values of the matrix D are in the proportion (300:1.2:1). The fact that the last two eigenvalues have nearly the same magnitude indicates that the translational direction is only weakly constrained within the plane spanned by the last two eigendirections.
References-found: 13

