URL: http://www.cs.ucl.ac.uk/staff/a.hunter/lex.ps
Refering-URL: http://www.cs.ucl.ac.uk/staff/A.Hunter/papers.html
Root-URL: http://www.cs.ucl.ac.uk
Email: a.hunter@cs.ucl.ac.uk  
Title: Using default logic for lexical knowledge  
Author: Anthony Hunter 
Address: Gower Street London WC1E 6BT, UK  
Affiliation: Department of Computer Science University College London  
Abstract: Lexical knowledge is knowledge about the morphology, grammar, and semantics of words. This knowledge is increasingly important in language engineering, and more generally in information retrieval, information filtering, intelligent agents and knowledge management. Here we present a framework, based on default logic, called Lexica, for capturing lexical knowledge. We show how we can use contextual information about a given word to identify relations such as synonyms, antinyms, specializations, and meronyms for the word. We also show how we can use machine learning techniques to facilitate engineering a Lexica knowl-edgebase. 
Abstract-found: 1
Intro-found: 1
Reference: [Bes89] <author> Ph Besnard. </author> <title> An Introduction to Default Logic. </title> <publisher> Springer, </publisher> <year> 1989. </year>
Reference: [BFGM91] <author> R Beckworth, C Fellbaum, D Gross, and G Miller. </author> <title> WordNet: A lexical database organized on psycholinguistic principles. In U Zernik, editor, Lexical Acquisition: Exploiting On-line Resources to Build a Lexicon, </title> <address> pages 211-226. </address> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1991. </year>
Reference: [BH94] <author> P Bruza and T Huibers. </author> <title> Investigating aboutness axioms using information fields. </title> <booktitle> In Proceedings of the 18th ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'94), </booktitle> <pages> pages 112-121. </pages> <publisher> Springer, </publisher> <year> 1994. </year>
Reference: [BPK + 96] <author> U Borghoff, R Pareschi, H Karch, M Nohmeier, and J Schlichter. </author> <title> Constraint-based information gathering for a network publication system. </title> <booktitle> In Proceedings of the First International Conference on the Practical Application of Intelligent Agents and Multi-agent Technology. </booktitle> <publisher> Pratical Applications Company, </publisher> <year> 1996. </year>
Reference: [Bre91] <author> G Brewka. </author> <title> Common-sense Reasoning. </title> <publisher> Cambridge University Press, </publisher> <year> 1991. </year>
Reference: [Bri91] <author> T Briscoe. </author> <title> Lexical issues in natural language processing. </title> <editor> In E Klein and F Veltman, editors, </editor> <booktitle> Natural Language and Speech, </booktitle> <pages> pages 39-68. </pages> <publisher> Springer, </publisher> <year> 1991. </year>
Reference-contexts: This view has shifted and recent theories of language are using lexicons for significant proportions of linguistic knowledge <ref> [Bri91] </ref>. A wide variety of machine-readable lexicons have been developed (for reviews see [WSG96,GPWS96]), though many are oriented to specific approaches or tasks, such as EDR [Yok95] and Acquilex [Bri91] for machine translation. <p> This view has shifted and recent theories of language are using lexicons for significant proportions of linguistic knowledge <ref> [Bri91] </ref>. A wide variety of machine-readable lexicons have been developed (for reviews see [WSG96,GPWS96]), though many are oriented to specific approaches or tasks, such as EDR [Yok95] and Acquilex [Bri91] for machine translation. Yet there is a need to general purpose information about words, such as for intelligent agents, information retrieval, information filtering, and information extraction. Perhaps the most significant example of such a general purpose system is WordNet [BFGM91,Mil95].
Reference: [Buv95] <author> S Buvac. </author> <title> Resolving lexical ambiguity using a formal theory of context. </title> <editor> In K van Deemter and S Peters, editors, </editor> <booktitle> Semantic Ambiguity and Underspec-ification, </booktitle> <pages> pages 101-124. </pages> <publisher> CSLI Publications, </publisher> <year> 1995. </year>
Reference: [Cro93] <author> B Croft. </author> <title> Knowledge-based and statistical approaches to text retrieval. </title> <journal> IEEE Expert, </journal> <pages> pages 8-12, </pages> <year> 1993. </year>
Reference-contexts: Statistical techniques are well suited to repositories of information where comprehensive statistical analyses can be undertaken (see for example <ref> [Cro93] </ref>), but are less well suited to heterogeneous distributed sources, such as the Internet, where the topics are so diverse, locally managed, and constantly evolving. An increasingly important alternative is the approach of intelligent agents (for example [Mae94,GLC + 95,GF95,Lie95,BPK + 96,FJ96,Mou96]).
Reference: [Cru86] <author> D Cruse. </author> <title> Lexical Semantics. </title> <publisher> Cambridge University Press, </publisher> <year> 1986. </year>
Reference: [Eng96] <author> B Engleder. </author> <title> Filtering News Articles. </title> <type> MSc Thesis, </type> <institution> Department of Computing, Imperial College, </institution> <address> London, </address> <year> 1996. </year>
Reference-contexts: Moreover, there is no logical reasoning with the relations in the semantic network. Another kind of problem with WordNet is that the knowledge is very general. WordNet has been applied to information retrieval [Voo94] and information filtering <ref> [Eng96] </ref> and in these studies the utility of WordNet was limited by this generality.
Reference: [FJ96] <author> A Falk and I Jonsson. PAWS: </author> <title> An agent for WWW-retrieval and filtering. </title> <booktitle> In Proceedings of the First International Conference on the Practical Application of Intelligent Agents and Multi-agent Technology. Practical Applications Company, </booktitle> <year> 1996. </year>
Reference: [GF95] <editor> B Grosof and D Foulger. Globenet and RAISE: </editor> <title> Intelligent agents for net-worked newsgroups and customer service support. </title> <type> Technical report, </type> <institution> IBM Research Division, T J Watson Research Center, </institution> <address> New York, </address> <year> 1995. </year>
Reference: [GLC + 95] <author> B Grosof, D Levine, H Chan, C Parris, and J Auerbach. </author> <title> Reusable architecture for embedding rule-based intelligence in information agents. </title> <type> Technical report, </type> <institution> IBM Research Division, T J Watson Research Center, </institution> <address> New York, </address> <year> 1995. </year>
Reference: [GPWS96] <author> L Gutherie, J Pustejovsky, Y Wilks, and B Slator. </author> <title> The role of lexicons in natural language processing. </title> <journal> Communications of the ACM, </journal> <volume> 39(1) </volume> <pages> 63-72, </pages> <year> 1996. </year>
Reference: [Hun96] <author> A Hunter. </author> <title> Intelligent text handling using default logic. </title> <booktitle> In Proceedings of the IEEE Conference on Tools with Artificial Intelligence, </booktitle> <pages> pages 34-40. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference: [Len95] <author> D Lenat. </author> <title> CYC:a large-scale investment in knowledge infrastructure. </title> <journal> Communications of the ACM, </journal> <volume> 38(11) </volume> <pages> 33-38, </pages> <year> 1995. </year>
Reference-contexts: Finally, we can consider the Lexica approach as a move towards reusable knowledgebases or general knowledge systems. CYC is perhaps the best known, and certainly the most intensively developed example of a general knowledge system <ref> [Len95] </ref>. There are many problems with reuse of knowledge, as highlighted during the development of CYC.
Reference: [Lie95] <author> H Lieberman. Letizia: </author> <title> An agent that assists web browsing. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference: [LS95] <author> T Linke and T Schaub. </author> <title> Lemma handling in default logic theorem provers. In Symbolic and Qualitative Approaches to Reasoning and Uncertainty, </title> <booktitle> volume 946 of Lecture Notes in Computer Science, </booktitle> <pages> pages 285-292. </pages> <publisher> Springer, </publisher> <year> 1995. </year>
Reference: [Mae94] <author> P Maes. </author> <title> Agents that reduce work and information overload. </title> <journal> Communications of the ACM, </journal> <volume> 37(7) </volume> <pages> 31-40, </pages> <year> 1994. </year>
Reference: [Mei93] <author> W Meijs. </author> <title> Exploring lexical knowledge. </title> <editor> In C Souter and E Atwell, editors, </editor> <booktitle> Corpus-based Computational Linguistics, </booktitle> <pages> pages 249-260. </pages> <address> Rodopi, </address> <year> 1993. </year>
Reference-contexts: investigate a number of avenues: (1) Using the framework in restricted domains that require a limited number of default rules; (2) Using inductive logic programming ([Mug92]) to generate default rules for a domain; (3) Using co-locational data for knowledge engineering; and (4) Using machine-readable dictionaries and thesauri for knowledge engineering <ref> [Mei93] </ref>. The Lexica framework is complementary to formalizations of the notion of "aboutness" such as [BH94,Buv95,Hun96]. A Lexica knowledgebase could potentially be used in such frameworks to allow identification of, and reasoning with, relations such as "article A is about topic T ".
Reference: [Mil95] <author> G Miller. </author> <title> WordNet: A lexical database for English. </title> <journal> Communications of the ACM, </journal> <volume> 38(11) </volume> <pages> 39-41, </pages> <year> 1995. </year>
Reference: [Mou96] <author> A Moukas. Amalthaea: </author> <title> Information discovery and filtering using a multia-gent evolving ecosystem. </title> <type> Technical report, </type> <institution> MIT Media Laboratory, </institution> <address> Cam-bridge MA, </address> <year> 1996. </year>
Reference: [Mug92] <author> S Muggleton. </author> <title> Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference: [Nie94] <author> I Niemela. </author> <title> A decision method for non-monotonic reasoning based on au-toepistemic reasoning. </title> <booktitle> In Proceedings of the Fourth International Conference Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 473-484. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference: [Qui86] <author> J Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: To handle this see section 2.3. 2.2 Learning classification trees In this work, we have used the ID3 inductive learning algorithm developed by Ross Quinlan <ref> [Qui86] </ref>. ID3 is an approach to machine learning based on constructing a classification, or decision, tree for a set of training examples. Training examples are presented as a table | each row is an example and each column is an attribute of the examples.
Reference: [Rei80] <author> R Reiter. </author> <title> Default logic. </title> <journal> Artificial Intelligence, </journal> <volume> 13 </volume> <pages> 81-132, </pages> <year> 1980. </year>
Reference-contexts: Default logic was proposed by Reiter <ref> [Rei80] </ref>, and good reviews are available (see for example [Bes89,Bre91]). In default logic, knowledge is represented as a default theory, which consists of a set of first-order formulae and a set of default rules for representing default information.
Reference: [Sch95] <author> T Schaub. </author> <title> A new methodology for query-answering in default logics via structure-oriented theorem proving. </title> <journal> Journal of Automated Reasoning, </journal> <volume> 15 </volume> <pages> 95-165, </pages> <year> 1995. </year>
Reference-contexts: For simplicity, we chose Reiter's version of default logic. But, for efficiency, a goal-directed form of default reasoning is more appropriate. In particular, we are investigating the use of the XRay query answering system for default logics <ref> [Sch95] </ref>. For an application, it is possible that a relatively large number of default rules would be required for an acceptable level of performance.
Reference: [Sha96] <author> A Shaikh. </author> <title> Data Mining Using Inductive Logic Programming. </title> <type> MSc Thesis, </type> <institution> Department of Computing, Imperial College, </institution> <address> London, </address> <year> 1996. </year>
Reference-contexts: Each attribute in the table of training examples is a word. If a training example contains that word, then "yes" is entered into the corresponding position in the table, and "no" otherwise. Tables up to 236 attributes have been constructed containing up to 60 examples (rows) <ref> [Sha96] </ref>. We equate the notion of topic with that of context. So we can use these classification trees as described in section 2.1. 2.3 Reasoning with contextual information We assume a set of atomic contexts. These are disjoint and exhaustive. They are the most focussed contexts we consider.
Reference: [Tra93] <author> R Trask. </author> <title> A Dictionary of Grammatical Terms in Linguistics. </title> <publisher> Routledge, </publisher> <year> 1993. </year>
Reference-contexts: This raises the need to incorporate default knowledgebases as a repository for background knowledge. Using explicit defaults offers a lucid representation for users, and it aids maintainability and incrementality. 1.2 Lexicons What is a lexicon? According to Trask <ref> [Tra93] </ref>, a lexicon within the study of grammar has traditionally been used as a repository of miscellaneous facts, with little in the way of generalizations. This view has shifted and recent theories of language are using lexicons for significant proportions of linguistic knowledge [Bri91].
Reference: [Voo94] <author> E Voorhees. </author> <title> Query expansion using lexical-semantic relations. </title> <editor> In W Croft and C van Rijsbergen, editors, </editor> <booktitle> Proceedings of the Seventeenth International ACM-SIGIR Conference on Research and Developement in Information Retrieval, </booktitle> <pages> pages 61-69, </pages> <year> 1994. </year>
Reference-contexts: Moreover, there is no logical reasoning with the relations in the semantic network. Another kind of problem with WordNet is that the knowledge is very general. WordNet has been applied to information retrieval <ref> [Voo94] </ref> and information filtering [Eng96] and in these studies the utility of WordNet was limited by this generality.
Reference: [WCH87] <author> M Winston, R Chaffin, and D Herrman. </author> <title> A taxonomy of part-whole relations. </title> <journal> Cognitive Science, </journal> <volume> 11 </volume> <pages> 417-444, </pages> <year> 1987. </year>
Reference: [WSG96] <author> Y Wilks, B Slator, and L Guthrie. </author> <title> Electric Words: Dictionaries, Computers, and Meanings. </title> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference: [Yok95] <author> T Yokoi. </author> <title> The EDR Electronic Dictionary. </title> <journal> Communications of the ACM, </journal> <volume> 38(11) </volume> <pages> 42-44, </pages> <year> 1995. </year>
Reference-contexts: This view has shifted and recent theories of language are using lexicons for significant proportions of linguistic knowledge [Bri91]. A wide variety of machine-readable lexicons have been developed (for reviews see [WSG96,GPWS96]), though many are oriented to specific approaches or tasks, such as EDR <ref> [Yok95] </ref> and Acquilex [Bri91] for machine translation. Yet there is a need to general purpose information about words, such as for intelligent agents, information retrieval, information filtering, and information extraction. Perhaps the most significant example of such a general purpose system is WordNet [BFGM91,Mil95].
References-found: 33

