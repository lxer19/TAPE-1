URL: http://www.cs.utexas.edu/users/plaxton/ps/1998/spaa.ps
Refering-URL: http://www.cs.utexas.edu/users/plaxton/html/abc.html
Root-URL: 
Email: fnimar,rdb,plaxtong@cs.utexas.edu  
Title: Thread Scheduling for Multiprogrammed Multiprocessors  
Author: Nimar S. Arora Robert D. Blumofe C. Greg Plaxton 
Address: Austin  
Affiliation: Department of Computer Science, University of Texas at  
Abstract: We present a user-level thread scheduler for shared-memory multiprocessors, and we analyze its performance under multiprogramming. We model multiprogramming with two scheduling levels: our scheduler runs at user-level and schedules threads onto a fixed collection of processes, while below, the operating-system kernel schedules processes onto a fixed collection of processors. We consider the kernel to be an adversary, and our goal is to schedule threads onto processes such that we make efficient use of whatever processor resources are provided by the kernel. Our thread scheduler is a non-blocking implementation of the work-stealing algorithm. For any multithreaded computation with work T 1 and critical-path length T 1 , and for any number P of processes, our scheduler executes the computation in expected time O(T 1 =P A + T 1 P=P A ), where P A is the average number of processors allocated to the computation by the kernel. This time bound is optimal to within a constant factor, and achieves linear speedup whenever P is small relative to the average parallelism T 1 =T 1 . 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Accetta, R. Baron, W. Bolosky, D. Golub, R. Rashid, A. Tevanian, and M. Young. </author> <title> Mach: A new kernel foundation for UNIX development. </title> <booktitle> In Proceedings of the Summer 1986 USENIX Conference, </booktitle> <pages> pages 93112, </pages> <month> July </month> <year> 1986. </year> <month> 10 </month>
Reference-contexts: Specifically, before the execution begins the oblivious adversary commits itself to a complete kernel schedule. To deal with an oblivious adversary, we employ a directed yield <ref> [1, 25] </ref> that we call yieldTo. If at round i process q calls yieldTo (r), then the kernel cannot schedule process q again until it has scheduled process r.
Reference: [2] <author> Noga Alon and Joel H. Spencer. </author> <title> The Probabilistic Method. </title> <publisher> John Wiley & Sons, </publisher> <year> 1992. </year>
Reference-contexts: Each phase succeeds with probability at least p = 1=4, so the expected number of successes is at least np = 8T 1 + m=4. We now compute the probability that the number X of successes is less than 8T 1 . We use the Chernoff bound <ref> [2, Theorem A.13] </ref>, Pr fX &lt; np ag &lt; e 2np ; with a = m=4.
Reference: [3] <author> Ken Arnold and James Gosling. </author> <title> The Java Programming Language. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction For shared-memory multiprocessors, parallel applications use multiple threads and are coded using a parallelizing compiler, a threads library, or a multithreaded language such as Cilk [7, 18] or Java <ref> [3] </ref>. In addition to supporting multithreaded applications, multiprocessors also support multiprogrammed workloads in which a mix of serial and parallel, interactive and batch applications may execute concurrently. A major factor in the performance of such workloads is the operation of the thread scheduler. <p> In the near future, we plan to transfer this technology into production-quality systems and investigate its application to real-time and multimedia systems. Likely targets for technology transfer include the Cilk [7, 18] and Java <ref> [3] </ref> multithreaded languages, a POSIX threads library, and a parallelizing compiler. We conjecture that if our user-level thread scheduler is coupled with a kernel scheduler that can deliver guarantees on P A , then we can deliver end-to-end execution-time guarantees for real-time and multimedia applications.
Reference: [4] <author> Guy E. Blelloch, Phillip B. Gibbons, and Yossi Matias. </author> <title> Provably efficient scheduling for languages with fine-grained parallelism. </title> <booktitle> In Proceedings of the 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 112, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: In addition to supporting multithreaded applications, multiprocessors also support multiprogrammed workloads in which a mix of serial and parallel, interactive and batch applications may execute concurrently. A major factor in the performance of such workloads is the operation of the thread scheduler. Prior work on thread scheduling <ref> [4, 5, 8, 11, 12] </ref> has dealt exclusively with non-multiprogrammed environments in which a multi-threaded computation executes on P dedicated processors. Such scheduling algorithms dynamically map threads onto the processors with the goal of achieving P -fold speedups. <p> For these reasons, work stealing is practical and variants have been implemented in many systems [7, 16, 17, 21, 30, 34]. For general multithreaded computations, other scheduling algorithms have also been shown to be simultaneously efficient with respect to time and space <ref> [4, 5, 11, 12] </ref>. Of particular interest here is the idea of deriving parallel schedules from serial schedules [4, 5], which produces strong upper bounds on time and space. The practical application and possible adaptation of this idea to multiprogrammed environments is an open question. <p> For general multithreaded computations, other scheduling algorithms have also been shown to be simultaneously efficient with respect to time and space [4, 5, 11, 12]. Of particular interest here is the idea of deriving parallel schedules from serial schedules <ref> [4, 5] </ref>, which produces strong upper bounds on time and space. The practical application and possible adaptation of this idea to multiprogrammed environments is an open question. Prior work that has considered multiprogrammed environments has focused on the kernel-level scheduler.
Reference: [5] <author> Guy E. Blelloch, Phillip B. Gibbons, Yossi Matias, and Girija J. Narlikar. </author> <title> Space-efficient scheduling of parallelism with synchronization variables. </title> <booktitle> In Proceedings of the 9th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 1223, </pages> <address> Newport, Rhode Island, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: In addition to supporting multithreaded applications, multiprocessors also support multiprogrammed workloads in which a mix of serial and parallel, interactive and batch applications may execute concurrently. A major factor in the performance of such workloads is the operation of the thread scheduler. Prior work on thread scheduling <ref> [4, 5, 8, 11, 12] </ref> has dealt exclusively with non-multiprogrammed environments in which a multi-threaded computation executes on P dedicated processors. Such scheduling algorithms dynamically map threads onto the processors with the goal of achieving P -fold speedups. <p> For these reasons, work stealing is practical and variants have been implemented in many systems [7, 16, 17, 21, 30, 34]. For general multithreaded computations, other scheduling algorithms have also been shown to be simultaneously efficient with respect to time and space <ref> [4, 5, 11, 12] </ref>. Of particular interest here is the idea of deriving parallel schedules from serial schedules [4, 5], which produces strong upper bounds on time and space. The practical application and possible adaptation of this idea to multiprogrammed environments is an open question. <p> For general multithreaded computations, other scheduling algorithms have also been shown to be simultaneously efficient with respect to time and space [4, 5, 11, 12]. Of particular interest here is the idea of deriving parallel schedules from serial schedules <ref> [4, 5] </ref>, which produces strong upper bounds on time and space. The practical application and possible adaptation of this idea to multiprogrammed environments is an open question. Prior work that has considered multiprogrammed environments has focused on the kernel-level scheduler.
Reference: [6] <author> Robert D. Blumofe, Matteo Frigo, Christopher F. Joerg, Charles E. Leiserson, and Keith H. Randall. </author> <title> An analysis of dag-consistent distributed shared-memory algorithms. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 297308, </pages> <address> Padua, Italy, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: For the restricted class of fully strict multithreaded computations, the work stealing algorithm is efficient with respect to both space and communication [8]. Moreover, when coupled with dag-consistent distributed shared memory, work stealing is also efficient with respect to page faults <ref> [6] </ref>. For these reasons, work stealing is practical and variants have been implemented in many systems [7, 16, 17, 21, 30, 34]. For general multithreaded computations, other scheduling algorithms have also been shown to be simultaneously efficient with respect to time and space [4, 5, 11, 12].
Reference: [7] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 37(1):5569, </volume> <month> August </month> <year> 1996. </year>
Reference-contexts: 1 Introduction For shared-memory multiprocessors, parallel applications use multiple threads and are coded using a parallelizing compiler, a threads library, or a multithreaded language such as Cilk <ref> [7, 18] </ref> or Java [3]. In addition to supporting multithreaded applications, multiprocessors also support multiprogrammed workloads in which a mix of serial and parallel, interactive and batch applications may execute concurrently. A major factor in the performance of such workloads is the operation of the thread scheduler. <p> Moreover, when coupled with dag-consistent distributed shared memory, work stealing is also efficient with respect to page faults [6]. For these reasons, work stealing is practical and variants have been implemented in many systems <ref> [7, 16, 17, 21, 30, 34] </ref>. For general multithreaded computations, other scheduling algorithms have also been shown to be simultaneously efficient with respect to time and space [4, 5, 11, 12]. <p> In the near future, we plan to transfer this technology into production-quality systems and investigate its application to real-time and multimedia systems. Likely targets for technology transfer include the Cilk <ref> [7, 18] </ref> and Java [3] multithreaded languages, a POSIX threads library, and a parallelizing compiler. We conjecture that if our user-level thread scheduler is coupled with a kernel scheduler that can deliver guarantees on P A , then we can deliver end-to-end execution-time guarantees for real-time and multimedia applications.
Reference: [8] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Scheduling multi-threaded computations by work stealing. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations of Computer Science (FOCS), </booktitle> <pages> pages 356368, </pages> <address> Santa Fe, New Mexico, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: In addition to supporting multithreaded applications, multiprocessors also support multiprogrammed workloads in which a mix of serial and parallel, interactive and batch applications may execute concurrently. A major factor in the performance of such workloads is the operation of the thread scheduler. Prior work on thread scheduling <ref> [4, 5, 8, 11, 12] </ref> has dealt exclusively with non-multiprogrammed environments in which a multi-threaded computation executes on P dedicated processors. Such scheduling algorithms dynamically map threads onto the processors with the goal of achieving P -fold speedups. <p> This assumption is consistent with our convention that a node represents a single instruction. Second, we assume that the dag has exactly one root node with in-degree 0 and one final node with out-degree 0. We present a non-blocking implementation of the work-stealing algorithm <ref> [8] </ref>, and we analyze the performance of this non-blocking work stealer in multiprogrammed environments. In this implementation, all concurrent data structures are non-blocking [23, 24] so that if the kernel preempts a process, it does not hinder other pro cesses, for example by holding locks. <p> We also show that for any " &gt; 0, with probability at least 1 ", the execution time is O (T 1 =P A + (T 1 + lg (1="))P=P A ). This result improves on previous results <ref> [8] </ref> in two ways. First, we consider arbitrary multithreaded computations as opposed to the special case of fully strict computations. Second, we consider multiprogrammed environments as opposed to dedicated environments. <p> We remark that the latter fact does not imply the existence of a polynomial-time algorithm for computing an optimal execution schedule. In fact, the related decision problem is NP-complete [33]. 3 Non-blocking work stealing In this section we review the work-stealing algorithm <ref> [8] </ref>, and then describe our non-blocking implementation, which involves the use of a yield system call and a non-blocking implementation of the concurrent data structures. <p> For the restricted class of fully strict multithreaded computations, the work stealing algorithm is efficient with respect to both space and communication <ref> [8] </ref>. Moreover, when coupled with dag-consistent distributed shared memory, work stealing is also efficient with respect to page faults [6]. For these reasons, work stealing is practical and variants have been implemented in many systems [7, 16, 17, 21, 30, 34].
Reference: [9] <author> Robert D. Blumofe and Dionisios Papadopoulos. </author> <title> The performance of work stealing in multiprogrammed environments (extended abstract). </title> <booktitle> In Proceedings of the 1998 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, Poster Session, </booktitle> <address> Madison, Wisconsin, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: Though such algorithms will work in some multiprogrammed environments, in particular those that employ static space partitioning [13, 26] or co-scheduling [15, 26, 29], they do not work in the multiprogrammed environments being supported by modern shared-memory multiprocessors and operating systems <ref> [9, 13, 14, 20] </ref>. The problem This research is supported in part by the Defense Advanced Research Projects Agency (DARPA) under Grant F30602-97-1-0150 from the U.S. Air Force Research Laboratory. In addition, Greg Plaxton is supported by the National Science Foundation under Grant CCR9504145. <p> Our non-blocking work stealer has been implemented and numerous performance studies have been conducted <ref> [9] </ref>. These studies show that application performance conforms to the O (T 1 =P A + T 1 P=P A ) bound and that the constant hidden in the big-Oh notation is small, roughly 1. <p> In particular, with work stealing, adding and removing processes is easy, and with our non-blocking implementation, we can be lazy about removing processes, because the penalty for operating with more processes than processors is small. 6 Conclusion Whereas traditional thread schedulers demonstrate poor performance in multiprogrammed environments <ref> [9, 13, 14, 20] </ref>, the non-blocking work stealer executes with guaranteed high performance in such environments. <p> We prove this result under the assumption that the kernel, which schedules processes on processors and determines P A , is an adversary. In addition to the analytical results reported here, we also have empirical results <ref> [9] </ref> that attest to the practical application of the non-blocking work stealer. We have implemented a prototype C++ threads library using the non-blocking work stealer. This library employs a combination of the UNIX priocntl (priority control) and yield system calls to implement a yieldToAll.
Reference: [10] <author> Richard P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 21(2):201206, </volume> <month> April </month> <year> 1974. </year>
Reference-contexts: Note that an on-line user-level sched--uler cannot always produce greedy execution schedules, because some amount of scheduling overhead is often unavoidable. The following theorem about greedy execution schedules also holds for level-by-level (Brent <ref> [10] </ref>) execution schedules, with only trivial changes to the proof. Theorem 2 (Greedy Schedules) Consider any multithreaded computation with work T 1 and critical-path length T 1 , any number P of processes, and any kernel schedule.
Reference: [11] <author> F. Warren Burton. </author> <title> Guaranteeing good space bounds for parallel programs. </title> <type> Technical Report 92-10, </type> <institution> Simon Fraser University, School of Computing Science, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: In addition to supporting multithreaded applications, multiprocessors also support multiprogrammed workloads in which a mix of serial and parallel, interactive and batch applications may execute concurrently. A major factor in the performance of such workloads is the operation of the thread scheduler. Prior work on thread scheduling <ref> [4, 5, 8, 11, 12] </ref> has dealt exclusively with non-multiprogrammed environments in which a multi-threaded computation executes on P dedicated processors. Such scheduling algorithms dynamically map threads onto the processors with the goal of achieving P -fold speedups. <p> For these reasons, work stealing is practical and variants have been implemented in many systems [7, 16, 17, 21, 30, 34]. For general multithreaded computations, other scheduling algorithms have also been shown to be simultaneously efficient with respect to time and space <ref> [4, 5, 11, 12] </ref>. Of particular interest here is the idea of deriving parallel schedules from serial schedules [4, 5], which produces strong upper bounds on time and space. The practical application and possible adaptation of this idea to multiprogrammed environments is an open question.
Reference: [12] <author> F. Warren Burton and David J. Simpson. </author> <title> Space efficient execution of deterministic parallel programs. </title> <type> Unpublished manuscript, </type> <year> 1994. </year>
Reference-contexts: In addition to supporting multithreaded applications, multiprocessors also support multiprogrammed workloads in which a mix of serial and parallel, interactive and batch applications may execute concurrently. A major factor in the performance of such workloads is the operation of the thread scheduler. Prior work on thread scheduling <ref> [4, 5, 8, 11, 12] </ref> has dealt exclusively with non-multiprogrammed environments in which a multi-threaded computation executes on P dedicated processors. Such scheduling algorithms dynamically map threads onto the processors with the goal of achieving P -fold speedups. <p> For these reasons, work stealing is practical and variants have been implemented in many systems [7, 16, 17, 21, 30, 34]. For general multithreaded computations, other scheduling algorithms have also been shown to be simultaneously efficient with respect to time and space <ref> [4, 5, 11, 12] </ref>. Of particular interest here is the idea of deriving parallel schedules from serial schedules [4, 5], which produces strong upper bounds on time and space. The practical application and possible adaptation of this idea to multiprogrammed environments is an open question.
Reference: [13] <author> Mark Crovella, Prakash Das, Czarek Dubnicki, Thomas LeBlanc, and Evangelos Markatos. </author> <title> Multiprogramming on multiprocessors. </title> <booktitle> In Proceedings of the Third IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> December </month> <year> 1991. </year>
Reference-contexts: Such scheduling algorithms dynamically map threads onto the processors with the goal of achieving P -fold speedups. Though such algorithms will work in some multiprogrammed environments, in particular those that employ static space partitioning <ref> [13, 26] </ref> or co-scheduling [15, 26, 29], they do not work in the multiprogrammed environments being supported by modern shared-memory multiprocessors and operating systems [9, 13, 14, 20]. The problem This research is supported in part by the Defense Advanced Research Projects Agency (DARPA) under Grant F30602-97-1-0150 from the U.S. <p> Though such algorithms will work in some multiprogrammed environments, in particular those that employ static space partitioning [13, 26] or co-scheduling [15, 26, 29], they do not work in the multiprogrammed environments being supported by modern shared-memory multiprocessors and operating systems <ref> [9, 13, 14, 20] </ref>. The problem This research is supported in part by the Defense Advanced Research Projects Agency (DARPA) under Grant F30602-97-1-0150 from the U.S. Air Force Research Laboratory. In addition, Greg Plaxton is supported by the National Science Foundation under Grant CCR9504145. <p> In particular, with work stealing, adding and removing processes is easy, and with our non-blocking implementation, we can be lazy about removing processes, because the penalty for operating with more processes than processors is small. 6 Conclusion Whereas traditional thread schedulers demonstrate poor performance in multiprogrammed environments <ref> [9, 13, 14, 20] </ref>, the non-blocking work stealer executes with guaranteed high performance in such environments.
Reference: [14] <author> Andrea C. Dusseau, Remzi H. Arpaci, and David E. Culler. </author> <title> Effective distributed scheduling of parallel workloads. </title> <booktitle> In Proceedings of the ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 2536, </pages> <address> Philadelphia, Pennsyl-vania, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Though such algorithms will work in some multiprogrammed environments, in particular those that employ static space partitioning [13, 26] or co-scheduling [15, 26, 29], they do not work in the multiprogrammed environments being supported by modern shared-memory multiprocessors and operating systems <ref> [9, 13, 14, 20] </ref>. The problem This research is supported in part by the Defense Advanced Research Projects Agency (DARPA) under Grant F30602-97-1-0150 from the U.S. Air Force Research Laboratory. In addition, Greg Plaxton is supported by the National Science Foundation under Grant CCR9504145. <p> Interestingly, it has recently been shown that coscheduling can be achieved with little or no modification to existing multiprocessor operating systems <ref> [14, 31] </ref>. Unfortunately, for some job mixes, coscheduling is not appropriate. For example, a job mix consisting of one parallel computation and one serial computation cannot be coscheduled efficiently. <p> In particular, with work stealing, adding and removing processes is easy, and with our non-blocking implementation, we can be lazy about removing processes, because the penalty for operating with more processes than processors is small. 6 Conclusion Whereas traditional thread schedulers demonstrate poor performance in multiprogrammed environments <ref> [9, 13, 14, 20] </ref>, the non-blocking work stealer executes with guaranteed high performance in such environments.
Reference: [15] <author> Dror G. Feitelson and Larry Rudolph. </author> <title> Coscheduling based on run-time identification of activity working sets. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 23(2):135160, </volume> <month> April </month> <year> 1995. </year>
Reference-contexts: Such scheduling algorithms dynamically map threads onto the processors with the goal of achieving P -fold speedups. Though such algorithms will work in some multiprogrammed environments, in particular those that employ static space partitioning [13, 26] or co-scheduling <ref> [15, 26, 29] </ref>, they do not work in the multiprogrammed environments being supported by modern shared-memory multiprocessors and operating systems [9, 13, 14, 20]. The problem This research is supported in part by the Defense Advanced Research Projects Agency (DARPA) under Grant F30602-97-1-0150 from the U.S. Air Force Research Laboratory. <p> The practical application and possible adaptation of this idea to multiprogrammed environments is an open question. Prior work that has considered multiprogrammed environments has focused on the kernel-level scheduler. With coscheduling (also called gang scheduling) <ref> [15, 29] </ref>, all of the processes belonging to a computation are scheduled simultaneously, thereby giving the computation the illusion of running on a dedicated machine. Interestingly, it has recently been shown that coscheduling can be achieved with little or no modification to existing multiprocessor operating systems [14, 31].
Reference: [16] <author> Raphael Finkel and Udi Manber. </author> <title> DIB A distributed implementation of backtracking. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(2):235256, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: Moreover, when coupled with dag-consistent distributed shared memory, work stealing is also efficient with respect to page faults [6]. For these reasons, work stealing is practical and variants have been implemented in many systems <ref> [7, 16, 17, 21, 30, 34] </ref>. For general multithreaded computations, other scheduling algorithms have also been shown to be simultaneously efficient with respect to time and space [4, 5, 11, 12].
Reference: [17] <author> Vincent W. Freeh, David K. Lowenthal, and Gregory R. Andrews. </author> <title> Distributed Filaments: Efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, pages 201213, </booktitle> <address> Monterey, Cali-fornia, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Moreover, when coupled with dag-consistent distributed shared memory, work stealing is also efficient with respect to page faults [6]. For these reasons, work stealing is practical and variants have been implemented in many systems <ref> [7, 16, 17, 21, 30, 34] </ref>. For general multithreaded computations, other scheduling algorithms have also been shown to be simultaneously efficient with respect to time and space [4, 5, 11, 12].
Reference: [18] <author> Matteo Frigo, Charles E. Leiserson, and Keith H. Randall. </author> <booktitle> The implementation of the Cilk-5 multithreaded language. In Proceedings of the 1998 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: 1 Introduction For shared-memory multiprocessors, parallel applications use multiple threads and are coded using a parallelizing compiler, a threads library, or a multithreaded language such as Cilk <ref> [7, 18] </ref> or Java [3]. In addition to supporting multithreaded applications, multiprocessors also support multiprogrammed workloads in which a mix of serial and parallel, interactive and batch applications may execute concurrently. A major factor in the performance of such workloads is the operation of the thread scheduler. <p> As an alternative, the process may preempt the thread that it was working on, push that thread onto the bottom of its deque, and commence executing the newly ready thread. This alternative admits optimizations in thread management such as lazy-task creation <ref> [18, 19, 27] </ref>. So long as the deque of a process is non-empty, the process manipulates the deque in a LIFO (stack-like) manner. <p> In the near future, we plan to transfer this technology into production-quality systems and investigate its application to real-time and multimedia systems. Likely targets for technology transfer include the Cilk <ref> [7, 18] </ref> and Java [3] multithreaded languages, a POSIX threads library, and a parallelizing compiler. We conjecture that if our user-level thread scheduler is coupled with a kernel scheduler that can deliver guarantees on P A , then we can deliver end-to-end execution-time guarantees for real-time and multimedia applications.
Reference: [19] <author> Seth Copen Goldstein, Klaus Erik Schauser, and David E. Culler. </author> <title> Lazy threads: Implementing a fast parallel call. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 37(1):520, </volume> <month> August </month> <year> 1996. </year>
Reference-contexts: As an alternative, the process may preempt the thread that it was working on, push that thread onto the bottom of its deque, and commence executing the newly ready thread. This alternative admits optimizations in thread management such as lazy-task creation <ref> [18, 19, 27] </ref>. So long as the deque of a process is non-empty, the process manipulates the deque in a LIFO (stack-like) manner.
Reference: [20] <author> Anoop Gupta, Andrew Tucker, and Shigeru Urushibara. </author> <title> The impact of operating system scheduling policies and synchronization methods on the performance of parallel applications. </title> <booktitle> In Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: Though such algorithms will work in some multiprogrammed environments, in particular those that employ static space partitioning [13, 26] or co-scheduling [15, 26, 29], they do not work in the multiprogrammed environments being supported by modern shared-memory multiprocessors and operating systems <ref> [9, 13, 14, 20] </ref>. The problem This research is supported in part by the Defense Advanced Research Projects Agency (DARPA) under Grant F30602-97-1-0150 from the U.S. Air Force Research Laboratory. In addition, Greg Plaxton is supported by the National Science Foundation under Grant CCR9504145. <p> In particular, with work stealing, adding and removing processes is easy, and with our non-blocking implementation, we can be lazy about removing processes, because the penalty for operating with more processes than processors is small. 6 Conclusion Whereas traditional thread schedulers demonstrate poor performance in multiprogrammed environments <ref> [9, 13, 14, 20] </ref>, the non-blocking work stealer executes with guaranteed high performance in such environments.
Reference: [21] <author> Robert H. Halstead, Jr. </author> <title> Implementation of Multilisp: Lisp on a multiprocessor. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 917, </pages> <address> Austin, Texas, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: Moreover, when coupled with dag-consistent distributed shared memory, work stealing is also efficient with respect to page faults [6]. For these reasons, work stealing is practical and variants have been implemented in many systems <ref> [7, 16, 17, 21, 30, 34] </ref>. For general multithreaded computations, other scheduling algorithms have also been shown to be simultaneously efficient with respect to time and space [4, 5, 11, 12].
Reference: [22] <author> M. Herlihy and J. Wing. </author> <title> Axioms for concurrent objects. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1326, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: A deque implementation meets the ideal semantics if and only if for any execution, the associated set of invocations meets the ideal semantics. We remark that a deque implementation meets the ideal semantics if and only if each of the three deque methods is linearizable, as defined in <ref> [22] </ref>. It is convenient to define a set of invocations to be good if and only if no two pushBottom or popBottom invocations are concurrent.
Reference: [23] <author> Maurice Herlihy. </author> <title> A methodology for implementing highly concurrent data structures. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), pages 197206, </booktitle> <address> Seattle, Washington, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: We present a non-blocking implementation of the work-stealing algorithm [8], and we analyze the performance of this non-blocking work stealer in multiprogrammed environments. In this implementation, all concurrent data structures are non-blocking <ref> [23, 24] </ref> so that if the kernel preempts a process, it does not hinder other pro cesses, for example by holding locks. <p> Any constant-time deque implementation meeting the ideal semantics is wait-free [24]. Unfortunately, we are not aware of any constant-time wait-free deque implementation. For this reason, we go on to define a relaxed semantics for the deque methods. Any constant-time deque implementation meeting the relaxed semantics is non-blocking <ref> [23, 24] </ref> and is sufficient for us to prove our performance bounds. We now define the ideal deque semantics. To do so, we first define whether a given set of invocations of the deque methods meets the ideal semantics.
Reference: [24] <author> Maurice Herlihy. </author> <title> Wait-free synchronization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(1):124149, </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: We present a non-blocking implementation of the work-stealing algorithm [8], and we analyze the performance of this non-blocking work stealer in multiprogrammed environments. In this implementation, all concurrent data structures are non-blocking <ref> [23, 24] </ref> so that if the kernel preempts a process, it does not hinder other pro cesses, for example by holding locks. <p> A deque implementation is defined to be constant-time if and only if each of the three methods terminates within a constant number of instructions. Below we define the ideal semantics of these methods. Any constant-time deque implementation meeting the ideal semantics is wait-free <ref> [24] </ref>. Unfortunately, we are not aware of any constant-time wait-free deque implementation. For this reason, we go on to define a relaxed semantics for the deque methods. Any constant-time deque implementation meeting the relaxed semantics is non-blocking [23, 24] and is sufficient for us to prove our performance bounds. <p> Any constant-time deque implementation meeting the ideal semantics is wait-free [24]. Unfortunately, we are not aware of any constant-time wait-free deque implementation. For this reason, we go on to define a relaxed semantics for the deque methods. Any constant-time deque implementation meeting the relaxed semantics is non-blocking <ref> [23, 24] </ref> and is sufficient for us to prove our performance bounds. We now define the ideal deque semantics. To do so, we first define whether a given set of invocations of the deque methods meets the ideal semantics. <p> Such an implementation requires the use of a universal primitive such as compare-and-swap or load-linked/store-conditional <ref> [24] </ref>. Almost all modern microprocessors have such instructions. In our deque implementation we employ a compare-and-swap instruction, but this instruction can be replaced with a load-linked/store-conditional pair in a straightforward manner [28]. The compare-and-swap instruction cas operates as follows.
Reference: [25] <author> M. Frans Kaashoek, Dawson R. Engler, Gregory R. Ganger, Hector M. Brice no, Russell Hunt, David Mazieres, Thomas Pinckney, Robert Grimm, John Jannotti, and Kenneth Mackenzie. </author> <title> Application performance and flexibility on exokernel systems. </title> <booktitle> In Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles, </booktitle> <address> Saint-Malo, France, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: Specifically, before the execution begins the oblivious adversary commits itself to a complete kernel schedule. To deal with an oblivious adversary, we employ a directed yield <ref> [1, 25] </ref> that we call yieldTo. If at round i process q calls yieldTo (r), then the kernel cannot schedule process q again until it has scheduled process r.
Reference: [26] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Mon-ica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272285, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Such scheduling algorithms dynamically map threads onto the processors with the goal of achieving P -fold speedups. Though such algorithms will work in some multiprogrammed environments, in particular those that employ static space partitioning <ref> [13, 26] </ref> or co-scheduling [15, 26, 29], they do not work in the multiprogrammed environments being supported by modern shared-memory multiprocessors and operating systems [9, 13, 14, 20]. The problem This research is supported in part by the Defense Advanced Research Projects Agency (DARPA) under Grant F30602-97-1-0150 from the U.S. <p> Such scheduling algorithms dynamically map threads onto the processors with the goal of achieving P -fold speedups. Though such algorithms will work in some multiprogrammed environments, in particular those that employ static space partitioning [13, 26] or co-scheduling <ref> [15, 26, 29] </ref>, they do not work in the multiprogrammed environments being supported by modern shared-memory multiprocessors and operating systems [9, 13, 14, 20]. The problem This research is supported in part by the Defense Advanced Research Projects Agency (DARPA) under Grant F30602-97-1-0150 from the U.S. Air Force Research Laboratory.
Reference: [27] <author> Eric Mohr, David A. Kranz, and Robert H. Halstead, Jr. </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3):264 280, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: As an alternative, the process may preempt the thread that it was working on, push that thread onto the bottom of its deque, and commence executing the newly ready thread. This alternative admits optimizations in thread management such as lazy-task creation <ref> [18, 19, 27] </ref>. So long as the deque of a process is non-empty, the process manipulates the deque in a LIFO (stack-like) manner.
Reference: [28] <author> Mark Moir. </author> <title> Practical implementations of non-blocking synchronization primitives. </title> <booktitle> In Proceedings of the 16th ACM Symposium on Principles of Distributed Computing, </booktitle> <address> Santa Barbara, California, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: Such an implementation requires the use of a universal primitive such as compare-and-swap or load-linked/store-conditional [24]. Almost all modern microprocessors have such instructions. In our deque implementation we employ a compare-and-swap instruction, but this instruction can be replaced with a load-linked/store-conditional pair in a straightforward manner <ref> [28] </ref>. The compare-and-swap instruction cas operates as follows. It takes three operands: a register addr that holds an address and two other registers, old and new, holding arbitrary values. <p> For simplicity, in Figure 3 we show the tag being manipulated as a counter, with a new tag being selected by incrementing the old tag (Line 12 of popBottom). Such a tag might wrap around, so in practice, we implement the tag by adapting the bounded tags algorithm <ref> [28] </ref>.
Reference: [29] <author> John K. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Proceedings of the 3rd International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1982. </year>
Reference-contexts: Such scheduling algorithms dynamically map threads onto the processors with the goal of achieving P -fold speedups. Though such algorithms will work in some multiprogrammed environments, in particular those that employ static space partitioning [13, 26] or co-scheduling <ref> [15, 26, 29] </ref>, they do not work in the multiprogrammed environments being supported by modern shared-memory multiprocessors and operating systems [9, 13, 14, 20]. The problem This research is supported in part by the Defense Advanced Research Projects Agency (DARPA) under Grant F30602-97-1-0150 from the U.S. Air Force Research Laboratory. <p> The practical application and possible adaptation of this idea to multiprogrammed environments is an open question. Prior work that has considered multiprogrammed environments has focused on the kernel-level scheduler. With coscheduling (also called gang scheduling) <ref> [15, 29] </ref>, all of the processes belonging to a computation are scheduled simultaneously, thereby giving the computation the illusion of running on a dedicated machine. Interestingly, it has recently been shown that coscheduling can be achieved with little or no modification to existing multiprocessor operating systems [14, 31].
Reference: [30] <author> Jaswinder Pal Singh, Anoop Gupta, and Marc Levoy. </author> <title> Parallel visualization algorithms: Performance and architectural implications. </title> <journal> IEEE Computer, </journal> <volume> 27(7):4555, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: Moreover, when coupled with dag-consistent distributed shared memory, work stealing is also efficient with respect to page faults [6]. For these reasons, work stealing is practical and variants have been implemented in many systems <ref> [7, 16, 17, 21, 30, 34] </ref>. For general multithreaded computations, other scheduling algorithms have also been shown to be simultaneously efficient with respect to time and space [4, 5, 11, 12].
Reference: [31] <author> Patrick G. Sobalvarro and William E. Weihl. </author> <title> Demand-based cosched-uling of parallel jobs on multiprogrammed multiprocessors. </title> <booktitle> In Proceedings of the IPPS '95 Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Interestingly, it has recently been shown that coscheduling can be achieved with little or no modification to existing multiprocessor operating systems <ref> [14, 31] </ref>. Unfortunately, for some job mixes, coscheduling is not appropriate. For example, a job mix consisting of one parallel computation and one serial computation cannot be coscheduled efficiently.
Reference: [32] <author> Andrew Tucker and Anoop Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <pages> pages 159166, </pages> <address> Litchfield Park, Arizona, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: Unfortunately, for some job mixes, coscheduling is not appropriate. For example, a job mix consisting of one parallel computation and one serial computation cannot be coscheduled efficiently. With process control <ref> [32] </ref>, processors are dynamically partitioned among the running computations so that each computation runs on a set of processors that grows and shrinks over time, and each computation creates and kills processes so that the number of processes matches the number of processors.
Reference: [33] <author> Jeffrey D. Ullman. </author> <title> NP-complete scheduling problems. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 10:384393, </volume> <year> 1975. </year>
Reference-contexts: In addition, though we shall not prove it, for any kernel schedule, some greedy execution schedule is optimal. We remark that the latter fact does not imply the existence of a polynomial-time algorithm for computing an optimal execution schedule. In fact, the related decision problem is NP-complete <ref> [33] </ref>. 3 Non-blocking work stealing In this section we review the work-stealing algorithm [8], and then describe our non-blocking implementation, which involves the use of a yield system call and a non-blocking implementation of the concurrent data structures.
Reference: [34] <author> Mark T. Vandevoorde and Eric S. Roberts. WorkCrews: </author> <title> An abstraction for controlling parallelism. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(4):347366, </volume> <month> August </month> <year> 1988. </year> <month> 11 </month>
Reference-contexts: Moreover, when coupled with dag-consistent distributed shared memory, work stealing is also efficient with respect to page faults [6]. For these reasons, work stealing is practical and variants have been implemented in many systems <ref> [7, 16, 17, 21, 30, 34] </ref>. For general multithreaded computations, other scheduling algorithms have also been shown to be simultaneously efficient with respect to time and space [4, 5, 11, 12].
References-found: 34

