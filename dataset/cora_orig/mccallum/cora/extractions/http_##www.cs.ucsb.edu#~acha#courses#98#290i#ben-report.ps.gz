URL: http://www.cs.ucsb.edu/~acha/courses/98/290i/ben-report.ps.gz
Refering-URL: http://www.cs.ucsb.edu/~acha/courses/98/290i.html
Root-URL: http://www.cs.ucsb.edu
Email: fveho, besmith, tyangg@cs.ucsb.edu  
Title: Cooperative Caching of Dynamic Content on a Distributed Web Server  
Author: Vegard Holmedahl, Ben Smith, Tao Yang 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California  
Abstract: In this paper we propose a new method for improving the average response time of Web servers by cooperatively caching the results of requests for dynamic content. The work is motivated by our recent study of access logs from the Alexandria Digital Library server at UCSB, which demonstrates that approximately a 30 percent decrease in average response time could be achieved by caching dynamically generated content. We have developed a distributed Web server called Swala, in which the nodes cooperatively cache the results of CGI requests. We use a two-level cache table consistency protocol and a replicated global cache dictionary to maximize the system performance and minimize overhead in responding to dynamic Web requests. Our experiments show that the single-node performance of Swala without caching is comparable to the Netscape Enterprise server, substantial speedups are obtained using caching, and cache hit rate is substantially higher with cooperative cache than with standalone cache.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.Andresen, L.Carver, R.Dolin, C.Fischer, J.Frew, M.Goodchild, O.Ibarra, R.Kothuri, M.Larsgaard, B.Manjunath, D.Nebert, J.Simpson, T.Smith, T.Yang, Q.Zheng, </author> <title> "The WWW Prototype of the Alexan-dria Digital Library", </title> <booktitle> Proceedings of ISDL'95: International Symposium on Digital Libraries, </booktitle> <address> Japan August 22 - 25, </address> <year> 1995. </year> <month> 15 </month>
Reference-contexts: We have developed a distributed Web server, called Swala, which cooperatively caches the results of CGI requests. Our work is motivated, in part, by our experience with the Alexandria Digital Library (ADL) system <ref> [1] </ref> developed at UCSB. The current ADL system provides on-line browsing and processing of digitized maps and other geo-spatially mapped data through the Web. <p> However, these papers do not study caching. 2 3 Access log analysis In this section we illustrate the benefits of dynamic request results caching by analyzing an access log from the Alexandria Digital Library (ADL) at UCSB <ref> [1] </ref>. We have studied its log for September and October 1997, which contains a total of 69,990 requests. After filtering out HEAD and POST requests [5], we have re-sent the requests to the server and timed them.
Reference: [2] <author> D.Andresen, T.Yang, V.Holmedahl, O.Ibarra, "SWEB: </author> <title> Towards a Scalable World Wide Web Server on Multicomputers", </title> <booktitle> Proc. of 10th IEEE International Symp. on Parallel Processing (IPPS'96), </booktitle> <pages> pp. 850-856. </pages> <month> April, </month> <year> 1996. </year>
Reference-contexts: 1 Introduction World Wide Web usage has had an explosive growth in the last few years. In order to accommodate the growth, improving Web performance has become increasingly important. Typical methods for improving performance are file caching with proxies [6, 16, 7], and load balancing multi-node Web servers <ref> [2, 9, 14] </ref>. Research shows that for file fetches on the Web, the network is responsible for a significant portion of the response time. Thus, Web proxy caching has been found to be effective because it reduces the network bottleneck by keeping copies of files closer to clients. <p> For call mechanisms such as CGI, the operating system overhead for this call is significant, as we demonstrate in one of our experiments in Section 4. Our work overcomes both limitations, since our cache is built into the Web server. Several papers <ref> [2, 9, 14] </ref> address load balancing within a group of Web servers to improve scalability. <p> However, the next time a client wants to access information at the Web site, it will reuse the address from the previous request, bypassing the round-robin distribution of the DNS. This imbalance can be further adjusted by using redirection, as addressed in the SWEB project <ref> [2] </ref>. Our focus is to measure the effect of distributed caching without any help from load balancing. nodes varying from 1 to 8.
Reference: [3] <author> D.Andresen, T.Yang, O.Egecioglu, O.H.Ibarra, T.R.Smith, </author> <title> "Scalability Issues for High Performance Digital Libraries on the World Wide Web", </title> <booktitle> Proc. of the 3rd IEEE Forum on Research and Tech. Advances in Digital Libraries (ADL96), </booktitle> <pages> pp. 139-148, </pages> <month> May, </month> <year> 1996. </year>
Reference: [4] <author> G.Bang, P.Druschel, </author> <title> "Measuring the Capacity of a Web Server", </title> <booktitle> Proc. of the USENIX Symposium on Internet Technologies and Systems, </booktitle> <month> December, </month> <year> 1997. </year>
Reference: [5] <author> T. Berners-Lee, R. Fielding, and H. Frystyk, </author> <title> Hypertext Transfer Protocol - HTTP/1.0, RFC 1945, </title> <institution> HTTP Working Group, </institution> <month> May, </month> <year> 1996. </year>
Reference-contexts: We have studied its log for September and October 1997, which contains a total of 69,990 requests. After filtering out HEAD and POST requests <ref> [5] </ref>, we have re-sent the requests to the server and timed them. Illegal requests have been removed from the result file before analyzing the statistics, so the total number of requests studied is 69,337, of which 28,663 (41.3program. The longest running request among those studied is 127.7 seconds.
Reference: [6] <author> P.Cao, S.Irani, </author> <title> "Cost-Aware WWW Proxy Caching Algorithms", </title> <booktitle> Proc. of the USENIX Symposium on Internet Technologies and Systems, </booktitle> <month> December, </month> <year> 1997. </year>
Reference-contexts: 1 Introduction World Wide Web usage has had an explosive growth in the last few years. In order to accommodate the growth, improving Web performance has become increasingly important. Typical methods for improving performance are file caching with proxies <ref> [6, 16, 7] </ref>, and load balancing multi-node Web servers [2, 9, 14]. Research shows that for file fetches on the Web, the network is responsible for a significant portion of the response time. <p> Section 3 discusses our analysis of a recent access log from the ADL server. Section 4 presents the design of our system and consistency protocol. Section 5 presents the experimental results. Section 6 concludes the paper, and section 7 contains our acknowledgements. 2 Related work A number of papers <ref> [6, 16, 7] </ref> discuss Web caching; however, they focus on file caching rather than caching of dynamic content. Furthermore, they concentrate on proxy caching rather than server-side caching. <p> There are two reasons for this. First, our access log analysis in Section 3 shows that execution of dynamic requests generally takes orders of magnitude more time than file fetches. Secondly, previous work on Web file caching <ref> [6, 16] </ref> has determined that for file requests, the network is the bottleneck, so file caching should occur as close to the client as possible, i.e. at a proxy server rather than at the base Web server.
Reference: [7] <author> A.Chankhunthod, P.Danzig, C.Neerdaels, M.Schwartz and K.Worrell, </author> <title> "A Hierarchical Internet Object Cache", </title> <type> Technical Report 95-611, </type> <institution> Computer Science Department, University of Southern California, </institution> <address> Los Angeles, California, </address> <month> March </month> <year> 1995. </year> <title> [8] "The Common Gateway Interface", </title> <address> http://hoohoo.ncsa.uiuc.edu/cgi/. </address>
Reference-contexts: 1 Introduction World Wide Web usage has had an explosive growth in the last few years. In order to accommodate the growth, improving Web performance has become increasingly important. Typical methods for improving performance are file caching with proxies <ref> [6, 16, 7] </ref>, and load balancing multi-node Web servers [2, 9, 14]. Research shows that for file fetches on the Web, the network is responsible for a significant portion of the response time. <p> Section 3 discusses our analysis of a recent access log from the ADL server. Section 4 presents the design of our system and consistency protocol. Section 5 presents the experimental results. Section 6 concludes the paper, and section 7 contains our acknowledgements. 2 Related work A number of papers <ref> [6, 16, 7] </ref> discuss Web caching; however, they focus on file caching rather than caching of dynamic content. Furthermore, they concentrate on proxy caching rather than server-side caching.
Reference: [9] <author> D.Dias, W.Kish, R.Mukherjee, R.Tewari, </author> <title> "A Scalable and Highly Available Web Server", </title> <booktitle> Proc. of COMPCON 1996, Forty-First IEEE Computer Society International Conference: Technologies for the Information Superhighway, </booktitle> <address> Santa Clara, California, </address> <month> February, </month> <year> 1996. </year>
Reference-contexts: 1 Introduction World Wide Web usage has had an explosive growth in the last few years. In order to accommodate the growth, improving Web performance has become increasingly important. Typical methods for improving performance are file caching with proxies [6, 16, 7], and load balancing multi-node Web servers <ref> [2, 9, 14] </ref>. Research shows that for file fetches on the Web, the network is responsible for a significant portion of the response time. Thus, Web proxy caching has been found to be effective because it reduces the network bottleneck by keeping copies of files closer to clients. <p> For call mechanisms such as CGI, the operating system overhead for this call is significant, as we demonstrate in one of our experiments in Section 4. Our work overcomes both limitations, since our cache is built into the Web server. Several papers <ref> [2, 9, 14] </ref> address load balancing within a group of Web servers to improve scalability.
Reference: [10] <author> S.Gadde, M.Rabinovich, J.Chase, </author> <title> "Reduce, Reuse, Recycle: An Approach to Building Large Internet Caches", </title> <booktitle> Workshop on Hot Topics in Operating Systems (HotOS), </booktitle> <month> May </month> <year> 1997. </year>
Reference: [11] <author> James Gwertzman, Margo Seltzer, </author> <title> "World Wide Web Cache Consistency," </title> <booktitle> Proceedings of the 1996 USENIX Technical Conference, </booktitle> <address> San Diego, CA, </address> <month> Jan </month> <year> 1996. </year>
Reference-contexts: Furthermore, they concentrate on proxy caching rather than server-side caching. Proxy caching of dynamic content may not be feasible because it does not permit caching of authenticated content, and hinders cache replacement algorithms based on job execution time. Gwertzman and Seltzer <ref> [11] </ref> have given a motivation for our research, by noting a tenfold increase in dynamically generated pages in six months, and suggesting that this increase in dynamic requests must be met by caching the scripts that generate the dynamic results. Their proposed approach has three disadvantages.
Reference: [12] <author> J.Hu, I.Pyarali, D.Schmidt, </author> <title> "Measuring the Impact of Event Dispatching and Concurrency Models on Web Server Performance Over High-speed Networks", </title> <booktitle> Proceedings of the 2nd Global Internet Conference, </booktitle> <month> November 4-8, </month> <year> 1997. </year>
Reference-contexts: The concurrency of multi-threading raises consistency issues, which we discuss below. We use memory-mapped I/O whenever possible, to minimize the number of system calls and eliminate double-buffering. Multi-threading and memory-mapped I/O are shown to be important components of efficient Web servers <ref> [12] </ref>. 4.1 Module design As illustrated in Figure 1, there are two primary runtime modules in Swala on every node: 1. The control module starts and shuts down all other modules. <p> NCSA HTTPd is a widely used research package, and Netscape Enterprise is considered one of the most efficient commercial Web servers available <ref> [12] </ref>. <p> We do not have access to the Enterprise source code, but we suspect that the reason for this is the thread management; Enterprise uses a thread-pool instead of starting the threads as requests come in <ref> [12] </ref>, and the management of the pool may become cumbersome as the number of clients increases. We further compare the performance of Netscape, HTTPd and Swala (with and without caching) for CGI program execution.
Reference: [13] <author> A.Iyengar, J.Challenger, </author> <title> "Improving Web Server Performance by Caching Dynamic Data", </title> <booktitle> Proc. of the USENIX Symposium on Internet Technologies and Systems, </booktitle> <month> December, </month> <year> 1997. </year>
Reference-contexts: Finally, the proxy must be trusted by the original server to receive the scripts and source files. Most proxies operate as normal HTTP clients toward servers; this is not possible with Gwertzman and Seltzer's approach. Caching the results of dynamic requests has been studied recently by IBM <ref> [13] </ref>. They have written a cache server and rewritten their server applications to insert and delete items in this cache. There are two main drawbacks to this approach. First, they require that the server application be rewritten to take advantage of the cache. <p> This is discussed further in the next section. While our analysis has been limited to the digital library application, we expect that other Web sites making extensive use of dynamic requests also can benefit from dynamic content caching. This is verified by recent work <ref> [13] </ref>. 4 Design of the Swala distributed Web server Swala is a multi-threaded, distributed Web server that runs on a cluster of workstations and shares cache information and cache data between nodes. The caches contain the results of requests for dynamic content, produced by CGI programs [8]. <p> Our scheme is reasonable for a Web site with infrequent updates of documents and CGI scripts. This is true in digital library applications, where the material available through the Web server normally is read-only. Iyengar et al. <ref> [13] </ref> maintain content consistency by requiring that the application, rather than the server, be responsible for deletion of stale cache entries.
Reference: [14] <author> E.D. Katz, M. Butler, R. McGrath, </author> <title> A Scalable HTTP Server: the NCSA Prototype, </title> <journal> Computer Networks and ISDN Systems. </journal> <volume> vol. 27, </volume> <year> 1994, </year> <pages> pp. 155-164. </pages>
Reference-contexts: 1 Introduction World Wide Web usage has had an explosive growth in the last few years. In order to accommodate the growth, improving Web performance has become increasingly important. Typical methods for improving performance are file caching with proxies [6, 16, 7], and load balancing multi-node Web servers <ref> [2, 9, 14] </ref>. Research shows that for file fetches on the Web, the network is responsible for a significant portion of the response time. Thus, Web proxy caching has been found to be effective because it reduces the network bottleneck by keeping copies of files closer to clients. <p> For call mechanisms such as CGI, the operating system overhead for this call is significant, as we demonstrate in one of our experiments in Section 4. Our work overcomes both limitations, since our cache is built into the Web server. Several papers <ref> [2, 9, 14] </ref> address load balancing within a group of Web servers to improve scalability. <p> The DNS answers with different addresses in round-robin fashion, so that the clients initially are distributed evenly among the available Web servers in the group <ref> [14] </ref>. However, the next time a client wants to access information at the Web site, it will reuse the address from the previous request, bypassing the round-robin distribution of the DNS. This imbalance can be further adjusted by using redirection, as addressed in the SWEB project [2].
Reference: [15] <author> Paolo Lorenzetti, Luigi Rizzo, Lorenzo Vicisano, </author> <title> "Replacement policies for a proxy cache," </title> <type> draft, </type> <note> http://www.iet.unipi.it/ luigi/caching.ps.gz. </note>
Reference-contexts: With regard to the first issue, Lorenzetti et.al. claim that it is better to delete cache entries on demand (to make room for a new entry) than to run garbage collection in the background <ref> [15] </ref>. They do not give a basis for their claim, but it might be true in a setting where deleting a document from the cache is quite cheap.
Reference: [16] <author> R.McGrath, </author> <title> "Caching for Large Scale Systems", </title> <journal> D-Lib Magazine, </journal> <month> January </month> <year> 1996. </year> <title> [17] "Netscape Server Central Index Page", </title> <note> http://home.netscape.com/comprod/server central/. 16 [18] "The NCSA HTTPd Home Page", http://hoohoo.ncsa.uiuc.edu/. </note>
Reference-contexts: 1 Introduction World Wide Web usage has had an explosive growth in the last few years. In order to accommodate the growth, improving Web performance has become increasingly important. Typical methods for improving performance are file caching with proxies <ref> [6, 16, 7] </ref>, and load balancing multi-node Web servers [2, 9, 14]. Research shows that for file fetches on the Web, the network is responsible for a significant portion of the response time. <p> Section 3 discusses our analysis of a recent access log from the ADL server. Section 4 presents the design of our system and consistency protocol. Section 5 presents the experimental results. Section 6 concludes the paper, and section 7 contains our acknowledgements. 2 Related work A number of papers <ref> [6, 16, 7] </ref> discuss Web caching; however, they focus on file caching rather than caching of dynamic content. Furthermore, they concentrate on proxy caching rather than server-side caching. <p> There are two reasons for this. First, our access log analysis in Section 3 shows that execution of dynamic requests generally takes orders of magnitude more time than file fetches. Secondly, previous work on Web file caching <ref> [6, 16] </ref> has determined that for file requests, the network is the bottleneck, so file caching should occur as close to the client as possible, i.e. at a proxy server rather than at the base Web server.
Reference: [19] <author> A. Vahdat, T. Anderson, </author> <title> "Transparent Result Caching," </title> <note> unpublished. [20] "WebStone", http://www.sgi.com/Products/WebFORCE/WebStone/index.html. 17 </note>
Reference-contexts: We plan to investigate cache entry invalidation methods in future versions of Swala, by receiving invalidation messages from applications after the model of Iyengar et al., and by monitoring the input of the CGI programs whose output is being cached, to detect invalidation, as suggested by Vahdat and Anderson <ref> [19] </ref>. We address cache table consistency with a two-level consistency protocol: Intra-server and inter-server consistency. Every node can handle multiple concurrent requests, and the caches on the different nodes need to be coordinated to allow cache data sharing. Below, we describe our design of the protocol. * Intra-node consistency.
References-found: 16

