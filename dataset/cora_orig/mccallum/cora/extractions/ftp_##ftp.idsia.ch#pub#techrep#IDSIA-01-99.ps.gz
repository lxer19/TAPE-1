URL: ftp://ftp.idsia.ch/pub/techrep/IDSIA-01-99.ps.gz
Refering-URL: http://www.idsia.ch/techrep.html
Root-URL: http://www.idsia.ch/techrep.html
Email: felix@idsia.ch juergen@idsia.ch fred@idsia.ch  
Title: Learning to Forget: Continual Prediction with  
Author: LSTM Felix A. Gers Jurgen Schmidhuber Fred Cummins 
Web: www.idsia.ch  
Address: Corso Elvezia 36 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Date: January, 1999  
Pubnum: Technical Report IDSIA-01-99  
Abstract: Long Short-Term Memory (LSTM, Hochreiter & Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive "forget gate" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them in an elegant way. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bengio, Y., Simard, P., and Frasconi, P. </author> <year> (1994). </year> <title> Learning long-term dependencies with gradient descent is difficult. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 157-166. </pages>
Reference: <author> Cleeremans, A., Servan-Schreiber, D., and McClelland, J. L. </author> <year> (1989). </year> <title> Finite-state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 372-381. </pages>
Reference-contexts: fraction" 25,000 ELM 15 435 0 &gt;200,000 Std. 3bl., LSTM size 2 276 0.5 100 8,440 Table 1: Standard embedded Reber grammar (ERG): percentage of successful trials and number of sequence presentations until success for RTRL (results taken from Smith and Zipser 1989 ), "Elman net trained by Elman's procedure" <ref> (results taken from Cleeremans et al. 1989 ) </ref>, "Recurrent Cascade-Correlation" (results taken from Fahlman 1991 ) and LSTM (results taken from Hochreiter and Schmid-huber 1997 ). Weight numbers in the first 4 rows are estimates.
Reference: <author> Cummins, F., Gers, F., and Schmidhuber, J. </author> <year> (1999). </year> <title> Automatic discrimination among languages based on prosody alone. </title> <type> Technical Report IDSIA-03-99, </type> <institution> IDSIA, Lugano, CH. </institution>
Reference: <author> Doya, K. and Shuji, Y. </author> <year> (1989). </year> <title> Adaptive neural oscillator using continuous-time backpropagation learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 2(5) </volume> <pages> 375-385. </pages>
Reference-contexts: In fact, state decay does not significantly improve experimental performance (see "State Decay" in Table 2). Of course we might try to "teacher force" (Jordan, 1986) <ref> (Doya and Shuji, 1989) </ref> the internal states s c by resetting them once a new training sequence starts. But this requires an external teacher that knows how to segment the input stream into training subsequences.
Reference: <author> Elman, J. L. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14(2) </volume> <pages> 179-211. </pages>
Reference: <author> Fahlman, S. E. </author> <year> (1991). </year> <title> The recurrent cascade-correlation learning algorithm. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> NIPS 3, </booktitle> <pages> pages 190-196. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: size 2 276 0.5 100 8,440 Table 1: Standard embedded Reber grammar (ERG): percentage of successful trials and number of sequence presentations until success for RTRL (results taken from Smith and Zipser 1989 ), "Elman net trained by Elman's procedure" (results taken from Cleeremans et al. 1989 ), "Recurrent Cascade-Correlation" <ref> (results taken from Fahlman 1991 ) </ref> and LSTM (results taken from Hochreiter and Schmid-huber 1997 ). Weight numbers in the first 4 rows are estimates.
Reference: <author> Haffner, P. and Waibel, A. </author> <year> (1992). </year> <title> Multi-state time delay networks for continuous speech recognition. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippmann, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 135-142. </pages> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Hinton, G. </author> <year> (1986). </year> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cogni tive Science Society, </booktitle> <pages> pages 1-12, </pages> <address> Amherst 1986. </address> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale. </address>
Reference-contexts: It could only slow down the growth of cell states indirectly by decreasing the overall activity in the network. We tested several weight decay algorithms <ref> (Hinton, 1986) </ref>, (Weigend et al., 1991) without any encouraging results. Variants of "focused backpropagation" (Mozer, 1989) also do not work well. These let the internal state decay via a self-connection whose weight is smaller than 1.
Reference: <author> Hochreiter, S. </author> <year> (1991). </year> <title> Untersuchungen zu dynamischen neuronalen Netzen. </title> <type> Diploma thesis, </type> <institution> Institut fur Informatik, Lehrstuhl Prof. Brauer, Technische Universitat Munchen. </institution> <note> See www7.informatik.tu-muenchen.de/~hochreit. </note>
Reference-contexts: The temporal evolution of the path integral over all error signals "flowing back in time" exponentially depends on the magnitude of the weights <ref> (Hochreiter, 1991) </ref>. This implies that the backpropagated error quickly either vanishes or blows up (Hochreiter and Schmidhuber, 1997; Bengio et al., 1994). Hence standard RNNs fail to learn in the presence of time lags greater than 5 - 10 discrete time steps between relevant input events and target signals. <p> The effect is that the CECs are the only part of the system through which errors can flow back forever. This makes LSTM's updates efficient without significantly affecting learning power: error flow outside of cells tends to decay exponentially anyway <ref> (Hochreiter, 1991) </ref>. In the equations below, tr = will indicate where we use error truncation and, for simplicity, unless otherwise indicated, we assume only a single cell per block.
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1997). </year> <title> Long short-term memory. </title> <journal> Neural Computation, </journal> <volume> 9(8) </volume> <pages> 1735-1780. </pages>
Reference-contexts: The vanishing error problem casts doubt on whether standard RNNs can indeed exhibit significant practical advantages over time window-based feedforward networks. A recent model, "Long Short-Term Memory" (LSTM), <ref> (Hochreiter and Schmidhuber, 1997) </ref> is not affected by this problem. LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error flow through "constant error carrousels" (CECs) within special units, called cells. <p> Bias weights for LSTM gates are initialized with negative values for input and output gates <ref> (see Hochreiter & Schmidhuber 1997) </ref> for details), positive values for forget gates. This implies| compare equations (10) and (11)|that in the beginning of the training phase the forget gate activation will be almost 1.0, and the entire cell will behave like a standard LSTM cell. <p> It will not explicitly forget anything until it has learned to forget. 3.2 Backward Pass of Extended LSTM with Forget Gates LSTM's backward pass <ref> (see Hochreiter & Schmidhuber 1997 for details) </ref> is an efficient fusion of slightly modified, truncated back propagation through time (BPTT) (e.g Williams & Peng 1990 ) and a customized version of real time recurrent learning (RTRL) (e.g. Robinson & Fallside 1987). <p> Standard embedded Reber grammar (ERG): percentage of successful trials and number of sequence presentations until success for RTRL (results taken from Smith and Zipser 1989 ), "Elman net trained by Elman's procedure" (results taken from Cleeremans et al. 1989 ), "Recurrent Cascade-Correlation" (results taken from Fahlman 1991 ) and LSTM <ref> (results taken from Hochreiter and Schmid-huber 1997 ) </ref>. Weight numbers in the first 4 rows are estimates. <p> Table 2 compares extended LSTM to standard LSTM and an LSTM variant with decay of the internal cell state s c (with a self recurrent weight &lt; 1). Our results for standard LSTM with external resets are slightly better than those based on a different topology <ref> (Hochreiter and Schmidhuber, 1997) </ref>. <p> But can standard LSTM solve problems which extended LSTM cannot? We tested extended LSTM on one of the most difficult nonlinear long time lag tasks ever solved by an RNN: "Noisy Temporal Order" (NTO) <ref> (task 6b taken from Hochreiter & Schmidhuber 1997 ) </ref>. NTO. The goal is to classify sequences of locally represented symbols.
Reference: <author> Jordan, M. I. </author> <year> (1986). </year> <title> Attractor dynamics and parallelism in a connectionist sequential machine. </title> <booktitle> In Proceedings of the Eighth Annual Cognitive Science Society Conference, </booktitle> <address> Hillsdale, NJ. </address> <publisher> Erlbaum. </publisher>
Reference-contexts: In fact, state decay does not significantly improve experimental performance (see "State Decay" in Table 2). Of course we might try to "teacher force" <ref> (Jordan, 1986) </ref> (Doya and Shuji, 1989) the internal states s c by resetting them once a new training sequence starts. But this requires an external teacher that knows how to segment the input stream into training subsequences.
Reference: <author> Lin, T., Horne, B. G., Ti~no, P., and Giles, C. L. </author> <year> (1996). </year> <title> Learning long-term dependencies in NARX recurrent neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(6) </volume> <pages> 1329-1338. </pages>
Reference-contexts: In general the network will not automatically reset itself to a neutral state once a new training sequence starts. How can we solve this problem without losing LSTM's advantages over time delay neural networks (TDNN) (Waibel, 1989) or NARX ("Nonlinear AutoRegressive models with eXogenous Inputs") <ref> (Lin et al., 1996) </ref>, which depend on a priori knowledge of typical time lag sizes? Weight decay does not work. It could only slow down the growth of cell states indirectly by decreasing the overall activity in the network.
Reference: <author> Mozer, M. C. </author> <year> (1989). </year> <title> A focused backpropagation algorithm for temporal pattern processing. </title> <journal> Complex Systems, </journal> <volume> 3 </volume> <pages> 349-381. </pages>
Reference-contexts: It could only slow down the growth of cell states indirectly by decreasing the overall activity in the network. We tested several weight decay algorithms (Hinton, 1986), (Weigend et al., 1991) without any encouraging results. Variants of "focused backpropagation" <ref> (Mozer, 1989) </ref> also do not work well. These let the internal state decay via a self-connection whose weight is smaller than 1.
Reference: <author> Mozer, M. C. </author> <year> (1993). </year> <title> Neural net architectures for temporal sequences processing. </title> <editor> In Weigend, A. S. and Gershenfeld, N. A., editors, </editor> <title> Time series prediction: </title> <booktitle> Forecasting the future and understanding the past, </booktitle> <volume> volume 15, </volume> <pages> pages 243-264. </pages> <publisher> Addison Wesley, </publisher> <address> Reading, MA. </address>
Reference: <author> Mozer, M. C. and Soukup, T. </author> <year> (1991). </year> <title> Connectionist music composition based on melodic and stylistic constraints. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 789-796. </pages> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> O'Reilly, R. C., Braver, T. S., and Cohen, J. D. </author> <year> (1999). </year> <title> A biologically based computational model of working memory. </title> <editor> In Miyake, A. and Shah, P., editors, </editor> <title> Models of Working Memory: Mechanisms of Active Maintenance and Executive Control. </title> <publisher> in press. </publisher>
Reference: <author> Pearlmutter, B. A. </author> <year> (1995). </year> <title> Gradient calculation for dynamic recurrent neural networks: a survey. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 6(5) </volume> <pages> 1212-1228. </pages>
Reference: <author> Robinson, A. J. and Fallside, F. </author> <year> (1987). </year> <title> The utility driven dynamic error propagation network. </title> <type> Technical Report CUED/F-INFENG/TR.1, </type> <institution> Cambridge University Engineering Department. </institution>
Reference: <author> Schmidhuber, J. </author> <year> (1989). </year> <title> The Neural Bucket Brigade: A local learning algorithm for dynamic feedforward and recurrent networks. </title> <journal> Connection Science, </journal> <volume> 1(4) </volume> <pages> 403-412. </pages>
Reference-contexts: So the storage complexity does not depend on the length of the input sequence. Hence extended LSTM is local in space and time <ref> (Schmidhuber, 1989) </ref>, just like standard LSTM. 4 Experiments 4.1 Continual Embedded Reber Grammar Problem To generate an infinite input stream we extend the well-known "embedded Reber grammar" (ERG) benchmark problem, e.g., Smith and Zipser (1989), Cleeremans et al. (1989), Fahlman (1991), Hochreiter & Schmidhuber (1997). Consider Figure 3. ERG.
Reference: <author> Schraudolph, N. </author> <year> (1999). </year> <title> A fast, compact approximation of the exponential function. </title> <journal> Neural Computation, </journal> <volume> 11(4) </volume> <pages> 853-862. </pages>
Reference-contexts: Memory blocks equipped with forget gates may also be capable of developing into internal oscillators or timers, allowing the recognition and generation of hierarchical rhythmic patterns. Acknowledgment This work was supported by SNF grant 2100-49'144.96 "Long Short-Term Memory". Thanks to Nici Schraudolph for providing his fast exponentiation code <ref> (Schraudolph, 1999) </ref> employed to accelerate the computation of exponentials. 16 6 Summary of Extended LSTM in Pseudo-code REPEAT learning loop forward pass net input to hidden layer (self recurrent and from input) reset all net values with bias connection or to zero input gates (1): net in j (t) = P
Reference: <author> Smith, A. W. and Zipser, D. </author> <year> (1989). </year> <title> Learning sequential structures with the real-time recurrent learning algorithm. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(2) </volume> <pages> 125-131. </pages>
Reference-contexts: rate after RTRL 3 170 0.05 "some fraction" 173,000 RTRL 12 494 0.1 "some fraction" 25,000 ELM 15 435 0 &gt;200,000 Std. 3bl., LSTM size 2 276 0.5 100 8,440 Table 1: Standard embedded Reber grammar (ERG): percentage of successful trials and number of sequence presentations until success for RTRL <ref> (results taken from Smith and Zipser 1989 ) </ref>, "Elman net trained by Elman's procedure" (results taken from Cleeremans et al. 1989 ), "Recurrent Cascade-Correlation" (results taken from Fahlman 1991 ) and LSTM (results taken from Hochreiter and Schmid-huber 1997 ). Weight numbers in the first 4 rows are estimates.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference-contexts: This well-known form of forgetting <ref> (Sutton, 1988) </ref> is different from the main topic of this paper, which focuses on learning (via weight changes) to forget short-term memory (STM) contents stored as quickly changing activations 1 . Still, LSTM is affected by the general problem.
Reference: <author> Tani, J. and Nolfi, S. </author> <year> (1998). </year> <title> Learn to perceive the world as articulated: An approach for hierarchical learning. </title> <booktitle> In Proceedings of the Fifth International Conference of the Society for Adaptive Behavior, </booktitle> <pages> pages 633-639. </pages> <publisher> The MIT Press. 18 Tsioutsias, </publisher> <editor> D. I. and Mjolsness, E. </editor> <year> (1996). </year> <title> A mulitscale attentional framework for relaxation neural networks. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 633-639. </pages> <publisher> The MIT Press. </publisher>
Reference: <author> Tsoi, A. C. and Back, A. D. </author> <year> (1994). </year> <title> Locally recurrent globally feedforward networks: A critical review of architectures. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 229-239. </pages>
Reference: <author> Waibel, A. </author> <year> (1989). </year> <title> Modular construction of time-delay neural networksfor speech recognition. </title> <journal> Neural Computation, </journal> <volume> 1(1) </volume> <pages> 39-46. </pages>
Reference-contexts: In general the network will not automatically reset itself to a neutral state once a new training sequence starts. How can we solve this problem without losing LSTM's advantages over time delay neural networks (TDNN) <ref> (Waibel, 1989) </ref> or NARX ("Nonlinear AutoRegressive models with eXogenous Inputs") (Lin et al., 1996), which depend on a priori knowledge of typical time lag sizes? Weight decay does not work. It could only slow down the growth of cell states indirectly by decreasing the overall activity in the network.
Reference: <author> Weigend, A. S., Rumelhart, D. E., and Huberman, B. A. </author> <year> (1991). </year> <title> Generalization by weight-elimination with application to forecasting. </title> <booktitle> In NIPS 3, </booktitle> <pages> pages 875-882. </pages>
Reference-contexts: It could only slow down the growth of cell states indirectly by decreasing the overall activity in the network. We tested several weight decay algorithms (Hinton, 1986), <ref> (Weigend et al., 1991) </ref> without any encouraging results. Variants of "focused backpropagation" (Mozer, 1989) also do not work well. These let the internal state decay via a self-connection whose weight is smaller than 1.
Reference: <author> Werbos, P. J. </author> <year> (1988). </year> <title> Generalization of backpropagation with application to a recurrent gas market model. Neural Networks, </title> <type> 1. </type>
Reference: <author> Williams, R. J. and Peng, J. </author> <year> (1990). </year> <title> An efficient gradient-based algorithm for on-line training of recurrent network trajectories. </title> <journal> Neural Computation, </journal> <volume> 2(4) </volume> <pages> 490-501. </pages>
Reference-contexts: It will not explicitly forget anything until it has learned to forget. 3.2 Backward Pass of Extended LSTM with Forget Gates LSTM's backward pass (see Hochreiter & Schmidhuber 1997 for details) is an efficient fusion of slightly modified, truncated back propagation through time (BPTT) <ref> (e.g Williams & Peng 1990 ) </ref> and a customized version of real time recurrent learning (RTRL) (e.g. Robinson & Fallside 1987). Output units use BPTT; output gates use slightly modified, truncated BPTT. Weights to cells, input gates and the novel forget gates, however, use a truncated version of RTRL.
Reference: <author> Williams, R. J. and Zipser, D. </author> <year> (1992). </year> <title> Gradient-based learning algorithms for recurrent networks and their computational complexity. In Back-propagation: Theory, Architectures and Applications. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher> <pages> 19 </pages>
References-found: 29

