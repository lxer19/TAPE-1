URL: ftp://ftp.cs.washington.edu/tr/1993/12/UW-CSE-93-12-09.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: Abstractions for Portable, Scalable Parallel Programming  
Author: Gail A. Alverson William G. Griswold Calvin Lin David Notkin Lawrence Snyder 
Date: January 26, 1994  
Abstract: In parallel programming, the need to manage communication costs, load imbalance, and irregularities in the computation puts substantial demands on the programmer. Key properties of the architecture, such as the number of processors and the costs of communication, must be exploited to achieve good performance. Coding these properties directly into a program compro mises the portability and flexibility of the code because significant changes are usually needed to port or enhance the program. We describe a parallel programming model that supports the concise, independent description of key aspects of a parallel program|such as data distribution, communication, and boundary conditions|without reference to machine idiosyncrasies. The independence of such components improves portability by allowing the components of a program to be tuned independently, and encourages reuse by supporting the composition of existing components. The architecture-sensitive aspects of a computation are isolated from the rest of the program, reducing the need to make extensive changes to port a program. This model is effective in exploiting both data parallelism and functional parallelism. This paper provides programming examples, compares this work to related languages, and presents performance results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Alverson, W. Griswold, D. Notkin, and L. Snyder. </author> <title> A flexible communication abstraction for nonshared memory parallel computing. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <month> November </month> <year> 1990. </year>
Reference-contexts: The Phase Abstractions extend the CTA in the same way that the sequential imperative programming model extends the von Neumann model. The main components of the Phase Abstractions are the XYZ levels of programming and ensembles <ref> [1, 19, 43] </ref>. 2.1 XYZ Programming Levels A programmer's problem-solving abilities can be improved by dividing a problem into small, man ageable pieces|assuming the pieces are sufficiently independent to be considered separately. Additionally, these pieces can often be reused in other programs, saving time on future problems. <p> Examples of phases include parallel implementations of the FFT, matrix multiplication, matrix transposition, sort, and global maximum. A phase has a characteristic communication structure induced by the data dependencies among the processes. For example, the FFT induces a butterfly, while Batcher's sort induces a hypercube <ref> [1] </ref>. Finally, the Z level corresponds to the actions of the CTA's global controller, where sequences of parallel phases are invoked and synchronized. <p> 16 xproc TYPE [1:s][1:t] operator+(TYPE x [1:s][1:t], TYPE y [1:s][1:t]) - TYPE result [1:s][1:t]; int i, j; for (j=1; j&lt;=t; i++) result [i][j] = x [i][j] + y [i][j]; return result; - xproc void shift (TYPE val [1:s][1:t]) port write_neighbor, read_neighbor; - TYPE temp <ref> [1] </ref>[1:t]; int i; write_neighbor &lt;== val [1]; temp &lt;== read_neighbor; for (i=2; i&lt;=t; i++) val [s] = temp; - xproc int reduce (TYPE val [1:k], TYPE*() op) port Parent, Child [1:n]; - TYPE accum; accum = val [1]; for (i=2; i&lt;=k; i++) accum = op (accum,val [i]); for (i=1; i&lt;=n; i++) accum = op (accum,Child [i]); Parent <p> - xproc void shift (TYPE val [1:s][1:t]) port write_neighbor, read_neighbor; - TYPE temp <ref> [1] </ref>[1:t]; int i; write_neighbor &lt;== val [1]; temp &lt;== read_neighbor; for (i=2; i&lt;=t; i++) val [s] = temp; - xproc int reduce (TYPE val [1:k], TYPE*() op) port Parent, Child [1:n]; - TYPE accum; accum = val [1]; for (i=2; i&lt;=k; i++) accum = op (accum,val [i]); for (i=1; i&lt;=n; i++) accum = op (accum,Child [i]); Parent &lt;== accum; - begin Z double X [1:J][1:K], OldX [1:J][1:K]; ... phase operator+; phase Left; phase Reduce; ... operator+.code = operator+; Left.code = shift; Left.port = WriteLeft (Zero); ... <p> The port ensemble declaration for the latter approach is shown below. /* Rows and Columns communication structure */ Max [i][j].port.P &lt;--&gt; Max [i][j-1].port.C [0] 0 &lt;= i &lt; r, 1 &lt;= j &lt; c Max [i][0].port.P &lt;--&gt; Max [i-1][0].port.C <ref> [1] </ref> 1 &lt;= i &lt; r With the code suitably parameterized, this program can now execute efficiently on a variety of architectures by selecting the proper port ensemble. Locality.
Reference: [2] <author> G. Alverson and D. Notkin. </author> <title> Program structuring for effective parallel portability. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(9), </volume> <month> September </month> <year> 1993. </year>
Reference-contexts: the resulting code in a machine-specific manner, and allow a programmer to perform architecture-specific performance tuning without making extensive modifications to the source code. 1 We define a program to be portable with respect to a given machine if its performance is competitive with machine-specific programs solving the same problem <ref> [2] </ref>. 1 In recent years, a parallel programming style has evolved that might be termed aggregate data- parallel computing. This style of programming is characterized by: * Data parallelism. The program's parallelism comes from executing the same function on many elements of a collection.
Reference: [3] <author> T. S. Axelrod, P. F. Dubois, and P. G. Eltgroth. </author> <title> A simulator for MIMD performance prediction application to the S-1 MkIIa multiprocessor. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 350-358, </pages> <year> 1983. </year>
Reference-contexts: Since its creation it has been studied widely in both sequential and parallel forms <ref> [3, 9, 13, 16, 17, 23, 24, 37, 39] </ref>. Hardware. The portability of our parallel SIMPLE was investigated on the iPSC/2 S, iPSC/2 F, nCUBE/7, Sequent Symmetry, BBN Butterfly GP1000, and a detailed Transputer simulator. These machines are summarized in Table 1.
Reference: [4] <author> G. E. Blelloch. NESL: </author> <title> A nested data-parallel language. </title> <type> Technical Report CMU-CS-92-103, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Low level approaches [25] allow programmers to hand-code data placement. The resulting code typically assumes one particular data decomposition, so if the program is ported to a platform that favors some other decomposition, extensive changes must be made or performance suffers. Other languages <ref> [4, 5, 15] </ref> give the programmer no control over data decomposition, leaving these issues to the compiler or hardware. But because the best choice of data decomposition depends on characteristics of the application, compilers can make poor data placement decisions.
Reference: [5] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Low level approaches [25] allow programmers to hand-code data placement. The resulting code typically assumes one particular data decomposition, so if the program is ported to a platform that favors some other decomposition, extensive changes must be made or performance suffers. Other languages <ref> [4, 5, 15] </ref> give the programmer no control over data decomposition, leaving these issues to the compiler or hardware. But because the best choice of data decomposition depends on characteristics of the application, compilers can make poor data placement decisions.
Reference: [6] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Vienna Fortran a Fortran language extension for distributed memory multiprocessors. </title> <type> Technical Report No. 91-72, </type> <institution> ICASE, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Other languages [4, 5, 15] give the programmer no control over data decomposition, leaving these issues to the compiler or hardware. But because the best choice of data decomposition depends on characteristics of the application, compilers can make poor data placement decisions. Many recent languages <ref> [6, 22] </ref> provide support for data decompositions, but hide communication operations from the programmer and thus do not encourage locality at the algorithmic level. Consequently, there is a reliance on automated means of hiding latency. <p> The communication structure of the processor is not visible to the programmer, but the programmer can change the partitioning clauses on the data aggregates. SPMD processing is allowed, but there are no special facilities for handling edge effects. Parallel Fortrans. Recent languages such as Kali [26], Vienna Fortran <ref> [6] </ref>, and HPF [22] focus on data decomposition as the expression of parallelism. Their data decompositions are similar to the Phase Abstractions notion of data ensembles, but the overall approach differs fundamentally from Phase Abstractions.
Reference: [7] <author> W. Crowley, C. P. Hendrickson, and T. I. Luby. </author> <title> The Simple code. </title> <type> Technical Report UCID-17715, </type> <institution> Lawrence Livermore Laboratory, </institution> <year> 1978. </year>
Reference-contexts: GP1000 simulator nodes 20 32 32 64 24 64 processors Intel 80386 Intel 80386 Intel 80386 custom Motorola 68020 T800 memory 32MB 4 MB/node 8 MB/node 512 KB/node 4 MB/node N/A cache 64KB 64 KB 64KB none none network bus hypercube hypercube hypercube omega mesh Table 1: Machine Characteristics compilers <ref> [7] </ref>. Since its creation it has been studied widely in both sequential and parallel forms [3, 9, 13, 16, 17, 23, 24, 37, 39]. Hardware. The portability of our parallel SIMPLE was investigated on the iPSC/2 S, iPSC/2 F, nCUBE/7, Sequent Symmetry, BBN Butterfly GP1000, and a detailed Transputer simulator.
Reference: [8] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eiken. </author> <title> Logp: Towards a realistic model of parallel computation. </title> <booktitle> In Proceedings of the Fourth Symposium on Principle and Practice of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The network topology is intentionally left unbound to provide maximum generality. Finally, the model includes a global controller that can communicate with all processors through a low bandwidth network. Logically, 2 The more recent BSP [44] and LogP <ref> [8] </ref> models present a similar view of a parallel machine and for the most part suggest a similar way of programming parallel computers. 4 the controller provides synchronization and low bandwidth communication such as a broadcast of a single value.
Reference: [9] <author> D. E. Culler and Arvind. </author> <title> Resource requirements of dataflow programs. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 141-150, </pages> <year> 1988. </year>
Reference-contexts: Since its creation it has been studied widely in both sequential and parallel forms <ref> [3, 9, 13, 16, 17, 23, 24, 37, 39] </ref>. Hardware. The portability of our parallel SIMPLE was investigated on the iPSC/2 S, iPSC/2 F, nCUBE/7, Sequent Symmetry, BBN Butterfly GP1000, and a detailed Transputer simulator. These machines are summarized in Table 1.
Reference: [10] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The Phase Abstractions' support for loose synchrony naturally supports the use of refined grids in conjunction with the base grid. Split-C. Split-C is a shared-memory SPMD language with memory reference operations that support latency-hiding <ref> [10] </ref>. Split-C procedures are concurrently applied in an "owner-computes" fashion to the partitions of an aggregate data structure such as an array or pointer-based graph. A process reads data that it does not own with a global pointer (a Split-C data type).
Reference: [11] <author> T. Dunigan. </author> <title> Hypercube performance. </title> <booktitle> In Proceedings of the 2nd Conference on Hypercube Architectures, </booktitle> <pages> pages 178-192, </pages> <year> 1987. </year>
Reference-contexts: In this discussion we concentrate on communication costs relative to computational speed, the feature that best distinguishes these machines. For example, the iPSC/2 F and nCUBE/7 have identical interconnection topologies but the ratio of computation speed to communication speed is greater on the iPSC/2 <ref> [11, 12] </ref>. This has the effect of reducing speedup because it decreases the percentage of time spent computing and increases the fraction of time spent on non-computation overhead. Similarly, since message passing latency is lowest on the Sequent's shared bus, the Sequent shows the best speedup.
Reference: [12] <author> T. Dunigan. </author> <title> Performance of the Intel iPSC/860 and NCUBE 6400 hypercubes. </title> <type> Technical Report ONRL/TM-11790, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: In this discussion we concentrate on communication costs relative to computational speed, the feature that best distinguishes these machines. For example, the iPSC/2 F and nCUBE/7 have identical interconnection topologies but the ratio of computation speed to communication speed is greater on the iPSC/2 <ref> [11, 12] </ref>. This has the effect of reducing speedup because it decreases the percentage of time spent computing and increases the fraction of time spent on non-computation overhead. Similarly, since message passing latency is lowest on the Sequent's shared bus, the Sequent shows the best speedup.
Reference: [13] <author> K. Ekanadham and Arvind. </author> <title> SIMPLE: Part I, an exercise in future scientific programming. </title> <type> Technical Report CSG Technical Report 273, </type> <institution> MIT, </institution> <year> 1987. </year>
Reference-contexts: Since its creation it has been studied widely in both sequential and parallel forms <ref> [3, 9, 13, 16, 17, 23, 24, 37, 39] </ref>. Hardware. The portability of our parallel SIMPLE was investigated on the iPSC/2 S, iPSC/2 F, nCUBE/7, Sequent Symmetry, BBN Butterfly GP1000, and a detailed Transputer simulator. These machines are summarized in Table 1.
Reference: [14] <author> W. Fenton, B. Ramkumar, V. Saletore, A. Sinha, and L. Kale. </author> <title> Supporting machine independent programming on diverse parallel architectures. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages II 193-201, </pages> <year> 1991. </year>
Reference-contexts: The trend towards relatively faster processors and relatively slower memory access speeds exacerbates the situation. 2 Other languages provide inadequate control over the granularity of parallelism, requiring either one data point per process [21, 40], assuming some larger fixed granularity <ref> [14, 29] </ref>, or including no notion of granularity at all, forcing the compiler or runtime system to choose the best granularity [15]. Given the diversity of parallel computers, no particular granularity can be best for all machines.
Reference: [15] <author> J. Feo, D. C. Cann, and R. Oldehoeft. </author> <title> A report on the Sisal language project. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10 </volume> <pages> 349-366, </pages> <month> December </month> <year> 1990. </year> <month> 27 </month>
Reference-contexts: Low level approaches [25] allow programmers to hand-code data placement. The resulting code typically assumes one particular data decomposition, so if the program is ported to a platform that favors some other decomposition, extensive changes must be made or performance suffers. Other languages <ref> [4, 5, 15] </ref> give the programmer no control over data decomposition, leaving these issues to the compiler or hardware. But because the best choice of data decomposition depends on characteristics of the application, compilers can make poor data placement decisions. <p> speeds exacerbates the situation. 2 Other languages provide inadequate control over the granularity of parallelism, requiring either one data point per process [21, 40], assuming some larger fixed granularity [14, 29], or including no notion of granularity at all, forcing the compiler or runtime system to choose the best granularity <ref> [15] </ref>. Given the diversity of parallel computers, no particular granularity can be best for all machines. Computers such as the CM-5 prefer coarse granularities; those such as the J Machine prefer finer granularity; and those such as the MIT Alewife and Tera computer benefit from having multiple threads per process.
Reference: [16] <author> D. Gannon and J. Panetta. </author> <title> SIMPLE on the CHiP. </title> <type> Technical Report 469, </type> <institution> Computer Science Department, Purdue University, </institution> <year> 1984. </year>
Reference-contexts: Since its creation it has been studied widely in both sequential and parallel forms <ref> [3, 9, 13, 16, 17, 23, 24, 37, 39] </ref>. Hardware. The portability of our parallel SIMPLE was investigated on the iPSC/2 S, iPSC/2 F, nCUBE/7, Sequent Symmetry, BBN Butterfly GP1000, and a detailed Transputer simulator. These machines are summarized in Table 1.
Reference: [17] <author> D. Gannon and J. Panetta. </author> <title> Restructuring Simple for the CHiP architecture. </title> <booktitle> In Parallel Computing, </booktitle> <pages> pages 3 305-326, </pages> <year> 1986. </year>
Reference-contexts: Since its creation it has been studied widely in both sequential and parallel forms <ref> [3, 9, 13, 16, 17, 23, 24, 37, 39] </ref>. Hardware. The portability of our parallel SIMPLE was investigated on the iPSC/2 S, iPSC/2 F, nCUBE/7, Sequent Symmetry, BBN Butterfly GP1000, and a detailed Transputer simulator. These machines are summarized in Table 1.
Reference: [18] <author> K. Gates. </author> <title> Simple: An exercise in programming in Poker. </title> <type> Technical report, </type> <institution> Applied Mathematics Department, University of Washington, </institution> <year> 1989. </year>
Reference-contexts: For example, SIMPLE can have up to nine different cases|depending on which portions of the boundaries are contained within a process|and these conditionals can lead to code that is dominated by the treatment of exceptional cases <ref> [18, 38] </ref>.
Reference: [19] <author> W. Griswold, G. Harrison, D. Notkin, and L. Snyder. </author> <title> Scalable abstractions for parallel programming. </title> <booktitle> In Proceedings of the Fifth Distributed Memory Computing Conference, 1990. </booktitle> <address> Charleston, South Carolina. </address>
Reference-contexts: The Phase Abstractions extend the CTA in the same way that the sequential imperative programming model extends the von Neumann model. The main components of the Phase Abstractions are the XYZ levels of programming and ensembles <ref> [1, 19, 43] </ref>. 2.1 XYZ Programming Levels A programmer's problem-solving abilities can be improved by dividing a problem into small, man ageable pieces|assuming the pieces are sufficiently independent to be considered separately. Additionally, these pieces can often be reused in other programs, saving time on future problems.
Reference: [20] <author> R. H. Halstead. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <year> 1985. </year>
Reference: [21] <author> P. J. Hatcher, M. J. Quinn, R. J. Anderson, A. J. Lapadula, B. K. Seevers, and A. F. Bennett. </author> <title> Architecture-independent scientific programming in Dataparallel C: Three case studies. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 208-217, </pages> <year> 1991. </year>
Reference-contexts: The trend towards relatively faster processors and relatively slower memory access speeds exacerbates the situation. 2 Other languages provide inadequate control over the granularity of parallelism, requiring either one data point per process <ref> [21, 40] </ref>, assuming some larger fixed granularity [14, 29], or including no notion of granularity at all, forcing the compiler or runtime system to choose the best granularity [15]. Given the diversity of parallel computers, no particular granularity can be best for all machines. <p> Nor do any provide general support for handling boundary conditions or controlling granularity. This section discusses how some of these systems address scalability and portability in the aggregate data parallel programming style. 23 Dataparallel C. Dataparallel C <ref> [21] </ref> (DPC) is a portable shared-memory SIMD-style language that has similarities to C++. Unlike the Phase Abstractions, DPC supports only point-wise par allelism. DPC has point-wise processor (poly) variables that are distributed across the processors of the machine.
Reference: [22] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Specification. </title> <month> January </month> <year> 1993. </year>
Reference-contexts: Other languages [4, 5, 15] give the programmer no control over data decomposition, leaving these issues to the compiler or hardware. But because the best choice of data decomposition depends on characteristics of the application, compilers can make poor data placement decisions. Many recent languages <ref> [6, 22] </ref> provide support for data decompositions, but hide communication operations from the programmer and thus do not encourage locality at the algorithmic level. Consequently, there is a reliance on automated means of hiding latency. <p> The data-partitioning aspect of ensembles is analogous to the data-partitioning features supplied in languages such as HPF <ref> [22] </ref>, but since ensembles also incorporate communication and code components, ensembles provide greater control over performance. For instance, the code ensemble may logically have identical procedures in each section, but they may be different, effectively yielding MIMD execution with the convenience of SPMD programming. <p> SPMD processing is allowed, but there are no special facilities for handling edge effects. Parallel Fortrans. Recent languages such as Kali [26], Vienna Fortran [6], and HPF <ref> [22] </ref> focus on data decomposition as the expression of parallelism. Their data decompositions are similar to the Phase Abstractions notion of data ensembles, but the overall approach differs fundamentally from Phase Abstractions.
Reference: [23] <author> R. E. Hiromoto, O. M. Lubeck, and J. Moore. </author> <title> Experiences with the Denelcor HEP. </title> <booktitle> In Parallel Computing, </booktitle> <pages> pages 1 197-206, </pages> <year> 1984. </year>
Reference-contexts: Since its creation it has been studied widely in both sequential and parallel forms <ref> [3, 9, 13, 16, 17, 23, 24, 37, 39] </ref>. Hardware. The portability of our parallel SIMPLE was investigated on the iPSC/2 S, iPSC/2 F, nCUBE/7, Sequent Symmetry, BBN Butterfly GP1000, and a detailed Transputer simulator. These machines are summarized in Table 1. <p> Similarly, since message passing latency is lowest on the Sequent's shared bus, the Sequent shows the best speedup. This claim assumes little or no bus contention, which is a valid assumption considering the modest bandwidth required by SIMPLE. data points <ref> [23] </ref>, which indicate that our portable program is roughly competitive with machine-specific code. The many differences with our results|including different problem sizes, different architectures, and possibly even different problem specifications|make it difficult to draw any stronger conclusions.
Reference: [24] <author> T. J. Holman. </author> <title> Processor Element Architecture for Non-Shared Memory Parallel Computers. </title> <type> PhD thesis, </type> <institution> University of Washington, Department of Computer Science, </institution> <year> 1988. </year>
Reference-contexts: Since its creation it has been studied widely in both sequential and parallel forms <ref> [3, 9, 13, 16, 17, 23, 24, 37, 39] </ref>. Hardware. The portability of our parallel SIMPLE was investigated on the iPSC/2 S, iPSC/2 F, nCUBE/7, Sequent Symmetry, BBN Butterfly GP1000, and a detailed Transputer simulator. These machines are summarized in Table 1. <p> The simulator is a detailed Transputer-based non-shared memory machine. Using detailed information about arithmetic, logical and communication operators of the T800 <ref> [24] </ref>, this simulator executes a program written in a Phase Abstraction language and produces time estimates for the program execution. Implementation The SIMPLE program was written in Orca C.
Reference: [25] <institution> Intel Corporation. </institution> <note> iPSC/2 User's Guide. </note> <month> October </month> <year> 1989. </year>
Reference-contexts: As a consequence, several languages have been introduced to support key aspects of this style. However, unless all aspects of this style are supported, performance, scalability, portability, or development cost can suffer. For instance, good locality of reference is an important aspect of this programming style. Low level approaches <ref> [25] </ref> allow programmers to hand-code data placement. The resulting code typically assumes one particular data decomposition, so if the program is ported to a platform that favors some other decomposition, extensive changes must be made or performance suffers.
Reference: [26] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The communication structure of the processor is not visible to the programmer, but the programmer can change the partitioning clauses on the data aggregates. SPMD processing is allowed, but there are no special facilities for handling edge effects. Parallel Fortrans. Recent languages such as Kali <ref> [26] </ref>, Vienna Fortran [6], and HPF [22] focus on data decomposition as the expression of parallelism. Their data decompositions are similar to the Phase Abstractions notion of data ensembles, but the overall approach differs fundamentally from Phase Abstractions.
Reference: [27] <author> S. R. Kohn and S. B. Baden. </author> <title> Lattice parallelism: A parallel programming model for nonuniform, structured scientific computations. </title> <type> Technical Report CS92-261, </type> <institution> University of Cali-fornia, San Diego, Dept. of Computer Science and Engineering, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: In addition to data distribution directives, Kali allows the programmer to control the assignment of loop iterations to processors through the use of the On clause, which can help in maintaining locality. LPAR. LPAR is a portable language extension that supports structured, irregular scientific parallel computations <ref> [28, 27] </ref>. In particular, LPAR provides mechanisms for describing non-rectangular distributed partitions of the data space to manage load-balancing and locality. These partitions are created through the union, intersection and set difference of arrays.
Reference: [28] <author> S. R. Kohn and S. B. Baden. </author> <title> An implementation of the LPAR parallel programming model for scientific computations. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> March </month> <year> 1993. </year>
Reference-contexts: Irregular Problems. Until now this paper has only described statically defined ensembles that are array-based. However, this should not imply that Phase Abstractions are ill suited to dynamic or unstructured problems. In fact, to some extent LPAR <ref> [28] </ref>, a set of language extensions for irregular scientific computations (see Section 7), can be described in terms of the Phase Abstractions. <p> In addition to data distribution directives, Kali allows the programmer to control the assignment of loop iterations to processors through the use of the On clause, which can help in maintaining locality. LPAR. LPAR is a portable language extension that supports structured, irregular scientific parallel computations <ref> [28, 27] </ref>. In particular, LPAR provides mechanisms for describing non-rectangular distributed partitions of the data space to manage load-balancing and locality. These partitions are created through the union, intersection and set difference of arrays.
Reference: [29] <author> M. S. Lam and M. C. Rinard. </author> <title> Coarse-grain parallel programming in Jade. </title> <booktitle> In Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: The trend towards relatively faster processors and relatively slower memory access speeds exacerbates the situation. 2 Other languages provide inadequate control over the granularity of parallelism, requiring either one data point per process [21, 40], assuming some larger fixed granularity <ref> [14, 29] </ref>, or including no notion of granularity at all, forcing the compiler or runtime system to choose the best granularity [15]. Given the diversity of parallel computers, no particular granularity can be best for all machines.
Reference: [30] <author> C. Lin. </author> <title> The Portability of Parallel Programs Across MIMD Computers. </title> <type> PhD thesis, </type> <institution> University of Washington, Department of Computer Science and Engineering, </institution> <year> 1992. </year>
Reference-contexts: This parameterization is illustrated in the next section. 3 Ensemble Example: Jacobi To provide a better understanding of the ensembles and the Phase Abstractions, we now complete the description of the Jacobi program. We adopt notation from the proposed Orca C language <ref> [30, 31] </ref>, but other languages based on the Phase Abstractions are possible (see Section 4). 3.1 Overall Program Structure As shown in Figure 5, a Phase Abstractions program consists of X, Y, and Z descriptions, plus a list of machine configuration parameters that are used by the program to adapt to <p> This section summarizes these results for just one program, SIMPLE, but similar results were also achieved for QR factorization and matrix multiplication <ref> [30] </ref>. Here we briefly describe SIMPLE, the machines on which this program was run, the manner in which this portable program was implemented, and the significant results.
Reference: [31] <author> C. Lin and L. Snyder. </author> <title> A portable implementation of SIMPLE. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(5) </volume> <pages> 363-401, </pages> <year> 1991. </year> <month> 28 </month>
Reference-contexts: In the parallel world, the Candidate Type Architecture (CTA) plays the role of the von Neumann model, 2 and the Phase Abstractions the role of the programming model. Finally, the sequential languages are replaced by languages based on the Phase Abstractions, of which Orca C is an example <ref> [31, 33] </ref>. The CTA. The CTA [42] is an asynchronous MIMD model. It consists of P von Neumann processors that execute independently. Each processor has its own local memory, and the processors communicate through some sparse but otherwise unspecified communication network. <p> This parameterization is illustrated in the next section. 3 Ensemble Example: Jacobi To provide a better understanding of the ensembles and the Phase Abstractions, we now complete the description of the Jacobi program. We adopt notation from the proposed Orca C language <ref> [30, 31] </ref>, but other languages based on the Phase Abstractions are possible (see Section 4). 3.1 Overall Program Structure As shown in Figure 5, a Phase Abstractions program consists of X, Y, and Z descriptions, plus a list of machine configuration parameters that are used by the program to adapt to <p> The shared memory model also provides notational convenience, especially when pointer-based structures are involved. 6 Portability Results Experimental evidence suggests that the Phase Abstractions can provide portability across a diverse set of MIMD computers <ref> [31, 32] </ref>. This section summarizes these results for just one program, SIMPLE, but similar results were also achieved for QR factorization and matrix multiplication [30].
Reference: [32] <author> C. Lin and L. Snyder. </author> <title> Portable parallel programming: Cross machine comparisons for SIMPLE. </title> <booktitle> In Fifth SIAM Conference on Parallel Processing, </booktitle> <year> 1991. </year>
Reference-contexts: The shared memory model also provides notational convenience, especially when pointer-based structures are involved. 6 Portability Results Experimental evidence suggests that the Phase Abstractions can provide portability across a diverse set of MIMD computers <ref> [31, 32] </ref>. This section summarizes these results for just one program, SIMPLE, but similar results were also achieved for QR factorization and matrix multiplication [30].
Reference: [33] <author> C. Lin and L. Snyder. </author> <title> Data ensembles in Orca C. </title> <booktitle> In 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: In the parallel world, the Candidate Type Architecture (CTA) plays the role of the von Neumann model, 2 and the Phase Abstractions the role of the programming model. Finally, the sequential languages are replaced by languages based on the Phase Abstractions, of which Orca C is an example <ref> [31, 33] </ref>. The CTA. The CTA [42] is an asynchronous MIMD model. It consists of P von Neumann processors that execute independently. Each processor has its own local memory, and the processors communicate through some sparse but otherwise unspecified communication network.
Reference: [34] <author> C. Lin and L. Snyder. </author> <title> Accommodating polymorphic data decompositions in explicitly parallel programs. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: For example, the Car-Parrinello molecular dynamics program [45] consists of several phases, one of which is computed using the Modified Gram-Schmidt (MGS) method of solving QR factorization. Empirical results have shown that the MGS method performs best with 20 a 2D data decomposition <ref> [34] </ref>. However, other phases of the Car-Parrinello computation require a 1D decomposition, so in this case a 1D decomposition for MGS yields the best performance since it avoids data movement between phases.
Reference: [35] <author> C. Lin and W. D. Weathersby. </author> <title> Towards a machine-independent solution of sparse cholesky factorization. </title> <note> In Proceedings of Parallel Computing '93, (to appear) 1993. </note>
Reference-contexts: For certain solutions to this problem, a shared memory model performs better because the single address space leads to better load balance through the use of a work queue model <ref> [35] </ref>. The shared memory model also provides notational convenience, especially when pointer-based structures are involved. 6 Portability Results Experimental evidence suggests that the Phase Abstractions can provide portability across a diverse set of MIMD computers [31, 32].
Reference: [36] <author> P. Mehrotra and J. Rosendale. </author> <title> Compiling high level constructs to distributed memory architectures. </title> <type> Technical Report ICASE Report No. 89-20, </type> <institution> Institute for Computer Applications in Science and Engineering, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Writes to such a variable are broadcast to all copies. Dino handles edge effects in the same fashion as C*. Because Dino only supports point-wise communication, as in C*, the compiler or runtime system must combine messages. Mehrotra and Rosendale. A system described by Mehrotra and Rosendale <ref> [36] </ref> is much like Dino in that it supports a small set of data distributions. However, this system provides no way to control or determine precisely which points are local to each other, so it is not possible to control communication costs or algorithm choice based on locality.
Reference: [37] <author> J. M. Meyers. </author> <title> Analysis of the SIMPLE code for dataflow computation. </title> <type> Technical Report MIT/LCS/TR-216, </type> <institution> MIT, </institution> <year> 1979. </year>
Reference-contexts: Since its creation it has been studied widely in both sequential and parallel forms <ref> [3, 9, 13, 16, 17, 23, 24, 37, 39] </ref>. Hardware. The portability of our parallel SIMPLE was investigated on the iPSC/2 S, iPSC/2 F, nCUBE/7, Sequent Symmetry, BBN Butterfly GP1000, and a detailed Transputer simulator. These machines are summarized in Table 1.
Reference: [38] <author> D. Notkin, D. Socha, M. Bailey, B. Forstall, K. Gates, R. Greenlaw, W. Griswold, T. Holman, R. Korry, G. Lasswell, R. Mitchell, P. Nelson, and L. Snyder. </author> <title> Experiences with Poker. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: For example, SIMPLE can have up to nine different cases|depending on which portions of the boundaries are contained within a process|and these conditionals can lead to code that is dominated by the treatment of exceptional cases <ref> [18, 38] </ref>.
Reference: [39] <author> K. Pingali and A. Rogers. </author> <title> Compiler parallelization of SIMPLE for a distributed memory machine. </title> <type> Technical Report 90-1084, </type> <institution> Cornell University, </institution> <year> 1990. </year>
Reference-contexts: Since its creation it has been studied widely in both sequential and parallel forms <ref> [3, 9, 13, 16, 17, 23, 24, 37, 39] </ref>. Hardware. The portability of our parallel SIMPLE was investigated on the iPSC/2 S, iPSC/2 F, nCUBE/7, Sequent Symmetry, BBN Butterfly GP1000, and a detailed Transputer simulator. These machines are summarized in Table 1. <p> As another reference point, Figure 12 (b) compares our results on the iPSC/2 S against those of Pingali and Rogers' parallelizing compiler for Id Nouveau, a functional language <ref> [39] </ref>.
Reference: [40] <author> J. Rose and G. L. Steele Jr. </author> <title> C*: An extended C language for data parallel programming. </title> <booktitle> In 2nd International Conference on Supercomputing, </booktitle> <month> March </month> <year> 1987. </year>
Reference-contexts: The trend towards relatively faster processors and relatively slower memory access speeds exacerbates the situation. 2 Other languages provide inadequate control over the granularity of parallelism, requiring either one data point per process <ref> [21, 40] </ref>, assuming some larger fixed granularity [14, 29], or including no notion of granularity at all, forcing the compiler or runtime system to choose the best granularity [15]. Given the diversity of parallel computers, no particular granularity can be best for all machines. <p> Also, few languages provide sufficient control over the algorithm that is applied on aggregate data, preferring to multiplex the parallel algorithm when there are multiple data points on a processor <ref> [40, 41] </ref>. Many language models do not adequately support loose synchrony. The boundaries of parallel computations often introduce irregularities that require significant coding effort. Languages with SIMD semantics force all processes to execute the branches of a conditional in lock-step. <p> Dataparallel C [21] (DPC) is a portable shared-memory SIMD-style language that has similarities to C++. Unlike the Phase Abstractions, DPC supports only point-wise par allelism. DPC has point-wise processor (poly) variables that are distributed across the processors of the machine. Unlike its predecessor C* <ref> [40] </ref>, DPC supports data decompositions of its data to improve performance on coarse-grained architectures. However, because DPC only supports point-wise communication, the compiler or runtime system must detect when several point sends on a processor are destined for the same processor and bundle them.
Reference: [41] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The Dino parallel programming language. </title> <type> Technical Report CU-CS-457-90, </type> <institution> Dept. of Computer Science, University of Colorado, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: Also, few languages provide sufficient control over the algorithm that is applied on aggregate data, preferring to multiplex the parallel algorithm when there are multiple data points on a processor <ref> [40, 41] </ref>. Many language models do not adequately support loose synchrony. The boundaries of parallel computations often introduce irregularities that require significant coding effort. Languages with SIMD semantics force all processes to execute the branches of a conditional in lock-step. <p> Reusing such code is harder because the boundary conditions may change from problem to problem. Constant and variable boundary conditions, however, can be supported by expanding the data space and leaving some processes idle. Dino. Dino <ref> [41] </ref> is a C-like, SPMD language. Like C*, it constructs distributed data structures by replicating structures over processors and executing a single procedure over every element of the data set.
Reference: [42] <author> L. Snyder. </author> <title> Type architecture, shared memory and the corollary of modest potential. </title> <booktitle> In Annual Review of Computer Science, </booktitle> <pages> pages I:289-318, </pages> <year> 1986. </year>
Reference-contexts: First, we provide abstractions that are efficiently implementable on all MIMD architectures, along with specific mechanisms to handle the common types of parallelism, data distribution, and boundary conditions. Our model is based on a practical MIMD computing model called the Candidate Type Architecture (CTA) <ref> [42] </ref>. Second, the insignificant but diverse aspects of computer architectures are hidden. If exposed to the programmer, assumptions based on these characteristics can be sprinkled throughout a program, making portability difficult. <p> Finally, the sequential languages are replaced by languages based on the Phase Abstractions, of which Orca C is an example [31, 33]. The CTA. The CTA <ref> [42] </ref> is an asynchronous MIMD model. It consists of P von Neumann processors that execute independently. Each processor has its own local memory, and the processors communicate through some sparse but otherwise unspecified communication network. Here "sparse" means that the network has a constant degree of connectivity.
Reference: [43] <author> L. Snyder. </author> <title> The XYZ abstraction levels of Poker-like languages. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 470-489. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The Phase Abstractions extend the CTA in the same way that the sequential imperative programming model extends the von Neumann model. The main components of the Phase Abstractions are the XYZ levels of programming and ensembles <ref> [1, 19, 43] </ref>. 2.1 XYZ Programming Levels A programmer's problem-solving abilities can be improved by dividing a problem into small, man ageable pieces|assuming the pieces are sufficiently independent to be considered separately. Additionally, these pieces can often be reused in other programs, saving time on future problems.
Reference: [44] <author> L. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: Here "sparse" means that the network has a constant degree of connectivity. The network topology is intentionally left unbound to provide maximum generality. Finally, the model includes a global controller that can communicate with all processors through a low bandwidth network. Logically, 2 The more recent BSP <ref> [44] </ref> and LogP [8] models present a similar view of a parallel machine and for the most part suggest a similar way of programming parallel computers. 4 the controller provides synchronization and low bandwidth communication such as a broadcast of a single value.
Reference: [45] <author> J. Wiggs. </author> <title> A parallel implementation of the Car-Parrinello method. </title> <type> Technical Report General Exam, </type> <institution> Dept. of Chemistry, University of Washington, </institution> <month> June </month> <year> 1993. </year> <month> 29 </month>
Reference-contexts: For example, the Car-Parrinello molecular dynamics code simulates the behavior of a collection of atoms by iteratively invoking a series of phases that perform FFT's, matrix products, and other computations <ref> [45] </ref>. In Z-Y-X order, these three levels give a top-down view of a parallel program. Example: XYZ Levels of the Jacobi Iteration. Figure 1 illustrates the XYZ levels of programming for the Jacobi Iteration. <p> Instead of cluttering up the process code, special cases due to boundary conditions are handled at the problem level where they naturally belong. Reusability. The same characteristics that provide flexibility in the Phase Abstractions also encourage reusability. For example, the Car-Parrinello molecular dynamics program <ref> [45] </ref> consists of several phases, one of which is computed using the Modified Gram-Schmidt (MGS) method of solving QR factorization. Empirical results have shown that the MGS method performs best with 20 a 2D data decomposition [34].
References-found: 45

