URL: ftp://ftp.cs.toronto.edu/pub/radford/over.ps.Z
Refering-URL: http://www.cs.toronto.edu/~radford/over.abstract.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: radford@stat.toronto.edu  
Title: Random Walks in Markov Chain Monte Carlo Using Ordered Overrelaxation  
Author: Radford M. Neal 
Date: 21 June 1995  
Web: Web: http://www.cs.toronto.edu/~radford  
Address: Toronto, Ontario, Canada  
Affiliation: Dept. of Statistics and Dept. of Computer Science University of Toronto  
Note: Suppressing  World Wide  
Abstract: Technical Report No. 9508, Department of Statistics, University of Toronto Markov chain Monte Carlo methods such as Gibbs sampling and simple forms of the Metropolis algorithm typically move about the distribution being sampled via a random walk. For the complex, high-dimensional distributions commonly encountered in Bayesian inference and statistical physics, the distance moved in each iteration of these algorithms will usually be small, because it is difficult or impossible to transform the problem to eliminate dependencies between variables. The inefficiency inherent in taking such small steps is greatly exacerbated when the algorithm operates via a random walk, as in such a case moving to a point n steps away will typically take around n 2 iterations. Such random walks can sometimes be suppressed using "overrelaxed" variants of Gibbs sampling (a.k.a. the heatbath algorithm), but such methods have hitherto been largely restricted to problems where all the full conditional distributions are Gaussian. I present an overrelaxed Markov chain Monte Carlo algorithm based on order statistics that is more widely applicable. In particular, the algorithm can be applied whenever the full conditional distributions are such that their cumulative distribution functions and inverse cumulative distribution functions can be efficiently computed. The method is demonstrated on an inference problem for a simple hierarchical Bayesian model. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Adler, S. L. </author> <title> (1981) "Over-relaxation method for the Monte Carlo evaluation of the partition function for multiquadratic actions", </title> <journal> Physical Review D, </journal> <volume> vol. 23, </volume> <pages> pp. 2901-2904. </pages> <note> 20 Barone, </note> <author> P. and Frigessi, A. </author> <title> (1990) "Improving stochastic relaxation for Gaussian random fields", </title> <journal> Probability in the Engineering and Informational Sciences, </journal> <volume> vol. 4, </volume> <pages> pp. 369-389. </pages>
Reference: <author> Brown, F. R. and Woch, T. J. </author> <title> (1987) "Overrelaxed heat-bath and Metropolis algorithms for accelerating pure gauge Monte Carlo calculations", </title> <journal> Physical Review Letters, </journal> <volume> vol. 58, </volume> <pages> pp. 2394-2396. </pages>
Reference: <author> Creutz, M. </author> <title> (1987) "Overrelaxation and Monte Carlo simulation", </title> <journal> Physical Review D, </journal> <volume> vol. 36, </volume> <pages> pp. 515-519. </pages>
Reference: <author> David, H. A. </author> <title> (1970) Order Statistics, </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: As is well known <ref> (eg, David 1970, p. 11) </ref>, the k'th order statistic of a sample of size n from a uniform distribution 11 over [0; 1] has a beta (k; nk+1) distribution, with density proportional to u k1 (1u) nk , mean k=(n + 1), and variance k (n k + 1) = (n <p> Note that u 0 is the result of overrelaxing u with respect to the uniform distribution on [0; 1]. 4) Let the new value for component i be x 0 i = F 1 (u 0 ). Step (3) is based on the fact <ref> (David 1970, p. 11) </ref> that the k'th order statistic in a sample of size n from a uniform distribution on [0; 1] has a beta (k, n k + 1) distribution.
Reference: <author> Devroye, L. </author> <title> (1986) Non-uniform Random Variate Generation, </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Efficient methods for generating random variates from beta and binomial distributions in bounded expected time are known <ref> (Devroye 1986, Sections IX.4 and X.4) </ref>. When feasible, this implementation allows ordered overrelaxation to be performed in time independent of K, though this time will exceed that required for a simple Gibbs sampling update.
Reference: <author> Duane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. </author> <title> (1987) "Hybrid Monte Carlo", </title> <journal> Physics Letters B, </journal> <volume> vol. 195, </volume> <pages> pp. 216-222. </pages>
Reference-contexts: Unlike the original overrelaxation method of Adler (1981), it is applicable when the conditional distributions are not Gaussian, and it avoids the rejections that can undermine the performance of other generalized overrelaxation methods. Compared to the alternative of suppressing random walks using hybrid Monte Carlo <ref> (Duane, et al. 1987) </ref>, overrelaxation has the advantage that it does not require the setting of a stepsize parameter, making it potentially easier to apply on a routine basis.
Reference: <author> Fodor, Z. and Jansen, K. </author> <title> (1994) "Overrelaxation algorithm for coupled Gauge-Higgs systems", </title> <journal> Physics Letters B, </journal> <volume> vol. 331, </volume> <pages> pp. 119-123. </pages>
Reference: <author> Gilks, W. R. and Wild, P. </author> <title> (1992) "Adaptive rejection sampling for Gibbs sampling", </title> <journal> Applied Statistics, </journal> <volume> vol. 41, </volume> <pages> pp. 337-348. </pages>
Reference-contexts: This implementation can be used for many problems, but it is not as widely applicable as Gibbs sampling. Natural economies of scale will allow ordered overrelaxation to provide at least some benefit in many other contexts, without any special effort. By modifying adaptive rejection sampling <ref> (Gilks and Wild 1992) </ref> to rapidly perform ordered overrelax 18 Plot of t during Gibbs sampling run Plot of t during ordered overrelaxation run with K = 5 Plot of t during ordered overrelaxation run with K = 11 Plot of t during ordered overrelaxation run with K = 21 with
Reference: <author> Gelfand, A. E. and Smith, A. F. M. </author> <title> (1990) "Sampling-based approaches to calculating marginal densities", </title> <journal> Journal of the American Statistical Association, </journal> <volume> vol. 85, </volume> <pages> pp. 398-409. </pages>
Reference: <author> Green, P. J. and Han, X. </author> <title> (1992) "Metropolis methods, Gaussian proposals and antithetic variables", </title> <editor> in P. Barone, et al. </editor> <title> (editors) Stochastic Models, Statistical Methods, and Algorithms in Image Analysis, </title> <booktitle> Lecture Notes in Statistics, </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Hastings, W. K. </author> <title> (1970) "Monte Carlo sampling methods using Markov chains and their applications", </title> <journal> Biometrika, </journal> <volume> vol. 57, </volume> <pages> pp. 97-109. </pages>
Reference: <author> Kennedy, W. J. and Gentle, J. E. </author> <title> (1980) Statistical Computing, </title> <address> New York: </address> <publisher> Marcel Dekker. </publisher>
Reference-contexts: This approach requires that we be able to efficiently compute the cumulative distribution function and its inverse for each of the conditional distributions for which overrelaxation is to be done. This requirement is somewhat restrictive, but reasonably fast methods for computing these functions are known for many standard distributions <ref> (Kennedy and Gentle 1980) </ref>. This implementation of ordered overrelaxation produces exactly the same effect as would a direct implementation of the steps in Section 4.1.
Reference: <author> Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. </author> <title> (1953) "Equation of state calculations by fast computing machines", </title> <journal> Journal of Chemical Physics, </journal> <volume> vol. 21, </volume> <pages> pp. 1087-1092. </pages>
Reference: <author> Neal, R. M. </author> <title> (1993) "Probabilistic inference using Markov Chain Monte Carlo methods", </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Dept. of Computer Science, University of Toronto. </institution> <note> Obtainable in compressed Postscript by anonymous ftp to ftp.cs.toronto.edu, directory pub/radford, file review.ps.Z. </note>
Reference: <author> Neal, R. M. </author> <title> (1995) Bayesian Learning for Neural Networks, </title> <type> Ph.D. thesis, </type> <institution> Dept. of Computer Science, University of Toronto. </institution> <note> Obtainable in compressed Postscript by anonymous ftp to ftp.cs.toronto.edu, directory pub/radford, file thesis.ps.Z. 21 Smith, </note> <author> A. F. M. and Roberts, G. O. </author> <title> (1993) "Bayesian computation via the Gibbs sampler and related Markov chain Monte Carlo methods", </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> vol. 55, </volume> <pages> pp. 3-23. </pages> <note> (See also the other papers and discussion in the same issue.) </note> <author> Toussaint, D. </author> <title> (1989) "Introduction to algorithms for Monte Carlo simulations and their application to QCD", </title> <journal> Computer Physics Communications, </journal> <volume> vol. 56, </volume> <pages> pp. 69-92. </pages>
Reference-contexts: These trajectories will proceed in a consistent direction, until such time as they reach a region of low probability. By using states proposed by this deterministic process, random walk effects can be largely eliminated. In Bayesian inference problems for complex models based on neural networks, I have found <ref> (Neal 1995) </ref> that the hybrid Monte Carlo method can be hundreds or thousands of times faster than simple versions of the Metropolis algorithm. <p> There will of course be some distributions for which none of these implementations is feasible; this will certainly be the case when Gibbs sampling itself is not feasible. Such distributions include, for example, the complex posterior distributions that arise with neural network models <ref> (Neal 1995) </ref>.
Reference: <author> Whitmer, C. </author> <title> (1984) "Over-relaxation methods for Monte Carlo simulations of quadratic and multiquadratic actions", </title> <journal> Physical Review D, </journal> <volume> vol. 29, </volume> <pages> pp. 306-311. </pages>
Reference: <author> Wolff, U. </author> <title> (1992) "Dynamics of hybrid overrelaxation in the gaussian model", </title> <journal> Physics Letters B, </journal> <volume> vol. 288, </volume> <pages> pp. 166-170. </pages>
Reference: <author> Young, D. M. </author> <title> (1971) Iterative Solution of Large Linear Systems, </title> <address> New York: </address> <publisher> Academic Press. </publisher> <pages> 22 </pages>
Reference-contexts: I conclude by discussing how the method might be applied in practice, and some possibilities for future work. 2 Overrelaxation with Gaussian conditional distributions Overrelaxation methods have long been used in the iterative solution of systems of linear equations <ref> (Young 1971) </ref>, and hence also for the minimization of quadratic functions. The first Markov chain sampling method based on overrelaxation was introduced in the physics literature by Adler (1981), and later studied by Whitmer (1984).
References-found: 18

