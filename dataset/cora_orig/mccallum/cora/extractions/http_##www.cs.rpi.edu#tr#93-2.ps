URL: http://www.cs.rpi.edu/tr/93-2.ps
Refering-URL: http://www.cs.rpi.edu/tr/
Root-URL: 
Title: Computational Efficiency of Random Sampling Operators in Vision  
Author: Charles V. Stewart 
Date: February 24, 1997  
Address: Troy, New York 12180-3590  
Affiliation: Department of Computer Science Rensselaer Polytechnic Institute  
Abstract: fl The author would like to thank Kishore Bubna and Al Montillo for programming earlier versions of the algorithms, Robin Trahan for her comments on a draft of this paper, and Jon Berry for pointing out the simple form of fl(m; n) in Appendix A. 0 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brian G. Schunck. </author> <title> Robust computational vision. </title> <booktitle> In Proceedings of the International Workshop on Robust Computer Vision, </booktitle> <pages> pages 1-18, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Robust techniques based on random sampling are becoming popular in computer vision applications because they are insensitive to errors in image data (see <ref> [1, 2] </ref> for discussion, and [3, 4, 5, 6, 7, 8, 9, 10] for examples). <p> This may be insured using only S random integers based on algorithms that generate m distinct integers out of M possible integers with only m random numbers [14]. This is achieved by regarding the C (N; k) possible subsets as integers in the range <ref> [1; : : : ; C (N; k)] </ref>, generating S distinct integers, and decoding the integers into the unique subsets. <p> This observation is based on the evaluation of two functions derived in Appendix A (see equations 7 and 8): E 1 (S; M ), the expected number of distinct integers in a random sample of S integers in the range <ref> [1; : : : ; M ] </ref>, M = C (N; k), and E 2 (S; M ), the expected number of random integers that must be drawn from the range [1; : : : ; M ] before S are distinct. <p> (S; M ), the expected number of distinct integers in a random sample of S integers in the range <ref> [1; : : : ; M ] </ref>, M = C (N; k), and E 2 (S; M ), the expected number of random integers that must be drawn from the range [1; : : : ; M ] before S are distinct. If E 1 (S; M ) is close to S then relatively few samples will be redundant. If E 2 (S; M ) is close to S then relatively few additional samples are needed to obtain S unique ones. <p> M ) and E 2 (S; M ) Both E 1 (S; M ) and E 2 (S; M ), as defined in Section 2.2, depend on p s (n; m; M ), the probability that exactly n integers are distinct in a random sequence of m integers drawn from <ref> [1; : : : ; M ] </ref>. Since the random integers are independent and equally likely, each sequence of m integers is equally likely. As a result, p s (n; m; M ) may be evaluated as a combinatorial probability.
Reference: [2] <author> Martin A. Fischler and Oscar Firschein. </author> <title> Parallel guessing: A strategy for high-speed computation. </title> <journal> Pattern Recognition, </journal> <volume> 20 </volume> <pages> 257-263, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction Robust techniques based on random sampling are becoming popular in computer vision applications because they are insensitive to errors in image data (see <ref> [1, 2] </ref> for discussion, and [3, 4, 5, 6, 7, 8, 9, 10] for examples).
Reference: [3] <author> Robert C. Bolles and Martin A. Fischler. </author> <title> A Ransac-based approach to model fitting and its application to finding cylinders in range data. </title> <booktitle> In Proceedings Seventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 637-643, </pages> <year> 1981. </year>
Reference-contexts: 1 Introduction Robust techniques based on random sampling are becoming popular in computer vision applications because they are insensitive to errors in image data (see [1, 2] for discussion, and <ref> [3, 4, 5, 6, 7, 8, 9, 10] </ref> for examples). These techniques use small random subsets of the data points to generate fits to models, such as planar or cylindrical surfaces, and evaluate the fits based on the distances ("residuals") of the remaining points from each fitted model. <p> Examples include finding lines in point data [9], planes in intensity and range data [7, 8, 10], and cylinders in range data <ref> [3] </ref>.
Reference: [4] <author> Martin A. Fischler and Robert C. Bolles. </author> <title> Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. </title> <journal> Communications of the ACM, </journal> <volume> 24 </volume> <pages> 381-395, </pages> <year> 1981. </year>
Reference-contexts: 1 Introduction Robust techniques based on random sampling are becoming popular in computer vision applications because they are insensitive to errors in image data (see [1, 2] for discussion, and <ref> [3, 4, 5, 6, 7, 8, 9, 10] </ref> for examples). These techniques use small random subsets of the data points to generate fits to models, such as planar or cylindrical surfaces, and evaluate the fits based on the distances ("residuals") of the remaining points from each fitted model.
Reference: [5] <author> Jean-Michel Jolion, Peter Meer, and Samira Bataouche. </author> <title> Robust clustering with applications in computer vision. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13 </volume> <pages> 791-802, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Robust techniques based on random sampling are becoming popular in computer vision applications because they are insensitive to errors in image data (see [1, 2] for discussion, and <ref> [3, 4, 5, 6, 7, 8, 9, 10] </ref> for examples). These techniques use small random subsets of the data points to generate fits to models, such as planar or cylindrical surfaces, and evaluate the fits based on the distances ("residuals") of the remaining points from each fitted model.
Reference: [6] <author> Rakesh Kumar and ALlen R. Hanson. </author> <title> Random sampling for primitive extraction. </title> <booktitle> In Proceed--ings of the International Workshop on Robust Computer Vision, </booktitle> <pages> pages 352-366, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Robust techniques based on random sampling are becoming popular in computer vision applications because they are insensitive to errors in image data (see [1, 2] for discussion, and <ref> [3, 4, 5, 6, 7, 8, 9, 10] </ref> for examples). These techniques use small random subsets of the data points to generate fits to models, such as planar or cylindrical surfaces, and evaluate the fits based on the distances ("residuals") of the remaining points from each fitted model.
Reference: [7] <author> Peter Meer, Doron Mintz, and Azriel Rosenfeld. </author> <title> Least median of squares based robust analysis of image structure. </title> <booktitle> In Proceedings of the DARPA Image Understanding Workshop, </booktitle> <pages> pages 231-254, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Robust techniques based on random sampling are becoming popular in computer vision applications because they are insensitive to errors in image data (see [1, 2] for discussion, and <ref> [3, 4, 5, 6, 7, 8, 9, 10] </ref> for examples). These techniques use small random subsets of the data points to generate fits to models, such as planar or cylindrical surfaces, and evaluate the fits based on the distances ("residuals") of the remaining points from each fitted model. <p> In applying least-median-of-squares to finding planar surface patches in intensity or range data, Meer, et. al. <ref> [7, 8] </ref> calculate a fit in fixed size regions, typically 5 fi 5, 7 fi 7 or 9 fi 9, centered at each pixel throughout an image. (They only retain the intercept parameter of the fit, essentially making least-median-of-squares a non-linear "convolution" operator.) Overall, if fits are found at K pixels <p> Examples include finding lines in point data [9], planes in intensity and range data <ref> [7, 8, 10] </ref>, and cylinders in range data [3]. <p> Two different sets of results are presented. They show the following. 1. The effect of varying the window length, w l , for fixed window spacing, w s = 1. This corresponds to the parameters used by Meer, et. al. <ref> [7] </ref> in their least-median-of-squares algorithm for recovering polynomial image structure. In these experiments the density of the data was 25 100%. 2. The effect of varying the window spacing, w s , for fixed window length, w l = 12.
Reference: [8] <author> Peter Meer, Doron Mintz, Azriel Rosenfeld, and Dong Y. Kim. </author> <title> Robust regression methods for computer vision: A review. </title> <journal> International Journal of Computer Vision, </journal> <volume> 6 </volume> <pages> 59-70, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Robust techniques based on random sampling are becoming popular in computer vision applications because they are insensitive to errors in image data (see [1, 2] for discussion, and <ref> [3, 4, 5, 6, 7, 8, 9, 10] </ref> for examples). These techniques use small random subsets of the data points to generate fits to models, such as planar or cylindrical surfaces, and evaluate the fits based on the distances ("residuals") of the remaining points from each fitted model. <p> In applying least-median-of-squares to finding planar surface patches in intensity or range data, Meer, et. al. <ref> [7, 8] </ref> calculate a fit in fixed size regions, typically 5 fi 5, 7 fi 7 or 9 fi 9, centered at each pixel throughout an image. (They only retain the intercept parameter of the fit, essentially making least-median-of-squares a non-linear "convolution" operator.) Overall, if fits are found at K pixels <p> Examples include finding lines in point data [9], planes in intensity and range data <ref> [7, 8, 10] </ref>, and cylinders in range data [3].
Reference: [9] <author> Gerhard Roth and Martin D. Levine. </author> <title> Random sampling for primitive extraction. </title> <booktitle> In Proceedings of the International Workshop on Robust Computer Vision, </booktitle> <pages> pages 352-366, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Robust techniques based on random sampling are becoming popular in computer vision applications because they are insensitive to errors in image data (see [1, 2] for discussion, and <ref> [3, 4, 5, 6, 7, 8, 9, 10] </ref> for examples). These techniques use small random subsets of the data points to generate fits to models, such as planar or cylindrical surfaces, and evaluate the fits based on the distances ("residuals") of the remaining points from each fitted model. <p> This paper explores ways to improve the computational efficiency of random sampling operators that find geometric primitives of the form f (~x; ~a) = 0, where ~x represents the image domain variables, and ~a is the vector of parameters defining the primitive. Examples include finding lines in point data <ref> [9] </ref>, planes in intensity and range data [7, 8, 10], and cylinders in range data [3]. <p> This complexity is too large for practical purposes. 2 the best fit. "Fixed-band" methods, <ref> [9] </ref>, define the best fit as the one that maximizes the number of points within a fixed distance of a fit, whereas "variable-band" methods [9], define the best fit as the one that minimizes the interval around the fit that contains a fixed percentage of points. <p> This complexity is too large for practical purposes. 2 the best fit. "Fixed-band" methods, <ref> [9] </ref>, define the best fit as the one that maximizes the number of points within a fixed distance of a fit, whereas "variable-band" methods [9], define the best fit as the one that minimizes the interval around the fit that contains a fixed percentage of points. Most often, the latter involve minimizing the median square residual.
Reference: [10] <author> Sarvajit S. Sinha and Brian G. Schunck. </author> <title> A two-stage algorithm for discontinuity-preserving surface reconstruction. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14 </volume> <pages> 36-55, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Robust techniques based on random sampling are becoming popular in computer vision applications because they are insensitive to errors in image data (see [1, 2] for discussion, and <ref> [3, 4, 5, 6, 7, 8, 9, 10] </ref> for examples). These techniques use small random subsets of the data points to generate fits to models, such as planar or cylindrical surfaces, and evaluate the fits based on the distances ("residuals") of the remaining points from each fitted model. <p> Examples include finding lines in point data [9], planes in intensity and range data <ref> [7, 8, 10] </ref>, and cylinders in range data [3]. <p> In these experiments the density of the data was 25 100%. 2. The effect of varying the window spacing, w s , for fixed window length, w l = 12. This window length is typical of what might be used on sparse depth data, such as from stereo <ref> [10] </ref>.
Reference: [11] <author> Peter J. Rousseeuw and Annick M. Leroy. </author> <title> Robust Regression and Outlier Detection. </title> <publisher> John Wiley & Sons, </publisher> <year> 1987. </year>
Reference-contexts: Consider, for example, techniques that use random sampling to find a fit that minimizes the median of the squared residuals. This is intended as a robust alternative to the standard statistical technique of minimizing the sum of the squared residuals <ref> [11] </ref>. The random sampling techniques require O (SN log N ) time to evaluate S samples taken from N data points. 1 By contrast, minimizing the sum of squared residuals requires O (N ) time.
Reference: [12] <author> Hebert Edelsbrunner and Diane L. Souvaine. </author> <title> Computing median-of-squares regression lines and guided topological sweep. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 85 </volume> <pages> 115-119, </pages> <year> 1990. </year>
Reference-contexts: Aside from the choice of primitive, the main difference between these techniques is how they determine 1 There is a deterministic algorithm for finding the line in two-dimensional data with the global minimum median square residual in time O (N 2 ) <ref> [12, 13] </ref> and it is hypothesized that finding planes in three-dimensional data requires time O (N 3 ).
Reference: [13] <author> Diane L. Souvaine and J. Michael Steele. </author> <title> Time- and space-efficient algorithms for least median of squares regression. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 82 </volume> <pages> 794-801, </pages> <year> 1987. </year>
Reference-contexts: Aside from the choice of primitive, the main difference between these techniques is how they determine 1 There is a deterministic algorithm for finding the line in two-dimensional data with the global minimum median square residual in time O (N 2 ) <ref> [12, 13] </ref> and it is hypothesized that finding planes in three-dimensional data requires time O (N 3 ).
Reference: [14] <author> Jon Bentley and Robert W. Floyd. </author> <title> Programming pearls | a sample of brilliance. </title> <journal> Communications of the ACM, </journal> <volume> 30 </volume> <pages> 754-757, </pages> <year> 1987. </year> <month> 45 </month>
Reference-contexts: This may be insured using only S random integers based on algorithms that generate m distinct integers out of M possible integers with only m random numbers <ref> [14] </ref>. This is achieved by regarding the C (N; k) possible subsets as integers in the range [1; : : : ; C (N; k)], generating S distinct integers, and decoding the integers into the unique subsets.
Reference: [15] <author> Doron Mintz. </author> <title> Robustness by consensus. </title> <type> Technical Report CAR-TR-576, </type> <institution> University of Mary--land | Center for Automation Research, </institution> <year> 1991. </year>
Reference-contexts: This decoding process may be performed in worst-case time O (k 2 ) for each sample <ref> [15] </ref>. 2 In computer vision applications more than one image surface may overlap a window. In this case, the points on different surfaces are outliers relative to each other. <p> Note that k = 3 corresponds to planar fits and k = 6 corresponds to quadratic fits. This technique has been proposed recently as a way to improve the efficiency of random sampling operators by eliminating redundant samples <ref> [15] </ref>. Unfortunately, when C (N; k) S this technique only yields minor improvements over the naive techniques of either (1) generating each k point random sample without considering previous samples, or (2) generating each sample and testing against previous samples for repetition.
Reference: [16] <author> Gerhard Roth and Martin D. Levine. </author> <title> Segmentation of geometric signals using robust fitting. </title> <booktitle> In Proceedings Tenth International Conference on Pattern Recognition, </booktitle> <pages> pages 826-831, </pages> <year> 1990. </year>
Reference-contexts: This is correct for both linear models and for some approximations to the distance for non-linear models. A common approximation to the distance of a point ~x 0 to the surface f (~x; ~a) = 0 is <ref> [16] </ref>, f (~x 0 ; ~a) ; which is exact for the perpendicular distance of a point to a plane. Note that the definition of the residual depends on whether or not projection-pursuit is used.
Reference: [17] <author> Franco P. Preparata and Michael I. Shamos. </author> <title> Computational Geometry. </title> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: Searching the tree for a given planar fit requires O (N 11=d + m) to find all m points within a fixed distance of the plane <ref> [17] </ref>, although this worst-case performance is rarely seen in practice. Overall, there is a theoretical advantage to using k-D trees when O (d log N ) &lt; O (kS). For variable-band techniques, the evaluation process depends on whether or not projection-pursuit is used. <p> This requires time O (N log N ), which is optimal, since the optimal time for finding the smallest interval between two points is O (N log N ) <ref> [17] </ref>. When median-only is used, the evaluation process simply requires selecting the median square residual. <p> As with the single window algorithm, it is possible to organize the points into a spatial data structure such as a k-D tree <ref> [17] </ref>. The difficulty with the extended window version is that the size of the extended window region varies with the sample.
Reference: [18] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: When median-only is used, the evaluation process simply requires selecting the median square residual. For this, a practical O (N ) expected-case time algorithm that has O (N 2 ) worst-case complexity is used <ref> [18] </ref>. 3 This algorithm, which will be referred to as Select , uses a pivoting and partitioning technique similar to Quick-Sort. <p> Projection-pursuit techniques require O (SN log N ) per window. Finally, median-only requires O (SN ) expected case time per window. 3 There is a worst-case O (N ) algorithm for median finding <ref> [18] </ref>, but it has too large a constant factor to be practical. 8 windows not shown).
Reference: [19] <author> Richard Hoffman and Anil K. Jain. </author> <title> Segmentation and classification of range images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 9 </volume> <pages> 608-620, </pages> <year> 1987. </year>
Reference-contexts: This might occur, for example, in range data clustering algorithms where robust techniques could be used instead of least-squares techniques to calculate the surface normal at every pixel <ref> [19] </ref>. If a sample is chosen and evaluated in one window using the algorithms described in Section 2, and then chosen again in an overlapping window much of the computation for evaluating the sample will be repeated.
Reference: [20] <author> H. A. David. </author> <title> Order Statistics. </title> <publisher> John Wiley and Sons, </publisher> <year> 1970. </year>
Reference-contexts: Because the terms of the summation do not approach 0 quickly, we have been unable to develop a suitable approximation. (This is a common problem in the field of order statistics <ref> [20] </ref>, which is the source of this evaluation technique.) Because of the exponential complexity of calculating the expected value of N 0 , we chose to estimate it experimentally using our implementation of the extended window algorithm.
Reference: [21] <author> Donald E. Knuth. </author> <title> The Art of Computer Programming: Volume 3, Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Since both residual calculation and Select have O (N ) expected case time, there is no savings in complexity. However, Select has a larger constant factor ( 3:39 <ref> [21] </ref>) on the number of comparisons and includes exchanges, so there should be savings in actual execution time. The savings using this short-circuiting technique can be estimated easily if we assume that any random sample is just as likely to be the best as any other.
Reference: [22] <author> C. L. Liu. </author> <title> Introduction to Combinatorial Mathematics. </title> <publisher> McGraw-Hill, </publisher> <year> 1968. </year> <month> 46 </month>
Reference-contexts: The function fl (m; n) is equivalent to the number of ways of placing m distinct objects in n cells <ref> [22] </ref>, and is found to be fl (m; n) = i=0 where S (m; n) is a Sterling number of the second kind. More intuitively, fl (m; n) is an inclusion-exclusion summation, where each term represents the number of sequences missing at least i of the n integers.
References-found: 22

