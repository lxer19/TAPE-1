URL: http://theory.lcs.mit.edu/~sivan/tpds.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~sivan/papers.html
Root-URL: 
Email: Email: toledo@parc.xerox.com.  Email: gustav@watson.ibm.com.  
Title: The Design and Implementation of SOLAR, a Portable Library for Scalable Out-of-Core Linear Algebra Computations  
Author: Sivan Toledo, Fred G. Gustavson Sivan Toledo Fred G. Gustavson 
Address: 3333 Coyote Hill Road, Palo Alto, CA 94304.  P.O. Box 218, Yorktown Heights, NY 10598.  
Affiliation: Palo Alto Research Center,  IBM T.J. Watson Research Center,  
Date: 1999 1  
Note: IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. XX, NO. Y, MONTH  is with the Xerox  is with the  DRAFT  
Abstract: Part of this research was done while the first author was a postdoctoral fellow at the IBM T.J. Watson Research Center. Some of the research reported here was conducted in the FLEET Lab, Dartmouth College, which is supported in part by NASA Ames Research Center under agreements numbered NCC 2-849 and NSG 2-936. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen, </author> <title> LAPACK User's Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, 2nd edition, </address> <year> 1994, </year> <note> Also available online from http://www.netlib.org. </note>
Reference-contexts: The in-core subroutines consist of the so-called Basic Linear Algebra Subroutines [20], [21], [22] (BLAS), the parallel BLAS [23], LAPACK <ref> [1] </ref>, and ScaLA- PACK [24]. The generic components of the library consist of subroutines that schedule I/O operations and call the in-core subroutines to manipulate submatrices stored in in-core buffers. 2. The library will provide a large set of solvers by following a traditional layered approach to linear algebra software.
Reference: [2] <author> David A. Patterson and John L. Hennessy, </author> <title> Computer Architecture A Quantitative Approach, Second Edition, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <year> 1995. </year>
Reference: [3] <author> D. W. Barron and H. P. F. Swinnerton-Dyerm, </author> <title> "Solution of simultaneous linear equations using a magnetic tape store," </title> <journal> Computer Journal, </journal> <volume> vol. 3, </volume> <pages> pp. 28-33, </pages> <year> 1960. </year>
Reference: [4] <author> Jean-Philippe Brunet, Palle Pedersen, and S. Lennart Johnsson, </author> <title> "Load-balanced LU and QR factor and solve routines for scalable processors with scalable I/O," </title> <booktitle> in Proceedings of the 17th IMACS World Congress, </booktitle> <address> Atlanta, Georgia, </address> <month> July </month> <year> 1994, </year> <note> Also available as Harvard University Computer Science Technical Report TR20-94. </note>
Reference: [5] <author> J. J. Dongarra, S. Hammarling, and D. W. Walker, </author> <title> "Key concepts for parallel out-of-core LU factorization," </title> <type> Tech. Rep. </type> <institution> CS-96-324, University of Tennessee, </institution> <month> Apr. </month> <year> 1996, </year> <note> LAPACK Working Note 110. </note>
Reference: [6] <author> J. J. Du Cruz, S. M. Nugent, J. K. Reid, and D. B. Taylor, </author> <title> "Solving large full sets of linear equations in a paged virtual store," </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> vol. 7, no. 4, </volume> <pages> pp. 527-536, </pages> <year> 1981. </year>
Reference-contexts: When the factorization is complete, each block is read again, the row exchanges that were generated after it was factored are applied, and it is written back. This algorithm, which was first proposed in <ref> [6] </ref>, has two desirable features. First, the amount of I/O required for row exchanges is only one read and one write for every element in the strictly block-lower-triangular part of the matrix. Second, except for these writes, each element is written only once.
Reference: [7] <author> Charbel Farhat, </author> <title> "Large out-of-core calculation runs on the IBM SP2," </title> <journal> NAS News, </journal> <volume> vol. 2, no. 11, </volume> <month> Aug. </month> <year> 1995. </year>
Reference: [8] <author> Nikolaus Geers and Roland Klees, </author> <title> "Out-of-core solver for large dense nonsymmetric linear systems," </title> <journal> Manuscripta Geodetica, </journal> <volume> vol. 18, no. 6, </volume> <pages> pp. 331-342, </pages> <year> 1993. </year> <title> DRAFT TOLEDO AND GUSTAVSON: SOLAR, A SCALABLE OUT-OF-CORE LINEAR ALGEBRA LIBRARY 31 </title>
Reference: [9] <author> Roger G. Grimes and Horst D. Simon, </author> <title> "Solution of large, dense symmetric generalized eigenvalue problems using secondary storage," </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> vol. 14, no. 3, </volume> <pages> pp. 241-256, </pages> <year> 1988. </year>
Reference: [10] <author> Kenneth Klimkowski and Robert van de Geijn, </author> <title> "Anatomy of an out-of-core dense linear solver," </title> <booktitle> in Proceedings of the 1995 International Conference on Parallel Processing, </booktitle> <year> 1995, </year> <note> To appear. </note>
Reference: [11] <author> J. Rutledge and H. Rubinstein, </author> <title> "High order matrix computation on the UNIVAC," </title> <booktitle> Presented at the meeting of the Association for Computing Machinery, </booktitle> <month> May </month> <year> 1952. </year>
Reference: [12] <author> Joseph Rutledge and Harvey Rubinstein, </author> <title> "Matrix algebra programs for the UNIVAC," </title> <booktitle> Presented at the Wayne Conference on Automatic Computing Machinery and Applications, </booktitle> <month> Mar. </month> <year> 1951. </year>
Reference: [13] <author> Hans Riesel, </author> <title> "A note on large linear systems," Mathematical Tables and Other Aids to Computation, </title> <journal> vol. </journal> <volume> 10, </volume> <pages> pp. 226-227, </pages> <year> 1956. </year>
Reference: [14] <author> David S. Scott, </author> <title> "Out of core dense solvers on Intel parallel supercomputers," </title> <booktitle> in Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <year> 1992, </year> <pages> pp. 484-487. </pages>
Reference: [15] <author> David S. Scott, </author> <title> "Parallel I/O and solving out of core systems of linear equations," </title> <booktitle> in Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <address> Hanover, NH, </address> <month> June </month> <year> 1993, </year> <institution> Dartmouth Institute for Advanced Graduate Studies, </institution> <note> pp. 123-130. </note>
Reference: [16] <author> M. M. Stabrowski, </author> <title> "A block equation solver for large unsymmetric linear equation systems with dense coefficient matrices," </title> <journal> International Journal for Numerical Methods in Engineering, </journal> <volume> vol. 24, </volume> <pages> pp. 289-300, </pages> <year> 1982. </year>
Reference: [17] <author> David Womble, David Greenberg, Stephen Wheat, and Rolf Riesen, </author> <title> "Beyond core: Making paral-lel computer I/O practical," </title> <booktitle> in Proceeings of the 1993 DAGS/PC Symposium, </booktitle> <address> Hanover, NH, </address> <month> June </month> <year> 1993, </year> <pages> pp. 56-63, </pages> <institution> Dartmouth Institute for Advanced Graduate Studies, </institution> <note> Also available online from http://www.cs.sandia.gov/~dewombl. </note>
Reference: [18] <author> David Kotz, </author> <title> "Disk-directed I/O for MIMD multiprocessors," </title> <booktitle> in Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994, </year> <pages> pp. 61-74, </pages> <note> Updated as Dartmouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference: [19] <author> Rajeev Thakur and Alok Choudhary, </author> <title> "An extended two-phase method for accessing sections of out-of-core arrays," </title> <type> Tech. Rep. </type> <institution> CACR-103, Scalable I/O Initiative, Center for Advanced Computing Research, Caltech, </institution> <month> June </month> <year> 1995, </year> <note> Submitted to a special issue of Scientific Programming on implementations of HPF. </note>
Reference-contexts: First, such an interface allows an out-of-core subroutine to use multiple out-of-core storage layouts and multiple low-level I/O interfaces without explicit reference to all of these options. Second, a high level interface allows for I/O optimizations such as two-phase I/O <ref> [19] </ref> and disk-directed I/O [26]. The contract between the out-of-core subroutines and the MIOS is simple. The MIOS can transfer any rectangular block of a matrix between primary and secondary memories.
Reference: [20] <author> Jack J. Dongarra, Jeremy Du Cruz, Sven Hammarling, and Iain Duff, </author> <title> "An set of level 3 basic linear algebra subprograms," </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> vol. 16, no. 1, </volume> <pages> pp. 1-17, </pages> <year> 1990. </year>
Reference-contexts: The platform dependent layer of the library consists of a set of functions that perform I/O on subma- trices, as well as of high-performance in-core subroutines that are already implemented on most platforms. The in-core subroutines consist of the so-called Basic Linear Algebra Subroutines <ref> [20] </ref>, [21], [22] (BLAS), the parallel BLAS [23], LAPACK [1], and ScaLA- PACK [24]. The generic components of the library consist of subroutines that schedule I/O operations and call the in-core subroutines to manipulate submatrices stored in in-core buffers. 2. <p> We focus on the interaction between the design of these algorithms and on the design of SOLAR's LU factorization algorithms. We have already implemented three out of the nine level-3 BLAS <ref> [20] </ref>, namely general matrix multiply-add, symmetric rank-k update, and triangular solve, as well as factor and solve subroutines for positive-definite symmetric and general matrices. The BLAS use block-iterative algorithms.
Reference: [21] <author> Jack J. Dongarra, Jeremy Du Cruz, Sven Hammarling, and Richard J. Hanson, </author> <title> "An extended set of FORTRAN basic linear algebra subprograms," </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> vol. 14, no. 1, </volume> <pages> pp. 1-17, </pages> <year> 1988. </year>
Reference-contexts: The platform dependent layer of the library consists of a set of functions that perform I/O on subma- trices, as well as of high-performance in-core subroutines that are already implemented on most platforms. The in-core subroutines consist of the so-called Basic Linear Algebra Subroutines [20], <ref> [21] </ref>, [22] (BLAS), the parallel BLAS [23], LAPACK [1], and ScaLA- PACK [24]. The generic components of the library consist of subroutines that schedule I/O operations and call the in-core subroutines to manipulate submatrices stored in in-core buffers. 2.
Reference: [22] <author> C. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh, </author> <title> "Basic linear algebra subprogram for Fortran usage," </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> vol. 5, no. 3, </volume> <pages> pp. 308-323, </pages> <year> 1979. </year>
Reference-contexts: The platform dependent layer of the library consists of a set of functions that perform I/O on subma- trices, as well as of high-performance in-core subroutines that are already implemented on most platforms. The in-core subroutines consist of the so-called Basic Linear Algebra Subroutines [20], [21], <ref> [22] </ref> (BLAS), the parallel BLAS [23], LAPACK [1], and ScaLA- PACK [24]. The generic components of the library consist of subroutines that schedule I/O operations and call the in-core subroutines to manipulate submatrices stored in in-core buffers. 2.
Reference: [23] <author> J. Choi, J. Dongarra, S. Ostrouchov, A. Petitet, D. Walker, and R. C. Whaley, </author> <title> "A proposal for a set of parallel basic linear algebra subprograms," </title> <type> Tech. Rep. </type> <institution> CS-95-292, University of Tennessee, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The in-core subroutines consist of the so-called Basic Linear Algebra Subroutines [20], [21], [22] (BLAS), the parallel BLAS <ref> [23] </ref>, LAPACK [1], and ScaLA- PACK [24]. The generic components of the library consist of subroutines that schedule I/O operations and call the in-core subroutines to manipulate submatrices stored in in-core buffers. 2.
Reference: [24] <author> J. Choi, J. Dongarra, R. Pozo, and D. Walker, </author> <title> "ScaLAPACK: A scalable linear algebra for distributed memory concurrent computers," </title> <booktitle> in Proceedings of the 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <year> 1992, </year> <pages> pp. 120-127, </pages> <note> Also available as University of Tennessee Technical Report CS-92-181. </note>
Reference-contexts: The in-core subroutines consist of the so-called Basic Linear Algebra Subroutines [20], [21], [22] (BLAS), the parallel BLAS [23], LAPACK [1], and ScaLA- PACK <ref> [24] </ref>. The generic components of the library consist of subroutines that schedule I/O operations and call the in-core subroutines to manipulate submatrices stored in in-core buffers. 2. The library will provide a large set of solvers by following a traditional layered approach to linear algebra software. <p> (18) 99 (18) 68 (18) 70 (18) 276 (40) 315 (38) 194 (38) 215 (38) Distributed 4 209 (20) 184 (20) 124 (20) 101 (20) 51 (21) 64 (20) 268 (31) 360 (31) 188 (32) 223 (30) Subroutine Library (PESSL) version 1.1, which based on and completely compatible with ScaLAPACK <ref> [24] </ref>, a public domain linear algebra package for linear algebra computations. 3 For in-core computations on arrays that are local to a processor we used IBM's Engineering and Scientific Subroutine Library (ESSL) version 2.2. We used POWER2-specific versions of all the libraries.
Reference: [25] <author> Thomas H. Cormen and David Kotz, </author> <title> "Integrating theory and practice in parallel file systems," </title> <type> Tech. Rep. </type> <institution> PCS-TR93-188, Dept. of Math and Computer Science, Dartmouth College, </institution> <month> March </month> <year> 1993, </year> <note> Revised 9/20/94. DRAFT 32 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. XX, NO. Y, MONTH 1999 An ealier version appeared in the Proceedings of the 1993 DAGS/PC Symposium. </note>
Reference-contexts: Second, the specification of out-of-core data mappings depends on how I/O requests and file structures are specified to the operating system or to the file system, and it seems that too early to settle this issue <ref> [25] </ref>. Fortunately, it is possible to allow such diversity, because only the MIOS need to know the details of the out-of- core data mapping. Hence, the out-of-core storage descriptor can be viewed as an opaque object accessible only to the MIOS.
Reference: [26] <author> David Kotz, </author> <title> "Disk-directed I/O for an out-of-core computation," </title> <booktitle> in Proceedings of the Fourth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <month> August </month> <year> 1995, </year> <pages> pp. 159-166, </pages> <note> Also available as Dartmouth College Technical Report PCS-TR95-251. </note>
Reference-contexts: First, such an interface allows an out-of-core subroutine to use multiple out-of-core storage layouts and multiple low-level I/O interfaces without explicit reference to all of these options. Second, a high level interface allows for I/O optimizations such as two-phase I/O [19] and disk-directed I/O <ref> [26] </ref>. The contract between the out-of-core subroutines and the MIOS is simple. The MIOS can transfer any rectangular block of a matrix between primary and secondary memories.
Reference: [27] <author> John Bruno and Peter Cappello, </author> <title> "Implementing the 3D Alternating Direction Method on the hypercube," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 23, </volume> <pages> pp. 411-417, </pages> <year> 1994. </year>
Reference-contexts: SOLAR now uses a block-cyclic layout for laying out matrices on parallel disks, but we plan to support the 2-dimensional cyclically-shifted block layout in the future. A similar layout that satisfies somewhat different constraints was previously proposed by Bruno and Cappello <ref> [27] </ref>. We present this new layout in order to demonstrate that it is not wise to fix at this point the types of out-of-core matrix layouts that the MIOS module should support.
Reference: [28] <author> Sivan Toledo, </author> <title> "Locality of reference in LU decomposition with partial pivoting," </title> <type> Tech. Rep. </type> <institution> RC20344, IBM T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Each half is factored using the same strategy. Toledo <ref> [28] </ref> recently proved that this algorithm uses only fi (n 3 = p words worth of I/O in a simple two level memory hierarchy, where n is the order of the matrix and M is the size of main memory.
Reference: [29] <author> Nils Nieuwejaar, </author> <title> Galley: A New Parallel File System for Scientific Applications, </title> <type> Ph.D. thesis, </type> <institution> Dartmouth College, </institution> <year> 1996. </year>
Reference-contexts: The POSIX system calls can be also be used to access files stored in PIOFS, IBM's parallel file system. In addition to these low-level I/O interfaces, the implementation can also use Galley, a new explicitly parallel file system written by Nils Nieuwejaar and David Kotz <ref> [29] </ref>, [30]. SOLAR's matrix I/O subroutines module (MIOS) can use either a 2-phase approach or a strided approach to parallel I/O. In the 2-phase I/O, the MIOS read or write large blocks that match the matrix distribution on disks and redistribute the data in a separate phase. <p> The size of these submatrices is usually either the primary block size or the block size of the block-cyclic data layout. Using Galley's Strided-I/O Interface Galley <ref> [29] </ref>, [30] is a parallel file system that offers two main advantages over traditional I/O interfaces. <p> We expect that most of these restrictions in ScaLAPACK will be lifted in the near future [35]. This would make it even easier to implement parallel out-of-core solvers in SOLAR. Experiments using the Galley file system <ref> [29] </ref>, [30] indicate that SOLAR achieves better performance with the strided approach than with the 2-phase I/O approach.
Reference: [30] <author> Nils Nieuwejaar and David Kotz, </author> <title> "The Galley parallel file system," </title> <booktitle> Parallel Computing, </booktitle> <year> 1996, </year> <note> To appear. </note>
Reference-contexts: The POSIX system calls can be also be used to access files stored in PIOFS, IBM's parallel file system. In addition to these low-level I/O interfaces, the implementation can also use Galley, a new explicitly parallel file system written by Nils Nieuwejaar and David Kotz [29], <ref> [30] </ref>. SOLAR's matrix I/O subroutines module (MIOS) can use either a 2-phase approach or a strided approach to parallel I/O. In the 2-phase I/O, the MIOS read or write large blocks that match the matrix distribution on disks and redistribute the data in a separate phase. <p> The size of these submatrices is usually either the primary block size or the block size of the block-cyclic data layout. Using Galley's Strided-I/O Interface Galley [29], <ref> [30] </ref> is a parallel file system that offers two main advantages over traditional I/O interfaces. <p> We used simulated disks since we were not able to allocate a dedicated raw disk to Galley on each node; the same methodology was used by Nieuwejaar and Kotz in their performance evaluation of Galley <ref> [30] </ref>. To avoid unnecessary bias in the experiments due to different file systems, we used Galley for both 2-phase I/O and for strided I/O. <p> We expect that most of these restrictions in ScaLAPACK will be lifted in the near future [35]. This would make it even easier to implement parallel out-of-core solvers in SOLAR. Experiments using the Galley file system [29], <ref> [30] </ref> indicate that SOLAR achieves better performance with the strided approach than with the 2-phase I/O approach.
Reference: [31] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary, </author> <title> "Improved parallel I/O via a two-phase run-time access strategy," </title> <booktitle> in Proceedings of the IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <year> 1993, </year> <pages> pp. 56-70, </pages> <note> Also published in Computer Architecture News, </note> <month> 21(5) </month> <pages> 31-38, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: In such environments the MIOS use a 2-phase approach to I/O, first proposed by del Rosario, Bordawekar, and Choudhary <ref> [31] </ref>. In the 2-phase I/O compute nodes read the data using a data distribution that allows compute nodes to issue few I/O requests for large blocks of data. The nodes than redistribute the data to conform to the distribution requested by the application.
Reference: [32] <author> Sivan Toledo and Fred G. Gustavson, </author> <title> "The design and implementation of SOLAR, a portable library for scalable out-of-core linear algebra computations," </title> <booktitle> in Proceedings of the 4th Annual Workshop on I/O in Parallel and Distributed Systems, </booktitle> <address> Philadelphia, </address> <month> May </month> <year> 1996, </year> <pages> pp. 28-40. </pages>
Reference-contexts: The details of a porting experience that supports our claim that SOLAR is indeed portable, as well as additional experiments on workstations, are included in an earlier report on SOLAR <ref> [32] </ref> and are omitted from this paper.
Reference: [33] <author> T. Agerwala, J. L. Martin, J. H. Mirza, D. C. Sadler, D. M. Dias, and M. Snir, </author> <title> "SP2 system architecture," </title> <journal> IBM Systems Journal, </journal> <volume> vol. 34, no. 2, </volume> <pages> pp. 152-184, </pages> <year> 1995. </year>
Reference-contexts: We conclude that whereas LAPACK's block-iterative algorithms can deliver acceptable performance, specialized out-of-core algorithms can run several times faster. Experiments on Parallel Computers using 2-Phase I/O The experiments were performed on two IBM SP2 parallel computers <ref> [33] </ref> which we denote by machine T and machine W. Machine T was configured with so-called thin nodes with 128 Mbytes of main memory as both the compute and I/O nodes and ran AIX version 4.1.3.
Reference: [34] <author> P. F. Corbett, D. G. Feitelson, J.-P. Prost, G. S. Almasi, S. J. Baylor, A. S. Bolmarcich, Y. Shu, J. Satran, M. Snir, R. Colao, B. D. Herr, J. Kavaky, T. R. Morgan, and A. Zlotek, </author> <title> "Parallel file systems for the IBM SP computers," </title> <journal> IBM Systems Journal, </journal> <volume> vol. 34, no. 2, </volume> <pages> pp. 222-248, </pages> <year> 1995. </year>
Reference-contexts: Wide-2 nodes have a 77 MHz POWER2 processor, 256 Kbytes 4-way set associative level- 1 data-cache, no level-2 cache, and a 256-bit-wide main memory bus. Thin nodes are the slowest SP2 nodes and wide-2 nodes are currently the fastest. We used IBM's parallel file system <ref> [34] </ref> (PIOFS) version 1.1. On machine T PIOFS used 4 I/O nodes. The parallel file system used 1 Gbytes on each node, allocated on a 2.2 Gbytes 16-bit SCSI disk. On machine W PIOFS used 4 I/O nodes.
Reference: [35] <author> Jack Dongarra, </author> <title> "ScaLAPACK software update," </title> <journal> NA Digest, </journal> <volume> vol. 96, no. 20, </volume> <month> May </month> <year> 1996. </year> <note> DRAFT </note>
Reference-contexts: XX, NO. Y, MONTH 1999 selection that SOLAR performs for in-core buffers must be done very carefully, which slows down the implementation process for new parallel out-of-core solvers. We expect that most of these restrictions in ScaLAPACK will be lifted in the near future <ref> [35] </ref>. This would make it even easier to implement parallel out-of-core solvers in SOLAR. Experiments using the Galley file system [29], [30] indicate that SOLAR achieves better performance with the strided approach than with the 2-phase I/O approach.
References-found: 35

