URL: ftp://psyche.mit.edu/pub/jordan/colt94.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/jordan.html
Root-URL: 
Email: jordan@psyche.mit.edu  
Title: A Statistical Approach to Decision Tree Modeling  
Author: Michael I. Jordan 
Address: Cambridge, MA 02139  
Affiliation: Department of Brain and Cognitive Sciences Massachusetts Institute of Technology  
Abstract: A statistical approach to decision tree modeling is described. In this approach, each decision in the tree is modeled parametrically as is the process by which an output is generated from an input and a sequence of decisions. The resulting model yields a likelihood measure of goodness of fit, allowing ML and MAP estimation techniques to be utilized. An efficient algorithm is presented to estimate the parameters in the tree. The model selection problem is presented and several alternative proposals are considered. A hidden Markov version of the tree is described for data sequences that have temporal dependencies.
Abstract-found: 1
Intro-found: 1
Reference: <author> Baum, L.E., Petrie, T., Soules, G., & Weiss, N. </author> <year> (1970). </year> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> The Annals of Mathematical Statistics, </journal> <volume> 41, </volume> <pages> 164-171. </pages>
Reference: <author> Bengio, Y., & Frasconi, P. </author> <title> (in press). Credit assignment through time: Alternatives to backpropagation. </title> <booktitle> Neural Information Processing Systems 6. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: With the choice of a multinomial logit model for the decisions the probabilistic decision tree is closely related to the CART <ref> (Breiman, et al., 1984) </ref> and C4.5 (Quinlan, 1993) models. In the multinomial logit model, the decision probabilities are functions of linear discriminants. <p> For example, the probabilities at the top level of the tree are given as follows P (! i (t)j! i (t 1); x (t); ) Each decision in the tree is obtained in this manner, yielding an architecture (see Figure 7) in which the decisions probabil-be considered <ref> (Breiman, et al., 1984) </ref>.
Reference: <author> Cacciatore, T. & Nowlan, S. </author> <title> (in press). Mixtures of controllers for jump linear and non-linear plants. </title> <booktitle> Neural Information Processing Systems 6. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dempster, A. P., Laird, N. M., & Rubin, D. B. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 39, </volume> <pages> 1-38. </pages>
Reference-contexts: One example is the missing data problem (Quinlan, 1993). This problem can be addressed readily via the EM algorithm, which was originally developed as much to handle missing data problems as to handle mixture problems <ref> (Dempster, et al., 1977) </ref>.
Reference: <author> Draper, N. R., & Smith, H. </author> <year> (1981). </year> <title> Applied Regression Analysis. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference-contexts: In regression analysis, the randomness inherent in the data induces randomness in the estimates of the parameters for the slope and intercept of the regression line. The variance in these estimates is affected by the spread of the data on the x-axis, a phenomenon referred to as leverage <ref> (Draper & Smith, 1981) </ref>. Leverage is quadratic; that is, data points that are furthest from the mean x value have a substantially greater influence on the variance than data points near the mean. <p> This effective growth in complexity motivates a number of other approaches to the model selection problem. One approach involves using ridge regression for the decision models and the leaf models. Ridge regression shrinks the parameters of a regression toward zero, providing control over the complexity of the model <ref> (Draper & Smith, 1981) </ref>. This shrinkage can be viewed as a form of pruning; indeed, the ridge regression approach can be used in conjunction with a conservatively pruned CART or C4.5 tree. Another approach is to utilize a cross-validation procedure to stop the iterative process.
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference: <author> Ghahramani, Z., & Jordan, M. I. </author> <title> (in press). Supervised learning from incomplete data via the EM approach. </title> <booktitle> Neural Information Processing Systems 6. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Jordan, M. I. & Jacobs, R. A. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6, </volume> <pages> 181-214. </pages>
Reference-contexts: In this case the parameter update involves solving the following system of equations for ijk : X h ijk (y (p) f ( T where f is the logistic function <ref> (Jordan & Jacobs, 1994) </ref>. This problem can be solved efficiently by a quadratically-convergent algorithm known as (weighted) iteratively reweighted least squares (IRLS) (McCullagh & Nelder, 1983). In the case of multiway classification, a generalization of logistic regression known as the multinomial logit model can be used. <p> One approach is to utilize the multinomial logit model, a member of the generalized linear model (GLIM) family (McCullagh & Nelder, 1983). It can be shown that the likelihood for this model is a cross entropy of the form in Equation 2 <ref> (see Jordan & Jacobs, 1994) </ref>, making this model a natural choice for modeling decisions in decision trees. <p> In the multinomial logit model, the decision probabilities are functions of linear discriminants. In particular, the decision probabilities for the top level of the tree are as follows <ref> (Jordan & Jacobs, 1994) </ref>: P (! i jx; ) = i x j e j x where = f i g n i=1 . <p> is a function of the condition number of the matrix (P 1 1 2 ), where P is a covariance matrix and H is the Hessian of the log likelihood. 5 The final relative errors were .09 for the multilayer perceptron, .10 for HME, and .13 for CART. batch algorithms. <ref> (Copied with permission from Jordan and Jacobs, 1994) </ref>. 6 MODEL SELECTION The problem of parameter estimation is the problem of choosing best parameter values given a particular fixed choice of a parametric model.
Reference: <author> Jordan, M. I., & Xu, L. </author> <year> (1993). </year> <title> Convergence properties of the EM approach to learning in mixture-of-experts architectures. </title> <institution> MIT Artificial Intelligence Laboratory Tech. </institution> <type> Rep. 1458, </type> <address> Cambridge, MA. </address>
Reference: <author> McCullagh, P. & Nelder, J.A. </author> <year> (1983). </year> <title> Generalized Linear Models. </title> <publisher> London: Chapman and Hall. </publisher>
Reference-contexts: This problem can be solved efficiently by a quadratically-convergent algorithm known as (weighted) iteratively reweighted least squares (IRLS) <ref> (McCullagh & Nelder, 1983) </ref>. In the case of multiway classification, a generalization of logistic regression known as the multinomial logit model can be used. A variety of other classification models can also be considered. <p> See Jordan & Jacobs (in press). model this density. One approach is to utilize the multinomial logit model, a member of the generalized linear model (GLIM) family <ref> (McCullagh & Nelder, 1983) </ref>. It can be shown that the likelihood for this model is a cross entropy of the form in Equation 2 (see Jordan & Jacobs, 1994), making this model a natural choice for modeling decisions in decision trees.
Reference: <author> Meila, M. P., & Jordan, M. I. </author> <year> (1994). </year> <title> Learning the parameters of HMMs with auxiliary input. </title> <institution> MIT Computational Cognitive Science Tech. </institution> <type> Rep. 9401, </type> <address> Cambridge, MA. </address>
Reference: <author> Murthy, S. K., Kasif, S., & Salzberg, S. </author> <year> (1993). </year> <title> OC1: A randomized algorithm for building oblique decision trees. </title> <type> Technical Report, </type> <institution> Department of Computer Science, Johns Hopkins University. </institution>
Reference: <author> Neal, R., & Hinton, G. E. </author> <year> (1993). </year> <title> A new view of the EM algorithm that justifies incremental and other variants. </title> <note> Submitted to Biometrika. </note>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: With the choice of a multinomial logit model for the decisions the probabilistic decision tree is closely related to the CART (Breiman, et al., 1984) and C4.5 <ref> (Quinlan, 1993) </ref> models. In the multinomial logit model, the decision probabilities are functions of linear discriminants. <p> One example is the missing data problem <ref> (Quinlan, 1993) </ref>. This problem can be addressed readily via the EM algorithm, which was originally developed as much to handle missing data problems as to handle mixture problems (Dempster, et al., 1977).
Reference: <author> Quinlan, J. R., & Rivest, R. L. </author> <year> (1989). </year> <title> Inferring decision trees using the Minimum Description Length Principle. </title> <journal> Information and Computation, </journal> <volume> 80, </volume> <pages> 227-248. </pages>
Reference: <author> Utgoff, P. E., & Brodley, C. E. </author> <year> (1990). </year> <title> An incremental method for finding multivariate splits for decision trees. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <address> Los Altos, CA. </address>
Reference: <author> Xu, L., Jordan, M. I., & Hinton, G. E. </author> <year> (1994). </year> <title> An alternative mixture of experts model. </title> <institution> MIT Computational Cognitive Science Tech. </institution> <type> Rep. 9402, </type> <address> Cambridge, MA. </address>
References-found: 18

