URL: http://www.demo.cs.brandeis.edu/postscript.dump/gendiv.ps.gz
Refering-URL: http://www.cs.brandeis.edu/~darwen/publications.html
Root-URL: http://www.cs.brandeis.edu
Email: Email: darwen@canth.cs.adfa.oz.au  
Title: The Exploitation of Cooperation in Iterated Prisoner's Dilemma  
Author: P. J. Darwen and X. Yao 
Date: 11 November 1996  
Address: Canberra ACT 2600 AUSTRALIA  
Affiliation: Department of Computer Science University College, University of New South Wales Australian Defence Force Academy  
Abstract: We follow Axelrod [2] in using the genetic algorithm to play Iterated Prisoner's Dilemma. Each member of the population (i.e., each strategy) is evaluated by how it performs against the other members of the current population. This creates a dynamic environment in which the algorithm is optimising to a moving target instead of the usual evaluation against some fixed set of strategies, causing an "arms race" of innovation [3]. We conduct two sets of experiments. The first set investigates what conditions evolve the best strategies. The second set studies the robustness of the strategies thus evolved, that is, are the strategies useful only in the round robin of its population or are they effective against a wide variety of opponents? Our results indicate that the population has nearly always converged by about 250 generations, by which time the bias in the population has almost always stabilised at 85%. Our results confirm that cooperation almost always becomes the dominant strategy [1, 2]. We can also confirm that seeding the population with expert strategies is best done in small amounts so as to leave the initial population with plenty of genetic diversity [7]. The lack of robustness in strategies produced in the round robin evaluation is demonstrated by some examples of a population of nave cooperators being exploited by a defect-first strategy. This causes a sudden but ephemeral decline in the population's average score, but it recovers when less nave cooperators emerge and do well against the exploiting strategies. This example of runaway evolution is brought back to reality by a suitable mutation, reminiscent of punctuated equilibria [12]. We find that a way to reduce such navity is to make the GA population play against an extra, 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert M. Axelrod. </author> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, </publisher> <year> 1984. </year>
Reference-contexts: The GA works well in searching spaces with the same difficulties faced by machine learning when searching the space of possible strategies in games of conflict [4]. We use the GA to learn to play the iterated prisoner's dilemma (IPD) <ref> [1] </ref>, a simple two-player non-zero-sum game which is widely studied in such diverse fields as machine learning, economics, political science, and mathematical game theory. 1.1 Background Axelrod [1] investigated evolution and the IPD, and simulated a population (of strategies) in which each strategy plays IPD with every other individual [2, p <p> We use the GA to learn to play the iterated prisoner's dilemma (IPD) <ref> [1] </ref>, a simple two-player non-zero-sum game which is widely studied in such diverse fields as machine learning, economics, political science, and mathematical game theory. 1.1 Background Axelrod [1] investigated evolution and the IPD, and simulated a population (of strategies) in which each strategy plays IPD with every other individual [2, p 38]. In such a system, the environment (made up by the members of the evolving population) is constantly changing. <p> Even though the we simulate the most mercenary, dog-eat-dog form of evolution, cooperative strategies eventually proliferate and drive their non-cooperative competitors to extinction. Axelrod <ref> [1] </ref> describes this phenomenon in great detail. 'avg' 'best' The Evolution of Cooperation Generations (time steps) Best and v erage p erformances 200150100500 3 2.8 2.6 2.4 2.2 non-cooperative strategies proliferate and they run out of victims. <p> This is shown in figure 6. As we expect from the evolution of cooperation in a co-evolving population <ref> [1] </ref>, almost all final strategies do equally well against tit-for-tat. Surprisingly, the best results against random strategies are not achieved when the initial population is all random, as one might expect. <p> This is a genotype of 70 bits just like the other members of the population, but its contents are random, and stay the same for a generation, before being replaced by a different random genotype in the next generation. 3. As for 1, but including the expert strategy tit-for-tat <ref> [1] </ref>. 4. As for 1, but including both a (different) random strategy each generation, and the expert strategy tit-for-tat [1]. <p> As for 1, but including the expert strategy tit-for-tat <ref> [1] </ref>. 4. As for 1, but including both a (different) random strategy each generation, and the expert strategy tit-for-tat [1]. How can we evaluate the contents of a final population? That is, how do we tell if one round robin learned to play IPD better than another? For each of the 400 runs, we followed the following procedure:- 1. Run for 250 generations. 2.
Reference: [2] <author> Robert M. Axelrod. </author> <title> The evolution of strategies in the iterated prisoner's dilemma. </title> <editor> In Lawrence Davis, editor, </editor> <title> Genetic Algorithms and Simulated Annealing, </title> <booktitle> chapter 3, </booktitle> <pages> pages 32-41. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: (IPD) [1], a simple two-player non-zero-sum game which is widely studied in such diverse fields as machine learning, economics, political science, and mathematical game theory. 1.1 Background Axelrod [1] investigated evolution and the IPD, and simulated a population (of strategies) in which each strategy plays IPD with every other individual <ref> [2, p 38] </ref>. In such a system, the environment (made up by the members of the evolving population) is constantly changing. He found that this dynamic environment produced strategies that performed very well against their population. Fogel [5] studied an almost identical system. <p> This desirable quality is "robustness". Lindgren provided a radical insight [12]. He simulated a system similar to those of Axelrod <ref> [2] </ref> and Fogel [5]. He showed that (under certain conditions) when a population (of strategies) plays IPD against its own members, the high-performing strategies who dominate the population for long periods of time, are suddenly wiped out and replaced. <p> Why did they seemingly-invincible strategies fail? How can we prevent this disas-ter happening to strategies produced by evolutionary machine learning? This paper answers these questions. 1.2 The Answers We study a system similar to Axelrod's <ref> [2] </ref>, and find a reason for Lindgren's catastrophic collapses [12]. It turns out that when a near-expert strategy dominates a population, the homogeneity in their behaviour (in the case of IPD, they all cooperate) means certain features atrophy. <p> In iterated prisoner's dilemma (IPD), this step is repeated many times, and each player can remember previous steps. We used Axelrod's variant of prisoner's dilemma <ref> [2] </ref>, is shown in figure 2. 5 3 1 0 Cooperate Defect Cooperate Defect The number of rounds per game is fixed at 50 rounds; that is, each pair of opponents play the game in figure 2 for 50 rounds, remembering the previous outcomes. 3.2 Software Implementation Our implementation follows Axelrod <p> is shown in figure 2. 5 3 1 0 Cooperate Defect Cooperate Defect The number of rounds per game is fixed at 50 rounds; that is, each pair of opponents play the game in figure 2 for 50 rounds, remembering the previous outcomes. 3.2 Software Implementation Our implementation follows Axelrod <ref> [2] </ref>. Lindgren [12] used a variation of Axelrod's approach, and Fogel [5] used evolutionary programming which is strictly speaking not a GA. However, the three approaches are broadly similar, and our results are relevant to all of evolutionary machine learning. Several excellent public-domain genetic algorithm packages are available. <p> Several excellent public-domain genetic algorithm packages are available. We used Grefenstette's GENESIS 5.0, and modified the evaluation procedure to evaluate individuals from how they score against other members of the population. 3.2.1 Genotype We used the same genotype as Axelrod <ref> [2] </ref> Only the three most recent steps are remembered. Since each step has 4 possible outcomes (see figure 2), that means that there are 4 fi 4 fi 4 = 64 possible histories of 3 steps. <p> We did thirty runs, each starting from a (different) random initial population of 100 individuals. The round robin was the same as Axelrod's <ref> [2] </ref>; all members of the population compete against every member of the population, including itself. and standard deviation of the bias 1 of the population.
Reference: [3] <author> Richard Dawkins. </author> <title> The Blind Watchmaker. Longman, </title> <booktitle> first edition, </booktitle> <year> 1986. </year>
Reference-contexts: The results of Lindgren's simulation bear an uncanny resemblance to the "punctuated equilibria" of natural evolution <ref> [3] </ref>. "In particular, the large extinctions that appear in these simulations should be studied in more detail, since these collapses are triggered by the dynamical system itself and do not need external catastrophes for their explanation." [12, p 310].
Reference: [4] <author> Kenneth A. De Jong. </author> <title> Genetic-algorithm-based learning. </title> <editor> In Y. Kodratoff and R. Michalski, editors, </editor> <booktitle> Machine Learning, chapter 21, </booktitle> <pages> pages 611-638. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The genetic algorithm (GA) [10] is a search algorithm that emulates some basic features of natural evolution. The GA works well in searching spaces with the same difficulties faced by machine learning when searching the space of possible strategies in games of conflict <ref> [4] </ref>.
Reference: [5] <author> David B. Fogel. </author> <title> Evolving behaviours in the iterated prisoner's dilemma. </title> <journal> Evolutionary Computation, </journal> <volume> 1(1) </volume> <pages> 77-97, </pages> <year> 1993. </year>
Reference-contexts: In such a system, the environment (made up by the members of the evolving population) is constantly changing. He found that this dynamic environment produced strategies that performed very well against their population. Fogel <ref> [5] </ref> studied an almost identical system. <p> This desirable quality is "robustness". Lindgren provided a radical insight [12]. He simulated a system similar to those of Axelrod [2] and Fogel <ref> [5] </ref>. He showed that (under certain conditions) when a population (of strategies) plays IPD against its own members, the high-performing strategies who dominate the population for long periods of time, are suddenly wiped out and replaced. <p> From the point of view of machine learning, Lindgren [12] demonstrated that coevolution produced strategies that were not robust, i.e., the strategies did well against the local population, but when something new and innovative appeared (created by the the evolutionary process) they failed dismally. Fogel <ref> [5] </ref> also noticed that co-evolved strategies could do well against each other, but still have big flaws that could be exploited by the right opponent. <p> Lindgren [12] used a variation of Axelrod's approach, and Fogel <ref> [5] </ref> used evolutionary programming which is strictly speaking not a GA. However, the three approaches are broadly similar, and our results are relevant to all of evolutionary machine learning. Several excellent public-domain genetic algorithm packages are available. <p> This situation is described in the previous section, is shown in figure 7, and also occurred in Fogel's similar approach evolutionary learning to play iterated prisoner's dilemma <ref> [5] </ref>. One solution is to expand the round robin in the genetic algorithm; as well as the other members of the evolving population, each individual could also play against extra, static strategies. <p> The open question is still: how can co-evolution produce strategies that are robust performers against all possible opponents? That is, can a predator that evolved on one continent ever be sure of succeeding against a predator from another continent? 6 Discussion and Conclusion Lindgren [12] and Fogel <ref> [5] </ref> noticed that strategies produced by the co-evolution of closed populations may do well in that closed population, but can fail against outsiders. we have demonstrated a possible cause of this lack of robustness: the homogeneity of some high-performing populations leads to the atrophy of techniques (such as retaliation against unilateral
Reference: [6] <author> David E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Mutation is changing a character in a genotype (from 0 to 1 or vica versa) in a new individual. The mutation rate is usually low; 1 mutation per 1000 bits copied is normal <ref> [6, p 14] </ref>. Crossover is the dominant mechanism of genetic rearrangement for both real organisms and genetic algorithms [11].
Reference: [7] <author> John J. Grefenstette. </author> <title> Incorporating problem specific knowledge into genetic algorithms. </title> <editor> In Lawrence Davis, editor, </editor> <title> Genetic Algorithms and Simulated Annealing, </title> <booktitle> chapter 4, </booktitle> <pages> pages 42-60. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: The best strategies are produced when about 10% of the initial population is tit-for-tat, and the rest random, as shown in figure 6. This indicates that genetic diversity produces better strategies, outweighing the effect of the environment matching the task. That genetic diversity aids performance agrees with Grefenstette <ref> [7] </ref>.
Reference: [8] <author> John J. Grefenstette. </author> <title> Strategy acquisition with genetic algorithms. </title> <editor> In Lawrence Davis, editor, </editor> <booktitle> Handbook of Genetic Algorithms, chapter 14, </booktitle> <pages> pages 186-201. </pages> <publisher> Von Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Against the exploitable random genotypes, the populations which faced a random opponent in the round robin did better; as expected, the closer the learning situation is to the real thing, the better the results. A genetic algorithm will specialise to fit the problem presented <ref> [8] </ref>. If we include (say) tit-for-tat in the round robin, we expect the strategy produced to do better against tit-for-tat; table 1 agrees. This result also agrees with Grefenstette [9], where training under variable conditions created rule sets that performed well under variable conditions.
Reference: [9] <author> John J. Grefenstette, Connie L. Ramsey, and Alan C. Schultz. </author> <title> Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 355-381, </pages> <year> 1990. </year>
Reference-contexts: A genetic algorithm will specialise to fit the problem presented [8]. If we include (say) tit-for-tat in the round robin, we expect the strategy produced to do better against tit-for-tat; table 1 agrees. This result also agrees with Grefenstette <ref> [9] </ref>, where training under variable conditions created rule sets that performed well under variable conditions. As a method of overcoming the robustness problem in co-evolutionary machine learning, adding static strategies seems to work.
Reference: [10] <author> John H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> MIT Press, </publisher> <address> second edition, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Evolution works. In nature, evolution produces increasingly fit organisms in uncertain environments. Evolution is a proven form of adaptation to deal with a dynamic and complex environment. Emulating evolution shows great promise for machine learning. The genetic algorithm (GA) <ref> [10] </ref> is a search algorithm that emulates some basic features of natural evolution. The GA works well in searching spaces with the same difficulties faced by machine learning when searching the space of possible strategies in games of conflict [4]. <p> In our paper, this drama occurs between the players of iterated prisoner's dilemma. This sheds a light on the robustness problem in strategies produced by co-evolution, and on evolution in nature. 2 Genetic algorithms 2.1 Overview of the Genetic Algorithm A genetic algorithm (GA) <ref> [10] </ref> maintains a population of sample points from the search space. It represents a point by a string of (usually binary) characters, known as a genotype.
Reference: [11] <author> John H. Holland. </author> <title> Genetic algorithms. </title> <journal> Scientific American, </journal> <volume> 267(4) </volume> <pages> 44-50, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The mutation rate is usually low; 1 mutation per 1000 bits copied is normal [6, p 14]. Crossover is the dominant mechanism of genetic rearrangement for both real organisms and genetic algorithms <ref> [11] </ref>. The simplest implementation of crossover is to pick two genotypes (usually strings of characters), randomly choose a common crossover point, and cut and paste to create two new genotypes, as in figure 1.
Reference: [12] <author> Kristian Lindgren. </author> <title> Evolutionary phenomena in simple dynamics. </title> <editor> In Christopher G. Lang-ton, Charles Taylor, J. Doyne Farmer, and Steen Rasmussen, editors, </editor> <booktitle> Artificial Life 2, volume 10 of Santa Fe Institute Studies in the Sciences of Complexity, </booktitle> <pages> pages 295-312. </pages> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: This desirable quality is "robustness". Lindgren provided a radical insight <ref> [12] </ref>. He simulated a system similar to those of Axelrod [2] and Fogel [5]. He showed that (under certain conditions) when a population (of strategies) plays IPD against its own members, the high-performing strategies who dominate the population for long periods of time, are suddenly wiped out and replaced. <p> of Lindgren's simulation bear an uncanny resemblance to the "punctuated equilibria" of natural evolution [3]. "In particular, the large extinctions that appear in these simulations should be studied in more detail, since these collapses are triggered by the dynamical system itself and do not need external catastrophes for their explanation." <ref> [12, p 310] </ref>. From the point of view of machine learning, Lindgren [12] demonstrated that coevolution produced strategies that were not robust, i.e., the strategies did well against the local population, but when something new and innovative appeared (created by the the evolutionary process) they failed dismally. <p> From the point of view of machine learning, Lindgren <ref> [12] </ref> demonstrated that coevolution produced strategies that were not robust, i.e., the strategies did well against the local population, but when something new and innovative appeared (created by the the evolutionary process) they failed dismally. <p> Why did they seemingly-invincible strategies fail? How can we prevent this disas-ter happening to strategies produced by evolutionary machine learning? This paper answers these questions. 1.2 The Answers We study a system similar to Axelrod's [2], and find a reason for Lindgren's catastrophic collapses <ref> [12] </ref>. It turns out that when a near-expert strategy dominates a population, the homogeneity in their behaviour (in the case of IPD, they all cooperate) means certain features atrophy. <p> Lindgren <ref> [12] </ref> used a variation of Axelrod's approach, and Fogel [5] used evolutionary programming which is strictly speaking not a GA. However, the three approaches are broadly similar, and our results are relevant to all of evolutionary machine learning. Several excellent public-domain genetic algorithm packages are available. <p> In figure 8, put "horn size" on the y-axis and a similar drama occurs. Another "glitch" is shown in figure 9. Lindgren <ref> [12] </ref>, in a similar simulation, demonstrated that these events can happen repeatedly if the simulation runs for long enough. We want to avoid disastrously nave situations like this. <p> The open question is still: how can co-evolution produce strategies that are robust performers against all possible opponents? That is, can a predator that evolved on one continent ever be sure of succeeding against a predator from another continent? 6 Discussion and Conclusion Lindgren <ref> [12] </ref> and Fogel [5] noticed that strategies produced by the co-evolution of closed populations may do well in that closed population, but can fail against outsiders. we have demonstrated a possible cause of this lack of robustness: the homogeneity of some high-performing populations leads to the atrophy of techniques (such as
Reference: [13] <author> Alan C. Schultz and John J. Grefenstette. </author> <title> Improving tactical plans with genetic algorithms. </title> <booktitle> In Proceedings of IEEE Conference on Tools for Artificial Intelligence, </booktitle> <pages> pages 328-334. </pages> <publisher> IEEE Press, </publisher> <month> November </month> <year> 1990. </year>
Reference-contexts: appear then this diversity can continue to exist. 4.3 Experiment on seeding the initial population One improvement to a round-robin genetic algorithm (i.e., a GA where the fitness of an individual is found from how it competes with the other individuals of the evolving population) is being investigated by Grefenstette <ref> [13] </ref>: seed the initial population with strategies known to be good. We will find the best initial population that consists of a mix of a random initial population and a known effective strategy, tit-for-tat. Each run used a GA population of 100 individuals.
Reference: [14] <author> Xin Yao. </author> <title> An empirical study of genetic operators in genetic algorithms. </title> <journal> Microprocessing and Microprogramming, </journal> <volume> 38 </volume> <pages> 707-714, </pages> <year> 1993. </year>
Reference-contexts: Selection: The better performers of the population are selected for reproduction, i.e., the worse performers are erased. Reproduction: The genetic material of the better performers is made into a new population using various genetic operators, including crossover (emulating sexual reproduction), mutation, and numerous variations <ref> [14] </ref>. Mutation is changing a character in a genotype (from 0 to 1 or vica versa) in a new individual. The mutation rate is usually low; 1 mutation per 1000 bits copied is normal [6, p 14].
References-found: 14

