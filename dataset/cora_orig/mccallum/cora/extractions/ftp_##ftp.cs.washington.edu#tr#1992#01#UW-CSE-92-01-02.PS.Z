URL: ftp://ftp.cs.washington.edu/tr/1992/01/UW-CSE-92-01-02.PS.Z
Refering-URL: http://www.cs.washington.edu/research/arch/perf-mem-con.html
Root-URL: 
Title: A Performance Study of Memory Consistency Models  
Author: Richard N. Zucker and Jean-Loup Baer 
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Technical Report No. 92-01-02 January 1992 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Implementing sequential consistency in cache-based systems. </title> <booktitle> In 1990 International Conference on Parallel Processing, </booktitle> <pages> pages I-47-50, </pages> <year> 1990. </year>
Reference-contexts: all previous global data accesses have been performed and 3. no access to global data is issued by a processor before a previous access to a synchronizing variable has been performed. 2 For our purposes, strongly ordered and sequentially consistent accesses can be considered as synonymous (distinctions are examined in <ref> [1] </ref>). The formal definition of "performed" is given in [10]. Basically a store is performed when the value stored by the processor executing the instruction can be seen by all other processors.
Reference: [2] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering Anew definition. </title> <booktitle> In 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <year> 1990. </year>
Reference-contexts: Section 2 defines the various memory models, gives the rules that these models impose on programmers, and indicates how more freedom is allowed in the sequencing of memory accesses as the models become more relaxed (for a more in depth explanation of memory models, see <ref> [2] </ref>). The simulated architecture, the choices made in the implementations of the various models, and the selected benchmarks are described in section 3. In section 4 we present the results of our study. <p> Under this weak ordering model, writing parallel programs, already a difficult task, becomes even more difficult. However, it has been shown that weakly ordered hardware can appear sequentially consistent to programs with minimal constraints <ref> [2] </ref>, only two of which are significant: programs must not contain data races and all synchronization operations must be visible to the hardware. The first condition is not much of a restriction since a data race is usually an error in a sequentially consistent program (chaotic relaxation is an exception). <p> Also, the second and third rules of weak ordering say that whenever a synchronization operation is performed, all references to shared data must be performed and no new references to shared data may be issued until the synchronization has completed (this explains why in <ref> [2] </ref> it was said that programs on weakly ordered machines must use synchronization operations that are visible to the hardware). Except for these restrictions, the accesses may be performed in any order. Thus an order that maximizes system performance may be used.
Reference: [3] <author> Jean-Loup Baer and Richard N. Zucker. </author> <title> On synchronization patterns of parallel programs. </title> <booktitle> In 1991 International Conference on Parallel Processing, </booktitle> <pages> pages II-60-67, </pages> <year> 1991. </year>
Reference-contexts: With 8 or 16 byte lines the run-times are roughly the same. However, the systems with 64 byte line caches are the slowest in spite of the higher hit rates. At first we found the low write hit ratios we observed curious. In our previous study <ref> [3] </ref> we found that Qsort had a write hit ratio of close to 100%. This is because before values are swapped, they must be compared. So, they are always read before being written, and hence, are already in the cache. <p> These results are consistent with those obtained with a four cycle delay and do not bring any further insight. 6 Related Work and Comparisons 6.1 Methodology Simulation performance studies of relaxed consistency models have been done for two different architectures: a shared-bus multiprocessor <ref> [3] </ref> and a mesh connected multiprocessor based on DASH, an experimental system being built at Stanford [14, 23]. Our previous study [3] should not be compared to this one due to the modest number of processors (nine to twelve), the low memory latency, and the limitations of its trace driven nature. <p> not bring any further insight. 6 Related Work and Comparisons 6.1 Methodology Simulation performance studies of relaxed consistency models have been done for two different architectures: a shared-bus multiprocessor <ref> [3] </ref> and a mesh connected multiprocessor based on DASH, an experimental system being built at Stanford [14, 23]. Our previous study [3] should not be compared to this one due to the modest number of processors (nine to twelve), the low memory latency, and the limitations of its trace driven nature.
Reference: [4] <author> Eugene D. Brooks III. PCP: </author> <title> A Parallel Extension of C that is 99% Fat Free. </title> <type> Technical Report UCRL-99673, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1988. </year>
Reference-contexts: references as in WO1), and the ability to keep track of the pending release operation since the processor has continued executing past the location in the instruction stream where the release was encountered. 3.3 Benchmarks The benchmark programs used to compare the various memory models are all written using PCP <ref> [4] </ref>, a simple parallel extension to C. They were compiled to the simulator's machine code and linked using Cerberus's compiler. Due to compiler limitations there is no static allocation of private data. Any variable allocated globally is a shared variable and resides in a shared memory module.
Reference: [5] <author> Eugene D. Brooks III, Tim S. Axelrod, and Gregory A. Darmohray. </author> <title> The Cerberus multiprocessor simulator. </title> <editor> In G. Rodrigue, editor, </editor> <booktitle> Parallel Processing for Scientific Computing, </booktitle> <pages> pages 384-390. </pages> <publisher> SIAM, </publisher> <year> 1989. </year>
Reference-contexts: Processors and global memory are connected via two identical Omega networks, one for the processor requests to memory and one for memory's responses. Cache coherence is enforced by a full directory scheme [6]. We used the Cerberus instruction-level simulator <ref> [5] </ref> for our simulation studies. The processor it simulates is a RISC processor similar to the Ridge 32 [24]. The caches are two-way set associative and use a write-back write-allocate policy; we experimented with a range of cache and line sizes. The caches are only for shared data.
Reference: [6] <author> Lucien M. Censier and Paul Feautrier. </author> <title> A new solution to coherence problems in multicache systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> December </month> <year> 1978. </year>
Reference-contexts: Processors and global memory are connected via two identical Omega networks, one for the processor requests to memory and one for memory's responses. Cache coherence is enforced by a full directory scheme <ref> [6] </ref>. We used the Cerberus instruction-level simulator [5] for our simulation studies. The processor it simulates is a RISC processor similar to the Ridge 32 [24]. The caches are two-way set associative and use a write-back write-allocate policy; we experimented with a range of cache and line sizes.
Reference: [7] <author> Tien-Fu Chen and Jean-Loup Baer. </author> <title> Reducing memory latency via non-blocking and preloading caches, </title> <booktitle> 1992. submitted to Fifth International Conference on Architectural Support for Programming Languages and Operating Systems. </booktitle>
Reference-contexts: We see that there is a noticeable difference (up to 8%) in performance. Normally the manual scheduling of the code will be impractical. However, compiler scheduling can perform some of these optimizations <ref> [7] </ref>. 5.3 Two Cycle Load and Branch Delays Due to limitations of our simulator, our initial studies used a load and branch delay of four cycles. It can be argued that this is overly long (although superpipelined machines may have long delays as well).
Reference: [8] <author> Gregory A. Darmohray. </author> <title> Gaussian techniques on shared-memory multiprocessors. </title> <type> Master's thesis, </type> <institution> University of California, Davis, </institution> <month> April </month> <year> 1988. </year>
Reference-contexts: The exact number of memory references can change based upon the consistency model, and this can impact the hit rates. 7 Gauss Gauss <ref> [8] </ref> performs the gaussian elimination of a 250 fi 250 matrix. This program exhibits a large amount of spatial locality, as can be seen from the 16K cache data in Table 2 and Figure 2.
Reference: [9] <author> Edgar W. Dijkstra. </author> <title> Cooperating Sequential Processes. </title> <editor> In F. Genuys, editor, </editor> <booktitle> Programming Languages. </booktitle> <publisher> Academic Press, </publisher> <year> 1968. </year>
Reference-contexts: If this order is not maintained, then communication between processes, that is, synchronization, using loads and stores (e.g. Dekker's Algorithm <ref> [9] </ref>) may not work correctly. Until recently sequential consistency had been the usual model of memory access for multiprocessors. However, SC's strict ordering rules can lead to performance inefficiencies. For example, there is (very) limited use of write buffers and the execution of memory accesses must be in program order.
Reference: [10] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <year> 1986. </year>
Reference-contexts: The first proposed relaxed memory model was weak ordering, which is defined in <ref> [10] </ref> as: 1. accesses to global synchronizing variables are strongly ordered and 2. no access to a synchronizing variable is issued in a processor before all previous global data accesses have been performed and 3. no access to global data is issued by a processor before a previous access to a <p> The formal definition of "performed" is given in <ref> [10] </ref>. Basically a store is performed when the value stored by the processor executing the instruction can be seen by all other processors. A load is performed when the value to be returned by the load has been set and cannot be changed.
Reference: [11] <author> Susan J. Eggers and Randy H. Katz. </author> <title> The effect of sharing on the cache and bus performance of parallel programs. </title> <booktitle> In Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 257-270, </pages> <year> 1989. </year>
Reference-contexts: Therefore the relative cost of a cache miss is becoming more important, since processors are left idle for a greater number of cycles. This performance loss is even more noticeable in multiprocessors because of the higher miss rates <ref> [11] </ref> and because memory access times are increased by the latency of and contention for the interconnect that links processors and memory modules. We can alleviate the high cost of cache misses by reducing the frequency with which the processor must stall on a cache miss.
Reference: [12] <author> Kourosh Gharachorloo. </author> <type> personal communication. </type>
Reference-contexts: There is a write buffer between the two caches and if a relaxed consistency model is being implemented, loads can bypass the stores that are in the write buffer. The first level cache has a one word line size and a fetch size on read misses of four words <ref> [12] </ref>. Cache coherence is enforced by a hardware full directory scheme with the directories being associated with the home memories.
Reference: [13] <author> Kourosh Gharachorloo, Sarita Adve, Anoop Gupta, John Hennessy, and Mark Hill. </author> <title> Programming for different memory consistency models. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(2), </volume> <month> June </month> <year> 1992. </year>
Reference-contexts: There can be an outstanding release operation when an acquire is issued (assuming no data dependency). Gharachorloo et al. also showed that under program restrictions similar to those mentioned above for weak ordering, a release consistent system behaves in a sequentially consistent manner <ref> [13] </ref>. 3 3 Methodology M C M C P P t t t - - fl fl C C fl fl C C ? ?? ? Switch Switch t t t ? H H H H P P P P P P P P P P P P P P P
Reference: [14] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 245-257, </pages> <year> 1991. </year>
Reference-contexts: In section 5 various architectural variations are considered and results presented as well as compile-time improvements. Section 6 compares our results to a previous study of relaxed consistency models based on a simulation of DASH <ref> [14] </ref>. As will be seen, our results are qualitatively in agreement with those of that previous study. The quantitative differences can be explained by the architectural features being simulated. <p> four cycle delay and do not bring any further insight. 6 Related Work and Comparisons 6.1 Methodology Simulation performance studies of relaxed consistency models have been done for two different architectures: a shared-bus multiprocessor [3] and a mesh connected multiprocessor based on DASH, an experimental system being built at Stanford <ref> [14, 23] </ref>. Our previous study [3] should not be compared to this one due to the modest number of processors (nine to twelve), the low memory latency, and the limitations of its trace driven nature. <p> The memory latencies range from 20 to 80 cycles in the case of no contention on the interconnect. Gharachorloo et al. <ref> [14] </ref> studied a number of consistency models including: two forms of sequentially consistent systems, a weakly ordered system, and a release consistent system. The major difference between the systems involved the selection of events that led to processor stalls.
Reference: [15] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Two techniques to enhance the performance of memory consistency models. </title> <booktitle> In 1991 International Conference on Parallel Processing, </booktitle> <pages> pages I-355-364, </pages> <year> 1991. </year>
Reference-contexts: At the same time the source register may be overwritten by a register-register operation. Sequentially Consistent 2 - SC2 A more aggressive version of SC that does not change the programmer's view of the system was also simulated. Non-binding prefetches on stalls, as suggested in <ref> [15] </ref>, were added to SC1. The processor still stalls when there is an outstanding memory reference and another memory reference is about to be made. But a request for this second access is nonetheless sent to the cache.
Reference: [16] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <year> 1990. </year> <month> 21 </month>
Reference-contexts: This relaxation of access order lets reads take precedence over writes, memory references overlap, data be prefetched, instructions issue or complete out of order, and cache coherence invalidation signals be delayed until after the write to a line in the cache has been performed. 2.3 Release Consistency In <ref> [16] </ref>, Gharachorloo et al. proposed the release consistency model. As in weak ordering, the release consistency model has minimal restrictions on the ordering of the performing of normal accesses. There are some restrictions on the ordering of synchronization accesses.
Reference: [17] <author> Anoop Gupta, John Hennessy, Kourosh Gharachorloo, Todd Mowry, and Wolf-Dietrich Weber. </author> <title> Com--parative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <year> 1991. </year>
Reference-contexts: Relaxed consistency should be implemented in conjunction with other memory latency reducing techniques such as more sophisticated prefetching, speculative execution, fast context-switching or multithreaded architectures <ref> [17] </ref>. The effect of relaxed consistency on the programmer and the compiler needs to be investigated. We have shown, through an example, how the optimal code reordering depends upon the model of consistency implemented.
Reference: [18] <author> Simon Kahan and Larry Ruzzo. </author> <title> Parallel quicksand: Sorting on the sequent. </title> <type> Technical Report 91-01-01, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: However, when 64K caches are used the hit ratios are uniformly high and the run-times vary very little with the line sizes. Clearly the data set of each processor fits in a 64K cache, but not in a 16K one. Qsort Qsort <ref> [18] </ref> executes a parallel quicksort of 500,000 integers. It is dynamically scheduled, unlike the other benchmarks which are statically scheduled. Units of work are pushed onto and popped off of a shared stack and allocated to processors on a FCFS basis. <p> Since any change in the architecture influences the relative rate of execution of each processor, the order in which tasks are pushed onto and popped off the stack can change and thereby affect the way the work is partitioned as well as the size of the partitions <ref> [18] </ref>. In one case, when we changed the implementation from WO1 to WO2, we found that the number of synchronization operations increased by a third, thereby demonstrating the effect of dynamic scheduling. This natural variability prevents us from reaching general conclusions from a single run of the benchmark. <p> During the partition phase, which is done in parallel, a processor references every nth element. The locations are not strip-mined so that each processor references all the locations in a given cache line (for the shared-bus system for which the program was written <ref> [18] </ref>, the overhead of doing this was found to remove any benefit it would have otherwise generated. However, this may be different in the case of a higher latency system such as the one we are simulating).
Reference: [19] <author> David Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 81-87, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: However, we allowed one optimization, namely non-blocking loads. Therefore, the processor only stalls if it tries to read from a register that is the destination of an uncompleted load or it attempts another memory access. Since there can be only one outstanding reference at a time, lockup-free caches <ref> [19] </ref> are not necessary; however, some form of register interlock or scoreboarding is needed for the correctness of register-register operations. <p> With WO1 several memory references can be outstanding at a time and hence the cache must be lockup-free. Information on each outstanding reference must be maintained in a miss information/status holding register (MSHR) <ref> [19] </ref> (we simulate five MSHR's). The MSHR's will cause the processor to stall when it makes a reference which must be delayed due to data dependency requirements.
Reference: [20] <author> Monica Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <year> 1991. </year>
Reference-contexts: The reference to (i + 1,j + 1) will miss every time and only that reference will miss on any given relaxation (except at the beginning of a new row or when there is cross or self-interference <ref> [20] </ref>). <p> Any differences are due to self or cross-interference <ref> [20] </ref> or coherence effects due to sharing at the edges of the sub-matrices (although given the right matrix size relative to the cache size, the self-interference of row i 1 and row i could be very significant).
Reference: [21] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Under such models, the hardware is not required to stall every time there is an outstanding memory reference. However, for correct executions parallel programs must not exhibit data races and primitive synchronization operations must be visible to the hardware. In return the system appears sequentially consistent <ref> [21] </ref> and is generally faster than a truly sequentially consistent machine. In this paper we present the results of simulation studies of shared-memory multiprocessors that indicate the relative performance benefits of various implementations of systems that are sequentially consistent, weakly ordered, and release consistent. <p> On a uniprocessor, the implicit model is that of sequential consistency (SC), which is so intuitive that programmers who implicitly follow it do not even realize that they are adhering to a model's rules. SC is defined as <ref> [21] </ref>: [A system is sequentially consistent if] the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.
Reference: [22] <author> Dan Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <year> 1990. </year>
Reference-contexts: However, the Stanford study, which used instruction-level simulation, is similar in scope to our current study but their architectural framework and memory model implementations are significantly different. The Stanford study is based on a variant of the DASH <ref> [22] </ref> architecture with 16 processors connected 17 Cache Delay 8 byte lines 16 byte lines 64 byte lines Size Absolute Relative Absolute Relative Absolute Relative 16K Two cycles 7,938 46.8 3,528 22.9 855 6.1 Four cycles 7,278 36.2 3,705 20.2 1,075 6.2 64K Two cycles 308 2.4 239 1.9 157 1.2
Reference: [23] <author> Todd Mowry and Anoop Gupta. </author> <title> Tolerating latency through software-controlled prefetch in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: four cycle delay and do not bring any further insight. 6 Related Work and Comparisons 6.1 Methodology Simulation performance studies of relaxed consistency models have been done for two different architectures: a shared-bus multiprocessor [3] and a mesh connected multiprocessor based on DASH, an experimental system being built at Stanford <ref> [14, 23] </ref>. Our previous study [3] should not be compared to this one due to the modest number of processors (nine to twelve), the low memory latency, and the limitations of its trace driven nature.
Reference: [24] <institution> Ridge Computers. </institution> <note> Ridge 32 User's Guide. 22 </note>
Reference-contexts: Cache coherence is enforced by a full directory scheme [6]. We used the Cerberus instruction-level simulator [5] for our simulation studies. The processor it simulates is a RISC processor similar to the Ridge 32 <ref> [24] </ref>. The caches are two-way set associative and use a write-back write-allocate policy; we experimented with a range of cache and line sizes. The caches are only for shared data. We assume that there are no instruction cache misses and that the private data can be accessed from local memory.
References-found: 24

