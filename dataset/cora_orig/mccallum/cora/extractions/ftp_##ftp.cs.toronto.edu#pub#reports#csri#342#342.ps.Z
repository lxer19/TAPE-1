URL: ftp://ftp.cs.toronto.edu/pub/reports/csri/342/342.ps.Z
Refering-URL: http://www.cs.toronto.edu/~stergios/publications.html
Root-URL: http://www.cs.toronto.edu
Title: Parallel Application Scheduling on Networks of Workstations  
Author: by Stergios Anastasiadis 
Degree: A thesis submitted in conformity with the requirements for the Degree of Master of Science  
Note: c Copyright by Stergios Anastasiadis 1996  
Address: Toronto  
Affiliation: Graduate Department of Computer Science University of  
Abstract-found: 0
Intro-found: 1
Reference: <author> Agrawal, R. and A. K. </author> <title> Ezzat (1985, November). Processor Sharing in NEST: A Network of Computer Workstations. </title> <booktitle> In 1st Int'l Conference on Computer Workstations, </booktitle> <pages> pp. 198-208. </pages>
Reference: <author> Anderson, T. E., D. E. Culler, D. A. Patterson, </author> <title> and the NOW team (1995, February). A Case for NOW (Networks of Workstations). </title> <booktitle> IEEE Micro 15 (2), </booktitle> <pages> 54-64. </pages>
Reference-contexts: In Chapter 7, the conclusions of our work are summarized. 4 Chapter 2 Literature Review Today, massively parallel processors (MPPs) tend to be constructed as large collections of workstation-class nodes connected by dedicated low latency networks and exploiting commodity technologies <ref> (Anderson et al. 1995) </ref>. However, they usually lag technologically one or two years behind workstations with comparable parts. An additional drawback is the increased engineering effort required for customizing their software and hardware components, and the relatively small volume of sales to cover the incurred cost.
Reference: <author> Arpaci, R. H., A. C. Dusseau, A. M. Vahdat, L. T. Liu, T. E. Anderson, and D. </author> <title> A. </title>
Reference: <author> Patterson (1994). </author> <title> The Interaction of Parallel and Sequential Workloads on a Network of Workstations. </title> <type> Technical Report CS-94-838, </type> <institution> Computer Science Division, University of California, Berkeley. </institution>
Reference: <author> Black, D. L. </author> <year> (1990, </year> <month> May). </month> <title> Scheduling Support for Concurrency and Parallelism in the Mach Operating System. </title> <booktitle> Computer 23 (15), </booktitle> <pages> 35-43. </pages>
Reference: <author> Carriero, N., E. Freeman, D. Gelenter, and D. </author> <month> Kaminsky </month> <year> (1995, </year> <month> January). </month> <title> Adaptive Parallelism and Piranha. </title> <booktitle> Computer 28 (1), </booktitle> <pages> 40-49. </pages>
Reference: <author> Chandra, R., S. Devine, B. Verghese, A. Gupta, and M. Rosenblum (1994, </author> <month> November). </month> <title> Scheduling and Page Migration for Multiprocessor Computer Servers. </title> <booktitle> In 6th Int'l Conference on Architectural Support for Programming Language and Operating Systems (ASPLOS VI), </booktitle> <address> San Jose, CA, </address> <pages> pp. 12-24. </pages>
Reference: <author> Chase, J. S., F. G. Amador, E. D. Lazowska, H. M. Levy, and R. J. </author> <title> Littlefield (1989, De-cember). The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In 12th ACM Symposium Operating Systems Principles, </booktitle> <pages> pp. 147-158. </pages>
Reference-contexts: An abstract data type, called rcmd (Remote Command Multiprogramming Data), is used for describing a process, or for the communication among and synchronization of multiple processes running on different hosts. Amber <ref> (Chase et al. 1989) </ref> was designed to explore the use of a distributed object model as a tool for structuring parallel programs on networks of shared-memory multiprocessor workstations.
Reference: <author> Chiang, S.-H., R. K. Mansharamani, and M. K. Vernon (1994, </author> <month> May). </month> <title> Use of Application Characteristics and Limited Preemption for Run-To-Completion Parallel Processor Scheduling Policies. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of 78 Computer Systems, </booktitle> <pages> pp. 33-44. </pages>
Reference-contexts: In particular, for good speedup 62 (WK1), the policies SDF-Max=2, 6, 32 saturate at load 90%. For poor speedup (WK3), even a load of 50% makes the mean response time infinite. The excellent performance reported for SDF by a previous study <ref> (Chiang et al. 1994) </ref> is illustrated there only for good speedup jobs (WK1) and load 70%.
Reference: <author> Crovella, M., P. Das, C. Dubnicki, T. LeBlanc, and E. </author> <title> Markatos (1991, December). Multiprogramming on Multiprocessors. </title> <booktitle> In 3rd IEEE Symposium Parallel and Distributed Processing, </booktitle> <pages> pp. 590-597. </pages>
Reference-contexts: First, it is reported that a combination of cache and cluster affinity along with automatic page migration can improve considerably the response time over a standard Unix Scheduler. Then Gang Scheduling is compared to the Process Control policy and Processor Sets, which is equivalent to Hardware Partitions <ref> (Crovella et al. 1991) </ref>. Gang Scheduling, because of its co-scheduling property, provides the illusion of an exclusive machine and allows the programmer/compiler to successfully perform data locality optimizations. The dynamic nature of the other two policies make such optimizations difficult.
Reference: <author> Devarakonda, M. V. and R. K. Iyer (1989, </author> <month> December). </month> <title> Predictability of Process Resource Usage: A Measurement-Based Study on Unix. </title> <journal> IEEE Transactions on Software Engineering 15 (12), </journal> <pages> 1579-1586. </pages>
Reference: <author> Dussa, K., B. Carlson, L. Dowdy, and K.-H. Park (1990, </author> <month> May). </month> <title> Dynamic Partitioning in a Transputer Environment. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 203-213. </pages>
Reference-contexts: With low reconfiguration overhead, dynamic partitioning leads to maximum throughput, while with high reconfiguration overhead, dynamic partitioning should be avoided. The conclusions are validated using real applications on a distributed-memory system <ref> (Dussa et al. 1990) </ref>. Processor Working Set (pws) is defined as the minimum number of processors that maximizes the speedup per unit of a cost function incorporating the number of processors and the associated speedup.
Reference: <author> Eager, D. L., J. Zahorjan, and E. D. Lazowska (1989, </author> <month> March). </month> <title> Speedup versus efficiency in parallel systems. </title> <journal> IEEE Transactions on Computers 38 (3), </journal> <pages> 408-433. </pages>
Reference-contexts: In Gang Scheduling, threads interacting at fine granularity should be scheduled together and grouped into a gang. Thus, Gang Scheduling allows guarantees about the performance to be given. 11 is a good initial rule of thumb <ref> (Eager et al. 1989) </ref>. Then is is shown that a more sophisticated rule would include the system load for applications with highly variable parallelism. Thus the appropriate allocation would be equal to one at very heavy load and to the total number of processors at very light load.
Reference: <author> Efe, K. and M. A. </author> <title> Schaar (1993). Performance of Co-Scheduling on a Network of Workstations. </title> <booktitle> In 13th Int'l Conference on Distributed Computing Systems, </booktitle> <pages> pp. 525-531. </pages>
Reference: <author> Feeley, M. J., B. N. Bershad, J. S. Chase, and H. M. Levy (1991). </author> <title> Dynamic Node Reconfiguration in a Parallel-Distributed Environment. </title> <booktitle> In 3rd ACM Symposium Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 114-121. </pages>
Reference-contexts: Later, it was extended to allow the execution of parallel-distributed programs that adapt to three types of node reconfiguration: shrinkage in size, changes in membership, or growth <ref> (Feeley et al. 1991) </ref>. At the system level, all these cases require support for program state transfer and communication forwarding. At the application level, the program must resolve the load imbalances that occur.
Reference: <author> Feitelson, D. G. </author> <year> (1994, </year> <month> October). </month> <title> A Survey of Scheduling in Multiprogrammed Parallel Systems. </title> <note> Research Report RC-19790 (87657), </note> <institution> IBM T. J. Watson Research Center. </institution>
Reference: <author> Feitelson, D. G. and B. Nitzberg (1995, </author> <month> April). </month> <title> Job Characteristics of a Production Parallel Scientific Workload on the NASA Ames iPSC/860. </title> <booktitle> In IPPS '95 Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <pages> pp. 215-227. </pages>
Reference-contexts: The third one contains the number of occurrences, as derived from a bar chart. The total demand is taken by multiplying the runtime by the number of allocated processors. The horizontal line separates the jobs into two separate classes. This decision follows from the observation <ref> (Feitelson and Nitzberg 1995) </ref> that jobs with 32, 64 and 128 nodes use more than 90% of the system resources (node-seconds), while their individual demands are almost one order of magnitude larger than those with sizes 1-16.
Reference: <author> Feitelson, D. G. and L. Rudolph (1992a, </author> <month> October). </month> <title> Coscheduling based on Run-Time Identification of Activity Working Sets. </title> <note> Research Report RC-18416 (80519), </note> <institution> IBM T. </institution>
Reference: <institution> J. Watson Research Center. </institution>
Reference: <author> Feitelson, D. G. and L. Rudolph (1992b, </author> <month> December). </month> <title> Gang Scheduling Performance Benefits for Fine-Grain Synchronization. </title> <journal> Journal of Parallel and Distributed Computing 16 (4), </journal> <pages> 306-318. </pages>
Reference: <author> Feitelson, D. G. and L. Rudolph (1995, </author> <month> April). </month> <title> Parallel Job Scheduling: Issues and Approaches. </title> <booktitle> In IPPS '95 Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <pages> 79 pp. 1-8. </pages>
Reference-contexts: The third one contains the number of occurrences, as derived from a bar chart. The total demand is taken by multiplying the runtime by the number of allocated processors. The horizontal line separates the jobs into two separate classes. This decision follows from the observation <ref> (Feitelson and Nitzberg 1995) </ref> that jobs with 32, 64 and 128 nodes use more than 90% of the system resources (node-seconds), while their individual demands are almost one order of magnitude larger than those with sizes 1-16.
Reference: <author> Ghosal, D., G. Serazzi, and S. K. Tripathi (1991, </author> <month> May). </month> <title> The Processor Working Set and Its Use in Scheduling Multiprocessor Systems. </title> <journal> IEEE Transactions on Software Engineering 17 (5), </journal> <pages> 443-453. </pages>
Reference-contexts: It is shown that a simple work conserving strategy that attempts to allocate the minimum of the number of free processors and the job pws is robust and offers good throughput-response time characteristics over a wide range of arrival rates <ref> (Ghosal et al. 1991) </ref>. Setia and Tripathi (1993) introduce the Adaptive Static Partitioning (ASP) policy. If a job finds idle processors on arrival, it is allocated the minimum of the idle number of processors and its maximum parallelism.
Reference: <author> Graham, R. L., E. L. Lawler, J. K. Lenstra, and A. H. G. </author> <title> Rinnooy Kan (1979). Optimization and Approximation in Deterministic Sequencing and Scheduling: a Survey. </title> <journal> Annals of Discrete Mathematics 5, </journal> <pages> 287-326. </pages>
Reference: <author> Gupta, A., A. Tucker, and S. Urushibara (1991, </author> <month> May). </month> <title> The Impact of Operating System Scheduling Policies and Synchronization Methods on the Performance of Parallel Application. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 120-132. </pages>
Reference: <author> Hagmann, R. </author> <year> (1986, </year> <month> May). </month> <title> Process Server: Sharing Processing Power in a Workstation Environment. </title> <booktitle> In 6th IEEE Distributed Computing Conference, </booktitle> <pages> pp. 260-267. </pages>
Reference: <author> Ibaraki, T. and N. </author> <title> Katoh (1988). Resource Allocation Problems: Algorithmic Approaches. </title> <booktitle> Series in the Foundations of Computing. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: of J) dispatch J else move J in the waiting queue according to the new difference T (allocation+1)-T (allocation) od dispatch all jobs in waiting queue with allocation&gt;0 y ! 2 It can be proven that when the convexity assumption is satisfied, the problem can be solved in polynomial time <ref> (Ibaraki and Katoh 1988) </ref>. Another solution is described by Sevcik (1994). Here we will use a simpler algorithm which finds the optimal solution in exponential runtime of the input size.
Reference: <author> Jones, M. B. </author> <year> (1993, </year> <month> December). </month> <title> Interposition Agents: Transparently Interposing User Code at the System Interface. </title> <booktitle> In 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 80-93. </pages>
Reference: <author> Krueger, P. and R. </author> <month> Chawla </month> <year> (1991, </year> <month> May). </month> <title> The Stealth Distributed Scheduler. </title> <booktitle> In 11th Int'l Conference Distributed Computing Systems, </booktitle> <pages> pp. 336-343. </pages>
Reference: <author> Law, A. M. and W. D. </author> <title> Kelton (1991). Simulation Modeling and Analysis (Second ed.). </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Thus, we used a C implementation of the algorithm from Marse and Roberts (1983). It is considered to be a well-tested and acceptable generator that should work properly and consistently on virtually any computer apt to be used for serious simulation <ref> (Law and Kelton 1991) </ref>. 60 Chapter 6 Experimental Results In this chapter, details of our experiments are presented and explanations are given for the comparative performance among the policies under study. Most of the experimental configurations are examined using the four different workloads defined in Chapter 5.
Reference: <author> Leutenegger, S. T. and X.-H. </author> <title> Sun (1993, April). Distributed Computing Feasibility in a Non-Dedicated Homogeneous Distributed System. </title> <booktitle> In Int'l Conference Supercomputing, </booktitle> <pages> pp. 143-152. </pages>
Reference: <author> Leutenegger, S. T. and M. K. Vernon (1990, </author> <month> May). </month> <title> The Performance of Multiprogrammed Multiprocessor Scheduling Policies. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 226-236. </pages>
Reference: <author> Leuze, M. R., L. W. Dowdy, and K.-H. Park (1989, </author> <month> September). </month> <title> Multiprogramming a Distributed-Memory Multiprocessor. </title> <journal> Concurrency: Practice and Experience 1 (1), </journal> <pages> 80 19-33. </pages>
Reference-contexts: The nodes are interconnected by a network. Different applications can run on separate partitions without degradation in performance due to contention in the communication medium. Experiments conducted on a 32-node iPSC/2 justify this assumption <ref> (Leuze et al. 1989) </ref>. Also, independence between different running jobs can be considered reasonable for high bandwidth communication media, like those based on ATM.
Reference: <author> Litzkow, M. J., M. Livny, and M. W. </author> <month> Mutka </month> <year> (1988, </year> <month> June). </month> <title> Condor A Hunter of Idle Workstations. </title> <booktitle> In 8th IEEE Distributed Computing Conference, </booktitle> <pages> pp. 104-111. </pages>
Reference: <author> Majumdar, S., D. L. Eager, and R. B. Bunt (1988, </author> <month> May). </month> <title> Scheduling in Multiprogrammed Parallel Systems. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 104-113. </pages>
Reference-contexts: It is compared to other policies with respect to mean response time. It is concluded that the Smallest Number of Processes First <ref> (Majumdar et al. 1988) </ref> does not perform well for workloads with coefficient of variation of service demand larger than 1 or 2. Also, Round Robin policies have better performance than the Co-scheduling policies.
Reference: <author> Majumdar, S., D. L. Eager, and R. B. Bunt (1991, </author> <month> October). </month> <title> Characterization of programs for scheduling in multiprogrammed parallel systems. </title> <booktitle> Performance Evaluation 13 (2), </booktitle> <pages> 109-130. </pages>
Reference-contexts: Tighter bounds on the speedup function can be obtained when the execution time is known at a number of processors equal to the average parallelism of the application <ref> (Majumdar et al. 1991) </ref>. Analytic models are used to show the importance of both the application characteristics and the load in determining the partition size of the jobs. The execution time-efficiency knee is found to yield almost optimal performance for a wide range of applications and loads.
Reference: <author> Marse, K. and S. D. </author> <title> Roberts (1983). Implementing a Portable FORTRAN Uniform (0,1) Generator. </title> <booktitle> Simulation 41, </booktitle> <pages> 135-139. </pages>
Reference: <author> McCann, C., R. Vaswani, and J. Zahorjan (1993, </author> <month> May). </month> <title> A Dynamic Processor Allocation Policy for Multiprogrammed Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems 11 (2), </journal> <pages> 146-178. </pages>
Reference-contexts: Special provisions are taken so that no application is given more processors than it can use. Processors are forcibly preempted when they have been reallocated to a different job, and immediately the application readjusts the number of running processes <ref> (McCann et al. 1993) </ref>. A description of the policy as used in our experiments is given in figure 4.7. The policy requires that the applications should be able to adapt to a change in the allocated number of processors during execution.
Reference: <author> McCann, C. and J. </author> <title> Zahorjan (1989). Processor Scheduling in Shared Memory Multiprocessors. </title> <type> Technical Report 89-09-17, </type> <institution> Computer Science Department, University of Washington at Seattle. </institution>
Reference-contexts: The detailed structure description <ref> (for example, McCann and Zahorjan 1989) </ref> with all the associated difficulties is no longer necessary. The function allows generation of job pools with all the predefined properties, and gives precise control of the workload definition.
Reference: <author> McCann, C. and J. Zahorjan (1994, </author> <month> May). </month> <title> Processor Allocation Policies for Message-Passing Parallel Computers. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 19-32. </pages>
Reference: <author> McCann, C. and J. </author> <title> Zahorjan (1995). Scheduling Memory Constrained Jobs on Distributed Memory Parallel Computers. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 208-219. </pages>
Reference: <author> Mutka, M. W. and M. </author> <title> Livny (1987). Scheduling Remote Processing Capacity In a Workstation-Processor Bank Network. </title> <booktitle> In 7th Int'l Conference on Distributed Computing Systems, </booktitle> <pages> pp. 2-9. </pages>
Reference: <author> Mutka, M. W. and M. </author> <title> Livny (1991). The Available Capacity of a Privately Owned Workstation Environment. </title> <booktitle> Performance Evaluation 12 (4), </booktitle> <pages> 269-284. </pages>
Reference-contexts: At the University of Wisconsin, the Condor scheduling system was developed in order to manage efficiently the idle computing capacity of the local workstation network (Mutka and Livny 1987; Litzkow et al. 1988). In a study of the previously wasted processing resources <ref> (Mutka and Livny 1991) </ref>, a workstation was considered non-available if the average processor utilization was larger than one-fourth of one percent at any time during a previous predefined period.
Reference: <author> Naik, V. K., S. K. Setia, and M. S. </author> <title> Squillante (1993, November). Performance Analysis of Job Scheduling Policies in Parallel Supercomputing Environments. </title> <booktitle> In Int'l Conference 81 Supercomputing, </booktitle> <pages> pp. 824-833. </pages>
Reference-contexts: The MP policy is further investigated by Setia et al. (1994). Trends in supercomputing centers suggest environments in which jobs with very different resource requirements arrive at various intervals to systems with large numbers of processors <ref> (Naik et al. 1993) </ref>. In Fixed Partitioning, processors are divided at system configuration time into a fixed number of disjoint sets. The Fixed Partitioning with Job Priorities is similar with the additional requirement that each partition is designated as belonging to a certain class of jobs. <p> Fixed Partitioning with Job Priorities. It is based on Fixed Partitioning with the additional requirement that each partition is designated to a certain class of jobs. Large jobs cannot acquire partitions belonging to small jobs, while small jobs have non-preemptive priority over medium jobs for partitions of large jobs <ref> (Naik et al. 1993) </ref>. Variable Space Sharing: First Come First Served. When a job arrives, it is allowed to spawn an arbri-trary number of processes, which join a FIFO queue. <p> First, the entire data set representing the current state is trans-ferred from all processors in the old partition to a subset of processors in the new partition. Then, the data set is redistributed from this subset to all processors in the new partition <ref> (Naik et al. 1993) </ref>. Processor Sets. The processors are allocated equally among the jobs in the system. <p> In comparisons for distributed memory systems, the policies that have been proven most effective are the Adaptive Static Partitioning (Setia and Tripathi 1993), the Adaptive Policies (Rosti et al. 1994), and the Shortest Demand First (Sevcik 1994; Chiang et al. 1994). Although Dynamic Partitioning <ref> (Naik et al. 1993) </ref> was proven to perform well, the result is workload-dependent when the repartitioning overhead is taken into account (Park and Dowdy 1989; Dussa et al. 1990). However, we include it here with the name Dynamic.
Reference: <author> Nedeljkovic, N. and M. J. </author> <month> Quinn </month> <year> (1993, </year> <month> June). </month> <title> Data Parallel programming on a Network of Heterogeneous Workstations. </title> <journal> Concurrency: Practice and Experience 5 (4), </journal> <pages> 257-268. </pages>
Reference: <author> Ousterhout, J. K. </author> <year> (1982, </year> <month> October). </month> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> In 3rd Int'l Conference on Distributed Computing Systems, </booktitle> <pages> pp. 22-30. </pages>
Reference-contexts: Continuous Co-Scheduling. The processes are organized as a sequence. Scheduling is done by sliding across this sequence a window of length equal to the number of processors in the system <ref> (Ousterhout 1982) </ref>. Undivided Co-Scheduling. Its difference from the Continuous version is its additional requirement that the processes of each job be contiguous in the process sequence (Ousterhout 1982; Leutenegger and Vernon 1990). Round-Robin Policy.
Reference: <author> Park, K.-H. and L. W. </author> <title> Dowdy (1989). Dynamic Partitioning of Multiprocessor Systems. </title> <booktitle> International Journal of Parallel Programming 18 (2), </booktitle> <pages> 91-120. </pages>
Reference-contexts: of data distribution optimizations, cache interference and the operating point effect of the particular applications. 17 2.7 Scheduling in Distributed-Memory Systems The execution signature of a parallel application is defined as the rate of execution with respect to the number of allocated processors, the system architecture, and the program implementation <ref> (Park and Dowdy 1989) </ref>. It can be estimated by applying a least-squares approximation method on runtimes for different numbers of processors. <p> A least-squares approximation method is applied, and the only requirement is the runtime of the application for different numbers of processors. A few points are usually enough for an acceptable formulation. Also, it is shown how the simpler model T (p) = W P + a <ref> (Park and Dowdy 1989) </ref> contains less information and fails to approximate adequately the execution time curve of several real applications.
Reference: <author> Parsons, E. W. and K. C. </author> <title> Sevcik (1995, April). Multiprocessor Scheduling for High-Variability Service Time Distributions. </title> <booktitle> In IPPS '95 Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <pages> pp. 76-88. </pages> <institution> Platform Computing Corp. </institution> <year> (1994, </year> <month> December). </month> <title> LSF Administrator's Guide. </title> <institution> Toronto, ON: Platform Computing Corp. </institution>
Reference: <author> Prouty, R., S. Otto, and J. </author> <title> Walpole (1994, March). Adaptive Execution of Data Parallel Computations on Networks of Heterogeneous Workstations. </title> <type> Technical Report CSE-94-013, </type> <institution> DCSE, Oregon Graduate Institute of Science and Technology. </institution>
Reference: <author> Rosti, E., E. Smirni, L. Dowdy, G. Serazzi, and B. M. </author> <title> Carlson (1994). Robust Partitioning Policies of Multiprocessor Systems. </title> <booktitle> Performance Evaluation 19 (2-3), </booktitle> <pages> 141-165. </pages>
Reference-contexts: Insurance Policy. It is similar to the previous policy. Although there is no maximum allocation limit, only a fraction of the currently available processors is equally divided among the waiting jobs, with the remaining processors being reserved for future arrivals <ref> (Rosti et al. 1994) </ref>. Adaptive Policies. The system is in a certain state, which defines the ideal partition size. The transition from one state to another is determined by the queue length 24 of the waiting jobs. Actually, different transition rules lead to slightly different policies (Rosti et al. 1994). <p> reserved for future arrivals <ref> (Rosti et al. 1994) </ref>. Adaptive Policies. The system is in a certain state, which defines the ideal partition size. The transition from one state to another is determined by the queue length 24 of the waiting jobs. Actually, different transition rules lead to slightly different policies (Rosti et al. 1994). Adaptive Multiprogrammed Partitioning. An increase in processor utilization is attempted by combining Adaptive Static Partitioning with interleaving mul tiple processes from the same job on each node (Setia et al. 1993). Dynamic Space Sharing: Process Control. <p> Based on the results of previous studies, we identified those algorithms that demonstrated the best performance. In comparisons for distributed memory systems, the policies that have been proven most effective are the Adaptive Static Partitioning (Setia and Tripathi 1993), the Adaptive Policies <ref> (Rosti et al. 1994) </ref>, and the Shortest Demand First (Sevcik 1994; Chiang et al. 1994). Although Dynamic Partitioning (Naik et al. 1993) was proven to perform well, the result is workload-dependent when the repartitioning overhead is taken into account (Park and Dowdy 1989; Dussa et al. 1990). <p> These two rules have not been compared previously with respect to mean response time. In the article that introduces AP1, power, which is defined as the ratio of throughput to response time, is used as performance measure <ref> (Rosti et al. 1994) </ref>. 64 0 10 20 30 40 0 0.2 0.4 0.6 0.8 1 Mean Response Time Load WK1 0 10 20 30 40 0 0.2 0.4 0.6 0.8 1 Mean Response Time Load WK2 0 10 20 30 40 0 0.2 0.4 0.6 0.8 1 Mean Response Time
Reference: <author> Schwetman, H. </author> <year> (1992a, </year> <month> June). </month> <title> CSIM Reference Manual. </title> <institution> Microelectronics and Computer Technology Corp. </institution> <note> Revision 16. </note>
Reference: <author> Schwetman, H. </author> <year> (1992b, </year> <month> June). </month> <title> CSIM Users' Guide. </title> <institution> Microelectronics and Computer Technology Corp. </institution> <note> Revision 16. </note>
Reference: <author> Setia, S. and S. Tripathi (1993, </author> <month> January). </month> <title> A Comparative Analysis of Static Processor Partitioning Policies for Parallel Computers. </title> <booktitle> In Int'l Workshop on Modeling and Simulation of Computer and Telecommunication Systems (MASCOTS), </booktitle> <pages> pp. 283-286. </pages>
Reference-contexts: Actually, different transition rules lead to slightly different policies (Rosti et al. 1994). Adaptive Multiprogrammed Partitioning. An increase in processor utilization is attempted by combining Adaptive Static Partitioning with interleaving mul tiple processes from the same job on each node <ref> (Setia et al. 1993) </ref>. Dynamic Space Sharing: Process Control. Processors are dynamically allocated equally among the jobs in the system. No application is allocated more processors than its current parallelism. The applications dynamically control the number of runnable processes to match the number of processors available to them. <p> It is the responsibility of the application to determine how the allocated processors will be used and whether multiple threads will be interleaved on each individual processor or not. The programmer decides which choice fits better the computation, synchronization and communication needs of the problem <ref> (Setia et al. 1993) </ref>. 28 The scheduling decisions are made at a single node, where a central queue of job requests is maintained. <p> Based on the results of previous studies, we identified those algorithms that demonstrated the best performance. In comparisons for distributed memory systems, the policies that have been proven most effective are the Adaptive Static Partitioning <ref> (Setia and Tripathi 1993) </ref>, the Adaptive Policies (Rosti et al. 1994), and the Shortest Demand First (Sevcik 1994; Chiang et al. 1994).
Reference: <author> Setia, S. K., M. S. Squillante, and S. K. </author> <title> Tripathi (1993). Processor Scheduling on Multi-programmed Distributed Memory Parallel Computers. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 158-170. </pages>
Reference-contexts: Actually, different transition rules lead to slightly different policies (Rosti et al. 1994). Adaptive Multiprogrammed Partitioning. An increase in processor utilization is attempted by combining Adaptive Static Partitioning with interleaving mul tiple processes from the same job on each node <ref> (Setia et al. 1993) </ref>. Dynamic Space Sharing: Process Control. Processors are dynamically allocated equally among the jobs in the system. No application is allocated more processors than its current parallelism. The applications dynamically control the number of runnable processes to match the number of processors available to them. <p> It is the responsibility of the application to determine how the allocated processors will be used and whether multiple threads will be interleaved on each individual processor or not. The programmer decides which choice fits better the computation, synchronization and communication needs of the problem <ref> (Setia et al. 1993) </ref>. 28 The scheduling decisions are made at a single node, where a central queue of job requests is maintained. <p> Based on the results of previous studies, we identified those algorithms that demonstrated the best performance. In comparisons for distributed memory systems, the policies that have been proven most effective are the Adaptive Static Partitioning <ref> (Setia and Tripathi 1993) </ref>, the Adaptive Policies (Rosti et al. 1994), and the Shortest Demand First (Sevcik 1994; Chiang et al. 1994).
Reference: <author> Setia, S. K., M. S. Squillante, and S. K. Tripathi (1994, </author> <month> May). </month> <title> Analysis of Processor Allocation in Multiprogrammed Distributed-Memory Parallel Processing Systems. </title> <journal> 82 IEEE Trans. on Parallel and Distributed Systems 5 (4), </journal> <pages> 401-420. </pages>
Reference: <author> Sevcik, K. C. </author> <year> (1989, </year> <month> May). </month> <title> Characterizations of Parallelism in Applications and Their Use in Scheduling. </title> <booktitle> Performance Evaluation Review 17 (1), </booktitle> <pages> 171-180. </pages>
Reference-contexts: queuing system with P servers is defined as follows : Load = M ean Service T ime P fi M ean Interarrival T ime In studies of multiprocessor systems, the M ean Service T ime is set equal to the mean total execution time of the jobs at one processor <ref> (Sevcik 1989) </ref>: Load = T (1) P fi M ean Interarrival T ime Intuitively, the use of T (1) is reasonable since it is the minimum total execution time of a job, due to the overhead that arises with larger numbers of processors.
Reference: <author> Sevcik, K. C. </author> <year> (1994). </year> <title> Application Scheduling and Processor Allocation in Multipro-grammed Parallel Processing Systems. </title> <booktitle> Performance Evaluation 19 (2-3), </booktitle> <pages> 107-140. </pages>
Reference-contexts: Also, it is shown how the simpler model T (p) = W P + a (Park and Dowdy 1989) contains less information and fails to approximate adequately the execution time curve of several real applications. From equation 4.1 it is possible to derive the point p max <ref> (Sevcik 1994) </ref> that was mentioned above: p max = &gt; &gt; &lt; q fi if fi 6= 0 (4.2) 4.2 The Adaptive Static Partitioning Policy The first algorithm that used system load only in order to achieve effective scheduling was introduced by Setia and Tripathi (1993) with the name Adaptive Static <p> For instance, in the case of jobs with perfect speedups and in the absence of arrivals, the scheduling strategy that minimizes the average response time in the system is SDF with all processors being given to each application in turn <ref> (Sevcik 1994) </ref>. In addition to SDF as defined above, combinations of SDF with the previously defined adaptive policies can be introduced. <p> Actually, the essential computa tion W determines the potential of a job to reduce its response time when given additional processors. The optimization procedure is not affected by the value of ff, since ff does not participate in the derivative of the execution time function <ref> (Sevcik 1994) </ref>. Furthermore fi is important but in typical workloads is much less than W (Wu 1993). Thus the ordering of the jobs according to the runtime on a single processor separates the jobs into groups with similar potential for processor exploitation.
Reference: <author> Silverman, R. D. and S. J. Stuart (1989, </author> <month> December). </month> <title> A Distributed Batching System for Parallel Processing. </title> <journal> Software-Practice and Experience 19 (12), </journal> <pages> 1163-1174. </pages>
Reference: <author> Sun, X.-H. and L. M. Ni (1993, </author> <month> September). </month> <title> Scalable Problems and Memory-Bounded Speedup. </title> <journal> Journal of Parallel and Distributed Computing 19 (1), </journal> <pages> 27-37. </pages>
Reference-contexts: It is shown that memory-bounded speedup jobs exhibit better performance than jobs having fixed-load speedup, since the task ratio of the former is fixed, while that of the latter decreases with an increase in the number of workstations <ref> (Sun and Ni 1993) </ref>. Issues arising when mixing sequential and parallel workloads are examined by Arpaci et al. (1994). It is proven that lack of coordination in the round-robin scheduling of the threads can significantly slow down the jobs, especially those with fine-grained parallelism.
Reference: <author> Trivedi, K. S. </author> <year> (1982). </year> <title> Probability & Statistics with Reliability, Queuing, </title> <booktitle> and Computer Science Applications. </booktitle> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, N.J. </address>
Reference-contexts: We already know that E (W ) = 13:76. By using the theorem of total expectation <ref> (Trivedi 1982) </ref> : E (fi) =E (fi j p max = 4)P (p max = 4) + E (fi j p max = 64)P (p max = 64): we finally get E (fi) = 0:30.
Reference: <author> Tucker, A. and A. </author> <month> Gupta </month> <year> (1989, </year> <month> December). </month> <title> Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Multiprocessors. </title> <booktitle> In 12th ACM Symp. Operating System Principles, </booktitle> <pages> pp. 159-166. </pages>
Reference-contexts: The minimization of spin-waiting by Co-scheduling appears to improve the performance at very high lock demands, but even then not as effectively as the handling of variance in job demand by Round Robin. The Round Robin Job and Process Control <ref> (Tucker and Gupta 1989) </ref> policies, which allocate an equal fraction of the processing power to each job, perform best under nearly all workload assumptions considered. Black (1990) describes the parallel application scheduling support of the Mach OS.
Reference: <author> Vahdat, A., D. Ghormley, and T. Anderson (1994, </author> <month> December). </month> <title> Efficient, Portable and Robust Extension of Operating System Functionality. </title> <type> Technical Report CS-94-842, </type> <institution> Computer Science Division, University of California, </institution> <note> Berkeley. </note> <author> von Eicken, T., A. Basu, and V. </author> <title> Buch (1995, February). Low-Latency Communication Over ATM Networks Using Active Messages. </title> <booktitle> IEEE Micro 15 (2), </booktitle> <pages> 46-53. </pages>
Reference: <author> Wahbe, R., S. Lucco, T. E. Anderson, and S. L. Graham (1993, </author> <month> December). </month> <title> Efficient software-based fault isolation. </title> <booktitle> In 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 203-216. </pages>
Reference: <author> Wang, C.-J., P. Krueger, and M. T. Liu (1993, </author> <month> May). </month> <title> Intelligent Job Selection for Distributed Scheduling. </title> <booktitle> In 13th Int'l Conference on Distributed Computing Systems, </booktitle> <pages> pp. 517-524. </pages>
Reference: <author> Wang, Z. </author> <year> (1995). </year> <title> Trace Data Analysis and Job Scheduling Simulation for Large-Scale Distributed Systems. </title> <type> Master's thesis, </type> <institution> Department of Computer Science, University of Toronto. </institution>
Reference: <author> Welch, P. D. </author> <year> (1983). </year> <title> The Statistical Analysis of Simulation Results. </title> <editor> In S. S. Lavenberg (Ed.), </editor> <booktitle> The Computer Performance Modeling Handbook, </booktitle> <pages> pp. 268-328. </pages> <publisher> Academic Press, </publisher> <address> 83 New York. </address>
Reference: <author> Wu, C.-S. </author> <year> (1993). </year> <title> Processor Scheduling in Multiprogrammed Shared Memory NUMA Multiprocessors. </title> <type> Master's thesis, </type> <institution> Department of Computer Science, </institution> <type> Technical Report CSRI-341, </type> <institution> University of Toronto. </institution>
Reference-contexts: Its value determines the minimum total service demand of the job (both fi and ff are typically smaller <ref> (Wu 1993) </ref>) and therefore the variation of service required among different jobs. We tried to make our simulated workload realistic by using the 47 statistics gathered for a 128-node iPSC/860 message-passing system at the NASA Ames Center by Feitelson and Nitzberg (1995) (as described in Chapter 2). <p> The optimization procedure is not affected by the value of ff, since ff does not participate in the derivative of the execution time function (Sevcik 1994). Furthermore fi is important but in typical workloads is much less than W <ref> (Wu 1993) </ref>. Thus the ordering of the jobs according to the runtime on a single processor separates the jobs into groups with similar potential for processor exploitation. <p> improve its performance further 71 0 10 20 30 Mean Waiting Time Load WK1 0 10 20 30 Mean Waiting Time Load WK4 namic for WK1 and WK4. 0 10 20 Mean Partition Size Load WK1 0 10 20 Mean Partition Size Load WK4 and probably by use of OPT <ref> (Wu 1993) </ref>. However, it seems that estimation of the execution time function for a Dynamic policy is not as straightforward as in the case of static partitioning, due to the continuous change in the number of processors. <p> But, both SDF and OPT need the execution time representation in order to be realized. In addition, by comparing our algorithm AEP-SDF-OPT with ASP-SDF-OPT <ref> (Wu 1993) </ref>, we realize that, in the general case (WK4), the former manages to be 20 30% worse than Dynamic, while the latter is correspondingly 55 80% worse.
Reference: <author> Zahorjan, J. and C. </author> <month> McCann </month> <year> (1990, </year> <month> May). </month> <title> Processor Scheduling in Shared Memory Multiprocessors. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 214-225. </pages>
Reference-contexts: The performance is better than Gang Scheduling, as there are no cache flushes. Another comparative study is presented by McCann et al. (1993). The Dynamic policy is introduced where processors are reallocated among jobs in response to changes in the parallelism of the jobs 2 <ref> (Zahorjan and McCann 1990) </ref>. Also, the Equipartition policy is defined, which maintains an equal allocation of processors to all jobs. Reallocations take place only on job arrival and completion. The response time of Dynamic is compared with that of Round-Robin Job and Equiparti-tion on a shared-memory multiprocessor. <p> Equivalently we have that: W +fi ( x + (1 ) y) + ff x x It is enough to show that: 1 x 1 which is equivalent to : 40 %Based on RTC <ref> (Zahorjan and McCann 1990) </ref> %Initially zero allocation is assumed for the waiting jobs %The jobs are maintained in the waiting queue in increasing %order of T (allocation+1)-T (allocation) with T (0)=1 %Ties are broken in favor of the jobs that arrived earlier while (waiting_jobs&gt;0 and free_processors&gt;0) do let J be the <p> In one previous study, the waiting jobs were FIFO ordered and at each step as many jobs as possible were activated <ref> (Zahorjan and McCann 1990) </ref>. Wu (1993) assumed that the jobs are SDF ordered and as many jobs 41 as possible are activated at each step.
Reference: <author> Zhou, S. and T. </author> <month> Brecht </month> <year> (1991, </year> <month> May). </month> <title> Processor Pool-Based Scheduling for Large-Scale NUMA Multiprocessors. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 133-142. </pages>
Reference: <author> Zhou, S., X. Zheng, J. Wang, and P. </author> <title> Delisle (1993, December). Utopia: a Load Sharing Facility for Large, </title> <booktitle> Heterogeneous Distributed Computer Systems. Software-Practice and Experience 23 (12), </booktitle> <pages> 1305-1336. 84 </pages>
Reference-contexts: Centralized algorithms that can take advantage of the clustering properties in small or large scale distributed systems, have been proven successful in recent resource management environments <ref> (Zhou et al. 1993) </ref> without restricting the system scalability, as it was traditionally claimed. Also, we expect that the scheduling system guarantees that the parallel applications are kept apart from the sequential workload of the system, whether they are daemons or applications initiated by the workstation owners.
References-found: 69

