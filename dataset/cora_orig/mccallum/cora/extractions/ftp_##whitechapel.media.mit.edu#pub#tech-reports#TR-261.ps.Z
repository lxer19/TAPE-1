URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-261.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Title: for Visual Behaviors, CVPR-94 A Novel Environment for Situated Vision and Behavior  
Author: Trevor Darrell, Pattie Maes, Bruce Blumberg, Alex P. Pentland 
Note: Presented at IEEE Workshop  
Pubnum: MIT Media Laboratory  
Abstract: M.I.T. Media Laboratory Perceptual Computing Technical Report No. 261 Abstract We present a new environment for the development of situated vision and behavior algorithms. Our environment allows an unencumbered person to interact with autonomous agents in a simulated graphical world, though the use of situated vision techniques. An image of the participant is composited together with the graphical world and projected onto a large screen in front of the participant. No goggles, gloves, or wires are needed; agents and objects in the graphical world can be acted upon by the human participant through the use of domain-specific computer vision techniques that analyze the image of the person. The agents inhabiting the world are modeled as autonomous behaving entities which have their own sensors and goals and which can interpret the actions of the participant and react to them in real-time. We have demonstrated and tested our system with two prototypical worlds and describe the results obtained with over 500 people. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agre P. and Chapman D., Pengi: </author> <title> An Implementation of a Theory of Activity, </title> <booktitle> Proc. AAAI-87, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1987. </year>
Reference-contexts: Since multiple goals can exist and conflict with each other in an agent at any given time, the model should be able to mediate between heterogeneous goals in real time. Recent results with reactive systems [12], routines <ref> [1] </ref> and subsumption architectures [8] have have demonstrated remarkably reliable and successful performance in performing real-time action selection in autonomous agents.
Reference: [2] <author> Aloimonos Y., </author> <title> Active Perception, </title> <publisher> Lawrence Erlbaum Associates, Inc., </publisher> <address> Hillsdale, </address> <year> 1993. </year>
Reference-contexts: Second, the fact that people are not simply objects, and have intentions and communicate via semantically-laden signs strongly argues for an active or purposive approach to vision <ref> [4, 2, 7] </ref>. The traditional stated goal of a computer vision system has been to assume the world is in a particular state and to attempt to recover as complete and accurate a description of that state as possible.
Reference: [3] <author> Baerends, G., </author> <title> On drive, conflict and instinct, and the functional organization of behavior, in: Perspectives in Brain Research 45, </title> <editor> Corner M. and Swaab, D. (eds), </editor> <year> 1976. </year>
Reference-contexts: One of the distinguishing characteristics of the model used is that it is closely based on models of animal behavior. In particular, it borrows heavily from the work of classical Ethologists such as Lorenz, Tinbergen, Baerends, Ludlow and McFarland [14] [18] <ref> [3] </ref> [15] [17].
Reference: [4] <editor> Bajcsy, R., </editor> <title> Active Perception, </title> <journal> Proc. IEEE, </journal> <volume> Vol. 76, No. 8, </volume> <pages> pp. 996-1005, </pages> <year> 1988 </year>
Reference-contexts: Second, the fact that people are not simply objects, and have intentions and communicate via semantically-laden signs strongly argues for an active or purposive approach to vision <ref> [4, 2, 7] </ref>. The traditional stated goal of a computer vision system has been to assume the world is in a particular state and to attempt to recover as complete and accurate a description of that state as possible.
Reference: [5] <author> Bichel, M. and Pentland, A., </author> <title> Topological Matching for Human Face Recognition, </title> <booktitle> in: Looking at People Workshop, IJCAI '93, </booktitle> <address> Chamberry, France, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: A slightly more sophisticated approach is to allow the background to be an arbitrary, but static, pattern. Mean and variance information about the background pattern are computed using samples collected over a specified time-window. Using these statistics to determine thresholds for pixel class membership, accurate figure-ground membership is possible <ref> [5] </ref>.
Reference: [6] <editor> Blumberg, B., et. al, </editor> <booktitle> in Proc. Conference on Simulation and Adaptive Behavior (SAB-94), </booktitle> <address> Brighton, U.K., </address> <year> 1994 </year>
Reference-contexts: Arbitration among actions is a run-time process which differs according to the goals of the system and the situation it finds itself in. Full details of the behavior model and a discussion of typical locomotion, foraging, and exploration behaviors are reported upon in <ref> [6] </ref>.
Reference: [7] <author> Ballard, D., </author> <title> Animate Vision, </title> <journal> Artificial Intelligence, </journal> <volume> Vol 48, </volume> <pages> pp. 57-86, </pages> <year> 1991 </year>
Reference-contexts: Second, the fact that people are not simply objects, and have intentions and communicate via semantically-laden signs strongly argues for an active or purposive approach to vision <ref> [4, 2, 7] </ref>. The traditional stated goal of a computer vision system has been to assume the world is in a particular state and to attempt to recover as complete and accurate a description of that state as possible.
Reference: [8] <author> Brooks R.A., </author> <title> A robust layered control system for a mobile robot, </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> Volume RA-2, Number 1, </volume> <year> 1986 </year>
Reference-contexts: Since multiple goals can exist and conflict with each other in an agent at any given time, the model should be able to mediate between heterogeneous goals in real time. Recent results with reactive systems [12], routines [1] and subsumption architectures <ref> [8] </ref> have have demonstrated remarkably reliable and successful performance in performing real-time action selection in autonomous agents. The emphasis in these architectures is put on direct coupling of perception to action, distributedness and decentralization, dynamic interaction with the environment and intrinsic mechanisms to cope with resource limitations and incomplete knowledge.
Reference: [9] <author> Ballard, D., and Brown, C., </author> <title> Computer Vision, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood, </address> <year> 1982. </year>
Reference-contexts: We binarize the difference image, find connected regions, and compute the image bounding box and first-order moments of the largest connected region. (For a review of connected components and binary image processing methods see <ref> [9] </ref> and [16].) 5.3 Scene projection and calibration Once the figure of the user has been isolated from the background, we compute its rough location in the world. <p> Establishing the calibration of a camera is a well-studied problem, and several classical techniques are available to solve it in certain broad cases <ref> [9, 16] </ref>. Typically these methods model the camera optics as a pinhole perspective optical system, and establish its parameters by matching known 3D points with their 2D projection.
Reference: [10] <author> Darrell T., and Pentland A., </author> <title> Robust Estimation of a Multi-Layer Motion Representation, </title> <booktitle> in IEEE Motion Workshop, </booktitle> <address> Princeton, </address> <year> 1991. </year>
Reference: [11] <author> Darrell, T. and Pentland, A., </author> <booktitle> Space-Time Gestures Proceedings IEEE CVPR-93, </booktitle> <address> New York, </address> <year> 1993. </year>
Reference-contexts: Waving requires a predominantly side-to-side motion of the hand, while the user is otherwise stationary. These are special cases of our more general work on gesture recognition, which builds space and time separable template patterns for recognition <ref> [11] </ref>. However this work assumed a high resolution image of the object performing the gesture. As we have as yet no high-resolution camera to provide a foveated image of the hands (or face) of the user, we presently only model gesture as change in position (or lack thereof) over time.
Reference: [12] <author> Kaelbling L.P., </author> <title> An architecture for intelligent reactive systems, </title> <booktitle> in: Reasoning about Actions and Plans: Proceedings of the 1986 Workshop, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1987. </year>
Reference-contexts: Since multiple goals can exist and conflict with each other in an agent at any given time, the model should be able to mediate between heterogeneous goals in real time. Recent results with reactive systems <ref> [12] </ref>, routines [1] and subsumption architectures [8] have have demonstrated remarkably reliable and successful performance in performing real-time action selection in autonomous agents.
Reference: [13] <author> Krueger M.W., </author> <title> Artificial Reality II, </title> <publisher> Addison Wesley, </publisher> <year> 1990. </year>
Reference-contexts: Our system, ALIVE, or Artificial Life Interactive Video Environment uses the agent and vision modeling techniques described above. With few exceptions (i.e. <ref> [13] </ref>), to experience these environments previously required the use of gloves, goggles, and/or a helmet, and most likely a wired tether to a computer graphics workstation [19]. We implemented the magic-mirror model in ALIVE using a single CCD camera to obtain a color image of the scene.
Reference: [14] <author> Lorenz, K., </author> <title> Foundations of Ethology, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: One of the distinguishing characteristics of the model used is that it is closely based on models of animal behavior. In particular, it borrows heavily from the work of classical Ethologists such as Lorenz, Tinbergen, Baerends, Ludlow and McFarland <ref> [14] </ref> [18] [3] [15] [17].
Reference: [15] <author> Ludlow, A., </author> <title> The Evolution and Simulation of a Decision Maker, in: Analysis of Motivational Processes, </title> <editor> ed. Toates, F. and Halliday, T., </editor> <publisher> Academic Press, </publisher> <address> Lon-don, </address> <year> 1980. </year>
Reference-contexts: One of the distinguishing characteristics of the model used is that it is closely based on models of animal behavior. In particular, it borrows heavily from the work of classical Ethologists such as Lorenz, Tinbergen, Baerends, Ludlow and McFarland [14] [18] [3] <ref> [15] </ref> [17].
Reference: [16] <author> Horn, B.K.P.S., </author> <title> Robot Vision, </title> <publisher> M.I.T. Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1991. </year>
Reference-contexts: We binarize the difference image, find connected regions, and compute the image bounding box and first-order moments of the largest connected region. (For a review of connected components and binary image processing methods see [9] and <ref> [16] </ref>.) 5.3 Scene projection and calibration Once the figure of the user has been isolated from the background, we compute its rough location in the world. <p> Establishing the calibration of a camera is a well-studied problem, and several classical techniques are available to solve it in certain broad cases <ref> [9, 16] </ref>. Typically these methods model the camera optics as a pinhole perspective optical system, and establish its parameters by matching known 3D points with their 2D projection.
Reference: [17] <author> McFarland, D. and Sibley, R., </author> <title> The behavioral final common path, </title> <journal> Philosophical Transactions of the Royal Society, B. </journal> <volume> 270, </volume> <year> 1975. </year>
Reference-contexts: One of the distinguishing characteristics of the model used is that it is closely based on models of animal behavior. In particular, it borrows heavily from the work of classical Ethologists such as Lorenz, Tinbergen, Baerends, Ludlow and McFarland [14] [18] [3] [15] <ref> [17] </ref>.
Reference: [18] <author> Tinbergen, N., </author> <title> The Study of Instinct. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1950. </year>
Reference-contexts: One of the distinguishing characteristics of the model used is that it is closely based on models of animal behavior. In particular, it borrows heavily from the work of classical Ethologists such as Lorenz, Tinbergen, Baerends, Ludlow and McFarland [14] <ref> [18] </ref> [3] [15] [17].
Reference: [19] <author> Rheingold H., </author> <title> Virtual Reality, </title> <publisher> Simon and Schuster, </publisher> <year> 1991. </year>
Reference-contexts: Our system, ALIVE, or Artificial Life Interactive Video Environment uses the agent and vision modeling techniques described above. With few exceptions (i.e. [13]), to experience these environments previously required the use of gloves, goggles, and/or a helmet, and most likely a wired tether to a computer graphics workstation <ref> [19] </ref>. We implemented the magic-mirror model in ALIVE using a single CCD camera to obtain a color image of the scene. The image of the user was separated from the background using color differencing with a known background, and then composited into the 3D graphical world.
Reference: [20] <author> Tsotsos, J., </author> <title> A `Complexity Level' Analysis of Immediate Vision, </title> <journal> Int`l J. Computer Vision, pp. </journal> <volume> 303-320, Vol. 1, No. 3, </volume> <year> 1988 </year>
Reference-contexts: Vision algorithms which are too general, e.g. look for everything everywhere, will usually suffer from an explosion in search complexity, and fail to offer adequate performance. As many authors have noted, the solution to this dilemma lies in the use of an attention mechanism <ref> [20] </ref>. Attention mechanisms require some state to exist in the perceiving agent. One can say Attention requires Intention, in that without meaningful states and goals, a vision system has no principled way to prioritize what to look for next.
Reference: [21] <institution> Ultimatte-300, Ultimatte Corp, </institution> <address> Chatsworth, CA. </address> <month> 7 </month>
Reference-contexts: The simplest method of detecting differences is to constrain the background to be a known color, and detect instances of that hue in the input images. Several commer cially available video image processing boards are available which can automatically isolate figure/ground and perform video compositing based on this approach <ref> [21] </ref>. This approach essentially performs clustering in color space to determine pixel membership in figure/ground classes. The advantages of this approach are ease of implementation, and the availability of real-time hardware solutions to the problem. <p> A video camera at the front of the space captured the user's image. This version of the ALIVE system employed Chroma-keying to compose the user's image with the computer animated virtual world, which was performed by an Ultimatte system <ref> [21] </ref>. The composited image was displayed on a large screen (10' by 10') which faced the user. The machine used for the graphics and behavior modeling was an SGI Indigo Elan.
References-found: 21

