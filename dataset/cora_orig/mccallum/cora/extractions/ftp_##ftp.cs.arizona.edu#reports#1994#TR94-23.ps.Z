URL: ftp://ftp.cs.arizona.edu/reports/1994/TR94-23.ps.Z
Refering-URL: http://www.cs.arizona.edu/people/dorgival/
Root-URL: http://www.cs.arizona.edu
Title: SUPPORTING FAULT-TOLERANT PARALLEL PROGRAMMING IN LINDA  
Author: David Edward Bakken 
Degree: (Ph.D. Dissertation)  
Note: This work was supported in part by the National Science Foundation under grant CCR-9003161 and the Office of Naval Research under grant N00014-91-J-1015.  
Address: Tucson, Arizona 85721  
Affiliation: Department of Computer Science The University of Arizona  
Date: August 8, 1994  
Pubnum: TR 94-23  
Abstract-found: 0
Intro-found: 1
Reference: [ACG86] <author> Sudhir Ahuja, Nicholas Carriero, and David Gelernter. </author> <title> Linda and friends. </title> <journal> IEEE Computer, </journal> <volume> 19(8) </volume> <pages> 26-34, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Tuple space also provides temporal decoupling, because communicating processes do not have to exist at the same time, and spatial decoupling, because processes do not have to know each other's identity. These properties make Linda especially easy for use by application programmers <ref> [CS93, ACG86, CG88, CG90] </ref>. Linda implementations are available on a number of different architectures [CG86, Lei89, Bjo92, SBA93, CG93, Zen90] and for a number of different languages [Lei89, Jel90, Has92, Cia93, SC91]. <p> These design choices are tightly intertwined with the particular language extensions we offer and with the way in which we implement them, topics for Chapters 3 and 5, respectively. 2.1 Linda Overview Linda is a parallel programming language based on a communication abstraction known as tuple space (TS) <ref> [Gel85, ACG86, CG89, GC92, CG90] </ref>. Tuple space is an associative (i.e., content-addressable) unordered bag of data elements called tuples. Processes are created in the context of a given TS, which they use as a means for communicating and synchronizing. <p> true do in (subtask,?subtask args) calc (subtask args; var result args) for (all new subtasks created by this subtask) out (subtask, new subtask args) out (result,result args) end while end proc Bag-of-Tasks Linda lends itself nicely to a method of parallel programming called the bag-of-tasks or replicated worker programming paradigm <ref> [ACG86, CG88] </ref>. In this paradigm, the task to be solved is partitioned into independent subtasks.
Reference: [AD76] <author> P. A. Alsberg and J. D. Day. </author> <title> A principle for resilient sharing of distributed resources. </title> <booktitle> In Proceedings of the Second International Conference on Software Engineering, </booktitle> <pages> pages 627-644, </pages> <month> October </month> <year> 1976. </year>
Reference-contexts: They are also recoverable, i.e. executed completely or not at all. These actions are called transactions in the database context. The primary/backup paradigm features a service implemented by multiple processes; the primary process is active, while the backup processes are passive <ref> [AD76, BMST92] </ref>. Only the active process responds to requests for the service; in the case of the failure of the primary process, one backup process will become the primary, starting from a checkpointed state. A replicated state machine consists of a collection of servers [Sch90]. <p> First, it gives a replicated server example. This is an example of programming replicated state machines in FT-Linda. Next, it presents an FT-Linda recoverable server an example of the primary/backup approach <ref> [AD76, BMST92] </ref>. Since the server is not replicated, there are no redundant server computations in the absence of failures. Finally, this section presents an FT-Linda implementation of a transaction facility.
Reference: [AG91a] <author> Shakil Ahmed and David Gelernter. </author> <title> A higher-level environment for parallel programming. </title> <type> Technical Report YALEDU/DCS/RR-877, </type> <institution> Yale University Department of Computer Science, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: Support for disjunction has also been discussed in [Gel85, Lei89] and in the context of the Linda Program Builder <ref> [AG91a, AG91b] </ref>. The latter offers the abstraction of disjunction by mapping it onto ordinary Linda operations and hiding the details from the user. None of these efforts consider fault-tolerance. Summary FT-Linda has many novel features that distinguish it from other efforts to provide fault-tolerance to Linda.
Reference: [AG91b] <author> Shakil Ahmed and David Gelernter. </author> <title> Program builders as alternatives to high-level languages. </title> <type> Technical Report YALEDU/DCS/RR-887, </type> <institution> Yale University Department of Computer Science, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: Support for disjunction has also been discussed in [Gel85, Lei89] and in the context of the Linda Program Builder <ref> [AG91a, AG91b] </ref>. The latter offers the abstraction of disjunction by mapping it onto ordinary Linda operations and hiding the details from the user. None of these efforts consider fault-tolerance. Summary FT-Linda has many novel features that distinguish it from other efforts to provide fault-tolerance to Linda.
Reference: [AGMvR93] <author> Carlos Almedia, Brad Glade, Keith Marzullo, and Robbert van Renesse. </author> <title> High availability in a real-time system. </title> <journal> ACM Operating Systems Review, </journal> <volume> 27(2) </volume> <pages> 82-87, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Additionally, the state machine approach used to implement FT-Linda seems well-suited to provide real-time support, in addition to fault-tolerance. Indeed, one such project is already underway to provide real-time communication support for replicated state machine in the Corto project <ref> [AGMvR93] </ref>, a real-time successor to Isis. This could serve as an excellent basis for a real-time versions of FT-Linda. APPENDIX A FT-LINDA IMPLEMENTATION NOTES The C FT-Linda code in the appendices differs from the pseudocode given in previous chapters in a number of ways.
Reference: [Akl89] <author> Selim G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: Parallel programs running on these computers are now used to solve a wide variety of programs, including scientific computations, engineering design programs, financial transaction systems, and much more <ref> [Akl89, And91] </ref>. Parallel programming is much harder than sequential programming for a number of reasons. Coordinating the processors is difficult and error-prone. Also, there are different architectures that run parallel programs, and programs written on one cannot generally be run on another. <p> Examples of such problems include environmental and economic modelling, real-time speech and sight recognition, weather forecasting, molecular modelling, and aircraft testing <ref> [Akl89] </ref>. Parallel programming enables these problems to be solved more efficiently by performing multiple tasks simultaneously. The second reason for parallel programming is economics: It is generally cheaper to build a parallel computer with the same or even more aggregate computational power than a fast uniprocessor.
Reference: [And91] <author> Gregory R. Andrews. </author> <title> Concurrent Programming: </title> <booktitle> Principles and Practice. </booktitle> <address> Benjamin/Cummings, Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: Parallel programs running on these computers are now used to solve a wide variety of programs, including scientific computations, engineering design programs, financial transaction systems, and much more <ref> [Akl89, And91] </ref>. Parallel programming is much harder than sequential programming for a number of reasons. Coordinating the processors is difficult and error-prone. Also, there are different architectures that run parallel programs, and programs written on one cannot generally be run on another. <p> surfaces and plotting its course. 1.3 Simplifying Parallel Programming Parallel programming is increasing in both popularity and diversity, but writing parallel programs is difficult for a number of reasons.First, parallel programs are simply more complicated; they have to deal with concurrency, something that sequential programs do not have to do <ref> [And91] </ref>. For example, a procedure performing an operation on a complex data structure that operates correctly on a uniprocessor will generally not function properly if multiple processes simultaneously call that procedure. <p> Fault tolerance abstractions provide powerful models and mechanisms for the programmer to deal with failures in the underlying computing platform [MS92, Jal94, Cri91]. Process coordination mechanisms allow multiple pro 22 cesses to communicate and coordinate <ref> [And91] </ref>. These two categories overlap, but they differ so are described separately below. 1.3.1 Fault Tolerance Abstractions Fault-tolerant abstractions allow a programmer to construct fault-tolerant software, i.e., software that can continue to provide service despite failures. <p> Message passing allows processes on different computers to exchange messages to communicate and synchronize. PVM [Sun90] and MPI [For93b, For93a] are two of the more well-known libraries of message passing libraries; many parallel programming languages explicitly provide message passing as well <ref> [And91] </ref>. 25 A shared memory is memory that can be accessed by more than one process. It can be implemented in a multiprocessor by simply providing a shared memory bus that connects all processors to the memory. <p> Examples of such problems include partial differential equations, region labelling, parallel prefix computation, linked list operations, and grid computations <ref> [And91] </ref>. Such algorithms typically involve an array, with the same computation being performed on each iteration. Since the computations for a given portion of the array and for a given iteration are independent of the other computations for that iteration, such algorithms are ideal candidates for parallelization. <p> Multiprocessor Implementations of Barriers We now describe three techniques for implementing a barrier on a multiprocessor; this material is summarized from <ref> [And91] </ref>, where two other schemes are also given. The first technique is to maintain a shared counter that records the number of processes that have reached the barrier. <p> We also omit this reduction phase for brevity and clarity. However, it is fairly straightforward to implement these in both Linda and FT-Linda. 2 This figure is taken from <ref> [And91] </ref>. 74 procedure init barrier () out (barrier count; 0) # : : : initialize both copies of the global array : : : # Create worker (id:1..N) with local Linda's # process creation mechanism (omitted) end init barrier Linda Barriers A Linda barrier could reasonably be implemented with a shared <p> However, parallel iterative algorithms such as systolic-like algorithms 3 use message passing, not shared memory, to synchronize. Problems that can be solved with such algorithms include matrix multiplication, network topology, parallel sorting, and region labelling <ref> [And91] </ref>. Fortunately, the FT-Linda barrier solution given above already uses Linda's blocking operations to synchronize, so its basic ideas apply directly to systolic-like and other data flow algorithms that use message passing.
Reference: [AO93] <author> Gregory R. Andrews and Ronald A. Olsson. </author> <title> The SR Programming Language: Concurrency in Practice. </title> <address> Benjamin/Cummings, Redwood City, California, </address> <year> 1993. </year>
Reference-contexts: Parallel programming languages provide high-level abstractions for programmers to use. These generally permit a cleaner integration of the sequential and concurrent features of the language than do libraries of procedures. Examples of such languages include Ada [DoD83], Orca [Bal90], and SR <ref> [AO93] </ref>. Libraries of procedures allow the programmer to leverage experience with an existing language and reuse existing code. They can also support many languages, allowing processes written with different languages to communicate. <p> The notion of a guarded communication statement was introduced in [Dij75]; its guard contained both an optional expression and an optional communication statement. The guarded expression has since been used in various forms in parallel programming languages such as CSP [Hoa78], Ada [DoD83], SR <ref> [AO93] </ref>, and Orca [Bal90]. Recall that in a disjunctive AGS, to simplify the semantics and to fit into a programming language's type system, all guards must be the same: either all absent (true), blocking (in or rd), or boolean (inp or rdp).
Reference: [AS91] <author> Brian G. Anderson and Dennis Shasha. </author> <title> Persistent Linda: Linda + transactions + query processing. </title> <editor> In J.P. Ban atre and D. Le M etayer, editors, </editor> <booktitle> Research Directions in High-Level Parallel Programming Languages, number 574 in LNCS, </booktitle> <pages> pages 93-109. </pages> <publisher> Springer, </publisher> <year> 1991. </year>
Reference-contexts: In addition, the design is in keeping with the minimalist philosophy of Linda and permits an efficient implementation. These features help distinguish FT-Linda from other efforts aimed at introducing fault-tolerance into Linda <ref> [Xu88, XL89, Kam90, AS91, Kam91, CD94, CKM92, PTHR93, JS94] </ref>. FT-Linda is being implemented using Consul, a communication substrate for building fault-tolerant systems [Mis92, MPS93b, MPS93a], and the x-kernel, an operating system kernel that provides support for composing network protocols [HP91]. <p> That is, if inp or rdp returns false, FT-Linda guarantees that there is no matching tuple in TS. Of all other distributed Linda implementations of which we are aware, only PLinda <ref> [AS91, JS94] </ref> and MOM [CD94] offer similar semantics. Strong inp/rdp semantics can be very useful because they make a strong statement about the global state of the TS and hence of the parallel application or system built using FT-Linda. <p> Transactions and Linda Two projects have added transactions to Linda. PLinda allows the programmer to combine Linda tuple space operations in a transaction, and provides for resilient processes that are automatically restarted after failure <ref> [AS91, JS94] </ref>. The programmer can choose one of two modes to ensure the resilience of TS. In the first mode, all updates a transaction makes are logged to disk before the transaction is committed.
Reference: [ASC85] <author> Amr El Abbadi, Dale Skeen, and Flaviu Cristian. </author> <title> An efficient, fault-tolerant protocol for replicated data management. </title> <booktitle> In Proceedings of the 4th ACM SIGACT/SIGMOD Conference on Principles of Database Systems, </booktitle> <year> 1985. </year> <month> 166 </month>
Reference-contexts: Processor failures and recoveries and network partitions are handled in [XL89, Xu88] using a view change algorithm based on the virtual partitions protocol in <ref> [ASC85] </ref>. This allows all workers that are in a majority partition to continue to use TS despite network partitions. [PTHR93] also implements a stable TS by replication, but uses centralized algorithms to serialize tuple operations and achieve replica consistency for single TS operations. <p> The typical way to handle such a situation is to allow only those hosts that are in a majority partition to continue <ref> [ASC85] </ref>. This ensures that there will not be multiple, divergent versions of the data, because at most one partition contains a majority of the hosts. Hosts that are in the minority must wait until they are in the majority, update their copy of the replicated data, and then proceed.
Reference: [Bal90] <author> Henri E. Bal. </author> <title> Programming Distributed Systems. </title> <publisher> Silicon Press, Summit, </publisher> <address> New Jersey, </address> <year> 1990. </year>
Reference-contexts: Parallel programming languages provide high-level abstractions for programmers to use. These generally permit a cleaner integration of the sequential and concurrent features of the language than do libraries of procedures. Examples of such languages include Ada [DoD83], Orca <ref> [Bal90] </ref>, and SR [AO93]. Libraries of procedures allow the programmer to leverage experience with an existing language and reuse existing code. They can also support many languages, allowing processes written with different languages to communicate. <p> The notion of a guarded communication statement was introduced in [Dij75]; its guard contained both an optional expression and an optional communication statement. The guarded expression has since been used in various forms in parallel programming languages such as CSP [Hoa78], Ada [DoD83], SR [AO93], and Orca <ref> [Bal90] </ref>. Recall that in a disjunctive AGS, to simplify the semantics and to fit into a programming language's type system, all guards must be the same: either all absent (true), blocking (in or rd), or boolean (inp or rdp). <p> Of course, this is applicable to many common Linda usages, most notably a distributed variable. A second example is Orca, a language that is useful for many of the same kinds of applications <ref> [BKT92, Bal90, TKB92, KMBT92] </ref>. The language is based on the shared-object model and has been implemented on both multiprocessors and distributed systems. An Orca object consists of private data and external operations that it exports.
Reference: [BHJL86] <author> Andrew P. Black, Norman Hutchinson, Eric Jul, and Henry M. Levy. </author> <title> Object structure in the emerald system. </title> <booktitle> In Proceedings of the First ACM Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <pages> pages 78-86, </pages> <address> Portland, Oregon, </address> <month> September </month> <year> 1986. </year>
Reference-contexts: RPC is a fundamental building block for distributed systems today, including client/server interactions and many distributed operating systems such as Amoeba [TM81]. It is also supported in many distributed programming languages, including Aeolus [WL86], Argus [LS83], Avalon [HW87], and Emerald <ref> [BHJL86] </ref>. Message passing allows processes on different computers to exchange messages to communicate and synchronize.
Reference: [BJ87] <author> Kenneth P. Birman and Thomas A. Joseph. </author> <title> Reliable communication in the presence of failures. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(1) </volume> <pages> 47-76, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: A multicast service delivers a message to each process in a group of processes such that the message delivered to each process in a consistent order relative to all other messages despite failures and concurrency <ref> [BJ87, CASD85, PBS89, GMS91] </ref>. This is very important in many different kinds of fault-tolerant programs, especially those constructed using the state machine approach described below. Finally, a membership service provides processes in a group consistent information about the set of functioning processors at any given time [VM90].
Reference: [Bjo92] <author> Robert D. Bjornson. </author> <title> Linda on Distributed Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Yale University, </institution> <month> Novem-ber </month> <year> 1992. </year>
Reference-contexts: These properties make Linda especially easy for use by application programmers [CS93, ACG86, CG88, CG90]. Linda implementations are available on a number of different architectures <ref> [CG86, Lei89, Bjo92, SBA93, CG93, Zen90] </ref> and for a number of different languages [Lei89, Jel90, Has92, Cia93, SC91]. Linda has been used in many real-world applications, including VLSI design, oil exploration, pharmaceutical research, fluid-flow systems, and stock purchase and analysis programs [CS93]. <p> When they return false, they are making a very strong statement about the global state of TS: they are indicating that no matching tuple exists anywhere in TS. This has historically been considered too expensive to implement for a distributed system or on a multicomputer <ref> [Lei89, Bjo92] </ref>. Thus, implementations of Linda for such systems generally do not offer inp and rdp or they offer a weaker best effort semantics. Here, inp and rdp are not guaranteed to find a matching tuple; they may return false even if a matching tuple exists in TS. <p> Also, because out is asynchronous, there is no guarantee as to when the tuple it specifies will be deposited into TS. This, coupled with a best effort semantics for inp and rdp, makes some Linda code deceptive. Consider the following example from <ref> [Lei89, Bjo92] </ref>, which is used to argue that inp and rdp should not be supported: 1 If they do need to know, a process identifier or handle can be included in the tuple. <p> Other Features Related to FT-Linda Other features similar to those provided in FT-Linda have also been proposed at various times. [Gel85] briefly introduces composed statements, which provide a form of disjunction and conjunction. <ref> [Bjo92] </ref> includes an optimization that collapses in-out pairs on the same tuple pattern; it requires restrictions similar to FT-Linda's opcodes. [Lei89] discusses the idea of multiple tuple spaces, and some of the properties that might be supported in such a scheme. <p> Consider a systolic-like algorithm to multiply Arrays A and B to obtain the product array C, a (non-fault-tolerant) Linda example given in <ref> [Bjo92] </ref>. In this scheme, each worker is responsible for computing a given part of the result matrix C. <p> As mentioned in Section 3.4, one optimization in the Linda implementation described in <ref> [Bjo92] </ref> collapses an in and an out into one operation at the TS manager. However, in order for the compiler to be able to apply this optimization, the in and out have to use only simple calculations very similar to FT-Linda's opcodes. <p> Subsequent columns gives the marginal cost of including different types of in or out operations in the body. We note that the i386 figures are comparable to results reported elsewhere <ref> [Bjo92] </ref>. This is encouraging for two reasons. First, FT-Linda is largely unoptimized, while the work in [Bjo92] is based on a highly optimized implementation. Second, we have augmented the functionality of Linda, not just reimplemented existing functionality. <p> Subsequent columns gives the marginal cost of including different types of in or out operations in the body. We note that the i386 figures are comparable to results reported elsewhere <ref> [Bjo92] </ref>. This is encouraging for two reasons. First, FT-Linda is largely unoptimized, while the work in [Bjo92] is based on a highly optimized implementation. Second, we have augmented the functionality of Linda, not just reimplemented existing functionality.
Reference: [BKT92] <author> Henri E. Bal, M. Frans Kaashoek, and Andrew S. Tanenbaum. Orca: </author> <title> A language for parallel programming of distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3) </volume> <pages> 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Of course, this is applicable to many common Linda usages, most notably a distributed variable. A second example is Orca, a language that is useful for many of the same kinds of applications <ref> [BKT92, Bal90, TKB92, KMBT92] </ref>. The language is based on the shared-object model and has been implemented on both multiprocessors and distributed systems. An Orca object consists of private data and external operations that it exports.
Reference: [BLL94] <author> Ralph M. Butler, Alan L. Leveton, and Ewing L. Lusk. p4-linda: </author> <title> A portable implementation of linda. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 50-58, </pages> <address> Spokane, Washington, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: However, problems with its semantics and its lack of facilities to handle failures makes it inappropriate for long-running scientific applications and dependable systems, domains for which it would otherwise be suitable <ref> [WL88, CGM92, PTHR93, BLL94] </ref>. This chapter provides the background information and motivation for our extensions to Linda. First, it gives an overview of Linda. Next, it discusses semantic limitations of Linda that are a problem even in the absence of failures.
Reference: [BMST92] <author> Navin Budhiraja, Keith Marzullo, Fred B. Schneider, and Sam Toueg. </author> <title> Primary-backup protocols: Lower bounds and optimal implementations. </title> <booktitle> In Proceedings of the Third IFIP Working Conference on Dependable Computing for Critical Applications, </booktitle> <pages> pages 187-198, </pages> <address> Mondello, Italy, </address> <year> 1992. </year>
Reference-contexts: They are also recoverable, i.e. executed completely or not at all. These actions are called transactions in the database context. The primary/backup paradigm features a service implemented by multiple processes; the primary process is active, while the backup processes are passive <ref> [AD76, BMST92] </ref>. Only the active process responds to requests for the service; in the case of the failure of the primary process, one backup process will become the primary, starting from a checkpointed state. A replicated state machine consists of a collection of servers [Sch90]. <p> First, it gives a replicated server example. This is an example of programming replicated state machines in FT-Linda. Next, it presents an FT-Linda recoverable server an example of the primary/backup approach <ref> [AD76, BMST92] </ref>. Since the server is not replicated, there are no redundant server computations in the absence of failures. Finally, this section presents an FT-Linda implementation of a transaction facility.
Reference: [BN84] <author> Andrew D. Birrell and Bruce Jay Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: Examples of such languages include Ada [DoD83], Orca [Bal90], and SR [AO93]. Libraries of procedures allow the programmer to leverage experience with an existing language and reuse existing code. They can also support many languages, allowing processes written with different languages to communicate. Remote procedure call (RPC) <ref> [Nel81, BN84] </ref> is like a normal procedure call, except the invocation statement and the procedure body are executed by two different processes, potentially on different machines. RPC is a fundamental building block for distributed systems today, including client/server interactions and many distributed operating systems such as Amoeba [TM81]. <p> For example, Figure 5.8 demonstrates the differences in the processing of an AGS. Rather than requests being submitted to Consul directly from the FT-Linda library, a remote procedure call (RPC) <ref> [Nel81, BN84] </ref> would be used to forward the request to a request handler process on a tuple server. This handler immediately submits it to Consul's multicast service as before.
Reference: [BS91] <author> David E. Bakken and Richard D. Schlichting. </author> <title> Tolerating failures in the bag-of-tasks programming paradigm. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 248-255, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: We use the latter approach, which is also the replicated state machine approach. CHAPTER 3 FT-LINDA FT-Linda is a variant of Linda designed to facilitate the construction of fault-tolerant applications by including features for tuple stability, multi-op atomicity, and strong semantics <ref> [BS91, BS94, SBT94] </ref>. The system model assumed by the language consists of a collection of processors connected by a network with no physically shared memory.
Reference: [BS94] <author> David E. Bakken and Richard D. Schlichting. </author> <title> Supporting fault-tolerant parallel programming in Linda. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1994. To appear. </note>
Reference-contexts: We use the latter approach, which is also the replicated state machine approach. CHAPTER 3 FT-LINDA FT-Linda is a variant of Linda designed to facilitate the construction of fault-tolerant applications by including features for tuple stability, multi-op atomicity, and strong semantics <ref> [BS91, BS94, SBT94] </ref>. The system model assumed by the language consists of a collection of processors connected by a network with no physically shared memory.
Reference: [BSS91] <author> Kenneth Birman, Andr e Schiper, and Pat Stepheson. </author> <title> Lightweight causal and atomic group multicast. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(3) </volume> <pages> 272-314, </pages> <month> August </month> <year> 1991. </year> <month> 167 </month>
Reference-contexts: If the commands are deterministic and executed atomically with respect to concurrent access, then the state variables of each replica will remain consistent. The SMA is the basis for a large number of fault-tolerant distributed systems <ref> [BSS91, MPS93a, Pow91] </ref>. 2 This ordering can be relaxed in some cases; see [Sch90]. 36 The SMA is illustrated in Figure 2.3. Here the M clients use atomic multicast to submit commands to the N replicas of the state machine. <p> It also notifies the FT-Linda runtime of processor failures so that failure tuples can be deposited into the TS specified by the user application. While the implementation has been designed with Consul in mind, we note that any system that provides similar functionality (e.g., Isis <ref> [BSS91] </ref>) could be used instead. The runtime structure of a system consisting of N host processors is shown in Figure 5.1. At the top are the user processes, which consist of the generated code together with the FT-Linda library. Then comes the TS state machine, Consul, and the interconnect structure.
Reference: [CASD85] <author> Flaviu Cristian, Houtan Aghili, Ray Strong, and Danny Dolev. </author> <title> Atomic broadcast: From simple message diffusion to Byzantine agreement. </title> <booktitle> In Proceedings of the Fifteenth International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 200-206. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> June </month> <year> 1985. </year>
Reference-contexts: The weakest failure model is Byzantine or arbitrary [LSP82]. Here components may fail in arbitrary ways. The timing failure model assumes a component will respond to an input with the correct value, but not necessarily within a given timing specification <ref> [CASD85] </ref>. The omission failure model assumes that a component may fail to respond to an input [CASD85]. A stronger model yet is crash or fail-silent, where a component fails without making any incorrect state transitions [PSB + 88]. <p> Here components may fail in arbitrary ways. The timing failure model assumes a component will respond to an input with the correct value, but not necessarily within a given timing specification <ref> [CASD85] </ref>. The omission failure model assumes that a component may fail to respond to an input [CASD85]. A stronger model yet is crash or fail-silent, where a component fails without making any incorrect state transitions [PSB + 88]. The strongest model is fail-stop, which adds to the crash model the assumption that the component fails in a way that is detectable by other components [SS83]. <p> A multicast service delivers a message to each process in a group of processes such that the message delivered to each process in a consistent order relative to all other messages despite failures and concurrency <ref> [BJ87, CASD85, PBS89, GMS91] </ref>. This is very important in many different kinds of fault-tolerant programs, especially those constructed using the state machine approach described below. Finally, a membership service provides processes in a group consistent information about the set of functioning processors at any given time [VM90].
Reference: [CBZ91] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <year> 1991. </year>
Reference-contexts: DSMs thus allow distributed systems and multicomputers to be programmed much more like a multiprocessor. Munin is an example of a system that implements a DSM <ref> [CBZ91] </ref>. Coordination languages augment an existing computational language such as FORTRAN or C to allow different processes to communicate in a uniform fashion with each other and with their environment [GC92]. A coordination language is completely orthogonal to the computational language.
Reference: [CD94] <author> Scott R. Cannon and David Dunn. </author> <title> Adding fault-tolerant transaction processing to Linda. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 24(5) </volume> <pages> 449-466, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: In addition, the design is in keeping with the minimalist philosophy of Linda and permits an efficient implementation. These features help distinguish FT-Linda from other efforts aimed at introducing fault-tolerance into Linda <ref> [Xu88, XL89, Kam90, AS91, Kam91, CD94, CKM92, PTHR93, JS94] </ref>. FT-Linda is being implemented using Consul, a communication substrate for building fault-tolerant systems [Mis92, MPS93b, MPS93a], and the x-kernel, an operating system kernel that provides support for composing network protocols [HP91]. <p> That is, if inp or rdp returns false, FT-Linda guarantees that there is no matching tuple in TS. Of all other distributed Linda implementations of which we are aware, only PLinda [AS91, JS94] and MOM <ref> [CD94] </ref> offer similar semantics. Strong inp/rdp semantics can be very useful because they make a strong statement about the global state of the TS and hence of the parallel application or system built using FT-Linda. <p> Further, as discussed above, many applications do not need the overhead of a resilient process. (This overhead seems to be much less than that in [Kam90, Kam91], however, in part because PLinda provides a mechanism for a process to log its private state.) MOM provides a kind of lightweight transaction <ref> [CD94] </ref>. It extends in to return a tuple identifier and out to include the identifier of its parent tuple (e.g., the subtask it was generated by).
Reference: [CG86] <author> Nicholas Carriero and David Gelernter. </author> <title> The S/Net's Linda kernel. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(2) </volume> <pages> 110-129, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: These properties make Linda especially easy for use by application programmers [CS93, ACG86, CG88, CG90]. Linda implementations are available on a number of different architectures <ref> [CG86, Lei89, Bjo92, SBA93, CG93, Zen90] </ref> and for a number of different languages [Lei89, Jel90, Has92, Cia93, SC91]. Linda has been used in many real-world applications, including VLSI design, oil exploration, pharmaceutical research, fluid-flow systems, and stock purchase and analysis programs [CS93].
Reference: [CG88] <author> Nicholas Carriero and David Gelernter. </author> <title> Applications experience with Linda. </title> <journal> ACM SIGPLAN Notices (Proc. ACM SIGPLAN PPEALS), </journal> <volume> 23(9) </volume> <pages> 173-187, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Tuple space also provides temporal decoupling, because communicating processes do not have to exist at the same time, and spatial decoupling, because processes do not have to know each other's identity. These properties make Linda especially easy for use by application programmers <ref> [CS93, ACG86, CG88, CG90] </ref>. Linda implementations are available on a number of different architectures [CG86, Lei89, Bjo92, SBA93, CG93, Zen90] and for a number of different languages [Lei89, Jel90, Has92, Cia93, SC91]. <p> true do in (subtask,?subtask args) calc (subtask args; var result args) for (all new subtasks created by this subtask) out (subtask, new subtask args) out (result,result args) end while end proc Bag-of-Tasks Linda lends itself nicely to a method of parallel programming called the bag-of-tasks or replicated worker programming paradigm <ref> [ACG86, CG88] </ref>. In this paradigm, the task to be solved is partitioned into independent subtasks.
Reference: [CG89] <author> Nicholas Carriero and David Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: a process to perform I/O, communicate with a user, or communicate directly with another process, regardless of the location and lifetime of that process. 1.4 Linda Linda is a coordination language that provides a collection of primitives for process creation and interprocess communication that can be added to existing languages <ref> [Gel85, CG89] </ref>. The main abstraction provided by Linda is tuple space (TS), an associative shared memory. The abstraction is implemented by the Linda implementation transparently to user processes. <p> These design choices are tightly intertwined with the particular language extensions we offer and with the way in which we implement them, topics for Chapters 3 and 5, respectively. 2.1 Linda Overview Linda is a parallel programming language based on a communication abstraction known as tuple space (TS) <ref> [Gel85, ACG86, CG89, GC92, CG90] </ref>. Tuple space is an associative (i.e., content-addressable) unordered bag of data elements called tuples. Processes are created in the context of a given TS, which they use as a means for communicating and synchronizing.
Reference: [CG90] <author> Nicholas Carriero and David Gelernter. </author> <title> How to Write Parallel Programs: </title>
Reference-contexts: Tuple space also provides temporal decoupling, because communicating processes do not have to exist at the same time, and spatial decoupling, because processes do not have to know each other's identity. These properties make Linda especially easy for use by application programmers <ref> [CS93, ACG86, CG88, CG90] </ref>. Linda implementations are available on a number of different architectures [CG86, Lei89, Bjo92, SBA93, CG93, Zen90] and for a number of different languages [Lei89, Jel90, Has92, Cia93, SC91]. <p> These design choices are tightly intertwined with the particular language extensions we offer and with the way in which we implement them, topics for Chapters 3 and 5, respectively. 2.1 Linda Overview Linda is a parallel programming language based on a communication abstraction known as tuple space (TS) <ref> [Gel85, ACG86, CG89, GC92, CG90] </ref>. Tuple space is an associative (i.e., content-addressable) unordered bag of data elements called tuples. Processes are created in the context of a given TS, which they use as a means for communicating and synchronizing.
References-found: 28

