URL: http://www.cs.purdue.edu/coast/archive/clife/EA/papers/kras91.ps.gz
Refering-URL: http://www.cs.purdue.edu/coast/archive/clife/EA/papers/
Root-URL: http://www.cs.purdue.edu
Title: Global Optimization by Means of Evolutionary Algorithms  
Phone: 50  
Author: Thomas Back, Frank Hoffmeister 
Address: P.O. Box 50 05 00, W-4600 Dortmund  
Affiliation: University of Dortmund, Department of Computer Science, Chair XI,  
Abstract-found: 0
Intro-found: 1
Reference: [Bak85] <author> James Edward Baker. </author> <title> Adaptive selection methods for genetic algorithms. </title> <editor> In J. J. Grefenstette, editor, </editor> <booktitle> Proceedings of the first international conference on genetic algorithms and their applications, </booktitle> <pages> pages 101-111, </pages> <address> Hillsdale, New Jersey, 1985. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding [Jon75] and generation gap [Gre86], sharing [DG89, GR87], ranking <ref> [Bak85] </ref>, and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Summary In the course of a closer comparison of the general working scheme it turns out that GAs and ESs are nearly identical.
Reference: [CES89] <editor> Richard A. Caruna, Larry J. Eshelman, and J. David Schaffer. </editor> <title> Representation and hidden bias II: Eliminating defining length bias in genetic search via shu*e crossover. </title> <editor> In N. S. Sridharan, editor, </editor> <booktitle> Eleventh international joint conference on artificial intelligence, </booktitle> <pages> pages 750-755. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> August </month> <year> 1989. </year>
Reference-contexts: Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding [Jon75] and generation gap [Gre86], sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover <ref> [CES89, ECS89, SCED89] </ref>. 4 Summary In the course of a closer comparison of the general working scheme it turns out that GAs and ESs are nearly identical.
Reference: [DG89] <editor> Kalyanmoy Deb and David E. Goldberg. </editor> <title> An investigation of niche and species formation in genetic function optimization. </title> <editor> In J. David Schaffer, editor, </editor> <booktitle> Proceedings of the third international conference on genetic algorithms and their applications, </booktitle> <pages> pages 42-50. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1989. </year>
Reference-contexts: This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding [Jon75] and generation gap [Gre86], sharing <ref> [DG89, GR87] </ref>, ranking [Bak85], and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Summary In the course of a closer comparison of the general working scheme it turns out that GAs and ESs are nearly identical.
Reference: [ECS89] <editor> Larry L. Eshelman, Richard A. Caruna, and J. David Schaffer. </editor> <title> Biases in the crossover landscape. </title> <editor> In J. David Schaffer, editor, </editor> <booktitle> Proceedings of the third international conference on genetic algorithms and their applications, </booktitle> <pages> pages 10-19. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1989. </year>
Reference-contexts: Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding [Jon75] and generation gap [Gre86], sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover <ref> [CES89, ECS89, SCED89] </ref>. 4 Summary In the course of a closer comparison of the general working scheme it turns out that GAs and ESs are nearly identical.
Reference: [GB89] <author> John J. Grefenstette and James E. Baker. </author> <title> How genetic algorithms work: A critical look at implicit parallelism. </title> <editor> In J. David Schaffer, editor, </editor> <booktitle> Proceedings of the third international conference on genetic algorithms and their applications, </booktitle> <pages> pages 20-27. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1989. </year>
Reference-contexts: This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values <ref> [Jon75, Gol89, GB89] </ref>, crowding [Jon75] and generation gap [Gre86], sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Summary In the course of a closer comparison of the general working scheme it turns out that GAs and ESs are nearly identical.
Reference: [Gol89] <author> David E. Goldberg. </author> <title> Genetic algorithms in search, optimization and machine learning. </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction Evolution Strategies (ESs) [Rec73, Sch81a] and Genetic Algorithms (GAs) <ref> [Hol75, Gol89] </ref> are two types of Evolutionary Algorithms. This term denotes algorithms which rely upon the imitation of the collective learning paradigm of natural populations, including the features of selection, mutation, and recombination. <p> fitness function t : I ! f0; 1g termination criterion The GA is working on a genotype-level of binary encoded individuals, a choice, which is founded by the argument of maximizing the number of schemata (hyperplanes of varying dimension in the l-dimensional space) available with respect to a given code <ref> [Hol75, Gol89] </ref>. P 0 is the randomly generated initial population, and the parameters and l describe the number of individuals representing one generation and the length of the `genetic' representation of each individual, respectively. <p> This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values <ref> [Jon75, Gol89, GB89] </ref>, crowding [Jon75] and generation gap [Gre86], sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Summary In the course of a closer comparison of the general working scheme it turns out that GAs and ESs are nearly identical.
Reference: [GR87] <author> David E. Goldberg and J. Richardson. </author> <title> Genetic algorithms with sharing for multimodal function optimization. </title> <editor> In J. J. Grefenstette, editor, </editor> <booktitle> Genetic Algorithms and their Applications, </booktitle> <pages> pages 41-49, </pages> <address> Hillsdale, New Jersey, 1987. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding [Jon75] and generation gap [Gre86], sharing <ref> [DG89, GR87] </ref>, ranking [Bak85], and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Summary In the course of a closer comparison of the general working scheme it turns out that GAs and ESs are nearly identical.
Reference: [Gre86] <author> John J. Grefenstette. </author> <title> Optimization of control parameters for genetic algorithms. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> SMC-16(1):122-128, </volume> <year> 1986. </year>
Reference-contexts: This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding [Jon75] and generation gap <ref> [Gre86] </ref>, sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Summary In the course of a closer comparison of the general working scheme it turns out that GAs and ESs are nearly identical.
Reference: [HB90] <author> Frank Hoffmeister and Thomas Back. </author> <title> Genetic algorithms and evolution strategies: Similarities and differences. </title> <type> Technical Report "Grune Reihe" No. 365, </type> <institution> Department of Computer Science, University of Dortmund, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: It will become obvious, that they have many important features in common, and the most distinguishing property is the self-learning mechanism used by ESs to adapt their strategy parameters during the optimum search (adaptive search). A more detailed comparison (also in an experimental way) can be found in <ref> [HB90] </ref>. 2 Evolution Strategies Neglecting the earlier steps in the development of ESs ((1+1)-ES [Rec73]) it is concentrated here on the (+)-ES and the (,)-ES as developed by Schwefel [Sch77, Sch81a]. They provide the capability of self-adaptation of the strategy parameters. <p> However, besides some less important differences, ESs benefit from their capability of learning on two different levels, i.e. the level of the population and the level of the strategy parameters. A experimental comparison clarifies the power of this problem-depending adaptation capability of the algorithm <ref> [HB90] </ref>. The most important differences are summarized in table 1.
Reference: [Hol75] <author> John H. Holland. </author> <title> Adaptation in natural and artificial systems. </title> <publisher> The University of Michigan Press, </publisher> <address> Ann Arbor, </address> <year> 1975. </year>
Reference-contexts: 1 Introduction Evolution Strategies (ESs) [Rec73, Sch81a] and Genetic Algorithms (GAs) <ref> [Hol75, Gol89] </ref> are two types of Evolutionary Algorithms. This term denotes algorithms which rely upon the imitation of the collective learning paradigm of natural populations, including the features of selection, mutation, and recombination. <p> With a growing ratio of = the bias towards local search is shifted towards global search, which in the limit of a (,)-ES is nothing but random walk. 3 Genetic Algorithms Abstracting from the work of Holland <ref> [Hol75] </ref>, a GA can be formulated as an 8-tuple: GA = (P 0 ; ; l; s; ; ; f; t) (6) where P 0 = (a 0 ) 2 I I = f0; 1g l initial population 2 N population size l 2 N length of individuals' representation s : <p> fitness function t : I ! f0; 1g termination criterion The GA is working on a genotype-level of binary encoded individuals, a choice, which is founded by the argument of maximizing the number of schemata (hyperplanes of varying dimension in the l-dimensional space) available with respect to a given code <ref> [Hol75, Gol89] </ref>. P 0 is the randomly generated initial population, and the parameters and l describe the number of individuals representing one generation and the length of the `genetic' representation of each individual, respectively. <p> This selection scheme is called proportional selection <ref> [Hol75] </ref>. It leads to the expectation of individual a t i to occur t i = p s (a t i ) times in generation t + 1 (generally t i is called the expected value of a t i ). <p> determines an operator ! t i 2 for each individual a 0t i 2 P 0t which will be applied to this individual: (a 0t i ) = ! t The genetic operator set f! : I fi I ! P ! Ig includes genetic operators like crossover and mutation <ref> [Hol75] </ref>. The stochastic elements of these operators (application probabilities, e.g. p m 0:001 and p c 0:6, selection of loci) are included in a somehow abstract manner in the probability distribution p 2 P.
Reference: [HS90] <author> Frank Hoffmeister and Hans-Paul Schwefel. </author> <title> A taxonomy of parallel evolutionary algorithms. </title> <editor> In G. Wolf, T. Legendi, and U. Scchendel, editors, </editor> <booktitle> ParCella 90, volume 2 of Research in Informatics, </booktitle> <pages> pages 97-107, </pages> <address> East-Berlin, GDR, 1990. </address> <publisher> Akademie Verlag. </publisher>
Reference-contexts: In general, this topic has been discussed elsewhere <ref> [HS90] </ref>, but we would like to emphasize at least on the possibility of parallelism on different levels (parallel individuals, parallel populations), synchronously or asynchronously.
Reference: [Jon75] <author> Kenneth De Jong. </author> <title> An analysis of the behaviour of a class of genetic adaptive systems. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1975. </year> <note> Diss. Abstr. Int. 36(10), 5140B, University Microfilms No. 76-9381. </note>
Reference-contexts: This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values <ref> [Jon75, Gol89, GB89] </ref>, crowding [Jon75] and generation gap [Gre86], sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Summary In the course of a closer comparison of the general working scheme it turns out that GAs and ESs are nearly identical. <p> This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding <ref> [Jon75] </ref> and generation gap [Gre86], sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Summary In the course of a closer comparison of the general working scheme it turns out that GAs and ESs are nearly identical.
Reference: [Rec73] <author> Ingo Rechenberg. </author> <title> Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologis-chen Evolution. </title> <publisher> Frommann-Holzboog Verlag, Stuttgart, </publisher> <year> 1973. </year>
Reference-contexts: 1 Introduction Evolution Strategies (ESs) <ref> [Rec73, Sch81a] </ref> and Genetic Algorithms (GAs) [Hol75, Gol89] are two types of Evolutionary Algorithms. This term denotes algorithms which rely upon the imitation of the collective learning paradigm of natural populations, including the features of selection, mutation, and recombination. <p> A more detailed comparison (also in an experimental way) can be found in [HB90]. 2 Evolution Strategies Neglecting the earlier steps in the development of ESs ((1+1)-ES <ref> [Rec73] </ref>) it is concentrated here on the (+)-ES and the (,)-ES as developed by Schwefel [Sch77, Sch81a]. They provide the capability of self-adaptation of the strategy parameters.
Reference: [SCED89] <author> J. David Schaffer, Richard A. Caruna, Larry J. Eshelman, and Rajarshi Das. </author> <title> A study of control parameters affecting online performance of genetic algorithms for function optimization. </title> <editor> In J. David Schaffer, editor, </editor> <booktitle> Proceedings of the third international conference on genetic algorithms and their applications, </booktitle> <pages> pages 51-60. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1989. </year>
Reference-contexts: Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding [Jon75] and generation gap [Gre86], sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover <ref> [CES89, ECS89, SCED89] </ref>. 4 Summary In the course of a closer comparison of the general working scheme it turns out that GAs and ESs are nearly identical.
Reference: [Sch77] <editor> Hans-Paul Schwefel. </editor> <title> Numerische Optimierung von Computer-Modellen mittels der Evolutionsstrategie. Interdisciplinary systems research; 26. </title> <publisher> Birkhauser, </publisher> <address> Basel, FRG, </address> <year> 1977. </year>
Reference-contexts: A more detailed comparison (also in an experimental way) can be found in [HB90]. 2 Evolution Strategies Neglecting the earlier steps in the development of ESs ((1+1)-ES [Rec73]) it is concentrated here on the (+)-ES and the (,)-ES as developed by Schwefel <ref> [Sch77, Sch81a] </ref>. They provide the capability of self-adaptation of the strategy parameters.
Reference: [Sch81a] <editor> Hans-Paul Schwefel. </editor> <title> Numerical Optimization of Computer Models. </title> <publisher> Wiley, </publisher> <address> Chichester, </address> <year> 1981. </year>
Reference-contexts: 1 Introduction Evolution Strategies (ESs) <ref> [Rec73, Sch81a] </ref> and Genetic Algorithms (GAs) [Hol75, Gol89] are two types of Evolutionary Algorithms. This term denotes algorithms which rely upon the imitation of the collective learning paradigm of natural populations, including the features of selection, mutation, and recombination. <p> Due to their complexity, both algorithms lack the existence of mathematical proofs of general convergence and efficiency (with the exception of two simple, but with respect to their topological surfaces important objective functions in case of a simplified ES <ref> [Sch81a] </ref>). In this paper a uniform formal description of these algorithms is presented, and it is emphasized on their essential features. <p> A more detailed comparison (also in an experimental way) can be found in [HB90]. 2 Evolution Strategies Neglecting the earlier steps in the development of ESs ((1+1)-ES [Rec73]) it is concentrated here on the (+)-ES and the (,)-ES as developed by Schwefel <ref> [Sch77, Sch81a] </ref>. They provide the capability of self-adaptation of the strategy parameters.
Reference: [Sch81b] <editor> Hans-Paul Schwefel. </editor> <title> Optimum seeking methods: Subroutines for the minimization of non-linear functions of several variables by means of direct (derivative-free) methods. </title> <institution> Interner Bericht KFA-STE-IB-7/81, Kernforschungsanlage Julich GmbH, Julich, </institution> <address> FRG, </address> <month> October </month> <year> 1981. </year>
Reference-contexts: The individuals of P 0t are obtained by applying recombination and mutation, where prior to its mutation an offspring is produced by recombining two parents a = (x a ; a ) and b = (x b ; b ) 2 I <ref> [Sch81b] </ref>: r (P t ) = a 0 = (x 0 ; 0 ) 2 I v 0 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; : v a;i (A) no recombination v a;i or v b;i (B) discrete 1 2 (v a;i + v b;i ) (C) intermediate v a
References-found: 17

