URL: http://www.research.microsoft.com/~hoppe/vbr_eurographics_rendering97.ps.gz
Refering-URL: http://www.research.microsoft.com/~hoppe/
Root-URL: http://www.research.microsoft.com
Title: View-based Rendering: Visualizing Real Objects from Scanned Range and Color Data  
Author: Kari Pulli Michael Cohen Tom Duchamp Hugues Hoppe Linda Shapiro Werner Stuetzle 
Address: Seattle, WA  Redmond, WA  
Affiliation: University of Washington,  Microsoft Research,  
Abstract: Modeling arbitrary real objects is difficult and rendering textured models typically does not result in realistic images. We describe a new method for displaying scanned real objects, called view-based rendering. The method takes as input a collection of colored range images covering the object and creates a collection of partial object models. These partial models are rendered separately using traditional graphics hardware and blended together using various weights and soft z-buffering. We demonstrate interactive viewing of real, non-trivial objects that would be difficult to model using traditional methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. E. Chen. </author> <title> Quicktime VR an image-based approach to virtual environment navigation. </title> <booktitle> In SIGGRAPH 95 Conference Proceedings, </booktitle> <pages> pages 29-38. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: For exact implementation two passes are required: first find minimum reliable z, then blend using soft threshold based on that minimum z. 5 Related work Chen <ref> [1] </ref> and McMillan and Bishop [15] modeled environments by storing the light field function around a point. The rays visible from a point are texture mapped to a cylinder around that point, and any horizontal view can be created by warping a portion of the cylinder to the image plane.
Reference: [2] <author> S. E. Chen and L. Williams. </author> <title> View interpolation for image synthesis. </title> <booktitle> In Computer Graphics (SIGGRAPH '93 Proceedings), </booktitle> <volume> volume 27, </volume> <pages> pages 279-288, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: The pixel correspondence mapping between the input images is not easy to do reliably, especially within regions of homogeneous color. But fortunately, the regions where such pixels project have almost constant color, so a projection error of a few pixels typically does not cause visible artifacts. Chen and Williams <ref> [2] </ref> used similar methods to trade unbounded scene complexity to bounded image complexity. They render a large number of views of a complicated scene and obtain accurate pixel correspondences from depth values that are stored in addition to the color at each pixel. <p> Their proposed system would remotely render images from geometric models at 5 frames/sec and send them to a local computer that warps and interpolates two consecutive frames at about 60 frames/sec. The 3D warp is done as in <ref> [2] </ref>. Using the z-values at each pixel a dense triangle mesh is constructed for the two views between which the interpolation is performed. Normal vectors and z-values at each pixel are used to locate false connections across a step edge between an occluding and occluded surface.
Reference: [3] <author> Y. Chen and G. Medioni. </author> <title> Object modelling by registration of multiple range images. </title> <journal> Image and Vision Computing, </journal> <volume> 10(3) </volume> <pages> 145-155, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: We perform the initial registration interactively by marking identifiable object features in the color images. This initial registration is refined using Chen and Medioni's registration method <ref> [3] </ref> modified to deal with multiple data sets simulta-neously. Triangle mesh creation. We currently create the triangle meshes interactively. The user marks the boundaries of the object by inserting points into the color image, while the software incrementally updates a Delaunay triangulation of the vertices.
Reference: [4] <author> B. Curless and M. Levoy. </author> <title> A volumetric method for building complex models from range images. </title> <booktitle> In SIGGRAPH 96 Conference Proceedings, </booktitle> <pages> pages 303-312. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1996. </year>
Reference-contexts: Despite recent advances <ref> [4, 16] </ref>, automatically creating accurate surface models of complex objects (step 3) is still a difficult task, while the computation of accurate reflection models (step 4) has hardly been addressed.
Reference: [5] <author> L. Darsa, B. C. Silva, and A. Varshney. </author> <title> Navigating static environments using image-space simplification and morphing. </title> <booktitle> In Proc. 1997 Symposium on Interactive 3D Graphics, </booktitle> <pages> pages 25-34, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: We set w = cos . Darsa et al. <ref> [5] </ref> use a similar weight. The third weight w fl which we call the blend weight, is designed to smoothly blend the meshes at their boundaries. <p> Using the z-values at each pixel a dense triangle mesh is constructed for the two views between which the interpolation is performed. Normal vectors and z-values at each pixel are used to locate false connections across a step edge between an occluding and occluded surface. Darsa et al. <ref> [5] </ref> describe another approach for rapidly displaying complicated environments. The virtual environment is divided into cubes. From the center of each cube, six views (one for each face of the cube) are rendered.
Reference: [6] <author> P. E. Debevec, C. J. Taylor, and J. Malik. </author> <title> Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. </title> <booktitle> In SIGGRAPH 96 Conference Proceedings, </booktitle> <pages> pages 11-20. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1996. </year>
Reference-contexts: Whereas w fi is associated with a view, and w with the triangles approximat-ing the geometry of the view, w fl is associated with color texture of the view. A similar weight was used by Debevec et al. <ref> [6] </ref>. Most self-occlusions are handled during rendering of individual views using back-face culling and z-buffering. When combining the view-based partial models, part of one view's model may occlude part of another view's model. <p> However, rather than morphing the precomputed images, they reproject them pixel by pixel. Shade et al. [17] partition the geometric primitives in the scene, render images of them, and texture map the images onto quadrilaterals, which are displayed instead of the geometry. Debevec et al. <ref> [6] </ref> developed a system that fits user-generated geometric models of buildings to digitized images by interactively associating image features with model features and fit-ting model parameters to images. The buildings are view-dependently texture mapped using the color images.
Reference: [7] <author> T. Evgeniou. </author> <title> Image based rendering using algebraic techniques. </title> <type> Technical Report A.I. Memo No. 1592, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: A disadvantage is the difficulty of storing and accessing the enormous lumigraph representation. The algebraic approach to image-based rendering using pairs of images and pixel correspondences in the two images was introduced by Laveau and Faugeras [11]. It has since been used in several other systems <ref> [15, 18, 7] </ref>. Given correct dense pixel correspondences one can calculate the 3D coordinates of surface points visible in both images, and then project these to the image plane of the virtual camera. However, the projection can also be calculated directly without 3D reconstruction.
Reference: [8] <author> M. Garland and P. Heckbert. </author> <title> Fast polygonal approximation of terrains and height fields. </title> <type> Technical Report CMU-CS-95-181, </type> <institution> Dept. of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1995. </year>
Reference-contexts: First, we place a blue cloth to the background and scan the empty scene. Points whose geometry and color match the data scanned from the empty scene are classified as background. The adding of vertices is easily automated. For example, Garland and Heckbert <ref> [8] </ref> add vertices to image coordinates where the current approximation is worst. The drawback of this approach is that if the data contains step edges due to self-occlusions, the mesh is likely to become unnecessarily dense before a good approximation is achieved.
Reference: [9] <author> S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen. </author> <booktitle> The lumigraph. In SIGGRAPH 96 Conference Proceedings, </booktitle> <pages> pages 43-54. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1996. </year>
Reference-contexts: If we are content with rendering individual objects from some standoff distance, it suffices to know the light field function for the subset of IR 3 fi S 2 of inward rays originating from points on a convex surface M that encloses the object. Following Gortler et al. <ref> [9] </ref>, we call this simpler function a lumigraph. We call the surface M that encloses the object the lumigraph surface. Figure 2 shows a schematic of the lumi-graph domain for the case where the lumigraph surface is a sphere. <p> Both systems allow limited rotations about a vertical axis, but they do not support continuous translation of the viewpoint. Levoy and Hanrahan [12] and Gortler et al. <ref> [9] </ref> developed image synthesis systems that use a lumigraph and that support continuous translation and rotation of the view point. In fact, the term lumigraph that we use to describe the 4D slice of the light field is borrowed from [9]. <p> Levoy and Hanrahan [12] and Gortler et al. <ref> [9] </ref> developed image synthesis systems that use a lumigraph and that support continuous translation and rotation of the view point. In fact, the term lumigraph that we use to describe the 4D slice of the light field is borrowed from [9]. Both systems use a cube surrounding the object as the lumigraph surface. To create a lumigraph from digitized images of a real object, Levoy and Hanra-han moved the camera in a regular pattern into a known set of positions, and projected the camera images back to the lumigraph cube.
Reference: [10] <author> H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. </author> <title> Mesh optimization. </title> <booktitle> In Computer Graphics (SIGGRAPH '93 Proceedings), </booktitle> <volume> volume 27, </volume> <pages> pages 19-26, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: The drawback of this approach is that if the data contains step edges due to self-occlusions, the mesh is likely to become unnecessarily dense before a good approximation is achieved. To prevent this we perform a mesh simplification step using the mesh optimization methods by Hoppe et al. <ref> [10] </ref>. 4.2 Rendering We have built an interactive viewer for viewing the reconstructed images (see Figure 11). For each frame, we find three views whose view directions surround the current view direction on a unit sphere.
Reference: [11] <author> S. Laveau and O. D. Faugeras. </author> <title> 3-d scene representation as a collection of images and fundamental matrices. </title> <type> Technical Report RR 2205, </type> <institution> INRIA, France, </institution> <year> 1994. </year> <note> Available from ftp://ftp.inria.fr/INRIA/tech-reports/RR/RR-2205.ps.gz. </note>
Reference-contexts: A disadvantage is the difficulty of storing and accessing the enormous lumigraph representation. The algebraic approach to image-based rendering using pairs of images and pixel correspondences in the two images was introduced by Laveau and Faugeras <ref> [11] </ref>. It has since been used in several other systems [15, 18, 7]. Given correct dense pixel correspondences one can calculate the 3D coordinates of surface points visible in both images, and then project these to the image plane of the virtual camera.
Reference: [12] <author> M. Levoy and P. Hanrahan. </author> <title> Light field rendering. </title> <booktitle> In SIGGRAPH 96 Conference Proceedings, </booktitle> <pages> pages 31-42. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1996. </year>
Reference-contexts: A useful abstraction in this context is the light field function (also known as the plenoptic function). Levoy and Hanrahan <ref> [12] </ref> define the light field as the radiance at a point in a given direction. <p> Both systems allow limited rotations about a vertical axis, but they do not support continuous translation of the viewpoint. Levoy and Hanrahan <ref> [12] </ref> and Gortler et al. [9] developed image synthesis systems that use a lumigraph and that support continuous translation and rotation of the view point. In fact, the term lumigraph that we use to describe the 4D slice of the light field is borrowed from [9].
Reference: [13] <author> W. R. Mark, L. McMillan, and G. Bishop. </author> <title> Post-rendering 3d warping. </title> <booktitle> In Proc. 1997 Symposium on Interactive 3D Graphics, </booktitle> <pages> pages 7-16, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: The buildings are view-dependently texture mapped using the color images. The interpolation between different texture maps is improved by determining more accurate surface geometry using stereo from several input images and morphing the texture map accordingly. Two recent papers use similar techniques to ours. Mark et al. <ref> [13] </ref> investigate the use of image-based rendering to increase the frame rate for remotely viewing virtual worlds. Their proposed system would remotely render images from geometric models at 5 frames/sec and send them to a local computer that warps and interpolates two consecutive frames at about 60 frames/sec.
Reference: [14] <author> N. Max and K. Ohsaki. </author> <title> Rendering trees from precomputed Z-buffer views. </title> <booktitle> In Eurographics Rendering Workshop 1995, pages 74-81;359-360. Eurographics, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: The missing views needed for a walk-through of the virtual environment are interpolated from the stored ones. Max and Ohsaki <ref> [14] </ref> used similar techniques for rendering trees from precomputed Z-buffer views. However, rather than morphing the precomputed images, they reproject them pixel by pixel.
Reference: [15] <author> L. McMillan and G. Bishop. </author> <title> Plenoptic modeling: An image-based rendering system. </title> <booktitle> In SIGGRAPH 95 Conference Proceedings, </booktitle> <pages> pages 39-46. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: For exact implementation two passes are required: first find minimum reliable z, then blend using soft threshold based on that minimum z. 5 Related work Chen [1] and McMillan and Bishop <ref> [15] </ref> modeled environments by storing the light field function around a point. The rays visible from a point are texture mapped to a cylinder around that point, and any horizontal view can be created by warping a portion of the cylinder to the image plane. <p> A disadvantage is the difficulty of storing and accessing the enormous lumigraph representation. The algebraic approach to image-based rendering using pairs of images and pixel correspondences in the two images was introduced by Laveau and Faugeras [11]. It has since been used in several other systems <ref> [15, 18, 7] </ref>. Given correct dense pixel correspondences one can calculate the 3D coordinates of surface points visible in both images, and then project these to the image plane of the virtual camera. However, the projection can also be calculated directly without 3D reconstruction.
Reference: [16] <author> K. Pulli, T. Duchamp, H. Hoppe, J. McDonald, L. Shapiro, and W. Stuetzle. </author> <title> Robust meshes from multiple range maps. </title> <booktitle> In Proc. IEEE Int. Conf. on Recent Advances in 3-D Digital Imaging and Modeling, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: Despite recent advances <ref> [4, 16] </ref>, automatically creating accurate surface models of complex objects (step 3) is still a difficult task, while the computation of accurate reflection models (step 4) has hardly been addressed.
Reference: [17] <author> J. Shade, D. Lischinski, D. Salesin, T. DeRose, and J. Snyder. </author> <title> Hierarchical image caching for accelerated walkthroughsof complex environments. </title> <booktitle> In SIGGRAPH 96 ConferenceProceedings, </booktitle> <pages> pages 75-82. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1996. </year>
Reference-contexts: The missing views needed for a walk-through of the virtual environment are interpolated from the stored ones. Max and Ohsaki [14] used similar techniques for rendering trees from precomputed Z-buffer views. However, rather than morphing the precomputed images, they reproject them pixel by pixel. Shade et al. <ref> [17] </ref> partition the geometric primitives in the scene, render images of them, and texture map the images onto quadrilaterals, which are displayed instead of the geometry.
Reference: [18] <author> T. Werner, R. D. Hersch, and V. Hlavac. </author> <title> Rendering real-world objects using view interpolation. </title> <booktitle> In Proc. IEEE Int. Conf on Computer Vision (ICCV), </booktitle> <pages> pages 957-962, </pages> <month> June </month> <year> 1995. </year> <title> of the triangular mesh. (c) Weight w fl smoothly decreases towards the mesh boundary. view-based meshes from the viewpoint of the virtual camera as described in Section 3.1. (b) Using the weights and soft z-buffering described in Section 3.2 produces a much better result. viewpoint of the virtual camera. The final image is on the bottom right. </title>
Reference-contexts: A disadvantage is the difficulty of storing and accessing the enormous lumigraph representation. The algebraic approach to image-based rendering using pairs of images and pixel correspondences in the two images was introduced by Laveau and Faugeras [11]. It has since been used in several other systems <ref> [15, 18, 7] </ref>. Given correct dense pixel correspondences one can calculate the 3D coordinates of surface points visible in both images, and then project these to the image plane of the virtual camera. However, the projection can also be calculated directly without 3D reconstruction.
References-found: 18

