URL: http://cs.ua.edu/~rsun/sun.eccm98.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00455.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: rsun@cs.ua.edu  
Title: Skill Learning Using A Bottom-Up Hybrid Model  
Author: Ron Sun, Edward Merrill, Todd Peterson 
Address: Tuscaloosa, AL 35487, USA  
Affiliation: The University of Alabama  
Abstract: This paper presents a skill learning model Clarion. Different from existing models of mostly high-level skill learning that use a top-down approach (that is, turning declarative knowledge into procedural knowledge), we adopt a bottom-up approach toward low-level skill learning, where procedural knowledge develops first and declarative knowledge develops from it. Clarion which follows this approach is formed by integrating connectionist, reinforcement, and symbolic learning methods to perform on-line learning. We compare the model with human data in a minefield navigation task. A match between the model and human data is observed in sev eral comparisons.
Abstract-found: 1
Intro-found: 1
Reference: <author> J. R. Anderson, </author> <year> (1982). </year> <title> Acquisition of cognitive skill. </title> <journal> Psychological Review. Vol.89, pp.369-406. </journal>
Reference: <author> D. Berry and D. Broadbent, </author> <year> (1988). </year> <title> Interactive tasks and the implicit-explicit distinction. </title> <journal> British Journal of Psychology. </journal> <volume> 79, </volume> <pages> 251-272. </pages>
Reference-contexts: In research on implicit learning, Berry and Broadbent (1988), Willingham et al (1992), and Reber (1989) expressly demonstrate a dissociation between explicit knowledge and skilled performance in a variety of tasks including dynamic decision tasks <ref> (Berry and Broadbent 1988) </ref>, artificial grammar learning tasks (Reber 1989), and serial reaction tasks (Will-ingham et al 1992).
Reference: <author> A. Clark and A. Karmiloff-Smith, </author> <year> (1993). </year> <title> The cognizer's innards: a psychological and philosophical perspective on the development of thought. </title> <journal> Mind and Language. </journal> <volume> 8 (4), </volume> <pages> 487-519. </pages>
Reference-contexts: In the top level, declarative knowledge is captured in a simple propositional rule form. To facilitate correspondence with the bottom level and to encourage uniformity and integration <ref> (Clark and Karmiloff-Smith 1993) </ref>, we chose to use a localist connectionist model for implementing these rules (e.g., Sun 1992, Towell and Shavlik 1993). Basically, we translate the structure of a set of rules into that of a network.
Reference: <author> A. Damasio, </author> <year> (1994). </year> <title> Decartes' Error. </title> <address> Grosset/Putnam, NY. </address>
Reference: <editor> D. Gordon, et al. </editor> <year> (1994). </year> <title> User's Guide to the Navigation and Collision Avoidance Task. </title> <institution> Naval Research Lab. </institution> <address> DC. </address>
Reference-contexts: It involves an agent selecting and performing a sequence of actions to accomplish an objective on the basis of moment-to-moment information (hence the term "reactive"). An example of this kind of task is the minefield navigation task developed at The Naval Research Lab <ref> (see Gordon et al. 1994) </ref>. This kind of task setting appears to tap into real-world skills associated with decision making under conditions of time pressure and limited information. Thus, the results we obtain from human experiments will likely be transferable to real-world skill learning situations.
Reference: <author> H. Dreyfus and S. Dreyfus, </author> <year> (1987). </year> <title> Mind Over Machine: The Power of Human Intuition, </title> <publisher> The Free Press, </publisher> <address> New York, NY. </address>
Reference: <author> P. Fitts and M. Posner, </author> <year> (1967). </year> <title> Human Performance. </title> <address> Brooks/Cole, Monterey, CA. </address>
Reference: <author> M. Gluck and G. Bower, </author> <year> (1988). </year> <title> From conditioning to category learning. </title> <journal> Journal of Experimental Psychology: General. </journal> <volume> 117 (3), </volume> <pages> 227-247. </pages>
Reference-contexts: Instrumental conditioning also reflects a learning process that differs from the top-down approach, because the process is typically non-verbal and involves the formation of action sequences without requiring a priori explicit knowledge. It may be applied to simple organisms as well as humans <ref> (Gluck and Bower 1988) </ref>. In developmental psychology, Karmiloff-Smith (1986) proposed the idea of "representational redescription". During development, low-level implicit representations are transformed into more abstract and explicit representations and thereby made more accessible.
Reference: <author> W. James, </author> <title> (1890). </title> <booktitle> The Principles of Psychology. </booktitle> <publisher> Dover, </publisher> <address> NY. </address>
Reference: <author> A. Karmiloff-Smith, </author> <year> (1986). </year> <title> From meta-processes to conscious access. </title> <journal> Cognition. </journal> <volume> 23. </volume> <pages> 95-147. </pages>
Reference: <author> F. Keil, </author> <year> (1989). </year> <title> Concepts, Kinds, and Cognitive Development. </title> <publisher> MIT Press. </publisher> <address> Cambridge, MA. </address>
Reference: <author> P. Lewicki, et al. </author> <year> (1992). </year> <journal> Nonconscious acquisition of information. American Psychologist. </journal> <volume> 47, </volume> <pages> 796-801. </pages>
Reference: <author> L. Lin, </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. Machine Learning. </title> <publisher> Vol.8, pp.293-321. </publisher>
Reference-contexts: This input setup yielded a total of 43 primary perceptual inputs. Thus, there were more than 10 12 possible input states. Thus the model had to deal with the problem of high dimensionality. As a result, a lookup table implementation for Q-learning at the bottom level was not possible <ref> (Tesauro 1992, Lin 1992) </ref>. To deal with the situation. a functional approximator such as backpropagation networks must be used. Also in correspondence to the human experimental setting, the action outputs consisted of two clusters of nodes representing turn and speed. <p> The verbalization training condition. Obviously, we could not require verbalization from the model. However, we posited that much of the effect of verbalization on learning was associated with rehearsing previous steps and episodes (although there may be additional factors involved). Thus for the model, we used episode memory playback <ref> (Lin 1992) </ref> in a first attempt to capture this effect. Episode memory playback involves training the model with previously performed episodes between blocks of actual trial episodes in exactly the same manner as in human experiments.
Reference: <author> A. Reber, </author> <year> (1989). </year> <title> Implicit learning and tacit knowledge. </title> <journal> Journal of Exp Psychology: General. </journal> <volume> 118 (3), </volume> <pages> 219-235. </pages>
Reference-contexts: In research on implicit learning, Berry and Broadbent (1988), Willingham et al (1992), and Reber (1989) expressly demonstrate a dissociation between explicit knowledge and skilled performance in a variety of tasks including dynamic decision tasks (Berry and Broadbent 1988), artificial grammar learning tasks <ref> (Reber 1989) </ref>, and serial reaction tasks (Will-ingham et al 1992).
Reference: <author> J. Schooler, S. Ohlsson, and K. Brooks, </author> <year> (1993). </year> <title> Thoughts beyond words: when language overshadows insight. </title> <journal> Journal of Experimental Psychology: General. </journal> <volume> 122 (2). </volume> <pages> 166-183. </pages>
Reference: <author> D. Schacter, </author> <year> (1987). </year> <title> Implicit memory: History and current status. Journal of Experimental Psychology: Learning, Memory, </title> <journal> and Cognition, </journal> <volume> 13, </volume> <pages> 501-518. </pages>
Reference-contexts: Similar claims concerning the development of procedural knowledge prior to the development of declarative knowledge have surfaced in a number of research areas outside the skill learning literature and provided additional support for the bottom-up approach. Implicit memory research <ref> (e.g., Schacter 1987) </ref> demonstrates a dissociation between explicit and implicit knowledge/memories in that an individual's performance can improve by virtue of implicit "retrieval" from memory and the individual can be unaware of the process. This is not amenable to the exclusively top-down approach.
Reference: <author> R. Shiffrin and W. Schneider, </author> <year> (1977). </year> <title> Controlled and automatic human information processing II. </title> <journal> Psychological Review. </journal> <volume> 84. </volume> <pages> 127-190. </pages>
Reference: <author> P. Smolensky, </author> <year> (1988). </year> <title> On the proper treatment of connectionism. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 11(1) </volume> <pages> 1-74. </pages>
Reference: <author> W. Stanley, </author> <title> et al (1989). </title> <journal> Insight without awareness Quarterly Journal of Experimental Psychology. </journal> <volume> 41A (3), </volume> <pages> 553-577. </pages>
Reference-contexts: Willingham et al (1989) similarly demonstrate that procedural knowledge is not always preceded by declarative knowledge in human learning, and show that declarative and procedural learning are not necessarily correlated. There are even indications that explicit knowledge may arise from procedural skills in some circumstances <ref> (see Stanley et al 1989) </ref>. Using a dynamic decision task, Stanley et al. (1989) found that the development of declarative knowledge paralleled but lagged behind the development of procedural knowledge.
Reference: <author> R. Sun, </author> <year> (1992). </year> <title> On Variable Binding in Connectionist Networks, Connection Science, </title> <type> Vol.4, No.2, </type> <institution> pp.93-124. </institution>
Reference-contexts: In the top level, declarative knowledge is captured in a simple propositional rule form. To facilitate correspondence with the bottom level and to encourage uniformity and integration (Clark and Karmiloff-Smith 1993), we chose to use a localist connectionist model for implementing these rules <ref> (e.g., Sun 1992, Towell and Shavlik 1993) </ref>. Basically, we translate the structure of a set of rules into that of a network.
Reference: <author> R. Sun, </author> <year> (1995). </year> <title> Robust reasoning: integrating rule-based and similarity-based reasoning. </title> <journal> Artificial Intelligence. </journal> <volume> 75, 2. </volume> <pages> 241-296. </pages>
Reference-contexts: They range from simple motor movements and other routine tasks in everyday activities to high-level intellectual skills. We want to study "lower-level" cognitive skills, which have not received sufficient research attention. One type of task that exemplifies what we call low-level cognitive skill is reactive sequential decision making <ref> (Sun and Peterson 1995) </ref>. It involves an agent selecting and performing a sequence of actions to accomplish an objective on the basis of moment-to-moment information (hence the term "reactive"). <p> During development, low-level implicit representations are transformed into more abstract and explicit representations and thereby made more accessible. This process is not top-down either, but in the opposite direction. 2 The Model The difference between declarative and procedural knowledge leads naturally to "two-level" architectures <ref> (Sun 1995) </ref>. We thus developed the model Clarion, which stands for Connectionist Learning with Adaptive Rule Induction ON-line (Sun et al 1996). It embodies the distinction of declarative and procedural knowledge (or, conceptual and subconceptual knowledge), and it performs learning in a bottom-up direction.
Reference: <author> R. Sun, T. Peterson, and E. Merrill, </author> <year> (1996). </year> <title> Bottom-up skill learning. </title> <type> Technical Report TR-CS-96-0021, </type> <institution> University of Alabama. </institution> <note> shortened version in Proc.of 18th Cognitive Science Society Conference, </note> <author> T. Tesauro, </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning. </journal> <volume> Vol.8, </volume> <pages> 257-277. </pages>
Reference-contexts: Thus, the results we obtain from human experiments will likely be transferable to real-world skill learning situations. Yet this kind of task is suitable for computational modeling given the recent development of machine learning techniques <ref> (Sun et al 1996, Watkins 1989) </ref>. The distinction between procedural knowledge and declarative knowledge has been made in many theories of learning and cognition (for example, Ander-son 1982, 1993, Keil 1989, Damasio et al. 1994, and Sun 1995). <p> This process is not top-down either, but in the opposite direction. 2 The Model The difference between declarative and procedural knowledge leads naturally to "two-level" architectures (Sun 1995). We thus developed the model Clarion, which stands for Connectionist Learning with Adaptive Rule Induction ON-line <ref> (Sun et al 1996) </ref>. It embodies the distinction of declarative and procedural knowledge (or, conceptual and subconceptual knowledge), and it performs learning in a bottom-up direction.
Reference: <author> C. Watkins, </author> <year> (1989). </year> <title> Learning with Delayed Rewards. </title> <type> Ph.D Thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, UK. </address>
Reference-contexts: We can choose an action based on Q-values. To acquire the Q-values, supervised and/or reinforcement learning methods may be applied. A widely applicable option is the Q-learning algorithm <ref> (Watkins 1989) </ref>, a reinforcement learning algorithm. In the algorithm, Q (x; a) estimates the maximum discounted cumulative reinforcement that the agent will receive from the current state x on.
Reference: <editor> D. Willingham, et al (1989). </editor> <title> On the development of procedural knowledge. Journal of Experimental Psychology: Learning, Memory, </title> <journal> and Cognition. </journal> <volume> 15, </volume> <pages> 1047-1060. </pages>
References-found: 24

