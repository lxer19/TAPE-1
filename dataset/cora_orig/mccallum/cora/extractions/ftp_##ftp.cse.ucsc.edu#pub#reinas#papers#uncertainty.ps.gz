URL: ftp://ftp.cse.ucsc.edu/pub/reinas/papers/uncertainty.ps.gz
Refering-URL: http://www.cse.ucsc.edu/research/slvg/uncertainty.html
Root-URL: http://www.cse.ucsc.edu
Title: Approaches to Uncertainty Visualization  
Author: Alex T. Pang, Craig M. Wittenbrink and Suresh K. Lodha 
Keyword: classification, comparative visualization, differences, data quality, verity.  
Date: September 6, 1996  
Address: Santa Cruz, CA 95064, USA  1501 Page Mill Road, Palo Alto, CA 94304, USA  
Affiliation: Computer Science Department University of California  Hewlett-Packard Laboratories  
Abstract: Visualized data often have dubious origins and quality. Different forms of uncertainty and errors are also introduced as the data are derived, transformed, interpolated, and finally rendered. In the absence of integrated presentation of data and uncertainty, the analysis of the visualization is incomplete at best and often leads to inaccurate or incorrect conclusions. This paper surveys techniques for presenting data together with uncertainty. These uncertainty visualization techniques present data in such a manner that users are made aware of the locations and degree of uncertainties in their data so as to make more informed analyses and decisions. The techniques include adding glyphs, adding geometry, modifying geometry, modifying attributes, animation, sonification, and psycho-visual approaches. We present our results in uncertainty visualization for environmental visualization, surface interpolation, global illumination with radiosity, flow visualization, and figure animation. We also present a classification of the possibilities in uncertainty visualization, and locate our contributions within this classification. 
Abstract-found: 1
Intro-found: 1
Reference: [ATS94] <author> James Arvo, Kenneth Torrance, and Brian Smits. </author> <title> A framework for the analysis of error in global illumination algorithms. </title> <booktitle> In Proceedings SIGGRAPH, </booktitle> <pages> pages 75-84, </pages> <address> Orlando, FL, </address> <month> July </month> <year> 1994. </year> <note> ACM SIGGRAPH. </note>
Reference-contexts: For instance, in global illumination of 3D scenes, radiosity algorithms use approximations for calculating form factors. Some recent work in this area addressed the issue of controlling the errors <ref> [GK94, LSG94, ATS94] </ref>. As these researchers also pointed out, the rendering process introduces uncertainty arising from the data collection process, algorithmic errors, and computational accuracy and precision. <p> These are usually displayed using some straightforward method such as side by side comparison or differencing. For example, Lischinski et al. [LSG94] used line plots to render uncertainty, Greene et al. [GK94] used difference images, and Arvo et al. <ref> [ATS94] </ref> used norms for the entire image. In surface interpolation, Hagen et al. [HHS + 92] used pseudo-coloring of the surface curvature and other properties of the surface. This is an example of scalar value (first row) continuous extent visualization (second column) in Table 1.
Reference: [BBC91] <author> M. Kate Beard, Barbara P. Buttenfield, and Sarah B. Clapham. </author> <title> NCGIA research initiative 7: Visualization of spatial data quality. </title> <type> Technical Paper 91-26, </type> <institution> National Center for Geographic Information and Analysis, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: This has been recognized and is often stated as a worthy goal in scientific visualization (e.g. in the IEEE Visualization discussions on How to Lie with Visualization, IEEE Visualization panels and reports [GU95], and the NCGIA initiative on Visualization of Spatial Data Quality <ref> [BBC91] </ref>), but it has rarely been pursued or realized. In our investigation of uncertainty visualization, our approach has been to look at the needs of different application areas and to develop methods to address them. We found that in many instances, applications are orthogonal to methods.
Reference: [BCE + 92] <author> Ken W. Brodlie, Lesley Carpenter, Rae A. Earnshaw, Julian R. Gallop, Roger J. Hubbold, Anne M. Mumford, Chris D. Osland, and Peter Quarendon, </author> <title> editors. Scientific Visualization: Techniques and Applications, </title> <booktitle> chapter on Visualization Techniques, </booktitle> <pages> pages 37-85. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Carswell [Car92] and Cleveland [Cle85, CM86] use evaluation as a basis for the theory of specifiers, that fundamental parameters, length, area, ratios, etc. describe and determine the effectiveness of visualization. Bergeron and Grinstein [BG89] introduce a classification that uses lattice arrangements of data. Brodlie <ref> [BCE + 92] </ref> describes a classification based on the dimensionality of the entity that is being visualized. Application specific visualization classifications have been done by Hesselink et al. [HPvW94] for vector and tensor field visualization.
Reference: [BG89] <author> R. G. Bergeron and G. G. Grinstein. </author> <title> A reference model for the visualization of multi-dimensional data. </title> <booktitle> In Proceedings of Eurographics, </booktitle> <pages> pages 393-399. </pages> <address> Elsevier Science Publsihers, </address> <year> 1989. </year>
Reference-contexts: Tufte [Tuf83] classifies visualizations by developing evaluation and analysis methods such as data-ink maximization. Carswell [Car92] and Cleveland [Cle85, CM86] use evaluation as a basis for the theory of specifiers, that fundamental parameters, length, area, ratios, etc. describe and determine the effectiveness of visualization. Bergeron and Grinstein <ref> [BG89] </ref> introduce a classification that uses lattice arrangements of data. Brodlie [BCE + 92] describes a classification based on the dimensionality of the entity that is being visualized. Application specific visualization classifications have been done by Hesselink et al. [HPvW94] for vector and tensor field visualization.
Reference: [BHR + 94] <author> Manfred Brill, Hans Hagen, Hans-Christian Rodrian, Wladimir Djatschin, and Stanislav V. Klimenko. </author> <title> Streamball techniques for flow visualization. </title> <booktitle> In Proceedings: Visualization '94, </booktitle> <pages> pages 225-231. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year>
Reference-contexts: For continuous visualization extents, line integral convolution (LIC) [FC95] can use the uncertainty information to modulate the texture. Likewise, adding more variables into existing flow visualization methods such as streamlines result in streamballs <ref> [BHR + 94] </ref>, and to hyperstreamlines [DH93] as well. The taxonomy of existing methods of displaying uncertainty are summarized in Table 2.
Reference: [Bri84] <editor> D. R. Brillinger, editor. The Collected Works of John W. Tukey. Wadsworth and Brooks, </editor> <year> 1984. </year>
Reference-contexts: As a possible solution, one might consider setting free parameters to uncertainty values using existing surface, volume, flow, and multi-dimensional visualization methods [CBB91]. In fact, we do start with existing methods. However, even with the simple task of designing glyphs or icons that incorporate uncertainty information <ref> [Bri84, Tuf90, WPL96, MM94] </ref>, the process is sometimes counter-intuitive. For example, while a glyph may appear appropriate by itself, the user's perception of the glyph may be different when a group of them is presented in various scales and locations.
Reference: [Car92] <author> C. Melody Carswell. </author> <title> Choosing specifiers: An evaluation of the basic tasks model of graphical perception. </title> <booktitle> Human Factors, </booktitle> <volume> 34(5) </volume> <pages> 535-554, </pages> <month> October </month> <year> 1992. </year> <month> 21 </month>
Reference-contexts: Keller and Keller [KK93] classify visualization by using a taxonomy of visualization goals. Tufte [Tuf83] classifies visualizations by developing evaluation and analysis methods such as data-ink maximization. Carswell <ref> [Car92] </ref> and Cleveland [Cle85, CM86] use evaluation as a basis for the theory of specifiers, that fundamental parameters, length, area, ratios, etc. describe and determine the effectiveness of visualization. Bergeron and Grinstein [BG89] introduce a classification that uses lattice arrangements of data. <p> The basic methodology, is to use visual tests where users examine visualizations, and decode the information within the graphic. The amount of error between the user interpretation and the encoding is statistically evaluated to determine if the visualizations are effective. There is a developed theory of specifiers, by Carswell <ref> [Car92] </ref>, Cleveland [Cle85, CM86], and others which has shown that decoding 19 length is the most accurate, decoding area is less accurate, etc. Decoding of pseudo-colors, bump mapping, surface textures, etc. has not been shown to be straightforward yet.
Reference: [CBB91] <author> Elizabeth Cluff, Robert Burton, and William Barrett. </author> <title> A survey and characteriza-tion of multidimensional presentation techniques. </title> <journal> Journal of Imaging Technology, </journal> <volume> 17(4), </volume> <year> 1991. </year>
Reference-contexts: The common underlying problem in these areas is visually mapping data and uncertainty together into a holistic view. As a possible solution, one might consider setting free parameters to uncertainty values using existing surface, volume, flow, and multi-dimensional visualization methods <ref> [CBB91] </ref>. In fact, we do start with existing methods. However, even with the simple task of designing glyphs or icons that incorporate uncertainty information [Bri84, Tuf90, WPL96, MM94], the process is sometimes counter-intuitive.
Reference: [Cha83] <author> Christopher Chatfield. </author> <title> Statistics for Technology, A Course In Applied Statistics. </title> <publisher> Chapman and Hall, </publisher> <address> third edition, </address> <year> 1983. </year>
Reference-contexts: Uncertainty in acquisition: Starting with the data acquisition stage, one will note that nearly all data sets, whether from instrument measurements, numerical models, or data entry have a statistical variation <ref> [Cha83] </ref>. With instruments, there is an experimental variability whether the measurements are taken by a machine or by a scientist. The more times the measurement is taken, the more confident the measurement. But there will be a statistical variation in these measurements.
Reference: [Cle85] <author> William S. Cleveland. </author> <title> The Elements of Graphing Data. </title> <publisher> Wadsworth, </publisher> <year> 1985. </year>
Reference-contexts: Keller and Keller [KK93] classify visualization by using a taxonomy of visualization goals. Tufte [Tuf83] classifies visualizations by developing evaluation and analysis methods such as data-ink maximization. Carswell [Car92] and Cleveland <ref> [Cle85, CM86] </ref> use evaluation as a basis for the theory of specifiers, that fundamental parameters, length, area, ratios, etc. describe and determine the effectiveness of visualization. Bergeron and Grinstein [BG89] introduce a classification that uses lattice arrangements of data. <p> Our classification of uncertainty visualization techniques demonstrates that only the scalar value discrete visualization extent, or upper left entry in Table 1 has been adequately explored, where 7 the uncertainty may be shown with economy using Tukey's box plots [Tuk77], Tufte's quartile plots [Tuf83] and/or Cleveland's framed rectangles <ref> [Cle85] </ref>. What we describe in the following section are new methods for displaying higher dimensional uncertainty (e.g. a vector of uncertainty parameters) in surfaces and in animation applications. <p> The amount of error between the user interpretation and the encoding is statistically evaluated to determine if the visualizations are effective. There is a developed theory of specifiers, by Carswell [Car92], Cleveland <ref> [Cle85, CM86] </ref>, and others which has shown that decoding 19 length is the most accurate, decoding area is less accurate, etc. Decoding of pseudo-colors, bump mapping, surface textures, etc. has not been shown to be straightforward yet.
Reference: [CM86] <author> William S. Cleveland and Robert McGill. </author> <title> An experiment in graphical perception. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 25(5) </volume> <pages> 491-500, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: Keller and Keller [KK93] classify visualization by using a taxonomy of visualization goals. Tufte [Tuf83] classifies visualizations by developing evaluation and analysis methods such as data-ink maximization. Carswell [Car92] and Cleveland <ref> [Cle85, CM86] </ref> use evaluation as a basis for the theory of specifiers, that fundamental parameters, length, area, ratios, etc. describe and determine the effectiveness of visualization. Bergeron and Grinstein [BG89] introduce a classification that uses lattice arrangements of data. <p> The amount of error between the user interpretation and the encoding is statistically evaluated to determine if the visualizations are effective. There is a developed theory of specifiers, by Carswell [Car92], Cleveland <ref> [Cle85, CM86] </ref>, and others which has shown that decoding 19 length is the most accurate, decoding area is less accurate, etc. Decoding of pseudo-colors, bump mapping, surface textures, etc. has not been shown to be straightforward yet.
Reference: [DH93] <author> T. Delmarcelle and L. Hesselink. </author> <title> Visualizing second-order tensor fields with hyper-streamlines. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 13(4) </volume> <pages> 25-33, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: For continuous visualization extents, line integral convolution (LIC) [FC95] can use the uncertainty information to modulate the texture. Likewise, adding more variables into existing flow visualization methods such as streamlines result in streamballs [BHR + 94], and to hyperstreamlines <ref> [DH93] </ref> as well. The taxonomy of existing methods of displaying uncertainty are summarized in Table 2.
Reference: [DH96] <author> D.L. Darmofal and R. Haimes. </author> <title> An analysis of 3d particle path integration algorithms. </title> <journal> Journal of Computational Physics, </journal> <volume> 123(1) </volume> <pages> 182-195, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: For example, different integration methods, step sizes, orders, and seeding strategies lead to slightly different flow visualization results. Effects of uncertainty are more pronounced in the vicinity of or on critical points in the flow field. These differences may at times result in drastically different flow visualizations <ref> [DH96] </ref>. Animation allows visualization to include an additional parameter, usually time. Again, there are several opportunities for uncertainty to be introduced.
Reference: [dLvW93] <author> Willem C. de Leeuw and Jarke J. van Wijk. </author> <title> A probe for local flow field visualization. </title> <booktitle> In Proceedings of Visualization 93, </booktitle> <pages> pages 39-45. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1993. </year>
Reference-contexts: These are examples of scalar value (first row) continuous visualization extent (second column). We have not seen any visualization methods designed for presenting uncertainty information for vector or tensor data. However, some existing vector and tensor visualization methods can be modified to include uncertainty information. For example, tensor probes <ref> [dLvW93] </ref> with discrete visualization extents can be easily modified to incorporate uncertainty information. For continuous visualization extents, line integral convolution (LIC) [FC95] can use the uncertainty information to modulate the texture.
Reference: [Far88] <author> Gerald Farin. </author> <title> Curves and Surfaces for Computer Aided Geometric Design: A Practical Guide. </title> <publisher> Academic Press, </publisher> <pages> 88. </pages>
Reference-contexts: Surface approximation and interpolation is used in dealing with scattered data sets [Lod96]. Here, a variety of tradeoffs exist in performance and accuracy, and there is no ideal surface in many cases because of the many free parameters available <ref> [Far88, LSPW96] </ref>. In many cases, the data that are to be interpolated have numerous errors, and may even lack topology information [HDD + 94]. Similar difficulties and range of choices produce uncertainty in flow visualization methods.
Reference: [FC95] <author> L. K. Forssell and S. D. Cohen. </author> <title> Using line integral convolution for flow visualization: curvilinear grids, variable-speed animation, and unsteady flows. </title> <journal> IEEE Transactions on Visualization and Computer Graphics, </journal> <volume> 1(2) </volume> <pages> 133-141, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: However, some existing vector and tensor visualization methods can be modified to include uncertainty information. For example, tensor probes [dLvW93] with discrete visualization extents can be easily modified to incorporate uncertainty information. For continuous visualization extents, line integral convolution (LIC) <ref> [FC95] </ref> can use the uncertainty information to modulate the texture. Likewise, adding more variables into existing flow visualization methods such as streamlines result in streamballs [BHR + 94], and to hyperstreamlines [DH93] as well. The taxonomy of existing methods of displaying uncertainty are summarized in Table 2.
Reference: [Fis94] <author> P. Fisher. </author> <title> Visualization in Geographical Information Systems, chapter on Animation and Sound for the Visualization of Uncertain Spatial Information, </title> <address> pages 181-185. </address> <year> 1994. </year>
Reference-contexts: For example, aside from pseudo-coloring areas of maps to represent value uncertainty, they may also use contour lines to indicate regions of similar confidence levels. For cartographers, the contours may be for areas of similar spatial distortions from projections. Fisher <ref> [Fis94] </ref> proposed animation techniques such as blinking data points to represent data uncertainty. Gershon [Ger92] proposed animation for the display of uncertainty in fuzzily classified regions. These are examples of scalar value (first row) continuous visualization extent (second column).
Reference: [Ger92] <author> Nahum D. </author> <title> Gershon. Visualization of fuzzy data using generalized animation. </title> <booktitle> In Proceedings of Visualization 92, </booktitle> <pages> pages 268-273. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1992. </year>
Reference-contexts: For cartographers, the contours may be for areas of similar spatial distortions from projections. Fisher [Fis94] proposed animation techniques such as blinking data points to represent data uncertainty. Gershon <ref> [Ger92] </ref> proposed animation for the display of uncertainty in fuzzily classified regions. These are examples of scalar value (first row) continuous visualization extent (second column). We have not seen any visualization methods designed for presenting uncertainty information for vector or tensor data.
Reference: [GK94] <author> Ned Greene and Michael Kass. </author> <title> Error-bounded antialiased rendering of complex environments. </title> <booktitle> In Proceedings SIGGRAPH, </booktitle> <pages> pages 59-66, </pages> <address> Orlando, FL, </address> <month> July </month> <year> 1994. </year> <note> ACM SIGGRAPH. </note>
Reference-contexts: For instance, in global illumination of 3D scenes, radiosity algorithms use approximations for calculating form factors. Some recent work in this area addressed the issue of controlling the errors <ref> [GK94, LSG94, ATS94] </ref>. As these researchers also pointed out, the rendering process introduces uncertainty arising from the data collection process, algorithmic errors, and computational accuracy and precision. <p> Many researchers are fully aware of the uncertainty, usually in the form of errors, in their data. These are usually displayed using some straightforward method such as side by side comparison or differencing. For example, Lischinski et al. [LSG94] used line plots to render uncertainty, Greene et al. <ref> [GK94] </ref> used difference images, and Arvo et al. [ATS94] used norms for the entire image. In surface interpolation, Hagen et al. [HHS + 92] used pseudo-coloring of the surface curvature and other properties of the surface.
Reference: [GU95] <author> A. Globus and S. Uselton. </author> <title> Evaluation of visualization software. </title> <journal> Computer Graphics, </journal> <pages> pages 41-44, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: This has been recognized and is often stated as a worthy goal in scientific visualization (e.g. in the IEEE Visualization discussions on How to Lie with Visualization, IEEE Visualization panels and reports <ref> [GU95] </ref>, and the NCGIA initiative on Visualization of Spatial Data Quality [BBC91]), but it has rarely been pursued or realized. In our investigation of uncertainty visualization, our approach has been to look at the needs of different application areas and to develop methods to address them.
Reference: [HDD + 94] <author> Hugues Hoppe, Tony DeRose, Tom Duchamp, Mark Halstead, Hugert Jin, John McDonald, Jean Schweitzer, and Werner Stuetzle. </author> <title> Piecewise smooth surface reconstruction. </title> <booktitle> In Proceedings SIGGRAPH, </booktitle> <pages> pages 295-303, </pages> <address> Orlando, FL, </address> <month> July </month> <year> 1994. </year> <note> ACM SIGGRAPH. </note>
Reference-contexts: Here, a variety of tradeoffs exist in performance and accuracy, and there is no ideal surface in many cases because of the many free parameters available [Far88, LSPW96]. In many cases, the data that are to be interpolated have numerous errors, and may even lack topology information <ref> [HDD + 94] </ref>. Similar difficulties and range of choices produce uncertainty in flow visualization methods. For example, different integration methods, step sizes, orders, and seeding strategies lead to slightly different flow visualization results.
Reference: [HHS + 92] <author> Hans Hagen, Stefanie Hahmann, Thomas Schreiber, Yasuo Nakajima, Burkard Wordenweber, and Petra Hollemann-Grundstedt. </author> <title> Surface interrogation algorithms. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 12(5) </volume> <pages> 53-60, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: For example, Lischinski et al. [LSG94] used line plots to render uncertainty, Greene et al. [GK94] used difference images, and Arvo et al. [ATS94] used norms for the entire image. In surface interpolation, Hagen et al. <ref> [HHS + 92] </ref> used pseudo-coloring of the surface curvature and other properties of the surface. This is an example of scalar value (first row) continuous extent visualization (second column) in Table 1.
Reference: [HPvW94] <author> L. Hesselink, F. Post, and J.J. van Wijk. </author> <title> Research issues in vector and tensor field visualization. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 14(2) </volume> <pages> 76-79, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Bergeron and Grinstein [BG89] introduce a classification that uses lattice arrangements of data. Brodlie [BCE + 92] describes a classification based on the dimensionality of the entity that is being visualized. Application specific visualization classifications have been done by Hesselink et al. <ref> [HPvW94] </ref> for vector and tensor field visualization. We create a classification system that is similar to the systems of Brodlie and Hesselink et al. in certain aspects, but is extended to accommodate uncertainty visualization techniques. Brodlie classifies techniques using two characteristics.
Reference: [ID90] <author> A. Inselberg and B. Dimsdale. </author> <title> Parallel coordinates: a tool for visualizing multidimensional geometry. </title> <booktitle> In Proceedings of Visualization'90, </booktitle> <pages> pages 361-378. </pages> <publisher> IEEE, </publisher> <month> October </month> <year> 1990. </year>
Reference-contexts: The data extent is independent of the visualization extent, though some mappings are more natural and useful than others. It is important to note that discrete visualization extents, such as glyphs, can be used with continuous data, and that continuous visualization extents such as parallel coordinate display <ref> [Ins85, ID90] </ref> can be used with discrete data. Visualization extent corresponds with views of Brodlie and domain of Hesselink et al. 5. Axes mapping defines visualization mapping (experiential or abstract). It allows different variables or grouping of variables to be mapped together or to different axes.
Reference: [Ins85] <author> A. Inselberg. </author> <title> The plane with parallel coordinates. </title> <journal> The Visual Computer, </journal> <volume> 1(1) </volume> <pages> 69-91, </pages> <year> 1985. </year>
Reference-contexts: The data extent is independent of the visualization extent, though some mappings are more natural and useful than others. It is important to note that discrete visualization extents, such as glyphs, can be used with continuous data, and that continuous visualization extents such as parallel coordinate display <ref> [Ins85, ID90] </ref> can be used with discrete data. Visualization extent corresponds with views of Brodlie and domain of Hesselink et al. 5. Axes mapping defines visualization mapping (experiential or abstract). It allows different variables or grouping of variables to be mapped together or to different axes.
Reference: [KK93] <author> Peter Keller and Mary Keller. </author> <title> Visual Cues: Practical Data Visualization. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year> <month> 22 </month>
Reference-contexts: We next turn our attention to classifying visualization approaches, then uncertainty visualization approaches, and finally presenting several new methods for uncertainty visualization. 2 Classification of Methods To classify uncertainty visualization approaches, we first consider more general classifications. Keller and Keller <ref> [KK93] </ref> classify visualization by using a taxonomy of visualization goals. Tufte [Tuf83] classifies visualizations by developing evaluation and analysis methods such as data-ink maximization.
Reference: [Kra94] <author> G. Kramer. </author> <title> An introduction to auditory display. </title> <editor> In Gregory Kramer, editor, </editor> <title> Auditory Display, </title> <booktitle> Sonification, Audification, and Auditory Interfaces, </booktitle> <pages> pages 1-78. </pages> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Also, sound can provide redundancy of representation and allow data validation. An excellent discussion of additional benefits of auditory display in conjunction with other displays can be found in <ref> [Kra94, MF95] </ref>. We have explored the use of sonification in visualizing uncertainty of flow visualization and surface interpolation using LISTEN [LWS96]. LISTEN is a data sonification system that allows interactive mapping of data to sound parameters such as pitch, duration, volume, and timbre.
Reference: [LC87] <author> W. E. Lorensen and H. E. Cline. </author> <title> Marching cubes: A high resolution 3D surface construction algorithm. </title> <journal> Computer Graphics, </journal> <volume> 21(4):163 - 169, </volume> <year> 1987. </year>
Reference-contexts: Uncertainty introduced in the visualization process is not limited to radiosity and volume rendering, but are also present in more routine operations. For example, the use of interpolation is quite prevalent in taking slices through data sets, in contouring, as well as isosurface algorithms <ref> [LC87, VGW94] </ref>, to name a few. Surface approximation and interpolation is used in dealing with scattered data sets [Lod96]. Here, a variety of tradeoffs exist in performance and accuracy, and there is no ideal surface in many cases because of the many free parameters available [Far88, LSPW96].
Reference: [Lod96] <author> S. K. Lodha. </author> <title> Scattered data techniques for surfaces. </title> <editor> In I. Chakravarty and D. Silver, editors, </editor> <title> Geometry Detection, estimation and Synthesis for Scientific Visualization, page to appear. </title> <publisher> Academic Press, </publisher> <year> 1996. </year>
Reference-contexts: For example, the use of interpolation is quite prevalent in taking slices through data sets, in contouring, as well as isosurface algorithms [LC87, VGW94], to name a few. Surface approximation and interpolation is used in dealing with scattered data sets <ref> [Lod96] </ref>. Here, a variety of tradeoffs exist in performance and accuracy, and there is no ideal surface in many cases because of the many free parameters available [Far88, LSPW96].
Reference: [LPSW96] <author> Suresh K. Lodha, Alex Pang, Robert E. Sheehan, and Craig M. Wittenbrink. UFLOW: </author> <title> Visualizing uncertainty in fluid flow. In Proceedings of Visualization 96, </title> <note> page to appear, </note> <month> October </month> <year> 1996. </year>
Reference-contexts: Glyphs can also be used to visualize uncertainty in a variety of ways. We have investigated glyphs in the following applications, radiosity [PF96], vector fields [WPL96, WSF + 95], surface interpolation [LSPW96], flow visualization <ref> [LPSW96] </ref>, and key-framed animation [WPL95]. The primary issues in using glyphs for visualizing uncertainty are the sampling frequency/location and the placement orientation (e.g. [TB96]). For most of our applications to date, we have been able to specify the orientation through the nature of the data being presented.
Reference: [LSG94] <author> Dani Lischinski, Brian Smits, and Donald P. Greenberg. </author> <title> Bounds and error estimates for radiosity. </title> <booktitle> In Proceedings SIGGRAPH, </booktitle> <pages> pages 67-74, </pages> <address> Orlando, FL, </address> <month> July </month> <year> 1994. </year> <note> ACM SIGGRAPH. </note>
Reference-contexts: For instance, in global illumination of 3D scenes, radiosity algorithms use approximations for calculating form factors. Some recent work in this area addressed the issue of controlling the errors <ref> [GK94, LSG94, ATS94] </ref>. As these researchers also pointed out, the rendering process introduces uncertainty arising from the data collection process, algorithmic errors, and computational accuracy and precision. <p> Many researchers are fully aware of the uncertainty, usually in the form of errors, in their data. These are usually displayed using some straightforward method such as side by side comparison or differencing. For example, Lischinski et al. <ref> [LSG94] </ref> used line plots to render uncertainty, Greene et al. [GK94] used difference images, and Arvo et al. [ATS94] used norms for the entire image. In surface interpolation, Hagen et al. [HHS + 92] used pseudo-coloring of the surface curvature and other properties of the surface.
Reference: [LSPW96] <author> Suresh K. Lodha, Bob Sheehan, Alex T. Pang, and Craig M. Wittenbrink. </author> <title> Visualizing geometric uncertainty of surface interpolants. </title> <booktitle> Proceedings of Graphics Interface, </booktitle> <pages> pages 238-245, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Surface approximation and interpolation is used in dealing with scattered data sets [Lod96]. Here, a variety of tradeoffs exist in performance and accuracy, and there is no ideal surface in many cases because of the many free parameters available <ref> [Far88, LSPW96] </ref>. In many cases, the data that are to be interpolated have numerous errors, and may even lack topology information [HDD + 94]. Similar difficulties and range of choices produce uncertainty in flow visualization methods. <p> An example would be arrow glyphs, which are used to visualize magnitude and directional information in vector fields. Glyphs can also be used to visualize uncertainty in a variety of ways. We have investigated glyphs in the following applications, radiosity [PF96], vector fields [WPL96, WSF + 95], surface interpolation <ref> [LSPW96] </ref>, flow visualization [LPSW96], and key-framed animation [WPL95]. The primary issues in using glyphs for visualizing uncertainty are the sampling frequency/location and the placement orientation (e.g. [TB96]). For most of our applications to date, we have been able to specify the orientation through the nature of the data being presented.
Reference: [LWS96] <author> S. K. Lodha, C. M. Wilson, and R. E. Sheehan. </author> <title> LISTEN: sounding uncertainty visualization. </title> <booktitle> In Proceedings of Visualization 96. IEEE, </booktitle> <year> 1996. </year>
Reference-contexts: Also, sound can provide redundancy of representation and allow data validation. An excellent discussion of additional benefits of auditory display in conjunction with other displays can be found in [Kra94, MF95]. We have explored the use of sonification in visualizing uncertainty of flow visualization and surface interpolation using LISTEN <ref> [LWS96] </ref>. LISTEN is a data sonification system that allows interactive mapping of data to sound parameters such as pitch, duration, volume, and timbre. Here, we describe two examples of sonifying uncertainty together with animation.
Reference: [MF95] <author> R. Minghim and A.R. Forrest. </author> <title> An illustrated analysis of sonification for scientific visualization. </title> <booktitle> In Proceedings of Visualization 95, </booktitle> <pages> pages 110-117. </pages> <publisher> IEEE, </publisher> <year> 1995. </year>
Reference-contexts: Also, sound can provide redundancy of representation and allow data validation. An excellent discussion of additional benefits of auditory display in conjunction with other displays can be found in <ref> [Kra94, MF95] </ref>. We have explored the use of sonification in visualizing uncertainty of flow visualization and surface interpolation using LISTEN [LWS96]. LISTEN is a data sonification system that allows interactive mapping of data to sound parameters such as pitch, duration, volume, and timbre.
Reference: [MM94] <author> J.F. Moreno and J. Melia. </author> <title> An optimum interpolation method applied to the resampling of NOAA AVHRR data. </title> <journal> IEEE Transactions on Geoscience and Remote Sensing, </journal> <volume> 32(1) </volume> <pages> 131-151, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: As a possible solution, one might consider setting free parameters to uncertainty values using existing surface, volume, flow, and multi-dimensional visualization methods [CBB91]. In fact, we do start with existing methods. However, even with the simple task of designing glyphs or icons that incorporate uncertainty information <ref> [Bri84, Tuf90, WPL96, MM94] </ref>, the process is sometimes counter-intuitive. For example, while a glyph may appear appropriate by itself, the user's perception of the glyph may be different when a group of them is presented in various scales and locations.
Reference: [MMMY96] <author> Torsten Moller, Raghu Machiraju, Klaus Mueller, and Roni Yagel. </author> <title> Classification and local error estimation of interpolation and derivative filters for volume rendering. In Proceedings Volume Visualization, page to appear, </title> <address> San Francisco, CA, </address> <month> October </month> <year> 1996. </year> <note> IEEE. </note>
Reference-contexts: As these researchers also pointed out, the rendering process introduces uncertainty arising from the data collection process, algorithmic errors, and computational accuracy and precision. Similarly, there are different approaches to direct volume rendering of 3D data sets <ref> [UH90, MMMY96] </ref> resulting in slightly different renderings of the same data set.
Reference: [PA95] <author> Alex Pang and Naim Alper. </author> <title> Bump mapped vector fields. </title> <booktitle> In SPIE & IS&T Conference Proceedings on Electronic Imaging, </booktitle> <volume> Vol. 2410: </volume> <booktitle> Visual Data Exploration and Analysis II, </booktitle> <pages> pages 78-86, </pages> <note> color plate page 205. SPIE, </note> <month> February </month> <year> 1995. </year>
Reference-contexts: Examples include mapping different reflectivity coefficients, such as specular and diffuse, to uncertainty values in order to alter the appearance of the visualization primitive. Another example would be manipulation of surface normals, similar to bump mapping, and in combination with lighting controls to provide indications of uncertainty <ref> [PA95] </ref>. Below are some examples that illustrate how modifying attributes can be used to incorporate uncertainty.
Reference: [PF96] <author> Alex Pang and Adam Freeman. </author> <title> Methods for comparing 3D surface attributes. </title> <booktitle> In SPIE Vol. 2656 Visual Data Exploration and Analysis III, </booktitle> <pages> pages 58-64. SPIE, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Glyphs encode information through their shape and/or color. An example would be arrow glyphs, which are used to visualize magnitude and directional information in vector fields. Glyphs can also be used to visualize uncertainty in a variety of ways. We have investigated glyphs in the following applications, radiosity <ref> [PF96] </ref>, vector fields [WPL96, WSF + 95], surface interpolation [LSPW96], flow visualization [LPSW96], and key-framed animation [WPL95]. The primary issues in using glyphs for visualizing uncertainty are the sampling frequency/location and the placement orientation (e.g. [TB96]).
Reference: [PFN94] <author> Alex Pang, Jeff Furman, and Wendell Nuss. </author> <title> Data quality issues in visualization. </title> <booktitle> In SPIE Vol. 2178 Visual Data Exploration and Analysis, </booktitle> <pages> pages 12-23. SPIE, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: Note that the term data quality has an inverse relationship with data uncertainty <ref> [PFN94] </ref> and hence can also take advantage of the techniques presented in this paper. 1.3 Sources of Uncertainty In order to understand what is overlooked in visualization, we quickly review the sources of uncertainty, errors, and ranges within data.
Reference: [TB96] <author> Greg Turk and David Banks. </author> <title> Image-guided streamline placement. </title> <booktitle> In Proceedings SIGGRAPH, </booktitle> <pages> pages 453-460, </pages> <address> New Orleans, LA, </address> <month> August </month> <year> 1996. </year> <note> ACM SIGGRAPH. </note>
Reference-contexts: We have investigated glyphs in the following applications, radiosity [PF96], vector fields [WPL96, WSF + 95], surface interpolation [LSPW96], flow visualization [LPSW96], and key-framed animation [WPL95]. The primary issues in using glyphs for visualizing uncertainty are the sampling frequency/location and the placement orientation (e.g. <ref> [TB96] </ref>). For most of our applications to date, we have been able to specify the orientation through the nature of the data being presented.
Reference: [TK93] <author> Barry N. Taylor and Chris E. Kuyatt. </author> <title> Guidelines for evaluating and expressing the uncertainty of NIST measurement results. </title> <type> Technical report, </type> <institution> National Institute of Standards and Technology Technical Note 1297, Gaithersburg, MD, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: This broad umbrella is intended to capture most if not all the possible types and sources of uncertainty in data. NIST has written a standards report <ref> [TK93] </ref> which identifies four ways of expressing uncertainty.
Reference: [Tuf83] <author> Edward R. Tufte. </author> <title> The Visual Display of Quantitative Information. </title> <publisher> Graphics Press, </publisher> <year> 1983. </year>
Reference-contexts: Keller and Keller [KK93] classify visualization by using a taxonomy of visualization goals. Tufte <ref> [Tuf83] </ref> classifies visualizations by developing evaluation and analysis methods such as data-ink maximization. Carswell [Car92] and Cleveland [Cle85, CM86] use evaluation as a basis for the theory of specifiers, that fundamental parameters, length, area, ratios, etc. describe and determine the effectiveness of visualization. <p> Our classification of uncertainty visualization techniques demonstrates that only the scalar value discrete visualization extent, or upper left entry in Table 1 has been adequately explored, where 7 the uncertainty may be shown with economy using Tukey's box plots [Tuk77], Tufte's quartile plots <ref> [Tuf83] </ref> and/or Cleveland's framed rectangles [Cle85]. What we describe in the following section are new methods for displaying higher dimensional uncertainty (e.g. a vector of uncertainty parameters) in surfaces and in animation applications.
Reference: [Tuf90] <author> Edward R. Tufte. </author> <title> Envisioning Information. </title> <publisher> Graphics Press, </publisher> <year> 1990. </year>
Reference-contexts: As a possible solution, one might consider setting free parameters to uncertainty values using existing surface, volume, flow, and multi-dimensional visualization methods [CBB91]. In fact, we do start with existing methods. However, even with the simple task of designing glyphs or icons that incorporate uncertainty information <ref> [Bri84, Tuf90, WPL96, MM94] </ref>, the process is sometimes counter-intuitive. For example, while a glyph may appear appropriate by itself, the user's perception of the glyph may be different when a group of them is presented in various scales and locations.
Reference: [Tuk77] <author> John W. Tukey. </author> <title> Exploratory Data Analysis. </title> <publisher> Addison Wesley, </publisher> <year> 1977. </year>
Reference-contexts: Our classification of uncertainty visualization techniques demonstrates that only the scalar value discrete visualization extent, or upper left entry in Table 1 has been adequately explored, where 7 the uncertainty may be shown with economy using Tukey's box plots <ref> [Tuk77] </ref>, Tufte's quartile plots [Tuf83] and/or Cleveland's framed rectangles [Cle85]. What we describe in the following section are new methods for displaying higher dimensional uncertainty (e.g. a vector of uncertainty parameters) in surfaces and in animation applications.
Reference: [UH90] <author> J.K. Udupa and H.-M. Hung. </author> <title> Surface versus volume rendering: a comparative assessment. </title> <booktitle> In Proceedings of the First Conference on Visualization in Biomedical Computing, </booktitle> <pages> pages 83-91, </pages> <address> Atlanta, GA, </address> <month> May </month> <year> 1990. </year> <journal> IEEE Comput. Soc. </journal> <volume> 23 </volume>
Reference-contexts: As these researchers also pointed out, the rendering process introduces uncertainty arising from the data collection process, algorithmic errors, and computational accuracy and precision. Similarly, there are different approaches to direct volume rendering of 3D data sets <ref> [UH90, MMMY96] </ref> resulting in slightly different renderings of the same data set.
Reference: [VGW94] <author> Allen Van Gelder and Jane Wilhelms. </author> <title> Topological considerations in isosurface generation. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 13(4), </volume> <month> October </month> <year> 1994. </year> <note> Also UCSC technical report UCSC-CRL-90-14. </note>
Reference-contexts: Uncertainty introduced in the visualization process is not limited to radiosity and volume rendering, but are also present in more routine operations. For example, the use of interpolation is quite prevalent in taking slices through data sets, in contouring, as well as isosurface algorithms <ref> [LC87, VGW94] </ref>, to name a few. Surface approximation and interpolation is used in dealing with scattered data sets [Lod96]. Here, a variety of tradeoffs exist in performance and accuracy, and there is no ideal surface in many cases because of the many free parameters available [Far88, LSPW96].
Reference: [Wit95] <author> Craig M. Wittenbrink. </author> <title> IFS fractal interpolation for 2D and 3D visualization. </title> <booktitle> In IEEE Visualization '95, </booktitle> <pages> pages 77-84, </pages> <address> Atlanta, GA, </address> <month> November </month> <year> 1995. </year> <note> IEEE. </note>
Reference-contexts: We have found that smooth interpolations sometimes impart the message that the data itself represents a smooth phenomenon, or that there was a higher density of data collected than indicated. We developed interpolation methods to demonstrate uncertainty through the roughness of the surface or the variation in the volume <ref> [Wit95] </ref>. Fig. 12 shows a fractal interpolation of 2D scalar samples. Similar to Fig. 12, Fig. 13 produces a bumpy looking surface. This is achieved by displacing the surface up or down according to the scaled difference between different surface reconstruction methods.
Reference: [WPL95] <author> Craig M. Wittenbrink, Alex T. Pang, and Suresh Lodha. </author> <title> Verity visualization: Visual mappings. </title> <type> Technical Report UCSC-CRL-95-48, </type> <institution> Univ. of Cal. Santa Cruz, </institution> <year> 1995. </year>
Reference-contexts: These visualizations present a more complete and accurate rendition of data for users to analyze. The methods employed in uncertainty visualization may range from overloading of visual parameters such as those commonly found in multivariate visualization, to verity visualization <ref> [WPL95, WPL96] </ref> where the display of both data and uncertainty is inseparable within the same picture. Applications which can benefit from uncertainty visualization are those where there is a chance for uncertainty to be introduced in the visualization process, and where such uncertainty matters. <p> Glyphs can also be used to visualize uncertainty in a variety of ways. We have investigated glyphs in the following applications, radiosity [PF96], vector fields [WPL96, WSF + 95], surface interpolation [LSPW96], flow visualization [LPSW96], and key-framed animation <ref> [WPL95] </ref>. The primary issues in using glyphs for visualizing uncertainty are the sampling frequency/location and the placement orientation (e.g. [TB96]). For most of our applications to date, we have been able to specify the orientation through the nature of the data being presented.
Reference: [WPL96] <author> Craig M. Wittenbrink, Alex T. Pang, and Suresh K. Lodha. </author> <title> Glyphs for visualizing uncertainty in vector fields. </title> <journal> IEEE Transactions on Visualization and Computer Graphics, </journal> <month> September </month> <year> 1996. </year> <note> to appear. </note>
Reference-contexts: As a possible solution, one might consider setting free parameters to uncertainty values using existing surface, volume, flow, and multi-dimensional visualization methods [CBB91]. In fact, we do start with existing methods. However, even with the simple task of designing glyphs or icons that incorporate uncertainty information <ref> [Bri84, Tuf90, WPL96, MM94] </ref>, the process is sometimes counter-intuitive. For example, while a glyph may appear appropriate by itself, the user's perception of the glyph may be different when a group of them is presented in various scales and locations. <p> These visualizations present a more complete and accurate rendition of data for users to analyze. The methods employed in uncertainty visualization may range from overloading of visual parameters such as those commonly found in multivariate visualization, to verity visualization <ref> [WPL95, WPL96] </ref> where the display of both data and uncertainty is inseparable within the same picture. Applications which can benefit from uncertainty visualization are those where there is a chance for uncertainty to be introduced in the visualization process, and where such uncertainty matters. <p> An example would be arrow glyphs, which are used to visualize magnitude and directional information in vector fields. Glyphs can also be used to visualize uncertainty in a variety of ways. We have investigated glyphs in the following applications, radiosity [PF96], vector fields <ref> [WPL96, WSF + 95] </ref>, surface interpolation [LSPW96], flow visualization [LPSW96], and key-framed animation [WPL95]. The primary issues in using glyphs for visualizing uncertainty are the sampling frequency/location and the placement orientation (e.g. [TB96]).
Reference: [WSF + 95] <author> Craig M. Wittenbrink, Elijah Saxon, Jeff J. Furman, Alex T. Pang, and Suresh Lodha. </author> <title> Glyphs for visualizing uncertainty in environmental vector fields. </title> <booktitle> In Vol. 2410 SPIE & IS&T Conference Proceedings on Electronic Imaging: Visual Data Exploration and Analysis, </booktitle> <pages> pages 87-100. SPIE, </pages> <month> February </month> <year> 1995. </year> <month> 24 </month>
Reference-contexts: An example would be arrow glyphs, which are used to visualize magnitude and directional information in vector fields. Glyphs can also be used to visualize uncertainty in a variety of ways. We have investigated glyphs in the following applications, radiosity [PF96], vector fields <ref> [WPL96, WSF + 95] </ref>, surface interpolation [LSPW96], flow visualization [LPSW96], and key-framed animation [WPL95]. The primary issues in using glyphs for visualizing uncertainty are the sampling frequency/location and the placement orientation (e.g. [TB96]).
References-found: 50

