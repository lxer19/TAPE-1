URL: ftp://ftp.gmd.de/Learning/rl/papers/zhu.thesis.ps.Z
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/anneal/www/tech_reports.html
Root-URL: 
Title: NEURAL NETWORKS AND ADAPTIVE COMPUTERS THEORY AND METHODS OF STOCHASTIC ADAPTIVE COMPUTATION  
Author: HUAIYU ZHU 
Note: Liverpool Ph.D. Thesis 1993  
Affiliation: University of  
Abstract-found: 0
Intro-found: 1
Reference: [AA87] <editor> J. Alspector and R. B Allen. </editor> <title> A neuromorphic VLSI learning system. </title> <editor> In P. Loseleben, editor, </editor> <booktitle> Advanced Research in VLSI: Proceedings of the 1987 Stanford Conference. </booktitle> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: optimisation problems [AK89b, KA89, AK89a], but other applications 2 It should be mentioned here that according to our terminology, many of Grossberg's models are more like ACs than TIPs. 3 However, as mentioned in [Hin89a] the BM implemented on special chips might be over a million times faster than simulation <ref> [AA87] </ref>. CHAPTER 2. REVIEW OF RELATED WORK 13 are possible (For example, the prediction module in x9.2). Perhaps the most widely studied and used neural network is the DCF-NN (deterministic continuous feedforward neural network), called variably "feedforward perceptrons", "multilayer perceptrons", and "back-propagation networks".
Reference: [Abr63] <author> N. Abramson. </author> <title> Information Theory and Coding. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1963. </year>
Reference-contexts: There is even a general deterministic algorithm for doing this, namely the Huffmann coding <ref> [Abr63] </ref>. This seems to imply that a symbolic expert system consisting of rules and counter-rules can do as good as a stochastic system to simulate the reaction of a human adult, which is all that is needed for passing TT.
Reference: [Ada75] <author> R. A. Adams. </author> <title> Sobolev Spaces. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: The function to be approximated need not to be continuous [HSW89], it is only assumed to be in an appropriate Sobolev space (For introduction to Sobolev spaces see <ref> [Ada75] </ref>). The approximation is in the sense of a corresponding Sobolev norm, which can also take into account of the CHAPTER 5. CLASSICAL MODELS OF H. S. NEURAL NETWORKS 63 derivatives [HSW90] (in the weak sense if the function is discontinuous).
Reference: [AH89] <author> S. Amari and T. S. Han. </author> <title> Statistical inference under multi-terminal rate restrictions: A differential geometric approach. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 35(2) </volume> <pages> 217-227, </pages> <year> 1989. </year>
Reference-contexts: Same applies to "stochastic policy" and "mixed strategy". CHAPTER 1. INTRODUCTION 8 provided by the theory of information geometry (IG), which is the study of manifolds of probability distributions <ref> [AH89, AKN92] </ref>. * * * Our main purpose of studying NN is to develop more powerful computational devices. Therefore we shall mainly examine them from mathematical, and sometimes engineering, points of view. These "artificial neural networks" (ANNs) are systems that we can control their internal construction. <p> The necessary theoretical tools for dealing with this has been provided by the theory of information geometry (IG), which studies the differential manifold of probability distributions <ref> [AKN92, AH89, BNCR86] </ref>. The parameterised distributions of a S-NN is restricted to a submanifold of the information manifold. The learning rules of S-NN specify movements in this submanifold.
Reference: [AHM + 88] <author> B. Andersen, K. H. Hoffman, K. Mosgaard, J. D. Nulton, J. M. Pedersen, and P. Salamon. </author> <title> On lumped models for thermodynamic properties of simulated annealing problems. </title> <journal> J. de Phys., </journal> <volume> 49 </volume> <pages> 1485-1492, </pages> <year> 1988. </year>
Reference-contexts: One major theoretical breakthrough in the study of the SA method occurred in the research of cooling schedules which, in our interpretation, is the study of how to assign the value to the missing information. The results of constant thermodynamic speed (CTS) <ref> [NS88, AHM + 88, SNH + 88] </ref> were based on purely theoretical considerations, and are of a quite universal character.
Reference: [AHS85] <author> D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. </author> <title> A learning algorithm for Boltzmann machines. Cog. </title> <journal> Sci., </journal> <volume> 9 </volume> <pages> 147-169, </pages> <year> 1985. </year>
Reference-contexts: RL rules belong to ME learning rules, some SL rules, such as the back propagation (BP) rule [RHW86, Wer74, Par85] are special cases of RL. Some other SL rules, such as the Boltzmann machine (BM) learning rule <ref> [AHS85, HS86] </ref> is a ML learning rule. Furthermore, any UL rule must also have some kind of performance evaluation, the only difference being that they are built-in, but we shall not consider UL rules here. <p> This was generalised to SQB-NN (stochastic quantised feedback neural network) in <ref> [AHS85, HS86] </ref>, which they call "Boltzmann machine" (BM). The BM is perhaps the most versatile neural network in many respects, although the computation on BM simulated on a conventional computer is expensive 3 . <p> The reason this is allowed is that the original problem itself is usually an idealisation. CHAPTER 2. REVIEW OF RELATED WORK 19 networks (S-NN), such as Boltzmann machine <ref> [AHS85, HS86] </ref>. A S-NN is a variant GS which, instead of producing a sample in each member of a sequence of distributions approaching the Boltzmann distribution, produces a parameterised (in the form of neural network connection weights) distribution which can be continuously modified. <p> There are actually two variants of SA: The infinite SA method (SA0) takes ! 1, which gives an efficient combinatorial optimisation method, as in [KGV83]. The finite SA method (SA1) takes ! 0 &lt; 1, which gives a method of accelerating a Gibbs sampler, as in <ref> [AHS85] </ref>. Theorem 3.4.4 In the SA0 method, p k ! p fl . In the SA1 method, p k ! BD (e; 0 ). Proof. The solution sequence x k produced from steps 2-3 in the SA method satisfy p k ! p . <p> The function f i (s i ) := hr i i s i = r i r i P i (r i js i ) is called the activation function. The bias b can be theoretically merged into the connection matrix A <ref> [AHS85] </ref>. This is done by introducing an imaginary input node 0 which is connected to all the neurons in the network, and extending the matrix A with a column 0, such that x 0 = 1 and 8i : A i0 = b i . <p> SQF-NN: This is called belief network [Nea92, Pea87], and simple reinforcement learn ing networks [Wil87, Wil92]. SQB-NN: This is the most famous Boltzmann machine (BM) <ref> [AHS85, HS86] </ref>. 1 They are in fact the same network with minor variations in detail. CHAPTER 4. THEORY OF H. S. NEURAL NETWORKS 48 Remark 4.3.12 Readers familiar with neural network literature would find that many of the most important neural networks are included in this classification. <p> In the two special cases which concerns us, it has been proved <ref> [Hop82, Hop84, AHS85] </ref> that DCB-NN and SQB-NN are strongly stable. (See also x5.4, x5.2, and x6.2.) Therefore the SQFB-NN and DCFB-NN are also stable. Example 4.5.1 Consider the DQ-NN 1 1 1 1 ! 1. (Figure 4.2 (a)) Suppose that the initial state is [+ + ]. <p> Usually v = @E (w). SL can be either ME learning or ML learning. If the evaluation operator is defined by (4.6.8), the CHAPTER 4. THEORY OF H. S. NEURAL NETWORKS 56 learning rule is belong to ML (eg. learning rules in <ref> [AHS85] </ref>). <p> known as feedforward perceptrons, multilayer perceptrons, back-propagation network; the SQB-NN, also known as the Boltzmann machine; and the DCB-NN; also known as the (continuous) Hopfield net, the deterministic Boltzmann machine, and the feedback perceptrons. 5.2 SQB-NN: Boltzmann Machine The SQB-NN is known as the Boltzmann machine (BM) (with trivial variance) <ref> [AHS85, HS86] </ref>. <p> Furthermore, 8g 2 [R N ! R) : [g (r)] x = hg (r)i x . The proof of this theorem can be found in, for example, [GG84, Ami89, AK89b]. See also <ref> [MRR + 53, KGV83, AHS85, HS86] </ref> for supplementary information. <p> Remark 5.2.1 The random sequential updating (5.2.1) realizes a Gibbs sampler GS (BD (T; 1)). It can be accelerated by the finite simulated annealing (SA1) method, as being the common practice in the literature since <ref> [AHS85] </ref>. Since this hinders the parallelism of the network, and since this does not alter the final result in any way, we shall not consider this possibility any more. Some other authors (eg. [AK89b]) apply the infinite SA method (SA0) to the BM. <p> CHAPTER 5. CLASSICAL MODELS OF H. S. NEURAL NETWORKS 61 5.2.2 GF-ML Learning rules The SQB-NN can have several different learning rules, GF-ME, GF-ML, SA-ME and SA-ML. The BM learning rule <ref> [AHS85] </ref> is in theory a GF-ML learning rule, but the experiments reported in that paper is actually performed with a crude form of SA-ML learning (x6.6 and x7.6). In this section we only consider GF-ML learning rule. Later we shall show its relation to GF-ME learning rule. <p> Definition 5.2.4 (GF-ML learning rule for SQB-NN) w + = ffi (w; r; v): The BM learning rule <ref> [AHS85] </ref> is defined as GF learning on RCE, w T ~ @T @T 0 # CHAPTER 5. CLASSICAL MODELS OF H. S. <p> Some of the more important in this family are the Metropolis process [MRR + 53], the Glauber process [Gla63] and the local Boltzmann process <ref> [AHS85] </ref>. They will be discussed in detail in x7.2, when an efficient implementation on conventional computers is also introduced. 6.3 Relations with Other Networks 6.3.1 Relation with SQB-NN (Boltzmann machine) The SQFB-NN is a generalization of SQB-NN (x5.2). Therefore the structure of SQFB-NN is more flexible than that of SQB-NN. <p> The 92 CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 93 detailed balance gives p i (+j) so only one independent parameter left. There are two important GS in this one-parameter family: the Metropolis process [MRR + 53] and the local Boltzmann process <ref> [AHS85] </ref>. <p> CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 96 7.3.2 The class of encoder problems We use as our primary example the well known encoder problem <ref> [AHS85, RHW86] </ref>, which is described as the following: Let R = f1; 1g, n 2 N. Define R n 1 := fx 2 R n : j fi : x i &gt; 0g j = 1g. Let X = Y R n . <p> where e is normalised from d with Definition 7.3.1 ML: It is required that K (P 0 ; P ) ! 0 or K (P; P 0 ) ! 0, where K is the Kullback separation, P is the network distribution and P 0 is distribution corresponding to identity mapping <ref> [AHS85, LL88] </ref>. As we have seen in x6.6, these two criteria are related by @K (P; P 0 ) = @w where f = e log P (yjx) and P 0 = BD (e; ). <p> TC graphs of the ideal learning as closely as possible so as to maintain quasi-equilibrium at all temperatures, while on the other, converge as fast as possible on the IE, IS, IT, IC graphs. 7.3.5 The blurred identity mapping encoder problem The experiments on the Boltzmann machine (BM) reported in <ref> [AHS85] </ref> were conducted on the blurred identity mapping encoder problem: The network distribution P = P w is required to approximate, in the sense of K (P 0 ; P ), the target distribution P 0 which is formed by independently flipping each bit of x, with the probabilities log i <p> The maximum of heat capacity of AHS encoder can not be analytically derived unless a = b, but it is quite easy to find out numerically. The corresponding parameters for the particular experiments on the encoder problems described in <ref> [AHS85] </ref> are summarised in Table 7.1. at 0 at c problem a b 1= 0 C=n hei =n * 1= c C c =n 4-2-4 log :05 log :15 1.3210 0.4048 0.8672 1.20 1.1760 0.4121 :98 :85 40-10-40 log :0025 log :1 2.9471 0.0981 0.9932 7.90 1.1872 0.4323 Table 7.1: The <p> at c problem a b 1= 0 C=n hei =n * 1= c C c =n 4-2-4 log :05 log :15 1.3210 0.4048 0.8672 1.20 1.1760 0.4121 :98 :85 40-10-40 log :0025 log :1 2.9471 0.0981 0.9932 7.90 1.1872 0.4323 Table 7.1: The parameters corresponding to the test results reported <ref> [AHS85] </ref> The parameters a and b are from pages 158, 160 and 162 of [AHS85] for the 4-2-4, 8-3-8, and 40-10-40 encoders, respectively. The column labeled * is the total bit error of target (x7.5). <p> c =n 4-2-4 log :05 log :15 1.3210 0.4048 0.8672 1.20 1.1760 0.4121 :98 :85 40-10-40 log :0025 log :1 2.9471 0.0981 0.9932 7.90 1.1872 0.4323 Table 7.1: The parameters corresponding to the test results reported <ref> [AHS85] </ref> The parameters a and b are from pages 158, 160 and 162 of [AHS85] for the 4-2-4, 8-3-8, and 40-10-40 encoders, respectively. The column labeled * is the total bit error of target (x7.5). The TE, TS, TT and TC graphs for the Hamming distance encoder problem and the particular encoder problems as given in [AHS85], both shown in Figure 7.17 & 7.18, are <p> are from pages 158, 160 and 162 of <ref> [AHS85] </ref> for the 4-2-4, 8-3-8, and 40-10-40 encoders, respectively. The column labeled * is the total bit error of target (x7.5). The TE, TS, TT and TC graphs for the Hamming distance encoder problem and the particular encoder problems as given in [AHS85], both shown in Figure 7.17 & 7.18, are almost indistinguishable. 7.4 Speed of Averaging, Learning and Annealing The learning rules of SQFB-NN are derived under the assumption that all the processes can be performed arbitrarily slowly. <p> None of these graphs would be available if simulated annealing were not used in the learning process. The same is true for IC graph. Although IE, IS and IT graphs are well defined even without SA, normally only the IE graph is given in the literature (eg., <ref> [AHS85, PA87] </ref>, among many others). 7.4.3 Learning speeds Generally speaking, learning speed refers to the speed of changing the weights. We first derive some auxiliary results of a general parameterised distribution. CHAPTER 7. <p> IMPLEMENTATION OF SQFB NEURAL NETWORKS 120 CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 121 CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 122 CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 123 CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 124 CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 125 defined in <ref> [AHS85] </ref> (a) CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 126 defined in [AHS85] (b) CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 127 CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 128 CHAPTER 7. <p> IMPLEMENTATION OF SQFB NEURAL NETWORKS 122 CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 123 CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 124 CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 125 defined in <ref> [AHS85] </ref> (a) CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 126 defined in [AHS85] (b) CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 127 CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 128 CHAPTER 7. <p> So it is certain that the network has learned completely when hei &gt; 1 2=n 2 . This usually happens after = c =2, or, equivalently, = 2 c . Another method to assess the quality of learning is to inspect the synapse directly, as was done in <ref> [AHS85] </ref>. We found empirically from examining sample runs of the experiments that if the learning will eventually be complete, all the n possible codes of the hidden layer are most likely to be used 1 when is around c . <p> With larger speed (within 10 times, and with similar ratio between the parameters) the convergence is to some local optimum with * being one or two. 7.6 Comparison with Results of Others 7.6.1 Different uses of neural networks To compare our results to the results given in <ref> [AHS85] </ref>, it is necessary to analyse what is actually learned in those experiments. The BM learning rule is a ML learning rule, while ours is a ME learning rule. <p> They are equivalent 2 if we use the system-target interpretation of the ME learning rule, provided that the temperature is not zero in the ME learning, and that the target distribution is not degenerate in ML learning. The latter is satisfied by the experiments on BM in <ref> [AHS85] </ref> which blurs the target, but not in [PA87]. We explain the details as the following. <p> We explain the details as the following. First we note that there are basically two ways of using neural networks as a computational tool [Gs88, Pin88], either in the learning process, as in <ref> [AHS85, PA87] </ref> , or in the dynamics, as in [AK89b, Hop84]. We have to distinguish between the "computational temperature" and the "learning temperature". What is called "temperature" in the literature corresponds to computational temperature which appears in the SA1 method for accelerating the GS. <p> The SA1 used in the BM learning rule is only a way of arriving at the Boltzmann distribution at a faster speed, without it the Boltzmann distribution can also be reached eventually. In the examples given in <ref> [AHS85] </ref> the final computational temperature is 10 and the learning speed is 2. This network can be equivalently implemented using an annealing schedule where the computational temperature, the energy (corresponding to the connection weights) and the learning speed are scaled down by a factor of 10. <p> All the learning rules we know of are GF-rules, which corresponds to zero learning temperature SA learning rules, or, equivalently, infinite annealing speed SA learning rules. 7.6.2 Comparison of numerical results The learning time of experiments on BM in <ref> [AHS85] </ref> are measured in the following form C = (2n + 1)E; E = 20T;(7.6.1) where C is the number of "learning cycles", E is the equilibration time, and T is the "unit time", which is the average time of probing all the neurons in the network once. <p> Therefore the equilibration time E is comparable to that in our experiments. (In the SQFB-SA-ME learning rule, there is one equilibration in one presentation.) The criteria used in <ref> [AHS85] </ref> to determine the duration of learning is not quite clear, but it basically involves examining the sign of the weights. We have observed quite reliably that the sign of weights become fixed at about = c . <p> We have observed quite reliably that the sign of weights become fixed at about = c . Therefore we define the time it takes to arrive at = c from = 0 as the learning time of our method to be compared with <ref> [AHS85] </ref>. This criterion itself is not directly applicable to the experiments in [AHS85] since, as analysed earlier, their experiments are set in fixed = 0 c . The results of [AHS85] are as follows. <p> Therefore we define the time it takes to arrive at = c from = 0 as the learning time of our method to be compared with <ref> [AHS85] </ref>. This criterion itself is not directly applicable to the experiments in [AHS85] since, as analysed earlier, their experiments are set in fixed = 0 c . The results of [AHS85] are as follows. For 4-2-4 problems, the median run time is 110C = 990E, the maximum run time is 1810C = 16290E. <p> takes to arrive at = c from = 0 as the learning time of our method to be compared with <ref> [AHS85] </ref>. This criterion itself is not directly applicable to the experiments in [AHS85] since, as analysed earlier, their experiments are set in fixed = 0 c . The results of [AHS85] are as follows. For 4-2-4 problems, the median run time is 110C = 990E, the maximum run time is 1810C = 16290E. For the 8-3-8 problems, the median run time is 1570C = 26700E. For the 40-10-40 problem, the run time is 900C = 72900E. <p> Our corresponding results are as follows. For 4-2-4 problem, run time is about 1000E. For 8-3-8 problem, the runtime is about 40000E. Therefore, our general automatic learning rule performs comparably (to within a factor of two) with the hand-on learning rule in <ref> [AHS85] </ref> which is specially tailored to the particular encoder problems. Since there are so many technical differences 3 between these results which are likely to contribute to the overall speed, it is best to say that the numerical results are comparable. This is impressive considering that the results of [AHS85] is <p> in <ref> [AHS85] </ref> which is specially tailored to the particular encoder problems. Since there are so many technical differences 3 between these results which are likely to contribute to the overall speed, it is best to say that the numerical results are comparable. This is impressive considering that the results of [AHS85] is achieved mainly because of human intervention, while with the SA learning rule everything is automatic. 7.6.3 What the original BM learning rule learns The experiments in [AHS85] do not in any way support the BM learning rule studied there, since they actually implement a crude form of simulated annealing <p> This is impressive considering that the results of <ref> [AHS85] </ref> is achieved mainly because of human intervention, while with the SA learning rule everything is automatic. 7.6.3 What the original BM learning rule learns The experiments in [AHS85] do not in any way support the BM learning rule studied there, since they actually implement a crude form of simulated annealing performed at an artificially selected fixed temperature. <p> The TE, TS, TT and TC graphs for the Hamming distance encoder problem and the particular encoder problems as given in <ref> [AHS85] </ref> are shown in Figure 7.17 & 7.18. By carefully examining these graphs and the parameters listed in Table 7.1, we can conclude that in these experiments 1. <p> Such judicious choice of 0 is unlikely to be made randomly, but rather come after many trial-and-error experiments. However, the success reported in <ref> [AHS85] </ref> depends critically on these choices. Remark 7.6.1 The experiments in [AHS85] on 40-10-40 problem employed another technique, apart from the crude form simulated annealing: The network was effectively exposed to a changing environment with two phases. <p> Such judicious choice of 0 is unlikely to be made randomly, but rather come after many trial-and-error experiments. However, the success reported in <ref> [AHS85] </ref> depends critically on these choices. Remark 7.6.1 The experiments in [AHS85] on 40-10-40 problem employed another technique, apart from the crude form simulated annealing: The network was effectively exposed to a changing environment with two phases. In the first phase of learning, the network is only required to learn to produce output distribution unconditionally, ie., independent of the input. <p> These results show that simulated annealing, even in the primitive form as in <ref> [AHS85] </ref>, is quite indispensable in the learning process to overcome local optima. Since in our experimental setting there is no such artificial attributes as batch learning, two phase cycle, clamped output unit, two phase equilibration, etc, it is not possible to directly verify their results. <p> The contrast to Figure 7.5 & 7.6 is striking. Furthermore, these local optima are generally insensitive to the learning parameters (within a factor of 10). 7.6.4 The SA-ML learning rule for SQFB-NN The problem with the techniques employed in <ref> [AHS85] </ref> is that there is no efficient way to decide the correct amount of blurring, upon which the whole success depends. If the blurring is too small, the learning will be extremely slow, due to the time spent at local optima. <p> Choose any energy function whose ground state corresponds to the strict target distribution at zero temperature. For example, the Hamming distance with the identity mapping, or any of the choices made in <ref> [AHS85] </ref>. 2. Blur the target distribution to the Boltzmann distribution corresponding to a cer tain temperature. 3. Reduce temperature as learning proceeds according to a cooling schedule. This will do away with the necessity of choosing the amount of blurring artificially. <p> Several techniques most frequently cited in the literature were tested, including "momentum term" and random initial weights [RHW86], "weight decay" [HS86] and added noise <ref> [AHS85] </ref>, none of them completely overcome this problem. The mechanism by which the network converges to local optima is the following: since the network is fully connected, whenever one event happens, the evaluation and policy of any other events will be changed as well.
Reference: [AK89a] <author> E. Aarts and J. Korst. </author> <title> Boltzmann machine as a model for parallel annealing. </title> <journal> Algorithmica, </journal> <volume> 6 </volume> <pages> 437-465, </pages> <year> 1989. </year>
Reference-contexts: The BM learning rule was given as a gradient-following (GF) rule, but in their experiments with BM they actually used the entropy principle implicitly. Unfortunately the way they presented it as a technical skill prevented others from recognising its importance. SQB-NN are usually used in combinatorial optimisation problems <ref> [AK89b, KA89, AK89a] </ref>, but other applications 2 It should be mentioned here that according to our terminology, many of Grossberg's models are more like ACs than TIPs. 3 However, as mentioned in [Hin89a] the BM implemented on special chips might be over a million times faster than simulation [AA87].
Reference: [AK89b] <author> E. Aarts and J. Korst. </author> <title> Simulated Annealing and Boltzmann Machines. </title> <editor> J. </editor> <publisher> Wiley & Sons, </publisher> <address> Chichester, </address> <year> 1989. </year>
Reference-contexts: The BM learning rule was given as a gradient-following (GF) rule, but in their experiments with BM they actually used the entropy principle implicitly. Unfortunately the way they presented it as a technical skill prevented others from recognising its importance. SQB-NN are usually used in combinatorial optimisation problems <ref> [AK89b, KA89, AK89a] </ref>, but other applications 2 It should be mentioned here that according to our terminology, many of Grossberg's models are more like ACs than TIPs. 3 However, as mentioned in [Hin89a] the BM implemented on special chips might be over a million times faster than simulation [AA87]. <p> With this formal transform the propagation equation becomes s 0 = Ax. Remark 4.2.4 Some authors use the diagonal of B to represent the bias of the neurons <ref> [AK89b] </ref>. This is only applicable when the states of the neurons are in f0; 1g. The network equations have a recursive form: r depends on s, and vice versa. There are several possible interpretations of the network equations, each of them giving rise to a different dynamics. <p> Furthermore, 8g 2 [R N ! R) : [g (r)] x = hg (r)i x . The proof of this theorem can be found in, for example, <ref> [GG84, Ami89, AK89b] </ref>. See also [MRR + 53, KGV83, AHS85, HS86] for supplementary information. <p> Since this hinders the parallelism of the network, and since this does not alter the final result in any way, we shall not consider this possibility any more. Some other authors (eg. <ref> [AK89b] </ref>) apply the infinite SA method (SA0) to the BM. The resulting P (rjx) is a uniform distribution on fr : max T (rjx)g. This can be used as an optimization method if T is interpreted as an objective function. <p> The second identity follows Theorem 3.4.7, as d hei = x dP yjx e yjx = x t d = x ff The third identity then follows from the fact that = 1=. Remark 7.3.3 The second and third identities in the above appeared in <ref> [AK89b] </ref>, but they seem to have been well known in statistical mechanics (when there is no x) [KGV83, NS88]. 7.3.4 Ideal learning for the Hamming distance encoder problem We consider Hamming distance encoder here, which has some very nice features which makes it easy to be treated analytically. <p> We explain the details as the following. First we note that there are basically two ways of using neural networks as a computational tool [Gs88, Pin88], either in the learning process, as in [AHS85, PA87] , or in the dynamics, as in <ref> [AK89b, Hop84] </ref>. We have to distinguish between the "computational temperature" and the "learning temperature". What is called "temperature" in the literature corresponds to computational temperature which appears in the SA1 method for accelerating the GS.
Reference: [AKN92] <author> S. Amari, K. Kurata, and H. Nagaoka. </author> <title> Information geometry of Boltzmann machines. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 3(2) </volume> <pages> 260-271, </pages> <year> 1992. </year>
Reference-contexts: Same applies to "stochastic policy" and "mixed strategy". CHAPTER 1. INTRODUCTION 8 provided by the theory of information geometry (IG), which is the study of manifolds of probability distributions <ref> [AH89, AKN92] </ref>. * * * Our main purpose of studying NN is to develop more powerful computational devices. Therefore we shall mainly examine them from mathematical, and sometimes engineering, points of view. These "artificial neural networks" (ANNs) are systems that we can control their internal construction. <p> The necessary theoretical tools for dealing with this has been provided by the theory of information geometry (IG), which studies the differential manifold of probability distributions <ref> [AKN92, AH89, BNCR86] </ref>. The parameterised distributions of a S-NN is restricted to a submanifold of the information manifold. The learning rules of S-NN specify movements in this submanifold. <p> Intuitively, a manifold is a "smooth curve", or "smooth body", etc., depending on its dimension, but it is not necessarily a submanifold in the Euclidean space of the same dimension. Introduction to the theory of information geometry can be found in <ref> [AKN92] </ref>, but our notation is somewhat different. 3.3.1 Information manifolds Let be a sample space, X = fx 1 ; : : : ; x n g be a finite set. <p> In particular, let P (N n ) ~ = n1 . With differential structure of n1 , P (X) becomes an (n 1)-dimensional differentiable manifold, called the information manifold based on X <ref> [AKN92] </ref> 2 . The standard differential 2 The term probability manifold might be more descriptive, but we shall adhere to the standard terminology. CHAPTER 3. ADAPTIVE INFORMATION PROCESSING 23 manifold n1 has a role in stochastic computation similar to that of R n in numerical algebra. <p> We say that P (X) is spanned by P 0 (X), since p i = P Remark 3.3.1 The manifold P (X) is actually a Riemannian manifold if endowed with the Fisher information matrix [Kul59] defining the Riemannian metric <ref> [AKN92] </ref>, by which P (X) is a curved manifold. The Fisher information matrix is closely related to the concept of covariance, which is of the utmost importance for all the learning rules for stochastic neural networks, and underlies all the "Hebb-like" learning rules. <p> The Fisher information matrix is closely related to the concept of covariance, which is of the utmost importance for all the learning rules for stochastic neural networks, and underlies all the "Hebb-like" learning rules. The so-called dually flat property of P (X) in <ref> [AKN92] </ref> is also closely related to the P -coordinates and T -coordinates to be defined below. (Compare Theorem 3.4.5 with the properties of Riemannian metric of n1 [AKN92].) We shall not pursue this issue further. Notation. Let p; q 2 R n . <p> The so-called dually flat property of P (X) in <ref> [AKN92] </ref> is also closely related to the P -coordinates and T -coordinates to be defined below. (Compare Theorem 3.4.5 with the properties of Riemannian metric of n1 [AKN92].) We shall not pursue this issue further. Notation. Let p; q 2 R n . Then p ~ q : () p i q j = p j q i ; 8i; j: This notation will be used throughout the thesis. <p> It is not symmetric between the two distributions, but otherwise satisfies the axioms of a pseudo-metric: K (p; q) 0, K (p; q) = 0 () p = q <ref> [AKN92] </ref>. Notation. Let p 2 n1 , and 8k 2 N : p k 2 n1 .
Reference: [Alm87] <author> L. B. Almeida. </author> <title> A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. </title> <booktitle> In Proc. IEEE Intl. Conf. on Neural Networks, </booktitle> <pages> pages 609-618, </pages> <year> 1987. </year>
Reference-contexts: A glimpse of sample applications is provided by [MHP90]. The DQB-NN was also generalised in [Hop84] to DCB-NN (deterministic continuous feedback neural network), usually called "(continuous) Hopfield net", the stability of which was also guaranteed by a Lyapunov function. The ME learning rule for DQB-NN was developed by <ref> [Pin87, Alm87, Alm89b, Alm89a] </ref>. DCB-NN was also studied as mean field (MF) approximation of SQB-NN [PA87], where another learning rule was derived from BM learning rule, which is also a GF rule [Hin89b]. The applications of DCB-NN include, but are not restricted to, various optimisation problems [TH86, HT86, PH89, dBM90]. <p> This means, for example, we classify the (discrete) Hopfield net [Hop82] as DQB-Q.NN, since in it each neuron is activated deterministically, although it uses the random sequential updating. Remark 4.3.8 The DCB-NN discussed in <ref> [Hop84, Alm87] </ref> are slightly more general, using f i (s i ) = tanh ( i s i ) as activation function, or, equivalently use B ij j =: B 0 ij as connection weight. <p> DQB-NN: This is the (discrete) Hopfield net [Hop82], DCF-NN: This is the most famous multilayer (soft) perceptron network [RHW86], also called back-propagation network. DCB-NN: This is the continuous Hopfield net [Hop84, HT86], the mean field (MF) BM [PA87] or deterministic BM [Hin89b], and the feedback perceptron network <ref> [Alm87, Pin87] </ref> 1 . SQF-NN: This is called belief network [Nea92, Pea87], and simple reinforcement learn ing networks [Wil87, Wil92]. SQB-NN: This is the most famous Boltzmann machine (BM) [AHS85, HS86]. 1 They are in fact the same network with minor variations in detail. CHAPTER 4. THEORY OF H. S. <p> It is well-known that a dynamic system is stable if and only if it possess a Lyapunov function. The only known universal method of deriving a Lyapunov function is by integration, which necessarily leads to symmetric connections (apart from diagonal scaling as in <ref> [Hop84, Alm87] </ref>). Remark 4.5.5 The F and B structures are uniform: both are described by the number of input units, hidden units and output units, since there is only one kind of connections in each of them. <p> On the other hand, if there is a distance function d on the output space Y so that e (x; y) := d (' 0 (x); y) is well defined, and if the evaluation operator is defined by (4.6.7), the learning rule belongs to ME (eg. learning rules in <ref> [RHW86, Alm87] </ref>) The RL mode belongs to ME, where v = e. It applies to SQ-NN with jY j &lt; 1. In the UL mode, V = ;, but E 0 is rather restricted. <p> are cascaded, and can otherwise be discarded. 5.4 DCB-NN: Continuous Hopfield Net, Mean field Boltz mann Machine and Feedback Perceptron The DCB-NN can be introduced from three different points of view: * As a continuous variant of Hopfield net [Hop84], (DQB ! DCB). * As a variation of feedforward networks <ref> [Alm87, Pin87] </ref>, (DCF ! DCB). * As a mean field Boltzmann machine [PA87, Hin89b]. (SQB ! DCB). <p> Let A, B, C and D be matrices representing the synapse from the input to the neurons, among the neurons and from the neurons to the output, respectively. C and D can be arbitrary but fixed. B is symmetric with zero diagonal. The network of <ref> [Hop84, Alm87] </ref> allows slightly general B, which is equivalent to scaling the output of each neuron independently. Since we are only interested in homogeneous networks we shall not discuss this possibility. Definition 5.4.1 (Dynamics of DCB-NN) r = w (x), y = ' w (x): 1. Input. <p> CHAPTER 5. CLASSICAL MODELS OF H. S. NEURAL NETWORKS 65 The recursive equation (5.4.1) can be solved by the following three different methods. Process (P0) (5.4.1) is iterated and the fixed point is taken as solution [PA87]. Process (P1) The solution is defined as the attractor of differential equation <ref> [Alm87] </ref> s = Br + s 0 ; dt Process (P2) The solution is defined as the attractor of differential equation [Hop84] ds = ff (Br + s 0 s); r = f (s):(5.4.3) In the above ff is some positive constant. <p> Remark 5.4.2 The above proof follows [Hop84]. An equivalent proof of stability was directly applied to (P1) in <ref> [Alm87] </ref>. The free energy f is just the "energy" in [Hop84], with the correspondence ff ~ 1=C i , ~ 1=R i , and S ~ R tanh 1 (r)dr, where C i and R i are called capacitance and resistance in [Hop84]. <p> T v + @f (s)u 1 ; @A t @s @A = v T ff 1 : @B t @s @B = v T ff 1 : Equation (5.4.7) has a solution if and only if (@f (s)B) &lt; 1, which is always satisfied when the equilibrium (5.4.1) is reached <ref> [Alm87] </ref>. Definition 5.4.2 (GF-ME learning rule for DCB-NN) Set w + = ffi (w; r; v), u = (w; r; v), by 1. Output layer. Set v = @e @y T . 2. <p> However, these bounds are not quite practical since they are combinatorial. 6.3.3 Relation with DCB-NN (continuous Hopfield net, etc.) The DCFB-NN is also a generalization of the DCB-NN (x5.4), such as continuous Hop-field net [Hop84], the feedback perceptron network <ref> [Alm87] </ref>, the mean field (MF) model of BM [PA87, Hin89b]. It should be noted that in general the SQFB-NN cannot be replaced by its MF approximation DCFB-NN without compromising its representational power.
Reference: [Alm89a] <author> L. B. Almeida. </author> <title> Back propagation in non-feedforward networks. </title> <editor> In I. Alek-sander, editor, </editor> <booktitle> Neural Computing Architecture: The design of brain-like machines. </booktitle> <publisher> North Oxford Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: A glimpse of sample applications is provided by [MHP90]. The DQB-NN was also generalised in [Hop84] to DCB-NN (deterministic continuous feedback neural network), usually called "(continuous) Hopfield net", the stability of which was also guaranteed by a Lyapunov function. The ME learning rule for DQB-NN was developed by <ref> [Pin87, Alm87, Alm89b, Alm89a] </ref>. DCB-NN was also studied as mean field (MF) approximation of SQB-NN [PA87], where another learning rule was derived from BM learning rule, which is also a GF rule [Hin89b]. The applications of DCB-NN include, but are not restricted to, various optimisation problems [TH86, HT86, PH89, dBM90].
Reference: [Alm89b] <author> L. B. Almeida. </author> <title> Back propagation in perceptrons with feedback. </title> <editor> In Eckmiller, R. and C. </editor> <publisher> van der Malsburg [EvdM89]. </publisher>
Reference-contexts: A glimpse of sample applications is provided by [MHP90]. The DQB-NN was also generalised in [Hop84] to DCB-NN (deterministic continuous feedback neural network), usually called "(continuous) Hopfield net", the stability of which was also guaranteed by a Lyapunov function. The ME learning rule for DQB-NN was developed by <ref> [Pin87, Alm87, Alm89b, Alm89a] </ref>. DCB-NN was also studied as mean field (MF) approximation of SQB-NN [PA87], where another learning rule was derived from BM learning rule, which is also a GF rule [Hin89b]. The applications of DCB-NN include, but are not restricted to, various optimisation problems [TH86, HT86, PH89, dBM90].
Reference: [Ami89] <author> D. J. Amit. </author> <title> Modeling Brain Function: The World of Attractor Neural networks. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1989. </year> <note> 202 BIBLIOGRAPHY 203 </note>
Reference-contexts: One of our contributions will be the identification of a class of structures, the FB-structure, which guarantees stability and is a natural generalisation of F- and B-structures. Concerning the dynamics of NN, there is a formal duality between SQ-NN and DC-NN, informally discussed in <ref> [Hop89, Ami89] </ref>. This concept turned out to be very fruitful in our theory, both in designing networks of rich and versatile dynamics, and in providing penetrating insights of the properties of the networks from different points of view. It CHAPTER 2. <p> It can be proved by directly solving these optimisation problems, by using duality theorems in the theory of convex optimisation, or by translating the corresponding results from statistical mechanics <ref> [LL80, Ami89] </ref>. The above characterisation of the Boltzmann distribution has several different interpretations, all of them based on the fact that entropy is the measure of disorder, or missing information. The following two are most interesting to us. <p> Then the corresponding macro state variables are energy hei, temperature , free energy hf i and entropy hlog pi. The Boltzmann distribution p 0 is the equilibrium distribution in the Gibbs canonical ensemble <ref> [LL80, Ami89] </ref>. Example 3.4.3 (Decision theory) Suppose an agent is to choose actions a 2 N n with distribution p 2 n1 . Let e 2 R n be an evaluation function on N n . Then the expected return is hei. <p> This correspondence seems to be widely recognized (Cf. <ref> [Hop89, Ami89] </ref>), although there seems no explicit statement to this effect in the literature. Theorem 4.4.1 Let R = f1; 1g, e R = [1; 1]. <p> Furthermore, 8g 2 [R N ! R) : [g (r)] x = hg (r)i x . The proof of this theorem can be found in, for example, <ref> [GG84, Ami89, AK89b] </ref>. See also [MRR + 53, KGV83, AHS85, HS86] for supplementary information. <p> A sufficient and necessary condition for the stationary probability distribution to be the Boltzmann distribution is the detailed balance (eg. <ref> [Ami89] </ref>): p x t+1 jx t (~j) = exp (T (~) T ()); 8~; 2 X:(7.2.1) We shall only consider GS with U (x) = fy 2 X : d (x; y) 1g, where X = R n , R = f1; +1g and d is the Hamming distance.
Reference: [And83] <author> J. A. Anderson. </author> <title> Cognitive and psychological computation with neural models. </title> <journal> In IEEE Trans. Sys. Man and Cyber. </journal> <volume> [Vem88], </volume> <pages> pages 799-815. </pages>
Reference-contexts: Our goal is to see how much of the constructions of pure mathematics can be carried out by physical devices restricted in the universe. Considering that there are only about 10 10 neurons in the human brain <ref> [And83] </ref> and about 10 80 atoms in the whole observable universe [Dav82], the following concepts, closely related to the concept of NP-completeness [GJ79], are very important for comparing the computational viability of various theories.
Reference: [And89] <author> C. W. Anderson. </author> <title> Learning to control an inverted pendulum using neural networks. </title> <journal> IEEE Control Systems Magazine, </journal> <pages> pages 31-36, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The policy is also represented in the form of a look-up table, and is applicable to agents with only two choices of actions 6 . Use of neural network to approximate the policy and utility function was proposed in <ref> [And89] </ref>, and was later generalised to problems with more than two actions in the Dyna model [Sut90]. In this model the policy is represented as a Boltzmann distribution over actions. This is of fundamental importance, for it allows the introduction of information theory into the decision theory.
Reference: [BA85] <author> A. G. Barto and P. Anandan. </author> <title> Pattern-recognizing stochastic learning automata. </title> <journal> IEEE Trans. Sys. Man and Cyber., </journal> <volume> SMC-15(3):360-375, </volume> <year> 1985. </year>
Reference-contexts: The development of SQF-NN has several origins, most of them are related to DQF-NN, ie. logical circuits. One origin was the theory of "stochastic learning automata" [NT74], where each automaton was actually a neuron, and many of them were connected to perform certain tasks <ref> [Lak81, NL77, BA85, NT89] </ref>. A network of learning automata without loops is a SQF-NN. Another derivation was from the Bayesian inference, this results in "belief networks" [Pea87]. The ME learning rules for SQF-NN was studied extensively in [Wil92, Wil87, Wil88, Wil90]. <p> Despite their continued popularity in AI research, we shall not discuss these methods any further. The theory of learning automata (LA) was developed from yet another origin <ref> [NT74, BA85] </ref>, which attempts to parameterise the policies of simple agents in a game-like environment. Since the utility function usually does not feature in these theories, they have about the same efficiency as "average over histories", although their main virtue is that there is no need to enumerate the histories.
Reference: [Bar90] <author> A. G. Barto. </author> <title> Connectionist learning for control: An overview. </title> <editor> In Miller, III et al. </editor> <address> [MSW90], </address> <note> chapter 1. </note>
Reference-contexts: This is necessary since the decision module simply select the optimal action deterministically according to the evaluation module. Various models based on these methods have been tested on specific problems in [Lin92, dRMT92, Sin92]. A review of RL can be found in <ref> [Bar90] </ref>. * * * These above methods (game theory, dynamic programming and reinforcement learning) either update the policy in the action space (deterministic policies), or in the policy space by gradient following.
Reference: [BD62] <author> R. E. Bellman and S. E. Dreyfus. </author> <title> Applied Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1962. </year>
Reference-contexts: The theory of dynamic programming (DP) <ref> [Bel57, How60, BD62, DL76] </ref> provides a practical computational method to solve a special type of game, "games against Nature", in which an "agent" (a certain player) improves its policy (strategy) in an environment (all the other players and the rules of the game) which is stationary (ie., the transition probability of
Reference: [Bel57] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Prince-ton, NJ, </address> <year> 1957. </year>
Reference-contexts: The theory of dynamic programming (DP) <ref> [Bel57, How60, BD62, DL76] </ref> provides a practical computational method to solve a special type of game, "games against Nature", in which an "agent" (a certain player) improves its policy (strategy) in an environment (all the other players and the rules of the game) which is stationary (ie., the transition probability of <p> In our formulation the optimal solutions is defined as limits of equilibrium solutions there is no need to study them separately. Our formulation also unifies different approaches in DP, such as fixed horizon programming, discounted reward programming, total return programming and average gain programming <ref> [How60, Bel57, Ros83] </ref>, by the following theorem. Theorem 8.5.3 Let E 2 R nfin with (E) 1. Then 8r 2 R n : 1. The following holds whenever the limit of the denumerator exists: lim h X E k r k=0 1 h E k r = 1:(8.5.5) CHAPTER 8.
Reference: [BGMT89] <author> R. C. Brower, R. Giles, K. J. M. Moriarty, and P. Tamayo. </author> <title> Combining renormalization group and multigrid methods. </title> <journal> J. Comp. Phys., </journal> <volume> 80 </volume> <pages> 472-479, </pages> <year> 1989. </year>
Reference-contexts: Their interpretation of the approach to equilibrium as the approximation in the sense of Kullback separation is very convenient for our applications (also see <ref> [BGMT89] </ref>), and our experiments in x7.5 provide quantitative support for the CTS schedule.
Reference: [BJ90] <author> R. Beale and T. Jackson. </author> <title> Neural Computing: An Introduction. </title> <publisher> IOP Publishing, </publisher> <year> 1990. </year>
Reference-contexts: There were various types of neural networks developed since 1960's, which we shall not study here. These include, most notably, various models developed by Grossberg and colleagues 2 , see [Gro86] and references therein, and those of Kohonen, see [Koh77, Koh84]. An introduction to these can be found in <ref> [BJ90] </ref>. A survey of structures and dynamics of deterministic neural networks are given by [Gro88]. It especially covers those well known in the biological sciences and intended primarily as models of BNN. Many learning rules have been proposed for various neural networks.
Reference: [BL91] <author> E. K. Blum and L. K. Li. </author> <title> Approximation theory and feedforward networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 511-515, </pages> <year> 1991. </year>
Reference-contexts: An upper bound for the number of neurons needed for a two-hidden layer DQF-NN was given in <ref> [BL91] </ref>. This result is immediately applicable to DCF-NN and SQF-NN (stochastic quantised feedforward neural network), as they are generalisations of DQF-NN. Other studies in DCF-NN include how to accelerate convergence of learning [Sam91, RIV91, Fah88], and other aspects of learning [Sus92]. <p> These results directly apply to the SQFB-NN, since all of them are based on the assumption of a network of sufficiently large number of neurons. <ref> [BL91] </ref> gives upper bounds on the number of neurons needed for approximation for a three-layer DQF-NN, this result is also applicable to SQFB-NN, since SQFB-NN generalizes SQF-NN which in turn generalizes DQF-NN.
Reference: [BNCR86] <author> O. E. Barndorff-Nielsen, D. R. Cox, and N. Reid. </author> <title> The role of differential geometry in statistical theory. </title> <journal> Intern. Stat. Rev., </journal> <volume> 54(1) </volume> <pages> 83-96, </pages> <year> 1986. </year>
Reference-contexts: The necessary theoretical tools for dealing with this has been provided by the theory of information geometry (IG), which studies the differential manifold of probability distributions <ref> [AKN92, AH89, BNCR86] </ref>. The parameterised distributions of a S-NN is restricted to a submanifold of the information manifold. The learning rules of S-NN specify movements in this submanifold.
Reference: [Boc91] <author> P. Bock. </author> <title> Building the ultimate machine | the emergence of artificial cognition. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 496 </volume> <pages> 472-481, </pages> <year> 1991. </year>
Reference-contexts: It has been a quite widely accepted common sense that the main reason computers are not as "smart" as brains is that it is not as fast or as large as brain in the parallel sense <ref> [Boc91] </ref>. We suggest that the main difference is that brain computation only uses relevant information, instead of data.
Reference: [BR92] <author> A. L. Blum and R. L. Rivest. </author> <title> Training a 3-node neural network is NP-complete. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 117-127, </pages> <year> 1992. </year>
Reference-contexts: So DQ-NN can be viewed as extremes of both DC-NN and SQ-NN. On the other hand, DQ-NN is less useful in practice owing to the above singularity which prevent it to have a reasonable learning rule <ref> [BR92, Shv88] </ref>. This is hardly surprising, since for DQ-NN the parameterization of realizable processing by the weight vector is discontinuous. When the states of both SQ-NN and DC-NN in a dual pair are being referred at the same time, there need to be separate names for them.
Reference: [BS50] <author> H. W. Bode and C. E. Shannon. </author> <title> A simplified derivative of linear least square smoothing and prediction theory. </title> <journal> Proc. IRE, </journal> <volume> 38 </volume> <pages> 417-425, </pages> <year> 1950. </year> <note> reprinted in [ET73]. </note>
Reference-contexts: A control problem is a differentiable optimisation problem over time on a continuum (connected compact set), called control space. Control methods are fundamentally inadequate to solve global optimisation problems with multiple local optimum (See <ref> [BS50] </ref> for an interesting example). For a one-step problem, a decision problem reduces to an integer optimisation problem, while a control problem reduces to a differentiable opti-misation problem.
Reference: [BSA83] <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <title> Neuron-like elements that can solve difficult learning control problems. </title> <journal> IEEE Trans. Sys. Man and Cyber., </journal> <volume> 13 </volume> <pages> 834-846, </pages> <year> 1983. </year>
Reference-contexts: These methods are also called backpropagation through time (BTT) [Wer90a]. The first "average over states" method in which both the policy and evaluation are parameterised is usually traced to <ref> [BSA83] </ref>. The policy is also represented in the form of a look-up table, and is applicable to agents with only two choices of actions 6 . <p> The second factor is called the reinforcement factor. The Hebbian factor is also called "characteristic eligibility" in reinforcement learning literature [Wil87, Wil92], following <ref> [BSA83] </ref>. Notation. Denote by [] k1 and [] some approximate time averages approximating hi k1 and hi, respectively. The operator [] k1 can be obtained at the same time when r k is equilibrating. The operator [] is computed over the whole learning process. <p> If two functions e 1 2 F; e 2 2 G satisfies e 1 = U e 2 , e 2 = Q (e 1 ), then e 1 = u . Using R ; (e 1 ) generalises those methods used in <ref> [BSA83, Sut90] </ref>, while using U ; (e 2 ) generalises the Q-learning method [Wat89]. CHAPTER 8. ADAPTIVE MARKOV DECISION PROCESS 147 Approximate SA method 2 (AMDP-ASA2) 1. Set e 1 := 0, e 2 := 0. Set = 0. Set = rnd. 2. <p> This is more of a problem when the utility is monotonically related to L . Example 8.5.1 In the "pole-balancing problem" <ref> [BSA83] </ref>, the objective is to balance a "pole" on a "cart" for as long as possible by pushing the cart to left or right by fixed force in fixed intervals. Therefore L ! 1 as ! fl , since the optimal policy can keep the pole balanced forever. CHAPTER 8. <p> BAC can have two basic modules, decision module (D) and evaluation module (E1), and two optional modules, prediction module (P) and action evaluation module (E2). The modules (D) and (E) were present in <ref> [Sam59, BSA83] </ref>, the modules (D), (E1) and (E2) appear in [Wat89], and the module (D), (E1) and (P) appear in [Sut90]. 9.2 General Description of BAC In this chapter our description will be implementation oriented, and not as formal as the previous chapter. <p> BASIC ADAPTIVE COMPUTERS 158 of solutions so that we can compare experiments with theoretical predictions, and on the other, they have features which represent a class of problems more general than those treated in the literature so far <ref> [BSA83, Sut90, Lin92, Wat89, WB91, Tes92] </ref>. Variations of these test problems can serve as new test problems for basic adaptive computers (BAC) with more modules. <p> One such arbitrariness occur in the measure of performance. Various measures which are only meaningful to the (artificial) problems have been used, such as surviving time, error rate, words recognised, just to mention a few (eg., <ref> [BSA83, Sut90, Lin92, WB91, Tes92] </ref>). Example 9.5.1 A chess player should learn exactly the same strategy in the following two settings: (1) win= 1, lose= 1, tie=0; (2) win= 100, lose= 0, tie=50. <p> This is because in most problems the start and end of a play have distinctive characteristics, and learning methods should be gentle enough to actually experience several full circles of the game before changing behaviour too much. For example, in the "pole balancing problem" <ref> [BSA83] </ref>, the only non-zero reward comes after a play terminates, while the occurrence of such events are pushed to more distant future as the policy improves. We use 2 -L as the scaling on ff, fi, fl, and , where L is expected (un-discounted) lifetime of a play.
Reference: [BSW90] <author> A. G. Barto, R. S. Sutton, and C. J. C. H. Watkins. </author> <title> Learning and sequential decision making. In Gabriel and Moore [GM90]. </title>
Reference-contexts: The temporal difference (TD) method [Sut84, Sut88] for predicting the expected value of a function of the states of a Markov chain can be used for updating the evaluation module <ref> [BSW90] </ref>. Its convergence was proved with a local representation of state space [Day92]. It extracts the first order statistics in such a way as to achieve a balance between minimum bias and minimum variation.
Reference: [BW88] <author> E. B. Baum and F. Wilczek. </author> <title> Supervised learning of probability distributions by neural networks. </title> <editor> In D. Z. Anderson, editor, </editor> <booktitle> Neural Information Processing Systems. Amer. </booktitle> <institution> Inst. Phys., </institution> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The ME learning rules for SQF-NN was studied extensively in [Wil92, Wil87, Wil88, Wil90]. Recently the ML learning rule for SQF-NN was studied in [Nea92]. It was cited in [PH89] that a similar learning rule was studied for SQF-NN in <ref> [BW88] </ref>. A general framework for structure and dynamics of PDP models has been described in [RHM86]. In particular it defines the classes of quasilinear and semilinear neural networks, for which we shall develop a mathematical theory. See [Sej81, Pin88] for other attempts on general theory of NN.
Reference: [Cia89] <author> P. G. Ciarlet. </author> <title> Introduction to Numerical Linear Algebra and Optimization. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1989. </year> <note> BIBLIOGRAPHY 204 </note>
Reference-contexts: We introduce some new mathematical notations, most notably those on mappings, which will be used extensively. Basic concept from pure mathematics can be found in [Die60]. Most results on matrices which we shall use can be found in <ref> [Cia89, Sen73b, CM65, Sen73a] </ref>. Our reference to probability and stochastic processes are [Kol56a, CM65, GS82, GS74]. The slanted typeface generally denote the first or the most formal appearance of a technical term.
Reference: [CM59] <author> H. Chernoff and L. E. Moses. </author> <title> Elementary Decision Theory. </title> <editor> J. </editor> <publisher> Wiley & Sons, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: The mathematical part (ie. excluding the model building) of DP is also called theory of controlled Markov chains (CMC) or Markov CHAPTER 2. REVIEW OF RELATED WORK 15 decision processes (MDP) [Ros83, DY79, Der70]. A quite intuitive introduction to the general applicability of utility function was given in <ref> [CM59] </ref>. * * * It can be seen that games theory treats the environment as being of infinite intelligence (every player chooses optimal strategy), while the DP theory treats the environment as being of zero intelligence (Nature will neither cooperate nor compete).
Reference: [CM65] <author> D. R. Cox and H. D. Miller. </author> <title> The Theory of Stochastic Processes. </title> <publisher> Methuen, </publisher> <year> 1965. </year>
Reference-contexts: In fact our emphasis will be on those abilities of stochastic systems which a deterministic information system, which we call data processing systems, can not have. 3.2 Review of Probability Theory Our references to probability and stochastic processes are <ref> [Kol56a, CM65, GS82, GS74] </ref>. Definition 3.2.1 Let [; F ; Pr] be probability space. Let [A; A] be an arbitrary measure space. <p> The state space X with transition mapping t forms a Markov chain (MC). The concept of closed set and recurrent state of a MC are defined as in <ref> [CM65] </ref>. Closed set is also called recurrent chain in [How60] and commutative set in [Sen73b]. The following theorem gives a sufficient condition for the existence of utility function. Theorem 8.4.7 Suppose that each closed set of the MC (X; t ) has at least one terminal state. <p> We introduce some new mathematical notations, most notably those on mappings, which will be used extensively. Basic concept from pure mathematics can be found in [Die60]. Most results on matrices which we shall use can be found in <ref> [Cia89, Sen73b, CM65, Sen73a] </ref>. Our reference to probability and stochastic processes are [Kol56a, CM65, GS82, GS74]. The slanted typeface generally denote the first or the most formal appearance of a technical term. <p> Basic concept from pure mathematics can be found in [Die60]. Most results on matrices which we shall use can be found in [Cia89, Sen73b, CM65, Sen73a]. Our reference to probability and stochastic processes are <ref> [Kol56a, CM65, GS82, GS74] </ref>. The slanted typeface generally denote the first or the most formal appearance of a technical term. This is usually where it is defined, or the standard reference (cited at the beginning of the chapter or the section) is referred to for definition.
Reference: [Cun85] <author> Y. Le Cun. </author> <title> Une procedure d'apprentissage pour reseau a sequil assymetrique (a learning procedure for asymmetric threshold network). </title> <booktitle> Proceedings of Cognitiva, </booktitle> <volume> 85 </volume> <pages> 599-604, </pages> <year> 1985. </year>
Reference-contexts: CHAPTER 2. REVIEW OF RELATED WORK 13 are possible (For example, the prediction module in x9.2). Perhaps the most widely studied and used neural network is the DCF-NN (deterministic continuous feedforward neural network), called variably "feedforward perceptrons", "multilayer perceptrons", and "back-propagation networks". It was discovered independently by several authors <ref> [RHW86, Cun85, Par85, Wer74] </ref>. It is most famous for its learning rule, called back-propagation (BP) rule, which is the prototype for ME learning rules. A great volume of research has been devoted to various aspects of DCF-NN, of particular importance are the approximation properties [Fun89, HSW89, HSW90, Hor91]. <p> CE, @w It should be remarked that there is no practical implementation for the VBM learning rule as a ML rule, but it is closely related to a good ME rule (x6.6). 5.3 DCF-NN: Multilayer Perceptron The DCF-NN is often called back-propagation network, multilayer perceptron network or feedforward perceptron network <ref> [RHW86, Wer74, Cun85, Par85] </ref>. We give formulation for simple-layered DCF-NN, which consists of a sequence of layers of neurons where each neuron only activate neurons in immediately succeeding layer.
Reference: [Dav82] <author> P. C. W. Davis. </author> <title> The Accidental Universe. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1982. </year>
Reference-contexts: Our goal is to see how much of the constructions of pure mathematics can be carried out by physical devices restricted in the universe. Considering that there are only about 10 10 neurons in the human brain [And83] and about 10 80 atoms in the whole observable universe <ref> [Dav82] </ref>, the following concepts, closely related to the concept of NP-completeness [GJ79], are very important for comparing the computational viability of various theories.
Reference: [Dav87] <author> L. Davis, </author> <title> editor. Genetic Algorithms and Simulated Annealing. </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1987. </year>
Reference-contexts: The parameterised distributions of a S-NN is restricted to a submanifold of the information manifold. The learning rules of S-NN specify movements in this submanifold. Closely related to S-NN and SA are the genetic algorithms (GA) [Hol75], a comparison of GA and SA can be found in <ref> [Dav87] </ref>. Each of these three (S-NN, SA, GA) has an underlying stochastic model described by a Gibbs ensemble which changes with time so that the proportion of good solutions increases. In the original SA method exactly one sample from this ensemble is realized at each step.
Reference: [Day92] <author> P. Dayan. </author> <title> The convergence of TD() for general . Machine Learning, </title> <type> 8(3/4), </type> <year> 1992. </year>
Reference-contexts: The temporal difference (TD) method [Sut84, Sut88] for predicting the expected value of a function of the states of a Markov chain can be used for updating the evaluation module [BSW90]. Its convergence was proved with a local representation of state space <ref> [Day92] </ref>. It extracts the first order statistics in such a way as to achieve a balance between minimum bias and minimum variation. Another method for updating the evaluation module is the Q-learning method [Wat89], which requires a local representation of action space as well.
Reference: [dBM90] <author> D. E. Van den Bout and T. K. Miller, III. </author> <title> Graph partitioning using annealed neural networks. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 1(2) </volume> <pages> 192-203, </pages> <year> 1990. </year>
Reference-contexts: DCB-NN was also studied as mean field (MF) approximation of SQB-NN [PA87], where another learning rule was derived from BM learning rule, which is also a GF rule [Hin89b]. The applications of DCB-NN include, but are not restricted to, various optimisation problems <ref> [TH86, HT86, PH89, dBM90] </ref>. The development of SQF-NN has several origins, most of them are related to DQF-NN, ie. logical circuits.
Reference: [Der70] <author> C. Derman. </author> <title> Finite State Markovian Decision Process. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: The mathematical part (ie. excluding the model building) of DP is also called theory of controlled Markov chains (CMC) or Markov CHAPTER 2. REVIEW OF RELATED WORK 15 decision processes (MDP) <ref> [Ros83, DY79, Der70] </ref>.
Reference: [Die60] <author> J. A. Dieudonne. </author> <title> Foundations of Modern Analysis. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1960. </year>
Reference-contexts: When p is omitted uniform distribution is implied, ie., p (~) := 1=jXj. This is not a very rigorous concept so we shall only use it as a short hand notation. 3.3 Information Geometry We shall use the basic concepts of differential geometry as studied in <ref> [Die60] </ref>, including differentiable manifold, coordinate system, isomorphism, quotient space. Since this material is standard and our usage is rather intuitive, no review is provided in Appendix A. <p> Only default notations used throughout the thesis are listed here, special notations are defined in the text where necessary. We introduce some new mathematical notations, most notably those on mappings, which will be used extensively. Basic concept from pure mathematics can be found in <ref> [Die60] </ref>. Most results on matrices which we shall use can be found in [Cia89, Sen73b, CM65, Sen73a]. Our reference to probability and stochastic processes are [Kol56a, CM65, GS82, GS74]. The slanted typeface generally denote the first or the most formal appearance of a technical term.
Reference: [DL76] <author> S. E. Dreyfus and A. </author> <title> Law. The Art and Theory of Dynamic Programming. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: The theory of dynamic programming (DP) <ref> [Bel57, How60, BD62, DL76] </ref> provides a practical computational method to solve a special type of game, "games against Nature", in which an "agent" (a certain player) improves its policy (strategy) in an environment (all the other players and the rules of the game) which is stationary (ie., the transition probability of
Reference: [Dre79] <author> H. L. Dreyfus. </author> <title> What Computers Can't Do: Revised edition. </title> <publisher> Harper Colophon Books, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Remark 2.2.1 Heuristic rules became rather popular in AI research [NS72], but there has never been any solid theoretical foundation <ref> [Dre79] </ref> 5 . <p> GF methods are by far the most widely used learning methods to date, since they are so simple yet always guarantee local improvement, and are perceived as a first step to more powerful methods. Unfortunately, the first step cannot always proceed to the hoped-for second step <ref> [Dre79, MP88] </ref>. In fact, we shall see that the GF methods almost always produce undesirable results for many interesting problems. We shall also see later that simulated annealing (SA) methods are almost as simple as GF, and overcome most of the undesirable characteristics. <p> This includes four different questions.)? 6. (biology): Is the brain a SECM (B SECM )? 7. How far can we go toward AI in the framework of SECM with technological con straints (maybe simulated in ETM)? For the philosophical question refer to <ref> [Dre79] </ref>, which discusses the failure of symbolic AI 5 . It should be remarked that the final conclusions there (that computers cannot have intelligence) should not be read literally since our terminology is obviously quite different. <p> Early research of AI has generated substantial criticism by claiming too much generality than what is actually generalisable, in particular, by claiming that passing a RTT can be generalised to passing a TT (See <ref> [Dre79] </ref> for detailed discussions). In order to CHAPTER 10.
Reference: [dRMT92] <author> J. del R. Millan and C. Torras. </author> <title> A reinforcement connectionist approach to robot path finding in non-maze-like environments. </title> <journal> Machine Learning, </journal> 8(3/4):363-395, 1992. 
Reference-contexts: This is necessary since the decision module simply select the optimal action deterministically according to the evaluation module. Various models based on these methods have been tested on specific problems in <ref> [Lin92, dRMT92, Sin92] </ref>. A review of RL can be found in [Bar90]. * * * These above methods (game theory, dynamic programming and reinforcement learning) either update the policy in the action space (deterministic policies), or in the policy space by gradient following.
Reference: [DY79] <author> E. B. Dynkin and A. A. Yushkevich. </author> <title> Controlled Markov Processes. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: The mathematical part (ie. excluding the model building) of DP is also called theory of controlled Markov chains (CMC) or Markov CHAPTER 2. REVIEW OF RELATED WORK 15 decision processes (MDP) <ref> [Ros83, DY79, Der70] </ref>. <p> From another point of view, this is a generalisation of SA to multistage problems. Our presentation will be similar to that of <ref> [Ros83, DY79] </ref>, but substantially generalised to allow adaptive improvement. 8.2 The Mathematical Model Definition 8.2.1 (Markov decision process) A Markov decision process (MDP) is a tuple [X; A; E; T; ; t; r; fl], satisfying the following conditions E S fi A; T = N; P t 2 [T ! [E <p> By Theorem 8.3.2 this definition is equivalent to that based on fixed points of R . Note that the usual more intuitive definition of u as a weighted sum of the rewards <ref> [How60, Wit77, DY79, Ros83, Wat89, Sut90] </ref> are only applicable for uniform fl &lt; 1.
Reference: [ET73] <author> A. Ephremides and J. B. Thomas. </author> <title> Random Processes: Multiplicity theory and Canonical Decomposition. </title> <booktitle> Bench Mark Papers in Electrical Engineering and Computer Sciences. </booktitle> <address> Dowden, </address> <publisher> Hutchinson &Ross, </publisher> <address> Stroudsburg, Pennsylvania, </address> <year> 1973. </year>
Reference: [EvdM89] <editor> R. Eckmiller and C. van der Malsburg. </editor> <booktitle> Neural Computers. Proc. NATO Adv. Res. Workshop on Neural Computers. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference: [Fah88] <author> S. E. Fahlman. </author> <title> Fast-learning variations on back-propagation: An empirical study. </title> <editor> In D. S. Touretzky, G. E. Hinton, and Sejnowski, editors, </editor> <booktitle> Connectionist Models Summer School: 1988 Proceedings, </booktitle> <pages> pages 38-51, </pages> <address> San Mateo, CA, 1988. </address> <publisher> Morgan Kaufmann. BIBLIOGRAPHY 205 </publisher>
Reference-contexts: An upper bound for the number of neurons needed for a two-hidden layer DQF-NN was given in [BL91]. This result is immediately applicable to DCF-NN and SQF-NN (stochastic quantised feedforward neural network), as they are generalisations of DQF-NN. Other studies in DCF-NN include how to accelerate convergence of learning <ref> [Sam91, RIV91, Fah88] </ref>, and other aspects of learning [Sus92]. There are probably well over a thousand different published applications of DCF-NN, covering virtually every field where adaptive approximation of a nonlinear function might be of some use. A glimpse of sample applications is provided by [MHP90].
Reference: [FF63] <editor> E. A. Feigenbaum and J. Feldman, editors. </editor> <booktitle> Computers and Thought. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1963. </year>
Reference: [Fle87] <author> R. Fletcher. </author> <title> Practical Optimization. </title> <editor> J. </editor> <publisher> Wiley & Sons, </publisher> <address> Chichester, </address> <year> 1987. </year>
Reference-contexts: Definition 3.4.5 (Entropy Principle) The Entropy Principle is the transformation of problem (3.4.4) into the following associated problem 8 2 R + : max hei + H : p 2 n1 :(3.4.5) Remark 3.4.6 This can be interpreted as the barrier function method of solving con strained optimisation problem <ref> [Fle87] </ref>, where H acts as the barrier function. The entropy is a very good barrier function: it is symmetric, finite, and convex, and it has infinite derivative at the boundary of the simplex [Sha48, Kul59].
Reference: [FS90] <author> J. A. Franklin and O. G. Selfridge. </author> <title> Some new directions for adaptive control theory in robotics. </title> <editor> In Miller, III et al. </editor> <publisher> [MSW90]. </publisher>
Reference-contexts: REVIEW OF RELATED WORK 17 The theories discussed above are decision theories. A related large research area is the theory of adaptive control, which also deals with "optimisation over time". Although these two terms are often used interchangeably (see <ref> [FS90] </ref> and many other references in [MSW90]), they are almost mutually exclusive and complementary to each other in mathematical terms. The difference between them is that the former is concerned with global optimisation while the latter concerned with differentiable optimisation.
Reference: [Fun89] <author> K. Funahashi. </author> <title> On the approximate realization of continuous mappings by neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 183-192, </pages> <year> 1989. </year>
Reference-contexts: For the purpose of introduction, a brief recollection of the particular train of thought leading to this theory is probably more informative. The starting point is the backpropagation learning rule [RHW86] and the results that feedforward NNs are in a sense universal approximators of functions <ref> [Fun89, HSW89] </ref>. This means that NN can be regarded as a parameterised function which can adjust itself to suit the demand. The question I asked was: "Can such devices learn to play chess?" This question is quite legitimate since the rules of chess are well defined. <p> It is most famous for its learning rule, called back-propagation (BP) rule, which is the prototype for ME learning rules. A great volume of research has been devoted to various aspects of DCF-NN, of particular importance are the approximation properties <ref> [Fun89, HSW89, HSW90, Hor91] </ref>. For our purpose these results can be generally stated as showing that the DCF-NN is a universal approximator of arbitrary functions in [R m ! R n ), provided that there are enough neurons in the hidden layer. <p> Output layer. Set y = Cr L + Ds L , It has been proved that a two layer DCF-NN can approximate almost arbitrary functions given enough neurons in the hidden layer <ref> [Fun89] </ref>. The function to be approximated need not to be continuous [HSW89], it is only assumed to be in an appropriate Sobolev space (For introduction to Sobolev spaces see [Ada75]). <p> As has been mentioned earlier (x5.3), it is known that a two layer simple-layered fully-connected DCF-NN with sufficient number of neurons in the hidden layer is a universal approximator of functions and gradients in the sense of Sobolev space <ref> [Fun89, HSW89, HSW90] </ref>.
Reference: [Gen89] <author> C. T. Gens. </author> <title> Exploring three possibilities in network design: Spontaneous node activity, node plasticity and temporal coding. </title> <editor> In Eckmiller, R. and C. </editor> <publisher> van der Malsburg [EvdM89], </publisher> <pages> pages 301-310. </pages>
Reference-contexts: Of course nothing would be gained if we merely pretend that the whole AC is a large inhomogeneous NN. It was pointed out in <ref> [Gen89] </ref> that there are three different ways of using a NN. We have concentrated on the second method, namely, learning. A study on recurrent NN and temporal ordering might reveal shortcuts for implementing several modules of AC on one NN. CHAPTER 10.
Reference: [GG84] <author> S. Geman and D. Geman. </author> <title> Stochastic relaxation, Gibbs distribution, and the Beyessian restoration of images. </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <volume> 6 </volume> <pages> 721-741, </pages> <year> 1984. </year>
Reference-contexts: A continuous version of GS was provided in [Gla63]. Some of the most important properties of GSs have been studied in <ref> [GG84] </ref>, and a survey of their applications was given in [Yor92]. The study of NP-complete problems reveals that many combinatorial optimisation problems that are routinely encountered are inherently intractable if exact global opti-misation is attempted [GJ79]. <p> The two names "Boltzmann distribution" and "Gibbs distribution" seems equally popular, the historical origin is from statistical mechanics [LL80]. Remark 3.4.3 By definition, the computation of Z involves enumeration of all the states, which is generally intractable, see <ref> [GG84, p. 725] </ref>. CHAPTER 3. ADAPTIVE INFORMATION PROCESSING 26 Definition 3.4.3 (Gibbs sampler) Let X ~ = N n , p 2 n1 . <p> Furthermore, 8g 2 [R N ! R) : [g (r)] x = hg (r)i x . The proof of this theorem can be found in, for example, <ref> [GG84, Ami89, AK89b] </ref>. See also [MRR + 53, KGV83, AHS85, HS86] for supplementary information. <p> In Section 7.5, the simulation results and more technical details are given. The results are also compared with previous results. 7.2 Simulation of Gibbs Samplers At the heart of simulated annealing, the Boltzmann machine and many other statistical applicationsis a Gibbs sampler (GS) <ref> [GG84] </ref>. (See [Yor92] for a survey of the applications of GS.) Let X be the state space, e 2 [X ! R) and t 2 GS (BD (e; )). <p> Proof. The conditions imply that d=dt = (fl 0 = p n) sech , whose solution is (7.4.22). Remark 7.4.5 It has been proved in <ref> [GG84] </ref> that the cooling schedule = fl log (t + 1), which is equivalent to d=dt = fl=(t + 1) will guarantee global optimisation. The time t here is relative to the equilibrating speed of the Gibbs sampler, which does not have an exact correspondence here. <p> Since there is no quantitative results on equilibration speed ! in <ref> [GG84] </ref>, it is not known which of the two interpretation is correct. If the second interpretation is correct, then the CTS is both sufficient and necessary for good learning. CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 109 CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 110 CHAPTER 7.
Reference: [GJ79] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: Considering that there are only about 10 10 neurons in the human brain [And83] and about 10 80 atoms in the whole observable universe [Dav82], the following concepts, closely related to the concept of NP-completeness <ref> [GJ79] </ref>, are very important for comparing the computational viability of various theories. A finite set X is said enumerable 3 if, by a proper coding method, all of its elements can be examined, either simultaneously, or sequentially over a tolerable duration of time, in a physical device of current technology. <p> Some of the most important properties of GSs have been studied in [GG84], and a survey of their applications was given in [Yor92]. The study of NP-complete problems reveals that many combinatorial optimisation problems that are routinely encountered are inherently intractable if exact global opti-misation is attempted <ref> [GJ79] </ref>. When there are certain structures on the space of alternatives, local deterministic methods such as gradient-following (GF) methods will usually only lead to local optima. When GF methods are combined with the Monte Carlo principle, the resulting methods are called stochastic GF methods. <p> It is obvious that what is usually required is actually Type B computations, yet the conventional computational theory are usually expressed in terms of Type A computations <ref> [GJ79] </ref>, despite the fact that it is both unnatural and ill-posed. This seems to be mainly due to the fact that the only previously known computational model is the Turing machine and its equivalents, which is defined in terms of Type A computation.
Reference: [Gla63] <author> R. J. </author> <title> Glauber. Time dependent statistics of the Ising model. </title> <journal> J. Math. Phys., </journal> <volume> 4 </volume> <pages> 294-307, </pages> <year> 1963. </year>
Reference-contexts: The Metropolis method is now known as a Gibbs sampler (GS), one of the first in a large family, which produces an irreducible Markov chain whose stationary distribution is a certain Boltzmann (Gibbs) distribution. A continuous version of GS was provided in <ref> [Gla63] </ref>. Some of the most important properties of GSs have been studied in [GG84], and a survey of their applications was given in [Yor92]. The study of NP-complete problems reveals that many combinatorial optimisation problems that are routinely encountered are inherently intractable if exact global opti-misation is attempted [GJ79]. <p> Remark 6.2.2 The condition (6.2.3) only defines a one-parameter family of processes, since the two probabilities of flipping a bit can be scaled by a same factor. Some of the more important in this family are the Metropolis process [MRR + 53], the Glauber process <ref> [Gla63] </ref> and the local Boltzmann process [AHS85]. They will be discussed in detail in x7.2, when an efficient implementation on conventional computers is also introduced. 6.3 Relations with Other Networks 6.3.1 Relation with SQB-NN (Boltzmann machine) The SQFB-NN is a generalization of SQB-NN (x5.2). <p> To avoid unnecessary complications, it is better to change to continuous time formulation. The continuous-time counterpart of (7.2.3) is the Glauber process <ref> [Gla63] </ref>. Denote p i := p i (x i ) (x) as in (7.2.3).
Reference: [GM90] <author> M. Gabriel and J. W. Moore, </author> <title> editors. Learning and computational neuro-science: Foundations of adaptive networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference: [Gro86] <author> S. Grossberg. </author> <title> The Adaptive Brain I: Cognition, Learning, Reinforcement, and Rhythm, and The Adaptive Brain II: Vision, Speech, Language, and Motor Control. </title> <address> Elsevier/North-Holland, Amsterdam, </address> <year> 1986. </year>
Reference-contexts: There were various types of neural networks developed since 1960's, which we shall not study here. These include, most notably, various models developed by Grossberg and colleagues 2 , see <ref> [Gro86] </ref> and references therein, and those of Kohonen, see [Koh77, Koh84]. An introduction to these can be found in [BJ90]. A survey of structures and dynamics of deterministic neural networks are given by [Gro88].
Reference: [Gro88] <author> S. Grossberg. </author> <title> Nonlinear neural networks: </title> <booktitle> Principles, mechanisms, and architectures. Neural Networks, </booktitle> <volume> 1 </volume> <pages> 17-61, </pages> <year> 1988. </year>
Reference-contexts: These include, most notably, various models developed by Grossberg and colleagues 2 , see [Gro86] and references therein, and those of Kohonen, see [Koh77, Koh84]. An introduction to these can be found in [BJ90]. A survey of structures and dynamics of deterministic neural networks are given by <ref> [Gro88] </ref>. It especially covers those well known in the biological sciences and intended primarily as models of BNN. Many learning rules have been proposed for various neural networks. They are usually divided according to the learning "mode" into unsupervised learning (UL), supervised learning (SL), and reinforcement learning (RL). <p> The term "quasilinear" describes the fact that nonlinearity and dimensionality are separate: a high dimensional vector is first linearly combined into a single number and then nonlinearly (and stochastically) transformed. An important property of Q.NN is the so-called S exchange property <ref> [Gro88] </ref>: the network equations are equivalent to either of the following two equations r = e f (Br + s 0 ); or s = B e f (s) + s 0 : The extended vector w := [A; B; b] 2 W is call weight vector.
Reference: [GS74] <author> I. I. Gihman and A. V. Skorohod. </author> <title> The Theory of Stochastic Processes, volume 1. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1974. </year>
Reference-contexts: In fact our emphasis will be on those abilities of stochastic systems which a deterministic information system, which we call data processing systems, can not have. 3.2 Review of Probability Theory Our references to probability and stochastic processes are <ref> [Kol56a, CM65, GS82, GS74] </ref>. Definition 3.2.1 Let [; F ; Pr] be probability space. Let [A; A] be an arbitrary measure space. <p> Basic concept from pure mathematics can be found in [Die60]. Most results on matrices which we shall use can be found in [Cia89, Sen73b, CM65, Sen73a]. Our reference to probability and stochastic processes are <ref> [Kol56a, CM65, GS82, GS74] </ref>. The slanted typeface generally denote the first or the most formal appearance of a technical term. This is usually where it is defined, or the standard reference (cited at the beginning of the chapter or the section) is referred to for definition.
Reference: [GS82] <author> G. Grimmett and D. Stirzaker. </author> <title> Probability and Random Processes. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1982. </year>
Reference-contexts: In fact our emphasis will be on those abilities of stochastic systems which a deterministic information system, which we call data processing systems, can not have. 3.2 Review of Probability Theory Our references to probability and stochastic processes are <ref> [Kol56a, CM65, GS82, GS74] </ref>. Definition 3.2.1 Let [; F ; Pr] be probability space. Let [A; A] be an arbitrary measure space. <p> Basic concept from pure mathematics can be found in [Die60]. Most results on matrices which we shall use can be found in [Cia89, Sen73b, CM65, Sen73a]. Our reference to probability and stochastic processes are <ref> [Kol56a, CM65, GS82, GS74] </ref>. The slanted typeface generally denote the first or the most formal appearance of a technical term. This is usually where it is defined, or the standard reference (cited at the beginning of the chapter or the section) is referred to for definition.
Reference: [Gs88] <author> C. T. Gen s. </author> <title> Relaxation and neural learning: Points of convergence and divergence. </title> <journal> J. Parallel & Distributed Computing, </journal> <volume> 6 </volume> <pages> 217-244, </pages> <year> 1988. </year>
Reference-contexts: The latter is satisfied by the experiments on BM in [AHS85] which blurs the target, but not in [PA87]. We explain the details as the following. First we note that there are basically two ways of using neural networks as a computational tool <ref> [Gs88, Pin88] </ref>, either in the learning process, as in [AHS85, PA87] , or in the dynamics, as in [AK89b, Hop84]. We have to distinguish between the "computational temperature" and the "learning temperature".
Reference: [HCH91] <author> K. H. Hoffman, M. Christoph, and M. Hanf. </author> <title> Optimizing simulated annealing. </title> <booktitle> In Parallel Computation from Nature, volume 496 of Lecture Notes in Computer Sciences, </booktitle> <year> 1991. </year>
Reference-contexts: Their interpretation of the approach to equilibrium as the approximation in the sense of Kullback separation is very convenient for our applications (also see [BGMT89]), and our experiments in x7.5 provide quantitative support for the CTS schedule. A short survey of cooling schedules was given <ref> [HCH91] </ref>, and global optimisation methods are surveyed in [SSF92] A different approach to the Monte Carlo principle is through the stochastic neural 7 Something is called a principle if it involves a change of problem, which cannot be justified on purely logical ground. <p> We call ! the equilibration speed, and * the equilibration time. Following [NS88], we call v := (hei hei w )= the thermodynamic speed. 7.4.4 Cooling schedule There has been an intensive research on the cooling schedule of simulated annealing <ref> [KGV83, HCH91, SNH + 88, MW87] </ref>. Although SA is usually described as reducing temperature, in the annealing process it is better to keep some other quantity as a constant, instead of the speed of temperature reduction [KGV83]. One might suggest such as constant heat dissipation (CHD), etc.
Reference: [Heb49] <author> D. O. Hebb. </author> <title> The Organization of Behavior. </title> <editor> J. </editor> <publisher> Wiley & Sons, </publisher> <address> New York, </address> <year> 1949. </year>
Reference-contexts: The first NN learning rule is usually attributed to <ref> [Heb49, p.62] </ref>. The first formal theory on a type of artificial neural networks with a learning rule is usually traced to [Ros58, Ros59], where a perceptron network is actually one layer of DQF-NN preceded by a layer of fixed processors [MP69].
Reference: [HH64] <author> J. M. Hammersley and D. C. Handscomb. </author> <title> Monte Carlo Methods. </title> <publisher> Methuen, </publisher> <address> London, </address> <year> 1964. </year> <note> BIBLIOGRAPHY 206 </note>
Reference-contexts: An early attempt at applying the Monte Carlo method to the calculation of state variables in massively interacting systems led to the Metropolis method [MRR + 53], which is one of the so-called importance weighted methods <ref> [HH64] </ref>. The Monte Carlo method involves a change of problem, which we call the Monte Carlo principle 7 : instead of finding analytically the average of a random variable over a distribution which is computationally intractable, an average over samples of the random variable is used.
Reference: [Hin89a] <author> G. E. Hinton. </author> <title> Connectionist learning procedures. </title> <journal> Artif. Intell., </journal> <volume> 40(1) </volume> <pages> 185-234, </pages> <year> 1989. </year>
Reference-contexts: SQB-NN are usually used in combinatorial optimisation problems [AK89b, KA89, AK89a], but other applications 2 It should be mentioned here that according to our terminology, many of Grossberg's models are more like ACs than TIPs. 3 However, as mentioned in <ref> [Hin89a] </ref> the BM implemented on special chips might be over a million times faster than simulation [AA87]. CHAPTER 2. REVIEW OF RELATED WORK 13 are possible (For example, the prediction module in x9.2). <p> It CHAPTER 2. REVIEW OF RELATED WORK 14 should be noted that the formalism in [Sej81] even allows SQ and DC to be viewed as extremes of a one-parameter family. We discuss this briefly in x4.4. Neural network learning rules are surveyed in <ref> [Hin89a] </ref>, this reference contains original ideas on various important issues of learning. Learning based on a genuine entropy principle was given in [LTS90], although their method is somewhat theoretical rather than practical. <p> Fortunately, it turns out that SA is applicable to most SQ-NN. A variant of SA can also be applied to DC-NN in a different sense. Many different learning rules are applicable to various networks <ref> [Hin89a] </ref>. They are generally classified by the "learning mode" into supervised learning (SL), reinforcement learning (RL), and unsupervised learning (UL), depending on the nature of the assistance operator. <p> The idea of incorporating SA in the learning process was also suggested in <ref> [Hin89a] </ref>, in relation to the BM learning rule. To the best of our knowledge, we seem to be the first to show its general necessity and applicability to both the ME and ML learning rules. <p> Remark 6.7.1 The SA learning rule at least does away with those undesirable features of the original BM learning rule [HS86] ("Ways in which the learning algorithm can fail"). This is certainly a more reasonable method than commonly used technique of "weight decay" <ref> [Hin89a] </ref>, since the latter is equivalent to using the Euclidean norm on the weight space as a barrier function, which does not have a specific meaning. <p> It would be a major breakthrough if a general sufficient and necessary condition were to be found, but small extensions to the FB-structure is clearly possible in the near future. Our formulation of learning rules unifies and generalises many of the most important learning rules discussed in <ref> [Hin89a] </ref>, but it still leaves many variations and parameters to choose from. For lack of time, we have only tested certain variant of learning rules on certain variants of NN. It would be of immense value if a thorough investigation is done on this subject.
Reference: [Hin89b] <author> G. E. Hinton. </author> <title> Deterministic Boltzmann machine learning performs steepest descent in weight space. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 143-150, </pages> <year> 1989. </year>
Reference-contexts: The ME learning rule for DQB-NN was developed by [Pin87, Alm87, Alm89b, Alm89a]. DCB-NN was also studied as mean field (MF) approximation of SQB-NN [PA87], where another learning rule was derived from BM learning rule, which is also a GF rule <ref> [Hin89b] </ref>. The applications of DCB-NN include, but are not restricted to, various optimisation problems [TH86, HT86, PH89, dBM90]. The development of SQF-NN has several origins, most of them are related to DQF-NN, ie. logical circuits. <p> DQB-NN: This is the (discrete) Hopfield net [Hop82], DCF-NN: This is the most famous multilayer (soft) perceptron network [RHW86], also called back-propagation network. DCB-NN: This is the continuous Hopfield net [Hop84, HT86], the mean field (MF) BM [PA87] or deterministic BM <ref> [Hin89b] </ref>, and the feedback perceptron network [Alm87, Pin87] 1 . SQF-NN: This is called belief network [Nea92, Pea87], and simple reinforcement learn ing networks [Wil87, Wil92]. SQB-NN: This is the most famous Boltzmann machine (BM) [AHS85, HS86]. 1 They are in fact the same network with minor variations in detail. <p> Mean field Boltz mann Machine and Feedback Perceptron The DCB-NN can be introduced from three different points of view: * As a continuous variant of Hopfield net [Hop84], (DQB ! DCB). * As a variation of feedforward networks [Alm87, Pin87], (DCF ! DCB). * As a mean field Boltzmann machine <ref> [PA87, Hin89b] </ref>. (SQB ! DCB). They can be proved to define the same input-output relation, or, using a term used in [Wil90], to be "input-output equivalent". 5.4.1 Structure and dynamics The structure of the DCB-NN is the same as that of the SQB-NN. <p> This fact the f corresponds to free energy was mentioned in <ref> [Hin89b] </ref> without detail. CHAPTER 5. CLASSICAL MODELS OF H. S. NEURAL NETWORKS 67 5.4.3 GF-ME learning rules Notation. Let e 2 C 1 2 [X fi Y ! R). <p> However, these bounds are not quite practical since they are combinatorial. 6.3.3 Relation with DCB-NN (continuous Hopfield net, etc.) The DCFB-NN is also a generalization of the DCB-NN (x5.4), such as continuous Hop-field net [Hop84], the feedback perceptron network [Alm87], the mean field (MF) model of BM <ref> [PA87, Hin89b] </ref>. It should be noted that in general the SQFB-NN cannot be replaced by its MF approximation DCFB-NN without compromising its representational power. In particular, SQFB-NN can represent the correlations between the states of neurons in each layer, which are absent in the MF approximation.
Reference: [Hol75] <author> J. H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> Univ. of Michi-gan Press, </publisher> <address> Ann Arbor, </address> <year> 1975. </year>
Reference-contexts: The parameterised distributions of a S-NN is restricted to a submanifold of the information manifold. The learning rules of S-NN specify movements in this submanifold. Closely related to S-NN and SA are the genetic algorithms (GA) <ref> [Hol75] </ref>, a comparison of GA and SA can be found in [Dav87]. Each of these three (S-NN, SA, GA) has an underlying stochastic model described by a Gibbs ensemble which changes with time so that the proportion of good solutions increases.
Reference: [Hop82] <author> J. J. </author> <title> Hopfield. Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proc. </booktitle> <institution> Nat. Acad. Sci. USA, </institution> <month> 79 </month> <pages> 2554-2558, </pages> <year> 1982. </year>
Reference-contexts: One objective of this thesis is to show that SA can be adapted to replace GF in virtually all the learning rules. * * * The beginning of the current "rigorous development period" for neural networks was marked by <ref> [Hop82] </ref> who proved the stability of DQB-NN (deterministic quantised feedback neural network) by deriving a Lyapunov function for the network dynamics. This was generalised to SQB-NN (stochastic quantised feedback neural network) in [AHS85, HS86], which they call "Boltzmann machine" (BM). <p> Otherwise the neural network is called a stochastic Q.NN , denoted S-Q.NN. Remark 4.3.7 Note that the above definition depends only on local activation rules, not on the manner such activation rules are utilized globally. This means, for example, we classify the (discrete) Hopfield net <ref> [Hop82] </ref> as DQB-Q.NN, since in it each neuron is activated deterministically, although it uses the random sequential updating. <p> Important examples of this classification include the following. DQF-NN: This includes the so-called logical circuits or multilayer (hard-limit) perceptron network. The perceptron network described in [Ros58, Ros59] can be regarded as single layer DQF-NN. DQB-NN: This is the (discrete) Hopfield net <ref> [Hop82] </ref>, DCF-NN: This is the most famous multilayer (soft) perceptron network [RHW86], also called back-propagation network. DCB-NN: This is the continuous Hopfield net [Hop84, HT86], the mean field (MF) BM [PA87] or deterministic BM [Hin89b], and the feedback perceptron network [Alm87, Pin87] 1 . <p> In the two special cases which concerns us, it has been proved <ref> [Hop82, Hop84, AHS85] </ref> that DCB-NN and SQB-NN are strongly stable. (See also x5.4, x5.2, and x6.2.) Therefore the SQFB-NN and DCFB-NN are also stable. Example 4.5.1 Consider the DQ-NN 1 1 1 1 ! 1. (Figure 4.2 (a)) Suppose that the initial state is [+ + ].
Reference: [Hop84] <author> J. J. </author> <title> Hopfield. Neurons with graded response have collective computational properties like those of two-state neurons. </title> <booktitle> Proc. </booktitle> <institution> Nat. Acad. Sci. USA, </institution> <month> 81 </month> <pages> 3088-3092, </pages> <year> 1984. </year>
Reference-contexts: There are probably well over a thousand different published applications of DCF-NN, covering virtually every field where adaptive approximation of a nonlinear function might be of some use. A glimpse of sample applications is provided by [MHP90]. The DQB-NN was also generalised in <ref> [Hop84] </ref> to DCB-NN (deterministic continuous feedback neural network), usually called "(continuous) Hopfield net", the stability of which was also guaranteed by a Lyapunov function. The ME learning rule for DQB-NN was developed by [Pin87, Alm87, Alm89b, Alm89a]. <p> This means, for example, we classify the (discrete) Hopfield net [Hop82] as DQB-Q.NN, since in it each neuron is activated deterministically, although it uses the random sequential updating. Remark 4.3.8 The DCB-NN discussed in <ref> [Hop84, Alm87] </ref> are slightly more general, using f i (s i ) = tanh ( i s i ) as activation function, or, equivalently use B ij j =: B 0 ij as connection weight. <p> The perceptron network described in [Ros58, Ros59] can be regarded as single layer DQF-NN. DQB-NN: This is the (discrete) Hopfield net [Hop82], DCF-NN: This is the most famous multilayer (soft) perceptron network [RHW86], also called back-propagation network. DCB-NN: This is the continuous Hopfield net <ref> [Hop84, HT86] </ref>, the mean field (MF) BM [PA87] or deterministic BM [Hin89b], and the feedback perceptron network [Alm87, Pin87] 1 . SQF-NN: This is called belief network [Nea92, Pea87], and simple reinforcement learn ing networks [Wil87, Wil92]. <p> In the two special cases which concerns us, it has been proved <ref> [Hop82, Hop84, AHS85] </ref> that DCB-NN and SQB-NN are strongly stable. (See also x5.4, x5.2, and x6.2.) Therefore the SQFB-NN and DCFB-NN are also stable. Example 4.5.1 Consider the DQ-NN 1 1 1 1 ! 1. (Figure 4.2 (a)) Suppose that the initial state is [+ + ]. <p> It is well-known that a dynamic system is stable if and only if it possess a Lyapunov function. The only known universal method of deriving a Lyapunov function is by integration, which necessarily leads to symmetric connections (apart from diagonal scaling as in <ref> [Hop84, Alm87] </ref>). Remark 4.5.5 The F and B structures are uniform: both are described by the number of input units, hidden units and output units, since there is only one kind of connections in each of them. <p> The auxiliary output u is needed when two such networks are cascaded, and can otherwise be discarded. 5.4 DCB-NN: Continuous Hopfield Net, Mean field Boltz mann Machine and Feedback Perceptron The DCB-NN can be introduced from three different points of view: * As a continuous variant of Hopfield net <ref> [Hop84] </ref>, (DQB ! DCB). * As a variation of feedforward networks [Alm87, Pin87], (DCF ! DCB). * As a mean field Boltzmann machine [PA87, Hin89b]. (SQB ! DCB). <p> Let A, B, C and D be matrices representing the synapse from the input to the neurons, among the neurons and from the neurons to the output, respectively. C and D can be arbitrary but fixed. B is symmetric with zero diagonal. The network of <ref> [Hop84, Alm87] </ref> allows slightly general B, which is equivalent to scaling the output of each neuron independently. Since we are only interested in homogeneous networks we shall not discuss this possibility. Definition 5.4.1 (Dynamics of DCB-NN) r = w (x), y = ' w (x): 1. Input. <p> Process (P0) (5.4.1) is iterated and the fixed point is taken as solution [PA87]. Process (P1) The solution is defined as the attractor of differential equation [Alm87] s = Br + s 0 ; dt Process (P2) The solution is defined as the attractor of differential equation <ref> [Hop84] </ref> ds = ff (Br + s 0 s); r = f (s):(5.4.3) In the above ff is some positive constant. It is easy to see that if any of these three processes do converge to some fixed point, this fixed point will be a solution of (5.4.1). <p> CHAPTER 5. CLASSICAL MODELS OF H. S. NEURAL NETWORKS 66 5.4.2 Free energy and stability of dynamics The stability of (P1) and (P2) are guaranteed by the existence of a Lyapunov function, which is usually called "energy", following <ref> [Hop84] </ref>. We shall show that this function actually corresponds to "free energy" when the network is considered as a mean field system of the Boltzmann machine. <p> Remark 5.4.2 The above proof follows <ref> [Hop84] </ref>. An equivalent proof of stability was directly applied to (P1) in [Alm87]. The free energy f is just the "energy" in [Hop84], with the correspondence ff ~ 1=C i , ~ 1=R i , and S ~ R tanh 1 (r)dr, where C i and R i are called capacitance <p> Remark 5.4.2 The above proof follows <ref> [Hop84] </ref>. An equivalent proof of stability was directly applied to (P1) in [Alm87]. The free energy f is just the "energy" in [Hop84], with the correspondence ff ~ 1=C i , ~ 1=R i , and S ~ R tanh 1 (r)dr, where C i and R i are called capacitance and resistance in [Hop84]. This fact the f corresponds to free energy was mentioned in [Hin89b] without detail. CHAPTER 5. <p> The free energy f is just the "energy" in <ref> [Hop84] </ref>, with the correspondence ff ~ 1=C i , ~ 1=R i , and S ~ R tanh 1 (r)dr, where C i and R i are called capacitance and resistance in [Hop84]. This fact the f corresponds to free energy was mentioned in [Hin89b] without detail. CHAPTER 5. CLASSICAL MODELS OF H. S. NEURAL NETWORKS 67 5.4.3 GF-ME learning rules Notation. Let e 2 C 1 2 [X fi Y ! R). <p> However, these bounds are not quite practical since they are combinatorial. 6.3.3 Relation with DCB-NN (continuous Hopfield net, etc.) The DCFB-NN is also a generalization of the DCB-NN (x5.4), such as continuous Hop-field net <ref> [Hop84] </ref>, the feedback perceptron network [Alm87], the mean field (MF) model of BM [PA87, Hin89b]. It should be noted that in general the SQFB-NN cannot be replaced by its MF approximation DCFB-NN without compromising its representational power. <p> We explain the details as the following. First we note that there are basically two ways of using neural networks as a computational tool [Gs88, Pin88], either in the learning process, as in [AHS85, PA87] , or in the dynamics, as in <ref> [AK89b, Hop84] </ref>. We have to distinguish between the "computational temperature" and the "learning temperature". What is called "temperature" in the literature corresponds to computational temperature which appears in the SA1 method for accelerating the GS.
Reference: [Hop89] <author> J. J Hopfield. </author> <title> Learning algorithms and probability distributions in feed-forward and feed-back networks. </title> <booktitle> Proc. </booktitle> <institution> Nat. Acad. Sci. USA, </institution> <month> 84 </month> <pages> 8429-8433, </pages> <year> 1989. </year>
Reference-contexts: One of our contributions will be the identification of a class of structures, the FB-structure, which guarantees stability and is a natural generalisation of F- and B-structures. Concerning the dynamics of NN, there is a formal duality between SQ-NN and DC-NN, informally discussed in <ref> [Hop89, Ami89] </ref>. This concept turned out to be very fruitful in our theory, both in designing networks of rich and versatile dynamics, and in providing penetrating insights of the properties of the networks from different points of view. It CHAPTER 2. <p> This correspondence seems to be widely recognized (Cf. <ref> [Hop89, Ami89] </ref>), although there seems no explicit statement to this effect in the literature. Theorem 4.4.1 Let R = f1; 1g, e R = [1; 1]. <p> SQFB NEURAL NETWORKS 73 When the number of neurons in each layer is sufficiently large, the effect of r k1 on r k can be approximated by the mean field approximation hr k i = htanh (A k r k1 )i tanh (A k hr k1 i), see [PA87] and <ref> [Hop89] </ref>. As has been mentioned earlier (x5.3), it is known that a two layer simple-layered fully-connected DCF-NN with sufficient number of neurons in the hidden layer is a universal approximator of functions and gradients in the sense of Sobolev space [Fun89, HSW89, HSW90]. <p> Only under the condition that the correlation between neurons in one layer has little effect on that in the subsequent layer can the SQFB-NN be replaced by DCFB-NN in all layers except the last, which is still needed for producing correlated output <ref> [Hop89] </ref>. 6.4 GF-ME Learning Rule In this section we consider maximum evaluation (ME) learning for SQFB-NN based on gradient following (GF). In x6.6 the simulated annealing (SA) learning rule is studied, as well as the relation between ME learning and maximum likelihood (ML) learning.
Reference: [Hor91] <author> K. Hornik. </author> <title> approximation capability of multilayer feedforward networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 251-257, </pages> <year> 1991. </year>
Reference-contexts: It is most famous for its learning rule, called back-propagation (BP) rule, which is the prototype for ME learning rules. A great volume of research has been devoted to various aspects of DCF-NN, of particular importance are the approximation properties <ref> [Fun89, HSW89, HSW90, Hor91] </ref>. For our purpose these results can be generally stated as showing that the DCF-NN is a universal approximator of arbitrary functions in [R m ! R n ), provided that there are enough neurons in the hidden layer. <p> CLASSICAL MODELS OF H. S. NEURAL NETWORKS 63 derivatives [HSW90] (in the weak sense if the function is discontinuous). The activation function f can be any monotonic and bounded function for these approximation theorems to hold <ref> [Hor91] </ref>. Remark 5.3.2 These approximation properties are quite important theoretically, for they show that the multilayer networks overcome the major difficulties of single layer networks [MP69]. This also provides a justification that we are to restrict our attention to the class of H.S.NN.
Reference: [How60] <author> R. A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> New York, </address> <year> 1960. </year>
Reference-contexts: The theory of dynamic programming (DP) <ref> [Bel57, How60, BD62, DL76] </ref> provides a practical computational method to solve a special type of game, "games against Nature", in which an "agent" (a certain player) improves its policy (strategy) in an environment (all the other players and the rules of the game) which is stationary (ie., the transition probability of <p> By Theorem 8.3.2 this definition is equivalent to that based on fixed points of R . Note that the usual more intuitive definition of u as a weighted sum of the rewards <ref> [How60, Wit77, DY79, Ros83, Wat89, Sut90] </ref> are only applicable for uniform fl &lt; 1. <p> The state space X with transition mapping t forms a Markov chain (MC). The concept of closed set and recurrent state of a MC are defined as in [CM65]. Closed set is also called recurrent chain in <ref> [How60] </ref> and commutative set in [Sen73b]. The following theorem gives a sufficient condition for the existence of utility function. Theorem 8.4.7 Suppose that each closed set of the MC (X; t ) has at least one terminal state. Then kL k 1 &lt; 1. Proof. <p> In our formulation the optimal solutions is defined as limits of equilibrium solutions there is no need to study them separately. Our formulation also unifies different approaches in DP, such as fixed horizon programming, discounted reward programming, total return programming and average gain programming <ref> [How60, Bel57, Ros83] </ref>, by the following theorem. Theorem 8.5.3 Let E 2 R nfin with (E) 1. Then 8r 2 R n : 1. The following holds whenever the limit of the denumerator exists: lim h X E k r k=0 1 h E k r = 1:(8.5.5) CHAPTER 8.
Reference: [HS86] <author> G. E. Hinton and T. J. Sejnowski. </author> <title> Learning and relearning in Boltzmann machines. </title> <editor> In Rumelhart and McClelland [RM86], </editor> <booktitle> chapter 7. </booktitle>
Reference-contexts: RL rules belong to ME learning rules, some SL rules, such as the back propagation (BP) rule [RHW86, Wer74, Par85] are special cases of RL. Some other SL rules, such as the Boltzmann machine (BM) learning rule <ref> [AHS85, HS86] </ref> is a ML learning rule. Furthermore, any UL rule must also have some kind of performance evaluation, the only difference being that they are built-in, but we shall not consider UL rules here. <p> This was generalised to SQB-NN (stochastic quantised feedback neural network) in <ref> [AHS85, HS86] </ref>, which they call "Boltzmann machine" (BM). The BM is perhaps the most versatile neural network in many respects, although the computation on BM simulated on a conventional computer is expensive 3 . <p> The reason this is allowed is that the original problem itself is usually an idealisation. CHAPTER 2. REVIEW OF RELATED WORK 19 networks (S-NN), such as Boltzmann machine <ref> [AHS85, HS86] </ref>. A S-NN is a variant GS which, instead of producing a sample in each member of a sequence of distributions approaching the Boltzmann distribution, produces a parameterised (in the form of neural network connection weights) distribution which can be continuously modified. <p> SQF-NN: This is called belief network [Nea92, Pea87], and simple reinforcement learn ing networks [Wil87, Wil92]. SQB-NN: This is the most famous Boltzmann machine (BM) <ref> [AHS85, HS86] </ref>. 1 They are in fact the same network with minor variations in detail. CHAPTER 4. THEORY OF H. S. NEURAL NETWORKS 48 Remark 4.3.12 Readers familiar with neural network literature would find that many of the most important neural networks are included in this classification. <p> known as feedforward perceptrons, multilayer perceptrons, back-propagation network; the SQB-NN, also known as the Boltzmann machine; and the DCB-NN; also known as the (continuous) Hopfield net, the deterministic Boltzmann machine, and the feedback perceptrons. 5.2 SQB-NN: Boltzmann Machine The SQB-NN is known as the Boltzmann machine (BM) (with trivial variance) <ref> [AHS85, HS86] </ref>. <p> Furthermore, 8g 2 [R N ! R) : [g (r)] x = hg (r)i x . The proof of this theorem can be found in, for example, [GG84, Ami89, AK89b]. See also <ref> [MRR + 53, KGV83, AHS85, HS86] </ref> for supplementary information. <p> Remark 6.7.1 The SA learning rule at least does away with those undesirable features of the original BM learning rule <ref> [HS86] </ref> ("Ways in which the learning algorithm can fail"). This is certainly a more reasonable method than commonly used technique of "weight decay" [Hin89a], since the latter is equivalent to using the Euclidean norm on the weight space as a barrier function, which does not have a specific meaning. <p> Several techniques most frequently cited in the literature were tested, including "momentum term" and random initial weights [RHW86], "weight decay" <ref> [HS86] </ref> and added noise [AHS85], none of them completely overcome this problem. The mechanism by which the network converges to local optima is the following: since the network is fully connected, whenever one event happens, the evaluation and policy of any other events will be changed as well.
Reference: [HSW89] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-368, </pages> <year> 1989. </year>
Reference-contexts: For the purpose of introduction, a brief recollection of the particular train of thought leading to this theory is probably more informative. The starting point is the backpropagation learning rule [RHW86] and the results that feedforward NNs are in a sense universal approximators of functions <ref> [Fun89, HSW89] </ref>. This means that NN can be regarded as a parameterised function which can adjust itself to suit the demand. The question I asked was: "Can such devices learn to play chess?" This question is quite legitimate since the rules of chess are well defined. <p> It is most famous for its learning rule, called back-propagation (BP) rule, which is the prototype for ME learning rules. A great volume of research has been devoted to various aspects of DCF-NN, of particular importance are the approximation properties <ref> [Fun89, HSW89, HSW90, Hor91] </ref>. For our purpose these results can be generally stated as showing that the DCF-NN is a universal approximator of arbitrary functions in [R m ! R n ), provided that there are enough neurons in the hidden layer. <p> Output layer. Set y = Cr L + Ds L , It has been proved that a two layer DCF-NN can approximate almost arbitrary functions given enough neurons in the hidden layer [Fun89]. The function to be approximated need not to be continuous <ref> [HSW89] </ref>, it is only assumed to be in an appropriate Sobolev space (For introduction to Sobolev spaces see [Ada75]). The approximation is in the sense of a corresponding Sobolev norm, which can also take into account of the CHAPTER 5. CLASSICAL MODELS OF H. S. <p> As has been mentioned earlier (x5.3), it is known that a two layer simple-layered fully-connected DCF-NN with sufficient number of neurons in the hidden layer is a universal approximator of functions and gradients in the sense of Sobolev space <ref> [Fun89, HSW89, HSW90] </ref>.
Reference: [HSW90] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 551-560, </pages> <year> 1990. </year>
Reference-contexts: It is most famous for its learning rule, called back-propagation (BP) rule, which is the prototype for ME learning rules. A great volume of research has been devoted to various aspects of DCF-NN, of particular importance are the approximation properties <ref> [Fun89, HSW89, HSW90, Hor91] </ref>. For our purpose these results can be generally stated as showing that the DCF-NN is a universal approximator of arbitrary functions in [R m ! R n ), provided that there are enough neurons in the hidden layer. <p> The approximation is in the sense of a corresponding Sobolev norm, which can also take into account of the CHAPTER 5. CLASSICAL MODELS OF H. S. NEURAL NETWORKS 63 derivatives <ref> [HSW90] </ref> (in the weak sense if the function is discontinuous). The activation function f can be any monotonic and bounded function for these approximation theorems to hold [Hor91]. <p> As has been mentioned earlier (x5.3), it is known that a two layer simple-layered fully-connected DCF-NN with sufficient number of neurons in the hidden layer is a universal approximator of functions and gradients in the sense of Sobolev space <ref> [Fun89, HSW89, HSW90] </ref>.
Reference: [HT86] <author> J. J. Hopfield and D. W. Tank. </author> <title> Collective computation with continuous variables. </title> <editor> In E. Bienenstock, F. Fogelman Soulie, and G. Weisbuch, editors, </editor> <title> Disordered Systems and Biological Organization. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1986. </year>
Reference-contexts: DCB-NN was also studied as mean field (MF) approximation of SQB-NN [PA87], where another learning rule was derived from BM learning rule, which is also a GF rule [Hin89b]. The applications of DCB-NN include, but are not restricted to, various optimisation problems <ref> [TH86, HT86, PH89, dBM90] </ref>. The development of SQF-NN has several origins, most of them are related to DQF-NN, ie. logical circuits. <p> The perceptron network described in [Ros58, Ros59] can be regarded as single layer DQF-NN. DQB-NN: This is the (discrete) Hopfield net [Hop82], DCF-NN: This is the most famous multilayer (soft) perceptron network [RHW86], also called back-propagation network. DCB-NN: This is the continuous Hopfield net <ref> [Hop84, HT86] </ref>, the mean field (MF) BM [PA87] or deterministic BM [Hin89b], and the feedback perceptron network [Alm87, Pin87] 1 . SQF-NN: This is called belief network [Nea92, Pea87], and simple reinforcement learn ing networks [Wil87, Wil92].
Reference: [Jay57a] <author> E. T. </author> <title> Jaynes. </title> <journal> Information theory and statistical mechanics. Physics Review, </journal> <volume> 106(4) </volume> <pages> 620-630, </pages> <year> 1957. </year>
Reference-contexts: Some further significant contributions to information theory include [Kol56b] (information of continuous variables), [Khi57] (entropy of a Markov chain), and [Kul59] (the information divergence, also known as Kullback separation, and the cross entropy between two distributions). The maximum entropy principle (MAXENT) <ref> [Jay57a, Jay57b] </ref> is a further attempt at clearing the relation between the mathematical theory of information and the physical theory of statistical mechanics. <p> This problem is under-determined, since there are many different p satisfying the con dition. The Maximum Entropy Principle (MAXENT) <ref> [Jay57a] </ref> is the transformation of the above problem into the following well defined problem p : max H : hei = e 0 ; h1i = 1:(3.4.1) setting the solution as hgi := P The problem (3.4.1) is a constrained linear optimisation problem. <p> Then max hei + H : = 0 ; h1i = 1 () p = p 0 : This theorem is well known <ref> [Jay57a] </ref>. It can be proved by directly solving these optimisation problems, by using duality theorems in the theory of convex optimisation, or by translating the corresponding results from statistical mechanics [LL80, Ami89]. <p> The entropy is a very good barrier function: it is symmetric, finite, and convex, and it has infinite derivative at the boundary of the simplex [Sha48, Kul59]. In many aspects it can be regarded as the optimal one. (See <ref> [Jay57a] </ref> for similar ideas from a slightly different point of view.) Theorem 3.4.3 Denote the solution of (3.4.5) as p , then as ! 1, p ! p fl 2 P fl : p fl n and p fl i = 0 otherwise, where m := jN fl n j. <p> However, the qualitative similarity between Euclidean norm and the entropy at a gross level is high enough for the two methods to behave similarly in many circumstances, but weight decay would show its shortcoming in a long sequence of computations <ref> [Jay57a] </ref>. In general, Euclidean norm is good for linear spaces while entropy is good for simplexes. Remark 6.7.2 Now we remark briefly on the role of hidden neurons. Any NN which needs hidden units must have local optima.
Reference: [Jay57b] <author> E. T. </author> <title> Jaynes. </title> <journal> Information theory and statistical mechanics. Physics Review, </journal> <volume> 108(2) </volume> <pages> 171-190, </pages> <year> 1957. </year>
Reference-contexts: Some further significant contributions to information theory include [Kol56b] (information of continuous variables), [Khi57] (entropy of a Markov chain), and [Kul59] (the information divergence, also known as Kullback separation, and the cross entropy between two distributions). The maximum entropy principle (MAXENT) <ref> [Jay57a, Jay57b] </ref> is a further attempt at clearing the relation between the mathematical theory of information and the physical theory of statistical mechanics.
Reference: [KA89] <author> J. Korst and E. Aarts. </author> <title> Combinatorial optimization on a Boltzmann machine. </title> <journal> J. Parallel & Distributed Computing, </journal> <volume> 6 </volume> <pages> 331-357, </pages> <year> 1989. </year>
Reference-contexts: The BM learning rule was given as a gradient-following (GF) rule, but in their experiments with BM they actually used the entropy principle implicitly. Unfortunately the way they presented it as a technical skill prevented others from recognising its importance. SQB-NN are usually used in combinatorial optimisation problems <ref> [AK89b, KA89, AK89a] </ref>, but other applications 2 It should be mentioned here that according to our terminology, many of Grossberg's models are more like ACs than TIPs. 3 However, as mentioned in [Hin89a] the BM implemented on special chips might be over a million times faster than simulation [AA87].
Reference: [KGV83] <author> C. Kirkpatrick, D. Gelat, Jr., and M. P. Vecchi. </author> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680, </pages> <year> 1983. </year>
Reference-contexts: In general, learning rules are optimisation methods, almost all the currently available learning rules are based on gradient-following (GF) which may lead to local optima [MP88]. Simulated annealing (SA) <ref> [KGV83] </ref> can be used to avoid local optima in an optimisation problem. <p> In a sense, such methods are only qualitative methods, since gradient alone does not tell how much change should be made with limited supporting information. Monte Carlo methods were further developed into the simulated annealing method <ref> [KGV83] </ref>, which solves the combinatorial optimisation problems efficiently, by equating energy with the cost function to be optimised. This involves a further change of problem, which we call the entropy principle. <p> Increase , &lt; 0 . 5. Iterate from step 2, until 0 . There are actually two variants of SA: The infinite SA method (SA0) takes ! 1, which gives an efficient combinatorial optimisation method, as in <ref> [KGV83] </ref>. The finite SA method (SA1) takes ! 0 &lt; 1, which gives a method of accelerating a Gibbs sampler, as in [AHS85]. Theorem 3.4.4 In the SA0 method, p k ! p fl . In the SA1 method, p k ! BD (e; 0 ). Proof. <p> Furthermore, 8g 2 [R N ! R) : [g (r)] x = hg (r)i x . The proof of this theorem can be found in, for example, [GG84, Ami89, AK89b]. See also <ref> [MRR + 53, KGV83, AHS85, HS86] </ref> for supplementary information. <p> Remark 7.3.3 The second and third identities in the above appeared in [AK89b], but they seem to have been well known in statistical mechanics (when there is no x) <ref> [KGV83, NS88] </ref>. 7.3.4 Ideal learning for the Hamming distance encoder problem We consider Hamming distance encoder here, which has some very nice features which makes it easy to be treated analytically. Later we shall see that other encoder problems are almost identical numerically, although being much more complicated analytically. <p> We call ! the equilibration speed, and * the equilibration time. Following [NS88], we call v := (hei hei w )= the thermodynamic speed. 7.4.4 Cooling schedule There has been an intensive research on the cooling schedule of simulated annealing <ref> [KGV83, HCH91, SNH + 88, MW87] </ref>. Although SA is usually described as reducing temperature, in the annealing process it is better to keep some other quantity as a constant, instead of the speed of temperature reduction [KGV83]. One might suggest such as constant heat dissipation (CHD), etc. <p> Although SA is usually described as reducing temperature, in the annealing process it is better to keep some other quantity as a constant, instead of the speed of temperature reduction <ref> [KGV83] </ref>. One might suggest such as constant heat dissipation (CHD), etc. The one we find most convincing is the constant thermodynamic speed (CTS) schedule [SNH + 88].
Reference: [Khi57] <author> A. I. Khinchin. </author> <title> Mathematical Foundation of Information Theory. </title> <publisher> Dover Publications, </publisher> <address> New York, </address> <year> 1957. </year> <note> BIBLIOGRAPHY 207 </note>
Reference-contexts: One of the most significant contributions of [Sha48] was to show that entropy is the only measure of disorder, or missing information, under some very reasonable conditions one would usually attribute to such a measure. Some further significant contributions to information theory include [Kol56b] (information of continuous variables), <ref> [Khi57] </ref> (entropy of a Markov chain), and [Kul59] (the information divergence, also known as Kullback separation, and the cross entropy between two distributions). <p> , and the transferred entropy hS k+1 i k1 at step k, respectively, by S k (x k1 ) := ~ k S k (~ k1 ) := ~ k hS k+1 i k1 (~ k1 ) = ~ k Remark 3.5.2 These are generalisations of the definitions given in <ref> [Khi57] </ref>. Entropy of infinite steps may be infinite, in that case, the above formulae are interpreted by substituting k := N k;m = fk; k + 1; : : :; mg, where m is a number larger than any k which is of interest.
Reference: [Knu81] <author> D. E. Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> volume 2. </volume> <publisher> Addison-Wesley, </publisher> <address> Reading (Mass.), 2 edition, </address> <year> 1981. </year>
Reference-contexts: The second statement of the conjecture means that SETM can be (approximately) implemented on TM as a virtual machine (For stochastic approximations refer to <ref> [Rip87, Knu81] </ref>). In this thesis we have tacitly assumed that the use of pseudo-random number generator (PRNG) in the simulations produces no substantial difference from that of RNG. Turing has conjectured that SETM=ETM, by using, for example, the digit of as a RNG [Tur50] 6 .
Reference: [Koh77] <author> T. Kohonen. </author> <title> Associative Memory: A System Theoretic Approach. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1977. </year>
Reference-contexts: There were various types of neural networks developed since 1960's, which we shall not study here. These include, most notably, various models developed by Grossberg and colleagues 2 , see [Gro86] and references therein, and those of Kohonen, see <ref> [Koh77, Koh84] </ref>. An introduction to these can be found in [BJ90]. A survey of structures and dynamics of deterministic neural networks are given by [Gro88]. It especially covers those well known in the biological sciences and intended primarily as models of BNN.
Reference: [Koh84] <author> T. Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: There were various types of neural networks developed since 1960's, which we shall not study here. These include, most notably, various models developed by Grossberg and colleagues 2 , see [Gro86] and references therein, and those of Kohonen, see <ref> [Koh77, Koh84] </ref>. An introduction to these can be found in [BJ90]. A survey of structures and dynamics of deterministic neural networks are given by [Gro88]. It especially covers those well known in the biological sciences and intended primarily as models of BNN.
Reference: [Kol56a] <author> A. N. </author> <title> Kolmogorov. Foundations of the Theory of Probability. </title> <publisher> Chelsea Publishing, </publisher> <address> New York, </address> <year> 1956. </year> <title> English translation, with added bibliography. </title>
Reference-contexts: In fact our emphasis will be on those abilities of stochastic systems which a deterministic information system, which we call data processing systems, can not have. 3.2 Review of Probability Theory Our references to probability and stochastic processes are <ref> [Kol56a, CM65, GS82, GS74] </ref>. Definition 3.2.1 Let [; F ; Pr] be probability space. Let [A; A] be an arbitrary measure space. <p> and [B; B] be two measure spaces, x 2 A , y 2 B , then fx; yg is independent if 8X 2 A; Y 2 B : Pr fx 2 X; y 2 Y g = Pr fx 2 XgPr fy 2 Y g:(3.2.1) Remark 3.2.2 As noted in <ref> [Kol56a, p. 8] </ref>, from an axiomatic point of view, the theory of probability would be no more than a sub-branch of set theory and measure theory, if not for the additional notion of independence, which makes it an entire branch of mathematics 1 . <p> Basic concept from pure mathematics can be found in [Die60]. Most results on matrices which we shall use can be found in [Cia89, Sen73b, CM65, Sen73a]. Our reference to probability and stochastic processes are <ref> [Kol56a, CM65, GS82, GS74] </ref>. The slanted typeface generally denote the first or the most formal appearance of a technical term. This is usually where it is defined, or the standard reference (cited at the beginning of the chapter or the section) is referred to for definition.
Reference: [Kol56b] <author> A. N. </author> <title> Kolmogorov. On the Shannon theory of information transmission in the case of continuous signals. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> IT-2:102-108, </volume> <year> 1956. </year>
Reference-contexts: One of the most significant contributions of [Sha48] was to show that entropy is the only measure of disorder, or missing information, under some very reasonable conditions one would usually attribute to such a measure. Some further significant contributions to information theory include <ref> [Kol56b] </ref> (information of continuous variables), [Khi57] (entropy of a Markov chain), and [Kul59] (the information divergence, also known as Kullback separation, and the cross entropy between two distributions).
Reference: [Kul59] <author> S. Kullback. </author> <title> Information Theory and Statistics. </title> <editor> J. </editor> <publisher> Wiley & Sons, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: Some further significant contributions to information theory include [Kol56b] (information of continuous variables), [Khi57] (entropy of a Markov chain), and <ref> [Kul59] </ref> (the information divergence, also known as Kullback separation, and the cross entropy between two distributions). The maximum entropy principle (MAXENT) [Jay57a, Jay57b] is a further attempt at clearing the relation between the mathematical theory of information and the physical theory of statistical mechanics. <p> The detailed analysis will be given in x3.4, where its relation to the MAXENT and linear programming methods are also elucidated. The SA method is the first computational method in which the computational effort is directly related to the mutual information <ref> [Kul59] </ref> between data and performance, not on data alone. In fact, we shall show that the entity which is usually called temperature is simply a measure of the value of information in this context. <p> We say that P (X) is spanned by P 0 (X), since p i = P Remark 3.3.1 The manifold P (X) is actually a Riemannian manifold if endowed with the Fisher information matrix <ref> [Kul59] </ref> defining the Riemannian metric [AKN92], by which P (X) is a curved manifold. The Fisher information matrix is closely related to the concept of covariance, which is of the utmost importance for all the learning rules for stochastic neural networks, and underlies all the "Hebb-like" learning rules. <p> ADAPTIVE INFORMATION PROCESSING 24 The Kullback separation is a measure of difference between two probability distributions in the information theory <ref> [Kul59] </ref>. It is not symmetric between the two distributions, but otherwise satisfies the axioms of a pseudo-metric: K (p; q) 0, K (p; q) = 0 () p = q [AKN92]. Notation. Let p 2 n1 , and 8k 2 N : p k 2 n1 . <p> The entropy is a very good barrier function: it is symmetric, finite, and convex, and it has infinite derivative at the boundary of the simplex <ref> [Sha48, Kul59] </ref>.
Reference: [Lak81] <author> S. Lakshmivarahan. </author> <title> Learning Algorithms: Theory and Applications. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: The development of SQF-NN has several origins, most of them are related to DQF-NN, ie. logical circuits. One origin was the theory of "stochastic learning automata" [NT74], where each automaton was actually a neuron, and many of them were connected to perform certain tasks <ref> [Lak81, NL77, BA85, NT89] </ref>. A network of learning automata without loops is a SQF-NN. Another derivation was from the Bayesian inference, this results in "belief networks" [Pea87]. The ME learning rules for SQF-NN was studied extensively in [Wil92, Wil87, Wil88, Wil90].
Reference: [Lin92] <author> L.-J. Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> 8(3/4):293-321, 1992. 
Reference-contexts: Unlike DP, there are many methods but only a few scattered theories in RL, since the relationship between the parameterisation and the actual application is usually a crucial factor in the overall performance, but this relationship most often has only been analysed with regard to specific test problems (eg. <ref> [Tes92, Lin92] </ref>). The (implicit) application of RL as a computational method can be traced back to the "checker playing program" of Samuel [Sam59], in which the evaluation is represented by a polynomial. <p> This is necessary since the decision module simply select the optimal action deterministically according to the evaluation module. Various models based on these methods have been tested on specific problems in <ref> [Lin92, dRMT92, Sin92] </ref>. A review of RL can be found in [Bar90]. * * * These above methods (game theory, dynamic programming and reinforcement learning) either update the policy in the action space (deterministic policies), or in the policy space by gradient following. <p> It is not known which is better, and under what consideration. Several variants of these two methods have been empirically compared in an artificially designed game situation in <ref> [Lin92] </ref>, but since there are many other factors different in that setting, the result is not conclusive for general problems. Note that e 2 introduces systematic error (bias) while Q (e 1 ) introduces random error (variance). <p> The policy is therefore w := (' w ). This can be alternatively interpreted as having a neural network whose last layer is stochastic, with probability distribution restricted to the n unit vectors e i . See, for example, <ref> [YG90, Lin92] </ref>. Now we study what the auxiliary input e v to the decision network should be. We want w T = ff e v T @T = ff @w CHAPTER 9. <p> BASIC ADAPTIVE COMPUTERS 158 of solutions so that we can compare experiments with theoretical predictions, and on the other, they have features which represent a class of problems more general than those treated in the literature so far <ref> [BSA83, Sut90, Lin92, Wat89, WB91, Tes92] </ref>. Variations of these test problems can serve as new test problems for basic adaptive computers (BAC) with more modules. <p> One such arbitrariness occur in the measure of performance. Various measures which are only meaningful to the (artificial) problems have been used, such as surviving time, error rate, words recognised, just to mention a few (eg., <ref> [BSA83, Sut90, Lin92, WB91, Tes92] </ref>). Example 9.5.1 A chess player should learn exactly the same strategy in the following two settings: (1) win= 1, lose= 1, tie=0; (2) win= 100, lose= 0, tie=50.
Reference: [Lip87] <author> R. P. Lippman. </author> <title> An introduction to computing with neural nets. </title> <journal> In IEEE ASSP magazine [Vem88], </journal> <pages> pages 4-22. </pages>
Reference-contexts: As we have seen in x6.6, these two criteria are related by @K (P; P 0 ) = @w where f = e log P (yjx) and P 0 = BD (e; ). Remark 7.3.2 This formulation of encoder problem encompasses all the so-called "auto-association" problems in NN literature <ref> [Lip87] </ref>. The example in x6.5 is also an encoder problem, where the function d is stochastic. If d is the Hamming distance d (x; y) := jfi : x i 6= y i gj = kx yk 1 =2, the problem is called Hamming distance encoder.
Reference: [LL80] <author> L. D. Landau and E. M. Lifshitz. </author> <title> Statistical Physics. </title> <publisher> Pergamon, Oxford, </publisher> <year> 1980. </year>
Reference-contexts: problems, although our methodology may be useful in future solutions. 2.3 Information Theory and Related Topics The concepts of entropy, Boltzmann-Gibbs distribution and statistical ensembles, and their relations to irreversible processes (the Second Law of Thermodynamics) and disorder, have been known in statistical mechanics since the end of last century <ref> [LL80] </ref>. Shannon established the mathematical foundation of information theory in the monumental papers [Sha48], in which it became perfectly clear that entropy is a purely mathematical concept, just as probability is, and it is a more fundamental concept than the physical concept of irreversibility. <p> Remark 3.4.2 Any p 2 R n is the Boltzmann distribution corresponding to many different [e; ], such as [T =; ], where T is an arbitrary T -coordinate. The two names "Boltzmann distribution" and "Gibbs distribution" seems equally popular, the historical origin is from statistical mechanics <ref> [LL80] </ref>. Remark 3.4.3 By definition, the computation of Z involves enumeration of all the states, which is generally intractable, see [GG84, p. 725]. CHAPTER 3. ADAPTIVE INFORMATION PROCESSING 26 Definition 3.4.3 (Gibbs sampler) Let X ~ = N n , p 2 n1 . <p> It can be proved by directly solving these optimisation problems, by using duality theorems in the theory of convex optimisation, or by translating the corresponding results from statistical mechanics <ref> [LL80, Ami89] </ref>. The above characterisation of the Boltzmann distribution has several different interpretations, all of them based on the fact that entropy is the measure of disorder, or missing information. The following two are most interesting to us. <p> Then the corresponding macro state variables are energy hei, temperature , free energy hf i and entropy hlog pi. The Boltzmann distribution p 0 is the equilibrium distribution in the Gibbs canonical ensemble <ref> [LL80, Ami89] </ref>. Example 3.4.3 (Decision theory) Suppose an agent is to choose actions a 2 N n with distribution p 2 n1 . Let e 2 R n be an evaluation function on N n . Then the expected return is hei.
Reference: [LL88] <author> C.-Y. Liou and S.-L. Lin. </author> <title> The other variant boltzmann machine. </title> <booktitle> In IJCNN, editor, IJCNN Intl. Joint Conf. on Neural Networks, </booktitle> <volume> volume 1, </volume> <year> 1988. </year>
Reference-contexts: In this section we only consider GF-ML learning rule. Later we shall show its relation to GF-ME learning rule. There are two different kinds of ML learning for BM <ref> [LL88] </ref>, using the cross entropy (CE) K (P; P 0 ) and the reversed cross entropy (RCE) K (P 0 ; P ), respectively. Notation. Let v 2 R N . Denote hvi 0 P ~ v ~ . <p> CLASSICAL MODELS OF H. S. NEURAL NETWORKS 62 The variant Boltzmann machine learning rule (VBM) <ref> [LL88] </ref> is defined as GF learning on CE, @w It should be remarked that there is no practical implementation for the VBM learning rule as a ML rule, but it is closely related to a good ME rule (x6.6). 5.3 DCF-NN: Multilayer Perceptron The DCF-NN is often called back-propagation network, multilayer <p> where e is normalised from d with Definition 7.3.1 ML: It is required that K (P 0 ; P ) ! 0 or K (P; P 0 ) ! 0, where K is the Kullback separation, P is the network distribution and P 0 is distribution corresponding to identity mapping <ref> [AHS85, LL88] </ref>. As we have seen in x6.6, these two criteria are related by @K (P; P 0 ) = @w where f = e log P (yjx) and P 0 = BD (e; ). <p> We always set computational temperature to 1 in our experiments, ie., we do not use the SA1 method to accelerate the computation. 2 More precisely, our learning rule is equivalent to the VBM learning rule <ref> [LL88] </ref>. CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 132 There seems to be no comparable concept to our "learning temperature" in the literature. <p> The ME learning rule with RCE has been studied in [Nea92] while the ML learning rule with CE has been studied in <ref> [LL88] </ref>. No non-enumerative implementations were given in both references. Therefore it remains an interesting open question as whether such learning rules are possible. There have been many results on the issue of what a NN can represent, but it seems that little is known about what a NN can learn.
Reference: [LTS90] <author> E. Levine, N. Tishby, and S. Solla. </author> <title> A statistical approach to learning and generalization in layered neural networks. </title> <journal> Proc. IEEE, </journal> <volume> 78(10) </volume> <pages> 1568-1574, </pages> <year> 1990. </year>
Reference-contexts: We discuss this briefly in x4.4. Neural network learning rules are surveyed in [Hin89a], this reference contains original ideas on various important issues of learning. Learning based on a genuine entropy principle was given in <ref> [LTS90] </ref>, although their method is somewhat theoretical rather than practical. Much of this research was inspired by the two volume "PDP book" [RM86, MR86], which is something of a "connectionist manifesto".
Reference: [Mel92] <author> R. </author> <title> Melzark. </title> <journal> Phantom limbs. Sci. Amer., </journal> <pages> pages 90-96, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Remark 4.3.11 The scaling certainly matters in physical implementations, affecting damage tolerance. There is an interesting explanation to the biological phenomenon of "phantom limbs" <ref> [Mel92] </ref>. The biological neural networks have the scaling [0; 1], as dictated by physical restrictions, so the "no-signal" level of activation is about 0.5.
Reference: [MHP90] <author> A. J. Maren, C. T. Harston, and R. M. </author> <booktitle> Pap. Handbook of Neural Computing Applications. </booktitle> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: There are probably well over a thousand different published applications of DCF-NN, covering virtually every field where adaptive approximation of a nonlinear function might be of some use. A glimpse of sample applications is provided by <ref> [MHP90] </ref>. The DQB-NN was also generalised in [Hop84] to DCB-NN (deterministic continuous feedback neural network), usually called "(continuous) Hopfield net", the stability of which was also guaranteed by a Lyapunov function. The ME learning rule for DQB-NN was developed by [Pin87, Alm87, Alm89b, Alm89a].
Reference: [Min61] <author> M. L. Minsky. </author> <title> Steps toward artificial intelligence. </title> <journal> Proc. IRE, </journal> <volume> 49 </volume> <pages> 8-30, </pages> <year> 1961. </year> <note> Reprinted in [FF63]. </note>
Reference-contexts: The significance of such a "division of labour" can be somehow explained through the concept of credit assignment problem (CAP) for learning machines <ref> [Min61] </ref>, although we shall seldom have chance to mention this concept in technical discussions. The study of TIPs corresponds to the so-called structural credit assignment problem (SCAP), where given an evaluation of a system, it is required to assign the credit proportionately to various subsystems. <p> CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 102 Remark 7.4.1 The polynomial formula with s = 1 and the exponential formula have been used in learning methods in as early as [Sam59] and their merits were discussed in <ref> [Min61] </ref>.
Reference: [Min87] <author> M. L. Minsky. </author> <title> The Society of Mind. </title> <publisher> William Heinemann, </publisher> <address> London, </address> <year> 1987. </year>
Reference-contexts: viewing computation as information processing rather than data processing has a strong implications on the debate over artificial intelligence (AI) [Sea80, Pen90]. 10.5.1 A tentative definition of intelligence There seems to be no existing definition of intelligence which can encompass all the special features one usually assigns to this concept <ref> [Min87] </ref>. We suggest a tangible definition which may serve as a working definition. This definition was partly inspired by the observation made in [Min87] that intelligence is something which slips away as soon as we know a procedure to do it. CHAPTER 10. <p> A tentative definition of intelligence There seems to be no existing definition of intelligence which can encompass all the special features one usually assigns to this concept <ref> [Min87] </ref>. We suggest a tangible definition which may serve as a working definition. This definition was partly inspired by the observation made in [Min87] that intelligence is something which slips away as soon as we know a procedure to do it. CHAPTER 10.
Reference: [MP43] <author> W. S. McCulloch and W. Pitts. </author> <title> A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5 </volume> <pages> 115-133, </pages> <year> 1943. </year>
Reference-contexts: REVIEW OF RELATED WORK 12 The first formal theory of neural network structures and dynamics is usually attributed to <ref> [MP43, PM47] </ref>, where it was shown that the DQF-NN (deterministic quan-tised feedforward neural network) can implement any logical calculations, provided that there are enough neurons and the connection weights are right. The first NN learning rule is usually attributed to [Heb49, p.62].
Reference: [MP69] <author> M. L. Minsky and S. A. Papert. </author> <title> Perceptrons: An Introduction to Computational Geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1969. </year> <note> BIBLIOGRAPHY 208 </note>
Reference-contexts: The first NN learning rule is usually attributed to [Heb49, p.62]. The first formal theory on a type of artificial neural networks with a learning rule is usually traced to [Ros58, Ros59], where a perceptron network is actually one layer of DQF-NN preceded by a layer of fixed processors <ref> [MP69] </ref>. The first rigorous theory on what can and cannot be done by a certain type of neural networks was given by [MP69], where they showed that single layer perceptrons are limited in their representational capabilities. <p> networks with a learning rule is usually traced to [Ros58, Ros59], where a perceptron network is actually one layer of DQF-NN preceded by a layer of fixed processors <ref> [MP69] </ref>. The first rigorous theory on what can and cannot be done by a certain type of neural networks was given by [MP69], where they showed that single layer perceptrons are limited in their representational capabilities. There were various types of neural networks developed since 1960's, which we shall not study here. <p> Remark 4.6.1 Some earlier learning rules, such as the perceptron learning rule <ref> [Ros59, MP69] </ref>, have evaluation operators which depend explicitly on w, so that they are only applicable to particular network structure. These are called direct learning rules. <p> The activation function f can be any monotonic and bounded function for these approximation theorems to hold [Hor91]. Remark 5.3.2 These approximation properties are quite important theoretically, for they show that the multilayer networks overcome the major difficulties of single layer networks <ref> [MP69] </ref>. This also provides a justification that we are to restrict our attention to the class of H.S.NN.
Reference: [MP88] <author> M. L. Minsky and S. A. Papert. </author> <title> Perceptrons: Expanded Edition. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: In general, learning rules are optimisation methods, almost all the currently available learning rules are based on gradient-following (GF) which may lead to local optima <ref> [MP88] </ref>. Simulated annealing (SA) [KGV83] can be used to avoid local optima in an optimisation problem. <p> GF methods are by far the most widely used learning methods to date, since they are so simple yet always guarantee local improvement, and are perceived as a first step to more powerful methods. Unfortunately, the first step cannot always proceed to the hoped-for second step <ref> [Dre79, MP88] </ref>. In fact, we shall see that the GF methods almost always produce undesirable results for many interesting problems. We shall also see later that simulated annealing (SA) methods are almost as simple as GF, and overcome most of the undesirable characteristics. <p> So it follows Corollary 3.3.3 that the answer is hei, which confirms his empirical observation. 6.5 GF Learning Converge to Local Optimum in Weight Space As was pointed out in <ref> [MP88] </ref>, most of the existing learning rules for neural networks are based on gradient-following (GF), and are vulnerable to local optima in the weight space. Numerical experiment on the GF learning rule for SQB-NN [PA87] show that hei as a function of the connection weights is plagued with local maxima.
Reference: [MR86] <editor> J. L. McClelland and D. E. Rumelhart, editors. </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 2. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: This is even more remarkable considering that Turing machines (TM) are universal computational machines. Recent exciting developments in connectionist models of computation <ref> [RM86, MR86] </ref> point out two possible extensions to TM which seems to be essential for intelligence: the stochastic nature of general computation and the accumulation of information. <p> Learning based on a genuine entropy principle was given in [LTS90], although their method is somewhat theoretical rather than practical. Much of this research was inspired by the two volume "PDP book" <ref> [RM86, MR86] </ref>, which is something of a "connectionist manifesto".
Reference: [MRR + 53] <author> N. Metropolis, A. Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller. </author> <title> Equations of state calculations by fast computing machines. </title> <journal> J. Chem. Phys., </journal> <volume> 21:1087, </volume> <year> 1953. </year>
Reference-contexts: An early attempt at applying the Monte Carlo method to the calculation of state variables in massively interacting systems led to the Metropolis method <ref> [MRR + 53] </ref>, which is one of the so-called importance weighted methods [HH64]. <p> Furthermore, 8g 2 [R N ! R) : [g (r)] x = hg (r)i x . The proof of this theorem can be found in, for example, [GG84, Ami89, AK89b]. See also <ref> [MRR + 53, KGV83, AHS85, HS86] </ref> for supplementary information. <p> Remark 6.2.2 The condition (6.2.3) only defines a one-parameter family of processes, since the two probabilities of flipping a bit can be scaled by a same factor. Some of the more important in this family are the Metropolis process <ref> [MRR + 53] </ref>, the Glauber process [Gla63] and the local Boltzmann process [AHS85]. They will be discussed in detail in x7.2, when an efficient implementation on conventional computers is also introduced. 6.3 Relations with Other Networks 6.3.1 Relation with SQB-NN (Boltzmann machine) The SQFB-NN is a generalization of SQB-NN (x5.2). <p> The 92 CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 93 detailed balance gives p i (+j) so only one independent parameter left. There are two important GS in this one-parameter family: the Metropolis process <ref> [MRR + 53] </ref> and the local Boltzmann process [AHS85].
Reference: [MSW90] <author> W. Thomas Miller, III, R. S. Sutton, and P. J. Werbos, </author> <title> editors. Neural Networks for Control. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: REVIEW OF RELATED WORK 17 The theories discussed above are decision theories. A related large research area is the theory of adaptive control, which also deals with "optimisation over time". Although these two terms are often used interchangeably (see [FS90] and many other references in <ref> [MSW90] </ref>), they are almost mutually exclusive and complementary to each other in mathematical terms. The difference between them is that the former is concerned with global optimisation while the latter concerned with differentiable optimisation. <p> We shall not be concerned with control problems in the thesis except in x10.4.2, where the relations between decision and control theories are further explored. For a survey of adaptive control see [WRS89] and the collection <ref> [MSW90] </ref>. The theories mentioned above are based on the assumption that the "state of world" at any moment can be perceived directly by the agent, and processed in detail. In practice such an assumption may not be granted, and the problem does not belong to MDP.
Reference: [MW87] <author> I. Morgenstern and D. Wurtz. </author> <title> Simulated annealing for "spin-glass-like" optimization problems. </title> <journal> Z. Phys. B, </journal> <volume> 67 </volume> <pages> 397-403, </pages> <year> 1987. </year>
Reference-contexts: We call ! the equilibration speed, and * the equilibration time. Following [NS88], we call v := (hei hei w )= the thermodynamic speed. 7.4.4 Cooling schedule There has been an intensive research on the cooling schedule of simulated annealing <ref> [KGV83, HCH91, SNH + 88, MW87] </ref>. Although SA is usually described as reducing temperature, in the annealing process it is better to keep some other quantity as a constant, instead of the speed of temperature reduction [KGV83]. One might suggest such as constant heat dissipation (CHD), etc.
Reference: [Nea92] <author> R. M. Neal. </author> <title> Connectionist learning of belief networks. </title> <journal> Artif. Intell., </journal> <volume> 56 </volume> <pages> 71-113, </pages> <year> 1992. </year>
Reference-contexts: A network of learning automata without loops is a SQF-NN. Another derivation was from the Bayesian inference, this results in "belief networks" [Pea87]. The ME learning rules for SQF-NN was studied extensively in [Wil92, Wil87, Wil88, Wil90]. Recently the ML learning rule for SQF-NN was studied in <ref> [Nea92] </ref>. It was cited in [PH89] that a similar learning rule was studied for SQF-NN in [BW88]. A general framework for structure and dynamics of PDP models has been described in [RHM86]. <p> DCB-NN: This is the continuous Hopfield net [Hop84, HT86], the mean field (MF) BM [PA87] or deterministic BM [Hin89b], and the feedback perceptron network [Alm87, Pin87] 1 . SQF-NN: This is called belief network <ref> [Nea92, Pea87] </ref>, and simple reinforcement learn ing networks [Wil87, Wil92]. SQB-NN: This is the most famous Boltzmann machine (BM) [AHS85, HS86]. 1 They are in fact the same network with minor variations in detail. CHAPTER 4. THEORY OF H. S. <p> It is noticeable that the only known ME learning rules with practical implementation (x6.4, x6.6) use cross entropy (CE) while the only known ML learning rules with practical implementation ([AHS85], x5.2, x7.6.4) use reverse cross entropy (RCE). The ME learning rule with RCE has been studied in <ref> [Nea92] </ref> while the ML learning rule with CE has been studied in [LL88]. No non-enumerative implementations were given in both references. Therefore it remains an interesting open question as whether such learning rules are possible.
Reference: [NL77] <author> K. S. Narendra and S. Lakshmivarahan. </author> <title> Learning automata|a critique. </title> <journal> J. of Cyber. & Inf. Sci., </journal> <volume> 1 </volume> <pages> 53-65, </pages> <year> 1977. </year>
Reference-contexts: The development of SQF-NN has several origins, most of them are related to DQF-NN, ie. logical circuits. One origin was the theory of "stochastic learning automata" [NT74], where each automaton was actually a neuron, and many of them were connected to perform certain tasks <ref> [Lak81, NL77, BA85, NT89] </ref>. A network of learning automata without loops is a SQF-NN. Another derivation was from the Bayesian inference, this results in "belief networks" [Pea87]. The ME learning rules for SQF-NN was studied extensively in [Wil92, Wil87, Wil88, Wil90].
Reference: [NS72] <author> A. Newell and H. A. Simon. </author> <title> Human Problem Solving. </title> <publisher> Prentice Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1972. </year>
Reference-contexts: Since pure polynomials are not good for this purpose (and, indeed, not good for most approximation problems), some "heuristic rules" were used in the program for managing a set of preferred polynomial terms. Remark 2.2.1 Heuristic rules became rather popular in AI research <ref> [NS72] </ref>, but there has never been any solid theoretical foundation [Dre79] 5 . <p> Chapter 3 Adaptive Information Processing 3.1 Introduction In this chapter we develop the theory of adaptive information processing. Our usage of the term information processing allows for stochastic systems, and, therefore, is significantly different from that of classical theories <ref> [NS72] </ref>. In fact our emphasis will be on those abilities of stochastic systems which a deterministic information system, which we call data processing systems, can not have. 3.2 Review of Probability Theory Our references to probability and stochastic processes are [Kol56a, CM65, GS82, GS74].
Reference: [NS88] <author> J. D. Nulton and P. Salamon. </author> <title> Statistical mechanics of combinatorial optimization. </title> <journal> Phys. Rev., </journal> <volume> A37:1351-1356, </volume> <year> 1988. </year>
Reference-contexts: One major theoretical breakthrough in the study of the SA method occurred in the research of cooling schedules which, in our interpretation, is the study of how to assign the value to the missing information. The results of constant thermodynamic speed (CTS) <ref> [NS88, AHM + 88, SNH + 88] </ref> were based on purely theoretical considerations, and are of a quite universal character. <p> This means that the SA-ME learning rule is equivalent to approximating P 0 2 M by P 2 M W in the sense of minimising K (P; P 0 ). This interpretation was inspired by <ref> [NS88] </ref>, where P 0 is called target distribution and P is called system distribution. As ! 1, P 0 moves towards @M, the boundary of M, while P follows in M W . <p> Remark 7.3.3 The second and third identities in the above appeared in [AK89b], but they seem to have been well known in statistical mechanics (when there is no x) <ref> [KGV83, NS88] </ref>. 7.3.4 Ideal learning for the Hamming distance encoder problem We consider Hamming distance encoder here, which has some very nice features which makes it easy to be treated analytically. Later we shall see that other encoder problems are almost identical numerically, although being much more complicated analytically. <p> This finally gives the formulae for the learning speed, by which hei tends to an equilibrium value at rate exp (!t). Compare this with <ref> [NS88] </ref> we see that 1=* = !. Since many of the above relations are only asymptotically true for p ! 0, and since that learning must occur when p is reasonably large, the exact numerical coefficients might not be optimal. <p> Simulation results for various combinations will be given in next section. We call ! the equilibration speed, and * the equilibration time. Following <ref> [NS88] </ref>, we call v := (hei hei w )= the thermodynamic speed. 7.4.4 Cooling schedule There has been an intensive research on the cooling schedule of simulated annealing [KGV83, HCH91, SNH + 88, MW87].
Reference: [NT74] <author> K. S. Narendra and M. A. L Thathatcher. </author> <title> Learning automata|a survey. </title> <journal> IEEE Trans. Sys. Man and Cyber., </journal> <volume> 4 </volume> <pages> 323-334, </pages> <year> 1974. </year>
Reference-contexts: The applications of DCB-NN include, but are not restricted to, various optimisation problems [TH86, HT86, PH89, dBM90]. The development of SQF-NN has several origins, most of them are related to DQF-NN, ie. logical circuits. One origin was the theory of "stochastic learning automata" <ref> [NT74] </ref>, where each automaton was actually a neuron, and many of them were connected to perform certain tasks [Lak81, NL77, BA85, NT89]. A network of learning automata without loops is a SQF-NN. Another derivation was from the Bayesian inference, this results in "belief networks" [Pea87]. <p> Despite their continued popularity in AI research, we shall not discuss these methods any further. The theory of learning automata (LA) was developed from yet another origin <ref> [NT74, BA85] </ref>, which attempts to parameterise the policies of simple agents in a game-like environment. Since the utility function usually does not feature in these theories, they have about the same efficiency as "average over histories", although their main virtue is that there is no need to enumerate the histories.
Reference: [NT89] <author> K. S. Narendra and M. A. L Thathatcher. </author> <title> Learning Automata: An Introduction. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: The development of SQF-NN has several origins, most of them are related to DQF-NN, ie. logical circuits. One origin was the theory of "stochastic learning automata" [NT74], where each automaton was actually a neuron, and many of them were connected to perform certain tasks <ref> [Lak81, NL77, BA85, NT89] </ref>. A network of learning automata without loops is a SQF-NN. Another derivation was from the Bayesian inference, this results in "belief networks" [Pea87]. The ME learning rules for SQF-NN was studied extensively in [Wil92, Wil87, Wil88, Wil90].
Reference: [OR70] <author> J. M. Ortega and W. C. Rheinboldt. </author> <title> Iterative Solution of Nonlinear Equations of Several Variables. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Remark 8.3.1 An operator R b satisfying the above conditions is known as a contraction mapping or simply a contraction. A theoretical upper bound for the convergence error is ([Wat89]) ku u b k 1 kR b (u) uk 1 (1 kAk 1 ) : See <ref> [OR70] </ref> for further discussions. This is hardly of any use for us here, since the evaluation of maximum norm requires enumeration over X. Definition 8.3.1 (Utility function) The function u k := R k (0) 2 F is called the utility function of finite horizon k under policy . <p> So there is reason to expect that this holds for some general class of problems under some general conditions. One way to prove the convergence to (unique) fixed point is to show that the iteration operator is a contraction <ref> [OR70] </ref>. Unfortunately, it seems that neither P nor V are contractions globally. The following example shows that the norm of V may be larger than unity.
Reference: [OS75] <author> A. V. Oppenheim and R. W. Schafer. </author> <title> Digital Signal Processing. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1975. </year>
Reference-contexts: Our aim is to find running average processes which is reasonably accurate in some suitable sense, and which is reasonably simple to be used in each synapse of a neural network. Running averages as defined above are special types of linear signal filters <ref> [OS75, Pri81] </ref>. In particular, our aim can be expressed as "low-pass" filter. On the other hand, since the research on linear filters are predominately concerned with stationary signals, which obviously does not apply to the learning process, we have to reformulate what is needed here.
Reference: [PA87] <author> C. Peterson and J. R. Anderson. </author> <title> A mean field algorithm for neural networks. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 995-1019, </pages> <year> 1987. </year>
Reference-contexts: The ME learning rule for DQB-NN was developed by [Pin87, Alm87, Alm89b, Alm89a]. DCB-NN was also studied as mean field (MF) approximation of SQB-NN <ref> [PA87] </ref>, where another learning rule was derived from BM learning rule, which is also a GF rule [Hin89b]. The applications of DCB-NN include, but are not restricted to, various optimisation problems [TH86, HT86, PH89, dBM90]. <p> DQB-NN: This is the (discrete) Hopfield net [Hop82], DCF-NN: This is the most famous multilayer (soft) perceptron network [RHW86], also called back-propagation network. DCB-NN: This is the continuous Hopfield net [Hop84, HT86], the mean field (MF) BM <ref> [PA87] </ref> or deterministic BM [Hin89b], and the feedback perceptron network [Alm87, Pin87] 1 . SQF-NN: This is called belief network [Nea92, Pea87], and simple reinforcement learn ing networks [Wil87, Wil92]. <p> Mean field Boltz mann Machine and Feedback Perceptron The DCB-NN can be introduced from three different points of view: * As a continuous variant of Hopfield net [Hop84], (DQB ! DCB). * As a variation of feedforward networks [Alm87, Pin87], (DCF ! DCB). * As a mean field Boltzmann machine <ref> [PA87, Hin89b] </ref>. (SQB ! DCB). They can be proved to define the same input-output relation, or, using a term used in [Wil90], to be "input-output equivalent". 5.4.1 Structure and dynamics The structure of the DCB-NN is the same as that of the SQB-NN. <p> Output. Set y = Cr + Ds. CHAPTER 5. CLASSICAL MODELS OF H. S. NEURAL NETWORKS 65 The recursive equation (5.4.1) can be solved by the following three different methods. Process (P0) (5.4.1) is iterated and the fixed point is taken as solution <ref> [PA87] </ref>. <p> THEORY OF SQFB NEURAL NETWORKS 73 When the number of neurons in each layer is sufficiently large, the effect of r k1 on r k can be approximated by the mean field approximation hr k i = htanh (A k r k1 )i tanh (A k hr k1 i), see <ref> [PA87] </ref> and [Hop89]. As has been mentioned earlier (x5.3), it is known that a two layer simple-layered fully-connected DCF-NN with sufficient number of neurons in the hidden layer is a universal approximator of functions and gradients in the sense of Sobolev space [Fun89, HSW89, HSW90]. <p> However, these bounds are not quite practical since they are combinatorial. 6.3.3 Relation with DCB-NN (continuous Hopfield net, etc.) The DCFB-NN is also a generalization of the DCB-NN (x5.4), such as continuous Hop-field net [Hop84], the feedback perceptron network [Alm87], the mean field (MF) model of BM <ref> [PA87, Hin89b] </ref>. It should be noted that in general the SQFB-NN cannot be replaced by its MF approximation DCFB-NN without compromising its representational power. In particular, SQFB-NN can represent the correlations between the states of neurons in each layer, which are absent in the MF approximation. <p> Numerical experiment on the GF learning rule for SQB-NN <ref> [PA87] </ref> show that hei as a function of the connection weights is plagued with local maxima. See also Figure 7.19 & 7.20. Local optima are unavoidable since in general the evaluation function hei is not convex with respect to w. <p> None of these graphs would be available if simulated annealing were not used in the learning process. The same is true for IC graph. Although IE, IS and IT graphs are well defined even without SA, normally only the IE graph is given in the literature (eg., <ref> [AHS85, PA87] </ref>, among many others). 7.4.3 Learning speeds Generally speaking, learning speed refers to the speed of changing the weights. We first derive some auxiliary results of a general parameterised distribution. CHAPTER 7. <p> The latter is satisfied by the experiments on BM in [AHS85] which blurs the target, but not in <ref> [PA87] </ref>. We explain the details as the following. First we note that there are basically two ways of using neural networks as a computational tool [Gs88, Pin88], either in the learning process, as in [AHS85, PA87] , or in the dynamics, as in [AK89b, Hop84]. <p> We explain the details as the following. First we note that there are basically two ways of using neural networks as a computational tool [Gs88, Pin88], either in the learning process, as in <ref> [AHS85, PA87] </ref> , or in the dynamics, as in [AK89b, Hop84]. We have to distinguish between the "computational temperature" and the "learning temperature". What is called "temperature" in the literature corresponds to computational temperature which appears in the SA1 method for accelerating the GS. <p> We shall not take any more time to consider it further. It is natural to ask how the algorithms will perform without simulated annealing. The BM learning rule without SA (a GF-ML learning rule) has been tested on the 4-2-4 encoder problems in <ref> [PA87] </ref>. The results are, as one can expect, very disappointing: The network saturates at about 1=4 correct for the 4-2-4 problems. (They consider 4-2-4 as the most difficult, and did not test 8-3-8.) This is so despite the fact that the learning rate is very slow.
Reference: [Par85] <author> D. B. Parker. </author> <title> Learning logic. </title> <type> Technical Report TR-47, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1985. </year>
Reference-contexts: We shall show that ME learning can be transformed into ML learning but not the reverse. RL rules belong to ME learning rules, some SL rules, such as the back propagation (BP) rule <ref> [RHW86, Wer74, Par85] </ref> are special cases of RL. Some other SL rules, such as the Boltzmann machine (BM) learning rule [AHS85, HS86] is a ML learning rule. <p> CHAPTER 2. REVIEW OF RELATED WORK 13 are possible (For example, the prediction module in x9.2). Perhaps the most widely studied and used neural network is the DCF-NN (deterministic continuous feedforward neural network), called variably "feedforward perceptrons", "multilayer perceptrons", and "back-propagation networks". It was discovered independently by several authors <ref> [RHW86, Cun85, Par85, Wer74] </ref>. It is most famous for its learning rule, called back-propagation (BP) rule, which is the prototype for ME learning rules. A great volume of research has been devoted to various aspects of DCF-NN, of particular importance are the approximation properties [Fun89, HSW89, HSW90, Hor91]. <p> CE, @w It should be remarked that there is no practical implementation for the VBM learning rule as a ML rule, but it is closely related to a good ME rule (x6.6). 5.3 DCF-NN: Multilayer Perceptron The DCF-NN is often called back-propagation network, multilayer perceptron network or feedforward perceptron network <ref> [RHW86, Wer74, Cun85, Par85] </ref>. We give formulation for simple-layered DCF-NN, which consists of a sequence of layers of neurons where each neuron only activate neurons in immediately succeeding layer.
Reference: [Pea87] <author> J. Pearl. </author> <title> Evidential reasoning using stochastic simulation of causal models. </title> <journal> Artif. Intell., </journal> <volume> 32 </volume> <pages> 254-257, </pages> <year> 1987. </year>
Reference-contexts: A network of learning automata without loops is a SQF-NN. Another derivation was from the Bayesian inference, this results in "belief networks" <ref> [Pea87] </ref>. The ME learning rules for SQF-NN was studied extensively in [Wil92, Wil87, Wil88, Wil90]. Recently the ML learning rule for SQF-NN was studied in [Nea92]. It was cited in [PH89] that a similar learning rule was studied for SQF-NN in [BW88]. <p> DCB-NN: This is the continuous Hopfield net [Hop84, HT86], the mean field (MF) BM [PA87] or deterministic BM [Hin89b], and the feedback perceptron network [Alm87, Pin87] 1 . SQF-NN: This is called belief network <ref> [Nea92, Pea87] </ref>, and simple reinforcement learn ing networks [Wil87, Wil92]. SQB-NN: This is the most famous Boltzmann machine (BM) [AHS85, HS86]. 1 They are in fact the same network with minor variations in detail. CHAPTER 4. THEORY OF H. S.
Reference: [Pen89] <author> R. Penrose. </author> <title> The Emperor's New Mind: Concerning Computers, Minds, and the Laws of Physics. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1989. </year> <note> BIBLIOGRAPHY 209 </note>
Reference-contexts: Turing has conjectured that SETM=ETM, by using, for example, the digit of as a RNG [Tur50] 6 . For the physics question we refer to numerous texts on quantum mechanics. A more quite description can be found in <ref> [Pen89] </ref>. For the biological question, it seems a common sense that the brain is a SECM, where the collection of genes is the meta-algorithm while the current personality, mind, mentality, or the like, is the instant SCM . <p> Our research is a step in this direction, as the BAC is clearly a SETM. It demonstrated that it is possible to construct artificial systems that is both flexible and can direct the flexibility (Remark 10.5.1). Remark 10.5.2 Many of the current discussions on AI ([Sea80] and commentaries, <ref> [Pen89, Slo92] </ref>, and [Pen90] and commentaries.) use the term "mechanical" as an equivalent term for "physical", without specifying whether it includes flipping a coin. <p> without PRNG 5 The word "failure" does not mean that AI failed to put the results of human intelligence into machines, but that it failed to put any intelligence into machines. 6 Note that can only be computed by a ETM, not a TM since the latter will never stop <ref> [Pen89] </ref>. CHAPTER 10. CONCLUSIONS, DISCUSSIONS, AND SUGGESTIONS 185 is approximately stochastic. In order to avoid cycling of the PRNG, it is necessary that the internal memory of the PRNG should not have a limited size. Therefore PRNG can not be regarded as an negligible part of the system.
Reference: [Pen90] <author> R. Penrose. </author> <title> Precis of the emperor's new mind: concerning computers, minds, and the laws of physics. </title> <type> Behav. </type> <institution> Brain Sci., </institution> <year> 1990. </year>
Reference-contexts: In the control literature, this is view as "how to assign intermediate goals". 10.5 Intelligence and Artificial Intelligence The philosophical implications of viewing computation as information processing rather than data processing has a strong implications on the debate over artificial intelligence (AI) <ref> [Sea80, Pen90] </ref>. 10.5.1 A tentative definition of intelligence There seems to be no existing definition of intelligence which can encompass all the special features one usually assigns to this concept [Min87]. We suggest a tangible definition which may serve as a working definition. <p> It demonstrated that it is possible to construct artificial systems that is both flexible and can direct the flexibility (Remark 10.5.1). Remark 10.5.2 Many of the current discussions on AI ([Sea80] and commentaries, [Pen89, Slo92], and <ref> [Pen90] </ref> and commentaries.) use the term "mechanical" as an equivalent term for "physical", without specifying whether it includes flipping a coin.
Reference: [PH89] <author> C. Peterson and E. Hartman. </author> <title> Exploration of the mean field theory learning algorithms. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 475-494, </pages> <year> 1989. </year>
Reference-contexts: DCB-NN was also studied as mean field (MF) approximation of SQB-NN [PA87], where another learning rule was derived from BM learning rule, which is also a GF rule [Hin89b]. The applications of DCB-NN include, but are not restricted to, various optimisation problems <ref> [TH86, HT86, PH89, dBM90] </ref>. The development of SQF-NN has several origins, most of them are related to DQF-NN, ie. logical circuits. <p> Another derivation was from the Bayesian inference, this results in "belief networks" [Pea87]. The ME learning rules for SQF-NN was studied extensively in [Wil92, Wil87, Wil88, Wil90]. Recently the ML learning rule for SQF-NN was studied in [Nea92]. It was cited in <ref> [PH89] </ref> that a similar learning rule was studied for SQF-NN in [BW88]. A general framework for structure and dynamics of PDP models has been described in [RHM86]. In particular it defines the classes of quasilinear and semilinear neural networks, for which we shall develop a mathematical theory.
Reference: [Pin87] <author> F. J. Pineda. </author> <title> Generalization of backpropagation to recurrent neural networks. </title> <journal> Physics Review Letters, </journal> <volume> 59 </volume> <pages> 2229-2232, </pages> <year> 1987. </year>
Reference-contexts: A glimpse of sample applications is provided by [MHP90]. The DQB-NN was also generalised in [Hop84] to DCB-NN (deterministic continuous feedback neural network), usually called "(continuous) Hopfield net", the stability of which was also guaranteed by a Lyapunov function. The ME learning rule for DQB-NN was developed by <ref> [Pin87, Alm87, Alm89b, Alm89a] </ref>. DCB-NN was also studied as mean field (MF) approximation of SQB-NN [PA87], where another learning rule was derived from BM learning rule, which is also a GF rule [Hin89b]. The applications of DCB-NN include, but are not restricted to, various optimisation problems [TH86, HT86, PH89, dBM90]. <p> DQB-NN: This is the (discrete) Hopfield net [Hop82], DCF-NN: This is the most famous multilayer (soft) perceptron network [RHW86], also called back-propagation network. DCB-NN: This is the continuous Hopfield net [Hop84, HT86], the mean field (MF) BM [PA87] or deterministic BM [Hin89b], and the feedback perceptron network <ref> [Alm87, Pin87] </ref> 1 . SQF-NN: This is called belief network [Nea92, Pea87], and simple reinforcement learn ing networks [Wil87, Wil92]. SQB-NN: This is the most famous Boltzmann machine (BM) [AHS85, HS86]. 1 They are in fact the same network with minor variations in detail. CHAPTER 4. THEORY OF H. S. <p> are cascaded, and can otherwise be discarded. 5.4 DCB-NN: Continuous Hopfield Net, Mean field Boltz mann Machine and Feedback Perceptron The DCB-NN can be introduced from three different points of view: * As a continuous variant of Hopfield net [Hop84], (DQB ! DCB). * As a variation of feedforward networks <ref> [Alm87, Pin87] </ref>, (DCF ! DCB). * As a mean field Boltzmann machine [PA87, Hin89b]. (SQB ! DCB).
Reference: [Pin88] <author> F. J. Pineda. </author> <title> Dynamics and architecture for neural computation. </title> <journal> J. of Complexity, </journal> <volume> 4 </volume> <pages> 216-245, </pages> <year> 1988. </year>
Reference-contexts: A general framework for structure and dynamics of PDP models has been described in [RHM86]. In particular it defines the classes of quasilinear and semilinear neural networks, for which we shall develop a mathematical theory. See <ref> [Sej81, Pin88] </ref> for other attempts on general theory of NN. Concerning neural network structures, we have seen that the feedforward (F-) and feedback (B-) neural networks have been proved stable. <p> The B connections results in stable computation because there is an energy function, while for F connections it is because there is no feedback so that input can be considered as constant. Remark 4.5.6 It may be the case that any NN is weakly stable <ref> [Pin88] </ref>. What we are interested in are those NN for which stationary distribution does not depend on the updating probability. <p> The latter is satisfied by the experiments on BM in [AHS85] which blurs the target, but not in [PA87]. We explain the details as the following. First we note that there are basically two ways of using neural networks as a computational tool <ref> [Gs88, Pin88] </ref>, either in the learning process, as in [AHS85, PA87] , or in the dynamics, as in [AK89b, Hop84]. We have to distinguish between the "computational temperature" and the "learning temperature".
Reference: [PM47] <author> W. Pitts and W. S. McCulloch. </author> <title> How we know universals: The perception of auditory and visual forms. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 9 </volume> <pages> 127-147, </pages> <year> 1947. </year>
Reference-contexts: REVIEW OF RELATED WORK 12 The first formal theory of neural network structures and dynamics is usually attributed to <ref> [MP43, PM47] </ref>, where it was shown that the DQF-NN (deterministic quan-tised feedforward neural network) can implement any logical calculations, provided that there are enough neurons and the connection weights are right. The first NN learning rule is usually attributed to [Heb49, p.62].
Reference: [Pri81] <author> M. B. Priestley. </author> <title> Spectral Analysis and Time Series. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: Our aim is to find running average processes which is reasonably accurate in some suitable sense, and which is reasonably simple to be used in each synapse of a neural network. Running averages as defined above are special types of linear signal filters <ref> [OS75, Pri81] </ref>. In particular, our aim can be expressed as "low-pass" filter. On the other hand, since the research on linear filters are predominately concerned with stationary signals, which obviously does not apply to the learning process, we have to reformulate what is needed here.
Reference: [RHM86] <author> D. E. Rumelhart, G. E. Hinton, and J. L. McClelland. </author> <title> A general framework for parallel distributed processing. </title> <editor> In Rumelhart and McClelland [RM86], </editor> <booktitle> chapter 4. </booktitle>
Reference-contexts: Recently the ML learning rule for SQF-NN was studied in [Nea92]. It was cited in [PH89] that a similar learning rule was studied for SQF-NN in [BW88]. A general framework for structure and dynamics of PDP models has been described in <ref> [RHM86] </ref>. In particular it defines the classes of quasilinear and semilinear neural networks, for which we shall develop a mathematical theory. See [Sej81, Pin88] for other attempts on general theory of NN. <p> We postpone until the next chapter a detailed study of the most important specialisations, including the standard NNs and their generalisations. 4.2 Definitions and Basic Properties The class of quasilinear neural networks (Q.NN) and its subset semilinear neural networks (S.NN) were first formalised in <ref> [RHM86] </ref>. Our definition is slightly different: In our definitions, the class of Q.NN is somewhat larger while that of S.NN is somewhat smaller than those in [RHM86]. <p> generalisations. 4.2 Definitions and Basic Properties The class of quasilinear neural networks (Q.NN) and its subset semilinear neural networks (S.NN) were first formalised in <ref> [RHM86] </ref>. Our definition is slightly different: In our definitions, the class of Q.NN is somewhat larger while that of S.NN is somewhat smaller than those in [RHM86].
Reference: [RHW86] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart and McClelland [RM86], </editor> <booktitle> chapter 8. </booktitle>
Reference-contexts: For the purpose of introduction, a brief recollection of the particular train of thought leading to this theory is probably more informative. The starting point is the backpropagation learning rule <ref> [RHW86] </ref> and the results that feedforward NNs are in a sense universal approximators of functions [Fun89, HSW89]. This means that NN can be regarded as a parameterised function which can adjust itself to suit the demand. <p> We shall show that ME learning can be transformed into ML learning but not the reverse. RL rules belong to ME learning rules, some SL rules, such as the back propagation (BP) rule <ref> [RHW86, Wer74, Par85] </ref> are special cases of RL. Some other SL rules, such as the Boltzmann machine (BM) learning rule [AHS85, HS86] is a ML learning rule. <p> CHAPTER 2. REVIEW OF RELATED WORK 13 are possible (For example, the prediction module in x9.2). Perhaps the most widely studied and used neural network is the DCF-NN (deterministic continuous feedforward neural network), called variably "feedforward perceptrons", "multilayer perceptrons", and "back-propagation networks". It was discovered independently by several authors <ref> [RHW86, Cun85, Par85, Wer74] </ref>. It is most famous for its learning rule, called back-propagation (BP) rule, which is the prototype for ME learning rules. A great volume of research has been devoted to various aspects of DCF-NN, of particular importance are the approximation properties [Fun89, HSW89, HSW90, Hor91]. <p> DQF-NN: This includes the so-called logical circuits or multilayer (hard-limit) perceptron network. The perceptron network described in [Ros58, Ros59] can be regarded as single layer DQF-NN. DQB-NN: This is the (discrete) Hopfield net [Hop82], DCF-NN: This is the most famous multilayer (soft) perceptron network <ref> [RHW86] </ref>, also called back-propagation network. DCB-NN: This is the continuous Hopfield net [Hop84, HT86], the mean field (MF) BM [PA87] or deterministic BM [Hin89b], and the feedback perceptron network [Alm87, Pin87] 1 . SQF-NN: This is called belief network [Nea92, Pea87], and simple reinforcement learn ing networks [Wil87, Wil92]. <p> On the other hand, if there is a distance function d on the output space Y so that e (x; y) := d (' 0 (x); y) is well defined, and if the evaluation operator is defined by (4.6.7), the learning rule belongs to ME (eg. learning rules in <ref> [RHW86, Alm87] </ref>) The RL mode belongs to ME, where v = e. It applies to SQ-NN with jY j &lt; 1. In the UL mode, V = ;, but E 0 is rather restricted. <p> CE, @w It should be remarked that there is no practical implementation for the VBM learning rule as a ML rule, but it is closely related to a good ME rule (x6.6). 5.3 DCF-NN: Multilayer Perceptron The DCF-NN is often called back-propagation network, multilayer perceptron network or feedforward perceptron network <ref> [RHW86, Wer74, Cun85, Par85] </ref>. We give formulation for simple-layered DCF-NN, which consists of a sequence of layers of neurons where each neuron only activate neurons in immediately succeeding layer. <p> CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 96 7.3.2 The class of encoder problems We use as our primary example the well known encoder problem <ref> [AHS85, RHW86] </ref>, which is described as the following: Let R = f1; 1g, n 2 N. Define R n 1 := fx 2 R n : j fi : x i &gt; 0g j = 1g. Let X = Y R n . <p> (yjx), try to modify w so that y will be an approximation of x in either of the following two senses: ME: It is required that hd (x; y)i ! 0, where d 2 [Y 2 P ! R + ) and d (~; ) = 0 () ~ = <ref> [RHW86] </ref>, or, equivalently, hei ! max, where e is normalised from d with Definition 7.3.1 ML: It is required that K (P 0 ; P ) ! 0 or K (P; P 0 ) ! 0, where K is the Kullback separation, P is the network distribution and P 0 is <p> Let w i = , then sech 2 w i = 2 =n = . This proves (7.4.11) by continuity. CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 106 Following <ref> [RHW86] </ref>, we use a second order formula to update the synapses, ffiw i = fi @w i ffiw i ; w i = ffffiw i ;(7.4.12) where ffiw is an auxiliary variable. <p> Several techniques most frequently cited in the literature were tested, including "momentum term" and random initial weights <ref> [RHW86] </ref>, "weight decay" [HS86] and added noise [AHS85], none of them completely overcome this problem.
Reference: [Rip87] <author> B. D. Ripley. </author> <title> Stochastic Simulations. </title> <editor> J. </editor> <publisher> Wiley & Sons, </publisher> <address> Chichester, </address> <year> 1987. </year>
Reference-contexts: Remark 7.2.3 It seems prudent to point out here that the above analysis are based on the assumption that the random numbers are genuine. How the regularities in the pseudo-random number generator <ref> [Rip87] </ref> might affect the above results is an open question, although we have not found any discernible such effects in the experiments. 7.3 Encoder Problems and Ideal Learning The learning rules of SQFB-NN are derived under the assumption that all the processes can be performed arbitrarily slowly. <p> The second statement of the conjecture means that SETM can be (approximately) implemented on TM as a virtual machine (For stochastic approximations refer to <ref> [Rip87, Knu81] </ref>). In this thesis we have tacitly assumed that the use of pseudo-random number generator (PRNG) in the simulations produces no substantial difference from that of RNG. Turing has conjectured that SETM=ETM, by using, for example, the digit of as a RNG [Tur50] 6 .
Reference: [RIV91] <author> A. K. Rigler, J. M. Irvine, and T. P. Vogl. </author> <title> Rescaling of variables in back propagation learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 225-229, </pages> <year> 1991. </year>
Reference-contexts: An upper bound for the number of neurons needed for a two-hidden layer DQF-NN was given in [BL91]. This result is immediately applicable to DCF-NN and SQF-NN (stochastic quantised feedforward neural network), as they are generalisations of DQF-NN. Other studies in DCF-NN include how to accelerate convergence of learning <ref> [Sam91, RIV91, Fah88] </ref>, and other aspects of learning [Sus92]. There are probably well over a thousand different published applications of DCF-NN, covering virtually every field where adaptive approximation of a nonlinear function might be of some use. A glimpse of sample applications is provided by [MHP90]. <p> Remark 4.3.10 This assumption also means for C-Q.NN we set R = [1; 1], while for Q-Q.NN we set R = f1; 1g. There is no fundamental difference between this scaling of R and any other, thanks to the quasilinear property, but <ref> [RIV91] </ref> suggests that the gradient of weights for DC-Q.NN with R = [1; 1] is better conditioned than that with R = [0; 1], resulting in faster learning. Remark 4.3.11 The scaling certainly matters in physical implementations, affecting damage tolerance.
Reference: [RM86] <editor> D. E. Rumelhart and J. L. McClelland, editors. </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: This is even more remarkable considering that Turing machines (TM) are universal computational machines. Recent exciting developments in connectionist models of computation <ref> [RM86, MR86] </ref> point out two possible extensions to TM which seems to be essential for intelligence: the stochastic nature of general computation and the accumulation of information. <p> Learning based on a genuine entropy principle was given in [LTS90], although their method is somewhat theoretical rather than practical. Much of this research was inspired by the two volume "PDP book" <ref> [RM86, MR86] </ref>, which is something of a "connectionist manifesto".
Reference: [Ros58] <author> F. Rosenblatt. </author> <title> The perceptron, a probabilistic model for information storage and organization in the brain. </title> <journal> Psychological Review, </journal> <volume> 65 </volume> <pages> 386-408, </pages> <year> 1958. </year>
Reference-contexts: The first NN learning rule is usually attributed to [Heb49, p.62]. The first formal theory on a type of artificial neural networks with a learning rule is usually traced to <ref> [Ros58, Ros59] </ref>, where a perceptron network is actually one layer of DQF-NN preceded by a layer of fixed processors [MP69]. <p> Important examples of this classification include the following. DQF-NN: This includes the so-called logical circuits or multilayer (hard-limit) perceptron network. The perceptron network described in <ref> [Ros58, Ros59] </ref> can be regarded as single layer DQF-NN. DQB-NN: This is the (discrete) Hopfield net [Hop82], DCF-NN: This is the most famous multilayer (soft) perceptron network [RHW86], also called back-propagation network.
Reference: [Ros59] <author> R. Rosenblatt. </author> <title> Principles of Neurodynamics. </title> <publisher> Spartan Books, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: The first NN learning rule is usually attributed to [Heb49, p.62]. The first formal theory on a type of artificial neural networks with a learning rule is usually traced to <ref> [Ros58, Ros59] </ref>, where a perceptron network is actually one layer of DQF-NN preceded by a layer of fixed processors [MP69]. <p> Important examples of this classification include the following. DQF-NN: This includes the so-called logical circuits or multilayer (hard-limit) perceptron network. The perceptron network described in <ref> [Ros58, Ros59] </ref> can be regarded as single layer DQF-NN. DQB-NN: This is the (discrete) Hopfield net [Hop82], DCF-NN: This is the most famous multilayer (soft) perceptron network [RHW86], also called back-propagation network. <p> Remark 4.6.1 Some earlier learning rules, such as the perceptron learning rule <ref> [Ros59, MP69] </ref>, have evaluation operators which depend explicitly on w, so that they are only applicable to particular network structure. These are called direct learning rules.
Reference: [Ros83] <author> S. M. Ross. </author> <title> Introduction to Stochastic Dynamic Programming. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: The mathematical part (ie. excluding the model building) of DP is also called theory of controlled Markov chains (CMC) or Markov CHAPTER 2. REVIEW OF RELATED WORK 15 decision processes (MDP) <ref> [Ros83, DY79, Der70] </ref>. <p> From another point of view, this is a generalisation of SA to multistage problems. Our presentation will be similar to that of <ref> [Ros83, DY79] </ref>, but substantially generalised to allow adaptive improvement. 8.2 The Mathematical Model Definition 8.2.1 (Markov decision process) A Markov decision process (MDP) is a tuple [X; A; E; T; ; t; r; fl], satisfying the following conditions E S fi A; T = N; P t 2 [T ! [E <p> By Theorem 8.3.2 this definition is equivalent to that based on fixed points of R . Note that the usual more intuitive definition of u as a weighted sum of the rewards <ref> [How60, Wit77, DY79, Ros83, Wat89, Sut90] </ref> are only applicable for uniform fl &lt; 1. <p> Theorem 8.3.6 (Optimality) Suppose the value function v exists. Then v = u fl , P fl 6= ;, and 8 2 P : The proof of the above two theorems can be found in <ref> [Ros83] </ref>. <p> The value iteration method can also be written in the following more convenient form. 1. Set u := 0. 3. Set u := R (u). 4. Goto step 2. These are well known results in DP (eg. <ref> [Ros83] </ref>). One of their distinctive features is that at each step the "optimal" action for current state under current utility function is chosen. <p> In our formulation the optimal solutions is defined as limits of equilibrium solutions there is no need to study them separately. Our formulation also unifies different approaches in DP, such as fixed horizon programming, discounted reward programming, total return programming and average gain programming <ref> [How60, Bel57, Ros83] </ref>, by the following theorem. Theorem 8.5.3 Let E 2 R nfin with (E) 1. Then 8r 2 R n : 1. The following holds whenever the limit of the denumerator exists: lim h X E k r k=0 1 h E k r = 1:(8.5.5) CHAPTER 8.
Reference: [Sam59] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal on Research and Development, </journal> <volume> 3 </volume> <pages> 210-229, </pages> <year> 1959. </year> <note> Reprinted in [FF63, 71-105]. </note>
Reference-contexts: The (implicit) application of RL as a computational method can be traced back to the "checker playing program" of Samuel <ref> [Sam59] </ref>, in which the evaluation is represented by a polynomial. Since pure polynomials are not good for this purpose (and, indeed, not good for most approximation problems), some "heuristic rules" were used in the program for managing a set of preferred polynomial terms. <p> CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 102 Remark 7.4.1 The polynomial formula with s = 1 and the exponential formula have been used in learning methods in as early as <ref> [Sam59] </ref> and their merits were discussed in [Min61]. <p> BAC can have two basic modules, decision module (D) and evaluation module (E1), and two optional modules, prediction module (P) and action evaluation module (E2). The modules (D) and (E) were present in <ref> [Sam59, BSA83] </ref>, the modules (D), (E1) and (E2) appear in [Wat89], and the module (D), (E1) and (P) appear in [Sut90]. 9.2 General Description of BAC In this chapter our description will be implementation oriented, and not as formal as the previous chapter. <p> More intuitively, to predict what the opponent will do, think what the player himself will do in that situation, and use the zero-sum property of the game. This is a quite old technique for testing such learning methods CHAPTER 9. BASIC ADAPTIVE COMPUTERS 159 <ref> [Sam59] </ref>. For the second assumption, a prediction module would be useful, although not necessary, for updating the evaluation. Under either assumption, the environment is not stationary. Therefore this game is slightly more general than our theory allows.
Reference: [Sam91] <author> T. Samad. </author> <title> Back propagation with expected source values. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 615-618, </pages> <year> 1991. </year>
Reference-contexts: An upper bound for the number of neurons needed for a two-hidden layer DQF-NN was given in [BL91]. This result is immediately applicable to DCF-NN and SQF-NN (stochastic quantised feedforward neural network), as they are generalisations of DQF-NN. Other studies in DCF-NN include how to accelerate convergence of learning <ref> [Sam91, RIV91, Fah88] </ref>, and other aspects of learning [Sus92]. There are probably well over a thousand different published applications of DCF-NN, covering virtually every field where adaptive approximation of a nonlinear function might be of some use. A glimpse of sample applications is provided by [MHP90].
Reference: [SB90] <author> R. S. Sutton and A. G. Barto. </author> <title> Time-derivative models of Pavlov reinforcement. </title> <editor> In Gabriel and Moore [GM90]. </editor> <publisher> BIBLIOGRAPHY 210 </publisher>
Reference-contexts: A remarkable feature of this reference is that it presented a theory, rather than a method specifically designed for a few problems. The diverse research field of reinforcement learning (RL) 4 originates from the study of animal behaviour under "classical conditioning" (Pavlov conditioning) <ref> [SB90] </ref>. One of its main features is to (continuously) parameterise the utility function and/or the policy, so that they can be improved "incrementally".
Reference: [Sea80] <author> J. R. </author> <title> Searle. </title> <journal> Minds, brains and science. Behav. Brain Sci., </journal> <volume> 3(3), </volume> <year> 1980. </year>
Reference-contexts: Does this reflect some fundamental differences between a machine and a conscious being? Or is this merely because the computers of today are not yet complicated enough? This is the core of the current intensive debate concerning the so-called "strong-AI thesis" <ref> [Sea80] </ref>. Although it is accepted common sense in the scientific community that there is no mystic force behind the phenomenon of intelligence, it is evident that the current computational theory lacks something essential to account for it. <p> In the control literature, this is view as "how to assign intermediate goals". 10.5 Intelligence and Artificial Intelligence The philosophical implications of viewing computation as information processing rather than data processing has a strong implications on the debate over artificial intelligence (AI) <ref> [Sea80, Pen90] </ref>. 10.5.1 A tentative definition of intelligence There seems to be no existing definition of intelligence which can encompass all the special features one usually assigns to this concept [Min87]. We suggest a tangible definition which may serve as a working definition. <p> There is a famous metaphor used in the "strong-AI debate" arguing against the TT as a test of intelligence, called the "Chinese room problem" <ref> [Sea80] </ref>: An English speaker knowing nothing about the Chinese language is locked in a room with formal rules written in English for handling Chinese characters written on slips passed through a window.
Reference: [Sej81] <author> T. J. Sejnowski. </author> <title> Skeleton filters in the brain. </title> <editor> In G. E. Hinton and J. R. An-derson, editors, </editor> <title> Parallel Models of Associative Memory. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1981. </year>
Reference-contexts: A general framework for structure and dynamics of PDP models has been described in [RHM86]. In particular it defines the classes of quasilinear and semilinear neural networks, for which we shall develop a mathematical theory. See <ref> [Sej81, Pin88] </ref> for other attempts on general theory of NN. Concerning neural network structures, we have seen that the feedforward (F-) and feedback (B-) neural networks have been proved stable. <p> It CHAPTER 2. REVIEW OF RELATED WORK 14 should be noted that the formalism in <ref> [Sej81] </ref> even allows SQ and DC to be viewed as extremes of a one-parameter family. We discuss this briefly in x4.4. Neural network learning rules are surveyed in [Hin89a], this reference contains original ideas on various important issues of learning. <p> The duality between SQ- and DC-NN has an even deeper origin than the MF in terpretation of DC-NN. In fact, they are two extremes in a same one-parameter family, proposed as early as <ref> [Sej81] </ref>. The Sejnowski class of NN is such that the neuronal input is the decaying time average (leaky integration) of the input we discussed above. <p> of linear compositions of r and x of infinite duration, which equals the ensemble average, so TNN reduces to the MF approximation of SQ-NN, namely, the DC-NN, described by s = Ax + Bf (s); where f = D E Remark 4.4.4 It is quite remarkable that the formulation in <ref> [Sej81] </ref> have all the essential ingredients of today's models: It is both semilinear and stochastic. Therefore it encompasses all the NN mentioned in x4.3.4. <p> Although it seems unlikely that a general theory on SCFB-NN would be possible, it is possible to include DCFB-NN and SQFB-NN into a one-parameter family, using the formulation given in <ref> [Sej81] </ref> (x4.4). One of the major obstacles so far to the development of neural network hardware have CHAPTER 10. CONCLUSIONS, DISCUSSIONS, AND SUGGESTIONS 177 been the great variety of neural networks which has appeared in the literature.
Reference: [Sen73a] <author> E. Seneta. </author> <title> Non-Negative Matrices: An Introduction to Theory and Applications. </title> <editor> George Allen & Unwin, </editor> <address> London, </address> <year> 1973. </year>
Reference-contexts: We introduce some new mathematical notations, most notably those on mappings, which will be used extensively. Basic concept from pure mathematics can be found in [Die60]. Most results on matrices which we shall use can be found in <ref> [Cia89, Sen73b, CM65, Sen73a] </ref>. Our reference to probability and stochastic processes are [Kol56a, CM65, GS82, GS74]. The slanted typeface generally denote the first or the most formal appearance of a technical term.
Reference: [Sen73b] <author> E. Seneta. </author> <title> Non-Negative Matrices and Markov Chains. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1973. </year>
Reference-contexts: The state space X with transition mapping t forms a Markov chain (MC). The concept of closed set and recurrent state of a MC are defined as in [CM65]. Closed set is also called recurrent chain in [How60] and commutative set in <ref> [Sen73b] </ref>. The following theorem gives a sufficient condition for the existence of utility function. Theorem 8.4.7 Suppose that each closed set of the MC (X; t ) has at least one terminal state. Then kL k 1 &lt; 1. Proof. <p> We introduce some new mathematical notations, most notably those on mappings, which will be used extensively. Basic concept from pure mathematics can be found in [Die60]. Most results on matrices which we shall use can be found in <ref> [Cia89, Sen73b, CM65, Sen73a] </ref>. Our reference to probability and stochastic processes are [Kol56a, CM65, GS82, GS74]. The slanted typeface generally denote the first or the most formal appearance of a technical term.
Reference: [Sha48] <author> C. E. Shannon. </author> <title> A mathematical theory of communication,I & II. </title> <journal> Bell Syst. Tech. J., </journal> <volume> 27 </volume> <pages> 379-423, 623-656, </pages> <year> 1948. </year>
Reference-contexts: Whether this is done artificially (by some outside programmer) or automatically (by AIP itself) differentiates between PIP and TIP. A physical device implementing a fixed (stochastic) mapping is called a fixed information processor (FIP), which is equivalent to a communication channel <ref> [Sha48] </ref> although with quite different purposes. A PIP with a given program is a deterministic FIP. The concept of AC is an abstraction of animal brains and artificial adaptive computational devices. <p> Shannon established the mathematical foundation of information theory in the monumental papers <ref> [Sha48] </ref>, in which it became perfectly clear that entropy is a purely mathematical concept, just as probability is, and it is a more fundamental concept than the physical concept of irreversibility. One of the most significant contributions of [Sha48] was to show that entropy is the only measure of disorder, or <p> Shannon established the mathematical foundation of information theory in the monumental papers <ref> [Sha48] </ref>, in which it became perfectly clear that entropy is a purely mathematical concept, just as probability is, and it is a more fundamental concept than the physical concept of irreversibility. One of the most significant contributions of [Sha48] was to show that entropy is the only measure of disorder, or missing information, under some very reasonable conditions one would usually attribute to such a measure. <p> The entropy is a very good barrier function: it is symmetric, finite, and convex, and it has infinite derivative at the boundary of the simplex <ref> [Sha48, Kul59] </ref>. <p> The tanh function, and equivalently the logistic function, has been singled out as the most important activation function, due to its close relationship with entropy. This enables us to give information theoretical interpretations of many results of NN. Since the form of entropy is unique <ref> [Sha48] </ref>, it is most likely that the tanh function is also unique in this regard. It is unclear how much of this holds for the formulations from the so-called Cauchy machine [Szu86]. <p> In order to avoid cycling of the PRNG, it is necessary that the internal memory of the PRNG should not have a limited size. Therefore PRNG can not be regarded as an negligible part of the system. Another possible objection follows Shannon's coding theorem <ref> [Sha48] </ref>, which says that any information source can be coded in arbitrary precision so as to use an amount of data almost as small as the amount of information. There is even a general deterministic algorithm for doing this, namely the Huffmann coding [Abr63].
Reference: [Sha50] <author> C. Shannon. </author> <title> Programming a digital computer for playing chess. </title> <journal> Phil. Mag., </journal> <volume> 41 </volume> <pages> 356-375, </pages> <year> 1950. </year>
Reference-contexts: The question I asked was: "Can such devices learn to play chess?" This question is quite legitimate since the rules of chess are well defined. It is also significant since, as many pioneers of computational theory had recognised <ref> [Wie48, Tur50, Sha50, Tur53] </ref>, if computers can learn to play games like chess, they can do things hitherto considered as the reserve of human intelligence. 2 CHAPTER 1. <p> INTRODUCTION 3 One aspect of games like chess of particular importance to computational theory is that it is impossible to enumerate all the possibilities of the game, even with the resource of the universe <ref> [Sha50] </ref>. This makes it a necessity that any successful method for such problems be adaptive, or incremental, so that it can provide a solution with limited resources, and can improve the solution whenever more resources are available.
Reference: [Shv88] <author> H. Shvaytser. </author> <title> Even simple neural nets cannot be trained reliably with a polynomial number of examples. </title> <booktitle> In IJCNN, editor, IJCNN Intl. Joint Conf. on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 141-146, </pages> <year> 1988. </year>
Reference-contexts: So DQ-NN can be viewed as extremes of both DC-NN and SQ-NN. On the other hand, DQ-NN is less useful in practice owing to the above singularity which prevent it to have a reasonable learning rule <ref> [BR92, Shv88] </ref>. This is hardly surprising, since for DQ-NN the parameterization of realizable processing by the weight vector is discontinuous. When the states of both SQ-NN and DC-NN in a dual pair are being referred at the same time, there need to be separate names for them.
Reference: [Sin92] <author> S. P. Singh. </author> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> 8(3/4):323-339, 1992. 
Reference-contexts: This is necessary since the decision module simply select the optimal action deterministically according to the evaluation module. Various models based on these methods have been tested on specific problems in <ref> [Lin92, dRMT92, Sin92] </ref>. A review of RL can be found in [Bar90]. * * * These above methods (game theory, dynamic programming and reinforcement learning) either update the policy in the action space (deterministic policies), or in the policy space by gradient following.
Reference: [Slo92] <author> A. Sloman. </author> <title> The emperor's real mind: Review of Roger Penrose's The Emperor's New Mind: Concerning Computers, Minds, and the Laws of Physics. </title> <journal> Artif. Intell., </journal> <volume> 56 </volume> <pages> 355-396, </pages> <year> 1992. </year>
Reference-contexts: Our research is a step in this direction, as the BAC is clearly a SETM. It demonstrated that it is possible to construct artificial systems that is both flexible and can direct the flexibility (Remark 10.5.1). Remark 10.5.2 Many of the current discussions on AI ([Sea80] and commentaries, <ref> [Pen89, Slo92] </ref>, and [Pen90] and commentaries.) use the term "mechanical" as an equivalent term for "physical", without specifying whether it includes flipping a coin.
Reference: [SNH + 88] <author> P. Salamon, J. D. Nulton, J. R. Harland, J. Pedersen, G. Ruppeiner, and L. Liao. </author> <title> Simulated annealing with constant thermodynamic speed. </title> <journal> Computer Phys. Commun., </journal> <volume> 49 </volume> <pages> 423-428, </pages> <year> 1988. </year>
Reference-contexts: One major theoretical breakthrough in the study of the SA method occurred in the research of cooling schedules which, in our interpretation, is the study of how to assign the value to the missing information. The results of constant thermodynamic speed (CTS) <ref> [NS88, AHM + 88, SNH + 88] </ref> were based on purely theoretical considerations, and are of a quite universal character. <p> We call ! the equilibration speed, and * the equilibration time. Following [NS88], we call v := (hei hei w )= the thermodynamic speed. 7.4.4 Cooling schedule There has been an intensive research on the cooling schedule of simulated annealing <ref> [KGV83, HCH91, SNH + 88, MW87] </ref>. Although SA is usually described as reducing temperature, in the annealing process it is better to keep some other quantity as a constant, instead of the speed of temperature reduction [KGV83]. One might suggest such as constant heat dissipation (CHD), etc. <p> One might suggest such as constant heat dissipation (CHD), etc. The one we find most convincing is the constant thermodynamic speed (CTS) schedule <ref> [SNH + 88] </ref>. For several technical reasons, it is better to describe cooling schedules in terms of = 1=, at least to avoid changing across zero in one step.
Reference: [SSF92] <author> J. A. Scales, M. L. Smith, and T. L. Fisher. </author> <title> Global optimization methods for multimodal inverse problems. </title> <journal> J. Comp. Phys., </journal> <volume> 103(2) </volume> <pages> 258-268, </pages> <year> 1992. </year>
Reference-contexts: A short survey of cooling schedules was given [HCH91], and global optimisation methods are surveyed in <ref> [SSF92] </ref> A different approach to the Monte Carlo principle is through the stochastic neural 7 Something is called a principle if it involves a change of problem, which cannot be justified on purely logical ground. The reason this is allowed is that the original problem itself is usually an idealisation.
Reference: [Sus92] <author> H. J. Susmann. </author> <title> Uniqueness of the weights for minimal feedforward nets with a given input-output map. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 589-593, </pages> <year> 1992. </year>
Reference-contexts: This result is immediately applicable to DCF-NN and SQF-NN (stochastic quantised feedforward neural network), as they are generalisations of DQF-NN. Other studies in DCF-NN include how to accelerate convergence of learning [Sam91, RIV91, Fah88], and other aspects of learning <ref> [Sus92] </ref>. There are probably well over a thousand different published applications of DCF-NN, covering virtually every field where adaptive approximation of a nonlinear function might be of some use. A glimpse of sample applications is provided by [MHP90]. <p> Since there is no a priori preference between 1 and 1 for the hidden units, and no preference to which hidden unit should represent what feature, any local optimum is accompanied by a permutation group of other local optima with the same evaluation <ref> [Sus92] </ref>. Chapter 7 Implementation of SQFB Neural Networks 7.1 Introduction We discuss the implementational issues concerning SQFB-NN which arise from the finite precision and finite speed of running average processes. <p> This happens at about the same time the synapse has the correct sign. 1 which means that the synapses will have correct sign of one member in a permutation group corresponding to ideal learning <ref> [Sus92] </ref>. CHAPTER 7. IMPLEMENTATION OF SQFB NEURAL NETWORKS 130 7.5.2 The 4-2-4 encoder We use the 4-2-4 encoder as our main test problem.
Reference: [Sut84] <author> R. S. Sutton. </author> <title> Temporal credit assignment in reinforcement learning. </title> <type> PhD thesis, </type> <institution> Dept. of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1984. </year>
Reference-contexts: We call these two parameterisations the "evaluation module" and the "policy module" in the "basic adaptive computer". Most of RL research concentrates on the evaluation module, ie., they assume that there is a mechanism to choose the optimal option, given the evaluation. The temporal difference (TD) method <ref> [Sut84, Sut88] </ref> for predicting the expected value of a function of the states of a Markov chain can be used for updating the evaluation module [BSW90]. Its convergence was proved with a local representation of state space [Day92].
Reference: [Sut88] <author> R. S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: We call these two parameterisations the "evaluation module" and the "policy module" in the "basic adaptive computer". Most of RL research concentrates on the evaluation module, ie., they assume that there is a mechanism to choose the optimal option, given the evaluation. The temporal difference (TD) method <ref> [Sut84, Sut88] </ref> for predicting the expected value of a function of the states of a Markov chain can be used for updating the evaluation module [BSW90]. Its convergence was proved with a local representation of state space [Day92]. <p> Therefore the total gradient can be computed through a back-propagation process (BP) through all the combined local gradient. The formulae (3.5.10) and (3.5.16) are called average over states, while (3.5.11) and (3.5.12) are called average over histories. It is well known (eg. <ref> [Sut88] </ref>), that average over states is in general much more efficient than average over histories for long Markov chains, since the number of histories usually grows in exponential with the length of the chain. Remark 3.5.3 If k represents layers of a neural network this gives back-propagation learning rule. <p> The ensemble average R (e 1 ) is used in the experiment in the following sections. The "temporal difference" method (TD) <ref> [Sut88] </ref> can be used in conjunction with e q. 9.3 Implementing the Decision Module by DC-NN When the action space A can be enumerated, the decision module can be constructed with DC-NN.
Reference: [Sut90] <author> R. S. Sutton. </author> <title> First results with dyna, an integrated architecture for learning, planning and reacting. </title> <editor> In Miller, III et al. </editor> <address> [MSW90], </address> <note> chapter 8. </note>
Reference-contexts: Use of neural network to approximate the policy and utility function was proposed in [And89], and was later generalised to problems with more than two actions in the Dyna model <ref> [Sut90] </ref>. In this model the policy is represented as a Boltzmann distribution over actions. This is of fundamental importance, for it allows the introduction of information theory into the decision theory. We call these two parameterisations the "evaluation module" and the "policy module" in the "basic adaptive computer". <p> By Theorem 8.3.2 this definition is equivalent to that based on fixed points of R . Note that the usual more intuitive definition of u as a weighted sum of the rewards <ref> [How60, Wit77, DY79, Ros83, Wat89, Sut90] </ref> are only applicable for uniform fl &lt; 1. <p> If two functions e 1 2 F; e 2 2 G satisfies e 1 = U e 2 , e 2 = Q (e 1 ), then e 1 = u . Using R ; (e 1 ) generalises those methods used in <ref> [BSA83, Sut90] </ref>, while using U ; (e 2 ) generalises the Q-learning method [Wat89]. CHAPTER 8. ADAPTIVE MARKOV DECISION PROCESS 147 Approximate SA method 2 (AMDP-ASA2) 1. Set e 1 := 0, e 2 := 0. Set = 0. Set = rnd. 2. <p> The modules (D) and (E) were present in [Sam59, BSA83], the modules (D), (E1) and (E2) appear in [Wat89], and the module (D), (E1) and (P) appear in <ref> [Sut90] </ref>. 9.2 General Description of BAC In this chapter our description will be implementation oriented, and not as formal as the previous chapter. <p> BASIC ADAPTIVE COMPUTERS 158 of solutions so that we can compare experiments with theoretical predictions, and on the other, they have features which represent a class of problems more general than those treated in the literature so far <ref> [BSA83, Sut90, Lin92, Wat89, WB91, Tes92] </ref>. Variations of these test problems can serve as new test problems for basic adaptive computers (BAC) with more modules. <p> The strategy the BAC invented turned out to be surprisingly simple. If a deterministic enumerative method is used, the game would be much more difficult than a maze, since all the visited squares becomes walls. This game is substantially more difficult than the maze problem played by "Dyna" in <ref> [Sut90] </ref>. 9.5 Scaling and Speed There have been numerous tests and applications of connectionist models reported in the literature. <p> One such arbitrariness occur in the measure of performance. Various measures which are only meaningful to the (artificial) problems have been used, such as surviving time, error rate, words recognised, just to mention a few (eg., <ref> [BSA83, Sut90, Lin92, WB91, Tes92] </ref>). Example 9.5.1 A chess player should learn exactly the same strategy in the following two settings: (1) win= 1, lose= 1, tie=0; (2) win= 100, lose= 0, tie=50. <p> These choices are by no means optimal. Experiments leading to SA learning rule We first tested this problem before developing the theory on SA learning rules, using the GF learning rule which is almost identical to that of <ref> [Sut90] </ref>. To our surprise the BAC almost never converged to the desired global optimum, despite the fact that the global optimum is representable (ie., if the weights are hand set to somewhere near the global optimum, it converges very fast automatically). <p> In principle, this can also be used to simulate chess. Our current implementation of the simulation on a serial machine is too slow to carry it out. (5) The moving point game is an extension of the maze problem played by Dyna in <ref> [Sut90] </ref>. The new factor is that there are combinatorially enumerable states and there are numerous local optima for the decision module. The combination of these properties seems have not been achieved by previous methods.
Reference: [Szu86] <author> H. Szu. </author> <title> Fast simulated annealing. </title> <editor> In J. Denker, editor, </editor> <booktitle> Neural Networks for Computing, volume 151 of AIP Conference Proceedings, </booktitle> <pages> pages 420-425, </pages> <address> Snowbird, UT, </address> <year> 1986. </year> <journal> Amer. Inst. Phys. </journal> <volume> BIBLIOGRAPHY 211 </volume>
Reference-contexts: Since the form of entropy is unique [Sha48], it is most likely that the tanh function is also unique in this regard. It is unclear how much of this holds for the formulations from the so-called Cauchy machine <ref> [Szu86] </ref>.
Reference: [Tes92] <author> G. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> 8(3/4):257-277, 1992. 
Reference-contexts: Unlike DP, there are many methods but only a few scattered theories in RL, since the relationship between the parameterisation and the actual application is usually a crucial factor in the overall performance, but this relationship most often has only been analysed with regard to specific test problems (eg. <ref> [Tes92, Lin92] </ref>). The (implicit) application of RL as a computational method can be traced back to the "checker playing program" of Samuel [Sam59], in which the evaluation is represented by a polynomial. <p> Many specific models have been proposed in the literature to deal with these issues separately, with rather artificial criteria for success ( x10.4.2). A comprehensive discussion of important problems relating to the aforementioned methods was given in <ref> [Tes92] </ref>. <p> BASIC ADAPTIVE COMPUTERS 158 of solutions so that we can compare experiments with theoretical predictions, and on the other, they have features which represent a class of problems more general than those treated in the literature so far <ref> [BSA83, Sut90, Lin92, Wat89, WB91, Tes92] </ref>. Variations of these test problems can serve as new test problems for basic adaptive computers (BAC) with more modules. <p> One such arbitrariness occur in the measure of performance. Various measures which are only meaningful to the (artificial) problems have been used, such as surviving time, error rate, words recognised, just to mention a few (eg., <ref> [BSA83, Sut90, Lin92, WB91, Tes92] </ref>). Example 9.5.1 A chess player should learn exactly the same strategy in the following two settings: (1) win= 1, lose= 1, tie=0; (2) win= 100, lose= 0, tie=50.
Reference: [TH86] <author> D. W. Tank and J. J. </author> <title> Hopfield. Simple "neural" optimization networks: An A/D converter, signal decision circuit, and a linear programming circuit. </title> <journal> IEEE Trans. Circ. Sys., </journal> <volume> 33(5) </volume> <pages> 533-541, </pages> <year> 1986. </year>
Reference-contexts: DCB-NN was also studied as mean field (MF) approximation of SQB-NN [PA87], where another learning rule was derived from BM learning rule, which is also a GF rule [Hin89b]. The applications of DCB-NN include, but are not restricted to, various optimisation problems <ref> [TH86, HT86, PH89, dBM90] </ref>. The development of SQF-NN has several origins, most of them are related to DQF-NN, ie. logical circuits.
Reference: [TTL84] <author> Y. Tikochinsky, N. Tishby, and R. D. Levine. </author> <title> Alternative approach to maximum entropy inference. </title> <journal> Physics Review, A, </journal> <volume> 30 </volume> <pages> 2638-2644, </pages> <year> 1984. </year>
Reference-contexts: We use the concept of entropy in the purely mathematical sense, so we shall not go into the details of statistical physics. There is a detailed discussion of CHAPTER 2. REVIEW OF RELATED WORK 18 MAXENT in <ref> [TTL84] </ref> from a physicist's point of view.
Reference: [Tur36] <author> A. M. </author> <title> Turing. On computable numbers, with an application to the entschei-dungsproblem. </title> <journal> Proc. London Math. Soc., </journal> <volume> 42 </volume> <pages> 230-265, </pages> <year> 1936. </year>
Reference-contexts: CHAPTER 10. CONCLUSIONS, DISCUSSIONS, AND SUGGESTIONS 183 10.5.3 Turing machine and its generalisations The following definitions are not formal, but can be easily formalised. Definition 10.5.3 A Turing machine (TM) <ref> [Tur36] </ref> is a device with finite internal states, which can perform the following operations on an infinitely long tape of binary digits (bits): read a bit, write a zero, write a one, move one bit to the left, move one bit to the right, and stop, as well as changing internal
Reference: [Tur50] <author> A. M. </author> <title> Turing. </title> <journal> Computing machinery and intelligence. Mind, </journal> <volume> 59 </volume> <pages> 433-460, </pages> <month> October </month> <year> 1950. </year> <note> Reprinted in [FF63, pp. 17-36]. </note>
Reference-contexts: Recent exciting developments in connectionist models of computation [RM86, MR86] point out two possible extensions to TM which seems to be essential for intelligence: the stochastic nature of general computation and the accumulation of information. Although these ideas were already present in Turing's first account of artificial intelligence <ref> [Tur50] </ref>, they remained largely ignored in the later developments of computational science. In this thesis we try to develop a unified theory and general methods of stochastic adaptive computations based on neural networks (NN). <p> The question I asked was: "Can such devices learn to play chess?" This question is quite legitimate since the rules of chess are well defined. It is also significant since, as many pioneers of computational theory had recognised <ref> [Wie48, Tur50, Sha50, Tur53] </ref>, if computers can learn to play games like chess, they can do things hitherto considered as the reserve of human intelligence. 2 CHAPTER 1. <p> Neither a deterministic system nor a totally random system can be intelligent. The former has no flexibility while the later has no direction. Neither of them can learn. 10.5.2 The Turing Test So far the only widely accepted criterion of artificial intelligence is the famous Turing test <ref> [Tur50] </ref>. <p> A restricted Turing Test (RTT) is a procedure which is identical to TT except that the topics of conversation is restricted to a fixed domain dependent on the machine. Turing's own guess <ref> [Tur50] </ref> was that at the end of this century the best machine would be powerful enough for an ordinary interrogator not to be able to derive 0.12 bit of mutual information between five minute's conversation and the correct identification. (1 + 0:3 log 2 0:3 + 0:7 log 2 0:7 0:12.) <p> We conjecture that there are SETM which is intelligent, and that the space of ETM is dense in the space of SETM in a suitable sense. The first statement in the conjecture is a formalisation of ideas in <ref> [Tur50] </ref>, where the necessity of "random element" and "child machine" for passing the TT is discussed. The second statement of the conjecture means that SETM can be (approximately) implemented on TM as a virtual machine (For stochastic approximations refer to [Rip87, Knu81]). <p> In this thesis we have tacitly assumed that the use of pseudo-random number generator (PRNG) in the simulations produces no substantial difference from that of RNG. Turing has conjectured that SETM=ETM, by using, for example, the digit of as a RNG <ref> [Tur50] </ref> 6 . For the physics question we refer to numerous texts on quantum mechanics. A more quite description can be found in [Pen89].
Reference: [Tur53] <author> A. M. </author> <title> Turing. Digital computers applied to games. </title> <editor> In B. V. Bowden, editor, </editor> <title> Faster than thought: </title> <booktitle> A Symposium on Digital Computing Machines, chapter 25, </booktitle> <pages> pages 286-310. </pages> <publisher> Pitman, </publisher> <address> London, </address> <year> 1953. </year>
Reference-contexts: The question I asked was: "Can such devices learn to play chess?" This question is quite legitimate since the rules of chess are well defined. It is also significant since, as many pioneers of computational theory had recognised <ref> [Wie48, Tur50, Sha50, Tur53] </ref>, if computers can learn to play games like chess, they can do things hitherto considered as the reserve of human intelligence. 2 CHAPTER 1.
Reference: [Vem88] <author> V. Vemuri. </author> <title> Artificial neural networks: An introduction. </title> <editor> In V. Vemuri, editor, </editor> <booktitle> Artificial Neural Networks: Theoretical Concepts, </booktitle> <pages> pages 1-12. </pages> <publisher> IEEE, </publisher> <year> 1988. </year>
Reference: [vNM47] <author> J. von Neumann and O. Morgenstern. </author> <title> Theory of Games and Economic Behavior. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ., </address> <year> 1947. </year>
Reference-contexts: Games theory is a general mathematical theory concerning the "optimal behaviour" of many players in an environment <ref> [vNM47] </ref>. Its most profound consequence is that in general there can be no consistent definition of optimal actions which is universally valid, but it is possible to define optimal strategies under very weak conditions, provided that "mixed strategy", also called "stochastic policy", is allowed.
Reference: [Was90] <author> P. D. Wasserman. NeuralSource: </author> <title> The Bibliographic Guide to Artificial Neural Networks. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: We shall give precise definitions of the terms used in the thesis, we shall not attempt to mention the different and often conflicting definitions in the literature. 2.1 SCAP: Neural Networks Neural networks are also called connectionist models or parallel distributed processing (PDP) models. The reference book <ref> [Was90] </ref> includes over 4000 entries up to 1989, where 1988 alone counts for more than 1000.
Reference: [Wat89] <author> C. J. C. H. Watkins. </author> <title> Learning with delayed rewards. </title> <type> PhD thesis, </type> <institution> Psychology Dept, Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: Its convergence was proved with a local representation of state space [Day92]. It extracts the first order statistics in such a way as to achieve a balance between minimum bias and minimum variation. Another method for updating the evaluation module is the Q-learning method <ref> [Wat89] </ref>, which requires a local representation of action space as well. Its convergence was shown in [WD92], under the assumption that the agent has an ability similar to "dreaming" which can replay state-action sequences with appropriate frequency. <p> By Theorem 8.3.2 this definition is equivalent to that based on fixed points of R . Note that the usual more intuitive definition of u as a weighted sum of the rewards <ref> [How60, Wit77, DY79, Ros83, Wat89, Sut90] </ref> are only applicable for uniform fl &lt; 1. <p> Note that the usual more intuitive definition of u as a weighted sum of the rewards [How60, Wit77, DY79, Ros83, Wat89, Sut90] are only applicable for uniform fl &lt; 1. Remark 8.3.3 The definition of action utility is inspired by the concept of "action values" in <ref> [Wat89, WD92] </ref> Q f (x; a) = Q (u f )(x; a); f 2 P; a 2 A; x 2 X; Definition 8.3.3 (Value function) The function v k := R k fl (0) 2 F is called the value function of finite horizon k. <p> Using R ; (e 1 ) generalises those methods used in [BSA83, Sut90], while using U ; (e 2 ) generalises the Q-learning method <ref> [Wat89] </ref>. CHAPTER 8. ADAPTIVE MARKOV DECISION PROCESS 147 Approximate SA method 2 (AMDP-ASA2) 1. Set e 1 := 0, e 2 := 0. Set = 0. Set = rnd. 2. Change to increase R ; (e 1 ) or to increase U ; (e 2 ). 3. <p> BAC can have two basic modules, decision module (D) and evaluation module (E1), and two optional modules, prediction module (P) and action evaluation module (E2). The modules (D) and (E) were present in [Sam59, BSA83], the modules (D), (E1) and (E2) appear in <ref> [Wat89] </ref>, and the module (D), (E1) and (P) appear in [Sut90]. 9.2 General Description of BAC In this chapter our description will be implementation oriented, and not as formal as the previous chapter. <p> BASIC ADAPTIVE COMPUTERS 158 of solutions so that we can compare experiments with theoretical predictions, and on the other, they have features which represent a class of problems more general than those treated in the literature so far <ref> [BSA83, Sut90, Lin92, Wat89, WB91, Tes92] </ref>. Variations of these test problems can serve as new test problems for basic adaptive computers (BAC) with more modules.
Reference: [WB91] <author> S. D. Whitehead and D. H. Ballard. </author> <title> Learning to perceive and act by trial and error. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 45-84, </pages> <year> 1991. </year>
Reference-contexts: Remark 8.2.2 (1) Some other equivalent definitions of the reward are possible. For example, it can be arranged that r t = r 0 t (x t ; x t+1 ), or even r t = r 00 t (x t+1 ). See, for example, <ref> [WB91] </ref>. (2) Terminal states are those states in which a game is finished and a new one is yet to start. <p> BASIC ADAPTIVE COMPUTERS 158 of solutions so that we can compare experiments with theoretical predictions, and on the other, they have features which represent a class of problems more general than those treated in the literature so far <ref> [BSA83, Sut90, Lin92, Wat89, WB91, Tes92] </ref>. Variations of these test problems can serve as new test problems for basic adaptive computers (BAC) with more modules. <p> One such arbitrariness occur in the measure of performance. Various measures which are only meaningful to the (artificial) problems have been used, such as surviving time, error rate, words recognised, just to mention a few (eg., <ref> [BSA83, Sut90, Lin92, WB91, Tes92] </ref>). Example 9.5.1 A chess player should learn exactly the same strategy in the following two settings: (1) win= 1, lose= 1, tie=0; (2) win= 100, lose= 0, tie=50. <p> CHAPTER 10. CONCLUSIONS, DISCUSSIONS, AND SUGGESTIONS 181 10.4.5 Sensory control of incomplete perception In some problems the "situation" cannot be perceived directly, so that the "perception" is not Markovian, even if the situation is Markovian. In addition the sensory can be actively controlled <ref> [WB91] </ref>. With an adequate active sensory control subsystem and an adaptive model of the situation of the outside world, a "perceived situation" formed from a sequence of "perceptions" should approximate the real situation. <p> They have the same property that their knowledge of environment is itself determined by their output, so the same problem of "ostrich behaviour", called "perception aliasing" in <ref> [WB91] </ref>, is also the most important issue for sensory control. The difference is that if the memorise discards some information, it cannot be recovered later, while the sensory system can always choose to look again at the same feature of situation.
Reference: [WD92] <author> C. J. C. H. Watkins and P. Dayan. </author> <title> Technical note: </title> <journal> Q-learning. Machine Learning, </journal> 8(3/4):279-292, 1992. 
Reference-contexts: It extracts the first order statistics in such a way as to achieve a balance between minimum bias and minimum variation. Another method for updating the evaluation module is the Q-learning method [Wat89], which requires a local representation of action space as well. Its convergence was shown in <ref> [WD92] </ref>, under the assumption that the agent has an ability similar to "dreaming" which can replay state-action sequences with appropriate frequency. This is necessary since the decision module simply select the optimal action deterministically according to the evaluation module. <p> Note that the usual more intuitive definition of u as a weighted sum of the rewards [How60, Wit77, DY79, Ros83, Wat89, Sut90] are only applicable for uniform fl &lt; 1. Remark 8.3.3 The definition of action utility is inspired by the concept of "action values" in <ref> [Wat89, WD92] </ref> Q f (x; a) = Q (u f )(x; a); f 2 P; a 2 A; x 2 X; Definition 8.3.3 (Value function) The function v k := R k fl (0) 2 F is called the value function of finite horizon k.
Reference: [Wer74] <author> P. J. Werbos. </author> <title> Beyond Regression: New tools for prediction and analysis in the behavioral science. </title> <type> PhD thesis, </type> <institution> Harvard University Committee on Applied Mathematics, </institution> <year> 1974. </year>
Reference-contexts: We shall show that ME learning can be transformed into ML learning but not the reverse. RL rules belong to ME learning rules, some SL rules, such as the back propagation (BP) rule <ref> [RHW86, Wer74, Par85] </ref> are special cases of RL. Some other SL rules, such as the Boltzmann machine (BM) learning rule [AHS85, HS86] is a ML learning rule. <p> CHAPTER 2. REVIEW OF RELATED WORK 13 are possible (For example, the prediction module in x9.2). Perhaps the most widely studied and used neural network is the DCF-NN (deterministic continuous feedforward neural network), called variably "feedforward perceptrons", "multilayer perceptrons", and "back-propagation networks". It was discovered independently by several authors <ref> [RHW86, Cun85, Par85, Wer74] </ref>. It is most famous for its learning rule, called back-propagation (BP) rule, which is the prototype for ME learning rules. A great volume of research has been devoted to various aspects of DCF-NN, of particular importance are the approximation properties [Fun89, HSW89, HSW90, Hor91]. <p> CE, @w It should be remarked that there is no practical implementation for the VBM learning rule as a ML rule, but it is closely related to a good ME rule (x6.6). 5.3 DCF-NN: Multilayer Perceptron The DCF-NN is often called back-propagation network, multilayer perceptron network or feedforward perceptron network <ref> [RHW86, Wer74, Cun85, Par85] </ref>. We give formulation for simple-layered DCF-NN, which consists of a sequence of layers of neurons where each neuron only activate neurons in immediately succeeding layer.
Reference: [Wer90a] <author> P. J. Werbos. </author> <title> Backpropagation through time: What it is and how to do it. </title> <booktitle> In IEEE Proceedings, </booktitle> <pages> pages 1550-1560, </pages> <year> 1990. </year>
Reference-contexts: Since the utility function usually does not feature in these theories, they have about the same efficiency as "average over histories", although their main virtue is that there is no need to enumerate the histories. These methods are also called backpropagation through time (BTT) <ref> [Wer90a] </ref>. The first "average over states" method in which both the policy and evaluation are parameterised is usually traced to [BSA83]. The policy is also represented in the form of a look-up table, and is applicable to agents with only two choices of actions 6 . <p> Remark 3.5.3 If k represents layers of a neural network this gives back-propagation learning rule. If k denotes time then (3.5.12) gives back-propagation through time (BTT) <ref> [Wer90a] </ref>. The direct application of BTT for a long temporal sequence is not practical, however, since it is average over histories. The dynamic programming method (DP), which uses averaging over states, is much better, and will be studied in detail in Chapter 8. <p> A tentative idea for constructing an AC which can act in continuous time is to cascade the backpropagation through time (BTT) learning in recurrent networks <ref> [Wer90a] </ref>, which can generate aperiodic output given a stationary input, and is useful for filling the gap between two consecutive time steps.
Reference: [Wer90b] <author> P. J. Werbos. </author> <title> Consistency of HDP applied to a simple reinforcement learning-problem. </title> <booktitle> Neural Networks, </booktitle> <volume> 3(2) </volume> <pages> 179-189, </pages> <year> 1990. </year>
Reference-contexts: There is a detailed discussion of this issue in <ref> [Wer90b] </ref>. 8.5.3 Relation between the discount and the temperature The convergence of the utility function depends on L &lt; 1.
Reference: [Wer90c] <author> P. J. Werbos. </author> <title> A menu of designs for reinforcement learning over time. </title> <editor> In Miller, III et al. [MSW90], </editor> <volume> chapter 3, </volume> <pages> pages 67-95. </pages>
Reference-contexts: starting state, that it should give a new state and an reward after being informed of the action taken, and that it should be able to tell when the game is over. (2) There are no spatial relations presented to the BAC, as demonstrated in the following metaphor due to <ref> [Wer90c] </ref>. The moving point game as "seen" by the BAC can be described as this: There is a display board with 32 randomly located lights, and a control board with 4 randomly located buttons. At the start, two lights on the display board are on.
Reference: [Wie48] <author> N. Wienner. </author> <title> Cybernetics. </title> <publisher> Wiley, </publisher> <address> New York, 48. BIBLIOGRAPHY 212 </address>
Reference-contexts: The question I asked was: "Can such devices learn to play chess?" This question is quite legitimate since the rules of chess are well defined. It is also significant since, as many pioneers of computational theory had recognised <ref> [Wie48, Tur50, Sha50, Tur53] </ref>, if computers can learn to play games like chess, they can do things hitherto considered as the reserve of human intelligence. 2 CHAPTER 1.
Reference: [Wil87] <author> R. J. Williams. </author> <title> Reinforcement-learning connectionist systems. </title> <type> Technical Report NU-CCS-87-3, </type> <institution> College of Computer Science, Northeastern University, </institution> <address> Boston, MA, </address> <year> 1987. </year>
Reference-contexts: A network of learning automata without loops is a SQF-NN. Another derivation was from the Bayesian inference, this results in "belief networks" [Pea87]. The ME learning rules for SQF-NN was studied extensively in <ref> [Wil92, Wil87, Wil88, Wil90] </ref>. Recently the ML learning rule for SQF-NN was studied in [Nea92]. It was cited in [PH89] that a similar learning rule was studied for SQF-NN in [BW88]. A general framework for structure and dynamics of PDP models has been described in [RHM86]. <p> DCB-NN: This is the continuous Hopfield net [Hop84, HT86], the mean field (MF) BM [PA87] or deterministic BM [Hin89b], and the feedback perceptron network [Alm87, Pin87] 1 . SQF-NN: This is called belief network [Nea92, Pea87], and simple reinforcement learn ing networks <ref> [Wil87, Wil92] </ref>. SQB-NN: This is the most famous Boltzmann machine (BM) [AHS85, HS86]. 1 They are in fact the same network with minor variations in detail. CHAPTER 4. THEORY OF H. S. <p> This is a form of backpropagation. 4.6.3 GF-ME learning in SQ-NN The gradient-following (GF) RL in SQF-NN have been discussed in detail in <ref> [Wil87, Wil92] </ref>. Theorem 4.6.4 Let p 0 2 P (X), e 2 [X fi Y ! R). Define e 0 (yjx) := he (x; y)i. Let W = R M , P 2 [W ! P (Y jX)), P w ~ exp T w . <p> The second factor is called the reinforcement factor. The Hebbian factor is also called "characteristic eligibility" in reinforcement learning literature <ref> [Wil87, Wil92] </ref>, following [BSA83]. Notation. Denote by [] k1 and [] some approximate time averages approximating hi k1 and hi, respectively. The operator [] k1 can be obtained at the same time when r k is equilibrating. The operator [] is computed over the whole learning process. <p> Remark 6.4.7 The above learning rule answers the question of choosing "reinforcement baseline" posed in <ref> [Wil87, Wil92] </ref>: in the learning rule ffiw k = ff t @w T (e k b) ;(6.4.14) what should be the best choice of b, the reinforcement baseline? It follows Theorem 3.4.7&3.4.8 that this is exactly the GF-ME learning rule.
Reference: [Wil88] <author> R. J. Williams. </author> <title> On the use of back propagation in associative reinforcement learning. </title> <booktitle> In Proc. IEEE Intl. Conf. on Neural Networks, </booktitle> <year> 1988. </year>
Reference-contexts: A network of learning automata without loops is a SQF-NN. Another derivation was from the Bayesian inference, this results in "belief networks" [Pea87]. The ME learning rules for SQF-NN was studied extensively in <ref> [Wil92, Wil87, Wil88, Wil90] </ref>. Recently the ML learning rule for SQF-NN was studied in [Nea92]. It was cited in [PH89] that a similar learning rule was studied for SQF-NN in [BW88]. A general framework for structure and dynamics of PDP models has been described in [RHM86].
Reference: [Wil90] <author> R. J. Williams. </author> <title> Adaptive state representation and estimation using recurrent connectionist networks. </title> <editor> In Miller, III et al. </editor> <address> [MSW90], </address> <note> chapter 4. </note>
Reference-contexts: A network of learning automata without loops is a SQF-NN. Another derivation was from the Bayesian inference, this results in "belief networks" [Pea87]. The ME learning rules for SQF-NN was studied extensively in <ref> [Wil92, Wil87, Wil88, Wil90] </ref>. Recently the ML learning rule for SQF-NN was studied in [Nea92]. It was cited in [PH89] that a similar learning rule was studied for SQF-NN in [BW88]. A general framework for structure and dynamics of PDP models has been described in [RHM86]. <p> They can be proved to define the same input-output relation, or, using a term used in <ref> [Wil90] </ref>, to be "input-output equivalent". 5.4.1 Structure and dynamics The structure of the DCB-NN is the same as that of the SQB-NN. Let A, B, C and D be matrices representing the synapse from the input to the neurons, among the neurons and from the neurons to the output, respectively.
Reference: [Wil92] <author> R. J. Williams. </author> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> 8(3/4):229-256, 1992. 
Reference-contexts: A network of learning automata without loops is a SQF-NN. Another derivation was from the Bayesian inference, this results in "belief networks" [Pea87]. The ME learning rules for SQF-NN was studied extensively in <ref> [Wil92, Wil87, Wil88, Wil90] </ref>. Recently the ML learning rule for SQF-NN was studied in [Nea92]. It was cited in [PH89] that a similar learning rule was studied for SQF-NN in [BW88]. A general framework for structure and dynamics of PDP models has been described in [RHM86]. <p> DCB-NN: This is the continuous Hopfield net [Hop84, HT86], the mean field (MF) BM [PA87] or deterministic BM [Hin89b], and the feedback perceptron network [Alm87, Pin87] 1 . SQF-NN: This is called belief network [Nea92, Pea87], and simple reinforcement learn ing networks <ref> [Wil87, Wil92] </ref>. SQB-NN: This is the most famous Boltzmann machine (BM) [AHS85, HS86]. 1 They are in fact the same network with minor variations in detail. CHAPTER 4. THEORY OF H. S. <p> This is a form of backpropagation. 4.6.3 GF-ME learning in SQ-NN The gradient-following (GF) RL in SQF-NN have been discussed in detail in <ref> [Wil87, Wil92] </ref>. Theorem 4.6.4 Let p 0 2 P (X), e 2 [X fi Y ! R). Define e 0 (yjx) := he (x; y)i. Let W = R M , P 2 [W ! P (Y jX)), P w ~ exp T w . <p> The second factor is called the reinforcement factor. The Hebbian factor is also called "characteristic eligibility" in reinforcement learning literature <ref> [Wil87, Wil92] </ref>, following [BSA83]. Notation. Denote by [] k1 and [] some approximate time averages approximating hi k1 and hi, respectively. The operator [] k1 can be obtained at the same time when r k is equilibrating. The operator [] is computed over the whole learning process. <p> Remark 6.4.7 The above learning rule answers the question of choosing "reinforcement baseline" posed in <ref> [Wil87, Wil92] </ref>: in the learning rule ffiw k = ff t @w T (e k b) ;(6.4.14) what should be the best choice of b, the reinforcement baseline? It follows Theorem 3.4.7&3.4.8 that this is exactly the GF-ME learning rule.
Reference: [Wit77] <author> I. H. Witten. </author> <title> An adaptive optimal controller for discrete-time Markov environment. </title> <journal> Information and Control, </journal> <volume> 34 </volume> <pages> 286-295, </pages> <year> 1977. </year>
Reference-contexts: So for a rather long period there had been little effort to parameterise the policy in DP. The first completely parameterised RL method seems to be the rather neglected reference <ref> [Wit77] </ref>, but his parameterisation is in the form of a large look-up table of probability distributions, and he did not mention the relation with either DP or RL. A remarkable feature of this reference is that it presented a theory, rather than a method specifically designed for a few problems. <p> By Theorem 8.3.2 this definition is equivalent to that based on fixed points of R . Note that the usual more intuitive definition of u as a weighted sum of the rewards <ref> [How60, Wit77, DY79, Ros83, Wat89, Sut90] </ref> are only applicable for uniform fl &lt; 1.
Reference: [WRS89] <author> T. Ward, P. A. S. Ralson, and K. E. Stoll. </author> <title> Intelligent control of machine and processes. Computers & Ind. </title> <address> Engng., 19(1-4):205-209, </address> <year> 1989. </year>
Reference-contexts: We shall not be concerned with control problems in the thesis except in x10.4.2, where the relations between decision and control theories are further explored. For a survey of adaptive control see <ref> [WRS89] </ref> and the collection [MSW90]. The theories mentioned above are based on the assumption that the "state of world" at any moment can be perceived directly by the agent, and processed in detail. In practice such an assumption may not be granted, and the problem does not belong to MDP.
Reference: [YG90] <author> E. Yair and A. Gersho. </author> <title> The Boltzmann perceptron network: A soft classifier. </title> <booktitle> Neural Networks, </booktitle> <pages> pages 203-231, </pages> <year> 1990. </year>
Reference-contexts: The policy is therefore w := (' w ). This can be alternatively interpreted as having a neural network whose last layer is stochastic, with probability distribution restricted to the n unit vectors e i . See, for example, <ref> [YG90, Lin92] </ref>. Now we study what the auxiliary input e v to the decision network should be. We want w T = ff e v T @T = ff @w CHAPTER 9.
Reference: [Yor92] <author> J. York. </author> <title> Use of the gibbs samplers in expert systems. </title> <journal> Artif. Intell., </journal> <volume> 56 </volume> <pages> 115-130, </pages> <year> 1992. </year> <booktitle> And Addendum, </booktitle> <pages> 397-398. </pages>
Reference-contexts: A continuous version of GS was provided in [Gla63]. Some of the most important properties of GSs have been studied in [GG84], and a survey of their applications was given in <ref> [Yor92] </ref>. The study of NP-complete problems reveals that many combinatorial optimisation problems that are routinely encountered are inherently intractable if exact global opti-misation is attempted [GJ79]. <p> In Section 7.5, the simulation results and more technical details are given. The results are also compared with previous results. 7.2 Simulation of Gibbs Samplers At the heart of simulated annealing, the Boltzmann machine and many other statistical applicationsis a Gibbs sampler (GS) [GG84]. (See <ref> [Yor92] </ref> for a survey of the applications of GS.) Let X be the state space, e 2 [X ! R) and t 2 GS (BD (e; )).
References-found: 173

