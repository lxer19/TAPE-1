URL: http://www.cs.dartmouth.edu/~nicol/papers/lapse.ps
Refering-URL: http://www.cs.dartmouth.edu/~nicol/papers/papers.html
Root-URL: http://www.cs.dartmouth.edu
Phone: 704  
Title: Parallelized Direct Execution Simulation of Message-Passing Parallel Programs  
Author: Phillip M. Dickens Philip Heidelberger David M. Nicol 
Note: Research supported by NASA contract number NAS1-19480, while the authors were in residence at the Institute for Computer Applications in  23681. Copyright owned by IEEE. Appeared in IEEE Trans. on Parallel and Distributed Systems, vol 7, no. 10, October 1996, pp. 1090-1105. This work was performed while this author was on sabbatical at ICASE. This work was performed while this author was on sabbatical at ICASE. It is also supported in part by NSF grant CCR 9201195  
Address: P.O. Box  Hampton, VA 23681 Yorktown Heights, NY 10598 Williamsburg, VA 23185  Hampton, VA  
Affiliation: ICASE IBM T.J. Watson Research Center Department of Computer Science NASA Langley Research Center  The College of William and Mary  Science Engineering (ICASE), NASA Langley Research Center,  
Abstract: As massively parallel computers proliferate, there is growing interest in finding ways by which performance of massively parallel codes can be efficiently predicted. This problem arises in diverse contexts such as parallelizing compilers, parallel performance monitoring, and parallel algorithm development. In this paper we describe one solution where one directly executes the application code, but uses a discrete-event simulator to model details of the presumed parallel machine, such as operating system and communication network behavior. Because this approach is computationally expensive, we are interested in its own parallelization, specifically the parallelization of the discrete-event simulator. We describe methods suitable for parallelized direct execution simulation of message-passing parallel programs, and report on the performance of such a system, LAPSE (Large Application Parallel Simulation Environment), we have built on the Intel Paragon. On all codes measured to date, LAPSE predicts performance well, typically within 10% relative error. Depending on the nature of the application code, we have observed low slowdowns (relative to natively executing code) and high relative speedups using up to 64 processors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Agrawal, M. Choy, H.V. Leong, and A. Singh. Maya: </author> <title> A simulation platform for distributed shared memories. </title> <booktitle> In Proceedings of the 8 th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 151-155, </pages> <month> July </month> <year> 1994. </year>
Reference: [2] <author> J.H. Bramble, J.E. Pasciak, and A.H. Schatz. </author> <title> The construction of preconditioners for elliptic problems by substructuring. </title> <journal> Math. Comp., </journal> <volume> 47 </volume> <pages> 103-134, </pages> <year> 1986. </year>
Reference-contexts: The algorithm is of Bramble-Pasciak-Schatz substructuring type consisting of a conjugate gradient method preconditioned by an approximation to the inverse of the matrix operator obtained from discretizing the PDE <ref> [2] </ref>. The conjugate gradient method applied to a sparse matrix is highly parallelizable, requiring at each iteration only nearest neighbor communication in the formation of matrix-vector products, and a global reduction operation in the formation of inner products. The domain of the PDE is partitioned into nonoverlapping subdomains.
Reference: [3] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl. Proteus: </author> <title> A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1991. </year> <month> 19 </month>
Reference: [4] <author> K.M. Chandy and J. </author> <title> Misra A Case Study in the Design and Verification of Distributed Programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> SE-5,5 May </month> <year> 1979, </year> <pages> 440-452. </pages>
Reference-contexts: There is a rich literature of solutions to this synchronization problem. The seminal articles are <ref> [4] </ref> and [22]. Good introductory surveys are found in [19, 38], and a survey of the state-of-the art is found in [31]. Conservative synchronization protocols prohibit a simulator from executing an event if there is any possibility of an earlier event being scheduled there later.
Reference: [5] <author> D.-K. Chen, H.-M. Su, and P.-C. Yew. </author> <title> The impact of synchronization and granularity on parallel systems. </title> <booktitle> In Int'l. Symp. on Computer Architecture, </booktitle> <pages> pages 239-248, </pages> <month> May </month> <year> 1990. </year>
Reference: [6] <author> J. Bruner, H. Cheong, A. Veidenbaum, and P.-C Yew. </author> <title> Chief: A parallel simulation environment for parallel systems. </title> <booktitle> In Proceedings of the 5 th IPPS, </booktitle> <pages> pages 568-575, </pages> <month> April </month> <year> 1991. </year>
Reference: [7] <author> R. Covington, S. Dwarkadas, J. Jump, S. Madala, and J. Sinclair. </author> <title> Efficient simulation of parallel computer systems. </title> <journal> International Journal on Computer Simulation, </journal> <volume> 1(1) </volume> <pages> 31-58, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: General users may be interested in performance tuning their codes for large numbers of processors (which are only infrequently available) using fewer, more readily available resources. Designers of new communication networks are interested in evaluating their designs under realistic workloads. The method of direct execution simulation <ref> [7, 8, 18, 20, 27] </ref> of application codes offers a solution to each of these problems. <p> While these techniques have proved accurate for the applications we have studied, it is reasonable to question 4 the applicability of LAPSE for other types of applications. First, other simulators (e.g., <ref> [7, 37] </ref>) also use instruction counts as a first order measure of time that is inexpensive to collect. Second, LAPSE can be adapted to allow more sophisticated methods.
Reference: [8] <author> R.C. Covington, S. Madala, V. Mehta, J.R. Jump, and J.B. Sinclair. </author> <title> The Rice Parallel Processing Testbed. </title> <booktitle> In Proceedings of the 1988 SIGMETRICS Conference, </booktitle> <pages> pages 4-11, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: General users may be interested in performance tuning their codes for large numbers of processors (which are only infrequently available) using fewer, more readily available resources. Designers of new communication networks are interested in evaluating their designs under realistic workloads. The method of direct execution simulation <ref> [7, 8, 18, 20, 27] </ref> of application codes offers a solution to each of these problems.
Reference: [9] <author> T.W. Crockett and T. Orloff. </author> <title> A parallel rendering algorithm for MIMD architectures. </title> <type> Technical Report 91-3, </type> <institution> ICASE, NASA Langley Research Center, Hampton Virginia, </institution> <year> 1991. </year>
Reference-contexts: Again, these categorizations are relative. The fourth application, PGL, is a parallel graphics library written to support visualization of scientific data <ref> [9] </ref>. In the sample driver program for this library, each processor generates some number of randomly spaced and colored triangles of a certain size. The processors then rotate and shade the vertices of their triangles. Display scan lines are distributed in an interleaved fashion over the processors.
Reference: [10] <author> W.P. Dawkins, V. Debbad, J.R. Jump, and J.B. Sinclair. </author> <title> Efficient simulation of multiprogramming. </title> <booktitle> In Proceedings of the 1990 SIGMETRICS Conference, </booktitle> <pages> pages 237-238, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: of the execution durations reported by the application processes, an evaluation of message-passing delays, assumed operating system overheads for message-passing and interrupt handling, and a model of how message-passing activity affects the execution of application processes (note that this method for modeling interrupt handling is similar to that described in <ref> [10] </ref>). Indeed, an application simulator in LAPSE maintains for each application process a data structure reflecting Figure 1 (b), called a time-line, that records observed application events and assigns simulation times to them.
Reference: [11] <author> H. Davis, S. Goldschmidt, and J. Hennessy. </author> <title> Multiprocessor simulation and tracing using Tango. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages II99-II107, </pages> <month> August </month> <year> 1991. </year>
Reference: [12] <author> P.M. Dickens, M. Haines, P. Mehotra, and D.M. Nicol. </author> <title> Towards A Thread-Based Parallel Direct Execution Simulator. </title> <booktitle> In Proceedings of the 29 th Hawaii International Conference of System Sciences, to appear. </booktitle>
Reference-contexts: One must either parse and manipulate the source code at compile time, or parse and manipulate the object code at run-time. Either approach could in principle be carried out. We are in fact currently investigating some of these issues <ref> [12] </ref>. In this first version of LAPSE, we simply create a separate OSF-1 Unix process for each virtual processor, and consequently use that operating system's mechanisms for scheduling, and suffer its costs for context switching. Since 1993, a handful of direct-execution simulators have been announced.
Reference: [13] <author> P.M. Dickens, P. Heidelberger, and D.M. Nicol. </author> <title> A distributed memory LAPSE: Parallel simulation of message-passing programs. </title> <booktitle> In Proceedings of the 8th Workshop on Parallel and Distributed Simulation (PADS), </booktitle> <address> Edinburgh, Scotland, </address> <year> 1994. </year> <booktitle> The Society of Computer Simulation, </booktitle> <pages> pp. 32-38. </pages>
Reference-contexts: It is harder to separate simulation overhead from operating system overhead, nor shall we attempt to do so here. However, some measurements are presented in <ref> [13] </ref> indicating that OSF-1 on the Paragon has process management overheads that increase superlinearly as the number of processes per node increase.
Reference: [14] <author> P.M. Dickens, P. Heidelberger, and D.M. Nicol. </author> <title> Parallelized Network Simulators for Message-Passing Parallel Programs. </title> <booktitle> Proceedings of the MASCOTS'95 Conference, </booktitle> <address> Durham, N.C., </address> <month> Jan. </month> <year> 1995, </year> <pages> pp. 18-20. </pages>
Reference-contexts: Maya is similar to LAPSE in the assumed underlying architecture and programming paradigm. However, Maya serializes the network simulation, and its authors report that this is a bottleneck. LAPSE can do this as well, our experience is that such serialization can exact a tremendous performance penalty <ref> [14] </ref>. HASE will use a Time Warp [22] synchronization layer in parallel, but at the time of this writing that aspect was not operational. We can say however that optimistic techniques come with their own overheads, and their own benefits. Optimistic methods require state-saving, to support roll-back. <p> However, we do include an example which induces extreme network contention; in this example the detailed network model is required to accurately represent performance. The cost/benefit of simulating the network to model contention is discussed in more detail in <ref> [14] </ref>. 3 Application/LAPSE Interaction In this section we briefly describe how an application code is transformed into a LAPSE simulation code. LAPSE preprocessing is invoked by modifying an application's makefile (i.e. the input file for the Unix make command). <p> Similar reductions in slowdown are obtained for PGL-low. This type of separation is especially beneficial when using the switch-level network, but can in other cases lead to high communication overheads <ref> [14] </ref>. We next measure (relative) speedups by running LAPSE on the Paragon using N virtual processors on n physical processors where N n. We again ran the applications for a variety of combinations of N and n. <p> It does so at a cost. As we point out in <ref> [14] </ref>, the degree to which the switch-level simulator is slower than the pure-delay simulator depends very much on the fraction of the overall computation that is executing the network simulator rather than executing the application code.
Reference: [15] <author> P.M. Dickens, P. Heidelberger, and D.M. Nicol. </author> <title> Timing Simulation of Paragon Codes Using Workstation Clusters. </title> <booktitle> Proceedings of the 1994 Winter Simulation Conference, </booktitle> <address> Orlando, FL, </address> <pages> pp. 1347-1353. </pages>
Reference: [16] <author> S. Dwarkadas, J.R. Jump, and J.B. Sinclair. </author> <title> Execution-driven simulation of multiprocessors: address and timing analysis. </title> <journal> ACM TOMACS, </journal> <volume> 4(4): </volume> <pages> 314-338, </pages> <month> October, </month> <year> 1994. </year>
Reference-contexts: The application was then run both natively and under LAPSE for a variety of numbers of nodes, and the timings compared. This is an admittedly simple way of approaching calibration. Others have studied the problem in more depth <ref> [16] </ref>, and we see ways of completing automating the translation process using the ideas of [26]. Whatever the best solutions are, LAPSE could adopt them.
Reference: [17] <author> W. Gropp, e. Lusk and A. Skellum. </author> <title> Using MPI. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass, </address> <year> 1994. </year>
Reference-contexts: For a deterministic application, the paths taken by the application running natively and running under LAPSE are identical (assuming the LAPSE timing model is accurate). One might expect that the Paragon's message-ordering guarantee is unusually weak, but the guarantees that the MPI <ref> [17] </ref> package gives are similar. To appreciate the distinction between temporally sensitive and insensitive code, compare the two fragments below.
Reference: [18] <author> R. M. Fujimoto. Simon: </author> <title> A simulator of multicomputer networks. </title> <type> Technical Report UCB/CSD 83/137, </type> <institution> ERL, University of California, Berkeley, </institution> <year> 1983. </year>
Reference-contexts: General users may be interested in performance tuning their codes for large numbers of processors (which are only infrequently available) using fewer, more readily available resources. Designers of new communication networks are interested in evaluating their designs under realistic workloads. The method of direct execution simulation <ref> [7, 8, 18, 20, 27] </ref> of application codes offers a solution to each of these problems.
Reference: [19] <author> R. M. Fujimoto. </author> <title> Parallel discrete event simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: There is a rich literature of solutions to this synchronization problem. The seminal articles are [4] and [22]. Good introductory surveys are found in <ref> [19, 38] </ref>, and a survey of the state-of-the art is found in [31]. Conservative synchronization protocols prohibit a simulator from executing an event if there is any possibility of an earlier event being scheduled there later. Optimistic protocols allow out-of-sequence event processing.
Reference: [20] <author> R. M. Fujimoto and W. B. Campbell. </author> <title> Efficient instruction level simulation of computers. </title> <journal> Transactions of the Society for Computer Simulation, </journal> <volume> 5(2) </volume> <pages> 109-124, </pages> <month> April </month> <year> 1988. </year> <month> 20 </month>
Reference-contexts: General users may be interested in performance tuning their codes for large numbers of processors (which are only infrequently available) using fewer, more readily available resources. Designers of new communication networks are interested in evaluating their designs under realistic workloads. The method of direct execution simulation <ref> [7, 8, 18, 20, 27] </ref> of application codes offers a solution to each of these problems.
Reference: [21] <author> F.W. Howell, R. Williams, and R.N. Ibbett. </author> <title> Hierarchical architecture design and simulation envi-ronment. </title> <booktitle> In MASCOTS '94, Proceedings of the Second International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, </booktitle> <pages> pages 363-370, </pages> <address> Durham, North Carolina, 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: [22] <author> D. R. </author> <title> Jefferson Virtual Time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <month> 7,3 </month> <year> (1985), </year> <pages> 404-425. </pages>
Reference-contexts: However, Maya serializes the network simulation, and its authors report that this is a bottleneck. LAPSE can do this as well, our experience is that such serialization can exact a tremendous performance penalty [14]. HASE will use a Time Warp <ref> [22] </ref> synchronization layer in parallel, but at the time of this writing that aspect was not operational. We can say however that optimistic techniques come with their own overheads, and their own benefits. Optimistic methods require state-saving, to support roll-back. Minimizing its cost remains an active research problem. <p> There is a rich literature of solutions to this synchronization problem. The seminal articles are [4] and <ref> [22] </ref>. Good introductory surveys are found in [19, 38], and a survey of the state-of-the art is found in [31]. Conservative synchronization protocols prohibit a simulator from executing an event if there is any possibility of an earlier event being scheduled there later. Optimistic protocols allow out-of-sequence event processing.
Reference: [23] <author> Intel Corporation. </author> <title> Paragon User's Guide, </title> <month> October </month> <year> 1993. </year> <title> Order Number 312489-002. </title>
Reference-contexts: Table 1 uses these attributes to categorize relevant existing work, and LAPSE. Among most current simulators other than our own, simulation of cache-coherency protocols are an important concern. LAPSE is implemented on the Intel Paragon <ref> [23] </ref>, which does not support virtual shared memory. Coherency protocols complicate the simulation problem considerably, but are a facet LAPSE need not deal with. However, existing work has identified context-switching overhead as a key performance consideration, and it is one that directly affects us.
Reference: [24] <author> D.E. Keyes and W.D. Gropp. </author> <title> A comparison of domain decomposition techniques for elliptic partial differential equations and their parallel implementation. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 8(2):s166-s202, </volume> <month> March </month> <year> 1987. </year>
Reference-contexts: The message passing calls are all synchronous, csend or crecv. Thus the application execution path is timing independent, and thereby has good application lookahead. The second application, BPS, is a domain decomposition solver for finite difference or finite element discretizations of two-dimensional elliptic partial differential equations <ref> [24] </ref>; this type of problem arises frequently in computational fluid dynamics. The algorithm is of Bramble-Pasciak-Schatz substructuring type consisting of a conjugate gradient method preconditioned by an approximation to the inverse of the matrix operator obtained from discretizing the PDE [2].
Reference: [25] <author> S. Lamberts, G. Stellner, A. Bode, and T. Ludwig. </author> <title> Paragon parallel programming environment on Sun workstations. </title> <booktitle> In Sun User Group Proceedings, </booktitle> <pages> pages 87-98. </pages> <publisher> Sun User Group, </publisher> <month> December </month> <year> 1993. </year>
Reference-contexts: There are a number of worthwhile extensions to LAPSE we are pursuing. First, LAPSE currently runs on the Paragon and provides Paragon timing estimates. We have ported LAPSE to work in conjunction with a software package, nx-lib <ref> [25] </ref>, that provides the Paragon message-passing library on networks of workstations. nx-lib provides the workstations with Paragon functionality, our port augments functionality with timing. Second, our model of the Paragon's operating system and hardware is currently fairly crude, and LAPSE is limited to the SPMD model where no multiprogramming occurs.
Reference: [26] <author> A. Lebeck and D. Wood. </author> <title> Active Memory: A New Abstraction for Memory System Simulation. </title> <booktitle> In Proceedings of ACM SIGMETRICS '95 / Performance '95 Conference, </booktitle> <pages> pages 220-230, </pages> <address> Ottawa, CA., </address> <month> May </month> <year> 1995. </year>
Reference-contexts: First, other simulators (e.g., [7, 37]) also use instruction counts as a first order measure of time that is inexpensive to collect. Second, LAPSE can be adapted to allow more sophisticated methods. There is no reason in principle why the object code manipulation that is described in <ref> [26] </ref> could not be used in LAPSE to quickly and accurately distinguish between instruction types and to compute the effects of cache misses. <p> This is an admittedly simple way of approaching calibration. Others have studied the problem in more depth [16], and we see ways of completing automating the translation process using the ideas of <ref> [26] </ref>. Whatever the best solutions are, LAPSE could adopt them. Table 2 presents the percentage differences between LAPSE predictions of execution times and actual native execution times. (Recall that the no-contention network model is used.) As can be seen in the table, the maximum error is 6%.
Reference: [27] <author> I. Mathieson and R. Francis. </author> <title> A dynamic-trace-driven simulator for evaluating parallelism. </title> <booktitle> In Proceedings of the 21 st Hawaii International Conference on System Sciences, </booktitle> <pages> pages 158-166, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: General users may be interested in performance tuning their codes for large numbers of processors (which are only infrequently available) using fewer, more readily available resources. Designers of new communication networks are interested in evaluating their designs under realistic workloads. The method of direct execution simulation <ref> [7, 8, 18, 20, 27] </ref> of application codes offers a solution to each of these problems.
Reference: [28] <author> D.M. Nicol. </author> <title> The cost of conservative synchronization in parallel discrete-event simulations. </title> <journal> Journal of the ACM, </journal> <volume> 40(2) </volume> <pages> 304-333, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Whoa is distinct from YAWNS <ref> [28] </ref> which is also window-based, in that YAWNS windows are so small that the receive times of messages sent from within a window must lie outside the window. To the degree allowed by temporal insensitivity, Whoa relaxes that requirement.
Reference: [29] <author> D.M. Nicol and P. Heidelberger. </author> <title> Parallel simulation of markovian queueing networks using adaptive uniformization. </title> <booktitle> In Proceedings of the 1993 SIGMETRICS Conference, </booktitle> <pages> pages 135-145, </pages> <address> Santa Clara, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This code, which is written in a combination of C and Fortran, was provided to us by David Keyes and Ion Stoica of ICASE and Old Dominion University. The third application, APUCS, is a continuous time Markov Chain discrete event simulator using the parallel algorithm described in <ref> [29] </ref>. The specific system simulated is the queueing network model of a large distributed computing system described in [29]. This application has highly irregular communications patterns involving any-to-any pairwise messages. This application also uses synchronous message passing 13 calls (csend, irecv and msgwaits) as well as global reductions. <p> The third application, APUCS, is a continuous time Markov Chain discrete event simulator using the parallel algorithm described in <ref> [29] </ref>. The specific system simulated is the queueing network model of a large distributed computing system described in [29]. This application has highly irregular communications patterns involving any-to-any pairwise messages. This application also uses synchronous message passing 13 calls (csend, irecv and msgwaits) as well as global reductions. The parameter settings were those of "Set I" of [29] with the "high" computation to communication ratio achieved by setting f <p> queueing network model of a large distributed computing system described in <ref> [29] </ref>. This application has highly irregular communications patterns involving any-to-any pairwise messages. This application also uses synchronous message passing 13 calls (csend, irecv and msgwaits) as well as global reductions. The parameter settings were those of "Set I" of [29] with the "high" computation to communication ratio achieved by setting f = 1 and the "low" computation to communication ratio achieved by setting f = 64. Again, these categorizations are relative. The fourth application, PGL, is a parallel graphics library written to support visualization of scientific data [9].
Reference: [30] <author> D.M. Nicol, C. Micheal, and P. Inouye. </author> <title> Efficient aggregation of multiple LP's in distributed memory parallel simulations. </title> <booktitle> In Proceedings of the 1989 Winter Simulation Conference, </booktitle> <pages> pages 680-685, </pages> <address> Washington, D.C., </address> <month> December </month> <year> 1989. </year>
Reference-contexts: Application object code is altered to cause WWT application processes to synchronize every B cycles. Any communication is deferred until the next barrier with the assurance that the barrier occurs before the communication can have affected its recipient. (This method of synchronization is a special case of the YAWNS <ref> [30, 33] </ref> protocol.) The WWT ignores any network contention by assuming that the latency of every message is fixed, and known. By contrast, Paragon processors are less tightly coupled.
Reference: [31] <author> D. M. Nicol and R. M. Fujimoto. </author> <title> Parallel simulation today. </title> <journal> Annals of Operations Research. </journal> <volume> vol. 53, </volume> <pages> 249-286, </pages> <year> 1994. </year>
Reference-contexts: There is a rich literature of solutions to this synchronization problem. The seminal articles are [4] and [22]. Good introductory surveys are found in [19, 38], and a survey of the state-of-the art is found in <ref> [31] </ref>. Conservative synchronization protocols prohibit a simulator from executing an event if there is any possibility of an earlier event being scheduled there later. Optimistic protocols allow out-of-sequence event processing.
Reference: [32] <author> D.M. Nicol. </author> <title> Parallel discrete-event simulation of FCFS stochastic queueing networks. </title> <booktitle> In Proceedings ACM/SIGPLAN PPEALS 1988: Experiences with Applications, Languages and Systems, </booktitle> <pages> pages 124-137. </pages> <publisher> ACM Press, </publisher> <year> 1988. </year>
Reference-contexts: Synchronization within a window is governed by the dynamic computation and distribution of lower bounds on future times at which one processor may affect another. These lower bounds are called appointments <ref> [32] </ref>.
Reference: [33] <author> D.M. Nicol. </author> <title> The cost of conservative synchronization in parallel discrete-event simulations. </title> <journal> Journal of the ACM, </journal> <volume> 40(2) </volume> <pages> 304-333, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Application object code is altered to cause WWT application processes to synchronize every B cycles. Any communication is deferred until the next barrier with the assurance that the barrier occurs before the communication can have affected its recipient. (This method of synchronization is a special case of the YAWNS <ref> [30, 33] </ref> protocol.) The WWT ignores any network contention by assuming that the latency of every message is fixed, and known. By contrast, Paragon processors are less tightly coupled.
Reference: [34] <author> D.M. Nicol and P. Heidelberger. </author> <title> On extending parallelism to serial simulators. </title> <booktitle> In Proceedings of the 9th Workshop on Parallel and Distributed Simulation (PADS '95), </booktitle> <pages> 60-67, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: Our new simulator, TAPS (Threaded Application Parallel System Simulator) [35] supports the modeling and simulation of arbitrarily complicated interactions between co-resident threads, including contention for memory, IO devices, access to network interface hardware, and so on. It is being built on top of our tool U.P.S. (Utilitarian Parallel Simulator) <ref> [34] </ref> which provides parallel simulation capability to simulation models built using the CSIM [39] modeling language. Acknowledgments We are grateful to Jeff Earickson of Intel Supercomputers for his generous assistance in helping us to better understand the Intel Paragon system.
Reference: [35] <author> D.M. Nicol and J. Liu. </author> <title> Parallelizable Execution-Driven Simulation of Threaded Distributed Memory Parallel Computations. </title> <booktitle> In Proceedings of the MASCOTS'96 Conference, </booktitle> <address> Santa Barbara CA., </address> <month> February </month> <year> 1996, </year> <note> to appear. 21 </note>
Reference-contexts: We are in the process of building a successor to LAPSE that extends it naturally into the multi-threaded domain. Our new simulator, TAPS (Threaded Application Parallel System Simulator) <ref> [35] </ref> supports the modeling and simulation of arbitrarily complicated interactions between co-resident threads, including contention for memory, IO devices, access to network interface hardware, and so on.
Reference: [36] <author> W.H. Press, B.P. Flannery, S.A. Teukolsky, and W.T. Vettering. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The applications arise in physics, computational fluid dynamics, performance modeling of computer and communications systems, and graphics. The first application, SOR, solves Poisson's boundary value equation in two dimensions (see Chapter 17 in <ref> [36] </ref>). This is a partial differential equation (PDE) in two dimensions with a given set of values on a boundary (in our case a rectangle). The PDE is discretized, resulting in a large, sparse system of K 2 linear equations where K 2 is the number of discretized grid points.
Reference: [37] <author> S. Reinhardt, M. Hill, J. Larus, A. Lebeck, J. Lewis, and D. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the 1993 ACM SIGMETRICS Conference, </booktitle> <pages> pages 48-60, </pages> <address> Santa Clara, CA., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: While these techniques have proved accurate for the applications we have studied, it is reasonable to question 4 the applicability of LAPSE for other types of applications. First, other simulators (e.g., <ref> [7, 37] </ref>) also use instruction counts as a first order measure of time that is inexpensive to collect. Second, LAPSE can be adapted to allow more sophisticated methods.
Reference: [38] <author> R. Righter and J.V. Walrand. </author> <title> Distributed simulation of discrete event systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(1) </volume> <pages> 99-113, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: There is a rich literature of solutions to this synchronization problem. The seminal articles are [4] and [22]. Good introductory surveys are found in <ref> [19, 38] </ref>, and a survey of the state-of-the art is found in [31]. Conservative synchronization protocols prohibit a simulator from executing an event if there is any possibility of an earlier event being scheduled there later. Optimistic protocols allow out-of-sequence event processing.

References-found: 38

