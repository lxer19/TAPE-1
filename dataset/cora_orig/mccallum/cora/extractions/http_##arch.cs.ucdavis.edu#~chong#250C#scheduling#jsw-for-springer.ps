URL: http://arch.cs.ucdavis.edu/~chong/250C/scheduling/jsw-for-springer.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C/scheduling/
Root-URL: http://www.cs.ucdavis.edu
Phone: 2  
Title: In Job Scheduling Strategies for Parallel  Demand-based Coscheduling of Parallel Jobs on Multiprogrammed Multiprocessors  
Author: Patrick G. Sobalvarro ? and William E. Weihl ; 
Address: Cambridge, MA 02139 USA  Palo Alto, CA 94301 USA  
Affiliation: 1 Laboratory for Computer Science Massachusetts Institute of Technology  Systems Research Center Digital Equipment Corporation  
Date: 949, 1995  
Note: Processing, Springer-Verlag LNCS Vol.  
Abstract: We present demand-based coscheduling, a new approach to scheduling parallel computations on multiprogrammed multiprocessors. In demand-based coscheduling, rather than making the pessimistic assumption that all the processes constituting a parallel job must be simultaneously scheduled in order to achieve good performance, we use information about which processes are communicating in order to coschedule only these; the result is more opportunities for coscheduling and fewer preemptions than in more traditional coscheduling schemes. We introduce two particular types of demand-based coscheduling. The first is dynamic coscheduling, which was conceived for use on message-passing architectures. We present an analytical model and a simulation of dynamic coscheduling that show that the algorithm can achieve good performance under pessimistic assumptions. The second is predictive cosche-duling, for which we present an algorithm that detects communication by using virtual memory system information on a bus-based shared-memory multiprocessor.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Chaiken, D., Kubiatowicz, J., and Agarwal, A. </author> <title> "LimitLESS Directories: A Scalable Coherence Scheme," </title> <booktitle> in Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <month> April </month> <year> 1991, </year> <pages> pp. 224-234. </pages>
Reference-contexts: Demand-based coscheduling should also work on many distributed-shared-memory multiprocessors. In a cache-coherence scheme such as the software schemes presented by Chaiken et al. in <ref> [1] </ref>, cache line invalidations can be treated in the same fashion as arriving messages. We can do even better on systems with network interface processors, such as FLASH [9] or Typhoon [11].
Reference: 2. <author> Chandra, R., et al. </author> <title> "Scheduling and Page Migration for Multiprocessor Compute Servers," </title> <booktitle> in Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <address> San Jose, California, Oc-tober, </address> <year> 1994, </year> <pages> pp. 12-24. </pages>
Reference-contexts: Timesharing Results in Poor Performance Crovella et al. have presented results in [3] that show that independent timesharing without regard for synchronization produced significantly greater slowdowns than coscheduling, in some cases a factor of two worse in total runtime of applications. 3 Chandra et al. have reported similar results in <ref> [2] </ref>: in some cases independent timesharing is as much as 40% slower than coscheduling. In [6], Feitelson and Rudolph compared the performance of gang scheduling using busy-waiting synchronization to that of independent (uncoordinated) timesharing using blocking synchronization. <p> Gupta et al. report in [7] that when cosche-duling was used with 25-millisecond timeslices on a simulated system, it achieved 71% utilization, as compared to 74% for batch scheduling (poorer performance is reported with 10-millisecond timeslices). Chandra et al. conclude in <ref> [2] </ref> that co-scheduling and process control achieve similar speedups running on the Stanford DASH distributed-shared-memory multiprocessor as compared to independent timesharing. However, traditional coscheduling suffers from two problems. <p> Several published works <ref> [2, 7, 13] </ref> cite good performance for process control, but these works also find that coscheduling can be modified to have equivalently good performance. <p> In [7], the LU application is found to perform very poorly under process control when run on three processors, and the authors point out that a drastically increased cache miss rate is to blame. Similarly, in <ref> [2] </ref>, the Ocean application suffers a twofold decrease in efficiency when run on eight processors as compared to when it is run on sixteen processors.
Reference: 3. <author> Crovella, M., et al. </author> <title> "Multiprogramming on Multiprocessors," </title> <booktitle> in Third IEEE Symposium on Parallel and Distributed Processing, </booktitle> <year> 1991, </year> <pages> pp. 590-597. </pages>
Reference-contexts: processes will communicate with other processes | under a protocol like Microsoft's Object Linking and Embedding (OLE), for example, an editor may communicate with a spreadsheet or a database depending on which document has been loaded. 2.2 Independent Timesharing Results in Poor Performance Crovella et al. have presented results in <ref> [3] </ref> that show that independent timesharing without regard for synchronization produced significantly greater slowdowns than coscheduling, in some cases a factor of two worse in total runtime of applications. 3 Chandra et al. have reported similar results in [2]: in some cases independent timesharing is as much as 40% slower than <p> The result is that even in the two-job case examined by Crovella et al. in <ref> [3] </ref>, when approximately 25% of the cycles in the multiprocessor were devoted to running alternates, their use decreased the runtime of the application to which they were devoted only about 1%. 2.4 Distributed Hierarchical Control Distributed hierarchical control was presented by Feitelson and Rudolph in [5].
Reference: 4. <author> Feitelson, D. G., and Rudolph, L. </author> <title> "Coscheduling Based on Run-Time Identification of Activity Working Sets," </title> <journal> in International Journal of Parallel Programming, </journal> <volume> Vol. 23, No. 2, </volume> <pages> pp. 135-160, </pages> <month> April, </month> <year> 1995. </year>
Reference-contexts: If process control as it is described in [14] were used as the only means of timesharing a multiprocessor, we would expect that such applications would show poor performance when the job load was high. 2.6 Runtime Activity Working Set Identification Recently, Feitelson and Rudolph have described in <ref> [4] </ref> an algorithm called "run-time activity working set identification" for scheduling parallel programs on a timeshared multiprocessor (we shall call this algorithm RAWSI, for brevity's sake).
Reference: 5. <author> Feitelson, D. G., and Rudolph, L. </author> <title> "Distributed Hierarchical Control for Parallel Processing," </title> <journal> in IEEE Computer, </journal> <volume> Vol. 25, No. 3, </volume> <pages> pp. 65-77, </pages> <month> May, </month> <year> 1990. </year>
Reference-contexts: by Crovella et al. in [3], when approximately 25% of the cycles in the multiprocessor were devoted to running alternates, their use decreased the runtime of the application to which they were devoted only about 1%. 2.4 Distributed Hierarchical Control Distributed hierarchical control was presented by Feitelson and Rudolph in <ref> [5] </ref>. The algorithm logically structures the multiprocessor as a binary tree in which the processing nodes are at the leaves and all the children of a tree node are considered a partition.
Reference: 6. <author> Feitelson, D. G., and Rudolph, L. </author> <title> "Gang Scheduling Performance Benefits for Fine-Grain Synchronization," </title> <journal> in Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 16, No. 4, </volume> <pages> pp. 306-318, </pages> <month> December, </month> <year> 1992. </year>
Reference-contexts: In <ref> [6] </ref>, Feitelson and Rudolph compared the performance of gang scheduling using busy-waiting synchronization to that of independent (uncoordinated) timesharing using blocking synchronization. They found that for applications with fine-grained synchronization, performance could degrade severely under uncoordinated timesharing as compared to gang scheduling.
Reference: 7. <author> Gupta, A., Tucker, A., and Urushibara, S. </author> <title> "The Impact of Operating System Scheduling Policies and Synchronization Methods on the Performance of Parallel Applications," </title> <booktitle> in Proceedings of SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May, </month> <year> 1991, </year> <pages> pp. 120-132. </pages>
Reference-contexts: The extra context switches result from attempts to synchronize with descheduled processes resulting in blocking. As Gupta et al. have shown in <ref> [7] </ref>, the use of non-blocking (spinning) synchronization primitives will result in even worse performance under moderate multiprogrammed loads, because, while the extra context switches are avoided, the spinning time is large. <p> Relatively good performance has been reported for competent implementations of traditional coscheduling. Gupta et al. report in <ref> [7] </ref> that when cosche-duling was used with 25-millisecond timeslices on a simulated system, it achieved 71% utilization, as compared to 74% for batch scheduling (poorer performance is reported with 10-millisecond timeslices). <p> Several published works <ref> [2, 7, 13] </ref> cite good performance for process control, but these works also find that coscheduling can be modified to have equivalently good performance. <p> This implies that in fact the jobs in question show superlinear speedup. In fact this is true in two examples in published works on process control. In <ref> [7] </ref>, the LU application is found to perform very poorly under process control when run on three processors, and the authors point out that a drastically increased cache miss rate is to blame.
Reference: 8. <author> Helmbold, D. P., and McDowell, C. E. </author> <title> "Modeling Speedup(n) Greater than n," </title> <journal> in IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 1, No. 2, </volume> <month> April </month> <year> 1990, </year> <pages> pp. 250-256. </pages>
Reference-contexts: Helmbold and McDowell have documented this sort of "superunitary speedup due to increasing cache size" in <ref> [8] </ref>. Because of this property of certain parallel applications, their ideal "operating point" is larger than one | possibly considerably larger than one. Thus forcing them to run on fewer processors will be very inefficient.
Reference: 9. <author> Kuskin, J. et al. </author> <title> "The Stanford FLASH Multiprocessor," </title> <booktitle> in Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <address> Chicago, Illinois, </address> <month> April, </month> <year> 1994. </year>
Reference-contexts: In a cache-coherence scheme such as the software schemes presented by Chaiken et al. in [1], cache line invalidations can be treated in the same fashion as arriving messages. We can do even better on systems with network interface processors, such as FLASH <ref> [9] </ref> or Typhoon [11]. In these systems, some of the scheduler state can be cached in the interface processor, so that the scheduling decision can be made without consulting the computation processor. The computation processor could be interrupted only when a preemption was needed.
Reference: 10. <author> Ousterhout, John K. </author> <title> "Scheduling Techniques for Concurrent Systems," </title> <booktitle> in Third International Conference on Distributed Computing Systems, </booktitle> <month> October, </month> <year> 1982, </year> <pages> pp. 22-30. </pages>
Reference-contexts: This work was supported in part by the Advanced Research Projects Agency under Contract N00014-91-J-1698, by grants from IBM and AT&T, and by an equipment grant from DEC. Because demand-based coscheduling uses more information than does Ouster--hout's form of coscheduling <ref> [10] </ref>, it can reduce the difficulty of the scheduling problem and exploit opportunities for coscheduling that traditional coscheduling cannot. Because it does not rely on a particular programming technique, such as task-queue based multithreading, demand-based coscheduling is applicable in domains where process control [14] is not. <p> In the sections ahead, we will describe algorithms for demand-based coscheduling on such platforms. 1.1 Goals Ousterhout compared parallel scheduling and virtual memory systems in <ref> [10] </ref>. He suggested that coscheduling is necessary on timeshared multiprocessors running parallel jobs in order to avoid a kind of process thrashing that is analogous to virtual memory thrashing. <p> This is as distinct from traditional coscheduling <ref> [10] </ref>, in which there is no clear means for scheduling jobs with more processes than there are nodes on the multiprocessor. Finally, we want an approach that is dynamic, and can adapt to changing conditions of load and communication between processes. <p> In an example where processes synchronized about every 160sec on a NUMA multiprocessor with 4-MIPS processing nodes, applications took roughly twice as long to execute under uncoordinated scheduling as they did under gang scheduling. In general, the results cited above agree with the claims advanced by Ouster-hout in <ref> [10] </ref>: under independent timesharing, multiprogrammed parallel job loads will suffer large numbers of context switches, with attendant overhead due to 3 Crovella et al. found that hardware partitions gave the best performance in their experiments, but, as we have discussed above, these are not feasible when one has a large number <p> Ousterhout's solution was coscheduling, described in <ref> [10] </ref>. Under this traditional form of coscheduling, the processes constituting a parallel job are scheduled simultaneously across as many of the nodes of a multiprocessor as they require.
Reference: 11. <author> Reinhardt, S. K., Larus, J. R., and Wood, D. A. "Tempest and Typhoon: </author> <title> User-level Shared Memory," </title> <booktitle> in Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <address> Chicago, Illinois, </address> <month> April, </month> <year> 1994. </year>
Reference-contexts: In a cache-coherence scheme such as the software schemes presented by Chaiken et al. in [1], cache line invalidations can be treated in the same fashion as arriving messages. We can do even better on systems with network interface processors, such as FLASH [9] or Typhoon <ref> [11] </ref>. In these systems, some of the scheduler state can be cached in the interface processor, so that the scheduling decision can be made without consulting the computation processor. The computation processor could be interrupted only when a preemption was needed.
Reference: 12. <author> Sobalvarro, P. G. </author> <title> "Adaptive Gang-Scheduling for Distributed-Memory Multiprocessors," </title> <booktitle> in Proceedings of the 1994 MIT Student Workshop on Scalable Computing, </booktitle> <institution> MIT Laboratory for Computer Science Technical Report No. </institution> <month> 622, July, </month> <year> 1994. </year>
Reference-contexts: 1 Introduction This paper describes demand-based coscheduling, a new approach to scheduling parallel computations on multiprogrammed multiprocessors, which generalizes earlier work presented in <ref> [12] </ref>. Under demand-based coscheduling, processes are scheduled simultaneously only if they communicate; communication is treated as a demand for synchronization. Processes that do not communicate need not be coscheduled. <p> While demand-based coscheduling was developed independently from RAWSI, the two have significant similarities: in both approaches, runtime mechanisms are used to identify communicating processes so that they can be coscheduled. However, RAWSI differs significantly from both dynamic coscheduling (described in Section 4 and <ref> [12] </ref>) and predictive coscheduling (described in Section 5), the two approaches to demand-based coscheduling we present here. <p> We present two methods for doing demand-based coscheduling, although there might of course be many more. These two methods are dynamic cosche-duling and predictive coscheduling. Dynamic coscheduling was called adaptive gang scheduling in an earlier work <ref> [12] </ref>, and is an approach suited for use on message-passing multiprocessors or on distributed shared-memory multiprocessors in which cache-line-invalidation events can interrupt the processor.
Reference: 13. <author> Tucker, A. </author> <title> Efficient Scheduling on Multiprogrammed Shared-Memory Multiprocessors. </title> <institution> Stanford University Department of Computer Science Technical Report CSL-TR-94-601, </institution> <month> November, </month> <year> 1993. </year>
Reference-contexts: Several published works <ref> [2, 7, 13] </ref> cite good performance for process control, but these works also find that coscheduling can be modified to have equivalently good performance.
Reference: 14. <author> Tucker, A. and Gupta, A. </author> <title> "Process Control and Scheduling Issues for Multipro-grammed Shared-Memory Multiprocessors," </title> <booktitle> in Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <year> 1989, </year> <pages> pp. </pages> <month> 159-186. </month> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Because it does not rely on a particular programming technique, such as task-queue based multithreading, demand-based coscheduling is applicable in domains where process control <ref> [14] </ref> is not. Demand-based coscheduling is intended for scheduling mixed loads of parallel and serial jobs, where the parallel jobs may be synchronous message-passing applications, sets of processes communicating using shared memory, or clients and servers communicating through kernel-mediated remote procedure calls. <p> For example, while demand-based coscheduling could be compatible with a task-queue-based multithreaded approach like process control <ref> [14] </ref>, we do not want to require that all parallel applications be coded in a multithreaded fashion in order not to suffer excessive context-switching. <p> However, distributed hierarchical control was not designed for smaller machines, such as the desktop machines and departmental servers we have described, on which we expect that it would suffer from the same problems as traditional coscheduling. 2.5 Process Control Tucker and Gupta suggested in <ref> [14] </ref> a strategy called process control , which has some of the characteristics of space partitioning and some of the characteristics of timesharing. Under process control, parallel jobs must be written as multithreaded applications keeping their threads in a task queue. <p> This pushing at the boundaries of available memory will probably mean that many commercial applications will show superlinear speedup. If process control as it is described in <ref> [14] </ref> were used as the only means of timesharing a multiprocessor, we would expect that such applications would show poor performance when the job load was high. 2.6 Runtime Activity Working Set Identification Recently, Feitelson and Rudolph have described in [4] an algorithm called "run-time activity working set identification" for scheduling
References-found: 14

