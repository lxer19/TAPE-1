URL: http://www.cs.umn.edu/Users/dept/users/kumar/sparse-cholesky-2.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: anshul@watson.ibm.com -karypis/kumar-@cs.umn.edu  
Phone: 218  
Title: Highly Scalable Parallel Algorithms for Sparse Matrix Factorization  
Author: Anshul Gupta George Karypis and Vipin Kumar 
Address: P.O. Box  Yorktown Heights, NY 10598 Minneapolis, MN 55455  
Affiliation: IBM T. J. Watson Research Center Department of Computer Science  University of Minnesota  
Abstract: In this paper, we describe scalable parallel algorithms for sparse matrix factorization, analyze their performance and scalability, and present experimental results for up to 1024 processors on a Cray T3D parallel computer. Through our analysis and experimental results, we demonstrate that our algorithms substantially improve the state of the art in parallel direct solution of sparse linear systemsboth in terms of scalability and overall performance. It is a well known fact that dense matrix factorization scales well and can be implemented efficiently on parallel computers. In this paper, we present the first algorithms to factor a wide class of sparse matrices (including those arising from two- and three-dimensional finite element problems) that are asymptotically as scalable as dense matrix factorization algorithms on a variety of parallel architectures. Our algorithms incur less communication overhead and are more scalable than any previously known parallel formulation of sparse matrix factorization. Although, in this paper, we discuss Cholesky factorization of symmetric positive definite matrices, the algorithms can be adapted for solving sparse linear least squares problems and for Gaussian elimination of diagonally dominant matrices that are almost symmetric in structure. An implementation of one of our sparse Cholesky factorization algorithms delivers up to 20 GFlops on a Cray T3D for medium-size structural engineering and linear programming problems. To the best of our knowledge, this is the highest performance ever obtained for sparse Cholesky factorization on any supercomputer. fl This work was supported by IST/BMDO through Army Research Office contract DA/DAAH04-93-G-0080, NSF grant NSG/1RI-9216941, and by Army High Performance Computing Research Center under the auspices of the Department of the Army, Army Research Laboratory cooperative agreement number DAAH04-95-2-0003/contract number DAAH04-95-C-0008, the content of which does not necessarily reflect the position or the policy of the government, and no official endorsement should be inferred. Access to computing facilities were provided by Minnesota Supercomputer Institute, Cray Research Inc. and by the Pittsburgh Supercomputing Center. Related papers are available via WWW at URL: http://www.cs.umn.edu/users/kumar/papers.html. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cleve Ashcraft. </author> <title> The domain/segment partition for the factorization of sparse symmetric positive definite matrices. </title> <type> Technical Report ECA-TR-148, </type> <institution> Boeing Computer Services, </institution> <address> Seattle, WA, </address> <year> 1990. </year> <month> 27 </month>
Reference: [2] <author> Cleve Ashcraft. </author> <title> The fan-both family of column-based distributed cholesky factorization algorithms. </title> <editor> In A. George, John R. Gilbert, and J. W.-H. Liu, editors, </editor> <title> Graph Theory and Sparse Matrix Computations. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: Since the overall computation is only O.N 1:5 / [13], the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased [59, 57]. In <ref> [2] </ref>, Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of 2.N p p log N /.
Reference: [3] <author> Cleve Ashcraft, S. C. Eisenstat, J. W.-H. Liu, and A. H. Sherman. </author> <title> A comparison of three column based distributed sparse factorization schemes. </title> <type> Technical Report YALEU/DCS/RR-810, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1990. </year> <booktitle> Also appears in Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1991. </year>
Reference-contexts: However, the broadcast takes log p steps of O.m/ time each; hence, the total communication overhead is O.m p log p/ (on a hypercube). In the context of matrix factorization, the experimental study by Ashcraft et al. <ref> [3] </ref> serves to demonstrate the importance of studying the total communication overhead rather than volume. In [3], the fan-in algorithm, which has a lower communication volume than the distributed multifrontal algorithm, has a higher overhead (and hence, a lower efficiency) than the multifrontal algorithm for the same distribution of the matrix <p> In the context of matrix factorization, the experimental study by Ashcraft et al. <ref> [3] </ref> serves to demonstrate the importance of studying the total communication overhead rather than volume. In [3], the fan-in algorithm, which has a lower communication volume than the distributed multifrontal algorithm, has a higher overhead (and hence, a lower efficiency) than the multifrontal algorithm for the same distribution of the matrix among the processors.
Reference: [4] <author> J. M. Conroy. </author> <title> Parallel nested dissection. </title> <journal> Parallel Computing, </journal> <volume> 16 </volume> <pages> 139-156, </pages> <year> 1990. </year>
Reference-contexts: Furthermore, as we show in Section 5, this reduction in communication overhead by a factor of O.log p/ results in an improvement in the scalability of the algorithm by a factor of O..log p/ 3 /; i.e., the rate at which 2 <ref> [4] </ref> and [63] could be possible exceptions, but neither a detailed communication analysis, nor any experimental results are available. 4 the problem size must increase with the number of processors to maintain a constant efficiency is lower by a factor of O..log p/ 3 /.
Reference: [5] <author> J. M. Conroy, S. G. Kratzer, and R. F. Lucas. </author> <title> Multifrontal sparse solvers in message passing and data parallel environments a comparitive study. </title> <booktitle> In Proceedings of PARCO, </booktitle> <year> 1993. </year>
Reference: [6] <author> Iain S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Users' guide for the Harwell-Boeing sparse matrix collection (release I). </title> <type> Technical Report TR/PA/92/86, </type> <institution> Research and Technology Division, Boeing Computer Services, </institution> <address> Seattle, WA, </address> <year> 1992. </year>
Reference-contexts: We have been able to achieve speedups of up to 364 on 1024 processors and 230 on 512 processors over a highly efficient sequential implementation for moderately sized problems from the Harwell-Boeing collection <ref> [6] </ref>. In [30], we have applied this algorithm to obtain a highly scalable parallel formulation of interior point algorithms and have observed significant speedups in solving linear programming problems. <p> The detailed experimental performance and scalability results of this implementation have been presented in [25]. Table 1 shows the results of factoring some matrices from the Harwell-Boeing collection of sparse matrices <ref> [6] </ref>. These results show that our algorithm can deliver good speedups on hundreds of processors for practical problems. Spectral nested dissection [50, 51, 52] was used to order these matrices. The algorithm presented in Section 3 relies on the ordering algorithm to yield a balanced elimination tree.
Reference: [7] <author> Iain S. Duff and J. K. Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 9 </volume> <pages> 302-325, </pages> <year> 1983. </year>
Reference-contexts: Section 7 contains the preliminary experimental results on a Cray T3D parallel computer. Section 8 contains concluding remarks. 2 The Multifrontal Algorithm for Sparse Matrix Factorization The multifrontal algorithm for sparse matrix factorization was proposed independently, and in somewhat different forms, by Speelpening [60] and Duff and Reid <ref> [7] </ref>, and later elucidated in a tutorial by Liu [40]. In this section, we briefly describe a condensed version of multifrontal sparse Cholesky factorization. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as shown in Figure 2.
Reference: [8] <author> K. A. Gallivan, R. J. Plemmons, and A. H. Sameh. </author> <title> Parallel algorithms for dense linear algebra computations. </title> <journal> SIAM Review, </journal> <volume> 32(1) </volume> <pages> 54-135, </pages> <month> March </month> <year> 1990. </year> <note> Also appears in K. </note> <author> A. Gallivan et al. </author> <title> Parallel Algorithms for Matrix Computations. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: We have developed highly scalable formulations of sparse Cholesky factorization that substantially improve the state of the art in parallel direct solution of sparse linear systemsboth in terms of scalability and overall performance. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [8, 47, 10, 35] </ref>. We show that the parallel Cholesky factorization algorithms described here are as scalable as the best parallel formulation of dense matrix factorization on both mesh and hypercube architectures for a wide class of sparse matrices, including those arising in two- and three-dimensional finite element problems.
Reference: [9] <author> G. A. Geist and E. G.-Y. Ng. </author> <title> Task scheduling for parallel sparse Cholesky factorization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18(4) </volume> <pages> 291-314, </pages> <year> 1989. </year>
Reference: [10] <author> G. A. Geist and C. H. Romine. </author> <title> LU factorization algorithms on distributed-memory multiprocessor architectures. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 639-649, </pages> <year> 1988. </year> <note> Also available as Technical Report ORNL/TM-10383, </note> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <year> 1987. </year>
Reference-contexts: We have developed highly scalable formulations of sparse Cholesky factorization that substantially improve the state of the art in parallel direct solution of sparse linear systemsboth in terms of scalability and overall performance. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [8, 47, 10, 35] </ref>. We show that the parallel Cholesky factorization algorithms described here are as scalable as the best parallel formulation of dense matrix factorization on both mesh and hypercube architectures for a wide class of sparse matrices, including those arising in two- and three-dimensional finite element problems.
Reference: [11] <author> A. George. </author> <title> Nested dissection of a regular finite-element mesh. </title> <journal> SIAM Journal on Numerical Ananlysis, </journal> <volume> 10 </volume> <pages> 345-363, </pages> <year> 1973. </year>
Reference-contexts: This method of analyzing the communication complexity of sparse Cholesky factorization has been used in [15] in the context of a column-based subtree-to-subcube scheme. Within very small constant factors, the analysis holds for the standard nested dissection <ref> [11] </ref> of grid graphs. We consider a cross-shaped separator (described in [15]) consisting of 2 p N 1 nodes that partitions the N -node square grid into four square subgrids of size . N 1/=2 fi . N 1/=2.
Reference: [12] <author> A. George, M. T. Heath, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Sparse Cholesky factorization on a local memory multiprocessor. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 327-340, </pages> <year> 1988. </year>
Reference-contexts: A simple fan-out algorithm <ref> [12] </ref> with column-wise partitioning of an N fi N matrix of this type on p processors results in an O.N p log N / total communication volume [15] (box A).
Reference: [13] <author> A. George and J. W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: Box D represents our algorithm, which is a significant improvement over other known classes of algorithms for this problem. 3 have a lower bound of O.N p/ on the total communication volume [15]. Since the overall computation is only O.N 1:5 / <ref> [13] </ref>, the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased [59, 57]. <p> This is because the properties of separators can be generalized from grids to all such graphs within the same order of magnitude bounds <ref> [39, 38, 13] </ref>. We derive these expressions for both hypercube and mesh architectures, and also extend the results to sparse matrices resulting from three-dimensional graphs whose n-node subgraphs have O.n 2=3 /-node separators. <p> the problem size needs to grow as fast as f E . p/ to maintain an efficiency E , then f E . p/ is defined as the isoefficiency function of the parallel algorithm-architecture combination for efficiency E . 5.2 Scalability of the parallel multifrontal algorithm It is well known <ref> [13] </ref> that the total work involved in factoring the adjacency matrix of an N -node graph with an O. N /-node separator using nested dissection ordering of nodes is O.N 1:5 /. We have shown in Section 4 that the overall communication overhead of our scheme is O.N p p/. <p> Thus, the complexity of the dense portion of factorization for these two types of matrices is 2.N 1:5 / and 2.N 2 /, respectively, which is of the same order as the computation required to factor the entire sparse matrix <ref> [13, 15] </ref>. Therefore, the isoefficiency function of sparse factorization of such matrices is bounded from below by the isoefficiency function of dense matrix factorization, which is 2. p 1:5 /. <p> As the overall problem size increases, so does the overall memory requirement. For an N -node two-dimensional constant node-degree graphs, the size of the lower triangular factor L is 2.N log N / <ref> [13] </ref>. For a fixed efficiency, W D N 1:5 / p 1:5 , which implies N / p and N log N / p log p. <p> In the three-dimensional case, size of the lower triangular factor L is 2.N 4=3 / <ref> [13] </ref>. For a fixed efficiency, W D N 2 / p 1:5 , which implies N / p 3=4 and N 4=3 / p.
Reference: [14] <author> A. George, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Communication reduction in parallel sparse Cholesky factorization on a hypercube. </title> <editor> In M. T. Heath, editor, </editor> <booktitle> Hypercube Multiprocessors 1987, </booktitle> <pages> pages 576-586. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1987. </year>
Reference-contexts: The communication volume of the column-based schemes represented in box A has been improved using smarter ways of mapping the matrix columns onto processors, such as, the subtree-to-subcube mapping <ref> [14] </ref> (box B). A number of column-based parallel factorization algorithms [41, 42, 3, 53, 54, 61, 12, 9, 29, 27, 59, 44, 5] 1 In [48], Pan and Reif describe a parallel sparse matrix factorization algorithm for a PRAM type architecture.
Reference: [15] <author> A. George, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Communication results for parallel sparse Cholesky factorization on a hypercube. </title> <journal> Parallel Computing, </journal> <volume> 10(3) </volume> <pages> 287-298, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: A simple fan-out algorithm [12] with column-wise partitioning of an N fi N matrix of this type on p processors results in an O.N p log N / total communication volume <ref> [15] </ref> (box A). The communication volume of the column-based schemes represented in box A has been improved using smarter ways of mapping the matrix columns onto processors, such as, the subtree-to-subcube mapping [14] (box B). <p> Box D represents our algorithm, which is a significant improvement over other known classes of algorithms for this problem. 3 have a lower bound of O.N p/ on the total communication volume <ref> [15] </ref>. Since the overall computation is only O.N 1:5 / [13], the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased [59, 57]. <p> In order to simplify the analysis, we assume a somewhat different form of nested-dissection than the one used in the actual implementation. This method of analyzing the communication complexity of sparse Cholesky factorization has been used in <ref> [15] </ref> in the context of a column-based subtree-to-subcube scheme. Within very small constant factors, the analysis holds for the standard nested dissection [11] of grid graphs. We consider a cross-shaped separator (described in [15]) consisting of 2 p N 1 nodes that partitions the N -node square grid into four square <p> This method of analyzing the communication complexity of sparse Cholesky factorization has been used in <ref> [15] </ref> in the context of a column-based subtree-to-subcube scheme. Within very small constant factors, the analysis holds for the standard nested dissection [11] of grid graphs. We consider a cross-shaped separator (described in [15]) consisting of 2 p N 1 nodes that partitions the N -node square grid into four square subgrids of size . N 1/=2 fi . N 1/=2. We call this the level-0 separator that partitions the original grid (or the level-0 grid) into four level-1 grids. <p> the size of level-l subgrids is approximately p p N =2 l , (2) the number of nodes in a level-l separator is approximately 2 p and hence, the length of a supernode at level l of the supernodal elimination tree is approximately 2 p It has been proved in <ref> [15] </ref> that the number of nonzeros that an i fi i subgrid can contribute to the nodes of its bordering separators is bounded by ki 2 , where k D 341=12. Hence, a level-l subgrid can contribute at most k N =4 l nonzeros to its bordering nodes. <p> The number of nonzeros that an i fi i fi i subgrid contributes to the nodes of its bordering separators is O.i 4 / <ref> [15] </ref>. At level l, due to l bisections, i is no more than N 1=3 =2 l . As a result, an update or a frontal matrix at level l of the supernodal elimination tree will contain O.N 4=3 =2 4l / entries distributed among p=8 l processors. <p> The problem size in the case of an N fi N sparse matrix resulting from a 18 three-dimensional grid is O.N 2 / <ref> [15] </ref>. We have shown in Section 4.3 that the overall communication overhead in this case is O.N 4=3 p p/. To maintain a fixed efficiency, N 2 / N 4=3 p p, or N 2=3 / p, or N 2 D W / p 1=5 . <p> Thus, the complexity of the dense portion of factorization for these two types of matrices is 2.N 1:5 / and 2.N 2 /, respectively, which is of the same order as the computation required to factor the entire sparse matrix <ref> [13, 15] </ref>. Therefore, the isoefficiency function of sparse factorization of such matrices is bounded from below by the isoefficiency function of dense matrix factorization, which is 2. p 1:5 /.
Reference: [16] <author> John R. Gilbert and Robert Schreiber. </author> <title> Highly parallel sparse Cholesky factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 13 </volume> <pages> 1151-1172, </pages> <year> 1992. </year>
Reference: [17] <author> Ananth Grama, Anshul Gupta, and Vipin Kumar. Isoefficiency: </author> <title> Measuring the scalability of parallel algorithms and architectures. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 1(3) </volume> <pages> 12-21, </pages> <month> August, </month> <year> 1993. </year> <note> Also available as Technical Report TR 93-24, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: Section 2 describes the serial multifrontal algorithm for sparse matrix factorization. Section 3 describes our parallel algorithm based on multifrontal elimination. In Section 4, we derive expressions for the communication overhead of the parallel algorithm. In Section 5, we use the isoefficiency analysis <ref> [35, 37, 17] </ref> to determine the scalability of our algorithm and compare it with the scalability of other parallel algorithms for sparse matrix factorization. In Section 6 we present a variation of the algorithm described in Section 3 that reduces the overhead due to load imbalance. <p> In this section we use the isoefficiency metric <ref> [35, 37, 17] </ref> to characterize the scalability of our algorithm. <p> We describe an implementation on Cray T3D that yields up to 20 GFlops on medium-size problems. We use the isoefficiency metric <ref> [35, 37, 17] </ref> to characterize the scalability of our algorithms. We show that the isoefficiency function of our algorithms is O. p 1:5 / on hypercube and mesh architectures for sparse matrices arising out of both two- and three-dimensional problems.
Reference: [18] <author> Anoop Gupta and Edward Rothberg. </author> <title> An efficient block-oriented approach to parallel sparse Cholesky factorization. </title> <booktitle> In Supercomputing '93 Proceedings, </booktitle> <year> 1993. </year>
Reference-contexts: In other words, the problem size must be increased as O. p 1:5 / to maintain a constant efficiency as p is increased. In comparison, a lower bound on the isoefficiency function of Rothberg and Gupta's scheme <ref> [57, 18] </ref> with a communication overhead of at least O.N p p log p/ is O. p 1:5 .log p/ 3 /. The isoefficiency function of any column-based scheme is at least O. p 3 / because the total communication overhead has a lower bound of O.N p/.
Reference: [19] <author> Anshul Gupta. </author> <title> Analysis and Design of Scalable Parallel Algorithms for Scientific Computing. </title> <type> PhD thesis, </type> <institution> University of Minnesota, Minneapolis, MN, </institution> <year> 1995. </year> <month> 28 </month>
Reference-contexts: This algorithm, while performing the ordering in parallel, also distributes the data among the processors in way that the remaining steps can be carried out with minimum data-movement. At the end of the parallel ordering step, the parallel symbolic factorization algorithm described in <ref> [19] </ref> can proceed without any redistribution. In [19, 26], we present efficient parallel algorithms for solving the upper and lower triangular systems. The experimental results in [19, 26] show that the data mapping scheme described in Section 3 works well for triangular solutions. <p> At the end of the parallel ordering step, the parallel symbolic factorization algorithm described in [19] can proceed without any redistribution. In <ref> [19, 26] </ref>, we present efficient parallel algorithms for solving the upper and lower triangular systems. The experimental results in [19, 26] show that the data mapping scheme described in Section 3 works well for triangular solutions. <p> At the end of the parallel ordering step, the parallel symbolic factorization algorithm described in [19] can proceed without any redistribution. In <ref> [19, 26] </ref>, we present efficient parallel algorithms for solving the upper and lower triangular systems. The experimental results in [19, 26] show that the data mapping scheme described in Section 3 works well for triangular solutions. We hope that the work presented in this paper, along with [19, 26, 33] will enable the development of efficient practical parallel solvers for a broad range of scientific computing problems. <p> In [19, 26], we present efficient parallel algorithms for solving the upper and lower triangular systems. The experimental results in [19, 26] show that the data mapping scheme described in Section 3 works well for triangular solutions. We hope that the work presented in this paper, along with <ref> [19, 26, 33] </ref> will enable the development of efficient practical parallel solvers for a broad range of scientific computing problems.
Reference: [20] <author> Anshul Gupta. </author> <title> Fast and effective algorithms for graph partitioning and sparse matrix reordering. </title> <note> Technical Report (Number to be assigned), </note> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <year> 1996. </year>
Reference-contexts: NUG15 is from a linear program-ming problem derived from a quadratic assignment problem obtained from AT&T. In all of our experiments, we used spectral nested dissection [50] to order the matrices. The factorization algorithms described in this paper will work well with any type of nested dissection. In <ref> [21, 22, 20, 32, 31] </ref>, we show that nested dissection orderings with proper selection of separators can yield better quality orderings that traditional heuristics, such as, the multiple minimum degree heuristic. The performance obtained by this algorithm in some of these matrices is shown in Table 2.
Reference: [21] <author> Anshul Gupta. </author> <title> Graph partitioning based sparse matrix ordering algorithms for interior-point methods. </title> <type> Technical Report RC 20467 (90480), </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> May 21, </month> <year> 1996. </year> <note> Submitted for publication in SIAM Journal on Optimization. </note>
Reference-contexts: NUG15 is from a linear program-ming problem derived from a quadratic assignment problem obtained from AT&T. In all of our experiments, we used spectral nested dissection [50] to order the matrices. The factorization algorithms described in this paper will work well with any type of nested dissection. In <ref> [21, 22, 20, 32, 31] </ref>, we show that nested dissection orderings with proper selection of separators can yield better quality orderings that traditional heuristics, such as, the multiple minimum degree heuristic. The performance obtained by this algorithm in some of these matrices is shown in Table 2.
Reference: [22] <author> Anshul Gupta. WGPP: </author> <title> Watson graph partitioning (and sparse matrix ordering) package: Users manual. </title> <type> Technical Report RC 20453 (90427), </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> May 6, </month> <year> 1996. </year> <note> Also available at http://www.cs.umn.edu/Research/ibm-cluster/refs/WGPP Users Manual.ps.Z. </note>
Reference-contexts: NUG15 is from a linear program-ming problem derived from a quadratic assignment problem obtained from AT&T. In all of our experiments, we used spectral nested dissection [50] to order the matrices. The factorization algorithms described in this paper will work well with any type of nested dissection. In <ref> [21, 22, 20, 32, 31] </ref>, we show that nested dissection orderings with proper selection of separators can yield better quality orderings that traditional heuristics, such as, the multiple minimum degree heuristic. The performance obtained by this algorithm in some of these matrices is shown in Table 2.
Reference: [23] <author> Anshul Gupta, George Karypis, and Vipin Kumar. </author> <title> Highly scalable parallel algorithms for sparse matrix factorization. </title> <type> Technical Report 94-63, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <note> To appear in IEEE Transactions on Parallel and Distributed Systems, 1997. Postscript file available via anonymous FTP from the site ftp://ftp.cs.umn.edu/users/kumar. </note>
Reference-contexts: Impact on Communication Overhead Note that the communication overhead of subforest-to-subcube mapping is somewhat higher than that of subtree-to-subcube mapping. This is is mainly because subforest-to-subcube mapping results in smaller frontal matrices being mapped onto larger groups of processor. However, we have proved in <ref> [23] </ref> that the asymptotic bounds on the communication overhead of subforest-to-subcube mapping are the same as those of subtree-to-subcube mapping. Therefore, the algorithm described in this section is equally scalable as the one discussed in 3.
Reference: [24] <author> Anshul Gupta and Vipin Kumar. </author> <title> Performance properties of large scale parallel systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 19 </volume> <pages> 234-244, </pages> <year> 1993. </year> <note> Also available as Technical Report TR 92-32, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: It is the total communication overhead that actually determines the overall efficiency and speedup, and is defined as the difference between the parallel processor-time product and the serial run time <ref> [24, 35] </ref>. The communication overhead can be asymptotically higher than the communication volume. For example, a one-to-all broadcast algorithm based on a binary tree communication pattern has a total communication volume of m. p 1/ for broadcasting m words of data among p processors.
Reference: [25] <author> Anshul Gupta and Vipin Kumar. </author> <title> A scalable parallel algorithm for sparse matrix factorization. </title> <type> Technical Report 94-19, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <note> A short version appears in Supercomputing '94 Proceedings. TR available in users/kumar at anonymous FTP site ftp.cs.umn.edu. </note>
Reference-contexts: The performance and scalability analysis of our algorithm is supported by experimental results on up to 1024 processors of nCUBE2 <ref> [25] </ref> and Cray T3D parallel computers. We have been able to achieve speedups of up to 364 on 1024 processors and 230 on 512 processors over a highly efficient sequential implementation for moderately sized problems from the Harwell-Boeing collection [6]. <p> Any elimination tree can be converted to a binary relaxed supernodal tree suitable for parallel multifrontal elimination by a simple preprocessing step described in detail in <ref> [25] </ref>. In order to factorize the sparse matrix in parallel, portions of the elimination tree are assigned to processors 7 + denotes an extend-add operation. <p> The detailed experimental performance and scalability results of this implementation have been presented in <ref> [25] </ref>. Table 1 shows the results of factoring some matrices from the Harwell-Boeing collection of sparse matrices [6]. These results show that our algorithm can deliver good speedups on hundreds of processors for practical problems. Spectral nested dissection [50, 51, 52] was used to order these matrices. <p> All times are in seconds. The suffix * indicates run time estimated by timing the computation on two processors. loss is due to load imbalance and the rest due to communication. We have experimentally shown in <ref> [25] </ref> that the overhead due do load imbalance tends to saturate as the number of processors increase and, therefore, does not affect the asymptotic scalability of the algorithm. However, load imbalance puts an upper bound on the achievable efficiency and results in a significant performance penalty.
Reference: [26] <author> Anshul Gupta and Vipin Kumar. </author> <title> Parallel algorithms for forward and back substitution in direct solution of sparse linear systems. </title> <booktitle> In Supercomputing '95 Proceedings, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: At the end of the parallel ordering step, the parallel symbolic factorization algorithm described in [19] can proceed without any redistribution. In <ref> [19, 26] </ref>, we present efficient parallel algorithms for solving the upper and lower triangular systems. The experimental results in [19, 26] show that the data mapping scheme described in Section 3 works well for triangular solutions. <p> At the end of the parallel ordering step, the parallel symbolic factorization algorithm described in [19] can proceed without any redistribution. In <ref> [19, 26] </ref>, we present efficient parallel algorithms for solving the upper and lower triangular systems. The experimental results in [19, 26] show that the data mapping scheme described in Section 3 works well for triangular solutions. We hope that the work presented in this paper, along with [19, 26, 33] will enable the development of efficient practical parallel solvers for a broad range of scientific computing problems. <p> In [19, 26], we present efficient parallel algorithms for solving the upper and lower triangular systems. The experimental results in [19, 26] show that the data mapping scheme described in Section 3 works well for triangular solutions. We hope that the work presented in this paper, along with <ref> [19, 26, 33] </ref> will enable the development of efficient practical parallel solvers for a broad range of scientific computing problems.
Reference: [27] <author> M. T. Heath, E. G.-Y. Ng, and Barry W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33 </volume> <pages> 420-460, </pages> <year> 1991. </year> <note> Also appears in K. </note> <author> A. Gallivan et al. </author> <title> Parallel Algorithms for Matrix Computations. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference: [28] <author> M. T. Heath and Padma Raghavan. </author> <title> Distributed solution of sparse linear systems. </title> <type> Technical Report 93-1793, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <year> 1993. </year>
Reference: [29] <author> Laurie Hulbert and Earl Zmijewski. </author> <title> Limiting communication in parallel sparse Cholesky factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12(5) </volume> <pages> 1184-1197, </pages> <month> September </month> <year> 1991. </year>
Reference: [30] <author> George Karypis, Anshul Gupta, and Vipin Kumar. </author> <title> Parallel formulation of interior point algorithms. </title> <type> Technical Report 94-20, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <month> April </month> <year> 1994. </year> <note> A short version appears in Supercomputing '94 Proceedings. </note>
Reference-contexts: We have been able to achieve speedups of up to 364 on 1024 processors and 230 on 512 processors over a highly efficient sequential implementation for moderately sized problems from the Harwell-Boeing collection [6]. In <ref> [30] </ref>, we have applied this algorithm to obtain a highly scalable parallel formulation of interior point algorithms and have observed significant speedups in solving linear programming problems. On the Cray T3D, we have been able to achieve up to 20 GFlops on medium-size structural engineering and linear programming problems.
Reference: [31] <author> George Karypis and Vipin Kumar. </author> <title> Analysis of multilevel graph partitioning. </title> <type> Technical Report TR 95-037, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: NUG15 is from a linear program-ming problem derived from a quadratic assignment problem obtained from AT&T. In all of our experiments, we used spectral nested dissection [50] to order the matrices. The factorization algorithms described in this paper will work well with any type of nested dissection. In <ref> [21, 22, 20, 32, 31] </ref>, we show that nested dissection orderings with proper selection of separators can yield better quality orderings that traditional heuristics, such as, the multiple minimum degree heuristic. The performance obtained by this algorithm in some of these matrices is shown in Table 2.
Reference: [32] <author> George Karypis and Vipin Kumar. </author> <title> A fast and high quality multilevel scheme for partitioning irregular graphs. </title> <type> Technical Report TR 95-035, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: NUG15 is from a linear program-ming problem derived from a quadratic assignment problem obtained from AT&T. In all of our experiments, we used spectral nested dissection [50] to order the matrices. The factorization algorithms described in this paper will work well with any type of nested dissection. In <ref> [21, 22, 20, 32, 31] </ref>, we show that nested dissection orderings with proper selection of separators can yield better quality orderings that traditional heuristics, such as, the multiple minimum degree heuristic. The performance obtained by this algorithm in some of these matrices is shown in Table 2.
Reference: [33] <author> George Karypis and Vipin Kumar. </author> <title> Parallel multilevel graph partitioning. </title> <type> Technical Report TR 95-036, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: We have developed parallel algorithms for the other phases that are tailored to work in conjunction with the numerical factorization algorithm. In <ref> [33] </ref>, we describe an efficient parallel algorithm for determining fill-reducing orderings for parallel factorization of sparse matrices. This algorithm, while performing the ordering in parallel, also distributes the data among the processors in way that the remaining steps can be carried out with minimum data-movement. <p> In [19, 26], we present efficient parallel algorithms for solving the upper and lower triangular systems. The experimental results in [19, 26] show that the data mapping scheme described in Section 3 works well for triangular solutions. We hope that the work presented in this paper, along with <ref> [19, 26, 33] </ref> will enable the development of efficient practical parallel solvers for a broad range of scientific computing problems.
Reference: [34] <author> S. G. Kratzer and A. J. Cleary. </author> <title> Sparse matrix factorization on simd parallel computers. </title> <editor> In A. George, John R. Gilbert, and J. W.-H. Liu, editors, </editor> <title> Graph Theory and Sparse Matrix Computations. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference: [35] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: We have developed highly scalable formulations of sparse Cholesky factorization that substantially improve the state of the art in parallel direct solution of sparse linear systemsboth in terms of scalability and overall performance. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [8, 47, 10, 35] </ref>. We show that the parallel Cholesky factorization algorithms described here are as scalable as the best parallel formulation of dense matrix factorization on both mesh and hypercube architectures for a wide class of sparse matrices, including those arising in two- and three-dimensional finite element problems. <p> It is the total communication overhead that actually determines the overall efficiency and speedup, and is defined as the difference between the parallel processor-time product and the serial run time <ref> [24, 35] </ref>. The communication overhead can be asymptotically higher than the communication volume. For example, a one-to-all broadcast algorithm based on a binary tree communication pattern has a total communication volume of m. p 1/ for broadcasting m words of data among p processors. <p> Section 2 describes the serial multifrontal algorithm for sparse matrix factorization. Section 3 describes our parallel algorithm based on multifrontal elimination. In Section 4, we derive expressions for the communication overhead of the parallel algorithm. In Section 5, we use the isoefficiency analysis <ref> [35, 37, 17] </ref> to determine the scalability of our algorithm and compare it with the scalability of other parallel algorithms for sparse matrix factorization. In Section 6 we present a variation of the algorithm described in Section 3 that reduces the overhead due to load imbalance. <p> The communication that takes place in this phase is the standard communication in pipelined grid-based dense Cholesky factorization <ref> [47, 35] </ref>. If the average size of the frontal matrices is t fi t during the processing of a relaxed supernode with m nodes on a q-processor subcube, then O.m/ messages of size O.t= p q/ are passed through the grid in a pipelined fashion. <p> It is shown in [36] that although this communication does not take place between the nearest neighbors on a subcube, the paths of all communications on any subcube are conflict free with e-cube routing <ref> [46, 35] </ref> and cut-through or worm-hole flow control. This is a direct consequence of the fact that a circular shift is conflict free on a hypercube with e-cube routing. <p> Figure 9 (b) shows a variation of the cyclic mapping, called block-cyclic mapping <ref> [35] </ref>, that can alleviate these problems at the cost of some added load imbalance. Recall that in the mapping of Figure 7 (e), the least significant dlog p=2e bits of a row or column index of the matrix determine the processor to which that row or column belongs. <p> This matrix is distributed on a p p logical mesh of processors. As shown in Figure 8, there are two communication operations involved with each elimination step of dense Cholesky. The average size of a message is .ff p p p shown <ref> [47, 35] </ref> that in a pipelined implementation on a p p q mesh of processors, the communication time for s elimination steps with an average message size of m is O.ms/. <p> In this section we use the isoefficiency metric <ref> [35, 37, 17] </ref> to characterize the scalability of our algorithm. <p> To maintain a fixed efficiency, N 2 / N 4=3 p p, or N 2=3 / p, or N 2 D W / p 1=5 . A lower bound on the isoefficiency function for dense matrix factorization is 2. p 1:5 / <ref> [35, 36] </ref> if the number of rank-1 updates performed by the serial algorithm is proportional to the rank of the matrix. The factorization of a sparse matrix derived from an N -node graph with an S.N /-node separator involves a dense S.N / fi S.N / matrix factorization. <p> If each of the matrices is factored by all the processors, then the total communication time for factoring the two matrices is n 2 = p p <ref> [35] </ref>. If A and B are factored concurrently by p=2 processors each, then the communication time is n 2 =.2 p p=2/ which is smaller. <p> We describe an implementation on Cray T3D that yields up to 20 GFlops on medium-size problems. We use the isoefficiency metric <ref> [35, 37, 17] </ref> to characterize the scalability of our algorithms. We show that the isoefficiency function of our algorithms is O. p 1:5 / on hypercube and mesh architectures for sparse matrices arising out of both two- and three-dimensional problems.
Reference: [36] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Solutions Manual for Introduction to Parallel Computing. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1994. </year> <month> 29 </month>
Reference-contexts: Figure 8 shows the communication for one step of dense Cholesky factorization of a hypothetical frontal matrix for q D 16. It is shown in <ref> [36] </ref> that although this communication does not take place between the nearest neighbors on a subcube, the paths of all communications on any subcube are conflict free with e-cube routing [46, 35] and cut-through or worm-hole flow control. <p> To maintain a fixed efficiency, N 2 / N 4=3 p p, or N 2=3 / p, or N 2 D W / p 1=5 . A lower bound on the isoefficiency function for dense matrix factorization is 2. p 1:5 / <ref> [35, 36] </ref> if the number of rank-1 updates performed by the serial algorithm is proportional to the rank of the matrix. The factorization of a sparse matrix derived from an N -node graph with an S.N /-node separator involves a dense S.N / fi S.N / matrix factorization.
Reference: [37] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing scalability of parallel algorithms and architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 379-391, </pages> <year> 1994. </year> <note> Also available as Technical Report TR 91-18, </note> <institution> Department of Computer Science Department, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: Section 2 describes the serial multifrontal algorithm for sparse matrix factorization. Section 3 describes our parallel algorithm based on multifrontal elimination. In Section 4, we derive expressions for the communication overhead of the parallel algorithm. In Section 5, we use the isoefficiency analysis <ref> [35, 37, 17] </ref> to determine the scalability of our algorithm and compare it with the scalability of other parallel algorithms for sparse matrix factorization. In Section 6 we present a variation of the algorithm described in Section 3 that reduces the overhead due to load imbalance. <p> In this section we use the isoefficiency metric <ref> [35, 37, 17] </ref> to characterize the scalability of our algorithm. <p> We describe an implementation on Cray T3D that yields up to 20 GFlops on medium-size problems. We use the isoefficiency metric <ref> [35, 37, 17] </ref> to characterize the scalability of our algorithms. We show that the isoefficiency function of our algorithms is O. p 1:5 / on hypercube and mesh architectures for sparse matrices arising out of both two- and three-dimensional problems.
Reference: [38] <author> R. J. Lipton, D. J. Rose, and R. E. Tarjan. </author> <title> Generalized nested dissection. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16 </volume> <pages> 346-358, </pages> <year> 1979. </year>
Reference-contexts: This is because the properties of separators can be generalized from grids to all such graphs within the same order of magnitude bounds <ref> [39, 38, 13] </ref>. We derive these expressions for both hypercube and mesh architectures, and also extend the results to sparse matrices resulting from three-dimensional graphs whose n-node subgraphs have O.n 2=3 /-node separators.
Reference: [39] <author> R. J. Lipton and R. E. Tarjan. </author> <title> A separator theorem for planar graphs. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 36 </volume> <pages> 177-189, </pages> <year> 1979. </year>
Reference-contexts: This is because the properties of separators can be generalized from grids to all such graphs within the same order of magnitude bounds <ref> [39, 38, 13] </ref>. We derive these expressions for both hypercube and mesh architectures, and also extend the results to sparse matrices resulting from three-dimensional graphs whose n-node subgraphs have O.n 2=3 /-node separators.
Reference: [40] <author> J. W.-H. Liu. </author> <title> The multifrontal method for sparse matrix solution: Theory and practice. </title> <type> Technical Report CS-90-04, </type> <institution> York University, </institution> <address> Ontario, Canada, </address> <year> 1990. </year> <note> Also appears in SIAM Review, </note> <month> 34 </month> <pages> 82-109, </pages> <year> 1992. </year>
Reference-contexts: Section 8 contains concluding remarks. 2 The Multifrontal Algorithm for Sparse Matrix Factorization The multifrontal algorithm for sparse matrix factorization was proposed independently, and in somewhat different forms, by Speelpening [60] and Duff and Reid [7], and later elucidated in a tutorial by Liu <ref> [40] </ref>. In this section, we briefly describe a condensed version of multifrontal sparse Cholesky factorization. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as shown in Figure 2.
Reference: [41] <author> Robert F. Lucas. </author> <title> Solving planar systems of equations on distributed-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <address> Palo Alto, CA, </address> <year> 1987. </year>
Reference: [42] <author> Robert F. Lucas, Tom Blank, and Jerome J. Tiemann. </author> <title> A parallel solution method for large sparse systems of equations. </title> <journal> IEEE Transactions on Computer Aided Design, </journal> <volume> CAD-6(6):981-991, </volume> <month> November </month> <year> 1987. </year>
Reference: [43] <author> F. Manne. </author> <title> Load Balancing in Parallel Sparse Matrix Computations. </title> <type> PhD thesis, </type> <institution> University of Bergen, Norway, </institution> <year> 1993. </year>
Reference: [44] <author> Mo Mu and John R. Rice. </author> <title> A grid-based subtree-subcube assignment strategy for solving partial differential equations on hypercubes. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 13(3) </volume> <pages> 826-839, </pages> <month> May </month> <year> 1992. </year>
Reference: [45] <author> Vijay K. Naik and M. Patrick. </author> <title> Data traffic reduction schemes Cholesky factorization on aynchronous multiprocessor systems. </title> <booktitle> In Supercomputing '89 Proceedings, </booktitle> <year> 1989. </year> <note> Also available as Technical Report RC 14500, </note> <institution> IBM T. J. Watson Research Center, Yorktown Heights, NY. </institution>
Reference: [46] <author> S. F. </author> <title> Nugent. </title> <booktitle> The iPSC/2 direct-connect communications technology. In Proceedings of the Third Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <pages> pages 51-60, </pages> <year> 1988. </year>
Reference-contexts: It is shown in [36] that although this communication does not take place between the nearest neighbors on a subcube, the paths of all communications on any subcube are conflict free with e-cube routing <ref> [46, 35] </ref> and cut-through or worm-hole flow control. This is a direct consequence of the fact that a circular shift is conflict free on a hypercube with e-cube routing.
Reference: [47] <author> Dianne P. O'Leary and G. W. Stewart. </author> <title> Assignment and scheduling in parallel matrix factorization. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 77 </volume> <pages> 275-299, </pages> <year> 1986. </year>
Reference-contexts: We have developed highly scalable formulations of sparse Cholesky factorization that substantially improve the state of the art in parallel direct solution of sparse linear systemsboth in terms of scalability and overall performance. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [8, 47, 10, 35] </ref>. We show that the parallel Cholesky factorization algorithms described here are as scalable as the best parallel formulation of dense matrix factorization on both mesh and hypercube architectures for a wide class of sparse matrices, including those arising in two- and three-dimensional finite element problems. <p> The communication that takes place in this phase is the standard communication in pipelined grid-based dense Cholesky factorization <ref> [47, 35] </ref>. If the average size of the frontal matrices is t fi t during the processing of a relaxed supernode with m nodes on a q-processor subcube, then O.m/ messages of size O.t= p q/ are passed through the grid in a pipelined fashion. <p> This matrix is distributed on a p p logical mesh of processors. As shown in Figure 8, there are two communication operations involved with each elimination step of dense Cholesky. The average size of a message is .ff p p p shown <ref> [47, 35] </ref> that in a pipelined implementation on a p p q mesh of processors, the communication time for s elimination steps with an average message size of m is O.ms/.
Reference: [48] <author> V. Pan and J. H. Reif. </author> <title> Efficient parallel solution of linear systems. </title> <booktitle> In 17th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 143-152, </pages> <year> 1985. </year>
Reference-contexts: A number of column-based parallel factorization algorithms [41, 42, 3, 53, 54, 61, 12, 9, 29, 27, 59, 44, 5] 1 In <ref> [48] </ref>, Pan and Reif describe a parallel sparse matrix factorization algorithm for a PRAM type architecture.
Reference: [49] <author> Christos H. Papadimitriou and Kenneth Steiglitz. </author> <title> Combinatorial Optimization, Algorithms and Complexity. </title> <publisher> Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: Fortunately, this is a typical bin-packing problem, and even though, bin-packing is NP complete, a number of 22 good approximate algorithms exist <ref> [49] </ref>. The use of bin-packing makes it possible to balance the computation and to significantly reduce the load imbalance. Acceptable Partitions A partition is acceptable if the percentage difference in the amount of work in the two parts is less than a small constant *.
Reference: [50] <author> Alex Pothen, H. D. Simon, and K.-P. Liou. </author> <title> Partioning sparce matrices with eigenvectors of graphs. </title> <journal> SIAM Journal of Mathematical Analysis and Applications, </journal> <volume> 11(3) </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference-contexts: Table 1 shows the results of factoring some matrices from the Harwell-Boeing collection of sparse matrices [6]. These results show that our algorithm can deliver good speedups on hundreds of processors for practical problems. Spectral nested dissection <ref> [50, 51, 52] </ref> was used to order these matrices. The algorithm presented in Section 3 relies on the ordering algorithm to yield a balanced elimination tree. Imbalances in the elimination tree result in a loss in the efficiency of the parallel implementation. <p> COPTER2 comes from a model of a 23 helicopter rotor. CUBE35 is a 35 fi 35 fi 35 regular three-dimensional grid. NUG15 is from a linear program-ming problem derived from a quadratic assignment problem obtained from AT&T. In all of our experiments, we used spectral nested dissection <ref> [50] </ref> to order the matrices. The factorization algorithms described in this paper will work well with any type of nested dissection.
Reference: [51] <author> Alex Pothen, H. D. Simon, and Lie Wang. </author> <title> Spectral nested dissection. </title> <type> Technical Report 92-01, </type> <institution> Computer Science Department, Pennsylvania State University, University Park, </institution> <address> PA, </address> <year> 1992. </year>
Reference-contexts: Table 1 shows the results of factoring some matrices from the Harwell-Boeing collection of sparse matrices [6]. These results show that our algorithm can deliver good speedups on hundreds of processors for practical problems. Spectral nested dissection <ref> [50, 51, 52] </ref> was used to order these matrices. The algorithm presented in Section 3 relies on the ordering algorithm to yield a balanced elimination tree. Imbalances in the elimination tree result in a loss in the efficiency of the parallel implementation.
Reference: [52] <author> Alex Pothen, H. D. Simon, Lie Wang, and Stephen T. Bernard. </author> <title> Towards a fast implementation of spectral nested dissection. </title> <booktitle> In Supercomputing '92 Proceedings, </booktitle> <pages> pages 42-51, </pages> <year> 1992. </year>
Reference-contexts: Table 1 shows the results of factoring some matrices from the Harwell-Boeing collection of sparse matrices [6]. These results show that our algorithm can deliver good speedups on hundreds of processors for practical problems. Spectral nested dissection <ref> [50, 51, 52] </ref> was used to order these matrices. The algorithm presented in Section 3 relies on the ordering algorithm to yield a balanced elimination tree. Imbalances in the elimination tree result in a loss in the efficiency of the parallel implementation.
Reference: [53] <author> Alex Pothen and Chunguang Sun. </author> <title> Distributed multifrontal factorization using clique trees. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 34-40, </pages> <year> 1991. </year>
Reference: [54] <author> Roland Pozo and Sharon L. Smith. </author> <title> Performance evaluation of the parallel multifrontal method in a distributed-memory environment. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 453-456, </pages> <year> 1993. </year>
Reference: [55] <author> Padma Raghavan. </author> <title> Distributed sparse matrix factorization: QR and Cholesky factorizations. </title> <type> PhD thesis, </type> <institution> Pennsylvania State University, University Park, </institution> <address> PA, </address> <year> 1991. </year> <month> 30 </month>
Reference: [56] <author> Edward Rothberg. </author> <title> Performance of panel and block approaches to sparse Cholesky factorization on the iPSC/860 and Paragon systems. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference: [57] <author> Edward Rothberg and Anoop Gupta. </author> <title> An efficient block-oriented approach to parallel sparse Cholesky factorization. </title> <booktitle> In Supercomputing '92 Proceedings, </booktitle> <year> 1992. </year>
Reference-contexts: Since the overall computation is only O.N 1:5 / [13], the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased <ref> [59, 57] </ref>. In [2], Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of 2.N p p log N /. <p> In other words, the problem size must be increased as O. p 1:5 / to maintain a constant efficiency as p is increased. In comparison, a lower bound on the isoefficiency function of Rothberg and Gupta's scheme <ref> [57, 18] </ref> with a communication overhead of at least O.N p p log p/ is O. p 1:5 .log p/ 3 /. The isoefficiency function of any column-based scheme is at least O. p 3 / because the total communication overhead has a lower bound of O.N p/.
Reference: [58] <author> Edward Rothberg and Robert Schreiber. </author> <title> Improved load distribution in parallel sparse Cholesky factorization. </title> <booktitle> In Supercomputing '94 Proceedings, </booktitle> <year> 1994. </year>
Reference: [59] <author> Robert Schreiber. </author> <title> Scalability of sparse direct solvers. </title> <type> Technical Report RIACS TR 92.13, </type> <institution> NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <month> May </month> <year> 1992. </year> <note> Also appears in A. </note> <editor> George, John R. Gilbert, and J. W.-H. Liu, editors, </editor> <title> Sparse Matrix Computations: Graph Theory Issues and Algorithms (An IMA Workshop Volume). </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: Since the overall computation is only O.N 1:5 / [13], the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased <ref> [59, 57] </ref>. In [2], Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of 2.N p p log N /. <p> We also show that O. p 1:5 / is asymptotically the best possible isoefficiency function for a parallel implementation of any direct method for solving a system of linear equations, either sparse or dense. In <ref> [59] </ref>, Schreiber concludes that it is not yet clear whether sparse direct solvers can be made competitive at all for highly ( p 256) and massively ( p 4096) parallel computers.
Reference: [60] <author> B. Speelpening. </author> <title> The generalized element method. </title> <type> Technical Report UIUCDCS-R-78-946, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <month> November </month> <year> 1978. </year>
Reference-contexts: Section 7 contains the preliminary experimental results on a Cray T3D parallel computer. Section 8 contains concluding remarks. 2 The Multifrontal Algorithm for Sparse Matrix Factorization The multifrontal algorithm for sparse matrix factorization was proposed independently, and in somewhat different forms, by Speelpening <ref> [60] </ref> and Duff and Reid [7], and later elucidated in a tutorial by Liu [40]. In this section, we briefly describe a condensed version of multifrontal sparse Cholesky factorization. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as shown in Figure 2.
Reference: [61] <author> Chunguang Sun. </author> <title> Efficient parallel solutions of large sparse SPD systems on distributed-memory multiprocessors. </title> <type> Technical Report CTC92TR102, </type> <institution> Advanced Computing Research Institute, Center for Theory and Simulation in Science and Engineering, Cornell University, </institution> <address> Ithaca, NY, </address> <month> August </month> <year> 1992. </year>
Reference: [62] <author> Sesh Venugopal and Vijay K. Naik. </author> <title> Effects of partitioning and scheduling sparse matrix factorization on communication and load balance. </title> <booktitle> In Supercomputing '91 Proceedings, </booktitle> <pages> pages 866-875, </pages> <year> 1991. </year>
Reference: [63] <author> P. H. Worley and Robert Schreiber. </author> <title> Nested dissection on a mesh connected processor array. </title> <editor> In Arthur Wouk, editor, </editor> <booktitle> New Computing Environments: Parallel, Vector, and Systolic, </booktitle> <pages> pages 8-38. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1986. </year> <month> 31 </month>
Reference-contexts: Furthermore, as we show in Section 5, this reduction in communication overhead by a factor of O.log p/ results in an improvement in the scalability of the algorithm by a factor of O..log p/ 3 /; i.e., the rate at which 2 [4] and <ref> [63] </ref> could be possible exceptions, but neither a detailed communication analysis, nor any experimental results are available. 4 the problem size must increase with the number of processors to maintain a constant efficiency is lower by a factor of O..log p/ 3 /.
References-found: 63

