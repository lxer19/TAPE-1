URL: ftp://ftp.cs.arizona.edu/xkernel/Papers/images.ps
Refering-URL: http://www.cs.arizona.edu/xkernel/bibliography.html
Root-URL: http://www.cs.arizona.edu
Title: Image Transfer: An End-to-End Design  
Author: Charles J. Turner and Larry L. Peterson TR -a 
Address: Tucson, AZ 85721  
Affiliation: Department of Computer Science The University of Arizona  
Date: 21, 1992  
Note: July  
Abstract: The transfer of digital images between data archives and scientific workstations is likely to consume a significant amount of network bandwidth in the very near future. This paper examines the image transfer problem from an end-to-end perspective, that is, it describes a complete image transfer protocol that takes into account both the nature of digital imagery and the properties of the underlying network. Specifically, it describes a simple algorithm for encoding images into network packets in such a way that the receiver can recover from dropped packets without requiring the sender to retransmit them. Avoiding retransmissions has the advantages of improving response time and eliminating the need to buffer data at the sender. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. H. T. Bates and M. J. </author> <title> McDonnell. Image Restoration and Reconstruction. </title> <publisher> Clarendon Press, </publisher> <address> Oxford England, </address> <year> 1986. </year>
Reference-contexts: Like many other parts of an end-to-end system, the right recovery method is a function of required image quality and execution speed requirements. The following list identifies a range of recovery algorithms ordered by increasing accuracy and processing requirements <ref> [1, 14, 12] </ref>. To date, we have implemented and experimented with the first six approaches. No-Recovery: Rely on the distributed nature of the lost pixels to minimize the overall impact on subjective image quality. Row-Based: Use neighboring rows to compute new values for a pixels in a lost row.
Reference: [2] <author> E. Biersack. </author> <title> Performance evaluation of forward error correction in ATM networks. </title> <booktitle> In Proceedings of SIGCOMM 92, </booktitle> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: Because the images contain redundant information, our image transfer protocol is able to recover from errors (i.e., dropped packets) introduced by the network. This is very much like forward error correction <ref> [2] </ref>, except we take advantage of the spatial redundancy already present in the images instead of explicitly adding redundant information to the data prior to transmission. Our approach is to not retransmit lost packets, but instead to use the spatial redundancy in the data to recover the missing information.
Reference: [3] <author> D. Cheriton. VMTP: </author> <title> Versatile message transaction protocol. Request For Comments 1045, </title> <booktitle> DARPA, </booktitle> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: In order to use this volume of data effectively, it is necessary to give researchers the fastest possible transfer methods. This paper addresses the problem of effectively transferring digital images across high-speed, wide-area networks. In contrast to the way images are currently transferred using general purpose bulk transfer protocols <ref> [11, 3, 4] </ref>, our approach is based on an end-to-end design that considers all aspects of the problem, including user requirements, characteristics of the data, data compression, and network performance.
Reference: [4] <author> D. Clark, M. Lambert, and L. Zhang. NETBLT: </author> <title> A high throughput transport protocol. </title> <booktitle> In Proceedings of the SIGCOMM '87 Symposium, </booktitle> <pages> pages 353-359, </pages> <address> Stowe, Vermont, </address> <month> Aug. </month> <year> 1987. </year>
Reference-contexts: In order to use this volume of data effectively, it is necessary to give researchers the fastest possible transfer methods. This paper addresses the problem of effectively transferring digital images across high-speed, wide-area networks. In contrast to the way images are currently transferred using general purpose bulk transfer protocols <ref> [11, 3, 4] </ref>, our approach is based on an end-to-end design that considers all aspects of the problem, including user requirements, characteristics of the data, data compression, and network performance.
Reference: [5] <author> D. D. Clark and D. L. Tennenhouse. </author> <title> Architectural considerations for a new generation of protocols. </title> <booktitle> In Proceedings of the SIGCOMM '90 Symposium, </booktitle> <pages> pages 200-208, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: Therefore, our encoding algorithm also preserves those qualities of the image that support data compression. Third, we have designed our protocol in such a way that all packets can be processed independently and in any order <ref> [5] </ref>. This has benefits with respect to parallel 1 protocol design and overall system simplification. The key characteristic of our design is that by properly encoding images in network packets, we are able to avoid the retransmission of lost packets.
Reference: [6] <author> P. J. Downey. </author> <title> Distribution-free bounds on the expectation of the maximum with scheduling applications. </title> <journal> Operations Research Letters, </journal> (9):189-201, 1990. 
Reference-contexts: Although one can imagine performing recovery incrementallythat is, at the same time as additional packets are arrivingthere is the added complication of deciding when a particular packet is missing as opposed to delayed <ref> [6] </ref>. On a uniprocessor, our approach is to not perform any recovery until the complete subimage is received, but to overlap the reconstruction of one subimage with the receipt the packets in the next subimage.
Reference: [7] <author> A. Fleig. </author> <title> The EOS data and information system. </title> <booktitle> In Proceedings of Scientific Data Compression Workshop, </booktitle> <pages> pages 73-83, </pages> <institution> NASA Office of Management. Scientific and Technical Information Division, </institution> <address> Washington, D.C., </address> <year> 1989. </year>
Reference-contexts: 1 Introduction In the near future, members of the scientific community will require the ability to transfer extremely large amounts of digital imagerystill images typically ranging in size from 1MByte to 25MBytes. Earth observing platforms will be collecting data continuously at a rate of several hundred gigabytes per day <ref> [7] </ref>, with researchers around the world performing interdisciplinary, multi-sensor experiments using data from distributed archives. At this collection rate, these users could become one of the largest consumers of wide-area network bandwidth.
Reference: [8] <author> S. Floyd and V. Jacobson. </author> <title> Traffic phase effects in packet-switched gateways. </title> <journal> In ACM SIGCOMM Computer Communications Review, </journal> <volume> volume 21, </volume> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: The second parameter used in our encoding algorithm, PacketOffset, is intended to compensate for bursty packet loss. When congestion occurs in the network, it is often the case that multiple adjacent packets will be lost <ref> [8] </ref>. In experiments on the TCP/IP Internet we observed an average loss burst of 3.3 packets, with 12% of the losses having a burst size of greater than four packets. 3 Figure ??, shows the effect of adding PacketOffset bytes between the start of each subsequent packet.
Reference: [9] <author> R. C. Gonzales and P. Wintz. </author> <title> Digital Image Processing, Second Edition. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA., </address> <year> 1989. </year>
Reference-contexts: For images that have large homogeneous regions, (e.g. binary data, maps, text) this technique is quite effective. However, for images with even a small degree of local variation, it is not uncommon for image sizes to increase. * Differential Pulse Code Modulation (DPCM) <ref> [9] </ref> is also a loss-less algorithm. It is better suited for digital satellite data in that it takes advantage of the local correlation between adjacent pixels. <p> The technique begins by outputting a reference pixel value and then a sequence of differences in gray level from the previous pixel value. If the difference is too great then the sequence is reset and a new reference value is output. * Discrete Cosine Transform (DCT) <ref> [9, 16] </ref> is the most commonly used lossy compression algorithm. It transforms the image into its constituent spatial frequencies. The number of bits allocated to each frequency is determined by the degree to which that frequency contributes to the overall image.
Reference: [10] <author> V. Jacobson. </author> <title> Congestion avoidance and control. </title> <booktitle> In Proceedings of the SIGCOMM '88 Symposium, </booktitle> <pages> pages 314-32, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Bandwidth is not reserved, and should a gateway become congested, it discards packets without regard to the requirements of the competing users. In this case, there is no resource allocation to speak of, and the transport protocol (TCP) is responsible for coping with highly variable delays and avoiding congestion <ref> [10] </ref>. A significant amount of recent research has focused on various resource allocation schemes. The idea is to implement a smarter queuing discipline in the gateways, and to allow end applications to request (and be guaranteed) a particular quality of service from the network.
Reference: [11] <author> J. Postel and J. Reynolds. </author> <title> File Transfer Protocol (FTP). Request For Comments 959, </title> <institution> USC Information Sciences Institute, Marina del Ray, Calif., </institution> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: In order to use this volume of data effectively, it is necessary to give researchers the fastest possible transfer methods. This paper addresses the problem of effectively transferring digital images across high-speed, wide-area networks. In contrast to the way images are currently transferred using general purpose bulk transfer protocols <ref> [11, 3, 4] </ref>, our approach is based on an end-to-end design that considers all aspects of the problem, including user requirements, characteristics of the data, data compression, and network performance.
Reference: [12] <author> W. K. Pratt. </author> <title> Digital Image Processing. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: An advantage of this approach is that information removed in the transform domain will be evenly distributed over the reconstructed image, making the lost information less noticeable. We have considered the following loss-less and lossy algorithms in conjunction with our encoding algorithm: * Run length encoding (RLE) <ref> [12] </ref> is a loss-less algorithm that compares adjacent pixels in gray level, encoding only changes. Each packet is encoded as a sequence of ordered pairs of bytes, each pair containing a gray level and a run length. <p> Like many other parts of an end-to-end system, the right recovery method is a function of required image quality and execution speed requirements. The following list identifies a range of recovery algorithms ordered by increasing accuracy and processing requirements <ref> [1, 14, 12] </ref>. To date, we have implemented and experimented with the first six approaches. No-Recovery: Rely on the distributed nature of the lost pixels to minimize the overall impact on subjective image quality. Row-Based: Use neighboring rows to compute new values for a pixels in a lost row.
Reference: [13] <author> B. R. Schatz. </author> <title> Building an electronic community system. </title> <journal> Journal Management Information Systems, </journal> <volume> 8(3) </volume> <pages> 87-107, </pages> <year> 1991. </year>
Reference-contexts: In particular, we plan to use the protocol to transfer journal page images as part of a nationwide collaboratory for a community of Molecular Biologists <ref> [13] </ref>. We also believe the techniques presented are applicable to the transfer of real-time video, speech, music, and instrument data.
Reference: [14] <author> R. Schowengerdt. </author> <title> Techniques for Image Processing and Classification in Remote Sensing. </title> <publisher> Academic Press, Inc., </publisher> <year> 1983. </year>
Reference-contexts: Like many other parts of an end-to-end system, the right recovery method is a function of required image quality and execution speed requirements. The following list identifies a range of recovery algorithms ordered by increasing accuracy and processing requirements <ref> [1, 14, 12] </ref>. To date, we have implemented and experimented with the first six approaches. No-Recovery: Rely on the distributed nature of the lost pixels to minimize the overall impact on subjective image quality. Row-Based: Use neighboring rows to compute new values for a pixels in a lost row.
Reference: [15] <author> C. J. Turner and L. L. Peterson. </author> <title> The effects of transfer encoding on image quality. </title> <booktitle> In Proceedings of the 2nd International Conference on Image Processing, </booktitle> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: In the case of DCT, because it is lossy, it is possible to get any desired compression ratio, but at the expense of image quality. The ratios reported for DCT result in a reasonable error, using both global and subjective quality measures <ref> [15] </ref>. RLE DPCM DCT TEXT 8.13 MSS 1.60 6.56 SPOT 1.53 6.10 Table 1: Compression Ratio Before Encoding As can be seen, different compression techniques work best for different types of images. <p> Table 3 shows the average RMS error for five SPOT images with packet loss rates of 1, 5, 10, and 20 percent; a more thorough analysis of the impact the encoding has on image quality can be found in a companion paper <ref> [15] </ref>. The row labeled No-Recovery shows the RMS error for the unrecovered image and the row labeled Row-Based Recovery corresponds to the recovery of a raster-based encoded image. The other four rows correspond to different recovery algorithms run on an image encoded using the scattered encoding.
Reference: [16] <author> G. K. Wallace. </author> <title> The JPEG still picture compression standard. </title> <journal> Communications of the ACM, </journal> <volume> 34(4) </volume> <pages> 30-44, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: The technique begins by outputting a reference pixel value and then a sequence of differences in gray level from the previous pixel value. If the difference is too great then the sequence is reset and a new reference value is output. * Discrete Cosine Transform (DCT) <ref> [9, 16] </ref> is the most commonly used lossy compression algorithm. It transforms the image into its constituent spatial frequencies. The number of bits allocated to each frequency is determined by the degree to which that frequency contributes to the overall image.
Reference: [17] <author> C. L. Williamson and D. R. Cheriton. </author> <title> Loss-load curves: Support for rate-based congestion control in high-speed datagram networks. </title> <booktitle> In Proceedings of SIGCOMM 91, </booktitle> <pages> pages 17-28, </pages> <month> Sept. </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: The more interesting property to the image transfer protocol is the loss rate it will realize if it transmits at a particular rate. Thus, load-loss curves proposed by Williamson and Cheriton <ref> [17] </ref> offer the ideal resource allocation support to an image transfer protocol. On a best-effort delivery network like the current TCP/IP Internet, the image transfer protocol sends IP datagrams and computes the load-loss curve itself.
References-found: 17

