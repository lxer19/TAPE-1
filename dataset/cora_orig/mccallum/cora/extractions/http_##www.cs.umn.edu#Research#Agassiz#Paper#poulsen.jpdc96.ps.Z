URL: http://www.cs.umn.edu/Research/Agassiz/Paper/poulsen.jpdc96.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: poulsen@kai.com  yew@cs.umn.edu  
Phone: 217-356-2288  
Title: INTEGRATING FINE-GRAINED MESSAGE PASSING IN CACHE COHERENT SHARED MEMORY MULTIPROCESSORS  
Author: David K. Poulsen and Pen-Chung Yew 
Address: 1906 Fox Drive Champaign, IL 61821  4-192 EE/CS Building 200 Union Street SE Minneapolis, MN 55455  
Affiliation: Kuck and Associates, Inc.  Department of Computer Science University of Minnesota  
Abstract: This work was supported in part by the National Science Foundation under grant nos. NSF MIP 93-07910 and NSF MIP 89-20891, the Department of Energy under grant no. DOE DE FG02-85ER25001, the National Security Agency, and an IBM Resident Study Fellowship. This work was performed while the authors were with the Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, Urbana, IL 61801. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Callahan, D., Kennedy, K., and Porterfield, A. K. </author> <title> Software prefetching. </title> <booktitle> Proc. 4th International Conference on Architectural Support for Programming Languages and Operating Systems. </booktitle> <year> 1991, </year> <pages> pp. 40-52. </pages>
Reference-contexts: From a technological standpoint, processor performance is advancing more rapidly than memory speed, which exacerbates the memory latency problem. Communication between processors distinguishes the multiprocessor memory latency problem from the uniprocessor problem. In uniprocessors, data prefetching <ref> [1, 2] </ref> has been used successfully to hide memory latency and increase performance by exploiting spatial and temporal locality. In cache coherent multiprocessors, however, sharing misses caused by communication between processors occur in addition to uniprocessor nonsharing misses. <p> Data prefetching is a popular latency reduction technique used in many uniprocessor and multiprocessor architectures. Many different prefetching architectures and algorithms have been described and evaluated in the literature. This paper considers software-initiated, non-binding prefetching <ref> [1, 2] </ref> into processor caches. Software-initiated schemes, as opposed to hardware-initiated schemes that do not require software intervention, accomplish prefetching by inserting explicit instructions into application codes. Such instructions may be inserted at the source or machine-language level; the algorithms described in this paper employ source-level techniques. <p> Such instructions may be inserted at the source or machine-language level; the algorithms described in this paper employ source-level techniques. One potential disadvantage of software-initiated schemes is the overhead introduced by prefetching and related instructions; this overhead can have a significant impact on performance <ref> [1, 2] </ref>. Non-binding prefetching refers to the idea that prefetched cache blocks are still exposed to the cache replacement policy and coherence protocol, and may be cast out or invalidated after being prefetched but before being used.
Reference: 2. <author> Mowry, T. C., Lam, M. S., and Gupta, A. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> Proc. 5th International Conference on Architectural Support for Programming Languages and Operating Systems. </booktitle> <year> 1992, </year> <pages> pp. 62-73. </pages>
Reference-contexts: From a technological standpoint, processor performance is advancing more rapidly than memory speed, which exacerbates the memory latency problem. Communication between processors distinguishes the multiprocessor memory latency problem from the uniprocessor problem. In uniprocessors, data prefetching <ref> [1, 2] </ref> has been used successfully to hide memory latency and increase performance by exploiting spatial and temporal locality. In cache coherent multiprocessors, however, sharing misses caused by communication between processors occur in addition to uniprocessor nonsharing misses. <p> This paper studies and compares the performance advantages of data prefetching and data forwarding for reducing memory latency caused by interprocessor communication in cache coherent, shared memory multiprocessors. Data prefetching is accomplished by using a multiprocessor software pipelined algorithm based on <ref> [2] </ref>. Data forwarding integrates fine-grained message passing into a shared memory architecture by allowing individual cache blocks to be sent explicitly to other processors' caches within the context of a cache coherence protocol [6]. <p> Data prefetching is a popular latency reduction technique used in many uniprocessor and multiprocessor architectures. Many different prefetching architectures and algorithms have been described and evaluated in the literature. This paper considers software-initiated, non-binding prefetching <ref> [1, 2] </ref> into processor caches. Software-initiated schemes, as opposed to hardware-initiated schemes that do not require software intervention, accomplish prefetching by inserting explicit instructions into application codes. Such instructions may be inserted at the source or machine-language level; the algorithms described in this paper employ source-level techniques. <p> Such instructions may be inserted at the source or machine-language level; the algorithms described in this paper employ source-level techniques. One potential disadvantage of software-initiated schemes is the overhead introduced by prefetching and related instructions; this overhead can have a significant impact on performance <ref> [1, 2] </ref>. Non-binding prefetching refers to the idea that prefetched cache blocks are still exposed to the cache replacement policy and coherence protocol, and may be cast out or invalidated after being prefetched but before being used. <p> (k) prefetch (B (k + D)) END DO prefetch (C (k + D)) A (k) = B (k) + C (k) DO k = j - D + 1, j END DO 3.1 Prefetching Algorithm Data prefetching is performed by using a software pipelined algorithm that extends techniques described in <ref> [2] </ref> to support prefetching for parallel loops, vector operations, and vectorizable loops in addition to serial loops [6, 8]. The algorithm uses loop splitting and software pipelining transformations, estimates loop execution times, computes prefetch distances, and schedules prefetching in a manner similar to that of [2]. Selective prefetching [2] is not <p> that extends techniques described in <ref> [2] </ref> to support prefetching for parallel loops, vector operations, and vectorizable loops in addition to serial loops [6, 8]. The algorithm uses loop splitting and software pipelining transformations, estimates loop execution times, computes prefetch distances, and schedules prefetching in a manner similar to that of [2]. Selective prefetching [2] is not currently implemented, significantly reducing the complexity (but as will be shown in Section 5, not the effectiveness) of the algorithm. <p> described in <ref> [2] </ref> to support prefetching for parallel loops, vector operations, and vectorizable loops in addition to serial loops [6, 8]. The algorithm uses loop splitting and software pipelining transformations, estimates loop execution times, computes prefetch distances, and schedules prefetching in a manner similar to that of [2]. Selective prefetching [2] is not currently implemented, significantly reducing the complexity (but as will be shown in Section 5, not the effectiveness) of the algorithm. An example of software pipelined prefetching for a simple serial loop, when a prefetch distance of D iterations is assumed, is given in Figure 1. <p> The prefetch distance, D, is chosen individually for each loop based on a specified memory latency as well as compiler-generated execution time estimates for loop bodies <ref> [2] </ref>. Software pipelined prefetching is supported for parallel loops when using either round-robin scheduling or self-scheduling; the experiments described in this paper employ round-robin scheduling. <p> The memory system provides a one cycle cache hit latency; round trip network and main memory access delay result in a base 100 cycle cache miss latency under light network loads (this latency was chosen largely to facilitate comparison with the results of other prefetching studies <ref> [2, 3, 4, 24] </ref>). Uniform memory access (UMA) to main memory is assumed to simplify simulations. <p> In FLO52 and ARC2D, aggressive forwarding caused increased numbers of conflict misses as needed data were replaced in destination processor caches. The performance of prefetching was similar in both larger and smaller caches for each of the five codes studied, despite the fact that selective prefetching <ref> [2] </ref> was not used. The large increases in memory access delay under forwarding in smaller caches were caused by significant increases in conflict miss ratios. This suggests that FWD was too aggressive and caused undesirable replacement in destination processors' caches. <p> An extension to the forwarding compiler algorithm implements a fully compiler-based version of the hybrid prefetching and forwarding scheme. This is accomplished by estimating write-to-read execution time distances while identifying write/read reference pairs. This type of execution time estimation is similar to that used in <ref> [2] </ref> for computing prefetch distances for software pipelined prefetching. The third step in the forwarding compiler algorithm attempts to determine, for a given write/read reference pair, an expression for a single processor number to which each write should be forwarded. <p> The second test prevents unnecessary forwarding to the processor that is issuing the write. Additional conditional tests, as well as loop splitting transformations (similar to those described in <ref> [2] </ref>), to prevent forwarding past loop bounds are described in [8].
Reference: 3. <author> Tullsen, D. M., and Eggers, S. J. </author> <title> Limitations of cache prefetching on a bus-based multiprocessor. </title> <booktitle> Proc. 20th Annual International Symposium on Computer Architecture. </booktitle> <year> 1993, </year> <pages> pp. 278-288. </pages>
Reference-contexts: In cache coherent multiprocessors, however, sharing misses caused by communication between processors occur in addition to uniprocessor nonsharing misses. Multiprocessor caches can hide memory latency for sharing accesses by exploiting spatial locality, but increasing cache block sizes may lead to undesirable false sharing <ref> [3] </ref>. <p> The memory system provides a one cycle cache hit latency; round trip network and main memory access delay result in a base 100 cycle cache miss latency under light network loads (this latency was chosen largely to facilitate comparison with the results of other prefetching studies <ref> [2, 3, 4, 24] </ref>). Uniform memory access (UMA) to main memory is assumed to simplify simulations. <p> Each processor has a four-way set-associative, write-back, write-allocate cache with write buffers and LRU replacement. Single word cache blocks are used to minimize false sharing effects, particularly those caused or aggravated by prefetching <ref> [3] </ref>. The selection of cache sizes is discussed in Section 5. Processor caches allow multiple outstanding prefetching and forwarding operations that are non-blocking to the issuing processor; regular reads block the issuing processor. Cache coherence is implemented by using a three-state, directory-based invalidation protocol. <p> Forwarding for FLO52 and ARC2D in smaller caches also caused increases in under-utilization time that were minimized through the use of the hybrid scheme. 5.2 Cache Miss Behavior down into contributions from various sources <ref> [3, 8] </ref>: 1. Nonsharing misses conflict, capacity, or cold start misses. (17) 2. Invalidation misses sharing misses caused by invalidations by other processors. 3. Prefetched vs. nonprefetched misses misses caused when accessing variables whose references were, or were not, prefetched. scheme, broken down into contributions from various sources [3, 8]: 1. <p> various sources <ref> [3, 8] </ref>: 1. Nonsharing misses conflict, capacity, or cold start misses. (17) 2. Invalidation misses sharing misses caused by invalidations by other processors. 3. Prefetched vs. nonprefetched misses misses caused when accessing variables whose references were, or were not, prefetched. scheme, broken down into contributions from various sources [3, 8]: 1. Processor misses total misses seen by processors, from Figure 5a Figure 9a. The following types of misses do not contribute directly to processor miss latency and occur only under prefetching: 2. <p> This was due, in part, to the ample memory bandwidth provided by the modeled system. Prefetching was effective in attacking both conflict (FLO52, ARC2D) and invalidation misses. Prefetching was found to be less effective in attacking invalidation misses in <ref> [3] </ref>; however, false sharing was shown to be a contributor to this effect. A small cache block size was used in the experiments described in Section 5 to eliminate false sharing and increase prefetching effectiveness for invalidation misses.
Reference: 4. <author> Mowry, T. C., and Gupta, A. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> J. Parallel Distrib. Comput. </journal> <volume> 12, </volume> <month> 2 (June </month> <year> 1991), </year> <pages> 87-106. </pages>
Reference-contexts: Multiprocessor caches can hide memory latency for sharing accesses by exploiting spatial locality, but increasing cache block sizes may lead to undesirable false sharing [3]. Data prefetching has been shown to be effective in reducing memory latency in shared memory multiprocessors <ref> [4, 5] </ref>; however, while data prefetching has the ability to hide memory latency for both sharing and nonsharing accesses, data forwarding [6, 7] may be a more effective technique than data prefetching for reducing the latency of sharing accesses. <p> Write buffers can be used to save the addresses of outstanding prefetch operations to eliminate the need for separate prefetch issue buffers <ref> [4] </ref>. Given a conventional cache design employing write buffers, the cost of adding support for prefetching and forwarding to processor caches should be small provided that write buffer sizes are reasonable. <p> Previous studies indicate that modest buffer sizes (e.g., a single, 16 word write buffer) are sufficient to achieve reasonable performance under prefetching <ref> [4, 23] </ref>. Data forwarding using Forwarding Write operations offers the additional benefit of easing write buffer size requirements. Since Forwarding Write operations replace regular writes, write buffer size requirements should not increase significantly under forwarding. <p> The memory system provides a one cycle cache hit latency; round trip network and main memory access delay result in a base 100 cycle cache miss latency under light network loads (this latency was chosen largely to facilitate comparison with the results of other prefetching studies <ref> [2, 3, 4, 24] </ref>). Uniform memory access (UMA) to main memory is assumed to simplify simulations.
Reference: 5. <author> Kuck, D., et al. </author> <title> The Cedar system and an initial performance study. </title> <booktitle> Proc. 20th Annual International Symposium on Computer Architecture. </booktitle> <year> 1993, </year> <pages> pp. 213-223. (30) </pages>
Reference-contexts: Multiprocessor caches can hide memory latency for sharing accesses by exploiting spatial locality, but increasing cache block sizes may lead to undesirable false sharing [3]. Data prefetching has been shown to be effective in reducing memory latency in shared memory multiprocessors <ref> [4, 5] </ref>; however, while data prefetching has the ability to hide memory latency for both sharing and nonsharing accesses, data forwarding [6, 7] may be a more effective technique than data prefetching for reducing the latency of sharing accesses. <p> Adding processor support for prefetching and forwarding instructions is straightforward. Several machines already support these types of operations, including the Stanford Dash multiprocessor [14], the Kendall Square KSR1 [22], and the Cedar multiprocessor <ref> [5] </ref>. The prefetch and Forwarding Write operations assumed in this work most closely resemble the Dash Prefetch and Dash Deliver instructions. A prefetch instruction is similar to a regular load, except that it is non-blocking and is dropped on exceptions [23, 24].
Reference: 6. <author> Poulsen, D. K., and Yew, P.-C. </author> <title> Data prefetching and data forwarding in shared memory multiprocessors. </title> <booktitle> Proc. 1994 International Conference on Parallel Processing. </booktitle> <year> 1994, </year> <pages> pp. </pages> <month> II-276-280. </month>
Reference-contexts: Data prefetching has been shown to be effective in reducing memory latency in shared memory multiprocessors [4, 5]; however, while data prefetching has the ability to hide memory latency for both sharing and nonsharing accesses, data forwarding <ref> [6, 7] </ref> may be a more effective technique than data prefetching for reducing the latency of sharing accesses. This paper studies and compares the performance advantages of data prefetching and data forwarding for reducing memory latency caused by interprocessor communication in cache coherent, shared memory multiprocessors. <p> Data prefetching is accomplished by using a multiprocessor software pipelined algorithm based on [2]. Data forwarding integrates fine-grained message passing into a shared memory architecture by allowing individual cache blocks to be sent explicitly to other processors' caches within the context of a cache coherence protocol <ref> [6] </ref>. The experiments described in this paper utilize data forwarding to optimize interprocessor data communication, rather than synchronization, and data forwarding is applied to application codes to target communication between, rather than within, parallel loops. Experimental results are presented to show the performance advantages of data prefetching and data forwarding. <p> Data forwarding may be a more effective mechanism than data prefetching for reducing the latency of sharing accesses. Data forwarding, as considered here, is a fine-grained message passing capability that is integrated into a shared memory architecture and is used to optimize interprocessor data communication <ref> [6, 8] </ref>. This capability is provided by a forwarding operation that allows data to be sent explicitly from one processor to another within the context of a cache coherence protocol. The forwarding capability is used to reduce the latency of communication-related accesses between nests of parallel loops. <p> (k) + C (k) DO k = j - D + 1, j END DO 3.1 Prefetching Algorithm Data prefetching is performed by using a software pipelined algorithm that extends techniques described in [2] to support prefetching for parallel loops, vector operations, and vectorizable loops in addition to serial loops <ref> [6, 8] </ref>. The algorithm uses loop splitting and software pipelining transformations, estimates loop execution times, computes prefetch distances, and schedules prefetching in a manner similar to that of [2].
Reference: 7. <author> Andrews, J. B., Beckmann, C. J., and Poulsen, D. K. </author> <title> Notification and multicast networks for synchronization and coherence. </title> <journal> J. Parallel Distrib. Comput. </journal> <volume> 15, </volume> <month> 4 (August </month> <year> 1992), </year> <pages> 332-350. </pages>
Reference-contexts: Data prefetching has been shown to be effective in reducing memory latency in shared memory multiprocessors [4, 5]; however, while data prefetching has the ability to hide memory latency for both sharing and nonsharing accesses, data forwarding <ref> [6, 7] </ref> may be a more effective technique than data prefetching for reducing the latency of sharing accesses. This paper studies and compares the performance advantages of data prefetching and data forwarding for reducing memory latency caused by interprocessor communication in cache coherent, shared memory multiprocessors. <p> This assumption facilitates the construction of a forwarding compiler algorithm and of a hybrid scheme that can integrate data forwarding with data prefetching. Other mechanisms have been proposed that could be used for data forwarding in a dynamic, self-scheduling environment <ref> [7, 14, 16, 18] </ref>. 3. PREFETCHING AND FORWARDING SCHEMES This section describes the data prefetching algorithm, the data forwarding scheme, and the new hybrid prefetching and forwarding scheme. These schemes are intended for use in shared memory multiprocessors executing parallel codes such as Cedar Fortran [11] codes.
Reference: 8. <author> Poulsen, D. K. </author> <title> Memory latency reduction via data prefetching and data forwarding in shared memory multiprocessors. </title> <type> Ph.D. dissertation, CSRD Report 1377, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: A hybrid prefetching and forwarding scheme is (1) described and evaluated that allows the relative amounts of prefetching and forwarding used to be adapted to the sharing and locality characteristics of particular applications, as well as to architectural characteristics such as cache size <ref> [8] </ref>. The use of this hybrid scheme is shown to increase performance stability, when compared to either prefetching or forwarding alone, over varying application characteristics and varying cache sizes. The hybrid scheme reduces processor instruction overhead, cache miss ratios, and memory system bandwidth requirements when compared to prefetching alone. <p> Data forwarding may be a more effective mechanism than data prefetching for reducing the latency of sharing accesses. Data forwarding, as considered here, is a fine-grained message passing capability that is integrated into a shared memory architecture and is used to optimize interprocessor data communication <ref> [6, 8] </ref>. This capability is provided by a forwarding operation that allows data to be sent explicitly from one processor to another within the context of a cache coherence protocol. The forwarding capability is used to reduce the latency of communication-related accesses between nests of parallel loops. <p> (k) + C (k) DO k = j - D + 1, j END DO 3.1 Prefetching Algorithm Data prefetching is performed by using a software pipelined algorithm that extends techniques described in [2] to support prefetching for parallel loops, vector operations, and vectorizable loops in addition to serial loops <ref> [6, 8] </ref>. The algorithm uses loop splitting and software pipelining transformations, estimates loop execution times, computes prefetch distances, and schedules prefetching in a manner similar to that of [2]. <p> This section considers the architectural support required for prefetching and forwarding and the concomitant cost of this support <ref> [8] </ref>. Adding processor support for prefetching and forwarding instructions is straightforward. Several machines already support these types of operations, including the Stanford Dash multiprocessor [14], the Kendall Square KSR1 [22], and the Cedar multiprocessor [5]. <p> These capabilities are provided by lockup-free caches [25]; however, such full lockup-free capabilities are not (9) necessarily required. Cache design to support software-initiated prefetching has been discussed in detail in [23, 24]. The cache architectures described therein can be used for forwarding as well as prefetching <ref> [8] </ref>. Provided that caches can accept incoming prefetched or forwarded data, prefetching and forwarding operations can be viewed as being similar to writes, which allows existing write buffer implementations to be used to make the operations appear non-blocking. <p> Since Forwarding Write operations replace regular writes, write buffer size requirements should not increase significantly under forwarding. Furthermore, write buffer pressure due to prefetching can be decreased through the use of hybrid prefetching and forwarding. Simulation results in <ref> [8] </ref> indicate that small write buffer sizes should be sufficient for prefetching or forwarding and that the memory system bandwidth of the modeled system (Section 4.1) is more than sufficient to allow caches to accept incoming prefetched and forwarded data. 4. <p> In these small cache size experiments, 2K word caches were used for TRFD, QCD, and DYFESM, and 8K word caches were used for FLO52 and TRFD. These sizes were determined to be appropriate to the working set sizes of each code <ref> [8] </ref>. The following specific schemes were evaluated: Scheme Meaning BASE No prefetching or forwarding; the base performance of each application. PREF Multiprocessor software pipelined prefetching. FWD Profile-based forwarding. HYBR Hybrid prefetching and forwarding that combines PREF and FWD. F (s) FWD results for smaller cache sizes. <p> Results of BASE and PREF simulations that use the smaller cache sizes are not shown. PREF results for smaller caches were almost identical to those for larger caches. BASE results for smaller caches were generally within ten percent of those for larger caches <ref> [8] </ref>. The relative amounts of prefetching and forwarding used under HYBR are indicated by the data in Table 2. The table shows the percentage of unique read and write references in each application code with write-to-read distances falling in various ranges. <p> Forwarding for FLO52 and ARC2D in smaller caches also caused increases in under-utilization time that were minimized through the use of the hybrid scheme. 5.2 Cache Miss Behavior down into contributions from various sources <ref> [3, 8] </ref>: 1. Nonsharing misses conflict, capacity, or cold start misses. (17) 2. Invalidation misses sharing misses caused by invalidations by other processors. 3. Prefetched vs. nonprefetched misses misses caused when accessing variables whose references were, or were not, prefetched. scheme, broken down into contributions from various sources [3, 8]: 1. <p> various sources <ref> [3, 8] </ref>: 1. Nonsharing misses conflict, capacity, or cold start misses. (17) 2. Invalidation misses sharing misses caused by invalidations by other processors. 3. Prefetched vs. nonprefetched misses misses caused when accessing variables whose references were, or were not, prefetched. scheme, broken down into contributions from various sources [3, 8]: 1. Processor misses total misses seen by processors, from Figure 5a Figure 9a. The following types of misses do not contribute directly to processor miss latency and occur only under prefetching: 2. <p> The majority of cache misses in TRFD were nonsharing misses. Most of the nonsharing misses were cold start misses resulting from the initial generation of program data structures and their communication to other processors at the start of the computation <ref> [8] </ref>. Many nonsharing accesses were prefetched under PREF, but some nonsharing prefetched misses still occurred; prefetching could not be used to eliminate cold start write misses. Prefetching was successful, as was FWD, in eliminating invalidation misses. <p> The experimental results described in Section 5 evaluate forwarding through the use of the profiling techniques described in Section 3.3. This section describes a compiler algorithm for software-initiated data forwarding <ref> [8] </ref> and discusses its effectiveness with respect to identifying and forwarding data for sharing accesses between successive parallel loops. (24) 7.1 Description of Algorithm The forwarding compiler algorithm consists of several steps: 1. Mark eligible write references for forwarding. <p> The second test prevents unnecessary forwarding to the processor that is issuing the write. Additional conditional tests, as well as loop splitting transformations (similar to those described in [2]), to prevent forwarding past loop bounds are described in <ref> [8] </ref>.
Reference: 9. <author> Poulsen, D. K., and Yew, P.-C. </author> <title> Execution-driven tools for parallel simulation of parallel architectures and applications. </title> <booktitle> Proc. Supercomputing '93. </booktitle> <year> 1993, </year> <pages> pp. 860-869. </pages>
Reference-contexts: Algorithms for the data prefetching, data forwarding, and hybrid prefetching and forwarding schemes are described. Experimental results are acquired through execution-driven simulations of large, optimized, parallel numerical benchmarks by using EPG-sim <ref> [9] </ref>, a system of execution-driven tools for studying parallel architectures, compiler algorithms, and applications. Simulations model a shared memory architecture with directory-based cache coherence and are driven by parallel versions of Perfect Benchmarks [10] codes that have been optimized for the modeled architecture.
Reference: 10. <author> Berry, M., et al. </author> <title> The Perfect Club benchmarks: effective performance evaluation of supercomputers. </title> <booktitle> Int'l. J. Supercomp. Applications 3, 3 (Fall 1989), </booktitle> <pages> 5-40. </pages>
Reference-contexts: Experimental results are acquired through execution-driven simulations of large, optimized, parallel numerical benchmarks by using EPG-sim [9], a system of execution-driven tools for studying parallel architectures, compiler algorithms, and applications. Simulations model a shared memory architecture with directory-based cache coherence and are driven by parallel versions of Perfect Benchmarks <ref> [10] </ref> codes that have been optimized for the modeled architecture. These parallel Cedar Fortran [11] codes have prefetching and forwarding applied via algorithms implemented with the EPG source code instrumentation tools [12], a set of passes implemented within the Parafrase-2 parallelizing compiler [13].
Reference: 11. <editor> Padua, D., et al. </editor> <title> The Cedar Fortran project. </title> <type> CSRD Report 1262, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Simulations model a shared memory architecture with directory-based cache coherence and are driven by parallel versions of Perfect Benchmarks [10] codes that have been optimized for the modeled architecture. These parallel Cedar Fortran <ref> [11] </ref> codes have prefetching and forwarding applied via algorithms implemented with the EPG source code instrumentation tools [12], a set of passes implemented within the Parafrase-2 parallelizing compiler [13]. The remainder of this paper is organized as follows. <p> PREFETCHING AND FORWARDING SCHEMES This section describes the data prefetching algorithm, the data forwarding scheme, and the new hybrid prefetching and forwarding scheme. These schemes are intended for use in shared memory multiprocessors executing parallel codes such as Cedar Fortran <ref> [11] </ref> codes. The pertinent language features of such codes, for purposes of this discussion, are parallel (DOALL) loops, vectorizable loops containing regular, strided array section references, serial loops, and vector statements containing array section references. <p> The particular versions of the codes used in these experiments are parallel, Cedar Fortran versions of the applications <ref> [11] </ref> that contain parallel loops and vector statements. These codes are optimized for the Cedar multiprocessor, a 32 processor machine with an architecture similar, in terms of constructs used to describe and exploit parallelism, to the architecture described in Section 4.1. <p> These loop types were modeled by logically partitioning the available 32 processors during simulations into four clusters of eight processors each. The codes were parallelized by using an optimizing compiler and then further hand-optimized to exploit available parallelism, increase locality, and reduce memory latency <ref> [11] </ref>. The characteristics of the resulting optimized, parallel Perfect codes vary because of the differences between the original applications and the differing methods used in their parallelization. TRFD is highly parallel but does not use particularly long vectors.
Reference: 12. <author> Poulsen, D. K., and Yew, P.-C. </author> <title> EPG source code instrumentation tools user manual. </title> <type> CSRD Report, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1994. </year>
Reference-contexts: These parallel Cedar Fortran [11] codes have prefetching and forwarding applied via algorithms implemented with the EPG source code instrumentation tools <ref> [12] </ref>, a set of passes implemented within the Parafrase-2 parallelizing compiler [13]. The remainder of this paper is organized as follows. Section 2 presents a brief overview of existing data prefetching and data forwarding schemes and provides additional motivation for the schemes presented and evaluated in the following sections.
Reference: 13. <author> Polychronopoulos, C. D., et al. </author> <title> Parafrase-2: an environment for parallelizing, partitioning, synchronizing and scheduling programs on multiprocessors. </title> <booktitle> Proc. 1989 International Conference on Parallel Processing. </booktitle> <year> 1989, </year> <pages> pp. </pages> <month> II-39-48. </month>
Reference-contexts: These parallel Cedar Fortran [11] codes have prefetching and forwarding applied via algorithms implemented with the EPG source code instrumentation tools [12], a set of passes implemented within the Parafrase-2 parallelizing compiler <ref> [13] </ref>. The remainder of this paper is organized as follows. Section 2 presents a brief overview of existing data prefetching and data forwarding schemes and provides additional motivation for the schemes presented and evaluated in the following sections.
Reference: 14. <author> Lenoski, D. E., et al. </author> <title> The Stanford Dash multiprocessor. </title> <booktitle> IEEE Computer 25, </booktitle> <month> 3 (March </month> <year> 1992), </year> <pages> 63-79. </pages>
Reference-contexts: Although many mechanisms have been proposed that could be used to implement data forwarding <ref> [14, 15, 16, 17, 18, 19] </ref>, most of these mechanisms have been introduced or evaluated for other purposes, such as reducing the latency of synchronization operations. Little performance information exists regarding the usefulness of these mechanisms for optimizing interprocessor data communication. <p> As in the case of data prefetching, this paper considers software-initiated, non-binding data forwarding into processor caches and employs source-level insertion of forwarding instructions. The forwarding mechanism used is based on the Dash Deliver instruction <ref> [14] </ref>, although the Deliver operation was not originally intended for this purpose. This data forwarding mechanism and its application are described in greater detail in the following section. The data forwarding scheme considered in this paper assumes static, round-robin processor scheduling (cyclic prescheduling) of parallel loop iterations. <p> This assumption facilitates the construction of a forwarding compiler algorithm and of a hybrid scheme that can integrate data forwarding with data prefetching. Other mechanisms have been proposed that could be used for data forwarding in a dynamic, self-scheduling environment <ref> [7, 14, 16, 18] </ref>. 3. PREFETCHING AND FORWARDING SCHEMES This section describes the data prefetching algorithm, the data forwarding scheme, and the new hybrid prefetching and forwarding scheme. These schemes are intended for use in shared memory multiprocessors executing parallel codes such as Cedar Fortran [11] codes. <p> A Forwarding Write is a single instruction that combines a write and a sender-initiated forwarding operation, where the semantics of the forwarding operation are similar to those of the Dash Deliver instruction <ref> [14] </ref>. A Forwarding Write operation transmits copies of specified operands to a set of explicitly specified destination caches. The operand size assumed is a single cache block, and destination processors are specified via a bit vector. <p> A profiling simulation determines the destination processors for each unique write reference and stores these destination processors for each element of each reference as a bit vector, similar to the bit vectors used in the Dash Deliver instruction <ref> [14] </ref>. The end result of profiling is a data structure containing arrays of bit vectors for each eligible written array, so that for a write to A (I), V (I) is a bit vector indicating the destination processors for the write. <p> This section considers the architectural support required for prefetching and forwarding and the concomitant cost of this support [8]. Adding processor support for prefetching and forwarding instructions is straightforward. Several machines already support these types of operations, including the Stanford Dash multiprocessor <ref> [14] </ref>, the Kendall Square KSR1 [22], and the Cedar multiprocessor [5]. The prefetch and Forwarding Write operations assumed in this work most closely resemble the Dash Prefetch and Dash Deliver instructions.
Reference: 15. <author> Frank, M. I., and Vernon, M. K. </author> <title> A hybrid shared memory / message passing parallel machine. </title> <booktitle> Proc. 1993 International Conference on Parallel Processing. </booktitle> <year> 1993, </year> <pages> pp. </pages> <month> I-232-236. </month>
Reference-contexts: Although many mechanisms have been proposed that could be used to implement data forwarding <ref> [14, 15, 16, 17, 18, 19] </ref>, most of these mechanisms have been introduced or evaluated for other purposes, such as reducing the latency of synchronization operations. Little performance information exists regarding the usefulness of these mechanisms for optimizing interprocessor data communication.
Reference: 16. <author> Hill, M. D., et al. </author> <title> Cooperative shared memory: software and hardware for scalable multiprocessors. </title> <booktitle> Proc. 5th International Conference on Architectural Support for Programming Languages and Operating Systems. </booktitle> <year> 1992, </year> <pages> pp. 262-273. </pages>
Reference-contexts: Although many mechanisms have been proposed that could be used to implement data forwarding <ref> [14, 15, 16, 17, 18, 19] </ref>, most of these mechanisms have been introduced or evaluated for other purposes, such as reducing the latency of synchronization operations. Little performance information exists regarding the usefulness of these mechanisms for optimizing interprocessor data communication. <p> This assumption facilitates the construction of a forwarding compiler algorithm and of a hybrid scheme that can integrate data forwarding with data prefetching. Other mechanisms have been proposed that could be used for data forwarding in a dynamic, self-scheduling environment <ref> [7, 14, 16, 18] </ref>. 3. PREFETCHING AND FORWARDING SCHEMES This section describes the data prefetching algorithm, the data forwarding scheme, and the new hybrid prefetching and forwarding scheme. These schemes are intended for use in shared memory multiprocessors executing parallel codes such as Cedar Fortran [11] codes.
Reference: 17. <author> Su, H.-M., and Yew, P.-C. </author> <title> Efficient doacross synchronization on distributed shared-memory multiprocessors. </title> <booktitle> Proc. Supercomputing '91. </booktitle> <year> 1991, </year> <pages> pp. 842-853. </pages>
Reference-contexts: Although many mechanisms have been proposed that could be used to implement data forwarding <ref> [14, 15, 16, 17, 18, 19] </ref>, most of these mechanisms have been introduced or evaluated for other purposes, such as reducing the latency of synchronization operations. Little performance information exists regarding the usefulness of these mechanisms for optimizing interprocessor data communication. <p> and forwarding operations, giving forwarding another potential performance advantage over prefetching. 3.3 Forwarding Algorithm The forwarding schemes presented in this paper target the reduction of memory latency for sharing accesses between successive parallel loops, rather than for fine-grained communication and synchronization within a particular parallel loop nest or DOACROSS loop <ref> [17] </ref>. <p> This is easily accomplished, if round-robin scheduling is employed, by analyzing loop index, loop bounds, and access subscript information. This type of analysis has been applied for generating explicit message passing within a single loop nest for forwarding in DOACROSS loops <ref> [17] </ref>, as well as for compiling languages such as Fortran D for message passing machines [29].
Reference: 18. <author> Lee, J., and Ramachandran, U. </author> <title> Architectural primitives for a scalable shared memory (31) multiprocessor. </title> <booktitle> Proc. 3rd Annual ACM Symposium on Parallel Algorithms and Architectures. </booktitle> <year> 1991, </year> <pages> pp. 103-114. </pages>
Reference-contexts: Although many mechanisms have been proposed that could be used to implement data forwarding <ref> [14, 15, 16, 17, 18, 19] </ref>, most of these mechanisms have been introduced or evaluated for other purposes, such as reducing the latency of synchronization operations. Little performance information exists regarding the usefulness of these mechanisms for optimizing interprocessor data communication. <p> This assumption facilitates the construction of a forwarding compiler algorithm and of a hybrid scheme that can integrate data forwarding with data prefetching. Other mechanisms have been proposed that could be used for data forwarding in a dynamic, self-scheduling environment <ref> [7, 14, 16, 18] </ref>. 3. PREFETCHING AND FORWARDING SCHEMES This section describes the data prefetching algorithm, the data forwarding scheme, and the new hybrid prefetching and forwarding scheme. These schemes are intended for use in shared memory multiprocessors executing parallel codes such as Cedar Fortran [11] codes.
Reference: 19. <author> Goodman, J. R., Vernon, M. K., and Woest, P. J. </author> <title> Efficient synchronization primitives for large-scale cache-coherent multiprocessors. </title> <booktitle> Proc. 3rd International Conference on Architectural Support for Programming Languages and Operating Systems. </booktitle> <year> 1989, </year> <pages> pp. 64-75. </pages>
Reference-contexts: Although many mechanisms have been proposed that could be used to implement data forwarding <ref> [14, 15, 16, 17, 18, 19] </ref>, most of these mechanisms have been introduced or evaluated for other purposes, such as reducing the latency of synchronization operations. Little performance information exists regarding the usefulness of these mechanisms for optimizing interprocessor data communication.
Reference: 20. <author> Weber, W.-D., and Gupta, A. </author> <title> Analysis of cache invalidation patterns in multiprocessors. </title> <booktitle> Proc. 3rd International Conference on Architectural Support for Programming Languages and Operating Systems. </booktitle> <year> 1989, </year> <pages> pp. 243-256. </pages>
Reference-contexts: This type of optimization can be especially useful for sequential (or migratory <ref> [20] </ref>) sharing patterns. Existing software-initiated prefetching schemes assume that prefetching operations are physically separated from loads in order to maximize latency hiding.
Reference: 21. <author> Mounes-Toussi, F., and Lilja, D. J. </author> <title> Using compile-time analysis to adapt the cache coherence enforcement strategy to the data sharing characteristics. </title> <journal> IEEE Trans. Parallel and Distr. Sys. </journal> <note> (1994), to appear. </note>
Reference-contexts: The hybrid scheme is adaptable to the sharing characteristics of different application codes and to varying architectural parameters. This scheme was inspired by techniques employed in compiler-assisted adaptive cache coherence schemes <ref> [21] </ref> that use cost functions to decide whether updating or invalidating is most appropriate for particular references based on their sharing patterns. In hybrid prefetching and forwarding, an analogous cost function is used for each write/read reference pair to decide whether to forward the write or to prefetch the read.
Reference: 22. <institution> KSR1 Principles of Operation. Kendall Square Research Corporation, </institution> <year> 1991. </year>
Reference-contexts: This section considers the architectural support required for prefetching and forwarding and the concomitant cost of this support [8]. Adding processor support for prefetching and forwarding instructions is straightforward. Several machines already support these types of operations, including the Stanford Dash multiprocessor [14], the Kendall Square KSR1 <ref> [22] </ref>, and the Cedar multiprocessor [5]. The prefetch and Forwarding Write operations assumed in this work most closely resemble the Dash Prefetch and Dash Deliver instructions. A prefetch instruction is similar to a regular load, except that it is non-blocking and is dropped on exceptions [23, 24].
Reference: 23. <author> Mowry, T. C. </author> <title> Tolerating latency through software-controlled data prefetching. </title> <type> Ph.D. dissertation, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: The prefetch and Forwarding Write operations assumed in this work most closely resemble the Dash Prefetch and Dash Deliver instructions. A prefetch instruction is similar to a regular load, except that it is non-blocking and is dropped on exceptions <ref> [23, 24] </ref>. A Forwarding Write is similar to a regular write, except that it has different cache coherence semantics (Section 3.2). Cache design to support prefetching and forwarding requires the ability to support multiple outstanding prefetches and the ability to accept incoming prefetched and forwarded data. <p> These capabilities are provided by lockup-free caches [25]; however, such full lockup-free capabilities are not (9) necessarily required. Cache design to support software-initiated prefetching has been discussed in detail in <ref> [23, 24] </ref>. The cache architectures described therein can be used for forwarding as well as prefetching [8]. <p> Previous studies indicate that modest buffer sizes (e.g., a single, 16 word write buffer) are sufficient to achieve reasonable performance under prefetching <ref> [4, 23] </ref>. Data forwarding using Forwarding Write operations offers the additional benefit of easing write buffer size requirements. Since Forwarding Write operations replace regular writes, write buffer size requirements should not increase significantly under forwarding.
Reference: 24. <author> Chen, T.-F. </author> <title> Data prefetching for high-performance processors. </title> <type> Ph.D dissertation, Technical Report 93-07-01, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: The prefetch and Forwarding Write operations assumed in this work most closely resemble the Dash Prefetch and Dash Deliver instructions. A prefetch instruction is similar to a regular load, except that it is non-blocking and is dropped on exceptions <ref> [23, 24] </ref>. A Forwarding Write is similar to a regular write, except that it has different cache coherence semantics (Section 3.2). Cache design to support prefetching and forwarding requires the ability to support multiple outstanding prefetches and the ability to accept incoming prefetched and forwarded data. <p> These capabilities are provided by lockup-free caches [25]; however, such full lockup-free capabilities are not (9) necessarily required. Cache design to support software-initiated prefetching has been discussed in detail in <ref> [23, 24] </ref>. The cache architectures described therein can be used for forwarding as well as prefetching [8]. <p> The memory system provides a one cycle cache hit latency; round trip network and main memory access delay result in a base 100 cycle cache miss latency under light network loads (this latency was chosen largely to facilitate comparison with the results of other prefetching studies <ref> [2, 3, 4, 24] </ref>). Uniform memory access (UMA) to main memory is assumed to simplify simulations.
Reference: 25. <author> Kroft, D. </author> <title> Lockup-free instruction fetch / prefetch cache organization. </title> <booktitle> Proc. 8th Annual International Symposium on Computer Architecture. </booktitle> <year> 1981, </year> <pages> pp. 81-87. </pages>
Reference-contexts: Cache design to support prefetching and forwarding requires the ability to support multiple outstanding prefetches and the ability to accept incoming prefetched and forwarded data. These capabilities are provided by lockup-free caches <ref> [25] </ref>; however, such full lockup-free capabilities are not (9) necessarily required. Cache design to support software-initiated prefetching has been discussed in detail in [23, 24]. The cache architectures described therein can be used for forwarding as well as prefetching [8].
Reference: 26. <author> Kruskal, C. P., and Snir, M. </author> <title> The performance of multistage interconnection networks for multiprocessors. </title> <journal> IEEE Trans. Comput. </journal> <volume> C-32, </volume> <month> 12 (December </month> <year> 1983), </year> <pages> 1091-1098. </pages>
Reference-contexts: Uniform memory access (UMA) to main memory is assumed to simplify simulations. The multistage networks (10) PE cache . . mem. . . . multistage . . . network PE cache mem. are composed of 2x2 switches which are of the output queue variety and which perform cut-through routing <ref> [26] </ref>. Each processor is capable of in-order single instruction issue and can retire single arithmetic results in one cycle. Forwarding Write instructions that specify multiple destination processors accomplish multicast or broadcast by issuing multiple point-to-point messages.
Reference: 27. <author> Gupta, A., Weber, W.-D., and Mowry, T. </author> <title> Reducing memory and traffic requirements for scalable directory-based cache coherence schemes. </title> <booktitle> Proc. 1990 International Conference on Parallel Processing. </booktitle> <year> 1990, </year> <pages> pp. </pages> <month> I-312-321. </month>
Reference-contexts: Processor caches allow multiple outstanding prefetching and forwarding operations that are non-blocking to the issuing processor; regular reads block the issuing processor. Cache coherence is implemented by using a three-state, directory-based invalidation protocol. Directories are distributed across the main memory modules, are full-mapped, and are organized as pointer caches <ref> [27] </ref> to reduce their size. 4.2 Application Codes The application codes studied in this work were selected from the Perfect Benchmarks (Table 1). The particular versions of the codes used in these experiments are parallel, Cedar Fortran versions of the applications [11] that contain parallel loops and vector statements.
Reference: 28. <author> Koufaty, D. A., et al. </author> <title> Data forwarding in scalable shared-memory multiprocessors. </title> <booktitle> Submitted to 1995 International Conference on Supercomputing. </booktitle>
Reference-contexts: The use of an alternative cost function in the hybrid scheme may also improve the performance of forwarding in smaller caches. For example, Koufaty et al. <ref> [28] </ref> propose a cost function based on the number of cache lines used by a destination processor between a Forwarding Write and its subsequent read on that processor. 7.
Reference: 29. <author> Hiranandani, S., Kennedy, K., and Tseng, C.-W. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Comm. ACM 35, </journal> <month> 8 (August </month> <year> 1992), </year> <pages> 66-80. </pages>
Reference-contexts: This type of analysis has been applied for generating explicit message passing within a single loop nest for forwarding in DOACROSS loops [17], as well as for compiling languages such as Fortran D for message passing machines <ref> [29] </ref>.

References-found: 29

