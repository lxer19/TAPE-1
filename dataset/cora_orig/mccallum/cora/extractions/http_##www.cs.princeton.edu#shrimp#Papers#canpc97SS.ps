URL: http://www.cs.princeton.edu/shrimp/Papers/canpc97SS.ps
Refering-URL: http://www.cs.princeton.edu/shrimp/html/papers_stack_12.html
Root-URL: http://www.cs.princeton.edu
Email: fsnd,dubnicki,felteng@cs.princeton.edu  
Title: Stream Sockets on SHRIMP  
Author: Stefanos N. Damianakis, Cezary Dubnicki, Edward W. Felten 
Address: Princeton NJ 08544, USA  
Affiliation: Department of Computer Science, Princeton University,  
Abstract: This paper describes an implementation of stream sockets for the SHRIMP multicomputer. SHRIMP supports protected, user-level data transfer, allows user-level code to perform its own buffer management, and separates data transfers from control transfers so that data transfers can be done without the interrupting the receiving node's CPU. Our sockets implementation exploits all of these features to provide high performance. End-to-end latency for 8 byte transfers is 11 microseconds, which is considerably lower than all previous implementations of the sockets interface. For large transfers, we obtain a bandwidth of 13.5 MBytes/sec, which is close to the hardware limit when the receiver must perform a copy. Further experiments with the public-domain benchmarks ttcp and netperf confirm the performance of our implementation. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Richard Alpert and Edward W. Felten. </author> <title> Design and Implementation of NX Message Passing Using SHRIMP Virtual Memory Mapped Communication. </title> <type> Technical Report TR-507-96, </type> <institution> Dept. of Computer Science, Princeton University, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: The SunRPC-compatible library achieves latency and bandwidth comparable to our sockets implementation. One layer of this RPC implementation implements a bidirectional stream which can be viewed as a specialized implementation of our sockets library. Alpert <ref> [1] </ref> describes an implementation of the NX/2 [20] message-passing library for SHRIMP; it also achieves latency similar to ours. 9 Conclusion Our sockets implementation was able to achieve very low latency by exploiting the features of SHRIMP. <p> As I/O buses and networks get faster, the effect of a memory copy will become more pronounced. We believe the next important step in improving sockets performance is to develop a general zero-copy implementation. Our sockets implementation, and implementations of other message-passing facilities on SHRIMP <ref> [1, 2] </ref> demonstrate that SHRIMP has met its goal of pro-viding flexible, very high performance communication. Acknowledgments We would like to thank Kai Li, Matt Blumrich, Liviu Iftode, Rob Shillner, and the rest of the SHRIMP Group at Princeton for their many useful suggestions that contributed to this work.
Reference: 2. <author> Angelos Bilas and Edward W. Felten. </author> <title> Fast RPC on the SHRIMP Virtual Memory Mapped Network Interface. </title> <type> Technical Report TR-512-96, </type> <institution> Dept. of Computer Science, Princeton University, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: Thus, every automatic update protocol does only one copy, on the receive side. 5.3 Caching Effects Our experience with SHRIMP shows that communication performance can be affected by the caching strategy used for send-buffer and receive-buffer memory; communication performance is better with write-through caching <ref> [2] </ref>. We do not want to change the caching strategy for the application's data, since that would probably degrade the application's performance. <p> Protection is provided between separate partitions, but not between processes within a partition. Since the application must build packet headers, message passing overhead is still hundreds of CPU instructions. Other high-performance networking libraries have been implemented on SHRIMP. Bilas <ref> [2] </ref> describes two implementations of remote procedure call, one that is compatible with SunRPC [14] and one that is not. The SunRPC-compatible library achieves latency and bandwidth comparable to our sockets implementation. <p> As I/O buses and networks get faster, the effect of a memory copy will become more pronounced. We believe the next important step in improving sockets performance is to develop a general zero-copy implementation. Our sockets implementation, and implementations of other message-passing facilities on SHRIMP <ref> [1, 2] </ref> demonstrate that SHRIMP has met its goal of pro-viding flexible, very high performance communication. Acknowledgments We would like to thank Kai Li, Matt Blumrich, Liviu Iftode, Rob Shillner, and the rest of the SHRIMP Group at Princeton for their many useful suggestions that contributed to this work.
Reference: 3. <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A Virtual Memory Mapped Network Interface for the Shrimp Multicomputer. </title> <booktitle> In Proceedings of 21th International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: In addition to the fast backplane interconnect, the PC nodes are connected by a commodity Ethernet, which is used for diagnostics, booting, and exchange of low-priority messages. The custom network interface <ref> [3, 4] </ref> is the system's key component. It connects each PC node to the routing backplane and implements the hardware support for VMMC. 3 Virtual Memory Mapped Communication Virtual memory-mapped communication (VMMC) addresses the need for a basic multicomputer communication mechanism with extremely low latency and high bandwidth.
Reference: 4. <author> Matthias A. Blumrich. </author> <title> Network Interface for Protected, </title> <type> User-Level Communication. PhD thesis, </type> <institution> Dept. of Computer Science, Princeton University, </institution> <month> June </month> <year> 1996. </year> <note> Available as technical report TR-522-96. </note>
Reference-contexts: In addition to the fast backplane interconnect, the PC nodes are connected by a commodity Ethernet, which is used for diagnostics, booting, and exchange of low-priority messages. The custom network interface <ref> [3, 4] </ref> is the system's key component. It connects each PC node to the routing backplane and implements the hardware support for VMMC. 3 Virtual Memory Mapped Communication Virtual memory-mapped communication (VMMC) addresses the need for a basic multicomputer communication mechanism with extremely low latency and high bandwidth.
Reference: 5. <author> Matthias A. Blumrich, Cezary Dubnicki, Edward W. Felten, and Kai Li. </author> <title> Protected, User-Level DMA for the SHRIMP Network Interface. </title> <booktitle> In Proc. 2nd Intl. Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 154-165, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Import and export requests are matched by a trusted daemon process. Second, at send initiation, the SHRIMP implementation uses the hardware virtual memory management unit (MMU) to verify permission to send data to a receive buffer. For more details please see <ref> [5] </ref>. 3.3 Transfer Strategies The VMMC model defines two user-level transfer strategies: deliberate update and automatic update. Deliberate update is an explicit transfer of data from a sender's memory to a receiver's memory. <p> These accesses specify the source address, destination address, and size of a transfer. The ordinary virtual memory protection mechanisms (MMU and page tables) are used to maintain protection <ref> [5] </ref>. The hardware requires that both source and destination addresses are word aligned. VMMC guarantees the in-order, reliable delivery of all data transfers. but does not include any buffer management since data is transferred directly between user-level address spaces.
Reference: 6. <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: U-Net [23] describes an architecture for user-level communications which is independent of the network interface hardware. Using Sun SparcStations and Fore Systems ATM interfaces, they measured a TCP one-way latency of 78 sec and a bandwidth of 14.4 MBytes/sec for 4 KByte packets. Boden et al. <ref> [6] </ref> describe an implementation of TCP/IP using the Myrinet API. This implementation had a minimum user-to-user latency of over 40 sec, which is considerably larger than ours.
Reference: 7. <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H.T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peterson, J. Susman, J. Sutton, J. Urbanski, and J. Webb. </author> <title> Supporting Systolic and Memory Communication in iWarp. </title> <booktitle> In Proceedings of 17th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: They have not tried to implement message-passing libraries using the underlying communication mechanism. Several projects have tried to lower overhead by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [7, 13, 8] </ref>. While this is efficient for fine-grain, low-latency communication, it requires the use of a non-standard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. The Connection Machine CM-5 implements user-level communication through memory-mapped network interface FIFOs [17, 12].
Reference: 8. <author> William J. Dally. </author> <title> The J-Machine System. </title> <editor> In P.H. Winston and S.A. Shellard, editors, </editor> <booktitle> Artificial Intelligence at MIT: Expanding Frontiers, </booktitle> <pages> pages 550-580. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: They have not tried to implement message-passing libraries using the underlying communication mechanism. Several projects have tried to lower overhead by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [7, 13, 8] </ref>. While this is efficient for fine-grain, low-latency communication, it requires the use of a non-standard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. The Connection Machine CM-5 implements user-level communication through memory-mapped network interface FIFOs [17, 12].
Reference: 9. <author> Stefanos Damianakis, Cezary Dubnicki, and Edward W. Felten. </author> <title> Stream Sockets on SHRIMP. </title> <type> Technical Report TR-513-96, </type> <institution> Dept. of Computer Science, Princeton University, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: Finally, our current implementation does not support asynchronous I/O to sockets. Doing so would require a straightforward use of SHRIMP notifications, but we have not implemented it yet. 8 Related Work We have previously presented this work in <ref> [9] </ref> and its results were summarized in [11]. Several groups have studied how to support the socket interface on experimental high-performance network interfaces. The most closely related work is by Rodrigues et al. [21] who have a user-level implementation of sockets using active messages [24].
Reference: 10. <author> Cezary Dubnicki, Liviu Iftode, Edward W. Felten, and Kai Li. </author> <title> Software Support for Virtual Memory-Mapped Communication. </title> <booktitle> In Proceedings of the IEEE 8th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: The network is a multicom-puter routing network [22] connected to the PC nodes via custom-designed network interfaces. The SHRIMP network interface closely cooperates with a thin layer of software to form a communication mechanism called virtual memory-mapped communication (VMMC) <ref> [10] </ref>. This mechanism delivers excellent performance and supports various message-passing packages and applications effectively [11]. The prototype system consists of four interconnected nodes.
Reference: 11. <author> Edward W. Felten, Richard Alpert, Angelos Bilas, Matthias A. Blumrich, Dou-glas W. Clark, Stefanos Damianakis, Cezary Dubnicki, Liviu Iftode, and Kai Li. </author> <title> Early Experience with Message-Passing on the Shrimp Multicomputer. </title> <booktitle> In Proceedings of 23th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: The SHRIMP network interface closely cooperates with a thin layer of software to form a communication mechanism called virtual memory-mapped communication (VMMC) [10]. This mechanism delivers excellent performance and supports various message-passing packages and applications effectively <ref> [11] </ref>. The prototype system consists of four interconnected nodes. Each node is a DEC 560ST PC containing an 60 MHz Intel Pentium processor with an external 256 Kbyte second-level cache, and 40 MBytes of main DRAM memory. <p> Finally, our current implementation does not support asynchronous I/O to sockets. Doing so would require a straightforward use of SHRIMP notifications, but we have not implemented it yet. 8 Related Work We have previously presented this work in [9] and its results were summarized in <ref> [11] </ref>. Several groups have studied how to support the socket interface on experimental high-performance network interfaces. The most closely related work is by Rodrigues et al. [21] who have a user-level implementation of sockets using active messages [24].
Reference: 12. <institution> FORE Systems. TCA-100 TURBOchannel ATM Computer Interface, </institution> <note> User's Manual, </note> <year> 1992. </year>
Reference-contexts: While this is efficient for fine-grain, low-latency communication, it requires the use of a non-standard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. The Connection Machine CM-5 implements user-level communication through memory-mapped network interface FIFOs <ref> [17, 12] </ref>. Protection is provided through the virtual memory system, which controls access to these FIFOs. However, there are a limited number of FIFOs, so they must be shared within a partition (subset of nodes), restricting the degree of multiprogramming.
Reference: 13. <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A Tightly-Coupled Processor-Network Interface. </title> <booktitle> In Proceedings of 5th International Conference on Architectur al Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 111-122, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: They have not tried to implement message-passing libraries using the underlying communication mechanism. Several projects have tried to lower overhead by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [7, 13, 8] </ref>. While this is efficient for fine-grain, low-latency communication, it requires the use of a non-standard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. The Connection Machine CM-5 implements user-level communication through memory-mapped network interface FIFOs [17, 12].
Reference: 14. <institution> Internet Request for Comments RFC 1057, Internet Network Working Group. </institution> <month> RPC: </month> <title> Remote Procedure Call Protocol Specification, </title> <type> Version 2, </type> <month> June </month> <year> 1988. </year>
Reference-contexts: Since the application must build packet headers, message passing overhead is still hundreds of CPU instructions. Other high-performance networking libraries have been implemented on SHRIMP. Bilas [2] describes two implementations of remote procedure call, one that is compatible with SunRPC <ref> [14] </ref> and one that is not. The SunRPC-compatible library achieves latency and bandwidth comparable to our sockets implementation. One layer of this RPC implementation implements a bidirectional stream which can be viewed as a specialized implementation of our sockets library.
Reference: 15. <author> Kimberly K. Keeton, Thomas E. Anderson, and David A. Patterson. </author> <title> LogP: The Case for Low-Overhead Local Area Networks. In Hot Interconnects III, </title> <month> August </month> <year> 1995. </year>
Reference-contexts: Our sockets library performs much better than all previous sockets implementations for small transfers, with an end-to-end latency of 11 microseconds for an 8-byte transfer. Small transfers are important because they are very common in many applications <ref> [15] </ref>. For large transfers our implementation obtains a bandwidth of 13.5 MBytes/sec, which is close to the hardware limit when the receiver must perform a copy. We achieve high performance by taking advantage of the features of the underlying SHRIMP architecture.
Reference: 16. <author> Samuel J. Le*er, Marshall Kirk McKusick, Michael J. Karels, and John S. Quar-terman. </author> <title> The Design and Implementation of the 4.3BSD Unix Operating System. </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Unlike signals, however, notifications are queued when blocked. 4 Shrimp Sockets The SHRIMP socket API is implemented as a user-level library, using the VMMC interface. It is compatible, and seamlessly integrated with the Unix stream sockets facility <ref> [16] </ref>. We introduce a new address family, AF SHRIMP, to support the new type of stream socket. We implement three variations of the socket library, two using deliberate update and one using automatic update.
Reference: 17. <author> C.E. Leiserson, Z.S. Abuhamdeh, D.C. Douglas, C.R. Feynman, M.N. Ganmukhi, J.V. Hill, D. Hillis, B.C. Kuszmaul, M.A. St. Pierre, D.S. Wells, M.C. Wong, S. Yang, and R. Zak. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of 4th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: While this is efficient for fine-grain, low-latency communication, it requires the use of a non-standard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. The Connection Machine CM-5 implements user-level communication through memory-mapped network interface FIFOs <ref> [17, 12] </ref>. Protection is provided through the virtual memory system, which controls access to these FIFOs. However, there are a limited number of FIFOs, so they must be shared within a partition (subset of nodes), restricting the degree of multiprogramming.
Reference: 18. <author> Chris Maeda and Brian N. Bershad. </author> <title> Protocol Service Decomposition for High-Performance Networking. </title> <booktitle> In Proceedings of 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 244-255, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: With fork, the problem is arbitrating socket access between the two resulting processes. exec is difficult because SHRIMP communicates through memory and exec allocates a new memory space for the process. Maeda and Bershad discuss how to implement fork and exec correctly in the presence of user-level networking software <ref> [18] </ref>. We intend to follow their solution. Our current implementation ignores all fcntl calls, passing them through to the underlying kernel socket. While this behavior is correct is some cases, in others it is not. We intend to implement a better fcntl that handles all of the fcntl directives correctly.
Reference: 19. <author> Scott Pakin, Mario Lauria, and Andrew Chien. </author> <title> High Performance Messaging on Workstations: Illinois Fast Message (FM) for Myrinet. </title> <booktitle> In Supercomputing '95, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: Note that the sender also does a copy from the source to the library's internal buffer but this copy is not counted because it acts as the actual data transfer. Finally, we use the following three metrics from <ref> [19] </ref> to characterize perfor mance: r 1 = peak bandwidth for infinitely large packets n 1 = packet size to achieve r 1 2 bandwidth l = one way packet latency 6.1 Micro-Benchmarks (a) Bandwidth (b) Latency Fig. 3. <p> To illustrate this, consider the following measurements of two APIs that are not compatible with sockets. Myricom's custom API using their interconnect results in a latency of 40 sec and a peak bandwidth of 27 MBytes/sec. Illinois Fast Messages (FM) <ref> [19] </ref> also implement a custom API using the Myrinet hardware. FM sacrifices some features available in the Myricom API and some peak bandwidth in order to reduce the small-message latency by a factor of almost two.
Reference: 20. <author> Paul Pierce. </author> <title> The NX/2 Operating System. </title> <booktitle> In Proceedings of Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 384-390, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: The SunRPC-compatible library achieves latency and bandwidth comparable to our sockets implementation. One layer of this RPC implementation implements a bidirectional stream which can be viewed as a specialized implementation of our sockets library. Alpert [1] describes an implementation of the NX/2 <ref> [20] </ref> message-passing library for SHRIMP; it also achieves latency similar to ours. 9 Conclusion Our sockets implementation was able to achieve very low latency by exploiting the features of SHRIMP. The freedom to use customized buffer management strategies allows us to design a very efficient implementation of bidirectional streams.
Reference: 21. <author> Steven H. Rodrigues, Thomas E. Anderson, and David E. Culler. </author> <title> High-Performance Local-Area Communication With Fast Sockets. </title> <booktitle> In Proc. of Winter 1997 USENIX Symposium, </booktitle> <month> January </month> <year> 1997. </year>
Reference-contexts: Several groups have studied how to support the socket interface on experimental high-performance network interfaces. The most closely related work is by Rodrigues et al. <ref> [21] </ref> who have a user-level implementation of sockets using active messages [24]. Their sockets can take advantage of receives that are posted before sends, which allows them a zero-copy implementation. They obtain similar bandwidth numbers, despite using different operating system, CPU, and networking hardware.
Reference: 22. <author> Roger Traylor and Dave Dunning. </author> <title> Routing Chip Set for Intel Paragon Parallel Supercomputer. </title> <booktitle> In Proceedings of Hot Chips '92 Symposium, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Each node is a Pentium PC running the Linux operating system. The network is a multicom-puter routing network <ref> [22] </ref> connected to the PC nodes via custom-designed network interfaces. The SHRIMP network interface closely cooperates with a thin layer of software to form a communication mechanism called virtual memory-mapped communication (VMMC) [10]. This mechanism delivers excellent performance and supports various message-passing packages and applications effectively [11]. <p> The caches snoop DMA transactions and automatically invalidate corresponding cache lines, thereby keeping consistent with all main memory updates, including those from EISA bus masters. The network connecting the nodes is an Intel routing backplane consisting of a two-dimensional mesh of Intel Mesh Routing Chips (iMRCs) <ref> [22] </ref>, and is the same network used in the Paragon multicomputer. The backplane supports deadlock-free, oblivious wormhole routing, and preserves the order of messages from each sender to each receiver.
Reference: 23. <author> Thorsten von Eicken, Anindya Basu, Vineet Buch, and Werner Vogels. </author> <month> U-Net: </month>
Reference-contexts: Their sockets can take advantage of receives that are posted before sends, which allows them a zero-copy implementation. They obtain similar bandwidth numbers, despite using different operating system, CPU, and networking hardware. Because of different network interface we obtain a significantly lower small message latency. U-Net <ref> [23] </ref> describes an architecture for user-level communications which is independent of the network interface hardware. Using Sun SparcStations and Fore Systems ATM interfaces, they measured a TCP one-way latency of 78 sec and a bandwidth of 14.4 MBytes/sec for 4 KByte packets.
References-found: 23

