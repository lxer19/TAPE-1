URL: ftp://ftp.cs.rochester.edu/pub/u/michael/ISCA97.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/michael/
Root-URL: 
Email: fmichael,scottg@cs.rochester.edu fashwini,bhlimg@watson.ibm.com  
Title: Coherence Controller Architectures for SMP-Based CC-NUMA Multiprocessors  
Author: Maged M. Michael Ashwini K. Nanda Beng-Hong Lim and Michael L. Scott 
Note: This work was supported and performed at IBM Thomas J. Watson Research Center. Michael Scott was supported in part by NSF grants (CDA9401142 and CCR 9319445). To appear in Proceedings of the 24th Anual In ternational Symposium on Computer Architecture (ISCA), Denver, CO, June 1997.  
Address: Rochester, NY 14627 Yorktown Heights, NY 10598  
Affiliation: University of Rochester IBM Research Department of Computer Science Thomas J. Watson Research Center  
Abstract: Scalable distributed shared-memory architectures rely on coherence controllers on each processing node to synthesize cache-coherent shared memory across the entire machine. The coherence controllers execute coherence protocol handlers that may be hardwired in custom hardware or programmed in a protocol processor within each coherence controller. Although custom hardware runs faster, a protocol processor allows the coherence protocol to be tailored to specific application needs and may shorten hardware development time. Previous research show that the increase in application execution time due to protocol processors over custom hardware is minimal. With the advent of SMP nodes and faster processors and networks, the tradeoff between custom hardware and protocol processors needs to be reexamined. This paper studies the performance of custom-hardware and protocol-processor-based coherence controllers in SMP-node-based CC-NUMA systems on applications from the SPLASH-2 suite. Using realistic parameters and detailed models of existing state-of-the-art system components, it shows that the occupancy of coherence controllers can limit the performance of applications with high communication requirements, where the execution time using protocol processors can be twice as long as using custom hardware. To gain a deeper understanding of the tradeoff, we investigate the effect of varying several architectural parameters that influence the communication characteristics of the applications and the underlying system on coherence controller performance. We identify measures of applications' communication requirements and their impact on the performance penalty of protocol processors, which can help system designers predict performance penalties for other applications. We also study the potential of improving the performance of hardware-based and protocol-processor-based coherence controllers by separating or duplicating critical components. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Bianchini, D. Chaiken, K. Johnson, D. Kranz, J. Kubiatowicz, B.-H. Lim, K. Mackenzie, and D. Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 213, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Previous research has shown convincingly that scalable shared-memory performance can be achieved on directory-based cache-coherent multiprocessors such as the Stanford DASH [6] and MIT Alewife <ref> [1] </ref> machines. A key component of these machines is the coherence controller on each node that provides cache coherent access to memory that is distributed among the nodes of the multiprocessor. <p> Note that memory and cache-to-cache data transfers drive the critical quad-word first on the bus to minimize latency. 2.2 Coherence Controller Architectures We consider two main coherence controller designs: a custom hardware coherence controller similar to that in the DASH [6] and Alewife <ref> [1] </ref> systems, and a coherence controller based on commodity protocol processors similar to those in the Typhoon [12] system and its prototypes [13]. The two designs share some common components and features (see Figures 2 and 3).
Reference: [2] <author> B. Falsafi, A. Lebeck, S. Reinhardt, I. Schoinas, M. Hill, J. Larus, and D. Wood. </author> <title> Application-Specific Protocols for User-Level Shared Memory. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Although a custom hardware design generally yields better performance than a protocol processor for a particular coherence protocol, the programmable nature of a protocol processor allows one to tailor the cache coherence protocol to the application <ref> [2, 8] </ref>, and may lead to shorter design times since protocol errors may be fixed in software. The study of the performance advantage of custom protocols is beyond the scope of this paper.
Reference: [3] <author> M. Heinrich, J. Kuskin, D. Ofelt, J. Heinlein, J. P. Singh, R. Simoni, K. Gharachorloo, J. Baxter, D. Nakahira, M. Horowitz, A. Gupta, M. Rosemblum, and J. Hennessy. </author> <title> The Performance Impact of Flexibility in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 274285, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Simulations of the Stanford FLASH, which uses a customized protocol processor optimized for handling coherence actions, show that the performance penalty of its protocol processor in comparison to custom hardware controllers is within 12% for most of their benchmarks <ref> [3] </ref>. Simulations of the Wiscon-sin Typhoon Simple-COMA system, which uses a protocol processor integrated with the other components of the coherence controller, also show competitive performance that is within 30% of custom-hardware CC-NUMA controllers [12] and within 20% of custom-hardware Simple-COMA controllers [13]. <p> The Stanford FLASH designers find that the performance penalty of using a protocol processor is less than 12% for the applications that they simulated, including Ocean and Radix <ref> [3] </ref>.
Reference: [4] <author> C. Holt, M. Heinrich, J. P. Singh, E. Rothberg, and J. Hen-nessy. </author> <title> The Effects of Latency, Occupancy, and Bandwidth in Distributed Shared Memory Multiprocessors. </title> <type> Technical Report CSL-TR-95-660, </type> <institution> Stanford University, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Other differences between the two studies are: a) they compare Simple COMA systems, while we compare CC-NUMA systems, b) they assume a slower network with a latency of 500 ns., which mitigates the penalty of protocol processors, and c) they consider only uniprocessor nodes. Holt et al. <ref> [4] </ref> perform a study similar to ours. They also find that the occupancy of coherence controllers is critical to the performance of high-bandwidth applications. However, their work uses abstract parameters to model coherence controller performance, whereas our work considers practical, state-of-the-art controller designs.
Reference: [5] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Instead of hardwiring protocol handlers, the Sun Microsystems S3.mp [10] multiprocessor uses hardware sequencers for modularity in implementing protocol handlers. Subsequent designs for scalable shared-memory multiprocessors, such as the Stanford FLASH <ref> [5] </ref> and the Wisconsin Typhoon machines [12], have touted the use of programmable protocol processors instead of custom hardware FSMs to implement the coherence protocols.
Reference: [6] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3):6379, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Previous research has shown convincingly that scalable shared-memory performance can be achieved on directory-based cache-coherent multiprocessors such as the Stanford DASH <ref> [6] </ref> and MIT Alewife [1] machines. A key component of these machines is the coherence controller on each node that provides cache coherent access to memory that is distributed among the nodes of the multiprocessor. <p> These latencies correspond to those of existing state-of-the-art components. Note that memory and cache-to-cache data transfers drive the critical quad-word first on the bus to minimize latency. 2.2 Coherence Controller Architectures We consider two main coherence controller designs: a custom hardware coherence controller similar to that in the DASH <ref> [6] </ref> and Alewife [1] systems, and a coherence controller based on commodity protocol processors similar to those in the Typhoon [12] system and its prototypes [13]. The two designs share some common components and features (see Figures 2 and 3).
Reference: [7] <author> T. Lovett and R. Clapp. STiNG: </author> <title> A CC-NUMA Computer System for the Commercial Marketplace. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 308317, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: We consider symmetric multiprocessor (SMP) nodes as well as uniprocessor nodes as the building block for a multiprocessor. The availability of cost-effective SMPs, such as those based on the Intel Pentium Pro [11] makes SMP nodes an attractive choice for CC-NUMA designers <ref> [7] </ref>. However, the added load presented to the coherence controller by multiple SMP processors may affect the choice between custom hardware FSMs and protocol processors. 1 We base our experimental evaluation of the alternative coher-ence controller architectures on realistic hardware parameters for state-of-the-art system components.
Reference: [8] <author> S. Mukherjee, S. Sharma, M. Hill, J. Larus, A. Rogers, and J. Saltz. </author> <title> Efficient Support for Irregular Applications on Distributed-Memory Machines. </title> <booktitle> In Proceedings of the Fifth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 6879, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Although a custom hardware design generally yields better performance than a protocol processor for a particular coherence protocol, the programmable nature of a protocol processor allows one to tailor the cache coherence protocol to the application <ref> [2, 8] </ref>, and may lead to shorter design times since protocol errors may be fixed in software. The study of the performance advantage of custom protocols is beyond the scope of this paper.
Reference: [9] <author> A.-T. Nguyen, M. Michael, A. Sharma, and J. Torrellas. </author> <title> The Augmint Multiprocessor Simulation Toolkit for Intel x86 Architectures. </title> <booktitle> In Proceedings of the 1996 IEEE International Conference on Computer Design, </booktitle> <pages> pages 486490, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: We start with the experimental methodology. 3.1 Experimental Methodology We use execution-driven simulation (based on a version of the Aug-mint simulation toolkit <ref> [9] </ref> that runs on the PowerPC architecture) to evaluate the performance of the four coherence controller designs, HWC, PPC, 2HWC, and 2PPC. Our simulator includes detailed contention models for SMP buses, memory controllers, interleaved memory banks, protocol engines, directory DRAM, and external point contention for the interconnection network.
Reference: [10] <author> A. Nowatzyk, G. Aybay, M. Browne, E. Kelly, M. Parkin, B. Radke, and S.Vishin. </author> <title> The S3.mp Scalable Shared Memory Multiprocessor. </title> <booktitle> In Proceedings of the 1995 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: In DASH and Alewife, the cache coherence protocol is hardwired in custom hardware finite state machines (FSMs) within the coherence controllers. Instead of hardwiring protocol handlers, the Sun Microsystems S3.mp <ref> [10] </ref> multiprocessor uses hardware sequencers for modularity in implementing protocol handlers. Subsequent designs for scalable shared-memory multiprocessors, such as the Stanford FLASH [5] and the Wisconsin Typhoon machines [12], have touted the use of programmable protocol processors instead of custom hardware FSMs to implement the coherence protocols. <p> We use the term protocol engine to refer to both the protocol processor in the PPC design and the protocol FSM in the HWC design. For distributing the protocol requests between the two engines, we use a policy similar to that used in the S3.mp system <ref> [10] </ref>, where protocol requests for memory addresses on the local node are handled by one protocol engine (LPE) and protocol requests for memory addresses on remote nodes are handled by the other protocol engine (RPE). Only the LPE needs to access the directory.
Reference: [11] <institution> Pentium Pro Family Developer's Manual. Intel Corporation, </institution> <year> 1996. </year>
Reference-contexts: We consider symmetric multiprocessor (SMP) nodes as well as uniprocessor nodes as the building block for a multiprocessor. The availability of cost-effective SMPs, such as those based on the Intel Pentium Pro <ref> [11] </ref> makes SMP nodes an attractive choice for CC-NUMA designers [7]. <p> The protocol processor access to the protocol dispatch controller 1 Although most processors use write-back caches, current processors (e.g. Pentium Pro <ref> [11] </ref>) allow users to designate regions of memory to be cached write-through. local and remote protocol processors (2PPC). register is read-only.
Reference: [12] <author> S. Reinhardt, J. Larus, and D. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Instead of hardwiring protocol handlers, the Sun Microsystems S3.mp [10] multiprocessor uses hardware sequencers for modularity in implementing protocol handlers. Subsequent designs for scalable shared-memory multiprocessors, such as the Stanford FLASH [5] and the Wisconsin Typhoon machines <ref> [12] </ref>, have touted the use of programmable protocol processors instead of custom hardware FSMs to implement the coherence protocols. <p> Simulations of the Wiscon-sin Typhoon Simple-COMA system, which uses a protocol processor integrated with the other components of the coherence controller, also show competitive performance that is within 30% of custom-hardware CC-NUMA controllers <ref> [12] </ref> and within 20% of custom-hardware Simple-COMA controllers [13]. Even so, the choice between custom hardware and protocol processors for implementing coherence protocols remains a key design issue for scalable shared-memory multiprocessors. <p> quad-word first on the bus to minimize latency. 2.2 Coherence Controller Architectures We consider two main coherence controller designs: a custom hardware coherence controller similar to that in the DASH [6] and Alewife [1] systems, and a coherence controller based on commodity protocol processors similar to those in the Typhoon <ref> [12] </ref> system and its prototypes [13]. The two designs share some common components and features (see Figures 2 and 3). Both designs use duplicate directories to allow fast response to common requests on the pipelined SMP bus (one directory lookup per 2 bus cycles). <p> In <ref> [12] </ref>, Reinhardt et al. introduce the Wisconsin Typhoon architecture that relies on a SPARC processor core integrated with the other components of the coherence controller to execute coherence handlers that implement a Simple COMA protocol.
Reference: [13] <author> S. Reinhardt, R. Pfile, and D. Wood. </author> <title> Decoupled Hardware Support for Distributed Shared Memory. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 3443, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Simulations of the Wiscon-sin Typhoon Simple-COMA system, which uses a protocol processor integrated with the other components of the coherence controller, also show competitive performance that is within 30% of custom-hardware CC-NUMA controllers [12] and within 20% of custom-hardware Simple-COMA controllers <ref> [13] </ref>. Even so, the choice between custom hardware and protocol processors for implementing coherence protocols remains a key design issue for scalable shared-memory multiprocessors. The goal of this research is to examine in detail the performance tradeoffs between these two alternatives in designing a CC-NUMA multiprocessor coherence controller. <p> to minimize latency. 2.2 Coherence Controller Architectures We consider two main coherence controller designs: a custom hardware coherence controller similar to that in the DASH [6] and Alewife [1] systems, and a coherence controller based on commodity protocol processors similar to those in the Typhoon [12] system and its prototypes <ref> [13] </ref>. The two designs share some common components and features (see Figures 2 and 3). Both designs use duplicate directories to allow fast response to common requests on the pipelined SMP bus (one directory lookup per 2 bus cycles). <p> The relative increase in latency from HWC to PPC is only 49%, which is consistent with the 33% increase reported for Typhoon <ref> [13] </ref>, taking into account that we consider a more decoupled coherence controller design and we use a faster network than that used in the Typhoon study. It is worth noting that in Table 3 there is no entry for updating the directory state at the home node. <p> It is hard to compare our results to theirs because of the difficulty in determining what fraction of the performance difference is due to Simple COMA vs. CC-NUMA, and what fraction is due to custom hardware vs. protocol processors. In <ref> [13] </ref>, Reinhardt et al. compare the Wisconsin Typhoon and its first-generation prototypes with an idealized Simple COMA system. Here, their results show that the performance penalty of using integrated protocol processors is less than 20%. In contrast, we find larger performance penalties of up to 106%.
Reference: [14] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2436, </pages> <month> June </month> <year> 1995. </year> <month> 10 </month>
Reference-contexts: What distinguishes our work from previous research is that we consider commodity protocol processors on SMP-based CC-NUMA and a wider range of architectural parameters. We simulate eight applications from the SPLASH-2 benchmark suite <ref> [14] </ref> to compare the application performance of the architectures. The results show that for a 64-processor system based on four-processor SMP nodes, protocol processors result in a performance penalty (increase in execution time relative to that of custom hardware controllers) of 4% 93%. <p> All coherence controller implementations use the same cache coherence protocol. We use eight benchmarks from the SPLASH-2 suite <ref> [14] </ref>, (Table 5) to evaluate the performance of the four coherence controller implementations. All the benchmarks are written in C and compiled using IBM XLC C compiler with optimization level -O2. All experimental results reported in this paper are for the parallel phase only of these applications. <p> Radix sort 1M integer keys, radix 1K FFT FFT computation 64K complex doubles Ocean Study of ocean movements 258fi258 ocean grid Table 5: Benchmark types and data sets. processor systems (8 nodes fi 4 processors each) as they suffer from load imbalance on 64 processors with the data sets used <ref> [14] </ref>. <p> We notice a significant increase in execution time (regardless of the coherence controller architecture) relative to the corresponding execution times on the base system, for FFT, Cholesky, Radix, and LU, which have high spatial locality <ref> [14] </ref>, and a minor increase in execution time for the other benchmarks. <p> However, the PP penalty can be as high as 79% (for Ocean) even on systems with one processor per node. 3 Applications like Radix maintain a constant communication rate with different data sizes <ref> [14] </ref>. byte) cache lines. network latency. large data sizes. 6 For each of the coherence controller architectures, performance of applications with high communication rates degrades with more processors per node, due to the increase in occupancy per coherence controller, which are already critical resources on systems with fewer processors per node. <p> Note that the high queuing delay for FFT is attributed to its bursty communication pattern <ref> [14] </ref>.
References-found: 14

