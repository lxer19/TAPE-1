URL: http://cobar.cs.umass.edu/pubfiles/UM-CS-1994-073.ps
Refering-URL: http://cobar.cs.umass.edu/pubfiles/
Root-URL: 
Email: Email: brown@cs.umass.edu.  
Title: An Approach for Improving Execution Performance in Inference Network Based Information Retrieval  
Author: Eric W. Brown 
Note: This work is supported by the National Science Foundation  
Address: Amherst, MA 01003 USA  Massachusetts.  
Affiliation: Department of Computer Science University of Massachusetts  Center for Intelligent Information Retrieval at the University of  
Abstract: Technical Report 94-73 September 1994 Abstract The inference network retrieval model provides the ability to combine a variety of retrieval strategies expressed in a rich query language. While this power yields impressive retrieval effectiveness, it also presents barriers to the incorporation of traditional optimization techniques intended to improve the execution efficiency, or speed, of retrieval. The essence of these optimization techniques is the ability to identify the indexing information that will be most useful for evaluating a query, combined with the ability to access from disk just that information. I survey a variety of techniques intended to identify useful indexing information and propose a new strategy for incorporating these techniques into the inference network retrieval model. Additionally, I describe an architecture based on a persistent object store that provides sophisticated management of the indexing data, allowing just the desired indexing data to be accessed from disk during retrieval. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. C. Bell, A. Moffat, C. G. Nevill-Manning, I. H. Witten, and J. Zobel. </author> <title> Data compression in full-text retrieval systems. </title> <journal> J. Amer. Soc. Inf. Sci., </journal> <volume> 44(9) </volume> <pages> 508-531, </pages> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: In the second case above where the decompression is more expensive than the read, ROIO will always suffer, regardless of the contribution of E, since it would have been cheaper to read in an uncompressed version. Compression techniques for inverted lists have received a fair amount of attention <ref> [58, 36, 30, 1, 5, 63] </ref>. I do not claim anything novel with respect to compression. Rather, for completeness I merely describe how it fits into my optimization strategy and give a necessary condition for providing benefit with respect to execution performance.
Reference: [2] <author> A. Biliris. </author> <title> An efficient database storage structure for large dynamic objects. </title> <booktitle> In Proc. 8th IEEE Inter. Conf. on Data Engineering, </booktitle> <pages> pages 301-308, </pages> <address> Tempe, AZ, </address> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: This scheme is intended to provide efficient sequential access to large objects, assuming they are typically read or written in their entirety. Unfortunately, this focus is inappropriate for the optimization techniques proposed here where fine grained access to arbitrary locations in a long list is required. Biliris <ref> [2] </ref> describes an object store which supports large objects using a combination of techniques from EXODUS and Starburst. A B+tree is used to index variable length segments allocated from disk pages managed by a buddy system.
Reference: [3] <author> A. Biliris. </author> <title> The performance of three database storage structures for managing large objects. </title> <booktitle> In Proc. of the ACM SIGMOD Inter. Conf. on Management of Data, </booktitle> <pages> pages 276-285, </pages> <address> San Diego, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: A B+tree is used to index variable length segments allocated from disk pages managed by a buddy system. This scheme provides the update characteristics of EXODUS with the sequential access characteristics of Starburst. A comparative performance evaluation of the three schemes can be found in <ref> [3] </ref>. Again, this scheme does not satisfy my requirements due to the lack of fine grained access based on content into a large object. The unsafe query optimization techniques have their roots in the upper bound optimizations used to solve the nearest neighbor problem in information retrieval.
Reference: [4] <author> D. C. Blair. </author> <title> An extended relational document retrieval model. </title> <journal> Inf. Process. & Mgmnt., </journal> <volume> 24(3) </volume> <pages> 349-371, </pages> <year> 1988. </year>
Reference-contexts: Some of the earliest work was done by 25 Crawford and MacLeod [13, 33, 12, 34], who describe how to use a relational database management system (RDBMS) to store document data and construct information retrieval queries. Similar work was presented more recently by Blair <ref> [4] </ref> and Grossman and Driscoll [23]. Others have chosen to extend the relational model to allow better support for IR. Lynch and Stonebraker [32] show how a relational model extended with abstract data types can be used to better support the queries that are typical of an IR system.
Reference: [5] <author> A. Bookstein, S. T. Klein, and D. A. Ziff. </author> <title> A systematic approach to compressing a full-text retrieval system. </title> <journal> Inf. Process. & Mgmnt., </journal> <volume> 28(6) </volume> <pages> 795-806, </pages> <year> 1992. </year>
Reference-contexts: In the second case above where the decompression is more expensive than the read, ROIO will always suffer, regardless of the contribution of E, since it would have been cheaper to read in an uncompressed version. Compression techniques for inverted lists have received a fair amount of attention <ref> [58, 36, 30, 1, 5, 63] </ref>. I do not claim anything novel with respect to compression. Rather, for completeness I merely describe how it fits into my optimization strategy and give a necessary condition for providing benefit with respect to execution performance.
Reference: [6] <author> E. W. Brown, J. P. Callan, and W. B. Croft. </author> <title> Fast incremental indexing for full-text information retrieval. </title> <booktitle> In Proc. of the 20th Inter. Conf. on VLDB, </booktitle> <pages> pages 192-202, </pages> <address> Santiago, </address> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: These modifications involve appending information for the new documents to the existing inverted lists, requiring the ability to grow inverted lists. This process was described by Brown et al. in <ref> [6] </ref> using an inverted file structure similar to that described here. The main extensions made here are the separation of weight and location information in long inverted lists and the use of directories into the long list objects rather than chaining long list objects in a linked list.
Reference: [7] <author> E. W. Brown, J. P. Callan, W. B. Croft, and J. E. B. Moss. </author> <title> Supporting full-text information retrieval with a persistent object store. </title> <booktitle> In Proc. of the 4th Inter. Conf. on Extending Database Technology, </booktitle> <pages> pages 365-378, </pages> <address> Cambridge, UK, </address> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: In this case, there will be terms that are common to a large number of queries, even across multiple users. The upshot of this is that caching inverted lists in main memory should prove beneficial. 3.2 The Integrated Architecture The initial integration is fully discussed and evaluated in <ref> [7] </ref>. Here I merely summarize the architecture and highlight its performance. The Mneme version of the inverted index was created by allocating a Mneme object for each inverted list in the inverted file. The inverted lists were divided into three distinct groups.
Reference: [8] <author> C. Buckley and A. F. Lewit. </author> <title> Optimization of inverted vector searches. </title> <booktitle> In Proc. of the 8th Inter. ACM SIGIR Conf. on Res. and Develop. in Infor. Retr., </booktitle> <pages> pages 97-110, </pages> <month> June </month> <year> 1985. </year> <month> 29 </month>
Reference-contexts: This strategy will cause the terms to be processed in order of inverted list length, from shortest to longest. The first stopping condition we will consider was originally described by Buckley and Lewit <ref> [8] </ref> and later discussed by Lucarella [31]. It is intended to eliminate processing of entire inverted lists, and is similar to the third stopping condition described in Section 4.3.1. Assume that we are to return the top n documents to 18 the user.
Reference: [9] <author> J. P. Callan, W. B. Croft, and S. M. Harding. </author> <title> The INQUERY retrieval system. </title> <booktitle> In Proc. of the 3rd Inter. Conf. on Database and Expert Sys. Apps., </booktitle> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: I will show that superior performance can be achieved with a persistent object store and that the functionality provided by the persistent object store is necessary to support the optimization techniques I will pursue. I will integrate the INQUERY inference network based document retrieval system <ref> [9] </ref> with the Mneme persistent object store [40] and demonstrate the execution performance improvements possible with my approach. A proper investigation of execution efficiency requires representative document collections with which to evaluate suggested optimization techniques.
Reference: [10] <author> M. J. Carey, D. J. DeWitt, J. E. Richardson, and E. J. Shekita. </author> <title> Object and file management in the EXODUS extensible database system. </title> <booktitle> In Proc. of the 12th Inter. Conf. on VLDB, </booktitle> <pages> pages 91-100, </pages> <address> Kyoto, </address> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: They still must read the entire list from disk. My inverted list structure is more general, allowing fine grained access of a long inverted list at the disk level. Generic support for storage of large objects has been pursued in the database community. The EXODUS storage manager <ref> [10] </ref> supports large objects by storing them in one or more fixed size pages indexed by a B+tree on byte address.
Reference: [11] <author> C. W. Cleverdon, J. Mills, and E. M. Keen. </author> <title> Factors determining the performance of indexing systems, </title> <booktitle> vol. 1: Design, </booktitle> <volume> vol. </volume> <month> 2: </month> <title> Test results. </title> <institution> Aslib Cranfield Research Project, </institution> <address> Cranfield, England, </address> <year> 1966. </year>
Reference-contexts: This, however, requires relevance scores for queries and collections, which can only be obtained through subjective, human interaction. Historically, large, realistic collections and queries with proper relevance scores have not been available to researchers. The standard document collections used for IR system evaluation include Cranfield <ref> [11] </ref>, CACM [21], and NPL [52]. All of these collections are less than 10 Mbytes of raw text and are considered tiny compared to current and anticipated IR system requirements. These small research collections simply do not challenge the physical capacities of today's modern computers.
Reference: [12] <author> R. G. Crawford. </author> <title> The relational model in information retrieval. </title> <journal> J. Amer. Soc. Inf. Sci., </journal> <volume> 32(1) </volume> <pages> 51-64, </pages> <year> 1981. </year>
Reference-contexts: Some of the earliest work was done by 25 Crawford and MacLeod <ref> [13, 33, 12, 34] </ref>, who describe how to use a relational database management system (RDBMS) to store document data and construct information retrieval queries. Similar work was presented more recently by Blair [4] and Grossman and Driscoll [23].
Reference: [13] <author> R. G. Crawford and I. A. MacLeod. </author> <title> A relational approach to modular information retrieval systems design. </title> <booktitle> In Proc. of the 41st Conf. of the Amer. </booktitle> <institution> Soc. for Inf. Sci., </institution> <year> 1978. </year>
Reference-contexts: Some of the earliest work was done by 25 Crawford and MacLeod <ref> [13, 33, 12, 34] </ref>, who describe how to use a relational database management system (RDBMS) to store document data and construct information retrieval queries. Similar work was presented more recently by Blair [4] and Grossman and Driscoll [23].
Reference: [14] <author> W. B. Croft. </author> <title> Document representation in probabilistic models of information retrieval. </title> <journal> J. Amer. Soc. Inf. Sci., </journal> <volume> 32(6) </volume> <pages> 451-457, </pages> <month> Nov. </month> <year> 1981. </year>
Reference-contexts: Rather than make such extreme judgments, we would prefer to use a finer granularity when expressing the degree to which a term should be assigned to a document. This was accomplished by Croft <ref> [14, 15] </ref> who expressed this degree as the probability of a term being assigned to a document, P (x i = 1 j d), such that documents should now be ranked by the expected value of Equation 1, or g (x) = i P (x i = 1 j d) C
Reference: [15] <author> W. B. Croft. </author> <title> Experiments with representation in a document retrieval system. </title> <journal> Inf. Tech.: Res. Dev., </journal> <volume> 2(1) </volume> <pages> 1-21, </pages> <year> 1983. </year>
Reference-contexts: Rather than make such extreme judgments, we would prefer to use a finer granularity when expressing the degree to which a term should be assigned to a document. This was accomplished by Croft <ref> [14, 15] </ref> who expressed this degree as the probability of a term being assigned to a document, P (x i = 1 j d), such that documents should now be ranked by the expected value of Equation 1, or g (x) = i P (x i = 1 j d) C
Reference: [16] <author> W. B. Croft and D. J. Harper. </author> <title> Using probabilistic models of document retrieval without relevance information. </title> <journal> J. Documentation, </journal> <volume> 35(4) </volume> <pages> 285-295, </pages> <month> Dec. </month> <year> 1979. </year>
Reference-contexts: The distribution of query terms in the relevant and non-relevant documents in this sample is then used to estimate p i and q i , and the query is re-evaluated probabilistically. Croft and Harper <ref> [16] </ref> showed how the probabilistic model could also be used for the initial search.
Reference: [17] <author> J. S. Deogun and V. V. Raghavan. </author> <title> Integration of information retrieval and database management systems. </title> <journal> Inf. Process. & Mgmnt., </journal> <volume> 24(3) </volume> <pages> 303-313, </pages> <year> 1988. </year>
Reference-contexts: Moreover, the functionality requirements imposed by my inverted list implementation are unlikely to be adequately supported in an RDBMS. Other work in this area has attempted to integrate information retrieval with database management <ref> [17, 47] </ref>. The services provided by a database management system (DBMS) and an IR system are distinct but complementary, making an integrated system very attractive. The integrated architecture consists of a DBMS component and a custom IR system component.
Reference: [18] <author> C. Faloutsos. </author> <title> Access methods for text. </title> <journal> ACM Comput. Surv., </journal> <volume> 17 </volume> <pages> 50-74, </pages> <year> 1985. </year>
Reference-contexts: Given that retrieval is driven by evaluating the inference network for the documents that contain the query terms, the logical choice of index to support this process is an inverted file <ref> [46, 18, 26] </ref>. An inverted file index consists of a record, or inverted list, for each term that appears in the document collection. A term's inverted list stores a document identifier and weight for every document in which the term appears. <p> However, I am not concerned with integrated support for more traditional data, and my emphasis is on providing data management functionality to support I/O related query optimization. Efficient management of full-text database indexes has received a fair amount of attention. Faloutsos <ref> [18] </ref> gives an early survey of the common indexing techniques. Zobel et al. [64] investigate the efficient implementation of an inverted file index for a full-text database system. Their focus is on compression techniques to limit the size of the inverted file index.
Reference: [19] <author> C. Faloutsos and H. V. Jagadish. </author> <title> Hybrid index organizations for text databases. </title> <booktitle> In Proc. of the 3rd Inter. Conf. on Extending Database Technology, </booktitle> <pages> pages 310-327, </pages> <year> 1992. </year>
Reference-contexts: My proposed work can be considered an extension of these more traditional inverted list implementations, which simply do not provide the functionality required by the query processing optimizations I am considering. A more sophisticated inverted list implementation was proposed by Faloutsos and Jagadish <ref> [19] </ref>. In their scheme, small lists are stored as inverted lists, while large lists are stored as signature files. They have a similar goal of reducing the processing costs for long inverted lists, but their solution is inappropriate for the inference network model.
Reference: [20] <author> C. Faloutsos and H. V. Jagadish. </author> <title> On b-tree indices for skewed distributions. </title> <booktitle> In Proc. of the 18th Inter. Conf. on VLDB, </booktitle> <pages> pages 363-374, </pages> <address> Vancouver, </address> <year> 1992. </year>
Reference-contexts: In their scheme, small lists are stored as inverted lists, while large lists are stored as signature files. They have a similar goal of reducing the processing costs for long inverted lists, but their solution is inappropriate for the inference network model. In <ref> [20] </ref>, Faloutsos and Jagadish examine storage and update costs for a family of long inverted list implementations.
Reference: [21] <author> E. A. Fox. </author> <title> Characterization of two new experimental collections in computer and information science containing textual and bibliographic concepts. </title> <type> Technical Report 83-561, </type> <institution> Cornell University, </institution> <address> Ithaca, NY, </address> <month> Sept. </month> <year> 1983. </year>
Reference-contexts: This, however, requires relevance scores for queries and collections, which can only be obtained through subjective, human interaction. Historically, large, realistic collections and queries with proper relevance scores have not been available to researchers. The standard document collections used for IR system evaluation include Cranfield [11], CACM <ref> [21] </ref>, and NPL [52]. All of these collections are less than 10 Mbytes of raw text and are considered tiny compared to current and anticipated IR system requirements. These small research collections simply do not challenge the physical capacities of today's modern computers.
Reference: [22] <author> J. Gray and A. Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: This strategy has a number of attractive features. First, it attempts to reduce the most expensive operation in the systemretrieving data from disk. Disk reads are six orders of magnitude slower than main memory reads <ref> [22] </ref>. Reducing the amount of I/O that must be performed should produce a significant reduction in execution time. Second, a reduction in I/O during query processing will inevitably lead to a reduction in computation (in the absence of compression).
Reference: [23] <author> D. A. Grossman and J. R. Driscoll. </author> <title> Structuring text within a relational system. </title> <booktitle> In Proc. of the 3rd Inter. Conf. on Database and Expert Sys. Apps., </booktitle> <pages> pages 72-77, </pages> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: Some of the earliest work was done by 25 Crawford and MacLeod [13, 33, 12, 34], who describe how to use a relational database management system (RDBMS) to store document data and construct information retrieval queries. Similar work was presented more recently by Blair [4] and Grossman and Driscoll <ref> [23] </ref>. Others have chosen to extend the relational model to allow better support for IR. Lynch and Stonebraker [32] show how a relational model extended with abstract data types can be used to better support the queries that are typical of an IR system.
Reference: [24] <editor> D. Harman, editor. </editor> <booktitle> The Second Text REtrieval Conference (TREC2), </booktitle> <address> Gaithersburg, MD, </address> <year> 1994. </year> <institution> National Institute of Standards and Technology Special Publication 500-215. </institution>
Reference-contexts: Fortunately, just within the last few years, larger and more realistic collections have become available to IR researchers. For example, the TREC evaluations <ref> [24] </ref> currently involve 3 Gbytes of raw text and provide queries and relevance scores for retrieval and routing, allowing IR system evaluations and comparisons on relatively large document collections.
Reference: [25] <author> D. Harman and G. Candela. </author> <title> Retrieving records from a gigabyte of text on a minicomputer using statistical ranking. </title> <journal> J. Amer. Soc. Inf. Sci., </journal> <volume> 41(8) </volume> <pages> 581-589, </pages> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: There are three variations on this stopping condition, all of which are similar to the last stopping condition described in Section 4.3.1. The first variation was proposed by Harman and Candela <ref> [25] </ref>, called pruning. Rather than place a limit on the number of documents returned to the user, we can establish an insertion threshold for placing new documents in the candidate set. <p> While they describe linked list implementations for long inverted lists, they do not consider a scheme which supports the fine granularity of access into a long inverted list required by the optimizations considered here. Harman and Candela <ref> [25] </ref> use linked lists for a temporary inverted file created during indexing. However, their linked list nodes are quite small, consisting only of a single document posting.
Reference: [26] <author> D. Harman, E. Fox, R. Baeza-Yates, and W. Lee. </author> <title> Inverted files. </title> <editor> In W. B. Frakes and R. Baeza-Yates, editors, </editor> <booktitle> Information Retrieval: Data Structures & Algorithms, chapter 3, </booktitle> <pages> pages 28-43. </pages> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: Given that retrieval is driven by evaluating the inference network for the documents that contain the query terms, the logical choice of index to support this process is an inverted file <ref> [46, 18, 26] </ref>. An inverted file index consists of a record, or inverted list, for each term that appears in the document collection. A term's inverted list stores a document identifier and weight for every document in which the term appears.
Reference: [27] <author> D. Knaus and P. Schauble. </author> <title> Effective and efficient retrieval from large and dynamic document collections. </title> <booktitle> In Harman [24], </booktitle> <pages> pages 163-170. </pages>
Reference-contexts: The process of identifying a candidate document set followed by evaluating the query for just those documents is similar in spirit to the two stage query evaluation strategy of the SPIDER information retrieval system <ref> [48, 27] </ref>. In SPIDER, a signature file is used to identify documents that potentially match the query, and an upper bound is calculated for each document's similarity to the query.
Reference: [28] <author> C. Lamb, G. Landis, J. Orenstein, and D. Weinreb. </author> <title> The ObjectStore database system. </title> <journal> Commun. ACM, </journal> <volume> 34(10) </volume> <pages> 50-63, </pages> <month> Oct. </month> <year> 1991. </year> <month> 30 </month>
Reference-contexts: Resident objects are quickly located using the resident object table, and non-resident objects are faulted in with little additional processing. This can be contrasted with page mapping architectures of other object stores <ref> [28, 50] </ref> which have a fairly high penalty for accessing a non-resident object. These systems are optimized for localized processing of a large number of small objects, where the cost of faulting a page of objects can be amortized over many access to the objects in the page.
Reference: [29] <author> T. J. Lehman and B. G. Lindsay. </author> <title> The starburst long field manager. </title> <booktitle> In Proc. of the 15th Inter. Conf. on VLDB, </booktitle> <pages> pages 375-383, </pages> <address> Amsterdam, </address> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: The difference is that my directory structure provides customized access to individual pages based on the contents of the page (i.e., document identifier), rather than a byte offset. The Starburst long field manager <ref> [29] </ref> supports large objects using a sequence of variable length segments indexed by a descriptor. As an object grows, a newly allocated segment will be twice as large as the previously allocated segment.
Reference: [30] <author> G. Linoff and C. Stanfill. </author> <title> Compression of indexes with full positional information in very large text databases. </title> <booktitle> In Proc. of the 16th Inter. ACM SIGIR Conf. on Res. and Develop. in Infor. Retr., </booktitle> <pages> pages 88-95, </pages> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: In the second case above where the decompression is more expensive than the read, ROIO will always suffer, regardless of the contribution of E, since it would have been cheaper to read in an uncompressed version. Compression techniques for inverted lists have received a fair amount of attention <ref> [58, 36, 30, 1, 5, 63] </ref>. I do not claim anything novel with respect to compression. Rather, for completeness I merely describe how it fits into my optimization strategy and give a necessary condition for providing benefit with respect to execution performance.
Reference: [31] <author> D. Lucarella. </author> <title> A document retrieval system based on nearest neighbour searching. </title> <journal> J. Inf. Sci., </journal> <volume> 14(1) </volume> <pages> 25-33, </pages> <year> 1988. </year>
Reference-contexts: This strategy will cause the terms to be processed in order of inverted list length, from shortest to longest. The first stopping condition we will consider was originally described by Buckley and Lewit [8] and later discussed by Lucarella <ref> [31] </ref>. It is intended to eliminate processing of entire inverted lists, and is similar to the third stopping condition described in Section 4.3.1. Assume that we are to return the top n documents to 18 the user.
Reference: [32] <author> C. A. Lynch and M. Stonebraker. </author> <title> Extended user-defined indexing with application to textual databases. </title> <booktitle> In Proc. of the 14th Inter. Conf. on VLDB, </booktitle> <pages> pages 306-317, </pages> <year> 1988. </year>
Reference-contexts: Similar work was presented more recently by Blair [4] and Grossman and Driscoll [23]. Others have chosen to extend the relational model to allow better support for IR. Lynch and Stonebraker <ref> [32] </ref> show how a relational model extended with abstract data types can be used to better support the queries that are typical of an IR system.
Reference: [33] <author> I. A. MacLeod. </author> <title> SEQUEL as a language for document retrieval. </title> <journal> J. Amer. Soc. Inf. Sci., </journal> <volume> 30(5) </volume> <pages> 243-249, </pages> <year> 1979. </year>
Reference-contexts: Some of the earliest work was done by 25 Crawford and MacLeod <ref> [13, 33, 12, 34] </ref>, who describe how to use a relational database management system (RDBMS) to store document data and construct information retrieval queries. Similar work was presented more recently by Blair [4] and Grossman and Driscoll [23].
Reference: [34] <author> I. A. MacLeod and R. G. Crawford. </author> <title> Document retrieval as a database application. </title> <journal> Inf. Tech.: Res. Dev., </journal> <volume> 2(1) </volume> <pages> 43-60, </pages> <year> 1983. </year>
Reference-contexts: Some of the earliest work was done by 25 Crawford and MacLeod <ref> [13, 33, 12, 34] </ref>, who describe how to use a relational database management system (RDBMS) to store document data and construct information retrieval queries. Similar work was presented more recently by Blair [4] and Grossman and Driscoll [23].
Reference: [35] <author> M. E. Maron and J. L. Kuhns. </author> <title> On relevance, probabilistic indexing and information retrieval. </title> <journal> J. ACM, </journal> <volume> 7(3) </volume> <pages> 216-244, </pages> <month> July </month> <year> 1960. </year>
Reference-contexts: This review moves quickly, so fasten your seat-belt. Following that, I will give a more detailed discussion of the Bayesian inference network model as implemented by INQUERY. 2.1.1 Probabilistic Retrieval The probabilistic retrieval model was first suggested in 1960 by Maron and Kuhns <ref> [35] </ref>. The basic idea is to rank the documents in a collection based on their probability of being relevant to the current information need. This is expressed as P (relevant j d), or the probability that the information need is met given document d.
Reference: [36] <author> A. Moffat and J. Zobel. </author> <title> Compression and fast indexing for multi-gigabyte text databases. </title> <journal> Australian Comput. J., </journal> <volume> 26(1) </volume> <pages> 1-9, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: In the second case above where the decompression is more expensive than the read, ROIO will always suffer, regardless of the contribution of E, since it would have been cheaper to read in an uncompressed version. Compression techniques for inverted lists have received a fair amount of attention <ref> [58, 36, 30, 1, 5, 63] </ref>. I do not claim anything novel with respect to compression. Rather, for completeness I merely describe how it fits into my optimization strategy and give a necessary condition for providing benefit with respect to execution performance.
Reference: [37] <author> A. Moffat and J. Zobel. </author> <title> Fast ranking in limited space. </title> <booktitle> In Proc. 10th IEEE Inter. Conf. on Data Engineering, </booktitle> <pages> pages 428-437, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: This scheme can make no guarantees about the membership of the set. It does, however, calculate complete scores for the documents in the candidate set, guaranteeing a correct relative ranking. The second variation was proposed by Moffat and Zobel <ref> [39, 37, 38] </ref>. Rather than use an insertion threshold related to a term's potential score contribution, a hard limit is placed on the size of the candidate document set. The disjunctive phase proceeds until the candidate set is full.
Reference: [38] <author> A. Moffat and J. Zobel. </author> <title> Self-indexing inverted files. </title> <booktitle> In Proc. Australasian Database Conf., </booktitle> <address> Christchurch, New Zealand, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: This scheme can make no guarantees about the membership of the set. It does, however, calculate complete scores for the documents in the candidate set, guaranteeing a correct relative ranking. The second variation was proposed by Moffat and Zobel <ref> [39, 37, 38] </ref>. Rather than use an insertion threshold related to a term's potential score contribution, a hard limit is placed on the size of the candidate document set. The disjunctive phase proceeds until the candidate set is full.
Reference: [39] <author> A. Moffat and J. Zobel. </author> <title> Self-indexing inverted files for fast text retrieval. </title> <type> Technical Report 94/2, </type> <institution> Collaborative Information Technology Research Institute, Department of Computer Science, Royal Melbourne Institute of Technology, Australia, </institution> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: This scheme can make no guarantees about the membership of the set. It does, however, calculate complete scores for the documents in the candidate set, guaranteeing a correct relative ranking. The second variation was proposed by Moffat and Zobel <ref> [39, 37, 38] </ref>. Rather than use an insertion threshold related to a term's potential score contribution, a hard limit is placed on the size of the candidate document set. The disjunctive phase proceeds until the candidate set is full. <p> A compression strategy is described that allows the new inverted file structure to be stored in less space than that required by an inverted file using document identifier ordering and conventional compression techniques. Moffat and Zobel <ref> [39] </ref> describe an inverted list implementation that supports jumping forward in the list using skip pointers. This is useful for document based access into the list during conjunctive style processing.
Reference: [40] <author> J. E. B. Moss. </author> <title> Design of the Mneme persistent object store. </title> <journal> ACM Trans. Inf. Syst., </journal> <volume> 8(2) </volume> <pages> 103-139, </pages> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: I will integrate the INQUERY inference network based document retrieval system [9] with the Mneme persistent object store <ref> [40] </ref> and demonstrate the execution performance improvements possible with my approach. A proper investigation of execution efficiency requires representative document collections with which to evaluate suggested optimization techniques. Moreover, for any efficiency claims to be valid, it must be shown that retrieval effectiveness has been maintained. <p> intermediate lists of term-at-a-time processing, then term-at-a-time processing will most likely be less expensive than document-at-a-time processing since the later must evaluate the query tree multiple times, once for each document that appears in the inverted lists for the terms in the query. 2.2 Mneme The Mneme persistent object store <ref> [40] </ref> was designed to be efficient and extensible. The basic services provided by Mneme are storage and retrieval of objects, where an object is a chunk of contiguous bytes that has been assigned a unique identifier. Mneme has no notion of type or class for objects.
Reference: [41] <author> S. A. Perry and P. Willett. </author> <title> A review of the use of inverted files for best match searching in information retrieval systems. </title> <institution> J. Inf. Sci., 6(2-3):59-66, </institution> <year> 1983. </year>
Reference-contexts: The accumulated similarity is based solely on the information stored in the inverted lists, thus eliminating the need to retrieve the document representation vectors. After all inverted lists have been processed, the nearest neighbor is identified by selecting the maximum similarity from the counters. Perry and Willett <ref> [41] </ref> show how the upper bound technique can be applied to this processing strategy to reduce main memory requirements. The upper bound on the similarity of a previously unseen document is calculated in the same way as before.
Reference: [42] <author> M. Persin. </author> <title> Document filtering for fast ranking. </title> <booktitle> In Proc. of the 17th Inter. ACM SIGIR Conf. on Res. and Develop. in Infor. Retr., </booktitle> <pages> pages 339-348, </pages> <address> Dublin, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: We can return the top n documents, but we cannot guarantee their relative ranking. Rather than place a hard limit on the size of the set of documents returned, thresholds can be established that determine how a belief value is processed. Such a scheme is described by Persin <ref> [42] </ref>.
Reference: [43] <author> M. Persin, J. Zobel, and R. Sacks-Davis. </author> <title> Fast document ranking for large scale information retrieval. </title> <booktitle> In Proc. ADB'94 Inter. Conf. on Applications of Databases, </booktitle> <address> Vadstena, Sweden, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Access into the inverted lists must be document based in order to ensure that complete scores are calculated for documents. A similar inverted list structure is described by Persin in <ref> [43] </ref>, where inverted list entries are also sorted by within document term frequency. The emphasis in this work, however, is the efficient storage of the information contained in the lists.
Reference: [44] <author> U. Pfeifer, S. Pennekamp, and N. Fuhr. </author> <title> Incremental processing of vague queries in interactive retrieval systems. </title> <institution> University of Dortmund internal document, </institution> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: The disadvantage of this scheme is the computational costs of the required bookkeeping, which may exceed any savings in belief value processing. This scheme is described by Pfeifer et al. <ref> [44] </ref>. If we are more concerned with obtaining the top n documents and less concerned with their relative ranking, we can define another stopping condition.
Reference: [45] <author> S. E. Robertson and K. Sparck Jones. </author> <title> Relevance weighting of search terms. </title> <journal> J. Amer. Soc. Inf. Sci., </journal> <volume> 27(3) </volume> <pages> 129-146, </pages> <month> May </month> <year> 1976. </year>
Reference-contexts: Now the estimation task amounts to estimating the probability of the terms appearing in a relevant document, P (x j relevant), and the a priori probability of a document, P (x). P (relevant) will be constant for a given query and so may be ignored. Robertson and Sparck Jones <ref> [45] </ref> revised the probabilistic model into its current form. They observed that a document should be retrieved if its probability of being relevant is greater than its probability of being not relevant, P (relevant j d) &gt; P (not relevant j d).
Reference: [46] <author> G. Salton and M. J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Given that retrieval is driven by evaluating the inference network for the documents that contain the query terms, the logical choice of index to support this process is an inverted file <ref> [46, 18, 26] </ref>. An inverted file index consists of a record, or inverted list, for each term that appears in the document collection. A term's inverted list stores a document identifier and weight for every document in which the term appears. <p> The first six functions operate on belief values, while the last four functions operate on proximity values. The first three functions are probabilistic implementations of the boolean operators. In traditional boolean systems, the documents in the answer set must exactly match the boolean query formula <ref> [46] </ref>. In the probabilistic model, the assignment of a term to a document is expressed with a probability or belief. A boolean formula of terms will therefore have some probability of matching a given document, allowing documents to be ranked by this probability.
Reference: [47] <author> L. V. Saxton and V. V. Raghavan. </author> <title> Design of an integrated information retrieval/database management system. </title> <journal> IEEE Trans. Know. Data Eng., </journal> <volume> 2(2) </volume> <pages> 210-219, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Moreover, the functionality requirements imposed by my inverted list implementation are unlikely to be adequately supported in an RDBMS. Other work in this area has attempted to integrate information retrieval with database management <ref> [17, 47] </ref>. The services provided by a database management system (DBMS) and an IR system are distinct but complementary, making an integrated system very attractive. The integrated architecture consists of a DBMS component and a custom IR system component.
Reference: [48] <author> P. Schauble. SPIDER: </author> <title> A multiuser information retrieval system for semistructured and dynamic data. </title> <booktitle> In Proc. of the 16th Inter. ACM SIGIR Conf. on Res. and Develop. in Infor. Retr., </booktitle> <pages> pages 318-327, </pages> <address> Pittsburgh, </address> <month> June </month> <year> 1993. </year> <month> 31 </month>
Reference-contexts: The process of identifying a candidate document set followed by evaluating the query for just those documents is similar in spirit to the two stage query evaluation strategy of the SPIDER information retrieval system <ref> [48, 27] </ref>. In SPIDER, a signature file is used to identify documents that potentially match the query, and an upper bound is calculated for each document's similarity to the query.
Reference: [49] <author> K. Shoens, A. Tomasic, and H. Garcia-Molina. </author> <title> Synthetic workload performance analysis of incremental updates. </title> <booktitle> In Proc. of the 17th Inter. ACM SIGIR Conf. on Res. and Develop. in Infor. </booktitle> <address> Retr., Dublin, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Tomasic et al. [53] propose a new data structure to support incremental indexing, and present a detailed simulation study over a variety of disk allocation schemes. The study is extended with a larger synthetic document collection in <ref> [49] </ref>, and a comparison is made with traditional indexing techniques. Their data structure manages small inverted lists in buckets and dynamically selects large inverted lists to be managed separately. Again, they do not support the granularity of access into the long inverted lists required by the optimizations considered here.
Reference: [50] <author> V. Singhal, S. V. Kakkad, and P. R. Wilson. </author> <title> Texas, an efficient, portable persistent store. </title> <booktitle> In Proc. of the 5th Inter. Workshop on Persistent Object Systems, </booktitle> <pages> pages 11-33, </pages> <address> San Miniato, Italy, </address> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: Resident objects are quickly located using the resident object table, and non-resident objects are faulted in with little additional processing. This can be contrasted with page mapping architectures of other object stores <ref> [28, 50] </ref> which have a fairly high penalty for accessing a non-resident object. These systems are optimized for localized processing of a large number of small objects, where the cost of faulting a page of objects can be amortized over many access to the objects in the page.
Reference: [51] <author> A. F. Smeaton and C. J. van Rijsbergen. </author> <title> The nearest neighbour problem in information retrieval. An algorithm using upperbounds. </title> <booktitle> In Proc. of the 4th Inter. ACM SIGIR Conf. on Res. and Develop. in Infor. Retr., </booktitle> <pages> pages 83-87, </pages> <address> Oakland, CA, </address> <year> 1981. </year>
Reference-contexts: When the inverted lists for all of the terms in the query have been processed, the current nearest neighbor is returned as the answer to the query. Smeaton and van Rijsbergen <ref> [51] </ref> describe how an upper bound on the similarity of any unseen document can be calculated based on the unprocessed query terms. If this upper bound is less than the similarity of the current nearest neighbor, processing may stop.
Reference: [52] <author> K. Spark Jones and C. A. Webster. </author> <title> Research in relevance weighting. British Library Research and Development Report 5553, </title> <institution> Computer Laboratory, University of Cambridge, </institution> <year> 1979. </year>
Reference-contexts: Historically, large, realistic collections and queries with proper relevance scores have not been available to researchers. The standard document collections used for IR system evaluation include Cranfield [11], CACM [21], and NPL <ref> [52] </ref>. All of these collections are less than 10 Mbytes of raw text and are considered tiny compared to current and anticipated IR system requirements. These small research collections simply do not challenge the physical capacities of today's modern computers.
Reference: [53] <author> A. Tomasic, H. Garcia-Molina, and K. Shoens. </author> <title> Incremental updates of inverted lists for text document retrieval. </title> <booktitle> In Proc. of the ACM SIGMOD Inter. Conf. on Management of Data, </booktitle> <pages> pages 289-300, </pages> <address> Minneapolis, MN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Accessing the inverted file in this format during query processing is much too inefficient, so the nodes in a linked list are ultimately conglomerated into a single inverted list before the file is used for retrieval. Tomasic et al. <ref> [53] </ref> propose a new data structure to support incremental indexing, and present a detailed simulation study over a variety of disk allocation schemes. The study is extended with a larger synthetic document collection in [49], and a comparison is made with traditional indexing techniques.
Reference: [54] <author> H. R. Turtle and W. B. Croft. </author> <title> Inference networks for document retrieval. </title> <booktitle> In Proc. of the 13th Inter. ACM SIGIR Conf. on Res. and Develop. in Infor. Retr., </booktitle> <pages> pages 1-24, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: Moreover, if our belief in any given proposition should change, its probability can be adjusted and the network can be used to update the probabilities at the rest of the nodes. The application of Baysien inference networks to information retrieval was advanced by Turtle and Croft <ref> [54, 56, 55] </ref>. The inference network used for information retrieval is divided into two parts, a document network and a query network, shown in Figure 1. The document network consists of document nodes (d i 's), text representation nodes (t i 's), and concept representation nodes (r i 's).
Reference: [55] <author> H. R. Turtle and W. B. Croft. </author> <title> Efficent probabilistic inference for text retrieval. </title> <booktitle> In Proc. of RIAO'91, </booktitle> <pages> pages 644-661, </pages> <address> Barcelona, </address> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Moreover, if our belief in any given proposition should change, its probability can be adjusted and the network can be used to update the probabilities at the rest of the nodes. The application of Baysien inference networks to information retrieval was advanced by Turtle and Croft <ref> [54, 56, 55] </ref>. The inference network used for information retrieval is divided into two parts, a document network and a query network, shown in Figure 1. The document network consists of document nodes (d i 's), text representation nodes (t i 's), and concept representation nodes (r i 's).
Reference: [56] <author> H. R. Turtle and W. B. Croft. </author> <title> Evaluation of an inference network-based retrieval model. </title> <journal> ACM Trans. Inf. Syst., </journal> <volume> 9(3) </volume> <pages> 187-222, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: If r of the D 1 documents are actually relevant (i.e., from the R relevant documents), then the recall is r/R and the precision is r/D. One of the more powerful retrieval models to emerge from this research is the inference network retrieval model <ref> [56] </ref>. This model treats information retrieval as an evidential reasoning process. Using a Baysien inference network, evidence about a document's relevance to a query is combined from different sources to produce a final relevance judgement. <p> Moreover, if our belief in any given proposition should change, its probability can be adjusted and the network can be used to update the probabilities at the rest of the nodes. The application of Baysien inference networks to information retrieval was advanced by Turtle and Croft <ref> [54, 56, 55] </ref>. The inference network used for information retrieval is divided into two parts, a document network and a query network, shown in Figure 1. The document network consists of document nodes (d i 's), text representation nodes (t i 's), and concept representation nodes (r i 's).
Reference: [57] <author> H. R. Turtle and J. Flood. </author> <title> Query evaluation: Strategies and optimizations. under review, </title> <year> 1994. </year>
Reference-contexts: Many documents will be eliminated from consideration before these frequent terms are processed, such that much of the inverted list information for these terms can be skipped. This scheme is called max-score by Turtle and Flood in <ref> [57] </ref>. The document processing order used above will attempt to calculate a score for every document that appears in the inverted lists of the query terms. In fact, we can identify another stopping condition at which point all document processing can stop. <p> The disjunctive phase proceeds until the candidate set is full. Then, the conjunctive phase proceeds until all of the query terms have been processed. This variation makes the same guarantees as the previous one. The third variation is a term-at-a-time version of max-score described by Turtle and Flood <ref> [57] </ref>. New documents are added to the candidate set until the upper bound score of an unseen document (determined from the maximum possible belief score contributions of the unprocessed terms) falls below the current partial score of the n th document.
Reference: [58] <author> I. H. Witten, A. Moffat, and T. C. Bell. </author> <title> Managing Gigabytes: Compressing and Indexing Documents and Images. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: However, a concordance for a large document such as the Bible might require a good portion of a lifetime to construct by hand, and in one case, the effort involved in such a task is believed to have lead to the insanity of the concordance compiler <ref> [58] </ref>. With the advent of the computer age in the latter half of the 20th century, concordance construction could be automated, greatly simplifying the task. What used to take years could now be accomplished in minutes. <p> In the second case above where the decompression is more expensive than the read, ROIO will always suffer, regardless of the contribution of E, since it would have been cheaper to read in an uncompressed version. Compression techniques for inverted lists have received a fair amount of attention <ref> [58, 36, 30, 1, 5, 63] </ref>. I do not claim anything novel with respect to compression. Rather, for completeness I merely describe how it fits into my optimization strategy and give a necessary condition for providing benefit with respect to execution performance.
Reference: [59] <author> D. Wolfram. </author> <title> Applying informetric characteristics of databases to IR system file design, Part I: informetric models. </title> <journal> Inf. Process. & Mgmnt., </journal> <volume> 28(1) </volume> <pages> 121-133, </pages> <year> 1992. </year>
Reference-contexts: Finally, properly modeling the size distribution of inverted file index records and the frequency of use of terms in queries is addressed by Wolfram in <ref> [59, 60] </ref>. He suggests that the informetric characteristics of document databases should be taken into consideration when designing the files used by an IR system.
Reference: [60] <author> D. Wolfram. </author> <title> Applying informetric characteristics of databases to IR system file design, Part II: simulation comparisons. </title> <journal> Inf. Process. & Mgmnt., </journal> <volume> 28(1) </volume> <pages> 135-151, </pages> <year> 1992. </year>
Reference-contexts: Finally, properly modeling the size distribution of inverted file index records and the frequency of use of terms in queries is addressed by Wolfram in <ref> [59, 60] </ref>. He suggests that the informetric characteristics of document databases should be taken into consideration when designing the files used by an IR system.
Reference: [61] <author> W. Y. P. Wong and D. L. Lee. </author> <title> Implementations of partial document ranking using inverted files. </title> <journal> Inf. Process. & Mgmnt., </journal> <volume> 29(5) </volume> <pages> 647-669, </pages> <year> 1993. </year>
Reference-contexts: Determining what these functions actually look like might be done experimentally or analytically. The problem with this scheme is that, short of processing all of the belief values, it gives us no guarantees on the correctness of the final ranking obtained. This scheme was proposed by Wong and Lee <ref> [61] </ref>, who describe two estimation techniques for determining how many belief values must be processed to achieve a given level of retrieval effectiveness. An alternative to this ad-hoc stopping condition would be a stopping condition that takes advantage of the organization of the belief values. <p> My inverted list implementation will have similar incremental update characteristics, but is actually intended 26 to provide superior query processing performance. Wong and Lee <ref> [61] </ref> describe an inverted list implementation where the inverted list entries are sorted by within document term frequency and each inverted list is divided into pages.
Reference: [62] <author> G. K. Zipf. </author> <title> Human Behavior and the Principle of Least Effort. </title> <publisher> Addison-Wesley Press, </publisher> <year> 1949. </year>
Reference-contexts: This approach is motivated by the skewed distribution of sizes of the data objects being manipulated and the relative inefficiency of I/O operations compared to main memory and CPU operations. The objects in an inverted file, or the inverted lists, have a skewed size distribution due to the Zipfian <ref> [62] </ref> distribution of term frequencies in a document collection. For a multi-gigabyte document collection, the inverted lists will range in size from a few bytes to multiple megabytes. In practice, the larger inverted lists are accessed quite often during query processing. <p> In the next subsection I briefly examine these characteristics, and following that I describe the integrated system and its performance. 10 720 Mb total. 3.1 Inverted List Characteristics The size of an inverted list depends on the number of occurrences of the corresponding term in the document collection. Zipf <ref> [62] </ref> observed that if the terms in a document collection are ranked by decreasing number of occurrences (i.e., starting with the term that occurs most frequently), there is a constant for the collection that is approximately equal to the product of any given term's frequency and rank order number.
Reference: [63] <author> J. Zobel and A. Moffat. </author> <title> Adding compression to a full-text retrieval system. </title> <booktitle> In Proc. 15th Australian Comp. Sci. Conf., </booktitle> <pages> pages 1077-1089, </pages> <address> Hobart, Australia, </address> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: In the second case above where the decompression is more expensive than the read, ROIO will always suffer, regardless of the contribution of E, since it would have been cheaper to read in an uncompressed version. Compression techniques for inverted lists have received a fair amount of attention <ref> [58, 36, 30, 1, 5, 63] </ref>. I do not claim anything novel with respect to compression. Rather, for completeness I merely describe how it fits into my optimization strategy and give a necessary condition for providing benefit with respect to execution performance.
Reference: [64] <author> J. Zobel, A. Moffat, and R. Sacks-Davis. </author> <title> An efficient indexing technique for full-text database systems. </title> <booktitle> In Proc. of the 18th Inter. Conf. on VLDB, </booktitle> <pages> pages 352-362, </pages> <address> Vancouver, </address> <year> 1992. </year> <month> 32 </month>
Reference-contexts: Efficient management of full-text database indexes has received a fair amount of attention. Faloutsos [18] gives an early survey of the common indexing techniques. Zobel et al. <ref> [64] </ref> investigate the efficient implementation of an inverted file index for a full-text database system. Their focus is on compression techniques to limit the size of the inverted file index.
References-found: 64

