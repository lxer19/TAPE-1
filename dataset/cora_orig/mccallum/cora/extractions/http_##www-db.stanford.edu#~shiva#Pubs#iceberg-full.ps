URL: http://www-db.stanford.edu/~shiva/Pubs/iceberg-full.ps
Refering-URL: http://google.stanford.edu/google_papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: ullmang@cs.stanford.edu  
Title: Computing Iceberg Queries Efficiently  
Author: Min Fang, Narayanan Shivakumar, Hector Garcia-Molina, Rajeev Motwani, Jeffrey D. Ullman ffangmin, shiva, hector, rajeev, 
Date: 234  
Note: Paper Number  
Address: Stanford, CA 94305.  
Affiliation: Department of Computer Science,  
Abstract: Many applications compute aggregate functions (such as COUNT, SUM) over an attribute (or set of attributes) to find aggregate values above some specified threshold. We call such queries iceberg queries because the number of above-threshold results is often very small (the tip of an iceberg), relative to the large amount of input data (the iceberg). Such iceberg queries are common in many applications, including data warehousing, information-retrieval, market basket analysis in data mining, clustering and copy detection. We propose efficient algorithms to evaluate iceberg queries using very little memory and significantly fewer passes over data, as compared to current techniques that use sorting or hashing. We present an experimental case study using over three gigabytes of Web data to illustrate the savings obtained by our algorithms.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules in large databases. </title> <booktitle> In Proceedings of International Conference on Very Large Databases (VLDB '94), </booktitle> <pages> pages 487 - 499, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Many data mining queries are fundamentally iceberg queries. For instance, market analysts execute market basket queries on large data warehouses that store customer sales transactions. These queries identify user buying patterns, by finding item pairs (and triples) that are bought together by many customers <ref> [1, 3, 4] </ref>. (Target sets are item-pairs, and T is the minimum number of transactions required to support the item pair.) Since these queries operate on very large datasets, fl Phone Number: (650) 723-3605, FAX: (650) 725-2588 1 target1 target2 rest a e joe b f fred a e sally b <p> Our prototypical iceberg query (Section 1) selects the target values with frequencies higher than a threshold T . That is, if we define r t to be maxfrjF req (r) T g, then the answer to our query is the set H = fV <ref> [1] </ref>; V [2]; : : :; V [r t ]g. We call the values in H the heavy targets, and we define L to be the remaining light values. The algorithms we describe next answer the prototypical iceberg query, although they can be easily adapted to other iceberg queries. <p> MULTI-STAGE has the same pre-scan sampling phase as MULTI-LEVEL, where it identifies potentially heavy buckets. However, MULTI-STAGE does not allocate auxiliary buckets per potentially heavy bucket. Rather it allocates a common pool of auxiliary buckets B <ref> [1; 2; : : :; m 3 ] </ref>. Then it performs a hashing scan of the data as follows. For each target v in the data, it increments A [h (v)] if the bucket corresponding to h (v) is not marked as potentially heavy.
Reference: [2] <author> D. Bitton and D. J. DeWitt. </author> <title> Duplicate record elimination in large data files. </title> <journal> ACM Transactions in Database Systems (TODS), </journal> <volume> 8(2):255 - 265, </volume> <year> 1983. </year>
Reference-contexts: For each of these passes we need to read and write the entire relation R (or at least all the values for the target attribute). We encounter similar problems if we use other popular techniques such as early aggregation <ref> [2] </ref>, or hashing based aggregation. Until now, we have assumed R is materialized. However, in many cases R may be too large to be explicitly materialized even on disk. For instance, in the market basket application, the input data is often not R itself, but a set of transaction records. <p> Our prototypical iceberg query (Section 1) selects the target values with frequencies higher than a threshold T . That is, if we define r t to be maxfrjF req (r) T g, then the answer to our query is the set H = fV [1]; V <ref> [2] </ref>; : : :; V [r t ]g. We call the values in H the heavy targets, and we define L to be the remaining light values. The algorithms we describe next answer the prototypical iceberg query, although they can be easily adapted to other iceberg queries. <p> MULTI-STAGE has the same pre-scan sampling phase as MULTI-LEVEL, where it identifies potentially heavy buckets. However, MULTI-STAGE does not allocate auxiliary buckets per potentially heavy bucket. Rather it allocates a common pool of auxiliary buckets B <ref> [1; 2; : : :; m 3 ] </ref>. Then it performs a hashing scan of the data as follows. For each target v in the data, it increments A [h (v)] if the bucket corresponding to h (v) is not marked as potentially heavy. <p> In the following experiments, if the final F (input to Count (F )) does not fit in main memory, we stream the tuples in F onto disk, and we execute Count (F ) using a disk-based sorting algorithm. Our implementation is enhanced with early aggregation <ref> [2] </ref> so that it integrates counting into the sorting and merging processes, for efficient execution. As we discussed earlier, this is merely one way to execute Count (F ). Hence the reader should not interpret the results of this section as absolute predictions, but rather as illustrations of performance trends.
Reference: [3] <author> S. Brin, R. Motwani, and C. Silverstein. </author> <title> Beyond market baskets: Generalizing association rules to correlations. </title> <booktitle> In Proceedings of ACM SIGMOD Conference, </booktitle> <pages> pages 265 - 276, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: Many data mining queries are fundamentally iceberg queries. For instance, market analysts execute market basket queries on large data warehouses that store customer sales transactions. These queries identify user buying patterns, by finding item pairs (and triples) that are bought together by many customers <ref> [1, 3, 4] </ref>. (Target sets are item-pairs, and T is the minimum number of transactions required to support the item pair.) Since these queries operate on very large datasets, fl Phone Number: (650) 723-3605, FAX: (650) 725-2588 1 target1 target2 rest a e joe b f fred a e sally b <p> In MULTISCAN, A [h 2 (e)] is incremented for each of the 20 occurrences of e. However in MULTISCAN-SHARED, A [h 2 (e)] is not incremented for the 20 occurences of e, since we already know that e is light (because BIT M AP 1 <ref> [3] </ref> = 0). Since e does not increment A [0] in the second hashing scan, d is no longer a part of the candidate set.
Reference: [4] <author> S. Brin, R. Motwani, J.D. Ullman, and S. Tsur. </author> <title> Dynamic itemset counting and implication rules for masket basket data. </title> <booktitle> In Proceedings of ACM SIGMOD Conference, </booktitle> <pages> pages 255 - 264, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: Many data mining queries are fundamentally iceberg queries. For instance, market analysts execute market basket queries on large data warehouses that store customer sales transactions. These queries identify user buying patterns, by finding item pairs (and triples) that are bought together by many customers <ref> [1, 3, 4] </ref>. (Target sets are item-pairs, and T is the minimum number of transactions required to support the item pair.) Since these queries operate on very large datasets, fl Phone Number: (650) 723-3605, FAX: (650) 725-2588 1 target1 target2 rest a e joe b f fred a e sally b
Reference: [5] <author> S. Brin and L. </author> <title> Page. Google search engine/ backrub web crawler. </title>
Reference-contexts: Case 1: Market basket query We use the market basket query to find commonly occuring word pairs. For this we use 100; 000 web documents crawled and stored by the Stanford BackRub webcrawler <ref> [5] </ref>. The average length of each document is 118 words. From this data we computed the DocWord relation to be hdocID, wordIDi, if document with identifier docID had a word with identifier wordID. This relation was about 80 MBs, when we used 4-byte integers for docIDs and wordIDs.
Reference: [6] <author> A. Broder. </author> <title> On the resemblance and containment of documents. </title> <type> Technical report, </type> <institution> DIGITAL Systems Research Center Tech. </institution> <type> Report, </type> <year> 1997. </year>
Reference-contexts: This is of course because they do not use the given threshold to execute the query faster first, they perform the aggregation and later apply the thresholding. 2 EXAMPLE 2.2 DocumentOverlap Query Web-searching engines such as AltaVista cluster web documents based on "syntactic similarity" of documents <ref> [6, 7] </ref>, The goal of clustering is to develop better web crawlers by identifying documents that are replicated or are near-replicas of other documents (such as JAVA 1.1.3 manuals and FAQs [20]). <p> CREATE VIEW DocumentOverlaps SELECT D1.doc, D2.doc, COUNT (D1.chunk) FROM D1 as DocSign, D2 as DocSign WHERE D1. chunk = D2. chunk AND D1.doc NOT = D2.doc GROUP BY D1. doc, D2. doc HAVING COUNT (D1.chunk) &gt;= T2 Currently, the DEC prototype <ref> [6, 7] </ref> uses sorting to execute the above self-join, as follows.
Reference: [7] <author> A. Broder, S.C. Glassman, and M. S. Manasse. </author> <title> Syntactic Clustering of the Web. </title> <booktitle> In Sixth International World Wide Web Conference, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: This is of course because they do not use the given threshold to execute the query faster first, they perform the aggregation and later apply the thresholding. 2 EXAMPLE 2.2 DocumentOverlap Query Web-searching engines such as AltaVista cluster web documents based on "syntactic similarity" of documents <ref> [6, 7] </ref>, The goal of clustering is to develop better web crawlers by identifying documents that are replicated or are near-replicas of other documents (such as JAVA 1.1.3 manuals and FAQs [20]). <p> CREATE VIEW DocumentOverlaps SELECT D1.doc, D2.doc, COUNT (D1.chunk) FROM D1 as DocSign, D2 as DocSign WHERE D1. chunk = D2. chunk AND D1.doc NOT = D2.doc GROUP BY D1. doc, D2. doc HAVING COUNT (D1.chunk) &gt;= T2 Currently, the DEC prototype <ref> [6, 7] </ref> uses sorting to execute the above self-join, as follows.
Reference: [8] <author> M. Fang, R. Motwani, and J. Ullman. </author> <title> Improvements over hash-based algorithms for mining association rules. </title> <type> Technical report, Stanford DBGroup Technical Report, </type> <month> October </month> <year> 1997. </year>
Reference-contexts: If the user enters the word "stock" in a query, the system may suggest "market, "price," and "chicken" as useful words to add to the query to distinguish the way in which "stock" is used. Computing co-occurring words again involves an iceberg query, where target-sets are pairs of words <ref> [8] </ref>. We will study this application again in more detail in our experimental case-study. <p> Park et al. [16] proposed coarse counting in the context of mining association rules. All the above approaches use a single hash function for their coarse counting, and hence tend to have many false positives <ref> [8] </ref>.
Reference: [9] <author> M. Fang, N. Shivakumar, H. Garcia-Molina, R. Motwani, and J.D. Ullman. </author> <title> Computing iceberg queries effciently. </title> <type> Technical report, Stanford DBGroup Technical Report, </type> <month> February </month> <year> 1997. </year>
Reference-contexts: Indeed in the figure, we present the case when p and q do fall into the same bucket. We have analysed MULTI-LEVEL based on the above intuition, in the full version of the paper <ref> [9] </ref>. The main intuition behind sharing a common pool of auxiliary buckets across potentially heavy buckets is that several heavy targets when rehashed into B could fall into the same bucket as other heavy targets (as illustrated in the example). <p> We now discuss a few representative schemes for specific values of K to illustrate some of the trade-offs involved. (We study the performance of all schemes in greater detail, in the full version of this paper <ref> [9] </ref>.) Specifically, we present results for MULTISCAN/D, MULTISCAN-SHARED/D and UNISCAN/D, the corresponding multi-bucket optimization of DEFER-COUNT. We also evaluate MULTI-STAGE for K = 1. <p> iceberg query, it is easy to see that sorting based execution would require too much disk space to materialize and sort R, and will take much longer than our schemes. 7.1 Summary Based on our case studies (and from experiments we do not report here due to lack of space <ref> [9] </ref>), we propose the following informal "rules of thumb" to combine schemes from the HYBRID and MULTIBUCKET algorithms: 1. HYBRID algorithms: MULTI-LEVEL rarely performs well in our experiments, while DEFER-COUNT and MULTI-STAGE tend to do very well under different circumstances. <p> For relatively large values of memory, we recommend UNISCAN with multiple hash functions, since we can choose K &gt; 1 and apply multiple hash functions within one hashing scan, as we discuss in the full version of this paper <ref> [9] </ref>. 8 Related Work Flajolet and Martin [10], and Whang et al. [23] proposed a simple form of coarse counting for estimating the number of distinct elements in a multiset. Park et al. [16] proposed coarse counting in the context of mining association rules.
Reference: [10] <author> P. Flajolet and G.N. Martin. </author> <title> Probabilistic counting algorithms for database applications. </title> <journal> Journal of Computer System Sciences, </journal> <volume> 31:182 - 209, </volume> <year> 1985. </year>
Reference-contexts: We will show how to remove these errors using our HYBRID algorithms in the next section. 3.2 Coarse counting by bucketizing elements (COARSE-COUNT) "Coarse counting" or "probabilistic counting" is a technique often used for query size estimation, for computing the number of distinct targets in a relation <ref> [10, 23] </ref>, for mining association rules [16], and for other applications. The simplest form of coarse counting uses an array A [1::m] of m counters and a hash function h 1 , which maps target values from log 2 n bits to log 2 m bits, m &lt;< n. <p> For relatively large values of memory, we recommend UNISCAN with multiple hash functions, since we can choose K &gt; 1 and apply multiple hash functions within one hashing scan, as we discuss in the full version of this paper [9]. 8 Related Work Flajolet and Martin <ref> [10] </ref>, and Whang et al. [23] proposed a simple form of coarse counting for estimating the number of distinct elements in a multiset. Park et al. [16] proposed coarse counting in the context of mining association rules.
Reference: [11] <author> R. L. Graham, D. E. Knuth, and O. Patashnik. </author> <title> Concrete Mathematics. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference: [12] <author> P.J. Haas, J.F. Naughton, S. Seshadri, </author> <title> and A.N. Swami. Selectivity and cost estimation for joins based on random sampling. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 52(3):550 - 569, </volume> <month> June </month> <year> 1996. </year>
Reference-contexts: In Section 7 we evaluate the memory requirements more carefully. 3.1 A Sampling-Based Algorithm (SCALED-SAMPLING) Sampling procedures are widely adopted in practice to estimate sizes of query results <ref> [12] </ref>, and to perform online aggregation [13]. (See [15] for a good discussion of sampling techniques to efficiently obtain unbiased samples.) We now consider a simple sampling-based algorithm for iceberg queries. The basic idea is as follows: Take a random sample of size s from R.
Reference: [13] <author> J.M. Hellerstein, P.J. Haas, and H.J. Wang. </author> <title> Online aggregation. </title> <booktitle> In Proceedings of ACM SIGMOD International Conference on Management of Data (SIGMOD'97), </booktitle> <address> Tuscon, Arizona, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: In Section 7 we evaluate the memory requirements more carefully. 3.1 A Sampling-Based Algorithm (SCALED-SAMPLING) Sampling procedures are widely adopted in practice to estimate sizes of query results [12], and to perform online aggregation <ref> [13] </ref>. (See [15] for a good discussion of sampling techniques to efficiently obtain unbiased samples.) We now consider a simple sampling-based algorithm for iceberg queries. The basic idea is as follows: Take a random sample of size s from R.
Reference: [14] <author> C. Jordan. </author> <title> The Calculus of Finite Differences. </title> <booktitle> Chelsea (2nd edition), </booktitle> <year> 1949. </year>
Reference: [15] <author> F. Olken. </author> <title> Random sampling from databases. </title> <type> Ph.D. dissertation, </type> <institution> UC Berkeley, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: In Section 7 we evaluate the memory requirements more carefully. 3.1 A Sampling-Based Algorithm (SCALED-SAMPLING) Sampling procedures are widely adopted in practice to estimate sizes of query results [12], and to perform online aggregation [13]. (See <ref> [15] </ref> for a good discussion of sampling techniques to efficiently obtain unbiased samples.) We now consider a simple sampling-based algorithm for iceberg queries. The basic idea is as follows: Take a random sample of size s from R.
Reference: [16] <author> J.S. Park, M.S. Chen, and P.S. Yu. </author> <title> An effective hash based algorithm for mining association rules. </title> <booktitle> In Proceedings of ACM SIGMOD Conference, </booktitle> <pages> pages 175 - 186, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: In fact, Park et al. claim that the time to execute the above query dominates the cost of producing interesting association rules <ref> [16] </ref>. <p> how to remove these errors using our HYBRID algorithms in the next section. 3.2 Coarse counting by bucketizing elements (COARSE-COUNT) "Coarse counting" or "probabilistic counting" is a technique often used for query size estimation, for computing the number of distinct targets in a relation [10, 23], for mining association rules <ref> [16] </ref>, and for other applications. The simplest form of coarse counting uses an array A [1::m] of m counters and a hash function h 1 , which maps target values from log 2 n bits to log 2 m bits, m &lt;< n. <p> Park et al. <ref> [16] </ref> proposed coarse counting in the context of mining association rules. All the above approaches use a single hash function for their coarse counting, and hence tend to have many false positives [8].
Reference: [17] <author> G. Salton and C. Buckley. </author> <title> Term-weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24(5), </volume> <year> 1988. </year>
Reference-contexts: This relation was about 80 MBs, when we used 4-byte integers for docIDs and wordIDs. Note that we removed entries corresponding to 500 pre-defined stop words from this relation <ref> [17] </ref>. Recall that the R over which the iceberg query is to be executed has all pairs of words that occur in the same document.
Reference: [18] <author> N. Shivakumar and H. Garcia-Molina. </author> <title> SCAM: A copy detection mechanism for digital documents. </title> <booktitle> In Proceedings of 2nd International Conference in Theory and Practice of Digital Libraries (DL'95), </booktitle> <address> Austin, Texas, </address> <month> June </month> <year> 1995. </year> <month> 21 </month>
Reference-contexts: By identifying these popular chunks, we can improve phrase searching and indexing. For instance, chunks such as "Netscape Mozilla/1.0" occur frequently in web documents and may not even be indexed in certain implementations of IR systems (such as in <ref> [18, 19] </ref>), to reduce storage requirements. For this set of experiments, we used 300; 000 documents we obtained from the Stanford BackRub crawler (as above). We defined chunks based on sliding windows of words as in [18]. <p> For this set of experiments, we used 300; 000 documents we obtained from the Stanford BackRub crawler (as above). We defined chunks based on sliding windows of words as in <ref> [18] </ref>. We say we use "C = i" chunking, if the j th chunk of a given document is the sequence of words from j through j + i 1.
Reference: [19] <author> N. Shivakumar and H. Garcia-Molina. </author> <title> Building a scalable and accurate copy detection mechanism. </title> <booktitle> In Proceedings of 1st ACM Conference on Digital Libraries (DL'96), </booktitle> <address> Bethesda, Maryland, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: By identifying these popular chunks, we can improve phrase searching and indexing. For instance, chunks such as "Netscape Mozilla/1.0" occur frequently in web documents and may not even be indexed in certain implementations of IR systems (such as in <ref> [18, 19] </ref>), to reduce storage requirements. For this set of experiments, we used 300; 000 documents we obtained from the Stanford BackRub crawler (as above). We defined chunks based on sliding windows of words as in [18].
Reference: [20] <author> N. Shivakumar and H. Garcia-Molina. </author> <title> Computing replicas and near-replicas of documents on the web. </title> <note> In To appear in Workshop on WebDatabases (WebDB'98), </note> <institution> Valencia, Spain, </institution> <month> March </month> <year> 1998. </year>
Reference-contexts: 2 EXAMPLE 2.2 DocumentOverlap Query Web-searching engines such as AltaVista cluster web documents based on "syntactic similarity" of documents [6, 7], The goal of clustering is to develop better web crawlers by identifying documents that are replicated or are near-replicas of other documents (such as JAVA 1.1.3 manuals and FAQs <ref> [20] </ref>). The engines break up each web document into a set of signatures, such as hashed 8-byte integers of sequences of words, or sentences. Then they maintain a relation DocSign with tuples hd i ; c i i if document d i contains signature c i .
Reference: [21] <author> TPC-Committee. </author> <title> Transaction processing council (TPC). </title> <address> http://www.tpc.org. </address>
Reference-contexts: 2 Why are iceberg queries important? We now illustrate using a few examples why executing iceberg queries efficiently is important, and why traditional techniques such as sorting and hashing can lead to very high query times and inordinately large disk requirements. 3 EXAMPLE 2.1 PopularItem Query Consider a TPC-D benchmark <ref> [21] </ref> style relation LineItem with attributes partKey, the key for parts being sold, price, the price of the corresponding item, and numSales, the number of units sold in a transaction, in region, the area where the part is being sold.
Reference: [22] <author> J.D. Ullman. </author> <title> Principles of Database and Knowledge-Base Systems (Volume 1). </title> <publisher> Computer Science Press, </publisher> <year> 1988. </year>
Reference-contexts: Note that we do not explicitly store the targets in the auxiliary buckets as indicated in the figure; we continue to maintain counters in them. The idea behind the MULTI-LEVEL algorithm is very similar to the concept of extensible indices commonly used in databases <ref> [22] </ref> these indices grow over-populated buckets by adding auxiliary buckets dynamically. However, the difference is that in the case of extensible indices the entire key that is being indexed, is stored. Hence when buckets are over-populated, we can dynamically add auxiliary buckets efficiently.
Reference: [23] <author> K. Whang, B.T. Vander-Zanden, and H.M. Taylor. </author> <title> A linear-time probabilistic counting algorithm for db applications. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 15(2):208 - 229, </volume> <year> 1990. </year>
Reference-contexts: We will show how to remove these errors using our HYBRID algorithms in the next section. 3.2 Coarse counting by bucketizing elements (COARSE-COUNT) "Coarse counting" or "probabilistic counting" is a technique often used for query size estimation, for computing the number of distinct targets in a relation <ref> [10, 23] </ref>, for mining association rules [16], and for other applications. The simplest form of coarse counting uses an array A [1::m] of m counters and a hash function h 1 , which maps target values from log 2 n bits to log 2 m bits, m &lt;< n. <p> relatively large values of memory, we recommend UNISCAN with multiple hash functions, since we can choose K &gt; 1 and apply multiple hash functions within one hashing scan, as we discuss in the full version of this paper [9]. 8 Related Work Flajolet and Martin [10], and Whang et al. <ref> [23] </ref> proposed a simple form of coarse counting for estimating the number of distinct elements in a multiset. Park et al. [16] proposed coarse counting in the context of mining association rules.

References-found: 23

