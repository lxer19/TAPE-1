URL: http://www.research.att.com/~yoav/papers/rel.ps.Z
Refering-URL: http://www.research.att.com/~yoav/publications.html
Root-URL: 
Title: Sifting informative examples from a random source.  
Author: Yoav Freund 
Address: 600 Mountain Ave. Murray Hill, NJ, 07974  
Affiliation: AT&T Bell Labs,  
Abstract: We discuss two types of algorithms for selecting relevant examples that have been developed in the context of computation learning theory. The examples are selected out of a stream of examples that are generated independently at random. The first two algorithms are the so-called "boosting" algorithms of Schapire [ Schapire, 1990 ] and Fre-und [ Freund, 1990 ] , and the Query-by-Committee algorithm of Seung [ Seung et al., 1992 ] . We describe the algorithms and some of their proven properties, point to some of their commonalities, and suggest some possible future implications. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Atkinson, A. C. and Donev, A. N. </author> <year> 1992. </year> <title> Optimum Experimental Designs. </title> <publisher> Oxford science publications. </publisher>
Reference: <author> Blumer, Anselm; Ehrenfeucht, Andrzej; Haussler, David; and Warmuth, Manfred K. </author> <year> 1989. </year> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery 36(4) </journal> <pages> 929-965. </pages>
Reference: <author> Cohn, David; Atlas, Les; and Ladner, Richard 1990. </author> <title> Training connectionist networks with queries and selective sampling. </title> <editor> In Touretzky, D., editor 1990, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Drucker, Harris; Schapire, Robert; and Simard, </author> <title> Patrice up . Boosting performance in neural networks. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence 7(4) </journal> <pages> 705-719. </pages>
Reference: <author> Drucker, Harris; Schapire, Robert; and Simard, </author> <title> Patrice 1993. Improving performance in neural networks using a boosting algorithm. </title> <booktitle> In Advances in Neural Informations Processing Systems 5, </booktitle> <address> San Ma-teo, CA. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 42-49. </pages>
Reference: <author> Dyer, M.; Frieze, A.; and Kannan, R. </author> <year> 1991. </year> <title> A random polynomial time algorithm for approximating the volume of convex bodies. </title> <journal> Journal of the ACM, </journal> <volume> JACM 38(1) </volume> <pages> 1-17. </pages>
Reference: <author> Ehrenfeucht, Andrzej; Haussler, David; Kearns, Michael; and Valiant, </author> <title> Leslie 1989. A general lower bound on the number of examples needed for learning. </title> <booktitle> Information and Computation 82 </booktitle> <pages> 247-261. </pages>
Reference: <author> Fedorov, V. V. </author> <year> 1972. </year> <title> Theory of Optimal Experiments. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Freund, Y.; Seung, H.S.; Shamir, E.; and Tishby, N. </author> <year> 1992. </year> <title> Information, prediction, and query by committee. </title> <booktitle> In Advances in Neural Informations Processing Systems 5, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 483-490. </pages>
Reference: <author> Freund, Y. </author> <year> 1990. </year> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> In Proceedings of the Third Workshop on Computational Learning Theory, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 202-216. </pages>
Reference: <author> Freund, Y. </author> <year> 1992. </year> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> In Proceedings of the Fifth Workshop on Computational Learning Theory, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kauf-mann. </publisher> <pages> 391-398. </pages>
Reference: <author> Haussler, David; Kearns, Michael; and Schapire, Robert 1994. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <booktitle> Machine Learning 14(1) </booktitle> <pages> 83-113. </pages>
Reference: <author> Lindley, D. V. </author> <year> 1956. </year> <title> On a measure of the information provided by an experiment. </title> <journal> Ann. Math. Statist. </journal> <volume> 27 </volume> <pages> 986-1005. </pages> <editor> Lovasz, and Simonovits, </editor> <year> 1993. </year> <title> Random walks in a convex body and an improved volume algorithm. Random Structures & Algorithms 4. </title>
Reference: <author> Schapire, Robert E. </author> <year> 1990. </year> <title> The strength of weak learn-ability. </title> <booktitle> Machine Learning 5(2) </booktitle> <pages> 197-226. </pages>
Reference: <author> Seung, H.S; Opper, M.; and Sompolinsky, H. </author> <year> 1992. </year> <title> Query by committee. </title> <booktitle> In Proceedings of the Fifth Workshop on Computational Learning Theory, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 287-294. </pages>
Reference: <author> Valiant, L. G. </author> <year> 1984. </year> <title> A theory of the learnable. </title> <journal> Comm. ACM 27 </journal> <pages> 1134-1142. </pages>
Reference: <author> Vapnik, V. N. </author> <year> 1982. </year> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
References-found: 17

