URL: ftp://gaia.cs.umass.edu/pub/Nahu96:Parallelized.ps.Z
Refering-URL: http://www.cs.umass.edu/~nahum/home.html
Root-URL: 
Email: -nahum,yates-@cs.umass.edu -sean,ho,rcs-@cs.arizona.edu  
Title: multiprocessors make attractive server platforms, for example as secure World-Wide Web servers. These machines are
Author: David J. Yates Sean O'Malley Hilarie Orman and Richard Schroeppel 
Keyword: Parallelized Network Security Protocols  
Address: Amherst, MA 01003 Tucson, AZ 85721  
Affiliation: Department of Computer Science 1 Department of Computer Science 2 University of Massachusetts University of Arizona  
Note: Erich Nahum 1  Shared-memory  
Abstract: Copyright (c) 1996 IEEE. See full copyright notice at the end of this paper. Abstract Security and privacy are growing concerns in the Internet community, due to the Internet's rapid growth and the desire to conduct business over it safely. This desire has led to the advent of several proposals for security standards, such as secure IP, secure HTTP, and the Secure Socket Layer. All of these standards propose using cryptographic protocols such as DES and RSA. Thus, the need to use encryption protocols is increasing. This paper is an experimental performance study that examines how encryption protocol performance can be improved by using parallelism. We show linear speedup for several different Internet-based cryptographic protocol stacks running on a symmetric shared-memory multiprocessor using two different approaches to parallelism. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. N. S. I. </author> <title> (ANSI). American national standard data encryption standard. </title> <type> Technical report ANSI X3.92-1981, </type> <month> Dec. </month> <year> 1980. </year>
Reference: [2] <author> R. Atkinson. </author> <title> Security architecture for the Internet Protocol. Request for Comments (Draft Standard) RFC 1825, </title> <institution> Internet Engineering Task Force, </institution> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: We focus on available throughput; we do not examine connection setup or teardown, or the attendant issues of key exchange. In these experiments, connections are already established, and keys are assumed to be available, as needed. We use the terminology defined by the proposed secure IP standard <ref> [2] </ref>. Authentication is the property of knowing that the data received is the same as the data sent by the sender, and that the claimed sender is in fact the actual sender. Integrity is the property that the data is transmitted from source to destination without undetected alteration.
Reference: [3] <author> M. Bjorkman and P. Gunningberg. </author> <title> Locking effects in multiprocessor implementations of protocols. </title> <booktitle> In ACM SIGCOMM Symposium on Communications Architecturesand Protocols, </booktitle> <pages> pages 74-83, </pages> <address> San Francisco, CA, </address> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Section 5 discusses related issues. In Section 6 we summarize our results. 2 Parallelism in Network Protocol Processing Parallelism can take many forms in network protocol processing. Many approaches to parallelism have been proposed and are briefly described here; more detailed surveys can be found in <ref> [3, 13] </ref>. In general, we attempt to classify approaches by the unit of concurrency, or what it is that processing elements do in parallel. Here a processing element is a locus of execution for protocol processing, and can be a dedicated processor, a heavyweight process, or a lightweight thread. <p> The disadvantage is that it requires locking shared state, most significantly the protocol state at each layer. Systems using this approach include <ref> [3, 13] </ref>. In functional parallelism, a protocol layer's functions are the unit of concurrency. Functions within a single protocol layer (e.g., checksum, ACK generation) are decomposed, and each assigned to a processing element. <p> To avoid this socket-crossing cost, we replaced the simulated driver with in-memory device drivers for the TCP protocol stacks. The drivers emulate a high-speed FDDI interface, and support the FDDI maximum transmission unit (MTU) of slightly over 4K bytes. This is similar to the approaches taken in <ref> [3, 13, 20, 34] </ref>. The drivers act as senders or receivers, producing or consuming packets as fast as possible, to simulate the behavior of simplex data transfer over an error-free network. To minimize execution time and experimental perturbation, the receive-side drivers use preconstructed packet templates. <p> Using Triple-DES is 3 times slower at 1.5 Mbits/sec. the send-side tests, where speedup is throughput normalized relative to the uniprocessor throughput for the appropriate stack. The theoretical ideal linear speedup is included for comparison. Previous work <ref> [3, 24] </ref> has shown limited performance gains when using packet-level parallelism for a single TCP connection, barring any other protocol processing, and this is reflected by the baseline TCP/IP stack's minimal speedup.
Reference: [4] <author> D. Borman, R. Braden, and V. Jacobson. </author> <title> TCP extensions for high performance. Request for Comments (Proposed Standard) RFC 1323, </title> <institution> Internet Engineering Task Force, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: However, wherever possible, we have strived to make the implementations consistent. For example, both use the same TCP uniprocessor source code base, which is derived from Berkeley's Net/2 TCP [18] with BSD 4.4 fixes, but not the RFC1323 extensions <ref> [4] </ref>. 1 We use a double-Sbox implementation in our tests. 3.3 In-Memory Drivers Since our platform runs in user space, accessing the FDDI adaptor involves crossing the IRIX socket layer, which is prohibitively expensive.
Reference: [5] <author> D. D. Clark. </author> <title> Modularity and efficiency in protocol implementation. Request for Comments RFC 817, </title> <institution> Internet Engineering Task Force, </institution> <month> July </month> <year> 1982. </year>
Reference-contexts: The main advantage of layered parallelism is that it is simple and defines a clean separation between protocol boundaries. The disadvantages are that concurrency is limited to the number of layers in the stack, and that associating processing with layers results in increased context switching and synchronization between layers <ref> [5, 6, 34] </ref>. Performance gains are limited to throughput, mainly achieved through pipelining effects. An example is found in [12]. Connections form the unit of concurrency in connection-level parallelism, where connections are assigned to processing elements. Speedup is achieved using multiple connections, each of which is processed in parallel.
Reference: [6] <author> D. D. Clark. </author> <title> The structuring of systems using upcalls. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 171-180, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: The main advantage of layered parallelism is that it is simple and defines a clean separation between protocol boundaries. The disadvantages are that concurrency is limited to the number of layers in the stack, and that associating processing with layers results in increased context switching and synchronization between layers <ref> [5, 6, 34] </ref>. Performance gains are limited to throughput, mainly achieved through pipelining effects. An example is found in [12]. Connections form the unit of concurrency in connection-level parallelism, where connections are assigned to processing elements. Speedup is achieved using multiple connections, each of which is processed in parallel.
Reference: [7] <author> W. Diffie and M. E. Hellman. </author> <title> New directions in cryptography. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 22(6) </volume> <pages> 644-654, </pages> <month> Nov. </month> <year> 1976. </year>
Reference: [8] <author> H. Eberle. </author> <title> A high-speed DES implementation for network applications. </title> <type> Technical Report 90, </type> <institution> Digital Equipment Corporation Systems Research Center, </institution> <month> Sept. </month> <year> 1992. </year>
Reference: [9] <editor> M. Cekleov et al. </editor> <booktitle> SPARCCenter 2000:Multiprocessing for the 90's! In Proceedings IEEE COMPCON, </booktitle> <pages> pages 345-353, </pages> <address> San Francisco CA, </address> <month> February </month> <year> 1993. </year> <month> 9 </month>
Reference-contexts: introductions of machines such as SGI's Challenge [11], Sun's SPARCCenter <ref> [9] </ref>, and DEC's AlphaServer [10]. The spread of these machines is due to a number of factors: binary compatibility with lower-end workstations, good price/performance relative to high-end machines such as Crays, and their ease of programming compared to more elaborate parallel machines such as Hypercubes.
Reference: [10] <author> D. M. Fenwick, D. J. Foley, W. B. Gist, S. R. VanDoren, and D. Wissel. </author> <title> The AlphaServer 8000 series: High-end server platform development. </title> <journal> Digital Technical Journal, </journal> <volume> 7(1) </volume> <pages> 43-65, </pages> <year> 1995. </year>
Reference-contexts: introductions of machines such as SGI's Challenge [11], Sun's SPARCCenter [9], and DEC's AlphaServer <ref> [10] </ref>. The spread of these machines is due to a number of factors: binary compatibility with lower-end workstations, good price/performance relative to high-end machines such as Crays, and their ease of programming compared to more elaborate parallel machines such as Hypercubes.
Reference: [11] <author> M. Galles and E. Williams. </author> <title> Performance optimizations, implementation, and verification of the SGI Challenge multiprocessor. </title> <type> Technical report, </type> <institution> Silicon Graphics Inc., Mt. View, </institution> <address> CA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: introductions of machines such as SGI's Challenge <ref> [11] </ref>, Sun's SPARCCenter [9], and DEC's AlphaServer [10]. The spread of these machines is due to a number of factors: binary compatibility with lower-end workstations, good price/performance relative to high-end machines such as Crays, and their ease of programming compared to more elaborate parallel machines such as Hypercubes.
Reference: [12] <author> D. Giarrizzo, M. Kaiserswerth, T. Wicki, and R. C. Williamson. </author> <title> High-speed parallel protocol implementation. </title> <booktitle> First IFIP WG6.1/WG6.4 International Workshop on Protocols for High-Speed Networks, </booktitle> <pages> pages 165-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: The disadvantages are that concurrency is limited to the number of layers in the stack, and that associating processing with layers results in increased context switching and synchronization between layers [5, 6, 34]. Performance gains are limited to throughput, mainly achieved through pipelining effects. An example is found in <ref> [12] </ref>. Connections form the unit of concurrency in connection-level parallelism, where connections are assigned to processing elements. Speedup is achieved using multiple connections, each of which is processed in parallel.
Reference: [13] <author> M. W. Goldberg, G. W. Neufeld, and M. R. Ito. </author> <title> A parallel approach to OSI connection-oriented protocols. </title> <booktitle> Third IFIP WG6.1/WG6.4 International Workshop on Protocols for High-Speed Networks, </booktitle> <pages> pages 219-232, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Section 5 discusses related issues. In Section 6 we summarize our results. 2 Parallelism in Network Protocol Processing Parallelism can take many forms in network protocol processing. Many approaches to parallelism have been proposed and are briefly described here; more detailed surveys can be found in <ref> [3, 13] </ref>. In general, we attempt to classify approaches by the unit of concurrency, or what it is that processing elements do in parallel. Here a processing element is a locus of execution for protocol processing, and can be a dedicated processor, a heavyweight process, or a lightweight thread. <p> The disadvantage is that it requires locking shared state, most significantly the protocol state at each layer. Systems using this approach include <ref> [3, 13] </ref>. In functional parallelism, a protocol layer's functions are the unit of concurrency. Functions within a single protocol layer (e.g., checksum, ACK generation) are decomposed, and each assigned to a processing element. <p> To avoid this socket-crossing cost, we replaced the simulated driver with in-memory device drivers for the TCP protocol stacks. The drivers emulate a high-speed FDDI interface, and support the FDDI maximum transmission unit (MTU) of slightly over 4K bytes. This is similar to the approaches taken in <ref> [3, 13, 20, 34] </ref>. The drivers act as senders or receivers, producing or consuming packets as fast as possible, to simulate the behavior of simplex data transfer over an error-free network. To minimize execution time and experimental perturbation, the receive-side drivers use preconstructed packet templates.
Reference: [14] <author> K. E. Hickman and T. Elgamal. </author> <title> The SSL protocol. Work in progress, Internet Draft (ftp://ds.internic.net/internet-drafts/draft-hickman-netscape-ssl-01.txt, </title> <month> June </month> <year> 1995. </year>
Reference: [15] <author> N. C. Hutchinson and L. L. Peterson. </author> <title> The x-Kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: In this paper we demonstrate that parallelism is an effective vehicle for improving software cryptographic performance. We show linear speedup results of several different Internet-based cryptographic protocol stacks using two different approaches to parallelism. Our implementation consists of parallelized versions of the x-kernel <ref> [15] </ref> that run in user space on Silicon Graphics shared-memory multiprocessors. We give performance results on a 12 processor 100MHz MIPS R4400 SGI Challenge XL. The remainder of the paper is organized as follows: In Section 2, we discuss several approaches to parallelism. Section 3 describes our implementation and experiments. <p> We do discuss them in more depth in Section 5. 3 Implementation and Experiments In this section we describe our implementation and experimental environment. Our implementation consists of parallelized versions of the x-kernel <ref> [15] </ref> extended for packet-level parallelism [24] and connection-level parallelism [38]. Our parallel implementations run in user space on Silicon Graphics shared-memory multiprocessors using the IRIX operating system. In this study, we examine the impact of cryptographic protocols under the respective paradigms, and show how parallelism improves cryptographic performance.
Reference: [16] <author> V. Jacobson. </author> <title> Efficient protocol implementation. </title> <booktitle> In ACM SIG-COMM 1990 Tutorial Notes, </booktitle> <address> Philadelphia, PA, </address> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: Thus, lock 2 ing is kept to a minimum along the "fast path" of data trans-fer. The disadvantage with connection-level parallelism is that no concurrency within a single connection can be achieved, which may be a problem if traffic exhibits locality <ref> [16, 19, 22, 27] </ref>, i.e., is bursty. Systems using this approach include [29, 33, 34, 38]. In packet-level parallelism, packets are the unit of concurrency. Sometimes referred to as thread-per-packet or processor-per-message, packet-level parallelism assigns each packet or message to a single processing element.
Reference: [17] <author> O. G. Koufopavlou and M. Zitterbart. </author> <title> Parallel TCP for high performance communication subsystems. </title> <booktitle> In Proceedings of the IEEE Global Telecommunications Conference (GLOBE-COM), </booktitle> <pages> pages 1395-1399, </pages> <year> 1992. </year>
Reference-contexts: The advantage to this approach is that it is relatively fine-grained, and thus can improve latency as well as throughput. The disadvantage is that it requires synchronizing within a protocol layer, and is dependent upon the concurrency available between the functions of a particular layer. Examples include <ref> [17, 28, 25] </ref>. In data-level parallelism, the pieces of data are the units of concurrency, analogous to SIMD processing. Processing elements are assigned to the same function of a particular layer, but perform processing on separate pieces of data from the same message.
Reference: [18] <author> S. J. Leffler, M. McKusick, M. Karels, and J. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: The two schemes do have some implementation differences that are necessitated by their respective approaches to concurrency. However, wherever possible, we have strived to make the implementations consistent. For example, both use the same TCP uniprocessor source code base, which is derived from Berkeley's Net/2 TCP <ref> [18] </ref> with BSD 4.4 fixes, but not the RFC1323 extensions [4]. 1 We use a double-Sbox implementation in our tests. 3.3 In-Memory Drivers Since our platform runs in user space, accessing the FDDI adaptor involves crossing the IRIX socket layer, which is prohibitively expensive.
Reference: [19] <author> W. E. Leland, M. S. Taqqu, W. Willinger, and D. V. Wilson. </author> <title> On the self-similar nature of Ethernet traffic. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 2(1) </volume> <pages> 1-15, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: Thus, lock 2 ing is kept to a minimum along the "fast path" of data trans-fer. The disadvantage with connection-level parallelism is that no concurrency within a single connection can be achieved, which may be a problem if traffic exhibits locality <ref> [16, 19, 22, 27] </ref>, i.e., is bursty. Systems using this approach include [29, 33, 34, 38]. In packet-level parallelism, packets are the unit of concurrency. Sometimes referred to as thread-per-packet or processor-per-message, packet-level parallelism assigns each packet or message to a single processing element.
Reference: [20] <author> B. Lindgren, B. Krupczak, M. Ammar, and K. Schwan. </author> <title> Parallel and configurable protocols: Experience with a prototype and an architectural framework. </title> <booktitle> In Proceedings of the International Conference on Network Protocols, </booktitle> <pages> pages 234-242, </pages> <address> San Francisco, CA, </address> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: To avoid this socket-crossing cost, we replaced the simulated driver with in-memory device drivers for the TCP protocol stacks. The drivers emulate a high-speed FDDI interface, and support the FDDI maximum transmission unit (MTU) of slightly over 4K bytes. This is similar to the approaches taken in <ref> [3, 13, 20, 34] </ref>. The drivers act as senders or receivers, producing or consuming packets as fast as possible, to simulate the behavior of simplex data transfer over an error-free network. To minimize execution time and experimental perturbation, the receive-side drivers use preconstructed packet templates.
Reference: [21] <author> J. Mogul, R. Rashid, and M. Accetta. </author> <title> The packet filter: An efficient mechanism for user-level network code. </title> <booktitle> In Proceedings 11th Symposium on Operating System Principles, </booktitle> <pages> pages 39-51, </pages> <address> Austin, TX, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: In the connection-level parallel testbed, connections are assigned to threads on one-to-one basis, called thread-per-connection. The entire connection forms the unit of con-currency, only allowing one thread to perform protocol processing for any particular connection. Arriving packets are demultiplexed to the appropriate thread via a packet-filter mechanism <ref> [21] </ref>. The notion of a connection is extended through the entire protocol stack. Where possible, data structures are replicated per-thread, in order to avoid locking and therefore contention. The two schemes do have some implementation differences that are necessitated by their respective approaches to concurrency.
Reference: [22] <author> J. C. Mogul. </author> <title> Network locality at the scale of processes. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(2) </volume> <pages> 81-109, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Thus, lock 2 ing is kept to a minimum along the "fast path" of data trans-fer. The disadvantage with connection-level parallelism is that no concurrency within a single connection can be achieved, which may be a problem if traffic exhibits locality <ref> [16, 19, 22, 27] </ref>, i.e., is bursty. Systems using this approach include [29, 33, 34, 38]. In packet-level parallelism, packets are the unit of concurrency. Sometimes referred to as thread-per-packet or processor-per-message, packet-level parallelism assigns each packet or message to a single processing element.
Reference: [23] <author> E. Nahum, S. O'Malley, H. Orman, and R. Schroeppel. </author> <title> Towards high-performance cryptographic software. </title> <booktitle> In Proceedings of the Third IEEE Workshop on the Architecture and Implementation of High Performance Communications Subsystems (HPCS), </booktitle> <address> Mystic, Conn, </address> <month> Aug. </month> <year> 1995. </year>
Reference: [24] <author> E. M. Nahum, D. J. Yates, J. F. Kurose, and D. Towsley. </author> <title> Performance issues in parallelized network protocols. </title> <booktitle> In First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 125-137, </pages> <address> Monterey, CA, </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: We do discuss them in more depth in Section 5. 3 Implementation and Experiments In this section we describe our implementation and experimental environment. Our implementation consists of parallelized versions of the x-kernel [15] extended for packet-level parallelism <ref> [24] </ref> and connection-level parallelism [38]. Our parallel implementations run in user space on Silicon Graphics shared-memory multiprocessors using the IRIX operating system. In this study, we examine the impact of cryptographic protocols under the respective paradigms, and show how parallelism improves cryptographic performance. <p> Our protocols are taken from the cryptographic suite available with the x-kernel [26]. 3.2 Parallel Infrastructure The implementations for packet-level parallel protocols and connection-level parallel protocols are described in detail in <ref> [24, 38] </ref>. We briefly outline them here. In the packet-level parallel protocol testbed, packets are assigned to threads as they arrive, regardless of the connections they are associated with. <p> Using Triple-DES is 3 times slower at 1.5 Mbits/sec. the send-side tests, where speedup is throughput normalized relative to the uniprocessor throughput for the appropriate stack. The theoretical ideal linear speedup is included for comparison. Previous work <ref> [3, 24] </ref> has shown limited performance gains when using packet-level parallelism for a single TCP connection, barring any other protocol processing, and this is reflected by the baseline TCP/IP stack's minimal speedup.
Reference: [25] <author> A. N. Netravali, W. D. Roome, and K. Sabnani. </author> <title> Design and implementation of a high-speed transport protocol. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 38(11) </volume> <pages> 2010-2024, </pages> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: The advantage to this approach is that it is relatively fine-grained, and thus can improve latency as well as throughput. The disadvantage is that it requires synchronizing within a protocol layer, and is dependent upon the concurrency available between the functions of a particular layer. Examples include <ref> [17, 28, 25] </ref>. In data-level parallelism, the pieces of data are the units of concurrency, analogous to SIMD processing. Processing elements are assigned to the same function of a particular layer, but perform processing on separate pieces of data from the same message.
Reference: [26] <author> H. Orman, S. O'Malley, R. Schroeppel, and D. Schwartz. </author> <title> Paving the road to network security, or the value of small cobblestones. </title> <booktitle> In Proceedings of the 1994 Internet Society Symposium on Network and Distributed System Security, </booktitle> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: To guard against this, we fill each buffer at the beginning of a test with a random sequence, with each test using a different initial seed value. Our protocols are taken from the cryptographic suite available with the x-kernel <ref> [26] </ref>. 3.2 Parallel Infrastructure The implementations for packet-level parallel protocols and connection-level parallel protocols are described in detail in [24, 38]. We briefly outline them here. In the packet-level parallel protocol testbed, packets are assigned to threads as they arrive, regardless of the connections they are associated with.
Reference: [27] <author> V. Paxson and S. Floyd. </author> <title> Wide-area traffic: The failure of poisson modeling. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 3(3) </volume> <pages> 226-244, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Thus, lock 2 ing is kept to a minimum along the "fast path" of data trans-fer. The disadvantage with connection-level parallelism is that no concurrency within a single connection can be achieved, which may be a problem if traffic exhibits locality <ref> [16, 19, 22, 27] </ref>, i.e., is bursty. Systems using this approach include [29, 33, 34, 38]. In packet-level parallelism, packets are the unit of concurrency. Sometimes referred to as thread-per-packet or processor-per-message, packet-level parallelism assigns each packet or message to a single processing element.
Reference: [28] <author> T. F. L. Porta and M. Schwartz. </author> <title> Performance analysis of MSP: Feature-rich high-speed transport protocol. </title> <journal> IEEE Transactions on Networking, </journal> <volume> 1(6) </volume> <pages> 740-753, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: The advantage to this approach is that it is relatively fine-grained, and thus can improve latency as well as throughput. The disadvantage is that it requires synchronizing within a protocol layer, and is dependent upon the concurrency available between the functions of a particular layer. Examples include <ref> [17, 28, 25] </ref>. In data-level parallelism, the pieces of data are the units of concurrency, analogous to SIMD processing. Processing elements are assigned to the same function of a particular layer, but perform processing on separate pieces of data from the same message.
Reference: [29] <author> D. Presotto. </author> <title> Multiprocessor Streams for Plan 9. </title> <booktitle> In Proceedings United Kingdom UNIX Users Group, </booktitle> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: The disadvantage with connection-level parallelism is that no concurrency within a single connection can be achieved, which may be a problem if traffic exhibits locality [16, 19, 22, 27], i.e., is bursty. Systems using this approach include <ref> [29, 33, 34, 38] </ref>. In packet-level parallelism, packets are the unit of concurrency. Sometimes referred to as thread-per-packet or processor-per-message, packet-level parallelism assigns each packet or message to a single processing element.
Reference: [30] <author> E. Rescorla and A. M. Schiffman. </author> <title> The secure hypertext transfer protocol. Work in progress, Internet Draft (ftp://ds.internic.net/internet-drafts/draft-ietf-wts-shttp-00.txt, </title> <month> July </month> <year> 1995. </year>
Reference: [31] <author> R. Rivest. </author> <title> The MD5 message-digest algorithm. Request for Comments (Informational) RFC 1321, </title> <institution> Internet Engineering Task Force, </institution> <month> Apr. </month> <year> 1992. </year>
Reference: [32] <author> R. Rivest, A. Shamir, and L. Adleman. </author> <title> A method for obtaining digital signatures and public-key cryptosystems. </title> <journal> Communications of the ACM, </journal> <pages> pages 120-126, </pages> <month> Feb. </month> <year> 1978. </year>
Reference: [33] <author> S. Saxena, J. K. Peacock, F. Yang, V. Verma, and M. Krish-nan. </author> <title> Pitfalls in multithreading SVR4 STREAMS and other weightless processes. </title> <booktitle> In Winter 1993 USENIX Technical Conference, </booktitle> <pages> pages 85-96, </pages> <address> San Diego, CA, </address> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: The disadvantage with connection-level parallelism is that no concurrency within a single connection can be achieved, which may be a problem if traffic exhibits locality [16, 19, 22, 27], i.e., is bursty. Systems using this approach include <ref> [29, 33, 34, 38] </ref>. In packet-level parallelism, packets are the unit of concurrency. Sometimes referred to as thread-per-packet or processor-per-message, packet-level parallelism assigns each packet or message to a single processing element.
Reference: [34] <author> D. C. Schmidt and T. Suda. </author> <title> Measuring the impact of alternative parallel process architectures on communication subsystem performance. </title> <booktitle> Fourth IFIP WG6.1/WG6.4 International Workshop on Protocols for High-Speed Networks, </booktitle> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: The main advantage of layered parallelism is that it is simple and defines a clean separation between protocol boundaries. The disadvantages are that concurrency is limited to the number of layers in the stack, and that associating processing with layers results in increased context switching and synchronization between layers <ref> [5, 6, 34] </ref>. Performance gains are limited to throughput, mainly achieved through pipelining effects. An example is found in [12]. Connections form the unit of concurrency in connection-level parallelism, where connections are assigned to processing elements. Speedup is achieved using multiple connections, each of which is processed in parallel. <p> The disadvantage with connection-level parallelism is that no concurrency within a single connection can be achieved, which may be a problem if traffic exhibits locality [16, 19, 22, 27], i.e., is bursty. Systems using this approach include <ref> [29, 33, 34, 38] </ref>. In packet-level parallelism, packets are the unit of concurrency. Sometimes referred to as thread-per-packet or processor-per-message, packet-level parallelism assigns each packet or message to a single processing element. <p> Most importantly, they depend on the available concurrency within a protocol stack. The most comprehensive study to date comparing different approaches to parallelism on a shared-memory multiprocessor is by Schmidt and Suda <ref> [34, 35] </ref>. They show that packet-level parallelism and connection-level parallelism generally perform better than layer parallelism, due to the context-switching overhead incurred crossing protocol boundaries using layer parallelism. <p> To avoid this socket-crossing cost, we replaced the simulated driver with in-memory device drivers for the TCP protocol stacks. The drivers emulate a high-speed FDDI interface, and support the FDDI maximum transmission unit (MTU) of slightly over 4K bytes. This is similar to the approaches taken in <ref> [3, 13, 20, 34] </ref>. The drivers act as senders or receivers, producing or consuming packets as fast as possible, to simulate the behavior of simplex data transfer over an error-free network. To minimize execution time and experimental perturbation, the receive-side drivers use preconstructed packet templates.
Reference: [35] <author> D. C. Schmidt and T. Suda. </author> <title> Measuring the performance of parallel message-based process architectures. </title> <booktitle> In Proceedings of the Conference on Computer Communications (IEEE Infocom), </booktitle> <address> Boston, MA, </address> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Most importantly, they depend on the available concurrency within a protocol stack. The most comprehensive study to date comparing different approaches to parallelism on a shared-memory multiprocessor is by Schmidt and Suda <ref> [34, 35] </ref>. They show that packet-level parallelism and connection-level parallelism generally perform better than layer parallelism, due to the context-switching overhead incurred crossing protocol boundaries using layer parallelism. <p> The most comprehensive study to date comparing different approaches to parallelism on a shared-memory multiprocessor is by Schmidt and Suda [34, 35]. They show that packet-level parallelism and connection-level parallelism generally perform better than layer parallelism, due to the context-switching overhead incurred crossing protocol boundaries using layer parallelism. In <ref> [35] </ref>, they suggest that packet-level parallelism is preferable when the workload is a relatively small number of active connections, and that connection-level parallelism is preferable for large numbers of connections. In this paper, we restrict our focus to connection-level and packet-level parallel approaches to network cryptographic protocols on shared-memory multiprocessors.
Reference: [36] <author> B. Schneier. </author> <title> Applied Cryptography. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference: [37] <author> J. </author> <title> Touch. Performance analysis of MD5. </title> <booktitle> In ACM SIGCOMM Symposium on Communications Architecturesand Protocols, </booktitle> <address> Boston MA, </address> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: For example, the MD5 stack achieves a speedup of 8 with 12 processors, and the DES and Triple-DES stacks produce very close to linear speedup. This 2 MD5 runs 30-50% slower on big-endian hosts <ref> [37] </ref>, such as our Challenge. Protocol Send Inc. Recv Inc.

References-found: 37

