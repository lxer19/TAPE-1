URL: http://www.medg.lcs.mit.edu/ftp/psz/globcond.ps
Refering-URL: http://www.medg.lcs.mit.edu/ftp/psz/
Root-URL: 
Email: shachter@camis.stanford.edu  ska@miba.auc.dk  psz@lcs.mit.edu  
Title: Global Conditioning for Probabilistic Inference in Belief Networks  
Author: Ross D. Shachter Stig. K. Andersen Peter Szolovits 
Keyword: causality, belief networks, causal networks, planning  
Note: under uncertainty, troubleshooting.  
Address: Stanford, CA 94305-4025  DK-9220 Aalborg SO DENMARK  545 Technology Square Cambridge, MA 02139  
Affiliation: Department of Engineering-Economic Systems Stanford University  Dept. of Medical Informatics and Image Analysis Aalborg University  Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: In this paper we propose a new approach to probabilistic inference on belief networks, global conditioning, which is a simple generalization of Pearl's (1986b) method of loop-cutset conditioning. We show that global conditioning, as well as loop-cutset conditioning, can be thought of as a special case of the method of Lauritzen and Spiegelhalter (1988) as refined by Jensen et al (1990a; 1990b). Nonetheless, this approach provides new opportunities for parallel processing and, in the case of sequential processing, a tradeoff of time for memory. We also show how a hybrid method (Suermondt and others 1990) combining loop-cutset conditioning with Jensen's method can be viewed within our framework. By exploring the relationships between these methods, we develop a unifying framework in which the advantages of each approach can be combined successfully.
Abstract-found: 1
Intro-found: 1
Reference: <author> Beeri, C., R. Fagin, D. Maier, and M. Yannakakis. </author> <title> "On the Desirability of Acyclic Database Schemes." </title> <editor> J. </editor> <booktitle> ACM 30 (3 1983): </booktitle> <pages> 479-513. </pages>
Reference-contexts: A set of elements for this purpose will be called a cluster. An undirected tree of clusters will be called a join tree if every element which appears in more than one cluster appears in every cluster on the path between them <ref> (Beeri and others 1983) </ref>. A cluster tree for a particular belief network is a join tree in which every family from the belief network is contained in at least one cluster.
Reference: <author> Cannings, C, E. A. Thompson, and M. H. Skolnick. </author> <title> "Recursive derivation of likelihoods on pedigrees of arbitrary complexity." </title> <journal> Adv. Appl. Probabil. </journal> <volume> 8 (1976): </volume> <pages> 622-625. </pages>
Reference: <author> Cannings, C, E. A. Thompson, and M. H. Skolnick. </author> <title> "Probability functions on complex pedigrees." </title> <journal> Adv. Appl. </journal> <note> Probabil. 10 (1978): 26- 61. </note>
Reference: <author> Cooper, G.G. </author> <title> Bayesian belief-network inference using recursive decomposition. </title> <institution> Knowledge Systems Laboratory, Stanford University, </institution> <year> 1990. </year> <month> KSL 90-05. </month>
Reference: <author> D'Ambrosio, B. </author> <title> "Local Expression Languages for Probabilistic Dependence." </title> <booktitle> In Uncertainty in Artificial Intelligence: Proceedings of the Seventh Conference, </booktitle> <editor> eds. B D'Ambrosio, P Smets, and P Bonissone. </editor> <address> 95- 102. San Mateo, CA: </address> <publisher> Morgan Kauf-mann, </publisher> <year> 1991. </year>
Reference: <author> Geiger, D., T. Verma, and J. Pearl. </author> <title> "Identifying independence in Bayesian networks." </title> <booktitle> Networks 20 (1990): </booktitle> <pages> 507-534. </pages>
Reference: <author> Heckerman, D. E. </author> <title> "Probabilistic Similarity Networks." </title> <booktitle> Networks 20 (1990a): </booktitle> <pages> 607-636. </pages>
Reference: <author> Heckerman, D E. </author> <title> "Probabilistic Similarity Networks." </title> <type> PhD Thesis, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <year> 1990b. </year>
Reference: <author> Jensen, F and S K Andersen. </author> <title> "Approximations in Bayesian Belief Universes for Knowledge-Based Systems." </title> <booktitle> In Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence in July 27-29, </booktitle> <address> Cambridge, MA, 162-169, </address> <year> 1990. </year>
Reference-contexts: There are also tremendous opportunities for savings in the compiled compression of zeros from all of the distributions <ref> (Jensen and Andersen 1990) </ref>, which means that most components of a sparse message and joint distribution are never computed. The only cost comes when evidence is retracted, and the whole network might have to be reinitialized.
Reference: <author> Jensen, F. V., S. L. Lauritzen, and K. G. Olesen. </author> <title> "Bayesian Updating in Causal Probabilistic Networks by Local Computations." </title> <journal> Comp. Stats. Q. </journal> <volume> 4 (1990a): </volume> <pages> 269-282. </pages>
Reference-contexts: For example, the four trees drawn in Figure 2 are cluster trees for the belief network drawn in Figure 1a. Cluster trees have been recognized before (as "junction trees") and some of their special properties are well known <ref> (Jensen and others 1990a) </ref>. The moral graph corresponding to a cluster tree consists of nodes for each element with an undirected arc between any two nodes whose corresponding elements appear in the same cluster.
Reference: <author> Jensen, F. V., K. G. Olesen, and S. K. Andersen. </author> <title> "An algebra of Bayesian belief universes for knowledge based systems." </title> <booktitle> Networks 20 (1990b): </booktitle> <pages> 637-659. </pages>
Reference-contexts: Such a directed graph has many names in the literature, including belief network (Pearl 1986b), probabilistic influence diagram (Shachter 1988), and causal probabilistic network <ref> (Jensen and others 1990b) </ref>. We refer to a particular element with a lower case letter, j 2 N; and a set of elements using upper case letters, J N . <p> Second, if all that is required is the computation of the posterior distribution for a particular cluster S i , that can be performed with local computations through a collect operation <ref> (Jensen and others 1990b) </ref>: 1. S i sends request messages to all of its neighbors; 2. They in turn send request messages to all of their other neighbors and so forth; 3. Eventually a request message reaches a cluster with only one neighbor. <p> Third, if all clusters are to be updated, then a distribute operation can also be performed with local operations <ref> (Jensen and others 1990b) </ref>: S i sends updated cluster messages to each of its neighbors, and they in turn send them to their other neighbors and so forth until every cluster has received such a mes sage.
Reference: <author> Kim, J.H. and J. Pearl. </author> <title> "A computational model for causal and diagnostic reasoning in inference engines." </title> <booktitle> In 8th International Joint Conference on Artificial Intelligence in Karlsruhe, </booktitle> <address> West Germany 1983. </address>
Reference: <author> Kjrulff, U. </author> <title> Triangulation of Graphs|Algorithms Giving Small Total State Space. </title> <institution> Department of Mathematics and Computer Science, Institute for Electronic Systems, Aalborg Denmark, </institution> <year> 1990. </year> <pages> R 90-09. </pages>
Reference-contexts: The key step is triangulation, in which the moral graph for the original belief network is made into a chordal graph. Although determining the "optimal" triangulation is NP-complete, there are excellent heuristics for this task <ref> (Kjrulff 1990) </ref>. Some of the methods considered in this paper also present alternative triangulation strategies (Cooper 1990; Pearl 1986a; Pearl 1988; Suermondt and Cooper 1988; Suermondt and Cooper 1990).
Reference: <author> Lauritzen, S. L., A. P. Dawid, B. N. Larsen, and H.-G. Leimer. </author> <title> "Independence properties of directed markov fields." </title> <booktitle> Networks 20 (1990): </booktitle> <pages> 491-505. </pages>
Reference: <author> Lauritzen, </author> <title> S.L. and D.J. Spiegelhalter. "Local computations with probabilities on graphical structures and their application to expert systems." </title> <booktitle> JRSS B 50 (2 1988): </booktitle> <pages> 157-224. </pages>
Reference-contexts: It might seem counterintuitive that Loop-Cutset Conditioning is related to the Clustering Algorithm. Consider the network shown in Figure 5a suggested by Pearl in the discussion following <ref> (Lauritzen and Spiegelhalter 1988) </ref>. A "standard" chordal graph for this problem in shown in Figure 5b.
Reference: <author> Pearl, J. </author> <title> "A constraint-propagation approach to probabilistic reasoning." </title> <booktitle> In Uncertainty in Artificial Intelligence, </booktitle> <editor> eds. L.N. Kanal and J.F. Lemmer. </editor> <address> 357-369. Amsterdam: </address> <publisher> North-Holland, </publisher> <year> 1986a. </year>
Reference-contexts: Initialization is not difficult however, with P old ij set to scalar 1, and the remaining initialization process similar to the factored form. 4 GLOBAL CONDITIONING The Methods of Global and Loop-Cutset Conditioning are presented in this section. Loop-Cutset Condition ing <ref> (Pearl 1986a) </ref> is one of the oldest methods for solving multiply-connected belief networks. We show that both conditioning methods can be viewed as special cases of the clustering algorithm. <p> Theorem 3 (Loop-Cutset Conditioning) The Method of Loop-Cutset Conditioning <ref> (Pearl 1986a) </ref> in modified form (Peot and Shachter 1991) is a special case of the Clustering Algorithm. It is less efficient than the Clustering Algorithm unless the conditioning set already appears in every family in the belief network.
Reference: <author> Pearl, J. </author> <title> "Fusion, propagation and structuring in belief networks." </title> <booktitle> AIJ 29 (3 1986b): </booktitle> <pages> 241-288. </pages>
Reference-contexts: Such a directed graph has many names in the literature, including belief network <ref> (Pearl 1986b) </ref>, probabilistic influence diagram (Shachter 1988), and causal probabilistic network (Jensen and others 1990b). We refer to a particular element with a lower case letter, j 2 N; and a set of elements using upper case letters, J N .
Reference: <author> Pearl, J. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference: <author> Peot, M. A. and R. D. Shachter. </author> <title> "Fusion and Propagation with Multiple Observations in Belief Networks." </title> <booktitle> AIJ 48 (3 1991): </booktitle> <pages> 299-318. </pages>
Reference-contexts: The Clustering Algorithm is then performing precisely the Revised Polytree Algorithm, in which causal support messages are unconditional rather than conditional <ref> (Peot and Shachter 1991) </ref>. Consider the belief network shown in to render the network singly-connected. The cluster tree for the Polytree method is shown in Figure 3b. In general, the following theorem applies. <p> <ref> (Peot and Shachter 1991) </ref>. Consider the belief network shown in to render the network singly-connected. The cluster tree for the Polytree method is shown in Figure 3b. In general, the following theorem applies. Theorem 1 (The Polytree Algorithm) The Polytree Algorithm (Kim and Pearl 1983; Pearl 1986b) in modified form (Peot and Shachter 1991) is a special case of the Clustering Algorithm. There are also choices in the representation of the cluster messages. We have presented the messages in factored form, which simplifies the revision of prior probability distributions and the retraction of evidence. <p> Theorem 3 (Loop-Cutset Conditioning) The Method of Loop-Cutset Conditioning (Pearl 1986a) in modified form <ref> (Peot and Shachter 1991) </ref> is a special case of the Clustering Algorithm. It is less efficient than the Clustering Algorithm unless the conditioning set already appears in every family in the belief network.
Reference: <author> Shachter, R. D. </author> <title> "Probabilistic Inference and Influence Diagrams." </title> <booktitle> Ops. Rsrch. 36 (July-August 1988): </booktitle> <pages> 589-605. </pages>
Reference-contexts: Such a directed graph has many names in the literature, including belief network (Pearl 1986b), probabilistic influence diagram <ref> (Shachter 1988) </ref>, and causal probabilistic network (Jensen and others 1990b). We refer to a particular element with a lower case letter, j 2 N; and a set of elements using upper case letters, J N .
Reference: <author> Shachter, R. D. </author> <title> "Evidence Absorption and Propagation through Evidence Reversals." </title> <booktitle> In Uncertainty in Artificial Intelligence 5, </booktitle> <editor> eds. M. Henrion, </editor> <publisher> R. </publisher>
Reference: <editor> D. Shachter, J.F. Lemmer, and L.N. Kanal. </editor> <address> 173-190. Amsterdam: </address> <publisher> North-Holland, </publisher> <year> 1990a. </year>
Reference-contexts: Having "observed" X K , we can cut the outgoing arcs from K in the belief network, which is equivalent to separation by K in the moral graph <ref> (Shachter 1990a) </ref>.
Reference: <author> Shachter, R. D. </author> <title> "An Ordered Examination of Influence Diagrams." </title> <booktitle> Networks 20 (1990b): </booktitle> <pages> 535-563. </pages>
Reference: <author> Shafer, G. and P. P. Shenoy. </author> <title> "Probability Propagation." </title> <journal> Ann. Math. and AI 2 (1990): </journal> <pages> 327-352. </pages>
Reference: <author> Shenoy, P. P. </author> <title> "Propagating Belief Functins using Local Computations." </title> <booktitle> IEEE Expert 1 (3 1986): </booktitle> <pages> 43-52. </pages>
Reference: <author> Suermondt, H.J. and G.F. Cooper. </author> <title> "Updating probabilities in multiply connected belief networks." </title> <booktitle> In Fourth Workshop on Uncertainty in Artificial Intelligence in University of Minnesota, Min-neapolis, </booktitle> <pages> 335-343, </pages> <year> 1988. </year>
Reference: <author> Suermondt, H.J. and G.F. Cooper. </author> <title> "Probabilistic inference in multiply connected belief networks using loop cutsets." </title> <booktitle> IJAR 4 (1990): </booktitle> <pages> 283-306. </pages>
Reference-contexts: Even though Global Conditioning is shown to be a special case of the Clustering Algorithm, the insights it provides suggest new applications with parallel processing and under memory restrictions. We also provide a Clustering Algorithm interpretation to a hybrid algorithm which combines Jensen's method with Loop-Cutset Conditioning <ref> (Suermondt and others 1990) </ref>. Section 2 defines the notation and terms to be used throughout the paper. Section 3 introduces the Clustering Algorithm while Section 4 presents the Method of Global Conditioning. They also show the relationship to the Polytree and Loop-Cutset Conditioning Methods. <p> In this section, we explore a subtle trick that emerges from an ingenious application of Loop-Cutset Conditioning <ref> (Suermondt and others 1990) </ref> to a "star-shaped" diagnostic network constructed using similarity networks (Heckerman 1990a; Heckerman 1990b). In that problem there is a single variable (disease) whose observation splits the belief network into many small disconnected pieces.
Reference: <author> Suermondt, H J, G F Cooper, and D E Heckerman. </author> <title> "A Combination of Cutset Conditioning with Clique-Tree Propagation in the Pathfinder System." </title> <booktitle> In Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence in July 27-29, </booktitle> <address> Cambridge, MA, 273-279, </address> <year> 1990. </year>
Reference-contexts: Even though Global Conditioning is shown to be a special case of the Clustering Algorithm, the insights it provides suggest new applications with parallel processing and under memory restrictions. We also provide a Clustering Algorithm interpretation to a hybrid algorithm which combines Jensen's method with Loop-Cutset Conditioning <ref> (Suermondt and others 1990) </ref>. Section 2 defines the notation and terms to be used throughout the paper. Section 3 introduces the Clustering Algorithm while Section 4 presents the Method of Global Conditioning. They also show the relationship to the Polytree and Loop-Cutset Conditioning Methods. <p> In this section, we explore a subtle trick that emerges from an ingenious application of Loop-Cutset Conditioning <ref> (Suermondt and others 1990) </ref> to a "star-shaped" diagnostic network constructed using similarity networks (Heckerman 1990a; Heckerman 1990b). In that problem there is a single variable (disease) whose observation splits the belief network into many small disconnected pieces.
References-found: 28

