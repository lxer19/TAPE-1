URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/pricai_rat-96.ps
Refering-URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/
Root-URL: http://www.csc.ncsu.edu
Email: singh@ncsu.edu  
Title: Commitments in the Architecture of a Limited, Rational Agent  
Author: Munindar P. Singh 
Address: Raleigh, NC 27695-8206, USA  
Affiliation: Department of Computer Science North Carolina State University  
Abstract: Rationality is a useful metaphor for understanding autonomous, intelligent agents. A persuasive view of intelligent agents uses cognitive primitives such as intentions and beliefs to describe, explain, and specify their behavior. These primitives are often associated with a notion of commitment that is internal to the given agent. However, at first sight, there is a tension between commitments and rationality. We show how the two concepts can be reconciled for the important and interesting case of limited, intelligent agents. We show how our approach extends to handle more subtle issues such as precommitments, which have previously been assumed to be conceptually too complex. We close with a proposal to develop conative policies as a means to represent commitments in a generic, declarative manner. 
Abstract-found: 1
Intro-found: 1
Reference: [ Brand, 1984 ] <author> Brand, </author> <month> Myles; </month> <year> 1984. </year> <title> Intending and Acting. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Our interest here is in intentions. Intentions denote an agent's pro-attitude toward a proposition or action. Intentions are usually defined to be mutually consistent, compatible with beliefs, and direct or immediate causes of action (e.g., <ref> [ Brand, 1984, p. 46 ] </ref> ). For the above reasons, intentions are distinct from desires (which may be mutually inconsistent or incompatible with beliefs, and may not lead to actions) and beliefs (which do not in themselves lead to action). <p> For the above reasons, intentions are distinct from desires (which may be mutually inconsistent or incompatible with beliefs, and may not lead to actions) and beliefs (which do not in themselves lead to action). This view is supported by a number of philosophers, e.g., <ref> [ Brand, 1984, pp. 121-125 ] </ref> , [ Bratman, 1987, pp. 18-23 ] , and [ Harman, 1986, pp. 78-79 ] . We restrict ourselves to intentions that are future-directed, i.e., geared toward future actions or conditions.
Reference: [ Bratman, 1987 ] <author> Bratman, Michael E.; </author> <year> 1987. </year> <title> Intention, Plans, and Practical Reason. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: This view is supported by a number of philosophers, e.g., [ Brand, 1984, pp. 121-125 ] , <ref> [ Bratman, 1987, pp. 18-23 ] </ref> , and [ Harman, 1986, pp. 78-79 ] . We restrict ourselves to intentions that are future-directed, i.e., geared toward future actions or conditions.
Reference: [ Brooks, 1991 ] <author> Brooks, </author> <title> Rodney; 1991. Intelligence without reason. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI). Computers and Thought Award Lecture. </booktitle>
Reference-contexts: However, if the agent has to decide whether a given intention is beneficial or not repeatedly, the concept of commitment is both descriptively and prescriptively redundant|the agent can just perform the optimal action at each moment! Indeed, this unwittingly supports the position taken by <ref> [ Brooks, 1991 ] </ref> and others that cognitive concepts can be dispensed with entirely in the study of agents. Our chief reason for including cognitive concepts, however, is that they provide a high-level, flexible, declarative means to describe agents.
Reference: [ Castelfranchi, 1995 ] <author> Castelfranchi, </author> <title> Cristiano; 1995. Commitments: From individual intentions to groups and organizations. </title> <booktitle> In Proceedings of the International Conference on Multiagent Systems. </booktitle>
Reference-contexts: We restrict ourselves to intentions that are future-directed, i.e., geared toward future actions or conditions. The literature over the past decade or so agrees on the idea that intentions involve some commitment on part of the given agent. This commitment is "psy-chological" rather than "social" <ref> [ Castelfranchi, 1995; Singh, 1996 ] </ref> . An agent is committed privately to his intentions, independently of his public obligations. 2.1 Why Commitments are Useful Commitments cause an agent to continue to hold on to his intentions over time, and to try repeatedly to achieve them. Example 1.
Reference: [ Cohen & Levesque, 1990 ] <author> Cohen, Philip R. and Levesque, Hector J.; </author> <year> 1990. </year> <title> Intention is choice with commitment. </title> <booktitle> Artificial Intelligence 42 </booktitle> <pages> 213-261. </pages>
Reference-contexts: tasks quickly; (2) the cost of reasoning is high; (3) the agent cannot consider all relevant aspects of the world on the fly; or (4) the agent has a pretty good model of the world, so that the losses of opportunity are limited. 2.4 Traditional Approaches Briefly, traditional theories, e.g., <ref> [ Rao & Georgeff, 1991; Cohen & Levesque, 1990 ] </ref> , appear to suggest that an agent ought to be committed to an intention only as long as it is beneficial, and ought to give it up as soon as it is not. <p> Our chief reason for including cognitive concepts, however, is that they provide a high-level, flexible, declarative means to describe agents. Commitments as Persistence A well-known traditional approach captures commitment as a form of persistence over time (this is in the definition of "persistent goal") <ref> [ Cohen & Levesque, 1990, p. 236 ] </ref> . Intentions are defined as special kinds of persistent goals (pp. 245, 248, 254-255).
Reference: [ Emerson, 1990 ] <author> Emerson, E. A.; </author> <year> 1990. </year> <title> Temporal and modal logic. </title> <editor> In Leeuwen, J.van, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume B. </booktitle> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, The Netherlands. </address>
Reference-contexts: Agents can have beliefs and intentions that involve objective probability and utility statements. A formal semantics is presented in Appendix A. 4.3 Formal Language The formal language of this paper, C (for CONATE ), is based on CTL* (a propositional branching time logic <ref> [ Emerson, 1990 ] </ref> ). It is augmented with (1) quantification over basic actions; (2) functions: Uti, Cost; (3) operators: B and h i; (4) predicates: C, PreC, Delib, I, and For; and (5) arithmetical operators and relations.
Reference: [ Harman, 1986 ] <author> Harman, </author> <title> Gilbert; 1986. Change in View. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: This view is supported by a number of philosophers, e.g., [ Brand, 1984, pp. 121-125 ] , [ Bratman, 1987, pp. 18-23 ] , and <ref> [ Harman, 1986, pp. 78-79 ] </ref> . We restrict ourselves to intentions that are future-directed, i.e., geared toward future actions or conditions. The literature over the past decade or so agrees on the idea that intentions involve some commitment on part of the given agent.
Reference: [ Kinny & Georgeff, 1991 ] <author> Kinny, David N. and Georgeff, Michael P.; </author> <year> 1991. </year> <title> Commitment and effectiveness of situated agents. </title> <booktitle> In IJCAI. </booktitle>
Reference-contexts: These policies could accommodate not only the general rationality requirements studied above, but also be made conditional upon properties of the domain, and the qualities of the agent, e.g., the qualities studied by <ref> [ Kinny & Georgeff, 1991 ] </ref> . One can impose constraints on the conative policies of agents, e.g., to prevent them from adopting intentions that they believe are mutually inconsistent or inconsistent with their beliefs. The conative policies embodied in an agent would not change due to ordinary deliberations.
Reference: [ McCarthy, 1979 ] <author> McCarthy, </author> <title> John; 1979. Ascribing mental qualities to machines. </title> <editor> In Ringle, Martin, editor, </editor> <booktitle> Philosophical Perspectives in Artificial Intelligence. </booktitle> <publisher> Harvester Press. </publisher> <pages> Page nos. </pages> <note> from a revised version, issued as a report in 1987. </note>
Reference-contexts: There are two dominant views about intelligent agency. Cognitive: The cognitive view borrows folk psychological metaphors, and treats agents as loci of beliefs, desires, intentions, and so on. This view is called the knowledge level [ Newell, 1982 ] or the intentional stance <ref> [ McCarthy, 1979 ] </ref> . Economic: The economic view borrows economic metaphors, and treats agents as rational beings.
Reference: [ Newell, 1982 ] <author> Newell, </author> <title> Allen; 1982. The knowledge level. </title> <booktitle> Artificial Intelligence 18(1) </booktitle> <pages> 87-127. </pages>
Reference-contexts: There are two dominant views about intelligent agency. Cognitive: The cognitive view borrows folk psychological metaphors, and treats agents as loci of beliefs, desires, intentions, and so on. This view is called the knowledge level <ref> [ Newell, 1982 ] </ref> or the intentional stance [ McCarthy, 1979 ] . Economic: The economic view borrows economic metaphors, and treats agents as rational beings.
Reference: [ Norman & Long, 1996 ] <author> Norman, Timothy J. and Long, </author> <title> Derek; 1996. Alarms: An implementation of motivated agency. In Intelligent Agents II: Agent Theories, Architectures, </title> <booktitle> and Languages. </booktitle> <pages> 219-234. </pages>
Reference-contexts: D7. A [(Delib (x) ^ C (p; c) ^ c &gt; 0) ! B (Uti (p) = c)] Constraints Putting together the above, we require constraint D1, D5, D6, and D7. The above constraints handle cases of irrational over-commitment. However, additional primitives, e.g., those of <ref> [ Norman & Long, 1996 ] </ref> , are needed to cause agents to deliberate when the reasons for an intention are invalidated. We lack the space to formalize these here. Means and Ends Rational agents must relate their means to their ends. Intentions correspond to ends, and plans to means.
Reference: [ Pollack et al., 1994 ] <author> Pollack, Martha E.; Joslin, David; Nunes, Arthur; Ur, Sigalit; and Ephrati, </author> <month> Eithan; </month> <year> 1994. </year> <title> Experimental investigation of an agent commitment strategy. </title> <type> Technical Report 94-13, </type> <institution> Department of Computer Science, University of Pittsburgh, Pittsburgh. </institution>
Reference: [ Rao & Georgeff, 1991 ] <author> Rao, Anand S. and Georgeff, Michael P.; </author> <year> 1991. </year> <title> Modeling rational agents within a BDI-architecture. </title> <booktitle> In Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning. </booktitle> <pages> 473-484. </pages>
Reference-contexts: The formal semantics is outlined in the appendix. 2 Commitments and Rationality The cognitive view of agency leads to a BDI architecture|one which assigns beliefs, desires, and intentions to agents <ref> [ Rao & Georgeff, 1991 ] </ref> . Beliefs describe the information available to an agent; desires describe an agent's wants; intentions describe what an agent wants and has decided to act upon. Our interest here is in intentions. Intentions denote an agent's pro-attitude toward a proposition or action. <p> tasks quickly; (2) the cost of reasoning is high; (3) the agent cannot consider all relevant aspects of the world on the fly; or (4) the agent has a pretty good model of the world, so that the losses of opportunity are limited. 2.4 Traditional Approaches Briefly, traditional theories, e.g., <ref> [ Rao & Georgeff, 1991; Cohen & Levesque, 1990 ] </ref> , appear to suggest that an agent ought to be committed to an intention only as long as it is beneficial, and ought to give it up as soon as it is not.
Reference: [ Sen & Durfee, 1994 ] <author> Sen, Sandip and Durfee, Edmund H.; </author> <year> 1994. </year> <title> The role of commitment in cooperative negotiation. </title> <journal> International Journal of Intelligent and Cooperative Information Systems 3(1) </journal> <pages> 67-81. </pages>
Reference: [ Simon, 1981 ] <editor> Simon, </editor> <booktitle> Herbert; 1981. The Sciences of the Artificial. </booktitle> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference-contexts: Economic: The economic view borrows economic metaphors, and treats agents as rational beings. It has long been realized that perfect rationality is not realizable in limited agents, and theories of bounded rationality have been proposed <ref> [ Simon, 1981 ] </ref> . ? This paper synthesizes and enhances some ideas that were introduced in papers that appear in the Proceedings of the 13th Annual Conference of the Cognitive Science Society (1991) and the Proceedings of the IJCAI-91 Workshop on the Theoretical and Practical Design of Rational Agents. ??
Reference: [ Singh & Asher, 1993 ] <author> Singh, Munindar P. and Asher, Nicholas M.; </author> <year> 1993. </year> <title> A logic of intentions and beliefs. </title> <journal> Journal of Philosophical Logic 22 </journal> <pages> 513-544. </pages>
Reference-contexts: These concepts involve probabilistic and utilitarian generalizations of a framework previously used to give a logical characterization of intentions and know-how <ref> [ Singh, 1994; Singh & Asher, 1993 ] </ref> . 4.1 Actions, Branching Time, Probabilities For concepts such as intentions, commitments, and expected utility to be formalized, we need a model that includes not just time and action, but also possibility, probability, and choice.
Reference: [ Singh et al., 1993 ] <author> Singh, Munindar P.; Huhns, Michael N.; and Stephens, Larry M.; </author> <year> 1993. </year> <title> Declarative representations for multiagent systems. </title> <journal> IEEE Transactions on Knowledge and Data Engineering 5(5) </journal> <pages> 721-739. </pages>
Reference: [ Singh, 1992 ] <author> Singh, Munindar P.; </author> <year> 1992. </year> <title> A critical examination of the Cohen-Levesque theory of intentions. </title> <booktitle> In Proceedings of the 10th European Conference on Artificial Intelligence. </booktitle>
Reference-contexts: The traditional theories separately require that all goals and intentions are eventually dropped. This would eliminate the GI (p) subexpression below, but causes other repercussions, which are discussed in <ref> [ Singh, 1992 ] </ref> . D3.
Reference: [ Singh, 1994 ] <author> Singh, Munindar P.; </author> <year> 1994. </year> <title> Multiagent Systems: A Theoretical Framework for Intentions, Know-How, and Communications. </title> <publisher> Springer Verlag, </publisher> <address> Heidelberg, Germany. </address>
Reference-contexts: These concepts involve probabilistic and utilitarian generalizations of a framework previously used to give a logical characterization of intentions and know-how <ref> [ Singh, 1994; Singh & Asher, 1993 ] </ref> . 4.1 Actions, Branching Time, Probabilities For concepts such as intentions, commitments, and expected utility to be formalized, we need a model that includes not just time and action, but also possibility, probability, and choice. <p> C x (p; c) means that agent x is committed to achieving p to a level of c. Then I x (p) j (9c &gt; 0 : C x (p; c)). I x (p) means that agent x intends p. For simplicity, unlike <ref> [ Singh, 1994 ] </ref> , we assume that p has an explicit temporal component to capture the future-directedness of intentions.
Reference: [ Singh, 1996 ] <author> Singh, Munindar P.; </author> <year> 1996. </year> <title> A conceptual analysis of commitments in multiagent systems. </title> <type> Technical Report TR-96-09, </type> <institution> Department of Computer Science, North Carolina State University, </institution> <address> Raleigh, NC. </address> <note> Available at http://www4.ncsu.edu/eos/info/ dblab/www/ mpsingh/ papers/ mas/ commit.ps. </note>
Reference-contexts: We restrict ourselves to intentions that are future-directed, i.e., geared toward future actions or conditions. The literature over the past decade or so agrees on the idea that intentions involve some commitment on part of the given agent. This commitment is "psy-chological" rather than "social" <ref> [ Castelfranchi, 1995; Singh, 1996 ] </ref> . An agent is committed privately to his intentions, independently of his public obligations. 2.1 Why Commitments are Useful Commitments cause an agent to continue to hold on to his intentions over time, and to try repeatedly to achieve them. Example 1.
References-found: 20

