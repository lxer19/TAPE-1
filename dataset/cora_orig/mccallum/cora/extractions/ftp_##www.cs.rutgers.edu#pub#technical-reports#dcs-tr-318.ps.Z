URL: ftp://www.cs.rutgers.edu/pub/technical-reports/dcs-tr-318.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: MAPPING TECHNIQUES AND PERFORMANCE ANALYSIS FOR AN INTERCONNECTION CACHED MULTIPROCESSOR NETWORK  Written under the direction of  
Author: BY VIPUL GUPTA Prof. Miles Murdocca 
Degree: A dissertation submitted to the Graduate School|New Brunswick  in partial fulfillment of the requirements for the degree of Doctor of Philosophy  and Dr. Eugen Schenfeld and approved by  
Date: October, 1994  
Note: Graduate Program in Computer Science  
Address: New Jersey  Brunswick, New Jersey  
Affiliation: Rutgers, The State University of  New  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Sheldon B. Akers and Balakrishnan Krishnamurthy. </author> <title> A Group-Theoretic Model for Symmetric Interconnection Networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(4) </volume> <pages> 555-566, </pages> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: In the absence of this information, the ICN may be configured to a fixed topology and operated as a static network. Previous research has looked at the problem of finding dense graphs <ref> [1, 4, 32, 91] </ref>. Given two integers d and L, the problem calls for finding a graph with bounded degree d and diameter L that has the maximum number of nodes possible. <p> Deadlock avoidance in dense graphs: In Chapter 2 we mentioned that if no knowledge of the application's communication graph is available, a suitable dense graph topology might be considered for the global crossbar. Past research <ref> [1, 4, 75, 91] </ref> has resulted in the discovery of a number of dense graphs. Still, the general problem of deadlock free routing in such graphs is not very well understood.
Reference: [2] <author> R. Aleliunas and A. L. Rosenberg. </author> <title> On Embedding Rectangular Grids in Square Grids. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-31(9):907-913, </volume> <month> Sep </month> <year> 1982. </year>
Reference-contexts: In a network graph, nodes represent PEs and switching elements while the edges represent network links. Starting with the pioneering work of Bokhari [14], a number of researchers have looked at this mapping problem <ref> [2, 10, 13, 44, 45, 46, 63, 80] </ref>. <p> When directly applicable, we use the results of Ma and Tao [63] to map 3-D grids and tori in the 2-D mesh. In other cases, it is useful to combine their results with those of Aleliunas and Rosenberg <ref> [2] </ref>. For example, a 6fi6fi6 grid can be mapped in a 16 fi 16 mesh in two steps. First, a dilation-3 embedding of the 3-D grid in a 2-D grid of size 12 fi 18 is obtained.
Reference: [3] <author> George S. Almasi and Allan Gottlieb. </author> <title> Highly Parallel Computing. </title> <publisher> The Ben-jamin/Cummings Publishing Company, 2nd ed. </publisher> <address> edition, </address> <year> 1994. </year>
Reference-contexts: Each PE has a switch (router) which provides direct connectivity with at most four immediate neighbors (see Figure 6.1a). The Intel Paragon, the Stanford Dash and the MIT Alewife machine are all based on this topology <ref> [3] </ref>. An a b -Delta network consists of b stages of a (b1) switches of size a fi a. A shu*e connection is used between consecutive stages. The output links of PEs connect to switches in the first stage, and switches in the last stage are connected to PE inputs.
Reference: [4] <author> Bruce W. Arden and K. Wendy Tang. </author> <title> Representation and Routing of Cayley Graphs. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 39(11) </volume> <pages> 1533-1537, </pages> <month> Nov </month> <year> 1991. </year>
Reference-contexts: In the absence of this information, the ICN may be configured to a fixed topology and operated as a static network. Previous research has looked at the problem of finding dense graphs <ref> [1, 4, 32, 91] </ref>. Given two integers d and L, the problem calls for finding a graph with bounded degree d and diameter L that has the maximum number of nodes possible. <p> Deadlock avoidance in dense graphs: In Chapter 2 we mentioned that if no knowledge of the application's communication graph is available, a suitable dense graph topology might be considered for the global crossbar. Past research <ref> [1, 4, 75, 91] </ref> has resulted in the discovery of a number of dense graphs. Still, the general problem of deadlock free routing in such graphs is not very well understood.
Reference: [5] <author> A. Barak and R. Ben-Natan. </author> <title> Bounded Contractions of Full Trees. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 17(4) </volume> <pages> 363-369, </pages> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: Subsequently, other researchers <ref> [5, 60] </ref> have extended their work to include additional graphs. In this subsection, we review some of these results; and, in the next, augment them with a few new results of our own. CCCs and binary trees as described in [6]. <p> The partitioning scheme shown in Figure 2.4 (e) yields a bounded n-contraction for a CCC with n2 n nodes. For all of these graphs, except the CCCs, smaller bounded contractions do not exist <ref> [5, 6] </ref>. Partitioning Cube Connected Cycles Irrespective of its size, a bounded 4-contraction can always be found for a CCC graph.
Reference: [6] <author> Amnon Barak and Eugen Schenfeld. </author> <title> Embedding Classical Communication Topologies in the OPAM Architecture. </title> <type> Technical Report TR 90-12, </type> <institution> Department of Computer Science, The Hebrew University of Jerusalem, Israel, </institution> <month> Aug. </month> <year> 1990. </year> <note> A shorter version appears in Proc. of the IEEE Symposium on Parallel and Distributed Processing, </note> <month> Dec. </month> <year> 1991, </year> <pages> pp. 482-485. </pages>
Reference-contexts: The constraint on the degree of a subset and the minimization of ` represent two objectives that frustrate one another and make this problem so hard. 2.2.3 Sample Mappings Barak and Schenfeld <ref> [6] </ref> have developed bounded `-contractions with small values of ` for popular communication graphs such as rings, trees, grids, tori, cube connected 20 cycles (CCC) etc.. Subsequently, other researchers [5, 60] have extended their work to include additional graphs. <p> Subsequently, other researchers [5, 60] have extended their work to include additional graphs. In this subsection, we review some of these results; and, in the next, augment them with a few new results of our own. CCCs and binary trees as described in <ref> [6] </ref>. The partitioning scheme shown for the binary tree also works for k-ary trees and yields a bounded (k + 1)-contraction. Using the same partitioning scheme as shown for the grids, bounded 4-contractions can also be obtained for 2-D tori. <p> The partitioning scheme shown in Figure 2.4 (e) yields a bounded n-contraction for a CCC with n2 n nodes. For all of these graphs, except the CCCs, smaller bounded contractions do not exist <ref> [5, 6] </ref>. Partitioning Cube Connected Cycles Irrespective of its size, a bounded 4-contraction can always be found for a CCC graph. <p> Since each subset contains four nodes and is connected to at most three others, this partitioning corresponds to a bounded 4-contraction. In contrast, the partitioning used in <ref> [6] </ref> groups nodes labeled hx 1 x 2 : : : x n ; 0i, hx 1 x 2 : : : x n ; 1i, ... , hx 1 x 2 : : : x n ; (n 1)i in the same subset and results in a bounded n-contraction for <p> As shown in Figure 2.13c, a connected subgraph of size three in the triangular array has five neighbors and it is impossible to group them into three (or less) subsets which are themselves connected subgraphs. 29 Partitioning Hypercube graphs The following theorem is proved in <ref> [6] </ref> and it is conjectured that the bounded contraction, so obtained, is of the smallest size. Theorem 2.2 For a hypercube of degree d, there exists a bounded `-contraction for ` = 2 k where k is the smallest non-negative integer satisfying k &lt; d 2 k + k.
Reference: [7] <author> Kenneth E. Batcher. </author> <title> Design of a massively parallel processor. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(9):836-840, </volume> <month> Sep </month> <year> 1980. </year>
Reference-contexts: This compelling idea has inspired a number of research efforts some of which are mentioned below. Goodyear Aerospace's MPP The massively parallel processor (MPP) <ref> [7] </ref>, was designed in the early 80s. It consists of a 128 by 128 array of PEs, the opposing edges of which can be closed or left open under programmer's control. This allows the network to be configured as an open or cylindrical mesh.
Reference: [8] <author> J. Beetem, M. Denneau, and D. Weingarten. </author> <title> The GF11 Supercomputer. </title> <booktitle> In Proceedings of the 12th International Symposium on Computer Architecture, </booktitle> <pages> pages 108-115, </pages> <month> Jun </month> <year> 1985. </year>
Reference-contexts: The interconnections between the switches form a 2-dimensional 5 torus but direct connections can be established between non-adjacent PEs by `shorting' the intermediate switches. This architecture is limited by its SIMD design and restrictive configuration capabilities. IBM's GF11 The GF11 <ref> [8] </ref> (named for its peak rating of 11 GigaFlops) was conceived primarily for the numerical solution of problems in quantum chromodynamics. It connects 576 processors through a three stage Benes network. Each stage consists of 24 crossbar switches of size 24 by 24 and nine bits wide. <p> Within these restrictions, any directed graph is allowed. This model is general enough to represent many network topologies including the three shown in Figure 5.2. Figure 5.2a shows the GF11 network <ref> [8] </ref> consisting of three stages of twenty-four switching elements each. Consecutive stages are connected by a shu*e interconnection. Output links in the last stage wrap around to become input links for the PEs. Figure 5.2b shows a possible configuration of iWarp [15] chips.
Reference: [9] <author> Gordon Bell. </author> <title> Ultracomputers: A Teraflop before its time. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 27-47, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: This trend towards massively parallel-processing (MPP) machines is underlined by several recent (early 1990) developments in the high performance computing world. A number of multiprocessor machines, such as the Intel Paragon <ref> [9] </ref>, the Thinking Machines CM-5 [92], NCube's NCUBE2 and the Meiko CS2 [21] are currently available. The Cray T3D and the IBM SP1 are new MPP offerings from companies that have a traditional base in vector super computers and mainframes, respectively. <p> An application's performance can often benefit from a close match between its communication graph and 3 Based on their memory organization, parallel architectures can be classified as either centralized ("Dance-Hall") or distributed ("Boudoir") <ref> [9] </ref>. In the dance-hall arrangement, the entire memory is centralized and processors and memory modules are lined up on opposite sides of a network. a In the boudoir arrangement, physical memory comprises a set of memory units each connected directly to a unique processor.
Reference: [10] <author> Francine Berman. </author> <title> The Characteristics of Parallel Algorithms, chapter Experience with an Automatic Solution to the Mapping Problem, </title> <address> pages 307-334. </address> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: In a network graph, nodes represent PEs and switching elements while the edges represent network links. Starting with the pioneering work of Bokhari [14], a number of researchers have looked at this mapping problem <ref> [2, 10, 13, 44, 45, 46, 63, 80] </ref>. <p> However, the model supported by NetSim is more general. We assume that only one process is mapped to each PE. When the number of processes in the communication graph exceeds the number of available PEs, techniques similar to those described in <ref> [10] </ref> can be used to preprocess the communication graph. The network topology and routing information, the communication graph, and the mapping function are specified by the user in the form of three ASCII files. The format of these input files is explained in Figure 5.3 and Figure 5.4.
Reference: [11] <author> Francine Berman and Lawrence Snyder. </author> <title> On Mapping Parallel Algorithms into Parallel Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 4 </volume> <pages> 439-458, </pages> <year> 1987. </year>
Reference-contexts: We refer to these as single-segment mappings. We assume that enough PEs are available to map only one process to each PE. When the number of processes is larger than the number of PEs, techniques similar to those described in <ref> [11] </ref> can be used to pre-process the communication graph. The following terminology is useful in identifying communication graphs for which single-segment mappings exist.
Reference: [12] <author> J. Bhasker and S. Sahni. </author> <title> Optimal Linear Arrangement of Circuit Components. </title> <journal> Journal of VLSI and Computer Systems, </journal> <volume> 2 </volume> <pages> 87-109, </pages> <year> 1987. </year>
Reference-contexts: The histogram suggests that simulated annealing is inherently more powerful than the technique of local optimization. This observation is in agreement with that of other researchers [49]. Simulated Annealing has previously been used to solve a wide variety of other combinatorial optimization problems <ref> [12, 19, 39, 67, 71, 95] </ref>. 3.1.2 A generic implementation A generic simulated annealing algorithm is outlined in Figure 3.3. The algorithm starts with some initial solution and enters a loop. During each iteration of this loop, it considers a perturbation of the current solution.
Reference: [13] <author> Sandeep Bhatt, Fan Chung, and Chung Thomson. </author> <title> Efficient Embeddings of Trees in Hypercubes. </title> <journal> SIAM Journal on Computing, </journal> <volume> 21(1):151, </volume> <month> Feb. </month> <year> 1992. </year> <month> 106 </month>
Reference-contexts: In a network graph, nodes represent PEs and switching elements while the edges represent network links. Starting with the pioneering work of Bokhari [14], a number of researchers have looked at this mapping problem <ref> [2, 10, 13, 44, 45, 46, 63, 80] </ref>.
Reference: [14] <author> Shahid H. Bokhari. </author> <title> On the mapping problem. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(3):207-214, </volume> <month> Mar </month> <year> 1981. </year>
Reference-contexts: In the communication graph of an application, nodes represent concurrent processes and edges represent interprocess communication. In a network graph, nodes represent PEs and switching elements while the edges represent network links. Starting with the pioneering work of Bokhari <ref> [14] </ref>, a number of researchers have looked at this mapping problem [2, 10, 13, 44, 45, 46, 63, 80]. <p> Note that the notion of what constitutes a `good' mapping is unspecified. The reason for this is that a number of optimization criteria for the mapping problem have been proposed and shown to be useful. Bokhari <ref> [14] </ref> suggests maximizing the cardinality of a mapping, which he defines as the number of communicating processes that get mapped to neighboring PEs. This only accounts for those edges in the communication graph that have dilation one.
Reference: [15] <author> S. Borkar et al. </author> <title> iWarp: An integrated solution to high-speed parallel computing. </title> <booktitle> In Supercomputing'88, </booktitle> <address> Kissimmee, FL, </address> <month> Nov </month> <year> 1988. </year>
Reference-contexts: The Cray T3D and the IBM SP1 are new MPP offerings from companies that have a traditional base in vector super computers and mainframes, respectively. Other similar systems (or their components) have been developed in several universities, such as Carnegie Mellon University (iWarp <ref> [15] </ref>), Massachussetts Institute of Technology (J-machine [23, 69], *T [68]) and Stanford (DASH [57, 58]). The main motivation behind parallel computing is that by working concurrently on different parts of a problem, the problem can be solved faster and in larger instances than otherwise possible. <p> Figure 5.2a shows the GF11 network [8] consisting of three stages of twenty-four switching elements each. Consecutive stages are connected by a shu*e interconnection. Output links in the last stage wrap around to become input links for the PEs. Figure 5.2b shows a possible configuration of iWarp <ref> [15] </ref> chips. Each switching element is connected to one PE and four other switching elements in a 2-Dimensional mesh. Figure 5.2c shows a hypercube network topology of degree three.
Reference: [16] <author> CACI. </author> <title> Simscript II.5 Programming Language. CACI Products Company, </title> <address> La Jolla, CA, </address> <year> 1988. </year>
Reference-contexts: The down side of simulation is that it is more time consuming. In the space defined by all possible combinations of system parameters, each simulation run investigates only a single point. 74 A number of powerful simulation languages such as GPSS [84], SLAM [76], and SIM-SCRIPT <ref> [16] </ref> are commercially available. However, there is a considerable start up time associated with using them, since they require learning a new language. Libraries and toolkits based on popular programming languages (e.g. CSIM [85] and SIMPACK [34] based on C) do not suffer from this drawback.
Reference: [17] <editor> V. Cantoni and S. Levialdi, editors. </editor> <booktitle> Pyramidal System for Computer Vision, volume F25 of NATO ASI Series. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Hiedelberg, </address> <year> 1987. </year>
Reference-contexts: Graph Application Trees and X-trees [26] Min, Max, Sum computation, Searching algorithms, Parallel prefix computations Divide and Conquer algorithms Grids and Tori Matrix Algorithms, Solution of Partial Differential equations Shu*e-exchange and Fast Fourier transform, De Bruijn graphs [81] Sorting Hypercubes and Ascend-Descend algorithms [74] Cube-Connected Cycles (CCC) Pyramids Image processing <ref> [17] </ref> Mesh of trees [56] Graph algorithms graphs having randomly generated edges. Standard random graphs denoted by G n;p are defined in terms of two parameters, n and p.
Reference: [18] <author> M. Y. Chan, F. Chin, C. N. Chu, and W. K. Mak. </author> <title> Dilation-5 Embedding of 3-Dimensional Grids into Hypercubes. </title> <booktitle> In Proceedings of the Fifth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 285-288, </pages> <year> 1993. </year>
Reference-contexts: However, other edges which get mapped to longer paths can cause a communication bottleneck; they are not accounted for in this objective function. Another, often used, criterion is minimization of the maximum dilation of any edge in E Comm (see <ref> [18, 28, 31, 45, 61, 94] </ref> and references therein). This criterion does not distinguish between a mapping in which only a few communication edges have large dilations, and another in which all communication edges have large dilations.
Reference: [19] <author> W.-M. Chen, Y.-X. Wong, and X. Ping. </author> <title> Flow-shop scheduling by the knowledge of statistical mechanics and annealing. </title> <booktitle> In Proc. of the 26th IEEE Conference on Decision and Control, </booktitle> <volume> volume 1, </volume> <pages> pages 642-643, </pages> <year> 1987. </year>
Reference-contexts: The histogram suggests that simulated annealing is inherently more powerful than the technique of local optimization. This observation is in agreement with that of other researchers [49]. Simulated Annealing has previously been used to solve a wide variety of other combinatorial optimization problems <ref> [12, 19, 39, 67, 71, 95] </ref>. 3.1.2 A generic implementation A generic simulated annealing algorithm is outlined in Figure 3.3. The algorithm starts with some initial solution and enters a loop. During each iteration of this loop, it considers a perturbation of the current solution.
Reference: [20] <author> Suresh Chittor and Richard Enbody. </author> <title> Performance Degradation in Large Wormhole-Routed Interprocessor Communication Networks. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 424-428 (Vol. </pages> <note> I), </note> <year> 1990. </year>
Reference-contexts: Minimizing the maximum congestion on a link is an optimization criteria used by Dingle and Sudborough [28]. A more complex criteria called the path contention level has been proposed by Chittor and Enbody <ref> [20] </ref>. For any edge in G Comm which corresponds to a path in G N et , path contention level is the number of paths which share at least one edge of G N et with the given path. The example in Figure 2.3 illustrates all of these criteria. <p> The example in Figure 2.3 illustrates all of these criteria. This figure is a slight modification of that appearing in <ref> [20] </ref>. It shows a random mapping of a 15-node binary tree (G Comm ) onto a 4 fi 4 processor array (G N et ). For simplicity, only the communication traffic from each node to its children is considered. Also, the bidirectional network links in the mesh are not drawn.
Reference: [21] <author> Meiko Corporation. </author> <title> Computing Surface CS-2 Product Description, 1993. </title> <institution> Meiko Ltd., Bristol, UK. </institution>
Reference-contexts: This trend towards massively parallel-processing (MPP) machines is underlined by several recent (early 1990) developments in the high performance computing world. A number of multiprocessor machines, such as the Intel Paragon [9], the Thinking Machines CM-5 [92], NCube's NCUBE2 and the Meiko CS2 <ref> [21] </ref> are currently available. The Cray T3D and the IBM SP1 are new MPP offerings from companies that have a traditional base in vector super computers and mainframes, respectively.
Reference: [22] <author> William J. Dally. </author> <title> Performance Analysis of k-ary n-cube Interconnection Networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(6) </volume> <pages> 775-785, </pages> <month> Jun </month> <year> 1990. </year>
Reference-contexts: However, we show that these partitions result in very good embeddings in the ICN. In Chapter 4 we describe an analytical model for the performance evaluation of synchronous networks under different communication patterns. Previous studies on network analysis <ref> [22, 52, 72, 97] </ref> do not model the communication traffic of the application explicitly; instead, they assume that communication traffic is uniform randomly distributed. Recently reported evidence [47, 65] raises serious questions about the validity of this assumption. <p> This is the criterion used by Simtley and Lee [88] (which they call total expansion). Compared to store-and-forward, newer routing techniques like wormhole routing or circuit switching make communication delays only weakly dependent on the number of intervening switching nodes <ref> [22, 30] </ref>. This observation motivates other mapping criteria. The congestion of a network link (a.k.a. channel load) measures the number 2 These graphs are defined earlier in Chapter 1. 16 of communication paths that share that link. <p> Since the ICN exploits knowledge about the communication pattern of individual applications, models designed to study its performance must be able to model communication patterns explicitly. Much of the previous work on performance analysis of networks has assumed random, uniformly distributed communication <ref> [22, 27, 52, 72, 97] </ref> and is inapplicable to many parallel applications that lack this uniformity. In this chapter, we develop an analytical model to study the performance of synchronous, circuit-switched networks under different communication patterns [43]. <p> Our experience indicates that the special purpose features of NetSim make it five to ten times faster than a similar program based on SIMPACK. Tools for simulating computer networks have also been reported in <ref> [22, 27, 52, 97] </ref>. Those tools, however, are primarily designed to validate specific mathematical analyses and, as such, lack the versatility of NetSim. NetSim is programmable as to network topology, communication pattern, routing algorithm, mapping strategy and network operating assumptions (see Figure 5.1). <p> Similarly, genLength () can be modified to implement whichever message length distribution the user requires. Some choices that have been suggested for network evaluation include exponential [50], constant <ref> [22] </ref>, and distributions based on multi-stage probability density functions [47]. 5.4 Experimental Results NetSim has been implemented in C++ and validated by comparison against other simulations reported previously (e.g. [97]). Below we describe an experiment involving NetSim.
Reference: [23] <author> William J. Dally, J. A. Stuart Fiske, John S. Keen, Richard A. Lethin, Michael D. Noakes, Peter R. Nuth, Roy E. Davison, and Gregory A. Fyler. </author> <title> The Message-Driven Processor: A Multicomputer Processing Node with Efficient Mechanisms. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 23-39, </pages> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: The Cray T3D and the IBM SP1 are new MPP offerings from companies that have a traditional base in vector super computers and mainframes, respectively. Other similar systems (or their components) have been developed in several universities, such as Carnegie Mellon University (iWarp [15]), Massachussetts Institute of Technology (J-machine <ref> [23, 69] </ref>, *T [68]) and Stanford (DASH [57, 58]). The main motivation behind parallel computing is that by working concurrently on different parts of a problem, the problem can be solved faster and in larger instances than otherwise possible.
Reference: [24] <author> William J. Dally and Charles L. Seitz. </author> <title> Deadlock-Free Message Routing in Multiprocessor Interconnection Networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(5):547-553, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: By using different implementations of these functions, users can change the assumptions to be used in the simulations. In what follows, we describe NetSim under the assumption of wormhole routing <ref> [24] </ref>. As described in [42], NetSim can be used for simulating circuit-switched networks as well with only minor modifications. 1. In wormhole routing, all messages are composed of fixed sized data chunks called flits which flow through the network in pipelined fashion. <p> Paths for intra-cluster communication consist of a red channel followed by a blue channel. When the communication graph has a single-segment mapping, paths for inter-cluster communication consist of a red channel followed by a green channel followed by a blue channel. The channel dependency graph <ref> [24] </ref> in this case has only three kinds of dependencies from red channels to blue, from red to green and from green to blue.
Reference: [25] <author> Jarek Deminet. </author> <title> Experience with Multiprocessor Algorithms. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-31(4), </volume> <month> Apr </month> <year> 1982. </year>
Reference-contexts: Examples of parallel machines with distributed memory but shared address space are Kendall Square Research's KSR/Series, CRAY's T3D and Stanford University's DASH. a This is akin to men and women lining up on opposite sides of a dance floor, and hence, the name. 4 the machine's network graph <ref> [25, 48] </ref>. Since communication patterns show great variations among different applications [53], networks with a fixed (static) topology may not perform well over a wide range of applications. This compelling idea has inspired a number of research efforts some of which are mentioned below.
Reference: [26] <author> A. Despain and D. Patterson. X-Tree: </author> <title> a Structured Multiprocessor Computer Architecture. </title> <booktitle> In Proc. of the 5th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 144-151, </pages> <year> 1978. </year>
Reference-contexts: By enumerating all neighboring nodes for this subset, it is easy to verify that each subset is connected to at most four other subsets. This partitioning scheme, therefore, corresponds to a bounded 4-contraction. Partitioning X-Trees An X-tree <ref> [26] </ref> is a full binary tree in which extra edges, called cross edges, are added to connect each node with at most two immediate neighbors on the same level. <p> It is intuitively appealing to model such patterns with 37 Table 3.2: Some commonly occurring communication patterns. Graph Application Trees and X-trees <ref> [26] </ref> Min, Max, Sum computation, Searching algorithms, Parallel prefix computations Divide and Conquer algorithms Grids and Tori Matrix Algorithms, Solution of Partial Differential equations Shu*e-exchange and Fast Fourier transform, De Bruijn graphs [81] Sorting Hypercubes and Ascend-Descend algorithms [74] Cube-Connected Cycles (CCC) Pyramids Image processing [17] Mesh of trees [56] Graph
Reference: [27] <author> Daniel M. Dias and J. Robert Jump. </author> <title> Analysis and Simulation of Buffered Delta Networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(4):273-282, </volume> <month> Apr </month> <year> 1981. </year>
Reference-contexts: Since the ICN exploits knowledge about the communication pattern of individual applications, models designed to study its performance must be able to model communication patterns explicitly. Much of the previous work on performance analysis of networks has assumed random, uniformly distributed communication <ref> [22, 27, 52, 72, 97] </ref> and is inapplicable to many parallel applications that lack this uniformity. In this chapter, we develop an analytical model to study the performance of synchronous, circuit-switched networks under different communication patterns [43]. <p> This computation involves repeated application of the technique of Sect. 4.1.2. Note that m o is simply the network throughput per PE under the assumption that dropped messages are ignored. This assumption has been used in previous studies <ref> [27, 52, 72] </ref>. It has the effect of underestimating the actual throughput at low message generation rates and overestimating it at high request rates. 2. Resubmission of dropped messages causes the effective message generation rate to be higher than m. <p> Our experience indicates that the special purpose features of NetSim make it five to ten times faster than a similar program based on SIMPACK. Tools for simulating computer networks have also been reported in <ref> [22, 27, 52, 97] </ref>. Those tools, however, are primarily designed to validate specific mathematical analyses and, as such, lack the versatility of NetSim. NetSim is programmable as to network topology, communication pattern, routing algorithm, mapping strategy and network operating assumptions (see Figure 5.1).
Reference: [28] <author> Adair Dingle and Hal Sudborough. </author> <title> Simulation of Binary Trees and X-Trees on Pyramid Networks. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 19(2) </volume> <pages> 119-124, </pages> <month> Oct </month> <year> 1993. </year> <month> 107 </month>
Reference-contexts: However, other edges which get mapped to longer paths can cause a communication bottleneck; they are not accounted for in this objective function. Another, often used, criterion is minimization of the maximum dilation of any edge in E Comm (see <ref> [18, 28, 31, 45, 61, 94] </ref> and references therein). This criterion does not distinguish between a mapping in which only a few communication edges have large dilations, and another in which all communication edges have large dilations. <p> The congestion of a network link (a.k.a. channel load) measures the number 2 These graphs are defined earlier in Chapter 1. 16 of communication paths that share that link. Minimizing the maximum congestion on a link is an optimization criteria used by Dingle and Sudborough <ref> [28] </ref>. A more complex criteria called the path contention level has been proposed by Chittor and Enbody [20].
Reference: [29] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Sparse Matrix Test Problems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 15(1) </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference-contexts: Replacing all non-zero entries in the sparse matrix by ones yields the adjacency matrix of the communication graph. In Table 3.3, rows identified as DWT758, LSHP1270, BCSPWR07 represent communication graphs for sparse matrices 39 of the same name from the Harwell-Boeing collection <ref> [29] </ref>. Remaining rows in that table represent communication graphs arising in unstructured grid computations 2 (Figure 3.4 shows a sample grid). Typically, each grid point represents a computational entity that communicates iteratively only with its immediate neighbors.
Reference: [30] <author> T. H. Dunigan. </author> <title> Performance of the Intel iPSC/860 hypercube. </title> <type> Technical Report ORNL/TM-11491, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, Tennessee 37831, </institution> <month> Jun </month> <year> 1990. </year>
Reference-contexts: This is the criterion used by Simtley and Lee [88] (which they call total expansion). Compared to store-and-forward, newer routing techniques like wormhole routing or circuit switching make communication delays only weakly dependent on the number of intervening switching nodes <ref> [22, 30] </ref>. This observation motivates other mapping criteria. The congestion of a network link (a.k.a. channel load) measures the number 2 These graphs are defined earlier in Chapter 1. 16 of communication paths that share that link.
Reference: [31] <author> Kamal Efe. </author> <title> Embedding Mesh of Trees in the Hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11(3) </volume> <pages> 222-230, </pages> <month> Mar </month> <year> 1991. </year>
Reference-contexts: However, other edges which get mapped to longer paths can cause a communication bottleneck; they are not accounted for in this objective function. Another, often used, criterion is minimization of the maximum dilation of any edge in E Comm (see <ref> [18, 28, 31, 45, 61, 94] </ref> and references therein). This criterion does not distinguish between a mapping in which only a few communication edges have large dilations, and another in which all communication edges have large dilations.
Reference: [32] <author> Bernard Elspas. </author> <title> Topological Constraints on Interconnection-limited Logic. Switching Circuit Theory and Logical Design, </title> <address> S-164:133-147, </address> <month> Oct </month> <year> 1964. </year>
Reference-contexts: In the absence of this information, the ICN may be configured to a fixed topology and operated as a static network. Previous research has looked at the problem of finding dense graphs <ref> [1, 4, 32, 91] </ref>. Given two integers d and L, the problem calls for finding a graph with bounded degree d and diameter L that has the maximum number of nodes possible.
Reference: [33] <institution> Federal Coordinating Council for Science, Engineering and Technology. Grand Challenges 1993: High Performance Computing and Communications. A Report by the Committee on Physical, Mathematical, and Engineering Sciences, National Science Foundation, </institution> <year> 1993. </year>
Reference-contexts: Introduction Many fundamental problems in science and engineering require higher processing power than is currently available. A number of such "grand challenge" problems have been identified by the United State government's High Performance Computing and Communications (HPCC) Program <ref> [33] </ref>. These include: prediction of weather, climate, and global change; determination of molecular, atomic, and nuclear structure; understanding turbulence, pollution dispersion and combustion systems; and understanding the nature of new materials and biological macromolecules.
Reference: [34] <author> Paul A. Fishwick. SimPack: </author> <title> Getting started with simulation programming in C and C++. </title> <type> Technical Report TR92-022, </type> <institution> Dept. of Computer and Information Science, Univ. of Florida, </institution> <address> Gainesville, FL, </address> <year> 1992. </year>
Reference-contexts: However, there is a considerable start up time associated with using them, since they require learning a new language. Libraries and toolkits based on popular programming languages (e.g. CSIM [85] and SIMPACK <ref> [34] </ref> based on C) do not suffer from this drawback. Still, they require a computer architect to work with unfamiliar abstractions. NetSim is a versatile simulation tool, designed specifically for modeling the performance of multiprocessor networks.
Reference: [35] <author> M. J. Flynn. </author> <title> Some Computer Organizations and their Effectiveness. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-21:948-960, </volume> <year> 1972. </year>
Reference-contexts: This allows the network to be configured as an open or cylindrical mesh. Optionally, it is also possible to connect all 16,384 PEs in a linear chain or a ring. The MPP is an example of a Single Instruction stream Multiple Data stream <ref> [35] </ref> (SIMD, pronounced sim-dee) architecture and offers very limited reconfiguration. Configurable Highly Parallel Computer (CHiP) The CHiP [89] consists of homogeneous microprocessors embedded in a regular lattice of switches. A central controller can set these switches to any one of a set of pre-stored configuration settings.
Reference: [36] <author> J. M. Garcia and J. Duato. </author> <title> An Algorithm for Dynamic Reconfiguration of a Multicomputer Network. </title> <booktitle> In Proceedings of the 3rd IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 848-855, </pages> <month> Dec </month> <year> 1991. </year>
Reference-contexts: For sufficiently long messages, the use of pipelined communication techniques (circuit switching is another example) makes network latency only weakly dependent on the communication distance. This has undermined the importance of trying to map application tasks to PEs in a way that minimizes non-neighbor communication <ref> [36] </ref>. However, when a substantial portion of the communication involves distant PEs, more links are kept busy during each data transfer operation. At higher traffic rates, this 84 causes greater contention in the network and increases communication delays.
Reference: [37] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-completeness. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: Therefore, we are interested in those ICN topologies and mappings that minimize the number of switching elements through which each communication request must pass. The problem of obtaining these communication efficient topologies and mappings, in general, is NP-complete <ref> [37] </ref>. We develop a heuristic approach, based on simulated annealing, for this problem. As with any implementation of simulated annealing, this implementation requires extensive experimentation with a wide range of algorithmic parameters. Some of these are problem specific while others are of a generic nature.
Reference: [38] <author> Christopher J. Glass and Lionel M. Ni. </author> <title> The Turn Model for Adaptive Routing. </title> <booktitle> In Proc. of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 278-287, </pages> <address> Los Alamitos, California, 1992. </address> <publisher> IEEE CS Press. Order No. </publisher> <pages> 2940. </pages>
Reference-contexts: Path length now represents the 3 This is commonly referred to as row-column routing or XY-routing (see <ref> [38] </ref>). 17 number of resources (network links) each message must acquire. When each message requires more resources, there is increased resource contention. <p> In others, appropriate restrictions on the routing algorithm can help avoid deadlocks [66]. These restrictions are assumed to be part of the routing information included in the user's specification of network topology. Our programs that generate such specifications can be made to choose between row-column, west-first or negative-first routing <ref> [38] </ref> for 2-D meshes and e-cube routing for hypercubes. Other routing schemes can be similarly supported. 5.3 The Structure of NetSim Simulation of networks in NetSim is clock-driven. Each clock tick corresponds to one cycle. The structure of the main loop is outlined in Figure 5.5.
Reference: [39] <author> L. Goldstein and M. S. Waterman. </author> <title> Mapping DNA by stochastic relaxation. </title> <booktitle> Advances in Applied Mathematics, </booktitle> <volume> 8 </volume> <pages> 194-207, </pages> <year> 1987. </year>
Reference-contexts: The histogram suggests that simulated annealing is inherently more powerful than the technique of local optimization. This observation is in agreement with that of other researchers [49]. Simulated Annealing has previously been used to solve a wide variety of other combinatorial optimization problems <ref> [12, 19, 39, 67, 71, 95] </ref>. 3.1.2 A generic implementation A generic simulated annealing algorithm is outlined in Figure 3.3. The algorithm starts with some initial solution and enters a loop. During each iteration of this loop, it considers a perturbation of the current solution.
Reference: [40] <author> Vipul Gupta and Eugen Schenfeld. </author> <title> A Heuristic Approach for Embedding Communication Patterns in an Interconnection Cached Parallel Processing Network. </title> <booktitle> In Proceedings of the 7th International Parallel Processing Symposium, </booktitle> <pages> pages 291-297, </pages> <month> Apr </month> <year> 1993. </year>
Reference-contexts: A preliminary version of this algorithm is reported in <ref> [40] </ref>. As described in Chapter 2, a solution to the bounded contraction problem also leads to a communication efficient embedding of the corresponding parallel application in the Interconnection Cached Network. We start with an overview of simulated annealing.
Reference: [41] <author> Vipul Gupta and Eugen Schenfeld. </author> <title> A Comparative Performance Study of an Interconnection Cached Network. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <month> Aug </month> <year> 1994. </year> <note> (to appear). </note>
Reference-contexts: A similar study based on the use of circuit-switching is reported in <ref> [41] </ref>. Each simulation is run for 105,000 cycles and data gathering is suppressed during the first 5,000 cycles to avoid transient effects.
Reference: [42] <author> Vipul Gupta and Eugen Schenfeld. </author> <title> NetSim-A Tool for Modeling the Performance of Circuit Switched Multicomputer Networks. </title> <booktitle> In 7th International Conference on Modeling Techniques and Tools for Computer Performance Evaluation, </booktitle> <pages> pages 180-192, </pages> <month> May </month> <year> 1994. </year> <note> Published as Springer-Verlag's Lecture Notes in Computer Science No. 794. 108 </note>
Reference-contexts: Another drawback of the analytical approach is the difficulty of dealing with non-symmetric communication patterns, e.g. a binary tree pattern or 2-D or 3-D grid (as opposed to tori) patterns. To allow for greater flexibility in our modeling assumptions, we have developed a versatile network simulator <ref> [42] </ref>. <p> By using different implementations of these functions, users can change the assumptions to be used in the simulations. In what follows, we describe NetSim under the assumption of wormhole routing [24]. As described in <ref> [42] </ref>, NetSim can be used for simulating circuit-switched networks as well with only minor modifications. 1. In wormhole routing, all messages are composed of fixed sized data chunks called flits which flow through the network in pipelined fashion.
Reference: [43] <author> Vipul Gupta and Eugen Schenfeld. </author> <title> Performance Analysis of a Synchronous, Circuit-switched Interconnection Cached Network. </title> <booktitle> In Proceedings of the 8th ACM International Conference on Supercomputing, </booktitle> <pages> pages 246-255, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: In this chapter, we develop an analytical model to study the performance of synchronous, circuit-switched networks under different communication patterns <ref> [43] </ref>. We use this model to study the performance of the ICN in comparison to more popular reconfigurable networks: the delta [72] and the crossbar.
Reference: [44] <author> Steven W. Hammond. </author> <title> Mapping Unstructured Grid Computations to Massively Parallel Computers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rennsselaer Polytechnic Institute, </institution> <address> Troy, New York, </address> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: In a network graph, nodes represent PEs and switching elements while the edges represent network links. Starting with the pioneering work of Bokhari [14], a number of researchers have looked at this mapping problem <ref> [2, 10, 13, 44, 45, 46, 63, 80] </ref>.
Reference: [45] <author> Ching-Tien Ho and S. Lennart Johnsson. </author> <title> Embedding Meshes in Boolean Cubes by Graph Decomposition. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 325-339, </pages> <year> 1990. </year>
Reference-contexts: In a network graph, nodes represent PEs and switching elements while the edges represent network links. Starting with the pioneering work of Bokhari [14], a number of researchers have looked at this mapping problem <ref> [2, 10, 13, 44, 45, 46, 63, 80] </ref>. <p> However, other edges which get mapped to longer paths can cause a communication bottleneck; they are not accounted for in this objective function. Another, often used, criterion is minimization of the maximum dilation of any edge in E Comm (see <ref> [18, 28, 31, 45, 61, 94] </ref> and references therein). This criterion does not distinguish between a mapping in which only a few communication edges have large dilations, and another in which all communication edges have large dilations.
Reference: [46] <author> Jia-Wei Hong, Kurt Melhorn, and Arnold L. Rosenberg. </author> <title> Cost Trade-offs in Graph Embeddings, with Applications. </title> <journal> Journal of the ACM, </journal> <volume> 30(4) </volume> <pages> 709-728, </pages> <month> Oct. </month> <year> 1983. </year>
Reference-contexts: In a network graph, nodes represent PEs and switching elements while the edges represent network links. Starting with the pioneering work of Bokhari [14], a number of researchers have looked at this mapping problem <ref> [2, 10, 13, 44, 45, 46, 63, 80] </ref>.
Reference: [47] <author> Jiun-Ming Hsu and Prithviraj Banerjee. </author> <title> Performance Measurement and Trace Driven Simulation of Parallel CAD and Numeric Applications on a Hypercube Multicomputer. </title> <booktitle> In Proceedings 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 260-269, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Previous studies on network analysis [22, 52, 72, 97] do not model the communication traffic of the application explicitly; instead, they assume that communication traffic is uniform randomly distributed. Recently reported evidence <ref> [47, 65] </ref> raises serious questions about the validity of this assumption. When communication paths are short, our analytical model produces results that are in good agreement with those obtained from simulations, but are obtained in a fraction of the simulation time. <p> This allows the communication pattern of an application to be considered explicitly. Uniform, randomly distributed, communication traffic does not adequately model the behavior of many practical applications <ref> [47, 65] </ref>. This has prompted the 76 development of more sophisticated models of network traffic, such as spherical distribution [50] and hot-spot traffic [54]. However, the model supported by NetSim is more general. We assume that only one process is mapped to each PE. <p> Similarly, genLength () can be modified to implement whichever message length distribution the user requires. Some choices that have been suggested for network evaluation include exponential [50], constant [22], and distributions based on multi-stage probability density functions <ref> [47] </ref>. 5.4 Experimental Results NetSim has been implemented in C++ and validated by comparison against other simulations reported previously (e.g. [97]). Below we describe an experiment involving NetSim. <p> While NetSim has the capability of handling many different assumptions, it is still up to the user to determine which assumptions are most realistic. Some work has been reported on characterizing applications empirically <ref> [47] </ref>. However, it is not clear which of these characteristics are inherent in the application, and which reflect peculiarities of the particular architecture used in data collection.
Reference: [48] <author> Leah H. Jamieson. </author> <booktitle> The Chracteristics of Parallel Algorithms, chapter Characterizing Parallel Algorithms, </booktitle> <pages> pages 65-100. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Examples of parallel machines with distributed memory but shared address space are Kendall Square Research's KSR/Series, CRAY's T3D and Stanford University's DASH. a This is akin to men and women lining up on opposite sides of a dance floor, and hence, the name. 4 the machine's network graph <ref> [25, 48] </ref>. Since communication patterns show great variations among different applications [53], networks with a fixed (static) topology may not perform well over a wide range of applications. This compelling idea has inspired a number of research efforts some of which are mentioned below.
Reference: [49] <author> David S. Johnson, Cecilia R. Aragon, Lyle A. McGeoch, and Catharine Schevon. </author> <title> Optimization by simulated annealing: An experimental evaluation; Part I, Graph Partitioning. </title> <journal> Operations Research, </journal> <volume> 37(6) </volume> <pages> 865-892, </pages> <month> Nov-Dec </month> <year> 1989. </year>
Reference-contexts: The X-axis corresponds to the cost of the final solution and the Y-axis to the number of times each cost was encountered. The histogram suggests that simulated annealing is inherently more powerful than the technique of local optimization. This observation is in agreement with that of other researchers <ref> [49] </ref>. Simulated Annealing has previously been used to solve a wide variety of other combinatorial optimization problems [12, 19, 39, 67, 71, 95]. 3.1.2 A generic implementation A generic simulated annealing algorithm is outlined in Figure 3.3. The algorithm starts with some initial solution and enters a loop. <p> Return S. include choosing the initial solution, the perturbation mechanism and the cost function. Other available choices are in the settings of the following generic parameters introduced by Johnson et al. <ref> [49] </ref>. INITPROB Used in determining an initial temperature (INITTEMP) for annealing. Based on an abbreviated trial annealing run, a temperature is found at which the fraction of accepted perturbations is approximately INITPROB. TEMPFACTOR This is a descriptive name for the cooling ratio r in Fig ure 3.3.
Reference: [50] <author> Jong Kim and Chita R. Das. </author> <title> Modeling Wormhole Routing in a Hypercube. </title> <booktitle> In 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 386-393, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: This allows the communication pattern of an application to be considered explicitly. Uniform, randomly distributed, communication traffic does not adequately model the behavior of many practical applications [47, 65]. This has prompted the 76 development of more sophisticated models of network traffic, such as spherical distribution <ref> [50] </ref> and hot-spot traffic [54]. However, the model supported by NetSim is more general. We assume that only one process is mapped to each PE. <p> Similarly, genLength () can be modified to implement whichever message length distribution the user requires. Some choices that have been suggested for network evaluation include exponential <ref> [50] </ref>, constant [22], and distributions based on multi-stage probability density functions [47]. 5.4 Experimental Results NetSim has been implemented in C++ and validated by comparison against other simulations reported previously (e.g. [97]). Below we describe an experiment involving NetSim.
Reference: [51] <author> S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi. </author> <title> Optimization by Simulated Annealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680, </pages> <month> May </month> <year> 1983. </year>
Reference-contexts: Local optimization suffers due to its inability to back out of unattractive local optima since it never accepts a move to a new solution unless the direction of the move is downhill. Simulated annealing proposed by Kirkpatrick et al. <ref> [51] </ref> is an enhancement of local optimization that allows for occasional uphill moves (changes that worsen the solution) in an attempt to avoid getting trapped in poor local optima. Simulated annealing typically yields better results than local optimization. the graph G 500;5;5 (described in Section 3.2).
Reference: [52] <author> Clyde P. Kruskal and Marc Snir. </author> <title> The Performace of Multistage Interconnection Networks for Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(12):1091-1098, </volume> <month> Dec </month> <year> 1983. </year>
Reference-contexts: However, we show that these partitions result in very good embeddings in the ICN. In Chapter 4 we describe an analytical model for the performance evaluation of synchronous networks under different communication patterns. Previous studies on network analysis <ref> [22, 52, 72, 97] </ref> do not model the communication traffic of the application explicitly; instead, they assume that communication traffic is uniform randomly distributed. Recently reported evidence [47, 65] raises serious questions about the validity of this assumption. <p> Since the ICN exploits knowledge about the communication pattern of individual applications, models designed to study its performance must be able to model communication patterns explicitly. Much of the previous work on performance analysis of networks has assumed random, uniformly distributed communication <ref> [22, 27, 52, 72, 97] </ref> and is inapplicable to many parallel applications that lack this uniformity. In this chapter, we develop an analytical model to study the performance of synchronous, circuit-switched networks under different communication patterns [43]. <p> This computation involves repeated application of the technique of Sect. 4.1.2. Note that m o is simply the network throughput per PE under the assumption that dropped messages are ignored. This assumption has been used in previous studies <ref> [27, 52, 72] </ref>. It has the effect of underestimating the actual throughput at low message generation rates and overestimating it at high request rates. 2. Resubmission of dropped messages causes the effective message generation rate to be higher than m. <p> Our experience indicates that the special purpose features of NetSim make it five to ten times faster than a similar program based on SIMPACK. Tools for simulating computer networks have also been reported in <ref> [22, 27, 52, 97] </ref>. Those tools, however, are primarily designed to validate specific mathematical analyses and, as such, lack the versatility of NetSim. NetSim is programmable as to network topology, communication pattern, routing algorithm, mapping strategy and network operating assumptions (see Figure 5.1).
Reference: [53] <author> H. T. Kung. </author> <title> The Structure of Parallel Algorithms. </title> <booktitle> Advances in Computers, </booktitle> <volume> 19 </volume> <pages> 65-112, </pages> <year> 1980. </year>
Reference-contexts: In these distributed memory machines (see sidebox), data access times can vary significantly depending on the location of required data. Fortunately, in most parallel applications, not all processes need to communicate with all others <ref> [53] </ref>. This offers the potential of improving an application's performance by mapping heavily communicating processes to `near-by' processing elements. The extent to which the mapping objective can be attained depends on the communication structure of the application as well as the network topology. <p> Since communication patterns show great variations among different applications <ref> [53] </ref>, networks with a fixed (static) topology may not perform well over a wide range of applications. This compelling idea has inspired a number of research efforts some of which are mentioned below. Goodyear Aerospace's MPP The massively parallel processor (MPP) [7], was designed in the early 80s. <p> In most cases, these communication graphs are sparse <ref> [53] </ref>. This phenomenon is captured in the notion of switching locality. We say that an application exhibits switching locality, if each computational entity switches the bulk of its communication between a small set of other entities; and changes, if any, in the membership of these sets are infrequent. <p> Partitioning 2-D Regular Arrays Consider the following problem: how can processors be distributed in a two-dimensional area so that they can be connected in a simple and regular way, in the sense that all connections are symmetric and of the same length? Kung <ref> [53] </ref> notes that there are 27 only three solutions to the problem. This problem is related to that of finding regular figures that can be closely packed to completely cover a two dimensional area. The only figures that possess this property are the square, the hexagon, and the equilateral triangle.
Reference: [54] <author> Gyungho Lee. </author> <title> A Performance Bound of Multistage Combining Networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(10) </volume> <pages> 1387-1395, </pages> <month> Oct </month> <year> 1989. </year>
Reference-contexts: Uniform, randomly distributed, communication traffic does not adequately model the behavior of many practical applications [47, 65]. This has prompted the 76 development of more sophisticated models of network traffic, such as spherical distribution [50] and hot-spot traffic <ref> [54] </ref>. However, the model supported by NetSim is more general. We assume that only one process is mapped to each PE.
Reference: [55] <author> Insup Lee and David Smitley. </author> <title> A Synthesis Algorithm for Reconfigurable Interconnection Networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(6) </volume> <pages> 691-699, </pages> <month> Jun. </month> <year> 1988. </year>
Reference-contexts: This corresponds to minimizing the total number of segments traversed. Lee and Smitley <ref> [55, 88] </ref> have considered restricted versions of these problems (all d i s are equal) and shown them to be NP-complete. It might be possible to extend their solutions to work with more general assumptions.
Reference: [56] <author> Frank T. Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: As with other graphs, in some instances depending on the actual number of nodes it might be possible to obtain smaller bounded contractions. 2.2.4 New Results Partitioning a 2-D Mesh of Trees A two dimensional mesh of trees <ref> [56] </ref> is constructed by arranging n 2 nodes in an n fi n grid (without edges), and building a full binary tree on each row and each column of this grid. If k denotes the height of the binary tree, then n = 2 k1 . <p> X-trees [26] Min, Max, Sum computation, Searching algorithms, Parallel prefix computations Divide and Conquer algorithms Grids and Tori Matrix Algorithms, Solution of Partial Differential equations Shu*e-exchange and Fast Fourier transform, De Bruijn graphs [81] Sorting Hypercubes and Ascend-Descend algorithms [74] Cube-Connected Cycles (CCC) Pyramids Image processing [17] Mesh of trees <ref> [56] </ref> Graph algorithms graphs having randomly generated edges. Standard random graphs denoted by G n;p are defined in terms of two parameters, n and p.
Reference: [57] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Henessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25 </volume> <pages> 63-79, </pages> <month> Mar. </month> <year> 1992. </year> <month> 109 </month>
Reference-contexts: Other similar systems (or their components) have been developed in several universities, such as Carnegie Mellon University (iWarp [15]), Massachussetts Institute of Technology (J-machine [23, 69], *T [68]) and Stanford (DASH <ref> [57, 58] </ref>). The main motivation behind parallel computing is that by working concurrently on different parts of a problem, the problem can be solved faster and in larger instances than otherwise possible.
Reference: [58] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessy. </author> <title> The DASH Prototype: Logic Overhead and Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 44-61, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Other similar systems (or their components) have been developed in several universities, such as Carnegie Mellon University (iWarp [15]), Massachussetts Institute of Technology (J-machine [23, 69], *T [68]) and Stanford (DASH <ref> [57, 58] </ref>). The main motivation behind parallel computing is that by working concurrently on different parts of a problem, the problem can be solved faster and in larger instances than otherwise possible.
Reference: [59] <author> Hungwen Li and Massimo Maresca. </author> <title> Polymorphic-Torus Network. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(9) </volume> <pages> 1345-1351, </pages> <month> Sep </month> <year> 1989. </year>
Reference-contexts: The number of switches, and hence latency, on a tree edge varies with its distance from the root | edges closer to the root are longer than those near the leaves. Polymorphic Torus The Polymorphic torus <ref> [59] </ref> is architecturally similar to the CHiP, with the following differences: network topology can be reconfigured in every machine cycle and there is only one switch per PE.
Reference: [60] <author> Yuh-Dauh Lyuu and Eugen Schenfeld. </author> <title> Parallel Graph Contraction with Applications to a Reconfigurable Parallel Architecture. </title> <booktitle> In Proc. of the 1994 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1994. </year> <note> (to appear). </note>
Reference-contexts: Subsequently, other researchers <ref> [5, 60] </ref> have extended their work to include additional graphs. In this subsection, we review some of these results; and, in the next, augment them with a few new results of our own. CCCs and binary trees as described in [6]. <p> This label-masking mechanism is also useful in developing bounded 4-contractions for binary shu*e-exchange and de-Bruijn graphs <ref> [60] </ref>. Partitioning undirected Shu*e-Exchange graphs A shu*e-exchange graph consists of 2 n nodes and each node can be assigned a unique n-bit label. <p> Only the first and the last groups may have fewer than six nodes. The first group only contains one node. This partitioning is shown in Figure 2.8 (a). Lyuu and Schenfeld <ref> [60] </ref> have developed an improved partitioning scheme which yields a bounded 5-contraction. This scheme is shown in Figure 2.8 (b). 25 Partitioning Pyramids The interested reader is referred to [60] for a partitioning scheme that yields a bounded 12-contraction for pyramid graphs. <p> The first group only contains one node. This partitioning is shown in Figure 2.8 (a). Lyuu and Schenfeld <ref> [60] </ref> have developed an improved partitioning scheme which yields a bounded 5-contraction. This scheme is shown in Figure 2.8 (b). 25 Partitioning Pyramids The interested reader is referred to [60] for a partitioning scheme that yields a bounded 12-contraction for pyramid graphs.
Reference: [61] <author> Eva Ma and Lixin Tao. </author> <title> Embeddings among Meshes and Tori. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18(1) </volume> <pages> 44-55, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: However, other edges which get mapped to longer paths can cause a communication bottleneck; they are not accounted for in this objective function. Another, often used, criterion is minimization of the maximum dilation of any edge in E Comm (see <ref> [18, 28, 31, 45, 61, 94] </ref> and references therein). This criterion does not distinguish between a mapping in which only a few communication edges have large dilations, and another in which all communication edges have large dilations.
Reference: [62] <author> Y. W. Ma, R. Krishnamurti, L. Tao, D. G. Shea, B. Narahari, and R. Varadarajan. </author> <title> Reconfigurable Special-purpose Computers. </title> <booktitle> In Proc. of the International Conference on Supercomputing, </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: If the same mapping is used to map a torus, however, wraparound connections force the use of long communication paths. We use a dilation-2 embedding for mapping tori into meshes of the same size. This mapping is described in <ref> [62] </ref> and illustrated in Figure 6.3b. For binary trees, we use the mapping scheme proposed by Snyder [89] (see Figure 6.3a). When directly applicable, we use the results of Ma and Tao [63] to map 3-D grids and tori in the 2-D mesh.
Reference: [63] <author> Y. W. Ma and L. Tao. </author> <title> Embeddings Among Toruses and Meshes. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 178-187, </pages> <year> 1987. </year>
Reference-contexts: In a network graph, nodes represent PEs and switching elements while the edges represent network links. Starting with the pioneering work of Bokhari [14], a number of researchers have looked at this mapping problem <ref> [2, 10, 13, 44, 45, 46, 63, 80] </ref>. <p> This mapping is described in [62] and illustrated in Figure 6.3b. For binary trees, we use the mapping scheme proposed by Snyder [89] (see Figure 6.3a). When directly applicable, we use the results of Ma and Tao <ref> [63] </ref> to map 3-D grids and tori in the 2-D mesh. In other cases, it is useful to combine their results with those of Aleliunas and Rosenberg [2]. For example, a 6fi6fi6 grid can be mapped in a 16 fi 16 mesh in two steps.
Reference: [64] <author> R. McGill, J. W. Tukey, and W. A. Desarbo. </author> <title> Variations on box plots. </title> <journal> Am. Stat., </journal> <volume> 32(1) </volume> <pages> 12-16, </pages> <year> 1978. </year>
Reference-contexts: Data for these plots were obtained from 120 runs of the algorithm on a CCC of degree 6 with 42 ` = 4. The variation in running time and badness are represented by box and whiskers plots (McGill et al. <ref> [64] </ref>). Each box delimits the middle two quartiles of the result (the middle line is the median). The whiskers above and below the box go to the farthest values that are not in the top or bottom fifth percentile. Values beyond the whiskers are individually plotted.
Reference: [65] <author> Jeffrey C. Mogul. </author> <title> Network locality at the scale of processes. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(2) </volume> <pages> 81-109, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Previous studies on network analysis [22, 52, 72, 97] do not model the communication traffic of the application explicitly; instead, they assume that communication traffic is uniform randomly distributed. Recently reported evidence <ref> [47, 65] </ref> raises serious questions about the validity of this assumption. When communication paths are short, our analytical model produces results that are in good agreement with those obtained from simulations, but are obtained in a fraction of the simulation time. <p> This allows the communication pattern of an application to be considered explicitly. Uniform, randomly distributed, communication traffic does not adequately model the behavior of many practical applications <ref> [47, 65] </ref>. This has prompted the 76 development of more sophisticated models of network traffic, such as spherical distribution [50] and hot-spot traffic [54]. However, the model supported by NetSim is more general. We assume that only one process is mapped to each PE.
Reference: [66] <author> Lionel M. Ni and Philip K. McKinley. </author> <title> A Survey of Wormhole Routing Techniques in Direct Networks. </title> <booktitle> Computer, </booktitle> <pages> pages 62-76, </pages> <month> Feb </month> <year> 1993. </year>
Reference-contexts: A deadlock occurs when there is cyclic wait situation between multiple messages, none of which can advance due to lack of buffer space. This is not a problem for acyclic networks such as multi-stage interconnection networks. In others, appropriate restrictions on the routing algorithm can help avoid deadlocks <ref> [66] </ref>. These restrictions are assumed to be part of the routing information included in the user's specification of network topology. Our programs that generate such specifications can be made to choose between row-column, west-first or negative-first routing [38] for 2-D meshes and e-cube routing for hypercubes.
Reference: [67] <author> D. M. Nicholson, A. Chowdhary, and L. Schwartz. </author> <title> Monte Carlo optimization of pair distributed functions | application to the electronic structure of disordered metals. </title> <journal> Physical Review B, </journal> <volume> 19 </volume> <pages> 1633-1637, </pages> <year> 1984. </year>
Reference-contexts: The histogram suggests that simulated annealing is inherently more powerful than the technique of local optimization. This observation is in agreement with that of other researchers [49]. Simulated Annealing has previously been used to solve a wide variety of other combinatorial optimization problems <ref> [12, 19, 39, 67, 71, 95] </ref>. 3.1.2 A generic implementation A generic simulated annealing algorithm is outlined in Figure 3.3. The algorithm starts with some initial solution and enters a loop. During each iteration of this loop, it considers a perturbation of the current solution.
Reference: [68] <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind. </author> <title> *T: A Multithreaded Massively Parallel Architecture. </title> <booktitle> In Proceedings of the 1992 International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <year> 1992. </year>
Reference-contexts: Other similar systems (or their components) have been developed in several universities, such as Carnegie Mellon University (iWarp [15]), Massachussetts Institute of Technology (J-machine [23, 69], *T <ref> [68] </ref>) and Stanford (DASH [57, 58]). The main motivation behind parallel computing is that by working concurrently on different parts of a problem, the problem can be solved faster and in larger instances than otherwise possible. <p> This places an artificial restriction on the rate at which new messages are injected in the network and keeps network load low. As message processing times decrease (newer architectures like the J-machine [69] and *T <ref> [68] </ref> have a message processing overhead of only a few instruction cycles), we expect the performance advantages offered by the ICN to assume even greater importance.
Reference: [69] <author> M. D. Noakes, D. A. Wallach, and W. J. Dally. </author> <title> The J-machine multicomputer: An architectural evaluation. </title> <booktitle> In 20th Annual International Symposium of Computer Architecture, </booktitle> <pages> pages 224-235, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The Cray T3D and the IBM SP1 are new MPP offerings from companies that have a traditional base in vector super computers and mainframes, respectively. Other similar systems (or their components) have been developed in several universities, such as Carnegie Mellon University (iWarp [15]), Massachussetts Institute of Technology (J-machine <ref> [23, 69] </ref>, *T [68]) and Stanford (DASH [57, 58]). The main motivation behind parallel computing is that by working concurrently on different parts of a problem, the problem can be solved faster and in larger instances than otherwise possible. <p> This places an artificial restriction on the rate at which new messages are injected in the network and keeps network load low. As message processing times decrease (newer architectures like the J-machine <ref> [69] </ref> and *T [68] have a message processing overhead of only a few instruction cycles), we expect the performance advantages offered by the ICN to assume even greater importance.
Reference: [70] <author> J. Ortega and R. Voigt. </author> <title> Solution of Partial Differential Equations on Vector and Parallel Computers. </title> <journal> SIAM Review, </journal> <volume> 27(2) </volume> <pages> 149-240, </pages> <year> 1985. </year>
Reference-contexts: Consider computational fluid dynamics (CFD) applications for weather forecasting or aerodynamic structure design. These involve the simulation of air flow in 2- or 3-dimensional space <ref> [70] </ref>. In this case, software components corresponding to one area in space communicate mostly with those corresponding to immediately adjacent areas. This results in a 2-D or 3-D grid shaped communication graph.
Reference: [71] <author> R. H. J. M. Otten and L. P. P. P. van Ginneken. </author> <title> Floorplan design using simulated annealing. </title> <booktitle> In Proc. of the IEEE International Conference on Computer-Aided Design, </booktitle> <pages> pages 96-98, </pages> <address> Santa Clara, </address> <year> 1984. </year>
Reference-contexts: The histogram suggests that simulated annealing is inherently more powerful than the technique of local optimization. This observation is in agreement with that of other researchers [49]. Simulated Annealing has previously been used to solve a wide variety of other combinatorial optimization problems <ref> [12, 19, 39, 67, 71, 95] </ref>. 3.1.2 A generic implementation A generic simulated annealing algorithm is outlined in Figure 3.3. The algorithm starts with some initial solution and enters a loop. During each iteration of this loop, it considers a perturbation of the current solution.
Reference: [72] <author> Janak H. Patel. </author> <title> Performance of Processor-Memory Interonnections for Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(10):771-780, </volume> <month> Oct </month> <year> 1981. </year> <month> 110 </month>
Reference-contexts: However, we show that these partitions result in very good embeddings in the ICN. In Chapter 4 we describe an analytical model for the performance evaluation of synchronous networks under different communication patterns. Previous studies on network analysis <ref> [22, 52, 72, 97] </ref> do not model the communication traffic of the application explicitly; instead, they assume that communication traffic is uniform randomly distributed. Recently reported evidence [47, 65] raises serious questions about the validity of this assumption. <p> Since the ICN exploits knowledge about the communication pattern of individual applications, models designed to study its performance must be able to model communication patterns explicitly. Much of the previous work on performance analysis of networks has assumed random, uniformly distributed communication <ref> [22, 27, 52, 72, 97] </ref> and is inapplicable to many parallel applications that lack this uniformity. In this chapter, we develop an analytical model to study the performance of synchronous, circuit-switched networks under different communication patterns [43]. <p> In this chapter, we develop an analytical model to study the performance of synchronous, circuit-switched networks under different communication patterns [43]. We use this model to study the performance of the ICN in comparison to more popular reconfigurable networks: the delta <ref> [72] </ref> and the crossbar. Network throughput which measures the number of messages that can be exchanged across the network in unit time is used as the performance metric for this study. We illustrate our analytical technique on two communication graphs. <p> For the special case 59 when m i = m in , for all i, and p ij = 1=k, for all i; j, the above expression simplifies to m 0 = 1 1 m in k . This is the same expression as obtained by Patel <ref> [72] </ref>. <p> This computation involves repeated application of the technique of Sect. 4.1.2. Note that m o is simply the network throughput per PE under the assumption that dropped messages are ignored. This assumption has been used in previous studies <ref> [27, 52, 72] </ref>. It has the effect of underestimating the actual throughput at low message generation rates and overestimating it at high request rates. 2. Resubmission of dropped messages causes the effective message generation rate to be higher than m.
Reference: [73] <author> David Patterson and John L. Hennessy. </author> <title> Computer Architecture: A Quantitaive Approach. </title> <publisher> Morgan Kaufman Publishers, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: It offers completely scalable access to memory access whose bandwidth increases in proportion to the number of processors. While, at first, this may seem an irresolvable conflict; it is possible to implement a shared, single address space on top of physically distributed memory (see <ref> [73] </ref>).
Reference: [74] <author> Franco P. Preparata and Jean Vuillemin. </author> <title> The Cube-Connected Cycles: A versatile network for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 24(5) </volume> <pages> 300-309, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: For all of these graphs, except the CCCs, smaller bounded contractions do not exist [5, 6]. Partitioning Cube Connected Cycles Irrespective of its size, a bounded 4-contraction can always be found for a CCC graph. This graph, introduced by Preparata and Vuillemin <ref> [74] </ref>, contains n2 n nodes and each node can be uniquely identified by a label of the form hx 1 x 2 : : : x i+1 : : : x n ; ii (0 i &lt; n). <p> Graph Application Trees and X-trees [26] Min, Max, Sum computation, Searching algorithms, Parallel prefix computations Divide and Conquer algorithms Grids and Tori Matrix Algorithms, Solution of Partial Differential equations Shu*e-exchange and Fast Fourier transform, De Bruijn graphs [81] Sorting Hypercubes and Ascend-Descend algorithms <ref> [74] </ref> Cube-Connected Cycles (CCC) Pyramids Image processing [17] Mesh of trees [56] Graph algorithms graphs having randomly generated edges. Standard random graphs denoted by G n;p are defined in terms of two parameters, n and p.
Reference: [75] <author> David J. Pritchard and Denis A. Nicole. </author> <title> Cube Connected Mobius Ladders: An Inherently Deadlock-Free Fixed Degree Network. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 111-117, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Deadlock avoidance in dense graphs: In Chapter 2 we mentioned that if no knowledge of the application's communication graph is available, a suitable dense graph topology might be considered for the global crossbar. Past research <ref> [1, 4, 75, 91] </ref> has resulted in the discovery of a number of dense graphs. Still, the general problem of deadlock free routing in such graphs is not very well understood.
Reference: [76] <author> A. Alan B. Pritsker. </author> <title> Introduction to Simulation and SLAM II. </title> <publisher> Halsted Press, 3rd edition, </publisher> <year> 1986. </year>
Reference-contexts: The down side of simulation is that it is more time consuming. In the space defined by all possible combinations of system parameters, each simulation run investigates only a single point. 74 A number of powerful simulation languages such as GPSS [84], SLAM <ref> [76] </ref>, and SIM-SCRIPT [16] are commercially available. However, there is a considerable start up time associated with using them, since they require learning a new language. Libraries and toolkits based on popular programming languages (e.g. CSIM [85] and SIMPACK [34] based on C) do not suffer from this drawback.
Reference: [77] <author> Badrinath Ramamurthy and M. S. Krishnamoorthy. </author> <title> Bounded p-contractability is NP-complete. </title> <type> Technical Report 92-17, </type> <institution> Department of Computer Science, Rennse-laer Polytechnic Institute, </institution> <address> Troy, New York, </address> <month> Jun </month> <year> 1992. </year>
Reference-contexts: Their algorithm has a time complexity of O (n 3 ) where n is the number of nodes in the graph. They have also shown that the problem of determining whether a graph has a bounded `-contraction for ` &gt; 2 is NP-complete <ref> [77] </ref>.
Reference: [78] <author> Badrinath Ramamurthy and Mukkai S. Krishnamoorthy. </author> <title> Bounded 2-contractions of graphs. </title> <booktitle> In Proceedings of the 1991 Annual Allerton Conference on Communication, Control and Computing, </booktitle> <pages> pages 498-507, </pages> <year> 1991. </year>
Reference-contexts: Also, K m (m+1) contains a node with degree m (m + 1) 1. From the second lemma above, it follows that K m (m+1) can not have a bounded `-contraction for ` &lt; m. 2 Ramamurthy and Krishnamoorthy <ref> [78] </ref> have developed an algorithm to determine if a graph has a bounded 2-contraction. Their algorithm has a time complexity of O (n 3 ) where n is the number of nodes in the graph.
Reference: [79] <author> Daniel A. Reed and Richard M. Fujimoto. </author> <title> Multicomputer Networks Message-Based Parallel Processing. </title> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: For any message arriving at a switch, the next required link is 2 This corresponds to having one flit buffer per link. 79 determined by calling the function getRouterLink (). Routing in NetSim is based on the use of lookup tables <ref> [79] </ref> maintained at every switch. For each destination, the table lists all possible outgoing links on which an incoming message may be advanced. In the case of multiple choices, a tie-breaking strategy is built into getRouterLink (). 3. A function okToGenerate () controls when a PE generates new messages.
Reference: [80] <author> Ponnuswany Sadayappan and Fikret Ercal. </author> <title> Nearest-Neighbor Mapping of Finite Element Graphs onto Processor Meshes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1408-1424, </volume> <month> Dec </month> <year> 1987. </year>
Reference-contexts: In a network graph, nodes represent PEs and switching elements while the edges represent network links. Starting with the pioneering work of Bokhari [14], a number of researchers have looked at this mapping problem <ref> [2, 10, 13, 44, 45, 46, 63, 80] </ref>.
Reference: [81] <author> Maheswara R. Samatham and Dhiraj K. Pradhan. </author> <title> The De Bruijn multiprocessor network: A versatile parallel processing and sorting network for VLSI. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(4) </volume> <pages> 567-581, </pages> <month> Apr </month> <year> 1989. </year>
Reference-contexts: Graph Application Trees and X-trees [26] Min, Max, Sum computation, Searching algorithms, Parallel prefix computations Divide and Conquer algorithms Grids and Tori Matrix Algorithms, Solution of Partial Differential equations Shu*e-exchange and Fast Fourier transform, De Bruijn graphs <ref> [81] </ref> Sorting Hypercubes and Ascend-Descend algorithms [74] Cube-Connected Cycles (CCC) Pyramids Image processing [17] Mesh of trees [56] Graph algorithms graphs having randomly generated edges. Standard random graphs denoted by G n;p are defined in terms of two parameters, n and p.
Reference: [82] <author> A. A. Sawchuk, B. K. Jenkins, C. S. Raghavendra, and A. Varma. </author> <title> Optical crossbar networks. </title> <journal> Computer, </journal> <volume> 20(6) </volume> <pages> 50-60, </pages> <month> Jun </month> <year> 1987. </year>
Reference-contexts: The answer to this question lies in the speed/size trade-off for switches, as mentioned in Chapter 1. For a scalable ICN architecture, N might be 1000 or even larger. While crossbars of this size are physically realizable (e.g. optically as described in <ref> [82] </ref>), they switch rather slowly. For many applications where each process needs to communicate with multiple (not just one) other processes, frequent reconfiguration of such a crossbar will cause a large switching overhead. This overhead could dominate any advantage offered by a reconfigurable network. <p> However, when information on the application's communication traffic is available, the ICN can exploit it better than conventional networks. As noted by Sawchuk et al. <ref> [82] </ref>, the large crossbar can have a much higher channel bandwidth than smaller crossbars. This may be utilized to multiplex more than one local channel on each segment, without any local channel experiencing a loss in bandwidth due to sharing. <p> The second, t set is the time required to establish direct paths between the selected pairs of input and output links. The value of t select depends on the complexity of making routing decisions and t set is dependent on factors such as switch size and technology used <ref> [82] </ref>. Note that even for the same switch size and technology, cycle times can vary, e.g. t select for a switch used in a Clos network (requiring centralized routing) is likely to be higher than for a similar switch used in a self-routing MIN such as Delta.
Reference: [83] <author> Eugen Schenfeld. </author> <title> The Optical Parallel Architecture Model (OPAM) experiment | project plan and motivations. </title> <type> Technical Report TR 88-17, </type> <institution> Department of Computer Science, Hebrew University of Jerusalem, Israel, </institution> <month> Aug </month> <year> 1988. </year>
Reference-contexts: All of these approaches suffer from one or more of the following constraints: lack of MIMD (multiple instruction, multiple data streams) support, limited reconfigurability and poor resource utilization when implementing complex topologies. The Interconnection Cached Network (ICN) <ref> [83] </ref> is a novel reconfigurable network that addresses these shortcomings by exploiting characteristic features of parallel applications and available technologies. 6 1.1.2 Characteristics of Parallel Applications Many parallel applications can be modeled as proceeding in a small number of phases during which the communication graph is stable [90], e.g. the conjugate
Reference: [84] <author> Thomas J. Schriber. </author> <title> An Introduction to Simulation using GPSS/H. </title> <publisher> John Wiley, </publisher> <year> 1991. </year>
Reference-contexts: The down side of simulation is that it is more time consuming. In the space defined by all possible combinations of system parameters, each simulation run investigates only a single point. 74 A number of powerful simulation languages such as GPSS <ref> [84] </ref>, SLAM [76], and SIM-SCRIPT [16] are commercially available. However, there is a considerable start up time associated with using them, since they require learning a new language. Libraries and toolkits based on popular programming languages (e.g.
Reference: [85] <author> H. D. Schwetman. CSIM: </author> <title> A C-based Process-oriented Simulation Language. </title> <booktitle> In Proceedings of the 1986 Winter Simulation Conference, </booktitle> <month> Dec </month> <year> 1986. </year>
Reference-contexts: However, there is a considerable start up time associated with using them, since they require learning a new language. Libraries and toolkits based on popular programming languages (e.g. CSIM <ref> [85] </ref> and SIMPACK [34] based on C) do not suffer from this drawback. Still, they require a computer architect to work with unfamiliar abstractions. NetSim is a versatile simulation tool, designed specifically for modeling the performance of multiprocessor networks.
Reference: [86] <author> H. J. Siegel. </author> <title> The Theory Underlying the Partitioning of Permutation Networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(9):791-800, </volume> <month> Sep </month> <year> 1980. </year>
Reference-contexts: For the parallel system to be cost-effective, it is imperative that multiple applications be able to share it simultaneously. This important feature, called space-sharing, requires that the underlying network be partitionable. A network is said to be partitionable <ref> [86] </ref> if it can be divided into independent subnetworks; each subnetwork of size N 0 must 14 Segments used by the "Tree" application are shown using thick lines. have all of the interconnection capabilities of a complete network of that same type built of size N 0 .
Reference: [87] <author> H. D. Simon. </author> <title> Partitioning of Unstructured Problems for Parallel Processing. </title> <journal> Computing Systems in Engineering, </journal> 2(2/3):135-148, 1991. <volume> 111 </volume>
Reference-contexts: An efficient algorithm for solving this graph partitioning problem is the recursive spectral bi-partitioning approach <ref> [87] </ref>. Unless the cluster size is one, the resulting communication graph is different from the underlying grid on which the application operates. <p> Designing other partitioning algorithms: Simulated annealing is just one example of heuristic techniques useful for solving hard combinatorial problems. The use of 1 This could be done using the recursive spectral bipartitioning (RSB) algorithm <ref> [87] </ref>. 104 other techniques such as genetic algorithms or algorithms based on a linear programming formulation of the problem have not been studied. Quite often, communication graphs have specific properties, e.g. planarity in the case of trees, and 2-D arrays.
Reference: [88] <author> D. Smitley and I. Lee. </author> <title> Synthesizing Minimum Total Expansion Topologies for Reconfigurable Interconnection Networks. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 7 </volume> <pages> 178-199, </pages> <year> 1989. </year>
Reference-contexts: These differences can be accounted for by considering the total dilation of all edges in E Comm . This is the criterion used by Simtley and Lee <ref> [88] </ref> (which they call total expansion). Compared to store-and-forward, newer routing techniques like wormhole routing or circuit switching make communication delays only weakly dependent on the number of intervening switching nodes [22, 30]. This observation motivates other mapping criteria. <p> This corresponds to minimizing the total number of segments traversed. Lee and Smitley <ref> [55, 88] </ref> have considered restricted versions of these problems (all d i s are equal) and shown them to be NP-complete. It might be possible to extend their solutions to work with more general assumptions.
Reference: [89] <author> Lawrence Snyder. </author> <title> Introduction to the Configurable, Highly Parallel Computer. </title> <journal> Computer, </journal> <volume> 15 </volume> <pages> 47-56, </pages> <month> Jan </month> <year> 1982. </year>
Reference-contexts: Optionally, it is also possible to connect all 16,384 PEs in a linear chain or a ring. The MPP is an example of a Single Instruction stream Multiple Data stream [35] (SIMD, pronounced sim-dee) architecture and offers very limited reconfiguration. Configurable Highly Parallel Computer (CHiP) The CHiP <ref> [89] </ref> consists of homogeneous microprocessors embedded in a regular lattice of switches. A central controller can set these switches to any one of a set of pre-stored configuration settings. The number of switches separating two adjacent PEs is called the corridor width. <p> We use a dilation-2 embedding for mapping tori into meshes of the same size. This mapping is described in [62] and illustrated in Figure 6.3b. For binary trees, we use the mapping scheme proposed by Snyder <ref> [89] </ref> (see Figure 6.3a). When directly applicable, we use the results of Ma and Tao [63] to map 3-D grids and tori in the 2-D mesh. In other cases, it is useful to combine their results with those of Aleliunas and Rosenberg [2]. <p> This suggests that, unlike previous patterns, the binary tree does not scale well on the mesh. When a full binary tree is embedded into a mesh, the maximum dilation of the mapping grows as 99 the tree size increases <ref> [89] </ref>. This growth, although quite slow, is enough to adversely affect the binary tree's scalability on the 2-D mesh.
Reference: [90] <author> Lawrence Snyder. </author> <title> Languages, Compilers and Run-time Environments for Distributed Memory Machines, chapter Applications of the "Phase Abstractions" for Portable and Scalable Parallel Programming, </title> <address> pages 79-102. </address> <publisher> Elsevier, </publisher> <year> 1992. </year>
Reference-contexts: A distinguishing feature of our work is that it is done in the context of a reconfigurable network rather than a static network. 1.1.1 Reconfigurable Networks A reconfigurable network is one whose topology can be altered between applications or between different phases (see <ref> [90] </ref>) of the same application. An application's performance can often benefit from a close match between its communication graph and 3 Based on their memory organization, parallel architectures can be classified as either centralized ("Dance-Hall") or distributed ("Boudoir") [9]. <p> Cached Network (ICN) [83] is a novel reconfigurable network that addresses these shortcomings by exploiting characteristic features of parallel applications and available technologies. 6 1.1.2 Characteristics of Parallel Applications Many parallel applications can be modeled as proceeding in a small number of phases during which the communication graph is stable <ref> [90] </ref>, e.g. the conjugate gradient method for solving partial differential equations uses a "grid" phase for matrix multiplication and a "tree" phase for summation and broadcast. In most cases, these communication graphs are sparse [53]. This phenomenon is captured in the notion of switching locality. <p> This overhead could dominate any advantage offered by a reconfigurable network. In the ICN, the 12 of a 16-node hypercube. global crossbar is switched only when the communication pattern of the application changes. Since this does not happen very often <ref> [90] </ref> for a number of important applications, a large switching overhead is avoided. The global crossbar is used to establish point to point connections, called segments, between the local crossbars. 1 The choice of which pairs of clusters to connect depends on the communication graph of the application.
Reference: [91] <author> K. Wendy Tang and Bruce W. Arden. </author> <title> Representation of Borel Cayley Graphs. </title> <booktitle> In Proceedings of the 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 194-201, </pages> <month> Oct </month> <year> 1992. </year>
Reference-contexts: In the absence of this information, the ICN may be configured to a fixed topology and operated as a static network. Previous research has looked at the problem of finding dense graphs <ref> [1, 4, 32, 91] </ref>. Given two integers d and L, the problem calls for finding a graph with bounded degree d and diameter L that has the maximum number of nodes possible. <p> For the 64 PE ICN, this is achieved by connecting the 16 clusters in a bidirectional ring and using the remaining links to connect clusters that are four away on the ring (Figure 4.5). This corresponds to a 16-node generalized chordal ring (GCR) <ref> [91] </ref> of degree four and diameter three. Some messages might need to pass through as many as four local crossbars before reaching their destination. The circuit building phase in this case, therefore, takes up the first four cycles in the network cycle. <p> Deadlock avoidance in dense graphs: In Chapter 2 we mentioned that if no knowledge of the application's communication graph is available, a suitable dense graph topology might be considered for the global crossbar. Past research <ref> [1, 4, 75, 91] </ref> has resulted in the discovery of a number of dense graphs. Still, the general problem of deadlock free routing in such graphs is not very well understood.
Reference: [92] <institution> Thinking Machines Corporation. </institution> <type> Connection Machine CM-5 Technical Summary, </type> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: This trend towards massively parallel-processing (MPP) machines is underlined by several recent (early 1990) developments in the high performance computing world. A number of multiprocessor machines, such as the Intel Paragon [9], the Thinking Machines CM-5 <ref> [92] </ref>, NCube's NCUBE2 and the Meiko CS2 [21] are currently available. The Cray T3D and the IBM SP1 are new MPP offerings from companies that have a traditional base in vector super computers and mainframes, respectively.
Reference: [93] <author> Jeffrey Scott Vitter and P. Krishnan. </author> <title> Optimal Prefetching via Data Compression. </title> <booktitle> In Proceedings of the 32nd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 121-130, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: The ability to predict future evolution of the traffic pattern, based on knowledge of the past, is crucial to the feasibility of this effort. In this regard, the recent emergence of a predictive computing paradigm is an exciting development. Vitter and Krishnan <ref> [93] </ref> have introduced the idea of using data compressors to predict future memory references for purposes of prefetching. While it is not clear that their technique is directly applicable to network traffic prediction, it certainly presents an interesting research opportunity. 105
Reference: [94] <author> A. S. Wagner. </author> <title> Embedding All Binary Trees in the Hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18(1) </volume> <pages> 33-43, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: However, other edges which get mapped to longer paths can cause a communication bottleneck; they are not accounted for in this objective function. Another, often used, criterion is minimization of the maximum dilation of any edge in E Comm (see <ref> [18, 28, 31, 45, 61, 94] </ref> and references therein). This criterion does not distinguish between a mapping in which only a few communication edges have large dilations, and another in which all communication edges have large dilations.
Reference: [95] <author> L. T. Willie. </author> <title> The football pool problem for 6 matches: a new upper bound obtained by simulated annealing. </title> <journal> Journal of Combinatorial Theory A, </journal> <volume> 45 </volume> <pages> 171-177, </pages> <year> 1987. </year>
Reference-contexts: The histogram suggests that simulated annealing is inherently more powerful than the technique of local optimization. This observation is in agreement with that of other researchers [49]. Simulated Annealing has previously been used to solve a wide variety of other combinatorial optimization problems <ref> [12, 19, 39, 67, 71, 95] </ref>. 3.1.2 A generic implementation A generic simulated annealing algorithm is outlined in Figure 3.3. The algorithm starts with some initial solution and enters a loop. During each iteration of this loop, it considers a perturbation of the current solution.
Reference: [96] <author> C.-L. Wu, Tse-Yun Feng, and Min-Chang Lin. </author> <title> Star: A Local Network System for Real Time Management of Imagery Data. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-31(10):923-933, </volume> <month> Oct </month> <year> 1982. </year>
Reference-contexts: This flexibility allows effective operation of the GF11 on many different communication patterns. Like the Polymorphic Torus and the MPP, the GF11 is a SIMD architecture. For some applications, the tight synchronization inherent in SIMD machines is overly restrictive. The Star System The Star System <ref> [96] </ref> developed at the University of Texas at Austin consists of two modified baseline networks and a set of interface units. A modified baseline network is a cascade of a bit-reversal permutation and baseline network. The system allows establishment of different topologies in different processor partitions.
Reference: [97] <author> Chuan-Lin Wu and Manjai Lee. </author> <title> Performance Analysis of Multistage Interconnection Network Configurations and Operations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(1) </volume> <pages> 18-27, </pages> <month> Jan </month> <year> 1992. </year>
Reference-contexts: However, we show that these partitions result in very good embeddings in the ICN. In Chapter 4 we describe an analytical model for the performance evaluation of synchronous networks under different communication patterns. Previous studies on network analysis <ref> [22, 52, 72, 97] </ref> do not model the communication traffic of the application explicitly; instead, they assume that communication traffic is uniform randomly distributed. Recently reported evidence [47, 65] raises serious questions about the validity of this assumption. <p> Since the ICN exploits knowledge about the communication pattern of individual applications, models designed to study its performance must be able to model communication patterns explicitly. Much of the previous work on performance analysis of networks has assumed random, uniformly distributed communication <ref> [22, 27, 52, 72, 97] </ref> and is inapplicable to many parallel applications that lack this uniformity. In this chapter, we develop an analytical model to study the performance of synchronous, circuit-switched networks under different communication patterns [43]. <p> Our experience indicates that the special purpose features of NetSim make it five to ten times faster than a similar program based on SIMPACK. Tools for simulating computer networks have also been reported in <ref> [22, 27, 52, 97] </ref>. Those tools, however, are primarily designed to validate specific mathematical analyses and, as such, lack the versatility of NetSim. NetSim is programmable as to network topology, communication pattern, routing algorithm, mapping strategy and network operating assumptions (see Figure 5.1). <p> Some choices that have been suggested for network evaluation include exponential [50], constant [22], and distributions based on multi-stage probability density functions [47]. 5.4 Experimental Results NetSim has been implemented in C++ and validated by comparison against other simulations reported previously (e.g. <ref> [97] </ref>). Below we describe an experiment involving NetSim. Effect of Mapping on Performance Consider the transmission of a message, consisting of d flits, from one PE to another that is k hops away.
Reference: [98] <author> David W. L. Yen, Janak H. Patel, and Edward S. Davidson. </author> <title> Memory Interference in Synchronous Multiprocessor Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-31(11):1116-1121, </volume> <month> Nov. </month> <year> 1982. </year> <month> 112 </month>
Reference-contexts: It has the effect of underestimating the actual throughput at low message generation rates and overestimating it at high request rates. 2. Resubmission of dropped messages causes the effective message generation rate to be higher than m. We use the following formula developed by Yen et al. <ref> [98] </ref> to compute the adjusted request rate: m a = 1 + m o m 1) This computation of the adjusted probability assumes that, during resubmission, the message destination is randomly reassigned from amongst the set of valid 60 destinations for that process.
References-found: 98

