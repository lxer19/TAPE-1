URL: http://www.daimi.aau.dk/~rmunk/Thesis/lancbpro.ps.gz
Refering-URL: http://www.daimi.aau.dk/~rmunk/publications.html
Root-URL: http://www.daimi.aau.dk
Email: e-mail: rmunk@daimi.aau.dk  
Title: Lanczos bidiagonalization with partial reorthogonalization  
Author: Rasmus Munk Larsen 
Date: October 1998  
Address: Ny Munkegade, building 540 DK-8000 Aarhus C  
Affiliation: Department of Computer Science University of Aarhus  
Abstract: A partial reorthogonalization procedure (BPRO) for maintaining semi-orthogonality among the left and right Lanczos vectors in the Lanczos bidiagonalization (LBD) is presented. The resulting algorithm is mathematically equivalent to the symmetric Lanczos algorithm with partial reorthogonalization (PRO) developed by Simon, but works directly on the Lanczos bidiagonalization of A. For computing the singular values and vectors of a large sparse matrix with high accuracy, the BPRO algorithm uses only half the amount of storage and a factor of 3-4 less work compared to methods based on PRO applied to an equivalent symmetric system. Like PRO, the algorithm presented here is based on simple recurrences, which enable it to monitor the loss of orthogonality among the Lanczos vectors directly without forming inner products. These recurrences are used to develop a Lanczos bidiagonalization algorithm with partial reorthogonalization, which has been implemented in a MATLAB package for sparse SVD and eigenvalue problems called PROPACK. Numerical experiments with the routines from PROPACK are conducted using a test problem from inverse helioseismology to illustrate the properties of the method. In addition, a number of test matrices from the Harwell-Boeing collection are used to compare the accuracy and efficiency of the MATLAB implementations of BPRO and PRO with the svds routine in MATLAB 5.1, which uses an implicitly restarted Lanczos algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. M. </author> <title> Amdahl, Validity of the single processor approach to achieving large-scale computing capabilities, </title> <booktitle> in AFIPS Conference Proceedings, </booktitle> <volume> 30, </volume> <publisher> AFIPS Press, </publisher> <address> Montvale, NJ, </address> <year> 1967, </year> <pages> 483-485. </pages>
Reference-contexts: the (k i)th Chebyshev polynomial and K i = j=1 j i K 1 = 1 ; 2 ( i i+1 ) : The Chebyshev polynomial P j , which satisfies the recurrence P j (fl) = 2flP j1 (fl) P j2 (fl), is bounded by unity on the interval <ref> [1; 1] </ref>, but grows exponentially in j outside. Thus provided that jfl i j &gt; 1 the largest eigenvalues (where k i is large) will converge rapidly. <p> The speedup is calculated from the formula S = (t mem =t cache ) 1 F c N dot (p) + (1 F c )N dot (p) ; which is the well-known Amdahl law (cf. <ref> [1] </ref>) in disguise. In the table we have further assumed that t mem =t cache = 10, which is quite reasonable (on the SGI PowerChallenge, for example, this ratio is around 15-20).
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, O. Ostrouchow and D. Sorensen, </author> <title> LAPACK Users' Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: In this situation we have that joe i oe i j p (m; n) u oe i see e.g. <ref> [2, Section 4.7, 4.9] </ref>. It follows from the error analysis of Paige [48] that the symmetric Lanczos algorithm applied to C (and therefore also the LBD, as we shall see below) satisfy bounds on this form. <p> The Ritz values are found by computing the Schur decomposition of T 2k : S T 2k T 2k S 2k = diag ( 1 ; : : : ; 2k ) ; (3.18) This can be done by using, e.g., the symmetric QR algorithm (see <ref> [29, 2] </ref>, LAPACK routine STEQR), or bisection based on Sturm sequences followed by inverse iteration (see [3, 2], LAPACK routines STEBZ and STEIN). <p> 2k : S T 2k T 2k S 2k = diag ( 1 ; : : : ; 2k ) ; (3.18) This can be done by using, e.g., the symmetric QR algorithm (see [29, 2], LAPACK routine STEQR), or bisection based on Sturm sequences followed by inverse iteration (see <ref> [3, 2] </ref>, LAPACK routines STEBZ and STEIN).
Reference: [3] <author> W. Bath, R. S. Martin and J. H. Wilkinson, </author> <title> Calculation of the Eigenvalues of a Symmetric Tridiagonal Matrix by the Method of Bisection, </title> <journal> Numerische Mathematik, </journal> <volume> 9 (1967), </volume> <pages> 386-393. </pages>
Reference-contexts: 2k : S T 2k T 2k S 2k = diag ( 1 ; : : : ; 2k ) ; (3.18) This can be done by using, e.g., the symmetric QR algorithm (see [29, 2], LAPACK routine STEQR), or bisection based on Sturm sequences followed by inverse iteration (see <ref> [3, 2] </ref>, LAPACK routines STEBZ and STEIN).
Reference: [4] <author> M. W. Berry, </author> <title> Large-scale sparse singular value computations, </title> <journal> Int. J. Supercomputer Appl., </journal> <volume> 6 (1992), no. 1, </volume> <pages> 1-21. </pages>
Reference-contexts: Parallel implementations of both the lanso subroutine (cf. [67]) and the ARPACK package (cf. [43]), which implements the implicitly restarted Lanczos algorithms, are now available. The routines in SVDPACK (cf. <ref> [4] </ref>), which build on an older sequential version of lanso are available from Netlib. This software is freely available, and in Table 1 we have listed the relevant Web addresses from where it may be downloaded. Table 1: Available software for large sparse SVD computations.
Reference: [5] <author> A. Bjorck, </author> <title> A bidiagonalization algorithm for solving large and sparse ill-posed systems of linear equations, </title> <journal> BIT, </journal> <volume> 28 (1988), </volume> <pages> 659-670. </pages>
Reference-contexts: Thus one can simply save the Lanczos vectors along the way and only when the convergence criterion has been fulfilled, does the entire S 2k need to be computed in order to calculate the Ritz vectors. This fact is mentioned in several texts <ref> [5, 55, 51, 53] </ref> and involves a small computational trick, which is crucial to get an efficient implementation: If the QR algorithm is used for computing i , then S 2k a product of the series of orthogonal transformations G 1 ; G 2 ; : : : (plane rotations, or <p> Below we review the fundamental relations used to implement some frequently used methods, including the LSQR algorithm by Paige and Saunders [49, 50] and the hybrid algorithm based on Lanczos bidiagonalization which was discovered independently by Bjorck <ref> [5] </ref> and O'Leary and Simmons [46]. The latter is especially used in connection with regularization of ill-posed systems and for solving systems with many right-hand sides. The LSQR algorithm is usually carried out without reorthogonalization, but the convergence can be slowed down significantly by the loss of orthogonality. <p> In the hybrid algorithm it is necessary to reorthogonalize to maintain stability when solving systems with many right-hand sides or when generalized cross-validation [25] is used for choosing the amount of regularization in connection with the solution of ill-posed problems. Both the simple GCV formula used in, e.g., <ref> [5] </ref>, and the more advanced scheme based on implicit restarts proposed in [8] require that V k and U k+1 are kept orthogonal. <p> attention in (cf. [63, 59, 52, 66, 13]), the questions 5 LANCZOS BIDIAGONALIZATION IN FINITE PRECISION ARITHMETIC 24 discussed in this section in relation to the solution of sparse linear least squares problems with multiple right-hand sides have received little attention so far, maybe except for a few remarks in <ref> [5, 46] </ref>. 5 Lanczos bidiagonalization in finite precision arithmetic Knabe sprach: Ich breche dich, Roslein auf der Heiden! Roslein sprach: Ich steche dich, Da du ewig denkst an mich, Und ich will's nicht leiden. Roslein, Roslein, Roslein rot, Roslein auf der Heiden.
Reference: [6] <author> A. Bjorck, </author> <title> Numerical Methods for Least Squares Problems, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: For further details including the corresponding convergence results for the Ritz vectors see, e.g., [58], [27] and <ref> [6, Section 7.6.4] </ref>. 3.3.2 Connection to Lanczos bidiagonalization After having reviewed the theoretical background for the symmetric Lanczos process and its application in SVD calculations, let us return to discuss its connection with the Lanczos bidiagonalization. <p> of the LBD recurrences and without having to sacrifice numerical stability by resorting to the normal equations. 4.1 Least squares solvers based on LBD We will now describe how the linear least squares problem in Equation (4.1) may be solved using LBD (our presentation is based on section 7.6.2 in <ref> [6] </ref>). <p> This algorithm, which we will refer to as MCG (Multi-CG) in the following, applies the conjugate gradient algorithm to construct a sequence of Krylov subspaces. The same idea can be applied to the CGLS algorithm (e.g. <ref> [6, Section 7.4] </ref>) to obtain a stable Lanczos-Galerkin projection method for solving least squares problems with multiple right-hand sides. The resulting algorithm, which we call MCGLS (or Multi-CGLS) has the advantage that it does not require any reorthogonalization. <p> we mention that in the actual implementation, the reorthogonalization is computed using either iterated classical Gram-Schmidt (CGS) or iterated modified Gram-Schmidt (MGS), since this is guaranteed to reduce the ju T j+1 u i j; i 2 L j+1 and jv T j+1 to order u; see the discussion in <ref> [6, pp. 68-69] </ref> and [17]. In practice even the CGS is very rarely iterated, unless one attempts to compute small singular values of an ill-conditioned matrix, so the cost of this safeguard is negligible.
Reference: [7] <author> A. Bjorck, T. Elfving and Z. Strakos. </author> <title> Stability of conjugate gradient and Lanczos methods for linear least squares problems, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 19 (1998), no. 3, </volume> <pages> 720-736. </pages>
Reference-contexts: We use consistently 2 LANCZOS BIDIAGONALIZATION 7 throughout the paper the variant described by Paige and Saunders [49], which is the appropriate version for solving least squares problems (cf. <ref> [7] </ref>). For this reason, our formulas in the section on SVD calculations differ slightly from the presentation in e.g. [29], however in that context there are no differences in computational efficiency or accuracy between the two forms. <p> The alternative is to resort to solving the normal equations A T A x = A T b ; but that would greatly reduce the accuracy in the solution when the system is ill-posed as demonstrated in <ref> [49, 7] </ref>. <p> T (j) x 0 = x 0 + j j p i (j) (j) end end Notice, that as in the CGLS steps executed for the seed system, the true residuals for the non-seed systems are also recurred directly, and hence no serious loss of information (see the discussion in <ref> [7] </ref>) occurs in the projection phase. Since this is the key to the stability of CGLS we suspect the algorithm to be able to be solve all the systems just as accurately as by applying CGLS to each system independently. This will also be the topic of future investigations.
Reference: [8] <author> A. Bjorck, E. Grimme and P. Van Dooren, </author> <title> An implicit shift bidiagonalization algorithm for ill-posed systems, </title> <journal> BIT, </journal> <volume> 34 (1994), no. 4, </volume> <pages> 510-534. </pages>
Reference-contexts: Both the simple GCV formula used in, e.g., [5], and the more advanced scheme based on implicit restarts proposed in <ref> [8] </ref> require that V k and U k+1 are kept orthogonal. However, the algorithm from a more recent paper by Golub and von Matt [30], computes an approximation to the minimizer of the GCV function using LBD with no reorthogonalization.
Reference: [9] <author> R. Boisvert, R. Pozo, K. Remington, R. Barrett, J. Dongarra, </author> <title> Matrix Market : a web resource for test matrix collections, The Quality of Numerical Software: Assessment and Enhancement, </title> <editor> (R. Boisvert, ed.), </editor> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1997, </year> <pages> pp. 125-137. </pages> <address> URL: http://math.nist.gov/MatrixMarket </address>
Reference-contexts: The matrices together with MATLAB, C or FORTRAN subroutines for input and output, can be downloaded from the web-site called "Matrix Marked" <ref> [9] </ref>. The characteristics of the testmatrices are given in Table 6, and the 10 largest singular values of each matrix are listed in Appendix A.
Reference: [10] <author> C. G. </author> <title> Broyden, Look-ahead block-CG algorithms, in "Algorithms for large scale linear algebraic systems" (Gran Canaria 1996), </title> <editor> eds. G. A. Althaus & E. </editor> <title> Spedicato, </title> <journal> NATO Adv. Sci. Inst. Ser. C Math. Phys. Sci., </journal> <volume> 508, </volume> <pages> 197-215, </pages> <publisher> Kluwer Acad. Publ., </publisher> <address> Dordrecht, </address> <year> 1998. </year>
Reference-contexts: Chan and Wan discuss a similar block-MCG algorithm, and show that it can be superior to the single-seed version in terms of convergence rate when solving linear systems with multiple right-hand sides. However, as the original block-CG algorithm it is liable to so-called breakdowns (see, e.g., <ref> [10] </ref>), and convergence cannot be guaranteed even when the matrix is well-conditioned (something which we have also observed on a few occasions for block-MCGLS).
Reference: [11] <author> D. Calvetti, G. H. Golub, L. Reichel, </author> <title> Estimation of the L-Curve via Lanczos Bidiago-nalization, </title> <type> Tech. Rep. </type> <institution> SCCM-97-12, Stanford University, </institution> <year> 1997. </year>
Reference: [12] <author> D. Calvetti, L. Reichel and D. C. Sorensen, </author> <title> An Implicitly Restarted Lanczos Method for Large Symmetric Eigenvalue Problems, </title> <journal> Elec. Trans. Num. Anal., </journal> <volume> 2 (1994), </volume> <pages> 1-21. </pages>
Reference-contexts: 1 INTRODUCTION 5 length m+n must be used. Also, the number iterations required to compute a given number of singular values is doubled. More recently, methods based on the implicitly restarted Lanczos algorithm have appeared (cf. <ref> [12, 65] </ref>). Various studies, see e.g. [23], indicate that these are robust and efficient tools for computing a few of the largest singular values and vectors. <p> As mentioned in the introduction, we have also included routines from a recent software package for large sparse eigenvalue problems and singular value decomposition called ARPACK in the comparison. The ARPACK subroutines are built on the implicitly restarted Lanczos algorithm described in <ref> [43, 12] </ref>. A subset of ARPACK has been included in MATLAB version 5.1 as the function eigs. A sparse SVD routine called svds that calls eigs to compute the eigenvalues of C in the way described in Section 3, is also available.
Reference: [13] <author> T. Chan and W. L. Wan, </author> <title> Analysis of Projection Methods for solving Linear Systems with Multiple Right-hand Sides, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 18 (1997), no. 6, </volume> <pages> 1698-1721. </pages>
Reference-contexts: These methods include the modified Lanczos process discovered by Parlett in [52] and analyzed by Saad in [59]. One such algorithm, which we have started investigating, is a generalization of the Lanczos-Galerkin projection method described in <ref> [13, 66] </ref>. This algorithm, which we will refer to as MCG (Multi-CG) in the following, applies the conjugate gradient algorithm to construct a sequence of Krylov subspaces. <p> With respect to solving ill-posed problems, it is interesting (but not surprising) that by applying Lemma 3.2 from the paper by Chan and Wan <ref> [13] </ref> to the normal equations, it can 4 SPARSE LEAST SQUARES 23 0 10 20 30 40 50 60 70 80 90 100 10 -8 10 -6 10 -4 10 -2 10 0 10 1 Filter factors for seed system Component number i Filter factors 0 10 20 30 40 50 <p> This is also what one would expect from the discussion above. It is our general impression that while methods for solving linear systems with multiple right-hand sides have received some attention in (cf. <ref> [63, 59, 52, 66, 13] </ref>), the questions 5 LANCZOS BIDIAGONALIZATION IN FINITE PRECISION ARITHMETIC 24 discussed in this section in relation to the solution of sparse linear least squares problems with multiple right-hand sides have received little attention so far, maybe except for a few remarks in [5, 46]. 5 Lanczos
Reference: [14] <author> J. Christensen-Dalsgaard, J. Schou and M. J. Thompson, </author> <title> A comparison of methods for inverting helioseismic data, Mon. Not. </title> <editor> R. Astr. </editor> <publisher> Soc., </publisher> <month> 242 </month> <year> (1990), </year> <pages> 353-369. REFERENCES 52 </pages>
Reference-contexts: illustrate the properties of the resulting algorithm, by applying a MATLAB implementation of the algorithm to a number of test problems from the Harwell-Boeing collection, in addition to a discrete ill-posed problem from inverse helioseismology (see [62] for a general introduction), which was also used as a test problem in <ref> [14, 36, 42] </ref>.
Reference: [15] <author> J. K. Cullum and R. A. Willoughby, </author> <title> Lanczos Algorithms for Large Symmetric Eigenvalue Computations. Vol. I Theory, </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1985. </year>
Reference-contexts: sparsity in A, since this is quickly destroyed by the transformations that are applied to the matrix. 3.3.1 Approximate SVD using Lanczos on matrix C When A is large and sparse or structured a more efficient method for computing the SVD is to use the symmetric Lanczos process (see e.g. <ref> [15, 51] </ref>) applied to one of the equivalent symmetric systems, since then the matrix is only accesses via matrix-vector products with A and A T . <p> In any case, the error bounds can be computed in O (k 2 ) operations, which means that the convergence of the Ritz values can be monitored during the iteration without too much overhead. For further discussions see <ref> [53, 15] </ref>. We have not yet discussed how the Ritz values converge. This is a rather complicated matter, which depends on the properties of the spectrum of A. Kaniel [39] and Paige [48] were the first to give bounds on the rate of convergence of the Ritz values. <p> Notice in both panels how the overall pattern shows the well separated extreme Ritz values converging most rapidly as predicted by Theorem 4. We refer to <ref> [15, 55, 51, 64] </ref>, where effects of finite precision arithmetic are discussed in great detail. 5 LANCZOS BIDIAGONALIZATION IN FINITE PRECISION ARITHMETIC 26 5.1 Lanczos algorithms with no reorthogonalization Du Doppelganger, du bleicher Geselle! Was affst du nach mein Liebesleid, das mich gequalt auf dieser Stelle so manche Nacht, in alter <p> One approach which has been advocated by Paige, Cullum and Willoughby, among others, is to apply the simple Lanczos process as it is, and subsequently use some criterion to weed out the ghost and doppelganger eigenvalues. The criterion used in <ref> [15] </ref> for the symmetric Lanczos process, is based on the curious fact that the spurious eigenvalues of T 2k+1 are nearly eigenvalues of ^ T , which is the matrix T 2k+1 with the first row and the first column removed.
Reference: [16] <author> J. K. Cullum, R. A. Willoughby and M. </author> <title> Lake, A Lanczos algorithm for computing singular values and vectors of large matrices, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 4 (1983), no. 2, </volume> <pages> 197-215. </pages>
Reference: [17] <author> J. W. Daniel, W. B. Gragg, L. Kaufman and G. W. Stewart, </author> <title> Reorthogonalization and stable algorithms for updating the Gram-Schmidt QR factorization, </title> <journal> Math. Comp., </journal> <volume> 30 (1976), no. 136, </volume> <pages> 772-795. </pages>
Reference-contexts: the actual implementation, the reorthogonalization is computed using either iterated classical Gram-Schmidt (CGS) or iterated modified Gram-Schmidt (MGS), since this is guaranteed to reduce the ju T j+1 u i j; i 2 L j+1 and jv T j+1 to order u; see the discussion in [6, pp. 68-69] and <ref> [17] </ref>. In practice even the CGS is very rarely iterated, unless one attempts to compute small singular values of an ill-conditioned matrix, so the cost of this safeguard is negligible.
Reference: [18] <author> J. W. Demmel, M. T. Heath, H. A. van der Vorst, </author> <title> Parallel Numerical Linear Algebra, </title> <booktitle> Acta Numerica 1993, </booktitle> <pages> 111-197. </pages>
Reference-contexts: This saves half the work and half the storage needed for the eigenvectors of T 2k+1 , and also makes the eigenvalue calculations easy to parallelize (see e.g. <ref> [18] </ref>).
Reference: [19] <author> J. W. Demmel and W. Kahan, </author> <title> Accurate singular values of bidiagonal matrices, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 11 (1990), </volume> <pages> 712-719. </pages>
Reference-contexts: Instead, the difference is in the algorithms that are used to calculate the eigenvalues of T 2k and the singular values of B k : The bidiagonal SVD algorithm (see <ref> [19] </ref>) can compute even the smallest singular values with guaranteed high relative accuracy; the same is not true for the QL/QR algorithm used for computing the eigenvalues of a symmetric tridiagonal.
Reference: [20] <author> J. J. Dongarra, </author> <title> Improving the accuracy of computed singular values, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 4 (1983), no. 4, </volume> <pages> 712-719. </pages>
Reference: [21] <author> J. J. Dongarra, J. R. Bunch, C. B. Moler and G. W. Stewart, </author> <title> LINPACK Users' Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: After 100 iterations the algorithm terminates after having computed a full SVD of the matrix. As shown in the right panel of Figure 13 the computed singular values are comparable in accuracy to those computed by the LINPACK (cf. <ref> [21] </ref>) SVD algorithm used by MATLAB. Although a sparse SVD algorithm is typically used to compute only a few of the largest (or smallest) singular values, we feel that a robust implementation should be able to handle problems, where singular values of highly differing magnitudes must be computed accurately.
Reference: [22] <author> L. Elden, </author> <title> Algorithms for the regularization of ill-conditioned least-squares problems, </title> <journal> BIT, </journal> <volume> 17 (1977), </volume> <pages> 134-145. </pages>
Reference: [23] <author> L. Elden and E. Sjostrom, </author> <title> Fast computation of the principal singular values of Toeplitz matrices arising in exponential data fitting, </title> <booktitle> Signal Processing, 50 (1996), </booktitle> <pages> 151-164. </pages>
Reference-contexts: 1 INTRODUCTION 5 length m+n must be used. Also, the number iterations required to compute a given number of singular values is doubled. More recently, methods based on the implicitly restarted Lanczos algorithm have appeared (cf. [12, 65]). Various studies, see e.g. <ref> [23] </ref>, indicate that these are robust and efficient tools for computing a few of the largest singular values and vectors. <p> The upper and lower dotted horizontal line marks p elements per row and column. Other examples where the bounds could be reduced include applications where the matrix-vector product is computed using the fast Fourier transform <ref> [23, 34] </ref> (when A is Toeplitz or circulant) or involve Kronecker products [41, 24]. Another point of great importance for the stability and efficiency of the algorithm is the proper estimation of kAk 2 , which enter into the bounds on the round-off terms. <p> At iteration 71 semiorthogonality can no longer be maintained and the algorithm switches to full reorthogonalization. Linestyles as in Figure 8. In <ref> [23] </ref> Elden and Sjostrom analyze a class of rank deficient Toeplitz matrices that arise in signal analysis applications. They use the lanso subroutine found in SVDPACK, and compare the performance and accuracy with a number of other iterative methods for sparse SVD calculations including the ARPACK codes ssaupd.f and sseupd.f.
Reference: [24] <author> D. W. Fausett, C. T. Fulton, </author> <title> Large least squares problems involving Kronecker products, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 15 (1994), </volume> <pages> 219-227. </pages>
Reference-contexts: The upper and lower dotted horizontal line marks p elements per row and column. Other examples where the bounds could be reduced include applications where the matrix-vector product is computed using the fast Fourier transform [23, 34] (when A is Toeplitz or circulant) or involve Kronecker products <ref> [41, 24] </ref>. Another point of great importance for the stability and efficiency of the algorithm is the proper estimation of kAk 2 , which enter into the bounds on the round-off terms.
Reference: [25] <author> G. H. Golub, M. Heath and G. Wahba, </author> <title> Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter, </title> <journal> Technometrics, </journal> <volume> 21 (1979), </volume> <pages> 215-223. </pages>
Reference-contexts: The LSQR algorithm is usually carried out without reorthogonalization, but the convergence can be slowed down significantly by the loss of orthogonality. In the hybrid algorithm it is necessary to reorthogonalize to maintain stability when solving systems with many right-hand sides or when generalized cross-validation <ref> [25] </ref> is used for choosing the amount of regularization in connection with the solution of ill-posed problems. Both the simple GCV formula used in, e.g., [5], and the more advanced scheme based on implicit restarts proposed in [8] require that V k and U k+1 are kept orthogonal.
Reference: [26] <author> G. H. Golub and W. Kahan, </author> <title> Calculating the singular values and pseudo-inverse of a matrix, </title> <journal> J. Soc. Indust. Appl. Math. Ser. B Numer. Anal., </journal> <volume> 2 (1965), </volume> <pages> 205-224. </pages>
Reference-contexts: This forms the basis of any SVD algorithm. As an example, the standard algorithm, which in its original form is due to Golub and Kahan <ref> [26] </ref> and used in e.g. LAPACK, computes the SVD by implicitly applying the QR algorithm for the symmetric eigenvalue problem to A T A. 3.2 Fundamental error analysis for SVD calculations The fact that A T A is formed only implicitly is crucial to the numerical stability of the algorithm.
Reference: [27] <author> G. H. Golub, F. T. Luk and M. L. Overton, </author> <title> A block Lanczos method for computing the singular values and corresponding vectors of a matrix, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 7 (1981), </volume> <pages> 149-169. </pages>
Reference-contexts: A similar presentation for the block Lanczos bidiagonalization is given in <ref> [27] </ref>. <p> For further details including the corresponding convergence results for the Ritz vectors see, e.g., [58], <ref> [27] </ref> and [6, Section 7.6.4]. 3.3.2 Connection to Lanczos bidiagonalization After having reviewed the theoretical background for the symmetric Lanczos process and its application in SVD calculations, let us return to discuss its connection with the Lanczos bidiagonalization.
Reference: [28] <author> G. H. Golub, R. Underwood and J. Wilkinson, </author> <title> The Lanczos Algorithm for the Ax = Bx Problem, </title> <type> Report STAN-CS-72-270, </type> <institution> Department of Computer Science, Stanford University, Stanford, California, </institution> <year> 1972. </year>
Reference-contexts: This will cause the performance of FRO to be poor on machines with a memory hierarchy. An alternative implementation of reorthogonalization was developed by Golub, Under-wood and Wilkinson <ref> [28] </ref>, who suggested that U k+1 and V k be represented as a product of Householder reflectors. If the reflectors are accumulated as so-called block reflectors (see [60]), this is probably the most efficient way of implementing full reorthogonalization.
Reference: [29] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, 2. </title> <editor> Ed., </editor> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: We use consistently 2 LANCZOS BIDIAGONALIZATION 7 throughout the paper the variant described by Paige and Saunders [49], which is the appropriate version for solving least squares problems (cf. [7]). For this reason, our formulas in the section on SVD calculations differ slightly from the presentation in e.g. <ref> [29] </ref>, however in that context there are no differences in computational efficiency or accuracy between the two forms. <p> Now, perturbation theory for the symmetric eigenvalue problem (see e.g. <ref> [29, Sections 8.1, 8.3] </ref>) can tell us how much the eigenvalues of fl (A T A) differ from those of A T A. <p> The Ritz values are found by computing the Schur decomposition of T 2k : S T 2k T 2k S 2k = diag ( 1 ; : : : ; 2k ) ; (3.18) This can be done by using, e.g., the symmetric QR algorithm (see <ref> [29, 2] </ref>, LAPACK routine STEQR), or bisection based on Sturm sequences followed by inverse iteration (see [3, 2], LAPACK routines STEBZ and STEIN). <p> i y i = fi k+1 u k+1 2k S 2k e i : The theorem follows by taking norms and noting that ku k+1 k 2 = 1. 2 Theorem 2 gives the following error bounds for the eigenvalue approximations min j i j ki ; (3.20) (see, e.g. <ref> [29, Chapter 8] </ref>). 3 THE LANCZOS ALGORITHM AND SPARSE SVD CALCULATIONS 14 The Ritz vectors y i ; i = 1; : : : ; 2k are approximations to the eigenvectors of C.
Reference: [30] <author> G. H. Golub, U. von Matt, </author> <title> Generalized Cross-Validation for Large Scale Problems, </title> <journal> Journal of Computational and Graphical Statistics, </journal> <volume> 6 (1997), </volume> <pages> 1-34. REFERENCES 53 </pages>
Reference-contexts: Both the simple GCV formula used in, e.g., [5], and the more advanced scheme based on implicit restarts proposed in [8] require that V k and U k+1 are kept orthogonal. However, the algorithm from a more recent paper by Golub and von Matt <ref> [30] </ref>, computes an approximation to the minimizer of the GCV function using LBD with no reorthogonalization.
Reference: [31] <author> G. H. Golub, U. von Matt, </author> <title> Tikhonov Regularization for Large Scale Problems, </title> <booktitle> in Workshop on Scientific Computing, </booktitle> <editor> eds. G. H. Golub, S. H. Lui, F. Luk. and R. Plemmons, </editor> <publisher> Springer, </publisher> <address> New York, </address> <year> 1997. </year>
Reference: [32] <author> A. Greenbaum and J. J. Dongarra, </author> <title> Experiments with QL/QR methods for the symmetric tridiagonal eigenproblem, </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-89-92, </type> <institution> University of Tennessee, Knoxville, </institution> <year> 1989. </year> <note> (LAPACK Working Note 17). </note>
Reference: [33] <author> M. Hanke and P. C. Hansen, </author> <title> Regularization methods for large-scale problems, </title> <institution> Surv. Math. Ind., </institution> <month> 3 </month> <year> (1993), </year> <pages> 253-315. </pages>
Reference: [34] <author> M. Hanke, J. Nagy and R. Plemmons, </author> <title> Preconditioned iterative regularization for ill-posed problems, Numerical linear algebra (Kent, </title> <address> OH, </address> <year> 1992), </year> <pages> 141-163, </pages> <publisher> de Gruyter, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: The upper and lower dotted horizontal line marks p elements per row and column. Other examples where the bounds could be reduced include applications where the matrix-vector product is computed using the fast Fourier transform <ref> [23, 34] </ref> (when A is Toeplitz or circulant) or involve Kronecker products [41, 24]. Another point of great importance for the stability and efficiency of the algorithm is the proper estimation of kAk 2 , which enter into the bounds on the round-off terms.
Reference: [35] <author> P. C. Hansen, </author> <title> The discrete Picard condition for discrete ill-posed problems, </title> <journal> BIT, </journal> <volume> 30 (1990), </volume> <pages> 658-672. </pages>
Reference-contexts: This will for instance often be the case for discrete ill-posed problems when the right-hand sides satisfy the discrete Picard criterion (see <ref> [35] </ref>). In this case the vectors b (i) will be dominated by components lying in a subspace spanned by the singular vectors corresponding to the largest singular values.
Reference: [36] <author> P. C. Hansen, </author> <title> Rank-Deficient and Discrete Ill-Posed Problems: Numerical Aspects of Linear Inversion, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1998. </year>
Reference-contexts: illustrate the properties of the resulting algorithm, by applying a MATLAB implementation of the algorithm to a number of test problems from the Harwell-Boeing collection, in addition to a discrete ill-posed problem from inverse helioseismology (see [62] for a general introduction), which was also used as a test problem in <ref> [14, 36, 42] </ref>. <p> However, the rate of convergence can be slowed down quite significantly due to the unavoidable loss of orthogonality, see <ref> [36, Section 6.4] </ref> for a detailed explanation. <p> The figure illustrates the so-called filter factors f i for the seed system (left panel) and the first non-seed system, and they show how the different SVD-components contribute to the solution: x k = i=1 u T oe i see <ref> [36] </ref> for a further discussions of filter factors and the regularizing effect of the CGLS algorithm.
Reference: [37] <author> N. J. Higham, </author> <title> Accuracy and Stability of Numerical Algorithms, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: Furthermore we use the symbol u to denote the unit of machine round-off, which in IEEE double precision is 2 53 1:11 10 16 , and the symbol fl n = nu=(1 nu) introduced by Higham in <ref> [37] </ref>. <p> To state this more quantitatively, we regard the forward error bound for matrix multiplication (see <ref> [37, Section 3.5] </ref>): fl (A T A) = A T A + E ; kEk 2 fl m kA T k 2 kAk 2 fl m oe 2 where oe 1 is the largest singular value of A and fl m = mu=(1mu) 1:01mu when mu 0:01 (for IEEE double precision
Reference: [38] <author> W. Kahan, </author> <title> The Improbability of Probabilistic Error Analyses for Numerical Computations, </title> <booktitle> Prepared for the UCB Statistics Colloquium, </booktitle> <month> February </month> <year> 1996, </year> <note> URL: http://http.cs.berkeley.edu/~wkahan/improber.ps. </note>
Reference-contexts: Kahan <ref> [38] </ref>, we shall nonetheless proceed to use this estimate. 6 A PARTIAL REORTHOGONALIZATION ALGORITHM FOR LBD 32 0 5 10 15 20 10 0 Lanczos steps j Level of orthogonality, U max i m ij 10 0 Level of orthogonality, V max i n ij 10 -10 Lanczos steps j Level
Reference: [39] <author> S. Kaniel, </author> <title> Estimates for Some Computational Techniques in Linear Algebra, </title> <journal> Math. Comp., </journal> <volume> 20 (1966), </volume> <pages> 369-78. </pages>
Reference-contexts: For further discussions see [53, 15]. We have not yet discussed how the Ritz values converge. This is a rather complicated matter, which depends on the properties of the spectrum of A. Kaniel <ref> [39] </ref> and Paige [48] were the first to give bounds on the rate of convergence of the Ritz values.
Reference: [40] <author> C. </author> <title> Lanczos, An Iteration methods Method for the Solution of the Eigenvalue Problem of Linear Differential and Integral Operators, </title> <institution> J. of Res. Nat. Bur. Stand. </institution> <month> 45 </month> <year> (1950), </year> <pages> 255-282. </pages>
Reference-contexts: reorthogonalization we shall use the following abbreviations LBD : Lanczos bidiagonalization FRO : Symmetric Lanczos with full reorthogonalization BFRO : Lanczos bidiagonalization with full reorthogonalization PRO : Symmetric Lanczos with Partial reorthogonalization BPRO : Lanczos bidiagonalization with partial reorthogonalization, where by "Symmetric Lanczos" we mean the original Lanczos algorithm from <ref> [40] </ref> for reducing a symmetric matrix to tridiagonal form. The Lanczos bidiagonalization algorithm is defined in the following section. 2 Lanczos bidiagonalization We begin this section by stating the fundamental recurrences that define the Lanczos bidiag-onalization.
Reference: [41] <author> R. M. Larsen, </author> <title> Iterative algorithms for two-dimensional helioseismic inversion, </title> <booktitle> in Proc. of the Interdisciplinary Inversion Workshop 5 (Ed. B. </booktitle> <institution> H Jacobsen), Dept. Earth Sciences, Aarhus University (1997), </institution> <month> 123-138. </month>
Reference-contexts: Without reorthogonalization, the behavior is a little erratic and it remains to be seen if the regularized solutions for the non-seed systems computed thus are acceptable in general. We have made a few experiments with systems arising in large-scale 2-dimensional helioseismic rotational inversions (see e.g. <ref> [61, 41] </ref>), which seem to indicate that the computed solutions are quite acceptable, but many extra iterations are required for systems where the right-hand side looks very different from b (1) when no reorthogonalization is used. This is also what one would expect from the discussion above. <p> The number of extra iterations required compared to executing the Lanczos process in exact arithmetic can be very large. Up to six times the original number has been reported in e.g. [54]. This is in agreement with our own experiences from solving highly ill-conditioned problems in inverse helioseismology <ref> [41] </ref> using algorithms based on Lanczos bidiagonalization. Here many iterations are also wasted generating multiple copies of a few large isolated singular values, and a reduction in the number of iterations between a factor of 5 and 10 is typically observed when using reorthogonalization. <p> The upper and lower dotted horizontal line marks p elements per row and column. Other examples where the bounds could be reduced include applications where the matrix-vector product is computed using the fast Fourier transform [23, 34] (when A is Toeplitz or circulant) or involve Kronecker products <ref> [41, 24] </ref>. Another point of great importance for the stability and efficiency of the algorithm is the proper estimation of kAk 2 , which enter into the bounds on the round-off terms.
Reference: [42] <author> R. M. Larsen and P. C. Hansen, </author> <title> Efficient Implementations of the SOLA Mollifier Method, </title> <journal> Astron. Astrophys. </journal> <volume> Suppl., 121 (1997), </volume> <pages> 587-598. </pages>
Reference-contexts: illustrate the properties of the resulting algorithm, by applying a MATLAB implementation of the algorithm to a number of test problems from the Harwell-Boeing collection, in addition to a discrete ill-posed problem from inverse helioseismology (see [62] for a general introduction), which was also used as a test problem in <ref> [14, 36, 42] </ref>. <p> is when LBD is used to solve sparse linear least squares problems with multiple right-hand sides min kA x (i) b (i) k 2 ; i = 1; : : : ; N ; (4.4) which arise, e.g., when implementing the so-called SOLA mollifier method used in helioseismic inversion (see <ref> [56, 42] </ref>). A straightforward solution would be to apply the LSQR algorithm to each system independently, but often the following procedure (which was analyzed by Saad [59] in the context of (square) linear systems) is more efficient: 1. <p> This is illustrated in Figure 6 where we have executed the MCGLS algorithm shown above for 50 iteration on a least-squares problem with multiple right-hand sides that arises in the so-called SOLA method used in helioseismic inversion (see <ref> [42] </ref>); the coefficient matrix is in fact the testmatrix HELIO212b used elsewhere in this report.
Reference: [43] <author> R. B. Lehoucq, D. C Sorensen and C. Yang, </author> <title> ARPACK Users's Guide: Solution of Large Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods, </title> <publisher> SIAM, </publisher> <address> Philadel-phia, </address> <year> 1998, </year> <note> URL: http://www.caam.rice.edu/software/ARPACK. </note>
Reference-contexts: Parallel implementations of both the lanso subroutine (cf. [67]) and the ARPACK package (cf. <ref> [43] </ref>), which implements the implicitly restarted Lanczos algorithms, are now available. The routines in SVDPACK (cf. [4]), which build on an older sequential version of lanso are available from Netlib. <p> For implementation on parallel computers the iterated CGS, which can be implemented as two dense matrix-vector multiplications, often performs 6 A PARTIAL REORTHOGONALIZATION ALGORITHM FOR LBD 38 better than the inherently sequential MGS algorithm. As an example, iterated CGS is used in ARPACK for this reason (cf. <ref> [43, p. 50] </ref>). 6.3 Computing small singular values Before we go on to the evaluate the performance of the algorithm developed in this section, we wish to discuss a situation that requires special attention when a semiorthogonalization scheme such as BPRO is implemented. <p> As mentioned in the introduction, we have also included routines from a recent software package for large sparse eigenvalue problems and singular value decomposition called ARPACK in the comparison. The ARPACK subroutines are built on the implicitly restarted Lanczos algorithm described in <ref> [43, 12] </ref>. A subset of ARPACK has been included in MATLAB version 5.1 as the function eigs. A sparse SVD routine called svds that calls eigs to compute the eigenvalues of C in the way described in Section 3, is also available.
Reference: [44] <author> The Mathworks, </author> <title> Private communication, </title> <year> 1998. </year>
Reference-contexts: It should also be mentioned that we used a new version of eigs in which a number of bugs in the interface routine have been removed <ref> [44] </ref> no significant changes, which might have affected the results in Section 7.3, were made to the main computational routines. <p> Our experiments have shown that especially for problems where the loss of orthogonality is slow, in these routines will be corrected in the next version of MATLAB <ref> [44] </ref>. 8 CONCLUSION 50 the simple estimates used in the current implementation can be up to 100 times above the true values, and up to 40% of the work done in the reorthogonalization is unnecessary.
Reference: [45] <author> D. P. O'Leary, </author> <title> The block conjugate gradient algorithm and related methods, </title> <journal> Linear Algebra Appl., </journal> <volume> 29 (1980), </volume> <pages> 292-322. REFERENCES 54 </pages>
Reference-contexts: This will also be the topic of future investigations. The MCGLS algorithm also has a block (multi-seed) generalization, based on the block conjugate gradient algorithm by O'Leary <ref> [45] </ref> applied to the normal equations. Chan and Wan discuss a similar block-MCG algorithm, and show that it can be superior to the single-seed version in terms of convergence rate when solving linear systems with multiple right-hand sides.
Reference: [46] <author> D. P. O'Leary and J. A. Simmons, </author> <title> A bidiagonalization-regularization procedure for large scale discretizations of ill-posed problems, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 2 (1981), </volume> <pages> 474-489. </pages>
Reference-contexts: Below we review the fundamental relations used to implement some frequently used methods, including the LSQR algorithm by Paige and Saunders [49, 50] and the hybrid algorithm based on Lanczos bidiagonalization which was discovered independently by Bjorck [5] and O'Leary and Simmons <ref> [46] </ref>. The latter is especially used in connection with regularization of ill-posed systems and for solving systems with many right-hand sides. The LSQR algorithm is usually carried out without reorthogonalization, but the convergence can be slowed down significantly by the loss of orthogonality. <p> attention in (cf. [63, 59, 52, 66, 13]), the questions 5 LANCZOS BIDIAGONALIZATION IN FINITE PRECISION ARITHMETIC 24 discussed in this section in relation to the solution of sparse linear least squares problems with multiple right-hand sides have received little attention so far, maybe except for a few remarks in <ref> [5, 46] </ref>. 5 Lanczos bidiagonalization in finite precision arithmetic Knabe sprach: Ich breche dich, Roslein auf der Heiden! Roslein sprach: Ich steche dich, Da du ewig denkst an mich, Und ich will's nicht leiden. Roslein, Roslein, Roslein rot, Roslein auf der Heiden.
Reference: [47] <author> G. Orwell, </author> <year> 1984, </year> <title> Copyright c fl 1949 Harcourt Brace Jovanovich, </title> <publisher> Inc. </publisher>
Reference-contexts: In the case of a sparse matrix, such information could be the maximum number of non-zero 2 Keeping in mind, yet ignoring (which can be achieved using the technique commonly known as Doublethink, see the appendix of <ref> [47] </ref>) the fundamental improbability (or doubleplusungodness) of such probabilistic error analyzes, as pointed out in the very thought-provoking notes by W.
Reference: [48] <author> C. C. Paige, </author> <title> The Computation of Eigenvalues and Eigenvectors of Very Large Sparse Matrices, </title> <type> Ph. D. thesis, </type> <institution> University of London, </institution> <address> United Kingdom, </address> <year> 1971. </year>
Reference-contexts: In this situation we have that joe i oe i j p (m; n) u oe i see e.g. [2, Section 4.7, 4.9]. It follows from the error analysis of Paige <ref> [48] </ref> that the symmetric Lanczos algorithm applied to C (and therefore also the LBD, as we shall see below) satisfy bounds on this form. <p> This saves half the work and half the storage needed for the eigenvectors of T 2k+1 , and also makes the eigenvalue calculations easy to parallelize (see e.g. [18]). The accuracy of the computed Ritz values may be estimated using the following result, which is originally due to Paige <ref> [48] </ref>: Theorem 2 Suppose 2k steps of the symmetric Lanczos process on C have been performed and that the Schur decomposition of the tridiagonal matrix T 2k is given by (3.18). <p> If bisection is used to compute the Ritz values then the last row of S 2k may be computed after the desired Ritz values have been found either by inverse iteration, or by using the following formula from Paige <ref> [48] </ref> s 2 2k ( i ) ; where O 2k () is the characteristic polynomial of T 2k (Beware: The latter approach is unstable if O and O 0 are evaluated using the simple three-term recurrences; a stable implementation is described by Parlett and Nour-Omid in [53, p. 206]). <p> For further discussions see [53, 15]. We have not yet discussed how the Ritz values converge. This is a rather complicated matter, which depends on the properties of the spectrum of A. Kaniel [39] and Paige <ref> [48] </ref> were the first to give bounds on the rate of convergence of the Ritz values. <p> In contrast, the orthogonality among the left and right Lanczos vectors is gradually lost such that equations (2.2) and (2.3) no longer hold. The loss of orthogonality goes hand in hand with the convergence of Ritz pairs in a very systematic way: A now famous result by Paige <ref> [48] </ref> shows that after 2k steps, say, of the symmetric Lanczos process, the newly generated Lanczos vector q 2k+1 satisfies the following relation (using the notation of Theorem 2): jq T ukCk 2 jfi k+1 j js 2k;i j If we recall Equation (3.19) kCy i i y i k 2 <p> The central idea in partial reorthogonalization is that the level of orthogonality among the Lanczos vectors satisfies a recurrence relation that can be derived from the recurrence used to generate the vectors themselves. This was already shown by Paige in his pioneering thesis <ref> [48] </ref>.
Reference: [49] <author> C. C. Paige and M. A. Saunders, </author> <title> LSQR: an algorithm for sparse linear equations and sparse least squares, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 8 (1982), </volume> <pages> 43-71. </pages>
Reference-contexts: In the paragraphs below we describe in further detail how this iterative process may be used in SVD calculations and for solving least squares problems. We use consistently 2 LANCZOS BIDIAGONALIZATION 7 throughout the paper the variant described by Paige and Saunders <ref> [49] </ref>, which is the appropriate version for solving least squares problems (cf. [7]). For this reason, our formulas in the section on SVD calculations differ slightly from the presentation in e.g. [29], however in that context there are no differences in computational efficiency or accuracy between the two forms. <p> Below we review the fundamental relations used to implement some frequently used methods, including the LSQR algorithm by Paige and Saunders <ref> [49, 50] </ref> and the hybrid algorithm based on Lanczos bidiagonalization which was discovered independently by Bjorck [5] and O'Leary and Simmons [46]. The latter is especially used in connection with regularization of ill-posed systems and for solving systems with many right-hand sides. <p> The alternative is to resort to solving the normal equations A T A x = A T b ; but that would greatly reduce the accuracy in the solution when the system is ill-posed as demonstrated in <ref> [49, 7] </ref>. <p> Using the orthogonality of U k+1 it follows that kA x k bk 2 is minimized in K k by choosing y k to be the solution to the least squares problem min kB k y fi 1 e 1 k 2 : The LSQR algorithm by Paige and Saunders <ref> [49] </ref> is based on the relations described above, and is constructed in such a way that x k is updated recursively from x k1 without ever forming y k .
Reference: [50] <author> C. C. Paige and M. A. Saunders, </author> <title> Algorithm 583. LSQR: sparse linear equations and least squares problems, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 8 (1982), </volume> <pages> 195-209. </pages>
Reference-contexts: Below we review the fundamental relations used to implement some frequently used methods, including the LSQR algorithm by Paige and Saunders <ref> [49, 50] </ref> and the hybrid algorithm based on Lanczos bidiagonalization which was discovered independently by Bjorck [5] and O'Leary and Simmons [46]. The latter is especially used in connection with regularization of ill-posed systems and for solving systems with many right-hand sides.
Reference: [51] <author> B. N. Parlett, </author> <title> The Symmetric Eigenvalue Problem, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1980. </year>
Reference-contexts: sparsity in A, since this is quickly destroyed by the transformations that are applied to the matrix. 3.3.1 Approximate SVD using Lanczos on matrix C When A is large and sparse or structured a more efficient method for computing the SVD is to use the symmetric Lanczos process (see e.g. <ref> [15, 51] </ref>) applied to one of the equivalent symmetric systems, since then the matrix is only accesses via matrix-vector products with A and A T . <p> Define the gap 6= ^ then sin 6 (y i ; ^v) fl i ki : (3.22) Proof. See <ref> [51, Chapter 11] </ref>. 2 The theorem shows that the convergence of a given Ritz vector is essentially like that of the corresponding Ritz value, except when the latter is an approximation to an eigenvalue which is in a cluster (this has nothing to do with the Lanczos algorithm being used, but <p> We also mention that the gap-structure can be used to sharpen the error estimates in Theorem 2 to min 2 fl i ! see, e.g., <ref> [51, Section 13.2] </ref> and [53]. In our implementation of the BPRO algorithm described below, we use these improved error estimates to reduce the necessary number of steps in the Lanczos bidiagonalization. <p> Thus one can simply save the Lanczos vectors along the way and only when the convergence criterion has been fulfilled, does the entire S 2k need to be computed in order to calculate the Ritz vectors. This fact is mentioned in several texts <ref> [5, 55, 51, 53] </ref> and involves a small computational trick, which is crucial to get an efficient implementation: If the QR algorithm is used for computing i , then S 2k a product of the series of orthogonal transformations G 1 ; G 2 ; : : : (plane rotations, or <p> Notice in both panels how the overall pattern shows the well separated extreme Ritz values converging most rapidly as predicted by Theorem 4. We refer to <ref> [15, 55, 51, 64] </ref>, where effects of finite precision arithmetic are discussed in great detail. 5 LANCZOS BIDIAGONALIZATION IN FINITE PRECISION ARITHMETIC 26 5.1 Lanczos algorithms with no reorthogonalization Du Doppelganger, du bleicher Geselle! Was affst du nach mein Liebesleid, das mich gequalt auf dieser Stelle so manche Nacht, in alter <p> The disadvantage is that many iterations are wasted on simply generating multiple copies of the large Ritz values, or as Parlett puts it in <ref> [51, pp. 262] </ref>: "Its fault is in not quitting while it is ahead because it continues to compute many redundant copies of each Ritz pair". The number of extra iterations required compared to executing the Lanczos process in exact arithmetic can be very large. <p> All this work is not necessary. It was found in <ref> [55, 51, 64] </ref> that it is sufficient to maintain semiorthogonality, that is to keep u j and v j below p u=k, to obtain accurate approximations to the singular values and avoid ghosts and doppelgangers from appearing.
Reference: [52] <author> B. N. Parlett, </author> <title> A new look at the Lanczos algorithm for solving symmetric systems of linear equations, </title> <journal> Linear Algebra Appl., </journal> <volume> 29 (1980), </volume> <pages> 323-346. </pages>
Reference-contexts: In a future paper we intend to investigate the possibility of generalizing a number of the algorithms developed for solving symmetric systems of equations with multiple right-hand sides to the least squares case. These methods include the modified Lanczos process discovered by Parlett in <ref> [52] </ref> and analyzed by Saad in [59]. One such algorithm, which we have started investigating, is a generalization of the Lanczos-Galerkin projection method described in [13, 66]. <p> This is also what one would expect from the discussion above. It is our general impression that while methods for solving linear systems with multiple right-hand sides have received some attention in (cf. <ref> [63, 59, 52, 66, 13] </ref>), the questions 5 LANCZOS BIDIAGONALIZATION IN FINITE PRECISION ARITHMETIC 24 discussed in this section in relation to the solution of sparse linear least squares problems with multiple right-hand sides have received little attention so far, maybe except for a few remarks in [5, 46]. 5 Lanczos
Reference: [53] <author> B. N. Parlett and B. Nour-Omid, </author> <title> The use of a refined error bound when updating the eigenvalues of tridiagonals, </title> <journal> Lin. Alg. Appl., </journal> <volume> 68 (1985), </volume> <pages> 179-219. </pages>
Reference-contexts: We also mention that the gap-structure can be used to sharpen the error estimates in Theorem 2 to min 2 fl i ! see, e.g., [51, Section 13.2] and <ref> [53] </ref>. In our implementation of the BPRO algorithm described below, we use these improved error estimates to reduce the necessary number of steps in the Lanczos bidiagonalization. <p> Thus one can simply save the Lanczos vectors along the way and only when the convergence criterion has been fulfilled, does the entire S 2k need to be computed in order to calculate the Ritz vectors. This fact is mentioned in several texts <ref> [5, 55, 51, 53] </ref> and involves a small computational trick, which is crucial to get an efficient implementation: If the QR algorithm is used for computing i , then S 2k a product of the series of orthogonal transformations G 1 ; G 2 ; : : : (plane rotations, or <p> formula from Paige [48] s 2 2k ( i ) ; where O 2k () is the characteristic polynomial of T 2k (Beware: The latter approach is unstable if O and O 0 are evaluated using the simple three-term recurrences; a stable implementation is described by Parlett and Nour-Omid in <ref> [53, p. 206] </ref>). If the Lanczos bidiagonalization is used, error estimates may be generated as a byproduct when calculating the singular values of B k (see below), e.g. using the bidiagonal SVD routine BDSQR from LAPACK. <p> In any case, the error bounds can be computed in O (k 2 ) operations, which means that the convergence of the Ritz values can be monitored during the iteration without too much overhead. For further discussions see <ref> [53, 15] </ref>. We have not yet discussed how the Ritz values converge. This is a rather complicated matter, which depends on the properties of the spectrum of A. Kaniel [39] and Paige [48] were the first to give bounds on the rate of convergence of the Ritz values.
Reference: [54] <author> B. N. Parlett, J. K. Reid, </author> <title> Tracking the progress of the Lanczos algorithm for large symmetric eigenproblems, </title> <journal> IMA J. Numer. Anal., </journal> <volume> 1 (1981), no. 2, </volume> <pages> 135-155. </pages>
Reference-contexts: The number of extra iterations required compared to executing the Lanczos process in exact arithmetic can be very large. Up to six times the original number has been reported in e.g. <ref> [54] </ref>. This is in agreement with our own experiences from solving highly ill-conditioned problems in inverse helioseismology [41] using algorithms based on Lanczos bidiagonalization.
Reference: [55] <author> B. N Parlett and D. S. </author> <title> Scott The Lanczos algorithm with selective orthogonalization, </title> <journal> Math. Comp., </journal> <volume> 33 (1979), no. 145, </volume> <pages> 217-238. </pages>
Reference-contexts: Thus one can simply save the Lanczos vectors along the way and only when the convergence criterion has been fulfilled, does the entire S 2k need to be computed in order to calculate the Ritz vectors. This fact is mentioned in several texts <ref> [5, 55, 51, 53] </ref> and involves a small computational trick, which is crucial to get an efficient implementation: If the QR algorithm is used for computing i , then S 2k a product of the series of orthogonal transformations G 1 ; G 2 ; : : : (plane rotations, or <p> Notice in both panels how the overall pattern shows the well separated extreme Ritz values converging most rapidly as predicted by Theorem 4. We refer to <ref> [15, 55, 51, 64] </ref>, where effects of finite precision arithmetic are discussed in great detail. 5 LANCZOS BIDIAGONALIZATION IN FINITE PRECISION ARITHMETIC 26 5.1 Lanczos algorithms with no reorthogonalization Du Doppelganger, du bleicher Geselle! Was affst du nach mein Liebesleid, das mich gequalt auf dieser Stelle so manche Nacht, in alter <p> For the symmetric Lanczos method, a number of different schemes for reducing the work associated with keeping the Lanczos vectors orthogonal have been developed by B. N. Parlett and co-workers at the University of California at Berkeley, see e.g. <ref> [55, 63] </ref>. The goal of these methods is to cut down the number of orthogonalizations yet obtain a Lanczos algorithm that computes a result, which is close to what would have been computed in the absence of rounding errors. <p> All this work is not necessary. It was found in <ref> [55, 51, 64] </ref> that it is sufficient to maintain semiorthogonality, that is to keep u j and v j below p u=k, to obtain accurate approximations to the singular values and avoid ghosts and doppelgangers from appearing.
Reference: [56] <author> F. P. Pijpers and M. J. Thompson, </author> <title> Faster formulation of the optimally localized averages method for helioseismic inversions, </title> <institution> Astr. Astrophys. </institution> <month> 262 </month> <year> (1992), </year> <month> L33-L36. </month>
Reference-contexts: is when LBD is used to solve sparse linear least squares problems with multiple right-hand sides min kA x (i) b (i) k 2 ; i = 1; : : : ; N ; (4.4) which arise, e.g., when implementing the so-called SOLA mollifier method used in helioseismic inversion (see <ref> [56, 42] </ref>). A straightforward solution would be to apply the LSQR algorithm to each system independently, but often the following procedure (which was analyzed by Saad [59] in the context of (square) linear systems) is more efficient: 1.
Reference: [57] <author> A. Ruhe, </author> <title> Numerical aspects of Gram-Schmidt orthogonalization of vectors, </title> <journal> Lin. Alg. Appl., </journal> <volume> 52/53 (1983), </volume> <pages> 591-601. </pages>
Reference: [58] <author> Y. Saad, </author> <title> On the rates of convergence of the Lanczos and the block-Lanczos Methods, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 17 (1980), no. 5, </volume> <pages> 687-706 </pages>
Reference-contexts: This is a rather complicated matter, which depends on the properties of the spectrum of A. Kaniel [39] and Paige [48] were the first to give bounds on the rate of convergence of the Ritz values. These bounds, which have later been improved by Saad <ref> [58] </ref>, show that while we can expect rapid convergence of the Ritz values approximating the extreme (the algebraically smallest and largest) eigenvalues of C, the interior eigenvalues will in general converge more slowly. <p> These bounds, which have later been improved by Saad [58], show that while we can expect rapid convergence of the Ritz values approximating the extreme (the algebraically smallest and largest) eigenvalues of C, the interior eigenvalues will in general converge more slowly. In <ref> [58, Theorem 2] </ref> Saad proves the following result: Theorem 4 Let B be a symmetric n fi n matrix with eigenvalues 1 2 n and corresponding orthonormal eigenvectors z 1 ; z 2 ; : : : ; z n . <p> For further details including the corresponding convergence results for the Ritz vectors see, e.g., <ref> [58] </ref>, [27] and [6, Section 7.6.4]. 3.3.2 Connection to Lanczos bidiagonalization After having reviewed the theoretical background for the symmetric Lanczos process and its application in SVD calculations, let us return to discuss its connection with the Lanczos bidiagonalization.
Reference: [59] <author> Y. Saad, </author> <title> On the Lanczos Method for Solving Symmetric Linear Systems with Several Right-Hand Sides, </title> <journal> Math. Comp., </journal> <volume> 48 (1987), no. 178, </volume> <pages> 651-662. </pages>
Reference-contexts: A straightforward solution would be to apply the LSQR algorithm to each system independently, but often the following procedure (which was analyzed by Saad <ref> [59] </ref> in the context of (square) linear systems) is more efficient: 1. Solve the first system by computing k steps of the LBD with starting vector b (1) . 2. <p> Now the speed of convergence depends on the size of the two term. Let us follow Saad <ref> [59] </ref> and first consider the two extreme cases. If b (i) is in span (U k+1 ) then the second term on the right-hand side in (4.7) vanishes, and the method provides an accurate approximation. <p> A more quantitative description of the convergence properties including bounds on the residual norms kA x (i) k b (i) k 2 ; i = 1; : : : ; N after k steps can be derived from the results in <ref> [59] </ref>. There are a number of alternative ways to implement the bidiagonalization-projection process outlined above. In a future paper we intend to investigate the possibility of generalizing a number of the algorithms developed for solving symmetric systems of equations with multiple right-hand sides to the least squares case. <p> These methods include the modified Lanczos process discovered by Parlett in [52] and analyzed by Saad in <ref> [59] </ref>. One such algorithm, which we have started investigating, is a generalization of the Lanczos-Galerkin projection method described in [13, 66]. This algorithm, which we will refer to as MCG (Multi-CG) in the following, applies the conjugate gradient algorithm to construct a sequence of Krylov subspaces. <p> This is also what one would expect from the discussion above. It is our general impression that while methods for solving linear systems with multiple right-hand sides have received some attention in (cf. <ref> [63, 59, 52, 66, 13] </ref>), the questions 5 LANCZOS BIDIAGONALIZATION IN FINITE PRECISION ARITHMETIC 24 discussed in this section in relation to the solution of sparse linear least squares problems with multiple right-hand sides have received little attention so far, maybe except for a few remarks in [5, 46]. 5 Lanczos
Reference: [60] <author> R. Schreiber and C. Van Loan, </author> <title> A storage-efficient W Y representation for products of Householder transformations, </title> <journal> SIAM J. Sci. Statist. Comput. </journal> <volume> 10 (1989), no. 1, </volume> <pages> 53-57. </pages>
Reference-contexts: An alternative implementation of reorthogonalization was developed by Golub, Under-wood and Wilkinson [28], who suggested that U k+1 and V k be represented as a product of Householder reflectors. If the reflectors are accumulated as so-called block reflectors (see <ref> [60] </ref>), this is probably the most efficient way of implementing full reorthogonalization. However, this representation is not possible in connection with the partial reorthogonalization schemes treated in the remaining part of this paper, and we will not discuss the Householder approach any further.
Reference: [61] <author> J. Schou, J. Christensen-Dalsgaard, and M. J. Thompson, </author> <title> On Comparing Helioseismic 2-dimensional Inversion Methods, Astrophys. </title> <journal> J., </journal> <volume> 433 (1994), </volume> <pages> 389. </pages>
Reference-contexts: Without reorthogonalization, the behavior is a little erratic and it remains to be seen if the regularized solutions for the non-seed systems computed thus are acceptable in general. We have made a few experiments with systems arising in large-scale 2-dimensional helioseismic rotational inversions (see e.g. <ref> [61, 41] </ref>), which seem to indicate that the computed solutions are quite acceptable, but many extra iterations are required for systems where the right-hand side looks very different from b (1) when no reorthogonalization is used. This is also what one would expect from the discussion above.
Reference: [62] <institution> Science special issue on helioseismology, Science, </institution> <month> 272 </month> <year> (1996), </year> <pages> 1281-1309. </pages>
Reference-contexts: In Section 7 we illustrate the properties of the resulting algorithm, by applying a MATLAB implementation of the algorithm to a number of test problems from the Harwell-Boeing collection, in addition to a discrete ill-posed problem from inverse helioseismology (see <ref> [62] </ref> for a general introduction), which was also used as a test problem in [14, 36, 42].
Reference: [63] <author> H. D. Simon, </author> <title> The Lanczos algorithm with partial reorthogonalization, </title> <journal> Math. Comp., </journal> <volume> 42 (1984), no. 165, </volume> <pages> 115-142. REFERENCES 55 </pages>
Reference-contexts: This is also what one would expect from the discussion above. It is our general impression that while methods for solving linear systems with multiple right-hand sides have received some attention in (cf. <ref> [63, 59, 52, 66, 13] </ref>), the questions 5 LANCZOS BIDIAGONALIZATION IN FINITE PRECISION ARITHMETIC 24 discussed in this section in relation to the solution of sparse linear least squares problems with multiple right-hand sides have received little attention so far, maybe except for a few remarks in [5, 46]. 5 Lanczos <p> For the symmetric Lanczos method, a number of different schemes for reducing the work associated with keeping the Lanczos vectors orthogonal have been developed by B. N. Parlett and co-workers at the University of California at Berkeley, see e.g. <ref> [55, 63] </ref>. The goal of these methods is to cut down the number of orthogonalizations yet obtain a Lanczos algorithm that computes a result, which is close to what would have been computed in the absence of rounding errors. <p> The central idea in partial reorthogonalization is that the level of orthogonality among the Lanczos vectors satisfies a recurrence relation that can be derived from the recurrence used to generate the vectors themselves. This was already shown by Paige in his pioneering thesis [48]. It was Simon <ref> [63, 64] </ref>, however, who realized that these recurrences can be used as a practical tool for computing estimates of the level of orthogonality in an efficient way, and devised scheme by which this information can be used to decide when to reorthogonalize, and which Lanczos vectors it is necessary to include <p> Below we present a result, which introduces the LBD equivalent of the !-recurrence derived by Simon in <ref> [63] </ref> for the symmetric Lanczos process: Theorem 6 Let ji j u T j u i and ji j v T j v i . <p> Notice that when no estimates exceed the limit the corresponding L will be the empty set. The idea of using the parameter j was introduced by Simon in <ref> [63] </ref> as part of the original PRO algorithm, and he demonstrated that the strategy could significantly reduce the amount of work without affecting the accuracy of the final results.
Reference: [64] <author> H. D. Simon, </author> <title> Analysis of the symmetric Lanczos algorithm with reorthogonalization methods, </title> <journal> Linear Algebra Appl., </journal> <volume> 61 (1984), </volume> <pages> 101-131. </pages>
Reference-contexts: Notice in both panels how the overall pattern shows the well separated extreme Ritz values converging most rapidly as predicted by Theorem 4. We refer to <ref> [15, 55, 51, 64] </ref>, where effects of finite precision arithmetic are discussed in great detail. 5 LANCZOS BIDIAGONALIZATION IN FINITE PRECISION ARITHMETIC 26 5.1 Lanczos algorithms with no reorthogonalization Du Doppelganger, du bleicher Geselle! Was affst du nach mein Liebesleid, das mich gequalt auf dieser Stelle so manche Nacht, in alter <p> All this work is not necessary. It was found in <ref> [55, 51, 64] </ref> that it is sufficient to maintain semiorthogonality, that is to keep u j and v j below p u=k, to obtain accurate approximations to the singular values and avoid ghosts and doppelgangers from appearing. <p> Proof. Since the Lanczos bidiagonalization is equivalent to applying the symmetric Lanczos algorithm to the symmetric matrix C in (3.1), the result follows directly from <ref> [64, Theorem 4] </ref>. 2 The theorem says that if the Lanczos vectors are just kept semiorthogonal, then the computed B k is up to roundoff the Ritz-Galerkin projection of A on the subspaces span (U k+1 ) and span (V k ). <p> The central idea in partial reorthogonalization is that the level of orthogonality among the Lanczos vectors satisfies a recurrence relation that can be derived from the recurrence used to generate the vectors themselves. This was already shown by Paige in his pioneering thesis [48]. It was Simon <ref> [63, 64] </ref>, however, who realized that these recurrences can be used as a practical tool for computing estimates of the level of orthogonality in an efficient way, and devised scheme by which this information can be used to decide when to reorthogonalize, and which Lanczos vectors it is necessary to include
Reference: [65] <author> D. C. Sorensen, </author> <title> Implicitly restarted Arnoldi/Lanczos methods for large scale eigenvalue calculations, </title> <institution> Dept. Comp. Appl. Math., Rice University, Houston, Texas, </institution> <year> 1995. </year>
Reference-contexts: 1 INTRODUCTION 5 length m+n must be used. Also, the number iterations required to compute a given number of singular values is doubled. More recently, methods based on the implicitly restarted Lanczos algorithm have appeared (cf. <ref> [12, 65] </ref>). Various studies, see e.g. [23], indicate that these are robust and efficient tools for computing a few of the largest singular values and vectors.
Reference: [66] <author> H. A. van der Vorst, </author> <title> An iterative solution method for solving f (A)x = b, using Krylov subspace information obtained for the symmetric positive definite matrix A, </title> <journal> J. Comput. Appl. Math., </journal> <volume> 18 (1987), no. 2, </volume> <pages> 249-263. </pages>
Reference-contexts: These methods include the modified Lanczos process discovered by Parlett in [52] and analyzed by Saad in [59]. One such algorithm, which we have started investigating, is a generalization of the Lanczos-Galerkin projection method described in <ref> [13, 66] </ref>. This algorithm, which we will refer to as MCG (Multi-CG) in the following, applies the conjugate gradient algorithm to construct a sequence of Krylov subspaces. <p> This is also what one would expect from the discussion above. It is our general impression that while methods for solving linear systems with multiple right-hand sides have received some attention in (cf. <ref> [63, 59, 52, 66, 13] </ref>), the questions 5 LANCZOS BIDIAGONALIZATION IN FINITE PRECISION ARITHMETIC 24 discussed in this section in relation to the solution of sparse linear least squares problems with multiple right-hand sides have received little attention so far, maybe except for a few remarks in [5, 46]. 5 Lanczos
Reference: [67] <author> K. Wu and H. D. Simon, </author> <title> A Parallel Lanczos Method for Symmetric Generalized Eigenvalue Problems, </title> <type> Tech. Report., </type> <institution> NERSC, </institution> <year> 1997, </year> <note> URL: http://www.nersc.gov/research/SIMON/planso.html. </note>
Reference-contexts: Parallel implementations of both the lanso subroutine (cf. <ref> [67] </ref>) and the ARPACK package (cf. [43]), which implements the implicitly restarted Lanczos algorithms, are now available. The routines in SVDPACK (cf. [4]), which build on an older sequential version of lanso are available from Netlib. <p> Finally we wish to mention that an efficient parallel implementation of the LANSO package has recently appeared. The structure of PRO and BPRO is very similar, and we see no reason why the good parallel performance reported in <ref> [67] </ref> should not be attainable by a parallel implementation of BPRO. It should be emphasized that our experiments have been carried out in MATLAB, and it is therefore difficult to compare the computational efficiency of our methods with the routines from, e.g., ARPACK.
Reference: [68] <author> K. Wu and H. D. Simon, </author> <title> Private communication, </title> <year> 1998. </year>
Reference-contexts: This will be corrected in the next version of the LANSO package <ref> [68] </ref>. 6.4 A hybrid method to improve performance on cache-based architectures In this section we propose a small modification of BPRO that will help improving the performance on computers with a memory hierarchy.
References-found: 68

