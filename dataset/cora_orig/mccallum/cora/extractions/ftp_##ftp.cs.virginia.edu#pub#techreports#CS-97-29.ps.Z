URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-97-29.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: ferrari@cs.virginia.edu  
Title: 1 JPVM: Network Parallel Computing in Java  
Author: Adam J. Ferrari 
Date: 8, 1997  
Note: December  
Abstract: Technical Report CS-97-29 Department of Computer Science University of Virginia, Charlottesville, VA 22903, USA Abstract The JPVM library is a software system for explicit message-passing based distributed memory MIMD parallel programming in Java. The library supports an interface similar to the C and Fortran interface provided by the Parallel Virtual Machine (PVM) system, but with syntax and semantics modifications afforded by Java and better matched to Java programming styles. The similarity between JPVM and the widely used PVM system supports a quick learning curve for experienced PVM programmers, thus making the JPVM system an accessible, low-investment target for migrating parallel applications to the Java platform. At the same time, JPVM offers novel features not found in standard PVM such as thread safety, multiple communication endpoints per task, and default-case direct message routing. JPVM is implemented entirely in Java, and is thus highly portable among platforms supporting some version of the Java Virtual Machine. This feature opens up the possibility of utilizing resources commonly excluded from network parallel computing systems such as Macintosh and Windows-NT based systems. Initial applications performance results achieved with a prototype JPVM system indicate that the Java-implemented approach can offer good performance at appropriately coarse granularities. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T.E. Anderson, D.E. Culler, D.A. Patterson, </author> <title> and the NOW team, A Case for NOW (networks of Workstations), </title> <journal> IEEE Micro, </journal> <volume> vol. 15, no. 1, </volume> <pages> pp. 54-64, </pages> <month> February, </month> <year> 1995. </year>
Reference-contexts: Network parallel computing systems allow individual applications to harness the aggregate power of the increasingly powerful, well-networked, heterogeneous, and often largely under-utilized collections of resources available to many users <ref> [1] </ref>. In this paper, we describe a network parallel computing software system for use with and implemented in the Java language.
Reference: [2] <author> P. Cappello, B.O. Christiansen, M.F. Ionescu, M.O. Neary, K.E. Schauser, and D. Wu, Javelin: </author> <title> Internet-based Parallel Computing Using Java, </title> <booktitle> ACM Workshop on Java for Science and Engineering Computation, </booktitle> <month> June, </month> <year> 1997. </year>
Reference-contexts: Round-trip message costs, milliseconds - 10 - 5 Related Work A number of other systems have been developed to support network parallel programming in Java. One common approach to this problem is the use of volunteer-based systems such as Bayani-han [7] and Javelin <ref> [2] </ref>. As opposed to JPVM which is based on standalone Java applications, these systems are based on Java applets that execute within the context of a web browser.
Reference: [3] <author> A.J. Ferrari and V.S. Sunderam, </author> <title> Multiparadigm Distributed Computing with TPVM, </title> <journal> Journal of Concurrency, Practice and Experience, </journal> <note> (to appear). </note>
Reference-contexts: For example, from the application perspective, Java provides a portable, uniform interface to threads. Using threads instead of traditional heavyweight processes has been found to be an avenue for increasing latency tolerance and allowing finer-grained computations to achieve good performance in distributed memory parallel processing environments <ref> [3] </ref>. From the system implementation perspective, Java supports a high degree of code portability and a uniform API for operating system services such as network communications. The JPVM (Java Parallel Virtual Machine) library is a software system for explicit message-passing based distributed memory MIMD parallel programming in Java. <p> Threads can be employed in a variety of design schemes in JPVM programs. For example, a traditional threaded server JPVM task is one possibility. Another possibility is the use of threads as the basic units of parallel execution, as in TPVM <ref> [3] </ref>.
Reference: [4] <author> A. Geist, A Beguelin, J. Dongarra, W. Jiang, R. Manchek, </author> <title> and V.S. Sunderam, PVM: Parallel Virtual Machine, </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Although numerous software systems support some form of network parallel computing, the majority of use has thus far been based on a small set of popular packages that provide an explicit message-passing, distributed memory MIMD programming model such as Parallel Virtual Machine (PVM) <ref> [4] </ref>, and the Message Passing Interface (MPI) [6]. These software systems support simple, portable library interfaces for typical high-performance computing languages such as C and Fortran. For example, PVM provides the programmer with a library routines to perform task creation, data marshalling, and asynchronous message passing.
Reference: [5] <author> P.A. Gray and V.S. Sunderam, IceT: </author> <title> Distributed Computing and Java, </title> <note> available from: http://www.mathcs.emory.edu/~gray/abstract7.html </note>
Reference-contexts: For example, JavaPVM programs retain the single communication endpoint PVM model, the thread-unfriendly PVM buffer interface, and so on. Another software package for network parallel computing in Java is the IceT system <ref> [5] </ref>. This problem tasks multiply speedup total speedup size time time 1 3.2 - 3.2 - 9 1.5 2.1 6.7 0.5 256 4 7.7 3.5 10.3 2.6 1 223.3 - 223.3 - 9 28.9 7.7 33.5 6.7 Table 3.
Reference: [6] <author> W. Gropp, E. Lusk, and A. Skjellum, </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface, </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Although numerous software systems support some form of network parallel computing, the majority of use has thus far been based on a small set of popular packages that provide an explicit message-passing, distributed memory MIMD programming model such as Parallel Virtual Machine (PVM) [4], and the Message Passing Interface (MPI) <ref> [6] </ref>. These software systems support simple, portable library interfaces for typical high-performance computing languages such as C and Fortran. For example, PVM provides the programmer with a library routines to perform task creation, data marshalling, and asynchronous message passing.
Reference: [7] <author> L.F.G. Sarmenta, Bayanihan: </author> <title> Web-Based Volunteer Computing Using Java, </title> <note> available from: http://www.cag.lcs.mit.edu/bayanihan/ </note>
Reference-contexts: Round-trip message costs, milliseconds - 10 - 5 Related Work A number of other systems have been developed to support network parallel programming in Java. One common approach to this problem is the use of volunteer-based systems such as Bayani-han <ref> [7] </ref> and Javelin [2]. As opposed to JPVM which is based on standalone Java applications, these systems are based on Java applets that execute within the context of a web browser.
Reference: [8] <author> D. Thurman, JavaPVM, </author> <note> available from: http://www.isye.gatech.edu/chmsr/JavaPVM/ </note>
Reference-contexts: Although this is possible for many applications such as parameter space studies, it rules out most potentially successful network parallel applications. A system similar to JPVM in its programming interface and model is the JavaPVM library <ref> [8] </ref>. This system also provides a PVM-like interface for Java applications. The primary difference between JavaPVM and JPVM is in implementation. Unlike JPVM which is implemented entirely in Java, JavaPVM is based on the Java Native Methods mechanism, providing native method wrappers around the existing standard PVM routines.
Reference: [9] <author> S. White, A. lund, </author> <title> and V.S. Sunderam, Performance of the NAS Parallel Benchmarks on PVM Based Networks, </title> <journal> Journal of Parallel and Distributed Computing , vol. </journal> <volume> 26, no. 1, </volume> <pages> pp. 61-71, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Results obtained with network parallel computing systems have been encouraging. For example, a performance study of the NAS benchmark suite implemented in PVM demonstrated that relatively small clusters of workstations could provide performance comparable to significantly more expensive supercomputers <ref> [9] </ref>. However, the utilization of distributed, heterogeneous, shared resources - 2 - connected by commodity networks as a single, virtual parallel computer poses serious problems for both the application and system software programmer. <p> In this section we discuss some of the core features of the JPVM system implementation. 3.1 Communications Implementation In standard PVM, the use of direct task-to-task TCP connections has been found to significantly outperform the older UDP-based daemon-routed message passing implementation <ref> [9] </ref>. Thus, the current internal JPVM message passing implementation is based on direct task-to-task communication over TCP sockets using the Java object serialization interface for message transfer.
References-found: 9

