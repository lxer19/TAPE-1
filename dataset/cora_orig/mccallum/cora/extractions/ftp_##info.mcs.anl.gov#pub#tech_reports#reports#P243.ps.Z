URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P243.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/preprints.htm
Root-URL: http://www.mcs.anl.gov
Title: SPARSE JACOBIAN ESTIMATION AND FACTORIZATION ON A MULTIPROCESSOR  
Author: PAUL E. PLASSMANN 
Abstract: In this paper we present algorithms and experimental results for the estimation and QR factorization of large, sparse Jacobians on a message-passing multiprocessor. The gist of this work is the development of paradigms for the efficient solution of the "inner loop" of a nonlinear optimization algorithm: the estimation of the Jacobian, its factorization, and the solution of the resulting trust-region problem. A parallel sparse QR factorization based on the global row reduction algorithm is introduced. We emphasize the commonality between row partitions that allow for the efficient parallel factorization of the Jacobian and its estimation. We also note that the interprocessor communication structure constructed for the QR factorization can be used to solve an associated trust-region problem. Finally, experimental results obtained on the Intel iPSC/2 are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Byrd, R. Schnabel, and G. Shultz, </author> <title> Parallel quasi-Newton methods for unconstrained optimization, </title> <type> Tech. Rep., </type> <institution> Department of Computer Science, University of Colorado at Boulder, </institution> <year> 1988. </year>
Reference-contexts: Or if we overlap the function evaluation involved in the step acceptance computation with the Jacobian estimation, as suggested by Byrd, Schnabel, and Shultz <ref> [1] </ref> and Coleman and Li [3], we may require the estimation to be done with one less evaluation.
Reference: [2] <author> T. F. Coleman, A. Edenbrandt, and J. R. Gilbert, </author> <title> Predicting fill for sparse orthogonal factorization, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 33 (1986), </volume> <pages> pp. 517-532. </pages>
Reference-contexts: However, if A is reordered into block upper triangular (Dulmage-Mendelsohn) form, it can be shown <ref> [2] </ref> that for each diagonal block these two structures are equal (assuming no accidental numerical cancellation). Equivalently, if the bipartite graph representation of A has the strong Hall property, then the two structures are equal; we assume this to be the case for the remainder of this paper.
Reference: [3] <author> T. F. Coleman and G. Li, </author> <title> Solving systems of nonlinear equations on a message-passing multiprocessor, </title> <type> Tech. Rep. </type> <institution> CS-87-887, Computer Science Department, Cornell University, </institution> <year> 1987. </year>
Reference-contexts: Or if we overlap the function evaluation involved in the step acceptance computation with the Jacobian estimation, as suggested by Byrd, Schnabel, and Shultz [1] and Coleman and Li <ref> [3] </ref>, we may require the estimation to be done with one less evaluation.
Reference: [4] <author> T. F. Coleman and J. J. Mor e, </author> <title> Estimation of sparse Jacobian matrices and graph coloring problems, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 20 (1983), </volume> <pages> pp. 187-209. </pages>
Reference-contexts: If ae max is the maximum number of nonzeros in a row of the Jacobian, we know that at least that many function evaluations are required to estimate J (x) <ref> [4] </ref>. Thus an optimal parallel algorithm with p processors would take at least d ae max p e times the time required for one function evaluation. <p> On a variety of test problems it has been shown by Coleman and More <ref> [4] </ref> that intersection graph coloring algorithms often generate a set of differencing vectors which requires very close to the optimal number of function evaluations. Also, the incidence degree order (IDO) coloring algorithm, which they find to be very effective, has essentially the same structure as the minimum-degree ordering algorithm.
Reference: [5] <author> T. F. Coleman and P. Plassmann, </author> <title> Solution of nonlinear least-squares problems on a multiprocessor, </title> <type> Tech. Rep. </type> <institution> CS-88-923, Computer Science Department, Cornell University, </institution> <year> 1988. </year>
Reference-contexts: Denote this algorithm as the nonlocal merge algorithm. Since each step requires the merging of two upper trapezoidal sparse matrices, the approach we might consider is a generalization of a dense upper triangular matrix merging algorithm, originally proposed in the context of solving positive definite trust region problems <ref> [5] </ref>. In this nonlocal merging algorithm, the rows of the two upper trapezoidal matrices are wrapped onto an embedded ring of processors using the structure of the parent in the binary merge. This algorithm is discussed in more detail elsewhere [13].
Reference: [6] <author> W. Gentleman, </author> <title> Row elimination for solving sparse linear systems and least squares problems, </title> <booktitle> in Conference in Numerical Analysis, Lecture Notes in Mathematics 506, </booktitle> <editor> G. Watson, ed., </editor> <publisher> Springer-Verlag, </publisher> <year> 1975, </year> <pages> pp. 122-133. </pages>
Reference-contexts: The amount of computation required to reduce A to upper triangular form can vary dramatically depending on what type of orthogonal transformations are used, and in what manner they are applied. Sequences of Givens rotations have been shown to be very effective in performing this reduction <ref> [6, 8] </ref>. Consider the Givens rotation G (k) ij which operates on rows r i and r j of the matrix to zero the k-th element of row r j .
Reference: [7] <author> A. George and J. W.-H. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: Given the equivalence of the nonzero structure of the orthogonal and Cholesky factors, a column ordering can be chosen using existing heuristics for reduction of fill in factoring symmetric positive definite systems <ref> [7, 8] </ref>. Hence, the resulting nonzero structure of R will be reasonably sparse and therefore one expects that the computation required to obtain R will be reduced with respect to column orderings that produce more fill. <p> Determine a column ordering to reduce the fill in Cholesky (A T A). In our experiments a simple parallel implementation of the minimum-degree algorithm <ref> [7] </ref> was used to determine the column ordering. 2. Swap columns based on the column ordering determined. Since a row mapping is used, this is a local operation. 3.
Reference: [8] <author> J. George and M. Heath, </author> <title> Solution of sparse least squares problems using Givens rotations, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 34 (1980), </volume> <pages> pp. 69-83. </pages>
Reference-contexts: Given the equivalence of the nonzero structure of the orthogonal and Cholesky factors, a column ordering can be chosen using existing heuristics for reduction of fill in factoring symmetric positive definite systems <ref> [7, 8] </ref>. Hence, the resulting nonzero structure of R will be reasonably sparse and therefore one expects that the computation required to obtain R will be reduced with respect to column orderings that produce more fill. <p> The amount of computation required to reduce A to upper triangular form can vary dramatically depending on what type of orthogonal transformations are used, and in what manner they are applied. Sequences of Givens rotations have been shown to be very effective in performing this reduction <ref> [6, 8] </ref>. Consider the Givens rotation G (k) ij which operates on rows r i and r j of the matrix to zero the k-th element of row r j .
Reference: [9] <author> J. W.-H. Liu, </author> <title> A compact row storage scheme for Cholesky factors using elimination trees, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 12 (1986), </volume> <pages> pp. </pages> <month> 127-148. </month> <title> [10] , On general row merging schemes for sparse Givens transformations, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 7 (1986), </volume> <pages> pp. 1190-1211. </pages>
Reference-contexts: The elimination forest construction is a more compact representation of the information required for the factorization because the nonzero structure of the rows of the Cholesky factor can be easily computed from the elimination tree and the original matrix <ref> [9, 14] </ref>. In the row merge heap, the structure of a vertex is only a subset of the set of its ancestors.
Reference: [11] <author> J. J. Mor e, </author> <title> The Levenberg-Marquardt algorithm: Implementation and theory, </title> <note> in Lecture Notes in Mathematics, No. 630-Numerical Analysis, </note> <editor> G. Watson, ed., </editor> <publisher> Springer-Verlag, </publisher> <year> 1978, </year> <pages> pp. 105-116. </pages>
Reference-contexts: In addition, it seems that this approach is more amenable to different load balancing schemes and can be employed with any column ordering heuristic. To solve nonlinear least-squares problems, the Levenberg-Marquardt approach requires the solution of a sequence of trust-region problems <ref> [11] </ref>. In section 5 we show that the global row reduction algorithm can be modified to solve the matrix problem that arises in solving these trust-region problems. <p> A Sparse R-S Reduction Algorithm. The solution of the trust-region problem involved in the Levenberg-Marquardt approach to solving nonlinear least-squares problems requires the repeated reduction of a system of the form " I (5) to upper triangular form <ref> [11] </ref>. In this system R is the upper triangular factor obtained from the QR factorization of the Jacobian and is the current estimate of the Levenberg-Marquardt parameter.
Reference: [12] <author> G. Newsam and J. Ramsdell, </author> <title> Estimation of sparse Jacobian matrices, </title> <type> Tech. Rep. </type> <institution> TR-17-81, Aiken Computation Laboratory, Harvard University, </institution> <year> 1981. </year>
Reference-contexts: In this case we would like to be able to turn to an alternate approach which does not exceed the limit of K function evaluations. The alternatives we consider are the multicoloring methods and full matrix methods. The full matrix method was originally proposed by Newsam and Ramsdell <ref> [12] </ref>. Consider some ae ae max difference vectors combined into a n fi ae matrix D and the corresponding ae function differences combined into a m fi ae matrix F .
Reference: [13] <author> P. E. Plassmann, </author> <title> The Parallel Solution of Nonlinear Least-Squares Problems, </title> <type> Cornell University Ph.D. thesis, </type> <year> 1990. </year>
Reference-contexts: In this nonlocal merging algorithm, the rows of the two upper trapezoidal matrices are wrapped onto an embedded ring of processors using the structure of the parent in the binary merge. This algorithm is discussed in more detail elsewhere <ref> [13] </ref>. A different approach to the problem of interprocessor communication is the global row reduction algorithm. In this algorithm the redistribution of rows required by the binary merges is avoided, instead a global reduction of rows is executed to compute each row of R. <p> Given that the row partition is generated in this manner, it has been shown that the interprocessor communication required by the global row reduction algorithm can be computed by performing a certain vertex elimination upon a quotient graph <ref> [13] </ref>. The advantage of this approach is that the quotient graph is a much smaller structure than a representation of the entire nonzero structure of the matrix, yet it captures the essential information necessary to determine the interprocessor communication during the symbolic phase of the sparse factorization. 3.1. <p> Ideally, one would like to generate a candidate set as close to the correct set of processors as possible in order to increase the efficiency of the symbolic factorization. Several possible algorithms for constructing these candidate sets based on a quotient graph vertex elimination model have been discussed elsewhere <ref> [13] </ref>. Another simple possibility would be to let the candidate set be the set of all processors which are assigned a row in the set desc (c k ) of the row merge heap. <p> Since the foundation vertices are labeled after them, column vertices local to a processor do not need to be included in the quotient graph. (This step is not discussed in this paper, but is presented in reference <ref> [13] </ref>. Briefly, the quotient graph is a compact way to represent the structures of the upper trapezoidal matrices at each processor during the symbolic factorization. <p> The symbolic factorization phase: (a) Use a quotient graph elimination algorithm to determine the candidate set at each step of the factorization (see reference <ref> [13] </ref> for a lengthy discussion on possible quotient graph elimination models). (b) Reduce the minimum depth spanning tree generated from the candidate set, using the algorithm shown in Figure 16, to obtain the the reduction trees and store the result. (c) Determine the intermediate storage required for the upper triangular matrices. <p> Times required for various phases of the factorization. that have been developed to streamline the symbolic factorization phase of the algorithm <ref> [13] </ref>. It is likely that a better implementation of the symbolic factorization phase would significantly decrease the execution time of this phase from the times presented here. Overall, these results are preliminary, but encouraging.
Reference: [14] <author> R. Schreiber, </author> <title> A new implementation of sparse Gaussian elimination, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 8 (1982), </volume> <pages> pp. 256-276. </pages>
Reference-contexts: The elimination forest construction is a more compact representation of the information required for the factorization because the nonzero structure of the rows of the Cholesky factor can be easily computed from the elimination tree and the original matrix <ref> [9, 14] </ref>. In the row merge heap, the structure of a vertex is only a subset of the set of its ancestors.
Reference: [15] <author> R. Tarjan, </author> <title> Data Structures and Network Algorithms, </title> <publisher> SIAM, </publisher> <year> 1983. </year>
Reference-contexts: Note that this merging algorithm requires only the disjoint set primitives find and link, hence, as Zmijewski pointed out in the elimination forest case, an implementation using path compression and set union <ref> [15] </ref> obtains a nearly linear running time.
Reference: [16] <author> E. Zmijewski, </author> <title> Sparse Cholesky factorization on a multiprocessor, </title> <type> Tech. Rep. 87-856, </type> <institution> Cornell University, </institution> <year> 1987. </year> <month> 28 </month>
Reference-contexts: In this section we describe several algorithmic results related to the symbolic factorization phase of the global row reduction algorithm. First, we show that the problem of determining the row merge heap in parallel can be done by a simple extension of an elimination forest merging algorithm <ref> [16] </ref>. Then we rephrase the problem of finding an initial row partition as a problem in finding a special set of vertices, called a foundation, in the row merge heap. <p> A sequential algorithm to compute the row merge heap. related to the complete row merge heap H (A). Following the work of Zmijewski <ref> [16] </ref> on the parallel merging of elimination forests, an algorithm can be developed for merging these heap forests to obtain the entire row merge heap. <p> But after we have added c 6 to H ab , the root of the tree containing these vertices is c 6 , hence there is nothing left to be done. A proof of correctness for the heap forest merge algorithm is similar to the proof presented by Zmijewski <ref> [16] </ref> and we will not present it here. Note that this merging algorithm requires only the disjoint set primitives find and link, hence, as Zmijewski pointed out in the elimination forest case, an implementation using path compression and set union [15] obtains a nearly linear running time.
References-found: 15

