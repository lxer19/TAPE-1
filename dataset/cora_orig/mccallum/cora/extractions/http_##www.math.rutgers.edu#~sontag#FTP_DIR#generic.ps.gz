URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/generic.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Title: Shattering all sets of k points in "general position" requires (k 1)=2 parameters  
Author: Eduardo D. Sontag 
Address: New Brunswick, NJ 08903  
Affiliation: Department of Mathematics Rutgers University,  
Abstract: For classes of concepts defined by certain classes of analytic functions depending on n parameters, there are nonempty open sets of samples of length 2n + 2 which cannot be shattered. A slighly weaker result is also proved for piecewise-analytic functions. The special case of neural networks is discussed.
Abstract-found: 1
Intro-found: 1
Reference: [Cover 1988] <author> Cover, </author> <title> T.M., "Capacity problems for linear machines", in Pattern Recognition, </title> <editor> L. Kanal ed., </editor> <publisher> Thompson Book Co., </publisher> <year> 1988, </year> <pages> pp. 283-289. </pages>
Reference-contexts: Cover's work on capacity of perceptrons <ref> [Cover 1988] </ref>), then an upper bound of O (n) might hold. Strong evidence for this possibility was provided by Adam Kowalczyk, who showed in [Kowalczyk 1996] that this indeed happens for the (very special) case of hard-threshold neural networks with fully connected first layer.
Reference: [Karpinski and Macintyre 1996] <author> Karpinski, M., and A. Macintyre, </author> <title> "Polynomial bounds for VC dimension of sigmoidal and general Pfaffian neural networks," </title> <journal> J. </journal> <note> Computer Systems Sciences (1996), to appear. </note> <editor> (Summarized version: </editor> <title> "Polynomial bounds for VC dimension of sigmoidal neural networks," </title> <booktitle> in Proc. 27th ACM Symposium on Theory of Computing, </booktitle> <year> 1995, </year> <pages> pp. 200-208.) </pages>
Reference-contexts: For the best known (obviously larger) upper bounds for VC dimension, for closely related classes of responses, see <ref> [Karpinski and Macintyre 1996] </ref>, and see also [Maass 1994, Koiran and Sontag 1996] for super-linear lower bounds. 2 3 Proof of Main Result We need to show that if k 2n + 2 then S k is not dense, that is to say, there is some nonempty open subset Q R
Reference: [Koiran and Sontag 1996] <author> Koiran, P., and E.D. Sontag, </author> <title> "Neural networks with quadratic VC dimension," </title> <journal> J. </journal> <note> Computer Systems Sciences (1996), to appear. (Summarized version in Advances in Neural Information Processing Systems 8 (NIPS95) (D.S. </note> <editor> Touretzky, M.C. Moser, and M.E. Hasselmo, eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996, </year> <pages> pp. 197-203.) </pages>
Reference-contexts: Thus, it is of interest to obtain useful upper bounds on this number, and in particular to study the Vapnik-Chervonenkis (VC) dimension, which is the size of the largest set of inputs that can be shattered (arbitrary binary labeling is possible). Recent results (cf. <ref> [Koiran and Sontag 1996] </ref>, and also [Maass 1994] for closely related work and a survey) show that the VC dimension grows at least as fast as the square n 2 of the number of adjustable weights n in the net, and this number might grow as fast as n 4 ([Karpinski <p> For the best known (obviously larger) upper bounds for VC dimension, for closely related classes of responses, see [Karpinski and Macintyre 1996], and see also <ref> [Maass 1994, Koiran and Sontag 1996] </ref> for super-linear lower bounds. 2 3 Proof of Main Result We need to show that if k 2n + 2 then S k is not dense, that is to say, there is some nonempty open subset Q R mk so that Q T S k
Reference: [Kowalczyk 1996] <author> Kowalczyk, A., </author> <title> "Estimates of storage capacity of mutilayer perceptron with threshold logic units," Neural Networks (1996), to appear. (Preliminary version appeared as "Counting function theorem for multi-layer networks," </title> <booktitle> in Advances in Neural Information Processing Systems 6 , Cowan, </booktitle> <editor> J.D., Tesauro, G., and Alspector, J., editors, </editor> <publisher> Morgan Kaufman, </publisher> <year> 1994, </year> <pages> pp 375-382.) </pages>
Reference-contexts: Cover's work on capacity of perceptrons [Cover 1988]), then an upper bound of O (n) might hold. Strong evidence for this possibility was provided by Adam Kowalczyk, who showed in <ref> [Kowalczyk 1996] </ref> that this indeed happens for the (very special) case of hard-threshold neural networks with fully connected first layer.
Reference: [Maass 1994] <author> Maass, M., </author> <booktitle> "Perspectives of current research about the complexity of learning in neural nets," in Theoretical Advances in Neural Computation and Learning, </booktitle> <editor> V.P. Roychow-dhury, K.Y. Siu, and A. Orlitsky, editors, </editor> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1994, </year> <pages> pp. 295-336. </pages>
Reference-contexts: Recent results (cf. [Koiran and Sontag 1996], and also <ref> [Maass 1994] </ref> for closely related work and a survey) show that the VC dimension grows at least as fast as the square n 2 of the number of adjustable weights n in the net, and this number might grow as fast as n 4 ([Karpinski and Macintyre 1996]). <p> For the best known (obviously larger) upper bounds for VC dimension, for closely related classes of responses, see [Karpinski and Macintyre 1996], and see also <ref> [Maass 1994, Koiran and Sontag 1996] </ref> for super-linear lower bounds. 2 3 Proof of Main Result We need to show that if k 2n + 2 then S k is not dense, that is to say, there is some nonempty open subset Q R mk so that Q T S k
Reference: [Maass 1996a] <author> Maass, W., </author> <title> "Lower bounds for the computational power of networks of spiking neurons," </title> <booktitle> Neural Computation 8(1996): </booktitle> <pages> 1-40. </pages>
Reference: [Maass 1996b] <author> Maass, W., </author> <title> "The third generation of neural network models," </title> <booktitle> in Proc. 7th Aus-tralian Conference on Neural Networks 1996 , Canberra, to appear. </booktitle>
Reference-contexts: The estimate is also useful in the very different context of understanding computational abilities; as an illustration of this fact, we mention that our main result is being employed by Maass in <ref> [Maass 1996b] </ref> for contrasting the computational power of spiking neurons ([Maass 1996a]) with that of sigmoidal neural networks.
Reference: [Macintyre and Sontag 1993] <author> Macintyre, A., and E.D. Sontag, </author> <title> "Finiteness results for sigmoidal `neural' networks," </title> <booktitle> in Proc. 25th Annual Symp. Theory Computing, </booktitle> <address> San Diego, </address> <month> May </month> <year> 1993, </year> <pages> pp. 325-334. </pages>
Reference-contexts: Finally, we review several facts about exp-ra definable functions. This discussion is based on recent work in model theory due to Gabrielov, Van den Dries, Wilkie, and many others; see <ref> [Macintyre and Sontag 1993, Sontag 1996] </ref> for more details and extensive bibliographical references.
Reference: [Sontag 1992] <author> Sontag, E.D., </author> <title> "Feedforward nets for interpolation and classification," </title> <journal> J. Comp. Syst. Sci. </journal> <volume> 45(1992): </volume> <pages> 20-48. </pages>
Reference-contexts: In fact, it is possible to define a fixed analytic function fi, with m = 1, for which S k consists of all sequences of k distinct elements of R, not merely dense in R km , for all k; see <ref> [Sontag 1992] </ref>. Thus, analyticity is in itself not enough to obtain nontrivial bounds. We will assume that fi is analytic and definable.
Reference: [Sontag 1996] <author> Sontag, E.D., </author> <title> "Critical points for least-squares problems involving certain analytic functions, with applications to sigmoidal nets," </title> <note> Advances in Computational Mathematics (Special Issue on Neural Networks) (1996), to appear. 13 </note>
Reference-contexts: Then the image (Z) is a -analytic subset of N , having dim (Z) dim Z, and the following inequality holds: dim Z dim (Z) + max dim M y Z : (4) (This inequality is a simple consequence of stratification theory; it is proved in the Appendix of <ref> [Sontag 1996] </ref>. <p> Note, incidentally, that a strict inequality may hold, as illustrated by the case M = R 2 , N = R, = projection on first factor, and Z = the union of the x and the y axes.) We prove the following fact, which is basically Theorem 1 in <ref> [Sontag 1996] </ref>. (That theorem could also be applied more directly, but the slight generalization given here should be useful for other purposes.) Lemma A.1 Let X; U 1 ; : : : ; U ` be (analytic, second countable) manifolds, with dimensions n; m 1 ; : : : ; m <p> Finally, we review several facts about exp-ra definable functions. This discussion is based on recent work in model theory due to Gabrielov, Van den Dries, Wilkie, and many others; see <ref> [Macintyre and Sontag 1993, Sontag 1996] </ref> for more details and extensive bibliographical references. <p> Notice that any set obtained from a function fi by logical operations (such as the set A ` in Corollary (A.2)) is definable provided that fi is definable. The following fact is a nontrivial consequence of the work in logic cited above; see precise references in <ref> [Sontag 1996] </ref>: Lemma A.3 Let S be a definable subset of R q , for some q. Then, either S contains an open subset, or it is a finite union of connected embedded submanifolds of R q (and in particular is nowhere dense). 2 12
References-found: 10

