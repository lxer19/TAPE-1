URL: ftp://ftp.eecs.umich.edu/people/wellman/mlconjeq.ps.Z
Refering-URL: http://ai.eecs.umich.edu/people/junling/publication.html
Root-URL: http://www.cs.umich.edu
Email: fwellman, junlingg@umich.edu  
Title: Conjectural Equilibrium in Multiagent Learning  
Author: Michael P. Wellman and Junling Hu 
Date: February 9, 1997  
Address: Ann Arbor, MI 48109-2110 USA  
Affiliation: University of Michigan  
Abstract: Learning in a multiagent environment is complicated by the fact that as other agents learn, the environment effectively changes. Moreover, other agents' actions are often not directly observable, and the actions taken by the learning agent can strongly bias which range of behaviors are encountered. We define the concept of a conjectural equilibrium, where all agents' expectations are realized, and each agent responds optimally to its expectations. We present a generic multia-gent exchange situation, in which competitive behavior constitutes a conjectural equilibrium. We then introduce an agent that executes a more sophisticated strategic learning strategy, building a model of the response of other agents. We find that the system reliably converges to a conjectural equilibrium, but that the final result achieved is highly sensitive to initial belief. In essence, the strategic learner's actions tend to fulfill its expectations. Depending on the starting point, the agent may be better or worse off than had it not attempted to learn a model of the other agents at all. This report is a revised and extended version of our ICMAS-96 paper [12]. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Craig Boutilier. </author> <title> Learning conventions in multiagent stochastic domains using likelihood estimates. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 106-114, </pages> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: First, it applies at each stage of an extensive form game, rather than for single-stage games or in the limit of a repeated game. Second, it takes individual actions of other agents as observable, whereas in our framework the agents observe only resulting state. Boutilier <ref> [1] </ref> also considers a model where only outcomes are observable, comparing the effectiveness of alternate learning mechanisms for solving mul-tiagent coordination problems. The basic concept of conjectural equilibrium was first introduced by Hahn, in the context of a market model [10]. <p> Let z 0 ; z 00 2 S, and z = z 0 + (1 )z 00 , where 2 <ref> [0; 1] </ref>. We need to show that z 2 S.
Reference: [2] <author> Kevin Boyle. </author> <title> Starting point bias in contingent valuation bidding games. </title> <journal> Land Economics, </journal> <volume> 61 </volume> <pages> 188-194, </pages> <year> 1985. </year>
Reference-contexts: Our theoretical and experimental investigations show that even when convergence to a "correct" model obtains, improvement in result does not always follow. To our knowledge, the phenomenon of self-fulfilling bias as defined here has not been well studied in multiagent learning. Economists studying bidding games <ref> [18, 2] </ref> have noticed that biased starting bid prices strongly influence final bids. But these empirical findings have not been incorporated into a general framework in terms of learning.
Reference: [3] <author> Adam Brandenburger. </author> <title> Knowledge and equilibrium in games. </title> <journal> Journal of Economic Perspectives, </journal> <volume> 6(4) </volume> <pages> 83-101, </pages> <year> 1992. </year>
Reference-contexts: For example, choice of iterated undominated strategies follows from common knowledge of rationality and the game setup <ref> [3] </ref>. 5 Each agent has some belief about the state that would result from per-forming its available actions. Let ~s i (a) represent the state that agent i believes would result if it selected action a.
Reference: [4] <author> Fred Brauer and John A. Nohel. </author> <title> The Qualitative Theory of Ordinary Differential Equations. </title> <address> New York: W.A. </address> <publisher> Benjamin, </publisher> <year> 1969. </year> <month> 25 </month>
Reference: [5] <author> John Q. Cheng and Michael P. Wellman. </author> <title> The WALRAS algorithm: A convergent distributed implementation of general-equilibrium outcomes. </title> <note> To appear in Computational Economics, </note> <year> 1997. </year>
Reference-contexts: If the aggregate demand obeys gross substitutability (an increase in the price of one good raises demand for others, which hence serve as substitutes), then this method is guaranteed to converge to a competitive equilibrium (under the conditions under which it is guaranteed to exist) [15]. The walras algorithm <ref> [5] </ref> is a variant of tatonnement. In walras, agent i submits to the auction for good j at time t its solution to (2), expressed as a function of P j , assuming that the prices of goods other than j take their expected values. <p> Given the bidding behavior described, with expectations formed as by the simple competitive agent, the walras algorithm is guaranteed to converge to competitive equilibrium, under the standard conditions <ref> [5] </ref>. Such an equilibrium also represents a conjectural equilibrium, according to the definition above. Thus, the simple competitive learning regime is convergent, with respect to both the tatonnement and walras price adjustment protocols. 10 4 Learning Agents As defined above, agents learn when they modify their conjectures based on observations.
Reference: [6] <author> Drew Fudenberg and David K. Levine. </author> <title> Self-confirming equilibrium. </title> <journal> Econometrica, </journal> <volume> 61 </volume> <pages> 523-545, </pages> <year> 1993. </year>
Reference-contexts: As we see below, competitive, or Walrasian, equilibria are also con-jectural equilibria. The concept of self-confirming equilibrium <ref> [6] </ref> is another relaxation of Nash equilibrium which applies to a situation where no agent ever observes actions of other agents contradicting its beliefs. Conjectures are on the play of other agents, and must be correct for all reachable information sets. This is stronger than conjectural equilibrium in two respects.
Reference: [7] <author> Drew Fudenberg and Jean Tirole. </author> <title> Game Theory. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: When an agent has incomplete knowledge about the preference space of other agents, its interaction with them may not reveal their true preferences even over time. This situation differs from the traditional game theory framework <ref> [7, 8] </ref>, where the joint payoff matrix is known to every agent. 2 Uncertainty can be accommodated in the standard game-theoretic concept of incomplete information, where agents have probabilities over the payoffs of other agents.
Reference: [8] <author> Robert Gibbsons. </author> <title> Game Theory for Applied Economists. </title> <publisher> Princeton University Press, </publisher> <year> 1992. </year>
Reference-contexts: When an agent has incomplete knowledge about the preference space of other agents, its interaction with them may not reveal their true preferences even over time. This situation differs from the traditional game theory framework <ref> [7, 8] </ref>, where the joint payoff matrix is known to every agent. 2 Uncertainty can be accommodated in the standard game-theoretic concept of incomplete information, where agents have probabilities over the payoffs of other agents.
Reference: [9] <editor> John J. Grefenstette et al., editors. </editor> <booktitle> AAAI Spring Symposium on Adaptation, Coevolution, and Learning in Multiagent Systems. </booktitle> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Machine learning researchers have recently begun to investigate the special issues that multiagent environments present to the learning task. Recent workshops on the topic <ref> [9, 19] </ref>. have helped to frame research problems for the field. Multiagent environments are distinguished in particular by the fact that as the agents learn, they change their behavior, thus effectively changing the environment for all of the other agents.
Reference: [10] <author> Frank H. Hahn. </author> <title> Exercises in conjectural equilibrium analysis. </title> <journal> Scandi-navian Journal of Economics, </journal> <volume> 79 </volume> <pages> 210-226, </pages> <year> 1977. </year>
Reference-contexts: Boutilier [1] also considers a model where only outcomes are observable, comparing the effectiveness of alternate learning mechanisms for solving mul-tiagent coordination problems. The basic concept of conjectural equilibrium was first introduced by Hahn, in the context of a market model <ref> [10] </ref>. Though we also focus on market interactions, our central definition applies the concept to the more general case. <p> Using Hahn's notion of a conjecture function <ref> [10] </ref>, we provide some more general notation for characterizing the form of an agent's conjectures.
Reference: [11] <author> Reiner Horst, Panos Pardalos, and Nguyen Thoai. </author> <title> Introduction to Global Optimization. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995. </year>
Reference-contexts: Then there exists a solution to the following optimization problem: max U (z 1 ; : : : ; z m ) (12) m X (ff j + fi j z j )z j 0: (13) Proof : To establish the existence of an optimum, we apply Weierstrass's Maximum Theorem <ref> [11] </ref> : If S is a nonempty compact set in &lt; m , and f (x) is a continuous function on S, then f (x) has at least one global optimum point in S. By our assumption, the objective function U is continuous.
Reference: [12] <author> Junling Hu and Michael P. Wellman. </author> <title> Self-fulfilling bias in multiagent learning. </title> <booktitle> In Second International Conference on Multiagent Systems, </booktitle> <pages> pages 118-125, </pages> <address> Kyoto, Japan, </address> <year> 1996. </year>
Reference: [13] <author> J. O. Kephart, T. Hogg, and B. A. Huberman. </author> <title> Dynamics of computational ecosystems. </title> <journal> Physical Review A, </journal> <volume> 40 </volume> <pages> 404-421, </pages> <year> 1989. </year>
Reference-contexts: However, there is certainly a possibility of non-optimal expectations equilibrium even in this expanded setting. A simple lesson of this exercise is that attempting to be a little bit more sophisticated than the other agents can be a dangerous thing, especially if 10 Kephart et al. <ref> [13] </ref> describe another setting where sophisticated agents that try to anticipate the actions of others often make results worse for themselves. In this model, the sophisticated agents' downfall is their failure to account properly for simultaneous adaptation by the other agents. 22 one's learning method is prone to systematic bias.
Reference: [14] <author> Paul Milgrom and John Roberts. </author> <title> Adaptive and sophisticated learning in normal form games. </title> <journal> Games and Economic Behavior, </journal> <volume> 3 </volume> <pages> 82-100, </pages> <year> 1991. </year>
Reference-contexts: Although not strictly part of the solution concept, an agent behaving within an equilibrium is often explained in terms of the 2 agent's beliefs about the policies of other agents. How agents reach such beliefs through repeated interactions is what game theorists mean by learning <ref> [14] </ref>, and that is the sense of the term we adopt as well.
Reference: [15] <author> Takashi Negishi. </author> <title> The stability of a competitive economy: A survey article. </title> <journal> Econometrica, </journal> <volume> 30 </volume> <pages> 635-669, </pages> <year> 1962. </year>
Reference-contexts: If the aggregate demand obeys gross substitutability (an increase in the price of one good raises demand for others, which hence serve as substitutes), then this method is guaranteed to converge to a competitive equilibrium (under the conditions under which it is guaranteed to exist) <ref> [15] </ref>. The walras algorithm [5] is a variant of tatonnement. In walras, agent i submits to the auction for good j at time t its solution to (2), expressed as a function of P j , assuming that the prices of goods other than j take their expected values.
Reference: [16] <author> Norihiko Ono and Kenji Fukumoto. </author> <title> Multi-agent reinforcement learning: A modular approach. </title> <booktitle> In Second International Conference on Multiagent Systems, </booktitle> <pages> pages 252-258, </pages> <address> Kyoto, Japan, </address> <year> 1996. </year>
Reference-contexts: The basic idea of reinforcement learning is to revise beliefs and policies based on the success or failure of observed performance. The complication in a multiagent environment is that the rewards to alternate policies may change as other agents' beliefs evolve simultaneously <ref> [16] </ref>. If the environment is well structured and agents have some knowledge about this structure, it would seem advantageous to exploit that knowledge. In a multiagent environment, such structure may help in learning about other agents' policies.
Reference: [17] <author> Stuart Russell and Peter Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year> <month> 26 </month>
Reference-contexts: Here bias is defined as in the standard machine learning literature| the preference for one hypothesis over another, beyond mere consistency with the examples <ref> [17] </ref>. In reinforcement learning, the initial hypothesis is a source of bias, as is the hypothesis space (in multiagent environments, expressible models of the other agents).
Reference: [18] <author> Karl Samples. </author> <title> A note on the existence of starting point bias in iterative bidding games. </title> <journal> Western Journal of Agricultural Economics, </journal> <volume> 10 </volume> <pages> 32-40, </pages> <year> 1985. </year>
Reference-contexts: Our theoretical and experimental investigations show that even when convergence to a "correct" model obtains, improvement in result does not always follow. To our knowledge, the phenomenon of self-fulfilling bias as defined here has not been well studied in multiagent learning. Economists studying bidding games <ref> [18, 2] </ref> have noticed that biased starting bid prices strongly influence final bids. But these empirical findings have not been incorporated into a general framework in terms of learning.
Reference: [19] <author> Sandip Sen. </author> <title> IJCAI-95 Workshop on Adaptation and Learning in Mul-tiagent Systems. </title> <journal> AI Magazine, </journal> <volume> 17(1) </volume> <pages> 87-89, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Machine learning researchers have recently begun to investigate the special issues that multiagent environments present to the learning task. Recent workshops on the topic <ref> [9, 19] </ref>. have helped to frame research problems for the field. Multiagent environments are distinguished in particular by the fact that as the agents learn, they change their behavior, thus effectively changing the environment for all of the other agents.
Reference: [20] <author> John B. Shoven and John Whalley. </author> <title> Applying General Equilibrium. </title> <publisher> Cam-bridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: In the experiments, we generate agents with standard parametrized utility functions. The competitive agents have CES (constant elasticity of substitution) utility function|a common functional form in general equilibrium modeling <ref> [20] </ref>. A standard CES utility function is defined as U (x 1 ; : : : ; x m ) = @ j 1 1 : In our experiments, we set = 1 2 , and a j = 1 for all j.
Reference: [21] <author> Brandeis Spivak. </author> <title> Calculus on Manifolds. </title> <address> Benjamin/Cummings, </address> <year> 1965. </year>
Reference-contexts: Then by the Implicit Function Theorem <ref> [21] </ref>, there exists an open set P containing P fl and an open set B containing (P fl ; 0) such that for each P 2 P, there is a unique g (P ) 2 B such that F (P; g (P )) = 0.
Reference: [22] <author> A. Takayama. </author> <title> Mathematical Economics. </title> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference-contexts: The excess demand set for consumer i is Z i = fz i 2 &lt; m j e i + z i 2 X i g. A basic result of general equilibrium theory <ref> [22] </ref> states that if the utility function of every agent is quasiconcave and twice differentiable, then E has a unique competitive equilibrium. 4 Observe that any competitive equilibrium can be viewed as a conjectural equilibrium, for an appropriate interpretation of conjectures. <p> For example, the simple competitive agent takes the latest actual price as its expectation, ~ P i (t) = P (t 1): (3) 5 In the standard model, no exchanges are executed until the system reaches equilibrium. In so-called non-tatonnement processes <ref> [22] </ref>, agents can trade at any time, and so the endowment e is also a function of time.
Reference: [23] <author> Ming Tan. </author> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, MA, June 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The combination of a limited modeling language (in our experiments, linear demand functions) with an arbitrarily assigned initial hypothesis strongly influences the equilibrium state reached by the multiagent system. Much work on multiagent learning to date has investigated some form of reinforcement learning (e.g., <ref> [23, 24] </ref>). The basic idea of reinforcement learning is to revise beliefs and policies based on the success or failure of observed performance. The complication in a multiagent environment is that the rewards to alternate policies may change as other agents' beliefs evolve simultaneously [16].
Reference: [24] <author> Gerhard Wei. </author> <title> Learning to coordinate actions in multi-agent systems. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 311-316, </pages> <year> 1993. </year>
Reference-contexts: The combination of a limited modeling language (in our experiments, linear demand functions) with an arbitrarily assigned initial hypothesis strongly influences the equilibrium state reached by the multiagent system. Much work on multiagent learning to date has investigated some form of reinforcement learning (e.g., <ref> [23, 24] </ref>). The basic idea of reinforcement learning is to revise beliefs and policies based on the success or failure of observed performance. The complication in a multiagent environment is that the rewards to alternate policies may change as other agents' beliefs evolve simultaneously [16].
Reference: [25] <author> Michael P. Wellman. </author> <title> A market-oriented programming environment and its application to distributed multicommodity flow problems. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 1-22, </pages> <year> 1993. </year> <month> 27 </month>
Reference-contexts: The market context is generic enough to capture a wide range of interesting multiagent systems, yet affords analytically simple characterizations of conjectures and dynamics. Our model is based on the framework of general equilibrium theory from economics, and our implementation uses the walras market-oriented programming system <ref> [25] </ref>, which is also based on general equilibrium theory. 3.1 General Equilibrium Model Definition 2 A pure exchange economy, E fhX i ; U i ; e i i j i = 1; : : : ; ng, consists of n consumer agents, each defined by: 7 * a consumption set,
References-found: 25

