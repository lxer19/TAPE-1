URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/thrun.learning-robot-navg.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/thrun.learning-robot-navg.html
Root-URL: http://www.cs.cmu.edu
Email: E-mail: thrun@carbon.cs.bonn.edu  
Title: A Lifelong Learning Perspective for Mobile Robot Control  
Author: Sebastian Thrun 
Date: 1994  
Note: In: Proceedings of the IEEE/RSJ/GI Conference on Intelligent Robots and Systems,  
Address: Romerstr. 164, 53117 Bonn, Germany  
Affiliation: Universitat Bonn Institut fur Informatik III  
Abstract: Designing robots that learn by themselves to perform complex real-world tasks is a still-open challenge for the field of Robotics and Artificial Intelligence. In this paper we present the robot learning problem as a lifelong problem, in which a robot faces a collection of tasks over its entire lifetime. Such a scenario provides the opportunity to gather general-purpose knowledge that transfers across tasks. We illustrate a particular learning mechanism, explanation-based neural network learning, that transfers knowledge between related tasks via neural network action models. The learning approach is illustrated using a mobile robot, equipped with visual, ultrasonic and laser sensors. In less than 10 minutes operation time, the robot is able to learn to navigate to a marked target object in a natural office environment. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Richard Caruana. </author> <title> Multitask learning: A knowledge-based of source inductive bias. </title> <note> submitted for publication, </note> <year> 1993. </year>
Reference-contexts: Others proposed hierarchical approaches, in which the building blocks, once learned, can be applied to multiple tasks [3, 7, 21]. A third way for the transfer of knowledge is concerned with the construction of better internal representations, which improve generalization across multiple tasks <ref> [1, 16, 19, 23] </ref>.
Reference: [2] <author> Lonnie Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinction approach. </title> <booktitle> In Proceedings of 1992 AAAI Conference, </booktitle> <address> Menlo Park, CA, July 1992. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: 1 This restrictive assumption is frequently referred to as the Markov assumption. See <ref> [2, 7, 10, 22] </ref> for approaches to reinforcement learning in partially observable worlds. Sebastian Thrun A Lifelong Learning Perspective for Mobile Robot Control 3 the expected reward at a certain time t), which measures its current performance.
Reference: [3] <author> Peter Dayan and Geoffrey E. Hinton. </author> <title> Feudal reinforcement learning. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Ma-teo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, in the context of reinforcement learning, models have been successfully employed for the transfer of knowledge via planning [9, 25] or replay [7]. Others proposed hierarchical approaches, in which the building blocks, once learned, can be applied to multiple tasks <ref> [3, 7, 21] </ref>. A third way for the transfer of knowledge is concerned with the construction of better internal representations, which improve generalization across multiple tasks [1, 16, 19, 23].
Reference: [4] <author> Gerald DeJong and Raymond Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: This mechanism, which is called LOB*, has empirically been found to be important for successfully learning from weak action models [26]. Notice that EBNN employs a neural network version of Sebastian Thrun A Lifelong Learning Perspective for Mobile Robot Control 5 explanation-based learning <ref> [4, 12] </ref>. To summarize, EBNN employs action models to analyze training episodes, and to derive slopes that are used for generalizing these episodes. These slopes generalize training instances in input space, since they indicate how small changes will affect the target function, Q.
Reference: [5] <author> Vijaykumar Gullapalli, Judy A. Franklin, and Hamid Benbrahim. </author> <title> Acquiring robot skills via reinforcement learning. </title> <journal> IEEE Control Systems, </journal> <volume> 272(1708) </volume> <pages> 13-24, </pages> <month> February </month> <year> 1994. </year>
Reference: [6] <author> Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <type> Technical Report 9307, </type> <institution> Department of Brain and Cognitive Sciences, Massachusetts Institut of Technology, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Notice if Q (s; a) is represented by a lookup-table, the Q-Learning rule has been shown to converge to a value function that yields optimal policies (under certain conditions concerning the environment, the exploration scheme and the learning rate) <ref> [6, 28] </ref>. In the experiments reported in this paper, the number of actions is finite, and Q is represented by neural networks Q a , one for each action a2A. III.
Reference: [7] <author> Long-Ji Lin. </author> <title> Self-supervised Learning by Reinforcement and Artificial Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <year> 1992. </year>
Reference-contexts: 1 This restrictive assumption is frequently referred to as the Markov assumption. See <ref> [2, 7, 10, 22] </ref> for approaches to reinforcement learning in partially observable worlds. Sebastian Thrun A Lifelong Learning Perspective for Mobile Robot Control 3 the expected reward at a certain time t), which measures its current performance. <p> In all our experiments the action models were trained first, prior to learning Q, and frozen during learning control. When training the Q networks (46 input, eight hidden and one output unit), we explicitly memorized all training data, and used a replay technique similar to experience replay described in <ref> [7] </ref>. This procedure memorizes all past experiences explicitly. After each learning episode, it re-estimates the target values of the Q function by recursively replaying past episodes, as if they had just been observed. <p> The reader should notice that others have studies the transfer of knowledge. For example, in the context of reinforcement learning, models have been successfully employed for the transfer of knowledge via planning [9, 25] or replay <ref> [7] </ref>. Others proposed hierarchical approaches, in which the building blocks, once learned, can be applied to multiple tasks [3, 7, 21]. A third way for the transfer of knowledge is concerned with the construction of better internal representations, which improve generalization across multiple tasks [1, 16, 19, 23]. <p> For example, in the context of reinforcement learning, models have been successfully employed for the transfer of knowledge via planning [9, 25] or replay [7]. Others proposed hierarchical approaches, in which the building blocks, once learned, can be applied to multiple tasks <ref> [3, 7, 21] </ref>. A third way for the transfer of knowledge is concerned with the construction of better internal representations, which improve generalization across multiple tasks [1, 16, 19, 23].
Reference: [8] <author> Pattie Maes and Rodney A. Brooks. </author> <title> Learning to coordinate behaviors. </title> <booktitle> In Proceedings Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 796-802, </pages> <address> Cambridge, MA, 1990. </address> <publisher> AAAI, The MIT Press. </publisher>
Reference: [9] <author> Sridhar Mahadevan. </author> <title> Enhancing transfer in reinforcement learning by building stochastic models of robot actions. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 290-299. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: The reader should notice that others have studies the transfer of knowledge. For example, in the context of reinforcement learning, models have been successfully employed for the transfer of knowledge via planning <ref> [9, 25] </ref> or replay [7]. Others proposed hierarchical approaches, in which the building blocks, once learned, can be applied to multiple tasks [3, 7, 21].
Reference: [10] <author> R. Andrew McCallum. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <editor> In Paul E. Utgoff, editor, </editor> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 190-196, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 This restrictive assumption is frequently referred to as the Markov assumption. See <ref> [2, 7, 10, 22] </ref> for approaches to reinforcement learning in partially observable worlds. Sebastian Thrun A Lifelong Learning Perspective for Mobile Robot Control 3 the expected reward at a certain time t), which measures its current performance.
Reference: [11] <author> Tom M. Mitchell. </author> <title> Becoming increasingly reactive. </title> <booktitle> In Proceedings of 1990 AAAI Conference, </booktitle> <address> Menlo Park, CA, </address> <month> August </month> <year> 1990. </year> <booktitle> AAAI, </booktitle> <publisher> AAAI Press / The MIT Press. </publisher>
Reference: [12] <author> Tom M. Mitchell, Rich Keller, and Smadar Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: This mechanism, which is called LOB*, has empirically been found to be important for successfully learning from weak action models [26]. Notice that EBNN employs a neural network version of Sebastian Thrun A Lifelong Learning Perspective for Mobile Robot Control 5 explanation-based learning <ref> [4, 12] </ref>. To summarize, EBNN employs action models to analyze training episodes, and to derive slopes that are used for generalizing these episodes. These slopes generalize training instances in input space, since they indicate how small changes will affect the target function, Q.
Reference: [13] <author> Tom M. Mitchell and Sebastian B. Thrun. </author> <title> Explanation-based neural network learning for robot control. </title> <editor> In S. J. Hanson, J. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 287-294, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: From a lifelong learning perspective, much of the work presented in this paper is preliminary. While we have not yet studied robot control in the context of multiple tasks in practice, in experiments described here and elsewhere <ref> [13, 26, 27] </ref> we consistently found that EBNN outperforms pure inductive neural network learning, which does not employ background knowledge and hence learns from scratch. In a related paper we have illustrated superior generalization due to EBNN in a robot perception task [14].
Reference: [14] <author> Joseph O'Sullivan. </author> <title> Xavier manual. </title> <institution> Carnegie Mellon University, Learning Robot Lab Internal Document contact josullvn@cs.cmu.edu, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: In a related paper we have illustrated superior generalization due to EBNN in a robot perception task <ref> [14] </ref>. Learning mechanisms that allows for the effective knowledge transfer, like EBNN, is a necessary prerequisite for successful approaches to the lifelong robot learning problem. ACKNOWLEDGMENT The author thanks Tom Mitchell and the CMU robot learning group for their invaluable feedback.
Reference: [15] <author> D. A. Pomerleau. ALVINN: </author> <title> an autonomous land vehicle in a neural network. </title> <type> Technical Report CMU-CS-89-107, </type> <institution> Computer Science Dept. Carnegie Mellon University, </institution> <address> Pittsburgh PA, </address> <year> 1989. </year>
Reference: [16] <author> Lori Y. Pratt. </author> <title> Discriminability-based transfer between neural networks. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Others proposed hierarchical approaches, in which the building blocks, once learned, can be applied to multiple tasks [3, 7, 21]. A third way for the transfer of knowledge is concerned with the construction of better internal representations, which improve generalization across multiple tasks <ref> [1, 16, 19, 23] </ref>.
Reference: [17] <author> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing. </booktitle> <volume> Vol. I + II. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: In order to transfer knowledge, EBNN learns general-purpose predictive action models. Action models, denoted by M :SfiA!A, model the effect of the robots actions. In EBNN, these models are represented using artificial neural networks, which are trained with observed state transitions using the Backpropagation algorithm <ref> [17] </ref> (or EBNN itself). Once Sebastian Thrun A Lifelong Learning Perspective for Mobile Robot Control 4 and hx 3 ; f (x 3 )i are known. Based on these points the learner might generate the hypothesis g. <p> Hence, a combined error function E = patterns E value + ff E slope is minimized, which takes both value error, E value , and slope error, E slope , weighted by a gain parameter ff into account. Target values are fitted using the Backpropagation algorithm <ref> [17] </ref>. Target slopes are fitted using the Tangent-Prop algo rithm, which is an analogue of Backpropagation for fitting slopes [20]. Clearly, slopes extracted by EBNN can be wrong. This is because they are computed using artificial neural networks, which themselves are constructed from training examples.
Reference: [18] <author> Jacob T. Schwartz, Micha Scharir, and John Hopcroft. </author> <title> Planning, Geometry and Complexity of Robot Motion. </title> <publisher> Ablex Publishing Corporation, </publisher> <address> Norwood, NJ, </address> <year> 1987. </year>
Reference: [19] <author> Noel E. Sharkey and Amanda J.C. Sharkey. </author> <title> Adaptive generalization and the transfer of knowledge. </title> <booktitle> In Proceedings of the Second Irish Neural Networks Conference, </booktitle> <address> Belfast, </address> <year> 1992. </year>
Reference-contexts: Others proposed hierarchical approaches, in which the building blocks, once learned, can be applied to multiple tasks [3, 7, 21]. A third way for the transfer of knowledge is concerned with the construction of better internal representations, which improve generalization across multiple tasks <ref> [1, 16, 19, 23] </ref>.
Reference: [20] <author> Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. </author> <title> Tangent prop a formalism for specifying selected invariances in an adaptive network. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 895-903, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Target values are fitted using the Backpropagation algorithm [17]. Target slopes are fitted using the Tangent-Prop algo rithm, which is an analogue of Backpropagation for fitting slopes <ref> [20] </ref>. Clearly, slopes extracted by EBNN can be wrong. This is because they are computed using artificial neural networks, which themselves are constructed from training examples. Consequently, target slopes can mislead the generalization. EBNN provides a simple but effective mechanism to recover from malicious slopes.
Reference: [21] <author> Satinder P. Singh. </author> <title> Transfer of learning by composing solutions for elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <year> 1992. </year>
Reference-contexts: For example, in the context of reinforcement learning, models have been successfully employed for the transfer of knowledge via planning [9, 25] or replay [7]. Others proposed hierarchical approaches, in which the building blocks, once learned, can be applied to multiple tasks <ref> [3, 7, 21] </ref>. A third way for the transfer of knowledge is concerned with the construction of better internal representations, which improve generalization across multiple tasks [1, 16, 19, 23].
Reference: [22] <author> Satinder Pal Singh, Tommi Jaakkola, and Micheal I. Jordan. </author> <title> Learning without state-estimation in partially observable marko-vian decision processes. </title> <booktitle> In Proceedings of the Eleventh Machine Learning Conference, </booktitle> <year> 1994. </year> <note> (to appear). </note>
Reference-contexts: 1 This restrictive assumption is frequently referred to as the Markov assumption. See <ref> [2, 7, 10, 22] </ref> for approaches to reinforcement learning in partially observable worlds. Sebastian Thrun A Lifelong Learning Perspective for Mobile Robot Control 3 the expected reward at a certain time t), which measures its current performance.
Reference: [23] <author> Steven C. Suddarth and Y. L. Kergosien. </author> <title> Rule-injection hints as a means of improving network performance and learning time. </title> <booktitle> In Proceedings of the EURASIP Workshop on Neural Networks, </booktitle> <address> Sesimbra, Portugal, </address> <month> Feb </month> <year> 1990. </year> <month> EURASIP. </month>
Reference-contexts: Others proposed hierarchical approaches, in which the building blocks, once learned, can be applied to multiple tasks [3, 7, 21]. A third way for the transfer of knowledge is concerned with the construction of better internal representations, which improve generalization across multiple tasks <ref> [1, 16, 19, 23] </ref>.
Reference: [24] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <year> 1988. </year>
Reference-contexts: The exact update equation, combined with a modified temporal differenc ing rule <ref> [24] </ref>, is: Q target (s t ; a t ) = 8 &gt; &gt; &gt; &gt; &gt; &gt; : +100 if a t final action, robot reached goal 100 if a t final action, robot failed fl (1) max a action Q (s t+1 ; a) + Q target (s t+1
Reference: [25] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedingsof the Seventh International Conference on Machine Learning, </booktitle> <month> June </month> <year> 1990, </year> <pages> pages 216-224, </pages> <year> 1990. </year>
Reference-contexts: The reader should notice that others have studies the transfer of knowledge. For example, in the context of reinforcement learning, models have been successfully employed for the transfer of knowledge via planning <ref> [9, 25] </ref> or replay [7]. Others proposed hierarchical approaches, in which the building blocks, once learned, can be applied to multiple tasks [3, 7, 21].
Reference: [26] <author> Sebastian B. Thrun and Tom M. Mitchell. </author> <title> Integrating inductive neural network learning and explanation-based learning. </title> <booktitle> In Proceedings of IJCAI-93, </booktitle> <address> Chamberry, France, </address> <month> July </month> <year> 1993. </year> <title> IJCAI, </title> <publisher> Inc. </publisher>
Reference-contexts: Notice that the EBNN algorithm is a general learning algorithm, albeit the fact that EBNN is described in the context of Q-Learning. A more detailed description can be found in <ref> [26, 27] </ref>. In order to transfer knowledge, EBNN learns general-purpose predictive action models. Action models, denoted by M :SfiA!A, model the effect of the robots actions. <p> When refining the weights of the Q-network, the learning rate for the tth slope is multiplied with ff t . This mechanism, which is called LOB*, has empirically been found to be important for successfully learning from weak action models <ref> [26] </ref>. Notice that EBNN employs a neural network version of Sebastian Thrun A Lifelong Learning Perspective for Mobile Robot Control 5 explanation-based learning [4, 12]. To summarize, EBNN employs action models to analyze training episodes, and to derive slopes that are used for generalizing these episodes. <p> From a lifelong learning perspective, much of the work presented in this paper is preliminary. While we have not yet studied robot control in the context of multiple tasks in practice, in experiments described here and elsewhere <ref> [13, 26, 27] </ref> we consistently found that EBNN outperforms pure inductive neural network learning, which does not employ background knowledge and hence learns from scratch. In a related paper we have illustrated superior generalization due to EBNN in a robot perception task [14].
Reference: [27] <author> Sebastian B. Thrun and Tom M. Mitchell. </author> <title> Lifelong robot learning. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <year> 1993. </year> <note> (to appear). Also appeared as Technical Report IAI-TR-93-7, </note> <institution> University of Bonn, Dept. of Computer Science III. </institution>
Reference-contexts: Notice that the EBNN algorithm is a general learning algorithm, albeit the fact that EBNN is described in the context of Q-Learning. A more detailed description can be found in <ref> [26, 27] </ref>. In order to transfer knowledge, EBNN learns general-purpose predictive action models. Action models, denoted by M :SfiA!A, model the effect of the robots actions. <p> White boxes refer to negative and black boxes to positive values. Box sizes indicate absolute magnitudes. Notice the bulk of positive gradients along the main diagonal, and the cross-dependencies between different sensors. a bird eye's view, using a sonar map techniques described in <ref> [27] </ref>. In all cases Xavier learned to navigate to a static target location in less than 19 episodes (with action models) and 24 episodes (without action models). Each episode was between two and eleven actions in length. <p> Note that the location of the target object (marked by a cross) is held constant in this experiment. robot was also blocked by obstacles. Unlike plain inductive neural network learning, EBNN almost always manages these cases successfully. tance for transferring knowledge for scaling machine learning to more complex domains <ref> [27] </ref>. From a lifelong learning perspective, much of the work presented in this paper is preliminary. <p> From a lifelong learning perspective, much of the work presented in this paper is preliminary. While we have not yet studied robot control in the context of multiple tasks in practice, in experiments described here and elsewhere <ref> [13, 26, 27] </ref> we consistently found that EBNN outperforms pure inductive neural network learning, which does not employ background knowledge and hence learns from scratch. In a related paper we have illustrated superior generalization due to EBNN in a robot perception task [14].
Reference: [28] <author> Chris J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: Notice if Q (s; a) is represented by a lookup-table, the Q-Learning rule has been shown to converge to a value function that yields optimal policies (under certain conditions concerning the environment, the exploration scheme and the learning rate) <ref> [6, 28] </ref>. In the experiments reported in this paper, the number of actions is finite, and Q is represented by neural networks Q a , one for each action a2A. III.
References-found: 28

