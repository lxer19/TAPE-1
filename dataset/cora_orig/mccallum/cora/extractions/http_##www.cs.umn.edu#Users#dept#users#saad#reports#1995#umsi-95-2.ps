URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1995/umsi-95-2.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1995/
Root-URL: http://www.cs.umn.edu
Keyword: NUMERICAL SOLUTION OF MARKOV CHAINS  
Note: 2ND INTERNATIONAL WORKSHOP ON THE  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. E. </author> <title> Arnoldi. The principle of minimized iteration in the solution of the matrix eigenvalue problem. </title> <journal> Quart. Appl. Math., </journal> <volume> 9 </volume> <pages> 17-29, </pages> <year> 1951. </year>
Reference-contexts: Orthogonal Projection Methods: L m = K m = K m (A; r 0 ). This is the orthogonal projection or Galerkin case. A method in this class is the Full Orthogonalization Method (FOM) [18] which is closely related to Arnoldi's method for solving eigenvalue problems <ref> [1] </ref>. Also in this class is ORTHORES [12], a method that is mathematically equivalent to FOM. Axelsson [2] also 4 Chapter 1 derived an algorithm of this class for general nonsymmetric matrices. <p> Enddo Define Z m := [z 1 ; ::::; z m ]. 3. Form the approximate solution: Compute x m = x 0 + Z m y m where y m = argmin y kfie 1 H m yk 2 and e 1 = <ref> [1; 0; : : :; 0] </ref> T . 4. Restart: If satisfied stop, else set x 0 x m and goto 2.
Reference: [2] <author> O. Axelsson. </author> <title> Conjugate gradient type-methods for unsymmetric and inconsistent systems of linear equations. </title> <journal> Linear Algebra Appl., </journal> <volume> 29 </volume> <pages> 1-16, </pages> <year> 1980. </year>
Reference-contexts: This is the orthogonal projection or Galerkin case. A method in this class is the Full Orthogonalization Method (FOM) [18] which is closely related to Arnoldi's method for solving eigenvalue problems [1]. Also in this class is ORTHORES [12], a method that is mathematically equivalent to FOM. Axelsson <ref> [2] </ref> also 4 Chapter 1 derived an algorithm of this class for general nonsymmetric matrices.
Reference: [3] <author> O. Axelsson. </author> <title> A generalized conjugate gradient, least squares method. </title> <journal> Num. Math., </journal> <volume> 51 </volume> <pages> 209-227, </pages> <year> 1987. </year>
Reference-contexts: In contrast, there is no similar optimality property known for methods of the first class when A in nonsymmetric. Because of this, many methods of this type have been derived for the nonsymmetric case <ref> [3, 12, 7, 27] </ref>. The Conjugate Residual method [4] is the analogue of conjugate gradient method that is in this class. The GMRES algorithm [27] and the Generalized Conjugate Residual algorithms [7] is an extension of the Conjugate Residual method to nonsymmetric problems.
Reference: [4] <author> R. Chandra. </author> <title> Conjugate Gradient Methods for Partial Differential Equations. </title> <type> PhD thesis, </type> <institution> Yale University, Computer Science Dept., </institution> <address> New Haven, CT. 06520, </address> <year> 1978. </year>
Reference-contexts: In contrast, there is no similar optimality property known for methods of the first class when A in nonsymmetric. Because of this, many methods of this type have been derived for the nonsymmetric case [3, 12, 7, 27]. The Conjugate Residual method <ref> [4] </ref> is the analogue of conjugate gradient method that is in this class. The GMRES algorithm [27] and the Generalized Conjugate Residual algorithms [7] is an extension of the Conjugate Residual method to nonsymmetric problems.
Reference: [5] <author> I. S. Duff and G. A. Meurant. </author> <title> The effect of reordering on preconditioned conjugate gradients. </title> <journal> BIT, </journal> <volume> 29 </volume> <pages> 635-657, </pages> <year> 1989. </year>
Reference-contexts: When * = 0, then the higher the parameter p, the more accurate the factorization. Reordering for reducing fill-in can help improve the quality of 10 Chapter 1 the factorization, and we refer to <ref> [5] </ref> and [6] for similar experiments performed with the ILU (0) factorization. 4 EXPERIMENTS We compared a few preconditioned Krylov subspace techniques on four test matrices generated by the software package MARCA [30]. These examples are variants of the ones used in the numerical experiments of [15].
Reference: [6] <author> L. C. Dutto. </author> <title> The effect of reordering on the preconditioned GMRES algorithm for solving the compressible Navier-Stokes equations. </title> <journal> International Journal for Numerical Methods in Engineering, </journal> <volume> 36 </volume> <pages> 457-497, </pages> <year> 1993. </year>
Reference-contexts: When * = 0, then the higher the parameter p, the more accurate the factorization. Reordering for reducing fill-in can help improve the quality of 10 Chapter 1 the factorization, and we refer to [5] and <ref> [6] </ref> for similar experiments performed with the ILU (0) factorization. 4 EXPERIMENTS We compared a few preconditioned Krylov subspace techniques on four test matrices generated by the software package MARCA [30]. These examples are variants of the ones used in the numerical experiments of [15].
Reference: [7] <author> S. C. Eisenstat, H. C. Elman, and M. H. Schultz. </author> <title> Variational iterative methods for nonsymmetric systems of linear equations. </title> <journal> SIAM J Num. Anal., </journal> <volume> 20 </volume> <pages> 345-357, </pages> <year> 1983. </year>
Reference-contexts: In contrast, there is no similar optimality property known for methods of the first class when A in nonsymmetric. Because of this, many methods of this type have been derived for the nonsymmetric case <ref> [3, 12, 7, 27] </ref>. The Conjugate Residual method [4] is the analogue of conjugate gradient method that is in this class. The GMRES algorithm [27] and the Generalized Conjugate Residual algorithms [7] is an extension of the Conjugate Residual method to nonsymmetric problems. <p> Because of this, many methods of this type have been derived for the nonsymmetric case [3, 12, 7, 27]. The Conjugate Residual method [4] is the analogue of conjugate gradient method that is in this class. The GMRES algorithm [27] and the Generalized Conjugate Residual algorithms <ref> [7] </ref> is an extension of the Conjugate Residual method to nonsymmetric problems. Bi-conjugate gradient: L m = K m (A T ; r 0 ); K m = K m (A; r 0 ). Clearly, in the symmetric case this class of methods reduces to the first one.
Reference: [8] <author> R. Fletcher. </author> <title> Conjugate gradient methods for indefinite systems. </title> <editor> In G. A. Watson, editor, </editor> <booktitle> Proceedings of the Dundee Biennal Conference on Numerical Analysis 1974, </booktitle> <pages> pages 73-89, </pages> <address> New York, 1975. </address> <publisher> University of Dundee,Scotland, Springer Verlag. </publisher>
Reference-contexts: Clearly, in the symmetric case this class of methods reduces to the first one. In the nonsymmetric case, the biconjugate gradient method (BCG) due to Lanczos [13] and Fletcher <ref> [8] </ref> is a good representative of this class. There are various mathematically equivalent formulations of the biconjugate gradient method [19], some of which are more numerically viable than others. An efficient variation on this method, called CGS (Conjugate gradient squared) was proposed by Sonneveld [29, 17].
Reference: [9] <author> Roland W. Freund. </author> <title> A Transpose-Free Quasi-Minimal Residual algorithm for non-Hermitian linear systems. </title> <journal> SIAM J. Sci. Comp., </journal> <volume> 14(2) </volume> <pages> 470-482, </pages> <year> 1993. </year>
Reference-contexts: An efficient variation on this method, called CGS (Conjugate gradient squared) was proposed by Sonneveld [29, 17]. More recently, a number of methods based on the CGS principle have been developed, see for example the TFQMR algorithm <ref> [9] </ref> and the BiCGSTAB algorithm [32]. CGNR: L m = K m = K m (A T A; A T r 0 ). This is nothing but the conjugate gradient method applied to the normal equations A T Ax = A T b, often referred to as CGNR. <p> BiCGSTAB. This a variant of CGS [29] developed by Van-der Vorst [32]. This is denoted by BCGST in the table. TFQMR. This is yet another variant of CGS which use quasi-minimization. The algorithm was developed by Freund <ref> [9] </ref>. Full Orthogonalization Method (FOM), using a Krylov subspace of dimension 15. Generalized Minimum Residual method (GMRES) using a Krylov sub-space of dimension 15. Direct Quasi-Generalized Minimum Residual method (DQGMRES). This consists of truncating the Arnoldi sequence during the orthogonalization.
Reference: [10] <author> E. Gallopoulos and Y. Saad. </author> <title> Efficient solution of parabolic equations by polynomial approximation methods. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 13 </volume> <pages> 1236-1264, </pages> <year> 1992. </year>
Reference-contexts: The problem here is to solve the simple system of Ordinary Differential Equations y 0 = Qy, whose exact solution y (t) = exp (Qt)y 0 can be approximated using a Krylov subspace approach. Thus, the Krylov sub-spaces are used to approximate the exponential propagator exp (Qt)y directly <ref> [21, 10] </ref>. This approach has been shown to be quite successful in a recent paper [16]. In this paper we will give an overview of preconditioned Krylov subspace methods. We will emphasize the preconditioning techniques since these are currently the most critical component of a preconditioned Krylov approach.
Reference: [11] <author> G. H. Golub and C. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: EndDo EndDo EndDo From an implementation viewpoint, this is an (i; k; j) version of Gaussian elimination <ref> [11] </ref> which is restricted to the N Z (A) part of the matrix. This algorithm Preconditioned Krylov Subspace Methods 9 can be generalized to any preset nonzero pattern.
Reference: [12] <author> K. C. Jea and D. M. Young. </author> <title> Generalized conjugate gradient acceleration of nonsymmetrizable iterative methods. </title> <journal> Linear Algebra Appl., </journal> <volume> 34 </volume> <pages> 159-194, </pages> <year> 1980. </year> <note> 16 Chapter 1 </note>
Reference-contexts: This is the orthogonal projection or Galerkin case. A method in this class is the Full Orthogonalization Method (FOM) [18] which is closely related to Arnoldi's method for solving eigenvalue problems [1]. Also in this class is ORTHORES <ref> [12] </ref>, a method that is mathematically equivalent to FOM. Axelsson [2] also 4 Chapter 1 derived an algorithm of this class for general nonsymmetric matrices. <p> In contrast, there is no similar optimality property known for methods of the first class when A in nonsymmetric. Because of this, many methods of this type have been derived for the nonsymmetric case <ref> [3, 12, 7, 27] </ref>. The Conjugate Residual method [4] is the analogue of conjugate gradient method that is in this class. The GMRES algorithm [27] and the Generalized Conjugate Residual algorithms [7] is an extension of the Conjugate Residual method to nonsymmetric problems.
Reference: [13] <author> C. </author> <title> Lanczos. Solution of systems of linear equations by minimized iterations. </title> <journal> J. Res. Nat. Bur. Standards, </journal> <volume> 49 </volume> <pages> 33-53, </pages> <year> 1952. </year>
Reference-contexts: Bi-conjugate gradient: L m = K m (A T ; r 0 ); K m = K m (A; r 0 ). Clearly, in the symmetric case this class of methods reduces to the first one. In the nonsymmetric case, the biconjugate gradient method (BCG) due to Lanczos <ref> [13] </ref> and Fletcher [8] is a good representative of this class. There are various mathematically equivalent formulations of the biconjugate gradient method [19], some of which are more numerically viable than others. An efficient variation on this method, called CGS (Conjugate gradient squared) was proposed by Sonneveld [29, 17].
Reference: [14] <author> C. C. Paige and M. A. Saunders. </author> <title> An algorithm for sparse linear equations and sparse least squares. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 8 </volume> <pages> 43-71, </pages> <year> 1982. </year>
Reference-contexts: The condition number of the normal equations is likely to be too large for most problems to make this approach competitive with the approaches 1 to 3, except possibly for indefinite problems, i.e., problems for which the symmetric part is not positive definite. LSQR <ref> [14] </ref> is an implementation that is somewhat less sensitive to large condition numbers. Moreover, for least squares problems with non-square matrices, one must either explicitly or implicitly use an approach based on the normal equations.
Reference: [15] <author> B. Philippe, Y. Saad, and W. J. Stewart. </author> <title> Numerical methods in Markov chain modeling. </title> <journal> Journal of Operations Research, </journal> <volume> 40(6) </volume> <pages> 1156-1179, </pages> <year> 1992. </year>
Reference-contexts: Among the many possible options available to precondition a system, are to use a standard iterations, e.g., SOR, or one of several known `incomplete LU' factorizations of the matrix <ref> [15] </ref>. In this context, the standard and inexpensive approaches, such as ILU (0), or SSOR, have a great rate of failure for the harder, nearly decomposable cases. However, alternatives based on the more accurate factorizations, such as ILUT [22] yield more robust techniques. <p> These examples are variants of the ones used in the numerical experiments of <ref> [15] </ref>. All experiments have been performed on a Sparcstation 10 in double precision. 4.1 The test problems The first example models the behavior of a multiprogrammed, time-shared, virtual memory computer system consisting of N terminals, a CPU, a secondary memory, and a filing device. This example was also used in [15]. <p> <ref> [15] </ref>. All experiments have been performed on a Sparcstation 10 in double precision. 4.1 The test problems The first example models the behavior of a multiprogrammed, time-shared, virtual memory computer system consisting of N terminals, a CPU, a secondary memory, and a filing device. This example was also used in [15]. However, one difference with the example in [15] is that we increased the number of users to n = 30 from n = 20. This gives rise to a matrix of size 5; 456. Our second matrix is also taken from [15] and was generated using Marca. <p> This example was also used in <ref> [15] </ref>. However, one difference with the example in [15] is that we increased the number of users to n = 30 from n = 20. This gives rise to a matrix of size 5; 456. Our second matrix is also taken from [15] and was generated using Marca. <p> This example was also used in <ref> [15] </ref>. However, one difference with the example in [15] is that we increased the number of users to n = 30 from n = 20. This gives rise to a matrix of size 5; 456. Our second matrix is also taken from [15] and was generated using Marca. This is a telecommunicatoin model which has been used to to determine the effect of impatient telephone customers. When a request is made by a customer for service the customer is prepared to wait for a certain period of time for a reply. <p> The sizes and number of nonzero elements of all four matrices are shown in Table 1. For details on these problems and their origins, see <ref> [15] </ref> and the references therein.
Reference: [16] <author> B. Philippe and R. B. Sidje. </author> <title> Transient solutions of Markov processes by Krylov subspaces. </title> <type> Technical Report 736, </type> <institution> IRISA, University of Rennes, Campus de Beaulieu, Rennes, France, </institution> <year> 1993. </year>
Reference-contexts: Thus, the Krylov sub-spaces are used to approximate the exponential propagator exp (Qt)y directly [21, 10]. This approach has been shown to be quite successful in a recent paper <ref> [16] </ref>. In this paper we will give an overview of preconditioned Krylov subspace methods. We will emphasize the preconditioning techniques since these are currently the most critical component of a preconditioned Krylov approach. The ideas presented herein constitute only a brief introduction in the most part.
Reference: [17] <author> S. J. Polak, C. Den Heijer, W. H. A. Schilders, and P. Markowich. </author> <title> Semiconductor device modelling from the numerical point of view. </title> <journal> Internat. J. Numer. Meth. Eng., </journal> <volume> 24 </volume> <pages> 763-838, </pages> <year> 1987. </year>
Reference-contexts: There are various mathematically equivalent formulations of the biconjugate gradient method [19], some of which are more numerically viable than others. An efficient variation on this method, called CGS (Conjugate gradient squared) was proposed by Sonneveld <ref> [29, 17] </ref>. More recently, a number of methods based on the CGS principle have been developed, see for example the TFQMR algorithm [9] and the BiCGSTAB algorithm [32]. CGNR: L m = K m = K m (A T A; A T r 0 ).
Reference: [18] <author> Y. Saad. </author> <title> Krylov subspace methods for solving large unsymmetric linear systems. </title> <journal> Mathematics of Computation, </journal> <volume> 37 </volume> <pages> 105-126, </pages> <year> 1981. </year>
Reference-contexts: Specifically, the most popular choices of K m and L m are the following. Orthogonal Projection Methods: L m = K m = K m (A; r 0 ). This is the orthogonal projection or Galerkin case. A method in this class is the Full Orthogonalization Method (FOM) <ref> [18] </ref> which is closely related to Arnoldi's method for solving eigenvalue problems [1]. Also in this class is ORTHORES [12], a method that is mathematically equivalent to FOM. Axelsson [2] also 4 Chapter 1 derived an algorithm of this class for general nonsymmetric matrices.
Reference: [19] <author> Y. Saad. </author> <title> The Lanczos biorthogonalization algorithm and other oblique projection methods for solving large unsymmetric systems. </title> <journal> SIAM J. Nu-mer. Anal., </journal> <volume> 19 </volume> <pages> 470-484, </pages> <year> 1982. </year>
Reference-contexts: Clearly, in the symmetric case this class of methods reduces to the first one. In the nonsymmetric case, the biconjugate gradient method (BCG) due to Lanczos [13] and Fletcher [8] is a good representative of this class. There are various mathematically equivalent formulations of the biconjugate gradient method <ref> [19] </ref>, some of which are more numerically viable than others. An efficient variation on this method, called CGS (Conjugate gradient squared) was proposed by Sonneveld [29, 17].
Reference: [20] <author> Y. Saad. </author> <title> Practical use of some Krylov subspace methods for solving indefinite and unsymmetric linear systems. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 5 </volume> <pages> 203-228, </pages> <year> 1984. </year>
Reference-contexts: CGNR; Equivalent to Conjugate Gradient applied to A T Ax = A b ; Bi-Conjugate gradient (bcg in the table). DBCG. This is a Lanczos implementation of the biCG algorithm in which partial pivoting is applied when solving the tridiagonal systems, <ref> [20] </ref>. BiCGSTAB. This a variant of CGS [29] developed by Van-der Vorst [32]. This is denoted by BCGST in the table. TFQMR. This is yet another variant of CGS which use quasi-minimization. The algorithm was developed by Freund [9]. Full Orthogonalization Method (FOM), using a Krylov subspace of dimension 15.
Reference: [21] <author> Y. Saad. </author> <title> Analysis of some Krylov subspace approximations to the matrix exponential operator. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 29 </volume> <pages> 209-228, </pages> <year> 1992. </year>
Reference-contexts: The problem here is to solve the simple system of Ordinary Differential Equations y 0 = Qy, whose exact solution y (t) = exp (Qt)y 0 can be approximated using a Krylov subspace approach. Thus, the Krylov sub-spaces are used to approximate the exponential propagator exp (Qt)y directly <ref> [21, 10] </ref>. This approach has been shown to be quite successful in a recent paper [16]. In this paper we will give an overview of preconditioned Krylov subspace methods. We will emphasize the preconditioning techniques since these are currently the most critical component of a preconditioned Krylov approach.
Reference: [22] <author> Y. Saad. ILUT: </author> <title> a dual threshold incomplete ILU factorization. </title> <type> Technical Report 92-38, </type> <institution> Minnesota Supercomputer Institute, University of Min-nesota, Minneapolis, </institution> <year> 1992. </year> <note> to appear. </note>
Reference-contexts: In this context, the standard and inexpensive approaches, such as ILU (0), or SSOR, have a great rate of failure for the harder, nearly decomposable cases. However, alternatives based on the more accurate factorizations, such as ILUT <ref> [22] </ref> yield more robust techniques. In terms of overall cost, the more accurate ILU factorizations are usually less expensive. However, they do require more storage, since denser LU factors must be saved. Krylov subspace methods can also be used to compute transient solutions of Markov chain models. <p> However, this property is no longer true for non-diagonally dominant matrices. Another strategy altogether is to use the same general structure of the ILU factorization, namely the i; k; j variant of Gaussian Elimination, and to drop elements according to their magnitude. One such strategy defined in <ref> [22] </ref>, was referred to as ILUT (ILU with threshold).
Reference: [23] <author> Y. Saad. </author> <title> Numerical Methods for Large Eigenvalue Problems. </title> <publisher> Halstead Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: We will emphasize the preconditioning techniques since these are currently the most critical component of a preconditioned Krylov approach. The ideas presented herein constitute only a brief introduction in the most part. For details, the reader is referred to the recent literature, in particular, see e.g., [31], <ref> [25, 23] </ref>. Preconditioned Krylov Subspace Methods 3 2 PRECONDITIONED KRYLOV SUBSPACE METHODS Iterative techniques based on Krylov subspace projection coupled with suitable preconditioners are currently considered to be the best compromise between efficiency and robustness in solving general sparse linear systems.
Reference: [24] <author> Y. Saad. </author> <title> A flexible inner-outer preconditioned GMRES algorithm. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 14 </volume> <pages> 461-469, </pages> <year> 1993. </year>
Reference-contexts: The main practical difference between these two approaches is that in the right-preconditioned case, the actual residual norm is available at each step of the iterative process whereas in the second case only the preconditioned residual is explicitly available. Here we will describe a method presented in <ref> [24] </ref> and known as FGMRES which can be used when the preconditioning operations varies from step to step, a property which can be very useful. <p> The approximate solution x m obtained from this modified algorithm minimizes the residual norm kb Ax m k 2 over x 0 + SpanfZ m g, <ref> [24] </ref>. In addition, if at a given step k, we have Az k = v k (i.e., if the preconditioning is `exact' at step k) and if the k fi k Hessenberg matrix H k = fh ij g i;j=1;:::;k is nonsingular then the approximation x k is exact.
Reference: [25] <author> Y. Saad. </author> <title> Iterative methods for sparse linear systems. </title> <publisher> PWS publishing, </publisher> <address> New York, </address> <year> 1995. </year> <note> to appear. </note>
Reference-contexts: We will emphasize the preconditioning techniques since these are currently the most critical component of a preconditioned Krylov approach. The ideas presented herein constitute only a brief introduction in the most part. For details, the reader is referred to the recent literature, in particular, see e.g., [31], <ref> [25, 23] </ref>. Preconditioned Krylov Subspace Methods 3 2 PRECONDITIONED KRYLOV SUBSPACE METHODS Iterative techniques based on Krylov subspace projection coupled with suitable preconditioners are currently considered to be the best compromise between efficiency and robustness in solving general sparse linear systems.
Reference: [26] <author> Y. Saad and M. H. Schultz. </author> <title> Conjugate gradient-like algorithms for solving nonsymmetric linear systems. </title> <journal> Mathematics of Computation, </journal> <volume> 44(170) </volume> <pages> 417-424, </pages> <year> 1985. </year> <title> Preconditioned Krylov Subspace Methods 17 </title>
Reference-contexts: In this case, FOM is mathematically to the conjugate gradient method. Minimum Residual methods: L m = AK m ; K m = K m (A; r 0 ). With this choice of L m , it can be shown, see e.g., <ref> [26] </ref> that the approximate solution x m minimizes the residual norm kb Axk 2 over all candidate vectors in x 0 + K m . In contrast, there is no similar optimality property known for methods of the first class when A in nonsymmetric.
Reference: [27] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: In contrast, there is no similar optimality property known for methods of the first class when A in nonsymmetric. Because of this, many methods of this type have been derived for the nonsymmetric case <ref> [3, 12, 7, 27] </ref>. The Conjugate Residual method [4] is the analogue of conjugate gradient method that is in this class. The GMRES algorithm [27] and the Generalized Conjugate Residual algorithms [7] is an extension of the Conjugate Residual method to nonsymmetric problems. <p> Because of this, many methods of this type have been derived for the nonsymmetric case [3, 12, 7, 27]. The Conjugate Residual method [4] is the analogue of conjugate gradient method that is in this class. The GMRES algorithm <ref> [27] </ref> and the Generalized Conjugate Residual algorithms [7] is an extension of the Conjugate Residual method to nonsymmetric problems. Bi-conjugate gradient: L m = K m (A T ; r 0 ); K m = K m (A; r 0 ). <p> Here we will describe a method presented in [24] and known as FGMRES which can be used when the preconditioning operations varies from step to step, a property which can be very useful. This variant is derived by observing that in the last step of the standard GMRES algorithm <ref> [27] </ref>, the approximate solution x m is formed as x m = x 0 + i=1 Here, the v i 's are the Arnoldi vectors, M the preconditioner, x 0 the initial guess and m the dimension of the Krylov subspace.
Reference: [28] <author> Y. Saad and K. Wu. DQGMRES: </author> <title> a quasi-minimal residual algorithm based on incomplete orthogonalization. </title> <type> Technical Report UMSI-93/131, </type> <institution> Minnesota Supercomputing Institute, Minneapolis,MN, </institution> <year> 1993. </year> <note> submitted. </note>
Reference-contexts: Full Orthogonalization Method (FOM), using a Krylov subspace of dimension 15. Generalized Minimum Residual method (GMRES) using a Krylov sub-space of dimension 15. Direct Quasi-Generalized Minimum Residual method (DQGMRES). This consists of truncating the Arnoldi sequence during the orthogonalization. A quasi-minimization is then performed in the spirit of QMR <ref> [28] </ref>. In our example, we keep the 15 most recent Arnoldi directions. This is referred to as dqg in the table. In Table 2 we show for each matrix the number of matrix-vector products and the total CPU time required to converge.
Reference: [29] <author> P. Sonneveld. </author> <title> CGS, a fast Lanczos-type solver for nonsymmetric linear systems. </title> <journal> SIAM J. Scient. Statist. Comput., </journal> <volume> 10(1) </volume> <pages> 36-52, </pages> <year> 1989. </year>
Reference-contexts: There are various mathematically equivalent formulations of the biconjugate gradient method [19], some of which are more numerically viable than others. An efficient variation on this method, called CGS (Conjugate gradient squared) was proposed by Sonneveld <ref> [29, 17] </ref>. More recently, a number of methods based on the CGS principle have been developed, see for example the TFQMR algorithm [9] and the BiCGSTAB algorithm [32]. CGNR: L m = K m = K m (A T A; A T r 0 ). <p> CGNR; Equivalent to Conjugate Gradient applied to A T Ax = A b ; Bi-Conjugate gradient (bcg in the table). DBCG. This is a Lanczos implementation of the biCG algorithm in which partial pivoting is applied when solving the tridiagonal systems, [20]. BiCGSTAB. This a variant of CGS <ref> [29] </ref> developed by Van-der Vorst [32]. This is denoted by BCGST in the table. TFQMR. This is yet another variant of CGS which use quasi-minimization. The algorithm was developed by Freund [9]. Full Orthogonalization Method (FOM), using a Krylov subspace of dimension 15.
Reference: [30] <author> W. J. Stewart. MARCA: </author> <title> markov chain analyzer, a software package for markov modeling. </title> <editor> In W. J. Stewart, editor, </editor> <title> Numerical solution of Markov chains, </title> <address> pages 37-62, New-York, 1991. </address> <publisher> Marcel Dekker Inc. </publisher>
Reference-contexts: for reducing fill-in can help improve the quality of 10 Chapter 1 the factorization, and we refer to [5] and [6] for similar experiments performed with the ILU (0) factorization. 4 EXPERIMENTS We compared a few preconditioned Krylov subspace techniques on four test matrices generated by the software package MARCA <ref> [30] </ref>. These examples are variants of the ones used in the numerical experiments of [15].
Reference: [31] <author> W. J. Stewart. </author> <title> Introduction to the Numerical Solution of Markov Chains. </title> <publisher> Princeton University Press, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: We will emphasize the preconditioning techniques since these are currently the most critical component of a preconditioned Krylov approach. The ideas presented herein constitute only a brief introduction in the most part. For details, the reader is referred to the recent literature, in particular, see e.g., <ref> [31] </ref>, [25, 23]. Preconditioned Krylov Subspace Methods 3 2 PRECONDITIONED KRYLOV SUBSPACE METHODS Iterative techniques based on Krylov subspace projection coupled with suitable preconditioners are currently considered to be the best compromise between efficiency and robustness in solving general sparse linear systems.
Reference: [32] <author> H. A. van der Vorst. </author> <title> Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of non-symmetric linear systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 12 </volume> <pages> 631-644, </pages> <year> 1992. </year>
Reference-contexts: An efficient variation on this method, called CGS (Conjugate gradient squared) was proposed by Sonneveld [29, 17]. More recently, a number of methods based on the CGS principle have been developed, see for example the TFQMR algorithm [9] and the BiCGSTAB algorithm <ref> [32] </ref>. CGNR: L m = K m = K m (A T A; A T r 0 ). This is nothing but the conjugate gradient method applied to the normal equations A T Ax = A T b, often referred to as CGNR. <p> DBCG. This is a Lanczos implementation of the biCG algorithm in which partial pivoting is applied when solving the tridiagonal systems, [20]. BiCGSTAB. This a variant of CGS [29] developed by Van-der Vorst <ref> [32] </ref>. This is denoted by BCGST in the table. TFQMR. This is yet another variant of CGS which use quasi-minimization. The algorithm was developed by Freund [9]. Full Orthogonalization Method (FOM), using a Krylov subspace of dimension 15. Generalized Minimum Residual method (GMRES) using a Krylov sub-space of dimension 15.
Reference: [33] <author> J. W. Watts-III. </author> <title> A conjugate gradient truncated direct method for the iterative solution of the reservoir simulation pressure equation. </title> <journal> Society of Petroleum Engineer Journal, </journal> <volume> 21 </volume> <pages> 345-353, </pages> <year> 1981. </year>
Reference-contexts: This algorithm Preconditioned Krylov Subspace Methods 9 can be generalized to any preset nonzero pattern. In particular, one can classify the fill-ins by assigning them a level which is defined from the parents which generated the element in the elimination <ref> [33] </ref>. For diagonally dominant matrices, the higher the level-of-fill the smaller the element.
References-found: 33

