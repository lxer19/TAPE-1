URL: http://www.research.digital.com/SRC/personal/Michael_Mitzenmacher/NEWWORK/postscripts/isit98-tn.ps.gz
Refering-URL: http://www.research.digital.com/SRC/personal/Michael_Mitzenmacher/NEWWORK/papers.html
Root-URL: http://www.research.digital.com
Title: Improved Low-Density Parity-Check Codes Using Irregular Graphs and Belief Propagation  
Author: Michael Luby, Michael Mitzenmacher, Amin Shokrollahi, and Dan Spielman d i g i t a l 
Note: Copyright c flDigital Equipment Corporation 1998. All rights reserved  
Web: http://www.research.digital.com/SRC/  
Address: 130 Lytton Avenue Palo Alto, California 94301  
Affiliation: Systems Research Center  
Date: 1998 009 April 15, 1998  
Pubnum: SRC Technical Note  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> C. Berrou, A Glavieux, and P. Thitimajshima, </author> <title> Near Shannon Limit Error-Correcting Coding and Decoding: </title> <booktitle> Turbo-Codes, Proceedings of IEEE International Communications Conference, </booktitle> <year> 1993. </year>
Reference-contexts: The interest in these codes stems from their near Shannon limit performance, their simple descriptions and implementations, and their amenability to rigorous theoretical analysis [6, 7, 8, 9, 11, 16, 17]. Moreover, there appears to be a connection between these codes and turbo codes, introduced by <ref> [1] </ref>. In particular, the turbo code decoding algorithm can be understood as a belief propagation based algorithm [10, 5], and hence any understanding of belief propagation on low-density parity-check codes may be applicable to turbo codes as well.
Reference: [2] <author> J.-F. Cheng and R. J. </author> <title> McEliece, Some High-Rate Near Capacity Codecs for the Gaussian Channel, </title> <booktitle> 34th Allerton Conference on Communications, Control and Computing, </booktitle> <year> 1996. </year>
Reference-contexts: 1 Introduction Low-density parity-check codes, introduced by Gallager in 1962 [6], and their performance under belief propagation decoding, has been the subject of much recent experimentation and analysis <ref> [2, 11, 12, 16] </ref>. The interest in these codes stems from their near Shannon limit performance, their simple descriptions and implementations, and their amenability to rigorous theoretical analysis [6, 7, 8, 9, 11, 16, 17]. <p> used to construct linear time encodable codes based on cascading series of bipartite graphs, as described for example in [15] or [7], but for convenience we will not address this issue here.) Most previously studied low-density parity-check codes have been constructed using sparse regular, or nearly regular, random bipartite graphs <ref> [2, 6, 11, 12] </ref>. We call these codes regular codes. Our improved performance comes from using codes based on irregular graphs. That is, the degrees of the nodes on each side of the graph can vary widely.
Reference: [3] <author> D. Divsalar and F. Pollara, </author> <title> On the Design of Turbo Codes, </title> <type> JPL TDA Progress Report 42-123. </type>
Reference-contexts: However, we believe the slower running time is not dramatic in light of the improved performance. 4.2 Gaussian Channel Figures 1 and 2 compare the performance (in terms of the bit error rate (BER)) of irregular codes of rate 1=2 and 1=4 with reported results for turbo codes <ref> [3] </ref> and regular codes [12] at these rates. Again, our experiments were with block lengths of 16,000 bits, and for this block length each data point is the result of 10,000 trials. (We compare with results using comparable block lengths. The results from [3] are available at http://www331.jpl.nasa.gov/public/TurboPerf.html. <p> 1=4 with reported results for turbo codes <ref> [3] </ref> and regular codes [12] at these rates. Again, our experiments were with block lengths of 16,000 bits, and for this block length each data point is the result of 10,000 trials. (We compare with results using comparable block lengths. The results from [3] are available at http://www331.jpl.nasa.gov/public/TurboPerf.html. The results we report from [12] are only approximate, as [12] does not provide actual numbers but only a graph.) For our irregular codes, the belief propagation algorithm terminated after 200 rounds if the solution was not found.
Reference: [4] <author> G. D. Forney, Jr. </author> <title> The Forward-Backward Algorithm, </title> <booktitle> Proceedings of the 34th Allerton Conference on Communications, Control and Computing, </booktitle> <year> 1996, </year> <pages> pp. 432-446. 7 </pages>
Reference-contexts: The presence of cycles in the graph skews the probabilities, but in practice the effect on the algorithm appears to be small. More details can be found in <ref> [4, 6, 11, 16] </ref>. 3 Irregular Graphs: Intuition Before we demonstrate irregular random graphs that improve the performance of low-density parity-check codes, we offer some intuition as to why irregular graphs should improve performance. Consider trying to build a regular low-density parity-check code that transmits at a fixed rate.
Reference: [5] <author> B. J. Frey and F. R. Kschischang, </author> <title> Probability Propagation and Iterative Decoding, </title> <booktitle> Proceedings of the 34th Allerton Conference on Communications, Control and Computing, </booktitle> <year> 1996. </year>
Reference-contexts: Moreover, there appears to be a connection between these codes and turbo codes, introduced by [1]. In particular, the turbo code decoding algorithm can be understood as a belief propagation based algorithm <ref> [10, 5] </ref>, and hence any understanding of belief propagation on low-density parity-check codes may be applicable to turbo codes as well. In this paper, we construct new families of low-density parity-check codes, which we call irregular codes, that have significantly improved performance over previously known codes of this type.
Reference: [6] <author> R. G. Gallager, </author> <title> Low-Density Parity-Check Codes, </title> <publisher> MIT Press, </publisher> <year> 1963. </year>
Reference-contexts: 1 Introduction Low-density parity-check codes, introduced by Gallager in 1962 <ref> [6] </ref>, and their performance under belief propagation decoding, has been the subject of much recent experimentation and analysis [2, 11, 12, 16]. <p> The interest in these codes stems from their near Shannon limit performance, their simple descriptions and implementations, and their amenability to rigorous theoretical analysis <ref> [6, 7, 8, 9, 11, 16, 17] </ref>. Moreover, there appears to be a connection between these codes and turbo codes, introduced by [1]. <p> used to construct linear time encodable codes based on cascading series of bipartite graphs, as described for example in [15] or [7], but for convenience we will not address this issue here.) Most previously studied low-density parity-check codes have been constructed using sparse regular, or nearly regular, random bipartite graphs <ref> [2, 6, 11, 12] </ref>. We call these codes regular codes. Our improved performance comes from using codes based on irregular graphs. That is, the degrees of the nodes on each side of the graph can vary widely. <p> We conclude with open questions. 2 Low-Density Parity-Check Codes and Belief Propagation We first review the low-density parity-check codes developed by Gallager and his suggested decoding algorithm, using the framework of MacKay and Neal <ref> [6, 11] </ref>. The parity check matrix H of the code is obtained by creating a matrix chosen uniformly (or near uniformly) at random such that the weight per column is a fixed constant and the weight per row is as uniform as possible. <p> The presence of cycles in the graph skews the probabilities, but in practice the effect on the algorithm appears to be small. More details can be found in <ref> [4, 6, 11, 16] </ref>. 3 Irregular Graphs: Intuition Before we demonstrate irregular random graphs that improve the performance of low-density parity-check codes, we offer some intuition as to why irregular graphs should improve performance. Consider trying to build a regular low-density parity-check code that transmits at a fixed rate.
Reference: [7] <author> M. Luby, M. Mitzenmacher, M. A. Shokrollahi, D. A. Spielman, and V. Stemann, </author> <title> Practical Loss-Resilient Codes, </title> <booktitle> Proc. 29 th Symp. on Theory of Computing, </booktitle> <year> 1997, </year> <pages> pp. 150159. </pages>
Reference-contexts: The interest in these codes stems from their near Shannon limit performance, their simple descriptions and implementations, and their amenability to rigorous theoretical analysis <ref> [6, 7, 8, 9, 11, 16, 17] </ref>. Moreover, there appears to be a connection between these codes and turbo codes, introduced by [1]. <p> / is a codeword if and only for each check node the exclusive-or of its incident message nodes is zero. (We note that our code construction can also be used to construct linear time encodable codes based on cascading series of bipartite graphs, as described for example in [15] or <ref> [7] </ref>, but for convenience we will not address this issue here.) Most previously studied low-density parity-check codes have been constructed using sparse regular, or nearly regular, random bipartite graphs [2, 6, 11, 12]. We call these codes regular codes. Our improved performance comes from using codes based on irregular graphs. <p> up to approximately 16.0% random errors with 16,000 message bits and approximately 16.2% on 64,000 message bits. (The Shannon bound for rate 1=4 codes is 21.45%.) That irregular structure improves performance is not surprising in light of recent work rigorously proving the power of irregular graphs in designing erasure codes <ref> [7, 8] </ref>. Irregular graphs appear to have been rarely studied in the setting of error-correcting codes because of the difficulty in determining what irregular structures might perform well. The erasure codes and the techniques for finding good erasure codes determined in [7, 8] provide the basis for the codes we define <p> the power of irregular graphs in designing erasure codes <ref> [7, 8] </ref>. Irregular graphs appear to have been rarely studied in the setting of error-correcting codes because of the difficulty in determining what irregular structures might perform well. The erasure codes and the techniques for finding good erasure codes determined in [7, 8] provide the basis for the codes we define here. Finally, we note that although we can not provide a formal analysis for the performance of our new codes under belief propagation, we have fully analyzed similar codes that use a simpler, hard decision decoding scheme. <p> Also, following the ideas of [11, 14], when necessary we remove double edges from our graphs. We describe the irregular graphs used in Table 1, using the notation of <ref> [7] </ref>. We say that an edge has degree i on the left if its adjacent node on the left has degree i , and we similarly define an edge with degree i on the right. <p> Our irregular graph based codes approach the performance of turbo codes at certain rates. Our graphs were designed using the techniques of <ref> [7] </ref>. These graphs are known to yield good loss-resilient codes in the model of [7], but there is no clear reason why there cannot be substantially better codes for error-correction based on other irregular graphs. <p> Our irregular graph based codes approach the performance of turbo codes at certain rates. Our graphs were designed using the techniques of <ref> [7] </ref>. These graphs are known to yield good loss-resilient codes in the model of [7], but there is no clear reason why there cannot be substantially better codes for error-correction based on other irregular graphs. If this is the case, then it may be possible to achieve performance similar to that of turbo codes using irregular codes.
Reference: [8] <author> M. Luby, M. Mitzenmacher, and M. A. Shokrollahi, </author> <title> Analysis of Random Processes via And-Or Trees, </title> <booktitle> Proc. 9 th Symp. on Discrete Algorithms, </booktitle> <year> 1998. </year>
Reference-contexts: The interest in these codes stems from their near Shannon limit performance, their simple descriptions and implementations, and their amenability to rigorous theoretical analysis <ref> [6, 7, 8, 9, 11, 16, 17] </ref>. Moreover, there appears to be a connection between these codes and turbo codes, introduced by [1]. <p> up to approximately 16.0% random errors with 16,000 message bits and approximately 16.2% on 64,000 message bits. (The Shannon bound for rate 1=4 codes is 21.45%.) That irregular structure improves performance is not surprising in light of recent work rigorously proving the power of irregular graphs in designing erasure codes <ref> [7, 8] </ref>. Irregular graphs appear to have been rarely studied in the setting of error-correcting codes because of the difficulty in determining what irregular structures might perform well. The erasure codes and the techniques for finding good erasure codes determined in [7, 8] provide the basis for the codes we define <p> the power of irregular graphs in designing erasure codes <ref> [7, 8] </ref>. Irregular graphs appear to have been rarely studied in the setting of error-correcting codes because of the difficulty in determining what irregular structures might perform well. The erasure codes and the techniques for finding good erasure codes determined in [7, 8] provide the basis for the codes we define here. Finally, we note that although we can not provide a formal analysis for the performance of our new codes under belief propagation, we have fully analyzed similar codes that use a simpler, hard decision decoding scheme.
Reference: [9] <author> M. Luby, M. Mitzenmacher, M. A. Shokrollahi, and D. Spielman, </author> <title> Analysis of Low Density Codes and Improved Designs Using Irregular Graphs, </title> <note> submitted to STOC 1998. </note>
Reference-contexts: The interest in these codes stems from their near Shannon limit performance, their simple descriptions and implementations, and their amenability to rigorous theoretical analysis <ref> [6, 7, 8, 9, 11, 16, 17] </ref>. Moreover, there appears to be a connection between these codes and turbo codes, introduced by [1]. <p> Finally, we note that although we can not provide a formal analysis for the performance of our new codes under belief propagation, we have fully analyzed similar codes that use a simpler, hard decision decoding scheme. This work appears in <ref> [9] </ref>. The rest of the paper proceeds as follows: we first review the fundamentals of low-density parity-check codes and belief propagation. We then provide some useful intuition for why irregular codes should provide better performance than regular codes.
Reference: [10] <author> D. J. C. MacKay, R, J. McEliece, and J.-F. Cheng, </author> <title> Turbo Coding as an Instance of Pearl's 'Belief Propagation' Algorithm, </title> <journal> to appear in IEEE Journal on Selected Areas in Communication. </journal>
Reference-contexts: Moreover, there appears to be a connection between these codes and turbo codes, introduced by [1]. In particular, the turbo code decoding algorithm can be understood as a belief propagation based algorithm <ref> [10, 5] </ref>, and hence any understanding of belief propagation on low-density parity-check codes may be applicable to turbo codes as well. In this paper, we construct new families of low-density parity-check codes, which we call irregular codes, that have significantly improved performance over previously known codes of this type.
Reference: [11] <author> D. J. C. MacKay and R. M. Neal, </author> <title> Good Error Correcting Codes Based on Very Sparse Matrices, </title> <note> available from http://wol.ra.phy.cam.ac.uk/mackay. </note>
Reference-contexts: 1 Introduction Low-density parity-check codes, introduced by Gallager in 1962 [6], and their performance under belief propagation decoding, has been the subject of much recent experimentation and analysis <ref> [2, 11, 12, 16] </ref>. The interest in these codes stems from their near Shannon limit performance, their simple descriptions and implementations, and their amenability to rigorous theoretical analysis [6, 7, 8, 9, 11, 16, 17]. <p> The interest in these codes stems from their near Shannon limit performance, their simple descriptions and implementations, and their amenability to rigorous theoretical analysis <ref> [6, 7, 8, 9, 11, 16, 17] </ref>. Moreover, there appears to be a connection between these codes and turbo codes, introduced by [1]. <p> used to construct linear time encodable codes based on cascading series of bipartite graphs, as described for example in [15] or [7], but for convenience we will not address this issue here.) Most previously studied low-density parity-check codes have been constructed using sparse regular, or nearly regular, random bipartite graphs <ref> [2, 6, 11, 12] </ref>. We call these codes regular codes. Our improved performance comes from using codes based on irregular graphs. That is, the degrees of the nodes on each side of the graph can vary widely. <p> We conclude with open questions. 2 Low-Density Parity-Check Codes and Belief Propagation We first review the low-density parity-check codes developed by Gallager and his suggested decoding algorithm, using the framework of MacKay and Neal <ref> [6, 11] </ref>. The parity check matrix H of the code is obtained by creating a matrix chosen uniformly (or near uniformly) at random such that the weight per column is a fixed constant and the weight per row is as uniform as possible. <p> Note that this may potentially lead to multi-edges; often in practice multi-edges and small cycles can be removed to improve performance <ref> [11] </ref>. Gallager's decoding algorithm uses the idea of belief propagation. As explained in [11], the algorithm runs two alternating phases, in which for each non-zero entry in H with row i and column j (or, in other terms, for each edge of the associated bipartite graph) two values q i j <p> Note that this may potentially lead to multi-edges; often in practice multi-edges and small cycles can be removed to improve performance <ref> [11] </ref>. Gallager's decoding algorithm uses the idea of belief propagation. As explained in [11], the algorithm runs two alternating phases, in which for each non-zero entry in H with row i and column j (or, in other terms, for each edge of the associated bipartite graph) two values q i j and r i j are iteratively updated. <p> The presence of cycles in the graph skews the probabilities, but in practice the effect on the algorithm appears to be small. More details can be found in <ref> [4, 6, 11, 16] </ref>. 3 Irregular Graphs: Intuition Before we demonstrate irregular random graphs that improve the performance of low-density parity-check codes, we offer some intuition as to why irregular graphs should improve performance. Consider trying to build a regular low-density parity-check code that transmits at a fixed rate. <p> These two competing requirements must be appropriately balanced. Previous work has shown that for regular graphs, low degree graphs yield the best performance <ref> [11, 12] </ref>. If one allows irregular graphs, however, there is significantly more flexibility in balancing these competing requirements. Message nodes with high degree will tend to their correct value quickly. These nodes then provide good information to the check nodes, which subsequently provide better information to lower degree message nodes. <p> A different random graph was conducted for each trial. No effort was made to test graphs and weed out potentially bad ones, and hence we expect that our results would be slightly better if several random graphs were tested and the best ones chosen. Also, following the ideas of <ref> [11, 14] </ref>, when necessary we remove double edges from our graphs. We describe the irregular graphs used in Table 1, using the notation of [7]. <p> Our results for regular codes (based on graphs in which all nodes on the left have degree 3) are slightly better than (but consistent with) previous results reported in <ref> [11] </ref>. In the table, n represents the block length, R represents the rate, f represents the fraction of errors introduced, and C represents the channel capacity. <p> They do appear notably more robust at higher error rates, however. At 64,000 bits, our code can handle over a half a percent more errors. While it has been previously noted that low-density parity-check codes perform better as the block length increases <ref> [11] </ref>, we believe that this effect is magnified for our irregular codes, because the degrees of the nodes can be quite high. For example, our irregular rate 1=2 codes have nodes on the left of degree 65 and nodes on the right of degree 85.
Reference: [12] <author> D. J. C. MacKay and R. M. Neal, </author> <title> Near Shannon Limit Performance of Low Density Parity Check Codes, </title> <note> to appear in Electronic Letters. </note>
Reference-contexts: 1 Introduction Low-density parity-check codes, introduced by Gallager in 1962 [6], and their performance under belief propagation decoding, has been the subject of much recent experimentation and analysis <ref> [2, 11, 12, 16] </ref>. The interest in these codes stems from their near Shannon limit performance, their simple descriptions and implementations, and their amenability to rigorous theoretical analysis [6, 7, 8, 9, 11, 16, 17]. <p> used to construct linear time encodable codes based on cascading series of bipartite graphs, as described for example in [15] or [7], but for convenience we will not address this issue here.) Most previously studied low-density parity-check codes have been constructed using sparse regular, or nearly regular, random bipartite graphs <ref> [2, 6, 11, 12] </ref>. We call these codes regular codes. Our improved performance comes from using codes based on irregular graphs. That is, the degrees of the nodes on each side of the graph can vary widely. <p> These two competing requirements must be appropriately balanced. Previous work has shown that for regular graphs, low degree graphs yield the best performance <ref> [11, 12] </ref>. If one allows irregular graphs, however, there is significantly more flexibility in balancing these competing requirements. Message nodes with high degree will tend to their correct value quickly. These nodes then provide good information to the check nodes, which subsequently provide better information to lower degree message nodes. <p> believe the slower running time is not dramatic in light of the improved performance. 4.2 Gaussian Channel Figures 1 and 2 compare the performance (in terms of the bit error rate (BER)) of irregular codes of rate 1=2 and 1=4 with reported results for turbo codes [3] and regular codes <ref> [12] </ref> at these rates. Again, our experiments were with block lengths of 16,000 bits, and for this block length each data point is the result of 10,000 trials. (We compare with results using comparable block lengths. The results from [3] are available at http://www331.jpl.nasa.gov/public/TurboPerf.html. The results we report from [12] are <p> codes <ref> [12] </ref> at these rates. Again, our experiments were with block lengths of 16,000 bits, and for this block length each data point is the result of 10,000 trials. (We compare with results using comparable block lengths. The results from [3] are available at http://www331.jpl.nasa.gov/public/TurboPerf.html. The results we report from [12] are only approximate, as [12] does not provide actual numbers but only a graph.) For our irregular codes, the belief propagation algorithm terminated after 200 rounds if the solution was not found. <p> The results from [3] are available at http://www331.jpl.nasa.gov/public/TurboPerf.html. The results we report from <ref> [12] </ref> are only approximate, as [12] does not provide actual numbers but only a graph.) For our irregular codes, the belief propagation algorithm terminated after 200 rounds if the solution was not found. <p> Our estimates for the bit error rate from 1,000 trials at 0.9 dB is 3:97 10 5 and at 0.85 dB is 6:03 10 5 . Again, this is much better than the performance of regular codes at a comparable block length presented in <ref> [12] </ref>. Our results for irregular codes at rate 1=4 (Figure 2) similarly show significant improvement over regular 6 1.00E-06 1.00E-04 1.00E-02 -0.1 0.2 0.5 0.8 1.1 1.4 B R Turbo Irregular Regular codes. At this lower rate and block length, however, turbo codes appear to have a significant edge.
Reference: [13] <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988. </year>
Reference: [14] <author> M. Sipser, D. A. Spielman, </author> <title> Expander Codes, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42(6), </volume> <month> November </month> <year> 1996, </year> <pages> pp. 1710-1722. </pages>
Reference-contexts: A different random graph was conducted for each trial. No effort was made to test graphs and weed out potentially bad ones, and hence we expect that our results would be slightly better if several random graphs were tested and the best ones chosen. Also, following the ideas of <ref> [11, 14] </ref>, when necessary we remove double edges from our graphs. We describe the irregular graphs used in Table 1, using the notation of [7].
Reference: [15] <author> D. A. Spielman, </author> <title> Linear Time Encodable and Decodable Error-Correcting Codes, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42(6), </volume> <month> November </month> <year> 1996, </year> <pages> pp. 1723-1731. </pages>
Reference-contexts: x n / is a codeword if and only for each check node the exclusive-or of its incident message nodes is zero. (We note that our code construction can also be used to construct linear time encodable codes based on cascading series of bipartite graphs, as described for example in <ref> [15] </ref> or [7], but for convenience we will not address this issue here.) Most previously studied low-density parity-check codes have been constructed using sparse regular, or nearly regular, random bipartite graphs [2, 6, 11, 12]. We call these codes regular codes.
Reference: [16] <author> N. Wiberg, </author> <title> Codes and decoding on general graphs Ph.D. </title> <type> dissertation, </type> <institution> Dept. Elec. Eng, U. Link oping, Sweeden, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Low-density parity-check codes, introduced by Gallager in 1962 [6], and their performance under belief propagation decoding, has been the subject of much recent experimentation and analysis <ref> [2, 11, 12, 16] </ref>. The interest in these codes stems from their near Shannon limit performance, their simple descriptions and implementations, and their amenability to rigorous theoretical analysis [6, 7, 8, 9, 11, 16, 17]. <p> The interest in these codes stems from their near Shannon limit performance, their simple descriptions and implementations, and their amenability to rigorous theoretical analysis <ref> [6, 7, 8, 9, 11, 16, 17] </ref>. Moreover, there appears to be a connection between these codes and turbo codes, introduced by [1]. <p> The presence of cycles in the graph skews the probabilities, but in practice the effect on the algorithm appears to be small. More details can be found in <ref> [4, 6, 11, 16] </ref>. 3 Irregular Graphs: Intuition Before we demonstrate irregular random graphs that improve the performance of low-density parity-check codes, we offer some intuition as to why irregular graphs should improve performance. Consider trying to build a regular low-density parity-check code that transmits at a fixed rate.
Reference: [17] <author> N. Wiberg, H.-A. Loeliger and R. Kotter, </author> <title> Codes and iterative decoding on general graphs, </title> <journal> European Transactions on Telecommunications, </journal> <volume> vol. 6, </volume> <month> September </month> <year> 1995, </year> <pages> pp. 513-526. 8 </pages>
Reference-contexts: The interest in these codes stems from their near Shannon limit performance, their simple descriptions and implementations, and their amenability to rigorous theoretical analysis <ref> [6, 7, 8, 9, 11, 16, 17] </ref>. Moreover, there appears to be a connection between these codes and turbo codes, introduced by [1].
References-found: 17

