URL: http://www.cs.umass.edu/~dprecup/publications/Precup-Sutton-NIPS97.ps
Refering-URL: http://www-anw.cs.umass.edu/Publications/recent.html
Root-URL: 
Email: fdprecupjrichg@cs.umass.edu  
Title: Multi-time Models for Temporally Abstract Planning  
Author: Doina Precup, Richard S. Sutton 
Address: Amherst, MA 01003  
Affiliation: University of Massachusetts  
Abstract: Planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence. In this paper we summarize an approach to this problem based on the mathematical framework of Markov decision processes and reinforcement learning. Current model-based reinforcement learning is based on one-step models that cannot represent common-sense higher-level actions, such as going to lunch, grasping an object, or flying to Denver. This paper generalizes prior work on temporally abstract models [Sutton, 1995] and extends it from the prediction setting to include actions, control, and planning. We introduce a more general form of temporally abstract model, the multi-time model, and establish its suitability for planning and learning by virtue of its relationship to the Bellman equations. This paper summarizes the theoretical framework of multi-time models and illustrates their potential advantages in a The need for hierarchical and abstract planning is a fundamental problem in AI (see, e.g., Sacerdoti, 1977; Laird et al., 1986; Korf, 1985; Kaelbling, 1993; Dayan & Hinton, 1993). Model-based reinforcement learning offers a possible solution to the problem of integrating planning with real-time learning and decision-making (Peng & Williams, 1993, Moore & Atkeson, 1993; Sutton and Barto, 1998). However, current model-based reinforcement learning is based on one-step models that cannot represent common-sense, higher-level actions. Modeling such actions requires the ability to handle different, interrelated levels of temporal abstraction. A new approach to modeling at multiple time scales was introduced by Sutton (1995) based on prior work by Singh , Dayan , and Sutton and Pinette . This approach enables models of the environment at different temporal scales to be intermixed, producing temporally abstract models. However, that work was concerned only with predicting the environment. This paper summarizes an extension of the approach including actions and control of the environment [Precup & Sutton, 1997]. In particular, we generalize the usual notion of a gridworld planning task.
Abstract-found: 1
Intro-found: 1
Reference: <author> Dayan, P. </author> <year> (1993). </year> <title> Improving generalization for temporal difference learning: </title> <booktitle> The successor representation. Neural Computation, </booktitle> <volume> 5, </volume> <pages> 613624. </pages>
Reference: <author> Dayan, P. & Hinton, G. E. </author> <year> (1993). </year> <title> Feudal reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> (pp. 271278)., </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kaelbling, L. P. </author> <year> (1993). </year> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning ICML'93, </booktitle> <pages> (pp. 167173)., </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Korf, R. E. </author> <year> (1985). </year> <title> Learning to Solve Problems by Searching for Macro-Operators. </title> <publisher> London: Pitman Publishing Ltd. </publisher>
Reference: <author> Laird, J. E., Rosenbloom, P. S., & Newell, A. </author> <year> (1986). </year> <title> Chunking in SOAR: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 1146. </pages>
Reference: <author> Moore, A. W. & Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 103130. </pages>
Reference: <author> Peng, J. & Williams, J. </author> <year> (1993). </year> <title> Efficient learning and planning within the Dyna framework. Adaptive Behavior, </title> <type> 4, 323334. </type>
Reference: <author> Precup, D. & Sutton, R. S. </author> <year> (1997). </year> <title> Multi-Time models for reinforcement learning. In ICML'97 Workshop: The Role of Models in Reinforcement Learning. </title>
Reference: <author> Sacerdoti, E. D. </author> <year> (1977). </year> <title> A Structure for Plans and Behavior. </title> <publisher> North-Holland, NY: Elsevier. </publisher>
Reference: <author> Singh, S. P. </author> <year> (1992). </year> <title> Scaling reinforcement learning by learning variable temporal resolution models. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning ICML'92, </booktitle> <pages> (pp. 202 207)., </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1995). </year> <title> TD models: Modeling the world as a mixture of time scales. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning ICML'95, </booktitle> <pages> (pp. 531539)., </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S. & Barto, A. G. </author> <year> (1998). </year> <title> Reinforcement Learning. An Introduction. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Sutton, R. S. & Pinette, B. </author> <year> (1985). </year> <title> The learning of world models by connectionist networks. </title> <booktitle> In Proceedings of the Seventh Annual Conference of the Cognitive Science Society, </booktitle> <pages> (pp. 5464). </pages>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University. </institution>
References-found: 14

