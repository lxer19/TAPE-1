URL: http://www.media.mit.edu/~eds/DAFX98.ps.gz
Refering-URL: http://www.media.mit.edu/~eds/papers.html
Root-URL: http://www.media.mit.edu
Email: eds@media.mit.edu  Riitta.Vaananen@hut.fi  jyri@ccrma.stanford.edu  
Phone: (2)  (3)  
Title: AudioBIFS: The MPEG-4 Standard for Effects Processing  
Author: Eric D. Scheirer () Riitta Vnnen () Jyri Huopaniemi () 
Web: http://sound.media.mit.edu/mpeg4  http://www.acoustics.hut.fi/  http://www-ccrma.stanford.edu/  
Affiliation: Group, MIT Media Laboratory  Laboratory of Acoustics, Helsinki University of Technology  CCRMA, Stanford University Laboratory of Acoustics, Helsinki University of Technology  
Date: Nov. 1998  
Note: Presented at the first COST-G6 Workshop on Digital Audio Effects Processing (DAFX 98), Barcelona,  (1) Machine Listening  
Abstract: We present a tutorial overview of the AudioBIFS system, part of the Binary Format for Scene Description in the MPEG-4 International Standard. AudioBIFS allows the flexible construction of sound scenes using streaming audio, interactive presentation, 3-D spatialization and environmental auralization, and dynamic download of custom signal-processing routines. MPEG-4 sound scenes are based on a model that is a s u-perset of the model in VRML 2.0, and a comparison between the two models is pr e-sented. We discuss the use of SAOL, the MPEG-4 Structured Audio Orchestra La n-guage, for writing downloadable effects. The current status of the standard is d e scribed.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Casey, M., and Smaragdis, P. </author> <year> 1996. </year> <title> Net sound. </title> <booktitle> Proc. Int. Comp. Music Conf, </booktitle> <address> Hong Kong. </address>
Reference: [2] <author> Eleftheriadis, A., Herpel, C., Rajan, G., and Ward, L., Editors. </author> <year> 1998. </year> <note> ISO 14496-1 (MPEG 4 Systems) Final Committee Draft. MPEG Document W2201, Tokyo. </note>
Reference-contexts: It is the scene, not the primitive media objects, which is shown to the person viewing the content. The instructions for composition are conveyed in the special BIFS format, which is specified in the MPEG-4 Systems standard <ref> [2] </ref>. In this tutorial, we focus only on the sound-compositing capabilities of MPEG-4. The sound coding tools are described in detail elsewhere, both in the technical literature [7][9], and in the MPEG-4 Audio standard itself [3], which is the official refe r-ence.
Reference: [3] <author> Grill, B., Edler, B., Kaneko, I., Lee, Y., Nishi guchi, M., Scheirer, E., and Vnnen, M., Editors. </author> <year> 1998. </year> <title> ISO 14496-3 (MPEG-4 Audio) Final Committee Draft. MPEG Document W2203, </title> <address> Tokyo. </address>
Reference-contexts: In this tutorial, we focus only on the sound-compositing capabilities of MPEG-4. The sound coding tools are described in detail elsewhere, both in the technical literature [7][9], and in the MPEG-4 Audio standard itself <ref> [3] </ref>, which is the official refe r-ence. There is an equivalent body of work on visual Presented at the first COST-G6 Workshop on Digital Audio Effects Processing (DAFX 98), Barcelona, Nov. 1998 aspects of the standard that is also outside the scope of our presentation.
Reference: [4] <author> ISO 14472. </author> <year> 1997. </year> <title> Virtual Reality Modeling Language (VRML). </title> <booktitle> International Organisation for Standardisation, </booktitle> <address> Geneva. </address>
Reference-contexts: Many of the BIFS concepts originate from the Virtual Reality Modeling Language (VRML) standard <ref> [4] </ref>, but the audio toolset is built from a different philosophy. AudioBIFS contains significant a d-vances in quality and flexibility over VRML audio.
Reference: [5] <author> Kleiner, M., Dalenbck, B.-I., and Svensson, P. </author> <year> 1993. </year> <title> "Auralization - an Overview", </title> <journal> J. Audio Eng. Soc., </journal> <pages> 41(11) pp. 861-875. </pages>
Reference-contexts: This involves modeling individual sound reflections off the walls, modeling sound propagation through objects, sim u-lating air absorption, and rendering of late diffuse reverberation, in addition to the 3D-spatialization of the sound location. This kind of "environmental spatialization" is often referred to as auralization <ref> [5] </ref>. Auralization consists of rendering the whole of sound propagation from the sound source to the listener. Therefore, it involves not only the properties of sound transmission in the acoustic medium, but also direction-dependent sound radiation at the source and the arrival of sound at both ears of the listener.
Reference: [6] <author> Meares, D., Watanabe, K., and Scheirer, E. </author> <title> Report on the MPEG-2 Advanced Audio Coding Stereo Verification Tests. MPEG Document W2003, </title> <address> San Jose, CA, USA. </address>
Reference-contexts: The natural-audio tools enable the compressed transmission of speech and wideband audio at ranges from 2 kbps to 64 kbps per channel for high-quality multichannel sound. At the upper end of this range, psychoacoustic evaluation has demonstrated that the MPEG-4 algorithms <ref> [6] </ref> allow perceptually tran s-parent coding. That is, even the most skilled liste n-ers under rigorous testing conditions cannot disti n-guish the coded signal from the original. There are two synthetic audio coders in MPEG-4.
Reference: [7] <author> Quackenbush, Q. </author> <year> 1998. </year> <title> Coding of Natural Audio in MPEG-4. </title> <booktitle> Proc. IEEE ICASSP, </booktitle> <address> Sea t tle. </address>
Reference-contexts: In this section, we give an overview of the MPEG-4 sound coding tools, discuss the sound-compositing philosophy in MPEG-4, and compare this philosophy with that of the popular VRML standard. 2.1 Sound coding in MPEG-4 There are two major groups of sound coding tools in MPEG-4: the natural tools <ref> [7] </ref> that allow recorded digital audio to be compressed and transmitted, and the synthetic tools [9] that allow parametric descri p-tions of sounds to be transmitted and used for real-time digital synthesis upon receipt.
Reference: [8] <author> Savioja, L., Huopaniemi, J., Lokki, T., and Vnnen, R. </author> <year> 1997. </year> <booktitle> Virtual environment simulation Advances in the DIVA project . Proc. Int. Conf. Auditory Display (ICAD'97), </booktitle> <address> Palo Alto, California, USA. </address>
Reference-contexts: Other examples of such systems, in which models of room acoustics are dynamically computed according to sound source and listener positions in a computer-modeled room, can be found in the literature <ref> [8] </ref>. 4.2 Extensions to AudioBIFS in MPEG-4 Version 2 In version 1 of the MPEG-4 standard, as in VRML, the virtual-reality soundsource model only provides techniques for placing sound sources in 3D space and coarse simulation of sound source directivity by the elliptical attenuation model.
Reference: [9] <author> Scheirer, E. D. </author> <year> 1998. </year> <title> The MPEG-4 Structured Audio standard. </title> <booktitle> Proc. IEEE ICASSP, </booktitle> <address> Seattle. </address>
Reference-contexts: the sound-compositing philosophy in MPEG-4, and compare this philosophy with that of the popular VRML standard. 2.1 Sound coding in MPEG-4 There are two major groups of sound coding tools in MPEG-4: the natural tools [7] that allow recorded digital audio to be compressed and transmitted, and the synthetic tools <ref> [9] </ref> that allow parametric descri p-tions of sounds to be transmitted and used for real-time digital synthesis upon receipt. The natural-audio tools enable the compressed transmission of speech and wideband audio at ranges from 2 kbps to 64 kbps per channel for high-quality multichannel sound.
Reference: [10] <author> Scheirer, E. D. </author> <year> 1998. </year> <title> The MPEG-4 Structured Audio Orchestra Language. </title> <booktitle> Proc. ICMC, </booktitle> <address> Ann Arbor, MI, USA. </address>
Reference-contexts: There are two synthetic audio coders in MPEG-4. One provides an interface to text-to-speech systems; the other is a very general music-and-sound-effects synthesis toolset called Structured Audio (SA). The Structured Audio coder allows transmission of soundsynthesis algorithms in a new Music V language called SAOL (Structured Audio Orchestra Language) <ref> [10] </ref>, and banks of wavetables in a fo r-mat called SASBF (Structured Audio Sample Bank Format), which was developed in collaboration with the MIDI Manufacturers Association. The alg o-rithmic and wavetable synthesis capabilities may be used in parallel or in sequence in a synthetic soun d-track [12]. <p> AudioFX The AudioFX node allows the dynamic download of custom signal-processing effects to apply to several channels of input sound. A special sound-processing language called SAOL <ref> [10] </ref> is standardized in the audio toolset; this language allows arbitrary effects-processing algorithms to be transmitted in the scene graph.
Reference: [11] <author> Scheirer, E. D. </author> <title> In press. Structured Audio and effects processing in the MPEG-4 multimedia standard. </title> <note> To appear in ACM Multimedia Sys. J. </note>
Reference-contexts: The music language SAOL is also important to the audio compositing tools; as we will describe in section 3.2, SAOL is used in MPEG-4 for downloading user-definable effects-processing algorithms. This convergence between structured audio and effects processing in MPEG-4 <ref> [11] </ref> is one of the elegant and important aspects of the standard. 2.2 Scene graph concepts Both VRML and MPEG-4 BIFS rely on the scene graph to describe the organization of audiovisual material. <p> In abstract-effects compositing, the goal is to provide content authors with a rich suite of tools from which artistic considerations can be used to choose the right effect for a given situation. As Scheirer <ref> [11] </ref> discusses in depth, the goal of sound designers for tr a-ditional media such as films, radio, and television is not to recreate a virtual acoustic environment (a l-though this would be well within the capability of todays film studios), but to apply a body of know ledge regarding what a
Reference: [12] <author> Scheirer, E. D. and Ray, L. </author> <year> 1998. </year> <title> Algorithmic and wavetable synthesis in the MPEG-4 mult i-media standard. </title> <booktitle> Proc. </booktitle> <institution> 1998 Audio Eng. Soc i ety Convention, </institution> <address> San Francisco. </address>
Reference-contexts: The alg o-rithmic and wavetable synthesis capabilities may be used in parallel or in sequence in a synthetic soun d-track <ref> [12] </ref>. The transmission of sound through the real-time execution of synthesis algorithms is a recent deve l-opment [1][14], and MPEG-4 is the first standard to make use of this capability.
Reference: [13] <author> Vnnen, R., Singer, D., Belknap, W., Shamoon, T., Herpel, C. </author> <year> 1998. </year> <title> ISO 14496-1 (MPEG-4 Systems version 2) Verification Model. MPEG Document W2359, </title> <publisher> Dublin. </publisher>
Reference-contexts: For the second phase of the standard, three new nodes have been proposed for advanced auralization of the sound (see <ref> [13] </ref>). Their textual names are AcousticScene , DirectiveSound and AcousticMate-rial ; together they can be used to form natural-sounding audio environments. AcousticScene and AcousticMaterial The AcousticScene node is a grouping node that is used to bind together an entire auralization process.
Reference: [14] <author> Vercoe, B. L, Gardner, W. G., and Scheirer, E. D. </author> <year> 1998. </year> <title> Structured audio: The creation, tran s-mission, and rendering of parametric sound re p resentations. </title> <booktitle> Proc. </booktitle> <pages> IEEE 86(5) pp. 922-940. </pages>
References-found: 14

