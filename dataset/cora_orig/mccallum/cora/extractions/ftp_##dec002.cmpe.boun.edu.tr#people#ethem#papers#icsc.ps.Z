URL: ftp://dec002.cmpe.boun.edu.tr/people/ethem/papers/icsc.ps.Z
Refering-URL: http://www.cmpe.boun.edu.tr/~ethem/
Root-URL: 
Email: alpaydin@boun.edu.tr  
Title: Combining Global vs Local Linear Perceptrons for Classification  
Author: Ethem Alpaydn 
Address: TR-80815 Istanbul, Turkey  
Affiliation: Department of Computer Engineering Bogazi~ci University,  
Abstract: Simple linear perceptrons learn fast, are simple and effective in many classification applications. We consider two ways to combine multiple such perceptrons for improved classification accuracy. In the first approach, we train multiple perceptrons on subsets of the training set and then take a simple vote. Because their training sets are different, different perceptrons converge to different solutions and averaging removes noise. In the second approach named the mixture of experts, different perceptrons converge to different parts of the input space thereby learning local boundaries. A gating perceptron is responsible from deciding which one to use for a given input. We compare these approaches on a real world set of handwritten digits. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Duda, R. O., Hart, P. E. </author> <title> (1973) Pattern Classification and Scene Analysis, </title> <publisher> Wiley and Sons. </publisher>
Reference: [2] <author> McLachlan, G. J. </author> <title> (1992) Discriminant Analysis and Statistical Pattern Recognition, </title> <publisher> Wiley. </publisher>
Reference: [3] <author> J. S. Bridle, </author> <title> "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition," in Neurocomputing, </title> <editor> F. Fogelman-Soulie, J. Herault, Eds. </editor> <publisher> Berlin: Springer, </publisher> <year> 1990, </year> <pages> pp. 227-236. </pages>
Reference-contexts: If we assume that all errors are equally costly, to minimize risk we assign the input to the most probable class: c = arg max [P (C j jx; )] (13) For classification, we can write a "softmax" model for the posterior class probabilities <ref> [3] </ref>, [15]: P (C j jx) = P (14) A j denotes the total output for class j.
Reference: [4] <author> B. D. Ripley, </author> <title> "Neural networks and related methods for classification," </title> <journal> J. R. Statist. Soc. B, </journal> <volume> vol. 56, </volume> <pages> pp. 409-456, </pages> <year> 1994. </year>
Reference: [5] <author> Hertz, J., Krogh, A., Palmer, R. </author> <title> (1991) An Introduction to the Theory of Neural Computation, </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The disadvantage however is that discriminants are rarely linear and thus the linear model should be extended to handle with these cases. In multi-layer perceptrons, the hidden units extract nonlinear input combinations to be able to define nonlinear discriminants <ref> [5] </ref>. In this paper, we take a different approach and consider models that lead to nonlinear discriminants using linear models as the basic building blocks. Thus we are advocating a model where more than one linear model is combined.
Reference: [6] <author> Hansen, L. K., Salamon, P. </author> <title> (1990) "Neural Network Ensembles," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12, </volume> <pages> 993-1001. </pages>
Reference-contexts: By suitably combining those, performance can be improved. Here we consider two approaches; voting and the mixture of experts: * Voting takes a linear, weighted sum of the output of the learners. Weights are nonnegative and sum up to one <ref> [6] </ref>, [9], [12]. * Mixture of experts is like voting where weights are input dependent [8]. It uses a number of local learners that partition the input space among themselves. A separate gating model is responsible from choosing the right expert for a given input. Fig. 1. <p> The final output is computed as: r i = f V (fd ji g m j=1 jffi j g m j=1 ) = j=1 The algorithms for training and testing the voting architecture are given in Tables I and II. It has been shown by Hansen and Salamon <ref> [6] </ref> that given independent classifiers with success probability higher than 1/2, by taking a majority vote, success increases as the number of voting classifiers increase. Mani [9] has shown that in the case of simple voting, variance decreases as the number of independent voters increase; see Appendix.
Reference: [7] <author> Lincoln, W. P., Skrzypek, J. </author> <title> (1990) "Synergy of Clustering Multiple Back Propagation Networks," </title> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <editor> D. Touretzky (Ed.), </editor> <publisher> Mor-gan Kaufmann, </publisher> <pages> 650-657. </pages>
Reference-contexts: Xu et al. [10] discuss various ways in which the outputs of several classifiers are combined. To compute the weights, they propose to use a belief measure or the Dempster-Schafer theory. Rogova [15] also uses the latter. Lincoln and Skrzypek <ref> [7] </ref> propose a way to learn the weights in a voting scheme. Model complexities can also be taken into account in a Bayesian framework to make sure that complex models are not given very large weights [11].
Reference: [8] <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., Hinton, G. E. </author> <title> (1991) "Adaptive Mixtures of Local Experts," </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 79-87. </pages>
Reference-contexts: Here we consider two approaches; voting and the mixture of experts: * Voting takes a linear, weighted sum of the output of the learners. Weights are nonnegative and sum up to one [6], [9], [12]. * Mixture of experts is like voting where weights are input dependent <ref> [8] </ref>. It uses a number of local learners that partition the input space among themselves. A separate gating model is responsible from choosing the right expert for a given input. Fig. 1. The architecture of voting. d j are the voting classifiers and r i are the classes. <p> IV. Mixtures of Experts A. Introduction In voting, the weights fi j are constant over the input space. In the mixtures of experts architecture proposed by Jacobs et al. <ref> [8] </ref>, they are a function of the input TABLE III The algorithm for training of Mixtures of Experts. <p> Through coupled training of expert mappings and positions, experts are placed in the input space in such a way so as to minimize error. We discuss this in more detail in Section C. 2) Competing Learners: First proposed by Jacobs et al. <ref> [8] </ref>, a measure that forces competition is to view the architecture as a mixture model [23]. The gating values are the mixture proportions and the expert perceptron outputs are the means.
Reference: [9] <author> Mani, G. </author> <title> (1991) "Lowering Variance of Decisions by using Artificial Neural Network Ensembles," </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 484-486. </pages>
Reference-contexts: By suitably combining those, performance can be improved. Here we consider two approaches; voting and the mixture of experts: * Voting takes a linear, weighted sum of the output of the learners. Weights are nonnegative and sum up to one [6], <ref> [9] </ref>, [12]. * Mixture of experts is like voting where weights are input dependent [8]. It uses a number of local learners that partition the input space among themselves. A separate gating model is responsible from choosing the right expert for a given input. Fig. 1. <p> It has been shown by Hansen and Salamon [6] that given independent classifiers with success probability higher than 1/2, by taking a majority vote, success increases as the number of voting classifiers increase. Mani <ref> [9] </ref> has shown that in the case of simple voting, variance decreases as the number of independent voters increase; see Appendix. Xu et al. [10] discuss various ways in which the outputs of several classifiers are combined.
Reference: [10] <author> Xu, L., Krzy_zak, A., Suen, C. Y. </author> <title> (1992) "Methods of Combining Multiple Classifiers and Their Applications to Handwriting Recognition," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 22, </volume> <pages> 418-435. </pages>
Reference-contexts: Mani [9] has shown that in the case of simple voting, variance decreases as the number of independent voters increase; see Appendix. Xu et al. <ref> [10] </ref> discuss various ways in which the outputs of several classifiers are combined. To compute the weights, they propose to use a belief measure or the Dempster-Schafer theory. Rogova [15] also uses the latter. Lincoln and Skrzypek [7] propose a way to learn the weights in a voting scheme.
Reference: [11] <author> Alpaydn, E. </author> <title> (1993) "Multiple Networks for Function Learning," </title> <booktitle> IEEE International Conference on Neural Networks, </booktitle> <address> March, San Francisco: CA, </address> <month> 1, </month> <pages> 9-14. </pages>
Reference-contexts: TO m DO END FOR Find r c such that r c = max j r i END TestVote vote for each class, then these can be converted to certainties and used as weights in an weighted voting scheme, for example proportional to the difference of the two highest outputs <ref> [11] </ref>. Another possibility is to assess the accuracies of the classifiers on a separate cross-validation set and use that information to compute the weights. <p> Rogova [15] also uses the latter. Lincoln and Skrzypek [7] propose a way to learn the weights in a voting scheme. Model complexities can also be taken into account in a Bayesian framework to make sure that complex models are not given very large weights <ref> [11] </ref>. Perrone [12] gives a number of didactic examples that depicts the advantage of voting. He also shows that for minimum square error, when the learners are unbiased and uncorrelated, weights should be inversely pro portional to variances; see Appendix. Fig. 2. The architecture of mixtures of experts.
Reference: [12] <author> Perrone, M. P. </author> <title> (1993) Improving Regression Estimation: Averaging Methods for Variance Reduction with Extensions to General Convex Measure Optimization, </title> <type> PhD Thesis, </type> <institution> Department of Physics, Brown University. </institution>
Reference-contexts: By suitably combining those, performance can be improved. Here we consider two approaches; voting and the mixture of experts: * Voting takes a linear, weighted sum of the output of the learners. Weights are nonnegative and sum up to one [6], [9], <ref> [12] </ref>. * Mixture of experts is like voting where weights are input dependent [8]. It uses a number of local learners that partition the input space among themselves. A separate gating model is responsible from choosing the right expert for a given input. Fig. 1. <p> Rogova [15] also uses the latter. Lincoln and Skrzypek [7] propose a way to learn the weights in a voting scheme. Model complexities can also be taken into account in a Bayesian framework to make sure that complex models are not given very large weights [11]. Perrone <ref> [12] </ref> gives a number of didactic examples that depicts the advantage of voting. He also shows that for minimum square error, when the learners are unbiased and uncorrelated, weights should be inversely pro portional to variances; see Appendix. Fig. 2. The architecture of mixtures of experts. Adapting what Perrone [12] stated <p> Perrone <ref> [12] </ref> gives a number of didactic examples that depicts the advantage of voting. He also shows that for minimum square error, when the learners are unbiased and uncorrelated, weights should be inversely pro portional to variances; see Appendix. Fig. 2. The architecture of mixtures of experts. Adapting what Perrone [12] stated for regression to classification, if we view of each learner as a random noise function added to the true class discriminant function and if these noise functions are uncorrelated with zero mean then the averaging of the individual estimates is like averaging over the noise.
Reference: [13] <author> LeBlanc, M., Tibshirani, R. </author> <title> (1994) Combining Estimates in Regression and Classification, </title> <institution> Department of Statistics, University of Toronto. </institution>
Reference-contexts: This has been shown empirically for the case of linear regression by Meir [14]. If the training set is not large enough to allow partitioning, one can use bootstrap which involves generating new datasets from one original dataset by sampling randomly with replacement <ref> [13] </ref>. IV. Mixtures of Experts A. Introduction In voting, the weights fi j are constant over the input space. In the mixtures of experts architecture proposed by Jacobs et al. [8], they are a function of the input TABLE III The algorithm for training of Mixtures of Experts.
Reference: [14] <author> Meir, R. </author> <title> (1994) Bias, Variance and the Combination of Estimators: The Case of Linear Least Squares, </title> <institution> Department of Electrical Engineering, Technion. </institution>
Reference-contexts: It has been pointed out before that voting averages over noisy discriminants. As the training set gets larger, the variance of an estimator decreases and the voters become more similar. This has been shown empirically for the case of linear regression by Meir <ref> [14] </ref>. If the training set is not large enough to allow partitioning, one can use bootstrap which involves generating new datasets from one original dataset by sampling randomly with replacement [13]. IV. Mixtures of Experts A. Introduction In voting, the weights fi j are constant over the input space.
Reference: [15] <author> Rogova, G. </author> <title> (1994) "Combining the Results of Several Neural Network Classifiers," </title> <booktitle> Neural Networks, </booktitle> <volume> 7, </volume> <pages> 777-781. </pages>
Reference-contexts: Xu et al. [10] discuss various ways in which the outputs of several classifiers are combined. To compute the weights, they propose to use a belief measure or the Dempster-Schafer theory. Rogova <ref> [15] </ref> also uses the latter. Lincoln and Skrzypek [7] propose a way to learn the weights in a voting scheme. Model complexities can also be taken into account in a Bayesian framework to make sure that complex models are not given very large weights [11]. <p> If we assume that all errors are equally costly, to minimize risk we assign the input to the most probable class: c = arg max [P (C j jx; )] (13) For classification, we can write a "softmax" model for the posterior class probabilities [3], <ref> [15] </ref>: P (C j jx) = P (14) A j denotes the total output for class j.
Reference: [16] <author> H. Ritter, T. Martinetz, and K. Schulten, </author> <title> "Topology-conserving maps for learning visuomotor coordination," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 159-168, </pages> <year> 1988. </year>
Reference: [17] <author> J. B. Hampshire II, A. Waibel, </author> <title> "Connectionist architectures for multi-speakerphoneme recognition," </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <volume> vol. 2, </volume> <editor> D. Touretzky, Ed. </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1990, </year> <pages> pp. 203-210. </pages>
Reference: [18] <author> K. Stokbro, D. K. Umberger, and J. A. Hertz, </author> <title> "Exploiting neurons with localized receptive fields to learn chaos," </title> <journal> Complex Systems, </journal> <volume> vol. 4, </volume> <pages> pp. 603-622, </pages> <year> 1990. </year>
Reference: [19] <author> L. Bottou and V. Vapnik, </author> <title> "Local learning algorithms," </title> <journal> Neural Computation, </journal> <volume> vol. 4, </volume> <pages> pp. 888-900, </pages> <year> 1992. </year>
Reference: [20] <author> T. M. Martinetz, S. G. Berkovich, and K. J. Schulten, </author> " <title> `Neural-Gas' network for vector quantization and its application to time-series prediction," </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 4, </volume> <pages> pp. 558-569, </pages> <year> 1993. </year>
Reference: [21] <author> R. Murray-Smith, </author> <title> A Local Model Network Approach to Nonlinear Modelling, </title> <type> PhD Thesis, </type> <institution> Department of Computer Science, University of Strathclyde, </institution> <year> 1994. </year>
Reference: [22] <author> M. I. Jordan and R. A. Jacobs, </author> <title> "Hierarchical mixtures of experts and the EM algorithm," </title> <journal> Neural Computation, </journal> <volume> vol. 6, </volume> <pages> pp. 181-214, </pages> <year> 1994. </year>
Reference-contexts: Methods similar to the mixture of experts have been proposed in the literature [16]-[21]. This approach has later been generalized to learn "hierarchies of experts" <ref> [22] </ref>. B. Formalization For each class C j , we are given a set of data pairs fx t ; y t g t where x is a d-dimensional input vector and y is a binary value that is 1 if x 2 C j and 0 otherwise. <p> Here we used a gradient-based method for computing the parameters. When as in normal mixtures, there is a probabilistic setting where the goodness function is maximum likelihood, the Expectation-Maximization algorithm can also be used to train the networks <ref> [22] </ref>. The next step is to compare these approaches on other problems with different characteristics, e.g., input dimensionality, continuous inputs, training set size etc and also on regression problems.
Reference: [23] <author> S. J. Nowlan, </author> <title> Soft Competitive Adaptation: Neural Network Learning Algorithms based on Fitting Statistical Mixtures, </title> <type> PhD Thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1990. </year>
Reference-contexts: We discuss this in more detail in Section C. 2) Competing Learners: First proposed by Jacobs et al. [8], a measure that forces competition is to view the architecture as a mixture model <ref> [23] </ref>. The gating values are the mixture proportions and the expert perceptron outputs are the means.
Reference: [24] <author> Garris, M. D., J. L. Blue, G. T. Candela, D. L. Dim-mick, J. Geist, P. J. Grother, S. A. Janet and C. L. </author> <title> Wilson (1994). NIST Form-Based Handprint Recognition System, </title> <type> NI-STIR 5469, </type> <institution> National Institute of Standards and Technology, Computer Systems Laboratory, Advanced Systems Division, Gaithersburg, MD, USA. </institution>
References-found: 24

