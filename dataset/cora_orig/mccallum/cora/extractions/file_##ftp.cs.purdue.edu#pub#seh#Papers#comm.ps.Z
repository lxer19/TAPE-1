URL: file://ftp.cs.purdue.edu/pub/seh/Papers/comm.ps.Z
Refering-URL: http://www.cs.purdue.edu/people/seh/
Root-URL: http://www.cs.purdue.edu
Email: seh@cs.purdue.edu  hameed@cs.purdue.edu  ashfaq@cs.purdue.edu  
Title: Communication Operations on Coarse-Grained Mesh Architectures  
Author: Susanne E. Hambrusch Farooq Hameed Ashfaq A. Khokhar 
Keyword: Parallel processing, coarse-grained machines, communication operations, scal ability.  
Date: September 19, 1994  
Address: West Lafayette, IN 47907, USA  West Lafayette, IN 47907, USA  West Lafayette, IN 47907, USA  
Affiliation: Department of Computer Sciences Purdue University  Department of Computer Sciences Purdue University  School of Electrical Engineering and Department of Computer Sciences Purdue University  
Abstract: In this paper we consider three frequently arising communication operations, one-to-all, all-to-one, and all-to-all. We describe architecture-independent solutions for each operation, as well as solutions tailored towards the mesh architecture. We show how the relationship among the parameters of a parallel machine and the relationship of these parameters to the message size determines the best solution. We discuss performance and scalability issues of our solutions on the Intel Touchstone Delta. Our results show that in order to cover a broad range of scalability for a particular operation, multiple solutions should be employed. fl Research supported in part by ARPA under contract DABT63-92-C-0022ONR. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing official policies, expressed or implied, of the U.S. government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. Bala, J. Bruck, R. Cypher, P. Elustondo, A. Ho, C.-T. Ho, S. Kipnis, and M. Snir, </author> <title> "CCL: A Portable and Tunable Collective Communication Library for Scalable Parallel Computers," </title> <booktitle> Proceedings of 8-th International Parallel Processing Symposium, </booktitle> <pages> pp. 835-844, </pages> <year> 1994. </year>
Reference-contexts: Our conclusion is that for a given operation, different algorithms scale well for different ranges of input and different machine characteristics. This agrees with related work reported in <ref> [1, 2, 3, 4, 5, ?, 14, 15] </ref>. We support our conclusion by presenting the performance of a number of diverse implementations on the Intel Touchstone Delta [13]. Some of our algorithms use well-know approaches, while others make use of characteristics intrinsic to the Intel Delta.
Reference: [2] <author> M. Barnett, S. Gupta, D. G. Payne, L. Shuler, R. van de Geijn, and J. Watts, </author> <title> "Inter-processor Collective Communication Library," </title> <booktitle> Proceedings of Scalable High-Performance Computing Conference, </booktitle> <pages> pp. 357-364, </pages> <year> 1994. </year>
Reference-contexts: Our conclusion is that for a given operation, different algorithms scale well for different ranges of input and different machine characteristics. This agrees with related work reported in <ref> [1, 2, 3, 4, 5, ?, 14, 15] </ref>. We support our conclusion by presenting the performance of a number of diverse implementations on the Intel Touchstone Delta [13]. Some of our algorithms use well-know approaches, while others make use of characteristics intrinsic to the Intel Delta.
Reference: [3] <author> M. Barnett, R. Littlefield, D. G. Payne, and R. van de Geijn, </author> <title> "Global Combine on Meshes Architecures with Wormhole Routing," </title> <booktitle> Proceedings of 7-th International Parallel Processing Symposium, </booktitle> <pages> pp. 156-162, </pages> <year> 1993. </year>
Reference-contexts: Our conclusion is that for a given operation, different algorithms scale well for different ranges of input and different machine characteristics. This agrees with related work reported in <ref> [1, 2, 3, 4, 5, ?, 14, 15] </ref>. We support our conclusion by presenting the performance of a number of diverse implementations on the Intel Touchstone Delta [13]. Some of our algorithms use well-know approaches, while others make use of characteristics intrinsic to the Intel Delta.
Reference: [4] <author> S.H. Bokhari, </author> <title> "Multiphase Complete Exchange on a Circuit Switched Hypercube," </title> <booktitle> Proceedings of 1991 International Conference on Parallel Processing, </booktitle> <pages> pp. 525-529, </pages> <year> 1991. </year> <month> 23 </month>
Reference-contexts: Our conclusion is that for a given operation, different algorithms scale well for different ranges of input and different machine characteristics. This agrees with related work reported in <ref> [1, 2, 3, 4, 5, ?, 14, 15] </ref>. We support our conclusion by presenting the performance of a number of diverse implementations on the Intel Touchstone Delta [13]. Some of our algorithms use well-know approaches, while others make use of characteristics intrinsic to the Intel Delta. <p> We start with the description of the first 2-level algorithm. It consists of 3 steps and we 18 refer to it as the 3-step algorithm. A similar approach for hypercube architectures has been described in <ref> [4] </ref>. In each step of the algorithm every processor sends out a total of pL bytes; the first and the last step send out pL bytes in the form of p p messages and the second step sends pL bytes as one single message. <p> After the received messages have been combined with the messages that remained in the processor, all-to-all is recursively performed on the two p=2-processor submachines. This approach has consistently 19 been judged as being expensive for large message sizes <ref> [4, 15] </ref>. 5.3 Implementations and Experimental Results We have implemented a total of eight all-to-all algorithms on the Delta. This includes four 1-level algorithms: Algorithm 1-lev-dir, in which each processor simply issues its p 1 sends and p 1 receives, and three algorithms that partition all-to-all communication into permutations.
Reference: [5] <author> Z. Bozkus, S. Ranka, G. </author> <title> Fox "Benchmarking the CM-5 Multicomputer," </title> <booktitle> Proceedings of 4-th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pp. 100-107, </pages> <year> 1992. </year>
Reference-contexts: Our conclusion is that for a given operation, different algorithms scale well for different ranges of input and different machine characteristics. This agrees with related work reported in <ref> [1, 2, 3, 4, 5, ?, 14, 15] </ref>. We support our conclusion by presenting the performance of a number of diverse implementations on the Intel Touchstone Delta [13]. Some of our algorithms use well-know approaches, while others make use of characteristics intrinsic to the Intel Delta.
Reference: [6] <author> J.J. Dongarra, R. Hempel, A.J.G. Hey, D.W. Walker. </author> <title> "A Proposal for a User-level, Message Passing Interface in a Distributed Memory Environment", </title> <type> Technical Report TM 12231, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: For a mesh, a scaled down version may be a smaller mesh with the same aspect ratio or a linear array. This is a stronger requirement than the use of process groups as proposed by the MPI Message Passing Standards <ref> [6] </ref>. (The MPI standard allows a process group to consists of an ordered collection of processors.) Throughout the paper, we refer to such scaled down versions as submachines. We assume that communication within different submachines occurs without interference from other submachines. <p> Our code was written in C. 3 One-to-all Communication In one-to-all communication, a source processor P s sends out p 1 distinct messages, each to a different destination. One-to-all is also refered to as the scatter or personalized broadcast operation <ref> [6, 10, 11] </ref>. The source processor is clearly the bottleneck. In Section 3.1, we use the concept of a k-level algorithm to describe different algorithms. <p> Which one of them gives the best performance will depend on the ratio between the send and receive times, the packet length, the ratio between processor and network bandwidth, and the message setup cost. 4 All-to-one Communication In all-to-one communication, also known as the gather operation <ref> [6] </ref>, every processor sends a message to a destination processor, P d . Processor P d is now the bottleneck. Conceptually, all-to-one is the inverse of one-to-all. However, from a practical point of view, the best one-to-all algorithms do not necessarily correspond to the best all-to-one algorithms.
Reference: [7] <author> G. Fox, M Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker, </author> <title> Solving Problems on Concurrent Processors, </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: One approach is to have processor P s issue p 1 direct sends. This strategy is commonly used by programmers not familiar with parallel processing. It is generally considered as not giving the best performance <ref> [7, 11] </ref> but may perform well on small machine sizes (e.g., fewer than 16 processors). Another approach for one-to-all is to have processor P s form one long message of size L (p 1) which is broadcast to every processor (i.e., the effective message size is L (p 1)).
Reference: [8] <author> S.E. Hambrusch, A. Khokhar, </author> <title> "C 3 : An Architecture-independent Model for Coarse-Grained Parallel Machines", </title> <type> Technical Report, </type> <institution> Purdue University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: We also address scalability issues and provide insight into the behavior of various algorithms on different machine sizes and message sizes. Our algorithms assume that computation is synchronized by a barrier-style synchronization 2 mechanism similar to the one described in <ref> [8, 17] </ref>. More precisely, an algorithm can be parti-tioned into a sequence of supersteps, with each superstep corresponding to local computation followed by sending and receiving messages. Synchronization occurs between supersteps. In order to classify different approaches used in our implementations, we introduce the notion of a k-level algorithm.
Reference: [9] <author> F. Harary, </author> <title> Graph Theory, </title> <publisher> Addison-Wesley, </publisher> <year> 1972. </year>
Reference-contexts: Next, determine a tournament involving k=2 "players". Such a tournament consists of k=2 1 rounds, where in each round one player is matched up with exactly one other player. The rounds can be generated by using, for example, the method given in <ref> [9] </ref> for finding the 1-factors of a complete graph. Assume i is matched up with j in a round, 0 i &lt; j &lt; k=2. Then, the cycle P i ! P j ! P ki1 ! P kj1 ! P i describes the sending of four messages.
Reference: [10] <author> S.L. Johnsson, C.-T. Ho, </author> <title> "Optimum Broadcasting and Personalized Communication in Hypercubes," </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 38, </volume> <pages> pp. 1249-1268, </pages> <year> 1989. </year>
Reference-contexts: Our code was written in C. 3 One-to-all Communication In one-to-all communication, a source processor P s sends out p 1 distinct messages, each to a different destination. One-to-all is also refered to as the scatter or personalized broadcast operation <ref> [6, 10, 11] </ref>. The source processor is clearly the bottleneck. In Section 3.1, we use the concept of a k-level algorithm to describe different algorithms.
Reference: [11] <author> V. Kumar, A. Grama, A.. Gupta, and G. Karypis, </author> <title> Introduction to Parallel Computing, </title> <publisher> Benjamin/Cummings Publishing, </publisher> <year> 1994. </year>
Reference-contexts: Our code was written in C. 3 One-to-all Communication In one-to-all communication, a source processor P s sends out p 1 distinct messages, each to a different destination. One-to-all is also refered to as the scatter or personalized broadcast operation <ref> [6, 10, 11] </ref>. The source processor is clearly the bottleneck. In Section 3.1, we use the concept of a k-level algorithm to describe different algorithms. <p> One approach is to have processor P s issue p 1 direct sends. This strategy is commonly used by programmers not familiar with parallel processing. It is generally considered as not giving the best performance <ref> [7, 11] </ref> but may perform well on small machine sizes (e.g., fewer than 16 processors). Another approach for one-to-all is to have processor P s form one long message of size L (p 1) which is broadcast to every processor (i.e., the effective message size is L (p 1)).
Reference: [12] <author> F. Thomson Leighton, </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays . Trees . Hypercubes, </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: When performing an all-to-all, the congestion arising because of the bisection width of the underlying architecture can significantly influence the performance. The bisection width of a machine is the minimum number of links that have to be removed to disconnect the machine into two equal-sized halves <ref> [12] </ref>. In a p-processor architecture with a bisection width of b, at least one of the b links partitioning the machine is used by at least p 2 =4b messages during an all-to-all communication.
Reference: [13] <author> S. Lillevik, </author> <title> "The Touchstone 30 Gigaflop DELTA Prototype," </title> <booktitle> Proceedings of 6-th Distributed Memory Computing Conference, </booktitle> <pages> pp. 671-677, </pages> <year> 1991. </year>
Reference-contexts: This agrees with related work reported in [1, 2, 3, 4, 5, ?, 14, 15]. We support our conclusion by presenting the performance of a number of diverse implementations on the Intel Touchstone Delta <ref> [13] </ref>. Some of our algorithms use well-know approaches, while others make use of characteristics intrinsic to the Intel Delta. We also address scalability issues and provide insight into the behavior of various algorithms on different machine sizes and message sizes. <p> For a more complete description we refer to <ref> [13] </ref>. The Intel Touchstone Delta is a coarse-grained multi-processor system with 512 nodes organized as a 16 fi 32 2-dimensional mesh. Each node is directly connected to its 4 nearest neighbors. The communication network uses wormhole routing.
Reference: [14] <author> R. Ponnusamy, A. Choudhary, G. Fox, </author> <title> "Communication Overhead on CM5: An Experimental Performance Evaluation," </title> <booktitle> Proceedings of 4-th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pp. 108-115, </pages> <year> 1992. </year>
Reference-contexts: Our conclusion is that for a given operation, different algorithms scale well for different ranges of input and different machine characteristics. This agrees with related work reported in <ref> [1, 2, 3, 4, 5, ?, 14, 15] </ref>. We support our conclusion by presenting the performance of a number of diverse implementations on the Intel Touchstone Delta [13]. Some of our algorithms use well-know approaches, while others make use of characteristics intrinsic to the Intel Delta. <p> When partitioning into exclusive-or permutations, all-to-all is partitioned such that in the i-th permutation processor P j sends a message to P ij , where represents the exculsive-or operation. Implementations of these approaches on different architectures have shown exclusive-or permutations to be superior to linear permutations <ref> [14, 15] </ref>. In order to evaluate different partitioning schemas, we define two quantities, max load and sum load. Assume all-to-all is partitioned into p permutations, 0 ; : : : p1 .
Reference: [15] <author> R. Thakur, A. Choudhary, </author> <title> "All-to-all Communication on Meshes with Wormhole Routing," </title> <booktitle> Proceedings of 8-th International Parallel Processing Symposium, </booktitle> <pages> pp. 561-565, </pages> <year> 1994. </year>
Reference-contexts: Our conclusion is that for a given operation, different algorithms scale well for different ranges of input and different machine characteristics. This agrees with related work reported in <ref> [1, 2, 3, 4, 5, ?, 14, 15] </ref>. We support our conclusion by presenting the performance of a number of diverse implementations on the Intel Touchstone Delta [13]. Some of our algorithms use well-know approaches, while others make use of characteristics intrinsic to the Intel Delta. <p> When partitioning into exclusive-or permutations, all-to-all is partitioned such that in the i-th permutation processor P j sends a message to P ij , where represents the exculsive-or operation. Implementations of these approaches on different architectures have shown exclusive-or permutations to be superior to linear permutations <ref> [14, 15] </ref>. In order to evaluate different partitioning schemas, we define two quantities, max load and sum load. Assume all-to-all is partitioned into p permutations, 0 ; : : : p1 . <p> After the received messages have been combined with the messages that remained in the processor, all-to-all is recursively performed on the two p=2-processor submachines. This approach has consistently 19 been judged as being expensive for large message sizes <ref> [4, 15] </ref>. 5.3 Implementations and Experimental Results We have implemented a total of eight all-to-all algorithms on the Delta. This includes four 1-level algorithms: Algorithm 1-lev-dir, in which each processor simply issues its p 1 sends and p 1 receives, and three algorithms that partition all-to-all communication into permutations. <p> We use Algorithm 1-lev-xor as the 1-level algorithm within the columns (and then the rows). For the sake of comparison, we also considered a variation of Algorithm 2-lev-c,r reported in <ref> [15] </ref>. As in 2 lev c; r, a processor in S i sends the corresponding data to processor P ij .
Reference: [16] <author> D.S. Scott, </author> <title> "Efficient All-to-All Communication Patterns in Hypercube and Mesh Topologies," </title> <booktitle> Proceedings of 6-th Distributed Memory Computing Conference, </booktitle> <pages> pp. 398-403, </pages> <year> 1991. </year>
Reference-contexts: Consider a p-processor square mesh architecture with p p being a multiple of 4. Any parti tioning into permutations gives max load p p 4 and sum load p 3=2 4 <ref> [16] </ref>. Linear and exclusive or permutation have max load = p 2 , which is a factor of 2 off from the optimal max load. <p> For exclusive-or permutations we have sum load = 3 7 p 3=2 , which is a factor of 12 7 off from the optimal sum load. Using an approach developed in <ref> [16] </ref>, all-to-all communication can be par titioned into p permutations achieving max load = p p 4 and sum load p 3=2 4 . We refer to this approach as partitioning into balanced permutations. For completeness sake, we describe the method given in [16] for generating balanced permutations. <p> Using an approach developed in <ref> [16] </ref>, all-to-all communication can be par titioned into p permutations achieving max load = p p 4 and sum load p 3=2 4 . We refer to this approach as partitioning into balanced permutations. For completeness sake, we describe the method given in [16] for generating balanced permutations. We start by describing balanced permutations for linear arrays. The permutations for the mesh are obtained by performing a cross product. Consider a k-processor linear array. Assume, for the time being, that k is a multiple of 4.
Reference: [17] <author> L.G. Valiant, </author> <title> "A Bridging Model for Parallel Computation," </title> <journal> Communications of the ACM, 1990, </journal> <volume> Vol. 33, No. 8, </volume> <pages> pp. 103-111. 24 </pages>
Reference-contexts: We also address scalability issues and provide insight into the behavior of various algorithms on different machine sizes and message sizes. Our algorithms assume that computation is synchronized by a barrier-style synchronization 2 mechanism similar to the one described in <ref> [8, 17] </ref>. More precisely, an algorithm can be parti-tioned into a sequence of supersteps, with each superstep corresponding to local computation followed by sending and receiving messages. Synchronization occurs between supersteps. In order to classify different approaches used in our implementations, we introduce the notion of a k-level algorithm.
References-found: 17

