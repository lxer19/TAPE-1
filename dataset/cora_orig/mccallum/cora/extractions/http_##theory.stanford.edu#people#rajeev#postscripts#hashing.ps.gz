URL: http://theory.stanford.edu/people/rajeev/postscripts/hashing.ps.gz
Refering-URL: http://www.cs.duke.edu/CGC/workshop97/schedule.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: indyk@cs.stanford.edu  rajeev@theory.stanford.edu  pragh@almaden.ibm.com  vempala@cs.cmu.edu  
Title: Locality-Preserving Hashing in Multidimensional Spaces  
Author: Piotr Indyk Rajeev Motwani Prabhakar Raghavan Santosh Vempala 
Address: Stanford University  Stanford University  
Affiliation: Department of Computer Science  Department of Computer Science  IBM Almaden Research Center  School of Computer Science Carnegie-Mellon University  
Abstract: We consider locality-preserving hashing | in which adjacent points in the domain are mapped to adjacent or nearly-adjacent points in the range | when the domain is a d-dimensional cube. This problem has applications to high-dimensional search and multimedia indexing. We show that simple and natural classes of hash functions are provably good for this problem. We complement this with lower bounds suggesting that our results are essentially the best possible. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.K. Agarwal and J. Matousek, </author> <title> "Ray shooting and parametric search," </title> <booktitle> Proc. 24th STOC (1992), </booktitle> <volume> 517 - 526. </volume>
Reference-contexts: 1 Introduction In a recent paper, Linial and Sasson [21] proved the following theorem about hash functions: Theorem 1 There exists a family G of functions from an integer line <ref> [1; : : : ; U ] </ref> to [1; : : : ; R] and a constant C such that for any S [1; : : : ; U ] with jSj C p * Pr f2G (f jS is one to one) 1 * all f 2 G are non-expansive, <p> 1 Introduction In a recent paper, Linial and Sasson [21] proved the following theorem about hash functions: Theorem 1 There exists a family G of functions from an integer line [1; : : : ; U ] to <ref> [1; : : : ; R] </ref> and a constant C such that for any S [1; : : : ; U ] with jSj C p * Pr f2G (f jS is one to one) 1 * all f 2 G are non-expansive, i.e., for any p; q 2 U d <p> a recent paper, Linial and Sasson [21] proved the following theorem about hash functions: Theorem 1 There exists a family G of functions from an integer line <ref> [1; : : : ; U ] </ref> to [1; : : : ; R] and a constant C such that for any S [1; : : : ; U ] with jSj C p * Pr f2G (f jS is one to one) 1 * all f 2 G are non-expansive, i.e., for any p; q 2 U d (f (p); f (q)) d (p; q). <p> Recently, Klein-berg [17] has developed a scheme for approximate nearest neighbor problem that achieves query time O (d 2 log n) with preprocessing n O (d) . There have been a number of other approaches and extensions (e.g. <ref> [31, 23, 25, 1, 2] </ref>). The best approaches from these studies are still impractical for the values of d encountered in the retrieval applications above. Overview of Paper. <p> Then r (p) = (bx 0 p c; by 0 The function r rotates the domain cube, and then rounds off the rotated points to the nearest lattice points. It will suffice to restrict the entries of M to the discrete set of multiples of 1=R in the range <ref> [0; 1] </ref>. Hence, the size of the family H 2 is equal to jGjR 2 . <p> It is now sufficient to show that Pr (jM z jx i j 1 for all i 2 K) = Pr (jlu i j 1 for all i 2 K) p R : d be the set &lt; i1 fi <ref> [1; 1] </ref> fi &lt; di and let = sin 1 (1=l). The probability Pr (ju i j 1 for i 2 K) is equal to the area of the surface of P d (l) = B d (l) " " i2K C d divided by jS d (l)j.
Reference: [2] <author> S. Arya, D. M. Mount, N.S. Netanyahu, R. Silverman, A. Wu, </author> <title> "An optimal algorithm for approximate nearest neighbor searching," </title> <booktitle> Proc. 5th SODA (1994), </booktitle> <pages> pp. 573 - 582. </pages>
Reference-contexts: Recently, Klein-berg [17] has developed a scheme for approximate nearest neighbor problem that achieves query time O (d 2 log n) with preprocessing n O (d) . There have been a number of other approaches and extensions (e.g. <ref> [31, 23, 25, 1, 2] </ref>). The best approaches from these studies are still impractical for the values of d encountered in the retrieval applications above. Overview of Paper.
Reference: [3] <author> C. Buckley, A. Singhal, M. Mitra, and G. Salton, </author> <title> New Retrieval Approaches Using SMART: </title> <booktitle> TREC 4. Proc. Fourth Text Retrieval Conference, </booktitle> <institution> National Institute of Standards and Technology, </institution> <year> 1995. </year>
Reference-contexts: Other instances of near-neighbor search appear in algorithms for pattern recognition [6, 10], statistics and data analysis [26, 8], machine learning [5], data compression [12], data mining [13] and image analysis [20]. In the case of text retrieval, vector-space methods <ref> [3, 28] </ref> map each document into a point in high-dimensional space.
Reference: [4] <author> K. Clarkson, </author> <title> "A randomized algorithm for closest-point queries," </title> <journal> SIAM J. Computing, </journal> <volume> 17 (1988), </volume> <pages> pp. 830-847. </pages>
Reference-contexts: This was improved by Clarkson <ref> [4] </ref>: he gave an algorithm with query time O (exp (d)log n) and pre-processing O (n dd=2e (1+") ); here exp (d) denotes a function that grows at least as quickly as 2 d .
Reference: [5] <author> S. Cost and S. Salzberg, </author> <title> "A weighted nearest neighbor algorithm for learning with symbolic features," </title> <booktitle> Machine Learning, 10 (1993), </booktitle> <pages> pp. 57-67. </pages>
Reference-contexts: The main application comes from information retrieval: the process of retrieving text and multimedia documents matching a specified query. Other instances of near-neighbor search appear in algorithms for pattern recognition [6, 10], statistics and data analysis [26, 8], machine learning <ref> [5] </ref>, data compression [12], data mining [13] and image analysis [20]. In the case of text retrieval, vector-space methods [3, 28] map each document into a point in high-dimensional space.
Reference: [6] <author> T.M. Cover and P.E. Hart, </author> <title> "Nearest neighbor pattern classification," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13 (1967), </volume> <pages> pp. 21-27. </pages>
Reference-contexts: However, efficient near-neighbor retrieval is considerably harder, and of growing importance, in higher dimensions. The main application comes from information retrieval: the process of retrieving text and multimedia documents matching a specified query. Other instances of near-neighbor search appear in algorithms for pattern recognition <ref> [6, 10] </ref>, statistics and data analysis [26, 8], machine learning [5], data compression [12], data mining [13] and image analysis [20]. In the case of text retrieval, vector-space methods [3, 28] map each document into a point in high-dimensional space.
Reference: [7] <author> S. Deerwester, S. T. Dumais, T.K. Landauer, G.W. Fur-nas, and R.A. Harshman, </author> <title> "Indexing by latent semantic analysis," </title> <journal> Journal of the Society for Information Science, </journal> <volume> 41 (1990), </volume> <pages> pp. 391-407. </pages>
Reference-contexts: In the case of text retrieval, vector-space methods [3, 28] map each document into a point in high-dimensional space. Sometimes, statistical techniques such as principal components analysis [14], latent semantic indexing <ref> [7] </ref> or the Karhunen-Loeve/Hotelling transform [16, 22] are used to reduce the dimensionality of the vector space in which the documents are represented, but the number of dimensions could still be very large (say, even 200).
Reference: [8] <author> L. Devroye and T.J. Wagner, </author> <title> "Nearest neighbor methods in discrimination," </title> <journal> Handbook of Statistics, </journal> <volume> vol. 2, </volume> <editor> P.R. Krishnaiah, L.N. Kanal, eds., </editor> <publisher> North-Holland, </publisher> <year> 1982. </year>
Reference-contexts: The main application comes from information retrieval: the process of retrieving text and multimedia documents matching a specified query. Other instances of near-neighbor search appear in algorithms for pattern recognition [6, 10], statistics and data analysis <ref> [26, 8] </ref>, machine learning [5], data compression [12], data mining [13] and image analysis [20]. In the case of text retrieval, vector-space methods [3, 28] map each document into a point in high-dimensional space.
Reference: [9] <author> D. Dobkin and R. Lipton, </author> <title> "Multidimensional search problems," </title> <journal> SIAM J. Computing, </journal> <volume> 5 (1976), </volume> <pages> pp. 181-186. </pages>
Reference-contexts: The field of computational geometry has developed a rich theory for the study of proximity problems. Establishing upper bounds on the time required to answer a nearest-neighbor query in &lt; d appears to have been first undertaken by Dobkin and Lipton <ref> [9] </ref>; they provided an algorithm with query time O (2 d log n) and pre-processing O (n 2 d ).
Reference: [10] <author> R.O. Duda and P.E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: However, efficient near-neighbor retrieval is considerably harder, and of growing importance, in higher dimensions. The main application comes from information retrieval: the process of retrieving text and multimedia documents matching a specified query. Other instances of near-neighbor search appear in algorithms for pattern recognition <ref> [6, 10] </ref>, statistics and data analysis [26, 8], machine learning [5], data compression [12], data mining [13] and image analysis [20]. In the case of text retrieval, vector-space methods [3, 28] map each document into a point in high-dimensional space.
Reference: [11] <author> C. Faloutsos, R. Barber, M. Flickner, W. Niblack, D. Petkovic and W. Equitz, </author> <title> "Efficient and effective querying by image content", </title> <journal> Journal of Intelligent Information Systems, </journal> <volume> 3 (1994), </volume> <pages> pp. 231-262. </pages>
Reference-contexts: In image and multimedia retrieval, a common first step is to extract a set of numerically-valued features or parameters from the document. For instance, IBM's Query-by-image-content <ref> [11] </ref> and MIT's Photobook [27] extract image features such as color histograms (hues, intensities), shape descriptors, as well as quantities measuring texture.
Reference: [12] <author> A. Gersho and R.M. Gray, </author> <title> Vector Quantization and Signal Compression, </title> <publisher> Kluwer Academic, </publisher> <year> 1991. </year>
Reference-contexts: The main application comes from information retrieval: the process of retrieving text and multimedia documents matching a specified query. Other instances of near-neighbor search appear in algorithms for pattern recognition [6, 10], statistics and data analysis [26, 8], machine learning [5], data compression <ref> [12] </ref>, data mining [13] and image analysis [20]. In the case of text retrieval, vector-space methods [3, 28] map each document into a point in high-dimensional space.
Reference: [13] <author> T. Hastie and R. Tibshirani, </author> <title> "Discriminant adaptive nearest neighbor classification," </title> <booktitle> First International Conference on Knowledge Discovery and Data Mining, </booktitle> <year> 1995. </year>
Reference-contexts: The main application comes from information retrieval: the process of retrieving text and multimedia documents matching a specified query. Other instances of near-neighbor search appear in algorithms for pattern recognition [6, 10], statistics and data analysis [26, 8], machine learning [5], data compression [12], data mining <ref> [13] </ref> and image analysis [20]. In the case of text retrieval, vector-space methods [3, 28] map each document into a point in high-dimensional space.
Reference: [14] <author> H. Hotelling, </author> <title> "Analysis of a complex of statistical variables into principal components", </title> <journal> Journal of educational psychology, </journal> <volume> 27 (1933), </volume> <pages> pp. 417-441. </pages>
Reference-contexts: In the case of text retrieval, vector-space methods [3, 28] map each document into a point in high-dimensional space. Sometimes, statistical techniques such as principal components analysis <ref> [14] </ref>, latent semantic indexing [7] or the Karhunen-Loeve/Hotelling transform [16, 22] are used to reduce the dimensionality of the vector space in which the documents are represented, but the number of dimensions could still be very large (say, even 200).
Reference: [15] <institution> IEEE Computer Special Issue on Content-based Image Retrieval Systems, </institution> <month> 28 </month> <year> (1995). </year>
Reference: [16] <author> K. Karhunen. </author> <title> Uber lineare Meth-oden in der Wahrscheinlichkeitsrechnung. </title> <journal> Ann. Acad. Sci. Fennicae, Ser. </journal> <volume> A137, </volume> <year> 1947. </year>
Reference-contexts: In the case of text retrieval, vector-space methods [3, 28] map each document into a point in high-dimensional space. Sometimes, statistical techniques such as principal components analysis [14], latent semantic indexing [7] or the Karhunen-Loeve/Hotelling transform <ref> [16, 22] </ref> are used to reduce the dimensionality of the vector space in which the documents are represented, but the number of dimensions could still be very large (say, even 200).
Reference: [17] <author> J. Kleinberg, </author> <title> "Two algorithms for nearest-neighbor search in high dimensions", </title> <booktitle> these proceedings. </booktitle>
Reference-contexts: The query time was later improved by Meiser [24] to O (d 5 log n) with pre-processing O (n d+* ). Recently, Klein-berg <ref> [17] </ref> has developed a scheme for approximate nearest neighbor problem that achieves query time O (d 2 log n) with preprocessing n O (d) . There have been a number of other approaches and extensions (e.g. [31, 23, 25, 1, 2]).
Reference: [18] <author> D. Knuth, </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> vol. </volume> <month> 3, </month> <title> Sorting and Searching, </title> <publisher> Addison Wesley, </publisher> <year> 1973. </year>
Reference-contexts: The advantage of the locality-preserving property is that it affords good paging performance: since the neighborhood of q (in the domain) is not scattered all over the range, the neighborhood of h (q) in the hash table exhibits good locality of reference. This is counter to Knuth's suggestion (see <ref> [18] </ref>, p. 540) that In a virtual memory enviroment we probably ought to use tree search or digital tree search, instead of creating a large scatter table that requires bringing a new page nearly every time we hash a key.
Reference: [19] <author> E. </author> <title> Kreyszig, Introductory Functional Analysis with Applications, </title> <publisher> Wiley, </publisher> <year> 1989. </year>
Reference-contexts: The reason is that the rotation is defined as a linear transformation described by a matrix with orthonormal columns. However, it is known that for R d with l p norm, where p 6= 2, no inner product exists (cf. <ref> [19] </ref>, p. 133), and so orthonormality in such spaces is not well-defined. 5 Negative Results Can small bucket size be achieved with non-expansive hash functions? In this section we prove lower bounds that indicate the contrary. First we focus on lower bounding the collision probability for non-expansive functions.
Reference: [20] <author> V. Koivune and S. Kassam, </author> <title> "Nearest neighbor filters for multivariate data," </title> <booktitle> IEEE Workshop on Nonlinear Signal and Image Processing, </booktitle> <year> 1995. </year>
Reference-contexts: Other instances of near-neighbor search appear in algorithms for pattern recognition [6, 10], statistics and data analysis [26, 8], machine learning [5], data compression [12], data mining [13] and image analysis <ref> [20] </ref>. In the case of text retrieval, vector-space methods [3, 28] map each document into a point in high-dimensional space.
Reference: [21] <author> N. Linial and O. Sasson, </author> <title> Non-Expansive Hashing, </title> <booktitle> In Proc. 28th STOC (1996), </booktitle> <pages> pp. 509-517. </pages>
Reference-contexts: 1 Introduction In a recent paper, Linial and Sasson <ref> [21] </ref> proved the following theorem about hash functions: Theorem 1 There exists a family G of functions from an integer line [1; : : : ; U ] to [1; : : : ; R] and a constant C such that for any S [1; : : : ; U ] <p> If c = 0, then h is said to be non-expansive under d. In our constructions we use the 1-dimensional family of hash functions G introduced by Linial and Sasson <ref> [21] </ref>. Their functions are obtained by "folding" the domain D along randomly-chosen turning points in such a way that any segment of D between two consecutive turning points has length fi (R) (see [21] for a formal definition). <p> In our constructions we use the 1-dimensional family of hash functions G introduced by Linial and Sasson <ref> [21] </ref>. Their functions are obtained by "folding" the domain D along randomly-chosen turning points in such a way that any segment of D between two consecutive turning points has length fi (R) (see [21] for a formal definition). Besides the properties described in the introduction, G has the property that Pr g2G (g (x) = g (y)) = O (1=R) for any x 6= y 2 D. <p> We conclude that with probability at least 1 A=R, both coordinates of r (p) and r (q) will be different. 2 In order to prove the next lemma, we need the following fact which follows from the analysis due to Linial and Sasson <ref> [21] </ref>. <p> In this way we can obtain perfect family of hash functions with constant multiplicative and p d additive expansion terms. Can one get small bucket size with O (R) elements (rather than O (R 1=2 ))? It is quite easy (by adopting the hashing scheme of <ref> [21] </ref>) to store O (R) elements in O (R) buckets such that each element can be stored in at most O (log log R + log d) buckets and each bucket has O (1) d O (log log R) size.
Reference: [22] <editor> M. Loeve. Fonctions aleastoires de second ordere. Pro-cessus Stochastiques et mouvement Brownian. Her-mann, </editor> <address> Paris, </address> <year> 1948. </year>
Reference-contexts: In the case of text retrieval, vector-space methods [3, 28] map each document into a point in high-dimensional space. Sometimes, statistical techniques such as principal components analysis [14], latent semantic indexing [7] or the Karhunen-Loeve/Hotelling transform <ref> [16, 22] </ref> are used to reduce the dimensionality of the vector space in which the documents are represented, but the number of dimensions could still be very large (say, even 200).
Reference: [23] <author> J. Matousek, </author> <title> "Reporting points in halfspaces," </title> <booktitle> Proc. 32nd FOCS, </booktitle> <year> 1991. </year>
Reference-contexts: Recently, Klein-berg [17] has developed a scheme for approximate nearest neighbor problem that achieves query time O (d 2 log n) with preprocessing n O (d) . There have been a number of other approaches and extensions (e.g. <ref> [31, 23, 25, 1, 2] </ref>). The best approaches from these studies are still impractical for the values of d encountered in the retrieval applications above. Overview of Paper.
Reference: [24] <author> S. Meiser, </author> <title> "Point location in arrangements of hyper-planes," </title> <journal> Information and Computation (1993), </journal> <volume> 106, 2, </volume> <pages> pp. 286-303. </pages>
Reference-contexts: This was improved by Clarkson [4]: he gave an algorithm with query time O (exp (d)log n) and pre-processing O (n dd=2e (1+") ); here exp (d) denotes a function that grows at least as quickly as 2 d . The query time was later improved by Meiser <ref> [24] </ref> to O (d 5 log n) with pre-processing O (n d+* ). Recently, Klein-berg [17] has developed a scheme for approximate nearest neighbor problem that achieves query time O (d 2 log n) with preprocessing n O (d) .
Reference: [25] <author> K. Mulmuley, </author> <title> "Randomized multi-dimensional search trees: further results in dynamic sampling," </title> <booktitle> Proc. 32nd FOCS, </booktitle> <year> 1991. </year>
Reference-contexts: Recently, Klein-berg [17] has developed a scheme for approximate nearest neighbor problem that achieves query time O (d 2 log n) with preprocessing n O (d) . There have been a number of other approaches and extensions (e.g. <ref> [31, 23, 25, 1, 2] </ref>). The best approaches from these studies are still impractical for the values of d encountered in the retrieval applications above. Overview of Paper.
Reference: [26] <institution> Panel on Discriminant Analysis and Clustering, National Research Council, Discriminant Analysis and Clustering, National Academy Press, </institution> <year> 1988. </year>
Reference-contexts: The main application comes from information retrieval: the process of retrieving text and multimedia documents matching a specified query. Other instances of near-neighbor search appear in algorithms for pattern recognition [6, 10], statistics and data analysis <ref> [26, 8] </ref>, machine learning [5], data compression [12], data mining [13] and image analysis [20]. In the case of text retrieval, vector-space methods [3, 28] map each document into a point in high-dimensional space.
Reference: [27] <author> A. Pentland, R.W. Picard, and S. Sclaroff, "Photo-book: </author> <title> tools for content-based manipulation of image databases", </title> <booktitle> In Proc. SPIE Conference on Storage and Retrieval of Image and Video Databases II, </booktitle> <volume> 2185, </volume> <year> 1994. </year>
Reference-contexts: In image and multimedia retrieval, a common first step is to extract a set of numerically-valued features or parameters from the document. For instance, IBM's Query-by-image-content [11] and MIT's Photobook <ref> [27] </ref> extract image features such as color histograms (hues, intensities), shape descriptors, as well as quantities measuring texture.
Reference: [28] <author> G. Salton, </author> <title> Automatic Text Processing, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: Other instances of near-neighbor search appear in algorithms for pattern recognition [6, 10], statistics and data analysis [26, 8], machine learning [5], data compression [12], data mining [13] and image analysis [20]. In the case of text retrieval, vector-space methods <ref> [3, 28] </ref> map each document into a point in high-dimensional space.
Reference: [29] <author> H. Samet, </author> <title> The Design and Analysis of Spatial Data Structures, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: Nevertheless it offers a simple approach for indexing problems (if the feature space after dimensionality reduction has moderate dimension) and in iterative computations for sparse finite-element relaxation methods. To put this in perspective, we survey some other approaches to near-neighbor finding. Approaches for Near-Neighbor Search. Samet <ref> [29] </ref> surveys a variety of data structures used for this problem including variants of k-d trees, R-trees, and structures based on space-filling curves.
Reference: [30] <editor> A.W.M. Smeulders and R. Jain, editors, </editor> <title> Image Databases and Multi-media Search, </title> <booktitle> Proceedings of the First International Workshop, </booktitle> <address> IDB-MMS '96, Amster-dam. </address> <publisher> Amsterdam University Press, </publisher> <year> 1996. </year>
Reference: [31] <author> A.C. Yao and F.F. Yao, </author> <title> "A general approach to d-dimensional geometric queries," </title> <booktitle> Proc. 17th STOC (1985), </booktitle> <pages> pp. 163 - 168. </pages>
Reference-contexts: Recently, Klein-berg [17] has developed a scheme for approximate nearest neighbor problem that achieves query time O (d 2 log n) with preprocessing n O (d) . There have been a number of other approaches and extensions (e.g. <ref> [31, 23, 25, 1, 2] </ref>). The best approaches from these studies are still impractical for the values of d encountered in the retrieval applications above. Overview of Paper.
References-found: 31

