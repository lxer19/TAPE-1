URL: http://robotics.stanford.edu/~ronnyk/option.ps
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: ronnyk@sgi.com  clay@cs.stanford.edu  
Title: Option Decision Trees with Majority Votes  
Author: Ron Kohavi Clayton Kunz 
Address: 2011 N. Shoreline Blvd Mountain View, CA 94043-1389  Stanford, CA. 94305  
Affiliation: Silicon Graphics, Inc.  Computer Science Dept. Stanford University  
Note: To appear in ICML97  
Abstract: We describe an experimental study of Option Decision Trees with majority votes. Option Decision Trees generalize regular decision trees by allowing option nodes in addition to decision nodes; such nodes allow for several possible tests to be conducted instead of the commonly used single test. Our goal was to explore when option nodes are most useful and to control the growth of the trees so that additional complexity of little utility is limited. Option Decision Trees can reduce the error of decision trees on real-world problems by combining multiple options, with the motivation similar to that of voting algorithms that learn multiple models and combine the predictions. However, unlike voting algorithms, an Option Decision Tree provides a single structured classifier (one decision tree), which can be interpreted more easily by humans. Our results show that for the tested problems, we can achieve significant reduction in error rates for trees restricted to two levels of option nodes at the top. When very large Option Decision Trees are built, Option Decision Trees outperform Bagging in reducing error, although the trees are much larger and cannot be reasonably interpreted by humans.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ali, K. M. </author> <year> (1996), </year> <title> Learning Probabilistic Relational Concept Descriptions, </title> <type> PhD thesis, </type> <institution> University of Cali-fornia, </institution> <address> Irvine. http://www.ics.uci.edu/~ali. </address>
Reference-contexts: This would indicate that most criteria are not very reliable near the root of the tree; specifically, minor differences between two attributes may not be good predictors of how good the sub-trees will be. Furthermore, researchers have shown <ref> (Ali 1996, Perrone 1993) </ref> that combining multiple models works best when the structures are not correlated or when they are anti-correlated.
Reference: <author> Ali, K. & Pazzani, M. </author> <year> (1995), </year> <title> `HYDRA-MM: Learning multiple descriptions to improve classification accuracy', </title> <booktitle> International Journal on Artificial Intelligence Tools 4(1/2), </booktitle> <pages> 115-133. </pages>
Reference: <author> Bahl, L. R., Brown, P. F., de Souza, P. V. & Mercer, R. L. </author> <year> (1989), </year> <title> `A tree-based statistical language model for natural language speech recognition', </title> <booktitle> IEEE Transactions on Acoustics, Speech, and Signal Processing 37(7), </booktitle> <pages> 1001-1008. </pages>
Reference: <author> Breiman, L. </author> <year> (1994), </year> <title> Heuristics of instability in model selection, </title> <institution> Technical Report Statistics Department, University of California at Berkeley. </institution>
Reference: <author> Breiman, L. </author> <year> (1996), </year> <title> `Bagging predictors', </title> <booktitle> Machine Learning 24, </booktitle> <pages> 123-140. </pages>
Reference: <author> Brodley, C. E. </author> <year> (1995), </year> <title> Automatic selection of split criterion during tree growing based on node location, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 73-80. </pages>
Reference: <author> Buntine, W. </author> <year> (1992a), </year> <title> `Learning classification trees', </title> <journal> Statistics and Computing 2(2), </journal> <pages> 63-73. </pages>
Reference: <author> Buntine, W. </author> <year> (1992b), </year> <title> A Theory of Learning Classification Rules, </title> <type> PhD thesis, </type> <institution> University of Technology, Sydney, School of Computing Science. </institution>
Reference: <author> Freund, Y. & Schapire, R. E. </author> <year> (1995), </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting, </title> <booktitle> in `Proceedings of the Second European Conference on Computational Learning Theory', </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 23-37. </pages>
Reference-contexts: It also uses all of the data to build the tree, while Bagging constructs multiple trees from samples that contain only a factor of about 0:632 unique instances from the training set. Unlike boosting <ref> (Freund & Schapire 1995) </ref>, it is easy to parallelize Option Decision Tree induction. Option Decision Trees provide a compact representation of many regular decision trees. The error reductions reported here are significant and large.
Reference: <author> Kohavi, R., Sommerfield, D. & Dougherty, J. </author> <year> (1996), </year> <title> Data mining using MLC ++ : A machine learning library in C ++ , in `Tools with Artificial Intelligence', </title> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 234-245. </pages> <address> http://www.sgi.com/Technology/mlc. </address>
Reference-contexts: As we will show, the tree growth has to be carefully controlled to avoid creating unmanageable trees that both exhaust memory and increase induction time. 3.1 The TDDT op Induction Algorithm and Classifier TDDT op is a Top-Down Decision-Tree (TDDT) induction algorithm implemented in MLC ++ <ref> (Kohavi, Som-merfield & Dougherty 1996) </ref> that creates option nodes. The algorithm is similar to C4.5 (Quinlan 1993) with the evaluation criterion being normalized information-gain (information gain divided by log the number of children). Unknowns were regarded as a separate value.
Reference: <author> Kwok, S. W. & Carter, C. </author> <year> (1990), </year> <title> Multiple decision trees, </title> <editor> in R. D. Schachter, T. S. Levitt, L. N. Kanal & J. F. Lemmer, eds, </editor> <booktitle> `Uncertainty in Artificial Intelligence', </booktitle> <publisher> Elsevier Science Publishers, </publisher> <pages> pp. 327-335. </pages>
Reference: <author> Murthy, S. & Salzberg, S. </author> <year> (1995), </year> <title> Lookahead and pathology in decision tree induction, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> `Proceedings of the 14th International Joint Conference on Artificial Intelligence', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 1025-1031. </pages>
Reference-contexts: Specifically, an evaluation criterion scores possible tests at a node based on the children the node would have. Such limited lookahead prefers attributes that score high in isolation and may overlook combinations of attributes. Multi-ply lookahead is compu-tationally expensive and has not proven itself useful <ref> (Murthy & Salzberg 1995) </ref>. The second reason why option trees might be better than regular decision trees is related to stability (bias-variance) and risk reduction.
Reference: <author> Oliver, J. & Hand, D. </author> <year> (1995), </year> <title> On pruning and averaging decision trees, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 430-437. </pages>
Reference: <author> Perrone, M. </author> <year> (1993), </year> <title> Improving regression estimation: averaging methods for variance reduction with extensions to general convex measure optimization, </title> <type> PhD thesis, </type> <institution> Brown University, Physics Dept. </institution>
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: The algorithm is similar to C4.5 <ref> (Quinlan 1993) </ref> with the evaluation criterion being normalized information-gain (information gain divided by log the number of children). Unknowns were regarded as a separate value. The algorithm grows the decision tree following the standard methodology of choosing the best attribute according to the evaluation criterion. <p> In the final comparison we add C4.5 <ref> (Quinlan 1993) </ref> and a bagged version of TDDT. The data sets used in our final experiment are a su-perset of those used in Breiman (1996) taken from the UCI repository (except that the heart database from UCI we used did not match the CART version Breiman used).
Reference: <author> Stolfo, S. </author> <year> (1996), </year> <title> Integrating multiple learned models for improving and scaling machine learning algorithms. </title> <booktitle> AAAI Workshop. </booktitle>
References-found: 16

