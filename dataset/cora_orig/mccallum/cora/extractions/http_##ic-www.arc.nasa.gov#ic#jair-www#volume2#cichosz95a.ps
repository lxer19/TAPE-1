URL: http://ic-www.arc.nasa.gov/ic/jair-www/volume2/cichosz95a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/cichosz95a.html
Root-URL: 
Email: cichosz@ipe.pw.edu.pl  
Title: Truncating Temporal Differences: On the Efficient Implementation of TD() for Reinforcement Learning  
Author: Pawe Cichosz 
Address: Nowowiejska 15/19, 00-665 Warsaw, Poland  
Affiliation: Institute of Electronics Fundamentals, Warsaw University of Technology  
Note: Journal of Artificial Intelligence Research 2 (1995) 287-318 Submitted 9/94; published 1/95  
Abstract: Temporal difference (TD) methods constitute a class of methods for learning predictions in multi-step prediction problems, parameterized by a recency factor . Currently the most important application of these methods is to temporal credit assignment in reinforcement learning. Well known reinforcement learning algorithms, such as AHC or Q-learning, may be viewed as instances of TD learning. This paper examines the issues of the efficient and general implementation of TD() for arbitrary , for use with reinforcement learning algorithms optimizing the discounted sum of rewards. The traditional approach, based on eligibility traces, is argued to suffer from both inefficiency and lack of generality. The TTD (Truncated Temporal Differences) procedure is proposed as an alternative, that indeed only approximates TD(), but requires very little computation per action and can be used with arbitrary function representation methods. The idea from which it is derived is fairly simple and not new, but probably unexplored so far. Encouraging experimental results are presented, suggesting that using &gt; 0 with the TTD procedure allows one to obtain a significant learning speedup at essentially the same cost as usual TD(0) learning.
Abstract-found: 1
Intro-found: 1
Reference: <author> Baird, III, L. C. </author> <year> (1993). </year> <title> Advantage updating. </title> <type> Tech. rep. </type> <institution> WL-TR-93-1146, Wright Laboratory, Wright-Patterson Air Force Base. </institution>
Reference-contexts: The remainder of this paper is organized as follows. In Section 2 a formal definition of TD methods is presented and their application to reinforcement learning is discussed. Three example RL algorithms are briefly described: AHC (Sutton, 1984), Q-learning (Watkins, 1989; Watkins & Dayan, 1992), and advantage updating <ref> (Baird, 1993) </ref>. Section 3 presents the traditional approach to TD () implementation, based on so called eligibility traces, which is criticized for inefficiency and lack of generality.
Reference: <author> Barto, A. G. </author> <year> (1992). </year> <title> Reinforcement learning and adaptive critic methods. In White, </title> <editor> D. A., & Sofge, D. A. (Eds.), </editor> <booktitle> Handbook of Intelligent Control, </booktitle> <pages> pp. 469-491. </pages> <publisher> Van Nostrand Reinhold, </publisher> <address> New York. </address> <note> 316 Truncating Temporal Differences Barto, </note> <author> A. G., Sutton, R. S., & Anderson, C. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13, </volume> <pages> 835-846. </pages>
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. </author> <year> (1990). </year> <title> Learning and sequential decision making. </title> <editor> In Gabriel, M., & Moore, J. (Eds.), </editor> <title> Learning and Computational Neuroscience. </title> <publisher> The MIT Press. </publisher>
Reference: <author> Cichosz, P. </author> <year> (1994). </year> <title> Reinforcement learning algorithms based on the methods of temporal differences. </title> <type> Master's thesis, </type> <institution> Institute of Computer Science, Warsaw University of Technology. </institution>
Reference: <author> Dayan, P. </author> <year> (1992). </year> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8, </booktitle> <pages> 341-362. </pages>
Reference: <author> Dayan, P., & Sejnowski, T. </author> <year> (1994). </year> <title> TD() converges with probability 1. </title> <journal> Machine Learning, </journal> <volume> 14, </volume> <pages> 295-301. </pages>
Reference: <author> Heger, M. </author> <year> (1994). </year> <title> Consideration of risk in reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning (ML-94). </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Jaakkola, T., Jordan, M. I., & Singh, S. P. </author> <year> (1993). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <type> Tech. rep. 9307, </type> <institution> MIT Computational Cognitive Science. </institution> <note> Submitted to Neural Computation. </note>
Reference: <author> Klopf, A. H. </author> <year> (1982). </year> <title> The Hedonistic Neuron: A Theory of Memory, Learning, </title> <booktitle> and Intelligence. </booktitle> <address> Washington D.C.: Hempisphere. </address>
Reference: <author> Lin, L.-J. </author> <year> (1992). </year> <title> Self-improving, reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 293-321. </pages>
Reference: <author> Lin, L.-J. </author> <year> (1993). </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> Ph.D. thesis, </type> <institution> School of Computer Science, Carnegie-Mellon University. </institution>
Reference-contexts: Cichosz it actually performs. It must learn an optimal policy by observing the consequences of its actions. The abstract formulation and generality of the reinforcement learning paradigm make it widely applicable, especially in such domains as game-playing (Tesauro, 1992), automatic control (Sutton et al., 1991), and robotics <ref> (Lin, 1993) </ref>. To formulate a particular task as a reinforcement learning task, one just has to design appropriate state and action representation, and a reinforcement mechanism specifying the goal of the task.
Reference: <author> Mitchie, D., & Chambers, R. A. </author> <year> (1968). </year> <title> BOXES: An experiment in adaptive control. </title> <journal> Machine Intelligence, </journal> <volume> 2, </volume> <pages> 137-152. </pages>
Reference: <author> Moore, A. W., & Atkeson, C. G. </author> <year> (1992). </year> <title> An investigation of memory-based function ap-proximators for learning control. </title> <type> Tech. rep., </type> <institution> MIT Artificial Intelligence Laboratory. </institution>
Reference-contexts: Unfortunately, the technique of eligibility traces is not general enough to be easy to implement with an arbitrary function representation method. It is not clear, for example, how it could be used with such an important class of function approximators as memory-based (or instance-based) function approximators <ref> (Moore & Atkeson, 1992) </ref>. Applied with a pure tabular representation, it has significant drawbacks. First, it requires additional memory locations, one per state. Second, and even more painful, is that it requires modifying both U (x) and e x for all x at each time step.
Reference: <author> Pendrith, M. </author> <year> (1994). </year> <title> On reinforcement learning of control actions in noisy and non-markovian domains. </title> <type> Tech. rep. </type> <institution> UNSW-CSE-TR-9410, School of Computer Science and Engineering, The University of New South Wales, Australia. </institution>
Reference: <author> Peng, J., & Williams, R. J. </author> <year> (1994). </year> <title> Incremental multi-step Q-learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning (ML-94). </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, experimental results obtained with TD ( &gt; 0) indicate that it often allows one to obtain a significant learning speedup (Sutton, 1988; Lin, 1993; Tesauro, 1992). It has been also suggested <ref> (e.g., Peng & Williams, 1994) </ref> that TD ( &gt; 0) should perform better in non-Markovian environments than TD (0) (i.e., it should be less sensitive to the potential violations of the Markov property).
Reference: <author> Ross, S. </author> <year> (1983). </year> <title> Introduction to Stochastic Dynamic Programming. </title> <publisher> Academic Press, </publisher> <address> New York. </address> <note> 317 Cichosz Schwartz, </note> <author> A. </author> <year> (1993). </year> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning (ML-93). </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The objective of learning is to identify a decision policy (i.e., a state-action mapping) that maximizes the reinforcement values received by the agent in the long term. A commonly assumed formal model of a reinforcement learning task is a Markovian decision problem <ref> (MDP, e.g., Ross, 1983) </ref>. The Markov property means that state transitions and reinforcement values always depend solely on the current state and the current action: there is no dependence on previous states, actions, or rewards, i.e., the state information supplied to the agent is sufficient for making optimal decisions.
Reference: <author> Singh, S. P. </author> <year> (1994). </year> <title> Reinforcement learning algorithms for average-payoff markovian decision processes. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94). </booktitle>
Reference: <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer and Information Science, University of Massachusetts. </institution>
Reference-contexts: The key problem that must be solved in order to learn an optimal policy under the conditions of delayed reinforcement is known as the temporal credit assignment problem <ref> (Sutton, 1984) </ref>. It is the problem of assigning credit or blame for the overall outcomes of a learning system (i.e., long-term reinforcement values) to each of its individual actions, possibly taken several steps before the outcomes could be observed. <p> Discussing reinforcement learning algorithms, we will concentrate on temporal credit assignment and ignore the issues of structural credit assignment <ref> (Sutton, 1984) </ref>, the other aspect of credit assignment in RL systems. 1.1 Temporal Difference Methods The temporal credit assignment problem in reinforcement learning is typically solved using algorithms based on the methods of temporal differences (TD). <p> This has been the motivation of this work. The remainder of this paper is organized as follows. In Section 2 a formal definition of TD methods is presented and their application to reinforcement learning is discussed. Three example RL algorithms are briefly described: AHC <ref> (Sutton, 1984) </ref>, Q-learning (Watkins, 1989; Watkins & Dayan, 1992), and advantage updating (Baird, 1993). Section 3 presents the traditional approach to TD () implementation, based on so called eligibility traces, which is criticized for inefficiency and lack of generality. <p> The most important application of these methods, however, is to reinforcement learning. As a matter of fact, TD methods were formulated by Sutton (1988) as a generalization of techniques he had previously used only in the context of temporal credit assignment in reinforcement learning <ref> (Sutton, 1984) </ref>. As already stated above, the most straightforward way to formulate temporal credit assignment as a prediction problem is to predict at each time step t the discounted sum of future reinforcement z t = k=0 called the TD return for time t. <p> TD-based reinforcement learning algorithms may be viewed as more or less direct implementations of the general rule described by Equation 6. To see this, we will consider three algorithms: well known AHC <ref> (Sutton, 1984) </ref> and Q-learning (Watkins, 1989; Watkins & Dayan, 1992), and a recent development of Baird (1993) called advantage updating. All the algorithms rely on learning certain real-valued functions defined over the state or state and action space of a task. <p> Sutton (1984) presented the technique of eligibility traces as an implementation of the recency and frequency heuristics. In this context, the phenomenon examined above may be considered a harmful effect of the frequency heuristic. Sutton discussed an example finite-state task where this heuristic might be misleading <ref> (Sutton, 1984, page 171) </ref>. 298 Truncating Temporal Differences and ;m m2 X (fl) k r t+k + fl (1 )U t+k (x t+k+1 ) + (fl) m1 r t+m1 + flU t+m1 (x t+m ) = k=0 h i We call ;m t the m-step truncated TD () error, or simply <p> This is not true for episodic tasks <ref> (Sutton, 1984) </ref> and for many real-world tasks, where learning must usually stop some time. This imposes the necessity of designing a special mechanism for the TTD procedure, that will be called the reset operation.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning (ML-90). </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S., Barto, A. G., & Williams, R. J. </author> <year> (1991). </year> <title> Reinforcement learning is direct adaptive optimal control. </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pp. 2143-2146. </pages> <address> Boston, MA. </address>
Reference-contexts: All rights reserved. Cichosz it actually performs. It must learn an optimal policy by observing the consequences of its actions. The abstract formulation and generality of the reinforcement learning paradigm make it widely applicable, especially in such domains as game-playing (Tesauro, 1992), automatic control <ref> (Sutton et al., 1991) </ref>, and robotics (Lin, 1993). To formulate a particular task as a reinforcement learning task, one just has to design appropriate state and action representation, and a reinforcement mechanism specifying the goal of the task.
Reference: <author> Sutton, R. S., & Singh, S. P. </author> <year> (1994). </year> <title> On step-size and bias in temporal-difference learning. </title> <booktitle> In Proceedings of the Eighth Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pp. 91-96. </pages> <institution> Center for Systems Science, Yale University. </institution>
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 257-277. </pages>
Reference-contexts: All rights reserved. Cichosz it actually performs. It must learn an optimal policy by observing the consequences of its actions. The abstract formulation and generality of the reinforcement learning paradigm make it widely applicable, especially in such domains as game-playing <ref> (Tesauro, 1992) </ref>, automatic control (Sutton et al., 1991), and robotics (Lin, 1993). To formulate a particular task as a reinforcement learning task, one just has to design appropriate state and action representation, and a reinforcement mechanism specifying the goal of the task.
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> Ph.D. thesis, </type> <institution> King's College, </institution> <address> Cambridge. </address>
Reference-contexts: t+k1 (x t+k ) U t+k (x t+k ) : If we relax for a moment our assumption about on-line learning mode and leave out time subscripts from U values, the last term disappears and we simply have: 1 Similarly for general , if we define the TD () return <ref> (Watkins, 1989) </ref> for time t as a weighted average of corrected truncated TD returns: z 1 X k z t = k=0 h i and again omit time subscripts, we will receive: t U (x t ): (18) The last equation brings more light on the exact nature of the computation
Reference: <author> Watkins, C. J. C. H., & Dayan, P. </author> <year> (1992). </year> <title> Technical note: </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8, </volume> <pages> 279-292. </pages>
References-found: 25

