URL: http://iew3.technion.ac.il:8080/~moshet/partcon.ps
Refering-URL: http://iew3.technion.ac.il:8080/~moshet/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: brafman@cs.ubc.ca  moshet@ie.technion.ac.il  
Title: On Partially Controlled Multi-Agent Systems  
Author: Ronen I. Brafman Moshe Tennenholtz 
Address: Vancouver, B.C., Canada V6L 1Z4  Haifa 32000, Israel  
Affiliation: Computer Science Department University of British Columbia  Industrial Engineering and Management Technion Israel Institute of Technology  
Abstract: Motivated by the control theoretic distinction between controllable and uncontrollable events, we distinguish between two types of agents within a multi-agent system: controllable agents, which are directly controlled by the system's designer, and uncontrollable agents, which are not under the designer's direct control. We refer to such systems as partially controlled multi-agent systems, and we investigate how one might influence the behavior of the uncontrolled agents through appropriate design of the controlled agents. In particular, we wish to understand which problems are naturally described in these terms? what methods can be applied to influence the uncontrollable agents? what is their effectiveness? and whether similar methods work across different domains? Using a game-theoretic framework, this paper studies the design of partially controlled multi-agent systems in two contexts: in one context, the uncontrollable agents are expected utility maximizers, while in the other they are reinforcement learners. We suggest different techniques for controlling agents' behavior in each domain, assess their success, and examine their relationship.
Abstract-found: 1
Intro-found: 1
Reference: <author> Altenberg, L., & Feldman, M. W. </author> <year> (1987). </year> <title> Selection, Generalized Transmission, and the Evolution of Modifier Genes. I. The reduction principle. </title> <booktitle> Genetics, </booktitle> <pages> 559-572. </pages>
Reference: <author> Bellman, R. </author> <year> (1962). </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press. </publisher>
Reference-contexts: In this section we start exploring the question of how a teacher should teach. First, we define what an optimal policy is. Then, we will define Markov decision processes (MDP) <ref> (Bellman, 1962) </ref>, and show that under certain assumptions teaching can be viewed as an MDP. This will allow us to tap into the vast knowledge that has accumulated on solving these problems. In particular, we can use well known methods, such as value iteration (Bellman, 1962), to find the optimal teaching <p> will define Markov decision processes (MDP) <ref> (Bellman, 1962) </ref>, and show that under certain assumptions teaching can be viewed as an MDP. This will allow us to tap into the vast knowledge that has accumulated on solving these problems. In particular, we can use well known methods, such as value iteration (Bellman, 1962), to find the optimal teaching policy. We start by defining an optimal teaching policy. A teaching policy is a function that returns an action at each iteration; possibly, it may depend on a complete history of the past joint actions. <p> In the experiments below we use a method based on value-iteration <ref> (Bellman, 1962) </ref>. Now suppose that the student can be in a set of possible states, that his set of actions is A s , and that the teacher's set of actions is A t .
Reference: <author> Bond, A. H., & Gasser, L. </author> <year> (1988). </year> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Ablex Publishing Corporation. </publisher>
Reference-contexts: One particular area both of these fields have been concerned with is multi-agent environments; examples include work in distributed AI <ref> (Bond & Gasser, 1988) </ref>, and work on decentralized supervisory control (Lin & Wonham, 1988). Each of these fields has developed its own techniques and has incorporated particular assumptions into its models. <p> Despite its somewhat futuristic flavor (although instances of such shared environments are beginning to appear in cyberspace), this scenario is useful in illustrating the vulnerability of some of the most popular coordination mechanism appearing in the multi-agent literature within AI (e.g., see <ref> (Bond & Gasser, 1988) </ref>) when we assume that the agents involved are fully rational.
Reference: <author> Briggs, W., & Cook, D. </author> <year> (1995). </year> <title> Flexible Social Laws. </title> <booktitle> In Proc. 14th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 688-693. </pages>
Reference: <author> Dean, T. L., & Wellman, M. P. </author> <year> (1991). </year> <title> Planning and Control. </title> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: 1. Introduction The control of agents is a central research topic in two engineering fields: Artificial Intelligence (AI) <ref> (Dean & Wellman, 1991) </ref> and Discrete Events Systems (DES) (Ramadge & Wonham, 1989). One particular area both of these fields have been concerned with is multi-agent environments; examples include work in distributed AI (Bond & Gasser, 1988), and work on decentralized supervisory control (Lin & Wonham, 1988).
Reference: <author> Dixit, A. K., & Nalebuff, B. J. </author> <year> (1991). </year> <title> Thinking strategically : the competitive edge in business, politics, and everyday life. </title> <publisher> Norton, </publisher> <address> New York. </address>
Reference: <author> Durfee, E. H., Lesser, V. R., & Corkill, D. D. </author> <year> (1987). </year> <title> Coherent Cooperation Among Communicating Problem Solvers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36, </volume> <pages> 1275-1291. </pages>
Reference: <author> Dwork, C., & Moses, Y. </author> <year> (1990). </year> <title> Knowledge and Common Knowledge in a Byzantine Environment: Crash Failures. </title> <journal> Information and Computation, </journal> <volume> 88 (2), </volume> <pages> 156-186. </pages>
Reference-contexts: Several approaches for coordination of agent activity are discussed in the distributed systems and the DAI literature, e.g., protocols for reaching consensus <ref> (Dwork & Moses, 1990) </ref>, rational deals and negotiations (Zlotkin & Rosenschein, 1993; Kreps & Wilson, 1982; Rosenschein & Genesereth, 1985), organizational structures (Durfee, Lesser, & Corkill, 1987; Fox, 1981; Malone, 1987), social laws (Moses & Tennenholtz, 1995; Shoham & Tennenholtz, 1995; Minsky, 1991; Briggs & Cook, 1995) and others.
Reference: <editor> Eatwell, J., Milgate, M., & Newman, P. (Eds.). </editor> <year> (1989). </year> <title> The New Palgrave: Game Theory. </title>
Reference-contexts: The prisoner's dilemma captures the essence of many important social and economic situations; in particular, it encapsulates the notion of cooperation. It has thus motivated enormous discussion among game-theorists and mathematical economists <ref> (Eatwell, Milgate, & New-man, 1989) </ref>. In the prisoner's dilemma, whatever the choice of one player, the second player can maximize its payoff by playing Defect. It thus seems "rational" for each player to defect. However, when both players defect, their payoffs are much worse than if they both cooperate. <p> Coop now . Similarly, the teacher will follow a Defect by the student with a Defect on her part. This strategy, called Tit-For-Tat (TFT for short), is well known <ref> (Eatwell et al., 1989) </ref>. Our experiments show that it is not very successful in teaching a BQL (see Figure 3). We also experimented with a variant of TFT, which we call 2TFT. <p> Within game theory there is an extensive body of work that tries to understand the evolution of cooperation in the iterated prisoner's dilemma and to find good playing strategies for it <ref> (Eatwell et al., 1989) </ref>. In that work both players have the same knowledge, and teaching is not an issue. Last but not least, our work has important links to work on conditioning and especially operant conditioning in psychology (Mackintosh, 1983).
Reference: <institution> W.W.Norton & Company, Inc. </institution>
Reference: <author> Fox, M. S. </author> <year> (1981). </year> <title> An organizational view of distributed systems. </title> <journal> IEEE Trans. Sys., Man., Cyber., </journal> <volume> 11, </volume> <pages> 70-80. </pages>
Reference: <author> Fudenberg, D., & Tirole, J. </author> <year> (1991). </year> <title> Game Theory. </title> <publisher> MIT Press. </publisher>
Reference: <author> Gilboa, I., & Matsui, A. </author> <year> (1991). </year> <title> Social stability and equilibrium. </title> <journal> Econometrica, </journal> <volume> 59 (3), </volume> <pages> 859-867. </pages>
Reference-contexts: Indeed, these examples constitute the two central models of self-motivated agents in game theory and decision theory, referred to as the educative and evolutive models (see <ref> (Gilboa & Matsui, 1991) </ref>). The special nature of the uncontrollable agents and the special structure of the uncontrollable events they induce is what differentiates PCMAS from corresponding models in the DES literature. This difference raises new questions and suggests a new perspective on the design of multi-agent systems.
Reference: <author> Goldberg, K. </author> <year> (1993). </year> <title> Orienting parts without sensors. </title> <journal> Algorithmica, </journal> <volume> 10 (2), </volume> <pages> 201-225. </pages>
Reference-contexts: An agent's ability to function in an environment is greatly affected by its knowledge of the environment. In some special cases, we can design agents with sufficient knowledge for performing a task <ref> (Goldberg, 1993) </ref>, but, in general, agents must acquire information on-line in order to optimize their performance, i.e., they must learn. One possible approach to improving the performance of learning algorithms is employing a teacher.
Reference: <author> Huberman, B. A., & Hogg, T. </author> <year> (1988). </year> <title> The Behavior of Computational Ecologies. </title> <editor> In Huberman, B. A. (Ed.), </editor> <booktitle> The Ecology of Computation. </booktitle> <publisher> Elsevier Science. </publisher>
Reference: <author> Kaelbling, L. </author> <year> (1990). </year> <title> Learning in embedded systems. </title> <type> Ph.D. thesis, </type> <institution> Stanford University. </institution> <note> 29 Kandori, </note> <author> M., Mailath, G., & Rob, R. </author> <year> (1991). </year> <title> Learning, Mutation and Long Equilibria in Games. </title> <institution> Mimeo. University of Pennsylvania, </institution> <year> 1991. </year>
Reference: <author> Kinderman, R., & Snell, S. L. </author> <year> (1980). </year> <title> Markov Random Fields and their Applications. </title> <publisher> American Mathematical Society. </publisher>
Reference: <author> Kittock, J. E. </author> <year> (1994). </year> <title> The impact of locality and authority on emergent conventions. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI '94), </booktitle> <pages> pp. 420-425. </pages>
Reference-contexts: Matraic (Mataric, 1995) and Parker (Parker, 1993) consider the use of reinforcement learning in physical robots. They consider features of real robots, which are not discussed in this paper. Shoham and Tennenholtz (Shoham & Tennenholtz, 1992) examine the evolution of conventions in a society of reinforcement learners. Kittock <ref> (Kittock, 1994) </ref> investigates the effects of societal structure on multi-agent learning. Littman (Littman, 1994) develops reinforcement learning techniques for agents whose goals are opposed, and Tan (Tan, 1993) examines the benefit of sharing information among reinforcement learners.
Reference: <author> Kreps, D. M., & Wilson, R. </author> <year> (1982). </year> <title> Sequential equilibria. </title> <journal> Econometrica, </journal> <volume> 50 (4), </volume> <pages> 863-894. </pages>
Reference: <author> Lin, F., & Wonham, W. </author> <year> (1988). </year> <title> Decentralized control and coordination of discrete-event systems. </title> <booktitle> In Proceedings of the 27th IEEE Conf. Decision and Control, </booktitle> <pages> pp. 1125-1130. </pages>
Reference-contexts: One particular area both of these fields have been concerned with is multi-agent environments; examples include work in distributed AI (Bond & Gasser, 1988), and work on decentralized supervisory control <ref> (Lin & Wonham, 1988) </ref>. Each of these fields has developed its own techniques and has incorporated particular assumptions into its models. Hence, it is only natural that techniques and assumptions used by one field may be adopted by the other or may lead to new insights for the other field.
Reference: <author> Lin, L. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <booktitle> Machine Learning, </booktitle> <pages> 8 (3-4). </pages>
Reference-contexts: One possible approach to improving the performance of learning algorithms is employing a teacher. For example, Lin <ref> (Lin, 1992) </ref> uses teaching by example to improve the performance of agents, supplying them with examples that show how the task can be achieved. Tan's work (Tan, 1993) can also be viewed as a form of teaching in which agents share experiences. <p> However, the above work is not concerned with teaching, or with the question of how much influence one agent can have over another. Lin <ref> (Lin, 1992) </ref> is explicitly concerned with teaching as a way of accelerating learning of enhanced Q-learners. He uses experience replay and supplies students with examples of how the task can be achieved.
Reference: <author> Littman, M. </author> <year> (1994). </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proc. of the 11th Int. Conf. on Mach. Learn. </booktitle>
Reference-contexts: They consider features of real robots, which are not discussed in this paper. Shoham and Tennenholtz (Shoham & Tennenholtz, 1992) examine the evolution of conventions in a society of reinforcement learners. Kittock (Kittock, 1994) investigates the effects of societal structure on multi-agent learning. Littman <ref> (Littman, 1994) </ref> develops reinforcement learning techniques for agents whose goals are opposed, and Tan (Tan, 1993) examines the benefit of sharing information among reinforcement learners.
Reference: <author> Luce, R. D., & Raiffa, H. </author> <year> (1957). </year> <title> Games and Decisions- Introduction and Critical Survey. </title> <publisher> John Wiley and Sons. </publisher>
Reference: <author> Mackintosh, N. </author> <year> (1983). </year> <title> Conditioning and Associative Learning. </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: In that work both players have the same knowledge, and teaching is not an issue. Last but not least, our work has important links to work on conditioning and especially operant conditioning in psychology <ref> (Mackintosh, 1983) </ref>. In conditioning experiments an experimenter tries to induce changes in its subjects by arranging that certain relationships will hold in their environment, or by explicitly (in operant conditioning) reinforcing the subjects' actions. In our framework the controlled agent plays a similar role to that of the experimenter.
Reference: <author> Malone, T. W. </author> <year> (1987). </year> <title> Modeling Coordination in Organizations and Markets. </title> <journal> Management Science, </journal> <volume> 33 (10), </volume> <pages> 1317-1332. </pages>
Reference: <author> Mataric, M. J. </author> <year> (1995). </year> <title> Reward Functions for Accelerating Learning. </title> <booktitle> In Proceedings of the 11th international conference on Machine Learning, </booktitle> <pages> pp. 181-189. </pages>
Reference-contexts: A number of authors have discussed reinforcement learning in multi-agent systems. Yanco and Stein (Yanco & Stein, 1993) examine the evolution of communication among cooperative reinforcement learners. Sen et al. (Sen et al., 1994) use Q-learning to induce cooperation between two block pushing robots. Matraic <ref> (Mataric, 1995) </ref> and Parker (Parker, 1993) consider the use of reinforcement learning in physical robots. They consider features of real robots, which are not discussed in this paper. Shoham and Tennenholtz (Shoham & Tennenholtz, 1992) examine the evolution of conventions in a society of reinforcement learners.
Reference: <author> Minsky, N. </author> <year> (1991). </year> <title> The imposition of protocols over open distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17 (2), </volume> <pages> 183-195. </pages>
Reference: <author> Moses, Y., & Tennenholtz, M. </author> <year> (1995). </year> <journal> Artificial Social Systems. Computers and Artificial Intelligence, </journal> <volume> 14 (6), </volume> <pages> 533-562. </pages>
Reference: <author> Narendra, K., & Thathachar, M. A. L. </author> <year> (1989). </year> <title> Learning Automata: An Introduction. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: We wish to emphasize that although BQL is a bit less sophisticated than "real" 17 reinforcement learners discussed in the AI literature (which is defined below), it is a popu-lar and powerful type of learning rule, which is much discussed and used in the literature <ref> (Narendra & Thathachar, 1989) </ref>. The second student is a Q-learner (QL). He can observe the teacher's actions and has a number of possible states. The QL maintains a Q-value for each state-action pair. His states encode his recent experiences, i.e., the past joint actions.
Reference: <author> Owen, G. </author> <year> (1982). </year> <title> Game Theory (2nd Ed.). </title> <publisher> Academic Press. </publisher>
Reference: <author> Parker, L. E. </author> <year> (1993). </year> <title> Learning in Cooperative Robot Teams. </title> <booktitle> In Proceedings of IJCAI-93 Workshop on Dynamically Interacting Robots. </booktitle>
Reference-contexts: A number of authors have discussed reinforcement learning in multi-agent systems. Yanco and Stein (Yanco & Stein, 1993) examine the evolution of communication among cooperative reinforcement learners. Sen et al. (Sen et al., 1994) use Q-learning to induce cooperation between two block pushing robots. Matraic (Mataric, 1995) and Parker <ref> (Parker, 1993) </ref> consider the use of reinforcement learning in physical robots. They consider features of real robots, which are not discussed in this paper. Shoham and Tennenholtz (Shoham & Tennenholtz, 1992) examine the evolution of conventions in a society of reinforcement learners.
Reference: <author> Ramadge, P., & Wonham, W. </author> <year> (1989). </year> <title> The Control of Discrete Event Systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77 (1), </volume> <pages> 81-98. </pages>
Reference-contexts: 1. Introduction The control of agents is a central research topic in two engineering fields: Artificial Intelligence (AI) (Dean & Wellman, 1991) and Discrete Events Systems (DES) <ref> (Ramadge & Wonham, 1989) </ref>. One particular area both of these fields have been concerned with is multi-agent environments; examples include work in distributed AI (Bond & Gasser, 1988), and work on decentralized supervisory control (Lin & Wonham, 1988).
Reference: <author> Rosenschein, J. S., & Genesereth, M. R. </author> <year> (1985). </year> <title> Deals Among Rational Agents. </title> <booktitle> In Proc. 9th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 91-99. </pages> <note> 30 Schelling, </note> <author> T. </author> <year> (1980). </year> <title> The Strategy of Conflict. </title> <publisher> Harvard University Press. </publisher>
Reference-contexts: Notice that the behavior of controllable agents may be considered irrational in some cases; however, it will eventually lead to desired behavior for all the agents. Some approaches to negotiations can be viewed as incorporating threats. In particular, Rosenschein and Genesereth <ref> (Rosenschein & Genesereth, 1985) </ref> consider a mechanism making deals among rational agents, where agents are asked to offer a joint strategy to be followed by all agents and declare the move they would take if there will be no agreement on the joint strategy.
Reference: <author> Sen, S., Sekaran, M., & Hale, J. </author> <year> (1994). </year> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proc. of AAAI-94, </booktitle> <pages> pp. 426-431. </pages>
Reference-contexts: A number of authors have discussed reinforcement learning in multi-agent systems. Yanco and Stein (Yanco & Stein, 1993) examine the evolution of communication among cooperative reinforcement learners. Sen et al. <ref> (Sen et al., 1994) </ref> use Q-learning to induce cooperation between two block pushing robots. Matraic (Mataric, 1995) and Parker (Parker, 1993) consider the use of reinforcement learning in physical robots. They consider features of real robots, which are not discussed in this paper.
Reference: <author> Shoham, Y., & Tennenholtz, M. </author> <year> (1992). </year> <title> Emergent Conventions in Multi-Agent Systems: initial experimental results and observations. </title> <booktitle> In KR-92, </booktitle> <pages> pp. 225-231. </pages>
Reference-contexts: Sen et al. (Sen et al., 1994) use Q-learning to induce cooperation between two block pushing robots. Matraic (Mataric, 1995) and Parker (Parker, 1993) consider the use of reinforcement learning in physical robots. They consider features of real robots, which are not discussed in this paper. Shoham and Tennenholtz <ref> (Shoham & Tennenholtz, 1992) </ref> examine the evolution of conventions in a society of reinforcement learners. Kittock (Kittock, 1994) investigates the effects of societal structure on multi-agent learning.
Reference: <author> Shoham, Y., & Tennenholtz, M. </author> <year> (1995). </year> <title> Social Laws for Artificial Agent Societies: Off-line Design. </title> <journal> Artificial Inteligence, </journal> <volume> 73. </volume>
Reference-contexts: Our first study is concerned with the enforcement of social laws. When a number of agents designed by different designers work within a shared environment, it can be beneficial to impose certain constraints on their behavior, so that, overall, the system will function better. For example, in <ref> (Shoham & Tennenholtz, 1995) </ref>, Shoham and Tennenholtz show that by imposing certain "traffic laws", they can considerably simplify the task of motion planning for each robot, while still enabling efficient motions. Indeed, as we see later, such conventions are at the heart of many coordination techniques in multi-agent systems.
Reference: <author> Sondik, E. J. </author> <year> (1978). </year> <title> The optimal control of partially observable markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Research, </journal> <volume> 26 (2). </volume>
Reference-contexts: When these assumptions are invalid, we obtain a partially observable Markov decision process (POMDP) <ref> (Sondik, 1978) </ref>. Unfortunately, although POMDPs can be used in principle to obtain the ideal policy for our agents, current techniques for solving POMDPs are limited to very small problems.
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by method of temporal differences. </title> <journal> Mach. Lear., </journal> <volume> 3 (1), </volume> <pages> 9-44. </pages>
Reference: <author> Tan, M. </author> <year> (1993). </year> <title> Multi-Agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents. In Proceedings of the 10th International Conference on Machine Learning. </booktitle>
Reference-contexts: One possible approach to improving the performance of learning algorithms is employing a teacher. For example, Lin (Lin, 1992) uses teaching by example to improve the performance of agents, supplying them with examples that show how the task can be achieved. Tan's work <ref> (Tan, 1993) </ref> can also be viewed as a form of teaching in which agents share experiences. In both methods some non-trivial form of communication or perception is required. We strive to model a broad notion of teaching that encompasses any behavior that can improve a learning agent's performance. <p> Shoham and Tennenholtz (Shoham & Tennenholtz, 1992) examine the evolution of conventions in a society of reinforcement learners. Kittock (Kittock, 1994) investigates the effects of societal structure on multi-agent learning. Littman (Littman, 1994) develops reinforcement learning techniques for agents whose goals are opposed, and Tan <ref> (Tan, 1993) </ref> examines the benefit of sharing information among reinforcement learners. Finally, Whitehead (Whitehead, 1991) has shown that n reinforcement learners that can observe everything about each other can decrease learning time by a factor of n.
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> Ph.D. thesis, </type> <institution> King's College, Cam-bridge. </institution>
Reference: <author> Watkins, C., & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 279-292. </pages>
Reference-contexts: For a QL with memory one or more, the problem is a fully observable Markov decision process once the teacher plays TFT, because TFT is a deterministic function of the previous joint action. We know that Q-learning converges to the optimal policy under such conditions <ref> (Watkins & 22 Dayan, 1992) </ref>. Adding more memory effectively adds irrelevant attributes, which, in turn, causes a slower learning rate. We have also examined whether 2TFT would be successful when agents have a memory of two.
Reference: <author> Weidlich, W., & Haag, G. </author> <year> (1983). </year> <title> Concepts and Models of a Quantitative Sociology; The Dynamics of Interacting Populations. </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Whitehead, S. </author> <year> (1991). </year> <title> A complexity analysis of cooperative mechanisms in reinforcement learning. </title> <booktitle> In Proceedings of AAAI-91, </booktitle> <pages> pp. 607-613. </pages>
Reference-contexts: Kittock (Kittock, 1994) investigates the effects of societal structure on multi-agent learning. Littman (Littman, 1994) develops reinforcement learning techniques for agents whose goals are opposed, and Tan (Tan, 1993) examines the benefit of sharing information among reinforcement learners. Finally, Whitehead <ref> (Whitehead, 1991) </ref> has shown that n reinforcement learners that can observe everything about each other can decrease learning time by a factor of n. However, the above work is not concerned with teaching, or with the question of how much influence one agent can have over another.
Reference: <author> Yanco, H., & Stein, L. </author> <year> (1993). </year> <title> An Adaptive Communication Protocol for Cooperating Mobile Robots. In From Animal to Animats: </title> <booktitle> Proc. 2nd Intl. Conf. on the Sim. of Adap. Behavior, </booktitle> <pages> pp. 478-485. </pages>
Reference-contexts: In the future we hope to examine other learning architectures and see whether the lessons learned in this domain can be generalized, and whether we can use these methods to accelerate learning in other domains. A number of authors have discussed reinforcement learning in multi-agent systems. Yanco and Stein <ref> (Yanco & Stein, 1993) </ref> examine the evolution of communication among cooperative reinforcement learners. Sen et al. (Sen et al., 1994) use Q-learning to induce cooperation between two block pushing robots. Matraic (Mataric, 1995) and Parker (Parker, 1993) consider the use of reinforcement learning in physical robots.
Reference: <author> Zlotkin, G., & Rosenschein, J. S. </author> <year> (1993). </year> <title> A Domain Theory for Task Oriented Negotiation. </title> <booktitle> In Proc. 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 416-422. 31 </pages>
References-found: 45

