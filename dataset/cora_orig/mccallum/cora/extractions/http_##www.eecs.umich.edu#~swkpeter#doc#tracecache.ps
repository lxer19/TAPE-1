URL: http://www.eecs.umich.edu/~swkpeter/doc/tracecache.ps
Refering-URL: http://www.eecs.umich.edu/~swkpeter/paper.html
Root-URL: http://www.eecs.umich.edu
Title: Enhancing the Trace Cache Fetch Mechanism  
Author: Sanjay Jeram Patel, Sang Wook Kim, Daniel Holmes Friendly, and Yale N. Patt 
Keyword: high bandwidth fetch mechanisms, instruction cache, wide issue machines, speculative execution  
Note: Draft paper, submitted to ISCA 97 PLEASE DO NOT REPRODUCE Wed  
Address: Ann Arbor, MI 48109-2122  
Affiliation: Advanced Computer Architecture Laboratory Department of Electrical Engineering and Computer Science The University of Michigan  
Email: fsanjayp, swkpeter, ites, pattg@eecs.umich.edu  
Phone: Tel: (313) 763-5396  
Date: Nov 13 1996  
Abstract: In order to meet the demands of wider issue processors, fetch mechanisms will need to fetch multiple basic blocks per cycle. The trace cache supplies several basic blocks per cycle by storing segments of the dynamic instruction stream. When a particular basic block is requested, the trace cache can potentially respond with the requested block along with the several blocks that followed it when the block was last encountered. The groundwork for the trace cache mechanism was detailed by Rotenberg et al [12]. They compared the trace cache to other high bandwidth fetch mechanisms and showed an advantage, in performance and latency, of the trace cache scheme. They showed that a 128KB instruction cache with a 4KB trace cache achieves an average 28% performance increase over a single block fetch mechanism on the SPECint92 suite and Instruction Benchmark Suite (IBS). In this paper, we enhance their trace cache mechanism and present a new branch predictor which supplies multiple predictions per cycle. Using these enhancements we show that a 128KB trace cache with a 4KB instruction cache can attain a 30% boost on the SPECint95 benchmarks even if the single block scheme uses an aggressive branch predictor. The enhancements boost the performance of the fetch mechanism, surpassing the scheme presented in [12] by 101% on eqntott, by 14% on gcc, and by 3% on sc, all from the SPECint92 suite. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. M. Conte, K. N. Menezes, P. M. Mills, and B. A. Patel, </author> <title> "Optimization of instruction fetch mechanisms for high issue rates," </title> <booktitle> in Proceedings of the 22st Annual International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: In [15], they demonstrated how a fill unit could help overcome the decoder bottleneck of a Pentium Pro type processor. Cache organization for simultaneously fetching multiple basic blocks have been studied by Yeh et al [18] and Conte et al <ref> [1] </ref> and Seznec et al [13]. By multiporting the instruction cache and/or the branch target buffer (BTB) and generating multiple fetch addresses and branch predictions per cycle, these schemes are able to overcome the single basic block fetch bottleneck.
Reference: [2] <author> S. Dutta and M. Franklin, </author> <title> "Control flow prediction with tree-like subgraphs for superscalar processors," </title> <booktitle> in Proceedings of the 28th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 258-263, </pages> <year> 1995. </year>
Reference-contexts: By multiporting the instruction cache and/or the branch target buffer (BTB) and generating multiple fetch addresses and branch predictions per cycle, these schemes are able to overcome the single basic block fetch bottleneck. Dutta and Franklin <ref> [2] </ref> proposed subgraph oriented branch prediction mechanisms for these schemes. A different approach to solving the fetch bottleneck problem is the Block-Structured ISA [8] [4].
Reference: [3] <author> M. Franklin and M. Smotherman, </author> <title> "A fill-unit approach to multiple instruction issue," </title> <booktitle> in Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 162-171, </pages> <year> 1994. </year>
Reference-contexts: In [9] the performance implications of the fill unit are discussed and idea of combining basic blocks is proposed. Two other extensions of the original scheme were presented by Smotherman and Franklin. In <ref> [3] </ref>, they reworked the fill unit finalization strategy by restricting the type of instruction dependencies allowed in a fill unit line and by filling both paths beyond a conditional branch. In [15], they demonstrated how a fill unit could help overcome the decoder bottleneck of a Pentium Pro type processor.
Reference: [4] <author> E. Hao, P.-Y. Chang, M. Evers, and Y. N. Patt, </author> <title> "Increasing the instruction fetch rate via block-structured instruction set architectures," </title> <booktitle> in To appear in Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1996. </year>
Reference-contexts: Dutta and Franklin [2] proposed subgraph oriented branch prediction mechanisms for these schemes. A different approach to solving the fetch bottleneck problem is the Block-Structured ISA [8] <ref> [4] </ref>. By organizing the static form of the program into enlarged atomic units composed of multiple traditional basic blocks, the compiler is used to address the fetch bandwidth issue. The trace cache concept was originally published by Rotenberg et al [12].
Reference: [5] <author> D. Kaeli and P. Emma, </author> <title> "Branch history table predictions of moving target branches due to subroutine returns," </title> <booktitle> in Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <year> 1991. </year>
Reference-contexts: This field encodes the direction of each branch in the segment and includes bits to identify whether a segment ends in a branch and whether that branch is a return from subroutine instruction. In the case of a return instruction, the return address stack <ref> [5] </ref> provides the next fetch address. The total size of a line is around 85 bytes for a typical architecture. An important feature of the trace cache is that instructions are stored into the trace cache after they have been partially processed by the machine.
Reference: [6] <author> J. K. F. Lee and A. J. Smith, </author> <title> "Branch prediction strategies and branch target buffer design," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 6-22, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: The instructions from the icache must be decoded, a step which can be eliminated in the case of a trace cache hit. Also, misses in the instruction cache are only serviced if they are the result of a trace cache miss. A branch target buffer <ref> [6] </ref> is used to determine the next fetch address when fetching from the instruction cache. 3.5 The Fetch Cycle At the beginning of the cycle the fetch address, determined in the previous cycle, is used to index into the trace cache, instruction cache, and the branch target buffer.
Reference: [7] <author> S. McFarling, </author> <title> "Combining branch predictors," </title> <type> Technical Report TN-36, </type> <institution> Digital Western Research Laboratory, </institution> <month> June </month> <year> 1993. </year> <month> 20 </month>
Reference-contexts: All three predictions are made with a single access to the pattern history table. For the trace cache predictor, the pattern history table is indexed using the gshare scheme outlined by McFarling <ref> [7] </ref>. The global branch history is XORed with the current fetch address forming an index into the PHT. The extra selection done after the PHT is indexed adds two extra bits of branch history. Thus such a predictor indexed with 14 bits of history actually utilizes 16 bits of history.
Reference: [8] <author> S. Melvin and Y. Patt, </author> <title> "Enhancing instruction scheduling with a block-structured ISA," </title> <journal> Interna--tional Journal on Parallel Processing, </journal> <year> 1994. </year>
Reference-contexts: Dutta and Franklin [2] proposed subgraph oriented branch prediction mechanisms for these schemes. A different approach to solving the fetch bottleneck problem is the Block-Structured ISA <ref> [8] </ref> [4]. By organizing the static form of the program into enlarged atomic units composed of multiple traditional basic blocks, the compiler is used to address the fetch bandwidth issue. The trace cache concept was originally published by Rotenberg et al [12].
Reference: [9] <author> S. W. Melvin and Y. N. Patt, </author> <title> "Performance benefits of large execution atomic units in dynamically scheduled machines," </title> <booktitle> in Proceedings of Supercomputing '89, </booktitle> <pages> pp. 427-432, </pages> <year> 1989. </year>
Reference-contexts: A hit in the decoded instruction cache results in a larger atomic unit of work than would be possible if the individual instructions were fetched from the instruction cache. In <ref> [9] </ref> the performance implications of the fill unit are discussed and idea of combining basic blocks is proposed. Two other extensions of the original scheme were presented by Smotherman and Franklin.
Reference: [10] <author> S. W. Melvin, M. C. Shebanow, and Y. N. Patt, </author> <title> "Hardware support for large atomic units in dynamically scheduled machines," </title> <booktitle> in Proceedings of the 21st Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 60-63, </pages> <year> 1988. </year>
Reference-contexts: Getting enough fetch bandwidth is serious problem for wide issue processors and several mechanisms have been proposed to help overcome the single basic block fetch hurdle. 3 Melvin, Shebanow and Patt <ref> [10] </ref> introduced the concept of the fill unit. They proposed the fill unit to compact the microinstructions of a basic block's worth of instructions into an entry in a decoded instruction cache.
Reference: [11] <author> Y. Patt, W. Hwu, and M. Shebanow, "HPS, </author> <title> a new microarchitecture: Rationale and introduction," </title> <booktitle> in Proceedings of the 18th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 103-107, </pages> <year> 1985. </year>
Reference-contexts: If both the trace cache and instruction cache respond with a miss, then a request for the missing icache lines is made to the second level cache. The fetch mechanism stalls until the missing line arrives. 4 Experimental Setup The HPS model of execution <ref> [11] </ref> was designed to exploit instruction level parallelism using Tomasulo's Algorithm for dynamic scheduling [17]. The HPS model maintains a window of instructions through node tables or reservation stations.
Reference: [12] <author> E. Rotenberg, S. Bennett, and J. E. Smith, </author> <title> "Trace cache: a low latency approach to high bandwidth instruction fetching," </title> <booktitle> in To appear in Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1996. </year>
Reference-contexts: By organizing the static form of the program into enlarged atomic units composed of multiple traditional basic blocks, the compiler is used to address the fetch bandwidth issue. The trace cache concept was originally published by Rotenberg et al <ref> [12] </ref>. They presented an outstanding groundwork comparing the trace cache scheme to the current hardware based high-bandwidth fetch schemes, clearly showing the advantage of using a trace cache, both in performance and latency. <p> The experiments compare various combinations of trace cache and instruction cache, both 4-way set associative, against the results presented in <ref> [12] </ref> for a 128KB instruction cache augmented with a 4KB trace cache. Figure 8 shows a marked 101% improvement in the eqntott benchmark and modest 14% improvement in gcc and a slight 3% improvement in sc. <p> To make a fair comparison due to differences in branch predictors, the new predictor uses 56Kbits (containing 4096 PHT entries) of storage. The predictor presented in <ref> [12] </ref> contains 32Kbits in a triple 16 Benchmark 64KB 128KB compress 99.985% 99.986% gcc 88.702% 93.530% ijpeg 99.029% 99.470% li 94.358% 94.358% perl 98.251% 99.196% vortex 95.344% 98.003% Table 2: Percent of fetches supplied by the trace cache for 64KB and 128KB trace cache on the SPECint95 benchmarks. read ported structure
Reference: [13] <author> A. Seznec, S. Jourdan, P. Sainrat, and P. Michaud, </author> <title> "Multiple-block ahead branch predictors," </title> <booktitle> in Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1996. </year>
Reference-contexts: In [15], they demonstrated how a fill unit could help overcome the decoder bottleneck of a Pentium Pro type processor. Cache organization for simultaneously fetching multiple basic blocks have been studied by Yeh et al [18] and Conte et al [1] and Seznec et al <ref> [13] </ref>. By multiporting the instruction cache and/or the branch target buffer (BTB) and generating multiple fetch addresses and branch predictions per cycle, these schemes are able to overcome the single basic block fetch bottleneck. Dutta and Franklin [2] proposed subgraph oriented branch prediction mechanisms for these schemes.
Reference: [14] <author> J. E. Smith, </author> <title> "A study of branch prediction strategies," </title> <booktitle> in Proceedings of the 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 135-148, </pages> <year> 1981. </year>
Reference-contexts: The second level history is maintained as a table, called the pattern history table (PHT). Previous studies have shown that a saturating two bit counter <ref> [14] </ref> is sufficient for maintaining second level history. In order to make three predictions per cycle, we expand each PHT entry from a single two bit counter into seven two bit counters. Figure 4 shows how the seven counters are used to supply three predictions per cycle.
Reference: [15] <author> M. Smotherman and M. Franklin, </author> <title> "Improving cisc instruction decoding performance using a fill unit," </title> <booktitle> in Proceedings of the 28th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 219-229, </pages> <year> 1995. </year>
Reference-contexts: Two other extensions of the original scheme were presented by Smotherman and Franklin. In [3], they reworked the fill unit finalization strategy by restricting the type of instruction dependencies allowed in a fill unit line and by filling both paths beyond a conditional branch. In <ref> [15] </ref>, they demonstrated how a fill unit could help overcome the decoder bottleneck of a Pentium Pro type processor. Cache organization for simultaneously fetching multiple basic blocks have been studied by Yeh et al [18] and Conte et al [1] and Seznec et al [13].
Reference: [16] <author> E. Sprangle and Y. Patt, </author> <title> "Facilitating superscalar processing via a combined static/dynamic register renaming scheme," </title> <booktitle> in Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 143-147, </pages> <year> 1994. </year>
Reference-contexts: If the register file is sourced before the instructions are added to the instruction window, this scheme also allows a reduction in register file read and write ports as demonstrated by Sprangle and Patt <ref> [16] </ref>. Finally, instructions can be stored in an order which permits simple dispatch. The ordering of instructions within the trace cache line is arbitrary; dependencies are explicitly marked.
Reference: [17] <author> R. M. Tomasulo, </author> <title> "An efficient algorithm for exploiting multiple arithmetic units," </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 11, </volume> <pages> pp. 25-33, </pages> <month> January </month> <year> 1967. </year> <month> 21 </month>
Reference-contexts: The fetch mechanism stalls until the missing line arrives. 4 Experimental Setup The HPS model of execution [11] was designed to exploit instruction level parallelism using Tomasulo's Algorithm for dynamic scheduling <ref> [17] </ref>. The HPS model maintains a window of instructions through node tables or reservation stations. Operand tags are passed from functional units producing results into the node tables, informing waiting instructions of ready operands, causing some to be scheduled for execution.
Reference: [18] <author> T.-Y. Yeh, D. Marr, and Y. N. Patt, </author> <title> "Increasing the instruction fetch rate via multiple branch predic-tion and branch address cache," </title> <booktitle> in Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pp. 67-76, </pages> <year> 1993. </year>
Reference-contexts: In [15], they demonstrated how a fill unit could help overcome the decoder bottleneck of a Pentium Pro type processor. Cache organization for simultaneously fetching multiple basic blocks have been studied by Yeh et al <ref> [18] </ref> and Conte et al [1] and Seznec et al [13]. By multiporting the instruction cache and/or the branch target buffer (BTB) and generating multiple fetch addresses and branch predictions per cycle, these schemes are able to overcome the single basic block fetch bottleneck.
Reference: [19] <author> T.-Y. Yeh and Y. N. Patt, </author> <title> "Two-level adaptive branch prediction," </title> <booktitle> in Proceedings of the 24th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 51-61, </pages> <year> 1991. </year> <month> 22 </month>
Reference-contexts: In the case of the trace cache mechanism, three predictions per cycle are required. Two-level adaptive branch prediction has been demonstrated to achieve high prediction accuracy over a wide set of applications <ref> [19] </ref>. In a two-level scheme, the first level of history records the outcomes of the most recently executed branches. The second level of history records the most likely outcome when a particular pattern in the first level history is encountered.
References-found: 19

