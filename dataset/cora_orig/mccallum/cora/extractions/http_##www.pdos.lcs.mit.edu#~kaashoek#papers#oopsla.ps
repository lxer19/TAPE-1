URL: http://www.pdos.lcs.mit.edu/~kaashoek/papers/oopsla.ps
Refering-URL: http://www.pdos.lcs.mit.edu/~kaashoek/papers.html
Root-URL: 
Email: bal@cs.vu.nl  kaashoek@lcs.mit.edu  
Title: Object Distribution in Orca using Compile-Time and Run-Time Techniques  
Author: Henri E. Bal M. Frans Kaashoek 
Address: Amsterdam, The Netherlands  Cambridge, MA  
Affiliation: Vrije Universiteit Dept. of Mathematics and Computer Science  M.I.T. Laboratory for Computer Science  
Abstract: Orca is a language for parallel programming on distributed systems. Communication in Orca is based on shared data-objects, which is a form of distributed shared memory. The performance of Orca programs depends strongly on how shared data-objects are distributed among the local physical memories of the processors. This paper studies a new and efficient solution to this problem, based on an integration of compile-time and run-time techniques. The Orca compiler has been extended to determine the access patterns of processes to shared objects. The compiler passes a summary of this information to the run-time system, which uses it to make good decisions about which objects to replicate and where to store nonreplicated objects. Measurements show that the new system gives better overall performance than any previous implementation of Orca. 3333333333333333 1 This research was supported in part by a PIONIER grant 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> H.E. Bal, </author> <title> Programming Distributed Systems, </title> <publisher> Prentice Hall Int'l, </publisher> <address> Hemel Hempstead, UK (1991). </address>
Reference-contexts: In this paper we will introduce a novel approach to the data distribution problem based on a collaboration between the compiler and RTS. This method has been designed for the Orca programming language <ref> [1, 2] </ref>. Orca is a language for implementing coarse-grained, explicitly-parallel applications on distributed systems, such as collections of workstations connected by a Local Area Network. Logically shared data structures in Orca are encapsulated in variables of abstract data types, called shared data-objects [3], which have user-defined indivisible operations. <p> THE ORCA LANGUAGE AND ITS IMPLEMENTATION In this section we will give a brief description of the Orca language and its implementation. The goal is to give just enough detail to make the rest of the paper understandable. More detailed descriptions are given elsewhere <ref> [1, 2, 3] </ref>. 2.1. Orca Orca is a language for writing parallel programs for systems that do not have physical shared memory. Processes in Orca communicate through shared data-objects, which are instances of abstract data types. Processes can share objects even if they run on different machines. <p> Still, the ability to share the global bound more than compensates this disadvantage, since it allows efficient pruning of the search tree <ref> [1] </ref>. Our scheme thus makes good decisions for object distribution at acceptable runtime costs. Let us also look at some disadvantages of our approach, however. One problem with the system described here is that it replicates objects either everywhere or nowhere.
Reference: 2. <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanen-baum, ``Orca: A Language for Parallel Programming of Distributed Systems,'' </title> <journal> IEEE Trans. on Software Engineering 18(3), </journal> <pages> pp. </pages> <month> 190-205 (March </month> <year> 1992). </year>
Reference-contexts: In this paper we will introduce a novel approach to the data distribution problem based on a collaboration between the compiler and RTS. This method has been designed for the Orca programming language <ref> [1, 2] </ref>. Orca is a language for implementing coarse-grained, explicitly-parallel applications on distributed systems, such as collections of workstations connected by a Local Area Network. Logically shared data structures in Orca are encapsulated in variables of abstract data types, called shared data-objects [3], which have user-defined indivisible operations. <p> THE ORCA LANGUAGE AND ITS IMPLEMENTATION In this section we will give a brief description of the Orca language and its implementation. The goal is to give just enough detail to make the rest of the paper understandable. More detailed descriptions are given elsewhere <ref> [1, 2, 3] </ref>. 2.1. Orca Orca is a language for writing parallel programs for systems that do not have physical shared memory. Processes in Orca communicate through shared data-objects, which are instances of abstract data types. Processes can share objects even if they run on different machines. <p> The semantics of the model are straightforward. All operations are applied to single objects, making the model efficient to implement. Operations are executed sequentially consistent [15]. They are also executed indivisibly, which simplifies programming, since mutual exclusion synchronization is done automatically <ref> [2] </ref>. Although Orca's communication model is based on objects, Orca is not an object-oriented language. It does not support inheritance or dynamic binding, and neither does it treat all entities as objects. <p> Hence, the state information on all processors is always consistent and all processors will make the same decisions. As an example, assume CPU 0 executes a statement fork AddOne (A, B) on 2; # AddOne was defined in 3 The RTS on CPU 0 broadcasts a message <ref> [FORK, AddOne, A, B, 2] </ref> to all processors. Each RTS receives this message and updates the state information for objects A and B on CPU 2. The Nreads value for object A is incremented by one, since A is read once by AddOne. <p> In other words, the operation to get a job will be sent point-to-point instead of being broadcast. The global minimum is still replicated, which is important since, in practice, it is read much more frequently than it is written <ref> [2] </ref>. The compiler and RTS make the right decisions for both objects.
Reference: 3. <author> A.S. Tanenbaum, M.F. Kaashoek, and H.E. Bal, </author> <title> ``Parallel Programming using Shared Objects and Broadcasting,'' </title> <booktitle> IEEE Computer 25(8), </booktitle> <pages> pp. </pages> <month> 10-19 (Aug. </month> <year> 1992). </year>
Reference-contexts: Orca is a language for implementing coarse-grained, explicitly-parallel applications on distributed systems, such as collections of workstations connected by a Local Area Network. Logically shared data structures in Orca are encapsulated in variables of abstract data types, called shared data-objects <ref> [3] </ref>, which have user-defined indivisible operations. Objects in Orca can be migrated or replicated by the system, without any intervention from the user. Orca has been designed to hide data distribution from the programmer, but to allow a compiler and RTS to implement shared objects efficiently. <p> THE ORCA LANGUAGE AND ITS IMPLEMENTATION In this section we will give a brief description of the Orca language and its implementation. The goal is to give just enough detail to make the rest of the paper understandable. More detailed descriptions are given elsewhere <ref> [1, 2, 3] </ref>. 2.1. Orca Orca is a language for writing parallel programs for systems that do not have physical shared memory. Processes in Orca communicate through shared data-objects, which are instances of abstract data types. Processes can share objects even if they run on different machines. <p> All other operations are write operations. A write operation may also read the object, but in contrast to a read operation it potentially modifies the object. Orca can be implemented efficiently on a distributed system by replicating shared objects in each processor's local memory <ref> [3] </ref>. If a processor has a local copy of an object, it can do read operations locally, without doing any communication. Write operations are broadcast to all nodes containing a copy. All these nodes update their copy by applying the write operation to the copy. <p> Write operations are broadcast to all nodes containing a copy. All these nodes update their copy by applying the write operation to the copy. We use Amoeba's [17] totally-ordered reliable broadcast protocol <ref> [3, 18] </ref>, which delivers all messages reliably and in the same order at all receivers. Because the broadcast is totally-ordered, copies of objects are updated in a consistent way. <p> The best-known example is Kai Li's Shared Virtual Memory [8], which is a page-based DSM system. Page-based DSM systems, however, have important performance disadvantages when compared to object-based systems such as Orca <ref> [3] </ref>. Several modern DSM systems address this problem and increase performance by relaxing the semantics of the memory model. Examples of such systems are Munin [10], Midway [35] and several others [36]. Also, a recent extension of the Amber language has been proposed supporting version consistency for shared objects [37].
Reference: 4. <author> H.E. Bal, M.F. Kaashoek, A.S. Tanenbaum, and J. Jansen, </author> <title> ``Replication Techniques for Speeding up Parallel Applications on Distributed Systems,'' </title> <journal> Concurrency Practice & Experience 4(5), </journal> <pages> pp. </pages> <month> 337-355 (Aug. </month> <year> 1992). </year>
Reference-contexts: The problem here is how to find heuristics that work well for all applications. A comparison of these two approaches has shown that in most (but not all) cases, full replication performs better <ref> [4] </ref>. To a certain extent, both solutions benefit from information generated by the compiler. For example, the compiler recognizes read-only operations on shared objects, which are usually executed locally by the RTS, thus reducing the number of messages generated. <p> Many networks have hardware support for multicast, for example Ethernet and some ring networks. Future Gigabit LANs [19] and ATM switches [20] are also expected to support multicast or broadcast. An important issue is the replication strategy <ref> [4] </ref>. The most efficient RTS we had implemented so far simply replicates shared objects on all processors. We have used this RTS as a starting point for our present work and we will also compare the performance of the new system with this RTS. 3. <p> The overhead of broadcasting a fork statement is roughly the same as that of a write operation on a replicated object, which is about 4 msec for operations with 4 bytes of parameters and 8 msec for operations with 1Kb of parameters <ref> [4] </ref>. Normal operations are not slowed down by the decision making process; the overhead only occurs during fork statements. Regarding memory overhead, the compiler recognizes most objects that are not shared, so the RTS does not keep track of the Nreads and Nwrites values.
Reference: 5. <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koel-bel, U. Kremer, C-W. Tseng, and M-Y. Wu, </author> <title> ``FORTRAN-D Language Specification,'' </title> <institution> TR90-141, Rice University (Dec. </institution> <year> 1990). </year>
Reference-contexts: This implementation gives better overall performance than any previous Orca implementation. The issue of data distribution is also addressed in several other languages and systems. In many parallel languages for numerical programming, the user indicates how shared arrays are to be decomposed and mapped onto the different processors <ref> [5, 6, 7] </ref>. In these languages, parallelism itself is often dealt with by the compiler, by analyzing FOR-loops, but data distribution is left (at least partly) to the programmer. The focus or our work, however, is more on nonnumerical applications. <p> RELATED WORK Most of the work on compiler optimizations for distributing shared data takes place in the area of numerical applications, where shared arrays have to be decomposed and partitioned among the memories of different machines. In FORTRAN-D <ref> [5] </ref>, for example, the programmer can specify a strategy for decomposing arrays into blocks. The compiler uses this information to distribute the array among the physical memories and automatically generates send/receive primitives when needed.
Reference: 6. <author> C. Koelbel, P. Mehrota, and J. van Rosendale, </author> <title> ``Supporting Shared Data Structures on Distributed Memory Architectures,'' </title> <booktitle> Proc. 2nd Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <pages> pp. </pages> <month> 177-186 (March </month> <year> 1990). </year>
Reference-contexts: This implementation gives better overall performance than any previous Orca implementation. The issue of data distribution is also addressed in several other languages and systems. In many parallel languages for numerical programming, the user indicates how shared arrays are to be decomposed and mapped onto the different processors <ref> [5, 6, 7] </ref>. In these languages, parallelism itself is often dealt with by the compiler, by analyzing FOR-loops, but data distribution is left (at least partly) to the programmer. The focus or our work, however, is more on nonnumerical applications. <p> Such systems usually adhere to the Single Program Multiple Data (SPMD) [24] style. A wide body of work exists on parallelizing array-oriented programs that run on distributed-memory message-passing machines <ref> [6, 7, 25, 26, 27, 28, 29] </ref>. Also, some work on functional languages is related to this approach [30, 31]. Our work on Orca is much less focused on numerical applications and partitioned arrays.
Reference: 7. <author> M. Rosing, R. Schnabel, and R. Weaver, </author> <title> ``The DINO parallel programming language,'' </title> <journal> Journal of Parallel and Distr. Computing 13(1), </journal> <pages> pp. </pages> <month> 30-42 (Sept. </month> <year> 1991). </year>
Reference-contexts: This implementation gives better overall performance than any previous Orca implementation. The issue of data distribution is also addressed in several other languages and systems. In many parallel languages for numerical programming, the user indicates how shared arrays are to be decomposed and mapped onto the different processors <ref> [5, 6, 7] </ref>. In these languages, parallelism itself is often dealt with by the compiler, by analyzing FOR-loops, but data distribution is left (at least partly) to the programmer. The focus or our work, however, is more on nonnumerical applications. <p> Such systems usually adhere to the Single Program Multiple Data (SPMD) [24] style. A wide body of work exists on parallelizing array-oriented programs that run on distributed-memory message-passing machines <ref> [6, 7, 25, 26, 27, 28, 29] </ref>. Also, some work on functional languages is related to this approach [30, 31]. Our work on Orca is much less focused on numerical applications and partitioned arrays.
Reference: 8. <author> K. Li and P. Hudak, </author> <title> ``Memory Coherence in Shared Virtual Memory Systems,'' </title> <journal> ACM Trans. Comp. Syst. </journal> <volume> 7(4) (Nov. </volume> <year> 1989). </year>
Reference-contexts: The focus or our work, however, is more on nonnumerical applications. Also, our goal is not to parallelize dusty deck programs, but to use modern language techniques for writing explicitly-parallel programs. The Shared Virtual Memory of Li and Hudak <ref> [8] </ref> implements data distribution in the operating system, by partitioning the shared address space into fixed-size pages that are moved and copied transparently among the local memories of the processors. Linda [9] also hides data distribution from programmers. <p> Distributed Shared Memory (DSM) systems also support logically shared data on distributed-memory machines, often by simulating a physical shared memory. The best-known example is Kai Li's Shared Virtual Memory <ref> [8] </ref>, which is a page-based DSM system. Page-based DSM systems, however, have important performance disadvantages when compared to object-based systems such as Orca [3]. Several modern DSM systems address this problem and increase performance by relaxing the semantics of the memory model.
Reference: 9. <author> S. Ahuja, N. Carriero, and D. Gelernter, ``Linda and Friends,'' </author> <booktitle> IEEE Computer 19(8), </booktitle> <pages> pp. </pages> <month> 26-34 (Aug. </month> <year> 1986). </year>
Reference-contexts: The Shared Virtual Memory of Li and Hudak [8] implements data distribution in the operating system, by partitioning the shared address space into fixed-size pages that are moved and copied transparently among the local memories of the processors. Linda <ref> [9] </ref> also hides data distribution from programmers. Replication and partitioning of the Linda Tuple Space is mainly taken care of by the RTS, sometimes also using information generated by the Linda compiler. <p> Like our results their trace driven simulations show that a data structure-specific data management policy is better than a single system-imposed policy. Totty and Reed, however, do not address how a policy is selected and who is selecting it. A system related to Orca is Linda <ref> [9] </ref>, which is also based on explicit parallelism and communication through shared data. Linda uses extensive compile-time optimization [33], but this is mainly aimed at reducing the overhead of associative addressing of Tuple Space.
Reference: 10. <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel, ``Munin: </author> <title> Distributed Shared Memory Based on Type-Specific Memory Coherence,'' </title> <booktitle> Proc. 2nd Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <pages> pp. </pages> <month> 168-176 (March </month> <year> 1990). </year>
Reference-contexts: Linda [9] also hides data distribution from programmers. Replication and partitioning of the Linda Tuple Space is mainly taken care of by the RTS, sometimes also using information generated by the Linda compiler. In Munin <ref> [10] </ref> the RTS takes care of data mapping, using annotations from the user about how shared variables are accessed. <p> Hence, we think we can easily adapt our system to different platforms. We believe that our approach of explicitly computing read-write patterns opens a whole area for research on further optimizations as well. One important class of such optimizations is adaptive caching, proposed in the Munin project <ref> [10] </ref>. The idea is to classify shared objects based on the way they are used and to use different coherence mechanisms for different classes of objects. The current Munin implementation leaves the classification up to the programmer. <p> Page-based DSM systems, however, have important performance disadvantages when compared to object-based systems such as Orca [3]. Several modern DSM systems address this problem and increase performance by relaxing the semantics of the memory model. Examples of such systems are Munin <ref> [10] </ref>, Midway [35] and several others [36]. Also, a recent extension of the Amber language has been proposed supporting version consistency for shared objects [37]. Orca does not relax the semantics of the model, since we believe that it would complicate programming.
Reference: 11. <author> E. Jul, H. Levy, N. Hutchinson, and A. Black, </author> <title> ``Fine-Grained Mobility in the Emerald System,'' </title> <journal> ACM Trans. Comp. Syst. </journal> <volume> 6(1), </volume> <pages> pp. </pages> <month> 109-133 (Feb. </month> <year> 1988). </year>
Reference-contexts: In some object-based languages the RTS provides the mechanisms for data distribution, but the actual decision to move objects is usually made by the programmer, using annotations or special statements <ref> [11, 12] </ref>. Finally, multiprocessor systems such as Dash [13] and Alewife [14] try to solve the data distribution in hardware. The structure of the rest of this paper is as follows. In Section 2, we briefly describe the Orca language and its implementation. <p> The protocols implementing these mechanisms are relatively straightforward, as Orca has only passive objects and since simple communication primitives with clean semantics (RPC and totally-ordered broadcast) are used. In Emerald, for example, object migration is more complex, because objects may contain processes <ref> [11] </ref>. 5. PERFORMANCE OF THE NEW SYSTEM We have used the new system for several Orca applications. Here, we will look at three such applications. <p> As far as we know, however, no existing Linda system integrates compile-time and run-time optimizations in the same way as our system does. Although Orca's communication model is based on objects, the language differs in many important ways from object-oriented languages such as Emerald <ref> [11] </ref> and POOL-T [34]. Objects in Orca are passive and are replicated automatically. Also, in Orca all operations on objects are guaranteed to be indivisible. In most concurrent object-oriented languages, objects can be active, are not replicated, and communicate by sending messages to each other, rather than through indivisible operations.
Reference: 12. <author> J.S. Chase, F.G. Amador, E.D. Lazowska, H.M. Levy, and R.J. Littlefield, </author> <title> ``The Amber System: Parallel Programming on a Network of Multiprocessors,'' </title> <booktitle> Proc. of the 12th ACM Symp. on Operating System Principles, </booktitle> <address> Litch-field Park, AZ, </address> <pages> pp. </pages> <month> 147-158 (Dec. </month> <year> 1989). </year>
Reference-contexts: In some object-based languages the RTS provides the mechanisms for data distribution, but the actual decision to move objects is usually made by the programmer, using annotations or special statements <ref> [11, 12] </ref>. Finally, multiprocessor systems such as Dash [13] and Alewife [14] try to solve the data distribution in hardware. The structure of the rest of this paper is as follows. In Section 2, we briefly describe the Orca language and its implementation. <p> For each point, SOR first computes the average value of its four neighbors and then updates the point using this value. Our implementation of SOR in Orca is based on the parallel Red/Black SOR algorithm used for the Amber system <ref> [12] </ref>. The grid is partitioned among a number of worker processes, one per processor. Each worker contains a vertical slice of the grid. The processes are organized in a linear row. At the beginning of an iteration, each worker needs to exchange edge values with its left and right neighbor.
Reference: 13. <author> D. Lenoski, J. Laudon, K. Gharachorloo, W-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M.S. Lam, </author> <title> ``The Stanford Dash Multiprocessor,'' </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 63-79, </pages> <address> Stanford Univ (March 1992). </address>
Reference-contexts: In some object-based languages the RTS provides the mechanisms for data distribution, but the actual decision to move objects is usually made by the programmer, using annotations or special statements [11, 12]. Finally, multiprocessor systems such as Dash <ref> [13] </ref> and Alewife [14] try to solve the data distribution in hardware. The structure of the rest of this paper is as follows. In Section 2, we briefly describe the Orca language and its implementation. In Section 3, we give an overview of the new compiler and RTS.
Reference: 14. <author> D. Chaiken, C. Fields, K. Kurihara, and A. Agarwal, </author> <title> ``Directory-Based Cache Coherence in Large-Scale Multiprocessors,'' </title> <note> IEEE Computer (June 1990). </note>
Reference-contexts: In some object-based languages the RTS provides the mechanisms for data distribution, but the actual decision to move objects is usually made by the programmer, using annotations or special statements [11, 12]. Finally, multiprocessor systems such as Dash [13] and Alewife <ref> [14] </ref> try to solve the data distribution in hardware. The structure of the rest of this paper is as follows. In Section 2, we briefly describe the Orca language and its implementation. In Section 3, we give an overview of the new compiler and RTS.
Reference: 15. <author> L. Lamport, </author> <title> ``How to Make a Multiprocessor Computer that Correctly Executes Multipro-cess Programs,'' </title> <journal> IEEE Trans. on Computers C-28(9), </journal> <pages> pp. </pages> <month> 690-691 (Sept. </month> <year> 1979). </year>
Reference-contexts: The semantics of the model are straightforward. All operations are applied to single objects, making the model efficient to implement. Operations are executed sequentially consistent <ref> [15] </ref>. They are also executed indivisibly, which simplifies programming, since mutual exclusion synchronization is done automatically [2]. Although Orca's communication model is based on objects, Orca is not an object-oriented language. It does not support inheritance or dynamic binding, and neither does it treat all entities as objects. <p> Also, a recent extension of the Amber language has been proposed supporting version consistency for shared objects [37]. Orca does not relax the semantics of the model, since we believe that it would complicate programming. Orca therefore supports sequential consistency <ref> [15] </ref> and leaves it to the implementation to obtain high performance, by using techniques like the ones discussed in this paper. 8. CONCLUSIONS We have described a new approach for implementing Orca's shared data-objects in a distributed environment.
Reference: 16. <author> L. Uljee and H.-J. Visscher, ``C++/Orca,'' </author> <type> Master's thesis, </type> <institution> Vrije Universiteit, </institution> <address> Amster-dam (Sept. </address> <year> 1992). </year>
Reference-contexts: Our primary research objectives are the design of an easy-to-use communication and synchronization model and its efficient distributed implementation. Integrating this model in an object-oriented framework thus is not our major concern, although we do have an experimental version of our model embedded in C++ <ref> [16] </ref>. 2.2. A distributed implementation of Orca The Orca compiler partitions operations on objects into two classes: read operations and write operations. A read operation is an operation that does not modify the object's local data. All other operations are write operations.
Reference: 17. <author> A.S. Tanenbaum, </author> <title> Modern Operating Systems, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ (1992). </address>
Reference-contexts: If a processor has a local copy of an object, it can do read operations locally, without doing any communication. Write operations are broadcast to all nodes containing a copy. All these nodes update their copy by applying the write operation to the copy. We use Amoeba's <ref> [17] </ref> totally-ordered reliable broadcast protocol [3, 18], which delivers all messages reliably and in the same order at all receivers. Because the broadcast is totally-ordered, copies of objects are updated in a consistent way.
Reference: 18. <author> M.F. Kaashoek, </author> <title> ``Group Communication in Distributed Computer Systems,'' </title> <type> Ph.D. thesis, </type> <institution> Vrije Universiteit, </institution> <address> Amsterdam (Dec. </address> <year> 1992). </year>
Reference-contexts: Write operations are broadcast to all nodes containing a copy. All these nodes update their copy by applying the write operation to the copy. We use Amoeba's [17] totally-ordered reliable broadcast protocol <ref> [3, 18] </ref>, which delivers all messages reliably and in the same order at all receivers. Because the broadcast is totally-ordered, copies of objects are updated in a consistent way. <p> A more precise analysis of the performance (in terms of number of messages sent and their sizes) is given in <ref> [18] </ref>. The Orca systems used for the measurements run on top of the Amoeba distributed operating system. The hardware we use is a collection of 20MHz MC68030s connected through a 10 Mbit/s Ethernet. 5.1.
Reference: 19. <author> H.T. Kung, </author> <title> ``Gigabit Local Area Networks: a Systems Perspective,'' </title> <journal> IEEE Communications Magazine 30(4), </journal> <pages> pp. </pages> <month> 79-89 (April </month> <year> 1992). </year>
Reference-contexts: Our broadcast protocol uses the hardware mul-ticast or broadcast facility of the underlying network, if available, and is most efficient on such networks. Many networks have hardware support for multicast, for example Ethernet and some ring networks. Future Gigabit LANs <ref> [19] </ref> and ATM switches [20] are also expected to support multicast or broadcast. An important issue is the replication strategy [4]. The most efficient RTS we had implemented so far simply replicates shared objects on all processors.
Reference: 20. <author> E. Biagioni, E. Cooper, and R. Sansom, </author> <title> ``Designing a Practical ATM LAN,'' </title> <booktitle> IEEE Network 7(2), </booktitle> <pages> pp. </pages> <month> 32-39 (March </month> <year> 1993). </year>
Reference-contexts: Our broadcast protocol uses the hardware mul-ticast or broadcast facility of the underlying network, if available, and is most efficient on such networks. Many networks have hardware support for multicast, for example Ethernet and some ring networks. Future Gigabit LANs [19] and ATM switches <ref> [20] </ref> are also expected to support multicast or broadcast. An important issue is the replication strategy [4]. The most efficient RTS we had implemented so far simply replicates shared objects on all processors.
Reference: 21. <author> T.E. Jeremiassen and S.J. Eggers, </author> <title> ``Computing Per-Process Summary Side-Effect Information,'' </title> <booktitle> Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <pages> pp. </pages> <month> 115-122 (Aug. </month> <year> 1992). </year>
Reference-contexts: Computing the patterns is straightforward, since features that make control flow and data flow analysis difficult, such as goto statements, global variables, and pointers have been intentionally omitted from Orca. Also, process types are part of the language syntax, so techniques such as separating control flow graphs <ref> [21] </ref> are not needed. The only difficulty is handling recursive (or mutually recursive) functions. The current compiler stops simulating inline substitutions after a certain depth. A more advanced implementation should at least handle tail-recursion similarly to iteration. The second phase is done by the pattern analyzer.
Reference: 22. <author> J.-F. Jenq and S. Sahni, </author> <title> ``All Pairs Shortest Paths on a Hypercube Multiprocessor,'' </title> <booktitle> Proc. 1987 Int. Conf. Parallel Processing , St. </booktitle> <address> Charles, IL, </address> <pages> pp. </pages> <month> 713-716 (Aug. </month> <year> 1987). </year>
Reference-contexts: In this problem, it is desired to find the length of the shortest path from any node i to any other node j in a given graph with N nodes. The parallel algorithm we use is similar to the one given in <ref> [22] </ref>, which itself is a parallel version of Floyd's algorithm. The distances between the nodes are represented in a matrix. Each processor contains a worker process that computes part of the result matrix. The parallel algorithm performs N iterations.
Reference: 23. <author> T. von Eicken, D.E. Culler, S.C. Goldstein, and K.E. Schauser, </author> <title> ``Active Messages: a Mechanism for Integrated Communication and Computation,'' </title> <booktitle> Proc. 19th Int. Symp. on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <pages> pp. </pages> <month> 256-266 (May </month> <year> 1992). </year>
Reference-contexts: With such optimizations, we hope to be able to hide part of the communication latency. We could hide latency further by continuing computation while sending out an operation, if possible. Other architecture-dependent optimizations like using split-phase transactions as implemented on the CM-5 could also be exploited <ref> [23] </ref>. 7. RELATED WORK Most of the work on compiler optimizations for distributing shared data takes place in the area of numerical applications, where shared arrays have to be decomposed and partitioned among the memories of different machines.
Reference: 24. <author> A.H. Karp, </author> <title> ``Programming for Parallelism,'' </title> <booktitle> IEEE Computer 20(5), </booktitle> <pages> pp. </pages> <month> 43-57 (May </month> <year> 1987). </year>
Reference-contexts: Parallelism in such programs can be obtained by executing different iterations of a loop on different processors, or by performing higher-level operations (e.g. matrix additions) in parallel. Such systems usually adhere to the Single Program Multiple Data (SPMD) <ref> [24] </ref> style. A wide body of work exists on parallelizing array-oriented programs that run on distributed-memory message-passing machines [6, 7, 25, 26, 27, 28, 29]. Also, some work on functional languages is related to this approach [30, 31].
Reference: 25. <author> B. Chapman, P. Mehrota, and H. Zima, </author> <title> ``User Defined Mappings in Vienna FORTRAN,'' </title> <booktitle> ACM SIGPLAN Notices (Proc. Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Multiprocessors) 28(1), </booktitle> <pages> pp. </pages> <month> 72-75 (Jan. </month> <year> 1993). </year>
Reference-contexts: Such systems usually adhere to the Single Program Multiple Data (SPMD) [24] style. A wide body of work exists on parallelizing array-oriented programs that run on distributed-memory message-passing machines <ref> [6, 7, 25, 26, 27, 28, 29] </ref>. Also, some work on functional languages is related to this approach [30, 31]. Our work on Orca is much less focused on numerical applications and partitioned arrays.
Reference: 26. <author> S.K.S. Gupta, S.D. Kaushik, C.-H. Huang, J.R. Johnson, R.W. Johnson, and P. Sadayap-pan, </author> <title> ``A Methodology for Generating Data Distributions to Optimize Communication,'' </title> <booktitle> Proc. 4th IEEE Symp. on Parallel and Distributed Processing , pp. </booktitle> <month> 436-441 (Dec. </month> <year> 1992). </year>
Reference-contexts: Such systems usually adhere to the Single Program Multiple Data (SPMD) [24] style. A wide body of work exists on parallelizing array-oriented programs that run on distributed-memory message-passing machines <ref> [6, 7, 25, 26, 27, 28, 29] </ref>. Also, some work on functional languages is related to this approach [30, 31]. Our work on Orca is much less focused on numerical applications and partitioned arrays.
Reference: 27. <author> M. Gupta and P. Banerjee, </author> <title> ``Demonstration of Automatic Data Partitioning Techniques for Parallelizing Compilers on Multicomputers,'' </title> <journal> IEEE Trans. on Parallel and Distributed Systems 3(2), </journal> <pages> pp. </pages> <month> 179-193 (March </month> <year> 1992). </year>
Reference-contexts: Such systems usually adhere to the Single Program Multiple Data (SPMD) [24] style. A wide body of work exists on parallelizing array-oriented programs that run on distributed-memory message-passing machines <ref> [6, 7, 25, 26, 27, 28, 29] </ref>. Also, some work on functional languages is related to this approach [30, 31]. Our work on Orca is much less focused on numerical applications and partitioned arrays.
Reference: 28. <author> E.M. Paalvast, A.J. van Gemund, and H.J. Sips, </author> <title> ``A Method for Parallel Program Generation with an Application to the Booster Language,'' </title> <booktitle> Proc. 1990 ACM Int. Conf. on Supercomputing, </booktitle> <address> Amsterdam (June 1990). </address>
Reference-contexts: Such systems usually adhere to the Single Program Multiple Data (SPMD) [24] style. A wide body of work exists on parallelizing array-oriented programs that run on distributed-memory message-passing machines <ref> [6, 7, 25, 26, 27, 28, 29] </ref>. Also, some work on functional languages is related to this approach [30, 31]. Our work on Orca is much less focused on numerical applications and partitioned arrays.
Reference: 29. <author> M.W. Hall, S. Hirananani, K. Kennedy, and C. Tseng, </author> <title> ``Interprocedural Compilation of FORTRAN-D for MIMD Distributed-Memory Machines,'' </title> <booktitle> Proc. Supercomputing'92, Min-neapolis, </booktitle> <pages> pp. </pages> <month> 522-534 (Nov. </month> <year> 1992). </year>
Reference-contexts: Such systems usually adhere to the Single Program Multiple Data (SPMD) [24] style. A wide body of work exists on parallelizing array-oriented programs that run on distributed-memory message-passing machines <ref> [6, 7, 25, 26, 27, 28, 29] </ref>. Also, some work on functional languages is related to this approach [30, 31]. Our work on Orca is much less focused on numerical applications and partitioned arrays.
Reference: 30. <author> A. Rogers and K. Pingali, </author> <title> ``Process Decomposition Through Locality of Reference,'' </title> <booktitle> ACM SIGPLAN Notices (Proc. SIGPLAN'89 Conf. on Progr. Lang. Design & Impl.), Portland, Oregon 24(7), </booktitle> <pages> pp. </pages> <month> 69-80 (July </month> <year> 1989). </year>
Reference-contexts: Such systems usually adhere to the Single Program Multiple Data (SPMD) [24] style. A wide body of work exists on parallelizing array-oriented programs that run on distributed-memory message-passing machines [6, 7, 25, 26, 27, 28, 29]. Also, some work on functional languages is related to this approach <ref> [30, 31] </ref>. Our work on Orca is much less focused on numerical applications and partitioned arrays. Parallelism in Orca is explicit (through a fork statement) and many forms of synchronization can be expressed, so Orca programs are not necessarily SPMD-like. Objects in Orca are not partitioned but replicated.
Reference: 31. <author> M. Chen, Y. Choo, and J. Li, </author> <title> ``Compiling parallel programs by optimizing performance,'' </title> <journal> Journal of Supercomputing 1(2), </journal> <pages> pp. </pages> <month> 171-207 (July </month> <year> 1988). </year>
Reference-contexts: Such systems usually adhere to the Single Program Multiple Data (SPMD) [24] style. A wide body of work exists on parallelizing array-oriented programs that run on distributed-memory message-passing machines [6, 7, 25, 26, 27, 28, 29]. Also, some work on functional languages is related to this approach <ref> [30, 31] </ref>. Our work on Orca is much less focused on numerical applications and partitioned arrays. Parallelism in Orca is explicit (through a fork statement) and many forms of synchronization can be expressed, so Orca programs are not necessarily SPMD-like. Objects in Orca are not partitioned but replicated.
Reference: 32. <author> B.K. Totty and D.A. Reed, </author> <title> ``Dynamic Object Management for Distributed Data Structures,'' </title> <booktitle> Proc. Supercomputing'92, Minneapolis, </booktitle> <pages> pp. </pages> <month> 692-701 (Nov. </month> <year> 1992). </year>
Reference-contexts: So, the goals of the new Orca compiler and the techniques used are different from those of the languages mentioned above. Totty and Reed run a number of trace driven simulations to determine whether data structure-specific data management is superior to a single system-imposed policy <ref> [32] </ref>. Like our results their trace driven simulations show that a data structure-specific data management policy is better than a single system-imposed policy. Totty and Reed, however, do not address how a policy is selected and who is selecting it.
Reference: 33. <author> N. Carriero, </author> <title> ``The Implementation of Tuple Space Machines,'' </title> <type> Research Report 567 (Ph.D. dissertation), </type> <institution> Yale University, </institution> <address> New Haven, CT (Dec. </address> <year> 1987). </year>
Reference-contexts: Totty and Reed, however, do not address how a policy is selected and who is selecting it. A system related to Orca is Linda [9], which is also based on explicit parallelism and communication through shared data. Linda uses extensive compile-time optimization <ref> [33] </ref>, but this is mainly aimed at reducing the overhead of associative addressing of Tuple Space. As far as we know, however, no existing Linda system integrates compile-time and run-time optimizations in the same way as our system does.
Reference: 34. <author> P. America, ``POOL-T: </author> <title> A Parallel Object-Oriented Language,'' pp. 199-220 in Object-Oriented Concurrent Programming, </title> <editor> ed. A. Yonezawa and M. Tokoro, </editor> <publisher> M.I.T. Press, </publisher> <address> Cam-bridge, MA (1987). </address>
Reference-contexts: As far as we know, however, no existing Linda system integrates compile-time and run-time optimizations in the same way as our system does. Although Orca's communication model is based on objects, the language differs in many important ways from object-oriented languages such as Emerald [11] and POOL-T <ref> [34] </ref>. Objects in Orca are passive and are replicated automatically. Also, in Orca all operations on objects are guaranteed to be indivisible. In most concurrent object-oriented languages, objects can be active, are not replicated, and communicate by sending messages to each other, rather than through indivisible operations.
Reference: 35. <author> B.N. Bershad and M.J. Zekauskas, ``Midway: </author> <title> Shared Memory Parallel Programming with Entry Consistency for Distributed Memory Multiprocessors,'' </title> <address> CMU-CS-91-170, CMU (Sept. </address> <year> 1991). </year>
Reference-contexts: Page-based DSM systems, however, have important performance disadvantages when compared to object-based systems such as Orca [3]. Several modern DSM systems address this problem and increase performance by relaxing the semantics of the memory model. Examples of such systems are Munin [10], Midway <ref> [35] </ref> and several others [36]. Also, a recent extension of the Amber language has been proposed supporting version consistency for shared objects [37]. Orca does not relax the semantics of the model, since we believe that it would complicate programming.
Reference: 36. <author> D. Mosberger, </author> <title> ``Memory Consistency Models,'' </title> <booktitle> ACM Operating Systems Reviews 28(1), </booktitle> <pages> pp. </pages> <month> 18-26 (Jan. </month> <year> 1993). </year>
Reference-contexts: Page-based DSM systems, however, have important performance disadvantages when compared to object-based systems such as Orca [3]. Several modern DSM systems address this problem and increase performance by relaxing the semantics of the memory model. Examples of such systems are Munin [10], Midway [35] and several others <ref> [36] </ref>. Also, a recent extension of the Amber language has been proposed supporting version consistency for shared objects [37]. Orca does not relax the semantics of the model, since we believe that it would complicate programming.
Reference: 37. <author> M.J. Freeley and H.M. Levy, </author> <title> ``Distributed Shared Memory with Versioned Objects,'' </title> <booktitle> Proc. Conf. Object-Oriented Programming Systems, Languages and Applications, </booktitle> <pages> pp. </pages> <month> 247-262 </month> <year> (1992). </year>
Reference-contexts: Several modern DSM systems address this problem and increase performance by relaxing the semantics of the memory model. Examples of such systems are Munin [10], Midway [35] and several others [36]. Also, a recent extension of the Amber language has been proposed supporting version consistency for shared objects <ref> [37] </ref>. Orca does not relax the semantics of the model, since we believe that it would complicate programming. Orca therefore supports sequential consistency [15] and leaves it to the implementation to obtain high performance, by using techniques like the ones discussed in this paper. 8.
References-found: 37

