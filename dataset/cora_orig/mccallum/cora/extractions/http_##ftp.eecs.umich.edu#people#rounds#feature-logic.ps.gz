URL: http://ftp.eecs.umich.edu/people/rounds/feature-logic.ps.gz
Refering-URL: http://ftp.eecs.umich.edu/people/rounds/
Root-URL: http://www.eecs.umich.edu
Title: CHAPTER 1 Feature Logics  
Author: William C. Rounds Van Benthem Ter Meulen 
Note: Contents HANDBOOK OF LOGIC AND LANGUAGE Edited by  c 1994 Elsevier Science B.V. All rights reserved  
Address: Ann Arbor, Michigan 48109 USA  
Affiliation: Artificial Intelligence Laboratory Department of Electrical Engineering and Computer Science University of Michigan  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Abramsky. </author> <title> Domain theory in logical form. </title> <journal> Annals of Pure and Applied Logic, </journal> <volume> 51, </volume> <year> 1991. </year>
Reference-contexts: And one can use other orderings on sets of domain elements to get new powerdomains. Two other very common constructions, for example, are the Hoare (lower) and the Plotkin (convex) powerdomains. See Gunter and Scott [25] for a good survey; other references are Abramsky <ref> [1] </ref>, or Zhang [76] for the details of passing to a logic from a domain. At this point, we have digressed pretty much from linguistics. To return thereto, and to close out the subsection, we sketch the details of another powerdomain construction due to Pollard and Moshier [57]. <p> Call these feature names "R-signs" for the moment. Then, the labels on feature arcs in SFG are sequences of R-signs, standing for the role that a given constituent may have played at different strata. For example, <ref> [2; 1] </ref> : glass tells us that the word "glass" is initially a direct object (2) and finally a subject (1). The label [2; 1] is not a path, but a single member of the label alphabet. Persistence in feature logics is in fact a persistent problem for linguistic analyses. <p> For example, <ref> [2; 1] </ref> : glass tells us that the word "glass" is initially a direct object (2) and finally a subject (1). The label [2; 1] is not a path, but a single member of the label alphabet. Persistence in feature logics is in fact a persistent problem for linguistic analyses. <p> Examples. Consider the sentence Joe breaks the glass. This is diagrammed as a feature structure in Figure 2. We present an (oversimplified) SFG lexical rule, stated in logical format, which accounts for the sentence: breaks ! [Head] 1 : f [Cat] : S ^ <ref> [1] </ref> : true ^ [2] : trueg: This rule states that the head verb "breaks" occurs as the value of the attribute "Head" in a structure which also requires a subject [1] and a direct object [2]. <p> stated in logical format, which accounts for the sentence: breaks ! [Head] 1 : f [Cat] : S ^ <ref> [1] </ref> : true ^ [2] : trueg: This rule states that the head verb "breaks" occurs as the value of the attribute "Head" in a structure which also requires a subject [1] and a direct object [2]. It actually is a logic formula, but with a new "inverse label" modal connective. <p> But the formula above serves to show the idea.) The feature structure in Figure 3 satisfies both these formulas. The reason it satisfies the lexical rule is that the label [2) occurring in the lexical entry for "breaks" extends to the label <ref> [2; 1] </ref> occurring in the feature structure. Similarly (2; 1) in the unaccusative rule extends to [2; 1] in the feature structure. <p> The reason it satisfies the lexical rule is that the label [2) occurring in the lexical entry for "breaks" extends to the label <ref> [2; 1] </ref> occurring in the feature structure. Similarly (2; 1) in the unaccusative rule extends to [2; 1] in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v [2; 1; 0) and [2; 1], but [2; 1] <p> Similarly (2; 1) in the unaccusative rule extends to [2; 1] in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example <ref> [2; 1) v [2; 1; 0) and [2; 1] </ref>, but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v <ref> [2; 1; 0) and [2; 1] </ref>, but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> unaccusative rule extends to <ref> [2; 1] </ref> in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v [2; 1; 0) and [2; 1], but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> to <ref> [2; 1] </ref> in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v [2; 1; 0) and [2; 1], but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> It might be argued that the unaccusative rule is superfluous, since it is logically implied by the right side of the lexical rule. But the lexical rule does not justify the occurrence of <ref> [2; 1] </ref>. It only justifies the occurrence of a [2]. The unaccusative rule does justify this occurrence. Here is another example showing the Dative construction. Consider the sentence Joe gave Mary tea. This appears in Figure 4. <p> I will not attempt to give all the rules involved in this structure. One other aspect of it needs to be pointed out: the occurrence of the "0" sign in the feature labels like <ref> [1; 8; 0] </ref> and [0; M arked]. "0" is a special "null" relational sign. When it occurrs at the end of a sequence, it means that the value of that attribute plays no role in the surface form of the sentence.
Reference: [2] <author> P. Aczel. </author> <title> Non-Well-Founded Sets. Center for Study of Language and Information 14, </title> <year> 1988. </year>
Reference-contexts: Call these feature names "R-signs" for the moment. Then, the labels on feature arcs in SFG are sequences of R-signs, standing for the role that a given constituent may have played at different strata. For example, <ref> [2; 1] </ref> : glass tells us that the word "glass" is initially a direct object (2) and finally a subject (1). The label [2; 1] is not a path, but a single member of the label alphabet. Persistence in feature logics is in fact a persistent problem for linguistic analyses. <p> For example, <ref> [2; 1] </ref> : glass tells us that the word "glass" is initially a direct object (2) and finally a subject (1). The label [2; 1] is not a path, but a single member of the label alphabet. Persistence in feature logics is in fact a persistent problem for linguistic analyses. <p> Examples. Consider the sentence Joe breaks the glass. This is diagrammed as a feature structure in Figure 2. We present an (oversimplified) SFG lexical rule, stated in logical format, which accounts for the sentence: breaks ! [Head] 1 : f [Cat] : S ^ [1] : true ^ <ref> [2] </ref> : trueg: This rule states that the head verb "breaks" occurs as the value of the attribute "Head" in a structure which also requires a subject [1] and a direct object [2]. It actually is a logic formula, but with a new "inverse label" modal connective. <p> accounts for the sentence: breaks ! [Head] 1 : f [Cat] : S ^ [1] : true ^ <ref> [2] </ref> : trueg: This rule states that the head verb "breaks" occurs as the value of the attribute "Head" in a structure which also requires a subject [1] and a direct object [2]. It actually is a logic formula, but with a new "inverse label" modal connective. <p> But the formula above serves to show the idea.) The feature structure in Figure 3 satisfies both these formulas. The reason it satisfies the lexical rule is that the label [2) occurring in the lexical entry for "breaks" extends to the label <ref> [2; 1] </ref> occurring in the feature structure. Similarly (2; 1) in the unaccusative rule extends to [2; 1] in the feature structure. <p> The reason it satisfies the lexical rule is that the label [2) occurring in the lexical entry for "breaks" extends to the label <ref> [2; 1] </ref> occurring in the feature structure. Similarly (2; 1) in the unaccusative rule extends to [2; 1] in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v [2; 1; 0) and [2; 1], but [2; 1] <p> Similarly (2; 1) in the unaccusative rule extends to [2; 1] in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example <ref> [2; 1) v [2; 1; 0) and [2; 1] </ref>, but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v <ref> [2; 1; 0) and [2; 1] </ref>, but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> unaccusative rule extends to <ref> [2; 1] </ref> in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v [2; 1; 0) and [2; 1], but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> to <ref> [2; 1] </ref> in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v [2; 1; 0) and [2; 1], but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> It might be argued that the unaccusative rule is superfluous, since it is logically implied by the right side of the lexical rule. But the lexical rule does not justify the occurrence of <ref> [2; 1] </ref>. It only justifies the occurrence of a [2]. The unaccusative rule does justify this occurrence. Here is another example showing the Dative construction. Consider the sentence Joe gave Mary tea. This appears in Figure 4. <p> It might be argued that the unaccusative rule is superfluous, since it is logically implied by the right side of the lexical rule. But the lexical rule does not justify the occurrence of [2; 1]. It only justifies the occurrence of a <ref> [2] </ref>. The unaccusative rule does justify this occurrence. Here is another example showing the Dative construction. Consider the sentence Joe gave Mary tea. This appears in Figure 4.
Reference: [3] <author> H. Ait-Kaci. </author> <title> A Lattice-Theoretic Approach to Computation based on a Calculus of Partially Ordered Type Structures. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <year> 1984. </year>
Reference-contexts: The formula is expected given that the distributive law holds in the logic of the domain of feature structures. But in fact the construction is general. As an aside, exactly these same methods were first used by Ait-Kaci <ref> [3] </ref> to pass from conjunctive types to disjunctive ones. Ait-Kaci's syntax for conjunctive types is called the calculus of -terms. In effect these are identifiable with, not so much conjunctive Kasper-Rounds formulae, but conjunctive Kasper-Rounds formulae without path equations and with nominals.
Reference: [4] <author> H. Ait-Kaci and A. </author> <title> Podelski. Is there a meaning to life? In Proceedings of the International Conference on Logic Programming, </title> <year> 1990. </year>
Reference-contexts: They form the basic data types in Ait-Kaci's logic programming language LIFE <ref> [4] </ref>.) One can pass to disjunctive types (*-terms) by introducing a connective for disjunctions and proceeding to give equational rules as suggested by the logic of feature structures. In effect we have now constructed the set of compact elements of the Smyth pow-erdomain of the domain of feature structures.
Reference: [5] <author> F. Baader, H. J. Burckert, B. Nebel, W. Nutt, and G. Smolka. </author> <title> On the expressivity of feature logics with negation, functional uncertainty, and sort equations. </title> <type> Technical Report RR-91-01, </type> <institution> DFKI, </institution> <year> 1991. </year>
Reference: [6] <author> R. Backofen. </author> <title> On the decidability of functional uncertainty. </title> <booktitle> In Proceedings of the 31st Annual meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 201-208, </pages> <year> 1993. </year>
Reference-contexts: and subsumption constraints, of the form xp v yq, where x and y are variables and p and q are paths, have an undecidable satisfiability problem (for the definition of v, see def. 2.4.) A recent difficult positive decidability result on so-called functional uncertainty constraints has been shown by Backofen <ref> [6] </ref>. The form of such constraints is xff = y, where ff is a regular expression over the feature alphabet L. The interpretation in a feature system is that some path given by the expression leads from the element denoted by x to that denoted by y.
Reference: [7] <author> M. Ben-Ari, J. Halpern, and A. Pnueli. </author> <title> Deterministic propositional dynamic logic: finite models, complexity, and completeness. </title> <journal> Journal of Computer and System Science, </journal> <volume> 25 </volume> <pages> 402-417, </pages> <year> 1982. </year>
Reference: [8] <author> P. Blackburn. </author> <title> Modal logic and attribute value structures. </title> <editor> In M. de Rijke, editor, Diamonds and defaults. </editor> <publisher> Kluwer, </publisher> <year> 1993. </year>
Reference-contexts: I will not attempt to give all the rules involved in this structure. One other aspect of it needs to be pointed out: the occurrence of the "0" sign in the feature labels like <ref> [1; 8; 0] </ref> and [0; M arked]. "0" is a special "null" relational sign. When it occurrs at the end of a sequence, it means that the value of that attribute plays no role in the surface form of the sentence.
Reference: [9] <author> P. Blackburn and E. Spaan. </author> <title> A modal perspective on the computational complexity of attribute value grammar. </title> <journal> Journal of Logic, Language, and Information, </journal> <volume> 2 </volume> <pages> 129-169, </pages> <year> 1993. </year>
Reference-contexts: When we move to more complicated versions of feature logic, things get more interesting. Blackburn and Spaan <ref> [9] </ref> have studied what happens when nominals are allowed, when one considers master modalities, and universal modalities. Since these modal logics admit classical negation, complexity results for validity are also not hard to derive.
Reference: [10] <author> A. Blass and Y. Gurevich. </author> <title> Existential fixed-point logic. </title> <editor> In E. Borger, editor, </editor> <booktitle> Computation Theory and Logic, </booktitle> <pages> pages 20-36. </pages> <publisher> Springer-Verlag Lecture Notes 270, </publisher> <year> 1987. </year>
Reference: [11] <author> G. Bouma. </author> <title> Feature structures and nonmonotonicity. </title> <journal> Computational Linguistics, </journal> <volume> 18 </volume> <pages> 183-203, </pages> <year> 1992. </year>
Reference-contexts: Johnson shows in [37] how some of the non-monotonic phenomena in LFG can be modeled using first-order circumscription [45]. Some results on unification of nonmonotonic structures can be found in [75], which also refers to other work like that of <ref> [11] </ref>. Evans gives an extensive proposal for GPSG in [20].
Reference: [12] <author> Ronald J. Brachman and J. Schmolze. </author> <title> An overview of the kl-one knowledge representation system. </title> <journal> Cognitive Science, </journal> <volume> 9(2) </volume> <pages> 171-216, </pages> <year> 1985. </year>
Reference: [13] <author> L. Cardelli and P. Wegner. </author> <title> On understanding types, data abstractions, and polymorphism. </title> <journal> Computing Surveys, </journal> <volume> 17(4) </volume> <pages> 471-522, </pages> <year> 1985. </year>
Reference: [14] <author> B. Carpenter. </author> <title> The Logic of Typed Feature Structures. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: This is not the case for A, as the elements d 0 and d 1 are distinct but indistinguishable. (We do not have space in the chapter to pursue the interesting topic of extensionality. Some discussion appears in Carpenter <ref> [14, chapter 8] </ref>.) The collection of isomorphism classes of feature structures is thus partially ordered by v. In fact a better representation is available. Moshier, in his thesis [47], shows that this collection is order-isomorphic to the partially ordered set of abstract feature structures.
Reference: [15] <author> N. Chomsky. </author> <title> Aspects of the Theory of Syntax. </title> <publisher> MIT Press, </publisher> <year> 1965. </year>
Reference: [16] <author> N. Chomsky and M. Halle. </author> <title> The Sound Pattern of English. </title> <publisher> Harper and Row, </publisher> <year> 1957. </year>
Reference: [17] <author> A. Dawar and K. Vijay-Shanker. </author> <title> A three-valued interpretation of negation in feature structure descriptions. </title> <booktitle> In Proceedings of 27th Annual meeting of the Association for Computational Linguistics, </booktitle> <year> 1991. </year>
Reference: [18] <author> R. Dionne, E. Mays, and F. J. Oles. </author> <title> A non well founded approach to terminological cycles. </title> <booktitle> In Proceedings of Tenth National Conference on Artificial Intelligence: AAAI 92, </booktitle> <pages> pages 761-766, </pages> <year> 1992. </year>
Reference-contexts: With regard to (i), we have already alluded to the work of Nebel and Smolka [53]. Work more in the spirit of domain theory, attempting to model subsumption in terminological systems, has been carried out by Dionne, Mays, and Oles <ref> [18] </ref>. Another extension of feature theory which is needed for linguistic applications is to give a treatment of non-monotonic phenomena. We have mentioned feature specification defaults in GPSG, for example. Johnson shows in [37] how some of the non-monotonic phenomena in LFG can be modeled using first-order circumscription [45].
Reference: [19] <author> J. Dorre and W. </author> <title> Rounds. On subsumption and semiunification in feature algebras. </title> <journal> J. Symbolic Computation, </journal> <volume> 13 </volume> <pages> 441-461, </pages> <year> 1992. </year>
Reference-contexts: Of course a strictly positive formula, with no assumptions on sorts, can always be satisfied in a one-element system. Also, with negation, no sort symbols are necessary. The above proof is an adaptation of one by Dorre and Rounds <ref> [19] </ref>, showing the unsolvability of subsumption constraints, a concept of Smolka which we will cover Section 5 Feature Logics 41 later in the section. The technique also leads to a proof, by further reductions, to a proof of the unsolvability of the semi-unification problem for rational terms. <p> This result is very much analogous to the undecidability result of Blackburn and Spaan for the universal modality. The technique was in fact shown in Theorem 5.1. This theme is continued in <ref> [19] </ref>.
Reference: [20] <author> R. Evans. </author> <title> Towards a formal specification for defaults in gpsg. </title> <editor> In E. Klein and J. van Benthem, editors, </editor> <title> Categories, Polymorphism, and Unification. </title> <institution> Centre for Cognitive Science, Edinburgh, </institution> <year> 1988. </year>
Reference-contexts: Johnson shows in [37] how some of the non-monotonic phenomena in LFG can be modeled using first-order circumscription [45]. Some results on unification of nonmonotonic structures can be found in [75], which also refers to other work like that of [11]. Evans gives an extensive proposal for GPSG in <ref> [20] </ref>.
Reference: [21] <author> M. J. Fischer and R. E. Ladner. </author> <title> Propositional dynamic logic of regular programs. </title> <journal> Journal of Computer and System Science, </journal> <volume> 18(2) </volume> <pages> 194-211, </pages> <year> 1979. </year>
Reference-contexts: The refinement of the argument to get a deterministic exponential procedure is more complex, and we omit the details here. Finally, the proof that the satisfiability problem is hard for exponential time requires an embedding of the corresponding problem for propositional dynamic logic <ref> [21] </ref>. Details are again omitted. The questions of satisfiability in a finite model for KR with path equations and the universal modality seems not to have been studied. Notice that the model constructed in the proof above is infinite.
Reference: [22] <author> G. Gazdar, E. Klein, G. Pullum, and I. Sag. </author> <title> Generalized Phrase Structure Grammar. </title> <publisher> Har-vard university Press, </publisher> <year> 1985. </year>
Reference: [23] <author> G. Gazdar, G. Pullum, R. Carpenter, E. Klein, T. Hukari, and T. Levine. </author> <title> Category structures. </title> <journal> Computational Linguistics, </journal> <volume> 14 </volume> <pages> 1-19, </pages> <year> 1988. </year>
Reference: [24] <author> R. I. Goldblatt. </author> <title> Varieties of complex algebras. </title> <journal> Annals of Pure and Applied Logic, </journal> <volume> 44 </volume> <pages> 173-242, </pages> <year> 1989. </year>
Reference: [25] <author> C. A. Gunter and D. S. Scott. </author> <title> Semantic domains. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume B: Formal Models and Semantics, chapter 12, </booktitle> <pages> pages 633-674. </pages> <publisher> Elsevier, </publisher> <year> 1990. </year> <note> 58 William C. Rounds Ch. 1 </note>
Reference-contexts: And one can use other orderings on sets of domain elements to get new powerdomains. Two other very common constructions, for example, are the Hoare (lower) and the Plotkin (convex) powerdomains. See Gunter and Scott <ref> [25] </ref> for a good survey; other references are Abramsky [1], or Zhang [76] for the details of passing to a logic from a domain. At this point, we have digressed pretty much from linguistics.
Reference: [26] <author> Y. Gurevich. </author> <title> The word problem for certain classes of semigroups. </title> <journal> Algebra and Logic, </journal> <volume> 5 </volume> <pages> 25-35, </pages> <year> 1966. </year>
Reference-contexts: Gurevich has shown that the word problem for finite semigroups is undecidable <ref> [26] </ref>; this result holds with jLj = 2. Theorem 5.1. Suppose jLj = 2. It is undecidable whether a formula ' in KR logic augmented by path equations and the universal modality is satisfiable in a finite feature system over L. Proof.
Reference: [27] <author> L. Haegeman. </author> <title> Introduction to government and binding theory. </title> <publisher> Basil Blackwell, </publisher> <year> 1991. </year>
Reference: [28] <author> D. Harel. </author> <title> Dynamic logic. </title> <editor> In D. Gabbay and F. Guenthner, editors, </editor> <booktitle> Handbook of Philosophical Logic, </booktitle> <volume> vol. II. </volume> <publisher> Reidel, </publisher> <year> 1984. </year>
Reference-contexts: The proof of the latter result is not too difficult. Blackburn and Spaan use a tiling problem <ref> [28] </ref> known to be hard for 0 1 [64]. Here I will prove undecidability of a slightly stronger result, using a technique of Smolka [70]. This involves the idea of word problems for Thue systems. We need some preliminary definitions on such word problems. Given a finite 40 William C.
Reference: [29] <author> M. Hennessy and R. Milner. </author> <title> Algebraic laws for nondeterminism and concurrency. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 32(1) </volume> <pages> 137-161, </pages> <year> 1985. </year>
Reference: [30] <author> M. Hohfeld and G. Smolka. </author> <title> Definite relations over constraint languages. </title> <type> Technical Report LILOG-REPORT 53, </type> <institution> IBM Deutschland, Stuttgart, </institution> <year> 1988. </year>
Reference: [31] <author> N. Immerman. </author> <title> Relational queries computable in polynomial time. </title> <booktitle> In Proceedings of the 14th ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 147-152, </pages> <year> 1982. </year>
Reference: [32] <author> D. Johnson, A. Meyers, and L. Moss. </author> <title> A unification-based parser for relational grammar. </title> <booktitle> In Proc. 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 97-104, </pages> <year> 1993. </year>
Reference-contexts: It also shows how it might be possible to use some of the computational algorithms available in unification-based systems to actually parse in a multistratal style. And in fact, such a parser has been implemented <ref> [32] </ref>. What are the essential features of stratified feature grammars? There are two main innovations. To be able to capture the multistratal point of view, Johnson and Moss introduce structure on the label alphabet L.
Reference: [33] <author> D. Johnson and L. Moss. </author> <title> Some formal properties of stratified feature grammars. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> 8, </volume> <year> 1993. </year>
Reference-contexts: Rounds Ch. 1 We will not dwell in this short exposition on the details of relational diagrams, or on the graphical versions formalized in arc-pair grammar [34] or multigraph grammar [59]. Instead we will illustrate a current version of the theory, due to Johnson and Moss <ref> [33] </ref>. Stratified feature grammar is a representation of multistratal grammar which integrates feature logic with the multistratal point of view. Feature logic is useful here because it allows for the natural expression of constructions working across strata, without having to adapt first-order logic for this purpose.
Reference: [34] <author> D. E. Johnson and P. M. </author> <title> Postal. Arc Pair Grammar. </title> <publisher> Princeton University Press, </publisher> <year> 1980. </year>
Reference-contexts: Rounds Ch. 1 We will not dwell in this short exposition on the details of relational diagrams, or on the graphical versions formalized in arc-pair grammar <ref> [34] </ref> or multigraph grammar [59]. Instead we will illustrate a current version of the theory, due to Johnson and Moss [33]. Stratified feature grammar is a representation of multistratal grammar which integrates feature logic with the multistratal point of view.
Reference: [35] <author> M. Johnson. </author> <title> Attribute-Value Logic and the Theory of Grammar. Center for Study of Language and Information, </title> <year> 1988. </year>
Reference-contexts: Thus such a deduction calculus need not be entirely self-contained. But it may be useful for deriving some of the other standard metamathematical properties of constraint theories such as compactness, decidability, or complexity. Johnson's book <ref> [35] </ref> uses similar techniques for his attribute-value system. We give an example of his techniques below, in the section on complexity issues. John-son addresses the issues of compactness as well in his book. <p> Ait-Kaci uses domain-theoretic methods to construct disjunctive types (*-terms) from conjunctive types ( -terms.) Pereira and Shieber suggest that types might be modeled by a so-called powerdomain construction. Johnson <ref> [35] </ref> also gives an interesting discussion on partiality issues for feature structures. His view, as we have mentioned, is that feature logic, without loss, can restrict itself to talking about total models. But the comparisons he makes between the two views are informative.
Reference: [36] <author> M. Johnson. </author> <title> Features and formulae. </title> <journal> Computational Linguistics, </journal> <volume> 17 </volume> <pages> 131-152, </pages> <year> 1991. </year>
Reference-contexts: The interpretation in a feature system is that some path given by the expression leads from the element denoted by x to that denoted by y. As a final excursion into complexity properties, we consider a method due to Johnson [37], <ref> [36] </ref> This relies on a "correspondence" theory between feature constraints and first order logic. The idea is to express natural feature constraints on first-order terms, but to rely on general decidability results for first order systems to derive them for feature constraint systems.
Reference: [37] <author> M. Johnson. </author> <title> Logic and feature structures. </title> <booktitle> In Proceedings of IJCAI 91, </booktitle> <pages> pages 992-996, </pages> <year> 1991. </year>
Reference-contexts: The interpretation in a feature system is that some path given by the expression leads from the element denoted by x to that denoted by y. As a final excursion into complexity properties, we consider a method due to Johnson <ref> [37] </ref>, [36] This relies on a "correspondence" theory between feature constraints and first order logic. The idea is to express natural feature constraints on first-order terms, but to rely on general decidability results for first order systems to derive them for feature constraint systems. The observation in [37] is that feature <p> due to Johnson <ref> [37] </ref>, [36] This relies on a "correspondence" theory between feature constraints and first order logic. The idea is to express natural feature constraints on first-order terms, but to rely on general decidability results for first order systems to derive them for feature constraint systems. The observation in [37] is that feature constraints often translate into a special decidable subclass of first-order formulas: the Schonfinkel-Bernays class SB. <p> Another extension of feature theory which is needed for linguistic applications is to give a treatment of non-monotonic phenomena. We have mentioned feature specification defaults in GPSG, for example. Johnson shows in <ref> [37] </ref> how some of the non-monotonic phenomena in LFG can be modeled using first-order circumscription [45]. Some results on unification of nonmonotonic structures can be found in [75], which also refers to other work like that of [11]. Evans gives an extensive proposal for GPSG in [20].
Reference: [38] <author> A. K. Joshi and Yves Schabes. </author> <title> Tree adjoining grammars and lexicalized grammars. In Tree Automata and Languages. </title> <publisher> Elsevier, </publisher> <year> 1992. </year>
Reference: [39] <author> R. Kaplan and J. Bresnan. </author> <title> Lexical-functional grammar: A formal system for grammatical representations. </title> <editor> In J. Bresnan, editor, </editor> <booktitle> The Mental Representation of Grammatical Relations, </booktitle> <pages> pages 173-281. </pages> <publisher> MIT Press, </publisher> <year> 1982. </year>
Reference: [40] <author> R. Kasper and W. </author> <title> Rounds. A logical semantics for feature structures. </title> <booktitle> In Proc. of 24th Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 257-266, </pages> <year> 1986. </year>
Reference: [41] <author> R. Kasper and W. </author> <title> Rounds. The logic of unification in grammar. </title> <journal> Linguistics and Philosophy, </journal> <volume> 13 </volume> <pages> 33-58, </pages> <year> 1990. </year>
Reference: [42] <author> M. Kay. </author> <title> Functional grammar. </title> <editor> In C. Chiarello, editor, </editor> <booktitle> Proceedings of the Fifth Annual Meeting of the Berkeley Linguistics Society, </booktitle> <pages> pages 142-158, </pages> <year> 1979. </year>
Reference: [43] <author> M. Kracht. </author> <title> On the logic of category definitions. </title> <journal> Computational Linguistics, </journal> <volume> 15 </volume> <pages> 111-113, </pages> <year> 1989. </year>
Reference-contexts: One thus gets a heterogeneous axiom system involving features and other classes of models (sets are just one example.) As we have mentioned above, Kracht <ref> [43] </ref> relates feature logics to other systems of modal logic, and adduces complete axiom systems as a result. 5. Complexity and decidability issues The natural questions to answer here have to do with the satisfiabiity and validity problems for various feature logics.
Reference: [44] <author> H. Lewis. </author> <title> Complexity results for classes of quantificational formulae. </title> <journal> Journal of Computer and System Science, </journal> <volume> 21 </volume> <pages> 317-353, </pages> <year> 1980. </year>
Reference-contexts: As to the complexity of deciding this problem, if the number of universal quantifiers is fixed, then the problem is NP-complete. The problem is in general complete for polynomial space if the number of such quantifiers is not fixed <ref> [44] </ref>. Johnson notes, however, that most of the linguistic uses require only fixed numbers of the quantifiers. This method thus does not lead to more complexity than in the original KR logic. 6.
Reference: [45] <author> J. McCarthy. </author> <title> Circumscription a form of non-monotonic reasoning. </title> <editor> In M. Ginsberg, editor, </editor> <booktitle> Readings in Nonmonotonic Reasoning, </booktitle> <pages> pages 145-152. </pages> <publisher> Morgan Kauffman, </publisher> <year> 1987. </year>
Reference-contexts: Another extension of feature theory which is needed for linguistic applications is to give a treatment of non-monotonic phenomena. We have mentioned feature specification defaults in GPSG, for example. Johnson shows in [37] how some of the non-monotonic phenomena in LFG can be modeled using first-order circumscription <ref> [45] </ref>. Some results on unification of nonmonotonic structures can be found in [75], which also refers to other work like that of [11]. Evans gives an extensive proposal for GPSG in [20].
Reference: [46] <author> R. Montague. </author> <title> The proper treatment of quantification in ordinary english. </title> <editor> In R. Thoma-son, editor, </editor> <booktitle> Formal Philosophy: Selected Writings of Richard Montague, </booktitle> <pages> pages 247-270. </pages> <publisher> Yale University Press, </publisher> <year> 1974. </year>
Reference: [47] <author> M. A. Moshier. </author> <title> Extensions to Unification Grammar for the Description of Programming Languages. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1988. </year>
Reference-contexts: Some discussion appears in Carpenter [14, chapter 8].) The collection of isomorphism classes of feature structures is thus partially ordered by v. In fact a better representation is available. Moshier, in his thesis <ref> [47] </ref>, shows that this collection is order-isomorphic to the partially ordered set of abstract feature structures.
Reference: [48] <author> M. A. Moshier. </author> <title> Completeness theorems for logics of feature structures. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> 8, </volume> <year> 1993. </year>
Reference: [49] <author> M. A. Moshier and W. </author> <title> Rounds. A logic for partially specified data structures. </title> <booktitle> In Proceedings of 14th ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1987. </year>
Reference: [50] <author> L. Moss. </author> <title> Completeness theorems for logics of feature structures. </title> <editor> In Y. Moschovakis, editor, </editor> <booktitle> Proceedings af MSRI Workshop on Logic from Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [51] <author> K. Mukai. </author> <title> A system of logic programming for linguistic analysis. </title> <type> Technical Report TR-540, </type> <institution> ICOT, </institution> <address> Tokyo, </address> <year> 1990. </year>
Reference: [52] <author> K. Mukai. Clp(afa): </author> <title> Coinductive semantics of horn clauses with compact constraints. </title> <editor> In J. Gawron, G. Plotkin, and S. Tutiya, editors, </editor> <title> Situation Theory and its Applications, vol. 2, Section References Feature Logics 59 pages 179-214. Center for Study of Language and Information, </title> <year> 1991. </year>
Reference: [53] <author> B. Nebel and G. Smolka. </author> <title> Representation and reasoning with attributive descriptions. </title> <editor> In K. H. Blasius, U. Hedstuck, and C-R. Rollinger, editors, </editor> <booktitle> Sorts and Types in Artificial Intelligence, </booktitle> <pages> pages 112-139. </pages> <booktitle> Springer-Verlag Lecture notes in Artificial Intelligence 418, </booktitle> <year> 1989. </year>
Reference-contexts: I will not attempt here to recount specific work on the subject. With regard to (i), we have already alluded to the work of Nebel and Smolka <ref> [53] </ref>. Work more in the spirit of domain theory, attempting to model subsumption in terminological systems, has been carried out by Dionne, Mays, and Oles [18]. Another extension of feature theory which is needed for linguistic applications is to give a treatment of non-monotonic phenomena.
Reference: [54] <author> F. Pereira and S. Shieber. </author> <title> The semantics of grammar formalisms seen as computer languages. </title> <booktitle> In Proceedings of 10th International Conference on Computational Linguistics: COLING 84, </booktitle> <year> 1984. </year>
Reference-contexts: This theory is generally about constructive information-theoretic ordering relations. Only a few of the results are needed for our applications. On the other hand, feature systems provide a very nice illustration of some of the domain-theoretic constructions. Historically, Pereira and Shieber <ref> [54] </ref> were the first to suggest using domain theory to give a semantics of grammars. Their work preceded the introduction of feature logic, and appeared at about the same time as Ait-Kaci's work on what might be called feature types.
Reference: [55] <author> F. Pereira and D. H. D. Warren. </author> <title> Definite clause grammars for language analysis: A survey of the fornmalism and a comparison with augmented transition networks. </title> <journal> Artificial Intelligence, </journal> <volume> 13 </volume> <pages> 231-278, </pages> <year> 1980. </year>
Reference: [56] <author> D. Perlmutter. </author> <title> Studies in Relational Grammar I. </title> <publisher> Univrersity of Chicago Press, </publisher> <year> 1983. </year>
Reference-contexts: This subject emerged in the 1970's as relational grammar, principally through the work of Postal and Perlmutter (see, for example, Perlmutter <ref> [56] </ref> or Postal and Joseph [60] for representative work.) Multistratal grammar harks back to the early days of transformational theory.
Reference: [57] <author> C. Pollard and M. A. Moshier. </author> <title> Unifying partial descriptions of sets. </title> <editor> In P. Hansen, editor, </editor> <booktitle> Vancouver Studies in Cognitive Science: </booktitle> <volume> vol. </volume> <editor> I. </editor> <publisher> University of British Columbia Press, </publisher> <year> 1990. </year>
Reference-contexts: At this point, we have digressed pretty much from linguistics. To return thereto, and to close out the subsection, we sketch the details of another powerdomain construction due to Pollard and Moshier <ref> [57] </ref>. It is one way in which set values might be handled order-theoretically. Consider the following sentence. He thinks he is smart. The sentence has a set of referents, which might occur as the value of a CONTEXT feature.
Reference: [58] <author> C. Pollard and I. Sag. </author> <title> Information-Based Syntax and Semantics: Volume I Fundamentals. </title> <booktitle> CSLI Lecture Notes 13, </booktitle> <publisher> Chicago University Press, </publisher> <year> 1987. </year>
Reference: [59] <author> P. M. </author> <title> Postal. Studies of Passive Clauses. </title> <institution> State University of New York Press, </institution> <year> 1986. </year>
Reference-contexts: Rounds Ch. 1 We will not dwell in this short exposition on the details of relational diagrams, or on the graphical versions formalized in arc-pair grammar [34] or multigraph grammar <ref> [59] </ref>. Instead we will illustrate a current version of the theory, due to Johnson and Moss [33]. Stratified feature grammar is a representation of multistratal grammar which integrates feature logic with the multistratal point of view.
Reference: [60] <author> P. M. Postal and B. Joseph, </author> <title> editors. Studies in Relational Grammar, III. </title> <publisher> University of Chicago Press, </publisher> <year> 1990. </year>
Reference-contexts: This subject emerged in the 1970's as relational grammar, principally through the work of Postal and Perlmutter (see, for example, Perlmutter [56] or Postal and Joseph <ref> [60] </ref> for representative work.) Multistratal grammar harks back to the early days of transformational theory. As we recall, the notion of a construction like "passive" in some intuitive sense identifies the surface subject John in the sentence John was hit by Bill with the "deep" or "initial" direct object.
Reference: [61] <author> V. Pratt. </author> <title> Semantical considerations on floyd-hoare logic. </title> <booktitle> In Proceedings of the 17th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1976. </year>
Reference: [62] <author> A. </author> <title> Prior. Past, Present, and Future. </title> <publisher> Oxford University Press, </publisher> <year> 1967. </year>
Reference: [63] <author> M. Reape. </author> <title> Introduction to Semantics of Unification-based Grammar Formalisms. </title> <publisher> Kluwer, </publisher> <year> 1994. </year>
Reference: [64] <author> R. Robinson. </author> <title> Undecidability and nonperiodicity for tilings of the plane. </title> <journal> Inventiones Math., </journal> <volume> 12 </volume> <pages> 177-209, </pages> <year> 1971. </year>
Reference-contexts: The proof of the latter result is not too difficult. Blackburn and Spaan use a tiling problem [28] known to be hard for 0 1 <ref> [64] </ref>. Here I will prove undecidability of a slightly stronger result, using a technique of Smolka [70]. This involves the idea of word problems for Thue systems. We need some preliminary definitions on such word problems. Given a finite 40 William C.
Reference: [65] <author> W. Rounds and A. Manaster Ramer. </author> <title> A logical version of functional grammar. </title> <booktitle> In Proceedings of the 25th Annual Conference of the Association for Computational Linguistics, </booktitle> <year> 1987. </year>
Reference: [66] <author> Dana S. Scott. </author> <title> Domains for denotational semantics. </title> <booktitle> In Lecture Notes in Computer Science 140, </booktitle> <year> 1982. </year>
Reference: [67] <author> S. Shieber. </author> <title> The design of a computer language for linguistic information. </title> <booktitle> In Proceedings of 12th COLING, </booktitle> <pages> pages 211-215, </pages> <year> 1986. </year>
Reference: [68] <author> S. Shieber. </author> <title> Parsing and Type Inference for Natural and Computer languages. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1989. </year>
Reference: [69] <author> S. Shieber. </author> <title> Constraint-Based Grammar Formalisms: Parsing and Type Inference for Natural and Computer Languages. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference: [70] <author> G. Smolka. </author> <title> Feature constraint logics for unification grammars. </title> <journal> Journal of Logic Programming, </journal> <volume> 12 </volume> <pages> 51-87, </pages> <year> 1992. </year>
Reference-contexts: Of course the term "feature logic" is ambiguous here, intentionally so. As an example, recall Smolka's constraint feature logic <ref> [70] </ref>. A feature constraint is one of the following: - xp = yq for variables x; y (path equation); - xp = a for atom a (path equation and constant); - xp " (divergence); Boolean combinations of constraints; - 9x (') where ' is a constraint. <p> The proof of the latter result is not too difficult. Blackburn and Spaan use a tiling problem [28] known to be hard for 0 1 [64]. Here I will prove undecidability of a slightly stronger result, using a technique of Smolka <ref> [70] </ref>. This involves the idea of word problems for Thue systems. We need some preliminary definitions on such word problems. Given a finite 40 William C. Rounds Ch. 1 alphabet L, consider the class of semigroups finitely generated by L.
Reference: [71] <author> Gaisi Takeuti. </author> <title> Proof Theory. </title> <publisher> North Holland, </publisher> <year> 1987. </year>
Reference: [72] <author> M. Vardi. </author> <title> The complexity of relational query languages. </title> <booktitle> In Proceedings of the 14th ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 137-146, </pages> <year> 1982. </year>
Reference: [73] <author> S. Vickers. </author> <title> Topology via Logic. </title> <publisher> Cambridge University Press, </publisher> <year> 1989. </year>
Reference-contexts: Such properties are sometimes called "affirmable" <ref> [73] </ref>. It is straightforward to prove the following in any algebraic cpo U .
Reference: [74] <author> K. Vijay-Shanker. </author> <title> A Study of tree-adjoining grammars. </title> <type> PhD thesis, </type> <institution> University of Pennsyl-vania, </institution> <year> 1987. </year>
Reference: [75] <author> M. Young. </author> <title> Non-monotonic sorts for feature structures. </title> <booktitle> In Proceedings of Tenth National Conference on Artificial Intelligence: AAAI 92, </booktitle> <pages> pages 596-601, </pages> <year> 1992. </year>
Reference-contexts: We have mentioned feature specification defaults in GPSG, for example. Johnson shows in [37] how some of the non-monotonic phenomena in LFG can be modeled using first-order circumscription [45]. Some results on unification of nonmonotonic structures can be found in <ref> [75] </ref>, which also refers to other work like that of [11]. Evans gives an extensive proposal for GPSG in [20].
Reference: [76] <author> Guo-Qiang Zhang. </author> <title> Logic of Domains. </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1991. </year> <title> 60 William C. </title> <journal> Rounds Ch. </journal> <volume> 1 2 6 4 AGR 1 " NUM sing PERS 3rd # SUBJ 1 3 7 5 2 6 4 </volume>
Reference-contexts: And one can use other orderings on sets of domain elements to get new powerdomains. Two other very common constructions, for example, are the Hoare (lower) and the Plotkin (convex) powerdomains. See Gunter and Scott [25] for a good survey; other references are Abramsky [1], or Zhang <ref> [76] </ref> for the details of passing to a logic from a domain. At this point, we have digressed pretty much from linguistics. To return thereto, and to close out the subsection, we sketch the details of another powerdomain construction due to Pollard and Moshier [57].
Reference: [Cat] <institution> S </institution>
Reference-contexts: Examples. Consider the sentence Joe breaks the glass. This is diagrammed as a feature structure in Figure 2. We present an (oversimplified) SFG lexical rule, stated in logical format, which accounts for the sentence: breaks ! [Head] 1 : f <ref> [Cat] </ref> : S ^ [1] : true ^ [2] : trueg: This rule states that the head verb "breaks" occurs as the value of the attribute "Head" in a structure which also requires a subject [1] and a direct object [2]. <p> Consider the "unac-cusative" sentence Glass breaks. This is diagrammed in Figure 3. In this case, we need to recognize that "break" is a verb that may have no explicit agent. So an additional entry might be breaks ! [Head] 1 : f <ref> [Cat] </ref> : S ^ :[1) : true ^ [2) : trueg: (The open and closed bracketing will be explained in a moment.) Next, there is a rule which embodies the unaccusative construction. <p> It only justifies the occurrence of a [2]. The unaccusative rule does justify this occurrence. Here is another example showing the Dative construction. Consider the sentence Joe gave Mary tea. This appears in Figure 4. An entry for "gave" might be gave ! [Head] 1 : f <ref> [Cat] </ref> : S ^ [1) : true ^ [2) : true ^ [3) : trueg: We also have a rule for Dative: (3) : true ^ (2) : true 7! (3; 2) : true ^ (2; 8) : true: This rule is an extension formula, and should not technically be read <p> To get the predicate-argument structure for "Mary was given tea by Joe" we delete from the structure in Fig. 5 all arcs beginning with a 0. In the remaining arcs, we remove every sign except the first. The result is the following. 2 6 6 6 6 6 <ref> [Cat] </ref> S [Head] was [Comp] 2 6 6 6 [Cat] V P [Head]given [3) M ary [1) J oe 7 7 7 5 7 7 7 7 7 5 Similarly, to get the surface structure, we remove all arcs ending in 0, and remove all signs in the remaining arcs except <p> In the remaining arcs, we remove every sign except the first. The result is the following. 2 6 6 6 6 6 <ref> [Cat] </ref> S [Head] was [Comp] 2 6 6 6 [Cat] V P [Head]given [3) M ary [1) J oe 7 7 7 5 7 7 7 7 7 5 Similarly, to get the surface structure, we remove all arcs ending in 0, and remove all signs in the remaining arcs except the last. <p> We obtain 2 6 6 6 6 6 6 6 4 (1] M ary [Head] was [Comp] 2 6 6 6 6 <ref> [Cat] </ref> V P [Head]given (8] tea 2 4 (F lag] by (M arked]Joe 3 5 7 7 7 7 5 7 7 7 7 7 7 7 7 (Notice that there are two (8]'s in the complement, so this last structure is actually a set-valued structure.) From the surface structure, it
Reference: [1] <institution> Joe [Head]breaks [2] the glass 3 7 5 2 4 </institution>
Reference-contexts: And one can use other orderings on sets of domain elements to get new powerdomains. Two other very common constructions, for example, are the Hoare (lower) and the Plotkin (convex) powerdomains. See Gunter and Scott [25] for a good survey; other references are Abramsky <ref> [1] </ref>, or Zhang [76] for the details of passing to a logic from a domain. At this point, we have digressed pretty much from linguistics. To return thereto, and to close out the subsection, we sketch the details of another powerdomain construction due to Pollard and Moshier [57]. <p> Call these feature names "R-signs" for the moment. Then, the labels on feature arcs in SFG are sequences of R-signs, standing for the role that a given constituent may have played at different strata. For example, <ref> [2; 1] </ref> : glass tells us that the word "glass" is initially a direct object (2) and finally a subject (1). The label [2; 1] is not a path, but a single member of the label alphabet. Persistence in feature logics is in fact a persistent problem for linguistic analyses. <p> For example, <ref> [2; 1] </ref> : glass tells us that the word "glass" is initially a direct object (2) and finally a subject (1). The label [2; 1] is not a path, but a single member of the label alphabet. Persistence in feature logics is in fact a persistent problem for linguistic analyses. <p> Examples. Consider the sentence Joe breaks the glass. This is diagrammed as a feature structure in Figure 2. We present an (oversimplified) SFG lexical rule, stated in logical format, which accounts for the sentence: breaks ! [Head] 1 : f [Cat] : S ^ <ref> [1] </ref> : true ^ [2] : trueg: This rule states that the head verb "breaks" occurs as the value of the attribute "Head" in a structure which also requires a subject [1] and a direct object [2]. <p> stated in logical format, which accounts for the sentence: breaks ! [Head] 1 : f [Cat] : S ^ <ref> [1] </ref> : true ^ [2] : trueg: This rule states that the head verb "breaks" occurs as the value of the attribute "Head" in a structure which also requires a subject [1] and a direct object [2]. It actually is a logic formula, but with a new "inverse label" modal connective. <p> But the formula above serves to show the idea.) The feature structure in Figure 3 satisfies both these formulas. The reason it satisfies the lexical rule is that the label [2) occurring in the lexical entry for "breaks" extends to the label <ref> [2; 1] </ref> occurring in the feature structure. Similarly (2; 1) in the unaccusative rule extends to [2; 1] in the feature structure. <p> The reason it satisfies the lexical rule is that the label [2) occurring in the lexical entry for "breaks" extends to the label <ref> [2; 1] </ref> occurring in the feature structure. Similarly (2; 1) in the unaccusative rule extends to [2; 1] in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v [2; 1; 0) and [2; 1], but [2; 1] <p> Similarly (2; 1) in the unaccusative rule extends to [2; 1] in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example <ref> [2; 1) v [2; 1; 0) and [2; 1] </ref>, but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v <ref> [2; 1; 0) and [2; 1] </ref>, but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> unaccusative rule extends to <ref> [2; 1] </ref> in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v [2; 1; 0) and [2; 1], but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> to <ref> [2; 1] </ref> in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v [2; 1; 0) and [2; 1], but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> It might be argued that the unaccusative rule is superfluous, since it is logically implied by the right side of the lexical rule. But the lexical rule does not justify the occurrence of <ref> [2; 1] </ref>. It only justifies the occurrence of a [2]. The unaccusative rule does justify this occurrence. Here is another example showing the Dative construction. Consider the sentence Joe gave Mary tea. This appears in Figure 4. <p> I will not attempt to give all the rules involved in this structure. One other aspect of it needs to be pointed out: the occurrence of the "0" sign in the feature labels like <ref> [1; 8; 0] </ref> and [0; M arked]. "0" is a special "null" relational sign. When it occurrs at the end of a sequence, it means that the value of that attribute plays no role in the surface form of the sentence.
Reference: [Cat] <institution> S [2; 1] glass [Head]breaks 3 5 2 6 6 6 </institution>
Reference-contexts: Examples. Consider the sentence Joe breaks the glass. This is diagrammed as a feature structure in Figure 2. We present an (oversimplified) SFG lexical rule, stated in logical format, which accounts for the sentence: breaks ! [Head] 1 : f <ref> [Cat] </ref> : S ^ [1] : true ^ [2] : trueg: This rule states that the head verb "breaks" occurs as the value of the attribute "Head" in a structure which also requires a subject [1] and a direct object [2]. <p> Consider the "unac-cusative" sentence Glass breaks. This is diagrammed in Figure 3. In this case, we need to recognize that "break" is a verb that may have no explicit agent. So an additional entry might be breaks ! [Head] 1 : f <ref> [Cat] </ref> : S ^ :[1) : true ^ [2) : trueg: (The open and closed bracketing will be explained in a moment.) Next, there is a rule which embodies the unaccusative construction. <p> It only justifies the occurrence of a [2]. The unaccusative rule does justify this occurrence. Here is another example showing the Dative construction. Consider the sentence Joe gave Mary tea. This appears in Figure 4. An entry for "gave" might be gave ! [Head] 1 : f <ref> [Cat] </ref> : S ^ [1) : true ^ [2) : true ^ [3) : trueg: We also have a rule for Dative: (3) : true ^ (2) : true 7! (3; 2) : true ^ (2; 8) : true: This rule is an extension formula, and should not technically be read <p> To get the predicate-argument structure for "Mary was given tea by Joe" we delete from the structure in Fig. 5 all arcs beginning with a 0. In the remaining arcs, we remove every sign except the first. The result is the following. 2 6 6 6 6 6 <ref> [Cat] </ref> S [Head] was [Comp] 2 6 6 6 [Cat] V P [Head]given [3) M ary [1) J oe 7 7 7 5 7 7 7 7 7 5 Similarly, to get the surface structure, we remove all arcs ending in 0, and remove all signs in the remaining arcs except <p> In the remaining arcs, we remove every sign except the first. The result is the following. 2 6 6 6 6 6 <ref> [Cat] </ref> S [Head] was [Comp] 2 6 6 6 [Cat] V P [Head]given [3) M ary [1) J oe 7 7 7 5 7 7 7 7 7 5 Similarly, to get the surface structure, we remove all arcs ending in 0, and remove all signs in the remaining arcs except the last. <p> We obtain 2 6 6 6 6 6 6 6 4 (1] M ary [Head] was [Comp] 2 6 6 6 6 <ref> [Cat] </ref> V P [Head]given (8] tea 2 4 (F lag] by (M arked]Joe 3 5 7 7 7 7 5 7 7 7 7 7 7 7 7 (Notice that there are two (8]'s in the complement, so this last structure is actually a set-valued structure.) From the surface structure, it
Reference: [Cat] <institution> S </institution>
Reference-contexts: Examples. Consider the sentence Joe breaks the glass. This is diagrammed as a feature structure in Figure 2. We present an (oversimplified) SFG lexical rule, stated in logical format, which accounts for the sentence: breaks ! [Head] 1 : f <ref> [Cat] </ref> : S ^ [1] : true ^ [2] : trueg: This rule states that the head verb "breaks" occurs as the value of the attribute "Head" in a structure which also requires a subject [1] and a direct object [2]. <p> Consider the "unac-cusative" sentence Glass breaks. This is diagrammed in Figure 3. In this case, we need to recognize that "break" is a verb that may have no explicit agent. So an additional entry might be breaks ! [Head] 1 : f <ref> [Cat] </ref> : S ^ :[1) : true ^ [2) : trueg: (The open and closed bracketing will be explained in a moment.) Next, there is a rule which embodies the unaccusative construction. <p> It only justifies the occurrence of a [2]. The unaccusative rule does justify this occurrence. Here is another example showing the Dative construction. Consider the sentence Joe gave Mary tea. This appears in Figure 4. An entry for "gave" might be gave ! [Head] 1 : f <ref> [Cat] </ref> : S ^ [1) : true ^ [2) : true ^ [3) : trueg: We also have a rule for Dative: (3) : true ^ (2) : true 7! (3; 2) : true ^ (2; 8) : true: This rule is an extension formula, and should not technically be read <p> To get the predicate-argument structure for "Mary was given tea by Joe" we delete from the structure in Fig. 5 all arcs beginning with a 0. In the remaining arcs, we remove every sign except the first. The result is the following. 2 6 6 6 6 6 <ref> [Cat] </ref> S [Head] was [Comp] 2 6 6 6 [Cat] V P [Head]given [3) M ary [1) J oe 7 7 7 5 7 7 7 7 7 5 Similarly, to get the surface structure, we remove all arcs ending in 0, and remove all signs in the remaining arcs except <p> In the remaining arcs, we remove every sign except the first. The result is the following. 2 6 6 6 6 6 <ref> [Cat] </ref> S [Head] was [Comp] 2 6 6 6 [Cat] V P [Head]given [3) M ary [1) J oe 7 7 7 5 7 7 7 7 7 5 Similarly, to get the surface structure, we remove all arcs ending in 0, and remove all signs in the remaining arcs except the last. <p> We obtain 2 6 6 6 6 6 6 6 4 (1] M ary [Head] was [Comp] 2 6 6 6 6 <ref> [Cat] </ref> V P [Head]given (8] tea 2 4 (F lag] by (M arked]Joe 3 5 7 7 7 7 5 7 7 7 7 7 7 7 7 (Notice that there are two (8]'s in the complement, so this last structure is actually a set-valued structure.) From the surface structure, it
Reference: [1] <institution> Joe [Head]gave </institution>
Reference-contexts: And one can use other orderings on sets of domain elements to get new powerdomains. Two other very common constructions, for example, are the Hoare (lower) and the Plotkin (convex) powerdomains. See Gunter and Scott [25] for a good survey; other references are Abramsky <ref> [1] </ref>, or Zhang [76] for the details of passing to a logic from a domain. At this point, we have digressed pretty much from linguistics. To return thereto, and to close out the subsection, we sketch the details of another powerdomain construction due to Pollard and Moshier [57]. <p> Call these feature names "R-signs" for the moment. Then, the labels on feature arcs in SFG are sequences of R-signs, standing for the role that a given constituent may have played at different strata. For example, <ref> [2; 1] </ref> : glass tells us that the word "glass" is initially a direct object (2) and finally a subject (1). The label [2; 1] is not a path, but a single member of the label alphabet. Persistence in feature logics is in fact a persistent problem for linguistic analyses. <p> For example, <ref> [2; 1] </ref> : glass tells us that the word "glass" is initially a direct object (2) and finally a subject (1). The label [2; 1] is not a path, but a single member of the label alphabet. Persistence in feature logics is in fact a persistent problem for linguistic analyses. <p> Examples. Consider the sentence Joe breaks the glass. This is diagrammed as a feature structure in Figure 2. We present an (oversimplified) SFG lexical rule, stated in logical format, which accounts for the sentence: breaks ! [Head] 1 : f [Cat] : S ^ <ref> [1] </ref> : true ^ [2] : trueg: This rule states that the head verb "breaks" occurs as the value of the attribute "Head" in a structure which also requires a subject [1] and a direct object [2]. <p> stated in logical format, which accounts for the sentence: breaks ! [Head] 1 : f [Cat] : S ^ <ref> [1] </ref> : true ^ [2] : trueg: This rule states that the head verb "breaks" occurs as the value of the attribute "Head" in a structure which also requires a subject [1] and a direct object [2]. It actually is a logic formula, but with a new "inverse label" modal connective. <p> But the formula above serves to show the idea.) The feature structure in Figure 3 satisfies both these formulas. The reason it satisfies the lexical rule is that the label [2) occurring in the lexical entry for "breaks" extends to the label <ref> [2; 1] </ref> occurring in the feature structure. Similarly (2; 1) in the unaccusative rule extends to [2; 1] in the feature structure. <p> The reason it satisfies the lexical rule is that the label [2) occurring in the lexical entry for "breaks" extends to the label <ref> [2; 1] </ref> occurring in the feature structure. Similarly (2; 1) in the unaccusative rule extends to [2; 1] in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v [2; 1; 0) and [2; 1], but [2; 1] <p> Similarly (2; 1) in the unaccusative rule extends to [2; 1] in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example <ref> [2; 1) v [2; 1; 0) and [2; 1] </ref>, but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v <ref> [2; 1; 0) and [2; 1] </ref>, but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> unaccusative rule extends to <ref> [2; 1] </ref> in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v [2; 1; 0) and [2; 1], but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> to <ref> [2; 1] </ref> in the feature structure. This explains the open and closed parentheses enclosing sequences of features; there is a natural partial ordering v (not, by the way, a Scott ordering) of the stratified label set L. For example [2; 1) v [2; 1; 0) and [2; 1], but [2; 1] does not extend to anything but itself. The definition of satisfaction for such an ordered label set now reads d j= l : ' iff for some unique f with l v f , df j= '. <p> It might be argued that the unaccusative rule is superfluous, since it is logically implied by the right side of the lexical rule. But the lexical rule does not justify the occurrence of <ref> [2; 1] </ref>. It only justifies the occurrence of a [2]. The unaccusative rule does justify this occurrence. Here is another example showing the Dative construction. Consider the sentence Joe gave Mary tea. This appears in Figure 4. <p> I will not attempt to give all the rules involved in this structure. One other aspect of it needs to be pointed out: the occurrence of the "0" sign in the feature labels like <ref> [1; 8; 0] </ref> and [0; M arked]. "0" is a special "null" relational sign. When it occurrs at the end of a sequence, it means that the value of that attribute plays no role in the surface form of the sentence.
Reference: [3; 2] <institution> M ary [2; 8] tea 3 7 7 7 Section References Feature Logics 61 2 6 6 6 6 6 6 6 6 6 6 </institution>
Reference: [Cat] <institution> S </institution>
Reference-contexts: Examples. Consider the sentence Joe breaks the glass. This is diagrammed as a feature structure in Figure 2. We present an (oversimplified) SFG lexical rule, stated in logical format, which accounts for the sentence: breaks ! [Head] 1 : f <ref> [Cat] </ref> : S ^ [1] : true ^ [2] : trueg: This rule states that the head verb "breaks" occurs as the value of the attribute "Head" in a structure which also requires a subject [1] and a direct object [2]. <p> Consider the "unac-cusative" sentence Glass breaks. This is diagrammed in Figure 3. In this case, we need to recognize that "break" is a verb that may have no explicit agent. So an additional entry might be breaks ! [Head] 1 : f <ref> [Cat] </ref> : S ^ :[1) : true ^ [2) : trueg: (The open and closed bracketing will be explained in a moment.) Next, there is a rule which embodies the unaccusative construction. <p> It only justifies the occurrence of a [2]. The unaccusative rule does justify this occurrence. Here is another example showing the Dative construction. Consider the sentence Joe gave Mary tea. This appears in Figure 4. An entry for "gave" might be gave ! [Head] 1 : f <ref> [Cat] </ref> : S ^ [1) : true ^ [2) : true ^ [3) : trueg: We also have a rule for Dative: (3) : true ^ (2) : true 7! (3; 2) : true ^ (2; 8) : true: This rule is an extension formula, and should not technically be read <p> To get the predicate-argument structure for "Mary was given tea by Joe" we delete from the structure in Fig. 5 all arcs beginning with a 0. In the remaining arcs, we remove every sign except the first. The result is the following. 2 6 6 6 6 6 <ref> [Cat] </ref> S [Head] was [Comp] 2 6 6 6 [Cat] V P [Head]given [3) M ary [1) J oe 7 7 7 5 7 7 7 7 7 5 Similarly, to get the surface structure, we remove all arcs ending in 0, and remove all signs in the remaining arcs except <p> In the remaining arcs, we remove every sign except the first. The result is the following. 2 6 6 6 6 6 <ref> [Cat] </ref> S [Head] was [Comp] 2 6 6 6 [Cat] V P [Head]given [3) M ary [1) J oe 7 7 7 5 7 7 7 7 7 5 Similarly, to get the surface structure, we remove all arcs ending in 0, and remove all signs in the remaining arcs except the last. <p> We obtain 2 6 6 6 6 6 6 6 4 (1] M ary [Head] was [Comp] 2 6 6 6 6 <ref> [Cat] </ref> V P [Head]given (8] tea 2 4 (F lag] by (M arked]Joe 3 5 7 7 7 7 5 7 7 7 7 7 7 7 7 (Notice that there are two (8]'s in the complement, so this last structure is actually a set-valued structure.) From the surface structure, it
Reference: [0; 1] <institution> (x) M ary [Head] was [Comp] 2 6 6 6 6 6 6 4 </institution>
Reference: [Cat] <institution> V P [Head] given [3; 2; 1; 0](x) [2; 8] tea </institution>
Reference-contexts: Examples. Consider the sentence Joe breaks the glass. This is diagrammed as a feature structure in Figure 2. We present an (oversimplified) SFG lexical rule, stated in logical format, which accounts for the sentence: breaks ! [Head] 1 : f <ref> [Cat] </ref> : S ^ [1] : true ^ [2] : trueg: This rule states that the head verb "breaks" occurs as the value of the attribute "Head" in a structure which also requires a subject [1] and a direct object [2]. <p> Consider the "unac-cusative" sentence Glass breaks. This is diagrammed in Figure 3. In this case, we need to recognize that "break" is a verb that may have no explicit agent. So an additional entry might be breaks ! [Head] 1 : f <ref> [Cat] </ref> : S ^ :[1) : true ^ [2) : trueg: (The open and closed bracketing will be explained in a moment.) Next, there is a rule which embodies the unaccusative construction. <p> It only justifies the occurrence of a [2]. The unaccusative rule does justify this occurrence. Here is another example showing the Dative construction. Consider the sentence Joe gave Mary tea. This appears in Figure 4. An entry for "gave" might be gave ! [Head] 1 : f <ref> [Cat] </ref> : S ^ [1) : true ^ [2) : true ^ [3) : trueg: We also have a rule for Dative: (3) : true ^ (2) : true 7! (3; 2) : true ^ (2; 8) : true: This rule is an extension formula, and should not technically be read <p> To get the predicate-argument structure for "Mary was given tea by Joe" we delete from the structure in Fig. 5 all arcs beginning with a 0. In the remaining arcs, we remove every sign except the first. The result is the following. 2 6 6 6 6 6 <ref> [Cat] </ref> S [Head] was [Comp] 2 6 6 6 [Cat] V P [Head]given [3) M ary [1) J oe 7 7 7 5 7 7 7 7 7 5 Similarly, to get the surface structure, we remove all arcs ending in 0, and remove all signs in the remaining arcs except <p> In the remaining arcs, we remove every sign except the first. The result is the following. 2 6 6 6 6 6 <ref> [Cat] </ref> S [Head] was [Comp] 2 6 6 6 [Cat] V P [Head]given [3) M ary [1) J oe 7 7 7 5 7 7 7 7 7 5 Similarly, to get the surface structure, we remove all arcs ending in 0, and remove all signs in the remaining arcs except the last. <p> We obtain 2 6 6 6 6 6 6 6 4 (1] M ary [Head] was [Comp] 2 6 6 6 6 <ref> [Cat] </ref> V P [Head]given (8] tea 2 4 (F lag] by (M arked]Joe 3 5 7 7 7 7 5 7 7 7 7 7 7 7 7 (Notice that there are two (8]'s in the complement, so this last structure is actually a set-valued structure.) From the surface structure, it
Reference: [1; 8; 0] <institution> (y) 2 4 </institution>
Reference-contexts: I will not attempt to give all the rules involved in this structure. One other aspect of it needs to be pointed out: the occurrence of the "0" sign in the feature labels like <ref> [1; 8; 0] </ref> and [0; M arked]. "0" is a special "null" relational sign. When it occurrs at the end of a sequence, it means that the value of that attribute plays no role in the surface form of the sentence.
Reference: [0; Cat] <institution> P P [0; F lag] by [0; M arked](y) J oe 3 5 7 7 7 7 7 7 7 3 7 7 7 7 7 7 7 7 7 7 </institution>
References-found: 87

