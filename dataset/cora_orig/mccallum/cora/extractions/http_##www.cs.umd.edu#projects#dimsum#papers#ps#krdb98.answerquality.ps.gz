URL: http://www.cs.umd.edu/projects/dimsum/papers/ps/krdb98.answerquality.ps.gz
Refering-URL: http://www.cs.umd.edu/users/bthj/bthj/publ.html
Root-URL: 
Email: ugur@cs.umd.edu  bthj@cs.umd.edu  franklin@cs.umd.edu  giles@research.nj.nec.com  divesh@research.att.com  
Title: Evaluating Answer Quality/Efficiency Tradeoffs  
Author: Ugur Cetintemel Bjorn T. Jonsson Michael J. Franklin C. Lee Giles Divesh Srivastava 
Address: Labs-Research  
Affiliation: University of Maryland  University of Maryland  University of Maryland  NEC Research Ins. UMIACS  AT&T  
Abstract: For many emerging applications and environments, information systems designers and implementers must consider the tradeoffs between efficiency and the quality of query answers. This flexibility, while allowing numerous opportunities for optimization, complicates the development of various system components and their performance evaluation. In this short paper, we first outline the issues that arise in evaluating answer quality/efficiency tradeoffs. We then describe how we address these issues in two ongoing projects that adopt notions of answer quality from the field of Information Retrieval.
Abstract-found: 1
Intro-found: 1
Reference: [ABF + 97] <author> Laurent Amsaleg, Philippe Bonnet, Micha-el Franklin, Anthony Tomasic, and Tolga Urhan. </author> <title> Improving responsiveness for wide-area data access. </title> <journal> IEEE Data Engineering Bulletin, </journal> <year> 1997. </year>
Reference-contexts: In such environments, users are not likely to get the perfect answers to their queries within a reasonable time, if ever. For these reasons, many recent database-related projects have been exploring techniques for providing approximate or incomplete answers, e.g., <ref> [HHW97, FJS97, ABF + 97, BDF + 97, GM98] </ref>. Performance studies and benchmarks for database systems have traditionally focused on throughput, response time, or other metrics related to the efficiency of the query execution.
Reference: [BDF + 97] <author> Daniel Barbara, William DuMouchel, Christos Faloutsos, Peter J. Haas, Joseph Hellerstein, Yannis E. Ioannidis, H. V. Ja-gadish, Theodore Johnson, Raymond T. Ng, Viswanath Poosala, Kenneth A. Ross, and Kenneth C. Sevcik. </author> <title> The New Jersey data reduction report. </title> <journal> IEEE Data Engineering Bulletin, </journal> <volume> 20(4) </volume> <pages> 3-45, </pages> <year> 1997. </year>
Reference-contexts: In such environments, users are not likely to get the perfect answers to their queries within a reasonable time, if ever. For these reasons, many recent database-related projects have been exploring techniques for providing approximate or incomplete answers, e.g., <ref> [HHW97, FJS97, ABF + 97, BDF + 97, GM98] </ref>. Performance studies and benchmarks for database systems have traditionally focused on throughput, response time, or other metrics related to the efficiency of the query execution.
Reference: [Bro97] <author> Kurt P. Brown. </author> <title> DBGuide industrial presentation. </title> <booktitle> ACM SIGMOD Conference, </booktitle> <address> Tuc-son, AZ, USA, </address> <year> 1997. </year>
Reference-contexts: We believe that ranked retrieval is also a better query model for many emerging database applications, for exactly the same reasons as in the IR case. There has been some work related to this, for example the query interface of <ref> [Bro97] </ref>, where the system uses summary screens to lead the user through a query refinement dialogue, until the posed queries have an answer set that is easy to display in ranked order.
Reference: [CFG98] <author> Ugur Cetintemel, Michael Franklin, and C. Lee Giles. </author> <title> Constructing user profiles incrementally: A multi-modal approach. </title> <note> Submitted for Publication, </note> <year> 1998. </year>
Reference-contexts: Additionally, given the fact that the user interests are changing over time, the profiles must be updated accordingly to reflect up to date information needs. The need for scalable, efficient profile management was clearly demonstrated by experience with the SIFT system [YGM96]. In our work on user profiles <ref> [CFG98] </ref>, we have developed MM, an incremental algorithm for constructing and maintaining user profiles for filtering text-based data items. MM can be tuned to trade off effectiveness (i.e., accuracy of the filtered data items) and efficiency of profile management. <p> Additionally, we were able to achieve precision values with small profiles that were comparable to, or in some cases even better than those obtained with maximum-sized profiles. The details of the algorithm, experimental settings, and the results are discussed in <ref> [CFG98] </ref>. 5 Conclusions In summary, emerging environments and applications require that information systems implementers consider the tradeoffs between efficiency and answer quality. Making intelligent choices for such tradeoffs requires a rethinking of metrics and evaluation procedures for databases and other information systems.
Reference: [FJS97] <author> Christos Faloutsos, H. V. Jagadish, and Nikolaos Sidiropoulos. </author> <title> Recovering information from summary data. </title> <booktitle> In Proc. of the VLDB Conf., </booktitle> <address> Athens, Greece, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: In such environments, users are not likely to get the perfect answers to their queries within a reasonable time, if ever. For these reasons, many recent database-related projects have been exploring techniques for providing approximate or incomplete answers, e.g., <ref> [HHW97, FJS97, ABF + 97, BDF + 97, GM98] </ref>. Performance studies and benchmarks for database systems have traditionally focused on throughput, response time, or other metrics related to the efficiency of the query execution.
Reference: [GM98] <author> Phillip B. Gibbons and Yossi Matias. </author> <title> New sampling-based summary statistics for improving approximate query answers. </title> <booktitle> In Proc. ACM SIGMOD Conf., </booktitle> <address> Seattle, WA, USA, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: In such environments, users are not likely to get the perfect answers to their queries within a reasonable time, if ever. For these reasons, many recent database-related projects have been exploring techniques for providing approximate or incomplete answers, e.g., <ref> [HHW97, FJS97, ABF + 97, BDF + 97, GM98] </ref>. Performance studies and benchmarks for database systems have traditionally focused on throughput, response time, or other metrics related to the efficiency of the query execution.
Reference: [Har96] <author> Donna Harman. </author> <booktitle> Overview of the fourth Text REtrieval Conference (TREC-4). In Proc. of the Fourth Text REtrieval Conference (TREC-4), </booktitle> <institution> National Institute of Standards and Technology, Gaithersburg, MD, </institution> <year> 1996. </year>
Reference-contexts: This assumes that the user correctly and exactly knows what query to submit.) In the ranked retrieval model, precision and recall can be cleverly combined into a single metric called non-interpolated average precision <ref> [Har96] </ref>. This metric is particularly appropriate when the user requests for the top k relevant documents. In many IR scenarios, users can improve their precision-recall (and, hence, the non-interpolated average precision) by interacting with the system. <p> In order to compute these metrics, we need to know if an individual document is relevant to a query, and what all the relevant documents are. In the TREC series <ref> [Har96] </ref>, which serves as the major benchmarking activity for IR systems, relevance of documents to particular queries are determined by a board of human experts. <p> For this setup, there exists a list of relevant documents for each benchmark query. These lists allowed us to calculate a metric called non-interpolated average precision, which cleverly combines precision and recall into a single number <ref> [Har96] </ref>. Using this metric, we were able to gauge the potential degradation of the results in an objective manner.
Reference: [HHW97] <author> Joseph M. Hellerstein, Peter J. Haas, and Helen J. Wang. </author> <title> Online aggregation. </title> <booktitle> In Proc. ACM SIGMOD Conf., </booktitle> <address> Tucson, AZ, USA, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: In such environments, users are not likely to get the perfect answers to their queries within a reasonable time, if ever. For these reasons, many recent database-related projects have been exploring techniques for providing approximate or incomplete answers, e.g., <ref> [HHW97, FJS97, ABF + 97, BDF + 97, GM98] </ref>. Performance studies and benchmarks for database systems have traditionally focused on throughput, response time, or other metrics related to the efficiency of the query execution.
Reference: [JFS98] <author> Bjorn T. Jonsson, Michael Franklin, and Divesh Srivastava. </author> <title> Interaction of query evaluation and buffer management for information retrieval. </title> <booktitle> In Proc. ACM SIG-MOD Conf., </booktitle> <address> Seattle, WA, USA, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: U. Cetintemel, B.T. Jonsson, M.J. Franklin, C.L. Giles, and D. Srivastava Page2 impact on the quality of the results. Some such op-timizations are quite successful in that they maintain the quality of results while reducing the query response time significantly [TF95]. In <ref> [JFS98] </ref> we proposed a new class of unsafe optimizations that interact with the buffer manager. We proposed a buffer-aware extension to IR query evaluation algorithms, which dynamically changes the query evaluation strategy based on buffer contents. <p> Details of the results, as well as the proposed buffer-aware extension, can be found in <ref> [JFS98] </ref>. We expect ranked retrieval for structured databases to be amenable to unsafe optimizations of the kind described above. The new wide-area environment, however, opens up yet another class of unsafe optimizations that apply at the network/server level.
Reference: [TF95] <author> Howard Turtle and James Flood. </author> <title> Query evaluation: Strategies and optimization. </title> <booktitle> Information Processing & Management, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: They simply make more sense with ranked retrieval. U. Cetintemel, B.T. Jonsson, M.J. Franklin, C.L. Giles, and D. Srivastava Page2 impact on the quality of the results. Some such op-timizations are quite successful in that they maintain the quality of results while reducing the query response time significantly <ref> [TF95] </ref>. In [JFS98] we proposed a new class of unsafe optimizations that interact with the buffer manager. We proposed a buffer-aware extension to IR query evaluation algorithms, which dynamically changes the query evaluation strategy based on buffer contents.
Reference: [Tur94] <author> Howard Turtle. </author> <title> Natural language vs. boolean query evaluation: A comparison of retrieval performance. </title> <booktitle> In Proc. ACM SIGIR Conf., </booktitle> <address> Dublin, Ireland, </address> <year> 1994. </year>
Reference-contexts: Likewise, existing database systems provide similar exact match query semantics (e.g., SQL). Current opinion in the IR community is that formulating effective boolean retrieval queries requires significant expertise <ref> [Tur94] </ref>. As in the IR case, the limitations of the exact match approach are quickly becoming apparent in emerging data retrieval applications and environments. In such environments queries are posed by hordes of novice users, who may have only a vague notion of what they are looking for. <p> Extensive studies have shown that the ranked retrieval model gives better results than the boolean model <ref> [Tur94] </ref>. We believe that ranked retrieval is also a better query model for many emerging database applications, for exactly the same reasons as in the IR case.
Reference: [Yah98] <author> Yahoo!. </author> <note> http://www.yahoo.com/, 1998. </note>
Reference-contexts: Therefore, it is more desirable to consider intermediate threshold values which will provide an optimal effectiveness/efficiency tradeoff for a given application. We evaluated the utility of MM by experimentally investigating its ability to categorize pages from the World Wide Web. More specifically, we used pages from Yahoo! <ref> [Yah98] </ref>. Yahoo! is a popular web guide that organizes web pages into a topic hierarchy formed by human editors. We used non-interpolated average precision as our primary effectiveness metric and focused on the profile size for quantifying the efficiency of our approach.
Reference: [YGM96] <author> Tak W. Yan and Hector Garcia-Molina. </author> <title> Efficient dissemination of information on the internet. </title> <journal> IEEE Data Engineering Bulletin, </journal> <volume> 19(3), </volume> <month> September </month> <year> 1996. </year> <editor> U. Cetintemel, B.T. Jonsson, M.J. Franklin, C.L. Giles, and D. </editor> <publisher> Srivastava Page4 </publisher>
Reference-contexts: Additionally, given the fact that the user interests are changing over time, the profiles must be updated accordingly to reflect up to date information needs. The need for scalable, efficient profile management was clearly demonstrated by experience with the SIFT system <ref> [YGM96] </ref>. In our work on user profiles [CFG98], we have developed MM, an incremental algorithm for constructing and maintaining user profiles for filtering text-based data items. MM can be tuned to trade off effectiveness (i.e., accuracy of the filtered data items) and efficiency of profile management.
References-found: 13

