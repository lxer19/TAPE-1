URL: http://www.eecs.umich.edu/~qstout/pap/DARPA92.ps.Z
Refering-URL: http://www.eecs.umich.edu/~qstout/abs/DARPA92.html
Root-URL: http://www.eecs.umich.edu
Email: qstout@eecs.umich.edu  
Title: Ultrafast Parallel Algorithms and Reconfigurable Meshes  
Author: Quentin F. Stout 
Address: Ann Arbor, MI 48109-2122 USA  
Affiliation: EECS Department University of Michigan  
Note: In Proc. DARPA Sortware Technology Conference 1992, pp. 184-188.  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P. Beame and J. Hastad, </author> <title> "Optimal bounds for decision problems on the CRCW PRAM", </title> <editor> J. </editor> <booktitle> ACM 36 (1989), </booktitle> <pages> pp. 643-670. </pages>
Reference-contexts: This result comes from a very general lower bound on the time required to determine parity <ref> [1] </ref>, and holds even if a polynomial number of processors are utilized. 2.1 Ultrafast Algorithms Our work has shown that lower bounds, such as that for parity, can sometimes be circumvented by combining randomization with the use of extra space.
Reference: [2] <editor> F.E. Fich, F.M. auf der Heide, P. Ragde, and A. </editor> <title> Wigderson, "One, two, three . . . infinity: Lower bounds for parallel computation", </title> <booktitle> Proc. 17 Symp. Theory Comput. </booktitle> <year> (1985), </year> <pages> pp. 48-58. </pages>
Reference-contexts: Earlier work [12] had shown that this could be determined in fi (log log n) time by a recursive divide-and-conquer approach. Recently we have shown that a PRAM lower bound argument from <ref> [2] </ref> can be carried over to the rmesh to show that this algorithm is optimal. This proof is based on very generic arguments, without requiring that the algorithm be comparison-based, and applies to rmeshes of all dimensions.
Reference: [3] <author> A.M. Frieze, </author> <title> "Parallel algorithms for finding hamilton cycles in random graphs", Info. </title> <booktitle> Proc. Let. 25 (1987), </booktitle> <pages> pp. 111-117. </pages>
Reference-contexts: show that any algorithm for finding a hamiltonian cycle in a random graph must take at least (log fl n) expected time, and he has also found an algorithm achieving this time. (An earlier ultrafast algorithm for this problem, taking fi ((log log n) 2 ) expected time, appeared in <ref> [3] </ref>). We believe that there should be several other problems which can be similarly solved.
Reference: [4] <author> J. Gil, Y. Matias, and U. Vishkin, </author> <title> "Towards a theory of nearly constant time parallel algorithms", </title> <booktitle> Proc. 32 Symp. on Theory of Comput. </booktitle> <year> (1990), </year> <pages> pp. 244-253. </pages>
Reference-contexts: The goal is to redistribute the tasks so that no processor has more than C. This approximate load balancing is a critical step in many randomized ultrafast algorithms, and it had been shown that it can be accomplished in fi (log fl n) expected time <ref> [4] </ref>. (log fl n is a function which grows extremely slowly, being 5 or less for all numbers less than 2 65;536 ). Note that achieving exact load balance takes (log n= log log n) expected time, but the extra measure of balance is not needed in many situations. <p> We also hope that this lower bound proof technique can be extended to other problems which are known to be solvable in fi (log fl n) time, including hashing, generation of random permutations, integer chain sorting, and some PRAM simulations <ref> [4] </ref>. Unfortunately we still do not know if our padded sort algorithm is optimal, and we are trying to find a lower bound for it.
Reference: [5] <author> E. Hao, P. MacKenzie, and Q.F. Stout, </author> <title> "Selection on the reconfigurable mesh", </title> <note> submitted. </note>
Reference-contexts: includes algorithms for various graph and image problems, algorithms for arithmetic operations, algorithms to sort n numbers in constant time on an n fi n rmesh (obtained nearly simultaneously by at least 4 groups), and an algorithm for determining the median of n 2 numbers in fi (log n) time <ref> [5] </ref>. Work is being started on extending the model to reconfigurable hypercubes, to studying how well an rmesh of one dimension or shape can simulate one of a different dimension or shape, and in combining PRAM and rmesh concepts into a single machine.
Reference: [6] <author> J-w. Jang and V.K. Prassana, </author> <title> "An optimal sorting algorithm on reconfigurable mesh", </title> <type> IRIS Tech. Rep. 277, </type> <institution> Univ. Southern Calif., </institution> <year> 1991. </year>
Reference-contexts: For rmeshes, our work has shown that they can deterministically solve many problems faster than regular mesh-connected computers, and in some cases can solve problems even faster than a CRCW PRAM. For example, it is possible to sort in constant time <ref> [6] </ref>. While various notions of reconfigurability have been around for some time, rmeshes are a class of machines which have only recently been studied and built, and which show great promise for a variety of tasks [7, 8].
Reference: [7] <author> H. Li and Q.F. Stout, </author> <title> "Reconfigurable SIMD parallel processors", </title> <booktitle> Proc. of the IEEE 79 (1991), </booktitle> <pages> pp. 429-443. </pages>
Reference-contexts: For example, it is possible to sort in constant time [6]. While various notions of reconfigurability have been around for some time, rmeshes are a class of machines which have only recently been studied and built, and which show great promise for a variety of tasks <ref> [7, 8] </ref>. This work is primarily being carried out by the author, who is the Principal Investigator, and Philip MacKenzie, a doctoral student finishing his thesis in this area [9].
Reference: [8] <editor> H. Li and Q.F. Stout, editors, </editor> <title> Reconfigurable Massively Parallel Computers, </title> <publisher> Prentice-Hall, </publisher> <year> 1991. </year>
Reference-contexts: For example, it is possible to sort in constant time [6]. While various notions of reconfigurability have been around for some time, rmeshes are a class of machines which have only recently been studied and built, and which show great promise for a variety of tasks <ref> [7, 8] </ref>. This work is primarily being carried out by the author, who is the Principal Investigator, and Philip MacKenzie, a doctoral student finishing his thesis in this area [9]. <p> We assume that only one value at a time can be broadcast on any bus, and in constant time it reaches all processors attached to the bus. Implementations of rmeshes include the Hughes/University of Massachusetts Image Understanding Architecture and the IBM Polymorphic Torus <ref> [8] </ref>. These are fine-grained SIMD machines with many processors per chip. The Hughes/U.
Reference: [9] <author> P. MacKenzie, </author> <title> Parallel Algorithms with Ultra-Fast Expected Times, </title> <type> Ph.D. thesis, </type> <institution> University of Michigan, </institution> <note> 1992 (in preparation). </note>
Reference-contexts: This work is primarily being carried out by the author, who is the Principal Investigator, and Philip MacKenzie, a doctoral student finishing his thesis in this area <ref> [9] </ref>. Recently two beginning graduate students, Douglas Van Wieren and Eric Hao, have started work on this project. 2 Randomization and Extra Space Randomization is an important technique for parallel and distributed computing. For example, the randomized backoff strategy in ethernet systems allows fast, simple, decentralized deadlock avoidance.
Reference: [10] <author> P. MacKenzie, </author> <title> "Load balancing requires (log fl n) expected time", </title> <booktitle> Proc. 3 Symp. on Disc. Alg. </booktitle> <year> (1992), </year> <pages> pp. 94-99. </pages>
Reference-contexts: Note that achieving exact load balance takes (log n= log log n) expected time, but the extra measure of balance is not needed in many situations. This is especially true in ultrafast algorithms, where the rest of the algorithm can be completed faster than exact load balance. P. MacKenzie <ref> [10] </ref> has now shown that any algorithm for approximate load balancing must take (log fl n) expected time, no matter how much memory is available. Thus the previous algorithm is optimal.
Reference: [11] <author> P. MacKenzie and Q.F. Stout, </author> <title> "Ultra-fast expected time parallel algorithms", </title> <booktitle> Proc. 2 Symp. on Disc. Alg. </booktitle> <year> (1991), </year> <pages> pp. 414-424. </pages>
Reference-contexts: For example, to find the minimum of n values using n processors in a CRCW PRAM takes fi (log log n) worst-case time [14], but it can be completed in constant expected time <ref> [11] </ref>. However, for some problems randomization by itself does not achieve the maximal possible speedup. <p> We call this a padded sort, and have shown that it can be completed in fi (log log n= log log log n) time <ref> [11] </ref>. This time can be achieved using only n log log log n= log log n processors, i.e., it can be achieved with linear speedup over the optimal linear expected time serial algorithm. <p> For example, given n planar points chosen uniformly and independently from the unit square, in fi (log log n= log log log n) expected time one can find the nearest neighbor of each point, construct the Voronoi diagram of the set, and determine the relative neighborhood graph <ref> [11] </ref>.
Reference: [12] <author> R. Miller, V.K. Prasanna-Kumar, D. Reisis, and Q.F. Stout, </author> <title> "Parallel computations on reconfigurable meshes", </title> <journal> IEEE Trans. Comp., </journal> <note> 1992 (to appear). </note>
Reference-contexts: Now the communication buses of the rmesh match the objects of the image, and operations such as object labeling, determining the size of objects, etc., can be carried on concurrently on each object, typically finishing in logarithmic or polylogarithmic time <ref> [12] </ref>. Similarly, for graph algorithms, if the graph is stored as an adjacency matrix then each row or column is a 1-dimensional rmesh devoted to a single vertex. For arithmetic algorithms, where numbers are stored one bit per processor, the rightmost column may be devoted to all lowest-order bits. <p> Or, in one of the significant early rmesh algorithms, there may be one bit per column, and the sum of these (and all prefix partial sums) can be determined in constant time <ref> [12] </ref>. (Note that the parity lower bound implies that this problem must take (log n= log log n) expected time on a PRAM). <p> One systematic area where they fail is when there are many more processors than data. One problem for which we have been able to achieve a new lower bound proof is that of finding the maximum of n 2 numbers in an n fi n rmesh. Earlier work <ref> [12] </ref> had shown that this could be determined in fi (log log n) time by a recursive divide-and-conquer approach. Recently we have shown that a PRAM lower bound argument from [2] can be carried over to the rmesh to show that this algorithm is optimal.
Reference: [13] <editor> Q.F. Stout, </editor> <booktitle> "Constant-time algorithms on PRAMs", Proc. 1988 Int'l. Conf. Parallel Proc., </booktitle> <pages> pp. 104-107. </pages>
Reference-contexts: In earlier work, similar use of randomization had shown that the extreme points of their convex hull, and the maximal points, can all be determined in constant expected time <ref> [13] </ref>. 2.2 Lower Bounds In conjunction with designing ultrafast algorithms, we are also interested in understanding why they cannot be still faster. Unfortunately the same power which permits ultrafast algorithms makes it difficult to prove nontrivial lower bounds.
Reference: [14] <author> L. Valiant, </author> <title> "Parallelism in comparison problems", </title> <journal> SIAM J. Comp. </journal> <volume> 4 (1975), </volume> <pages> pp. 348-355. </pages>
Reference-contexts: In the field of parallel algorithms, especially those for PRAMs, randomization has been used to provide faster expected time solutions to many problems. For example, to find the minimum of n values using n processors in a CRCW PRAM takes fi (log log n) worst-case time <ref> [14] </ref>, but it can be completed in constant expected time [11]. However, for some problems randomization by itself does not achieve the maximal possible speedup.
Reference: [15] <author> U. Vishkin, </author> <title> "Deterministic sampling anew technique for fast pattern matching", </title> <booktitle> Proc. 22 Symp. </booktitle> <institution> Found. Comp. Sci. </institution> <year> (1990), </year> <pages> pp. 170-180. </pages>
Reference-contexts: Unfortunately we still do not know if our padded sort algorithm is optimal, and we are trying to find a lower bound for it. Similarly we are looking at other problems with ultrafast solutions, such as pattern matching <ref> [15] </ref> or the geometry problems mentioned above, trying to establish lower bounds for them, or at least establish that they can be solved no faster than some more fundamental problem such as padded sort. 3 Reconfigurable Meshes To construct an nfin rmesh, imagine a grid of n horizontal and n vertical
References-found: 15

