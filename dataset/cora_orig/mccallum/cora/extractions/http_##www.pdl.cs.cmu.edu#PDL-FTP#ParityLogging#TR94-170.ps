URL: http://www.pdl.cs.cmu.edu/PDL-FTP/ParityLogging/TR94-170.ps
Refering-URL: http://www.pdl.cs.cmu.edu/Publications/publications.html
Root-URL: 
Title: A Redundant Disk Array Architecture for Efficient Small Writes  
Author: Daniel Stodolsky, Mark Holland, William V. Courtright II, and Garth A. Gibson 
Note: An abbreviated version of this report will appear in Transactions on Computer Systems,  
Address: Pittsburgh, Pennsylvania 15213-3890  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: July 29, 1994  September 1994.  
Pubnum: CMU-CS-94-170  
Abstract: Parity encoded redundant disk arrays provide highly reliable, cost effective secondary storage with high performance for reads and large writes. Their performance on small writes, however, is much worse than mirrored disks the traditional, highly reliable, but expensive organization for secondary storage. Unfortunately, small writes are a substantial portion of the I/O workload of many important, demanding applications such as on-line transaction processing. This paper presents parity logging, a novel solution to the small write problem for redundant disk arrays. Parity logging applies journalling techniques to substantially reduce the cost of small writes. We provide detailed models of parity logging and competing schemes mirroring, oating storage, and RAID level 5 and verify these models by simulation. Parity logging provides performance competitive with mirroring, but with capacity overhead close to the minimum offered by RAID level 5. Finally, parity logging can exploit data caching more effectively than all three alternative approaches. This research was supported by the ARPA, Information Science and Technology Office, under the title Research on Parallel Computing, ARPA Order No. 7330, the National Science Foundation under contract NSF ECD-8907068, and by IBM and AT&T graduate fellowships. Work furnished in connection with this research is provided under prime contract MDA972-90-C-0035 issued by ARPA/CMO to CMU. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of ARPA, NSF, IBM, AT&T, or the U.S. government. 
Abstract-found: 1
Intro-found: 1
Reference: [Bhide92] <author> Bhide, A., and Dias, D. </author> <title> RAID architectures for OLTP. </title> <institution> Computer Science Research Report RC 17879, IBM Corporation, </institution> <year> (1992). </year>
Reference-contexts: As long as parallel reads on different sublogs proceeded at nearly the same rate, this algorithm will not consume much extra buffer space. If buffer exhaustion occurs, the algorithm can simply serialize. 9. RELATED WORK Bhide and Dias <ref> [Bhide92] </ref> have independently developed a scheme similar to parity logging. Their LRAID-X4 organization maintains separate parity and parity-update log disks, and periodically applies the logged updates to the parity disk.
Reference: [Cao93] <author> Cao, P., Lim, S. B., Venkataraman, S., and Wilkes, J. </author> <title> The TickerTAIP parallel RAID architecture. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture. </booktitle> <address> San Diego CA (May 16-19, </address> <year> 1993). </year> <note> published as special issue of Computer Architecture News, ACM, 21, </note> <month> 2 (May </month> <year> 1993), </year> <pages> 52-63. </pages>
Reference-contexts: Each region is a miniature replica of the array proposed above. Small user writes for a particular region are 2. Our failure model treats disk and controller failures as independent. If concurrent controller and disk failures must be survived, controller state must be partitioned and replicated <ref> [Schulze89, Gibson93, Cao93] </ref>. 3. Notice that we make no attempt to reduce the cost of the overwrite of the target data block. Additional savings are possible if data writes are deferred and optimally scheduled [Solworth90, Orji93].
Reference: [Chen90] <author> Chen, P. M., and Patterson, D. A. </author> <title> Maximizing performance in a striped disk array. </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture. (Cat. </booktitle> <volume> No. </volume> <publisher> 90CH2887-8), </publisher> <address> Seattle WA (May 28-31, 1990), </address> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alamitos CA, </address> <year> (1990), </year> <pages> 322-331. </pages>
Reference-contexts: Nonredundant Parity Logging, no preread Parity Logging Mirroring RAID Level 5 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 Scaled Seek/Rotational Latency Ratio 0.0 0.4 0.8 Relative Throughput Page 15 of 25 is concentrated within one region. Choosing an appropriate data stripe unit <ref> [Chen90] </ref> will balance the user I/O across the actuators that contain data for this hot region; however, log and data traffic are partitioned over non-overlapping disks. If this traffic is not balanced, parity logging performance will fall short of Figure 14.
Reference: [DiskTrend94] <author> DISK/TREND, Inc. </author> <title> 1994 DISK/TREND Report: Disk Drive Arrays. </title> <address> 1925 Landings Drive, Mountain View CA (April 1994) SUM-3. </address>
Reference-contexts: 1. INTRODUCTION The market for disk arrays, collections of independent magnetic disks linked together as a single data store, is undergoing rapid growth and has been predicted to exceed 13 billion dollars by 1997 <ref> [DiskTrend94] </ref>. This growth has been driven by three factors. First, the growth in processor speed has outstripped the growth in disk data rate. This imbalance transforms traditionally compute-bound applications to I/O-bound applications. To achieve application speedup, I/O system bandwidth must be increased by increasing the number of disks.
Reference: [Feigel94] <author> Feigel, C. </author> <title> Flash memory heads toward mainstream. Microprocessor Report, </title> <journal> Vol. </journal> <volume> 8, No. 7, </volume> <month> May 30, </month> <year> 1994, </year> <pages> 19-25. </pages>
Reference-contexts: If the ratio of the cost of a byte of memory and a byte of disk is then the buffer memory space cost, relative to the cost of the array of disks is . If memory costs 30 times as much as disk <ref> [Feigel94] </ref>, then an array of 22 IBM 0661 (Figure 12) disks buffering a single log track per region ( ) requires about 5.6 MB of buffer, costing the equivalent of about 2% of the arrays cost. In practice, parameters such as the number of regions must be discrete.
Reference: [Geist87] <author> Geist, R. M., and Daniel, S. </author> <title> A continuum of disk scheduling algorithms. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5, </volume> <month> 1 (Feb. </month> <year> 1987), </year> <pages> 77-92. </pages>
Reference-contexts: With FIFO disk scheduling, used throughout the rest of this paper, parity logging is always superior to RAID level 5 and is equivalent to mirroring when data caching of writes is effective. 10 With CVSCAN <ref> [Geist87] </ref>, all configurations deliver higher throughput with lower average response times, but mirroring and nonredundant arrays benefit most. Nonetheless, parity logging remains superior to RAID level 5 and is comparable to mirroring when data caching of writes is effective. 7. <p> For RAID level 5 and parity logging, results are shown both for the case where all writes are blind, and when the old data for all writes is cached (no preread). While CVSCAN <ref> [Geist87] </ref> scheduling improves throughput and response of all workloads, mirrored and nonredundant disk arrays improve the most, since seek time is a larger proportion of their underlying I/Os.
Reference: [Gibson92] <author> Gibson, G. </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage. </title> <publisher> MIT Press, </publisher> <year> (1992). </year>
Reference-contexts: Without this redundancy, large disk arrays have unacceptably low data reliability because of their large number of component disks. For these three reasons, redundant disk arrays, also known as Redundant Arrays of Inexpensive 1 Disks (RAID), are strong candidates for nearly all on-line secondary storage systems <ref> [Patterson88, Gibson92] </ref>. variant, RAID level 5, employs distributed parity with data striped on a unit that is one or more disk sectors. RAID level 5 arrays exploit the low cost of parity encoding to provide high data reliability [Gibson93]. <p> MULTIPLE FAILURE TOLERATING ARRAYS A significant advantage of parity logging is its efficient extension to multiple failure tolerating arrays. Multiple failure tolerance provides much longer mean time to data loss and greater tolerance for bad blocks discovered during reconstruction <ref> [Gibson92] </ref>. Using codes more powerful than parity, RAID level 5 and its variants can all be extended to tolerate concurrent failures. Figure 24 gives an example of one of the more easily-understood double failure tolerant disk array organizations. <p> preread Mirroring Nonredundant 0 10 20 30 40 50 User I/Os per second per disk 0 40 80 120 Response T ime (ms) Page 21 of 25 bit symbol, dependent on a multiple-bit symbol from each of a subset of the data disks, then the code is a non-binary code <ref> [Macwilliams77, Gibson92] </ref>. Non-binary codes can achieve much lower check information space overhead in a multiple failure tolerating array. In particular, a variant of a Reed Solomon code called Parity has been used in disk array products to provide double failure tolerance with only two check information disks [ATC90].
Reference: [Gibson93] <author> Gibson, G., and Patterson, D. </author> <title> Designing disk arrays for high data reliability. </title> <journal> Journal of Parallel and Distributed Computing. </journal> <volume> 17, </volume> <month> 1-2 (Jan. - Feb., </month> <year> 1993), </year> <pages> 4-27. </pages>
Reference-contexts: RAID level 5 arrays exploit the low cost of parity encoding to provide high data reliability <ref> [Gibson93] </ref>. Data is striped over all disks so that large files can be fetched with high bandwidth. By distributing the parity, many small random blocks can also be accessed in parallel without hot spots on any disk. <p> Each region is a miniature replica of the array proposed above. Small user writes for a particular region are 2. Our failure model treats disk and controller failures as independent. If concurrent controller and disk failures must be survived, controller state must be partitioned and replicated <ref> [Schulze89, Gibson93, Cao93] </ref>. 3. Notice that we make no attempt to reduce the cost of the overwrite of the target data block. Additional savings are possible if data writes are deferred and optimally scheduled [Solworth90, Orji93]. <p> Simulation Parameters. The access size, alignment, and spatial distribution are representative of OLTP workloads, while a 100% write ratio emphasizes the performance differences of the various array organizations. Assuming disks have independent support hardware, disk failures will be independent, allowing a parity group to be single fault tolerant <ref> [Gibson93] </ref>. Disk parameters are modeled on the IBM Lightning drive [IBM0661]. Note that the dist term in the seek time model is the number of cylinders traversed, excluding the destination.
Reference: [Holland92] <author> Holland, M., and Gibson, G. </author> <title> Parity Declustering for Continuous Operation in Redundant Disk Arrays. </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V). </booktitle> <address> Boston MA (Oct. 12-15, </address> <note> 1992) published as special issue of SIGPLAN Notices, ACM, 27, 9 (Sept. </note> <year> 1992), </year> <pages> 23-35. </pages>
Reference-contexts: More dynamic assignment of controller memory should allow higher performance to be achieved or a substantial reduction in the amount of memory required. Application of data compression to the parity log should be very profitable. The interaction of parity logging and parity declustering <ref> [Holland92] </ref> merits exploration. Parity declustering provides high performance during degraded-mode and reconstruction while parity logging provides high performance during fault-free operation. The combination of the two should provide a cost-effective system for OLTP environments. Page 25 of 25 11.
Reference: [IBM0661] <author> IBM Corporation. </author> <title> IBM 0661 Disk Drive Product Description, Model 370, First Edition, Low End Storage Products, </title> <type> 504/114-2. </type> <institution> IBM Corporation, </institution> <year> (1989). </year>
Reference-contexts: Thus a disk servicing a small-access-dominated workload spends the majority of its time positioning instead of transferring data. Figure 3 shows the relative bandwidths of random block, track and cylinder accesses for a modern small-diameter disk <ref> [IBM0661] </ref>. This figure largely bears out the lore of disk bandwidth: random cylinder accesses move data twice as fast as random track accesses which, in turn, move data ten times faster than random block accesses. <p> Assuming disks have independent support hardware, disk failures will be independent, allowing a parity group to be single fault tolerant [Gibson93]. Disk parameters are modeled on the IBM Lightning drive <ref> [IBM0661] </ref>. Note that the dist term in the seek time model is the number of cylinders traversed, excluding the destination.
Reference: [Menon92] <author> Menon, J., and Kasson, J. </author> <title> Methods for improved update performance of disk arrays. </title> <booktitle> Proceedings of the Hawaii International Conference on System Sciences (Cat. </booktitle> <volume> No. </volume> <publisher> 91TH0394-7), </publisher> <address> Kauai, HI, </address> <month> (Jan. </month> <pages> 7-10, </pages> <address> 1992), </address> <publisher> IEEE Computer Society Press (vol. </publisher> <address> 1 of 4), </address> <year> (1991) </year> <month> 74-83. </month>
Reference-contexts: MODELING ALTERNATIVE SCHEMES Only a few array designs have addressed the problem of high performance, parity-based, disk storage for small write workloads. The most notable of these is oating data and parity <ref> [Menon92] </ref>. This section reviews and estimates the performance of four designs: nonredundant disk arrays (RAID level 0), mirrored disks (RAID level 1), distributed N+1 parity (RAID level 5), and oating data and parity. The notation and analysis methodology are the same as used in Section 3. <p> These can be combined into two read-rotate-write accesses, each of which takes disk seconds for a total disk busy time of . Again, no long-term controller storage is required. The oating data and parity modification to RAID level 5 was proposed by Menon and Kasson <ref> [Menon92] </ref>. In its most aggressive variant, this technique organizes data and parity into cylinders that contain either only data or parity. <p> When a disk block is written, a new location is chosen in a manner that minimizes the disk arm time devoted to the write, and a new physical-to-logical mapping is established. We have described one such scheme, oating data and parity <ref> [Menon92] </ref>, in this paper. An extreme example of this approach is the log structure filesystem (LFS), in which all data is written in a segmented log, and segments are periodically reclaimed by garbage collection [Rosenblum91].
Reference: [Menon93] <author> Menon, J. </author> <title> Performance of RAID5 Disk Arrays with Read and Write Caching. </title> <institution> Computer Science Research Report RJ9485(83363), IBM Corporation, </institution> <year> (1993). </year>
Reference-contexts: In these cases, the old data value is usually known (cached) at the time of the write and the preread of the data may be skipped <ref> [Menon93] </ref>. Under these conditions, the overhead for RAID levels 4 or 5 is just two random block accesses per small write, or random block accesses per small user writes, and the overhead for parity logging is random small accesses.
Reference: [Orji93] <author> Orji, C. U., and Solworth, J. A. </author> <title> Doubly distorted mirrors. </title> <booktitle> Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data. </booktitle> <address> Washington DC, </address> <month> (May 26-28, </month> <note> 1993) published as special issue of SIGMOD Record, ACM, 22, </note> <month> 2 (June </month> <year> 1993), </year> <pages> 307-318. </pages>
Reference-contexts: Notice that we make no attempt to reduce the cost of the overwrite of the target data block. Additional savings are possible if data writes are deferred and optimally scheduled <ref> [Solworth90, Orji93] </ref>. D T V TVD TVD TVD TV D TVD TVD 5TVD 4 5 = TV D/10 ( ) 3V T/2 D/10 ( )+ TVD/4= Page 5 of 25 journalled into that regions log. <p> Write-twice is typically combined with one of the write buffering techniques to improve the efficiency of the second write. This technique has been pursued most fully for mirroring systems <ref> [Solworth91, Orji93] </ref>. The oating location technique improves the efficiency of writes by eliminating the static association of logical disk blocks and fixed locations in the disk array. <p> The distorted mirror approach [Solworth91] uses the 100% storage overhead of mirroring to avoid this problem: one copy of each block is stored in fixed location, while the other copy is maintained in oating storage, achieving higher write throughput while maintaining data sequentiality <ref> [Orji93] </ref>. However, all oating location techniques require substantial host or controller storage for mapping information and buffered data. 10. CONCLUDING REMARKS This paper presents a novel solution to the small write problem in redundant disk arrays based on a distributed log.
Reference: [Ousterhout88] <author> J. Ousterhout, et. al. </author> <title> The Sprite network operating system. </title> <journal> Computer, </journal> <volume> 21, 2, </volume> <month> (Feb. </month> <year> 1988) </year> <month> 23-36. </month>
Reference-contexts: Parity logging was simulated with a single track of log buffer per region ( ) for several different degrees of log striping ( ). The simulations were performed using the RAIDSIM package, a disk array simulator derived from the Sprite operating system disk array driver <ref> [Ousterhout88] </ref>, which was extended with implementations of parity logging and oating data and parity. In each simulation, a request stream was generated by 66 user processes, an average of three per disk.
Reference: [Patterson88] <author> Patterson, D., Gibson, G., and Katz, R. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data. </booktitle> <address> Chicago IL, </address> <month> (June 1-3, </month> <note> 1988) published as special issue of SIGMOD Record, ACM, 17, 3 (Sept. </note> <year> 1988), </year> <pages> 109-116. </pages>
Reference-contexts: Without this redundancy, large disk arrays have unacceptably low data reliability because of their large number of component disks. For these three reasons, redundant disk arrays, also known as Redundant Arrays of Inexpensive 1 Disks (RAID), are strong candidates for nearly all on-line secondary storage systems <ref> [Patterson88, Gibson92] </ref>. variant, RAID level 5, employs distributed parity with data striped on a unit that is one or more disk sectors. RAID level 5 arrays exploit the low cost of parity encoding to provide high data reliability [Gibson93]. <p> D1 D13 Disk 0 D0 D12 4 D23 D22 D21 D20 P20-24 D24 D18 D19 D20 D21 3 D22 D23 Page 2 of 25 applications, they possess at least one critical limitation: their throughput is penalized by a factor of four over nonredundant arrays for workloads of mostly small writes <ref> [Patterson88] </ref>. This penalty arises because a small write request may require the old value of the users targeted data be read (we call this a preread), overwriting this with new user data, prereading the old value of the corresponding parity, then overwriting this second disk block with the updated parity. <p> when the data to be rewritten is not cached, while the full height indicates performance when data is cached. 0 1 2 3 Disk failures tolerated 0 20 40 IOs per disk-second Nonredundant Parity Logging Mirroring RAID Level 5 Floating D/P Page 23 of 25 about a factor of four <ref> [Patterson88] </ref>. This optimization can not be applied directly to parity logging disk arrays as we have described them so far because there may exist outstanding (not yet reintegrated) logged updates for a particular parity unit at the time when a large write overwrites that parity unit.
Reference: [Polyzois93] <author> Polyzois, C. A., Bhide, and A., Dias, D. M. </author> <title> Disk mirroring with alternating deferred updates. </title> <booktitle> Proceedings of the 19th Conference on Very Large Databases. </booktitle> <address> Dublin Ireland (Aug. 24-27, 1993). </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto CA, </address> <year> (1993), </year> <pages> 604-617. </pages>
Reference-contexts: Write buffering delays users write requests in a large disk or file cache to achieve deep queues, which can then be scheduled to substantially reduce seek and rotational positioning overheads <ref> [Seltzer90, Solworth90, Rosenblum91, Polyzois93] </ref>. Data loss on a single failure is possible in these systems unless fault-tolerant caches are used. The write-twice approach attempts to reduce the latency of writes without relying on fault-tolerant caches.
Reference: [Ramakrishnan92] <author> Ramakrishnan, K. K., Biswas, P., and Karedla, R. </author> <title> Analysis of file I/O traces in commercial computing environments. </title> <booktitle> Proceedings of the 1992 ACM SIGMETRICS Conference on Performance. </booktitle> <address> Newport RI, </address> <month> (June 1-5, </month> <title> 1992) published as a special issue of Performance Evaluation Review, </title> <journal> ACM, </journal> <volume> 20, </volume> <month> 1 (June </month> <year> 1992). </year> <booktitle> ACM, </booktitle> <pages> 78-90. </pages>
Reference-contexts: Performance in More General Workloads Up to this point, all of the analysis has been specialized for workloads whose accesses are 100% small (2KB) random writes. This section examines a mixed workload, defined in Figure 22, modeled on statistics taken from an airline reservation system <ref> [Ramakrishnan92] </ref>. With this more general workload, the results of the earlier sections are modified by two important effects: reads and medium to large writes. <p> Airline reservation workload. The I/O distribution shown above was selected to agree with general statistics from an airline reservation system <ref> [Ramakrishnan92] </ref>. This workload is reported as approximately 82% reads, a mean read size of 4.61 KB, and a median read size of 3 KB. The mean write size was larger, 5.71 KB, but the median write size was smaller, 1.5 KB. Locality of reference and overwrite percentages were not reported.
Reference: [Rosenblum91] <author> Rosenblum, R. and Ousterhout, J. </author> <title> The design and implementation of a log-structured file system. </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <address> Pacific Grove CA, </address> <month> (Oct. </month> <pages> 13-16, </pages> <note> 1991) published as special issue of Operating Systems Review, ACM, 25, </note> <month> 5 </month> <year> (1991), </year> <pages> 1-15. </pages>
Reference-contexts: Write buffering delays users write requests in a large disk or file cache to achieve deep queues, which can then be scheduled to substantially reduce seek and rotational positioning overheads <ref> [Seltzer90, Solworth90, Rosenblum91, Polyzois93] </ref>. Data loss on a single failure is possible in these systems unless fault-tolerant caches are used. The write-twice approach attempts to reduce the latency of writes without relying on fault-tolerant caches. <p> We have described one such scheme, oating data and parity [Menon92], in this paper. An extreme example of this approach is the log structure filesystem (LFS), in which all data is written in a segmented log, and segments are periodically reclaimed by garbage collection <ref> [Rosenblum91] </ref>. Using fault-tolerant caches to delay data writes, this approach converts all writes into long sequential transfers, greatly enhancing write throughput. However, because logically nearby blocks may not be physically nearby, the performance of LFS in read-intensive workloads may be degraded if the read and write access patterns differ widely.
Reference: [Schulze89] <author> Schulze, M. E., Gibson. G. A., Katz, R. H., and Patterson, D. A. </author> <booktitle> How reliable is a RAID? Proceedings of the 1989 IEEE Computer Society International Conference (COMPCON 89) (Cat. </booktitle> <volume> No. </volume> <publisher> 91TH0394-7), </publisher> <address> San Francisco CA, (Feb. 37-Mar. 3, 1989), </address> <publisher> IEEE Computer Society Press, </publisher> <address> Washington DC (1989) 118-123. </address> <note> [Seltzer90 Seltzer, </note> <author> M., Chen, P., and Ousterhout, J. </author> <title> Disk scheduling revisited. </title> <booktitle> Proceedings of the Winter 1990 USENIX Conference. </booktitle> <address> Washington DC, </address> <month> (Jan. </month> <year> 1990) </year> <month> 22-26 </month>
Reference-contexts: Each region is a miniature replica of the array proposed above. Small user writes for a particular region are 2. Our failure model treats disk and controller failures as independent. If concurrent controller and disk failures must be survived, controller state must be partitioned and replicated <ref> [Schulze89, Gibson93, Cao93] </ref>. 3. Notice that we make no attempt to reduce the cost of the overwrite of the target data block. Additional savings are possible if data writes are deferred and optimally scheduled [Solworth90, Orji93].
Reference: [Solworth90] <author> Solworth, J. A. and Orji, C. U. </author> <title> Write-only disk caches. </title> <booktitle> Proceedings of the ACM SIGMOD Conference. </booktitle> <address> Atlantic City NJ, </address> <note> (May 23-25 1990) published as special issue of SIGMOD Record, ACM, 19, </note> <month> 2 (June </month> <year> 1990), </year> <pages> 123-132. </pages>
Reference-contexts: Notice that we make no attempt to reduce the cost of the overwrite of the target data block. Additional savings are possible if data writes are deferred and optimally scheduled <ref> [Solworth90, Orji93] </ref>. D T V TVD TVD TVD TV D TVD TVD 5TVD 4 5 = TV D/10 ( ) 3V T/2 D/10 ( )+ TVD/4= Page 5 of 25 journalled into that regions log. <p> Write buffering delays users write requests in a large disk or file cache to achieve deep queues, which can then be scheduled to substantially reduce seek and rotational positioning overheads <ref> [Seltzer90, Solworth90, Rosenblum91, Polyzois93] </ref>. Data loss on a single failure is possible in these systems unless fault-tolerant caches are used. The write-twice approach attempts to reduce the latency of writes without relying on fault-tolerant caches.
Reference: [Solworth91] <author> Solworth, J. A. and Orji, C. U. </author> <title> Distorted Mirrors. </title> <booktitle> Proceedings of the First International Conference on Parallel and Distributed Information Systems. (Cat. </booktitle> <volume> No. </volume> <publisher> 91TH0393-4), </publisher> <address> Miami Beach FL, </address> <month> (Dec. </month> <pages> 4-6, </pages> <address> 1991), </address> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alamitos CA (1991) 10-17. </address>
Reference-contexts: Write-twice is typically combined with one of the write buffering techniques to improve the efficiency of the second write. This technique has been pursued most fully for mirroring systems <ref> [Solworth91, Orji93] </ref>. The oating location technique improves the efficiency of writes by eliminating the static association of logical disk blocks and fixed locations in the disk array. <p> However, because logically nearby blocks may not be physically nearby, the performance of LFS in read-intensive workloads may be degraded if the read and write access patterns differ widely. The distorted mirror approach <ref> [Solworth91] </ref> uses the 100% storage overhead of mirroring to avoid this problem: one copy of each block is stored in fixed location, while the other copy is maintained in oating storage, achieving higher write throughput while maintaining data sequentiality [Orji93].
Reference: [Salem86] <author> Salem, K., and Garcia-Molina, H. </author> <title> Disk striping. </title> <booktitle> Proceedings of the 2nd IEEE International Conference on Data Engineering. IEEE (1986). </booktitle>
Reference-contexts: This single access could be separated into two accesses each taking S+R+2R/D disk seconds for a total of 2S+(2+4/D)R. For most modern disks S is about twice R, so the single access is more efficient. 5. Disks that support zero-latency writes <ref> [Salem86] </ref> can eliminate the initial rotational positioning delay. This can reduce the I/O time by up to 26% in drives such as the IBM 0661 (which does not support this feature), if only a single track is buffered (K=1).
Reference: [Stodolsky93] <author> Stodolsky, D., Gibson, G., and Holland, M. </author> <title> Parity Logging: Overcoming the Small Write Problem in Redundant Disk Arrays. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture. </booktitle> <address> San Diego CA (May 16-19, </address> <year> 1993). </year> <note> published as special issue of Computer Architecture News, ACM, 21, </note> <month> 2 (May </month> <year> 1993), </year> <pages> 64-75. </pages>
Reference-contexts: LRAID-X4 reaches its performance maximum of 34.5 writes per disk per second with 20 disks (16 data, 2 parity, 2 log) for a 100% write workload with 5% of a disks worth of memory <ref> [Stodolsky93] </ref>. Additional disks do not increase performance.
Reference: [TPCA89] <author> The TPC-A Benchmark: </author> <title> A Standard Specification, Transaction Processing Performance Council, </title> <year> (1989). </year>
Reference-contexts: OLTP Workload Example. The transaction processing council (TPC) benchmark is an industry standard benchmark for OLTP systems stressing update-intensive database services <ref> [TPCA89] </ref>. It models the computer processing for customer withdrawals and deposits at a bank. The primary metric for TPC benchmarks is transactions per second (TPS). Systems are required to complete 90% of their transactions in under 2 seconds and to meet the scaling constraints listed above.
References-found: 24

