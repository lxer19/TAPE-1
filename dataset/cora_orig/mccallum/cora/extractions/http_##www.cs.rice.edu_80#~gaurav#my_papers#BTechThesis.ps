URL: http://www.cs.rice.edu:80/~gaurav/my_papers/BTechThesis.ps
Refering-URL: http://www.cs.rice.edu:80/~gaurav/gaurav_research.html
Root-URL: 
Title: Semi-Automatic Parallelization of Fortran Code  Under the guidance of  
Author: Gaurav Banga Gagan Hasteer Dr. Gautam M. Shroff 
Degree: A thesis submitted in partial fulfillment of the requirements for the degree of Bachelor of Technology in Computer Science and Engineering by  
Date: May 1994  
Address: Delhi  
Affiliation: Department of Computer Science and Engineering Indian Institute of Technology,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Chau-Wen Tseng. </author> <title> "An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines", </title> <type> PhD Thesis, </type> <institution> Rice University, Houston, Texas, </institution> <year> 1993. </year>
Reference-contexts: Automation of this process is therefore both desirable and necessary for widespread use of these machines to become a reality. Many approaches to the automatic parallelization of sequential code are known in the literature <ref> [1] </ref>, [2], [3]. The source language for these approaches is some data-parallel dialect of Fortran or some other conventional high level language. In these approaches, the natural low-level parallelism in the problem is expressed by "aligning" all arrays with respect to "templates". <p> This can be performed in two ways with differing levels of sophistication. For loop-nests which can be characterized at compile-time and have a relatively straightforward loop bound structure (e.g. constant bounds), we can use a strategy called compile-time resolution <ref> [1] </ref> to parallelize the code. For typical "real" code, however, loop-bounds are not simple enough to be characterized for compile-time resolution. <p> The theory for these transformations is similar to that used by the prototype Fortran D compiler being developed at Rice University <ref> [1] </ref>, [6]. Run time resolution on the other hand doesn't partition the iteration space and calculates owner processors at run time. All communication decisions are made at run time.
Reference: [2] <author> R. Hill. "MIMDizer: </author> <title> A new tool for parallelization", </title> <journal> Supercomputing Review, </journal> <volume> 3(4) </volume> <pages> 26-28, </pages> <year> 1990. </year>
Reference-contexts: Automation of this process is therefore both desirable and necessary for widespread use of these machines to become a reality. Many approaches to the automatic parallelization of sequential code are known in the literature [1], <ref> [2] </ref>, [3]. The source language for these approaches is some data-parallel dialect of Fortran or some other conventional high level language. In these approaches, the natural low-level parallelism in the problem is expressed by "aligning" all arrays with respect to "templates".
Reference: [3] <author> Thomas Brandes. </author> <title> "Adaptor: A Compilation System for Data Parallel Fortran Programs", </title> <type> GMD Technical Report, </type> <institution> GMD, </institution> <address> St, Augustin, Germany, </address> <year> 1993. </year>
Reference-contexts: Automation of this process is therefore both desirable and necessary for widespread use of these machines to become a reality. Many approaches to the automatic parallelization of sequential code are known in the literature [1], [2], <ref> [3] </ref>. The source language for these approaches is some data-parallel dialect of Fortran or some other conventional high level language. In these approaches, the natural low-level parallelism in the problem is expressed by "aligning" all arrays with respect to "templates". <p> -[no]sixchar [no]: catch nonunique names -[no]sort [no]: prerequisite-order sort of modules -[no]symtab [no]: print symbol table info -[no]truncation [yes]: check for truncation pitfalls -[no]verbose [yes]: verbose output -[no]volatile [no]: assume volatile common blocks -include=str [NONE]: include-file directory -output=str [NONE]: output file name Settings (legal range) [default]: -arguments=dd (0 to 3) <ref> [3] </ref>: check args: 0=none 1=number 2=type 3=all -array=dd (0 to 3) [3]: check array args: 0=none 1=dims 2=size 3=all -bigval=dd (0 to 100000) [20000]: threshold for big array printing -columns=dd (72 to 132) [72]: max line length processed -common=dd (0 to 3) [3]: common check: 0=none 3=most strict -usage=dd (0 to <p> -[no]symtab [no]: print symbol table info -[no]truncation [yes]: check for truncation pitfalls -[no]verbose [yes]: verbose output -[no]volatile [no]: assume volatile common blocks -include=str [NONE]: include-file directory -output=str [NONE]: output file name Settings (legal range) [default]: -arguments=dd (0 to 3) <ref> [3] </ref>: check args: 0=none 1=number 2=type 3=all -array=dd (0 to 3) [3]: check array args: 0=none 1=dims 2=size 3=all -bigval=dd (0 to 100000) [20000]: threshold for big array printing -columns=dd (72 to 132) [72]: max line length processed -common=dd (0 to 3) [3]: common check: 0=none 3=most strict -usage=dd (0 to 3) [3]: 0=no check, 1=used-not-set 2=unused 3=all -wordsize=dd (0 to 16) <p> Settings (legal range) [default]: -arguments=dd (0 to 3) <ref> [3] </ref>: check args: 0=none 1=number 2=type 3=all -array=dd (0 to 3) [3]: check array args: 0=none 1=dims 2=size 3=all -bigval=dd (0 to 100000) [20000]: threshold for big array printing -columns=dd (72 to 132) [72]: max line length processed -common=dd (0 to 3) [3]: common check: 0=none 3=most strict -usage=dd (0 to 3) [3]: 0=no check, 1=used-not-set 2=unused 3=all -wordsize=dd (0 to 16) [4]: standard wordsize in bytes (0=no default) -wrap=dd (0 to 999) [79]: width of page to wrap error messages (First 3 chars of option name significant) The analyser can be run <p> args: 0=none 1=number 2=type 3=all -array=dd (0 to 3) <ref> [3] </ref>: check array args: 0=none 1=dims 2=size 3=all -bigval=dd (0 to 100000) [20000]: threshold for big array printing -columns=dd (72 to 132) [72]: max line length processed -common=dd (0 to 3) [3]: common check: 0=none 3=most strict -usage=dd (0 to 3) [3]: 0=no check, 1=used-not-set 2=unused 3=all -wordsize=dd (0 to 16) [4]: standard wordsize in bytes (0=no default) -wrap=dd (0 to 999) [79]: width of page to wrap error messages (First 3 chars of option name significant) The analyser can be run with certain combination of options to produce various type of
Reference: [4] <author> D. Callahan and Ken Kennedy. </author> <title> "Compiling Programs for distributed-memory multiprocessors", </title> <journal> Journal of Supercomputing, </journal> 2 151-169,October 1988. 
Reference-contexts: Scalars will in general be non-shared and hence replicated with some exceptions (e.g. in the case of reductions). Sharing will involve partitioning of the data structures. Partitioning of arrays will lead to the partitioning of the computation space between the processors according to the owner's compute rule <ref> [4] </ref>, [5] and introduction of communication to take care of accesses to non-local data 1 . Our tool aims at automating the various stages of the process described above. It consists of three components Interactive Analyser, Simulator and Parallel Code Synthesiser (Figure 1.1). <p> check array args: 0=none 1=dims 2=size 3=all -bigval=dd (0 to 100000) [20000]: threshold for big array printing -columns=dd (72 to 132) [72]: max line length processed -common=dd (0 to 3) [3]: common check: 0=none 3=most strict -usage=dd (0 to 3) [3]: 0=no check, 1=used-not-set 2=unused 3=all -wordsize=dd (0 to 16) <ref> [4] </ref>: standard wordsize in bytes (0=no default) -wrap=dd (0 to 999) [79]: width of page to wrap error messages (First 3 chars of option name significant) The analyser can be run with certain combination of options to produce various type of information about the input program. APPENDIX A.
Reference: [5] <author> A. Rogers and K. </author> <title> Pingali "Process decomposition through locality of reference", </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Scalars will in general be non-shared and hence replicated with some exceptions (e.g. in the case of reductions). Sharing will involve partitioning of the data structures. Partitioning of arrays will lead to the partitioning of the computation space between the processors according to the owner's compute rule [4], <ref> [5] </ref> and introduction of communication to take care of accesses to non-local data 1 . Our tool aims at automating the various stages of the process described above. It consists of three components Interactive Analyser, Simulator and Parallel Code Synthesiser (Figure 1.1).
Reference: [6] <author> S. Hiranandani, Ken Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> "An overview of the Fortran D Compilation System", </title> <type> Technical Report, </type> <institution> Rice University, Houston, Texas, </institution> <year> 1991. </year>
Reference-contexts: The theory for these transformations is similar to that used by the prototype Fortran D compiler being developed at Rice University [1], <ref> [6] </ref>. Run time resolution on the other hand doesn't partition the iteration space and calculates owner processors at run time. All communication decisions are made at run time. Although data partitioning can still be done on run time code, at the time of present writing it has not been implemented.
Reference: [7] <author> T. O'Reilly. </author> <title> "X Toolkit Intrinsics Reference Manual", Nutshell Handbooks, </title> <address> Sebastopol, CA, </address> <year> 1990. </year>
Reference-contexts: CHAPTER 4. IMPLEMENTATION 25 4.5 Graphical User Interface We have build an integrated Graphical User Interface for our System. This X11R5 based GUI has been built on top of X-Toolkit using the Athena WidgetKit <ref> [7] </ref>. Structurally, the GUI has two main static components, a command panel and a viewport. The command panel has a set of buttons which either open pulldown menus, perform some translation action, or open up a transient popup.
Reference: [8] <author> Eric F. Van de Velde and Jens Lorenz. </author> <title> "Adaptive Data Distribution for Concurrent Continuation", </title> <type> Technical Report CRPC-89-4-revised, </type> <institution> Center For Research On Parallel Computing, Caltech, </institution> <year> 1989 </year>
Reference-contexts: Typically, we need consider only regular distributions. Two well known regular distributions are the Block and the Scatter distributions <ref> [8] </ref>. Distribution functions for these distributions (for 9 CHAPTER 2.
Reference: [9] <author> Gaurav Banga and Gautam M. Shroff. </author> <title> "Communication Efficient Parallel Mixed Radix FFTs", </title> <type> Technical Report 94/1, </type> <institution> Department of Computer Science and Engineering, </institution> <address> IIT Delhi, </address> <month> February </month> <year> 1994. </year>
Reference-contexts: One important such distribution is the Break-Scatter distribution <ref> [9] </ref>.
Reference: [10] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek and Vaidy Sunderam. </author> <title> "PVM 3 User's Guide and Reference Manual", </title> <institution> ORNL/TM-12187, Engineering 43 BIBLIOGRAPHY 44 Physics and Mathematics Division, Mathematical Sciences Section, Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: This generic procedure in own.c sets up the environment for execution of a parallel program on Parallel Virtual Machine (PVM) <ref> [10] </ref>.
Reference: [11] <author> Vasanth Balasundaram. </author> <title> "A Mechanism for Keeping Useful Internal Information in Parallel Programming Tools: The Data Access Descriptor", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9, </volume> <pages> 154-170, </pages> <year> 1990 </year>
Reference-contexts: Since intersection of RSDs is closed, the inverse subscript operation is also closed. C.2 Data Access Descriptors The Data Access Descriptor (DAD) introduced by Balasundaram <ref> [11] </ref> is a very versatile mechanism for implementing the various sets of Chapter 2's algorithms. The Simple Section described in [11] is able to characterize iteration spaces in which the bounds are a linear combination of the induction variables of the enclosing loops. <p> Since intersection of RSDs is closed, the inverse subscript operation is also closed. C.2 Data Access Descriptors The Data Access Descriptor (DAD) introduced by Balasundaram <ref> [11] </ref> is a very versatile mechanism for implementing the various sets of Chapter 2's algorithms. The Simple Section described in [11] is able to characterize iteration spaces in which the bounds are a linear combination of the induction variables of the enclosing loops. In addition, DADs are extremely useful in inter-procedural analysis a greay area for RSDs.
References-found: 11

