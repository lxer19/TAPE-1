URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/93-07.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Title: Sparse Implementation of Revised Simplex Algorithms on Parallel Computers  
Author: Wei Shu and Min-You Wu 
Abstract: Parallelizing sparse simplex algorithms is one of the most challenging problems. Because of very sparse matrices and very heavy communication, the ratio of computation to communication is extremely low. It becomes necessary to carefully select parallel algorithms, partitioning patterns, and communication optimization to achieve a speedup. Two implementations on Intel hypercubes are presented in this paper. The experimental results show that a nearly linear speedup can be obtained with the basic revised simplex algorithm. However, the basic revised simplex algorithm has many fill-ins. We also implement a revised simplex algorithm with LU decomposition. It is a very sparse algorithm, and it is very difficult to achieve a speedup with it.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Astfalk, I. Lusting, R. Marsten and D. Shanno, </author> <title> The Interior-Point Method for Linear Programming, </title> <journal> IEEE Software, </journal> <month> July, </month> <year> 1992, </year> <pages> pages 61-67. </pages>
Reference-contexts: Although it is well-known that the computational complexity of the simplex method is not polynomial in the number of equations, in practice it can quickly solve many linear programming problems. The interior-point algorithms are polynomial and potentially easy to parallelize <ref> [7, 1] </ref>. However, since the interior-point algorithm has not been well-developed yet, the simplex algorithm is still applied to most real applications. Given a linear programming problem, the simplex method starts from an initial feasible solution and moves toward the optimal solution. The execution is carried out iteratively.
Reference: [2] <author> R. H. Bartels and G. H. Golub, </author> <title> The Simplex Method of Linear Programming Using LU Decomposition, </title> <journal> Comm. ACM, </journal> <volume> 12, </volume> <pages> pages 266-268, </pages> <year> 1969. </year>
Reference-contexts: We implemented two sparse algorithms. The first one, the revised simplex algorithm, can be a good parallel algorithm but has many fill-ins. It is easy to parallelize and can yield good speedup. The second one, the revised simplex algorithm with LU decomposition <ref> [2] </ref>, is one of the best sequential simplex algorithms. It is a sophisticated algorithm with few fill-ins, however, it is extremely difficult to parallelize and to obtain a speedup, especially on distributed memory computers. <p> Consequently, large test data cannot fit in the memory. Another alternative is the method of Bartels and Golub that is based on the LU decomposition of the basis matrix B with row exchanges <ref> [2] </ref>. Compared to the standard one, the new one differs in the way to solve two linear equations: T B = c T With the original Bartels-Golub algorithm used, the LU decomposition needs to be applied for the newly constructed basic matrix B in each iteration. <p> As k increases, the storage of all eta matrices becomes large and the time spent on the E solver is slow too. Therefore, after a certain number of iterations, say r, the basis B 0 needs to be refactorized <ref> [2] </ref>. That is, all the eta matrixes will be discarded. The current B k is treated as a new B 0 to be decomposed. Such an LU decomposition needs to be conducted every r iterations to keep the time and storage of E solver within an acceptable range.
Reference: [3] <author> G. B. Dantzig. </author> <title> Linear Programming and Extensions, </title> <publisher> Princeton University Press, </publisher> <address> New Jersey, </address> <year> 1963. </year>
Reference-contexts: The execution is carried out iteratively. At each iteration, it improves the value of the objective function. This procedure will terminate after a finite number of iterations. The simplex method was first designed in 1947 by Dantzig <ref> [3] </ref>. The revised simplex method is a modification of the original one which significantly reduces the total number of calculations to be performed at each iteration [4]. In this paper, we will use the revised simplex algorithm to solve the linear programming problem.
Reference: [4] <author> G. B. Dantzig and W. Orchard-Hays, </author> <title> The product form for the inverse in the simplex method, Mathematical Tables and Other Aids to Computation, </title> <booktitle> 8, </booktitle> <pages> pages 64-67, </pages> <year> 1954. </year>
Reference-contexts: This procedure will terminate after a finite number of iterations. The simplex method was first designed in 1947 by Dantzig [3]. The revised simplex method is a modification of the original one which significantly reduces the total number of calculations to be performed at each iteration <ref> [4] </ref>. In this paper, we will use the revised simplex algorithm to solve the linear programming problem. Parallel implementations of the linear programming algorithms have been studied on different machine architectures, including both distributed and shared memory parallel machines.
Reference: [5] <author> R. A. Finkel, </author> <title> Large-grain Parallelism Three Cases Studies, The Characteristics of Parallel Algorithms, </title> <editor> L. H. Jamieson ed., </editor> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: The performance of parallel simplex algorithms was studied on the Sequent Balance shared memory machine and a 16-processor Transputer system from INMOS Corporation [12]. The parallel simplex algorithms for a loosely coupled message-passing based parallel systems were presented in <ref> [5] </ref>. The implementations fl This work appeared in the proceeding of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, March 22-24, 1993, Norfolk.
Reference: [6] <author> H. F. Ho, G. H. Chen, S. H. Lin, and J. P. Sheu, </author> <booktitle> Solving Linear Programming on Fixed-Size Hypercubes, </booktitle> <pages> pages 112-116, ICPP'88. </pages>
Reference-contexts: This research was partially supported by NSF grants CCR-9109114 and CCR-8809165 2 Shu and Wu of simplex algorithm on the fixed-size hypercubes was proposed by Ho et al. <ref> [6] </ref>. The performance of the simplex and revised simplex method on the Intel iPSC/2 hypercube was examined in [11]. All existing works are for dense algorithms and there is no sparse implementation. Parallel computers are used to solve large application problems. Large-scale linear programming problems are very sparse.
Reference: [7] <author> N. Karmarkar, </author> <title> A New Polynomial-Time Algorithm for Linear Programming, </title> <journal> Combinatorica, </journal> <volume> Vol. 4, No. 8, </volume> <year> 1984, </year> <pages> pages 373-395. </pages>
Reference-contexts: Although it is well-known that the computational complexity of the simplex method is not polynomial in the number of equations, in practice it can quickly solve many linear programming problems. The interior-point algorithms are polynomial and potentially easy to parallelize <ref> [7, 1] </ref>. However, since the interior-point algorithm has not been well-developed yet, the simplex algorithm is still applied to most real applications. Given a linear programming problem, the simplex method starts from an initial feasible solution and moves toward the optimal solution. The execution is carried out iteratively.
Reference: [8] <author> Y. Li, M. Wu, W. Shu and G. Fox, </author> <title> Linear Programming Algorithms and Parallel Implementations, </title> <type> SCCS Report 288, </type> <institution> Syracuse University, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: In most practical problems, some or all constraints may be specified by linear inequalities, which can be easily converted to linear equality constraints with the addition of nonnegative slack variables. Similarly, artificial variables may be added to maintain x nonnegative <ref> [8] </ref>. We use the two-phase revised simplex method, in which the artificial variables are driven to zero in the first phase, and an optimal solution is obtained in the second phase.
Reference: [9] <author> D. M. Nicol and J. H. Saltz. </author> <title> An analysis of scatter decomposition. </title> <journal> IEEE Trans. Computers, </journal> <volume> C-39(11):1337-1345, </volume> <month> November </month> <year> 1990. </year>
Reference-contexts: For example, the right portion of matrix A, consisting of slack and artificial variables with only one nonzero element for each column, is usually more sparse compared to the left portion. In order to make computation density evenly distributed, a scatter column partitioning scheme is used <ref> [9] </ref>. Vector is duplicated in all nodes to eliminate communications while doing the multiplication. Besides, vector is scattered into a full-length vector of storage.
Reference: [10] <author> T. Sheu and W. Lin, </author> <title> Mapping Linear Programming Algorithms onto the Butterfly Parallel Processor, </title> <year> 1988. </year>
Reference-contexts: Parallel implementations of the linear programming algorithms have been studied on different machine architectures, including both distributed and shared memory parallel machines. Sheu et al. presented a mapping technique of linear programming problems on the BNN Butterfly parallel computer <ref> [10] </ref>. The performance of parallel simplex algorithms was studied on the Sequent Balance shared memory machine and a 16-processor Transputer system from INMOS Corporation [12]. The parallel simplex algorithms for a loosely coupled message-passing based parallel systems were presented in [5].
Reference: [11] <author> C. B. Stunkel and D. C. Reed, </author> <title> Hypercube Implementation of the Simplex Algorithm, </title> <booktitle> Prof. 4th Conf. on Hypercube Concurrent Computer and Applications, </booktitle> <month> March </month> <year> 1989. </year>
Reference-contexts: This research was partially supported by NSF grants CCR-9109114 and CCR-8809165 2 Shu and Wu of simplex algorithm on the fixed-size hypercubes was proposed by Ho et al. [6]. The performance of the simplex and revised simplex method on the Intel iPSC/2 hypercube was examined in <ref> [11] </ref>. All existing works are for dense algorithms and there is no sparse implementation. Parallel computers are used to solve large application problems. Large-scale linear programming problems are very sparse. Therefore, dense algorithms are not of practical value. We implemented two sparse algorithms.
Reference: [12] <author> Y. Wu and T. G. Lewis, </author> <title> Performance of Parallel Simplex Algorithms, </title> <institution> Department of Computer Science, Oregon State University, </institution> <year> 1988. </year>
Reference-contexts: Sheu et al. presented a mapping technique of linear programming problems on the BNN Butterfly parallel computer [10]. The performance of parallel simplex algorithms was studied on the Sequent Balance shared memory machine and a 16-processor Transputer system from INMOS Corporation <ref> [12] </ref>. The parallel simplex algorithms for a loosely coupled message-passing based parallel systems were presented in [5]. The implementations fl This work appeared in the proceeding of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, March 22-24, 1993, Norfolk.
Reference: [13] <author> M. Wu, and Y. Li, </author> <title> Fast LU Decomposition for Sparse Simplex Method, </title> <booktitle> SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> March </month> <year> 1993. </year>
Reference-contexts: A new algorithm was used to factorize the basis matrix efficiently <ref> [13] </ref>. The idea is to take advantages of the fact that the basis matrix B is extremely sparse and has many column singletons and row singletons. As part of the matrix is phased out by rearranging singletons, the remaining matrix to be factorized may have new singletons generated.
References-found: 13

