URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/barua-lcpc96.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/
Root-URL: 
Title: Communication-Minimal Partitioning of Parallel Loops and Data Arrays for Cache-Coherent Distributed-Memory Multiprocessors  
Author: Rajeev Barua, David Kranz and Anant Agarwal 
Date: July 29, 1996  
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: Harnessing the full performance potential of cache-coherent distributed shared memory multiprocessors without inordinate user effort requires a compilation technology that can automatically manage multiple levels of memory hierarchy. This paper describes a working compiler for such machines that automatically partitions loops and data arrays to optimize locality of access. The compiler implements a solution to the problem of finding communication-minimal partitions of loops and data. Loop and data partitions specify the distribution of loop iterations and array data across processors. A good loop partition maximizes the cache hit rate while a good data partition minimizes remote cache misses. The problems of finding loop and data partitions interact when multiple loops access arrays with differing reference patterns. Our algorithm handles programs with multiple nested parallel loops accessing many arrays with array access indices being general affine functions of loop variables. It discovers communication-minimal partitions when communication-free partitions do not exist. The compiler also uses sub-blocking to handle finite cache sizes. A cost model that estimates the cost of a loop and data partition given machine parameters such as cache, local and remote access timings, is presented. Minimizing the cost as estimated by our model is an NP-complete problem, as is the fully general problem of partitioning. A heuristic method which provides good approximate solutions in polynomial time is presented. The loop and data partitioning algorithm has been implemented in the compiler for the MIT Alewife machine. The paper presents results obtained from a working compiler on a 16-processor machine for three real applications: Tomcatv, Erlebacher, and Conduct. Our results demonstrate that combined optimization of loops and data can result in improvements in runtime by nearly a factor of two over optimization of loops alone.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. G. Abraham and D. E. Hudak. </author> <title> Compile-time partitioning of iterative parallel loops to reduce cache coherency traffic. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 318-328, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Ju and Dietz [11] consider the problem of reducing cache-coherence traffic in bus-based multiprocessors. Their work involves finding a data layout (row or column major) for arrays in a uniform memory access (UMA) environment. We consider finding data partitions for a distributed shared memory NUMA machine. Abraham and Hudak <ref> [1] </ref> look at the problem of automatic loop partitioning for cache locality only for the case when array accesses have simple index expressions. Their method uses only a local per-loop analysis. A more general framework for loop partitioning was presented by Agarwal et. al. [2] for optimizing for cache locality.
Reference: [2] <author> A. Agarwal, D.A. Kranz, and V. Natarajan. </author> <title> Automatic Partitioning of Parallel Loops and Data Arrays for Distributed Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(9) </volume> <pages> 943-962, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: The iterative solution is seeded with an initial partitioning of each individual loop nest that disregards data locality. This initial loop partitioning is found using the method described in <ref> [2] </ref>. The iterative solution is also seeded with an initial data partition. This initial partitioning of each array is chosen to match the partitioning of the largest loop that accesses that array. Thus, by first partitioning each loop for cache locality, the initial seeding favors cache locality over data locality. <p> Abraham and Hudak [1] look at the problem of automatic loop partitioning for cache locality only for the case when array accesses have simple index expressions. Their method uses only a local per-loop analysis. A more general framework for loop partitioning was presented by Agarwal et. al. <ref> [2] </ref> for optimizing for cache locality. That framework handled fully general affine access functions, i.e. accesses of the form A [2i+j,j] and A [100-i,j] were handled. However, that work found local minima for each loop independently, giving possibly conflicting data partitioning requests across loops in NUMA machines. <p> This technique can be used before partitioning when the programming model is sequential to convert to parallel loops, and hence complements our work. 3 3 Overview of the Partitioning Framework This section overviews the notation and framework used for partitioning. The full reference presents the details <ref> [2] </ref>. The reason we describe this framework are two-fold. First, the work in [2] describes how to select loop and data partitions which optimize for cache locality. This is used by our method in computing the initial loop and data partition, used as a starting point for our heuristic search. <p> The full reference presents the details <ref> [2] </ref>. The reason we describe this framework are two-fold. First, the work in [2] describes how to select loop and data partitions which optimize for cache locality. This is used by our method in computing the initial loop and data partition, used as a starting point for our heuristic search. Second, the framework in [2] on how loop and data partitions are specified, is <p> First, the work in <ref> [2] </ref> describes how to select loop and data partitions which optimize for cache locality. This is used by our method in computing the initial loop and data partition, used as a starting point for our heuristic search. Second, the framework in [2] on how loop and data partitions are specified, is used in our cost model in section 4 to find memory access costs considering caches, and local and remote memory. The framework handles programs with loop nests where the array index expressions are affine functions of the loop variables. <p> This number is simply the size of the combined footprint with respect to all the accesses in the loop, assuming an infinitely large cache. A method to get essentially the same result with finite caches is presented in section 4. <ref> [2] </ref> shows how the combined footprint 4 in a UI-set can be computed assuming infinite caches. It also shows how the loop partitioning L can be chosen to minimize the number of cache misses. <p> The only communication for B then, (that is, memory accesses to remote memory), is at the periphery of LG, due to small offsets. This periphery is what was minimized in <ref> [2] </ref>, and is called the peripheral footprint. Indeed, D is chosen as shown above to seed the search process. Data partitioning performed according to the loop partitioning is termed local in our performance results. <p> F f is referred to as the peripheral footprint. See <ref> [2] </ref> for details on how the peripheral footprint is computed. <p> It has 20 loops and 20 arrays, in one and two dimensions. These programs were run using each of three compilation strategies: global Uses the algorithm described in this paper. local Uses the analysis in <ref> [2] </ref> to determine the loop partition, and then partitions each array by using the partition induced by the largest loop that accesses that array, to achieve some data locality.
Reference: [3] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Kenneth Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture (ISCA'95), </booktitle> <pages> pages 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: In contrast, 0-1 integer programming methods inherently try to solve an NP-complete problem, and could suffer from exponential slowdown for any detailed formulation, as discussed in section 2. 6 Results The algorithm described in this paper has been implemented as part of the compiler for the Alewife <ref> [3] </ref> machine. The Alewife machine is a cache-coherent multiprocessor with physically distributed memory. The nodes are configured in a 2-dimensional mesh network.
Reference: [4] <author> Jennifer M. Anderson and Monica S. Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Proceedings of SIGPLAN '93 Conference on Programming Languages Design and Implementation. ACM, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Another view of loop partitioning involving program transformations is presented by Carr et. al. [7]. This paper was focused on uniprocessors but their method could be integrated with data partitioning for multiprocessors as well. The work of Anderson and Lam <ref> [4] </ref> does a global analysis across loops. It finds partitions among a space of those which satisfy a specified system of constraints, in a framework of both sequential and parallel loops. <p> In our method, we use the iterative method to improve on the total communication cost, while maintaining load balance throughout. If no communication free partition exists, one which minimum cost, as determined by a detailed cost model, is found. <ref> [4] </ref> next attempts to use doacross loops to improve performance, but note that is not relevant for our programming model of only parallel loops. (2) Finally, the algorithm in [4] uses a heuristic with a greedy approach to find where data reorganization could be done to reduce communication. <p> If no communication free partition exists, one which minimum cost, as determined by a detailed cost model, is found. <ref> [4] </ref> next attempts to use doacross loops to improve performance, but note that is not relevant for our programming model of only parallel loops. (2) Finally, the algorithm in [4] uses a heuristic with a greedy approach to find where data reorganization could be done to reduce communication. We concentrate on the problem of finding the best static partition. <p> The reorganization heuristic uses a reorganization cost, while we directly minimize loop memory access time, while is more precise. (4) It does not take into account the combined effect on performance of caches and local memories. We optimize quantitatively for both cache and data locality. (5) Unlike in <ref> [4] </ref>, we allow for hyperparallelepiped data tiles, important for achieving good locality in general affine function array accesses. Results on only one program were presented. Bixby, Kennedy and Kremer [6] present a formulation of the problem of finding data layout as a 0-1 integer programming problem. <p> Cyclic partitions could be handled using this method but for simplicity we leave them out. 5.1 Graph formulation Our search procedure uses bipartite graphs to represent loops and data arrays. Bipartite graphs are a popular data structure used to represent partitioning problems for loops and data <ref> [11, 4] </ref>. For a graph G = (V l ; V d ; E), the loops are pictured as a set of nodes V l on the left hand side, and the data arrays as a set of nodes V d on the right. <p> These results indicate that significant performance improvements can be obtained by looking at data locality and cache locality in a global framework. In the future we would like to add the possibility of copying data at runtime to avoid remote references as in <ref> [4] </ref>. This factor could be added to our cost model.
Reference: [5] <author> R. Barua. </author> <title> Addressing Partitioned Arrays in Distributed Memory Multiprocessors the Software Page Translation Approach. </title> <institution> Massachusetts Institute of Technology LCS-TM, </institution> <year> 1996. </year>
Reference-contexts: The last number will be larger for larger machine configurations or when network contention is present. The data partitions specified by the different approaches we compare are implemented by a software page translation approach described in <ref> [5] </ref>. That is an independent piece of work, and in essence, provides an efficient addressing mechanism for any specified data partitions, by closely approximating data tile shapes by linear software pages. We compared performance on the following applications: Tomcatv A code from the SPEC suite. <p> This is implemented by using a feature of the data partition addressing mechanism used <ref> [5] </ref>. In this case, the pages are assigned to the processors in a round-robin manner, effectively generating random placement from the viewpoint of any loop's accesses. This was confirmed by the statistics we collected.
Reference: [6] <author> R. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic Data Layout Using 0-1 Integer Programming. </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT), </booktitle> <pages> pages 111-122, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 1994. </year> <month> 13 </month>
Reference-contexts: We optimize quantitatively for both cache and data locality. (5) Unlike in [4], we allow for hyperparallelepiped data tiles, important for achieving good locality in general affine function array accesses. Results on only one program were presented. Bixby, Kennedy and Kremer <ref> [6] </ref> present a formulation of the problem of finding data layout as a 0-1 integer programming problem. Though the problem is exponential time in the worst case, a case is made why for smaller problem sizes, the solution can be found in a reasonable amount of time. <p> However, a 0-1 integer programming approach is only as good as its formulation. In the case of finding loop and data partitions 0-1 programming may not be the best answer for the following reasons: (1) The formulation in <ref> [6] </ref> solves for data partitions only, which is a simpler problem. <p> An exact solution to a simplified formulation could be inferior to a good heuristic's solution to a more detailed problem. For example, in <ref> [6] </ref>, only two alternative partitions are examined per phase. (A phase in [6] roughly corresponds to a single set of loops nested one inside the other). (3) Hyperparallelepiped partitions are not allowed in [6], necessary for good performance on general affine functions. <p> An exact solution to a simplified formulation could be inferior to a good heuristic's solution to a more detailed problem. For example, in <ref> [6] </ref>, only two alternative partitions are examined per phase. (A phase in [6] roughly corresponds to a single set of loops nested one inside the other). (3) Hyperparallelepiped partitions are not allowed in [6], necessary for good performance on general affine functions. Gupta and Banerjee [9] have developed an algorithm for partitioning doing a global analysis across loops. <p> For example, in <ref> [6] </ref>, only two alternative partitions are examined per phase. (A phase in [6] roughly corresponds to a single set of loops nested one inside the other). (3) Hyperparallelepiped partitions are not allowed in [6], necessary for good performance on general affine functions. Gupta and Banerjee [9] have developed an algorithm for partitioning doing a global analysis across loops. They allow simple index expression accesses of the form c 1 fl i + c 2 , but not general affine functions.
Reference: [7] <author> Steve Carr, Kathryn S. McKinley, and Chau-Wen Tseng. </author> <title> Compiler Optimization for Improving Data Locality. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 252-262, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: However, that work found local minima for each loop independently, giving possibly conflicting data partitioning requests across loops in NUMA machines. Another view of loop partitioning involving program transformations is presented by Carr et. al. <ref> [7] </ref>. This paper was focused on uniprocessors but their method could be integrated with data partitioning for multiprocessors as well. The work of Anderson and Lam [4] does a global analysis across loops.
Reference: [8] <author> M. Cierniak and W. Li. </author> <title> Unifying Data and Control Transformations for Distributed Shared-Memory Machines. </title> <booktitle> Proceedings of the SIGPLAN PLDI, </booktitle> <year> 1995. </year>
Reference-contexts: In the case of finding loop and data partitions 0-1 programming may not be the best answer for the following reasons: (1) The formulation in [6] solves for data partitions only, which is a simpler problem. It been widely recognized <ref> [8] </ref> that for good performance, data and loop partitions need to be found simultaneously, not one following another. (2) For 0-1 formulations in general, to get one with few enough variables so as to avoid an exponential increase in solution time, we may need to simplify the problem.
Reference: [9] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of Automatic Data Partitioning Techniques for Parallelizing Compilers on Multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: For example, in [6], only two alternative partitions are examined per phase. (A phase in [6] roughly corresponds to a single set of loops nested one inside the other). (3) Hyperparallelepiped partitions are not allowed in [6], necessary for good performance on general affine functions. Gupta and Banerjee <ref> [9] </ref> have developed an algorithm for partitioning doing a global analysis across loops. They allow simple index expression accesses of the form c 1 fl i + c 2 , but not general affine functions.
Reference: [10] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD Distributed Memory Machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: One approach to the problem is to have programmers specify data partitions explicitly in the program, as in Fortran-D <ref> [10, 15] </ref>. Loop partitions are usually determined by the owner computes rule. Though simple to implement, this requires the user to thoroughly understand the access patterns of the program, a task which is not trivial even for small programs.
Reference: [11] <author> Y. Ju and H. Dietz. </author> <title> Reduction of Cache Coherence Overhead by Compiler Data Layout and Loop Transformation. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 344-358, </pages> <publisher> Springer Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Their theory produces communication-free hyperplane partitions for loops with affine index expressions when such partitions exist. However, when communication-free partitions do not exist, they deal only with index expressions of the form variable plus a constant. Ju and Dietz <ref> [11] </ref> consider the problem of reducing cache-coherence traffic in bus-based multiprocessors. Their work involves finding a data layout (row or column major) for arrays in a uniform memory access (UMA) environment. We consider finding data partitions for a distributed shared memory NUMA machine. <p> Cyclic partitions could be handled using this method but for simplicity we leave them out. 5.1 Graph formulation Our search procedure uses bipartite graphs to represent loops and data arrays. Bipartite graphs are a popular data structure used to represent partitioning problems for loops and data <ref> [11, 4] </ref>. For a graph G = (V l ; V d ; E), the loops are pictured as a set of nodes V l on the left hand side, and the data arrays as a set of nodes V d on the right.
Reference: [12] <author> Kathleen Knobe, Joan Lukas, and Guy Steele Jr. </author> <title> Data Optimization: Allocation of Arrays to Reduce Communication on SIMD Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: They allow simple index expression accesses of the form c 1 fl i + c 2 , but not general affine functions. They do not allow for the possibility of hyperparallelepiped data tiles, and do not account for caches. Knobe, Lucas and Steele <ref> [12] </ref> give a method of allocating arrays on SIMD machines. They align arrays to minimize communication for vector instructions, which access array regions specified by subranges on each dimension.
Reference: [13] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-Time Techniques for Data Distribution in Distributed Memory Machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 472-482, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: For real medium-sized or large programs, the task is a very difficult one. Presence of fully general 2 affine function accesses further complicates the process. Worse, the program would not be portable across machines with different architectural parameters. Ramanujam and Sadayappan <ref> [13] </ref> consider data partitioning in multicomputers and use a matrix formulation; their results do not apply to multiprocessors with caches. Their theory produces communication-free hyperplane partitions for loops with affine index expressions when such partitions exist.
Reference: [14] <author> Bart Selman, Henry Kautz, and Bram Cohen. </author> <title> Noise Strategies for Improving Local Search. </title> <booktitle> Proceedings, AAAI, </booktitle> <volume> 1, </volume> <year> 1994. </year>
Reference-contexts: Extensive work evaluating search techniques has been done by researchers in many disciplines. Simulated 7 annealing, gradient descent and genetic algorithms are some of these. See <ref> [14] </ref> for a comparison of some methods. All techniques rely on a cost function estimating some objective value to be optimized, and a search strategy. For specific problems more may be known than in the general case, and specific strategies may do better.
Reference: [15] <author> C.-W. Tseng. </author> <title> An Optimizing Fortran D compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> Jan </month> <year> 1993. </year> <note> Published as Rice COMP TR93-199. </note>
Reference-contexts: One approach to the problem is to have programmers specify data partitions explicitly in the program, as in Fortran-D <ref> [10, 15] </ref>. Loop partitions are usually determined by the owner computes rule. Though simple to implement, this requires the user to thoroughly understand the access patterns of the program, a task which is not trivial even for small programs.
Reference: [16] <author> R.P. Wilson, R.S. French, C.S. Wilson, S.P. Amarasinghe, J.M. Anderson, S.W.K. Tjiang, S.-W. Liao, C.-W. Tseng, M.W. Hall, M.S. Lam, and J.L. Hennessy. </author> <title> SUIF: An Infrastructure for Research on Parallelizing and Optimizing Compilers. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29(12) </volume> <pages> 31-37, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Formulating compiler problems as 0-1 integer programming problems is an exciting new approach, also used by the Stanford SUIF compiler <ref> [16] </ref>. However, a 0-1 integer programming approach is only as good as its formulation. In the case of finding loop and data partitions 0-1 programming may not be the best answer for the following reasons: (1) The formulation in [6] solves for data partitions only, which is a simpler problem.
Reference: [17] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Loop Transformation Theory and an Algorithm to Maximize Parallelism. </title> <booktitle> In The Third Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1990. </year> <institution> Irvine, </institution> <address> CA. </address> <month> 14 </month>
Reference-contexts: Knobe, Lucas and Steele [12] give a method of allocating arrays on SIMD machines. They align arrays to minimize communication for vector instructions, which access array regions specified by subranges on each dimension. Wolf and Lam <ref> [17] </ref> deal with the problem of taking sequential nested loops and applying transformations to attempt to convert them to a nest of parallel loops with at most one outer sequential loop.
References-found: 17

