URL: http://www.cs.tamu.edu/faculty/bhuyan/papers/fm+tree.ps
Refering-URL: http://www.cs.tamu.edu/faculty/bhuyan/
Root-URL: http://www.cs.tamu.edu
Email: E-mail: fychang, bhuyang@cs.tamu.edu  
Title: An Efficient Hybrid Cache Coherence Protocol for Shared Memory Multiprocessors  
Author: Yeimkuan Chang and Laxmi N. Bhuyan 
Address: College Station, Texas 77843-3112  
Affiliation: Department of Computer Science Texas A&M University  
Abstract: This paper presents a new tree-based cache coherence protocol which is a hybrid of the limited directory and the linked list schemes. By utilizing a limited number of pointers in the directory, the proposed protocol connects the nodes caching a shared block in a tree fashion. In addition to the low communication overhead, the proposed scheme also contains the advantages of the existing bit-map and tree-based linked list protocols, namely, scalable memory requirement and logarithmic invalidation latency. We evaluate the performance of our protocol by running four applications on an execution-driven simulator. Our simulation results show that the performance of the proposed protocol is very close to that of the full-map directory protocol. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. J. Lilja, </author> <title> "Cache coherence in large-scale shared-memory multiprocessors: Issues and comparisons," </title> <journal> ACM Computing Surveys, </journal> <pages> pp. 303-338, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Several cache coherence schemes have been proposed to solve the cache consistency problem in shared memory multiprocessors <ref> [1] </ref>. Most of the popular cache coherence protocols are based on snooping on the bus that connects the processing elements to the memory modules [2]. But the obvious limitation to such schemes is the limited number of processors that can be supported by a single bus.
Reference: [2] <author> Q. Yang and L.N. Bhuyan, </author> <title> "Analysis and Comparison of Cache Coherence Protocols for a Packet-Switched Multiprocessor" In IEEE Transactions on Computers, </title> <journal> vol. </journal> <volume> 38, no. 8, </volume> <pages> pp. 1143-1153, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Several cache coherence schemes have been proposed to solve the cache consistency problem in shared memory multiprocessors [1]. Most of the popular cache coherence protocols are based on snooping on the bus that connects the processing elements to the memory modules <ref> [2] </ref>. But the obvious limitation to such schemes is the limited number of processors that can be supported by a single bus. The single bus becomes the bottleneck in the system.
Reference: [3] <author> D. Chaiken et.al., </author> <title> "Directory-Based Cache Coherence in Large-scale Multiprocessors," </title> <journal> Computer, </journal> <volume> vol. 23, no. 6, </volume> <pages> pp. 49-58, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Since the broadcast procedure generates a lot of traffic on networks, non-broadcast based directory protocols are used to implement cache coherence Full-map and linked list schemes are two categories of directory protocols <ref> [3, 4] </ref>. The full-map directory scheme maintains a bit map which contains the information about which node in the system has a shared copy of an associated block. <p> Also, the latency of cache transactions is usually larger since these systems do not have a broadcasting medium like a shared bus to send invalidation signals. The limited directory approach <ref> [3, 5] </ref> limits the number of pointers associated with each block in order to keep the directory size manageable. However, this approach also limits the number of processors that can share a block. The existing schemes are discussed in more detail in Section 2 of this paper. <p> Finally, concluding remarks are presented in Section 5. 2 Discussion on Existing Schemes Existing directory schemes fall into two categories, namely bit-map and linked list protocols. A nomenclature, Dir i X, was introduced in <ref> [3] </ref> for bit-map coherence protocols. The index i in Dir i X represents the number of pointers for recording the owners of shared copies, and X is either B or NB depending on whether a broadcast is issued when the pointers overflow. <p> Two limited directory schemes, Dir i B and Dir i NB, have been proposed in the literature <ref> [3] </ref>. The broadcast scheme Dir i B employs an overflow bit to handle pointer overflow. If there is no pointer in the directory available for subsequent requests, the overflow bit is set.
Reference: [4] <editor> IEEE, IEEE Std 1596-1992: </editor> <title> IEEE Standard for Scalable Coherent Interface, </title> <publisher> IEEE, Inc., </publisher> <address> 345 East 47th Street, New York, NY 10017, USA., </address> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Since the broadcast procedure generates a lot of traffic on networks, non-broadcast based directory protocols are used to implement cache coherence Full-map and linked list schemes are two categories of directory protocols <ref> [3, 4] </ref>. The full-map directory scheme maintains a bit map which contains the information about which node in the system has a shared copy of an associated block. <p> One way to reduce the storage overhead in the directory scheme is to use linked lists instead of a sparsely filled table to keep track of multiple copies of a block. The IEEE Scalable Coherent Interface (SCI) standard project <ref> [4, 6] </ref> applies this approach to define a scalable cache coherence protocol. In this approach the storage overhead is minimal, but maintaining the linked list is complex and time consuming. <p> The subscript i in Dir i represents the number of pointers in the directory and subscript k in Tree k represents the number of pointers in the tree structure. For example, Stanford's singly linked list protocol [6] and SCI <ref> [4] </ref> belong to Dir 1 Tree 1 because they have a single pointer in the directory pointing to the head of the list. <p> The directory memory requirement for this protocol is (C +B)n log n bits, where B and C are the numbers of memory and cache blocks in each node. Scalable Coherent Interface Scalable Coherent Interface (SCI) is an IEEE standard (P1596) <ref> [4] </ref>. It is based on a doubly linked list. On a read miss, the reading cache sends a request to the memory. If the list is empty, the memory points to the requester and supplies the data. Otherwise, the old head of the list is returned to the requester.
Reference: [5] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal, </author> <title> "LimitLESS Directories: A Scalable Cache Coherence Scheme," </title> <booktitle> ASPLOS-IV Proceedings, </booktitle> <pages> pp. 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Also, the latency of cache transactions is usually larger since these systems do not have a broadcasting medium like a shared bus to send invalidation signals. The limited directory approach <ref> [3, 5] </ref> limits the number of pointers associated with each block in order to keep the directory size manageable. However, this approach also limits the number of processors that can share a block. The existing schemes are discussed in more detail in Section 2 of this paper. <p> This scheme does not perform well when the number of shared copies is much greater than the number of the pointers. In LimitLESS i <ref> [5] </ref> and Dir 1 SW [9], the pointer overflow problem is solved by software. All the pointers that can not fit into the limited hardware-supported directory space are stored in traditional memory by the software handler.
Reference: [6] <author> M. Thapar, B. Delagi, and M. J. Flynn, </author> <title> "Linked List Cache Coherence for Scalable Shared Memory Multiprocessors," </title> <booktitle> In Proc. International Parallel Processing Symposium (IPPS), </booktitle> <pages> pp. 34-43, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: One way to reduce the storage overhead in the directory scheme is to use linked lists instead of a sparsely filled table to keep track of multiple copies of a block. The IEEE Scalable Coherent Interface (SCI) standard project <ref> [4, 6] </ref> applies this approach to define a scalable cache coherence protocol. In this approach the storage overhead is minimal, but maintaining the linked list is complex and time consuming. <p> The subscript i in Dir i represents the number of pointers in the directory and subscript k in Tree k represents the number of pointers in the tree structure. For example, Stanford's singly linked list protocol <ref> [6] </ref> and SCI [4] belong to Dir 1 Tree 1 because they have a single pointer in the directory pointing to the head of the list. <p> All the pointers that can not fit into the limited hardware-supported directory space are stored in traditional memory by the software handler. The delay in calling the software handler is their major disadvantage. 2.2 Linked List Schemes Singly Linked List Protocol In this protocol <ref> [6] </ref>, a list of pointers is kept in the processors caches instead of main memory. Each shared block only keeps a pointer to a node which contains a valid copy of the block.
Reference: [7] <author> H. Nilsson and P. Stenstrom, </author> <title> "The Scalable Tree Protocol A Cache Coherence Approach for Large-Scale Multiprocessors," </title> <booktitle> In Proc. International Symposium on Parallel and Distributed Processing, </booktitle> <pages> pp. 498-506, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: The protocol is oblivious of the underlying interconnection network and therefore, a request may be forwarded to a distant node although it could have been satisfied by a neighboring node. The major disadvantage is the sequential nature of the invalidation process for write misses. The scalable tree protocol (STP) <ref> [7] </ref> and the SCI tree extension protocol [8] were proposed to reduce the latency of write misses. The low latency of read misses is sacrificed in order to construct a balanced tree connecting all the shared copies of a cache block. <p> The index i of Dir i Tree k represents the number of nodes having shared copies in their local caches. STP <ref> [7] </ref> belongs to Dir 2 Tree k because it maintains a k-ary tree and keeps pointers to the root of the tree and the latest node joining the tree. <p> It takes 2P messages to invalidate a list of P cached copies. Adding the four messages for inserting itself as a new head, the requester takes 2P + 4 messages to get the write permission. Scalable Tree Protocol The scalable Tree Protocol (STP) <ref> [7] </ref> uses a top-down approach to construct a balanced tree. Take a binary tree as an example. The first node issuing a read request to a specific memory block will be the root of the tree.
Reference: [8] <author> R. E. Johnson, </author> <title> Extending the Scalable Coherent Interface for Large-Scale Shared-Memory Multiprocessors, </title> <type> PhD thesis, </type> <institution> University of Wisconsin-Madison, </institution> <year> 1993. </year>
Reference-contexts: The major disadvantage is the sequential nature of the invalidation process for write misses. The scalable tree protocol (STP) [7] and the SCI tree extension protocol <ref> [8] </ref> were proposed to reduce the latency of write misses. The low latency of read misses is sacrificed in order to construct a balanced tree connecting all the shared copies of a cache block. <p> STP [7] belongs to Dir 2 Tree k because it maintains a k-ary tree and keeps pointers to the root of the tree and the latest node joining the tree. Similarly, the SCI tree extension (P1596.2 <ref> [8] </ref>) belongs to Dir 2 Tree 2 because it maintains a balanced binary tree and keeps two pointers, one to the root of the tree and the other to the head (latest node joining the tree). <p> Since most of the requests in an application are read misses, the protocol performs poorly when the degree of data sharing or write misses is low. SCI Tree Extension This scheme is proposed as an IEEE standard extension (P1596.2) of SCI <ref> [8] </ref>. It constructs a balanced tree by using AVL tree algorithm. This scheme has a read miss overhead similar to STP. Thus, it does not perform well for the applications with a low degree of data sharing and less frequent write misses.
Reference: [9] <author> M. Hill and et al., </author> <title> "Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors," </title> <booktitle> ASPLOS-V Proceedings, </booktitle> <pages> pp. 262-273, </pages> <month> Oc-tober </month> <year> 1992. </year>
Reference-contexts: This scheme does not perform well when the number of shared copies is much greater than the number of the pointers. In LimitLESS i [5] and Dir 1 SW <ref> [9] </ref>, the pointer overflow problem is solved by software. All the pointers that can not fit into the limited hardware-supported directory space are stored in traditional memory by the software handler.
Reference: [10] <author> W.-D. Weber and A. Gupta, </author> <title> "Analysis of Cache Invalidation patterns in Multiprocesors," </title> <booktitle> ASPLOS-III Proceedings, </booktitle> <pages> pp. 243-256, </pages> <year> 1989. </year>
Reference-contexts: The memory requirement is B n 2i log n + C k log n in an n-node system, where B and C are the numbers of memory and cache blocks per node, respectively. Dir 4 Tree 2 . The empirical results in <ref> [10] </ref> suggest that in many applications, the number of shared copies of a cache block is lower than four, regardless of the system size. Thus, we feel comfortabe in using i = 4 and k = 2 to construct binary trees in this study. <p> The message type Replace IN V is used for replacement operation to distinguish IN V generated by write misses because no acknowledgment is needed for replacement. The rationale of doing this is as follows. First, as noted in <ref> [10] </ref>, most of the time, the number of shared copies of a memory block is less than four. Thus, our replacement operations will perform as well as the bit-map scheme because the replaced cache block does not have any child most of the time.
Reference: [11] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl, "PROTEUS: </author> <title> A High-Performance Parallel Architecture Simulator," </title> <type> Technical Report MIT/ICS/TR516, </type> <institution> MIT, </institution> <year> 1991. </year>
Reference-contexts: Size 8 bytes Cache Associativity Fully Associative Network type binary n-cube Network Size 8, 16, 32 processors Network bandwidth 8 bits Switch/Wire Delay 1 cycle Memory Access Latency 5 cycles Cache Access Latency 1 cycle Table 5: Simulation Model. 4.1 Simulation Methodology We ported the proposed coherence scheme to Proteus <ref> [11] </ref> which is an execution driven simulator for shared memory multiprocessors. The simulator can be configured to either bus-based or k-ary n-cube networks. The networks use a wormhole routing technique. The specification of the simulated network and the cache memory is given in Table 5.
Reference: [12] <author> J. P. Singh, W. D. Weber, and A. Gupta, </author> <title> "SPLASH: Stanford Parallel Applications for Shared Memory," </title> <type> Technical Report CSL-TR-92-526, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: The full map scheme is denoted by fm, the limited directory schemes by L8, L4 L2, L1 and the Dir i T ree 2 scheme is represented by 8, 4, 2 and 1. MP3D: The MP3D application is taken from the SPLASH parallel benchmark suite <ref> [12] </ref>. It is a 3-dimensional particle simulation program used in the study of rarefied fluid flow problems. MP3D is notorious for its low speedups. For our simulation, we used 3000 particles and ran the application in 10 steps. <p> In a 32-node system, the performance of Dir 2 Tree 2 and Dir 4 Tree 2 are better than all other schemes. LU Decomposition: The LU application is also taken from the SPLASH parallel benchmark suite <ref> [12] </ref>. It is a parallel version of dense blocked LU factorization without pivoting. The data structure includes two dimensional arrays in which the first dimension is the block to be operated on, and the second contains all the data points in that block.
References-found: 12

