URL: http://www.iscs.nus.sg/~liuh/tai95.ps
Refering-URL: http://www.ai.mit.edu/people/jude/research/afspaper.html
Root-URL: 
Email: fliuh,rudysg@iscs.nus.sg  
Title: Chi2: Feature Selection and Discretization of Numeric Attributes  
Author: Huan Liu and Rudy Setiono 
Address: Ridge, Singapore 0511  
Affiliation: Department of Information Systems and Computer Science National University of Singapore Kent  
Abstract: Discretization can turn numeric attributes into discrete ones. Feature selection can eliminate some irrelevant attributes. This paper describes Chi2, a simple and general algorithm that uses the 2 statistic to discretize numeric attributes repeatedly until some inconsistencies are found in the data, and achieves feature selection via discretization. The empirical results demonstrate that Chi2 is effective in feature selection and discretization of numeric and ordinal attributes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Almuallim and T.G. Dietterich. </author> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <journal> Artificial Intelligence, </journal> <volume> 69(1-2):279-305, </volume> <month> November </month> <year> 1994. </year>
Reference-contexts: The basic two types of data are nominal (e.g., attribute color may have values of red, green, yellow) and ordinal (e.g., attribute winning position can have values of 1, 2, and 3, or attribute salary can have 22345.00, 46543.89, etc. as its values). Many feature selection algorithms <ref> [1, 3, 5] </ref> are shown to work effectively on discrete data or even more strictly, on binary data (and/or binary class value). In order to deal with numeric attributes, a common practice for those algorithms is to discretize the data before conducting feature selection. <p> The reasons for our choice are 1. C4.5 (or ID3) works well for many problems and is well known, thus requiring no further description; and 2. C4.5 selects relevant features by itself in tree branching so it can be used as a benchmark, as in <ref> [5, 9, 1] </ref>, to verify the effects of Chi2. In the second set of experiments, we have a closer examination of Chi2's ability of discretization and feature selection by introducing a synthetic data set and adding noise attributes to the existing data set.
Reference: [2] <author> J. Catlett. </author> <title> On changing continuous attributes into ordered discrete attributes. </title> <booktitle> In European Working Session on Learning, </booktitle> <year> 1991. </year>
Reference-contexts: This paper provides a way to select features directly from numeric attributes while discretizing them. Numeric data are very common in real world problems. However, many classification algorithms require that the training data contain only discrete attributes, and some would work better on discretized or binarized data <ref> [2, 4] </ref>. If those numeric data can be automatically transformed into discrete ones, these classification algorithms would be readily at our disposal. Chi2 is our effort towards this goal: discretize the numeric attributes as well as select features among them.
Reference: [3] <author> U.M. Fayyad and K.B. Irani. </author> <title> The attribute selection problem in decision tree generation. </title> <booktitle> In AAAI-92, Proceedings Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 104-110. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: The basic two types of data are nominal (e.g., attribute color may have values of red, green, yellow) and ordinal (e.g., attribute winning position can have values of 1, 2, and 3, or attribute salary can have 22345.00, 46543.89, etc. as its values). Many feature selection algorithms <ref> [1, 3, 5] </ref> are shown to work effectively on discrete data or even more strictly, on binary data (and/or binary class value). In order to deal with numeric attributes, a common practice for those algorithms is to discretize the data before conducting feature selection.
Reference: [4] <author> R. Kerber. Chimerge: </author> <title> Discretization of numeric attributes. </title> <booktitle> In AAAI-92, Proceedings Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 123-128. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: This paper provides a way to select features directly from numeric attributes while discretizing them. Numeric data are very common in real world problems. However, many classification algorithms require that the training data contain only discrete attributes, and some would work better on discretized or binarized data <ref> [2, 4] </ref>. If those numeric data can be automatically transformed into discrete ones, these classification algorithms would be readily at our disposal. Chi2 is our effort towards this goal: discretize the numeric attributes as well as select features among them. <p> This work stems from Kerber's ChiMerge <ref> [4] </ref> which is designed to discretize numeric attributes based on the 2 statistic. ChiMerge consists of an initialization step and a bottom-up merging process, where intervals are continuously merged until a termination condition, which is determined by a significance level ff (set manually), is met. <p> The above process is repeated with a decreased sigLevel until an inconsistency rate, ffi is exceeded in the discretized data. Phase 1 is, as a matter of fact, a generalized version of ChiMerge of Kerber <ref> [4] </ref>. Instead of specifying a 2 threshold, Chi2 wraps up ChiMerge with a loop that automatically increments the 2 threshold (decrementing sigLevel). A consistency checking is also introduced as a stopping criterion in order to guarantee that the discretized data set accurately represents the original one.
Reference: [5] <author> K. Kira and L.A. Rendell. </author> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In AAAI-92, Proceedings Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 129-134. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction Feature selection is a task to select the minimum number of attributes needed to represent the data accurately. By using relevant features, classification algorithms can in general improve their predictive accuracy, shorten the learning period, and result in the simpler concepts. There are abundant feature selection algorithms <ref> [5] </ref>. Our work adopts an approach that selects a subset of the original attributes since it not only has the above virtues, but also serves as an indicator on what kind of data (along those selected features) should be collected. <p> The basic two types of data are nominal (e.g., attribute color may have values of red, green, yellow) and ordinal (e.g., attribute winning position can have values of 1, 2, and 3, or attribute salary can have 22345.00, 46543.89, etc. as its values). Many feature selection algorithms <ref> [1, 3, 5] </ref> are shown to work effectively on discrete data or even more strictly, on binary data (and/or binary class value). In order to deal with numeric attributes, a common practice for those algorithms is to discretize the data before conducting feature selection. <p> The reasons for our choice are 1. C4.5 (or ID3) works well for many problems and is well known, thus requiring no further description; and 2. C4.5 selects relevant features by itself in tree branching so it can be used as a benchmark, as in <ref> [5, 9, 1] </ref>, to verify the effects of Chi2. In the second set of experiments, we have a closer examination of Chi2's ability of discretization and feature selection by introducing a synthetic data set and adding noise attributes to the existing data set. <p> This phase of Chi2 accomplishes feature selection. Another feature of Chi2 is that it can be applied to data with mixed attributes (e.g., Heart Disease Data). In addition, Chi2 can work with multi-calss data. This is an advantage over some statistic-based feature selection algorithms such as Relief <ref> [5] </ref> which is applicable only to the two-class data.
Reference: [6] <author> H. Liu and R. Setiono. </author> <title> Discretization of ordinal attributes and feature selection. </title> <type> Technical Report TRB4/95, </type> <institution> Department of Info Sys and Comp Sci, National University of Singapore, </institution> <month> April </month> <year> 1995, </year> <note> "http: //www.iscs.nus.sg/ ~ liuh/chi2.ps". </note>
Reference-contexts: In addition, Chi2 can work with multi-calss data. This is an advantage over some statistic-based feature selection algorithms such as Relief [5] which is applicable only to the two-class data. Other issues such as selecting ffi, limitations of Chi2 as well as its computational complexity can be found in <ref> [6] </ref>. 5 Conclusion Chi2 is a simple and general algorithm that can automatically select a proper 2 value, determine the intervals of a numeric attribute, as well as select features according to the characteristics of the data.
Reference: [7] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: In the first set of experiments, we want to establish that 1. Chi2 helps improve predictive accuracy; and 2. Chi2 properly and effectively discretizes data as well as eliminates some irrelevant attributes. C4.5 [8] (an extension of ID3 <ref> [7] </ref>) is used to verify the effectiveness of Chi2. The reasons for our choice are 1. C4.5 (or ID3) works well for many problems and is well known, thus requiring no further description; and 2.
Reference: [8] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: In the first set of experiments, we want to establish that 1. Chi2 helps improve predictive accuracy; and 2. Chi2 properly and effectively discretizes data as well as eliminates some irrelevant attributes. C4.5 <ref> [8] </ref> (an extension of ID3 [7]) is used to verify the effectiveness of Chi2. The reasons for our choice are 1. C4.5 (or ID3) works well for many problems and is well known, thus requiring no further description; and 2.
Reference: [9] <author> H. Ragavan and L. Rendell. </author> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> In Machine Learning: Proceedings of the Seventh International Conference, </booktitle> <pages> pages 252-259. </pages> <publisher> Morgan Kaufmann Pub. </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: The reasons for our choice are 1. C4.5 (or ID3) works well for many problems and is well known, thus requiring no further description; and 2. C4.5 selects relevant features by itself in tree branching so it can be used as a benchmark, as in <ref> [5, 9, 1] </ref>, to verify the effects of Chi2. In the second set of experiments, we have a closer examination of Chi2's ability of discretization and feature selection by introducing a synthetic data set and adding noise attributes to the existing data set.
References-found: 9

