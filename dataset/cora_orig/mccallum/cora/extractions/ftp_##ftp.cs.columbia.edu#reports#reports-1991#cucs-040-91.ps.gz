URL: ftp://ftp.cs.columbia.edu/reports/reports-1991/cucs-040-91.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1991.html
Root-URL: http://www.cs.columbia.edu
Title: Parallel Dynamic Programming  
Author: Zvi Galil 
Address: New York, 10027, USA and Tel-Aviv University, Tel-Aviv, Israel Kunsoo Park  King's College London Strand, London WC2R 2LS, UK  
Affiliation: Department of Computer Science Columbia University,  Department of Computing  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> Aggarwal, A., and Park, J. </author> <title> Parallel searching in multidimensional monotone arrays. </title> <type> Manuscript. </type>
Reference-contexts: It is this increased dependency that makes efficient parallel algorithms hard to find for the four problems we consider. Sequential algorithms for the five problems are surveyed in [9]. For the edit distance problem Apostolico et al. [3] and Aggarwal and Park <ref> [1] </ref> gave O (log n log m) time algorithms with mn= log m CREW processors. The processor bounds of their algorithms differ slightly in the case of CRCW processors. <p> Lemma 1. Solving Recurrence 1 reduces to the problem of finding the shortest paths in G from 0 to all vertices. Proof . We prove by induction that D [j] = D [0] + f (j) for 1 j n. It is obvious that D <ref> [1] </ref> = D [0] + f (1), since f (1) = w (0; 1). Suppose that D [i] = D [0] + f (i) for all i &lt; j. <p> Now we compute each diagonal of D separately. Note that D [i; j] = minfs + D [i 1; j 1]; f 0 (i) + f (j)g, and this is the prefix computation [6,14]. Suppose we compute D [0; 0]; D <ref> [1; 1] </ref>; . . . ; D [n; n] and n + 1 is a power of 2 for simplicity. Let v i = f 0 (i) + f (i) for 0 i n and k = 1 initially. 1.
Reference: 2. <author> Aho. A. V., Hopcroft, J. E., and Ullman, J. D. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: These approaches are based on well-known methods, but go further to improve the total number of operations. In Sections 4, 5, 6 we use the closure method for Problems 2, 3, 4. 2. The Closure Method 2.1. Closed semirings Aho et al. <ref> [2] </ref> introduced a closed semiring SR = (S; +; ; 0; 1), where S is a set of elements, + and are binary operations on S, and 0; 1 are additive and multiplicative identities, respectively.
Reference: 3. <author> Apostolico, A., Atallah, M. J., Larmore, L. L., and McFaddin, S. </author> <title> Efficient parallel algorithms for string editing and related problems. </title> <journal> SIAM J. Comput. </journal> <volume> 19 (1990), </volume> <pages> 968-988. </pages>
Reference-contexts: It is this increased dependency that makes efficient parallel algorithms hard to find for the four problems we consider. Sequential algorithms for the five problems are surveyed in [9]. For the edit distance problem Apostolico et al. <ref> [3] </ref> and Aggarwal and Park [1] gave O (log n log m) time algorithms with mn= log m CREW processors. The processor bounds of their algorithms differ slightly in the case of CRCW processors.
Reference: 4. <author> Brent, R. P. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> J. Assoc. Com-put. Mach. </journal> <volume> 21 (1974), </volume> <pages> 201-206. </pages>
Reference-contexts: The CREW (concurrent read exclusive write) PRAM allows several processors to read the same memory cell at once, but disallows concurrent writes to a cell. The following theorem by Brent <ref> [4] </ref> is useful in analyzing parallel algorithms, since it allows us to count only the time and total number of operations. 4 Theorem 1. [4] If a parallel computation can be performed in time t using q operations, then it can be performed in time t + (q t)=p using p <p> The following theorem by Brent <ref> [4] </ref> is useful in analyzing parallel algorithms, since it allows us to count only the time and total number of operations. 4 Theorem 1. [4] If a parallel computation can be performed in time t using q operations, then it can be performed in time t + (q t)=p using p processors.
Reference: 5. <author> Eppstein, D. </author> <title> Sequence comparison with mixed convex and concave costs. </title> <editor> J. </editor> <booktitle> Algorithms 11 (1990), </booktitle> <pages> 85-101. </pages>
Reference-contexts: Theorem 2. The 1D problem is solved in O ( p n log n) time using optimal O (n 2 ) operations. 2.4. Case w (i; j) = g (j i) When the cost function w (i; j) is a function of the difference j i <ref> [5] </ref>, we can solve the 1D problem more efficiently. In this case H becomes an upper Toeplitz matrix (i.e., H (i; j) = H (0; j i) for all i j). Lemma 4. For any k 1, H k is upper Toeplitz. Proof . By induction on k. <p> Using the square decomposition, H fl can be computed in O (log 2 n) time using O (n 2 ) operations. Part 2 of the reduction technique does not help because its second stage takes O (n 2 ) operations anyway. In sequential computation Eppstein <ref> [5] </ref> solved this case by dividing g into piecewise convex and concave functions, and gave an O (nsff (n=s)) algorithm (s is the number of piecewise functions), which is O (n 2 ) in the worst case. Thus our parallel algorithm is optimal. 8 Theorem 3.
Reference: 6. <author> Eppstein, D., and Galil, Z. </author> <title> Parallel algorithmic techniques for combinatorial computation. </title> <journal> Ann. Rev. Comput. Sci. </journal> <volume> 3 (1988), </volume> <pages> 233-283. </pages>
Reference: 7. <author> Fiduccia, C. M. </author> <title> An efficient formula for linear recurrence. </title> <journal> SIAM J. Comput. </journal> <volume> 14 (1985), </volume> <pages> 106-112. </pages>
Reference-contexts: Thus A i = A for all i. Now we can compute B = A n by successive squarings, but we still have O (n 3 ) operations because the last squaring itself requires O (n 3 ) operations. Lemma 5. <ref> [7] </ref> Let v 1n = (0; . . . ; 0; 1) and v i+1 = A v i for i 1 n. Then A n = [v n ; v n1 ; . . . ; v 1 ].
Reference: 8. <author> Galil, Z., and Giancarlo, R. </author> <title> Speeding up dynamic programming with applications to molecular biology. </title> <booktitle> Theoretical Computer Science 64 (1989), </booktitle> <pages> 107-118. </pages>
Reference-contexts: This is the problem of computing the edit distance when allowing gaps of insertions and deletions <ref> [8] </ref>. It will be called the gap problem. The gap problem arises in molecular biology, geology, and speech recognition. Problem 3.
Reference: 9. <author> Galil, Z., and Park, K. </author> <title> Dynamic programming with convexity, concavity, and sparsity. </title> <note> Theoretical Computer Science to appear. </note>
Reference-contexts: It is this increased dependency that makes efficient parallel algorithms hard to find for the four problems we consider. Sequential algorithms for the five problems are surveyed in <ref> [9] </ref>. For the edit distance problem Apostolico et al. [3] and Aggarwal and Park [1] gave O (log n log m) time algorithms with mn= log m CREW processors. The processor bounds of their algorithms differ slightly in the case of CRCW processors.
Reference: 10. <author> Greenberg, A. C., Ladner, R. E., Paterson, M. S., Galil, Z. </author> <title> Efficient parallel algorithms for linear recurrence computation. </title> <journal> Inform. Process. Lett. </journal> <volume> 15 (1982), </volume> <pages> 31-35. </pages>
Reference-contexts: Huang et al. [13] also gave an O ( n log n) time algorithm with n 3:5 = log n CREW processors. To the best of our knowledge, no work has explicitly dealt with Problems 1, 2, 4. Greenberg et al. <ref> [10] </ref> solved a linear recurrence in O (log 2 n) time with n 3 = log 2 n CREW processors, which solves the 1D problem as a special case in the same complexity. In this paper we present a unifying framework for the parallel computation of dynamic programming. <p> Since the number of operations decreases by a constant factor as level k goes down, computing B requires O (n 3 ) operations. Thus we get O (log 2 n) time using O (n 3 ) operations <ref> [10] </ref>. We can also improve the total number of operations. Since a r;i = 0 for i r &lt; 0 in the 1D problem, the leftmost matrix at each level has only one row of nontrivial values.
Reference: 11. <author> Gries, D., and Levin, G. </author> <title> Computing fibonacci numbers (and similarly defined functions) in log time. </title> <journal> Inform. Process. Lett. </journal> <volume> 11 (1980), </volume> <pages> 68-69. 17 </pages>
Reference-contexts: Then A n = [v n ; v n1 ; . . . ; v 1 ]. Thus the last column of A n is v 1 = (a 1 ; . . . ; a n ), and A n is computed by the following rules (also in <ref> [11] </ref>): 1. For i &lt; n and j &lt; n, A n (i; j) = a i A n (1; j + 1) + A n (i + 1; j + 1).
Reference: 12. <author> Hirschberg, D. S., and Larmore, L. L. </author> <title> The least weight subsequence problem. </title> <journal> SIAM J. Comput. </journal> <volume> 16, 4 (1987), </volume> <pages> 628-638. </pages>
Reference-contexts: We consider the following four problems. Problem 1. Given a real-valued function w and D [0], compute D [j] = min fD [i] + w (i; j)g for 1 j n: (1) This problem was called the least weight subsequence problem by Hirschberg and Larmore <ref> [12] </ref>. It will be called the 1D problem. Its applications include an optimum paragraph formation problem and the problem of finding a minimum height B-tree. Problem 2.
Reference: 13. <author> Huang, S., Liu, H., and Viswanathan, V. </author> <title> A sublinear parallel algorithm for some dynamic programming problems. </title> <booktitle> Proc. 1990 International Conf. Parallel Processing Vol. </booktitle> <volume> 3, </volume> <year> 1990, </year> <pages> pp. 261-264. </pages>
Reference-contexts: For the parenthesis problem Rytter gave an O (log 2 n) time algorithm with n 6 = log n CREW processors, which was later improved to n 6 = log 5 n by Viswanathan et al. [18]. Huang et al. <ref> [13] </ref> also gave an O ( n log n) time algorithm with n 3:5 = log n CREW processors. To the best of our knowledge, no work has explicitly dealt with Problems 1, 2, 4. <p> Note that for - = n we get O ( n log n) time in O (n 4 ) operations, which is the same complexity as <ref> [13] </ref>. Theorem 5. The parenthesis problem is solved in O (n 3=4 log n) time using optimal O (n 3 ) operations. 6.
Reference: 14. <author> Karp, R. M., and Ramachandran, V. </author> <title> A survey of parallel algorithms for shared-memory machines. </title> <booktitle> In Handbook of Theoretical Computer Science, </booktitle> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: Note that we solved the all-pair shortest path problem which is harder than the 1D problem (single-source shortest path). The basic closure method works for computing shortest paths of general graphs. For general graphs no better algorithms have been known <ref> [14] </ref>. For the special graph G we can reduce the total number of operations by the following technique, which will be called the reduction technique. 2.3. Reducing the number of operations Part 1: In the basic closure method there are redundant computations.
Reference: 15. <author> Pettorossi, A., and Burstall, R. M. </author> <title> Deriving very efficient algorithms for evaluating linear recurrence relations using the program transformation technique. </title> <booktitle> Acta Infor-matica 18 (1982), </booktitle> <pages> 181-206. </pages>
Reference-contexts: There is yet another method called program transformation: If we transform Recurrence 1 so that D [2j] depends on D [j]; D [j 1]; . . . ; D [0], then we can compute D in O (log 2 n) time. Pettorossi and Burstall <ref> [15] </ref> derived such transformation, but for the 1D problem the computation based on the transformation turns out to be equivalent to that of the matrix product method. There are many open problems. Among them are 1.
Reference: 16. <author> Rytter, W. </author> <title> On efficient parallel computations for some dynamic programming problems. </title> <booktitle> Theoretical Computer Science 59 (1988), </booktitle> <pages> 297-307. </pages>
Reference-contexts: The Parenthesis Problem The parenthesis problem is different from Problems 1, 2, 4 because this problem is to find a binary tree of minimum cost, while the three problems are to find a path of minimum length. First, we give a clean version of Rytter's algorithm <ref> [16] </ref>, and then improve the total number of operations. Let f (i; j) be the cost of the optimal binary tree rooted at (i; j) (i.e., f (i; j) = D [i; j]).
Reference: 17. <author> Shiloach, Y., and Vishkin, U. </author> <title> Finding the maximum, merging, and sorting in a parallel computation model. </title> <editor> J. </editor> <booktitle> Algorithms 2 (1981), </booktitle> <pages> 88-102. </pages>
Reference-contexts: Faster CRCW algorithms for the four problems can be obtained by replacing the complexity of the minimum finding by O (log log n) time with n= log log n CRCW processors <ref> [17] </ref>. In the following two sections we give two general approaches, the closure method and the matrix product method, for the 1D problem. These approaches are based on well-known methods, but go further to improve the total number of operations.
Reference: 18. <author> Viswanathan, V., Huang, S., and Liu, H. </author> <title> Parallel dynamic programming. </title> <booktitle> Proc. 2nd IEEE Symp. Parallel and Distributed Processing, </booktitle> <year> 1990, </year> <pages> pp. 497-500. </pages>
Reference-contexts: For the parenthesis problem Rytter gave an O (log 2 n) time algorithm with n 6 = log n CREW processors, which was later improved to n 6 = log 5 n by Viswanathan et al. <ref> [18] </ref>. Huang et al. [13] also gave an O ( n log n) time algorithm with n 3:5 = log n CREW processors. To the best of our knowledge, no work has explicitly dealt with Problems 1, 2, 4.
Reference: 19. <author> Waterman, M. S., and Smith, T. F. </author> <title> RNA secondary structure: a complete mathematical analysis. </title> <journal> Math. </journal> <volume> Biosciences 42 (1978), </volume> <pages> 257-266. </pages>
Reference-contexts: Problem 4. Given w and D [i; 0] and D [0; j] for 0 i; j n, compute D [i; j] = min 0q&lt;j 3 This problem has been used to compute the secondary structure of RNA without multiple loops <ref> [19] </ref>. It will be called the RNA problem. A fifth important dynamic programming problem is the edit distance problem [1,3,9]. In the edit distance problem an entry of its dynamic programming table depends on O (1) entries. <p> The RNA Problem In the RNA problem the cost function w (p; q; i; j) involves three different functions depending on p; q; i; j <ref> [19] </ref>. In sequential computation we can divide Recurrence 4 into three recurrences by the functions, and compute D [i; j] as the minimum of the three recurrences. In parallel computation, however, such a division imposes an order on D [i; j] so that linear time cannot be avoided.
Reference: 20. <author> Yao, F. F. </author> <title> Efficient dynamic programming using quadrangle inequalities. </title> <booktitle> Proc. 12th ACM Symp. Theory of Computing, </booktitle> <year> 1980, </year> <pages> pp. 429-435. 18 </pages>
Reference-contexts: Its applications include the matrix chain product, the construction of optimal binary search trees, and the maximum perimeter inscribed polygon problem <ref> [20] </ref>. Problem 4. Given w and D [i; 0] and D [0; j] for 0 i; j n, compute D [i; j] = min 0q&lt;j 3 This problem has been used to compute the secondary structure of RNA without multiple loops [19]. It will be called the RNA problem.
References-found: 20

