URL: http://theory.lcs.mit.edu/~slonim/AS94.ps
Refering-URL: http://theory.lcs.mit.edu/~slonim/incomplete.html
Root-URL: 
Email: angluin@cs.yale.edu  slonim@theory.lcs.mit.edu  
Title: Randomly Fallible Teachers: Learning Monotone DNF with an Incomplete Membership Oracle  
Author: DANA ANGLUIN DONNA K. SLONIM Editor: Ming Li 
Keyword: concept learning, imperfect teachers, monotone DNF formulas, equivalence queries, membership queries with missing information, persistent errors in queries  
Address: P. O. Box 2158, New Haven, CT 06520  545 Technology Square, Cambridge, MA 02139  
Affiliation: Department of Computer Science, Yale University,  MIT Laboratory for Computer Science,  
Note: Machine Learning, [Volumn Number], 1--?? ([Volumn Year]) c [Volumn Year] Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: We introduce a new fault-tolerant model of algorithmic learning using an equivalence oracle and an incomplete membership oracle, in which the answers to a random subset of the learner's membership queries may be missing. We demonstrate that, with high probability, it is still possible to learn monotone DNF formulas in polynomial time, provided that the fraction of missing answers is bounded by some constant less than one. Even when half the membership queries are expected to yield no information, our algorithm will exactly identify m-term, n-variable monotone DNF formulas with an expected O(mn 2 ) queries. The same task has been shown to require exponential time using equivalence queries alone. We extend the algorithm to handle some one-sided errors, and discuss several other possible error models. It is hoped that this work may lead to a better understanding of the power of membership queries and the effects of faulty teachers on query models of concept learning. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D. </author> <year> (1987). </year> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75, </volume> <pages> 87-106. </pages>
Reference-contexts: To address this problem, Goldman, Kearns, and Schapire (1990) consider a model of persistent noise in membership queries, which is related to the model we adopt here (see the discussion in Section 6). 1.2. Overview of the Model Our model relies on the definition of a minimally adequate teacher <ref> (Angluin, 1987) </ref>, in which a learner tries to learn a target concept h fl from a known concept class H. In this (error-free) model, the learner is assisted by a teacher that answers two types of queries. <p> Exactly how much power does an adversary need to prevent learning in a malicious model? Another question is whether we can find polynomial time algorithms in this model for other learning problems known to have polynomial time algorithms using equivalence and membership oracles. For example, deterministic finite state acceptors <ref> (Angluin, 1987) </ref>, simple deterministic languages (Ishizaka, 1990), read-once formulas (Angluin, Hellerstein, and Karpinski, 1993), -formula decision trees (Hancock, 1990), switch configurations (Raghavan and Schach, 1990), and propositional Horn sentences (Angluin, Frazier, and Pitt, 1992) all have such algorithms.
Reference: <author> Angluin, D. </author> <year> (1988). </year> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 319-342. </pages>
Reference: <author> Angluin, D. </author> <year> (1990). </year> <title> Negative results for equivalence queries. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 121-150. </pages>
Reference-contexts: However, the choice of counterexample may not depend on the answers to membership queries not yet made. This adversary is strong enough to generate the "worst-case" counterexam 4 D. ANGLUIN AND D. SLONIM ples used to provide lower bounds for equivalence queries <ref> (Angluin, 1990) </ref>, but it cannot predict the blind spots of the incomplete membership oracle. 1.3. Discussion of the Model It may at first seem odd to assume that membership queries are flawed while equivalence queries remain correct. <p> However, there is no algorithm that runs in time polynomial in n and m and exactly identifies any monotone DNF formula using equivalence queries only <ref> (Angluin, 1990) </ref>. Thus, the quantification of "with high probability" is necessary in the statement of our main result. 2. Preliminaries The target concepts are monotone DNF formulas over the variables x 1 ; : : :; x n for some positive integer n.
Reference: <author> Angluin, D., Frazier, M., & Pitt, L. </author> <year> (1992). </year> <title> Learning conjunctions of Horn clauses. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 147-164. </pages>
Reference-contexts: For example, deterministic finite state acceptors (Angluin, 1987), simple deterministic languages (Ishizaka, 1990), read-once formulas (Angluin, Hellerstein, and Karpinski, 1993), -formula decision trees (Hancock, 1990), switch configurations (Raghavan and Schach, 1990), and propositional Horn sentences <ref> (Angluin, Frazier, and Pitt, 1992) </ref> all have such algorithms. Of these, the problem of propositional Horn sentences is the closest to the case of monotone DNF, but the basic algorithms are quite different.
Reference: <author> Angluin, D., Hellerstein, L., & Karpinski, M. </author> <year> (1993). </year> <title> Learning read-once formulas with queries. </title> <journal> JACM, </journal> <volume> 40, </volume> <pages> 185-210. </pages>
Reference-contexts: For example, deterministic finite state acceptors (Angluin, 1987), simple deterministic languages (Ishizaka, 1990), read-once formulas <ref> (Angluin, Hellerstein, and Karpinski, 1993) </ref>, -formula decision trees (Hancock, 1990), switch configurations (Raghavan and Schach, 1990), and propositional Horn sentences (Angluin, Frazier, and Pitt, 1992) all have such algorithms.
Reference: <author> Angluin, D., & Kharitonov, M. </author> <year> (1991). </year> <booktitle> When won't membership queries help? In Proceedings of the Twenty-Third Annual ACM Symposium on Theory of Computing, </booktitle> <pages> (pp. 444-454). </pages> <address> New Orleans, LA: </address> <publisher> ACM Press. </publisher>
Reference-contexts: The majority of the work was done while the second author was a student at Yale University and at the University of California at Berkeley. A preliminary version of this paper appeared in the proceedings of the 1991 Workshop on Computational Learning Theory <ref> (Angluin and Slonim, 1991) </ref>. RANDOMLY FALLIBLE TEACHERS: LEARNING MONOTONE DNF 19
Reference: <author> Angluin, D., & Laird, P. </author> <year> (1988). </year> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 343-370. </pages>
Reference: <author> Angluin, D., & Slonim, D. </author> <year> (1991). </year> <title> Learning monotone DNF with an incomplete membership oracle. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> (pp. 139-146). </pages> <address> Santa Cruz, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The majority of the work was done while the second author was a student at Yale University and at the University of California at Berkeley. A preliminary version of this paper appeared in the proceedings of the 1991 Workshop on Computational Learning Theory <ref> (Angluin and Slonim, 1991) </ref>. RANDOMLY FALLIBLE TEACHERS: LEARNING MONOTONE DNF 19
Reference: <author> Goldman, S., Kearns, M., & Schapire, R. </author> <year> (1990). </year> <title> Exact identification of circuits using fixed points of amplification functions. </title> <booktitle> In Proceedings of the Thirty-First Annual Symposium on Foundations of Computer Science, </booktitle> <pages> (pp. 193-202). </pages> <address> St. Louis, MO: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Hancock, T. </author> <year> (1990). </year> <title> Identifying -formula decision trees with queries. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> (pp. 23-37). </pages> <address> Rochester, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, deterministic finite state acceptors (Angluin, 1987), simple deterministic languages (Ishizaka, 1990), read-once formulas (Angluin, Hellerstein, and Karpinski, 1993), -formula decision trees <ref> (Hancock, 1990) </ref>, switch configurations (Raghavan and Schach, 1990), and propositional Horn sentences (Angluin, Frazier, and Pitt, 1992) all have such algorithms. Of these, the problem of propositional Horn sentences is the closest to the case of monotone DNF, but the basic algorithms are quite different.
Reference: <author> Ishizaka, H. </author> <year> (1990). </year> <title> Polynomial time learnability of simple deterministic languages. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 151-164. </pages>
Reference-contexts: For example, deterministic finite state acceptors (Angluin, 1987), simple deterministic languages <ref> (Ishizaka, 1990) </ref>, read-once formulas (Angluin, Hellerstein, and Karpinski, 1993), -formula decision trees (Hancock, 1990), switch configurations (Raghavan and Schach, 1990), and propositional Horn sentences (Angluin, Frazier, and Pitt, 1992) all have such algorithms.
Reference: <author> Karp, R.M. </author> <year> (1991). </year> <title> Probabilistic recurrence relations. </title> <booktitle> In Proceedings of the Twenty Third Annual ACM Symposium on Theory of Computing, </booktitle> <pages> (pp. 190-197). </pages> <address> New Orleans, LA: </address> <publisher> ACM Press. </publisher>
Reference-contexts: Moreover, the probability that more than 2m (s + 1) calls to Reduce1 will be required before h contains all the terms of h fl is bounded by e s . To see this, we may apply Karp's method of probabilistic recurrence relations. In <ref> (Karp, 1991) </ref>, recurrence relations of the form T (x) = a (x) + T (r (x)) are used to analyze the running 16 D. ANGLUIN AND D. SLONIM time of recursive randomized algorithms. <p> Let (x) be an upper bound on the expectation of random variable r (x), and let u (x) be the least nonnegative solution to the deterministic equivalence relation t (x) = a (x) + t ((x)). Then the following theorem (Theorem 1 in <ref> (Karp, 1991) </ref>) is useful in showing the bound stated above: Theorem 6 (Karp) Suppose there is a constant d such that a (x) = 0; x &lt; d and a (x) = 1; x d. Let c t = minfx j u (x) tg.
Reference: <author> Kearns, M. </author> <year> (1990). </year> <title> The Computational Complexity of Machine Learning. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. (Also, Doctoral dissertation, Harvard University, </publisher> <year> 1989.) </year> <note> Kearns, </note> <author> M., & Li, M. </author> <year> (1988). </year> <title> Learning in the presence of malicious errors. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> (pp. 267-280). </pages> <address> Chicago, IL: </address> <publisher> ACM Press. </publisher>
Reference: <author> Kearns, M., Li, M., Pitt, L., & Valiant, L.G. </author> <year> (1987). </year> <title> On the learnability of Boolean formulae. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> (pp. 285-295). </pages> <address> New York, NY: </address> <publisher> ACM Press. </publisher>
Reference: <author> Kearns, M., & Valiant, L. </author> <year> (1989). </year> <title> Cryptographic limitations on learning boolean formulae and finite automata. </title> <booktitle> In Proceedings of the Twenty-First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> (pp. 433-444). </pages> <address> Seattle, WA: </address> <publisher> ACM Press. </publisher>
Reference: <author> Laird, P. </author> <year> (1987). </year> <title> Learning from Good Data and Bad. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer Science, Yale University, </institution> <address> New Haven, </address> <month> CT. </month> <title> (Published as Learning from Good and Bad Data, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988.) </year> <note> Littlestone, </note> <author> N. </author> <year> (1988). </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 285-318. </pages>
Reference: <author> Maass, W. </author> <year> (1991). </year> <title> On-line learning with an oblivious environment and the power of randomiza tion. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> (pp. 167-175). </pages> <address> Santa Cruz, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Raghavan, V., & Schach, S. </author> <year> (1990). </year> <title> Learning switch configurations. </title> <booktitle> In Proceedings of Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> (pp. 38-51). </pages> <address> Rochester, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, deterministic finite state acceptors (Angluin, 1987), simple deterministic languages (Ishizaka, 1990), read-once formulas (Angluin, Hellerstein, and Karpinski, 1993), -formula decision trees (Hancock, 1990), switch configurations <ref> (Raghavan and Schach, 1990) </ref>, and propositional Horn sentences (Angluin, Frazier, and Pitt, 1992) all have such algorithms. Of these, the problem of propositional Horn sentences is the closest to the case of monotone DNF, but the basic algorithms are quite different.
Reference: <author> Sakakibara, Y. </author> <year> (1991). </year> <title> On learning from queries and counterexamples in the presence of noise. </title> <journal> Information Processing Letters, </journal> <volume> 37, </volume> <pages> 279-284. </pages>
Reference: <author> Shackelford, G. & Volper, D. </author> <year> (1988). </year> <title> Learning k-DNF with noise in the attributes. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> (pp. 97-103). </pages> <address> Cambridge, MA: </address> <publisher> Morgan Kaufmann. 20 D. </publisher> <editor> ANGLUIN AND D. SLONIM Sloan, R. </editor> <year> (1988). </year> <title> Types of noise in data for concept learning. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> (pp. 91-96). </pages> <address> Cambridge, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sloan, R. </author> <year> (1989). </year> <title> Computational Learning Theory: New Models and Algorithms. </title> <type> Doctoral dis sertation, </type> <institution> MIT Laboratory for Computer Science. </institution>
Reference: <author> Valiant, L. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> CACM, </journal> <volume> 27, </volume> <pages> 1134-1142. </pages>
Reference: <author> Valiant, L. </author> <year> (1985). </year> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings of the 9th IJCAI, </booktitle> <pages> (pp. 560-566). </pages> <address> Los Angeles, CA: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 23

