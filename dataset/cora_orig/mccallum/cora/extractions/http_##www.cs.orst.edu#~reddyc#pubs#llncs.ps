URL: http://www.cs.orst.edu/~reddyc/pubs/llncs.ps
Refering-URL: http://www.cs.orst.edu/~reddyc/pubs.html
Root-URL: 
Email: freddyc,tadepallig@cs.orst.edu  
Phone: Phone: +1 541 753 2770; Fax: +1 541 737 3014  
Title: Learning Horn Definitions with Equivalence and Membership Queries  
Author: Chandra Reddy Prasad Tadepalli 
Address: Corvallis, OR-97331. USA  
Affiliation: Dearborn 303, Department of Computer Science Oregon State University,  
Abstract: A Horn definition is a set of Horn clauses with the same head literal. In this paper, we consider learning non-recursive, function-free first-order Horn definitions. We show that this class is exactly learnable from equivalence and membership queries. It follows then that this class is PAC learnable using examples and membership queries. Our results have been shown to be applicable to learning efficient goal-decomposition rules in planning domains.
Abstract-found: 1
Intro-found: 1
Reference: [ Ackerman & Kanfer, 1993 ] <author> Ackerman, P., and Kanfer, R. </author> <year> 1993. </year> <title> Kanfer-Ackerman Air Traffic Control Task c fl CD-ROM Database, Data-Collection Program, and Playback Program. </title> <institution> Dept. of Psychology, Univ. of Minn., Minneapolis, MN. </institution>
Reference-contexts: efficient version of this scheme, and demonstrated in [ Reddy, Tadepalli, & Roncagliolo, 1996, Reddy & Tadepalli, 1997b ] that it can successfully learn d-rules for efficient planning in two other domains: a variation of STRIPS world [ Fikes, Hart, & Nilsson, 1972 ] , and Kanfer-Ackerman air-traffic control domain <ref> [ Ackerman & Kanfer, 1993 ] </ref> . 5 Discussion In this work, we have shown that first-order function-free non-recursive Horn definitions are learnable utilizing reasonable resources and queries. From this, it is a straightforward extension that first-order monotone DNFs are PAC-learnable using membership queries.
Reference: [ Angluin, Frazier, & Pitt, 1992 ] <author> Angluin, D.; Frazier, M.; and Pitt, L. </author> <year> 1992. </year> <title> Learning conjunctions of horn clauses. </title> <booktitle> Machine Learning 9 </booktitle> <pages> 147-164. </pages>
Reference-contexts: It is shown that propositional Horn sentences are exactly learn-able from equivalence and membership queries <ref> [ Angluin, Frazier, & Pitt, 1992, Frazier & Pitt, 1993 ] </ref> . The entailment queries in [ Frazier & Pitt, 1993 ] are similar to our queries, but differ in one crucial aspect. <p> This class considered by Page handles functions, whereas the class we considered is function-free. There is a difference between what an example means in ILP, and what it means in our case along with the cases of [ Haussler, 1989 ] and <ref> [ Angluin, Frazier, & Pitt, 1992 ] </ref> . An example in ILP comprises a ground head literal and an indication whether it is a positive or a negative example.
Reference: [ Angluin, 1988 ] <author> Angluin, D. </author> <year> 1988. </year> <title> Queries and concept learning. </title> <booktitle> Machine Learning 2 </booktitle> <pages> 319-342. </pages>
Reference-contexts: 1 Introduction In this paper, we consider learning Horn definitions | multiple Horn clauses with a single head | in the exact learning model <ref> [ Angluin, 1988 ] </ref> . Learning Horn definitions is a fundamental problem both in the Inductive Logic Programming (ILP) literature as well as the Computational Learning Theory literature. <p> In this paper, we examine the learnability of a more general class. In particular, we show that first-order non-recursive function-free Horn definitions are exactly learnable from membership and equivalence queries with no other restrictions. Our approach is similar to Angluin's algorithm for learning Propositional Monotone DNF <ref> [ Angluin, 1988 ] </ref> . The algorithm exploits the fact that there is at most one positive literal in a Horn clause, which makes it possible to show that any clause which is implied by the target must be subsumed by one of the clauses in the target. <p> The learning problem in the exact learning framework <ref> [ Angluin, 1988 ] </ref> is the following. <p> Proof. This is a direct consequence of the Subsumption theorem [ Kowalski, 1970 ] . ut If a clause H 1 has variables, to determine j= H 1 is to determine whether all instances in H 1 are also in |which is the same as a subset query <ref> [ Angluin, 1988 ] </ref> . However, by substituting each variable in H 1 by a unique constant, we can form an instance of H 1 . We call this procedure Instantiate. Now, determining whether j= H 1 is equivalent to asking whether j= Instantiate (H 1 ). <p> This is also the upper limit on the running time of the algorithm. ut By the above theorem and the transformation result from the exact learning model to the PAC model <ref> [ Angluin, 1988 ] </ref> , we have the following. Corollary 17. Function-free non-recursive Horn definitions are PAC-learnable using membership queries. 4 Learnability of Goal-Decomposition Rules In AI planning, domain-specific control knowledge is necessary to make planning task computationally feasible. <p> From this, it is a straightforward extension that first-order monotone DNFs are PAC-learnable using membership queries. This is a more general result than the learnability of propositional monotone DNFs <ref> [ Angluin, 1988 ] </ref> . Another related concept class to Horn definitions is Horn sentences; Horn sentences allow different head literals in a set of Horn clauses, as opposed to single head literal for all Horn clauses in Horn definitions.
Reference: [ Cohen, 1995a ] <author> Cohen, W. </author> <year> 1995a. </year> <title> Pac-learning non-recursive prolog clauses. </title> <booktitle> Artificial Intelligence 79(1) </booktitle> <pages> 1-38. </pages>
Reference-contexts: Since it is NP-hard to test membership in this concept class, it immediately follows that even non-recursive function-free Horn definitions are hard to learn from examples alone [ Schapire, 1990 ] . Using only equivalence queries, single non-recursive Pro-log clauses are learnable <ref> [ Cohen, 1995a, Dzeroski, Muggleton, & Russell, 1992 ] </ref> with restrictions such as determinacy and bounded arity. Restricted versions of single recursive clauses are also shown to be learnable [ Cohen, 1995b ] .
Reference: [ Cohen, 1995b ] <author> Cohen, W. </author> <year> 1995b. </year> <title> Pac-learning recursive logic programs: efficient algorithms. </title> <booktitle> Jl. of AI Research 2 </booktitle> <pages> 500-539. </pages>
Reference-contexts: Using only equivalence queries, single non-recursive Pro-log clauses are learnable [ Cohen, 1995a, Dzeroski, Muggleton, & Russell, 1992 ] with restrictions such as determinacy and bounded arity. Restricted versions of single recursive clauses are also shown to be learnable <ref> [ Cohen, 1995b ] </ref> . However, learning multiple clauses or even slightly more general versions of either recursive or non-recursive clauses is shown to be hard without further help [ Cohen, 1995c ] . <p> In our case, it is a conjunction of ground relations (literals) describing a set of objects such that it satisfies a head predicate. Since our motivation is planning in a structural domain, this notion for an example is natural. However, the extended instances in the ILP work in <ref> [ Cohen, 1995b ] </ref> are similar to our notion of examples. The algorithm in Figure 1 is similar in spirit to an ILP system called CLINT [ De Raedt & Bruynooghe, 1992 ] in the sense that they both are incremental and interactive.
Reference: [ Cohen, 1995c ] <author> Cohen, W. </author> <year> 1995c. </year> <title> Pac-learning recursive logic programs: negative results. </title> <booktitle> Jl. of AI Research 2 </booktitle> <pages> 541-573. </pages>
Reference-contexts: Restricted versions of single recursive clauses are also shown to be learnable [ Cohen, 1995b ] . However, learning multiple clauses or even slightly more general versions of either recursive or non-recursive clauses is shown to be hard without further help <ref> [ Cohen, 1995c ] </ref> .
Reference: [ De Raedt & Bruynooghe, 1992 ] <author> De Raedt, L., and Bruynooghe, M. </author> <year> 1992. </year> <title> Interactive concept learning and constructive induction by analogy. </title> <booktitle> Machine Learning 8(2) </booktitle> <pages> 107-150. </pages>
Reference-contexts: However, the extended instances in the ILP work in [ Cohen, 1995b ] are similar to our notion of examples. The algorithm in Figure 1 is similar in spirit to an ILP system called CLINT <ref> [ De Raedt & Bruynooghe, 1992 ] </ref> in the sense that they both are incremental and interactive. Like in our algorithm, CLINT uses queries to eliminate irrelevant literals. CLINT raises the generality of hypotheses by proposing more complex hypothesis clauses, whereas our algorithm uses lgg.
Reference: [ Dzeroski, Muggleton, & Russell, 1992 ] <author> Dzeroski, S.; Muggleton, S.; and Russell, S. </author> <year> 1992. </year> <title> Pac-learnability of determinate logic programs. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> 128-135. </pages>
Reference-contexts: Since it is NP-hard to test membership in this concept class, it immediately follows that even non-recursive function-free Horn definitions are hard to learn from examples alone [ Schapire, 1990 ] . Using only equivalence queries, single non-recursive Pro-log clauses are learnable <ref> [ Cohen, 1995a, Dzeroski, Muggleton, & Russell, 1992 ] </ref> with restrictions such as determinacy and bounded arity. Restricted versions of single recursive clauses are also shown to be learnable [ Cohen, 1995b ] .
Reference: [ Erol, Hendler, & Nau, 1994 ] <author> Erol, K.; Hendler, J.; and Nau, D. </author> <year> 1994. </year> <title> HTN planning: complexity and expressivity. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94). </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: Goal-decomposition rules (d-rules) is a natural method for representing control knowledge [ Reddy, Tadepalli, & Roncagliolo, 1996 ] . They are similar to hierarchical transition networks <ref> [ Erol, Hendler, & Nau, 1994 ] </ref> . A d-rule is a 3-tuple hg; c; sgi that decomposes a goal g into a sequence of subgoals sg, provided condition c holds in the initial state.
Reference: [ Fikes, Hart, & Nilsson, 1972 ] <author> Fikes, R.; Hart, P.; and Nilsson, N. </author> <year> 1972. </year> <title> Learning and executing generalized robot plans. </title> <booktitle> Artificial Intelligence 3 </booktitle> <pages> 251-288. </pages>
Reference-contexts: We have implemented an efficient version of this scheme, and demonstrated in [ Reddy, Tadepalli, & Roncagliolo, 1996, Reddy & Tadepalli, 1997b ] that it can successfully learn d-rules for efficient planning in two other domains: a variation of STRIPS world <ref> [ Fikes, Hart, & Nilsson, 1972 ] </ref> , and Kanfer-Ackerman air-traffic control domain [ Ackerman & Kanfer, 1993 ] . 5 Discussion In this work, we have shown that first-order function-free non-recursive Horn definitions are learnable utilizing reasonable resources and queries.
Reference: [ Frazier & Pitt, 1993 ] <author> Frazier, M., and Pitt, L. </author> <year> 1993. </year> <title> Learning from entailment: An application to propositional Horn sentences. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 120-127. </pages>
Reference-contexts: It is shown that propositional Horn sentences are exactly learn-able from equivalence and membership queries <ref> [ Angluin, Frazier, & Pitt, 1992, Frazier & Pitt, 1993 ] </ref> . The entailment queries in [ Frazier & Pitt, 1993 ] are similar to our queries, but differ in one crucial aspect. <p> It is shown that propositional Horn sentences are exactly learn-able from equivalence and membership queries [ Angluin, Frazier, & Pitt, 1992, Frazier & Pitt, 1993 ] . The entailment queries in <ref> [ Frazier & Pitt, 1993 ] </ref> are similar to our queries, but differ in one crucial aspect. The queries in [ Frazier & Pitt, 1993 ] ask about intermediate predicates in a hierarchy of predicates, whereas our queries concern single-level clauses. <p> The entailment queries in <ref> [ Frazier & Pitt, 1993 ] </ref> are similar to our queries, but differ in one crucial aspect. The queries in [ Frazier & Pitt, 1993 ] ask about intermediate predicates in a hierarchy of predicates, whereas our queries concern single-level clauses. Learnability of first-order function-free Horn sentences from entailment and membership queries is an important open question, which we are investigating at present.
Reference: [ Frazier & Pitt, 1996 ] <author> Frazier, M., and Pitt, L. </author> <year> 1996. </year> <title> CLASSIC learning. </title> <booktitle> Machine Learning 26 </booktitle> <pages> 151-194. </pages>
Reference: [ Haussler, 1989 ] <author> Haussler, D. </author> <year> 1989. </year> <title> Learning conjunctive concepts in structural domains. </title> <booktitle> Machine Learning 4 </booktitle> <pages> 7-40. </pages>
Reference-contexts: This class considered by Page handles functions, whereas the class we considered is function-free. There is a difference between what an example means in ILP, and what it means in our case along with the cases of <ref> [ Haussler, 1989 ] </ref> and [ Angluin, Frazier, & Pitt, 1992 ] . An example in ILP comprises a ground head literal and an indication whether it is a positive or a negative example.
Reference: [ Kietz & Lubbe, 1994 ] <author> Kietz, J.-U., and Lubbe, M. </author> <year> 1994. </year> <title> An efficient subsumption algorithm for inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> 130-138. </pages>
Reference: [ Kowalski, 1970 ] <author> Kowalski, R. </author> <year> 1970. </year> <title> The case for using equality axioms in automatic demonstration. </title> <booktitle> In Lecture Notes in Mathematics, </booktitle> <volume> volume 125. </volume> <publisher> Springer-Verlag. </publisher>
Reference-contexts: We call C t the target clause of H 1 , and H 1 the hypothesis clause of C t . Proof. This is a direct consequence of the Subsumption theorem <ref> [ Kowalski, 1970 ] </ref> . ut If a clause H 1 has variables, to determine j= H 1 is to determine whether all instances in H 1 are also in |which is the same as a subset query [ Angluin, 1988 ] .
Reference: [ Lassez, Maher, & Marriott, 1988 ] <author> Lassez, J.-L.; Maher, M.; and Marriott, K. </author> <year> 1988. </year> <title> Unification revisited. </title> <editor> In Minker, J., ed., </editor> <booktitle> Foundations of Deductive Databases and Logic Programming. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Lloyd, 1987 ] <author> Lloyd, J. </author> <year> 1987. </year> <booktitle> Foundations of Logic Programming (2nd ed.). </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Section 5 concludes the paper by mentioning some implications of this result. 2 Preliminaries In this section, we define and describe the terms we use in the rest of the paper, omitting some of the standard terminology and notation of logic (as given in standard books such as <ref> [ Lloyd, 1987 ] </ref> ). Definition 1. A Horn clause is a finite (multi)set of literals that contains at most one positive literal. It is treated as a disjunction of the literals in the set with universal quantification over all the variables.
Reference: [ Mitchell, Utgoff, & Banerji, 1983 ] <author> Mitchell, T.; Utgoff, P.; and Banerji, R. </author> <year> 1983. </year> <title> Learning by experimentation: Acquiring and refining problem-solving heuristics. </title> <editor> In Michalski, R., and et al., eds., </editor> <booktitle> Machine learning: An artificial intelligence approach, </booktitle> <volume> volume 1. </volume> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Similar to our application of the learning algorithm to speedup learning in planning, Page [ 1993 ] has proposed a way to apply his algorithm to learning LEX's <ref> [ Mitchell, Utgoff, & Banerji, 1983 ] </ref> control rules. It is possible that in some cases the membership queries asked by the learner must respect a background domain theory. Consider a case where such queries are answered automatically by a theorem prover or by running a simulator.
Reference: [ Muggleton & Feng, 1990 ] <author> Muggleton, S., and Feng, C. </author> <year> 1990. </year> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Learning Theory, </booktitle> <pages> 368-381. </pages> <month> Ohmsha/Springer-Verlag. </month>
Reference-contexts: The definitions 3 and 6 are due to Plotkin [ 1970 ] . For a proof of existence of least general generalization see [ Plotkin, 1970 ] and [ Nienhuys-Cheng & de Wolf, 1996 ] . We follow the description in <ref> [ Muggleton & Feng, 1990 ] </ref> of Plotkin's algorithm to find the least general generalizations (lgg) of a set of clauses. The lgg of two clauses C 1 and C 2 is S p 1 2C 1 ;p 2 2C 2 lgg (p 1 ; p 2 ).
Reference: [ Nienhuys-Cheng & de Wolf, 1996 ] <author> Nienhuys-Cheng, S.-H., and de Wolf, R. </author> <year> 1996. </year> <title> Least generalizations and greatest specializations of sets of clauses. </title> <booktitle> Jl. of AI Research 4 </booktitle> <pages> 341-363. </pages>
Reference-contexts: The definitions 3 and 6 are due to Plotkin [ 1970 ] . For a proof of existence of least general generalization see [ Plotkin, 1970 ] and <ref> [ Nienhuys-Cheng & de Wolf, 1996 ] </ref> . We follow the description in [ Muggleton & Feng, 1990 ] of Plotkin's algorithm to find the least general generalizations (lgg) of a set of clauses.
Reference: [ Page, 1993 ] <author> Page, C. </author> <year> 1993. </year> <title> Anti-Unification in Constraint Logics: Foundations and Applications to Learnability in First-Order Logic, to Speed-up Learning, and to Deduction. </title> <type> Ph.D. Dissertation, </type> <institution> University of Illinois, Urbana, IL. </institution>
Reference-contexts: GOLEM mitigates the problem of combinatorial explosion due to lgg in two ways: (1) by restricting the hypothesis language to ij-determinate Horn clauses which guarantee polynomial-sized lgg; and (2) by using negative examples to elimiate literals from the hypotheses. In the case of <ref> [ Page, 1993 ] </ref> , the simplicity and the fixed arity restrictions make the size of lgg polynomial in the sizes of the hypotheses being generalized. For concept learning problems, answering membership queries can be unreasonably demanding.
Reference: [ Plotkin, 1970 ] <author> Plotkin, G. </author> <year> 1970. </year> <title> A note on inductive generalization. </title> <editor> In Meltzer, B., and Michie, D., eds., </editor> <booktitle> Machine Intelligence, volume 5. </booktitle> <address> New York: </address> <publisher> Elsevier North-Holland. </publisher> <pages> 153-163. </pages>
Reference-contexts: The definitions 3 and 6 are due to Plotkin [ 1970 ] . For a proof of existence of least general generalization see <ref> [ Plotkin, 1970 ] </ref> and [ Nienhuys-Cheng & de Wolf, 1996 ] . We follow the description in [ Muggleton & Feng, 1990 ] of Plotkin's algorithm to find the least general generalizations (lgg) of a set of clauses.
Reference: [ Reddy & Tadepalli, 1997a ] <author> Reddy, C., and Tadepalli, P. </author> <year> 1997a. </year> <title> Inductive logic programming for speedup learning. </title> <note> To appear in IJCAI-97 workshop on Frontiers of ILP. </note>
Reference-contexts: Note that the membership queries are d-rules hypothesized by the learner. In our application, the d-rule learner can automatically answer these queries by generating random planning problems that must use the hypthesized d-rules, and then trying out these d-rules in solving the planning problems <ref> [ Reddy & Tadepalli, 1997a, Reddy & Tadepalli, 1997b ] </ref> . Similar to our application of the learning algorithm to speedup learning in planning, Page [ 1993 ] has proposed a way to apply his algorithm to learning LEX's [ Mitchell, Utgoff, & Banerji, 1983 ] control rules.
Reference: [ Reddy & Tadepalli, 1997b ] <author> Reddy, C., and Tadepalli, P. </author> <year> 1997b. </year> <title> Learning goal-decomposition rules using exercises. </title> <booktitle> In Proceedings of the 14th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We have implemented an efficient version of this scheme, and demonstrated in <ref> [ Reddy, Tadepalli, & Roncagliolo, 1996, Reddy & Tadepalli, 1997b ] </ref> that it can successfully learn d-rules for efficient planning in two other domains: a variation of STRIPS world [ Fikes, Hart, & Nilsson, 1972 ] , and Kanfer-Ackerman air-traffic control domain [ Ackerman & Kanfer, 1993 ] . 5 Discussion <p> Note that the membership queries are d-rules hypothesized by the learner. In our application, the d-rule learner can automatically answer these queries by generating random planning problems that must use the hypthesized d-rules, and then trying out these d-rules in solving the planning problems <ref> [ Reddy & Tadepalli, 1997a, Reddy & Tadepalli, 1997b ] </ref> . Similar to our application of the learning algorithm to speedup learning in planning, Page [ 1993 ] has proposed a way to apply his algorithm to learning LEX's [ Mitchell, Utgoff, & Banerji, 1983 ] control rules.
Reference: [ Reddy, Tadepalli, & Roncagliolo, 1996 ] <author> Reddy, C.; Tadepalli, P.; and Roncagliolo, S. </author> <year> 1996. </year> <title> Theory-guided empirical speedup learning of goal-decomposition rules. </title> <booktitle> In Proceedings of the 13th International Conference on Machine Learning, </booktitle> <pages> 409-417. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Corollary 17. Function-free non-recursive Horn definitions are PAC-learnable using membership queries. 4 Learnability of Goal-Decomposition Rules In AI planning, domain-specific control knowledge is necessary to make planning task computationally feasible. Goal-decomposition rules (d-rules) is a natural method for representing control knowledge <ref> [ Reddy, Tadepalli, & Roncagliolo, 1996 ] </ref> . They are similar to hierarchical transition networks [ Erol, Hendler, & Nau, 1994 ] . A d-rule is a 3-tuple hg; c; sgi that decomposes a goal g into a sequence of subgoals sg, provided condition c holds in the initial state. <p> We have implemented an efficient version of this scheme, and demonstrated in <ref> [ Reddy, Tadepalli, & Roncagliolo, 1996, Reddy & Tadepalli, 1997b ] </ref> that it can successfully learn d-rules for efficient planning in two other domains: a variation of STRIPS world [ Fikes, Hart, & Nilsson, 1972 ] , and Kanfer-Ackerman air-traffic control domain [ Ackerman & Kanfer, 1993 ] . 5 Discussion
Reference: [ Schapire, 1990 ] <author> Schapire, R. </author> <year> 1990. </year> <title> The strength of weak learnability. Machine Learning 5 197-227. This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Since it is NP-hard to test membership in this concept class, it immediately follows that even non-recursive function-free Horn definitions are hard to learn from examples alone <ref> [ Schapire, 1990 ] </ref> . Using only equivalence queries, single non-recursive Pro-log clauses are learnable [ Cohen, 1995a, Dzeroski, Muggleton, & Russell, 1992 ] with restrictions such as determinacy and bounded arity. Restricted versions of single recursive clauses are also shown to be learnable [ Cohen, 1995b ] .
References-found: 26

