URL: ftp://ftp.csd.abdn.ac.uk/pub/pedwards/PUBS/iee97.ps.Z
Refering-URL: http://www.csd.abdn.ac.uk/~pedwards/res/publs.html
Root-URL: 
Email: tclg@csd.abdn.ac.uk  
Title: Exploiting Learning Technologies for World Wide Web Agents  
Author: P Edwards, C L Green, P C Lockier T C Lukins 
Address: Aberdeen, Scotland, AB24 3UE fpedwards, claire, pcl,  
Affiliation: Department of Computing Science, King's College, University of Aberdeen,  
Abstract: This paper illustrates how machine learning techniques can be utilised within intelligent software agents which assist users with the management of Web-based information. We discuss a number of recent activities within the Learning Agents & Systems 1 group at Aberdeen, namely: a generic Web browsing assistant; an agent which constructs personalised travel brochures; the application of clustering techniques to Web browser histories; and multi-agent support for adaptive user querying.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Maes. </author> <title> Agents that Reduce Work and Information Overload. </title> <journal> Communications of the ACM, </journal> <volume> 37(7) </volume> <pages> 30-40, </pages> <year> 1994. </year>
Reference-contexts: Interface agents have been proposed as one solution to this problem and can be characterised as systems which "employ Artificial Intelligence techniques to provide assistance to users dealing with a particular computer application" <ref> [1] </ref>. In order to be able to assist the user, an agent must be provided with knowledge of its domain. Two approaches have traditionally been employed to achieve this. The first and most common approach is for users to provide the agent with rules.
Reference: [2] <author> T.W. Malone, K.R. Grant, F.A. Turbak, S.A. Brobst, and M.D. Cohen. </author> <title> Intelligent Information-Sharing Systems. </title> <journal> Communications of the ACM, </journal> <volume> 30(5) </volume> <pages> 390-402, </pages> <year> 1987. </year>
Reference-contexts: In order to be able to assist the user, an agent must be provided with knowledge of its domain. Two approaches have traditionally been employed to achieve this. The first and most common approach is for users to provide the agent with rules. For example, the Oval system <ref> [2] </ref> employs rules to determine if a mail message is of interest, and to decide upon the relevant action to perform. A number of systems have employed scripting languages to allow users to specify rules.
Reference: [3] <author> P. Maes. </author> <title> Social Interface Agents: Acquiring Competence by Learning from Users and Other Agents. </title> <booktitle> In Software Agents: Papers from the 1994 AAAI Spring Symposium, </booktitle> <pages> pages 71-78, </pages> <year> 1994. </year>
Reference-contexts: A more practical approach that offers flexibility for a wide range of users is one that relies on the application of machine learning techniques. The agent is given a minimum of background knowledge, and learns appropriate behaviour from the user and perhaps other agents <ref> [3] </ref>. The use of machine learning methods to develop a profile of user preferences allows the agent to adapt to changes in user behaviour, as well as eliminating the need for explicit programming with rules or scripts. <p> Webhound [12] is a personalised World Wide Web document filtering system that recommends new documents to the user based on observation. An agent window enables users to interact with their own personal Web agent. The system employs social information filtering <ref> [3] </ref> to recommend documents to a user by comparing materials deemed to be of interest to one user with a database of other users' preferences.
Reference: [4] <author> T.R. Payne and P. Edwards. </author> <title> Interface Agents that Learn: An Investigation of Learning Issues in a Mail Agent Interface. </title> <journal> Applied Artificial Intelligence, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <year> 1997. </year>
Reference-contexts: This paper illustrates a number of ways in which machine learning techniques can be applied within intelligent software agents. We began our work in this area by developing a basic learning agent architecture <ref> [4] </ref>. As a user 1 http://www.csd.abdn.ac.uk/~pedwards/res/las.html interacts with an underlying software application, observations are made of his/her behaviour; such observations typically consist of documents (e.g. electronic mail messages, Web pages) and corresponding user actions (e.g. a numerical interest rating for a Web page). <p> Training examples summarise the content of a document as a collection of terms, selected using metrics such as frequency of occurrence. We have explored a variety of different learning approaches to create user-profiles, including the C4.5 [6] rule induction algorithm, and the IBPL <ref> [4] </ref> instance-based algorithm. Rule induction algorithms take a collection of training examples and induce symbolic rules which can be used to predict actions for new, unseen document instances. IBPL does not perform an explicit rule generation phase, rather it stores instances for use during the prediction phase.
Reference: [5] <author> P. Clark and T. Niblett. </author> <title> The CN2 Induction Algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-283, </pages> <year> 1989. </year>
Reference: [6] <author> J.R. Quinlan. </author> <title> C4.5 Programs for Machine Learning. </title> <address> San Mateo, </address> <publisher> CA:Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Training examples summarise the content of a document as a collection of terms, selected using metrics such as frequency of occurrence. We have explored a variety of different learning approaches to create user-profiles, including the C4.5 <ref> [6] </ref> rule induction algorithm, and the IBPL [4] instance-based algorithm. Rule induction algorithms take a collection of training examples and induce symbolic rules which can be used to predict actions for new, unseen document instances.
Reference: [7] <author> K. Lang. NewsWeeder: </author> <title> Learning to Filter Netnews. </title> <booktitle> In Proceedings of the 12th International Machine Learning Conference (ML95), </booktitle> <pages> pages 331-339. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: NewsWeeder <ref> [7] </ref> is a news-filtering system which prioritises USENET news articles for a user using the Minimum Description Length algorithm [8]. The user may choose to read a newsgroup from the normal newsgroup hierarchy, or read NewsWeeder's virtual newsgroup.
Reference: [8] <author> J. Rissanen. </author> <title> Modeling by Shortest Data Description. </title> <journal> Automatica, </journal> <volume> 14, </volume> <year> 1978. </year>
Reference-contexts: NewsWeeder [7] is a news-filtering system which prioritises USENET news articles for a user using the Minimum Description Length algorithm <ref> [8] </ref>. The user may choose to read a newsgroup from the normal newsgroup hierarchy, or read NewsWeeder's virtual newsgroup. This virtual newsgroup contains a personalised list of one-line article summaries, from which the user may select a group of articles to read.
Reference: [9] <author> B.D. Sheth. </author> <title> A Learning Approach to Personalized Information Filtering. </title> <type> Master's Thesis, </type> <institution> Department of Electrical Engineering and Computer Science, MIT, </institution> <year> 1994. </year>
Reference-contexts: The user may choose to read a newsgroup from the normal newsgroup hierarchy, or read NewsWeeder's virtual newsgroup. This virtual newsgroup contains a personalised list of one-line article summaries, from which the user may select a group of articles to read. NewT (News Tailor) <ref> [9] </ref> adopts a genetic algorithm-based approach to identify articles of interest to the user. A population of filtering agents exists, where each agent is responsible for filtering news from a specified news hierarchy.
Reference: [10] <author> G. Salton and M.J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <address> New York: </address> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: NewT (News Tailor) [9] adopts a genetic algorithm-based approach to identify articles of interest to the user. A population of filtering agents exists, where each agent is responsible for filtering news from a specified news hierarchy. New articles are first converted into a vector space representation <ref> [10] </ref> before being tested against an interest profile. Articles are ranked according to the closeness of the match, and the highest ranking articles are presented to the user. WebWatcher [11] is an information search assistant for the World Wide Web which attempts to recommend links that a user should follow. <p> Low entropy terms are removed, such as the, and, etc. and the remaining terms rated using a measure of term significance. A number of different measures have been compared, including term frequency, TFIDF (term frequency versus inverse document frequency), and a measure based on term relevance <ref> [10] </ref>. The modified Web browser can be set in one of four modes: inactive, learning, advising or learning and advising. When the agent is in learning mode, the data needed to generate the profiles is collected by observing the user's interaction with the browser.
Reference: [11] <author> R. Armstrong, D. Freitag, et al. WebWatcher: </author> <title> A Learning Apprentice for the World Wide Web. </title> <booktitle> In AAAI Spring Symposium on Information Gathering from Distributed, Heterogeneous Environments. </booktitle> <address> Menlo Park, CA:AAAI, </address> <year> 1995. </year>
Reference-contexts: New articles are first converted into a vector space representation [10] before being tested against an interest profile. Articles are ranked according to the closeness of the match, and the highest ranking articles are presented to the user. WebWatcher <ref> [11] </ref> is an information search assistant for the World Wide Web which attempts to recommend links that a user should follow. The system learns by observing the user's reaction to its advice and the eventual success or failure of the user's actions.
Reference: [12] <author> Y. Lashkari. </author> <title> The Webhound Personalized Document Filtering System. </title> <address> http://rg.media. mit.edu/projects/webhound. </address>
Reference-contexts: WebWatcher [11] is an information search assistant for the World Wide Web which attempts to recommend links that a user should follow. The system learns by observing the user's reaction to its advice and the eventual success or failure of the user's actions. Webhound <ref> [12] </ref> is a personalised World Wide Web document filtering system that recommends new documents to the user based on observation. An agent window enables users to interact with their own personal Web agent.
Reference: [13] <author> P. Edwards, D. Bayer, C.L. Green, and T.R. Payne. </author> <title> Experience with Learning Agents which Manage Internet-Based Information. </title> <booktitle> In AAAI Spring Symposium on Machine Learning in Information Access, </booktitle> <pages> pages 31-40. </pages> <address> Menlo Park, CA:AAAI, </address> <year> 1996. </year>
Reference-contexts: We will now describe four Web agents: LAW, ELVIS, CUSTARD and MAVA. 3 LAW A Learning Apprentice for the World Wide Web LAW <ref> [13] </ref> helps a user find new and interesting information on the World Wide Web. <p> The n highest scoring pages are presented to the user. A number of experiments have been performed to evaluate the effectiveness of LAW. Space does not permit us to present results here; see <ref> [13] </ref> for full details. 4 ELVIS Enhanced Learning Visitor Information System The ELVIS system is a specialisation of the LAW approach described above.
Reference: [14] <author> M. F. Porter. </author> <title> An Algorithm for Suffix Stripping. Program: </title> <journal> automated library and information systems, </journal> <volume> 14(1) </volume> <pages> 130-137, </pages> <year> 1980. </year>
Reference-contexts: Page relevance is determined as follows: first ELVIS checks whether the page exists, then the title, headings, and free text are extracted. The resulting lists are then processed to remove low entropy terms, before being passed through an implementation of Porter's stemming algorithm <ref> [14] </ref>.
Reference: [15] <author> D. Fisher and P. Langley. </author> <title> Approaches to Conceptual Clustering. </title> <booktitle> In Proceedings of the 9th International Joint Conference on AI, </booktitle> <pages> pages 691-697, </pages> <year> 1985. </year>
Reference-contexts: We are currently exploring the use of unsupervised learning (specifically conceptual clustering <ref> [15] </ref>) in order to present such information in a way that is more meaningful to a user. This involves grouping similar documents together, and presenting the user with clusters of documents, rather than a flat list.
References-found: 15

