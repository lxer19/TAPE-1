URL: http://www.cacs.usl.edu/Departments/CACS/Publications/Raghavan/CDRS97.ps.Z
Refering-URL: http://www.cacs.usl.edu/~raghavan/raghavan-1.html
Root-URL: http://www.cacs.usl.edu/~raghavan/raghavan-1.html
Email: deogun@cse.unl.edu,  raghavan@cacs.usl.edu,  -sever@eti.cc.hun.edu.tr,  
Title: On Feature Selection and Effective Classifiers  
Author: Suresh K. Choubey Jitender S. Deogun Vijay V. Raghavan Hayri Sever 
Keyword: Rough sets, feature selection, classification, data mining, knowledge discovery.  
Address: 70504,USA  Lincoln, NE 68588, USA  fayette, LA 70504, USA  06532 Beytepe,  
Affiliation: Center for Advanced Computer Studies, University of SW Louisiana,Lafayette,LA  Department of Computer Science, University of Nebraska,  Center for Advanced Computer Studies, University of SW Louisiana,La  Department of Computer Science, Hacettepe University,  
Note: This research was supported in part by the Army Research Office, Grant No. DAAH04-96-1-0325, under DEPSCoR program of Advanced Research Projects Agency, Department of Defense. skc@cacs.usl.edu, The  The  The  The  Ankara, TR  
Abstract: In this paper, we develop and analyze four algorithms for feature selection in the context of rough set methodology. The initial state and the feasibility criterion of all these algorithms are the same. That is, they start with a given feature set and progressively remove features, while controlling the amount of degradation in classification quality. These algorithms, however, differ in the heuristics used for pruning the search space of features. Our experimental results confirm the expected relationship between the time complexity of these algorithms and the classification accuracy of the resulting upper classifiers. Our experiments demonstrate that a -reduct of given feature set can be found efficiently. Although we have adopted upper classifiers in our investigations, the algorithms presented can however be used with any method of deriving a classifier where the quality of classification is a monotonically decreasing function of the size of the feature set. We compare the performance of upper classifiers with those of lower classifiers. We find that upper classifiers perform better than lower classifiers for duodenal ulcer data set. This should be generally true when there is a small number of elements in the boundary region. An upper classifier has some important features that make it suitable for data mining applications. In particular, we have shown that the upper classifiers can be summarized at a desired level of abstraction by using extended decision tables. We also point out that an upper classifier results in an inconsistent decision algorithm, which can be interpreted deterministically or non-deterministically to obtain a consistent decision algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bollmann, P. & Cherniavsky, V. S. </author> <year> (1981). </year> <title> Measurement theoretical investigation of the mz-metric, Information Retrieval Research (Oddy, </title> <editor> R. N., Robertson, S. E., VanRijsbergen, C. J. & Williams, R. W., eds.), </editor> <booktitle> Boston:Butterworths, </booktitle> <pages> 256-267. </pages>
Reference-contexts: as Bhattacharya coefficient, divergence, Kolmogorov variational distance, etc., in statistics (Devijver & Kittler, 1982; Miller, 1990); entropy or classification accuracy in pattern recognition and machine learning (Pal & Chakraborty, 1986; Duda & Hart, 1973; Fayyad & Irani, 1992); classification quality based on variations of MZ metric in information retrieval systems <ref> (Bollmann & Cherniavsky, 1981) </ref>. In such procedures, irrelevant features are either eliminated or assigned small coefficients. We have proposed in Deogun et al. (1995), ways to improve upper classifiers one of the classification methods in rough set theory.
Reference: <author> Chang, C. Y. </author> <year> (1973). </year> <title> Dynamic programming as applied to feature subset selection in a pattern recognition system, </title> <journal> IEEE Trans. Syst., Man, Cybern., </journal> <volume> Vol. SMC-3, </volume> <pages> 166-171. </pages>
Reference-contexts: This type of exhaustive search would be appropriate only 2 if l is small and J is computationally inexpensive. Greedy approaches like Sequential back-ward/forward techniques (James, 1985; Modrzejewski, 1993), and dynamic programming <ref> (Chang, 1973) </ref> are some of the efficient search techniques applied with some feature selection criterion.
Reference: <author> Choubey, S. K., Deogun, J. S., Raghavan, V. V. & Sever, H. </author> <year> (1996). </year> <title> A Comparison of Feature Selection Algorithms in the Context of Rough Classifiers, </title> <booktitle> Proceedings of Fifth IEEE International Conference on Fuzzy Systems, </booktitle> <volume> Vol. </volume> <pages> 2, </pages> <address> (New Orleans, LA), </address> <pages> 1122-1128. </pages>
Reference: <author> Deogun, J. S., Raghavan, V. V. & Sever, H. </author> <year> (1994). </year> <title> Rough set based classification methods and extended decision tables, </title> <booktitle> Proceedings of the International Workshop on Rough Sets and Soft Computing, </booktitle> <address> (San Jose, California), </address> <pages> 302-309. </pages>
Reference: <author> Deogun, J. S., Raghavan, V. V. & Sever, H. </author> <year> (1995). </year> <title> Exploiting upper approximations in the rough set methodology, </title> <booktitle> The First International Conference on Knowledge Discovery and Data Mining, </booktitle> <editor> (U. Fayyad and R. Uthurusamy, eds.), </editor> <address> (Montreal, Quebec, Canada), </address> <pages> 69-74. </pages>
Reference: <author> Deogun, J. S., Raghavan, V. V., Sarkar, A., & Sever, H. </author> <year> (1996). </year> <title> Data mining: Research trends, challenges, and applications, in Rough Sets and Data Mining: Analysis of Imprecise Data (Lin, </title> <editor> T. Y. & Cercone, N., eds.), </editor> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers, </publisher> <pages> 1-28. </pages>
Reference: <author> Devijver, R. A. & Kittler, J. </author> <title> (1982) Pattern Recognation: A statistical approach, </title> <publisher> London: Prentice Hall. 21 Duda, </publisher> <editor> R. O. & Hart, P. E. </editor> <year> (1973). </year> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Fayyad, U. M. & Irani, K. B. </author> <year> (1992). </year> <title> The attribute selection problem in decision tree generation, </title> <booktitle> Proceedings of AAAI-92, </booktitle> <publisher> AAAI Press, </publisher> <pages> 104-110. </pages>
Reference: <author> Fayyad, U. M. </author> <year> (1996). </year> <title> Data mining and knowledge discovery: Making sense out of data, </title> <journal> IEEE Expert, </journal> <volume> no. 10, </volume> <pages> 20-25. </pages>
Reference: <author> James, M. </author> <year> (1985). </year> <title> Classification Algorithms, </title> <address> New York, </address> <publisher> NY:John Wiley & Sons. </publisher>
Reference: <author> Kira, K. & Rendell, L. </author> <year> (1992). </year> <title> The feature selection problem: Tradational methods and a new algorithm, </title> <booktitle> Proceedings of AAAI-92, </booktitle> <publisher> AAAI Press, </publisher> <pages> 129-134. </pages>
Reference-contexts: The importance of feature selection in a broader sense is due to the potential for speeding up the processes of both concept learning and classifying objects, reducing the cost of classification (e.g., eliminating redundant tests in medical diagnosis), and improving the quality of classification <ref> (Kira & Rendell, 1992) </ref>. It is well known that searching for the smallest subset of features in the feature space takes time that is bounded by O (2 l J ); where: l is the number of features, and J is the computational effort required to evaluate each subset. <p> When the description of a given object does not match to known concepts we use 5NNR classification scheme with Euclidean distance function to determine the five closest concepts. The difference between two values of an attribute are computed as suggested in Relief al gorithm <ref> (Kira & Rendell, 1992) </ref>; that is, the difference between two non-quantitative values 14 data set Accuracy in % UC UC+BFS UC+AHS UC+HHS UC+KBS Glass 100 100 100 100 100 Breast cancer 100 99.6 99.6 99.6 99.6 Parity 5+10 100 100 98.7 98.7 100 Iris 92.2 95.2 95.2 95.2 95.2 Monk 1
Reference: <author> Lin, T. Y. & Cercone, N., eds., </author> <year> (1996). </year> <title> Rough Sets and Data Mining: Analysis of Imprecise Data, </title> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Matheus, C. J., Chan, P. K. & Piatetsky-Shapiro, G. </author> <year> (1993). </year> <title> Systems for knowledge discovery in databases, </title> <journal> IEEE Trans. on Knowledge and Data Engineering,vol. </journal> <volume> 5, no. 6, </volume> <pages> 903-912. </pages>
Reference: <author> Miller, A. J. </author> <year> (1990). </year> <title> Subset Selection in Regression, </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Modrzejewski, M. </author> <year> (1993). </year> <title> Feature selection using rough sets theory, </title> <booktitle> Machine Learning: Proceedings of ECML-93 (Brazdil, </booktitle> <editor> P. B., ed.), </editor> <publisher> Springer-Verlag, </publisher> <pages> 213-226. </pages>
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <title> UCI repository of machine learning databases, For information contact m-lrepository@ics.uci.edu. </title>
Reference: <author> Pal, S. K. & Chakraborty, B. </author> <year> (1986). </year> <title> Fuzzy set theoretic measure for automatic feature evaluation, </title> <journal> IEEE Trans. Syst., Man, Cybern., </journal> <volume> Vol. SMC-16, no. 5, </volume> <pages> 754-760. </pages>
Reference: <author> Pawlak, Z. </author> <year> (1982). </year> <title> Rough sets, </title> <journal> International Journal of Computer and Information Sciences, </journal> <volume> Vol. 11, </volume> <pages> 341-356. </pages>
Reference: <author> Pawlak, Z. </author> <year> (1991). </year> <title> Rough sets: theoretical aspects of reasoning about data. </title> <publisher> Kluwer. </publisher>
Reference: <author> Pawlak, Z., Grzymala-Busse, J., Slowinski, R., & Ziarko, W. </author> <year> (1995). </year> <title> Rough sets. </title> <journal> CACM, </journal> <volume> 38(11), </volume> <pages> 89-95. </pages>
Reference: <author> Pawlak, Z., Slowinski, K. & Slowinski, R. </author> <year> (1986). </year> <title> Rough classification of patients after highly selective vagotomy for duodenal ulcer, </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> Vol. 24, </volume> <pages> 413-433. </pages>
Reference: <author> Sever, H. </author> <year> (1995). </year> <title> Knowledge Structuring for Database Mining and Text Retrieval Using Past Optimal Queries, </title> <type> PhD thesis, </type> <institution> The University of Southwestern Louisiana. </institution>
Reference-contexts: The time complexity of the algorithm U CQ in the worst case is computed as follows: Step 1 and step 2 take O (mn) and O (lmk); respectively, by the lemma on the nature of the partition <ref> (Sever, 1995) </ref>. Step 8 takes O (mn); in the worst case, by the lemma on the computation of quality (Sever, 1995). <p> U CQ in the worst case is computed as follows: Step 1 and step 2 take O (mn) and O (lmk); respectively, by the lemma on the nature of the partition <ref> (Sever, 1995) </ref>. Step 8 takes O (mn); in the worst case, by the lemma on the computation of quality (Sever, 1995).
Reference: <author> Simoudis, E. </author> <year> (1996). </year> <title> Reality check for data mining, </title> <journal> IEEE Expert, </journal> <volume> no. 10, </volume> <pages> 26-33. 23 </pages>
References-found: 23

