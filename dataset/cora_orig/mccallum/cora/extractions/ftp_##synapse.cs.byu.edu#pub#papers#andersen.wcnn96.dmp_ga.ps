URL: ftp://synapse.cs.byu.edu/pub/papers/andersen.wcnn96.dmp_ga.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Email: email: tim@axon.cs.byu.edu, martinez@cs.byu.edu  
Title: The Effect of Decision Surface Fitness on Dynamic Multilayer Perceptron Networks (DMP1)  
Author: Tim L. Andersen and Tony R. Martinez 
Address: Provo, Utah 84602  
Affiliation: Computer Science Department, Brigham Young University,  
Note: Reference: WCNN, pp 177-181, 1996.  
Abstract: The DMP1 (Dynamic Multilayer Perceptron 1) network training method is based upon a divide and conquer approach which builds networks in the form of binary trees, dynamically allocating nodes and layers as needed. This paper introduces the DMP1 method, and compares the preformance of DMP1 when using the standard delta rule training method for training individual nodes against the performance of DMP1 when using a genetic algorithm for training. While the basic model does not require the use of a genetic algorithm for training individual nodes, the results show that the convergence properties of DMP1 are enhanced by the use of a genetic algorithm with an appropriate fitness function. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Andersen, Tim and Tony Martinez. </author> <title> A Provably Convergent Dynamic Training Method for Multilayer Perceptron Networks. </title> <booktitle> Proceedings of the 2nd International Symposium on Neuroinformatics and Neurocomputers, </booktitle> <year> 1995. </year> <title> Azimi-Sadjadi, Mahmood (1993). Recursive Dynamic Node Creation in Multilayer Neural Networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> Vol 4, No 2, </volume> <pages> pp 242-256. </pages>
Reference: <author> Bartlett, </author> <title> Eric (1994). Dynamic Node Architecture Learning: An Information Theoretic Approach. </title> <booktitle> Neural Networks, </booktitle> <volume> Vol 7, No 1, </volume> <pages> pp 129-140. </pages>
Reference: <author> Fahlman, S. and C. </author> <month> Lebiere </month> <year> (1991). </year> <title> The Cascade Correlation Learning Architecture. </title>
Reference-contexts: Several multilayer training algorithms have been proposed which do not require the user to specify the network architecture a-priori. Some of these include EGP (Romaniuk 1993), Cascade Correlation <ref> (Fahlman 1991) </ref>, Iterative Atrophy (Smotroff 1991), DCN (Romaniuk 1993), ASOCS (Martinez 1990), and DNAL (Bartlett 1993). All of these methods seek to dynamically generate an appropriate network structure for solving the given learning problem during the training phase.
Reference: <author> Martinez, Tony and Jacques Vidal (1988). </author> <title> Adaptive Parallel Logic Networks. </title> <journal> Journal of Parallel and Distributed Computing. </journal> <volume> Vol 5, </volume> <pages> pp 26-58. </pages>
Reference: <author> Martinez, T. R. and Campbell, D. M. </author> <year> (1991). </year> <title> "A Self-Adjusting Dynamic Logic Module", </title> <journal> Journal of Parallel and Distributed Computing, v11, </journal> <volume> no. 4, </volume> <pages> pp. 303-13. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: In addition to reporting the generalization results for DMP1 using the delta rule (DMP1-DR) and GA (DMP1-GA) training method, table 1 also reports results obtained using three well known learning methods for comparison purposes. The three other learning methods reported on are c4.5 <ref> (Quinlan 1986) </ref>, a mutli-layer network trained with back propogation, and a single layer perceptron network. The results obtained with these methods show that DMP1 is capable of performing comparably with other learning methods. 15 runs for each data set were conducted.
Reference: <author> Romaniuk, Steve and Lawrence Hall (1993). </author> <title> Divide and Conquer Neural Networks. </title> <booktitle> Neural Networks, </booktitle> <volume> Vol 6, </volume> <pages> pp 1105-1116. </pages>
Reference-contexts: Several multilayer training algorithms have been proposed which do not require the user to specify the network architecture a-priori. Some of these include EGP <ref> (Romaniuk 1993) </ref>, Cascade Correlation (Fahlman 1991), Iterative Atrophy (Smotroff 1991), DCN (Romaniuk 1993), ASOCS (Martinez 1990), and DNAL (Bartlett 1993). All of these methods seek to dynamically generate an appropriate network structure for solving the given learning problem during the training phase. <p> Several multilayer training algorithms have been proposed which do not require the user to specify the network architecture a-priori. Some of these include EGP <ref> (Romaniuk 1993) </ref>, Cascade Correlation (Fahlman 1991), Iterative Atrophy (Smotroff 1991), DCN (Romaniuk 1993), ASOCS (Martinez 1990), and DNAL (Bartlett 1993). All of these methods seek to dynamically generate an appropriate network structure for solving the given learning problem during the training phase. This paper discusses a dynamic method for building multilayer perceptron networks called DMP1 (Dynamic Multilayer Perceptron 1). <p> This paper discusses a dynamic method for building multilayer perceptron networks called DMP1 (Dynamic Multilayer Perceptron 1). This method is similar in spirit to the DCN method given in <ref> (Romaniuk 1993) </ref> in that: The network begins with a single perceptron node and dynamically adds nodes as needed. As nodes are added to the network, each node is trained independently from all other nodes. Each node is trained on only a portion of the training set.
Reference: <author> Rosenblatt, F. </author> <year> (1958). </year> <title> The Perceptron: A probabilistic Model for Information Storage and Organization in the Brain. </title> <journal> Psychological Review, </journal> <volume> Vol. 65, No. </volume> <pages> 6. </pages>
Reference-contexts: 1 Introduction One of the first models used in the field of neural networking is the single layer, simple perceptron <ref> (Rosenblatt 1958) </ref>. The well understood weakness of single layer networks is that they are able to learn only those functions which are linearly separable.
Reference: <author> Smotroff, Ira, David Friedman and Dennis Connolly (1991). </author> <title> Self Organizing Modular Neural Networks. </title> <booktitle> International Joint Conference on Neural Networks, II, </booktitle> <pages> 187-192. </pages>
Reference-contexts: Several multilayer training algorithms have been proposed which do not require the user to specify the network architecture a-priori. Some of these include EGP (Romaniuk 1993), Cascade Correlation (Fahlman 1991), Iterative Atrophy <ref> (Smotroff 1991) </ref>, DCN (Romaniuk 1993), ASOCS (Martinez 1990), and DNAL (Bartlett 1993). All of these methods seek to dynamically generate an appropriate network structure for solving the given learning problem during the training phase.
Reference: <institution> Zarndt, Frederick (1995). Masters Thesis at Brigham Young University. </institution>
Reference-contexts: The numbers in parenthesis give the standard deviations for the reported results. The results for the other methods are taken from <ref> (Zarndt 1995) </ref>. The last row of the table gives the average accuracy across all of the data sets for each of the methods.
References-found: 10

