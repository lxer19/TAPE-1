URL: http://www.research.att.com/~singer/papers/rocboost.ps.gz
Refering-URL: http://www.research.att.com/~singer/pub.html
Root-URL: 
Email: fschapire,singer,singhalg@research.att.com  
Title: Boosting and Rocchio Applied to Text Filtering  
Author: Robert E. Schapire Yoram Singer Amit Singhal 
Date: January 23, 1998  
Address: 180 Park Avenue, Florham Park, NJ 07932  
Affiliation: AT&T Labs Research  
Abstract: We discuss two learning algorithms for text filtering: modified Rocchio and a boosting algorithm called AdaBoost. We show how both algorithms can be adapted to maximize any general utility matrix that associates cost (or gain) for each pair of machine prediction and correct label. We first show that AdaBoost significantly outperforms another highly effective text filtering algorithm. We then compare AdaBoost and Rocchio over three large text filtering tasks. Overall both algorithms are comparable and are quite effective. AdaBoost produces better classifiers than Rocchio when the training collection contains a very large number of relevant documents. However, on these tasks, Rocchio runs much faster than AdaBoost.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Chidanand Apte, Fred Damerau, and Sholom Weiss. </author> <title> Towards language independent automated learning of text categorisation methods. </title> <editor> In W. Bruce Croft and C.J. van Rijsbergen, editors, </editor> <booktitle> Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 23-30. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Many algorithms for text classification have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [13, 16, 30, 1, 31, 11, 17, 5, 18] </ref>. Most studies use Rocchio's method [21], a well known algorithm in the IR community (usually used for relevance feedback and document routing), as a comparison baseline for their classifiers.
Reference: [2] <author> Avrim Blum. </author> <title> Empirical support for winnow and weighted-majority based algorithms: results on a calendar scheduling domain. </title> <booktitle> In ml95, </booktitle> <pages> pages 64-72, </pages> <year> 1995. </year>
Reference-contexts: Boosting algorithms have attractive theoretical properties, and have also been shown to perform well experimentally on more standard machine learning tasks [7, 19, 3]. We first compare AdaBoost to Sleeping-Experts an algorithm proposed by Blum <ref> [2] </ref>, studied further by Freund et al. [8], and first applied to text filtering by Cohen and Singer [5]. This algorithm has been shown to be more effective than many current text filtering algorithms [5]. We show that, for text filtering, AdaBoost is definitively superior to Sleeping-Experts.
Reference: [3] <author> Leo Breiman. </author> <title> Bias, variance, and arcing classifiers. </title> <type> Technical Report 460, </type> <institution> Statistics Department, University of California at Berkeley, </institution> <year> 1996. </year> <month> 12 </month>
Reference-contexts: The main idea of boosting is to generate many, relatively weak classification rules and to combine these into a single highly accurate classification rule. Boosting algorithms have attractive theoretical properties, and have also been shown to perform well experimentally on more standard machine learning tasks <ref> [7, 19, 3] </ref>. We first compare AdaBoost to Sleeping-Experts an algorithm proposed by Blum [2], studied further by Freund et al. [8], and first applied to text filtering by Cohen and Singer [5]. This algorithm has been shown to be more effective than many current text filtering algorithms [5].
Reference: [4] <author> Chris Buckley and Gerard Salton. </author> <title> Optimization of relevance feedback weights. </title> <editor> In Edward Fox, Peter Ingwersen, and Raya Fidel, editors, </editor> <booktitle> Proceedings of the Eighteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 351-357. </pages> <institution> Association for Computing Machinery, </institution> <address> New York, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: They proposed sampling of the non-relevant documents to form a query zone. 2. Dynamic Feedback Optimization: Buckley et al. <ref> [4] </ref> have shown that further optimizing the term weights proposed by Rocchio's formulation on the training collection improves the quality of a feedback query (i.e., the classifier) for the test data. 3 See [21] (page 315) for the assumptions behind Rocchio's definition of an optimal query. 4 We use vector inner-product <p> Term weights in this query that now includes n w words, n p phrases and n wp word-pairs are further optimized using three-pass dynamic feedback optimization (DFO) with pass ratios 1.00, 0.50, and 0.25 <ref> [4] </ref>. 6. The optimized feedback query is used to rank the Lnu weighted training documents (using inner product similarity). By going down in this ranked list of training documents, find a similarity threshold that would maximize the evaluation measure (error rate or utility) on the training data.
Reference: [5] <author> William W. Cohen and Yoram Singer. </author> <title> Context-sensitive learning methods for text categorization. </title> <booktitle> In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 307-315, </pages> <year> 1996. </year>
Reference-contexts: Many algorithms for text classification have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [13, 16, 30, 1, 31, 11, 17, 5, 18] </ref>. Most studies use Rocchio's method [21], a well known algorithm in the IR community (usually used for relevance feedback and document routing), as a comparison baseline for their classifiers. <p> We first compare AdaBoost to Sleeping-Experts an algorithm proposed by Blum [2], studied further by Freund et al. [8], and first applied to text filtering by Cohen and Singer <ref> [5] </ref>. This algorithm has been shown to be more effective than many current text filtering algorithms [5]. We show that, for text filtering, AdaBoost is definitively superior to Sleeping-Experts. Most previous studies on text classification use a simple version of Rocchio's algorithm as a baseline of comparison. <p> We first compare AdaBoost to Sleeping-Experts an algorithm proposed by Blum [2], studied further by Freund et al. [8], and first applied to text filtering by Cohen and Singer <ref> [5] </ref>. This algorithm has been shown to be more effective than many current text filtering algorithms [5]. We show that, for text filtering, AdaBoost is definitively superior to Sleeping-Experts. Most previous studies on text classification use a simple version of Rocchio's algorithm as a baseline of comparison. <p> we describe our experiments and discuss the corresponding results. 6.1 AdaBoost compared to Sleeping Experts We first give experimental results which show that that our adaptation of AdaBoost for text filtering achieves better results than, Sleeping-Experts another effective algorithm for text filtering studies recently by by Cohen and Singer in <ref> [5] </ref>. We compared the performance of AdaBoost and Sleeping-Experts on the AP-Body and the Reuters tasks. Figure 1 shows the results of this comparison.
Reference: [6] <author> Yoav Freund and Robert E. Schapire. </author> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 148-156, </pages> <year> 1996. </year>
Reference-contexts: One aim of this study is to examine the relative merits of a fairly new ML algorithm called "boosting," and an advanced version of Rocchio's method. We first develop a text filtering algorithm based on Freund and Schapire's AdaBoost algorithm <ref> [6] </ref>, which is currently the most successful of a family of boosting algorithms. The main idea of boosting is to generate many, relatively weak classification rules and to combine these into a single highly accurate classification rule.
Reference: [7] <author> Yoav Freund and Robert E. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 55(1) </volume> <pages> 119-139, </pages> <year> 1997. </year>
Reference-contexts: The main idea of boosting is to generate many, relatively weak classification rules and to combine these into a single highly accurate classification rule. Boosting algorithms have attractive theoretical properties, and have also been shown to perform well experimentally on more standard machine learning tasks <ref> [7, 19, 3] </ref>. We first compare AdaBoost to Sleeping-Experts an algorithm proposed by Blum [2], studied further by Freund et al. [8], and first applied to text filtering by Cohen and Singer [5]. This algorithm has been shown to be more effective than many current text filtering algorithms [5]. <p> Therefore, a learning algorithm that maximizes utility can be used for minimizing the classification error. The utility matrices used in this paper are given in Table 2. 3 Boosting for text filtering In this section, we describe how we have adapted Freund and Schapire's AdaBoost boosting algorithm <ref> [7] </ref> for text filtering. <p> See Freund and Schapire <ref> [7] </ref> for more complete motivation for this choice of ff s . For our task, we also allow ff s to be negative. This will be the case whenever a weak hypothesis h s is found with error * s greater than 1=2.
Reference: [8] <author> Yoav Freund, Robert E. Schapire, Yoram Singer, and Manfred K. Warmuth. </author> <title> Using and combining predictors that specialize. </title> <booktitle> In Proceedings of the Twenty-Ninth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 334-343, </pages> <year> 1997. </year>
Reference-contexts: Boosting algorithms have attractive theoretical properties, and have also been shown to perform well experimentally on more standard machine learning tasks [7, 19, 3]. We first compare AdaBoost to Sleeping-Experts an algorithm proposed by Blum [2], studied further by Freund et al. <ref> [8] </ref>, and first applied to text filtering by Cohen and Singer [5]. This algorithm has been shown to be more effective than many current text filtering algorithms [5]. We show that, for text filtering, AdaBoost is definitively superior to Sleeping-Experts.
Reference: [9] <author> D. K. Harman. </author> <title> Overview of the third Text REtrieval Conference (TREC-3). </title> <editor> In D. K. Harman, editor, </editor> <booktitle> Proceedings of the Third Text REtrieval Conference (TREC-3), </booktitle> <pages> pages 1-19. </pages> <note> NIST Special Publication 500-225, </note> <month> April </month> <year> 1995. </year>
Reference-contexts: All the classes used in our experiments are listed in Table 3, along with the number of relevant training and test documents. 5.1 AP-Body This test collection is made up of documents from the AP newswire included in the TREC disks 1-3 <ref> [9] </ref>. 142,791 documents from the years 1988 and 1989 are used as the training collection, and 66,992 documents from the year 1990 are used as the test collection. <p> Details of what documents were skipped in the creation of this collection are available from David Lewis (lewis@research.att.com). 5.2 TREC-3 This collection, once again from the TREC disks 1-3, was used in the routing task in the third Text REtrieval Conference (TREC-3) in 1994 <ref> [9] </ref>. The training collection contains all documents on TREC disks 1 and 2, whereas the test collection is made up of all the document contained in disk 3. There are a 9 total of 741,856 training documents and 336,310 test documents. <p> There are a 9 total of 741,856 training documents and 336,310 test documents. Fifty TREC topics, numbered 101-150 are used as individual classes in this collection <ref> [9] </ref>. Even though these TREC topics are long user-queries which contain many useful words for text filtering, we again emphasize that we do not use the topic texts in anyway in our system. The relevance judgments for these topics are also available with the TREC data.
Reference: [10] <author> David Hull. </author> <title> The TREC-6 filtering track: Description and analysis. </title> <editor> In D. K. Harman and E. Voorhees, editors, </editor> <booktitle> Proceedings of the Sixth Text REtrieval Conference (TREC-6), 1998 (to appear). </booktitle>
Reference-contexts: As described in the next section, most evaluation measures used in the past for evaluating filtering effectiveness are unfit for the purpose. Recently the TREC conferences have been moving toward the use of utility as the measure of choice for evaluating text filtering <ref> [14, 15, 10] </ref>. This study is also an effort to publish results using utility that can be used by other researchers for comparison purposes in the future. <p> To handle this common difficulty, we need to specifically reward a classifier for finding relevant documents. Recently, the TREC text filtering evaluations have been using utility measures, which assign unequal rewards (or penalties) for each pair of machine prediction and correct label <ref> [15, 10] </ref>. Let r + be the number of relevant documents that are classified relevant by the machine, and r the number of relevant documents misclassified as irrelevant. Similarly, n + and n are the number of nonrelevant documents classified as relevant and irrelevant.
Reference: [11] <author> David Hull, Jan Pedersen, and Hinrich Schutze. </author> <title> Method combination for document filtering. </title> <editor> In Hans Peter Frei, Donna Harman, Peter Schauble, and Ross Wilkinson, editors, </editor> <booktitle> Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 279-288. </pages> <institution> Association for Computing Machinery, </institution> <address> New York, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Many algorithms for text classification have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [13, 16, 30, 1, 31, 11, 17, 5, 18] </ref>. Most studies use Rocchio's method [21], a well known algorithm in the IR community (usually used for relevance feedback and document routing), as a comparison baseline for their classifiers.
Reference: [12] <author> David Lewis. </author> <type> Personal Communication. </type>
Reference-contexts: The aim of a filtering system is to obtain as high a break-even point as possible. This measure, though popular, has numerous problems for evaluating a filtering system <ref> [12] </ref>: Often, we need to interpolate the scores to obtain the break-even point. Interpolation gives values not achievable by the system. The point where Recall equals Precision is neither a desirable nor an informative target from a user's perspective. - Break-even point is often averaged across classes.
Reference: [13] <author> David Lewis. </author> <title> An evaluation of phrasal and clustered representations on a text categorization task. </title> <editor> In N. Belkin, P. Ingwersen, and A. Pejtersen, editors, </editor> <booktitle> Proceedings of the Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 37-50. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: Many algorithms for text classification have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [13, 16, 30, 1, 31, 11, 17, 5, 18] </ref>. Most studies use Rocchio's method [21], a well known algorithm in the IR community (usually used for relevance feedback and document routing), as a comparison baseline for their classifiers. <p> Even though the binary classification effectiveness of a system is related to its ranking effectiveness, this relationship is not strong enough to use ranking evaluation measures to evaluate text filtering. * Break-even point: Proposed by Lewis <ref> [13] </ref>, this measure has been the measure of choice in many studies on text categorization. Roughly speaking, break-even point is the point at which recall of a filtering system is the same as its precision.
Reference: [14] <author> David Lewis. </author> <title> Evaluating and optimizing autonomous text classification systems. </title> <editor> In Edward Fox, Peter Ingwersen, and Raya Fidel, editors, </editor> <booktitle> Proceedings of the Eighteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 246-255. </pages> <institution> Association for Computing Machinery, </institution> <address> New York, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: As described in the next section, most evaluation measures used in the past for evaluating filtering effectiveness are unfit for the purpose. Recently the TREC conferences have been moving toward the use of utility as the measure of choice for evaluating text filtering <ref> [14, 15, 10] </ref>. This study is also an effort to publish results using utility that can be used by other researchers for comparison purposes in the future.
Reference: [15] <author> David Lewis. </author> <title> The TERC-5 filtering track. </title> <editor> In E. M. Voorhees and D. K. Harman, editors, </editor> <booktitle> Proceedings of the Fifth Text REtrieval Conference (TREC-5), </booktitle> <pages> pages 75-96. </pages> <note> NIST Special Publication 500-238, </note> <month> November </month> <year> 1997. </year>
Reference-contexts: As described in the next section, most evaluation measures used in the past for evaluating filtering effectiveness are unfit for the purpose. Recently the TREC conferences have been moving toward the use of utility as the measure of choice for evaluating text filtering <ref> [14, 15, 10] </ref>. This study is also an effort to publish results using utility that can be used by other researchers for comparison purposes in the future. <p> To handle this common difficulty, we need to specifically reward a classifier for finding relevant documents. Recently, the TREC text filtering evaluations have been using utility measures, which assign unequal rewards (or penalties) for each pair of machine prediction and correct label <ref> [15, 10] </ref>. Let r + be the number of relevant documents that are classified relevant by the machine, and r the number of relevant documents misclassified as irrelevant. Similarly, n + and n are the number of nonrelevant documents classified as relevant and irrelevant.
Reference: [16] <author> David Lewis and William Gale. </author> <title> A sequential algorithm for training text classifiers. </title> <editor> In W. Bruce Croft and C.J. van Rijsbergen, editors, </editor> <booktitle> Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 3-12. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Many algorithms for text classification have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [13, 16, 30, 1, 31, 11, 17, 5, 18] </ref>. Most studies use Rocchio's method [21], a well known algorithm in the IR community (usually used for relevance feedback and document routing), as a comparison baseline for their classifiers.
Reference: [17] <author> David Lewis, Robert Schapire, James Callan, and Ron Papka. </author> <title> Training algorithm for linear text classi fiers. In Hans-Peter Frei, Donna Harman, </title> <editor> Peter Schauble, and Ross Wilkinson, editors, </editor> <booktitle> Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 298-306. </pages> <institution> Association for Computing Machinery, </institution> <address> New York, </address> <month> August </month> <year> 1996. </year> <month> 13 </month>
Reference-contexts: Many algorithms for text classification have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [13, 16, 30, 1, 31, 11, 17, 5, 18] </ref>. Most studies use Rocchio's method [21], a well known algorithm in the IR community (usually used for relevance feedback and document routing), as a comparison baseline for their classifiers. <p> We only use the body of a document in our experiments. There are twenty classes in this collection (see Table 4). This collection was first used by Lewis et al. in <ref> [17] </ref>. Lewis et al. used only the title portion of the documents in their study.
Reference: [18] <author> H.T. Ng, W.B. Gog, and K.L. </author> <title> Low. Feature selection, perceptron learning, and a usability case study for text categorization. </title> <editor> In Nick Belkin, Desai Narasimhalu, and Peter Willett, editors, </editor> <booktitle> Proceedings of the Twentieth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 67-73. </pages> <institution> Association for Computing Machinery, </institution> <address> New York, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: Many algorithms for text classification have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [13, 16, 30, 1, 31, 11, 17, 5, 18] </ref>. Most studies use Rocchio's method [21], a well known algorithm in the IR community (usually used for relevance feedback and document routing), as a comparison baseline for their classifiers.
Reference: [19] <author> J. R. Quinlan. Bagging, </author> <title> boosting, </title> <booktitle> and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 725-730, </pages> <year> 1996. </year>
Reference-contexts: The main idea of boosting is to generate many, relatively weak classification rules and to combine these into a single highly accurate classification rule. Boosting algorithms have attractive theoretical properties, and have also been shown to perform well experimentally on more standard machine learning tasks <ref> [7, 19, 3] </ref>. We first compare AdaBoost to Sleeping-Experts an algorithm proposed by Blum [2], studied further by Freund et al. [8], and first applied to text filtering by Cohen and Singer [5]. This algorithm has been shown to be more effective than many current text filtering algorithms [5].
Reference: [20] <author> J.J. Rocchio. </author> <title> Document Retrieval Systems-Optimization and Evaluation. </title> <type> PhD thesis, </type> <institution> Harvard Compu tational Laboratory, </institution> <address> Cambridge, MA, </address> <year> 1966. </year>
Reference-contexts: A feedback query creation algorithm developed by Rocchio in the mid-1960's has, over the years, proven to be one of the best relevance feedback algorithms <ref> [20, 21] </ref>. Rocchio's algorithm was developed in the framework of the vector space model [25]. Rocchio defines an optimal 3 query as the query that maximizes the average query-document similarity for the relevant articles, and simultaneously minimizes the average query-document similarity for the non-relevant documents.
Reference: [21] <author> J.J. Rocchio. </author> <title> Relevance feedback in information retrieval. </title> <booktitle> In The SMART Retrieval System| Experiments in Automatic Document Processing, </booktitle> <pages> pages 313-323, </pages> <address> Englewood Cliffs, NJ, 1971. </address> <publisher> Prentice Hall, Inc. </publisher>
Reference-contexts: Many algorithms for text classification have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more [13, 16, 30, 1, 31, 11, 17, 5, 18]. Most studies use Rocchio's method <ref> [21] </ref>, a well known algorithm in the IR community (usually used for relevance feedback and document routing), as a comparison baseline for their classifiers. One aim of this study is to examine the relative merits of a fairly new ML algorithm called "boosting," and an advanced version of Rocchio's method. <p> A feedback query creation algorithm developed by Rocchio in the mid-1960's has, over the years, proven to be one of the best relevance feedback algorithms <ref> [20, 21] </ref>. Rocchio's algorithm was developed in the framework of the vector space model [25]. Rocchio defines an optimal 3 query as the query that maximizes the average query-document similarity for the relevant articles, and simultaneously minimizes the average query-document similarity for the non-relevant documents. <p> Dynamic Feedback Optimization: Buckley et al. [4] have shown that further optimizing the term weights proposed by Rocchio's formulation on the training collection improves the quality of a feedback query (i.e., the classifier) for the test data. 3 See <ref> [21] </ref> (page 315) for the assumptions behind Rocchio's definition of an optimal query. 4 We use vector inner-product as our similarity measure in all our experiments [22]. 7 We use both these techniques in our version of Rocchio's formulation.
Reference: [22] <author> Gerard Salton. </author> <title> Automatic text processing|the transformation, analysis and retrieval of information by computer. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: proposed by Rocchio's formulation on the training collection improves the quality of a feedback query (i.e., the classifier) for the test data. 3 See [21] (page 315) for the assumptions behind Rocchio's definition of an optimal query. 4 We use vector inner-product as our similarity measure in all our experiments <ref> [22] </ref>. 7 We use both these techniques in our version of Rocchio's formulation. In addition, Singhal [27] has shown that enriching the feature set beyond just the usual words and phrases by adding word co-occurrence pairs improves the quality of a feedback query.
Reference: [23] <author> Gerard Salton and Chris Buckley. </author> <title> Improving retrieval performance by relevance feedback. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(4) </volume> <pages> 288-297, </pages> <year> 1990. </year>
Reference-contexts: Coefficients have been introduced in Rocchio's formulation which control the contribution of the original query, the relevant articles, and the non-relevant articles to the feedback query. These modifications yield the following query reformulation function <ref> [23] </ref>: ~ Q new = ff ~ Q orig + fi 1 X ~ d fl N R d62Rel This formulation, which was developed for ranking documents after relevance feedback, mainly in interactive settings, has also been used successfully for text filtering.
Reference: [24] <author> Gerard Salton and M.J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw Hill Book Co., </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: along with a short discussion on the problems that arise when using these measures for the evaluation of the performance of text-filtering methods: * Recall and precision (and average precision, or precision at a fixed rank cutoff): All these measures are intended to evaluate the ranking effectiveness of a system <ref> [24] </ref>, not its binary classification effectiveness.
Reference: [25] <author> Gerard Salton, A. Wong, and C.S. Yang. </author> <title> A vector space model for information retrieval. </title> <journal> Communica tions of the ACM, </journal> <volume> 18(11) </volume> <pages> 613-620, </pages> <month> November </month> <year> 1975. </year>
Reference-contexts: A feedback query creation algorithm developed by Rocchio in the mid-1960's has, over the years, proven to be one of the best relevance feedback algorithms [20, 21]. Rocchio's algorithm was developed in the framework of the vector space model <ref> [25] </ref>. Rocchio defines an optimal 3 query as the query that maximizes the average query-document similarity for the relevant articles, and simultaneously minimizes the average query-document similarity for the non-relevant documents.
Reference: [26] <author> Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. </author> <title> Boosting the margin: A new explanation for the effectiveness of voting methods. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference, </booktitle> <year> 1997. </year>
Reference-contexts: See Schapire et al. <ref> [26] </ref> for further discussion. 6 error. We then continue boosting for an addition T 0 =10 rounds. Thus, the total number of rounds is (1:1)T 0 .
Reference: [27] <author> Amit Singhal. </author> <title> AT&T at TREC-6. </title> <editor> In D. K. Harman and E. Voorhees, editors, </editor> <booktitle> Proceedings of the Sixth Text REtrieval Conference (TREC-6), 1998 (to appear). </booktitle>
Reference-contexts: In addition, Singhal <ref> [27] </ref> has shown that enriching the feature set beyond just the usual words and phrases by adding word co-occurrence pairs improves the quality of a feedback query.
Reference: [28] <author> Amit Singhal, Mandar Mitra, and Chris Buckley. </author> <title> Learning routing queries in a query zone. </title> <editor> In Nick Belkin, Desai Narasimhalu, and Peter Willett, editors, </editor> <booktitle> Proceedings of the Twentieth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 25-32. </pages> <institution> Association for Computing Machinery, </institution> <address> New York, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: Several techniques are known to improve the effectiveness of Rocchio's method. The two new developments that have been quite effective in conjunction with Rocchio's algorithm are: 1. Query Zoning: Recently Singhal et al. <ref> [28] </ref> have proposed that only a selected set of non-relevant documents that have some relationship to the user query (or a class in text filtering) should be used in Rocchio's method. They proposed sampling of the non-relevant documents to form a query zone. 2. <p> We therefore used a subset of the training collection as a training set. We selected the top 10; 000 documents retrieved from the training collection by a query learned using Rocchio's method (following the idea of query-zoning as in <ref> [28] </ref>). In addition, we added all relevant documents not retrieved by the above procedure to the training set. We also applied the same procedure to the collection of test documents.
Reference: [29] <author> C. J. van Rijsbergen. </author> <title> Information Retrieval. </title> <publisher> Butterworths, </publisher> <address> London, </address> <year> 1979. </year>
Reference-contexts: For these reasons, we believe that break-even point is unfit for evaluating text categorization and should not be used for this purpose. * Van Rijsbergen's E and F measures: These are single valued measures depend upon the relative importance a user assigns to recall and precision (see <ref> [29] </ref>, pp. 168-176). The main drawback of this measure is that its value is not directly interpretable by a user. Also, it is usually hard for a user to 2 judge the relative importance of recall and precision.
Reference: [30] <author> Yiming Yang. </author> <title> Expert network: Effective and efficient learning from human decisions in text categoriza tion and retrieval. </title> <editor> In W. Bruce Croft and C.J. van Rijsbergen, editors, </editor> <booktitle> Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 13-22. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Many algorithms for text classification have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [13, 16, 30, 1, 31, 11, 17, 5, 18] </ref>. Most studies use Rocchio's method [21], a well known algorithm in the IR community (usually used for relevance feedback and document routing), as a comparison baseline for their classifiers.

References-found: 30

