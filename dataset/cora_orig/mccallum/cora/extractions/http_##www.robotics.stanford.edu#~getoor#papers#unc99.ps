URL: http://www.robotics.stanford.edu/~getoor/papers/unc99.ps
Refering-URL: http://www.robotics.stanford.edu/~getoor/papers.html
Root-URL: http://www.robotics.stanford.edu
Email: nir@cs.huji.ac.il  getoor@cs.stanford.edu  
Title: Efficient Learning using Constrained Sufficient Statistics  
Author: Nir Friedman Lise Getoor 
Address: Ross building, Givat Ram Jerusalem 91904 ISRAEL  Gates 1A-126  Stanford, CA 94305-9010  
Affiliation: Institute of Computer Science The Hebrew University  Computer Science Department  Stanford University  
Abstract: Learning Bayesian networks is a central problem for pattern recognition, density estimation and classification. In this paper, we propose a new method for speeding up the computational process of learning Bayesian network structure. This approach uses constraints imposed by the statistics already collected from the data to guide the learning algorithm. This allows us to reduce the number of statistics collected during learning and thus speed up the learning time. We show that our method is capable of learning structure from data more efficiently than traditional approaches. Our technique is of particular importance when the size of the datasets is large or when learning from incomplete data. The basic technique that we introduce is general and can be used to improve learning performance in many settings where sufficient statistics must be computed. In addition, our technique may be useful for alternate search strategies such as branch and bound algorithms.
Abstract-found: 1
Intro-found: 1
Reference: <author> Beinlich, I., Suermondt, G., Chavez, R. & Cooper, G. </author> <year> (1989), </year> <title> The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks, </title> <booktitle> in `Proc. 2nd Euro-pean Conf. on AI and Medicine', </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: It is easy to see that if we have true upper bounds, then this procedure returns the best possible change. We evaluated these two search procedures on two domains. We generated training data from two networks: alarm|a network for intensive care patient monitoring <ref> (Beinlich, Suermondt, Chavez & Cooper 1989) </ref> that has 37 variables; and Insurance|a network for classifying car insurance applications (Binder, Koller, Rus-sell & Kanazawa 1997) that has 26 variables. From each network we sampled training sets of sizes 10; 000, 50; 000 and 100; 000.
Reference: <author> Binder, J., Koller, D., Russell, S. & Kanazawa, K. </author> <year> (1997), </year> <title> `Adaptive probabilistic networks with hidden variables', </title> <booktitle> Machine Learning 29, </booktitle> <pages> 213-244. </pages>
Reference-contexts: We evaluated these two search procedures on two domains. We generated training data from two networks: alarm|a network for intensive care patient monitoring (Beinlich, Suermondt, Chavez & Cooper 1989) that has 37 variables; and Insurance|a network for classifying car insurance applications <ref> (Binder, Koller, Rus-sell & Kanazawa 1997) </ref> that has 26 variables. From each network we sampled training sets of sizes 10; 000, 50; 000 and 100; 000. We then plotted the performance profiles of both search procedures on these training sets.
Reference: <author> Bouckaert, R. </author> <year> (1995), </year> <title> Bayesian Belief Networks: From Construction to Inference., </title> <type> PhD thesis, </type> <institution> Utrecht University, Utrecht, The Netherlands. </institution>
Reference-contexts: The two scoring functions most commonly used to learn Bayesian networks are the Bayesian scoring metrics (Cooper & Herskovits 1992, Heckerman et al. 1995), and one based on the principle of minimal description length (MDL) <ref> (Bouckaert 1995, Lam & Bacchus 1994) </ref>. Let G be the first component of a Bayesian network, namely a directed acyclic graph, and let D be a training set. We denote the score of G by Score (G : D).
Reference: <author> Cooper, G. F. & Herskovits, E. </author> <year> (1992), </year> <title> `A Bayesian method for the induction of probabilistic networks from data', </title> <booktitle> Machine Learning 9, </booktitle> <pages> 309-347. </pages>
Reference-contexts: The two scoring functions most commonly used to learn Bayesian networks are the Bayesian scoring metrics <ref> (Cooper & Herskovits 1992, Heckerman et al. 1995) </ref>, and one based on the principle of minimal description length (MDL) (Bouckaert 1995, Lam & Bacchus 1994). Let G be the first component of a Bayesian network, namely a directed acyclic graph, and let D be a training set.
Reference: <author> Cover, T. M. & Thomas, J. A. </author> <year> (1991), </year> <title> Elements of Information Theory, </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: ) = x i ; i ) 2 2 This bound is based on the simple fact that the first term of the bound is non-positive (to see this, note that this term is the negative of the empirical conditional entropy H (X i j i ), which is non-negative <ref> (Cover & Thomas 1991) </ref>). Such a bound, however, totally ignores our (partial) knowledge about the data, and hence must be quite loose. A somewhat tighter bound can be found by considering properties of the conditional entropy. In contrast to these loose bounds, we aim to find the tightest possible bound.
Reference: <author> Friedman, N. </author> <year> (1997), </year> <title> Learning belief networks in the presence of missing values and hidden variables, </title> <editor> in D. Fisher, ed., </editor> <booktitle> `Proceedings of the Fourteenth International Conference on Machine Learning', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <pages> pp. 125-133. </pages>
Reference-contexts: Without going into details, the problem is that the scores no longer decompose, and each candidate structure has to be evaluated using expensive parameter learning methods, such as EM. One method of learning structure in this situation extends EM to structure search. In the Structural EM procedure <ref> (Friedman 1997, Friedman 1998) </ref>, we use the "current" candidate to compute expected counts from the data. Roughly speaking, the procedure computes the expected value of N X , given the conditional distribution over missing values given the observed values and the current estimate of the distribution.
Reference: <author> Friedman, N. </author> <year> (1998), </year> <title> The Bayesian structural EM algorithm, </title> <editor> in G. F. Cooper & S. Moral, eds., </editor> <booktitle> `Proc. Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI '98)', </booktitle> <publisher> Morgan Kauf-mann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Heckerman, D. </author> <year> (1995), </year> <title> A tutorial on learning Bayesian networks, </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Mi-crosoft Research. </institution>
Reference-contexts: The second term is a penalty term that biases the score metric to prefer simpler networks, where N is the number of random variables in the network and #(G) is the number of parameters in the network. (We refer the interested reader to <ref> (Heckerman 1995, Lam & Bacchus 1994) </ref> for detailed description of this score.) 1 The Bayesian score is derived using Bayesian reasoning.
Reference: <author> Heckerman, D., Geiger, D. & Chickering, D. M. </author> <year> (1995), </year> <title> `Learning Bayesian networks: The combination of knowledge and statistical data', </title> <booktitle> Machine Learning 20, </booktitle> <pages> 197-243. </pages>
Reference-contexts: The second term is a penalty term that biases the score metric to prefer simpler networks, where N is the number of random variables in the network and #(G) is the number of parameters in the network. (We refer the interested reader to <ref> (Heckerman 1995, Lam & Bacchus 1994) </ref> for detailed description of this score.) 1 The Bayesian score is derived using Bayesian reasoning.
Reference: <author> Lam, W. & Bacchus, F. </author> <year> (1994), </year> <title> `Learning Bayesian belief networks: An approach based on the MDL principle', </title> <booktitle> Computational Intelligence 10, </booktitle> <pages> 269-293. </pages>
Reference: <author> Luenbeger, D. </author> <year> (1984), </year> <title> Linear and Nonlinear Programming, </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: It is easy to see that the maximum of a convex function F is achievable at the boundary of the feasible region C. Furthermore, the maximum is achievable at an extreme point of the feasible region. This follows from a straightforward argument, see for example <ref> (Luenbeger 1984) </ref>. Theorem 3.3 : The global maximum of Score MDL (XjY; Z : N XY Z ) is bounded by the global maximum of F and is achieved at an extreme point of the feasible region C. Proof: Follows directly from the definition of F and the previous lemma.
Reference: <author> Moore, A. W. & Lee, M. S. </author> <year> (1997), </year> <title> `Cached sufficient statistics for efficient machine learning with large datasets', </title> <journal> Journal of A.I. Research 8, </journal> <pages> 67-91. </pages>
Reference: <author> Suzuki, J. </author> <year> (1996), </year> <title> Learning Bayesian belief networks based on the minimum description length principle: An efficient algorithm using the B & B technique, </title> <editor> in L. Saitta, ed., </editor> <booktitle> `Proceedings of the Thirteenth International Conference on Machine Learning', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <pages> 462-470. </pages>
References-found: 13

