URL: http://www.cs.columbia.edu/~hgs/papers/Sisa9506_ATM.ps.gz
Refering-URL: http://www.cs.columbia.edu/~hgs/resume/resume.html
Root-URL: http://www.cs.columbia.edu
Title: Rate Based Congestion Control and its Effects on TCP over ATM  
Author: By Dorgham Sisalem 
Degree: Submitted to the Department of Telecommunications Engineering at the Technical University of Berlin in Fulfillment of the Requirements for the Diplomarbeit Prof. Adam Wolisz Supervisor: Dr. Henning Schulzrinne  
Note: Berlin 25.5.1994  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Sathaye, </author> <title> "DRAFT ATM Forum traffic management specification version 4.0," </title> <type> ATM Forum 95-0013, </type> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: The presented work is divided into four main parts: * The ABR Service: This chapter discusses the ABR service and its quality of service requirements as they were specified by the ATM Forum 1 in the Traffic Management Specifications version 4.0 <ref> [1] </ref>. Different definitions of fairness in the ABR context are presented and simulation models for ABR sources are explained. * Congestion Control in ATM: The end-to-end rate control as well as the credit based link-by-link flow control approaches are briefly described and compared with one another. <p> For example <ref> [1] </ref>: * Some applications may become intolerable to human users if they are unable to send at a minimum rate at least, such as remote procedure calls. * Control messages may have to be sent at a minimum rate to ensure protocol liveness. <p> Considering these two delay bounds it is proposed that the conformance definition for ABR connection be as follows: 1. The end systems must follow the reference behavior as it was proposed by the ATM Forum in <ref> [1] </ref>. In the case of the rate based control mechanisms this would mean that a source can change its sending rate in accordance with the received messages from the network. 2.
Reference: [2] <author> D. Borman, R. Braden, and V. Jacobson, </author> <title> "TCP extensions for high performance," Request for Comments (Proposed Standard) RFC 1323, </title> <institution> Internet Engineering Task Force, </institution> <month> May </month> <year> 1992. </year> <note> (Obsoletes RFC1185). </note>
Reference-contexts: The final version of the simulator is based on the 4.3BSD Tahoe TCP with fast retransmission and some of the extensions for high performance networks as they were proposed by Jacobson et. al in <ref> [2] </ref>. This chapter describes the different congestion control mechanisms of TCP and presents a verification test for the simulator. * TCP over ATM: This chapter describes the behavior of TCP in a broadband environment and the effects of running it over an ATM network. <p> These options allow for window size scaling, protection against sequence number wrap up and a better round trip time estimation, as described in RFC 1323 <ref> [2] </ref>. 5. 4.3BSD Vegas (1994): Here an improved round trip time estimation algorithm is used and the congestion avoidance and slow start mechanisms were modified [29]. 4.3 4.3BSD Tahoe TCP Congestion Control Algorithm The Reno TCP release was mainly intended to improve the performance of the hosts. <p> To allow for transmission windows larger than the 64 kbytes that can be represented with the 16 bit field available in the header a window scale option was introduced in RFC 1323 by Jacobson et. al <ref> [2] </ref>. With this option the window sizes can be scaled up to 2 30 bytes. This means that the window size is actually limited by the available buffer at the hosts or some internal restrictions 1 . <p> The TCP model used is an enhanced version of the 4.3BSD Tahoe TCP with fast retransmission, delayed acknowledgments and the improved round trip time estimation algorithm as was presented by Jacobson et. al in RFC 1323 <ref> [2] </ref>. As the RTT values in our simulations are in the order of just a few milliseconds, the coarse-grain timer used in Unix TCP (typically, a granularity of 500 msec is used) would result in very imprecise timeout values. <p> Actually, the maximum window size is usually set to the maximum possible size. That is, a maximum value of 64 kbytes is usually used for TCP. For connections using the window scale option proposed by Jacobson et. al <ref> [2] </ref> the maximum window size can even be 2 30 bytes long. <p> During this time no other measurement can start. Looking at this problem as a signal processing problem, a data signal at some frequency, the packet rate, is being sampled at a lower frequency, the window rate <ref> [2] </ref>. This lower sampling frequency introduces aliasing effects as it violates the Nyquist criteria which states that a signal can be uniquely represented by a set of samples taken at twice the frequency of the signal itself [47]. These effects are still tolerable in a narrow band environment. <p> On the contrary, in a broadband environment with windows of hundreds or even thousands of packets the RTT estimator may be seriously in error. Jacobson et. al <ref> [2] </ref> recommend the introduction of a timestamp option in the header of the TCP packet. The sender writes the sending time in a timestamp field and the receiver echoes this value back in the timestamp echo field. <p> This allowed us to better consider round trip times of a few milliseconds as it would be the case for ATM simulations. In an enhanced version of the simulator the round trip time estimation algorithm as it was presented in <ref> [2] </ref> was additionally implemented. A.2 Testing Environment Here a network model has been chosen that was already tested in another study [26]. It consists of a bottleneck switch with a 20 packet buffer that is connected to the source and the receiver.
Reference: [3] <author> R. Jain, </author> <title> "Fairness: How to measure quantitatively?," </title> <type> ATM Forum 94-0881, </type> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: THE ABR SERVICE 2.2.2 Relative Commitments The network can assure that the bandwidth received by flows sharing the same path is fairly apportioned. For a quantitative description of the fairness of an allocation scheme Jain <ref> [3] </ref> suggests using the so called Fairness index. <p> To compare between the different algorithms a simple generic fairness configuration [6] was chosen. With this topology, see Fig. 3.3, the fairness of bandwidth allocation can be simply tested for the case of cascaded congested links. The fairness index presented by Jain <ref> [3] </ref> with which fairness can be characterized quantitatively is used to compare between the fairness of the different algorithms, see Sec. 2.2. Simulation results showing rate and buffer behavior of the different schemes are presented as well.
Reference: [4] <author> A. Berger, F. Bonomi, K. Fendick, and B. Holden, </author> <title> "Proposed TM baseline text on an ABR conformance definition," ATM Forum 95-0212(R1), </title> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Conformance indicates here that the arriving cells at the interface conform to the proper response of the sources to the RM feedback messages. In this section a definition of this feedback conformance as it was presented in <ref> [4] </ref> is given and the generic cell rate algorithm (GCRA) that is used for testing the conformance of the arriving cells is introduced. 2.3.1 The Generic Cell Rate Algorithm The flowchart in Fig. 2.1 presents the generic cell rate algorithm (GCRA) as a virtual scheduling algorithm. <p> N , i.e., the maximum number of cells that can be sent at the link rate can be calculated as follows: N = j1 + I ffi 2.3.2 The Conformance Definition To account for the cell delay between the ABR source and its interface to the network, Berger et. al <ref> [4] </ref> suggest the usage of two delay parameters t 1 and t 2 that the network equipment would negotiate for the connection and the network operator could assume for the conformance definition.
Reference: [5] <author> A. W. Barnhart, </author> <title> "Baseline model for rate-control simulations," </title> <type> ATM Forum 94-0399, </type> <month> May </month> <year> 1994. </year>
Reference-contexts: Here, two sources that can be used for simulating applications using the ABR service are introduced. 2.4.1 Persistent Sources This kind of sources always sends with the maximum permitted rate. Different simulations <ref> [5] </ref> have shown that this model imposes the heaviest constraints on the network and is therefore very appropriate for testing the fairness and the throughput of the ABR service. Also, this model eliminates statistical throughput and delay fluctuation that would be caused by random traffic generators. <p> The rate is changed in so called update intervals (UI). Receiving a resource management (RM) cell during an update interval, indicates congestion and causes a rate reduction by the multiplicative decrease factor (MDF) which was set to 0.875 due to a recommendation in <ref> [5] </ref>. The rate is additively increased by the additive increase rate (AIR) after an update interval in which no RM cells were received. If a source remained idle for an update interval then its rate is multiplicatively decreased as well.
Reference: [6] <author> L. Wojnaroski, </author> <title> "Baseline text for traffic management sub-working group," ATM Forum 94-0394r5, </title> <address> Oct. </address> <year> 1994. </year>
Reference-contexts: Also, this model eliminates statistical throughput and delay fluctuation that would be caused by random traffic generators. Thus, it would be possible to achieve deterministic and reproducible simulation results. 2.4.2 Bursty Sources <ref> [6] </ref> describes an active/idle source that is based on a three state model, see Fig. 2.3. The source can be either in an idle or active state. While in the active state, the source generates a series of packets which are interspersed by short pauses. <p> Here, three algorithms are introduced. They differ in their performance, complexity and the kind of congestion information conveyed back to the source. To compare between the different algorithms a simple generic fairness configuration <ref> [6] </ref> was chosen. With this topology, see Fig. 3.3, the fairness of bandwidth allocation can be simply tested for the case of cascaded congested links.
Reference: [7] <author> N. Yin and M. G. Hluchyj, </author> <title> "On closed-loop rate control for ATM cell relay networks," </title> <booktitle> in Proceedings of the Conference on Computer Communications (IEEE Infocom), </booktitle> <address> (Toronto, Canada), </address> <month> June </month> <year> 1994. </year>
Reference-contexts: THE ABR SERVICE Chapter 3 Congestion Control in ATM 3.1 Introduction Ensuring, at the same time, high utilization of the available bandwidth in a network with a minimum cell loss rate requires efficient as well as fair congestion control schemes. These schemes should achieve three important goals <ref> [7] </ref>: 1. At a minimum, the quality of service (QOS) negotiated during the connection establish ment has to be satisfied for each source. 2. Unused bandwidth should be fairly distributed among the active connections. 3.
Reference: [8] <author> D. Hunt, J. Scott, J. C. R. Bennet, A. Chapman, R. Nair, B. Simcoe, H. T. Kung, and K. Brinkerhoff, </author> <title> "Credit-based FCVC proposal for ATM traffic management -revision R1," ATM Forum 94-0168R1, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: Even though that the here presented algorithm works on a hop by hop basis a correct implementation should result in a closed flow control loop. 3.2.1 Credit Based Flow Controlled Virtual Connection The in <ref> [8] </ref> introduced credit based flow controlled virtual connection (FCVC) algorithm proposed a per VC buffer and flow control. In this algorithm upstream nodes maintain a credit balance for each outgoing FCVC.
Reference: [9] <author> H. T. Kung, T. Blackwell, and A. Chapman, </author> <title> "Adaptive credit allocation for flow-controlled VCs," </title> <type> ATM Forum 94-0282, </type> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: Adaptive Credit Allocation: The adaptive allocation scheme avoids the need of having to specify the necessary bandwidth for a VC and reduces the amount of buffer space needed for that VC. In <ref> [9] </ref> all VCs are dynamically allocated buffer space from a shared buffer. This is done in proportion to the actual bandwidth consumed by each VC. The actual bandwidth of a VC is calculated by counting the number of cells departing on that VC over a measurement time interval (MTI).
Reference: [10] <author> D. Huges and P. Daley, </author> <title> "Limitations of credit based flow control," </title> <type> ATM Forum 94-0776, </type> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: This is done in proportion to the actual bandwidth consumed by each VC. The actual bandwidth of a VC is calculated by counting the number of cells departing on that VC over a measurement time interval (MTI). Performance Analysis Simulation results obtained in <ref> [10] </ref> and [11] indicate that the credit based FCVC mechanism with static memory allocation guarantees a link utilization of nearly 100%, no cell loss, minimum response time to changing load conditions and fairness towards local as well as transit traffic. <p> However, with this scheme the optimal performance reached with the static scheme is no longer guaranteed. Actually, the utilization drops to a no longer satisfactory level. Also, the response times for changing traffic conditions reach unacceptable values <ref> [10] </ref>. As the bandwidth allocation policy depends on observed rates during a measurement interval and not on actual rates a source that was silent for the last MTI or a new source will only be able to increase its bandwidth share in a very slow way.
Reference: [11] <author> D. Huges and P. Daley, </author> <title> "More ABR simulation results," </title> <type> ATM Forum 94-0777, </type> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: This is done in proportion to the actual bandwidth consumed by each VC. The actual bandwidth of a VC is calculated by counting the number of cells departing on that VC over a measurement time interval (MTI). Performance Analysis Simulation results obtained in [10] and <ref> [11] </ref> indicate that the credit based FCVC mechanism with static memory allocation guarantees a link utilization of nearly 100%, no cell loss, minimum response time to changing load conditions and fairness towards local as well as transit traffic. However, this is only reached on the expense of large enough buffers.
Reference: [12] <author> B. Makrucki, T. Tofigh, A. Barnhart, B. Holden, J. Diagle, M. Hulchyj, H. Suzuki, G. Rama-murthy, P. Newman, N. Giroux, R. Kositpaiboon, S. Sathe, G. Garg, and N. Yin, </author> <title> "Closed-loop rate-based traffic management," </title> <type> ATM Forum 94-0438, </type> <month> May </month> <year> 1994. </year> <note> 97 98 BIBLIOGRAPHY </note>
Reference-contexts: CONGESTION CONTROL IN ATM 3.3.1 Rate Control Using EFCI Marking This simple algorithm uses the explicit forward congestion identification (EFCI) bit in the payload field of the cell header to determine the congestion state of the network <ref> [12] </ref>. 1. Source End System Behavior The source starts sending cells with the initial cell rate (ICR) that was defined at connection establishment with the EFCI state set to not congested (EFCI=0). The rate is changed in so called update intervals (UI).
Reference: [13] <author> L. Roberts, B. Makrucki, T. Tofigh, A. Barnhart, B. Holden, J. Diagle, M. Hulchyj, H. Suzuki, G. Ramamurthy, P. Newman, N. Giroux, R. Kositpaiboon, S. Sathe, G. Garg, and N. Yin, </author> <title> "Closed-loop rate-based traffic management," ATM Forum 94-0438R1, </title> <month> July </month> <year> 1994. </year>
Reference-contexts: This implies that a longer simulation would result in a much smaller fairness index. 3.3.2 Proportional Rate Control Algorithm The proportional rate control algorithm (PRCA) is based on the positive feedback rate control scheme <ref> [13] </ref>. In this scheme the rate can only be increased if a positive indication, in this case a RM cell, to do so was received. Otherwise, the rate is continually decreased after each sent cell. 3.3. <p> END-TO-END RATE CONTROL 21 3.3.3 Enhanced Proportional Rate Control Algorithm The enhanced proportional rate control algorithm (EPRCA) provides two major enhancements to the PRCA proposal presented in <ref> [13] </ref>: * Explicit rate indication: Here, the sources receive an explicit indication of their fair share of the bandwidth which they should use in order to avoid congestion. * Intelligent marking: With this approach only the rates of greedy connections that caused the congestion are reduced.
Reference: [14] <author> H. Hsiaw, K. Fendrick, J. Alpan, L. Roberts, B. Makrucki, T. Tofigh, A. Barnhart, B. Holden, J. Diagle, M. Hulchyj, H. Suzuki, G. Ramamurthy, P. Newman, N. Giroux, R. Kositpaiboon, S. Sathe, G. Garg, and N. Yin, </author> <title> "Closed-loop rate-based traffic management," ATM Forum 94-0438R2, </title> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: If the intelligent marking was used the switches can set a congestion bit in the RM cells passing over the backward stream of the high rate connections and thereby denying these connection the opportunity to increase their rates. The RM cells have to at least carry the following parameters <ref> [14] </ref>: * ACR: The allowed cell rate is used mainly to selectively indicate congestion on connections passing a congested link. That is, intermediate networks can signal VCs with a high ACR to reduce their rates while allowing other connections to keep their current rates or even increase them.
Reference: [15] <author> A. Charny, </author> <title> "An algorithm for rate allocation in a packet-switching network with feedback," </title> <type> Tech. Rep. </type> <address> MIT/TR-601, </address> <publisher> MIT, </publisher> <address> Cambridge, Massachusettts 02139, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: PRCA proposal which reduces the rate of all sources this scheme allows connections with low ACR to increase their rate and hence ensures a fair allocation of the bandwidth. (b) Explicit rate indication: This algorithm is based on determining the advertised rate (A-rate) as was presented by Anna Charny in <ref> [15] </ref>. Based on that the fair rate a switch should advertise is the capacity available minus the capacity of the constrained VCs over the total number of VCs minus the number of constrained VCs. <p> It is of the form 1 2 n and is set to This implementation has proved to be accurate as well as simple. Using the MCRA eliminated the need for a VC table as was suggested by Anna Charny in <ref> [15] </ref>. The implementation of the fair share calculation algorithm presented here has to be seen only as one possible solution that proved to work fine.
Reference: [16] <author> A. Roberts, </author> <title> "Enhanced PRCA proportional rate control algorithm," ATM Forum 94-0735R1, </title> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: As the A-rate is actually the average rate of all connections that face no constraints on any part of their paths <ref> [16] </ref> presents a heuristic scheme that tries to estimate the fair bandwidth share via an estimation of the exponential average. To ensure that the estimated average and ACR converge to some stable value under all conditions several multiplier factors were added to the algorithm to force convergence. <p> To ensure that the estimated average and ACR converge to some stable value under all conditions several multiplier factors were added to the algorithm to force convergence. The following pseudo code was taken completely from <ref> [16] </ref> and presents the basis for the switch behavior used in various simulation studies [17]. 3.3. END-TO-END RATE CONTROL 23 Initialization MACR=IMR !Initialize MACR to a small rate. <p> reduction ELSE IF ACR&gt;MACR*DPF !Select VC's for congestion marking IF mode=binary THEN CI=1 ! Set Congestion bit IF mode=explicit THEN ER= min (MACR*ERF, ER) ! Reduce rate The following explanation and initialization of the parameters that were used in the algorithm were, just like the pseudo code, taken completely from <ref> [16] </ref>. The values chosen for the parameters were determined from simulations and experiments done by L. <p> INTEGRATED PROPOSAL FOR ABR SERVICE CONGESTION CONTROL 33 3. For the EPRCA proposal the bandwidth loss due to management overhead is 2*Transmitted Cells Nrm where Nrm was set to 32 as recommended in <ref> [16] </ref>. Even though this is much higher than that for the other algorithms it has to be taken into consideration that the overall bandwidth gain with this proposal covers this loss and is thereby justifiable.
Reference: [17] <author> Y. Chang, N. Glomie, L. Benmohamed, and D. Su, </author> <title> "Simulation study on the new rate-based PRCA traffic management mechanism," </title> <type> ATM Forum 94-0809, </type> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: To ensure that the estimated average and ACR converge to some stable value under all conditions several multiplier factors were added to the algorithm to force convergence. The following pseudo code was taken completely from [16] and presents the basis for the switch behavior used in various simulation studies <ref> [17] </ref>. 3.3. END-TO-END RATE CONTROL 23 Initialization MACR=IMR !Initialize MACR to a small rate. <p> RM cell 32 Table 3.4: Simulation parameters for the EPRCA algorithm Whereas the last two algorithms only reached a network utilization of about 50% under the given condition of low buffer usage the EPRCA proposal provided high utilization, low buffer requirements and fair bandwidth allocation under various topologies, see also <ref> [17] </ref>. The high utilization during the steady state (around 97%) and the fair rate allocation can be clearly seen in Fig. 3.9. In the steady state the transit connection gets 49% of the utilized bandwidth and a fairness index of about 0.994 can thereby be reached.
Reference: [18] <author> A. W. Barnhart, </author> <title> "Explicit rate performance evaluations," </title> <type> ATM Forum 94-0983, </type> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: As the switch specifics will not be standardized by the ATM Forum the actual used algorithm can take any other shape that fulfills the fairness requirements, for example see <ref> [18] </ref>. Actually, this implementation is calculation intensive and would not be efficient in a real switch. The switch must update the MACR value every received forward RM cell and write the explicit rate in the backward RM cells. <p> The chosen buffer threshold for comparing the different algorithms was set to 50 cells. This value was used in other simulation studies <ref> [18] </ref> that we used for veryfing our simulation models. With this value EPRCA has worked very well achieving high utilization and a high fairness index for both persistent and TCP sources. On the other hand TCP showed a very oscillatory behavior.
Reference: [19] <author> C. Ikeda and M. Murata, </author> <title> "Transient state analysis and maximum buffer requirements for enhanced PRCA," </title> <type> ATM Forum 94-0910, </type> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Our version depends on the source code presented in September 1994 and until a final version is agreed on a true characterization of the behavior will not be possible. For a detailed analytical description of the buffer behavior and requirements of the EPRCA and the PRCA proposal see <ref> [19] </ref>. Management Overhead In order for the source end systems to regulate their rates they need congestion information from the network. These information are conveyed to the source through so called resource management cells.
Reference: [20] <author> D. Singer, D. Hunt, B. Buxton, J. Adams, and B. Fee, </author> <title> "An integrated proposal for ABR service congestion control," ATM Forum 94-0768R1, </title> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: As the cost issue is of main importance when introducing the ATM to the LAN community the more expensive NICs could represent a handicap to ATM in its race with advanced LAN technologies like fast-Ethernet. With this in mind Singer et al. <ref> [20] </ref> presented an integrated solution that proposed the following points: * To define a congestion control interface between subnetworks.
Reference: [21] <author> S. Keshav, </author> <title> "REAL: A network simulator," </title> <type> Tech. Rep. 88/472, </type> <institution> Department of Computer science, UC Berkeley, </institution> <year> 1988. </year>
Reference-contexts: Chapter 4 A TCP Simulator with PTOLEMY 4.1 Introduction Even though lots of TCP simulators and TCP traffic sources are already implemented in different programming languages, e.g., REAL <ref> [21] </ref>, the x-Kernel [22], tcplib [23] , we have decided to implement our own simulator. Building a simulator with PTOLEMY [24] would not just ease the integration and handling of the simulator as a TCP traffic source in general, but would add a very useful galaxy to PTOLEMY as well.
Reference: [22] <author> N. C. Hutchinson and L. L. Peterson, </author> <title> "The x-Kernel: An architecture for implementing network protocols," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 17, </volume> <pages> pp. 64-76, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Chapter 4 A TCP Simulator with PTOLEMY 4.1 Introduction Even though lots of TCP simulators and TCP traffic sources are already implemented in different programming languages, e.g., REAL [21], the x-Kernel <ref> [22] </ref>, tcplib [23] , we have decided to implement our own simulator. Building a simulator with PTOLEMY [24] would not just ease the integration and handling of the simulator as a TCP traffic source in general, but would add a very useful galaxy to PTOLEMY as well.
Reference: [23] <author> P. B. Danzig and S. Jamin, "tcplib: </author> <title> A library of TCP internetwork traffic characteristics," </title> <type> Tech. Rep. </type> <institution> USC-CS-91-495, Computer Science Department, University of Southern California, </institution> <address> Los Angeles, California, </address> <year> 1991. </year>
Reference-contexts: Chapter 4 A TCP Simulator with PTOLEMY 4.1 Introduction Even though lots of TCP simulators and TCP traffic sources are already implemented in different programming languages, e.g., REAL [21], the x-Kernel [22], tcplib <ref> [23] </ref> , we have decided to implement our own simulator. Building a simulator with PTOLEMY [24] would not just ease the integration and handling of the simulator as a TCP traffic source in general, but would add a very useful galaxy to PTOLEMY as well.
Reference: [24] <author> A. Y. Lao, </author> <title> "Heterogeneous cell-relay network simulation and performance analysis with PTOLEMY," </title> <type> Tech. Rep. </type> <institution> UCB/ERL M94/8, Electronics Research Laboratory, UC Berkeley, </institution> <year> 1993. </year>
Reference-contexts: Chapter 4 A TCP Simulator with PTOLEMY 4.1 Introduction Even though lots of TCP simulators and TCP traffic sources are already implemented in different programming languages, e.g., REAL [21], the x-Kernel [22], tcplib [23] , we have decided to implement our own simulator. Building a simulator with PTOLEMY <ref> [24] </ref> would not just ease the integration and handling of the simulator as a TCP traffic source in general, but would add a very useful galaxy to PTOLEMY as well. For this reason two versions of a 4.3BSD Tahoe based TCP simulator were implemented.
Reference: [25] <author> V. Jacobson, </author> <title> "Congestion avoidance and control," </title> <journal> ACM Computer Communication Review, </journal> <volume> vol. 18, </volume> <pages> pp. 314-329, </pages> <month> Aug. </month> <year> 1988. </year> <booktitle> Proceedings of the Sigcomm '88 Symposium in Stanford, </booktitle> <address> CA, </address> <month> August, </month> <year> 1988. </year>
Reference-contexts: For this reason two versions of a 4.3BSD Tahoe based TCP simulator were implemented. The basic version provides the user with the usual TCP-window based control mechanism, as well as the slow start, congestion avoidance and round trip estimation algorithms suggested by Jacobson <ref> [25] </ref>. In the enhanced version of the simulator the fast retransmission algorithm was implemented as well. It will be shown that with this enhancement the throughput and performance of the protocol increases considerably since the number of retransmitted packets decreases. <p> The relation between the congestion avoidance and the slow start will be explained when dis cussing the retransmission scheme used in TCP. 38 CHAPTER 4. A TCP SIMULATOR WITH PTOLEMY 5. Exponential backoff and round trip estimation: Jacobson described in <ref> [25] </ref> a method, in which the round trip time (RTT) estimation is based on calculating both the mean and the variance of the measured RTT. To simplify the calculation Jacobson suggested that only the mean deviation should be used instead of the the standard deviation. <p> While such a scheme avoids discarding packets its effectiveness is still to be investigated. Also, the integration of this scheme with the EPRCA should be considered in further studies. Appendix A Round Trip Time Estimation A.1 Introduction In <ref> [25] </ref> Jacobson describes an algorithm for estimating the round trip time using the average and the mean deviation of the measured round trip time samples.
Reference: [26] <author> S. Shenker, L. Zhang, and D. D. Clark, </author> <title> "Some observations on the dynamics of a congestion control algorithm," </title> <journal> ACM Computer Communication Review, </journal> <pages> pp. 30-39, </pages> <month> Oct. </month> <year> 1990. </year> <note> BIBLIOGRAPHY 99 </note>
Reference-contexts: It will be shown that with this enhancement the throughput and performance of the protocol increases considerably since the number of retransmitted packets decreases. To verify the simulator and to compare the two versions a network configuration has been chosen that has already been used in another study <ref> [26] </ref>. Whereas the results obtained from both simulators show a great similarity to the ones reached in [26], the basic version shows a reduced efficiency. 4.2 A Brief History of TCP Before moving to the actual simulator implementation and the description of its different features this section summarizes the basic development <p> To verify the simulator and to compare the two versions a network configuration has been chosen that has already been used in another study <ref> [26] </ref>. Whereas the results obtained from both simulators show a great similarity to the ones reached in [26], the basic version shows a reduced efficiency. 4.2 A Brief History of TCP Before moving to the actual simulator implementation and the description of its different features this section summarizes the basic development of TCP. 1. 4.2BSD (1983): The first widely available release of TCP/IP based on RFC 793 [27]. <p> Here, a network model has been chosen that has already been tested in another study <ref> [26] </ref>. This reduced the verification requirements to a simple comparison between the results obtained in [26] and the ones obtained when using the simulator to build up the same configuration. 4.6.1 Network Simulator The chosen topology is fairly straightforward as can be seen from Fig. 4.3. <p> Here, a network model has been chosen that has already been tested in another study <ref> [26] </ref>. This reduced the verification requirements to a simple comparison between the results obtained in [26] and the ones obtained when using the simulator to build up the same configuration. 4.6.1 Network Simulator The chosen topology is fairly straightforward as can be seen from Fig. 4.3. A bottleneck switch with a 20 packet buffer is connected to the source and the receiver. <p> SIMULATOR VERIFICATION 43 4.6.2 Simulation Results As can be seen from Fig. 4.4 the graph showing the length of the switch buffer obtained with the enhanced version shows a great resemblance to that presented in <ref> [26] </ref>. A typical cycle consists of a short phase of exponential growth and a longer period in which the congestion avoidance algorithm is used to increase the congestion window. The exponential phase can't unfortunately be shown clear enough as is was too short. <p> In an enhanced version of the simulator the round trip time estimation algorithm as it was presented in [2] was additionally implemented. A.2 Testing Environment Here a network model has been chosen that was already tested in another study <ref> [26] </ref>. It consists of a bottleneck switch with a 20 packet buffer that is connected to the source and the receiver. The bottleneck transmission between the switch and the receiver has a bandwidth of 50 kbps and a propagation delay of t (here, it was set to 10 msec).
Reference: [27] <author> J. Postel, </author> <title> "Transmission control protocol," Request for Comments (Standard) STD 7, </title> <type> RFC 793, </type> <institution> Internet Engineering Task Force, </institution> <month> Sept. </month> <year> 1981. </year>
Reference-contexts: [26], the basic version shows a reduced efficiency. 4.2 A Brief History of TCP Before moving to the actual simulator implementation and the description of its different features this section summarizes the basic development of TCP. 1. 4.2BSD (1983): The first widely available release of TCP/IP based on RFC 793 <ref> [27] </ref>. 2. 4.3BSD Tahoe (1988): The version implemented here. The main protocol improvements introduced were slow start and congestion avoidance algorithms. 3. 4.3BSD Reno (1990): This implementation increased the efficiency of the protocol through a better implementation.
Reference: [28] <author> V. Jacobson, </author> <title> "Berkeley TCP evolution from 4.3-Tahoe to 4.3-Reno," </title> <booktitle> in Proceedings of the Eighteenth Internet Engineering Task Force, </booktitle> <institution> (University of British Columbia, </institution> <address> Vancouver), </address> <pages> pp. 363-366, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: A TCP SIMULATOR WITH PTOLEMY alter the protocol itself. Other changes aimed at reducing the spurious retransmissions through invoking slow start on idle links and a better accounting for the variance of the round trip time in the round trip time estimation, for more information see <ref> [28] </ref>. 4. 4.4BSD (1993): To allow TCP to perform well over the so called long fat pipes, i.e., links with large bandwidth-delay products, a few new options had to be included.
Reference: [29] <author> L. S. Brakmo and S. W. O'Malley, </author> <title> "TCP Vegas: New techniques for congestion detection and avoidance," </title> <booktitle> in SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <address> (London, United Kingdom), </address> <pages> pp. 34-35, </pages> <publisher> ACM, </publisher> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: These options allow for window size scaling, protection against sequence number wrap up and a better round trip time estimation, as described in RFC 1323 [2]. 5. 4.3BSD Vegas (1994): Here an improved round trip time estimation algorithm is used and the congestion avoidance and slow start mechanisms were modified <ref> [29] </ref>. 4.3 4.3BSD Tahoe TCP Congestion Control Algorithm The Reno TCP release was mainly intended to improve the performance of the hosts. This was done without altering the protocol or adding any new algorithms to the Tahoe version.
Reference: [30] <author> W. R. Stevens, </author> <title> TCP/IP illustrated: </title> <booktitle> the protocols, </booktitle> <volume> vol. </volume> <pages> 1. </pages> <address> Reading, Massachusetts: </address> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: In what remains of this section the implemented algorithms are briefly described. For more detailed information, see <ref> [30] </ref>. 1. Sliding window: On the arrival of a data packet at the receiver, an acknowledgment for the last correctly received byte, is generated. The acknowledgment also contains information about the amount of data the source could still transmit without congesting the receiver, the so called advertised window.
Reference: [31] <author> P. Karn and C. Partridge, </author> <title> "Improving round-trip time estimates in reliable transport protocols," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 9, </volume> <pages> pp. 365-373, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: As this would introduce a certain error to the estimation of the round trip time, Karn <ref> [31] </ref> specifies that when the acknowledgment for a retransmitted packet arrives the RTT estimators should not be updated. 6.
Reference: [32] <author> S. Floyd, </author> <title> "Simulator tests." </title> <note> submitted for publication, </note> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: The effects of introducing the fast retransmission algorithm were already investigated in 44 CHAPTER 4. A TCP SIMULATOR WITH PTOLEMY various other studies, see <ref> [32] </ref>, so there is no need to go much deeper into this subject. Still, a simple comparison between the two versions of the simulator should not just restate what we already know, but also confirm the correctness of the implementation.
Reference: [33] <author> W. R. Stevens, </author> <title> TCP/IP illustrated: </title> <booktitle> the implementation, </booktitle> <volume> vol. </volume> <pages> 2. </pages> <address> Reading, Massachusetts: </address> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Parameter value Distance between two switches 1000 km Distance between host and switch 0.4 km Link delay 5 sec/km Link rate 140 Mbps 2 Table 5.1: Parameters for testing TCP 1 For Example the Net/3 implementation restricts the maximum allowed send buffer to 262144 bytes <ref> [33] </ref>. 2 Note that we have chosen the user data rate as the comparison criteria and not the physical link rate. Compared to the throughput of plain TCP, running TCP over ATM incurs a throughput reduction of 5/53, due to the addition of the ATM cell headers. <p> When there is no more space in the send buffer the sending process is put to sleep. As the send buffer also contains a copy of the sent but not yet acknowledged data, the maximum transmission window that a source can have is as large as the send buffer <ref> [33] </ref>. When a source has finally increased its transmission window up to the maximum size the send buffer can be seen as divided into two parts: 1. A copy of the maximum amount of data that the source can have on the connection itself, i.e., its bandwidth-delay product. 2.
Reference: [34] <author> T. Dwight, </author> <title> "Guidelines for the simulation of TCP/IP over ATM," </title> <type> ATM Forum 95-0077, </type> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: The parameters of the network are listed in Tab. 5.1. We set the maximum segment size (MSS) of the TCP packets so that segmenting the packets resulted in exactly 10 ATM cells, i.e., 480 bytes (this sounded like a nice round number). The sources used were persistent ones <ref> [34] </ref> that generate packets at the link rate, as long as they are allowed to do so by the window mechanism.
Reference: [35] <author> S. Floyd and V. Jacobson, </author> <title> "Random early detection gateways for congestion avoidance," </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> vol. 1, </volume> <pages> pp. 397-413, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: So, in order to avoid this situation and to provide a more accurate picture of the performance of TCP we enhanced the switches by the early random drop (RED) strategy proposed by Floyd and Jacobson in <ref> [35] </ref>. In this scheme the switch detects incipient congestion by computing the average queue size 3 . When the average queue size exceeds a preset minimum threshold (Min th )the switch drops each incoming packet with some probability.
Reference: [36] <author> C. Villamizar and C. Song, </author> <title> "High performance TCP in ANSNET," technical note, Advanced Network & Services, </title> <publisher> Inc. </publisher> <address> (ANSNET), </address> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: For the TCP Reno version with the fast recovery and fast retransmission algorithms the transmission window is just reduced in half. However, if congestion causes a number of packets to be dropped, fast retransmission and fast recovery can be triggered a number of times in one RTT <ref> [36] </ref>. This can cause the window to be reduced in half a number of times and then grow slowly. This lead to the different shapes of the oscillations between the bit marking schemes and TCP. 5.3. <p> In both cases these oscillations were caused by the inadequate queuing 67 68 CHAPTER 6. SUMMARY AND FUTURE WORK capacity at the intermediate switches. This lead to congestion collapse <ref> [36] </ref> and an overall low throughput. The congestion collapse can best be seen in the queue length changes of the intermediate switches.
Reference: [37] <author> L. Hongqing, T. Hong-Yi, and S. Kai-Yeung, </author> <title> "IPX and TCP performance over ATM networks with cell loss," </title> <type> ATM Forum 94-0151, </type> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Then we move on to investigate ATM networks that provide some support through different packet discard mechanisms. Finally, the behavior of TCP running over a rate controlled ATM network is investigated. 56 CHAPTER 5. TCP OVER ATM 5.3.1 TCP over Plain ATM Various simulation and analytical studies <ref> [37, 38] </ref> have shown that through the segmentation of TCP packets into ATM cells the packet loss probability increases considerably. The throughput decreases nearly linearly with increasing the packet sizes for random and uncorrelated cell losses.
Reference: [38] <author> T. Chen, L. Jones, S. Liu, and V. K. Samalam, </author> <title> "Effect of ATM cell loss rate on TCP packet loss," </title> <type> ATM Forum 94-0914, </type> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Then we move on to investigate ATM networks that provide some support through different packet discard mechanisms. Finally, the behavior of TCP running over a rate controlled ATM network is investigated. 56 CHAPTER 5. TCP OVER ATM 5.3.1 TCP over Plain ATM Various simulation and analytical studies <ref> [37, 38] </ref> have shown that through the segmentation of TCP packets into ATM cells the packet loss probability increases considerably. The throughput decreases nearly linearly with increasing the packet sizes for random and uncorrelated cell losses. <p> Early packet discard (EPD): With the EPD algorithm the switch drops entire packets prior to buffer overflow. This strategy prevents the congested link from transmitting useless cells. Chen et. al <ref> [38] </ref> show with the help of theoretical models that for bursty 5.3. INTEGRATION OF TCP AND ATM 57 correlated cell losses the packet discard probability is lower than that for random cell losses by a factor of the mean burst length of the lost cells.
Reference: [39] <author> C. Fang, H. Chen, and J. Hutchins, </author> <title> "Simulation analysis of TCP performance in congested ATM LAN using DEC's flow control scheme and two selective cell-drop schemes.," </title> <type> ATM Forum 94-0119, </type> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: As already mentioned due to the segmentation of the packets into cells we would expect the throughput to decrease for TCP over plain ATM. The results depicted in Tab. 5.3 confirm this expectation. However, the reduction in throughput is not as severe as the results obtained in <ref> [39, 40] </ref>. While in [40] the throughput was reduced from 90% to 34% of the available link bandwidth in our example only a 7% reduction was noticed. This can, however, be explained by the small maximum segment size chosen here. <p> ATM with EPD TCP over EPRCA 0.67 0.62 0.63 0.99 Table 5.5: A comparison of the achieved fairness index for the different presented schemes The fairness results depicted in Tab. 5.5, except for TCP over EPRCA, are actually very bad compared with the results achieved in similar studies such as <ref> [39, 43, 40] </ref>. For example Kalampoukas and Varma [43] present results that suggest that TCP with EPD or even TCP over plain ATM reach a fairness index of nearly 0.9. However, while a very similar network topology to the one used in our studies was tested different parameters were used. <p> On the other hand this restates what we mentioned about the dependency of TCP's performance on the network parameters. Floyd [40] and Chien <ref> [39] </ref> achieve fairness indexes of nearly 1 as well. This stems, however, from the topology used for testing the different mechanisms. In their test network all connections 5.3. INTEGRATION OF TCP AND ATM 61 had the same parameters and the same round trip time. <p> Plain TCP TCP over ATM TCP over ATM with EPD TCP over EPRCA 70% 64% 71% 97% 4 Table 5.6: A comparison of the achieved link bandwidth utilization for the different presented schemes Comparing the results achieved here with those in <ref> [39, 43, 40] </ref> reveals again substantial differences. While TCP's throughput in our study shows an oscillating behavior the simulation results obtained by Romanow and Floyd [40], Chien [39] and Varma [43] suggest that TCP takes on a very stable behavior during the steady state. <p> While TCP's throughput in our study shows an oscillating behavior the simulation results obtained by Romanow and Floyd [40], Chien <ref> [39] </ref> and Varma [43] suggest that TCP takes on a very stable behavior during the steady state. However, those studies used a maximum window size of 64 kbytes and packet lengths of 8 kbytes.
Reference: [40] <author> A. Romanow and S. Floyd, </author> <title> "Dynamics of TCP traffic over ATM networks." </title> <note> see also 6th IEEE LAN/MAN Workshop, </note> <year> 1993. </year>
Reference-contexts: As already mentioned due to the segmentation of the packets into cells we would expect the throughput to decrease for TCP over plain ATM. The results depicted in Tab. 5.3 confirm this expectation. However, the reduction in throughput is not as severe as the results obtained in <ref> [39, 40] </ref>. While in [40] the throughput was reduced from 90% to 34% of the available link bandwidth in our example only a 7% reduction was noticed. This can, however, be explained by the small maximum segment size chosen here. <p> The results depicted in Tab. 5.3 confirm this expectation. However, the reduction in throughput is not as severe as the results obtained in [39, 40]. While in <ref> [40] </ref> the throughput was reduced from 90% to 34% of the available link bandwidth in our example only a 7% reduction was noticed. This can, however, be explained by the small maximum segment size chosen here. While an MSS of 8 kbytes was used in [40] the TCP packets used in <p> While in <ref> [40] </ref> the throughput was reduced from 90% to 34% of the available link bandwidth in our example only a 7% reduction was noticed. This can, however, be explained by the small maximum segment size chosen here. While an MSS of 8 kbytes was used in [40] the TCP packets used in our simulations were divided into only 10 cells. So, as the cell drop was also bursty in nature no severe throughput reduction could be expected. <p> This implies that it would be advantageous if those cells that are going to be discarded at the destination anyway, could be dropped directly in the network itself. Romanow and Floyd present in <ref> [40] </ref> two methods that achieve this goal and thereby improve the throughput considerably. 1. Partial packet discard (PPD): After discarding a cell from a packet the switch discards all subsequent arriving cells that belong to that packet. Thereby, full utilization of the available buffer space can be ensured. <p> ATM with EPD TCP over EPRCA 0.67 0.62 0.63 0.99 Table 5.5: A comparison of the achieved fairness index for the different presented schemes The fairness results depicted in Tab. 5.5, except for TCP over EPRCA, are actually very bad compared with the results achieved in similar studies such as <ref> [39, 43, 40] </ref>. For example Kalampoukas and Varma [43] present results that suggest that TCP with EPD or even TCP over plain ATM reach a fairness index of nearly 0.9. However, while a very similar network topology to the one used in our studies was tested different parameters were used. <p> On the other hand this restates what we mentioned about the dependency of TCP's performance on the network parameters. Floyd <ref> [40] </ref> and Chien [39] achieve fairness indexes of nearly 1 as well. This stems, however, from the topology used for testing the different mechanisms. In their test network all connections 5.3. INTEGRATION OF TCP AND ATM 61 had the same parameters and the same round trip time. <p> Plain TCP TCP over ATM TCP over ATM with EPD TCP over EPRCA 70% 64% 71% 97% 4 Table 5.6: A comparison of the achieved link bandwidth utilization for the different presented schemes Comparing the results achieved here with those in <ref> [39, 43, 40] </ref> reveals again substantial differences. While TCP's throughput in our study shows an oscillating behavior the simulation results obtained by Romanow and Floyd [40], Chien [39] and Varma [43] suggest that TCP takes on a very stable behavior during the steady state. <p> While TCP's throughput in our study shows an oscillating behavior the simulation results obtained by Romanow and Floyd <ref> [40] </ref>, Chien [39] and Varma [43] suggest that TCP takes on a very stable behavior during the steady state. However, those studies used a maximum window size of 64 kbytes and packet lengths of 8 kbytes.
Reference: [41] <author> J. Heinanen, </author> <title> "Signalling support for frame discard," </title> <type> ATM Forum 95-0059, </type> <month> Feb. </month> <year> 1995. </year> <title> [42] "FORE systems announces FORETHOUGHT bandwidth management." </title> <publisher> Press Information, </publisher> <month> Feb. </month> <year> 1995. </year> <note> 100 BIBLIOGRAPHY </note>
Reference-contexts: At the switches the possible options for each VC are: 1. treat user data as cells 2. allow treatment of user data as frames. Heinanen describes in <ref> [41] </ref> the signaling information needed to support these options and some ATM switch manufacturers have already implemented these algorithms [42]. The frame boundaries are assumed to be indicated by the SDU-type value in the payload type field of the ATM cell header.
Reference: [43] <author> L. Kalampoukas and A. Varma, </author> <title> "Performance of TCP over multi-hop ATM networks: A comparative study of ATM-Layer congestion control schemes," </title> <type> Tech. Rep. </type> <institution> UCSC-CRL-95-13, University of California, </institution> <address> Santa Cruz, Santa Cruz, CA 95064 USA, </address> <year> 1995. </year>
Reference-contexts: It is interesting to note that with the EPD scheme the bandwidth distribution is slightly better than that for plain TCP. Varma and Kalampoukas <ref> [43] </ref> explain this like follows: In an ATM network, each source transmits a TCP packet as a burst of ATM cells. As the cells travel through the switches, the cells are interleaved with the cells belonging to packets from other connections. <p> ATM with EPD TCP over EPRCA 0.67 0.62 0.63 0.99 Table 5.5: A comparison of the achieved fairness index for the different presented schemes The fairness results depicted in Tab. 5.5, except for TCP over EPRCA, are actually very bad compared with the results achieved in similar studies such as <ref> [39, 43, 40] </ref>. For example Kalampoukas and Varma [43] present results that suggest that TCP with EPD or even TCP over plain ATM reach a fairness index of nearly 0.9. However, while a very similar network topology to the one used in our studies was tested different parameters were used. <p> For example Kalampoukas and Varma <ref> [43] </ref> present results that suggest that TCP with EPD or even TCP over plain ATM reach a fairness index of nearly 0.9. However, while a very similar network topology to the one used in our studies was tested different parameters were used. <p> Plain TCP TCP over ATM TCP over ATM with EPD TCP over EPRCA 70% 64% 71% 97% 4 Table 5.6: A comparison of the achieved link bandwidth utilization for the different presented schemes Comparing the results achieved here with those in <ref> [39, 43, 40] </ref> reveals again substantial differences. While TCP's throughput in our study shows an oscillating behavior the simulation results obtained by Romanow and Floyd [40], Chien [39] and Varma [43] suggest that TCP takes on a very stable behavior during the steady state. <p> While TCP's throughput in our study shows an oscillating behavior the simulation results obtained by Romanow and Floyd [40], Chien [39] and Varma <ref> [43] </ref> suggest that TCP takes on a very stable behavior during the steady state. However, those studies used a maximum window size of 64 kbytes and packet lengths of 8 kbytes.
Reference: [44] <author> J. Crowcroft, </author> <title> "Why lossy internetworking and lossless ABR ATM services do not go together," Research Note RN/94/21, </title> <address> University College London, London, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: As the introduction of ATM will not cause other existing networks to just disappear, a more realistic model should consider the emergence of ATM provision in only some parts of TCP/IP based networks like the Internet <ref> [44] </ref>. In this section we make a first approach to investigating such a model. Fig. 5.11 presents a heterogeneous network topology consisting of an ATM cloud that connects various TCP/IP based subnetworks.
Reference: [45] <author> C. Fang and H. Chen, </author> <title> "TCP performance simulations of enhanced PRCA scheme," </title> <type> ATM Forum 94-0932, </type> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The results presented for the EPRCA scheme for both persistent sources and TCP hosts suggest its superiority over the bit marking schemes and packet drop mechanisms. With a utilization of around 100% and a fairness index of nearly 1, EPRCA sounds like the optimal solution. However, other simulation studies <ref> [45] </ref> report that with more elaborate testing environments, more connections and bi-directional traffic link underutilization, lower fairness indexes and higher buffer requirements were observed. While the performance was still substantially better than that of the bit marking schemes the results show that EPRCA does not just solve every problem.
Reference: [46] <author> S. Floyd, </author> <title> "TCP and explicit congestion notification." </title> <note> A PostScript version of this document is available from ftp://ftp.ee.lbl.gov/papers/tcp ecn.4.ps.Z, </note> <year> 1994. </year>
Reference-contexts: It might be advisable to think of some scheme that encourages TCP hosts to reduce their sending windows without actually dropping packets. Thereby, congestion can be handled in advance and not when it is already too late as it is done now. Floyd <ref> [46] </ref> presents a bit marking based scheme for congestion control with TCP. While such a scheme avoids discarding packets its effectiveness is still to be investigated. Also, the integration of this scheme with the EPRCA should be considered in further studies.
Reference: [47] <author> C. McGillem and G. Cooper, </author> <title> Continuous and Discrete Signal and System Analysis. </title> <publisher> Holt, Rinehart and Winston; New York, </publisher> <address> NY, </address> <year> 1984. </year> <note> [48] "The Almagest." A PostScript version of this document is available from ftp://ptolemy.Berkely.EDU, </note> <year> 1994. </year>
Reference-contexts: This lower sampling frequency introduces aliasing effects as it violates the Nyquist criteria which states that a signal can be uniquely represented by a set of samples taken at twice the frequency of the signal itself <ref> [47] </ref>. These effects are still tolerable in a narrow band environment. With a window of 8 packets the sample rate is 1/8 the data rate, which is less than an order of magnitude different.
References-found: 46

