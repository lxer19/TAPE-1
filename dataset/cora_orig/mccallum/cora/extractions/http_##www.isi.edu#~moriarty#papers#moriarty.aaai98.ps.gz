URL: http://www.isi.edu/~moriarty/papers/moriarty.aaai98.ps.gz
Refering-URL: http://www.isi.edu/~moriarty/mypapers.html
Root-URL: http://www.isi.edu
Email: moriarty@isi.edu  langley@rtna.daimlerbenz.com  
Title: Learning Cooperative Lane Selection Strategies for Highways  
Author: David E. Moriarty Pat Langley 
Address: 4676 Admiralty Way Marina Del Rey, CA 90292  1510 Page Mill Road Palo Alto, CA 94304  
Affiliation: Information Sciences Institute University of Southern California  Adaptive Systems Group Daimler-Benz Research Technology Center  
Note: In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98). Madison, WI  
Abstract: This paper presents a novel approach to traffic management by coordinating driver behaviors. Current traffic management systems do not consider lane organization of the cars and only affect traffic flows by controlling traffic signals or ramp meters. However, drivers should be able to increase traffic throughput and more consistently maintain desired speeds by selecting lanes intelligently. We pose the problem of intelligent lane selection as a challenging and potentially rewarding problem for artificial intelligence, and we propose a methodology that uses supervised and reinforcement learning to form distributed control strategies. Initial results are promising and demonstrate that intelligent lane selection can better approximate desired speeds and reduce the total number of lane changes. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Carrara, M., & Morello, E. </author> <title> Advanced control strategies and methods for motorway of the future. In The Drive Project DOMINC: New Concepts and Research Under Way. </title>
Reference: <author> Dietterich, T. G. </author> <year> (1990). </year> <title> Exploratory research in machine learning. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 5-9. </pages>
Reference: <author> Eskafi, F. </author> <year> (1996). </year> <title> Modeling and Simulation of the Automated Highway System. </title> <type> Ph.D. thesis, </type> <institution> Department of EECS, University of California, Berkeley. </institution>
Reference-contexts: Removing these assumptions unnecessarily complicates the model, which creates unacceptable run times for exploratory research. In future work, we will expand our experiments to a more realistic simulator such as SmartPATH <ref> (Eskafi, 1996) </ref>. During training, the learning system uses the traffic simulator to evaluate candidate lane-selection strategies. Each evaluation or trial lasts 400 simulated seconds and begins with a random dispersement of 200 cars over three lanes on the 3.3 mile roadway.
Reference: <author> Forbes, J., Huang, T., Kanazawa, K., & Russell, S. </author> <year> (1995). </year> <title> The BATmobile: Towards a Bayesian automated taxi. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence Montreal, </booktitle> <address> CA. </address>
Reference-contexts: Finally, the Bayesian Automated Taxi (BAT) project, an attempt to build a fully automated vehicle that can drive in normal traffic <ref> (Forbes, Huang, Kanazawa, & Russell, 1995) </ref>, will eventually contain a module for lane selection. Each of these systems were designed to maximize the performance of a single vehicle and do not form cooperative controllers. Our approach is directed at the global traffic management problem, where cooperation is important.
Reference: <author> Gilmore, J. F., Elibiary, K. J., & Forbes, H. C. </author> <year> (1994). </year> <title> Knowledge-based advanced traffic management system. </title> <booktitle> In Proceedings of IVHS America Atlanta, </booktitle> <address> GA. </address>
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: SANE maintains a population of possible strategies, evaluates the goodness of each from its performance in the domain, and uses an evolutionary algorithm to generate new strategies. The evolutionary algorithm modifies the strategies through genetic operators like selection, crossover, and mutation <ref> (Goldberg, 1989) </ref>. SANE represents its decision strategies as artificial neural networks that form a direct mapping from sensors to decisions and provide effective generalization over the state space.
Reference: <author> Kagolanu, K., Fink, R., Smartt, H., Powell, R., & Lar-son, E. </author> <year> (1995). </year> <title> An intelligent traffic controller. </title> <booktitle> In Proceedings of the Second World Congress on Intelligent Transport Systems, </booktitle> <pages> pp. </pages> <address> 259-264 Yoko-hama, Japan. </address>
Reference: <author> McCallum, A. K. </author> <year> (1996). </year> <title> Learning to use selective attention and short-term memory in sequential tasks. </title> <booktitle> In Proceedings of Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pp. </pages> <address> 315-324 Cape Cod, MA. </address>
Reference: <author> Moriarty, D. E. </author> <year> (1997). </year> <title> Symbiotic Evolution of Neural Networks in Sequential Decision Tasks. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Sciences, The University of Texas at Austin. </institution>
Reference: <author> Moriarty, D. E., & Langley, P. </author> <year> (1997). </year> <title> Automobile traffic management through intelligent lane selection: A distributed, machine learning approach. </title> <type> Tech. rep. 98-2, </type> <institution> Daimler-Benz Research & Technology Center, </institution> <address> Palo Alto, CA. </address>
Reference: <author> Moriarty, D. E., & Miikkulainen, R. </author> <year> (1996). </year> <title> Efficient reinforcement learning through symbiotic evolution. </title> <journal> Machine Learning, </journal> <volume> 22, </volume> <pages> 11-32. </pages>
Reference: <author> Pooran, F. J., Tarnoff, P. J., & Kalaputapu, R. </author> <year> (1996). </year> <title> RT-TRACS: </title> <booktitle> Development of the real-time control logic. In Proceedings of the 1996 Annual Meeting of ITS America, </booktitle> <pages> pp. </pages> <address> 422-430 Houston, TX. </address>
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., & McClel-land, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, </booktitle> <pages> pp. 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: These pairs denote examples of good behavior that can be fed to a supervised learning method to form initial strategies. Since SANE's strategies are represented as neural networks, the population seeder employs the backpropagation algorithm <ref> (Rumelhart, Hinton, & Williams, 1986) </ref> to train the networks over the training examples. To maintain diversity within the initial population of neural networks and not overly bias SANE toward the rules of thumb, only a subset of the networks are seeded using the default knowledge.
Reference: <author> Sukthankar, R., Baluja, S., & Hancock, J. </author> <year> (1997). </year> <title> Evolving an intelligent vehicle for tactical reasoning in traffic. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation. </booktitle>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: A negative training signal in the previous example would generate a target output of (1.0, 0.0, 1.0), and the resulting network would be less likely to choose change left in similar situations. The learning strategy is somewhat similar to the temporal difference methods for reinforcement learning <ref> (Sutton, 1988) </ref>, in that updates are based on the performance differences over successive time periods. However, temporal difference methods treat performance differences as prediction errors from which they can learn to predict future rewards. Our local learning component uses the differences to determine whether to reinforce or penalize specific decisions.
References-found: 15

